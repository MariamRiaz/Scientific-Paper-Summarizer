0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.",text,0,[0]
"Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision.",1 Introduction,0,[0]
"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence.",1 Introduction,0,[0]
"That is, how to capture the
1https://www.github.com/ facebookresearch/InferSent
relationships among multiple words and phrases in a single vector remains an question to be solved.
",1 Introduction,0,[0]
"In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.",1 Introduction,0,[0]
"Two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained.",1 Introduction,0,[0]
"Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought (Kiros et al., 2015) or FastSent (Hill et al., 2016).",1 Introduction,0,[0]
"Here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the ImageNet (Deng et al., 2009) before being transferred.",1 Introduction,0,[0]
"We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.",1 Introduction,0,[0]
"We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.
",1 Introduction,0,[0]
"Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.",1 Introduction,0,[0]
"Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes.",1 Introduction,0,[0]
"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings com-
670
pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.",1 Introduction,0,[0]
We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.,1 Introduction,0,[0]
"Transfer learning using supervised features has been successful in several computer vision applications (Razavian et al., 2014).",2 Related work,0,[0]
"Striking examples include face recognition (Taigman et al., 2014) and visual question answering (Antol et al., 2015), where image features trained on ImageNet (Deng et al., 2009) and word embeddings trained on large unsupervised corpora are combined.
",2 Related work,0,[0]
"In contrast, most approaches for sentence representation learning are unsupervised, arguably because the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence.",2 Related work,0,[0]
"Another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.",2 Related work,0,[0]
Learning models on large unsupervised task makes it harder for the model to specialize.,2 Related work,0,[0]
"Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder.",2 Related work,0,[0]
"They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect.
",2 Related work,0,[0]
"Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures.",2 Related work,0,[0]
"SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level.",2 Related work,0,[0]
"By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks.",2 Related work,0,[0]
"They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016).",2 Related work,0,[0]
Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality.,2 Related work,0,[0]
"In addition to unsupervised methods, they included supervised training in their comparison—namely, on
machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014).",2 Related work,0,[0]
"These models obtained significantly lower results compared to the unsupervised Skip-Thought approach.
",2 Related work,0,[0]
"Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015).",2 Related work,0,[0]
"The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017).",2 Related work,0,[0]
"To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders.",2 Related work,0,[0]
"As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data.",2 Related work,0,[0]
"This work combines two research directions, which we describe in what follows.",3 Approach,0,[0]
"First, we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task.",3 Approach,0,[0]
"We subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use.",3 Approach,0,[0]
"Specifically, we examine standard recurrent models such as LSTMs and GRUs, for which we investigate mean and maxpooling over the hidden representations; a selfattentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction.",3 Approach,0,[0]
"The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral.",3.1 The Natural Language Inference task,0,[0]
"It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.",3.1 The Natural Language Inference task,0,[0]
"We hypothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence
embeddings in a supervised way.",3.1 The Natural Language Inference task,0,[0]
"That is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features.
",3.1 The Natural Language Inference task,0,[0]
"Models can be trained on SNLI in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).
",3.1 The Natural Language Inference task,0,[0]
"Since our goal is to train a generic sentence encoder, we adopt the first setting.",3.1 The Natural Language Inference task,0,[0]
"As illustrated in Figure 1, a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v.",3.1 The Natural Language Inference task,0,[0]
"Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : (i) concatenation of the two representations (u, v); (ii) element-wise product u ∗ v; and (iii) absolute element-wise difference |u− v|.",3.1 The Natural Language Inference task,0,[0]
"The resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fullyconnected layers culminating in a softmax layer.",3.1 The Natural Language Inference task,0,[0]
"A wide variety of neural networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information.",3.2 Sentence encoder architectures,0,[0]
"We compare 7 different architectures: standard recurrent encoders with either Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), concatenation of last hidden states of forward and backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive network and hierarchical convolutional networks.",3.2 Sentence encoder architectures,0,[0]
"Our first, and simplest, encoders apply recurrent neural networks using either LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) modules, as in sequence to sequence encoders (Sutskever et al., 2014).",3.2.1 LSTM and GRU,0,[0]
"For a sequence of T words (w1, . . .",3.2.1 LSTM and GRU,0,[0]
", wT ), the network computes a set of T hidden representations h1, . . .",3.2.1 LSTM and GRU,0,[0]
", hT , with ht = −−−−→ LSTM(w1, . . .",3.2.1 LSTM and GRU,0,[0]
", wT ) (or using GRU units instead).",3.2.1 LSTM and GRU,0,[0]
"A sentence is represented by the last hidden vector, hT .
",3.2.1 LSTM and GRU,0,[0]
"We also consider a model BiGRU-last that concatenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.",3.2.1 LSTM and GRU,0,[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.2.2 BiLSTM with mean/max pooling,0,[0]
For t ∈,3.2.2 BiLSTM with mean/max pooling,0,[0]
"[1, . . .",3.2.2 BiLSTM with mean/max pooling,0,[0]
", T ], ht, is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions:
−→ ht = −−−−→ LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,0,[0]
", wT )←−
",3.2.2 BiLSTM with mean/max pooling,0,[0]
"ht = ←−−−− LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,0,[0]
", wT )",3.2.2 BiLSTM with mean/max pooling,0,[0]
ht =,3.2.2 BiLSTM with mean/max pooling,0,[0]
"[ −→ ht , ←− ht ]
We experiment with two ways of combining the varying number of {ht}t to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) (Collobert and Weston, 2008) or by considering the average of the representations (mean pooling).",3.2.2 BiLSTM with mean/max pooling,0,[0]
"The self-attentive sentence encoder (Liu et al., 2016; Lin et al., 2017) uses an attention mechanism over the hidden states of a BiLSTM to generate a representation u of an input sentence.",3.2.3 Self-attentive network,0,[0]
"The attention mechanism is defined as :
h̄i = tanh(Whi + bw)
αi = eh̄",3.2.3 Self-attentive network,0,[0]
T,3.2.3 Self-attentive network,0,[0]
"i uw∑
",3.2.3 Self-attentive network,0,[0]
"i e h̄Ti uw u = ∑
t
αihi
where {h1, . . .",3.2.3 Self-attentive network,0,[0]
", hT } are the output hidden vectors of a BiLSTM.",3.2.3 Self-attentive network,0,[0]
"These are fed to an affine transformation (W , bw) which outputs a set of keys (h̄1, . . .",3.2.3 Self-attentive network,0,[0]
", h̄T ).",3.2.3 Self-attentive network,0,[0]
The {αi} represent the score of similarity between the keys and a learned context query vector uw.,3.2.3 Self-attentive network,0,[0]
"These weights are used to produce the final representation u, which is a weighted linear combination of the hidden vectors.
",3.2.3 Self-attentive network,0,[0]
"Following Lin et al. (2017) we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task.",3.2.3 Self-attentive network,0,[0]
"Concretely, we have 4 context vectors u1w, u 2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u. Figure 3 illustrates this architecture.",3.2.3 Self-attentive network,0,[0]
"One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent (Zhao et al., 2015), which concatenates different representations of the sentences
at different level of abstractions.",3.2.4 Hierarchical ConvNet,0,[0]
"Inspired by this architecture, we introduce a faster version consisting of 4 convolutional layers.",3.2.4 Hierarchical ConvNet,0,[0]
"At every layer, a representation ui is computed by a max-pooling operation over the feature maps (see Figure 4).
",3.2.4 Hierarchical ConvNet,0,[0]
The final representation u =,3.2.4 Hierarchical ConvNet,0,[0]
"[u1, u2, u3, u4] concatenates representations at different levels of the input sentence.",3.2.4 Hierarchical ConvNet,0,[0]
The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.,3.2.4 Hierarchical ConvNet,0,[0]
"For all our models trained on SNLI, we use SGD with a learning rate of 0.1 and a weight decay of 0.99.",3.3 Training details,0,[0]
"At each epoch, we divide the learning rate by 5 if the dev accuracy decreases.",3.3 Training details,0,[0]
We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10−5.,3.3 Training details,0,[0]
"For the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units.",3.3 Training details,0,[0]
We use opensource GloVe vectors trained on Common Crawl 840B2 with 300 dimensions as fixed word embeddings.,3.3 Training details,0,[0]
"Our aim is to obtain general-purpose sentence embeddings that capture generic information that is
2https://nlp.stanford.edu/projects/ glove/
useful for a broad set of tasks.",4 Evaluation of sentence representations,0,[0]
"To evaluate the quality of these representations, we use them as features in 12 transfer tasks.",4 Evaluation of sentence representations,0,[0]
We present our sentence-embedding evaluation procedure in this section.,4 Evaluation of sentence representations,0,[0]
We constructed a sentence evaluation tool3 to automate evaluation on all the tasks mentioned in this paper.,4 Evaluation of sentence representations,0,[0]
"The tool uses Adam (Kingma and Ba, 2014) to fit a logistic regression classifier, with batch size 64.
",4 Evaluation of sentence representations,0,[0]
"Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).",4 Evaluation of sentence representations,1,"['Perhaps the most prominent work in the literature giving such guarantees is that of (Srinivas et al., 2010), who consider the cumulative regret: RT = T∑ t=1 ( max x f(x)− f(xt) ) , (1) where f is the function being optimized, and xt is the point chosen at time t. Under a Gaussian process (GP) prior and Gaussian noise, it is shown in (Srinivas et al., 2010) that an algorithm called Gaussian Process Upper Confidence Bound (GP-UCB) achieves a cumulative regret of the form RT = O ∗( √ TγT ), (2) where γT = maxx1,...,xT I(f ;y) (with function values f = (f(x1), .']"
We generate sentence vectors and train a logistic regression on top.,4 Evaluation of sentence representations,0,[0]
"A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited.",4 Evaluation of sentence representations,0,[0]
"We tune the L2 penalty of the logistic regression with grid-search on the validation set.
",4 Evaluation of sentence representations,0,[0]
Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R).,4 Evaluation of sentence representations,0,[0]
We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint representation.,4 Evaluation of sentence representations,0,[0]
"For semantic relatedness evaluation, we follow the approach of (Tai et al., 2015) and learn to predict the probability distribution of relatedness scores.",4 Evaluation of sentence representations,0,[0]
"We report Pearson correlation.
",4 Evaluation of sentence representations,0,[0]
"STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014).",4 Evaluation of sentence representations,0,[0]
"This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with
3https://www.github.com/ facebookresearch/SentEval
a similarity score between 0 and 5.",4 Evaluation of sentence representations,0,[0]
"These tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations.
",4 Evaluation of sentence representations,0,[0]
Paraphrase detection The Microsoft Research Paraphrase Corpus is composed of pairs of sentences which have been extracted from news sources on the Web.,4 Evaluation of sentence representations,0,[0]
Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship.,4 Evaluation of sentence representations,0,[0]
"We use the same approach as with SICK-E, except that our classifier has only 2 classes.
",4 Evaluation of sentence representations,0,[0]
"Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Hodosh et al., 2013; Lin et al., 2014).",4 Evaluation of sentence representations,0,[0]
"The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Caption Retrieval).",4 Evaluation of sentence representations,0,[0]
"We use a pairwise rankingloss Lcir(x, y): ∑ y ∑ k
max(0, α− s(V y, Ux) + s(V y, Uxk))",4 Evaluation of sentence representations,0,[0]
"+∑ x ∑ k′ max(0, α− s(Ux, V y) + s(Ux, V yk′))
",4 Evaluation of sentence representations,0,[0]
"where (x, y) consists of an image y with one of its associated captions x, (yk)k and (yk′)k′ are negative examples of the ranking loss, α is the margin and s corresponds to the cosine similarity.",4 Evaluation of sentence representations,0,[0]
U and V are learned linear transformations that project the caption x and the image y to the same embedding space.,4 Evaluation of sentence representations,0,[0]
We use a margin α = 0.2 and 30 contrastive terms.,4 Evaluation of sentence representations,0,[0]
"We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test.",4 Evaluation of sentence representations,0,[0]
"For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits.",4 Evaluation of sentence representations,0,[0]
"For fair comparison, we also report SkipThought results in our setting, using 2048-dimensional pretrained ResNet101 (He et al., 2016) with 113k training images.",4 Evaluation of sentence representations,0,[0]
"In this section, we refer to ”micro” and ”macro” averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples.",5 Empirical results,0,[0]
Model We observe in Table 3 that different models trained on the same NLI corpus lead to different transfer tasks results.,5.1 Architecture impact,0,[0]
The BiLSTM-4096 with the max-pooling operation performs best on both SNLI and transfer tasks.,5.1 Architecture impact,0,[0]
"Looking at the micro and macro averages, we see that it performs significantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet.
",5.1 Architecture impact,0,[0]
"Table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM-Mean for instance.
",5.1 Architecture impact,0,[0]
We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence.,5.1 Architecture impact,0,[0]
"For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks.",5.1 Architecture impact,0,[0]
"On the other hand, BiLSTM-Mean does not make sharp choices on which part of the sentence is more important than others.",5.1 Architecture impact,0,[0]
"The difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand.
",5.1 Architecture impact,0,[0]
"For a given model, the transfer quality is also sensitive to the optimization algorithm: when training with Adam instead of SGD, we observed that the BiLSTM-max converged faster on SNLI (5 epochs instead of 10), but obtained worse results on the transfer tasks, most likely because of the model and classifier’s increased capability to over-specialize on the training task.
",5.1 Architecture impact,0,[0]
"Embedding size Figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with
Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57",5.1 Architecture impact,0,[0]
ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0,5.1 Architecture impact,0,[0]
90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7,5.1 Architecture impact,0,[0]
.65/.64 fastText BOW†,5.1 Architecture impact,0,[0]
76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62,5.1 Architecture impact,0,[0]
GloVe BOW†,5.1 Architecture impact,0,[0]
78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9,5.1 Architecture impact,0,[0]
.51/.54,5.1 Architecture impact,0,[0]
"BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48
Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35",5.1 Architecture impact,0,[0]
"SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45
Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70",5.1 Architecture impact,0,[0]
NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65,5.1 Architecture impact,0,[0]
"BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67
Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - -
Table 4: Transfer test results for various architectures trained in different ways.",5.1 Architecture impact,0,[0]
"Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way.",5.1 Architecture impact,0,[0]
"† indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016).",5.1 Architecture impact,0,[0]
"For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014).",5.1 Architecture impact,0,[0]
(*),5.1 Architecture impact,0,[0]
"Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods.
regard to the embedding size.
",5.1 Architecture impact,0,[0]
"Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models.",5.1 Architecture impact,0,[0]
"However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows.",5.1 Architecture impact,0,[0]
We hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task (results on SNLI are relatively stable with regard to embedding size) but that can nevertheless be useful as features for transfer tasks.,5.1 Architecture impact,0,[0]
We report in Table 4 transfer tasks results for different architectures trained in different ways.,5.2 Task transfer,0,[0]
We group models by the nature of the data on which they were trained.,5.2 Task transfer,0,[0]
The first group corresponds to models trained with unsupervised unordered sentences.,5.2 Task transfer,0,[0]
"This includes bag-of-words models such as word2vec-SkipGram, the UnigramTFIDF model, the Paragraph Vector model (Le and Mikolov, 2014), the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016) and the SIF model (Arora et al., 2017), all trained on the Toronto book corpus (Zhu et al., 2015).",5.2 Task transfer,0,[0]
"The second group consists of models trained with unsu-
pervised ordered sentences such as FastSent and SkipThought (also trained on the Toronto book corpus).",5.2 Task transfer,0,[0]
We also include the FastSent variant “FastSent+AE” and the SkipThought-LN version that uses layer normalization.,5.2 Task transfer,0,[0]
"We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches.
",5.2 Task transfer,0,[0]
"Comparison with SkipThought The best performing sentence encoder to date is the SkipThought-LN model, which was trained on a very large corpora of ordered sentences.",5.2 Task transfer,0,[0]
"With much less data (570k compared to 64M sentences) but with high-quality supervision from the SNLI dataset, we are able to consistently outperform the results obtained by SkipThought vectors.",5.2 Task transfer,0,[0]
We train our model in less than a day on a single GPU compared to the best SkipThought-LN network trained for a month.,5.2 Task transfer,0,[0]
"Our BiLSTM-max trained on SNLI performs much better than released SkipThought vectors on MR, CR, MPQA, SST, MRPC-accuracy, SICK-R, SICK-E and STS14 (see Table 4).",5.2 Task transfer,0,[0]
"Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA.",5.2 Task transfer,0,[0]
We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST-LN).,5.2 Task transfer,0,[0]
"We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance (element-wise product and absolute difference) during training.
",5.2 Task transfer,0,[0]
"NLI as a supervised training set Our findings indicate that our model trained on SNLI obtains much better overall results than models trained on other supervised tasks such as COCO, dictionary definitions, NMT, PPDB (Ganitkevitch et al., 2013) and SST.",5.2 Task transfer,0,[0]
"For SST, we tried exactly the same models as for SNLI; it is worth noting that SST is smaller than NLI.",5.2 Task transfer,0,[0]
Our representations constitute higher-quality features for both classification and similarity tasks.,5.2 Task transfer,0,[0]
"One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform NLI is generally discriminative and informative.
",5.2 Task transfer,0,[0]
Domain adaptation on SICK tasks Our transfer learning approach obtains better results than previous state-of-the-art on the SICK task - can be seen as an out-domain version of SNLI - for both entailment and relatedness.,5.2 Task transfer,0,[0]
"We obtain a pearson score of 0.885 on SICK-R while (Tai et al., 2015) obtained 0.868, and we obtain 86.3% test accuracy on SICK-E while previous best handengineered models (Lai and Hockenmaier, 2014) obtained 84.5%.",5.2 Task transfer,0,[0]
"We also significantly outperformed previous transfer learning approaches on SICK-E (Bowman et al., 2015) that used the parameters of an LSTM model trained on SNLI to fine-tune on SICK (80.8% accuracy).",5.2 Task transfer,0,[0]
"We hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task.
",5.2 Task transfer,0,[0]
"Image-caption retrieval results In Table 5, we report results for the COCO image-caption retrieval task.",5.2 Task transfer,0,[0]
We report the mean recalls of 5 random splits of 1K test images.,5.2 Task transfer,0,[0]
"When trained with
ResNet features and 30k more training data, the SkipThought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval R@1, and from 25.9 to 30.6 on image retrieval R@1.",5.2 Task transfer,0,[0]
"Our approach pushes the results even further, from 37.9 to 42.4 on caption retrieval, and 30.6 to 33.2 on image retrieval.",5.2 Task transfer,0,[0]
"These results are comparable to previous approach of (Ma et al., 2015) that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task.",5.2 Task transfer,0,[0]
"This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task.
",5.2 Task transfer,0,[0]
MultiGenre NLI,5.2 Task transfer,0,[0]
"The MultiNLI corpus (Williams et al., 2017) was recently released as a multi-genre version of SNLI.",5.2 Task transfer,0,[0]
"With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language.",5.2 Task transfer,0,[0]
We augment Table 4 with our model trained on both SNLI and MultiNLI (AllNLI).,5.2 Task transfer,0,[0]
We observe a significant boost in performance overall compared to the model trained only on SLNI.,5.2 Task transfer,0,[0]
"Our model even reaches AdaSent performance on CR, suggesting that having a larger coverage for the training task helps learn even better general representations.",5.2 Task transfer,0,[0]
"On semantic textual similarity STS14, we are also competitive with PPDB based paragramphrase embeddings with a pearson score of 0.70.",5.2 Task transfer,0,[0]
"Interestingly, on caption-related transfer tasks such as the COCO image caption retrieval task, training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI (which contains mostly captions), which confirms the generalization power of our embeddings.",5.2 Task transfer,0,[0]
This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks.,6 Conclusion,0,[0]
We showed that models learned on NLI can perform better than models trained in unsupervised conditions or on other supervised tasks.,6 Conclusion,0,[0]
"By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.
",6 Conclusion,0,[0]
We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.,6 Conclusion,0,[0]
Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level.,6 Conclusion,0,[0]
"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features.",abstractText,0,[0]
"Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful.",abstractText,0,[0]
Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted.,abstractText,0,[0]
"In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks.",abstractText,0,[0]
"Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.",abstractText,0,[0]
Our encoder is publicly available1.,abstractText,0,[0]
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,title,0,[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 242–251, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,0,[0]
"Current virtual personal assistants (PAs) require users to either formulate complex intents in one utterance (e.g., “call Peter Miller on his mobile phone”) or go through tedious sub-dialogues (e.g., “phone call” – who would you like to call? – “Peter Miller” – I have a mobile number and a work number.",1 Introduction,0,[0]
Which one do you want?).,1 Introduction,0,[0]
"This is not how one would interact with a human assistant, where the request would be naturally structured into smaller chunks that individually get acknowledged (e.g., “Can you make a connection for me?” – sure – “with Peter Miller” - uh huh",1 Introduction,0,[0]
- “on his mobile” - dialling now).,1 Introduction,0,[0]
"Current PAs signal ongoing understanding by displaying the state of
the recognised speech (ASR) to the user, but not their semantic interpretation of it.",1 Introduction,0,[0]
Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context.,1 Introduction,0,[0]
"GoogleNow, for example, might present traffic information to a user picking up their mobile phone at their typical commute time.",1 Introduction,0,[0]
"These systems display their “understanding” state, but do not allow any type of interaction with it apart from dismissing the provided information.
",1 Introduction,0,[0]
"In this work, we explore adding a graphical user interface (GUI) modality that makes it possible to see these interaction styles as extremes on a continuum, and to realise positions between these extremes and present a mixed graphical/voice enabled PA that can provide feedback of understanding to the user incrementally as the user’s utterance unfolds–allowing users to make requests in instalments instead of fully thought-out requests.",1 Introduction,0,[0]
It does this by signalling ongoing understanding in an intuitive tree-like GUI that can be displayed on a mobile device.,1 Introduction,0,[0]
"We evaluate our system by directing users to perform tasks using it under nonincremental (i.e., ASR endpointing) and incremental conditions and then compare the two conditions.",1 Introduction,0,[0]
"We further compare a non-adaptive with an adaptive (i.e., infers likely events) version of our system.",1 Introduction,0,[0]
"We report that the users found the interface intuitive and easy to use, and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system.",1 Introduction,0,[0]
This work builds upon several threads of previous research: Chai et al. (2014),2 Related Work,0,[0]
"addressed misalignments in understanding (i.e., common ground (Clark and Schaefer, 1989)) between robots and humans by informing the human of the internal system state via speech.",2 Related Work,0,[0]
"We take this idea and ap-
242
ply it to a PA by displaying the internal state of the system to the user via a GUI (explained in Section 3.5), allowing the user to determine if system understanding has taken place–a way of providing feedback and backchannels to the user.",2 Related Work,0,[0]
"Dethlefs et al. (2016) provide a good review of work that show how backchannels facilitate grounding, feedback, and clarifications in human spoken dialogue, and apply an information density approach to determine when to backchannel using speech.",2 Related Work,0,[0]
"Because we don’t backchannel using speech here, there is no potential overlap between the user and the system; rather, our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps.
",2 Related Work,0,[0]
"Though different in many ways, our work is similar in some regards to Larsson et al. (2011), which displays information to the user and allows the user to navigate the display itself (e.g., by saying up or down in a menu list)–functionality that we intend to apply to our GUI in future work.",2 Related Work,0,[0]
"Our work is also comparable to SDS toolkits such as IrisTK (Skantze and Moubayed, 2012) and OpenDial (Lison, 2015) which enable SDS designers to visualise the internal state of their systems, though not for end user interpretability.
",2 Related Work,0,[0]
"Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service (LUIS) project (Williams et al., 2015).",2 Related Work,0,[0]
"While our system by no means achieves the scale that LUIS does, we offer here an additional contribution of an open source LUIS-like system (with the important addition of the graphical interface) that is authorable (using JSON files; we leave authoring using a web interface like that of LUIS to future work), extensible (affordances can be easily added), incremental (in that respect going beyond LUIS), trainable (i.e., can learn from examples, but can still function well without examples), and can learn through interacting (here we apply a user model that learns during interaction).",2 Related Work,0,[0]
"This section introduces and describes our SDS, which is modularised into four main components: ASR, natural language understanding (NLU), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree.",3 System Description,0,[0]
The overall system is represented in Figure 1.,3 System Description,0,[0]
"For the remainder of this section, each module is explained in
turn.",3 System Description,0,[0]
"As each module processes input incrementally (i.e., word for word), we first explain our framework for incremental processing.",3 System Description,0,[0]
An aspect of our SDS that sets it apart from others is the requirement that it process incrementally.,3.1 Incremental Dialogue,0,[0]
"One potential concern with incremental processing is regarding informativeness: why act early when waiting might provide additional information, resulting in better-informed decisions?",3.1 Incremental Dialogue,0,[0]
The trade off is naturalness as perceived by the user who is interacting with the SDS.,3.1 Incremental Dialogue,0,[0]
"Indeed, it has been shown that human users perceive incremental systems as being more natural than traditional, turn-based systems (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991; Asri et al., 2014), offer a more human-like experience (Edlund et al., 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007).",3.1 Incremental Dialogue,0,[0]
"Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002).
",3.1 Incremental Dialogue,0,[0]
The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired.,3.1 Incremental Dialogue,0,[0]
"Such mechanisms are offered by the incremental unit (IU) framework for SDS (Schlangen and Skantze, 2011), which we apply here.",3.1 Incremental Dialogue,0,[0]
"Following Kennington et al. (2014), the IU framework consists of a network of processing modules.",3.1 Incremental Dialogue,0,[0]
"A typical module takes input, performs some kind of processing on that data, and produces output.
",3.1 Incremental Dialogue,0,[0]
The data are packaged as the payload of incremental units (IUs) which are passed between modules.,3.1 Incremental Dialogue,0,[0]
"The IUs themselves are interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (see Figure 2 for an example of incremental ASR).",3.1 Incremental Dialogue,0,[0]
"Thus IUs can be added, but can be later revoked and replaced in light of new information.",3.1 Incremental Dialogue,0,[0]
"The IU framework can take advantage of up-to-date information, but have the potential to function in such a way that users perceive as more natural.
",3.1 Incremental Dialogue,0,[0]
The modules explained in the remainder of this section are implemented as IU-modules and process incrementally.,3.1 Incremental Dialogue,0,[0]
Each will now be explained.,3.1 Incremental Dialogue,0,[0]
The module that takes speech input from the user in our SDS is the ASR component.,3.2 Speech Recognition,0,[0]
"Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible (i.e., the ASR must not wait for endpointing to produce output).",3.2 Speech Recognition,0,[0]
"Each module that follows must also process incrementally, acting in lock-step upon input as it is received.",3.2 Speech Recognition,0,[0]
"Incremental ASR is not new (Baumann et al., 2009) and many of the current freely-accessible ASR systems can produce output (semi-) incrementally.",3.2 Speech Recognition,0,[0]
We opt for Google ASR for its vocabulary coverage of our evaluation language (German).,3.2 Speech Recognition,0,[0]
"Following, Baumann et al. (2016), we package output from the Google service into IUs which are passed to the NLU module, which we now explain.",3.2 Speech Recognition,0,[0]
We approach the task of NLU as a slot-filling task (a very common approach; see Tur et al. (2012)) where an intent is complete when all slots of a frame are filled.,3.3 Language Understanding,0,[0]
"The main driver of the NLU in
our SDS is the SIUM model of NLU introduced in Kennington et al. (2013).",3.3 Language Understanding,0,[0]
"SIUM has been used in several systems which have reported substantial results in various domains, languages, and tasks (Han et al., 2015; Kennington et al., 2015; Kennington and Schlangen, 2017)",3.3 Language Understanding,0,[0]
"Though originally a model of reference resolution, it was always intended to be used for general NLU, which we do here.",3.3 Language Understanding,0,[0]
"The model is formalised as follows:
",3.3 Language Understanding,0,[0]
P (I|U) = 1 P (U) P (I) ∑ r∈R P (U |R = r)P,3.3 Language Understanding,0,[0]
"(R = r|I) (1)
That is, P (I|U) is the probability of the intent",3.3 Language Understanding,0,[0]
"I (i.e., a frame slot) behind the speaker’s (ongoing) utterance U .",3.3 Language Understanding,0,[0]
"This is recovered using the mediating variable R, a set of properties which map between aspects of U and aspects of I .",3.3 Language Understanding,0,[0]
"We opt for abstract properties here (e.g., the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta, mediterranean, vegetarian, etc.).",3.3 Language Understanding,0,[0]
Properties are pre-defined by a system designer and can match words that might be uttered to describe the intent in question.,3.3 Language Understanding,0,[0]
"For P (R|I), probability is distributed uniformly over all properties that a given intent is specified to have.",3.3 Language Understanding,0,[0]
"(If other information is available, more informative priors could be used as well.)",3.3 Language Understanding,0,[0]
The mapping between properties and aspects of U can be learned from data.,3.3 Language Understanding,0,[0]
"During application, R is marginalised over, resulting in a distribution over possible intents.1",3.3 Language Understanding,0,[0]
"This occurs at each word increment, where the distribution from the previous increment is combined via P (I), keeping track of the distribution over time.
",3.3 Language Understanding,0,[0]
We further apply a simple rule to add in apriori knowledge: if some r ∈ R and w ∈ U are such that r,3.3 Language Understanding,0,[0]
".= w (where .= is string equality; e.g., an intent has the property of pasta and the word pasta is uttered), then we set C(U=w|R=r)=1.",3.3 Language Understanding,0,[0]
"To allow for possible ASR confusions, we also apply C(U=w|R=r)= 1",3.3 Language Understanding,0,[0]
"− ld(w, r)/max(len(w), len(r)), where ld is the Levenshtein distance (but we only apply this if the calculated value is above a threshold of 0.6; i.e., the two strings are mostly similar).",3.3 Language Understanding,0,[0]
"For all otherw, C(w|r)=0.",3.3 Language Understanding,0,[0]
"This results in a distribution C, which we renormalise and blend with learned distribution to yield P (U |R).
",3.3 Language Understanding,0,[0]
"1In Kennington et al. (2013) the authors apply Bayes’ Rule to allow P (U |R) to produce a distribution over properties, which we adopt here.
",3.3 Language Understanding,0,[0]
We apply an instantiation of SIUM for each slot.,3.3 Language Understanding,0,[0]
"The candidate slots which are processed depends on the state of the dialogue; only slots represented by visible nodes are considered, thereby reducing the possible frames that could be predicted.",3.3 Language Understanding,0,[0]
"At each word increment, the updated slots (and their corresponding) distributions are given to the DM, which will now be explained.",3.3 Language Understanding,0,[0]
"The DM plays a crucial role in our SDS: as well as determining how to act, the DM is called upon to decide when to act, effectively giving the DM the control over timing of actions rather than relying on ASR endpointing–further separating our SDS from other systems.",3.4 Dialogue Manager,0,[0]
"The DM policy is based on a confidence score derived from the NLU (in this case, we used the distribution’s argmax value) using thresholds for the actions (see below), set by hand (i.e., trial and error).",3.4 Dialogue Manager,0,[0]
"At each word and resulting distribution from NLU, the DM needs to choose one of the following:
• wait – wait for more information (i.e., for the next word)
• select – as the NLU is confident enough, fill the slot can with the argmax from NLU
• request – signal a (yes/no) clarification request on the current slot and the proposed filler
• confirm – act on the confirmation of the user; in effect, select the proposed slot value
Though the thresholds are statically set, we applied OpenDial (Lison, 2015) as an IU-module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning (which OpenDial could provide).",3.4 Dialogue Manager,0,[0]
"The DM processes and makes a decision for each slot, with the assumption that only one slot out of all that are processed will result in an non-wait action (though this is not enforced).",3.4 Dialogue Manager,0,[0]
The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding.,3.5 Graphical User Interface,0,[0]
"One motivation for this is that the user can determine if the system understood the user’s intent before providing the user with a response
(e.g., a list of restaurants of a certain type); i.e., if any misunderstanding takes place, it happens before the system commits to an action and is potentially more easily repaired.
",3.5 Graphical User Interface,0,[0]
"The display is a rightbranching tree, where the branches directly off the root node display the affordances of the system (i.e., what domains of things it can understand and do something about).",3.5 Graphical User Interface,0,[0]
"When the first tree is displayed, it represents a state of the NLU where none of the slots are filled, as in Figure 3.
",3.5 Graphical User Interface,0,[0]
"When a user verbally selects a domain to ask about, the tree is adjusted to make that domain the only one displayed and
the slots that are required for that domain are shown as branches.",3.5 Graphical User Interface,0,[0]
"The user can then fill those slots (i.e., branches) by uttering the displayed name, or, alternatively, by uttering the item to fill the slot directly.",3.5 Graphical User Interface,0,[0]
"For example, at a minimum, the user could utter the name of the domain then an item for each slot (e.g., food Thai downtown) or the speech could be more natural (e.g., I’m quite hungry, I am looking for some Thai food maybe in the downtown area).",3.5 Graphical User Interface,0,[0]
"Crucially, the user can also hesitate within and between chunks, as advancement is not triggered by silence thresholding, but rather semantically.",3.5 Graphical User Interface,0,[0]
"When something is uttered that falls into the request state of the DM as explained above, the display expands the subtree under question and marks the item with a question mark (see Figure 4).",3.5 Graphical User Interface,0,[0]
"At this point, the user can utter any kind of confirmation.",3.5 Graphical User Interface,0,[0]
A positive confirmation fills the slot with the item in question.,3.5 Graphical User Interface,0,[0]
"A negative confirmation retracts the question, but leaves the branch expanded.",3.5 Graphical User Interface,0,[0]
The expanded branches are displayed according to their rank as given by the NLU’s probability distribution.,3.5 Graphical User Interface,0,[0]
"Though a branch in the display can theoretically display an unlimited number of children, we opted to only show 7 children; if a branch had more, the final child displayed as an ellipsis.
",3.5 Graphical User Interface,0,[0]
"A completed branch is collapsed, visually marking its corresponding slot as filled.",3.5 Graphical User Interface,0,[0]
"At any
time, a user can backtrack by saying no (or equivalent) or start the entire interaction over from the beginning with a keyword, e.g., restart.",3.5 Graphical User Interface,0,[0]
"To aid the user’s attention, the node under question is marked in red, where completed slots are represented by outlined nodes, and filled nodes represent candidates for the current slot in question (see examples of all three in Figure 4).",3.5 Graphical User Interface,0,[0]
"For cases where the system is in the wait state for several words (during which there is no change in the tree), the system signals activity at each word by causing the red node in question to temporarily change to white, then back to red (i.e., appearing as a blinking node to the user).",3.5 Graphical User Interface,0,[0]
"Figure 5 shows a filled frame, represented as tree with one branch for each filled slot.
",3.5 Graphical User Interface,0,[0]
Figure 5: Example tree where all of the slots are filled.,3.5 Graphical User Interface,0,[0]
"(i.e., domain:food, location:university, type:thai)
",3.5 Graphical User Interface,0,[0]
Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far.,3.5 Graphical User Interface,0,[0]
"It is designed to aid the user’s attention to the slot in question, and clearly indicates the affordances that the system has.",3.5 Graphical User Interface,0,[0]
"The interface is currently a read-only display that is purely speech-driven, but it could be augmented with additional functionalities, such as tapping a node for expansion or typing input that the system might not yet display.",3.5 Graphical User Interface,0,[0]
"It is currently implemented as a web-based interface (using the JavaScript D3 library), allowing it to be usable as a web application on any machine or mobile device.
",3.5 Graphical User Interface,0,[0]
"Adaptive Branching The GUI as explained affords an additional straight-forward extension: in order to move our system towards adaptivity on the above-mentioned continuum, the GUI can be used to signal what the system thinks the user might say next.",3.5 Graphical User Interface,0,[0]
"This is done by expanding a branch and displaying a confirmation on that branch, signalling that the system predicts that the user will choose that particular branch.",3.5 Graphical User Interface,0,[0]
"Alternatively, if the system is confident that a user will fill a slot with a particular value, that particular slot can be filled without confirmation.",3.5 Graphical User Interface,0,[0]
This is displayed as a collapsed tree branch.,3.5 Graphical User Interface,0,[0]
"A system that perfectly predicts a user’s intent would fill an entire tree (i.e., all slots) only requiring the user to confirm once.",3.5 Graphical User Interface,0,[0]
A more careful system would confirm at each step (such an interaction would only require the user to utter confirmations and nothing else).,3.5 Graphical User Interface,0,[0]
We applied this adaptive variant of the tree in one of our experiments explained below.,3.5 Graphical User Interface,0,[0]
"In this section, we describe two experiments where we evaluated our system.",4 Experiments,0,[0]
It is our primary goal to show that our GUI is useful and signals understanding to the user.,4 Experiments,0,[0]
We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system.,4 Experiments,0,[0]
We further want to show that an adaptive system is more effective than a non-adaptive system (though both would process incrementally).,4 Experiments,0,[0]
"In order to best evaluate our system, we recruited participants to interact with our system in varied settings to compare endpointed (i.e., non-incremental) and nonadaptive as well as adaptive versions.",4 Experiments,0,[0]
"We describe how the data were collected from the participants, then explain each experiment and give results.",4 Experiments,0,[0]
The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time.,4.1 Task & Procedure,0,[0]
Figure 6 shows some example tasks as they would be displayed (one at a time) to the user.,4.1 Task & Procedure,0,[0]
"A screen, tablet, and keyboard were on the desk in front of the user (see Figure",4.1 Task & Procedure,0,[0]
7).2,4.1 Task & Procedure,0,[0]
"The user was instructed to convey the task presented on the screen to the system such
2We used a Samsung 8.4 Pro tablet turned to its side to show a larger width for the tree to grow to the right.",4.1 Task & Procedure,0,[0]
"The tablet only showed the GUI; the SDS ran on a separate computer.
that the GUI on the tablet would have a completed tree (e.g., as in Figure 5).",4.1 Task & Procedure,0,[0]
"When the participant was satisfied that the system understood her intent, she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet (as in Figure 3).
",4.1 Task & Procedure,0,[0]
"The possible task domains were call, which had a single slot for name to be filled (i.e., one out of the 22 most common German given names); message which had a slot for name and a slot for the message (which, when invoked, would simply fill in directly from the
ASR until 1 second of silence was detected); eat which had slots for type (in this case, 6 possible types) and location (in this case, 6 locations based around the city of Bielefeld); route which had slots for source city and the destination city (which shared the same list of the top 100 most populous German cities); and reminder which had a slot for message.
",4.1 Task & Procedure,0,[0]
"For each task, the domain was first randomly chosen from the 5 possible domains, and then each slot value to be filled was randomly chosen (the message slot for the name and message domains was randomly selected from a list of 6 possible “messages”, each with 2-3 words; e.g., feed the cat, visit grandma, etc.).",4.1 Task & Procedure,0,[0]
The system kept track of which tasks were already presented to the participant.,4.1 Task & Procedure,0,[0]
"At any time after the first task, the system could choose a task that was previously presented and present it again to the participant (with a 50% chance) so the user would often see tasks that she had seen before (with the assumption that humans who use PAs often do perform similar, if not the same, tasks more than once).
",4.1 Task & Procedure,0,[0]
"The participant was told that she would interact with the system in three different phases, each for 4 minutes, and to accomplish as many tasks as possible in that time allotment.",4.1 Task & Procedure,0,[0]
The participant was not told what the different phases were.,4.1 Task & Procedure,0,[0]
"The experiments described in Sections 4.2 and
4.3 respectively describe and report a comparison first between the Phase 1 and 2 (denoted as the endpointed and incremental variants of the system) in order to establish whether or not the incremental variant produced better results than the endpointed variant.",4.1 Task & Procedure,0,[0]
We also report a comparison between Phase 2 and 3 (incremental and incremental-adaptive phases).,4.1 Task & Procedure,0,[0]
Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2.,4.1 Task & Procedure,0,[0]
"Because of this, we fixed the order of the phase presentation for all participants.",4.1 Task & Procedure,0,[0]
Each of these phases are described below.,4.1 Task & Procedure,0,[0]
"Before the participant began Phase 1, they were able to try it out for up to 4 minutes (in Phase 1 settings) and ask for help from the experimenter, allowing them to get used to the Phase 1 interface before the actual experiment began.",4.1 Task & Procedure,0,[0]
"After this trial phase, the experiment began with Phase 1.
",4.1 Task & Procedure,0,[0]
"Phase 1: Non-incremental In this phase, the system did not appear to work incrementally; i.e., the system displayed tree updates after ASR endpointing (of 1.2 seconds–a reasonable amount of time to expect a response from a commercial spoken PA).",4.1 Task & Procedure,0,[0]
The system displayed the ongoing ASR on the tablet as it was recognised (as is often done in commercial PAs).,4.1 Task & Procedure,0,[0]
"At the end of Phase 1, a pop up window notified the user that the phase was complete.",4.1 Task & Procedure,0,[0]
"They then moved onto Phase 2.
",4.1 Task & Procedure,0,[0]
"Phase 2: Incremental In this phase, the system displayed the tree information incrementally without endpointing.",4.1 Task & Procedure,0,[0]
"The ASR was no longer displayed; only the tree provided feedback in understanding, as explained in Section 3.5.
",4.1 Task & Procedure,0,[0]
"After Phase 2, a 10-question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2.",4.1 Task & Procedure,0,[0]
"For each question, they had the choice of Phase 1, Phase
2, Both, and Neither.",4.1 Task & Procedure,0,[0]
(See Appendix for full list of questions.),4.1 Task & Procedure,0,[0]
"After completing the questionnaire, they moved onto Phase 3.
",4.1 Task & Procedure,0,[0]
"Phase 3: Incremental-adaptive In this phase, the incremental system was again presented to the participant with an added user model that “learned” about the user.",4.1 Task & Procedure,0,[0]
"If the user saw a task more than once, the user model would predict that, if the user chose that task domain again (e.g., route) then the system would automatically ask a clarification using the previously filled values (except for the message slot, which the user always had to fill).",4.1 Task & Procedure,0,[0]
"If the user saw a task more than 3 times, the system skipped asking for clarifications and filled in the domain slots completely, requiring the user only to press the space bar to confirm it was the correct one (i.e., to complete the task).",4.1 Task & Procedure,0,[0]
"An example progression might be as follows: a participant is presented with the task route from Bielefeld to Berlin, then the user would attempt to get the system to fill in the tree (i.e., slots) with those values.",4.1 Task & Procedure,0,[0]
"After some interaction in other domains, the user sees the same task again, and now after indicating the intent type route, the user must only say “yes” for each slot to confirm the system’s prediction.",4.1 Task & Procedure,0,[0]
"Later, if the task is presented a third time, when entering that domain (i.e, route), the two slots would already be filled.",4.1 Task & Procedure,0,[0]
"If later a different route task was presented, e.g., route from Bielefeld to Hamburg, the system would already have the two slots filled, but the user could backtrack by saying “no, to Hamburg” which would trigger the system to fill the appropriate slot with the corrected value.",4.1 Task & Procedure,0,[0]
"Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant, but continue to fill the from slot with Bielefeld.
",4.1 Task & Procedure,0,[0]
"After Phase 3, the participants were presented with another questionnaire on the screen to fill out with the same questions (plus two additional questions), this time comparing Phase 2 and Phase 3.",4.1 Task & Procedure,0,[0]
"For each item, they had the choice of Phase 2, Phase 3, Both, and Neither.",4.1 Task & Procedure,0,[0]
"At the end of the three phases and questionnaires, the participants were given a final questionnaire to fill out by hand on their general impressions of the systems.
",4.1 Task & Procedure,0,[0]
We recruited 14 participants for the evaluation.,4.1 Task & Procedure,0,[0]
"We used the Mint tools data collection framework (Kousidis et al., 2012) to log the interactions.",4.1 Task & Procedure,0,[0]
"Due to some technical issues, one of the participants
did not log interactions.",4.1 Task & Procedure,0,[0]
"We collected data from 13 participants, post-Phase 2 questionnaires from 12 participants, post-Phase 3 questionnaires from all 14 participants, and general questionnaires from all 14 participants.",4.1 Task & Procedure,0,[0]
"In the experiments that follow, we report objective and subjective measures to determine the settings that produced superior results.
",4.1 Task & Procedure,0,[0]
Metrics We report the subjective results of the participant questionnaires.,4.1 Task & Procedure,0,[0]
We only report those items that were statistically significant (see Appendix for a full list of the questions).,4.1 Task & Procedure,0,[0]
"We further report objective measures for each system variant: total number of completed tasks, fully correct frames, average frame f-score, and average time elapsed (averages are taken over all participants for each variant; we only used the 10 participants who fully interacted with all three phases).",4.1 Task & Procedure,0,[0]
Discussion is left to the end of this section.,4.1 Task & Procedure,0,[0]
"In this section we report the results of the evaluation between the endpointed (i.e., nonincremental; Phase 1) variant vs the incremental (Phase 2) variant of our system.
",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"Subjective Results We applied a multinomial test of significance to the results, treating all four possible answers as equally likely (with Bonferroni correction of 10).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"The item The interface was useful and easy to understand with the answer of Both was significant (χ2 (4, N = 12)",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"= 9.0, p < .005), as was The assistant was easy and intuitive to use also with the answer Both (χ2 (4, N = 12) = 9.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"The item I always understood what the system wanted from me was also answered Both significantly more times than other answers (χ2 (4, N = 14) = 9.0, p< .005), similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both (χ2 (4, N = 12) = 10.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"These responses tell us that though the participants did not report preference for either system variant, they reported a general positive impression of the GUI (in both variants).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"This is a nice result; the GUI could be used in either system with benefit to the users.
",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
Objective Results The endpointed (Phase 1) and incremental (Phase 2) columns in Table 1 show the results of the objective evaluation.,4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"Though the average time per task and fscore for the endpointed variant are better than those of the
incremental variant, the total number of tasks for the incremental variant was higher.
",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"Manual inspection of logs indicate that participants took advantage of the system’s flexibility of understanding instalments (i.e., filling frames incrementally).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"This is evidenced in that participants often uttered words understood by the system as being negative (e.g., nein/no), either as a result of an explicit confirmation request by the system (e.g., Thai?) or after a slot was incorrectly filled (something very easily determined through the GUI).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"This is a desired outcome of using our system; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent (i.e., frame).",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"However, we cannot fully empirically measure these tendencies given our data.",4.2 Experiment 1: Endpointed vs. Incremental,0,[0]
"In this section we report results for the evaluation between the incremental (Phase 2) and incremental-adaptive (henceforth just adaptive; Phase 3) systems.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
Subjective Results We applied the same significance test as Experiment 1 (with Bonferroni correction of 12).,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"The item The interface was useful and easy to understand was answered with Both significantly (χ2 (4, N = 14)",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"= 10.0, p < .0042), The item I had the feeling that the assistant attempted to learn about me was answered with Neither (χ2 (4, N = 14) = 8.0, p < .0042), though Phase 3 was also marked (6 times).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
All other items were not significant.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
Here again we see that there is a general positive impression of the GUI under all conditions.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"If anyone noticed that a system variant was attempting to learn a user model at all, they noticed that it was in Phase 3, as expected.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"Objective Results The incremental (Phase 2) and adaptive (Phase 3) columns in Table 1 show
the results for the objective evaluation for this experiment.",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"There is a clear difference between the two variants, with the adaptive showing more completed tasks, more fully correct frames, and a higher average fscore (all three likely due to the fact that frames were potentially pre-filled).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,0,[0]
"While the responses don’t express any preference for a particular system variant, the overall impression of the GUI was positive.",4.4 Discussion,0,[0]
"The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level, due to the higher number of completed tasks and locallymade repairs.",4.4 Discussion,0,[0]
"There are further gains to be made when the system applies simple user modelling (i.e., adaptivity) by attempting to predict what the user might want to do in a chosen domain, decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks.",4.4 Discussion,0,[0]
"Participants also didn’t just get used to the system over time, as the average time per episode was fairly similar in all three phases.
",4.4 Discussion,0,[0]
The open-ended questionnaire sheds additional light.,4.4 Discussion,0,[0]
"Most of the suggestions for improvement related to ASR misrecognition and speed (i.e., not about the system itself).",4.4 Discussion,0,[0]
Two participants suggested an ability to add “free input” or select alternatives from the tree.,4.4 Discussion,0,[0]
"Two participants suggested that the system be more responsive (i.e., in wait states), and give more feedback (i.e., backchannels) more often.",4.4 Discussion,0,[0]
"For those participants that expressed preference to the non-incremental system (Phase 1), none of them had used a speech-based PA before, whereas those that expressed preference to the incremental versions (Phases 2 and 3) use them regularly.",4.4 Discussion,0,[0]
"We conjecture that people without SDS experience equate understanding with ASR, whereas those that are more familiar with PAs know that perfect ASR doesn’t translate to perfect understanding–hence the need for a GUI.",4.4 Discussion,0,[0]
"A potential remedy would be to display ASR with the tree, signalling understanding despite ASR errors.",4.4 Discussion,0,[0]
"Given the results and analysis, we conclude that an intuitive presentation that signals a system’s ongoing understanding benefits end users who perform simple tasks which might be performed by a PA.",5 Conclusion & Future Work,0,[0]
"The GUI that we provided, using a right-branching
tree, worked well; indeed, the participants who used it found it intuitive and easy to understand.",5 Conclusion & Future Work,0,[0]
There are gains to be made when the system signals understanding at finer-grained levels than just at the end of a pre-formulated utterance.,5 Conclusion & Future Work,0,[0]
There are further gains to be made when a PA attempts to learn (even a rudimentary) user model to predict what the user might want to do next.,5 Conclusion & Future Work,0,[0]
"The adaptivity moves our system from one extreme of the continuum–simple slot filling–closer towards the extreme that is fully predictive, with the additional benefit of being able to easily correct mistakes in the predictions.
",5 Conclusion & Future Work,0,[0]
"For future work, we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy.",5 Conclusion & Future Work,0,[0]
We want to improve the NLU and scale to larger domains.3,5 Conclusion & Future Work,0,[0]
"We also plan on implementing this as a standalone application that could be run on a mobile device, which could actually perform the tasks.",5 Conclusion & Future Work,0,[0]
"It would further be beneficial to compare the GUI with a system that responds with speech (i.e., without a GUI).",5 Conclusion & Future Work,0,[0]
"Lastly, we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system.
",5 Conclusion & Future Work,0,[0]
Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions.,5 Conclusion & Future Work,0,[0]
Thanks also to Julian Hough for helping with experiments.,5 Conclusion & Future Work,0,[0]
"We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and the BMBF KogniHome project.
",5 Conclusion & Future Work,0,[0]
"Appendix The following questions were asked on both questionnaires following Phase 2 and Phase 3 (comparing the two most latest used system versions; as translated into English):
•",5 Conclusion & Future Work,0,[0]
The interface was useful and easy to understand.,5 Conclusion & Future Work,0,[0]
• The assistant was easy and intuitive to use.,5 Conclusion & Future Work,0,[0]
• The assistant understood what I wanted to say.,5 Conclusion & Future Work,0,[0]
• I always understood what the system wanted from me.,5 Conclusion & Future Work,0,[0]
•,5 Conclusion & Future Work,0,[0]
The assistant made many mistakes.,5 Conclusion & Future Work,0,[0]
• The assistant did not respond while I spoke.,5 Conclusion & Future Work,0,[0]
"3Kennington and Schlangen (2017) showed that our chosen NLU approach can scale fairly well, but the GUI has some limits when applied to larger domains with thousands of items.",5 Conclusion & Future Work,0,[0]
"We leave improved scaling to future work.
",5 Conclusion & Future Work,0,[0]
"• It was sometimes unclear to me if the assistant understood me.
",5 Conclusion & Future Work,0,[0]
• The assistant responded while I spoke.,5 Conclusion & Future Work,0,[0]
• The assistant sometimes did things that I did not expect.,5 Conclusion & Future Work,0,[0]
"• When the assistant made mistakes, it was easy for me
to correct them.
",5 Conclusion & Future Work,0,[0]
"In addition to the above 10 questions, the following were also asked on the questionnaire following Phase 3: • I had the feeling that the assistant attempted to learn
about me.
",5 Conclusion & Future Work,0,[0]
"• I had the feeling that the assistant made incorrect guesses.
",5 Conclusion & Future Work,0,[0]
The following questions were used on the general questionnaire:,5 Conclusion & Future Work,0,[0]
"• I regularly use personal assistants such as Siri, Cortana,
Google now or Amazon Echo:",5 Conclusion & Future Work,0,[0]
"Yes/No
• I have never used a speech-based personal assistant: Yes/No
• What was your general impression of our personal assistants?
",5 Conclusion & Future Work,0,[0]
• Would you use one of these assistants on a smart phone or tablet if it were available?,5 Conclusion & Future Work,0,[0]
"If yes, which one?
• Do you have suggestions that you think would help us improve our assistants?
",5 Conclusion & Future Work,0,[0]
"• If you have used other speech-based interfaces before, do you prefer this interface?",5 Conclusion & Future Work,0,[0]
"Arguably, spoken dialogue systems are most often used not in hands/eyes-busy situations, but rather in settings where a graphical display is also available, such as a mobile phone.",abstractText,0,[0]
We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system.,abstractText,0,[0]
"By visualising the current dialogue state and possible continuations of it as a simple tree, and allowing interaction with that visualisation (e.g., for confirmations or corrections), the system provides both feedback on past user actions and guidance on possible future ones, and it can span the continuum from slot filling to full prediction of user intent (such as GoogleNow).",abstractText,0,[0]
"We evaluate our system with real users and report that they found the system intuitive and easy to use, and that incremental and adaptive settings enable users to accomplish more tasks.",abstractText,0,[0]
Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 640–645 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
640",text,0,[0]
"In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output.",1 Introduction,0,[0]
"Many attention mechanisms have been proposed (Xu et al., 2015; Bahdanau et al., 2014; Luong et al., 2015; Martins and Astudillo, 2016) but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted ’soft’ aligned input state, which finally derives the output distribution.",1 Introduction,0,[0]
"This method is end to end differentiable and easy to implement.
",1 Introduction,0,[0]
Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state.,1 Introduction,0,[0]
"When successfully trained, hard attention is often found to be more accurate (Xu et al., 2015; Zaremba and Sutskever, 2015).",1 Introduction,0,[0]
"In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks (Yu et al., 2016; Aharoni and Goldberg, 2017).",1 Introduction,0,[0]
"For general seq2seq learning, methods like SparseMax (Martins and Astudillo, 2016) and local attention (Luong et al., 2015) were proposed to bridge the gap between soft and hard attention.
",1 Introduction,0,[0]
"∗Both authors contributed equally to this work
In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations.",1 Introduction,0,[0]
"The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft attention.",1 Introduction,0,[0]
"When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint.",1 Introduction,0,[0]
"This approximation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.
",1 Introduction,0,[0]
"We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft attention, which in turn was better than hard and the recent Sparsemax (Martins and Astudillo, 2016) attention.",1 Introduction,0,[0]
"More importantly, the training process was as easy as soft attention.",1 Introduction,0,[0]
"For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.",1 Introduction,0,[0]
For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we review it here.,2 Background and Related Work,0,[0]
We then review related work on attention mechanisms on these models.,2 Background and Related Work,0,[0]
"Let x1, . . .",2.1 Attention-based Encoder Decoder Model,0,[0]
", xm denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x1, . . .",2.1 Attention-based Encoder Decoder Model,0,[0]
", xm, which we jointly denote as x1...m. Let y1, . . .",2.1 Attention-based Encoder Decoder Model,0,[0]
", yn denote the output tokens in the target sequence.",2.1 Attention-based Encoder Decoder Model,0,[0]
"The Encoder-Decoder (ED) network factorizes Pr(y1, . . .",2.1 Attention-based Encoder Decoder Model,0,[0]
", yn|x1...m) as ∏n t=1 Pr(yt|x1...m, st) where st is a decoder state summarizing y1, . . .",2.1 Attention-based Encoder Decoder Model,0,[0]
yt−1.,2.1 Attention-based Encoder Decoder Model,0,[0]
"For each t, a hidden attention variable at is used to denote which part of x1...m aligns with yt.",2.1 Attention-based Encoder Decoder Model,0,[0]
"Let P (at = j|x1...m, st) denote the
probability that encoder state xj is relevant for output yt.",2.1 Attention-based Encoder Decoder Model,0,[0]
"Typically this is estimated using a softmax function over attention scores computed from xj and decoder state st as follows.
",2.1 Attention-based Encoder Decoder Model,0,[0]
"P (at = j|x1...m, st) =",2.1 Attention-based Encoder Decoder Model,0,[0]
"eAθ(xj ,st)∑m r=1",2.1 Attention-based Encoder Decoder Model,0,[0]
"e Aθ(xr,st) (1)
where Aθ(., .) is the attention unit that scores each input state xj as per the decoder state st.",2.1 Attention-based Encoder Decoder Model,0,[0]
"Thereafter, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each yt as
log Pr(yt|x1...m) = log Pr(yt| ∑ a Pt(a)xa) (2)
where Pt(at = j) is the short form for P (at = j|x1...m, st).",2.1 Attention-based Encoder Decoder Model,0,[0]
"Also, here and in the rest of the paper we drop st from P (yt) and Pt(a) for ease of notation.",2.1 Attention-based Encoder Decoder Model,0,[0]
The weighted sum ∑ a Pt(a)xa is called an input context ct which is fed to the decoder RNN along with yt for computing the next state st+1.,2.1 Attention-based Encoder Decoder Model,0,[0]
"We next review existing attention types.
",2.2 Related Work,0,[0]
"Soft Attention is the attention method described in the previous section and is the current standard for seq2seq learning (Xu Chen, 2018; Koehn, 2017).",2.2 Related Work,0,[0]
"It was proposed for translation in (Bahdanau et al., 2014) and refined further in (Luong et al., 2015).",2.2 Related Work,0,[0]
"As shown in Eq 2, here each output is derived from an attention averaged input.",2.2 Related Work,0,[0]
This diffuses the coupling between the input and output.,2.2 Related Work,0,[0]
"The advantage of soft attention is end to end differentiability, and fast training and inference.
",2.2 Related Work,0,[0]
"Hard Attention was proposed in its current form in (Xu et al., 2015) and attends to exactly one input state for an output1.",2.2 Related Work,0,[0]
"During training, log-likelihood is an expectation over sampled attentions:
logPt(yt|x1...m) = M∑ l=1 logPt(yt|xãl) (3)
where ã1, . . .",2.2 Related Work,0,[0]
", ãM are sampled from the multinomial Pt(a).",2.2 Related Work,0,[0]
"Because of the sampling, the gradient has to be computed by Monte Carlo gradient/REINFORCE (Williams, 1992) and is subject to high variance.",2.2 Related Work,0,[0]
"Many tricks are required to train
1Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.
hard attention and there is little standardization across implementations.",2.2 Related Work,0,[0]
Xu et al (2015) use a combination of REINFORCE and soft attention.,2.2 Related Work,0,[0]
Zaremba et al(2015) uses curriculum learning that starts as soft-attention and gradually becomes discrete.,2.2 Related Work,0,[0]
"Ling& Rush (2017) aggregates multiple samples during training, and a single sampled attention while testing.",2.2 Related Work,0,[0]
"However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance (Xu et al., 2015; Shankar and Sarawagi, 2018).
",2.2 Related Work,0,[0]
Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention.,2.2 Related Work,0,[0]
Luong et al (2015) proposes local attention that averages a window of input.,2.2 Related Work,0,[0]
"This has been refined later to include syntax (Chen et al., 2017; Sennrich and Haddow, 2016; Chen et al., 2018).",2.2 Related Work,0,[0]
"Another idea is to replace the softmax in soft attention with sparsity inducing operators (Martins and Astudillo, 2016; Niculae and Blondel, 2017).",2.2 Related Work,0,[0]
"However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.",2.2 Related Work,0,[0]
"We start from an explicit joint representation of the uncertainty of the attention and output variables.
",3 Joint Attention-Output Models,0,[0]
logPt(yt|x1...m) = log ∑ a Pt(a)Pt(yt|xa),3 Joint Attention-Output Models,0,[0]
"(4)
The joint model directly couples individual input states to the output, and thus is a type of hard attention.",3 Joint Attention-Output Models,0,[0]
"Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention.",3 Joint Attention-Output Models,0,[0]
"We call this the full-joint method.
",3 Joint Attention-Output Models,0,[0]
"Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical.",3 Joint Attention-Output Models,0,[0]
Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention.,3 Joint Attention-Output Models,0,[0]
We show a surprisingly simple alternative approximation that provides hard attention without its training complexity.,3 Joint Attention-Output Models,0,[0]
"Our method called Beam-joint deterministically selects the top-k highest attention values and approximates the full joint log probability as
logPt(yt|x1...m)",3 Joint Attention-Output Models,0,[0]
"≈ log ∑
a∈TopK(Pt(a))
Pt(a)Pt(yt|xa)",3 Joint Attention-Output Models,0,[0]
"(5)
Thus, in beam-joint, we first compute the multinomial attention distribution in O(m) time using
Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each position compute K output softmax, and finally compute the attention weighted output mixture distribution.",3 Joint Attention-Output Models,0,[0]
The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20–30% for translation tasks.,3 Joint Attention-Output Models,0,[0]
We used the default pass-through TopK operator (which is not differentiable) and optimize the beamapproximation directly.,3 Joint Attention-Output Models,0,[0]
"We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beamapproximation directly leads to best results.
",3 Joint Attention-Output Models,0,[0]
We show empirically that this very simple scheme is surprisingly effective compared to existing hard and soft attention over several translation tasks.,3 Joint Attention-Output Models,0,[0]
"Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as softattention, without significant increase in training time because even K = 6 works well enough.
",3 Joint Attention-Output Models,0,[0]
"Another reason why our ’sum of probabilities’ form performs better could be the softmax barrier effect highlighted in (Yang et al., 2018).",3 Joint Attention-Output Models,0,[0]
The authors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix.,3 Joint Attention-Output Models,0,[0]
They improve performance using a Mixture of Softmax model.,3 Joint Attention-Output Models,0,[0]
Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax.,3 Joint Attention-Output Models,0,[0]
"However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.",3 Joint Attention-Output Models,0,[0]
We compare attention models on two NLP tasks: machine translation and morphological inflection.,4 Experiments,0,[0]
We experiment on five language pairs from three datasets:,4.1 Machine translation,0,[0]
"IWSLT15 English↔Vietnamese (Cettolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German↔English (Cettolo et al., 2014) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respectively ; Workshop on Asian Translation 2017 Japanese→English",4.1 Machine translation,0,[0]
"(Nakazawa et al., 2016) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively.",4.1 Machine translation,0,[0]
"We use a 2 layer bi-
directional encoder and a 2 layer unidirectional decoder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer.",4.1 Machine translation,0,[0]
We base our implementation2 on the NMT code3 in Tensorflow.,4.1 Machine translation,0,[0]
"We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.
",4.1 Machine translation,0,[0]
Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention.,4.1 Machine translation,0,[0]
"To further dissect the reasons behind beam-joint’s gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples.",4.1 Machine translation,0,[0]
We train samplejoint as well as hard-attention with REINFORCE with 6-samples.,4.1 Machine translation,0,[0]
"Also to ascertain that our gains are not explained by sparsity alone, we compare with Sparsemax (Martins and Astudillo, 2016).
",4.1 Machine translation,0,[0]
In Table 1 we show perplexity and BLEU with three beam sizes (B).,4.1 Machine translation,0,[0]
"Beam-joint significantly outperforms all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points.",4.1 Machine translation,0,[0]
The perplexity shows even a more impressive drop in all five datasets.,4.1 Machine translation,0,[0]
"Also we observe training times for beam-joint to be only 20–30% higher than softattention, establishing that beam-joint is both practical and more accurate.
",4.1 Machine translation,0,[0]
Sample-joint is much worse than beam-joint.,4.1 Machine translation,0,[0]
"Apart from the problem of high variance of gradients in the reinforce step, another problem is that sampling repeats states whereas TopK in beamjoint gets distinct states.",4.1 Machine translation,0,[0]
"Hard attention too faces training issues and performs worse than soft attention, explaining why it is not commonly used in NMT.",4.1 Machine translation,0,[0]
"Sample-joint is better than Hard attention, further highlighting the merits of the joint distribution.",4.1 Machine translation,0,[0]
Sparsemax is competitive but marginally worse than soft attention.,4.1 Machine translation,0,[0]
"This is concordant with the recent experiments of (Niculae and Blondel, 2017).
",4.1 Machine translation,0,[0]
Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention.,4.1 Machine translation,0,[0]
"Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32.",4.1 Machine translation,0,[0]
Figure 1a shows final BLEU of these methods as well as BLEU against increasing training steps.,4.1 Machine translation,0,[0]
"Beam-joint both converges faster and to a higher score than soft-
2https://github.com/sid7954/beam-joint-attention 3https://github.com/tensorflow/nmt
attention.",4.1 Machine translation,0,[0]
"For example by 10000 steps ( 5 epochs), beam-joint has surpassed soft-attention by almost 2 BLEU points (20 vs 22).",4.1 Machine translation,0,[0]
"Moreover beam-joint tracks full-joint well, and both converge finally to similar BLEUs near 27 against 26 for soft attention.",4.1 Machine translation,0,[0]
"This shows that an attention-beam of size 6 suffices to approximate full joint almost perfectly.
",4.1 Machine translation,0,[0]
"Next, in Figure 1b, we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets.",4.1 Machine translation,0,[0]
"For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.
",4.1 Machine translation,0,[0]
Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in Figure 2 on the En-Vi and De-En tasks.,4.1 Machine translation,0,[0]
On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly.,4.1 Machine translation,0,[0]
Similar behavior is observed in the other dataset.,4.1 Machine translation,0,[0]
"This shows that small K values like 6 suffice for translation.
",4.1 Machine translation,0,[0]
We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in Table 2.,4.1 Machine translation,0,[0]
"We used our models trained with K=6, and deployed them for test-time greedy decoding with K set to 1.",4.1 Machine translation,0,[0]
"Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention.",4.1 Machine translation,0,[0]
"One can observe that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrating that the performance gain is not solely due to the effect of ensembling or softmax-barrier.",4.1 Machine translation,0,[0]
"To demonstrate the use of this approach beyond translation, we next consider two morphological
inflection tasks.",4.2 Morphological Inflection,0,[0]
"We use (Durrett and DeNero, 2013)’s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V).",4.2 Morphological Inflection,0,[0]
The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each.,4.2 Morphological Inflection,0,[0]
"We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam(Kingma and Ba, 2014) and measure 0/1 accuracy for soft, hard and full-joint attention models.",4.2 Morphological Inflection,0,[0]
"Due to limited input length and vocabulary, we were able to run directly the full-joint model.",4.2 Morphological Inflection,0,[0]
"We also ran the 100 units wide two layer LSTM with hard-monotonic attention provided by (Aharoni and Goldberg, 2017) labeled Hard-Mono4.",4.2 Morphological Inflection,0,[0]
The table below shows that even for this task full-joint scores over existing attention models5.,4.2 Morphological Inflection,0,[0]
"The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention.
",4.2 Morphological Inflection,0,[0]
"Dataset Soft Hard HardMono
FullJoint
de-N 85.50 85.13 85.65 85.81 de-V 94.91 95.04 95.31 95.52
Conclusion
",4.2 Morphological Inflection,0,[0]
In this paper we showed a simple yet effective approximation of the joint attention-output distribution in sequence to sequence learning.,4.2 Morphological Inflection,0,[0]
Our joint model consistently provides higher accuracy without significant running time overheads in five translation and two morphological inflection tasks.,4.2 Morphological Inflection,0,[0]
"An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (Vaswani et al., 2017; Xu Chen, 2018).
",4.2 Morphological Inflection,0,[0]
"Acknowledgements We thank NVIDIA Corporation for supporting this research by the donation of Titan X GPU.
4https://github.com/roeeaharoni/morphologicalreinflection
5Our numbers are lower than earlier reported because ours use a single model whereas (Aharoni and Goldberg, 2017) and others report from an ensemble of five models.",4.2 Morphological Inflection,0,[0]
"In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning.",abstractText,0,[0]
The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention.,abstractText,0,[0]
On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.,abstractText,0,[0]
Surprisingly Easy Hard-Attention for Sequence to Sequence Learning,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
93
We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",text,0,[0]
"When we read a story, we bring to it a large body of implicit knowledge about the physical world.",1 Introduction,0,[0]
"For instance, given the context “on stage, a woman takes a seat at the piano,” shown in Table 1, we can easily infer what the situation might look like: a woman is giving a piano performance, with a crowd watching her.",1 Introduction,0,[0]
"We can furthermore infer her likely next action: she will most likely set her fingers on the piano keys and start playing.
",1 Introduction,0,[0]
"This type of natural language inference requires commonsense reasoning, substantially broadening the scope of prior work that focused primarily on
linguistic entailment (Chierchia and McConnellGinet, 2000).",1 Introduction,0,[0]
"Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed.",1 Introduction,0,[0]
"Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998).
",1 Introduction,0,[0]
A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset.,1 Introduction,0,[0]
"However, recent work has shown that human-written datasets are susceptible to annotation artifacts: unintended stylistic patterns that give out clues for the gold labels (Gururangan et al., 2018; Poliak et al., 2018).",1 Introduction,0,[0]
"As a result, models trained on such datasets with hu-
man biases run the risk of over-estimating the actual performance on the underlying task, and are vulnerable to adversarial or out-of-domain examples (Wang et al., 2018; Glockner et al., 2018).
",1 Introduction,0,[0]
"In this paper, we introduce Adversarial Filtering (AF), a new method to automatically detect and reduce stylistic artifacts.",1 Introduction,0,[0]
We use this method to construct Swag: an adversarial dataset with 113k multiple-choice questions.,1 Introduction,0,[0]
"We start with pairs of temporally adjacent video captions, each with a context and a follow-up event that we know is physically possible.",1 Introduction,0,[0]
We then use a state-of-theart language model fine-tuned on this data to massively oversample a diverse set of possible negative sentence endings (or counterfactuals).,1 Introduction,0,[0]
"Next, we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones.",1 Introduction,0,[0]
"Finally, these filtered counterfactuals are validated by crowd workers to further ensure data quality.
",1 Introduction,0,[0]
"Extensive empirical results demonstrate unique contributions of our dataset, complementing existing datasets for natural langauge inference (NLI) (Bowman et al., 2015; Williams et al., 2018) and commonsense reasoning (Roemmele et al., 2011; Mostafazadeh et al., 2016; Zhang et al., 2017).",1 Introduction,0,[0]
"First, our dataset poses a new challenge of grounded commonsense inference that is easy for humans (88%) while hard for current state-ofthe-art NLI models (<60%).",1 Introduction,0,[0]
"Second, our proposed adversarial filtering methodology allows for cost-effective construction of a large-scale dataset while substantially reducing known annotation artifacts.",1 Introduction,0,[0]
"The generality of adversarial filtering allows it to be applied to build future datasets, ensuring that they serve as reliable benchmarks.
",1 Introduction,0,[0]
"2 Swag: Our new dataset
We introduce a new dataset for studying physically grounded commonsense inference, called Swag.1",1 Introduction,0,[0]
Our task is to predict which event is most likely to occur next in a video.,1 Introduction,0,[0]
"More formally, a model is given a context c = (s,n): a complete sentence s and a noun phrase n that begins a second sentence, as well as a list of possible verb phrase sentence endings V = {v1, . . .",1 Introduction,0,[0]
",v4}.",1 Introduction,0,[0]
"See Figure 1 for an example triple (s,n,vi).",1 Introduction,0,[0]
"The model must then select the most appropriate verb phrase vî ∈ V .
1Short for Situations With Adversarial Generations.
",1 Introduction,0,[0]
"Overview Our corpus consists of 113k multiple choice questions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017).",1 Introduction,0,[0]
The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts).,1 Introduction,0,[0]
"For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors.",1 Introduction,0,[0]
"In this section, we outline the construction of Swag.",3 A solution to annotation artifacts,0,[0]
"We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases.",3 A solution to annotation artifacts,0,[0]
"For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance.
",3 A solution to annotation artifacts,0,[0]
"To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyapplicable treatment involving the iterative refinement of a set of assignments to increase the entropy under a chosen model family.",3 A solution to annotation artifacts,0,[0]
"We then discuss how we generate counterfactual endings, and
2We filter out sentences with rare tokens (≤3 occurrences), that are short (l ≤ 5), or that lack a verb phrase.
",3 A solution to annotation artifacts,0,[0]
Algorithm 1 Adversarial filtering (AF) of negative samples.,3 A solution to annotation artifacts,0,[0]
"During our experiments, we set Neasy = 2 for refining a population ofN− = 1023 negative examples to k = 9, and used a 80%/20% train/test split.
while convergence not reached do • Split the dataset D randomly up into training and testing portions Dtr and Dte. •",3 A solution to annotation artifacts,0,[0]
Optimize a model fθ on Dtr. for index i in Dte do •,3 A solution to annotation artifacts,0,[0]
Identify easy indices:,3 A solution to annotation artifacts,0,[0]
Aeasyi = {j ∈ Ai : fθ(x,3 A solution to annotation artifacts,0,[0]
+ i ) > fθ(x,3 A solution to annotation artifacts,0,[0]
"− i,j)}
• Replace N easy easy indices j ∈ Aeasyi with adversarial indices k 6∈",3 A solution to annotation artifacts,0,[0]
Ai satisfying fθ(x,3 A solution to annotation artifacts,0,[0]
"− i,k) > fθ(x",3 A solution to annotation artifacts,0,[0]
"− i,j).
end for end while
finally, the models used for filtering.",3 A solution to annotation artifacts,0,[0]
"In this section, we formalize what it means for a dataset to be adversarial.",3.1 Formal definition,0,[0]
"Intuitively, we say that an adversarial dataset for a model f is one on which f will not generalize, even if evaluated on test data from the same distribution.",3.1 Formal definition,0,[0]
"More formally, let our input space be X and the label space be Y .",3.1 Formal definition,0,[0]
"Our trainable classifier f , taking parameters θ is defined as fθ : X → R|Y|.",3.1 Formal definition,0,[0]
"Let our dataset of size N be defined as D = {(xi, yi)}1≤i≤N , and let the loss function over the dataset be L(fθ,D).",3.1 Formal definition,0,[0]
"We say that a dataset is adversarial with respect to f if we expect high empirical error I over all leave-one-out train/test splits (Vapnik, 2000):
I(D, f)",3.1 Formal definition,0,[0]
"= 1 N N∑ i=1 L(fθ?i , {(xi, yi)}), (1)
where θ?i = argmin θ L(fθ,D \ {(xi, yi)}), (2)
",3.1 Formal definition,0,[0]
with regularization terms omitted for simplicity.,3.1 Formal definition,0,[0]
"In this section, we outline an approach for generating an adversarial dataset D, effectively maximizing empirical error I with respect to a family of trainable classifiers f .",3.2 Adversarial filtering (AF) algorithm,0,[0]
"Without loss of generality, we consider the situation where we have N contexts, each associated with a single positive example (x+i , 1)∈X ×Y , and a large population of context-specific negative examples (x−i,j , 0)∈X ×Y , where 1≤j≤N− for each i. For instance, the negative examples could be incorrect relations in knowledge-base completion (Socher et al., 2013), or all words in a dictionary for a
single-word cloze task (Zweig and Burges, 2011).",3.2 Adversarial filtering (AF) algorithm,0,[0]
Our goal will be to filter the population of negative examples for each instance i to a size of k N−.,3.2 Adversarial filtering (AF) algorithm,0,[0]
"This will be captured by returning a set of assignments A, where for each instance the assignment will be a k-subset Ai = [1 . . .",3.2 Adversarial filtering (AF) algorithm,0,[0]
N−]k.,3.2 Adversarial filtering (AF) algorithm,0,[0]
"The filtered dataset will then be:
DAF = {(xi, 1), {(x−i,j , 0)}j∈Ai}1≤i≤N (3)
Unfortunately, optimizing I(DAF , f) is difficult as A is global and non-differentiable.",3.2 Adversarial filtering (AF) algorithm,0,[0]
"To address this, we present Algorithm 1.",3.2 Adversarial filtering (AF) algorithm,0,[0]
"On each iteration, we split the data into dummy ‘train’ and ‘test’ splits.",3.2 Adversarial filtering (AF) algorithm,0,[0]
"We train a model f on the training portion and obtain parameters θ, then use the remaining test portion to reassign the indices of A.",3.2 Adversarial filtering (AF) algorithm,0,[0]
"For each context, we replace some number of ‘easy’ negatives in A that fθ classifies correctly with ‘adversarial’ negatives outside ofA that fθ misclassifies.
",3.2 Adversarial filtering (AF) algorithm,0,[0]
"This process can be thought of as increasing the overall entropy of the dataset: given a strong model fθ that is compatible with a random subset of the data, we aim to ensure it cannot generalize to the held-out set.",3.2 Adversarial filtering (AF) algorithm,0,[0]
We repeat this for several iterations to reduce the generalization ability of the model family f over arbitrary train/test splits.,3.2 Adversarial filtering (AF) algorithm,0,[0]
"To generate counterfactuals for Swag, we use an LSTM (Hochreiter and Schmidhuber, 1997) language model (LM), conditioned on contexts from video captions.",3.3 Generating candidate endings,0,[0]
"We first pretrain on BookCorpus (Zhu et al., 2015), then finetune on the video caption datasets.",3.3 Generating candidate endings,0,[0]
The architecture uses standard best practices and was validated on held-out perplexity of the video caption datasets; details are in the appendix.,3.3 Generating candidate endings,0,[0]
"We use the LM to sample N−=1023 unique endings for a partial caption.3
Importantly, we greedily sample the endings, since beam search decoding biases the generated endings to be of lower perplexity (and thus easily distinguishable from found endings).",3.3 Generating candidate endings,0,[0]
"We find this process gives good counterfactuals: the generated endings tend to use topical words, but often make little sense physically, making them perfect for our task.",3.3 Generating candidate endings,0,[0]
"Further, the generated endings are marked as “gibberish” by humans only 9.1% of the time (Sec 3.5); in that case the ending is filtered out.
",3.3 Generating candidate endings,0,[0]
"3To ensure that the LM generates unique endings, we split the data into five validation folds and train five separate LMs, one for each set of training folds.",3.3 Generating candidate endings,0,[0]
This means that each LM never sees the found endings during training.,3.3 Generating candidate endings,0,[0]
"In creating Swag, we designed the model family f to pick up on low-level stylistic features that we posit should not be predictive of whether an event happens next in a video.",3.4 Stylistic models for adversarial filtering,0,[0]
"These stylistic features are an obvious case of annotation artifacts (Cai et al., 2017; Schwartz et al.,",3.4 Stylistic models for adversarial filtering,0,[0]
2017).4 Our final classifier is an ensemble of four stylistic models:,3.4 Stylistic models for adversarial filtering,0,[0]
1,3.4 Stylistic models for adversarial filtering,0,[0]
A multilayer perceptron (MLP) given LM perplexity features and context/ending lengths.,3.4 Stylistic models for adversarial filtering,0,[0]
2,3.4 Stylistic models for adversarial filtering,0,[0]
A bag-of-words model that averages the word embeddings of the second sentence as features.,3.4 Stylistic models for adversarial filtering,0,[0]
3,3.4 Stylistic models for adversarial filtering,0,[0]
"A one-layer CNN, with filter sizes ranging from 2-5, over the second sentence.",3.4 Stylistic models for adversarial filtering,0,[0]
4,3.4 Stylistic models for adversarial filtering,0,[0]
A bidirectional LSTM over the 100 most common words in the second sentence; uncommon words are replaced by their POS tags.,3.4 Stylistic models for adversarial filtering,0,[0]
We ensemble the models by concatenating their final representations and passing it through an MLP.,3.4 Stylistic models for adversarial filtering,0,[0]
"On every adversarial iteration, the ensemble is trained jointly to minimize cross-entropy.
",3.4 Stylistic models for adversarial filtering,0,[0]
"The accuracies of these models (at each iteration, evaluated on a 20% split of the test dataset before indices of A get remapped) are shown in Figure 2.",3.4 Stylistic models for adversarial filtering,0,[0]
"Performance decreases from 60% to close to random chance; moreover, confusing the perplexity-based MLP is not sufficient to lower performance of the ensemble.",3.4 Stylistic models for adversarial filtering,0,[0]
"Only once the other stylistic models are added does the ensemble accuracy drop substantially, suggesting that our approach is effective at reducing stylistic artifacts.
",3.4 Stylistic models for adversarial filtering,0,[0]
"4A broad definition of annotation artifacts might include aspects besides lexical/stylistic features: for instance, certain events are less likely semantically regardless of the context (e.g. riding a horse using a hose).",3.4 Stylistic models for adversarial filtering,0,[0]
"For this work, we erred more conservatively and only filtered based on style.",3.4 Stylistic models for adversarial filtering,0,[0]
The final data-collection step is to have humans verify the data.,3.5 Human verification,0,[0]
"Workers on Amazon Mechanical Turk were given the caption context, as well as six candidate endings: one found ending and five adversarially-sampled endings.",3.5 Human verification,0,[0]
The task was twofold:,3.5 Human verification,0,[0]
"Turkers ranked the endings independently as likely, unlikely, or gibberish, and selected the best and second best endings (Fig 3).
",3.5 Human verification,0,[0]
We obtained the correct answers to each context in two ways.,3.5 Human verification,0,[0]
"If a Turker ranks the found ending as either best or second best (73.7% of the time), we add the found ending as a gold example, with negatives from the generations not labelled best or gibberish.",3.5 Human verification,0,[0]
"Further, if a Turker ranks a generated ending as best, and the found ending as second best, then we have reason to believe that the generation is good.",3.5 Human verification,0,[0]
"This lets us add an additional training example, consisting of the generated best ending as the gold, and remaining generations as negatives.5 Examples with ≤3 nongibberish endings were filtered out.6
We found after 1000 examples that the annotators tended to have high agreement, also generally choosing found endings over generations (see Table 2).",3.5 Human verification,0,[0]
"Thus, we collected the remaining 112k examples with one annotator each, periodically verifying that annotators preferred the found endings.",3.5 Human verification,0,[0]
"In this section, we evaluate the performance of various NLI models on Swag.",4 Experiments,0,[0]
"Recall that models
5These two examples share contexts.",4 Experiments,0,[0]
"To prevent biasing the test and validation sets, we didn’t perform this procedure on answers from the evaluation sets’ context.
",4 Experiments,0,[0]
"6To be data-efficient, we reannotated filtered-out examples by replacing gibberish endings, as well as generations that outranked the found ending, with candidates from A.
for our dataset take the following form: given a sentence and a noun phrase as context c = (s,n), as well as a list of possible verb phrase endings V = {v1, . . .",4 Experiments,0,[0]
",v4}, a model fθ must select a verb î that hopefully matches igold:
î =",4 Experiments,0,[0]
"argmax i fθ(s,n,vi) (4)
To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi, or the entire second sentence (n,vi).",4 Experiments,0,[0]
"For our learned models, we train f by minimizing multi-class cross-entropy.",4 Experiments,0,[0]
"We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018).",4 Experiments,0,[0]
"We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data.",4 Experiments,0,[0]
See the appendix for more details.,4 Experiments,0,[0]
"The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage.",4.1 Unary models,0,[0]
a.,4.1 Unary models,0,[0]
"fastText (Joulin et al., 2017):",4.1 Unary models,0,[0]
"This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7",4.1 Unary models,0,[0]
"b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent
7The fastText model is trained using binary cross-entropy; at test time we extract the prediction by selecting the ending with the highest positive likelihood under the model.
",4.1 Unary models,0,[0]
"(Conneau et al., 2017).",4.1 Unary models,0,[0]
"SkipThoughts was trained by predicting adjacent sentences in book data, whereas InferSent was trained on supervised NLI data.",4.1 Unary models,0,[0]
"For each second sentence (or just the ending), we feed the encoding into an MLP.",4.1 Unary models,0,[0]
c. LSTM sentence encoder,4.1 Unary models,0,[0]
"Given an arbitrary span of text, we run a two-layer BiLSTM over it.",4.1 Unary models,0,[0]
"The final hidden states are then max-pooled to obtain a fixed-size representation, which is then used to predict the potential for that ending.",4.1 Unary models,0,[0]
The following models predict labels from two spans of text.,4.2 Binary models,0,[0]
"We consider two possibilties for these models: using just the second sentence, where the two text spans are n,vi, or using the context and the second sentence, in which case the spans are s, (n,vi).",4.2 Binary models,0,[0]
The latter case includes many models developed for the NLI task.,4.2 Binary models,0,[0]
"d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c,vi).",4.2 Binary models,0,[0]
We model the probability of picking an ending i using a bilinear model: softmaxi(cWvTi ).,4.2 Binary models,0,[0]
"8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations.",4.2 Binary models,0,[0]
"f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017).",4.2 Binary models,0,[0]
"We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example.",4.2 Binary models,0,[0]
We then train a log-linear model using these 3-way probabilities as features.,4.2 Binary models,0,[0]
g. SNLI models (retrained),4.2 Binary models,0,[0]
"Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi) with a softmax over i.",4.2 Binary models,0,[0]
We also considered the following models:,4.3 Other models,0,[0]
h. Length:,4.3 Other models,0,[0]
"Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias.",4.3 Other models,0,[0]
"For this baseline, we always choose the shortest ending.",4.3 Other models,0,[0]
i. ConceptNet,4.3 Other models,0,[0]
"As our task requires world knowledge, we tried a rule-based system on top of the
8We also tried using an MLP, but got worse results.
",4.3 Other models,0,[0]
"ConceptNet knowledge base (Speer et al., 2017).",4.3 Other models,0,[0]
"For an ending sentence, we use the spaCy dependency parser to extract the head verb and its dependent object.",4.3 Other models,0,[0]
The ending score is given by the number of ConceptNet causal relations9 between synonyms of the verb and synonyms of the object.,4.3 Other models,0,[0]
"j. Human performance To benchmark human performance, five Mechanical Turk workers were asked to answer 100 dataset questions, as did an ‘expert’ annotator (the first author of this paper).",4.3 Other models,0,[0]
Predictions were combined using a majority vote.,4.3 Other models,0,[0]
We present our results in Table 3.,4.4 Results,0,[0]
"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings, which obtains 43.6%.",4.4 Results,0,[0]
"This model, as with most models studied, greatly improves with more context: by 3.1% when given the initial noun phrase, and by an ad-
9We used the relations ‘Causes’, ‘CapableOf’, ‘ReceivesAction’, ‘UsedFor’, and ‘HasSubevent’.",4.4 Results,0,[0]
"Though their coverage is low (30.4% of questions have an answer with≥1 causal relation), the more frequent relations in ConceptNet, such as ‘IsA’, at best only indirectly relate to our task.
",4.4 Results,0,[0]
ditional 4% when also given the first sentence.,4.4 Results,0,[0]
Further improvement is gained from models that compute pairwise representations of the inputs.,4.4 Results,0,[0]
"While the simplest such model, DualBoW, obtains only 35.1% accuracy, combining InferSent sentence representations gives 40.5% accuracy (InferSent-Bilinear).",4.4 Results,0,[0]
"The best results come from pairwise NLI models: when fully trained on Swag, ESIM+ELMo obtains 59.2% accuracy.
",4.4 Results,0,[0]
"When comparing machine results to human results, we see there exists a lot of headroom.",4.4 Results,0,[0]
"Though there likely is some noise in the task, our results suggest that humans (even untrained) converge to a consensus.",4.4 Results,0,[0]
"Our in-house “expert” annotator is outperformed by an ensemble of 5 Turk workers (with 88% accuracy); thus, the effective upper bound on our dataset is likely even higher.",4.4 Results,0,[0]
"5.1 Swag versus existing NLI datasets The past few years have yielded great advances in NLI and representation learning, due to the availability of large datasets like SNLI and MultiNLI
(Bowman et al., 2015; Williams et al., 2018).",5 Analysis,0,[0]
"With the release of Swag, we hope to continue this trend, particularly as our dataset largely has the same input/output format as other NLI datasets.",5 Analysis,0,[0]
"We observe three key differences between our dataset and others in this space:
First, as noted in Section 1, Swag requires a unique type of temporal reasoning.",5 Analysis,0,[0]
"A state-of-theart NLI model such as ESIM, when bottlenecked through the SNLI notion of entailment (SNLIESIM), only obtains 36.1% accuracy.10 This implies that these datasets necessitate different (and complementary) forms of reasoning.
",5 Analysis,0,[0]
"Second, our use of videos results in wide coverage of dynamic and temporal situations Compared with SNLI, with contexts from Flickr30K (Plummer et al., 2017) image captions, Swag has more active verbs like ‘pull’ and ‘hit,’ and fewer static verbs like ‘sit’ and ‘wear’ (Figure 4).11
Third, our dataset suffers from few lexical biases.",5 Analysis,0,[0]
"Whereas fastText, a bag of n-gram model, obtains 67.0% accuracy on SNLI versus a 34.3% baseline (Gururangan et al., 2018), fastText obtains only 29.0% accuracy on Swag.12",5 Analysis,0,[0]
"We sought to quantify how human judgments differ from the best studied model, ESIM+ELMo.",5.2 Error analysis,0,[0]
"We randomly sampled 100 validation questions
10The weights of SNLI-ESIM pick up primarily on entailment probability (0.59), as with neutral (0.46), while contradiction is negatively correlated (-.42).
",5.2 Error analysis,0,[0]
"11Video data has other language differences; notably, character names in LSMDC were replaced by ‘someone’
12The most predictive individual words on SWAG are infrequent in number: ‘dotted‘ with P(+|dotted) = 77% with 10.3 counts, and P(−|similar)",5.2 Error analysis,0,[0]
= 81% with 16.3 counts.,5.2 Error analysis,0,[0]
"(Counts from negative endings were discounted 3x, as there are 3 times as many negative endings as positive endings).
",5.2 Error analysis,0,[0]
"that ESIM+ELMo answered incorrectly, for each extracting both the gold ending and the model’s preferred ending.",5.2 Error analysis,0,[0]
"We asked 5 Amazon Mechanical Turk workers to pick the better ending (of which they preferred the gold endings 94% of the time) and to select one (or more) multiple choice reasons explaining why the chosen answer was better.
",5.2 Error analysis,0,[0]
"The options, and the frequencies, are outlined in Table 4.",5.2 Error analysis,0,[0]
"The most common reason for the turkers preferring the correct answer is situational (52.3% of the time), followed by weirdness (17.5%) and plausibility (14.4%).",5.2 Error analysis,0,[0]
"This suggests that ESIM+ELMo already does a good job at filtering out weird and implausible answers, with the main bottleneck being grounded physical understanding.",5.2 Error analysis,0,[0]
"The ambiguous percentage is also relatively low (12.0%), implying significant headroom.",5.2 Error analysis,0,[0]
"Last, we show several qualitative examples in Table 5.",5.3 Qualitative examples,0,[0]
"Though models can do decently well by identifying complex alignment patterns between the two sentences (e.g. being “up a tree” implies that “tree” is the end phrase), the incorrect model predictions suggest this strategy is insuffi-
cient.",5.3 Qualitative examples,0,[0]
"For instance, answering “An old man rides a small bumper car” requires knowledge about bumper cars and how they differ from regular cars: bumper cars are tiny, don’t drive on roads, and don’t work in parking lots, eliminating the alternatives.",5.3 Qualitative examples,0,[0]
"However, this knowledge is difficult to extract from existing corpora: for instance, the ConceptNet entry for Bumper Car has only a single relation: bumper cars are a type of vehicle.",5.3 Qualitative examples,0,[0]
"Other questions require intuitive physical reasoning: e.g, for “he pours the raw egg batter into the pan,” about what happens next in making an omelet.",5.3 Qualitative examples,0,[0]
Our results suggest that Swag is a challenging testbed for NLI models.,5.4 Where to go next?,0,[0]
"However, the adversarial models used to filter the dataset are purely stylistic and focus on the second sentence; thus, subtle artifacts still likely remain in our dataset.",5.4 Where to go next?,0,[0]
"These patterns are ostensibly picked up by the NLI models (particularly when using ELMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset.",5.4 Where to go next?,0,[0]
"As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models.",5.4 Where to go next?,0,[0]
"Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018).",6 Related Work,0,[0]
"Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components
for performing better video captioning (Pasunuru and Bansal, 2017), summarization (Pasunuru and Bansal, 2018), and generation (Holtzman et al., 2018), confirming the importance of NLI research.",6 Related Work,0,[0]
"The NLI task requires a variety of commonsense knowledge (LoBue and Yates, 2011), which our work complements.",6 Related Work,0,[0]
"However, previous datasets for NLI have been challenged by unwanted annotation artifacts, (Gururangan et al., 2018; Poliak et al., 2018) or scale issues.",6 Related Work,0,[0]
"Our work addresses these challenges by constructing a new NLI benchmark focused on grounded commonsense reasoning, and by introducing an adversarial filtering mechanism that substantially reduces known and easily detectable annotation artifacts.
",6 Related Work,0,[0]
"Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)).",6 Related Work,0,[0]
These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model.,6 Related Work,0,[0]
"In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans.",6 Related Work,0,[0]
"In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality.
",6 Related Work,0,[0]
"13For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories.
",6 Related Work,0,[0]
"Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about.
",6 Related Work,0,[0]
Vision datasets Several resources have been introduced to study temporal inference in vision.,6 Related Work,0,[0]
"The Visual Madlibs dataset has 20k image captions about hypothetical next/previous events (Yu et al., 2015); similar to our work, the test portion is multiple-choice, with counterfactual answers retrieved from similar images and verified by humans.",6 Related Work,0,[0]
"The question of ‘what will happen next?’ has also been studied in photo albums (Huang et al., 2016), videos of team sports, (Felsen et al., 2017) and egocentric dog videos (Ehsani et al., 2018).",6 Related Work,0,[0]
"Last, annotation artifacts are also a recurring problem for vision datasets such as Visual Genome (Zellers et al., 2018) and Visual QA (Jabri et al., 2016); recent work was done to create a more challenging VQA dataset by annotating complementary image pairs (Goyal et al., 2016).
",6 Related Work,0,[0]
"Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017).",6 Related Work,0,[0]
"Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017).",6 Related Work,0,[0]
"However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017).",6 Related Work,0,[0]
"Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias.
",6 Related Work,0,[0]
"Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax parses (Forbes and Choi, 2017), word embeddings (Lucy and Gauthier, 2017), and images and dictionary definitions (Zellers and Choi, 2017).",6 Related Work,0,[0]
"An alternate thread of work has been to learn scripts: high-level representations of event chains (Schank and Abelson, 1975; Chambers and Jurafsky, 2009).",6 Related Work,0,[0]
"Swag evaluates both of these strands.
",6 Related Work,0,[0]
14Prior work on sentence completion filtered negatives with heuristics based on LM perplexities.,6 Related Work,0,[0]
"We initially tried something similar, but found the result to still be gameable.",6 Related Work,0,[0]
We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning.,7 Conclusion,0,[0]
"To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions.",7 Conclusion,0,[0]
"Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that can be easily detected by a committee of strong baseline models.",7 Conclusion,0,[0]
"Our adversarial filtering paradigm is general, allowing potential applications to other datasets that require human composition of question answer pairs.",7 Conclusion,0,[0]
"We thank the anonymous reviewers, members of the ARK and xlab at the University of Washington, researchers at the Allen Institute for AI, and Luke Zettlemoyer for their helpful feedback.",Acknowledgements,0,[0]
We also thank the Mechanical Turk workers for doing a fantastic job with the human validation.,Acknowledgements,0,[0]
"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook.",Acknowledgements,0,[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.",Acknowledgements,0,[0]
"Given a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (“then, she examined the engine”).",abstractText,0,[0]
"In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.",abstractText,0,[0]
"We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations.",abstractText,0,[0]
"To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data.",abstractText,0,[0]
"To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals.",abstractText,0,[0]
"Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task.",abstractText,0,[0]
We provide comprehensive analysis that indicates significant opportunities for future research.,abstractText,0,[0]
Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856–861 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
856
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",text,0,[0]
Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms.,1 Introduction and Related Work,0,[0]
"While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016).",1 Introduction and Related Work,0,[0]
"In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are both widely utilized and highly effective (Huang et al., 2016; Zagoruyko and Komodakis, 2016).
",1 Introduction and Related Work,0,[0]
"However, it is nontrivial to find simple equivalences for NLP tasks like machine translation, because even slight modifications of sentences can result in significant changes in their semantics, or
*: Equal contributions.
require corresponding changes in the translations in order to keep the data consistent.",1 Introduction and Related Work,0,[0]
"In fact, indiscriminate modifications of data in NMT can introduce noise that makes NMT systems brittle (Belinkov and Bisk, 2018).
",1 Introduction and Related Work,0,[0]
"Due to such difficulties, the literature in data augmentation for NMT is relatively scarce.",1 Introduction and Related Work,0,[0]
"To our knowledge, data augmentation techniques for NMT fall into two categories.",1 Introduction and Related Work,0,[0]
"The first category is based on back-translation (Sennrich et al., 2016b; Poncelas et al., 2018), which utilizes monolingual data to augment a parallel training corpus.",1 Introduction and Related Work,0,[0]
"While effective, back-translation is often vulnerable to errors in initial models, a common problem of self-training algorithms (Chapelle et al., 2009).",1 Introduction and Related Work,0,[0]
The second category is based on word replacements.,1 Introduction and Related Work,0,[0]
"For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly.",1 Introduction and Related Work,0,[0]
"While this method generates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets.",1 Introduction and Related Work,0,[0]
"Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.
",1 Introduction and Related Work,0,[0]
"In this paper, we derive an extremely simple and efficient data augmentation technique for NMT.",1 Introduction and Related Work,0,[0]
"First, we formulate the design of a data augmentation algorithm as an optimization problem, where we seek the data augmentation policy that maximizes an objective that encourages two desired properties: smoothness and diversity.",1 Introduction and Related Work,0,[0]
"This optimization problem has a tractable analytic solution,
which describes a generic framework of which both word dropout and RAML are instances.",1 Introduction and Related Work,0,[0]
"Second, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the target sentence by other words uniformly sampled from the source and the target vocabularies, respectively.",1 Introduction and Related Work,0,[0]
"Experiments show that this method, which we name SwitchOut, consistently improves over strong baselines on datasets of different scales, including the large-scale WMT 15 English-German dataset, and two medium-scale datasets: IWSLT 2016 German-English and IWSLT 2015 EnglishVietnamese.",1 Introduction and Related Work,0,[0]
"We use uppercase letters, such as X , Y , etc., to denote random variables and lowercase letters such as x, y, etc., to denote the corresponding actual values.",2.1 Notations,0,[0]
"Additionally, since we will discuss a data augmentation algorithm, we will use a hat to denote augmented variables and their values, e.g. bX , bY , bx, by, etc.",2.1 Notations,0,[0]
"We will also use boldfaced characters, such as p, q, etc., to denote probability distributions.",2.1 Notations,0,[0]
We facilitate our discussion with a probabilistic framework that motivates data augmentation algorithms.,2.2 Data Augmentation,0,[0]
"With X , Y being the sequences of words in the source and target languages (e.g. in machine translation), the canonical MLE framework maximizes the objective
JMLE(✓) =",2.2 Data Augmentation,0,[0]
"E x,y⇠bp(X,Y )",2.2 Data Augmentation,0,[0]
"[logp✓(y|x)] .
",2.2 Data Augmentation,0,[0]
"Here bp(X,Y ) is the empirical distribution over all training data pairs (x, y) and p
✓ (y|x) is a parameterized distribution that we aim to learn, e.g. a neural network.",2.2 Data Augmentation,0,[0]
"A potential weakness of MLE is the mismatch between bp(X,Y ) and the true data distribution p(X,Y ).",2.2 Data Augmentation,0,[0]
"Specifically, bp(X,Y ) is usually a bootstrap distribution defined only on the observed training pairs, while p(X,Y ) has a much larger support, i.e. the entire space of valid pairs.",2.2 Data Augmentation,0,[0]
"This issue can be dramatic when the empirical observations are insufficient to cover the data space.
",2.2 Data Augmentation,0,[0]
"In practice, data augmentation is often used to remedy this support discrepancy by supplying additional training pairs.",2.2 Data Augmentation,0,[0]
"Formally, let q( bX, bY ) be the augmented distribution defined on a larger support than the empirical distribution bp(X,Y ).",2.2 Data Augmentation,0,[0]
"Then,
MLE training with data augmentation maximizes
JAUG(✓) = Ebx,by⇠q( bX,bY )",2.2 Data Augmentation,0,[0]
"[logp✓(by|bx)] .
",2.2 Data Augmentation,0,[0]
"In this work, we focus on a specific family of q, which depends on the empirical observations by
q( bX, bY ) =",2.2 Data Augmentation,0,[0]
"E x,y⇠bp(x,y)
h q( bX, bY |x, y) i .
",2.2 Data Augmentation,0,[0]
"This particular choice follows the intuition that an augmented pair (bx, by) that diverges too far from any observed data is more likely to be invalid and thus harmful for training.",2.2 Data Augmentation,0,[0]
The reason will be more evident later.,2.2 Data Augmentation,0,[0]
"Certainly, not all q are equally good, and the more similar q is to p, the more desirable q will be.",2.3 Diverse and Smooth Augmentation,0,[0]
"Unfortunately, we only have access to limited observations captured by bp.",2.3 Diverse and Smooth Augmentation,0,[0]
"Hence, in order to use q to bridge the gap between bp and p, it is necessary to utilize some assumptions about p. Here, we exploit two highly generic assumptions, namely:
• Diversity: p(X,Y ) has a wider support set, which includes samples that are more diverse than those in the empirical observation set.
",2.3 Diverse and Smooth Augmentation,0,[0]
"• Smoothness: p(X,Y ) is smooth, and similar (x, y) pairs will have similar probabilities.
",2.3 Diverse and Smooth Augmentation,0,[0]
"To formalize both assumptions, let s(bx, by;x, y) be a similarity function that measures how similar an augmented pair (bx, by) is to an observed data pair (x, y).",2.3 Diverse and Smooth Augmentation,0,[0]
"Then, an ideal augmentation policy q( bX, bY |x, y) should have two properties.",2.3 Diverse and Smooth Augmentation,0,[0]
"First, based on the smoothness assumption, if an augmented pair (bx, by) is more similar to an empirical pair (x, y), it is more likely that (bx, by) is sampled under the true data distribution p(X,Y ), and thus q( bX, bY |x, y) should assign a significant amount of probability mass to (bx, by).",2.3 Diverse and Smooth Augmentation,0,[0]
"Second, to quantify the diversity assumption, we propose that the entropy H[q( bX, bY |x, y)] should be large, so that the support of q( bX, bY ) is larger than the support of bp and thus is closer to the support p(X,Y ).",2.3 Diverse and Smooth Augmentation,0,[0]
"Combining these assumptions implies that q( bX, bY |x, y) should maximize the objective
J(q;x, y) = Ebx,by⇠q( bX,bY |x,y) ⇥",2.3 Diverse and Smooth Augmentation,0,[0]
"s(bx, by;x, y) ⇤
+ ⌧H(q( bX, bY |x, y)), (1)
where ⌧ controls the strength of the diversity objective.",2.3 Diverse and Smooth Augmentation,0,[0]
"The first term in (1) instantiates the smoothness assumption, which encourages q to draw samples that are similar to (x, y).",2.3 Diverse and Smooth Augmentation,0,[0]
"Meanwhile, the second term in (1) encourages more diverse samples from q. Together, the objective J(q;x, y) extends the information in the “pivotal” empirical sample (x, y) to a diverse set of similar cases.",2.3 Diverse and Smooth Augmentation,0,[0]
"This echoes our particular parameterization of q in Section 2.2.
",2.3 Diverse and Smooth Augmentation,0,[0]
"The objective J(q;x, y) in (1) is the canonical maximum entropy problem that one often encounters in deriving a max-ent model (Berger et al., 1996), which has the analytic solution:
q⇤(bx, by|x, y) = exp {s(bx, by;x, y)/⌧}P bx0,by0 exp {s(bx0, by0;x, y)/⌧}
(2) Note that (2) is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good.",2.3 Diverse and Smooth Augmentation,0,[0]
"Next, we will show that some existing algorithms can be seen as specific instantiations under our framework.",2.3 Diverse and Smooth Augmentation,0,[0]
"Moreover, this leads us to propose a novel and effective data augmentation algorithm.",2.3 Diverse and Smooth Augmentation,0,[0]
Word Dropout.,2.4 Existing and New Algorithms,0,[0]
"In the context of machine translation, Sennrich et al. (2016a) propose to randomly choose some words in the source and/or target sentence, and set their embeddings to 0 vectors.",2.4 Existing and New Algorithms,0,[0]
"Intuitively, it regards every new data pair generated by this procedure as similar enough and then includes them in the augmented training set.",2.4 Existing and New Algorithms,0,[0]
"Formally, word dropout can be seen as an instantiation of our framework with a particular similarity function s(x̂, ŷ;x, y) (see Appendix A.1).
RAML.",2.4 Existing and New Algorithms,0,[0]
"From the perspective of reinforcement learning, Norouzi et al. (2016) propose to train the model distribution to match a target distribution proportional to an exponentiated reward.",2.4 Existing and New Algorithms,0,[0]
"Despite the difference in motivation, it can be shown (c.f. Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(bx, by;x, y) = r(by; y) if bx = x and 1 otherwise.",2.4 Existing and New Algorithms,0,[0]
"Here, r is a task-specific reward function which measures the similarity between by and y. Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.
SwitchOut.",2.4 Existing and New Algorithms,0,[0]
"After reviewing the two existing augmentation schemes, there are two immediate
insights.",2.4 Existing and New Algorithms,0,[0]
"Firstly, augmentation should not be restricted to only the source side or the target side.",2.4 Existing and New Algorithms,0,[0]
"Secondly, being able to incorporate prior knowledge, such as the task-specific reward function r in RAML, can lead to a better similarity measure.
",2.4 Existing and New Algorithms,0,[0]
"Motivated by these observations, we propose to perform augmentation in both source and target domains.",2.4 Existing and New Algorithms,0,[0]
"For simplicity, we separately measure the similarity between the pair (bx, x) and the pair (by, y) and then sum them together, i.e.
s(bx, by;x, y)/⌧ ⇡ r x (bx, x)/⌧ x + r y (by, y)/⌧ y , (3)
where r x and r y are domain specific similarity functions and ⌧
x , ⌧ y are hyper-parameters that absorb the temperature parameter ⌧ .",2.4 Existing and New Algorithms,0,[0]
"This allows us to factor q⇤(bx, by|x, y) into:
q⇤(bx, by|x, y) = exp {rx(bx, x)/⌧x}P bx0 exp {rx(bx0, x)/⌧x}
⇥",2.4 Existing and New Algorithms,0,[0]
"exp {ry(by, y)/⌧y}P by0 exp {ry(by0, y)/⌧y}
(4)
",2.4 Existing and New Algorithms,0,[0]
"In addition, notice that this factored formulation allows bx and by to be sampled independently.
",2.4 Existing and New Algorithms,0,[0]
Sampling Procedure.,2.4 Existing and New Algorithms,0,[0]
"To complete our method, we still need to define r
x and r y , and then design a practical sampling scheme from each factor in (4).",2.4 Existing and New Algorithms,0,[0]
"Though non-trivial, both problems have been (partially) encountered in RAML (Norouzi et al., 2016; Ma et al., 2017).",2.4 Existing and New Algorithms,0,[0]
"For simplicity, we follow previous work to use the negative Hamming distance for both r
x and r y .",2.4 Existing and New Algorithms,0,[0]
"For a more parallelized implementation, we sample an augmented sentence bs from a true sentence s as follows:
1.",2.4 Existing and New Algorithms,0,[0]
"Sample bn 2 {0, 1, ..., |s|} by p(bn) /",2.4 Existing and New Algorithms,0,[0]
"e bn/⌧ .
",2.4 Existing and New Algorithms,0,[0]
2,2.4 Existing and New Algorithms,0,[0]
"For each i 2 {1, 2, ..., |s|}, with probability bn/ |s|, we can replace s
i by a uniform bs",2.4 Existing and New Algorithms,0,[0]
"i 6= s i .
",2.4 Existing and New Algorithms,0,[0]
"This procedure guarantees that any two sentences bs1 and bs2 with the same Hamming distance to s have the same probability, but slightly changes the relative odds of sentences with different Hamming distances to s from the true distribution by negative Hamming distance, and thus is an approximation of the actual distribution.",2.4 Existing and New Algorithms,0,[0]
"However, this efficient sampling procedure is much easier to implement while achieving good performance.
",2.4 Existing and New Algorithms,0,[0]
"Algorithm 1 illustrates this sampling procedure, which can be applied independently and in parallel for each batch of source sentences and target
sentences.",2.4 Existing and New Algorithms,0,[0]
"Additionally, we open source our implementation in TensorFlow and in PyTorch (respectively in Appendix A.5 and A.6).
",2.4 Existing and New Algorithms,0,[0]
Algorithm 1: Sampling with SwitchOut.,2.4 Existing and New Algorithms,0,[0]
"Input : s: a sentence represented by vocab integral ids,
⌧ : the temperature, V : the vocabulary Output : bs: a sentence with words replaced
1 Function HammingDistanceSample(s, ⌧ , |V |): 2 Let Z(⌧) P|s| n=0 e
n/⌧ be the partition function.",2.4 Existing and New Algorithms,0,[0]
"3 Let p(n) e n/⌧/Z(⌧) for n = 0, 1, ..., |s|.",2.4 Existing and New Algorithms,0,[0]
4 Sample bn ⇠ p(n).,2.4 Existing and New Algorithms,0,[0]
5,2.4 Existing and New Algorithms,0,[0]
"In parallel, do: 6 Sample a
i ⇠ Bernoulli(bn/ |s|).",2.4 Existing and New Algorithms,0,[0]
7,2.4 Existing and New Algorithms,0,[0]
"if a
i = 1 then 8 bs
i Uniform(V \{s i })",2.4 Existing and New Algorithms,0,[0]
.,2.4 Existing and New Algorithms,0,[0]
"9 else
10 bs",2.4 Existing and New Algorithms,0,[0]
i s i .,2.4 Existing and New Algorithms,0,[0]
11 end 12 return bs,2.4 Existing and New Algorithms,0,[0]
Datasets.,3 Experiments,0,[0]
We benchmark SwitchOut on three translation tasks of different scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de).,3 Experiments,0,[0]
All translations are wordbased.,3 Experiments,0,[0]
"These tasks and pre-processing steps are standard, used in several previous works.",3 Experiments,0,[0]
"Detailed statistics and pre-processing schemes are in Appendix A.3.
",3 Experiments,0,[0]
Models and Experimental Procedures.,3 Experiments,0,[0]
"Our translation model, i.e. p
✓ (y|x), is a Transformer network (Vaswani et al., 2017).",3 Experiments,0,[0]
"For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results.",3 Experiments,0,[0]
(w.r.t.,3 Experiments,0,[0]
Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)),3 Experiments,0,[0]
.,3 Experiments,0,[0]
"Then, fixing all hyper-parameters, and fixing ⌧
y = 0, we tune the ⌧ x rate, which controls how far we are willing to let bx deviate from x.",3 Experiments,0,[0]
"Our hyper-parameters are listed in Appendix A.4.
Baselines.",3 Experiments,0,[0]
"While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4.",3 Experiments,0,[0]
"Additionally, on the en-de task, we compare SwitchOut against back-translation (Sennrich et al., 2016b).
",3 Experiments,0,[0]
SwitchOut vs. Word Dropout and RAML.,3 Experiments,0,[0]
"We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in Table 1.",3 Experiments,0,[0]
"To account for variance, we run each experiment multiple times and report the median BLEU.",3 Experiments,0,[0]
"Specifically, each experiment without SwitchOut is run for 4 times, while each experiment with SwitchOut is run for 9 times due to its inherently higher variance.",3 Experiments,0,[0]
"We also conduct pairwise statistical significance tests using paired bootstrap (Clark et al., 2011), and record the results in Table 1.",3 Experiments,0,[0]
"For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut.",3 Experiments,0,[0]
"For the remaining two settings, the differences are not statistically significant.",3 Experiments,0,[0]
The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p < 0.0002).,3 Experiments,0,[0]
"Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.
SwitchOut vs. Back Translation.",3 Experiments,0,[0]
"Traditionally, data-augmentation is viewed as a method to enlarge the training datasets (Krizhevsky et al., 2012; Szegedy et al., 2014).",3 Experiments,0,[0]
"In the context of neural MT, Sennrich et al. (2016b) propose to use artificial data generated from a weak back-translation model, effectively utilizing monolingual data to enlarge the bilingual training datasets.",3 Experiments,0,[0]
"In connection, we compare SwitchOut against back translation.",3 Experiments,0,[0]
"We only compare SwitchOut against back translation on the en-de task, where the amount of bilingual training data is already sufficiently large2.",3 Experiments,0,[0]
"The
2We add the extra monolingual data from http://data.statmt.org/rsennrich/wmt16_ backtranslations/en-de/
BLEU scores with back-translation are reported in Table 2.",3 Experiments,0,[0]
These results provide two insights.,3 Experiments,0,[0]
"First, the gain delivered by back translation is less significant than the gain delivered by SwitchOut.",3 Experiments,0,[0]
"Second, SwitchOut and back translation are not mutually exclusive, as one can additionally apply SwitchOut on the additional data obtained from back translation to further improve BLEU scores.
",3 Experiments,0,[0]
Effects of ⌧ x and ⌧ y .,3 Experiments,0,[0]
We empirically study the effect of these temperature parameters.,3 Experiments,0,[0]
"During the tuning process, we translate the dev set of the tasks and report the BLEU scores in Figure 1.",3 Experiments,0,[0]
"We observe that when fixing ⌧
y , the best performance is always achieved with a non-zero ⌧
.
",3 Experiments,0,[0]
Where does SwitchOut Help the Most?,3 Experiments,0,[0]
"Intuitively, because SwitchOut is expanding the support of the training distribution, we would expect that it would help the most on test sentences that are far from those in the training set and would thus benefit most from this expanded support.",3 Experiments,0,[0]
"To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their
nearest neighbor and measure the gain in BLEU afforded by SwitchOut for each bucket.",3 Experiments,0,[0]
"Specifically, we use (negative) word error rate (WER) as the similarity measure, and plot the bucket-by-bucket performance gain for each group in Figure 2.",3 Experiments,0,[0]
"As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training.",3 Experiments,0,[0]
This is the desirable effect of data augmentation techniques.,3 Experiments,0,[0]
"In this paper, we propose a method to design data augmentation algorithms by solving an optimization problem.",4 Conclusion,0,[0]
"These solutions subsume a few existing augmentation schemes and inspire a novel augmentation method, SwitchOut.",4 Conclusion,0,[0]
SwitchOut delivers improvements over translation tasks at different scales.,4 Conclusion,0,[0]
"Additionally, SwitchOut is efficient and easy to implement, and thus has the potential for wide application.",4 Conclusion,0,[0]
"We thank Quoc Le, Minh-Thang Luong, Qizhe Xie, and the anonymous EMNLP reviewers, for their suggestions to improve the paper.
",Acknowledgements,0,[0]
This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114.,Acknowledgements,0,[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgements,0,[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgements,0,[0]
"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT).",abstractText,0,[0]
"We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution.",abstractText,0,[0]
"This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies.",abstractText,0,[0]
We name this method SwitchOut.,abstractText,0,[0]
"Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a).",abstractText,0,[0]
Code to implement this method is included in the appendix.,abstractText,0,[0]
SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3772–3782 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3772",text,0,[0]
"As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited.",1 Introduction,0,[0]
"Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014).",1 Introduction,0,[0]
"Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017).
",1 Introduction,0,[0]
"Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization.",1 Introduction,0,[0]
"We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing?
",1 Introduction,0,[0]
"We propose a multitask learning approach to incorporating syntactic information into learned
representations of neural semantics models (§2).",1 Introduction,0,[0]
"Our approach, the syntactic scaffold, minimizes an auxiliary supervised loss function, derived from a syntactic treebank.",1 Introduction,0,[0]
"The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling.",1 Introduction,0,[0]
"We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications)",1 Introduction,0,[0]
the semantic analyzer has no additional cost over a syntax-free baseline.,1 Introduction,0,[0]
"Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task.
",1 Introduction,0,[0]
"Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016).",1 Introduction,0,[0]
These spans are usually syntactic constituents (cf.,1 Introduction,0,[0]
"PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold.",1 Introduction,0,[0]
See Figure 1 for an example sentence with syntactic and semantic annotations.,1 Introduction,0,[0]
"Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree.",1 Introduction,0,[0]
"This means we never need to run a syntactic parsing algorithm.
",1 Introduction,0,[0]
Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6).,1 Introduction,0,[0]
"Our models use the strongest available neural network architectures for these tasks, integrating deep representation learning (He et al., 2017) and structured prediction at the level of spans (Kong et al., 2016).",1 Introduction,0,[0]
"For SRL, the base-
line itself is a novel globally normalized structured conditional random field, which outperforms the previous state of the art.1 Syntactic scaffolds result in further improvements over prior work— 3.6 absolute F1 in FrameNet SRL, 1.1 absolute F1 in PropBank SRL, and 0.6 F1 in coreference resolution (averaged across three standard scores).",1 Introduction,0,[0]
Our code is open source and available at https: //github.com/swabhs/scaffolding.,1 Introduction,0,[0]
"Multitask learning (Caruana, 1997) is a collection of techniques in which two or more tasks are learned from data with at least some parameters shared.",2 Syntactic Scaffolds,0,[0]
"We assume there is only one task about whose performance we are concerned, denoted T1 (in this paper, T1 is either SRL or coreference resolution).",2 Syntactic Scaffolds,0,[0]
"We use the term “scaffold” to refer to a second task, T2, that can be combined with T1 during multitask learning.",2 Syntactic Scaffolds,0,[0]
"A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T1, and after learning is completed, the scaffold is discarded.
",2 Syntactic Scaffolds,0,[0]
"A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic
1This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours.
structure.",2 Syntactic Scaffolds,0,[0]
It could be defined through a syntactic parser that shares some parameters with T1’s model.,2 Syntactic Scaffolds,0,[0]
"Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees.
",2 Syntactic Scaffolds,0,[0]
"As with multitask learning in general, we do not assume that the same data are annotated with outputs for T1 and T2.",2 Syntactic Scaffolds,0,[0]
"In this work, T2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).",2 Syntactic Scaffolds,0,[0]
We experiment with three settings: one where the corpus for T2 does not overlap with the training datasets for T1 (frame-SRL) and two where there is a complete overlap (PropBank SRL and coreference).,2 Syntactic Scaffolds,0,[0]
"Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T1 and T2 output.",2 Syntactic Scaffolds,0,[0]
"We briefly contrast the syntactic scaffold with existing alternatives.
Pipelines.",3 Related Work,0,[0]
"In a typical pipeline, T1 and T2 are separately trained, with the output of T2 used to define the inputs to T1 (Wolpert, 1992).",3 Related Work,0,[0]
"Using syntax as T2 in a pipeline is perhaps the most
common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T2’s mistakes affect the performance, and perhaps the training, of T1; He et al., 2013).",3 Related Work,0,[0]
"To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006).",3 Related Work,0,[0]
"A syntactic scaffold is quite different from a pipeline since the output of T2 is never explicitly used.
",3 Related Work,0,[0]
Latent variables.,3 Related Work,0,[0]
Another solution is to treat the output of T2 as a (perhaps structured) latent variable.,3 Related Work,0,[0]
This approach obviates the need of supervision for T2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T1.,3 Related Work,0,[0]
Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012).,3 Related Work,0,[0]
"Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T2, and it need not overlap the T1 training data.
",3 Related Work,0,[0]
Joint learning of syntax and semantics.,3 Related Work,0,[0]
"The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Lluı́s and Màrquez, 2008; Lluı́s et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016).",3 Related Work,0,[0]
"This typically requires joint prediction of the outputs of T1 and T2, which tends to be computationally expensive at both training and test time.
",3 Related Work,0,[0]
Part of speech scaffolds.,3 Related Work,0,[0]
"Similar to our work, there have been multitask models that use partof-speech tagging as T2, with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T1.",3 Related Work,0,[0]
Both of the above approaches assumed parallel input data and used both tasks as supervision.,3 Related Work,0,[0]
"Notably, we simplify our T2, throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with.",3 Related Work,0,[0]
"While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations.",3 Related Work,0,[0]
"In-
2",3 Related Work,0,[0]
"There has been some recent work on SRL which completely forgoes syntactic processing (Zhou and Xu, 2015), however it has been shown that incorporating syntactic information still remains useful (He et al., 2017).
",3 Related Work,0,[0]
"stead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks.",3 Related Work,0,[0]
"Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture.",3 Related Work,0,[0]
"To the best of our knowledge, such simplified syntactic scaffolds have not been tried before.
",3 Related Work,0,[0]
Word embeddings.,3 Related Work,0,[0]
"Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018).",3 Related Work,0,[0]
"After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings.",3 Related Work,0,[0]
"A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T1 through a multitask objective.
",3 Related Work,0,[0]
Multitask learning.,3 Related Work,0,[0]
"Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017).",3 Related Work,0,[0]
"In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018).",3 Related Work,0,[0]
"Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013).",3 Related Work,0,[0]
"Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.",3 Related Work,0,[0]
"We assume two sources of supervision: a corpusD1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebankD2 with sentences x, each with a phrase-structure tree z.",4 Syntactic Scaffold Model,0,[0]
"Each task has an associated loss, and we seek to minimize the combination of task losses,∑
(x,y)∈D1 L1(x, y) + δ ∑ (x,z)∈D2 L2(x, z) (1)
with respect to parameters, which are partially shared, where δ is a tunable hyperparameter.",4.1 Loss,0,[0]
"In
the rest of this section, we describe the scaffold task.",4.1 Loss,0,[0]
"We define the primary tasks in Sections 5–6.
",4.1 Loss,0,[0]
"Each input is a sequence of tokens, x = 〈x1, x2, . . .",4.1 Loss,0,[0]
", xn〉, for some n. We refer to a span of contiguous tokens in the sentence as xi: j = 〈xi, xi+1, . . .",4.1 Loss,0,[0]
", x",4.1 Loss,0,[0]
"j〉, for any 1 6 i 6 j 6 n. In our experiments we consider only spans up to a maximum length D, resulting in O(nD) spans.
",4.1 Loss,0,[0]
"Supervision comes from a phrase-syntactic tree z for the sentence, comprising a syntactic category zi: j ∈ C for every span xi: j in x (many spans are given a null label).",4.1 Loss,0,[0]
"We experiment with different sets of labels C (§4.2).
",4.1 Loss,0,[0]
"In our model, every span xi: j is represented by an embedding vector vi: j (see details in §5.3).",4.1 Loss,0,[0]
"A distribution over the category assigned to zi: j is derived from vi: j:
p(zi: j = c | xi: j) = softmax c wc · vi: j (2)
where wc is a parameter vector associated with category c.",4.1 Loss,0,[0]
"We sum the log loss terms for all the spans in a sentence to give its loss:
L2(x, z) =",4.1 Loss,0,[0]
"− ∑
16i6 j6n j−i6D
log p(zi: j | xi: j).",4.1 Loss,0,[0]
-3,4.1 Loss,0,[0]
"Different kinds of syntactic labels can be used for learning syntactically-aware span representations: • Constituent identity: C = {0, 1}; is a span a
constituent, or not?",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"• Non-terminal: c is the category of a span,
including a null for non-constituents.",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"• Non-terminal and parent: c is the category
of a span, concatenated with the category of its immediate ancestor.",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"null is used for nonconstituents, and for empty ancestors.",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"• Common non-terminals: Since a majority
of semantic arguments and entity mentions are labeled with a small number of syntactic categories,3 we experiment with a threeway classification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null.",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER.
",4.2 Labels for the Syntactic Scaffold Task,0,[0]
"3In the OntoNotes corpus, which includes both syntactic and semantic annotations, 44% of semantic arguments are noun phrases and 13% are prepositional phrases.",4.2 Labels for the Syntactic Scaffold Task,0,[0]
We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds.,5 Semantic Role Labeling,0,[0]
"The performance of this baseline itself is competitive with state-of-the-art methods (§7).
",5 Semantic Role Labeling,0,[0]
FrameNet.,5 Semantic Role Labeling,0,[0]
"In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements.",5 Semantic Role Labeling,0,[0]
"A frame can be evoked by a word or phrase in a sentence, called a target.",5 Semantic Role Labeling,0,[0]
"Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized).",5 Semantic Role Labeling,0,[0]
"Arguments for a given frame do not overlap.
",5 Semantic Role Labeling,0,[0]
PropBank.,5 Semantic Role Labeling,0,[0]
PropBank similarly disambiguates predicates and identifies argument spans.,5 Semantic Role Labeling,0,[0]
"Targets are disambiguated to lexically specific senses rather than shared frames, and a set of generic roles is used for all targets, reducing the argument label space by a factor of 17.",5 Semantic Role Labeling,0,[0]
"Most importantly, the arguments were annotated on top of syntactic constituents, directly coupling syntax and semantics.",5 Semantic Role Labeling,0,[0]
"A detailed example for both formalisms is provided in Figure 1.
",5 Semantic Role Labeling,0,[0]
"Semantic structure prediction is the task of identifying targets, labeling their frames or senses, and labeling all their argument spans in a sentence.",5 Semantic Role Labeling,0,[0]
"Here we assume gold targets and frames, and consider only the SRL task.
",5 Semantic Role Labeling,0,[0]
"Formally, a single input instance for argument identification consists of: an n-word sentence x = 〈x1, x2, . . .",5 Semantic Role Labeling,0,[0]
", xn〉, a single target span t = 〈tstart, tend〉, and its evoked frame, or sense, f .",5 Semantic Role Labeling,0,[0]
"The argument labeling task is to produce a segmentation of the sentence: s = 〈s1, s2, . . .",5 Semantic Role Labeling,0,[0]
", sm〉 for each input x.",5 Semantic Role Labeling,0,[0]
A segment s = 〈,5 Semantic Role Labeling,0,[0]
"i, j, yi: j〉 corresponds to a labeled span of the sentence, where the label yi: j ∈ Y f ∪ {null} is either a role that the span fills, or null if the span does not fill any role.",5 Semantic Role Labeling,0,[0]
"In the case of PropBank, Y f consists of all possible roles.",5 Semantic Role Labeling,0,[0]
The segmentation is constrained so that argument spans cover the sentence and do not overlap (ik+1 = 1 + jk for sk; i1 = 1; jm = n).,5 Semantic Role Labeling,0,[0]
Segments of length 1 such that i = j are allowed.,5 Semantic Role Labeling,0,[0]
A separate segmentation is predicted for each target annotation in a sentence.,5 Semantic Role Labeling,0,[0]
"In order to model the non-overlapping arguments of a given target, we use a semi-Markov conditional random field (semi-CRF; Sarawagi et al., 2004).",5.1 Semi-Markov CRF,0,[0]
"Semi-CRFs define a conditional distribution over labeled segmentations of an input sequence, and are globally normalized.",5.1 Semi-Markov CRF,0,[0]
A single target’s arguments can be neatly encoded as a labeled segmentation by giving the spans in between arguments a reserved null label.,5.1 Semi-Markov CRF,0,[0]
"Semi-Markov models are more powerful than BIO tagging schemes, which have been used successfully for PropBank SRL (Collobert et al., 2011; Zhou and Xu, 2015, inter alia), because the semi-Markov assumption allows scoring variable-length segments, rather than fixed-length label n-grams as under an (n − 1)-order Markov assumption.",5.1 Semi-Markov CRF,0,[0]
Computing the marginal likelihood with a semi-CRF can be done using dynamic programming in O(n2) time (§5.2).,5.1 Semi-Markov CRF,0,[0]
"By filtering out segments longer than D tokens, this is reduced to O(nD).
",5.1 Semi-Markov CRF,0,[0]
"Given an input x, a semi-CRF defines a conditional distribution p(s | x).",5.1 Semi-Markov CRF,0,[0]
Every segment s = 〈,5.1 Semi-Markov CRF,0,[0]
"i, j, yi: j〉 is given a real-valued score, ψ(〈i, j, yi: j = r〉, xi: j) = wr · vi: j, where vi: j is an embedding of the span (§5.3) and wr is a parameter vector corresponding to its label.",5.1 Semi-Markov CRF,0,[0]
"The score of the entire segmentation s is the sum of the scores of its segments: Ψ(x, s) =",5.1 Semi-Markov CRF,0,[0]
"∑m k=1 ψ(sk, xik: jk ).",5.1 Semi-Markov CRF,0,[0]
These scores are exponentiated and normalized to define the probability distribution.,5.1 Semi-Markov CRF,0,[0]
The sum-product variant of the semi-Markov dynamic programming algorithm is used to calculate the normalization term (required during learning).,5.1 Semi-Markov CRF,0,[0]
"At test time, the maxproduct variant returns the most probable segmentation, ŝ = arg max sΨ(s, x).
",5.1 Semi-Markov CRF,0,[0]
The parameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2).,5.1 Semi-Markov CRF,0,[0]
"The learner evaluates and adjusts segment scores ψ(sk, x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3).",5.1 Semi-Markov CRF,0,[0]
Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective.,5.2 Softmax-Margin Objective,0,[0]
"In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant
of log-likelihood:
L1 = − ∑
(x,s∗)∈D1 log
exp Ψ(s∗, x) Z(x, s∗) , (4)
Z(x, s∗) =",5.2 Softmax-Margin Objective,0,[0]
"∑
s exp {Ψ(s, x) +",5.2 Softmax-Margin Objective,0,[0]
"cost(s, s∗)}.",5.2 Softmax-Margin Objective,0,[0]
"(5)
We design the cost function so that it factors by predicted span, in the same way Ψ does:
cost(s, s∗)",5.2 Softmax-Margin Objective,0,[0]
"= ∑ s∈s cost(s, s∗) = ∑ s∈s I(s < s∗).",5.2 Softmax-Margin Objective,0,[0]
"(6)
The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations.",5.2 Softmax-Margin Objective,0,[0]
"The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function:
α j = ∑
s=〈i, j,yi: j〉 j−i6D
αi−1 exp{Ψ(s, x) + cost(s, s∗)}, (7)
where Z = αn, under the base case α0 = 1.",5.2 Softmax-Margin Objective,0,[0]
"The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1:
γ j = max s=〈i, j,yi: j〉
j−i6D
γi−1 exp Ψ(s, x).",5.2 Softmax-Margin Objective,0,[0]
"(8)
Our model formulation enforces that arguments do not overlap.",5.2 Softmax-Margin Objective,0,[0]
"We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012).",5.2 Softmax-Margin Objective,0,[0]
"This section describes the neural architecture used to obtain the span embedding, vi: j, corresponding to a span xi: j and the target in consideration, t = 〈tstart, tend〉.",5.3 Input Span Representation,0,[0]
"For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used.",5.3 Input Span Representation,0,[0]
"If there are no verbs, we use the first token in the sentence as a placeholder target.",5.3 Input Span Representation,0,[0]
"The parameters used to learn v are shared between the tasks.
",5.3 Input Span Representation,0,[0]
"We construct an embedding for the span using • hi and h j: contextualized embeddings for the
words at the span boundary (§5.3.1), • ui: j: a span summary that pools over the con-
tents of the span (§5.3.2), and
• ai: j: and a hand-engineered feature vector for the span (§5.3.3).
",5.3 Input Span Representation,0,[0]
"This embedding is then passed to a feedforward layer to compute the span representation, vi: j.",5.3 Input Span Representation,0,[0]
"To obtain contextualized embeddings of each token in the input sequence, we run a bidirectional LSTM (Graves, 2012) with ` layers over the full input sequence.",5.3.1 Contextualized Token Embeddings,0,[0]
"To indicate which token is a predicate, a linearly transformed one-hot embedding v is used, following Zhou and Xu (2015) and He et al. (2017).",5.3.1 Contextualized Token Embeddings,0,[0]
The input vector representing the token at position q in the sentence is the concatenation of a fixed pretrained embedding xq and vq.,5.3.1 Contextualized Token Embeddings,0,[0]
"When given as input to the bidirectional LSTM, this yields a hidden state vector hq representing the qth token in the context of the sentence.",5.3.1 Contextualized Token Embeddings,0,[0]
Tokens within a span might convey different amounts of information necessary to label the span as a semantic argument.,5.3.2 Span Summary,0,[0]
"Following Lee et al. (2017), we use an attention mechanism (Bahdanau et al., 2014) to summarize each span.",5.3.2 Span Summary,0,[0]
"Each contextualized token in the span is passed through a feed-forward network to obtain a weight, normalized to give σk = softmax
i6k6 j whead · hk, where whead
is a learned parameter.",5.3.2 Span Summary,0,[0]
"The weights σ are then used to obtain a vector that summarizes the span, ui: j = ∑ i6k6 j; j−i<D σk · hk.",5.3.2 Span Summary,0,[0]
"We use the following three features for each span: • width of the span in tokens (Das et al., 2014) • distance (in tokens) of the span from the tar-
get (Täckström et al., 2015) • position of the span with respect to the tar-
get (before, after, overlap) (Täckström et al., 2015)
",5.3.3 Span Features,0,[0]
"Each of these features is encoded as a one-hotembedding and then linearly transformed to yield a feature vector, ai: j.",5.3.3 Span Features,0,[0]
Coreference resolution is the task of determining clusters of mentions that refer to the same entity.,6 Coreference Resolution,0,[0]
"Formally, the input is a document x = x1, x2, . . .",6 Coreference Resolution,0,[0]
", xn consisting of n words.",6 Coreference Resolution,0,[0]
"The goal is to predict a set of clusters c = {c1, c2, . . .",6 Coreference Resolution,0,[0]
"}, where each cluster c = {s1, s2, .",6 Coreference Resolution,0,[0]
. .,6 Coreference Resolution,0,[0]
"} is a set of spans and
each span s = 〈i, j〉 is a pair of indices such that 1 6 i 6 j 6 n.
As a baseline, we use the model of Lee et al. (2017), which we describe briefly in this section.",6 Coreference Resolution,0,[0]
This model decomposes the prediction of coreference clusters into a series of span classification decisions.,6 Coreference Resolution,0,[0]
"Every span s predicts an antecedent ws ∈ Y(s) = {null, s1, s2, . . .",6 Coreference Resolution,0,[0]
", sm}.",6 Coreference Resolution,0,[0]
"Labels s1 to sm indicate a coreference link between s and one of the m spans that precede it, and null indicates that s does not link to anything, either because it is not a mention or it is in a singleton cluster.",6 Coreference Resolution,0,[0]
"The predicted clustering of the spans can be recovered by aggregating the predicted links.
",6 Coreference Resolution,0,[0]
"Analogous to the SRL model (§5), every span s is represented by an embedding vs, which is central to the model.",6 Coreference Resolution,0,[0]
"For each span s and a potential antecedent a ∈ Y(s), pairwise coreference scores Ψ(vs, va, φ(s, a)) are computed via feedforward networks with the span embeddings as input.",6 Coreference Resolution,0,[0]
"φ(s, a) are pairwise discrete features encoding the distance between span s and span a and metadata, such as the genre and speaker information.",6 Coreference Resolution,0,[0]
"We refer the reader to Lee et al. (2017) for the details of the scoring function.
",6 Coreference Resolution,0,[0]
"The scores from Ψ are normalized over the possible antecedents Y(s) of each span to induce a probability distribution for every span:
p(ws = a) = softmax a∈Y(s) Ψ(vs, va, φ(s, a))",6 Coreference Resolution,0,[0]
"(9)
In learning, we minimize the negative loglikelihood marginalized over the possibly correct antecedents:
L1 = − ∑ s∈D log ∑ a∗∈G(s)∩Y(s) p(ws = a∗) (10)
whereD is the set of spans in the training dataset, and G(s) indicates the gold cluster of s if it belongs to one and {null} otherwise.
",6 Coreference Resolution,0,[0]
"To operate under reasonable computational requirements, inference under this model requires a two-stage beam search, which reduces the number of span pairs considered.",6 Coreference Resolution,0,[0]
"We refer the reader to Lee et al. (2017) for details.
",6 Coreference Resolution,0,[0]
Input span representation.,6 Coreference Resolution,0,[0]
"The input span embedding, vs for coreference resolution and its syntactic scaffold follow the definition used in §5.3, with the key difference of using no target features.",6 Coreference Resolution,0,[0]
"Since there is a complete overlap of input sentences betweenDsc andDpr as the coreference annotations are also from OntoNotes (Pradhan et al.,
2012), we reuse the v for the scaffold task.",6 Coreference Resolution,0,[0]
"Additionally, instead of the entire document, each sentence in it is independently given as input to the bidirectional LSTMs.",6 Coreference Resolution,0,[0]
We evaluate our models on the test set of FrameNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference.,7 Results,0,[0]
"For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material.
",7 Results,0,[0]
Frame SRL.,7 Results,0,[0]
Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold.,7 Results,0,[0]
"We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007).
",7 Results,0,[0]
"Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; Täckström",7 Results,0,[0]
"et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015).
",7 Results,0,[0]
The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017).,7 Results,0,[0]
"In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer.",7 Results,0,[0]
"In their relational model (Rel), they treat the same problem as a span classification problem.",7 Results,0,[0]
"Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints.",7 Results,0,[0]
"Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5
Notably, all prior systems for frame SRL listed in Table 1 use a pipeline of syntax and semantics.",7 Results,0,[0]
"Our semi-CRF baseline outperforms all prior work, without any syntax.",7 Results,0,[0]
"This highlights the ben-
4http://cemantix.org/data/ontonotes.html 5Yang and Mitchell (2017) also evaluated on the full frame-semantic parsing task, which includes frame-SRL as well as identifying frames.",7 Results,0,[0]
"Since our frame SRL performance improves over theirs, we expect that incorporation into a full system (e.g., using their frame identification module) would lead to overall benefits as well; this experiment is left to future work.
",7 Results,0,[0]
"efits of modeling spans and of global normalization.
",7 Results,0,[0]
"Turning to scaffolds, even the most coarsegrained constituent identity scaffold improves the performance of our syntax-agnostic baseline.",7 Results,0,[0]
"The nonterminal and nonterminal and parent scaffolds, which use more detailed syntactic representations, improve over this.",7 Results,0,[0]
"The greatest improvements come from the scaffold model predicting common nonterminal labels (NP and PP, which are the most common syntactic categories of semantic arguments, vs. others): 3.6% absolute improvement in F1 measure over prior work.
",7 Results,0,[0]
"Contemporaneously with this work, Peng et al. (2018) proposed a system for joint frame-semantic and semantic dependency parsing.",7 Results,0,[0]
"They report results for joint frame and argument identification, and hence cannot be directly compared in Table 1.",7 Results,0,[0]
"We evaluated their output for argument identification only; our semi-CRF baseline model exceeds their performance by 1 F1, and our common nonterminal scaffold by 3.1 F1.6
6This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set.
",7 Results,0,[0]
PropBank SRL.,7 Results,0,[0]
"We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL.",7 Results,0,[0]
"Table 2 reports results using gold predicates.
",7 Results,0,[0]
"Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax.",7 Results,0,[0]
"He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance.",7 Results,0,[0]
Tan et al. (2018) employ a similar approach but use feed-forward networks with selfattention.,7 Results,0,[0]
"He et al. (2018a) use a span-based classification to jointly identify and label argument spans.
",7 Results,0,[0]
"Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction.",7 Results,0,[0]
We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.,7 Results,0,[0]
"This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures.
",7 Results,0,[0]
"He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set.",7 Results,0,[0]
"The improvements from ELMo are methodologically orthogonal to syntactic scaffolds.
",7 Results,0,[0]
"Since the datasets for learning PropBank semantics and syntactic scaffolds completely overlap, the performance improvement cannot be attributed to a larger training corpus (or, by extension, a larger vocabulary), though that might be a factor for frame SRL.
",7 Results,0,[0]
"A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017).",7 Results,0,[0]
"This, along with other recent ap-
proaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL.
Coreference.",7 Results,0,[0]
"We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3.",7 Results,0,[0]
"Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax.
",7 Results,0,[0]
"Our baseline is the model from Lee et al. (2017), described in §6.",7 Results,0,[0]
"Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax.
",7 Results,0,[0]
We experiment with the best syntactic scaffold from the frame SRL task.,7 Results,0,[0]
"We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases.",7 Results,0,[0]
The syntactic scaffold outperforms the baseline by 0.6 absolute F1.,7 Results,0,[0]
"Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1.",7 Results,0,[0]
"All the above are orthogonal to our approach, and could be incorporated to yield higher gains.",7 Results,0,[0]
"To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline.
",8 Discussion,0,[0]
"We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2.",8 Discussion,0,[0]
"Not surpris-
7We used FrameNet syntactic phrase annotations for analysis only, and not in our models, since they are annotated only for the gold arguments.
",8 Discussion,0,[0]
"ingly, we observe large improvements in the common nonterminals used (NP and PP).",8 Discussion,0,[0]
"However, the phrase type annotations in FrameNet do not correspond exactly to the OntoNotes phrase categories.",8 Discussion,0,[0]
"For instance, FrameNet annotates nonmaximal (A) and standard adjective phrases (AJP), while OntoNotes annotations for noun-phrases are flat, ignore the underlying adjective phrases.",8 Discussion,0,[0]
"This explains why the syntax-agnostic baseline is able to recover the former while the scaffold is not.
",8 Discussion,0,[0]
"Similarly, for frequent frame elements, scaffolding improves performance across the board, as shown in Fig. 3.",8 Discussion,0,[0]
"The largest improvements come for Theme and Goal, which are predominantly realized as noun phrases and prepositional phrases.",8 Discussion,0,[0]
"We introduced syntactic scaffolds, a multitask learning approach to incorporate syntactic bias into semantic processing tasks.",9 Conclusion,0,[0]
"Unlike pipelines and approaches which jointly model syntax and semantics, no explicit syntactic processing is required at runtime.",9 Conclusion,0,[0]
"Our method improves the performance of competitive baselines for semantic role labeling on both FrameNet and PropBank, and for coreference resolution.",9 Conclusion,0,[0]
"While our focus was on span-based tasks, syntactic scaffolds could be applied in other settings (e.g., dependency and graph representations).",9 Conclusion,0,[0]
"Moreover, scaffolds need not be syntactic; we can imagine, for example, semantic scaffolds being used to improve NLP applications with limited annotated data.",9 Conclusion,0,[0]
"It remains an open empirical question to determine the relative merits of different kinds of scaffolds and multitask learners, and how they can be most produc-
tively combined.",9 Conclusion,0,[0]
Our code is publicly available at https://github.com/swabhs/scaffolding.,9 Conclusion,0,[0]
"We thank several members of UW-NLP, particularly Luheng He, as well as David Weiss and Emily Pitler for thoughtful discussions on prior versions of this paper.",Acknowledgments,0,[0]
We also thank the three anonymous reviewers for their valuable feedback.,Acknowledgments,0,[0]
This work was supported in part by NSF grant IIS1562364 and by the NVIDIA Corporation through the donation of a Tesla GPU.,Acknowledgments,0,[0]
"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks.",abstractText,0,[0]
"Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective.",abstractText,0,[0]
"We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",abstractText,0,[0]
Syntactic Scaffolds for Semantic Structures,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2061",text,0,[0]
"Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.",1 Introduction,0,[0]
"Specifically, we seek to identify arguments and label their semantic roles given a predicate.",1 Introduction,0,[0]
"SRL is an impor-
∗ These authors made equal contribution.† Corresponding author.",1 Introduction,0,[0]
"This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04).
tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016).
",1 Introduction,0,[0]
"There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.",1 Introduction,0,[0]
"The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments.",1 Introduction,0,[0]
"Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification.
",1 Introduction,0,[0]
"In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.",1 Introduction,0,[0]
"In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008).",1 Introduction,0,[0]
"However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance.
",1 Introduction,0,[0]
"To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input.",1 Introduction,0,[0]
"It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002).",1 Introduction,0,[0]
"This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL.
",1 Introduction,0,[0]
"This paper will focus on semantic dependency parsing and formulate SRL as one or two se-
quence tagging tasks with predicate-specific encoding.",1 Introduction,0,[0]
"With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.
",1 Introduction,0,[0]
"In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL2008 Shared Task1 as evaluation metric.",1 Introduction,0,[0]
"Considering that various syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study.",1 Introduction,0,[0]
"To fully disclose the predicate-argument structure, typical SRL systems have to step by step perform four subtasks.",2 Model,0,[0]
"Since the predicates in CoNLL2009 (Hajič et al., 2009) corpus have been preidentified, we need to tackle three other subtasks, which are formulized into two-step pipeline in this work, predicate disambiguation and argument labeling.",2 Model,0,[0]
"Namely, we do the work of argument identification and classification in one model.
",2 Model,0,[0]
Argument structure for each known predicate will be disclosed by our argument labeler over a sequence including possible arguments (candidates).,2 Model,0,[0]
"There are two ways to determine the sequence, one is to simply input the entire sentence as a syntax-agnostic SRL system does, the other is to select words according to syntactic parse tree around the predicate as most previous SRL systems did.",2 Model,0,[0]
The latter strategy usually works through a syntactic tree based argument pruning algorithm.,2 Model,0,[0]
"We will use the proposed k-order argument pruning algorithm (Section 2.1) to get a sequence w = (w1, . . .",2 Model,0,[0]
", wn) for each predicate.",2 Model,0,[0]
"Then, we represent each word wi ∈ w as xi (Section 2.2).",2 Model,0,[0]
"Eventually, we obtain contextual features with sequence encoder (Section 2.3).",2 Model,0,[0]
The overall role labeling model is depicted in Figure 1.,2 Model,0,[0]
"As pointed out by Punyakanok et al. (2008), syntactic information is most relevant in identifying
1CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one.",2.1 Argument Pruning,0,[0]
"Their main difference is that predicates have been beforehand indicated for the latter.
",2.1 Argument Pruning,0,[0]
"the arguments, and the most crucial contribution of full parsing is in the pruning stage.",2.1 Argument Pruning,0,[0]
"In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b).",2.1 Argument Pruning,0,[0]
"First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd).",2.1 Argument Pruning,0,[0]
"Then we define k-order descendants of given node satisfying D(n, nd) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order.",2.1 Argument Pruning,0,[0]
"Note that the definition of k-order traversal is somewhat different from tree traversal in terminology.
",2.1 Argument Pruning,0,[0]
A brief description of the proposed k-order pruning algorithm is given as follow.,2.1 Argument Pruning,0,[0]
"Initially, we set a given predicate as the current node in a syntactic dependency tree.",2.1 Argument Pruning,0,[0]
"Then, collect all its argument candidates by the strategy of k-order traversal.",2.1 Argument Pruning,0,[0]
"Afterwards, reset the current node to its syntactic head and repeat the previous step till the root of the tree.",2.1 Argument Pruning,0,[0]
"Finally, collect the root and stop.",2.1 Argument Pruning,0,[0]
The k-order argument algorithm is presented in Algorithm 1 in detail.,2.1 Argument Pruning,0,[0]
"An example of a syntactic dependency tree for sentence She began to trade the art for money is shown in Figure 2.
",2.1 Argument Pruning,0,[0]
"The main reasons for applying the extended korder argument pruning algorithm are two-fold.
",2.1 Argument Pruning,0,[0]
Algorithm 1 k-order argument pruning algorithm,2.1 Argument Pruning,0,[0]
"Input: A predicate p, the root node r given a syn-
tactic dependency tree T , the order k Output:",2.1 Argument Pruning,0,[0]
"The set of argument candidates S
1: initialization set p as current node c, c = p 2: for each descendant ni of c in T do 3: if D(c, ni) ≤ k",2.1 Argument Pruning,0,[0]
"and ni /∈ S then 4: S = S + ni 5: end if 6: end for 7: find the syntactic head ch of c, and let c = ch 8: if c = r then 9: S = S + r
10: else 11: goto step 2 12: end if 13: return argument candidates set S
First, previous standard pruning algorithm may hurt the argument coverage too much, even though indeed arguments usually tend to surround their predicate in a close distance.",2.1 Argument Pruning,0,[0]
"As a sequence tagging model has been applied, it can effectively handle the imbalanced distribution between arguments and non-arguments, which is hardly tackled by early argument classification models that commonly adopt the standard pruning algorithm.",2.1 Argument Pruning,0,[0]
"Second, the extended pruning algorithm provides a better trade-off between computational cost and performance by carefully tuning k.",2.1 Argument Pruning,0,[0]
"We produce a predicate-specific word representation xi for each word wi, where i stands for the word position in an input sequence, following Marcheggiani et al. (2017).",2.2 Word Representation,0,[0]
"However, we differ by (1) leveraging a predicate-specific indicator embedding, (2) using deeper refined representation, including character and dependency relation embeddings, and (3) applying recent advances in RNNs, such as highway connections (Srivastava et al., 2015).
",2.2 Word Representation,0,[0]
"In this work, word representation xi is the concatenation of four types of features: predicatespecific feature, character-level, word-level and linguistic features.",2.2 Word Representation,0,[0]
"Unlike previous work, we leverage a predicate-specific indicator embedding xiei rather than directly using a binary flag either 0 or 1.",2.2 Word Representation,0,[0]
"At character level, we exploit convolutional neural network (CNN) with bidirectional LSTM (BiLSTM) to learn character embedding
xcei .",2.2 Word Representation,0,[0]
"As shown in Figure 1, the representation calculated by the CNN is fed as input to BiLSTM.",2.2 Word Representation,0,[0]
"At word level, we use a randomly initialized word embedding xrei and a pre-trained word embedding xpei .",2.2 Word Representation,0,[0]
"For linguistic features, we employ a randomly initialized lemma embedding xlei and a randomly initialized POS tag embedding xposi .",2.2 Word Representation,0,[0]
"In order to incorporate more syntactic information, we adopt an additional feature, the dependency relation to syntactic head.",2.2 Word Representation,0,[0]
"Likewise, it is a randomly initialized embedding xdei .",2.2 Word Representation,0,[0]
The resulting word representation is concatenated as xi =,2.2 Word Representation,0,[0]
"[x ie i , x ce i , x re i , x pe",2.2 Word Representation,0,[0]
"i , x le i , x pos",2.2 Word Representation,0,[0]
"i , x de i ].",2.2 Word Representation,0,[0]
"As Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have shown significant representational effectiveness to NLP tasks, we thus use BiLSTM as the sentence encorder.",2.3 Sequence Encoder,0,[0]
"Given an input sequence x = (x1, . . .",2.3 Sequence Encoder,0,[0]
", xn), BiLSTM processes the sequence in both forward and backward direction to obtain two separated hidden states, −→ h i which handles data from x1 to xi and ←− h i which tackles data from xn to xi for each word representation.",2.3 Sequence Encoder,0,[0]
"Finally, we get a contextual representation hi =",2.3 Sequence Encoder,0,[0]
"[ −→ h i, ←− h",2.3 Sequence Encoder,0,[0]
i] by concatenating the states of BiLSTM networks.,2.3 Sequence Encoder,0,[0]
"To get the final predicted semantic roles, we exploit a multi-layer perceptron (MLP) with highway connections on the top of BiLSTM networks, which takes as input the hidden representation hi
of all time steps.",2.3 Sequence Encoder,0,[0]
The MLP network consists of 10 layers with highway connections and we employ ReLU activations for the hidden layers.,2.3 Sequence Encoder,0,[0]
"Finally, we use a softmax layer over the outputs to maximize the likelihood of labels.",2.3 Sequence Encoder,0,[0]
"Although predicates have been identified given a sentence, predicate disambiguation is an indispensable task, which aims to determine the predicate-argument structure for an identified predicate in a particular context.",2.4 Predicate Disambiguation,0,[0]
"Here, we also use the identical model (BiLSTM composed with MLP) for predicate disambiguation, in which the only difference is that we remove the syntactic dependency relation feature in corresponding word representation (Section 2.2).",2.4 Predicate Disambiguation,0,[0]
"Exactly, given a predicate p, the resulting word representation is pi =",2.4 Predicate Disambiguation,0,[0]
"[p ie i , p ce",2.4 Predicate Disambiguation,0,[0]
"i , p re i , p pe",2.4 Predicate Disambiguation,0,[0]
"i , p le i , p pos",2.4 Predicate Disambiguation,0,[0]
i ].,2.4 Predicate Disambiguation,0,[0]
"Our model2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits.",3 Experiments,0,[0]
"The hyperparameters in our model were selected based on the development set, and are summarized in Table 1.",3 Experiments,0,[0]
Note that the parameters of predicate model are the same as these in argument model.,3 Experiments,0,[0]
"All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors (Pennington et al., 2014).",3 Experiments,0,[0]
"For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov
2The code is available at https://github.com/ bcmi220/srl_syn_pruning.
",3 Experiments,0,[0]
"et al., 2013).",3 Experiments,0,[0]
"During training procedures, we use the categorical cross-entropy as objective, with Adam optimizer (Kingma and Ba, 2015).",3 Experiments,0,[0]
We train models for a maximum of 20 epochs and obtain the nearly best model based on development results.,3 Experiments,0,[0]
"For argument labeling, we preprocess corpus with k-order argument pruning algorithm.",3 Experiments,0,[0]
"In addition, we use four CNN layers with singlelayer BiLSTM to induce character representations derived from sentences.",3 Experiments,0,[0]
"For English3, to further enhance the representation, we adopt CNNBiLSTM character embedding structure from AllenNLP toolkit (Peters et al., 2018).",3 Experiments,0,[0]
"During the pruning of argument candidates, we use the officially predicted syntactic parses provided by CoNLL-2009 shared-task organizers on both English and Chinese.",3.1 Preprocessing,0,[0]
Figure 3 shows changing curves of coverage and reduction following k on the English train set.,3.1 Preprocessing,0,[0]
"According to our statistics, the number of non-arguments is ten times more than that of arguments, where the data distribution is fairly unbalanced.",3.1 Preprocessing,0,[0]
"However, a proper pruning strategy could alleviate this problem.",3.1 Preprocessing,0,[0]
"Accordingly, the first-order pruning reduces more than 50% candidates at the cost of missing 5.5% true ones on average, and the second-order prunes about 40% candidates with nearly 2.0% loss.",3.1 Preprocessing,0,[0]
"The coverage of third-order has achieved 99% and it reduces approximately 1/3 corpus size.
",3.1 Preprocessing,0,[0]
"It is worth noting that as k is larger than 19,
3For Chinese, we do not use character embedding.
",3.1 Preprocessing,0,[0]
"there will come full coverage on all argument candidates for English training set, which let our high order pruning algorithm degrade into a syntaxagnostic setting.",3.1 Preprocessing,0,[0]
"In this work, we use the tenthorder pruning for pursuing the best performance.",3.1 Preprocessing,0,[0]
"Our system performance is measured with the official script from CoNLL-2009 benchmarks, combining the output of our predicate disambiguation with our semantic role labeling.",3.2 Results,0,[0]
"Our predicate disambiguation model achieves the accuracy of 95.01% and 95.58%4 on development and test sets, respectively.",3.2 Results,0,[0]
"We compare our model performance with the state-of-the-art models for dependency SRL.5 Noteworthily, our model is local and single without reranking, which neither includes global inference nor combines multiple models.",3.2 Results,0,[0]
"The experimental results on the English in-domain (WSJ) and out-of-domain (Brown) test sets are shown in Tables 2 and 3, respectively.
",3.2 Results,0,[0]
"For English, our syntax-aware model outperforms previously published best single model, scoring 89.5% F1 with 1.5% absolute improvement on the in-domain (WSJ) test data.",3.2 Results,0,[0]
"Compared
4Note that we give a slightly better predicate model than Roth and Lapata (2016), with 94.77% and 95.47% accuracy on development and test sets, respectively.
5Here, we do not compare against span-based SRL models, which annotate roles for entire argument spans instead of semantic dependencies.
with ensemble models, our single model even provides better performance (+0.4% F1) than the system (Marcheggiani and Titov, 2017), and significantly surpasses all the rest models.",3.2 Results,0,[0]
"In the syntaxagnostic setting (without pruning and dependency relation embedding), we also reach the new stateof-the-art, achieving a performance gain of 1% F1.
",3.2 Results,0,[0]
"On the out-of-domain (Brown) test set, we achieve the new best results of 79.3% (syntaxaware) and 78.8% (syntax-agnostic) in F1 scores.",3.2 Results,0,[0]
"Moreover, our syntax-aware model performs better than the syntax-agnostic one.
",3.2 Results,0,[0]
Table 4 presents the results on Chinese test set.,3.2 Results,0,[0]
"Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.",3.2 Results,0,[0]
"To evaluate the contributions of key factors in our method, a series of ablation studies are performed on the English development set.
",3.3 Analysis,0,[0]
"In order to demonstrate the effectiveness of our k-order pruning algorithm, we report the SRL performance excluding predicate senses in evaluation, eliminating the performance gain from predicate disambiguation.",3.3 Analysis,0,[0]
Table 5 shows the results from our syntax-aware model with lower order argument pruning.,3.3 Analysis,0,[0]
"Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score.",3.3 Analysis,0,[0]
"It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition.
",3.3 Analysis,0,[0]
"Table 6 presents the performance of our syntaxagnostic SRL system with a basic configuration, which removes components, including indicator and character embeddings.",3.3 Analysis,0,[0]
"Note that the first row is the results of BiLSTM (removing MLP from basic model), whose encoding is the same as Marcheggiani et al. (2017).",3.3 Analysis,0,[0]
"Experiments show that both enhanced representations improve over our basic model, and our adopted labeling model is superior to the simple BiLSTM.
Figure 4 shows F1 scores in different k-order pruning together with our syntax-agnostic model.",3.3 Analysis,0,[0]
"It also indicates that the least first-order pruning fails to give satisfactory performance, the best performing setting coming from a moderate setting of k = 10, and the largest k shows that our argu-
ment pruning falls back to syntax-agnostic type.",3.3 Analysis,0,[0]
"Meanwhile, from the best k setting to the lower order pruning, we receive a much faster performance drop, compared to the higher order pruning until the complete syntax-agnostic case.",3.3 Analysis,0,[0]
"The proposed k-order pruning algorithm always works even it reaches the syntax-agnostic setting, which empirically explains why the current syntax-aware and syntax-agnostic SRL models hold little performance difference, as maximum k-order pruning actually removes few words just like syntaxagnostic model.",3.3 Analysis,0,[0]
"In this work, we consider additional model that integrates predicate disambiguation and argument labeling into one sequence labeling model.",3.4 End-to-end SRL,0,[0]
"In order to implement an end-to-end model, we introduce a virtual root (VR) for predicate disambiguation similar to Zhao et al. (2013) who handled the entire SRL task as word pair classification.",3.4 End-to-end SRL,0,[0]
"Concretely, we add a predicate sense feature to the input sequence by concatenating a VR.",3.4 End-to-end SRL,0,[0]
The word representation of VR is randomly initialized during training.,3.4 End-to-end SRL,0,[0]
"In Figure 5, we give an example sequence with the labels for the given sentence.
",3.4 End-to-end SRL,0,[0]
We also report results of our end-to-end model on CoNLL-2009 test set with syntax-aware and syntax-agnostic settings.,3.4 End-to-end SRL,0,[0]
"As shown in Table 7, our end-to-end model yields slightly weaker performance compared with our pipeline.",3.4 End-to-end SRL,0,[0]
"A reasonable account for performance degradation is that the training data has completely different genre distributions over predicate senses and argument roles, which may be somewhat confusing for integrative model to make classification decisions.",3.4 End-to-end SRL,0,[0]
"For a full SRL task, the predicate identification subtask is also indispensable, which has been included in CoNLL-2008 shared task.",3.5 CoNLL-2008 SRL Setting,0,[0]
"We thus evaluate our model in terms of data and setting of the CoNLL-2008 benchmark (WSJ).
",3.5 CoNLL-2008 SRL Setting,0,[0]
"To identify predicates, we train the BiLSTMMLP sequence labeling model with same parameters in Section 2.4 to tackle the predicate identification and disambiguation subtasks in one shot, and the only difference is that we remove the predicate-specific indicator feature.",3.5 CoNLL-2008 SRL Setting,0,[0]
The F1 score of our predicate labeling model is 90.53% on indomain (WSJ) data.,3.5 CoNLL-2008 SRL Setting,0,[0]
"Compared with the best reported results, we observe absolute improvements in semantic F1 of 0.8% (in Table 8).",3.5 CoNLL-2008 SRL Setting,0,[0]
"Note that as predicate identification is introduced, our same model shows about 6% performance loss for either syntax-agnostic or syntax-aware case, which indicates that predicate identification should be carefully handled, as it is very needed in a complete practical SRL system.",3.5 CoNLL-2008 SRL Setting,0,[0]
Syntactic information plays an informative role in semantic role labeling.,4 Syntactic Contribution,0,[0]
"However, few studies were done to quantitatively evaluate the syntactic contribution to SRL.",4 Syntactic Contribution,0,[0]
"Furthermore, we observe that most of the above compared neural SRL systems took the syntactic parser of (Björkelund et al., 2010) as syntactic inputs instead of the one from CoNLL-2009 shared task, which adopted a much weaker syntactic parser.",4 Syntactic Contribution,0,[0]
"Especially (Marcheggiani and Titov, 2017), adopted an external syntactic
parser with even higher parsing accuracy.",4 Syntactic Contribution,0,[0]
"Contrarily, our SRL model is based on the automatically predicted parse with moderate performance provided by CoNLL-2009 shared task, but outperforms their models.
",4 Syntactic Contribution,0,[0]
This section thus attempts to explore how much syntax contributes to dependency-based SRL in deep learning framework and how to effectively evaluate relative performance of syntax-based SRL.,4 Syntactic Contribution,0,[0]
"To this end, we conduct experiments for empirical analysis with different syntactic inputs.
",4 Syntactic Contribution,0,[0]
"Syntactic Input In order to obtain different syntactic inputs, we design a faulty syntactic tree generator (refer to STG hereafter), which is able to produce random errors in the output parse tree like a true parser does.",4 Syntactic Contribution,0,[0]
"To simplify implementation, we construct a new syntactic tree based on the gold standard parse tree.",4 Syntactic Contribution,0,[0]
"Given an input error probability distribution estimated from a true parser output, our algorithm presented in Algorithm 2 stochastically modifies the syntactic heads of nodes on the premise of a valid tree.
",4 Syntactic Contribution,0,[0]
"Evaluation Measure For SRL task, the primary evaluation measure is the semantic labeled F1 score.",4 Syntactic Contribution,0,[0]
"However, the score is influenced by the quality of syntactic input to some extent, leading to unfaithfully reflecting the competence of syntax-based SRL system.",4 Syntactic Contribution,0,[0]
"Namely, this is not the outcome of a true and fair quantitative comparison for these types of SRL models.",4 Syntactic Contribution,0,[0]
"To normalize the semantic score relative to syntactic parse, we take into account additional evaluation measure to estimate the actual overall performance of SRL.",4 Syntactic Contribution,0,[0]
"Here, we use the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies
Algorithm 2 Faulty Syntactic Tree Generator Input: A gold standard syntactic tree GT , the
specific error probability p Output: The new generative syntactic tree NT
1: N denotes the number of nodes in GT 2: for each node n ∈ GT do 3: r = random(0, 1), a random number 4: if r < p then 5: h = random(0, N ), a random integer 6: find the syntactic head nh of n in GT 7: modify nh = h, and get a new tree NT 8: if NT is a valid tree then 9: break
10: else 11: goto step 5 12: end if 13: end if 14: end for 15: return the new generative tree NT
proposed by Surdeanu et al. (2008) as evaluation metric.6 The benefits of this measure are twofold: quantitatively evaluating syntactic contribution to SRL and impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.
",4 Syntactic Contribution,0,[0]
Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.,4 Syntactic Contribution,0,[0]
"Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we
6The idea of ratio score in Surdeanu et al. (2008) actually was from author of this paper, Hai Zhao, which has been indicated in the acknowledgement part of Surdeanu et al. (2008).
7Note that several SRL systems without providing syntactic information are not listed in the table.
obtain the highest results both on Sem-F1 and the Sem-F1/LAS ratio, respectively.",4 Syntactic Contribution,0,[0]
These results show that our SRL component is relatively much stronger.,4 Syntactic Contribution,0,[0]
"Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself.",4 Syntactic Contribution,0,[0]
"All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a)",4 Syntactic Contribution,0,[0]
(Joint track using self-developed parser).,4 Syntactic Contribution,0,[0]
"We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task.
",4 Syntactic Contribution,0,[0]
"We also perform our first and tenth order pruning models with different erroneous syntactic inputs generated from STG and evaluate their per-
formance using the Sem-F1/LAS ratio.",4 Syntactic Contribution,0,[0]
Figure 6 shows Sem-F1 scores at different quality of syntactic parse inputs on the English test set whose LAS varies from 85% to 100%.,4 Syntactic Contribution,0,[0]
"Compared to previous state-of-the-arts (Marcheggiani and Titov, 2017).",4 Syntactic Contribution,0,[0]
"Our tenth-order pruning model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, while our firstorder pruning model yields overall lower results (1-5% F1 drop), owing to missing too many true arguments.",4 Syntactic Contribution,0,[0]
These results show that high-quality syntactic parses may indeed enhance dependency SRL.,4 Syntactic Contribution,0,[0]
"Furthermore, it indicates that our model with an accurate enough syntactic input as Marcheggiani and Titov (2017), namely, 90% LAS, will give a Sem-F1 exceeding 90% for the first time in the research timeline of semantic role labeling.",4 Syntactic Contribution,0,[0]
Semantic role labeling was pioneered by Gildea and Jurafsky (2002).,5 Related Work,0,[0]
"Most traditional SRL models rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Björkelund et al., 2009).",5 Related Work,0,[0]
"Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm.",5 Related Work,0,[0]
"Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task.
",5 Related Work,0,[0]
"With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed.",5 Related Work,0,[0]
"Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach.",5 Related Work,0,[0]
"Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning.",5 Related Work,0,[0]
Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success.,5 Related Work,0,[0]
Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models.,5 Related Work,0,[0]
"Differently, Marcheggiani et al. (2017) proposed a
syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models.
",5 Related Work,0,[0]
"However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance.",5 Related Work,0,[0]
"This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning.",5 Related Work,0,[0]
"Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017).",5 Related Work,0,[0]
"Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage.",5 Related Work,0,[0]
"He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance.",5 Related Work,0,[0]
"This paper presents a simple and effective neural model for dependency-based SRL, incorporating syntactic information with the proposed extended k-order pruning algorithm.",6 Conclusion and Future Work,0,[0]
"With a large enough setting of k, our pruning algorithm will result in a syntax-agnostic setting for the argument labeling model, which smoothly unifies syntax-aware and syntax-agnostic SRL in a consistent way.",6 Conclusion and Future Work,0,[0]
"Experimental results show that with the help of deep enhanced representation, our model outperforms the previous state-of-the-art models in both syntaxaware and syntax-agnostic situations.
",6 Conclusion and Future Work,0,[0]
"In addition, we consider the Sem-F1/LAS ratio as a mean of evaluating syntactic contribution to SRL, and true performance of SRL independent of the quality of syntactic parser.",6 Conclusion and Future Work,0,[0]
"Though we again confirm the importance of syntax to SRL with empirical experiments, we are aware that since (Pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic SRL has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work.",6 Conclusion and Future Work,0,[0]
"However, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic SRL system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input.",6 Conclusion and Future Work,0,[0]
Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence.,abstractText,0,[0]
Previous studies have shown syntactic information has a remarkable contribution to SRL performance.,abstractText,0,[0]
"However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone.",abstractText,0,[0]
This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework.,abstractText,0,[0]
We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information.,abstractText,0,[0]
"Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",abstractText,0,[0]
"Syntax for Semantic Role Labeling, To Be, Or Not To Be",title,0,[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,0,[0]
Simultaneous interpretation is challenging because it demands both quality and speed.,1 Introduction,0,[0]
Conventional batch translation waits until the entire sentence is completed before starting to translate.,1 Introduction,0,[0]
This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience.,1 Introduction,0,[0]
Simultaneous interpretation instead requires a tradeoff between quality and speed.,1 Introduction,0,[0]
A common strategy is to translate independently translatable segments as soon as possible.,1 Introduction,0,[0]
"Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders.",1 Introduction,0,[0]
We show an example of Japanese-English translation in Figure 1.,1 Introduction,0,[0]
"Consider the batch translation: in English, the verb change comes immediately after the subject",1 Introduction,0,[0]
"We, whereas in Japanese it comes at the end
of the sentence; therefore, to produce an intelligible English sentence, we must translate the object after the final verb is observed, resulting in one large and painfully delayed segment.
",1 Introduction,0,[0]
"To reduce structural discrepancy, we can apply syntactic transformations to make the word order of one language closer to the other.",1 Introduction,0,[0]
Consider the monotone translation in Figure 1.,1 Introduction,0,[0]
"By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb.",1 Introduction,0,[0]
"Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction.",1 Introduction,0,[0]
"These transformations enable us to divide the input into shorter segments, thus reducing translation delay.
",1 Introduction,0,[0]
"To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order.",1 Introduction,0,[0]
Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff.,1 Introduction,0,[0]
"However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010).",1 Introduction,0,[0]
"In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice.
",1 Introduction,0,[0]
We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ syntactic flexibility.,1 Introduction,0,[0]
"We propose to rewrite the reference translation in a way that uses the original lexicon, obeys standard grammar rules of
55
the target language, preserves the original semantics, and yields more monotonic translations.",1 Introduction,0,[0]
We then train the MT system with the rewritten references so that it learns how to produce low-latency translations from the data.,1 Introduction,0,[0]
A data-driven approach to learning these rewriting rules is hampered by the dearth of parallel data: we have few examples of text that have been both interpreted and translated.,1 Introduction,0,[0]
"Therefore, we design syntactic transformation rules based on linguistic analysis of the source and the target languages.",1 Introduction,0,[0]
We apply these rules to parsed text and decide whether to accept the rewritten sentence based on the amount of delay reduction.,1 Introduction,0,[0]
"In this work, we focus on Japanese to English translation, because (i) Japanese and English have significantly different word orders (SOV vs. SVO); and consequently, (ii) the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence.
",1 Introduction,0,[0]
We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting.,1 Introduction,0,[0]
Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations.,1 Introduction,0,[0]
Simultaneous interpretation has two goals: producing good translations and producing them promptly.,2 The Problem of Delay Reduction,0,[0]
"However, most existing parallel corpora and MT systems do not address the issue of delay during translation.",2 The Problem of Delay Reduction,0,[0]
We explicitly adapt the training data by rewriting rules to reduce delay.,2 The Problem of Delay Reduction,0,[0]
We first define translation delay and describe—in general terms— our rewriting rules.,2 The Problem of Delay Reduction,0,[0]
"In the next section, we describe the rules in more detail.
",2 The Problem of Delay Reduction,0,[0]
"While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input.",2 The Problem of Delay Reduction,0,[0]
"Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation.",2 The Problem of Delay Reduction,0,[0]
We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1,2 The Problem of Delay Reduction,0,[0]
"For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word.",2 The Problem of Delay Reduction,0,[0]
"At test time, when the input is segmented, the delay is the average segment length.",2 The Problem of Delay Reduction,0,[0]
"During the data preprocessing step of rewriting, we calculate delay from word alignments (Section 4).
",2 The Problem of Delay Reduction,0,[0]
"Given a reference batch translation x, we apply a set of rewriting rulesR to x to minimize its delay.",2 The Problem of Delay Reduction,0,[0]
"A rewriting rule r ∈ R is a mapping that takes the constituent parse tree of x as input and outputs a modified parse tree, which specifies a rewritten sentence x′.",2 The Problem of Delay Reduction,0,[0]
"The tree-editing operation includes node deletion, insertion, and swapping, as well as induced changes of word form and node label.",2 The Problem of Delay Reduction,0,[0]
"A valid transformation rule should rearrange constituents in x to follow the word order of the input sentence as closely as possible, subject to grammatical constraints and preservation of the original meaning.
",2 The Problem of Delay Reduction,0,[0]
"1Ideally, delay should be based on time lapse.",2 The Problem of Delay Reduction,0,[0]
"However, timestamping is not applicable to typical MT corpus, therefore we approximate it by number of tokens and ignore decoding time.",2 The Problem of Delay Reduction,0,[0]
We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences.,3 Transformation Rules,0,[0]
"Our rules cover verb, noun, and clause reordering.",3 Transformation Rules,0,[0]
"While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",3 Transformation Rules,0,[0]
The most significant difference between Japanese and English is that the head of a verb phrase comes at the end of Japanese sentences.,3.1 Verb Phrases,0,[0]
"In English, it occupies one of the initial positions.",3.1 Verb Phrases,0,[0]
"We now introduce rules that can postpone a head verb.
",3.1 Verb Phrases,0,[0]
Passivization and Activization,3.1 Verb Phrases,0,[0]
"In Japanese, the standard structure of a sentence is NP1 NP2 verb, where case markers following the verb indicate the voice of the sentence.",3.1 Verb Phrases,0,[0]
"However, in English, we have NP1 verb NP2, where the form of the verb indicates its voice.",3.1 Verb Phrases,0,[0]
Changing the voice is particularly useful when NP2 (object in an active-voice sentence and subject in a passive-voice sentence) is long.,3.1 Verb Phrases,0,[0]
"By reversing positions of verb and NP2, we are not held back by the upcoming verb and can start to translate NP2 immediately.",3.1 Verb Phrases,0,[0]
"Figure 1 shows an example in which passive voice can help make the target and source word orders more compatible, but it is not the case that passivizing every sentence would be a good idea; sometimes making a passive sentence active makes the word orders more compatible if the objects are relatively short:
",3.1 Verb Phrases,0,[0]
O: The talk was denied by the boycott group spokesman.,3.1 Verb Phrases,0,[0]
R:,3.1 Verb Phrases,0,[0]
"The boycott group spokesman denied the talk.
",3.1 Verb Phrases,0,[0]
"Quotative Verbs Quotative verbs are verbs that, syntactically and semantically, resemble said and often start an independent clause.",3.1 Verb Phrases,0,[0]
"Such verbs are frequent, especially in news, and can be moved to the end of a sentence:
",3.1 Verb Phrases,0,[0]
O: They announced that the president will restructure the division.,3.1 Verb Phrases,0,[0]
R:,3.1 Verb Phrases,0,[0]
"The president will restructure the division, they announced.
",3.1 Verb Phrases,0,[0]
"In addition to quotative verbs, candidates typically include factive (e.g., know, realize, observe), factive-like (e.g., announce, determine), belief (e.g., believe, think, suspect), and antifactive (e.g., doubt, deny) verbs.",3.1 Verb Phrases,0,[0]
"When these verbs are followed by a
clause (S or SBAR), we move the verb and its subject to the end of the clause.
",3.1 Verb Phrases,0,[0]
"While some exploratory work automatically extracts factive verbs, to our knowledge, an exhaustive list does not exist.",3.1 Verb Phrases,0,[0]
"To obtain a list with reasonable coverage, we exploit the fact that Japanese has an unambiguous quotative particle, to, that precedes such verbs.2 We identify all of the verbs in the Kyoto corpus (Neubig, 2011) marked by the quotative particle and translate them into English.",3.1 Verb Phrases,0,[0]
We then use these as our quotative verbs.3,3.1 Verb Phrases,0,[0]
Another difference between Japanese and English lies in the order of adjectives and the nouns they modify.,3.2 Noun Phrases,0,[0]
"We identify two situations where we can take advantage of the flexibility of English grammar to favor sentence structures consistent with positions of nouns in Japanese.
",3.2 Noun Phrases,0,[0]
"Genitive Reordering In Japanese, genitive constructions always occur in the form of X no Y, where Y belongs to X.",3.2 Noun Phrases,0,[0]
"In English, however, the order may be reversed through the of construction.",3.2 Noun Phrases,0,[0]
"Therefore, we transform constructions NP1 of NP2 to possessives using the apostrophe-s, NP2’(s) NP1 (Figure 1).",3.2 Noun Phrases,0,[0]
We use simple heuristics to decide if such a transformation is valid.,3.2 Noun Phrases,0,[0]
"For example, when X / Y contains proper nouns (e.g., the City of New York), numbers (e.g., seven pounds of sugar), or pronouns (e.g., most of them), changing them to the possessive case is not legal.
that Clause In English, clauses are often modified through a pleonastic pronoun.",3.2 Noun Phrases,0,[0]
"E.g., It is ADJP to/that SBAR/S.",3.2 Noun Phrases,0,[0]
"In Japanese, however, the subject (clause) is usually put at the beginning.",3.2 Noun Phrases,0,[0]
"To be consistent with the Japanese word order, we move the modified clause to the start of the sentence: To S/SBAR is ADJP.",3.2 Noun Phrases,0,[0]
"The rewritten English sentence is still grammatical, although its structure is less frequent in common English usage.",3.2 Noun Phrases,0,[0]
"For example,
O: It is important to remain watchful.",3.2 Noun Phrases,0,[0]
"R: To remain watchful is important.
",3.2 Noun Phrases,0,[0]
2We use a morphological analyzer to distinguish between the conjunction and quotative particles.,3.2 Noun Phrases,0,[0]
"Examples of words marked by this particle include 見られる (expect), 言う (say), 思われる (seem), する (assume), 信じる (believe) and so on.
",3.2 Noun Phrases,0,[0]
3We also include the phrase It looks like.,3.2 Noun Phrases,0,[0]
"In Japanese, clausal conjunctions are often marked at the end of the initial clause of a compound sentence.",3.3 Conjunction Clause,0,[0]
"In English, however, the order of clauses is more flexible.",3.3 Conjunction Clause,0,[0]
We can therefore reduce delay by reordering the English clauses to mirror how they typically appear in Japanese.,3.3 Conjunction Clause,0,[0]
"Below we describe rules reversing the order of clauses connected by these conjunctions:
• Clausal conjunctions: because (of), in order to • Contrastive conjunctions: despite, even though, although • Conditionals: (even) if, as a result (of) •",3.3 Conjunction Clause,0,[0]
"Misc: according to
In standard Japanese, such conjunctions include no de, kara, de mo and so on.",3.3 Conjunction Clause,0,[0]
"The sentence often appears in the form of S2 conj, S1.",3.3 Conjunction Clause,0,[0]
"In English, however, two common constructions are
S1 conj S2: We should march because winter is coming.",3.3 Conjunction Clause,0,[0]
"conj S2, S1: Because winter is coming, we should march.
",3.3 Conjunction Clause,0,[0]
"To follow the Japanese clause order, we adapt the above two constructions to
S2, conj’ S1: Winter is coming, because of this, we should march.
",3.3 Conjunction Clause,0,[0]
Here conj’ represents the original conjunction word appended with simple pronouns/phrases to refer to S2.,3.3 Conjunction Clause,0,[0]
"For example, because → because of this, even if → even if this is the case.",3.3 Conjunction Clause,0,[0]
We now turn our attention to the implementation of the syntactic transformation rules described above.,4 Sentence Rewriting Process,0,[0]
"Applying a transformation consists of three steps:
1.",4 Sentence Rewriting Process,0,[0]
Detection:,4 Sentence Rewriting Process,0,[0]
Identify nodes in the parse tree for which the transformation is applicable; 2.,4 Sentence Rewriting Process,0,[0]
Modification: Transform nodes and labels; 3.,4 Sentence Rewriting Process,0,[0]
"Evaluation: Compute delay reduction, and
decide whether to accept the rewritten sentence.
",4 Sentence Rewriting Process,0,[0]
Figure 2 illustrates the process using passivization as an example.,4 Sentence Rewriting Process,0,[0]
"In the detection step, we find the subtree that satisfies the condition of applying a rule.",4 Sentence Rewriting Process,0,[0]
"In this case, we look for an S node whose children include an NP (denoted by NP1), the subject, and a VP to its right, such that the VP node has a leaf VB*, the main verb,4 followed by another NP (denoted by NP2), the object.",4 Sentence Rewriting Process,0,[0]
We allow the parent nodes (S and VP) to have additional children besides the matched ones.,4 Sentence Rewriting Process,0,[0]
They are not affected during the transformation.,4 Sentence Rewriting Process,0,[0]
"In the modification step, we swap the subject node and object node; we add the verb be in its correct form by checking the tense of the verb and the form of NP2;5and we add the preposition by before the subject.",4 Sentence Rewriting Process,0,[0]
"The process is executed recursively throughout the parse tree.
",4 Sentence Rewriting Process,0,[0]
"4The main verb excludes be and have when it indicates tense (e.g., have done).
",4 Sentence Rewriting Process,0,[0]
"5We use the Nodebox linguistic library (https://www. nodebox.net/code) to detect and modify word forms.
",4 Sentence Rewriting Process,0,[0]
"Although our rules are designed to minimize long range reordering, there are exceptions.6 Thus applying a rule does not always reduce delay.",4 Sentence Rewriting Process,0,[0]
"In the evaluation step, we compare translation delay before and after applying the rule.",4 Sentence Rewriting Process,0,[0]
"We accept a rewritten sentence if its delay is reduced; otherwise, we revert to the input sentence.",4 Sentence Rewriting Process,0,[0]
"Since we do not segment sentences during rewriting, we must estimate the delay.
",4 Sentence Rewriting Process,0,[0]
"To estimate the delay, we use word alignments.",4 Sentence Rewriting Process,0,[0]
Figure 2c shows the source Japanese sentence in its word-for-word English translation and alignments from the target words to the source words.,4 Sentence Rewriting Process,0,[0]
"The first English word, We, is aligned to the first Japanese word; it can thus be treated as an independent segment and translated immediately.",4 Sentence Rewriting Process,0,[0]
"The second English word, love, is aligned to the last Japanese word, which means the system cannot start to translate until four more Japanese words are revealed.",4 Sentence Rewriting Process,0,[0]
This alignment therefore forms a segment with delay of four words/seg.,4 Sentence Rewriting Process,0,[0]
"Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay.",4 Sentence Rewriting Process,0,[0]
"In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg.",4 Sentence Rewriting Process,0,[0]
"Therefore, we accept the rewritten sentence.",4 Sentence Rewriting Process,0,[0]
"However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay.
",4 Sentence Rewriting Process,0,[0]
We can now formally define the delay.,4 Sentence Rewriting Process,0,[0]
Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to.,4 Sentence Rewriting Process,0,[0]
"We define the delay of ei as di = max(0, ai−maxj<i aj).",4 Sentence Rewriting Process,0,[0]
The delay of x is then ∑N i=1,4 Sentence Rewriting Process,0,[0]
"di/N , where the sum is over all aligned words except punctuation and stopwords.
",4 Sentence Rewriting Process,0,[0]
"Given a set of rules, we need to decide which rules to apply and in what order.",4 Sentence Rewriting Process,0,[0]
"Fortunately, our rules have little interaction with each other, and the order of application has a negligible effect.",4 Sentence Rewriting Process,0,[0]
"We apply the rules, roughly, sequentially in order of complexity: if the output of current rule is not accepted, the sentence is reverted to the last accepted version.",4 Sentence Rewriting Process,0,[0]
"We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003).",5 Experiments,0,[0]
"For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1.",5 Experiments,0,[0]
"The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents.
",5 Experiments,0,[0]
"We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences.",5 Experiments,0,[0]
"Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings.",5 Experiments,0,[0]
We use GIZA++,5 Experiments,0,[0]
"(Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning.",5 Experiments,0,[0]
"The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model.",5 Experiments,0,[0]
"After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted.",5.1 Quality of Rewritten Translations,0,[0]
The most generalizable rules are passivization and delaying quotative verbs.,5.1 Quality of Rewritten Translations,0,[0]
"We rewrite 32.2% of sentences, reducing the delay from 9.9 words/seg to 6.3 words/seg per segment for rewritten sentences and from 7.8 words/seg to 6.7 words/seg overall.
",5.1 Quality of Rewritten Translations,0,[0]
"6For example, in clause transformation, the Japanese conjunction moshi, which is clause initial, may appear at the beginning of a sentence to emphasize conditionals, although its appearance is relatively rare.
",5.1 Quality of Rewritten Translations,0,[0]
"7Available at http://eijiro.jp 8Available at http://www.atilika.org/ 9In contrast to BLEU, RIBES is an order-sensitive metric commonly used for translation between Japanese and English.
",5.1 Quality of Rewritten Translations,0,[0]
We evaluate the quality of our rewritten sentences from two perspectives: grammaticality and preserved semantics.,5.1 Quality of Rewritten Translations,0,[0]
"To examine how close the rewritten sentences are to standard English, we train a 5-gram language model using the English data from the Europarl corpus, consisting of 46 million words, and use it to compute perplexity.",5.1 Quality of Rewritten Translations,0,[0]
Rewriting references increases the perplexity under the language model only slightly: from 332.0 to 335.4.,5.1 Quality of Rewritten Translations,0,[0]
"To ensure that rewrites leave meaning unchanged, we use the SEMAFOR semantic role labeler (Das et al., 2014) on the original and modified sentence; for each role-labeled token in the reference sentence, we examine its corresponding role in the rewritten sentence and calculate the average accuracy acrosss all sentences.",5.1 Quality of Rewritten Translations,0,[0]
"Even ignoring benign lexical changes—for example, he becoming him in a passivized sentence—95.5% of the words retain their semantic roles in the rewritten sentences.
",5.1 Quality of Rewritten Translations,0,[0]
"Although our rules are conservative to minimize corruption, some errors are unavoidable propagation of parser errors.",5.1 Quality of Rewritten Translations,0,[0]
"For example, the sentence the London Stock Exchange closes at 1230 GMT today is parsed as:10 (S (NP the London Stock Exchange) (VP (VBZ closes)
(PP at 1230) (NP GMT today)))
",5.1 Quality of Rewritten Translations,0,[0]
GMT today is separated from the PP as an NP and is mistaken as the object.,5.1 Quality of Rewritten Translations,0,[0]
The passive version is then GMT today is closed at 1230 by the London Stock Exchange.,5.1 Quality of Rewritten Translations,0,[0]
"Such errors could be reduced by skipping nodes with low inside/outside scores given by the parser, or skipping low-frequency patterns.",5.1 Quality of Rewritten Translations,0,[0]
"However, we leave this for future work.",5.1 Quality of Rewritten Translations,0,[0]
"At test time, we use right probability (Fujita et al., 2013, RP) to decide when to start translating a
10For simplicity we show the shallow parse only.
sentence.",5.2 Segmentation,0,[0]
"As we read in the source Japanese sentence, if the input segment matches an entry in the learned phrase table, we query the RP of the Japanese/English phrase pair.",5.2 Segmentation,0,[0]
A higher RP indicates that the English translation of this Japanese phrase will likely be followed by the translation of the next Japanese phrase.,5.2 Segmentation,0,[0]
"In other words, translation of the two consecutive Japanese phrases is monotonic, thus, we can begin translating immediately.",5.2 Segmentation,0,[0]
"Following (Fujita et al., 2013), if the RP of the current phrase is lower than a fixed threshold, we cache the current phrase and wait for more words from the source sentence; otherwise, we translate all cached phrases.",5.2 Segmentation,0,[0]
"Finally, translations of segments are concatenated to form a complete translation of the input sentence.",5.2 Segmentation,0,[0]
"To show the effect of rewritten references, we compare the following MT systems:
• GD: only gold reference translations; • RW: only rewritten reference translations; • RW+GD: both gold and the rewritten refer-
ences; and • RW-LM+GD: using gold reference transla-
tions but using the rewritten references for training the LM and for tuning.
",5.3 Speed/Accuracy Trade-off,0,[0]
"For RW+GD and RW-LM+GD, we interpolate the language models of GD and RW.",5.3 Speed/Accuracy Trade-off,0,[0]
The interpolating weight is tuned with the rewritten sentences.,5.3 Speed/Accuracy Trade-off,0,[0]
"For RW+GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new.
Increasing the RP threshold increases interpretation delay but improves the quality of the translation.",5.3 Speed/Accuracy Trade-off,0,[0]
"We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation).",5.3 Speed/Accuracy Trade-off,0,[0]
Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold.,5.3 Speed/Accuracy Trade-off,0,[0]
Rewritten sentences alone do not significantly improve over the baseline.,5.3 Speed/Accuracy Trade-off,0,[0]
"We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing errors, which impairs learning.",5.3 Speed/Accuracy Trade-off,0,[0]
"However, combining RW and GD results in a better speed-accuracy tradeoff: the RW+GD curve completely dominates other curves in Figure 3a, 3c.",5.3 Speed/Accuracy Trade-off,0,[0]
"Thus, using more monotone translations improves simultaneous machine translation, and because RW-LM+GD is about
0 5 10 15 20 25 30 35
Average # of words per segment
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(a) BLEU w.r.t.",5.3 Speed/Accuracy Trade-off,0,[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD RW-LM+GD RW GD
(b) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,0,[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
10
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(c) BLEU",5.3 Speed/Accuracy Trade-off,0,[0]
w.r.t.,5.3 Speed/Accuracy Trade-off,0,[0]
"rewritten ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD",5.3 Speed/Accuracy Trade-off,0,[0]
"RW-LM+GD RW GD
(d) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,0,[0]
"rewritten ref
Figure 3: Speed/accuracy tradeoff curves: BLEU (left) /",5.3 Speed/Accuracy Trade-off,0,[0]
"RIBES (right) versus translation delay (average number of words per segment).
",5.3 Speed/Accuracy Trade-off,0,[0]
"the same as GD, the major improvement likely comes from the translation model from rewritten sentences.
",5.3 Speed/Accuracy Trade-off,0,[0]
The right two plots recapitulate the evaluation with the RIBES metric.,5.3 Speed/Accuracy Trade-off,0,[0]
"This result is less clear, as MT systems are optimized for BLEU and RIBES penalizes word reordering, making it difficult to compare systems that intentionally change word order.",5.3 Speed/Accuracy Trade-off,0,[0]
"Nevertheless, RW is comparable to GD on gold references and superior to the baseline on rewritten references.",5.3 Speed/Accuracy Trade-off,0,[0]
"Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation.",5.4 Effect on Verbs,0,[0]
One reason is that SOV to SVO translation often drops the verb because of long range reordering.,5.4 Effect on Verbs,0,[0]
"(We see this for Japanese here, but this is also true for German.)",5.4 Effect on Verbs,0,[0]
"Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins
et al., 2005; Xu et al., 2009).",5.4 Effect on Verbs,0,[0]
"Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW+GD, as well as the number in the gold reference translation.",5.4 Effect on Verbs,0,[0]
"Both RW and RW+GD produce more verbs (a statistically significant result), although RW+GD captures the most verbs.",5.4 Effect on Verbs,0,[0]
Table 4 compares translations by GD and RW.,5.5 Error Analysis,0,[0]
"RW correctly puts the verb said at the end, while GD drops the final verb.",5.5 Error Analysis,0,[0]
"However, RW still produces he at the beginning (also the first word in the Japanese source sentence).",5.5 Error Analysis,0,[0]
This is because our current segmentation strategy do not preserve words for later translation—a note-taking strategy used by human interpreters.,5.5 Error Analysis,0,[0]
Previous approaches to simultaneous machine translation have employed explicit interpretation strategies for coping with delay.,6 Related Work,0,[0]
"Two major approaches are segmentation and prediction.
",6 Related Work,0,[0]
"Most segmentation strategies are based on heuristics, such as pauses in speech (Fügen et al., 2007; Bangalore et al., 2009), comma prediction (Sridhar et al., 2013) and phrase reordering probability (Fujita et al., 2013).",6 Related Work,0,[0]
Learning-based methods have also been proposed.,6 Related Work,0,[0]
Oda et al. (2014) find segmentations that maximize the BLEU score of the final concatenated translation by dynamic programming.,6 Related Work,0,[0]
Grissom II et al. (2014) formulate simultaneous translation as a sequential decision making problem and uses reinforcement learning to decide when to translate.,6 Related Work,0,[0]
"One limitation of these methods is that when learning with standard batch MT corpus, their gain can be restricted by natural word reordering between the source and the target sentences, as explained in Section 1.
",6 Related Work,0,[0]
"In an SOV-SVO context, methods to predict unseen words are proposed to alleviate the above restriction.",6 Related Work,0,[0]
Matsubara et al. (1999) predict the English verb in the target sentence and integrates it syntactically.,6 Related Work,0,[0]
"Grissom II et al. (2014) predict the final verb in the source sentence and decide when to use the predicted verb with reinforcement learning.
",6 Related Work,0,[0]
"Nevertheless, unless the predictor considers contextual and background information, which human interpreters often rely on for prediction (Hönig, 1997; Camayd-Freixas, 2011), such a prediction task is inherently hard.
",6 Related Work,0,[0]
"Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one.",6 Related Work,0,[0]
"We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system.",6 Related Work,0,[0]
"In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined.
",6 Related Work,0,[0]
"This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders.",6 Related Work,0,[0]
"However, our problem is different in several ways.",6 Related Work,0,[0]
"First, while the approaches resemble each other, our motivation is to reduce translation delay.",6 Related Work,0,[0]
"Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed.",6 Related Work,0,[0]
"Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences.",6 Related Work,0,[0]
Training MT systems with more monotonic (interpretation-like) sentences improves the speedaccuracy tradeoff for simultaneous machine translation.,7 Conclusion,0,[0]
"By designing syntactic transformations and rewriting batch translations into more monotonic translations, we reduce the translation delay.",7 Conclusion,0,[0]
"MT systems trained on the rewritten reference translations learn interpretation strategies implicitly from the data.
",7 Conclusion,0,[0]
Our rewrites are based on linguistic knowledge and inspired by techniques used by human interpreters.,7 Conclusion,0,[0]
"They cover a wide range of reordering phenomena between Japanese and English, and more generally, between SOV and SVO languages.",7 Conclusion,0,[0]
A natural extension is to automatically extract such rules from parallel corpora.,7 Conclusion,0,[0]
"While there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing.",7 Conclusion,0,[0]
This work was supported by NSF grant IIS1320538.,Acknowledgments,0,[0]
Boyd-Graber is also partially supported by NSF grants CCF-1409287 and NCSE-1422492.,Acknowledgments,0,[0]
Daumé III,Acknowledgments,0,[0]
and He are also partially supported by NSF grant IIS-0964681.,Acknowledgments,0,[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,0,[0]
Divergent word order between languages causes delay in simultaneous machine translation.,abstractText,0,[0]
We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff.,abstractText,0,[0]
We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.,abstractText,0,[0]
We apply the rules to reference translations to make their word order closer to the source language word order.,abstractText,0,[0]
"On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",abstractText,0,[0]
Syntax-based Rewriting for Simultaneous Machine Translation,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1325–1337 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1325",text,0,[0]
Dependency parsing is a core task in natural language processing (NLP).,1 Introduction,0,[0]
"Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words.",1 Introduction,0,[0]
"While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareček, 2016).",1 Introduction,0,[0]
So it is extremely helpful to have some treebank of supervised parses for training purposes.,1 Introduction,0,[0]
"Unfortunately, manually constructing a treebank for a new target language is expensive (Böhmová et al., 2003).",1.1 Past work: Cross-lingual transfer,0,[0]
"As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017).",1.1 Past work: Cross-lingual transfer,0,[0]
The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages.,1.1 Past work: Cross-lingual transfer,0,[0]
"Although the parser cannot be expected to know the words of the target language, it can make do with parts of
speech (POS) (McDonald et al., 2011; Täckström",1.1 Past work: Cross-lingual transfer,0,[0]
"et al., 2013; Zhang and Barzilay, 2015) or crosslingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).",1.1 Past work: Cross-lingual transfer,0,[0]
"A more serious challenge is that the parser may not know how to handle the word order of the target language, unless the source treebank comes from a closely related language (e.g., using German to parse Luxembourgish).",1.1 Past work: Cross-lingual transfer,0,[0]
"Training the parser on trees from multiple source languages may mitigate this issue (McDonald et al., 2011) because the parser is more likely to have seen target part-of-speech sequences somewhere in the training data.",1.1 Past work: Cross-lingual transfer,0,[0]
"Some authors (Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016) have shown additional improvements by preferring source languages that are “close” to the target language, where the closeness is measured by distance between POS language models trained on the source and target corpora.",1.1 Past work: Cross-lingual transfer,0,[0]
"We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree.",1.2 This paper: Tailored synthetic data,0,[0]
"We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language.",1.2 This paper: Tailored synthetic data,0,[0]
"This is the setup of Zeman and Resnik (2008) and Søgaard (2011a).
",1.2 This paper: Tailored synthetic data,0,[0]
"Our novel ingredient is that rather than seek a close source language that already exists, we create one.",1.2 This paper: Tailored synthetic data,0,[0]
How?,1.2 This paper: Tailored synthetic data,0,[0]
"Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language.
",1.2 This paper: Tailored synthetic data,0,[0]
And how do we find this distribution?,1.2 This paper: Tailored synthetic data,0,[0]
We adopt the tree-permutation model of Wang and Eisner (2016).,1.2 This paper: Tailored synthetic data,0,[0]
"We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bigrams in the permuted source treebank.",1.2 This paper: Tailored synthetic data,0,[0]
"This allows us to evaluate p by computing the divergence between the bigram POS language model formed by these expected counts,
and the one formed by the observed counts of POS bigrams in the unparsed target language.",1.2 This paper: Tailored synthetic data,0,[0]
"In order to find a p that locally minimizes this divergence, we adjust the model parameters by stochastic gradient descent (SGD).",1.2 This paper: Tailored synthetic data,0,[0]
Better measures of surface closeness between two languages might be devised.,1.3 Key limitations in this paper,0,[0]
"However, even counting the expected POS N -grams is moderately expensive, taking time exponential in N if done exactly.",1.3 Key limitations in this paper,0,[0]
"So we compute only these local statistics, and only for N = 2.",1.3 Key limitations in this paper,0,[0]
We certainly need N > 1 because the 1-gram distribution is not affected by permutation at all.,1.3 Key limitations in this paper,0,[0]
"N = 2 captures useful bigram statistics: for example, to mimic a verb-final language with prenominal modifiers, we would seek constituent permutations that result in matching its relatively high rate of VERB–PUNCT and ADJ–NOUN bigrams.",1.3 Key limitations in this paper,0,[0]
"While N > 2 might have improved the results, it was too slow for our large-scale experimental design.",1.3 Key limitations in this paper,0,[0]
"§7 discusses how richer measures could be used in the future.
",1.3 Key limitations in this paper,0,[0]
"We caution that throughout this paper, we assume that our corpora are annotated with gold POS tags, even in the target language (which lacks any gold training trees).",1.3 Key limitations in this paper,0,[0]
This is an idealized setting that has often been adopted in work on unsupervised and cross-lingual transfer.§7 discusses a possible avenue for doing without gold tags.,1.3 Key limitations in this paper,0,[0]
We begin by motivating the idea of tree permutation.,2 Modeling Surface Realization,0,[0]
Let us suppose that the dependency tree for a sentence starts as a labeled graph—a tree in which siblings are not yet ordered with respect to their parent or one another.,2 Modeling Surface Realization,0,[0]
Each language has some systematic way to realize its unordered trees as surface strings:1 it imposes a particular order on the tree’s word tokens.,2 Modeling Surface Realization,0,[0]
"More precisely, a language specifies a distribution p(string | unordered tree) over a tree’s possible realizations.
",2 Modeling Surface Realization,0,[0]
"As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages.",2 Modeling Surface Realization,0,[0]
"That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface.
",2 Modeling Surface Realization,0,[0]
"1Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018).",2 Modeling Surface Realization,0,[0]
"Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018).
",2 Modeling Surface Realization,0,[0]
"Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees.",2 Modeling Surface Realization,0,[0]
"To obtain samples of the latter distribution, we use the treebanks of one or more other languages.",2 Modeling Surface Realization,0,[0]
The present paper evaluates our method when only a single source treebank is used.,2 Modeling Surface Realization,0,[0]
"In the future, we could try tuning a mixture of all available source treebanks.",2 Modeling Surface Realization,0,[0]
We presume that the target language applies the same stochastic realization model to all trees.,2.1 Realization is systematic,0,[0]
All that we can optimize is the parameter vector of this model.,2.1 Realization is systematic,0,[0]
"Thus, we deny ourselves the freedom to realize each individual tree in an ad hoc way.",2.1 Realization is systematic,0,[0]
"To see why this is important, suppose the target language is French, whose corpus u contains many NOUN–ADJ bigrams.",2.1 Realization is systematic,0,[0]
"We could achieve such a bigram from the unordered source tree
DET NOUN VERB PROPN ADJ
the cake made Sue sleepy
det nsubj dobj xcomp
by ordering
it to yield DET NOUN ADJ VERB PROPN the cake sleepy made Sue
det dobjxcomp nsubj
.",2.1 Realization is systematic,0,[0]
"However, that realization is not in fact appropriate for French, so that ordered tree would not be a useful training tree for French.",2.1 Realization is systematic,0,[0]
"Our approach should disprefer this tempting but incorrect realization, because any model with a high probability of this realization would, if applied systematically over the whole corpus, also yield sentences like He sleepy made Sue, with unwanted PRON–ADJ bigrams that would not match the surface statistics of French.",2.1 Realization is systematic,0,[0]
"We hope our approach will instead choose the realization model that is correct for French, in which the NOUN–ADJ bigrams arise instead from source trees where the ADJ is a dependent of the NOUN, yielding (e.g.)
DET NOUN ADJ VERB PROPN the cake tasty pleased Sue
dobjdet amod nsubj
.",2.1 Realization is systematic,0,[0]
"This has the same POS sequence as the example above (as it happens), but now assigns the correct tree to it.",2.1 Realization is systematic,0,[0]
"As our family of realization distributions, we adopt the log-linear model used for this purpose by Wang and Eisner (2016).",2.2 A parametric realization model,0,[0]
"The model assumes that the root node a of the unordered dependency tree selects an ordering π(a) of the na nodes consisting
of a and its na − 1 dependent children.",2.2 A parametric realization model,0,[0]
The procedure is repeated recursively at the child nodes.,2.2 A parametric realization model,0,[0]
"This method can produce only projective trees.
",2.2 A parametric realization model,0,[0]
"Each node a draws its ordering π(a) independently according to
pθ(π | a) = 1
Z(a) exp ∑ 1≤i<j≤na θ · f(π, i, j) (1)
which is a distribution over the na! possible orderings.",2.2 A parametric realization model,0,[0]
Z(a) is a normalizing constant.,2.2 A parametric realization model,0,[0]
"f is a feature vector extracted from the ordered pair of nodes πi, πj , and θ is the model’s parameter vector of feature weights.",2.2 A parametric realization model,0,[0]
"See Appendix A for the feature templates, which are a subset of those used by Wang and Eisner (2016).",2.2 A parametric realization model,0,[0]
These features are able to examine the tree’s node labels (POS tags) and edge labels (dependency relations).,2.2 A parametric realization model,0,[0]
"Thus, when a is a verb, the model can assign a positive weight to “subject precedes verb” or “subject precedes object,” thus preferring orderings with these features.
",2.2 A parametric realization model,0,[0]
"Following Wang and Eisner (2016, §3.1), we choose new orderings for the noun and verb nodes only,2 preserving the source treebank’s order at all other nodes a.",2.2 A parametric realization model,0,[0]
"Given a source treebank B and some parameters θ, we can use equation (1) to randomly sample realizations of the trees inB.",2.3 Generating training data,0,[0]
The effect is to reorder dependent phrases within those trees.,2.3 Generating training data,0,[0]
The resulting permuted treebank B′ can be used to train a parser for the target language.,2.3 Generating training data,0,[0]
So how do we choose θ that works for the target language?,2.4 Choosing parameters θ,0,[0]
"Suppose u is a corpus of targetlanguage POS sequences, using the same set of POS tags as B. We evaluate parameters θ according to whether POS tag sequences in B′ will be distributed like POS tag sequences in u.
To do this, first we estimate a bigram language model q̂ from the actual distribution q of POS sequences observed in u. Second, let pθ denote the distribution of POS sequences that we expect to see in B′, that is, POS sequences obtained by
2Specifically, the 93% of nodes tagged with NOUN, PROPN, PRON or VERB in Universal Dependencies format.",2.4 Choosing parameters θ,0,[0]
"In retrospect, this restriction was unnecessary in our setting, but it skipped only 4.4% of nodes on average (from 2% to 11% depending on language).",2.4 Choosing parameters θ,0,[0]
"The remaining nodes were nouns, verbs, or childless.
stochastically realizing observed trees in B according to θ.",2.4 Choosing parameters θ,0,[0]
"We estimate another bigram model p̂θ from this distribution pθ.
",2.4 Choosing parameters θ,0,[0]
"We then try to set θ, using SGD, to minimize a divergence D(p̂θ, q̂) that we will define below.",2.4 Choosing parameters θ,0,[0]
"Estimating q̂ is straightforward: q̂(t | s) = cq(st)/cq(s), where cq(st) is the count of POS bigram st in the average3 sentence of u and cq(s) =∑
t′ cq(st ′).",2.4.1 Estimation of bigram models,0,[0]
"We estimate p̂θ in the same way, where cp(st) denotes the expected count of st in a random POS sequence y ∼ pθ.",2.4.1 Estimation of bigram models,0,[0]
"This is equivalent to choosing q̂, p̂θ to minimize the KL-divergences KL(q ||",2.4.1 Estimation of bigram models,0,[0]
"q̂),KL(pθ || p̂θ).",2.4.1 Estimation of bigram models,0,[0]
"It ensures that each model’s expected bigram counts match those in the POS sequences.
",2.4.1 Estimation of bigram models,0,[0]
"However, these maximum-likelihood estimates might overfit on our finite data, u and B. We therefore smooth both models by first adding λ = 0.1 to all bigram counts cq(st) and cp(st).4",2.4.1 Estimation of bigram models,0,[0]
We need a metric to evaluate θ.,2.4.2 Divergence of bigram models,0,[0]
"If p and q are bigram language models over POS sequences y (sentences), their Kullback-Leibler divergence is
KL(p || q) def=",2.4.2 Divergence of bigram models,0,[0]
"Ey∼p[log p(y)− log q(y)] (2) = ∑ s,t cp(st) (3)
· (log p(t | s)− log q(t | s))
where y ranges over POS sequences and st ranges over POS bigrams.",2.4.2 Divergence of bigram models,0,[0]
"These include bigrams where s = BOS (“beginning of sequence”) or t = EOS (“end of sequence”), which are boundary tags that we take to surround y.
All quantities in equation (3) can be determined directly from the (expected) bigram counts given by cp and cq.",2.4.2 Divergence of bigram models,0,[0]
"No other model estimation is needed.
",2.4.2 Divergence of bigram models,0,[0]
A concern about equation (3) is that a single bigram st that is badly underrepresented in q may contribute an arbitrarily large term log p(t|s)q(t|s) .,2.4.2 Divergence of bigram models,0,[0]
"To limit this contribution to at most log 1α , for some small α ∈ (0, 1), we define KLα(p || q) by a variant of equation (3) in which q(t | s) has been replaced by q̃(t | s) def= αp(t",2.4.2 Divergence of bigram models,0,[0]
"| s) + (1− α)q(t | s).5
3A more familiar definition of cq would use the total count in u. Our definition, which yields the same bigram probabilities, is analogous to our definition of cp.",2.4.2 Divergence of bigram models,0,[0]
"This cp is needed for KL(p || q) in (3), and cq symmetrically for KL(q || p).
",2.4.2 Divergence of bigram models,0,[0]
"4Ideally one should tune λ to minimize the language model perplexity on held-out data (e.g., by cross-validation).
",2.4.2 Divergence of bigram models,0,[0]
"5This is inspired by the α-skew divergence of Lee (1999,
Our final divergence metric D(p̂θ, q̂) defines D as a linear combination of exclusive and inclusive KLα divergences, which respectively emphasize pθ’s precision and recall at matching q’s bigrams:
D(p, q) =",2.4.2 Divergence of bigram models,0,[0]
(1−β)·KLα1(p || q) Ey∼p[ |y| ] +β·KLα2(q ||,2.4.2 Divergence of bigram models,0,[0]
"p) Ey∼q[ |y| ] (4) where β, α1, α2 are tuned by cross-validation to maximize the downstream parsing performance.",2.4.2 Divergence of bigram models,0,[0]
"The division by average sentence length converts KL from nats per sentence to nats per word,6 so that the KL values have comparable scale even if B has much longer or shorter sentences than u.",2.4.2 Divergence of bigram models,0,[0]
"We now present a polynomial-time algorithm for computing the expected bigram counts cp under pθ (or equivalently p̂θ), for use above.",3.1 Efficiently computing expected counts,0,[0]
"This averages expected counts from each unordered tree x ∈ B. Algorithm 1 in the supplement gives pseudocode.
",3.1 Efficiently computing expected counts,0,[0]
"The insight is that rather than sampling a single realization of x (as B′ does), we can use dynamic programming to sum efficiently over all of its exponentially many realizations.",3.1 Efficiently computing expected counts,0,[0]
This gives an exact answer.,3.1 Efficiently computing expected counts,0,[0]
"It algorithmically resembles tree-to-string machine translation, which likewise considers the possible reorderings of a source tree and incorporates a language model by similarly tracking their surface N -grams (Chiang, 2007, §5.3.2).
",3.1 Efficiently computing expected counts,0,[0]
"For each node a of the tree x, let the POS string ya be the realization of the subtree rooted at a. Let ca(st) be the expected count of bigram st in ya, whose distribution is governed by equation (1).",3.1 Efficiently computing expected counts,0,[0]
"We allow s = BOS or t = EOS as defined in §2.4.2.
",3.1 Efficiently computing expected counts,0,[0]
The ca function can be represented as a sparse map from POS bigrams to reals.,3.1 Efficiently computing expected counts,0,[0]
We compute ca at each node a of x in a bottom-up order.,3.1 Efficiently computing expected counts,0,[0]
"The final step computes croot, giving the expected bigram counts in x’s realization y",3.1 Efficiently computing expected counts,0,[0]
"(that is, cp in §2.4).
",3.1 Efficiently computing expected counts,0,[0]
We find ca as follows.,3.1 Efficiently computing expected counts,0,[0]
"Let n = na and recall from §2.2 that π(a) is an ordering of a1, . . .",3.1 Efficiently computing expected counts,0,[0]
", an, where a1, . . .",3.1 Efficiently computing expected counts,0,[0]
", an−1 are the child nodes of a, and an is a dummy node representing a’s head token.
2001).",3.1 Efficiently computing expected counts,0,[0]
"Indeed, we may regard KLα(p || q) as the α-skew divergence between the unigram distributions p(· | s) and q(· | s), averaged over all s in proportion to cp(s).",3.1 Efficiently computing expected counts,0,[0]
"In principle, we could have used the α-skew divergence between the distributions p(·) and q(·) over POS sequences y, but computing that would have required a sampling-based approximation (§7).
6Recall that the units of negated log-probability are called bits for log base 2, but nats for log base e.
Also, let a0 and an+1 be dummy nodes that always appear at the start and end of any ordering.
",3.1 Efficiently computing expected counts,0,[0]
For all 0 ≤,3.1 Efficiently computing expected counts,0,[0]
i ≤ n,3.1 Efficiently computing expected counts,0,[0]
and 1 ≤ j ≤ n,3.1 Efficiently computing expected counts,0,[0]
"+ 1, let pa(i, j) denote the expected count of the aiaj node bigram—the probability that π(a) places node ai immediately before node aj .",3.1 Efficiently computing expected counts,0,[0]
"These node bigram probabilities can be obtained by enumerating all possible orderings π, a matter we return to below.
",3.1 Efficiently computing expected counts,0,[0]
"It is now easy to compute ca:
ca(st) =",3.1 Efficiently computing expected counts,0,[0]
"c within a (st) + c between a (st) (5)
",3.1 Efficiently computing expected counts,0,[0]
"cwithina (st) =
{∑n i=1 cai(st)",3.1 Efficiently computing expected counts,0,[0]
"if s 6= BOS, t 6=",3.1 Efficiently computing expected counts,0,[0]
"EOS
0 otherwise
cacrossa (st) = n∑ i=0 n+1∑ j=1 pa(i, j)cai(s EOS)caj (BOS t)
",3.1 Efficiently computing expected counts,0,[0]
"That is, ca inherits all non-boundary bigrams st that fall within its child constituents (via cwithina ).",3.1 Efficiently computing expected counts,0,[0]
"It also counts bigrams st that cross the boundary between consecutive nodes (via cacrossa ), where nodes ai and aj are consecutive with probability pa(i, j).
",3.1 Efficiently computing expected counts,0,[0]
"When computing ca via (5), we will have already computed ca1 , . . .",3.1 Efficiently computing expected counts,0,[0]
", can−1 bottom-up.",3.1 Efficiently computing expected counts,0,[0]
"As for the dummy nodes, an is realized by the length-1 string hwhere h is the head token of node a, while a0 and an+1 are each realized by the empty string.",3.1 Efficiently computing expected counts,0,[0]
"Thus, can simply assigns count 1 to the bigrams BOS h and h EOS, and ca0 and can+1 each assign expected count 1 to BOS EOS.",3.1 Efficiently computing expected counts,0,[0]
"(Notice that thus, cacrossa (st) counts ya’s boundary bigrams—the bigrams stwhere s = BOS or t = EOS—when i = 0 or j = n+ 1 respectively.)",3.1 Efficiently computing expected counts,0,[0]
"The main challenge above is computing the node bigram probabilities pa(i, j).",3.2 Efficient enumeration over permutations,0,[0]
"These are marginals of p(π | a) as defined by (1), which unfortunately is intractable to marginalize: there is no better way than enumerating all n! permutations.
",3.2 Efficient enumeration over permutations,0,[0]
"That said, there is a particularly efficient way to enumerate the permutations.",3.2 Efficient enumeration over permutations,0,[0]
"The SteinhausJohnson-Trotter (SJT) algorithm (Sedgewick, 1977) does so in O(1) time per permutation, obtaining each permutation by applying a single swap to the previous one.",3.2 Efficient enumeration over permutations,0,[0]
Only the features that are affected by this swap need to be recomputed.,3.2 Efficient enumeration over permutations,0,[0]
"For our features (Appendix A), this cuts the runtime per permutation from O(n2) to O(n).
",3.2 Efficient enumeration over permutations,0,[0]
"Furthermore, the single swap of adjacent nodes only changes 3 bigrams (possibly including boundary bigrams).",3.2 Efficient enumeration over permutations,0,[0]
"As a result, it is possible to
obtain the marginal probabilities with O(1) additional work per permutation.",3.2 Efficient enumeration over permutations,0,[0]
"When a node bigram is destroyed, we increment its marginal probability by the total probability of permutations encountered since the node bigram was last created.",3.2 Efficient enumeration over permutations,0,[0]
This can be found as a difference of partial sums.,3.2 Efficient enumeration over permutations,0,[0]
"The final partial sum is the normalizing constant Z(a), which can be applied at the end.",3.2 Efficient enumeration over permutations,0,[0]
"Pseudocode is given in supplementary material as Algorithm 2.
",3.2 Efficient enumeration over permutations,0,[0]
"When we train the parameters θ (§2.4), we must back-propagate through the whole computation of equation (4), which depends on tag bigram counts ca(st), which depend via (5) on expected node bigram counts pa(i, j), which depend via Algorithm 2 on the permutation probabilities p(π | a), which depend via (1) on the feature weights θ.",3.2 Efficient enumeration over permutations,0,[0]
"As a further speedup, we only train on trees with number of words < 40 and maxa na ≤ 5, so na!",4.1 Pruning high-degree trees,0,[0]
≤,4.1 Pruning high-degree trees,0,[0]
120.7 We then produce the synthetic treebank B′,4.1 Pruning high-degree trees,0,[0]
(§2.3) by drawing a single realization of each tree in B for which maxa na ≤ 7.,4.1 Pruning high-degree trees,0,[0]
"This requires sampling from up to 7! = 5040 candidates per node, again using SJT.8
That is, in this paper we run exact algorithms (§3), but only on a subset of B. The subset is not necessarily representative.",4.1 Pruning high-degree trees,0,[0]
"An improvement would use importance sampling, with a proposal distribution that samples the slower trees less often during SGD but upweights them to compensate.",4.1 Pruning high-degree trees,0,[0]
"§7 suggests a future strategy that would run on all trees in B via approximate, sampling-based algorithms.",4.1 Pruning high-degree trees,0,[0]
The exact methods would remain useful for calibrating the approximation quality.,4.1 Pruning high-degree trees,0,[0]
"To minimize (4), we use the Adam variant of SGD (Kingma and Ba, 2014), with learning rate 0.01 chosen by cross-validation (§5.1).
",4.2 Minibatch estimation of cp,0,[0]
SGD requires a stochastic estimate of the gradient of the training objective.,4.2 Minibatch estimation of cp,0,[0]
"Ordinarily this is done by replacing an expectation over the entire training set with an expectation over a minibatch.
",4.2 Minibatch estimation of cp,0,[0]
"7We found that this threshold worked much better than ≤ 4 and about as well as the much slower ≤ 6.
8This pruning heuristic retains 36.1% of the trees (averaging over the 20 development treebanks (§5.1)) for training, and 66.6% for actual realization.",4.2 Minibatch estimation of cp,0,[0]
"The latter restriction follows Wang and Eisner (2016, §4.2): they too discarded trees with nodes having na ≥ 8.
",4.2 Minibatch estimation of cp,0,[0]
Equation (2) with p = p̂θ is indeed an expectation over sentences of B. It can be stochastically estimated as (3) where cp gives the expected bigram counts averaged over only the sentences in a minibatch of B. These are found using §3’s algorithms with the current θ.,4.2 Minibatch estimation of cp,0,[0]
"Unfortunately, the term log p(t | s) depends on bigram counts that should be derived from the entire corpus B in the same way.",4.2 Minibatch estimation of cp,0,[0]
Our solution is to simply reuse the minibatch estimate of cp for the latter counts.,4.2 Minibatch estimation of cp,0,[0]
"We use a large minibatch of 500 sentences from B so that this drop-in estimate does not introduce too much bias into the stochastic gradient: after all, we only need to estimate bigram statistics on 17 POS types.9
By contrast, the cq values that are used for the expectation in the second term of (4) and in log q(t | s) do not change during optimization, so we simply compute them once from all of u.",4.2 Minibatch estimation of cp,0,[0]
"Unfortunately the objective (4) is not convex, so the optimizer is sensitive to initialization (see §5.3 below for empirical discussion).",4.3 Informed initialization,0,[0]
Initializing θ = 0,4.3 Informed initialization,0,[0]
(so that p(π | a) is uniform),4.3 Informed initialization,0,[0]
gave poor results in pilot experiments.,4.3 Informed initialization,0,[0]
"Instead, we initially choose θ to be the realization parameters of the source language, as estimated from the source",4.3 Informed initialization,0,[0]
treebank B.,4.3 Informed initialization,0,[0]
"This is at least a linguistically realistic θ, although it may not be close to the target language.10
For this initial estimation, we follow Wang and Eisner (2016) and perform supervised training on B of the log-linear realization model (1), by maximizing the conditional log-likelihood of B, namely ∑ (x,t)∈B log pθ(t | x), where (x, t) are an unordered tree and its observed ordering in B. This initial objective is convex.11",4.3 Informed initialization,0,[0]
We performed a large-scale experiment requiring hundreds of thousands of CPU-hours.,5 Experiments,0,[0]
"To our knowledge, this is the largest study of parsing transfer yet attempted.
",5 Experiments,0,[0]
"9We also used the minibatch to estimate the average sentence length Ey∼p[ |y| ] in (4), although here we could have simply used all of B since this value does not change.
",5 Experiments,0,[0]
"10As an improvement, one could also try initial realization parameters for B that are estimated from treebanks of other languages.",5 Experiments,0,[0]
"Concretely, the optimizer could start by selecting a “galactic” treebank from Wang and Eisner (2016) that is already close to the target language, according to (4), and try to make it even closer.",5 Experiments,0,[0]
"We leave this to future work.
",5 Experiments,0,[0]
"11Unfortunately, we did not regularize it, which probably resulted in initializing some parameters too close to ±∞ for the optimizer to change them meaningfully.",5 Experiments,0,[0]
"As our main dataset, we use Universal Dependencies version 1.2 (Nivre et al., 2015)—a set of 37 dependency treebanks for 33 languages, with a unified POS-tag set and relation label set.
",5.1 Data and setup,0,[0]
Our evaluation metric was unnormalized attachment score (UAS) when parsing a target treebank with a parser trained on a (possibly permuted) source treebank.,5.1 Data and setup,0,[0]
"For both evaluation and training, we used only the training portion of each treebank.
",5.1 Data and setup,0,[0]
"Our parser was Yara (Rasooli and Tetreault, 2015), a fast and accurate transition-based dependency parser that can be rapidly retrained.",5.1 Data and setup,0,[0]
We modified Yara to ignore the input words and use only the input gold POS tags (see §1.3).,5.1 Data and setup,0,[0]
"To train the Yara parser on a (possibly permuted) source treebank, we first train on 80% of the trees and use the remaining 20% to tune Yara’s hyperparameters.",5.1 Data and setup,0,[0]
"We then retrain Yara on 100% of the source trees and evaluate it on the target treebank.
Similar to Wang and Eisner (2017), we use 20 treebanks (18 distinct languages) as development data, and hold out the remaining 17 treebanks for the final evaluation.",5.1 Data and setup,0,[0]
"We chose the hyperparameters (α1, α2, β) of (4) to maximize the target-language UAS, averaged over all 376 transfer experiments where the source and target treebanks were development treebanks of different languages.12 (See Appendix C for details.)
",5.1 Data and setup,0,[0]
The next few sections perform some exploratory analysis on these 376 experiments.,5.1 Data and setup,0,[0]
"Then, for the final test in §5.4, we will evaluate UAS on all 337 transfer experiments where the source is a development treebank and the target is a test treebank of a different language.13",5.1 Data and setup,0,[0]
We have assumed that a smaller divergence between source and target treebanks results in better transfer parsing accuracy.,5.2 Exploratory analysis,0,[0]
"Figure 1 shows that these quantities are indeed correlated, both for the original source treebanks and for their “made to order” permuted versions.
",5.2 Exploratory analysis,0,[0]
"12We have 19*20=380 pairs in total, minus the four excluded pairs (grc, grc proiel), (grc proiel, grc), (la proiel, la itt) and (la itt, la proiel).",5.2 Exploratory analysis,0,[0]
"Unlike Wang and Eisner (2017), we exclude duplicated languages in development and testing.
",5.2 Exploratory analysis,0,[0]
"13Specifically, there are 3 duplicated sets: {grc, grc proiel}, {la, la proiel, la itt}, and {fi, fi ftb}.",5.2 Exploratory analysis,0,[0]
"Whenever one treebank is used as the target language, we exclude the other treebanks in the same set.
",5.2 Exploratory analysis,0,[0]
"15According to the family (and sub-family) information at http://universaldependencies.org.
",5.2 Exploratory analysis,0,[0]
"Thus, we hope that the optimizer will find a systematic permutation that reduces the divergence.",5.2 Exploratory analysis,0,[0]
Does it?,5.2 Exploratory analysis,0,[0]
"Yes: Figures 5 and 6 in the supplementary material show that the optimizer almost always manages to reduce the objective on training data, as expected.
",5.2 Exploratory analysis,0,[0]
"One concern is that our divergence metric might misguide us into producing dysfunctional languages whose trees cannot be easily recovered from their surface strings, i.e., they have no good parser.",5.2 Exploratory analysis,0,[0]
"In such a language, the word order might be extremely free (e.g., θ = 0), or common constructions might be syntactically ambiguous.",5.2 Exploratory analysis,0,[0]
"Fortunately, Appendix D shows that our synthetic languages appear natural with respect to their their parsability.
",5.2 Exploratory analysis,0,[0]
The above findings are promising.,5.2 Exploratory analysis,0,[0]
So does permuting the source language in fact result in better transfer parsing of the target language?,5.2 Exploratory analysis,0,[0]
"We experiment on the 376 development pairs.
",5.2 Exploratory analysis,0,[0]
"The solid lines in Figure 2 show our improvements on the dev data, with a simpler scatterplot given by in Figure 7 in the supplementary material.",5.2 Exploratory analysis,0,[0]
The upshot is that the synthetic source treebanks yield a transfer UAS of 52.92 on average.,5.2 Exploratory analysis,0,[0]
This is not yet a result on held-out test data: recall that 52.92 was the best transfer UAS achieved by any hyperparameter setting.,5.2 Exploratory analysis,0,[0]
"That said, it is 1.00 points better than transferring from the original source treebanks, a significant difference (paired permutation test by language pair, p < 0.01).
",5.2 Exploratory analysis,0,[0]
"Figure 2 shows that this average improvement is mainly due to the many cases where the source and target languages come from different families.
",5.2 Exploratory analysis,0,[0]
Permutation tends to improve source languages that were doing badly to start with.,5.2 Exploratory analysis,0,[0]
"However, it tends to hurt a source language that is already in the target language family.
",5.2 Exploratory analysis,0,[0]
A hypothetical experiment shows that permuting the source does have good potential to help (or at least not hurt) in both cases.,5.2 Exploratory analysis,0,[0]
"The dashed lines in Figure 2—and the scatterplot in Figure 8— show the potential of the method, by showing the improvement we would get from permuting each source treebank using an “oracle” realization policy—the supervised realization parameters θ that are estimated from the actual target treebank.",5.2 Exploratory analysis,0,[0]
"The usefulness of this oracle-permuted source varies depending on the source language, but it is usually much better than the automaticallypermuted version of the same source.
",5.2 Exploratory analysis,0,[0]
This shows that large improvements would be possible if we could only find the best permutation policy allowed by our model family.,5.2 Exploratory analysis,0,[0]
"The question for future work is whether such gains can be achieved by a more sensitive permutation model than (1), a better divergence objective than (4), or a better search algorithm than §4.2.",5.2 Exploratory analysis,0,[0]
"Identifying the best available source treebank, or the best mixture of all source treebanks, would also help greatly.",5.2 Exploratory analysis,0,[0]
Figure 2 makes clear that performance of the synthetic source treebanks is strongly correlated with that of their original versions.,5.3 Sensitivity to initializer,0,[0]
Most points in Figure 7 lie near the diagonal (Kendall’s τ = 0.85).,5.3 Sensitivity to initializer,0,[0]
"Even with oracle permutation in Figure 8, the correlation remains strong (τ = 0.59), suggesting that the choice of source treebank is important even beyond its effect on search initialization.
",5.3 Sensitivity to initializer,0,[0]
"We suspected that when “made to order” source treebanks (more than the oracle versions) have performance close to their original versions, this is in part because the optimizer can get stuck near the initializer (§4.3).",5.3 Sensitivity to initializer,0,[0]
"To examine this, we experimented with random restarts, as follows.",5.3 Sensitivity to initializer,0,[0]
"In addition to informed initialization (§4.3), we optimized from 5 other starting points θ ∼ N (0, I).",5.3 Sensitivity to initializer,0,[0]
"From these 6 runs, we selected the final parameters that achieved the best divergence (4).",5.3 Sensitivity to initializer,0,[0]
"As shown by
Figure 9 in the supplement, greater gains appear to be possible with more aggressive search methods of this sort, which we leave to future work.",5.3 Sensitivity to initializer,0,[0]
"We could also try non-random restarts based on the realization parameters of other languages, as suggested in footnote 10.",5.3 Sensitivity to initializer,0,[0]
"For our final evaluation (§5.1), we use the same hyperparameters (Appendix C) and report on single-source transfer to the 17 held-out treebanks.
",5.4 Final evaluation on the test languages,0,[0]
The development results hold up in Figure 3.,5.4 Final evaluation on the test languages,0,[0]
"Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p < 0.01).
",5.4 Final evaluation on the test languages,0,[0]
"In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer.",5.4 Final evaluation on the test languages,0,[0]
"Unsupervised parsing has remained challenging for decades (Mareček, 2016).",6.1 Unsupervised parsing,0,[0]
"Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse.",6.1 Unsupervised parsing,0,[0]
Some such approaches try to improve the grammar model.,6.1 Unsupervised parsing,0,[0]
"For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012).",6.1 Unsupervised parsing,0,[0]
"Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareček and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013).
",6.1 Unsupervised parsing,0,[0]
"The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences.",6.1 Unsupervised parsing,0,[0]
McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s).,6.1 Unsupervised parsing,0,[0]
"Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013) features (Naseem et al., 2012; Täckström",6.1 Unsupervised parsing,0,[0]
"et al., 2013;
Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2).",6.1 Unsupervised parsing,0,[0]
Our novel proposal ties into the recent interest in data augmentation in supervised machine learning.,6.2 Synthetic data generation,0,[0]
"In unsupervised parsing, the most widely
adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation.",6.2 Synthetic data generation,0,[0]
"Of course, this requires bilingual corpora as an additional resource.",6.2 Synthetic data generation,0,[0]
"Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014).",6.2 Synthetic data generation,0,[0]
"Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agić et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016).
",6.2 Synthetic data generation,0,[0]
"On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages.",6.2 Synthetic data generation,0,[0]
"They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages.",6.2 Synthetic data generation,0,[0]
"Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language.",6.2 Synthetic data generation,0,[0]
"Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016), typology
prediction (Wang and Eisner, 2017), and parsing unknown languages (Wang and Eisner, 2018).
",6.2 Synthetic data generation,0,[0]
Our work in this paper differs in that our synthetic treebanks are “made to order.”,6.2 Synthetic data generation,0,[0]
"Rather than combine aspects of different treebanks and hope to get at least one combination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source.
",6.2 Synthetic data generation,0,[0]
"Beyond unsupervised parsing, synthetic data has been used for several other tasks.",6.2 Synthetic data generation,0,[0]
"In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them.",6.2 Synthetic data generation,0,[0]
"In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order.",6.2 Synthetic data generation,0,[0]
We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams).,7 Conclusion & Future Work,0,[0]
The code is available at https://github. com/wddabc/ordersynthetic.,7 Conclusion & Future Work,0,[0]
"Our work is grounded in the notion that by trying to explain the POS bigram counts in a target corpus, we can discover a stochastic realization policy for the target language, which correctly “translates” the source trees into appropriate target trees.
",7 Conclusion & Future Work,0,[0]
"We formulated an objective for evaluating such a policy, based on KL-divergence between bigram models.",7 Conclusion & Future Work,0,[0]
"We showed that the objective could be computed efficiently by dynamic programming, thanks to the limitation to bigram statistics.
",7 Conclusion & Future Work,0,[0]
"Experimenting on the Universal Dependencies treebanks v1.2, we showed that the synthetic treebanks were—on average—modestly but significantly better than the corresponding real treebanks for single-source transfer (and in Appendix E, on multi-source transfer).
",7 Conclusion & Future Work,0,[0]
"On the downside, Figure 7 shows that with our current method, permuting the source language to be more like the target language is helpful (on average) only when the source language is from a different language family.",7 Conclusion & Future Work,0,[0]
"This contrast would be
even more striking if we had a better optimizer:
Figure 9 shows that SGD’s initialization bias limits permutation’s benefit for cross-family training, as well as its harm for within-family training.
",7 Conclusion & Future Work,0,[0]
Several opportunities for future work have already been mentioned throughout the paper.,7 Conclusion & Future Work,0,[0]
"We are also interested in experimenting with richer families of permutation distributions, as well as “conservative” distributions that tend to prefer the original source order.",7 Conclusion & Future Work,0,[0]
"We could use entropy regularization (Grandvalet and Bengio, 2005) to encourage more “deterministic” patterns of realization in the synthetic languages.
",7 Conclusion & Future Work,0,[0]
"We would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (RNNLMs) for q̂ and p̂θ.",7 Conclusion & Future Work,0,[0]
"This means abandoning our exact dynamic programming methods; we would also like to abandon exact exhaustive enumeration in order to drop §4.1’s bounds on n. Fortunately, there exist powerful MCMC methods (Eisner and Tromble, 2006) that can sample from interesting distributions over the space of n!",7 Conclusion & Future Work,0,[0]
"permutations, even for large n. Thus, we could approximately sample from pθ by drawing permuted versions of each tree in B.
Given this change, a very interesting direction would be to graduate from POS language models to word language models, using cross-lingual unsupervised word embeddings (Ruder et al., 2017).",7 Conclusion & Future Work,0,[0]
This would eliminate the need for the gold POS tags that we unrealistically assumed in this paper (which are typically unavailable for a low-resource target language).,7 Conclusion & Future Work,0,[0]
"Furthermore, it would enable us to harness richer lexical information beyond the 17 UD POS tags.",7 Conclusion & Future Work,0,[0]
"After all, even a (gold) POS corpus might not be sufficient to determine the word order of the target language: “NOUN VERB NOUN” could be either subject-verb-object or object-verbsubject.",7 Conclusion & Future Work,0,[0]
"However, “water drink boy” is presumably object-verb-subject.",7 Conclusion & Future Work,0,[0]
"Thus, using crosslingual embeddings, we would try to realize the unordered source trees so that their word strings, with few edits, can achieve high probability under a neural language model of the target.",7 Conclusion & Future Work,0,[0]
This work was supported by National Science Foundation Grants 1423276 & 1718846.,Acknowledgements,0,[0]
"We are grateful to the state of Maryland for the Maryland Advanced Research Computing Center, a crucial resource.",Acknowledgements,0,[0]
"We thank Shijie Wu and Adithya Renduchintala for early discussion, Argo lab members for further discussion, and the 3 reviewers for quality comments.",Acknowledgements,0,[0]
"To approximately parse an unfamiliar language, it helps to have a treebank of a similar language.",abstractText,0,[0]
But what if the closest available treebank still has the wrong word order?,abstractText,0,[0]
We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language.,abstractText,0,[0]
The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum).,abstractText,0,[0]
This optimization procedure yields trees for a new artificial language that resembles the target language.,abstractText,0,[0]
We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.,abstractText,0,[0]
Synthetic Data Made to Order: The Case of Parsing,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–757 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
752",text,0,[0]
"Story comprehension has been one of the longestrunning ambitions in artificial intelligence (Dijk, 1980; Charniak, 1972).",1 Introduction,0,[0]
One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested.,1 Introduction,0,[0]
"Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address
*",1 Introduction,0,[0]
"This work was performed at University of Rochester.
this issue.",1 Introduction,0,[0]
"This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’",1 Introduction,0,[0]
"Then, the system’s task is to choose the right ending.",1 Introduction,0,[0]
"In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).",1 Introduction,0,[0]
"Each story follows a character through a fairly simple series of events to a conclusion.
",1 Introduction,0,[0]
"Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.",1 Introduction,0,[0]
"A variety of new systems were proposed (Mihaylov and Frank, 2017; Schenk and Chiarcos, 2017; Schwartz et al., 2017b; Roemmele et al., 2017) as a part of the first shared task on SCT at LSDSem’17 workshop (Mostafazadeh et al., 2017).",1 Introduction,0,[0]
"Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.",1 Introduction,0,[0]
"Clearly, this success does not seem to reflect the intent of the original task, where the systems should leverage narrative understanding as opposed to the statistical biases in the data.",1 Introduction,0,[0]
"In this paper, we study the effect of such biases between the ending choices and present a new scheme to reduce such stylistic artifacts.
",1 Introduction,0,[0]
"The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety
of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).",1 Introduction,0,[0]
"The results show that the topperforming SCT system on the the leaderboard1 (Chaturvedi et al., 2017) fails to keep up the performance on our new dataset.",1 Introduction,0,[0]
We discuss the implications of this experiment to the greater research community in terms of data collection and benchmarking practices in Section 6.,1 Introduction,0,[0]
All the code and datasets for this paper will be released to the public.,1 Introduction,0,[0]
We hope that the availability of the new evaluation set can further support the continued research on story understanding.,1 Introduction,0,[0]
"This paper mainly extends the work on creating the Story Cloze Test set (Mostafazadeh et al., 2016), hereinafter SCT-v1.0.",2 Related Work,0,[0]
"The SCT-v1.0 dataset was created as follows: full five-sentence stories from the ROC Stories corpus were sampled, then, the initial four sentences were shown to a set of MTurk2 crowd workers who were prompted to author ‘right’ and ‘wrong’ endings.",2 Related Work,0,[0]
"Mostafazadeh et al. (Mostafazadeh et al., 2016) give special care to make sure there were no boundary cases for ‘right’ and ‘wrong’ endings by implementing extra rounds of data filtering.",2 Related Work,0,[0]
"The resulting SCT-v1.0 dataset had a validation (hereinafter, SCT-v1.0 Val) and a test set (SCT-v1.0 test), each with 1,871 cases.",2 Related Work,0,[0]
Table 1 shows two example story cloze test cases from SCT-v1.0 corpus.,2 Related Work,0,[0]
"As for positive training data, they had provided a collection of 100K five sentence stories.",2 Related Work,0,[0]
"Human performance is reported to be 100% on SCT-v1.0.
",2 Related Work,0,[0]
"Mostafazadeh et al. (2016) provide a variety of baseline models for SCT-v1.0, with the best model performing with an accuracy of 59%.",2 Related Work,0,[0]
"The first
1As of 15th February 2018.",2 Related Work,0,[0]
"2http://mturk.com
shared task on SCT-v1.0 was conducted at the LSDSem’17 workshop (Mostafazadeh et al., 2017), where most of the models performed with 60- 70% accuracy.",2 Related Work,0,[0]
"One of the top-performing models, msap (Schwartz et al., 2017b,a), built a classifier using linguistic features that have been previously useful in authorship style detection, using only the ending sentences.",2 Related Work,0,[0]
"They used stylistic features such as sentence length, word, and character level n-grams for each ending (fully discarding the context), achieving an accuracy of 72%.",2 Related Work,0,[0]
"In conjunction with their work, Cai et al., (Cai et al., 2017) reported similar observations separately, exposing that features such as sentiment, negation, and length are different between the right and wrong endings.",2 Related Work,0,[0]
"The best model on SCT-v1.0 to this date is cogcomp, which is a linear model that uses event sequences, sentiment trajectory, and topical consistency as features, and performs with an accuracy of 77.6%.
",2 Related Work,0,[0]
This paper takes all their analysis further and introduces a model aggregating all the pinpointed features to shed more light into the stylistic biases isolated in SCT-v1.0 endings.,2 Related Work,0,[0]
"Despite all the efforts made in the original SCT paper, there was never an extensive analysis of the features isolated in the endings of the stories.",3 Stylistic Feature Analysis,0,[0]
"We explored the differences among stylistic features such as word-token count, sentiment, and the sentence complexity between the endings, to determine a composite score for identifying sources of bias.",3 Stylistic Feature Analysis,0,[0]
"For determining the sentiment, we used Stanford CoreNLP",3 Stylistic Feature Analysis,0,[0]
"(Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).",3 Stylistic Feature Analysis,1,"['(See also (Shekhar & Javidi, 2017) for recent improvements over (Srinivas et al., 2010) under the Matérn kernel in higher dimensions and/or with smaller ν).']"
"For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).",3 Stylistic Feature Analysis,0,[0]
Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.,3 Stylistic Feature Analysis,0,[0]
"The feature distribution plots can be found in the supplementary material.
",3 Stylistic Feature Analysis,0,[0]
"Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b))",3 Stylistic Feature Analysis,0,[0]
as features.,3 Stylistic Feature Analysis,0,[0]
"We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation.",3 Stylistic Feature Analysis,0,[0]
"In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.",3 Stylistic Feature Analysis,0,[0]
"Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one.",3 Stylistic Feature Analysis,0,[0]
"An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material.",3 Stylistic Feature Analysis,0,[0]
"Following the analysis above, we developed a Story Cloze model, hereinafter EndingReg, that only uses the ending features while disregarding the story context for choosing the right ending.",4 Model,0,[0]
We expanded each Story Cloze Test case’s ending options into a set of two single sentences.,4 Model,0,[0]
"Then, for each sentence, we created the following features:
1.",4 Model,0,[0]
Number of tokens 2.,4 Model,0,[0]
VADER composite sentiment score 3.,4 Model,0,[0]
Yngve complexity score 4.,4 Model,0,[0]
Token-POS n-grams 5.,4 Model,0,[0]
POS n,4 Model,0,[0]
-grams 6.,4 Model,0,[0]
"Four length character-grams
All n-gram features needed to appear at least five times throughout the dataset.",4 Model,0,[0]
The features were collected for each five-sentence story and then fed into a logistic regression classifier.,4 Model,0,[0]
"As an initial experiment, we trained this model using the SCTv1.0 validation set and tested on the SCT-v1.0 test set.",4 Model,0,[0]
"An L2 regularization penalty was used to enforce a Gaussian prior on the feature-space, where a grid search was conducted for hyper-parameter tuning.",4 Model,0,[0]
This model achieves an accuracy of 71.5% on the SCT-v1.0 dataset which is on par with the highest score achieved by any model using only the endings.,4 Model,0,[0]
"Table 3 shows the accuracies ob-
tained by models using only those particular features.",4 Model,0,[0]
"We achieve minimal but sometimes important classification using token count, VADER, and Yngve in combination alone, better classification using POS or char-grams alone, and best classification using n-grams alone.",4 Model,0,[0]
By combining all of them we achieve the overall best results.,4 Model,0,[0]
"Based on the findings above, a new test set for the SCT was deemed necessary.",5 Data Collection,0,[0]
"The premise of predicting an ending to a short story, as opposed to predicting say a middle sentence, enables a more systematic evaluation where human can agree on the cases 100%.",5 Data Collection,0,[0]
"Hence, our goal was to come up with a data collection scheme that overcomes the data collection biases, while keeping the original evaluation format.",5 Data Collection,0,[0]
"As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.",5 Data Collection,0,[0]
We set the following goals for sourcing the new ‘right’ and ‘wrong’ endings.,5 Data Collection,0,[0]
"They both should:
1.",5 Data Collection,0,[0]
Contain a similar number of tokens 2.,5 Data Collection,0,[0]
"Have similar distributions of token n-grams
and char-grams 3.",5 Data Collection,0,[0]
"Occur as standalone events with the same
likelihood to occur, with topical, sentimental, or emotion consistencies when applicable.
",5 Data Collection,0,[0]
"First, we crowdsourced 5,000 new five-sentence stories through Amazon Mechanical Turk.",5 Data Collection,0,[0]
We prompted the users in the same manner described in Mostafazadeh et al. (2016).,5 Data Collection,0,[0]
"In order to source new ‘wrong’ endings, we tried two different methods.",5 Data Collection,0,[0]
"In Method #1, we kept the original ending sourcing format of Mostafazadeh et al., but imposed some further restrictions.",5 Data Collection,0,[0]
"This was done
by taking the first four sentences of the newly collected stories and asking an MTurker to write a ‘right’ and ‘wrong’ ending for each.",5 Data Collection,0,[0]
"The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’",5 Data Collection,0,[0]
"The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.
",5 Data Collection,0,[0]
"In Method #2, we took the five sentences stories and prompted a second set of MTurk workers to modify the fifth sentence in order to make a resulting five-sentence story non-sensible.",5 Data Collection,0,[0]
"Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.",5 Data Collection,0,[0]
"As a result, the workers had much less flexibility for changing the underlying linguistic structures which can help tackle the authorship style differences between the ‘right’ and ‘wrong’ endings.
",5 Data Collection,0,[0]
"The results in Table 4, which show classification accuracy when using EndingReg on the two new data sources, show that Method #2 is a slightly better data sourcing scheme in reducing the bias, since the EndingReg model’s performance is slightly worse.",5 Data Collection,0,[0]
The set was further filtered through human verification similar to Mostafazadeh et al. (2016).,5 Data Collection,0,[0]
"The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.",5 Data Collection,0,[0]
Stories were only selected if all the three MTurk users verified that the story with the ‘right ending’ and the corresponding story with the ‘wrong ending’ were verified to be indeed right and wrong respectively.,5 Data Collection,0,[0]
This ensured a higher quality of data and eliminating boundary cases.,5 Data Collection,0,[0]
"This entire process resulted in creating the Story Cloze Test v1.5 (SCT-v1.5) dataset, consisting of 1,571 stories for each validation and test sets.",5 Data Collection,0,[0]
"In order to test the decrease in n-gram bias, which was the most salient feature for the classification task using only the endings, we compare the variance between the n-gram counts from SCT-v1.0 to SCT-v1.5.",6 Results,0,[0]
"The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.",6 Results,0,[0]
Table 6 shows the classification results of various models on SCT-v1.5.,6 Results,0,[0]
"The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.
",6 Results,0,[0]
"Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.",6 Results,0,[0]
"Furthermore, the CogComp model’s performance has significantly dropped on SCT-v1.5.",6 Results,0,[0]
"Although this model seems to be using story comprehension features such as event sequencing, since the endings are included in the sequences, the biases within the endings have influenced the predictions and the weak performance of the model in SCT-v1.5 suggest that this model had picked up on the biases of SCT-v1.0 as opposed to really understanding the context.",6 Results,0,[0]
"In particular, the posterior probabilities for each ending choice using their features are quite similar on the SCT-v1.5.",6 Results,0,[0]
"These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.",6 Results,0,[0]
"Al-
though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.
",6 Results,0,[0]
It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.,6 Results,0,[0]
"This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.",6 Results,0,[0]
"This showcases the importance of maintaining benchmarks that evolve and improve over time, where systems should not be optimized for particular narrow test sets.",6 Results,0,[0]
"We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.",6 Results,0,[0]
"In this paper, we presented a comprehensive analysis of the stylistic features isolated in the endings of the original Story Cloze Test (SCT-v1.0).",7 Conclusion,0,[0]
"Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.",7 Conclusion,0,[0]
"Based on the results presented in this paper, we believe that our SCT-v1.5 is a better benchmark for story comprehension.",7 Conclusion,0,[0]
"However, as shown in multiple AI tasks (Ettinger et al., 2017; Antol et al., 2015; Jabri et al., 2016; Poliak et al., 2018), no collected dataset is entirely without its inherent biases and often the biases in datasets go undiscovered.",7 Conclusion,0,[0]
We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.,7 Conclusion,0,[0]
"All the new versions, along with a leader-board showcasing the stateof-the-art results, will be tracked via CodaLab
https://competitions.codalab.org/ competitions/15333.
",7 Conclusion,0,[0]
The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.,7 Conclusion,0,[0]
"We suggest the next SCT challenges to be completely blind, where the participants cannot deliberately leverage any particular data biases.",7 Conclusion,0,[0]
"Along with this paper, we are releasing the datasets and the developed models to the community.",7 Conclusion,0,[0]
"All the announcements, new supplementary material, and datasets can be accessed through http://cs.",7 Conclusion,0,[0]
rochester.edu/nlp/rocstories/.,7 Conclusion,0,[0]
We hope that this work ignites further interest in the community for making progress on story understanding.,7 Conclusion,0,[0]
We would like to thank Roy Schwartz for his valuable feedback regarding some of the experiments.,Acknowledgement,0,[0]
"We also thank the amazing crowd workers, without the work of whom this work would have been impossible.",Acknowledgement,0,[0]
This work was supported in part by grant W911NF15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA) as a part of the Communicating with Computers (CwC) program.,Acknowledgement,0,[0]
The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning.,abstractText,0,[0]
There have been a variety of models tackling the SCT so far.,abstractText,0,[0]
"Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset.",abstractText,0,[0]
"In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task.",abstractText,0,[0]
"Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases.",abstractText,0,[0]
We benchmark a few models on the new dataset and show that the topperforming model on the original SCT dataset fails to keep up its performance.,abstractText,0,[0]
Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.,abstractText,0,[0]
Tackling the Story Ending Biases in The Story Cloze Test,title,0,[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2026–2031, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,0,[0]
Online discussion forums are a popular platform for people to share their views about current events and learn about issues of concern to them.,1 Introduction,0,[0]
"Discussion forums tend to specialize on different topics, and people participating in them form communities of interest.",1 Introduction,0,[0]
The reaction of people within a community to comments posted provides an indication of community endorsement of opinions and value of information.,1 Introduction,0,[0]
"In most discussions, the vast majority of comments spawn little reaction.",1 Introduction,0,[0]
"In this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post.
",1 Introduction,0,[0]
"Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect).",1 Introduction,0,[0]
"Judging by differences in popularity of various discussion forums, topic is clearly important.",1 Introduction,0,[0]
"Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014;
Tan et al., 2014).",1 Introduction,0,[0]
"Teasing these different factors apart, however, is a challenge.",1 Introduction,0,[0]
The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest.,1 Introduction,0,[0]
"Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion.
",1 Introduction,0,[0]
"The primary contributions of this work include findings about the role of author reputation and variation across communities in terms of aspects of language use that matter, as well as the problem formulation, associated data collection, and development of a variety of features for characterizing informativeness, community response, relevance and mood.",1 Introduction,0,[0]
"Reddit1 is the largest public online discussion forum with a wide variety of subreddits, which makes it a good data source for studying how textual content in a discussion impacts the response of the crowd.",2 Data,0,[0]
"On Reddit, people initiate a discussion thread with a post (a question, a link to a news item, etc.), and others respond with comments.",2 Data,0,[0]
Registered users vote on which posts and comments are important.,2 Data,0,[0]
"The total amount of up votes minus the down votes (roughly) is called karma; it provides an indication of community endorsement and popularity of a comment, as used in (Lakkaraju et al., 2013).",2 Data,0,[0]
"Karma is valued as it impacts the order in which the posts or comments are displayed, with the high karma content rising to the top.",2 Data,0,[0]
"Karma points are also accumulated by members of the discussion forum as a function of the karma associated with their comments.
",2 Data,0,[0]
"1http://www.reddit.com
2026
The Reddit data is highly skewed.",2 Data,0,[0]
"Although there are thousands of active communities, only a handful of them are large.",2 Data,0,[0]
"Similarly, out of the more than a million comments made per day2, most of them receive little to no attention; the distributions of positive comment karma and author karma are Zipfian.",2 Data,0,[0]
"Slightly more than half of all comments have exactly one karma point (no votes beyond the author), and only 5% of comments have less than one karma point.
",2 Data,0,[0]
"For this study, we downloaded all the posts and associated comments made to six subreddits over a few weeks, as summarized in Table 1, as well as karma of participants in the discussion3.",2 Data,0,[0]
All available comments on each post were downloaded at least 48 hours after the post was made.4,2 Data,0,[0]
"Factors other than the language use that influence whether a comment will have uptake from the community include the topic, the timing of the message, and the messenger.",3 Uptake Factors,0,[0]
These factors are all evident in the Reddit discussions.,3 Uptake Factors,0,[0]
"Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic.",3 Uptake Factors,0,[0]
"Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses.
",3 Uptake Factors,0,[0]
"Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004).",3 Uptake Factors,0,[0]
"On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments.",3 Uptake Factors,0,[0]
"In order to see how important per-
2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours.",3 Uptake Factors,0,[0]
"Some comments are deleted before we are able to download them.
",3 Uptake Factors,0,[0]
"sonal reputation is, we looked at how often the top karma comments are associated with the top karma participants in the discussion.",3 Uptake Factors,0,[0]
"Since an individual’s karma can be skewed by a few very popular posts, we measure reputation instead using a measure we call the k-index, defined to be equal to the number of comments in each user’s history that have karma ≥ k.",3 Uptake Factors,0,[0]
"The k-index is analgous to the h-index (Hirsch, 2005) and arguably a better indicator of extended impact than total karma.
",3 Uptake Factors,0,[0]
The results in Table 2 address the question of whether the top karma comments always come from the top karma person.,3 Uptake Factors,0,[0]
The Top1 column shows the percentage of threads where the top karma comment in a discussion happens to be made by the highest k-index person participating in the discussion; the next column shows the percentage of threads where the comment comes from any one of the top 3 k-index people.,3 Uptake Factors,0,[0]
"We find that, in fact, the highest karma comment in a discussion is rarely from the highest k-index people.",3 Uptake Factors,0,[0]
"The highest percentage is in ASKSCIENCE, where expertise is more highly valued.",3 Uptake Factors,0,[0]
"If we consider whether any one of the multiple comments that the top k-index person made is the top karma comment in the discussion, then the frequency is even lower.",3 Uptake Factors,0,[0]
"Having shown that the reputation of the author of a post is not a dominating factor in predicting high karma comments, we propose to control for topic and timing by ranking a set of 10 comments that were made consecutively in a short window of time within one discussion thread according to the karma they finally received.",4.1 Tasks,0,[0]
The ranking has access to the comment history about these posts.,4.1 Tasks,0,[0]
"This simulates the view of an early reader of these posts, i.e., without influence of the ratings of oth-
ers, so that the language content of the post is more likely to have an impact.",4.1 Tasks,0,[0]
"Very long threads are sampled, so that these do not dominate the set of lists.",4.1 Tasks,0,[0]
"Approximately 75% of the comment lists are designated for training and the rest is for testing, with splits at the discussion thread level.",4.1 Tasks,0,[0]
"Here, feature selection is based on mean precision of the top-ranked comment (P@1), so as to emphasize learning the rare high karma events.",4.1 Tasks,0,[0]
(Note that P@1 is equivalent to accuracy but allows for any top-ranking comment to count as correct in the case of ties.),4.1 Tasks,0,[0]
"The system performance is evaluated using both P@1 and normalized discounted cumulative gain (NDCG) (Burges et al., 2005), which is a standard criterion for ranking evaluation when the samples to be ranked have meaningful differences in scores, as is the case for karma of the comments.
",4.1 Tasks,0,[0]
"In addition, for analysis purposes, we report results for three surrogate tasks that can be used in the ranking problem: i) the binary ranker trained on all comment pairs within each list, in which low karma comments dominate, ii) a positive vs. negative karma classifier, and iii) a high vs. medium karma classifier.",4.1 Tasks,0,[0]
"All use class-balanced data; the second two are trained and tested on a biased sampling of the data, where the pairs need not be from the same discussion thread.",4.1 Tasks,0,[0]
"We use the support vector machine (SVM) rank algorithm (Joachims, 2002) to predict a rank order for each list of comments.",4.2 Classifier,0,[0]
The SVM is trained to predict which of a pair of comments has higher karma.,4.2 Classifier,0,[0]
"The error term penalty parameter is tuned to maximize P@1 on a held-out validation set (20% of the training samples).
",4.2 Classifier,0,[0]
"Since much of the data includes low-karma comments, there will be a tendancy for the learning to emphasize features that discriminate comments at the lower end of the scale.",4.2 Classifier,0,[0]
"In order to learn features that improve P@1, and to understand the relative importance of different features, we use a greedy automatic feature selection process that incrementally adds one feature whose resulting feature set achives the highest P@1 on the validation set.",4.2 Classifier,0,[0]
"Once all features have been used, we select the model with the subset of features that obtains the best P@1 on the validation set.",4.2 Classifier,0,[0]
The features are designed to capture several key attributes that we hypothesize are predictive of comment karma motivated by related work.,4.3 Features,0,[0]
"The features are categorized in groups as summarized below, with details in supplementary material.",4.3 Features,0,[0]
"• Graph and Timing (G&T): A baseline that
captures discourse history (response structure) and comment timing, but no text content.",4.3 Features,0,[0]
"• Authority and Reputation (A&R): K-index,
whether the commenter was the original poster, and in some subreddits “flair” (display next to a comment author’s username that is subject to a cursory verification by moderators).",4.3 Features,0,[0]
•,4.3 Features,0,[0]
Informativeness (Info.):,4.3 Features,0,[0]
"Different indicators
suggestive of informative content and novelty, including various word counts, named entity counts, urls, and unseen n-grams.",4.3 Features,0,[0]
• Lexical Unigrams (Lex.):,4.3 Features,0,[0]
"Miscellaneous word
class indicators, puncutation, and part-ofspeech counts • Predicted Community Response (Resp.):
",4.3 Features,0,[0]
"Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors.",4.3 Features,0,[0]
• Relevance (Rel.):,4.3 Features,0,[0]
"Comment similarity to the
parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii)",4.3 Features,0,[0]
"the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000).",4.3 Features,0,[0]
•,4.3 Features,0,[0]
Mood: Mean and std.,4.3 Features,0,[0]
"deviation of sentence sen-
timent in the comment; word list indicators for politeness, argumentativeness and profanity.",4.3 Features,0,[0]
• Community Style (Comm.):,4.3 Features,0,[0]
"Posterior proba-
bility of each subreddit given the comment using a bag-of-words model.",4.3 Features,0,[0]
The various word lists are motivated by feature exploration studies in surrogate tasks.,4.3 Features,0,[0]
"For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-oriented pronouns were more likely to have no response and secondperson pronouns were more likely to have a negative response.",4.3 Features,0,[0]
"The politeness and argumentativeness/profanity lists are generated by starting with hand-specified seed lists used to train an SVM to classify word embeddings (Mikolov et al., 2013)
into these categories, and expanding the lists with 500 words farthest from the decision boundary.
",4.3 Features,0,[0]
"Both the NMF and the skip-gram topic models use a cosine distance to determine topic similarity, with 300 as the word embedding dimension.",4.3 Features,0,[0]
Both are trained on approximately 2 million comments in high karma posts taken across a wide variety of subreddits.,4.3 Features,0,[0]
"We use topic models in various measures of comment relevance to the discussion, but we do not use topic of the comment on its own since topic is controlled for by ranking within a thread.",4.3 Features,0,[0]
"We present three sets of experiments on comment karma ranking, all of which show very different behavior for the different subreddits.",5 Ranking Experiments,0,[0]
Fig. 1 shows the relative gain in P@1 over the G&T baseline associated with using different feature groups.,5 Ranking Experiments,0,[0]
The importance of the different features reflect the nature of the different communities.,5 Ranking Experiments,1,['One of the most attractive properties of BO is its efficiency in terms of the number of function samples used.']
"The authority/reputation features help most for ASKSCIENCE, consistent with our k-index study.",5 Ranking Experiments,0,[0]
Informativeness and relevance help all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,0,[0]
"Lexical, mood and community style features are useful in some cases, but hurt others.",5 Ranking Experiments,0,[0]
"The predicted probability of a reply was least useful, possibly because of the low-karma training bias.
",5 Ranking Experiments,0,[0]
Tables 3 and 4 summarize the results for the P@1 and NDCG criteria using the greedy selection procedure (which optimizes P@1) compared to a random baseline and the G&T baseline.,5 Ranking Experiments,0,[0]
The random baseline for P@1 is greater than 10% because of ties.,5 Ranking Experiments,0,[0]
"The G&T baseline results show that the graph and timing features alone obtain 21-32%
of top karma comments depending on subreddits.",5 Ranking Experiments,0,[0]
Adding the textual features gives an improvement in P@1 performance over the G&T baseline for all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,0,[0]
"The trends for performance measured with NDCG are similar, but the benefit from textual features is smaller.",5 Ranking Experiments,0,[0]
"The results in both tables show different ways of reporting performance of the same system, but the system has been optimized for P@1 in terms of feature selection.",5 Ranking Experiments,0,[0]
"In initial exploratory experiments, this seems to have a small impact: when optimizing for NDCG in feature selection we obtain 0.61 vs. 0.60 with the P@1-optimized features.
",5 Ranking Experiments,0,[0]
"A major challenge with identifying high karma comments (and negative karma comments) is that
they are so rare.",5 Ranking Experiments,0,[0]
"Although our feature selection tunes for high rank precision, it is possible that the low-karma data dominate the learning.",5 Ranking Experiments,0,[0]
"Alternatively, it may be that language cues are mainly useful for identifying distinguishing the negative or mid-level karma comments, and that the very high karma comments are a matter of timing.",5 Ranking Experiments,0,[0]
"To better understand the role of language for these different types, we trained classifiers on balanced data for positive vs. negative karma and high vs. mid levels of karma.",5 Ranking Experiments,0,[0]
"For these models, the training pairs could come from different threads, but topic is controlled for in that all topic features are relative (similarity to original post, parent, etc.).",5 Ranking Experiments,0,[0]
"We compared the results to the binary classifier used in ranking, where all pairs are considered.",5 Ranking Experiments,0,[0]
"In all three cases, random chance accuracy is 50%.
",5 Ranking Experiments,0,[0]
Table 5 shows the pairwise accuracy of these classifiers.,5 Ranking Experiments,0,[0]
"We find that distinguishing positive from negative classes is fairly easy, with the notable exception of the more information-oriented subreddit ASKSCIENCE.",5 Ranking Experiments,0,[0]
"Averaging across the different subreddits, the high vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial.",5 Ranking Experiments,0,[0]
"The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS.",5 Ranking Experiments,0,[0]
"Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets.",6 Related Work,0,[0]
Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes.,6 Related Work,0,[0]
Cheng et al. (2014) explore prediction of information cascades on Facebook.,6 Related Work,0,[0]
"Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis.",6 Related Work,0,[0]
"Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included.
",6 Related Work,0,[0]
Most relevant to this paper are studies of the effect of language in popularity predictions.,6 Related Work,0,[0]
"Tan et al. (2014) study how word choice affects the pop-
ularity of Twitter messages.",6 Related Work,0,[0]
"As in our work, they control for topic, but they also control for the popularity of the message authors.",6 Related Work,0,[0]
"On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous.",6 Related Work,0,[0]
Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit.,6 Related Work,0,[0]
They control for content by only making comparisons between reposts of the same image.,6 Related Work,0,[0]
"Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majority of language features.",6 Related Work,0,[0]
Althoff et al. (2014) use deeper language analysis on Reddit to predict the success of receiving a pizza in the Random Acts of Pizza subreddit.,6 Related Work,0,[0]
"To our knowledge, this is the first work on ranking comments in terms of community endorsement.",6 Related Work,0,[0]
This paper addresses the problem of how language affects the reaction of community in Reddit comments.,7 Conclusion,0,[0]
We collect a new dataset of six subredit discussion forums.,7 Conclusion,0,[0]
"We introduce a new task of ranking comments based on karma in Reddit discussions, which controls for topic and timing of comments.",7 Conclusion,0,[0]
Our results show that using language features improve the comment ranking task in most of the subreddits.,7 Conclusion,0,[0]
"Informativeness and relevance are the most broadly useful feature categories; reputation matters for ASKSCIENCE, and other categories could either help or hurt depending on the community.",7 Conclusion,0,[0]
Future work involves improving the classification algorithm by using new approaches to learning about rare events.,7 Conclusion,0,[0]
"This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger.",abstractText,0,[0]
"A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments.",abstractText,0,[0]
Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.,abstractText,0,[0]
Talking to the crowd: What do people react to in online discussions?,title,0,[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 896–905 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1083
Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).
Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated
topics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.
The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.
The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.
For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.
896",text,0,[0]
"The anchor algorithm computes the topic matrix A, where Av,k is the conditional probability of observing word v given topic k, e.g., the probability of seeing the word “lens” given the camera topic in a corpus of Amazon product reviews.",1 Vanilla Anchor Algorithm,0,[0]
Arora et al. (2012a) find these probabilities by assuming that every topic contains at least one ‘anchor’ word which has a non-zero probability only in that topic.,1 Vanilla Anchor Algorithm,0,[0]
"Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.
",1 Vanilla Anchor Algorithm,0,[0]
"To recover the topic matrix A using anchor words, we first compute a V × V cooccurrence matrix Q, where Qi,j is the conditional probability p(wj |wi) of seeing word type wj after having seen wi in the same document.",1 Vanilla Anchor Algorithm,0,[0]
A form of the Gram-Schmidt process on Q finds anchor words {g1 . . .,1 Vanilla Anchor Algorithm,0,[0]
"gk} (Arora et al., 2013).
",1 Vanilla Anchor Algorithm,0,[0]
"Once we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A).",1 Vanilla Anchor Algorithm,0,[0]
"This coefficient matrix C is defined row-wise for each word i
C∗i,· = argmin Ci,· DKL
( Qi,· ∥∥∥∥",1 Vanilla Anchor Algorithm,0,[0]
"K∑
k=1
Ci,kQgk,·
) ,
(1) which gives the best reconstruction (based on Kullback-Leibler divergence DKL) of non-anchor words given anchor words’ conditional probabilities.",1 Vanilla Anchor Algorithm,0,[0]
"For example, in our product review data, a word such as “battery” is a convex combination of the anchor words’ contexts (Qgk,·) such as “camera”, “phone”, and “car”.",1 Vanilla Anchor Algorithm,0,[0]
Solving each row of C is fast and is embarrassingly parallel.,1 Vanilla Anchor Algorithm,0,[0]
"Finally, we apply Bayes’ rule to recover the topic matrix A from the coefficient matrix",1 Vanilla Anchor Algorithm,0,[0]
"C.
The anchor algorithm can be orders of magnitude faster than probabilistic inference (Arora et al., 2013).",1 Vanilla Anchor Algorithm,0,[0]
The construction of Q has a runtime of O(DN2) where D is the number of documents and N is the average number of tokens per document.,1 Vanilla Anchor Algorithm,0,[0]
This computation requires only a single pass over the data and can be pre-computed for interactive use-cases.,1 Vanilla Anchor Algorithm,0,[0]
"Once Q is constructed, topic recovery requires O(KV 2 +K2V I), where K is the number of topics, V is the vocabulary size, and I is the average number of iterations (typically 100-1000).",1 Vanilla Anchor Algorithm,0,[0]
"In contrast, traditional topic
Anchor Top Words in Topics backpack backpack camera lens bag room carry fit cameras equipment comfortable camera camera lens pictures canon digital lenses batteries filter mm photos bag bag camera diaper lens bags genie smell
room diapers odor
Table 1: Three separate attempts to construct a topic concerning camera bags in Amazon product reviews with single word anchors.",1 Vanilla Anchor Algorithm,0,[0]
This example is drawn from preliminary experiments with an author as the user.,1 Vanilla Anchor Algorithm,0,[0]
The term “backpack” is a good anchor because it uniquely identifies the topic.,1 Vanilla Anchor Algorithm,0,[0]
"However, both “camera” and “bag” are poor anchors for this topic.
model inference typically requires multiple passes over the entire data.",1 Vanilla Anchor Algorithm,0,[0]
"Techniques such as Online LDA (Hoffman et al., 2010) or Stochastic Variation Inference (Hoffman et al., 2013) improves this to a single pass over the entire data.",1 Vanilla Anchor Algorithm,0,[0]
"However, from Heaps’ law (Heaps, 1978)",1 Vanilla Anchor Algorithm,0,[0]
"it follows that V 2 DN for large datasets, leading to much faster inference times for anchor methods compared to probabilistic topic modeling.",1 Vanilla Anchor Algorithm,0,[0]
"Further, even if online were to be adapted to incorporate human guidance, a single pass is not tractable for interactive use.",1 Vanilla Anchor Algorithm,0,[0]
Single word anchors can be opaque to users.,2 Tandem Anchor Extension,0,[0]
"For an example of bewildering anchor words, consider a camera bag topic from a collection of Amazon product reviews (Table 1).",2 Tandem Anchor Extension,0,[0]
The anchor word “backpack” may seem strange.,2 Tandem Anchor Extension,0,[0]
"However, this dataset contains nothing about regular backpacks; thus, “backpack” is unique to camera bags.",2 Tandem Anchor Extension,0,[0]
"Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic.
",2 Tandem Anchor Extension,0,[0]
The anchor selection strategy can mitigate this problem to some degree.,2 Tandem Anchor Extension,0,[0]
"For example, rather than selecting anchors using an approximate convex hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embedding (Lee and Mimno, 2014).",2 Tandem Anchor Extension,0,[0]
"This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling.
",2 Tandem Anchor Extension,0,[0]
"If we instead ask users to give us representative
words for this topic, we would expect combinations of words like “camera” and “bag.”",2 Tandem Anchor Extension,0,[0]
"However, with single word anchors we must choose a single word to anchor each topic.",2 Tandem Anchor Extension,0,[0]
"Unfortunately, because these words might appear in multiple topics, individually they are not suitable as anchor words.",2 Tandem Anchor Extension,0,[0]
"The anchor word “camera” generates a general camera topic instead of camera bags, and the topic anchored by “bag” includes bags for diaper pails (Table 1).
",2 Tandem Anchor Extension,0,[0]
"Instead, we need to use sets of representative terms as an interpretable, parsimonious description of a topic.",2 Tandem Anchor Extension,0,[0]
This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics.,2 Tandem Anchor Extension,0,[0]
This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings.,2 Tandem Anchor Extension,0,[0]
We first need to turn words into an anchor.,2.1 Anchor Facets,0,[0]
"If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V -dimensional space.",2.1 Anchor Facets,0,[0]
We then model each point as a convex combination of anchor words to reconstruct the topic matrix A (Equation 1).,2.1 Anchor Facets,0,[0]
"Instead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic.",2.1 Anchor Facets,0,[0]
"The facets for each anchor form a new pseudoword, or an invented point in V -dimensional space (described in more detail in Section 2.2).
",2.1 Anchor Facets,0,[0]
"While these new points do not correspond to words in the vocabulary, we can express nonanchor words as convex combinations of pseudowords.",2.1 Anchor Facets,0,[0]
"To construct these pseudowords from their facets, we combine the co-occurrence profiles of the facets.",2.1 Anchor Facets,0,[0]
These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors.,2.1 Anchor Facets,0,[0]
"We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified.
",2.1 Anchor Facets,0,[0]
Our augmented matrix S is therefore a (V + K) × V matrix.,2.1 Anchor Facets,0,[0]
"As before, V is the number of token types in the data and K is the number of topics.",2.1 Anchor Facets,0,[0]
"The first V rows of S correspond to the V token types observed in the data, while the additionalK rows correspond to the pseudowords constructed from anchor facets.",2.1 Anchor Facets,0,[0]
"Each entry of S en-
codes conditional probabilities so that Si,j is equal to p(wi |wj).",2.1 Anchor Facets,0,[0]
"For the additionalK rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities.
",2.1 Anchor Facets,0,[0]
"This modification is similar in spirit to supervised anchor words (Nguyen et al., 2015).",2.1 Anchor Facets,0,[0]
This supervised extension of the anchor words algorithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word.,2.1 Anchor Facets,0,[0]
"By extending the vector-space representation of each word, anchor words corresponding to metadata values can be found.",2.1 Anchor Facets,0,[0]
"In contrast, our extension does not add dimensions to the representation, but simply places additional points corresponding to pseudoword words in the vectorspace representation.",2.1 Anchor Facets,0,[0]
We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor.,2.2 Combining Facets into Pseudowords,0,[0]
"In tandem anchors, we create vector representations that combine the information from anchor facets.",2.2 Combining Facets into Pseudowords,0,[0]
Our anchor facets are G1 . .,2.2 Combining Facets into Pseudowords,0,[0]
.GK,2.2 Combining Facets into Pseudowords,0,[0]
", where Gk is a set of anchor facets which will form the kth pseudoword anchor.",2.2 Combining Facets into Pseudowords,0,[0]
The pseudowords are g1 . . .,2.2 Combining Facets into Pseudowords,0,[0]
"gK , where gk is the pseudoword from Gk.",2.2 Combining Facets into Pseudowords,0,[0]
"These pseudowords form the new rows of S. We give several candidates for combining anchors facets into a single multiword anchor; we compare their performance in Section 3.
",2.2 Combining Facets into Pseudowords,0,[0]
Vector Average An obvious function for computing the central tendency is the vector average.,2.2 Combining Facets into Pseudowords,0,[0]
"For each anchor facet,
Sgk,j = ∑
i∈Gk
Si,j |Gk| , (2)
where |Gk| is the cardinality of Gk.",2.2 Combining Facets into Pseudowords,0,[0]
"Vector average makes the pseudoword Sgk,j more central, which is intuitive but inconsistent with the interpretation from Arora et al. (2013) that anchors should be extreme points whose linear combinations explain more central words.
",2.2 Combining Facets into Pseudowords,0,[0]
Or-operator An alternative approach is to consider a cooccurrence with any anchor facet in Gk.,2.2 Combining Facets into Pseudowords,0,[0]
"For word j, we use De Morgan’s laws to set
Sgk,j = 1− ∏
i∈Gk (1− Si,j).",2.2 Combining Facets into Pseudowords,0,[0]
"(3)
Unlike the average, which pulls the pseudoword inward, this or-operator pushes the word outward,
increasing each of the dimensions.",2.2 Combining Facets into Pseudowords,0,[0]
"Increasing the volume of the simplex spanned by the anchors explains more words.
",2.2 Combining Facets into Pseudowords,0,[0]
Element-wise Min Vector average and oroperator are both sensitive to outliers and cannot account for polysemous anchor facets.,2.2 Combining Facets into Pseudowords,0,[0]
"Returning to our previous example, both “camera” and “bag” are bad anchors for camera bags because they appear in documents discussing other products.",2.2 Combining Facets into Pseudowords,0,[0]
"However, if both “camera” and “bag” are anchor facets, we can look at an intersection of their contexts: words that appear with both.",2.2 Combining Facets into Pseudowords,0,[0]
"Using the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags.
",2.2 Combining Facets into Pseudowords,0,[0]
"Mathematically, this is an element-wise min operator,
Sgk,j = min i∈Gk Si,j .",2.2 Combining Facets into Pseudowords,0,[0]
"(4)
This construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic.
",2.2 Combining Facets into Pseudowords,0,[0]
"Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean.",2.2 Combining Facets into Pseudowords,0,[0]
"Thus, for each anchor facet
Sgk,j = ∑
i∈Gk
( S−1i,j |Gk| )−1 .",2.2 Combining Facets into Pseudowords,0,[0]
"(5)
Since the harmonic mean tends towards the lowest values in the set, it is not sensitive to large outliers, giving us robustness to polysemous words.",2.2 Combining Facets into Pseudowords,0,[0]
"After constructing the pseudowords of S we then need to find the coefficients Ci,k which describe each word in our vocabulary as a convex combination of the multiword anchors.",2.3 Finding Topics,0,[0]
"Like standard anchor methods, we solve the following for each token type:
C∗i,· = argmin Ci,· DKL
( Si,· ∥∥∥∥ K∑
k=1
Ci,kSgk,·
) .
",2.3 Finding Topics,0,[0]
"(6) Finally, we appeal to Bayes’ rule, we recover the topic-word matrix A from the coefficients of C.
The correctness of the topic recovery algorithm hinges upon the assumption of separability.",2.3 Finding Topics,0,[0]
"Separability means that the occurrence pattern across
documents of the anchor words across the data mirrors that of the topics themselves.",2.3 Finding Topics,0,[0]
"For single word anchors, this has been observed to hold for a wide variety of data (Arora et al., 2012b).",2.3 Finding Topics,0,[0]
"With our tandem anchor extension, we make similar assumptions as the vanilla algorithm, except with pseudowords constructed from anchor facets.",2.3 Finding Topics,0,[0]
"So long as the occurrence pattern of our tandem anchors mirrors that of the underlying topics, we can use the same reasoning as Arora et al. (2012a) to assert that we can provably recover the topic-word matrix A with all of the same theoretical guarantees of complexity and robustness.",2.3 Finding Topics,0,[0]
"Furthermore, we runtime analysis given by Arora et al. (2013) applies to tandem anchors.
",2.3 Finding Topics,0,[0]
"If desired, we can also add further robustness and extensibility to tandem anchors by adding regularization to Equation 6.",2.3 Finding Topics,0,[0]
"Regularization allows us to add something which is mathematically similar to priors, and has been shown to improve the vanilla anchor word algorithm (Nguyen et al., 2014).",2.3 Finding Topics,0,[0]
"We leave the question of the best regularization for tandem anchors as future work, and focus our efforts on solving the problem of interactive topic modeling.",2.3 Finding Topics,0,[0]
"Before addressing interactivity, we apply tandem anchors to real world data, but with anchors gleaned from metadata.",3 High Water Mark for Tandem Anchors,0,[0]
Our purpose is twofold.,3 High Water Mark for Tandem Anchors,0,[0]
"First, we determine which combiner from Section 2.2 to use in our interactive experiments in Section 4 and second, we confirm that well-chosen tandem anchors can improve topics.",3 High Water Mark for Tandem Anchors,0,[0]
"In addition, we examine the runtime of tandem anchors and compare to traditional model-based interactive topic modeling techniques.",3 High Water Mark for Tandem Anchors,0,[0]
"We cannot assume that we will have metadata available to build tandem anchors, but we use them here because they provide a high water mark without the variance introduced by study participants.",3 High Water Mark for Tandem Anchors,0,[0]
"We use the well-known 20 Newsgroups dataset (20NEWS) used in previous interactive topic modeling work: 18,846 Usenet postings from 20 different newgroups in the early 1990s.1 We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes.",3.1 Experimental Setup,0,[0]
"We
1http://qwone.com/˜jason/20Newsgroups/
then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.
",3.1 Experimental Setup,0,[0]
"To seed the tandem anchors, we use the titles of newsgroups.",3.1 Experimental Setup,0,[0]
"To build each multiword anchor facet, we split the title on word boundaries and expand any abbreviations or acronyms.",3.1 Experimental Setup,0,[0]
"For example, the newsgroup title ‘comp.os.mswindows.misc’ becomes {“computer”, “operating”, “system”, “microsoft”, “windows”, “miscellaneous”}.",3.1 Experimental Setup,0,[0]
"We do not fully specify the topic; the title gives some intuition, but the topic modeling algorithm must still recover the complete topic-word distributions.",3.1 Experimental Setup,0,[0]
This is akin to knowing the names of the categories used but nothing else.,3.1 Experimental Setup,0,[0]
"Critically, the topic modeling algorithm has no knowledge of document-label relationships.",3.1 Experimental Setup,0,[0]
Our first evaluation is a classification task to predict documents’ newsgroup membership.,3.2 Experimental Results,0,[0]
"Thus, we do not aim for state-of-the-art accuracy,2 but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors.",3.2 Experimental Results,0,[0]
"After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tandem anchors and the Gram-Schmidt single word anchors.3 For multiword anchors, we use each of the combiner functions from Section 2.2.",3.2 Experimental Results,0,[0]
"The anchor algorithm only gives the topic-word distributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method.",3.2 Experimental Results,0,[0]
We use our own implementation of Gibbs sampling with fixed topics and a symmetric documenttopic Dirichlet prior with concentration α = .01.,3.2 Experimental Results,0,[0]
"Since the topics are fixed, this inference is very fast and can be parallelized on a per-document basis.",3.2 Experimental Results,0,[0]
We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit4 with topic-word pairs as features.,3.2 Experimental Results,0,[0]
"Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.",3.2 Experimental Results,0,[0]
"For both training and test, we exclude words outside
2The best system would incorporate topic features with other features, making it harder to study and understand the topical trends in isolation.
3With fixed anchors and data the anchor algorithm is deterministic, so we use random splits instead of the standard train/test splits so that we can compute variance.
",3.2 Experimental Results,0,[0]
"4http://hunch.net/˜vw/
the LDA vocabulary.",3.2 Experimental Results,0,[0]
The topics created from multiword anchor facets are more accurate than Gram-Schmidt topics (Figure 1).,3.2 Experimental Results,0,[0]
This is true regardless of the combiner function.,3.2 Experimental Results,0,[0]
"However, harmonic mean is more accurate than the other functions.5
Since 20NEWS has twenty classes, accuracy alone does not capture confusion between closely related newsgroups.",3.2 Experimental Results,0,[0]
"For example, accuracy penalizes a classifier just as much for labeling a document from ‘rec.sport.baseball’ with ‘rec.sport.hockey’ as with ‘alt.atheism’ despite the similarity between sports newsgroups.",3.2 Experimental Results,0,[0]
"Consequently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes.
",3.2 Experimental Results,0,[0]
"The first clustering metric is the adjusted Rand index (Yeung and Ruzzo, 2001), which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clustering.",3.2 Experimental Results,0,[0]
Adjusted Rand index (ARI) also accounts for chance groupings of documents.,3.2 Experimental Results,0,[0]
"Next we use F-measure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives.",3.2 Experimental Results,0,[0]
"Finally, we use variation of information (VI).",3.2 Experimental Results,0,[0]
"This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003).",3.2 Experimental Results,0,[0]
"Since we are measuring the amount of information lost, lower variation of information is better.
",3.2 Experimental Results,0,[0]
"Based on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1).",3.2 Experimental Results,0,[0]
"As with accuracy, this is true regardless of which combination function we use.",3.2 Experimental Results,0,[0]
"Furthermore, harmonic mean produces the least confusion between classes.5
The final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality.",3.2 Experimental Results,0,[0]
"Given V , the set of the n most probable words of a topic, coherence is
∑
v1,v2∈V log
D(v1, v2) +
D(v2) (7)
where D(v1, v2) is the co-document frequency of
5Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.2 Experimental Results,0,[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
word types v1 and v2, and D(v2) is the document frequency of word type v2.",3.2 Experimental Results,0,[0]
"A smoothing parameter prevents zero logarithms.
",3.2 Experimental Results,0,[0]
Figure 1 also shows topic coherence.,3.2 Experimental Results,0,[0]
"Although title-based anchor facets produce better classification features, topics from Gram-Schmidt anchors have better coherence than title-based anchors with the vector average or the or-operator.",3.2 Experimental Results,0,[0]
"However, when using the harmonic mean combiner, title-based anchors produce the most human interpretable topics.6
Harmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet.",3.2 Experimental Results,0,[0]
"Both the vector average and the or-operator are swayed by large outliers, making them sensitive to ambiguous terms in an anchor facet.",3.2 Experimental Results,0,[0]
"Element-wise min also has this robustness, but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min.",3.2 Experimental Results,0,[0]
Tandem anchors will enable users to direct topic inference to improve topic quality.,3.3 Runtime Considerations,0,[0]
"However, for the algorithm to be interactive we must also consider runtime.",3.3 Runtime Considerations,0,[0]
Cook and Thomas (2005) argue that for interactive applications with user-initiated actions like ours the response time should be less than ten seconds.,3.3 Runtime Considerations,0,[0]
"Longer waits can increase the cognitive load on the user and harm the user interaction.
6Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.3 Runtime Considerations,0,[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
",3.3 Runtime Considerations,0,[0]
"Fortunately, the runtime of tandem anchors is amenable to interactive topic modeling.",3.3 Runtime Considerations,0,[0]
"On 20NEWS, interactive updates take a median time of 2.13 seconds.",3.3 Runtime Considerations,0,[0]
This result was obtained using a single core of an AMD Phemon II X6 1090T processor.,3.3 Runtime Considerations,0,[0]
"Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets.
",3.3 Runtime Considerations,0,[0]
"Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time.",3.3 Runtime Considerations,0,[0]
"For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model updates with a median time of 24.8 seconds (Hu and Boyd-Graber, 2012), which is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors.
",3.3 Runtime Considerations,0,[0]
"Another promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words.",3.3 Runtime Considerations,0,[0]
Utopian is much slower than tandem anchors.,3.3 Runtime Considerations,0,[0]
"Even on the small InfoVisVAST dataset which contains only 515 documents, Utopian takes 48 seconds to converge.",3.3 Runtime Considerations,0,[0]
"While the times are not strictly comparable due to differing datasets, Utopian scales linearly with the size of the data, we can intuit that even for moderately sized datasets such as 20NEWS, Utopian is infeasible for interactive topic modeling due to run time.
",3.3 Runtime Considerations,0,[0]
"While each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for inter-
activity.",3.3 Runtime Considerations,0,[0]
"Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pronounced as we increase the amount of data.",3.3 Runtime Considerations,0,[0]
"Given high quality anchor facets, the tandem anchor algorithm can produce high quality topic models (particularly when the harmonic mean combiner is used).",4 Interactive Anchor Words,0,[0]
"Moreover, the tandem anchor algorithm is fast enough to be interactive (as opposed to model-based approaches such as the Interactive Topic Model).",4 Interactive Anchor Words,0,[0]
We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling.,4 Interactive Anchor Words,0,[0]
We compare both single word and tandem anchors in our study.,4 Interactive Anchor Words,0,[0]
"We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users.",4 Interactive Anchor Words,0,[0]
"To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to understand a dataset using the anchor word algorithm.",4.1 Interface and User Study,0,[0]
"For this user study, we recruit twenty participants drawn from a university student body.",4.1 Interface and User Study,0,[0]
The student median age is twenty-two.,4.1 Interface and User Study,0,[0]
"Seven are female, and thirteen are male.",4.1 Interface and User Study,0,[0]
"None of the students had any prior familiarity with topic modeling or the 20NEWS dataset.
",4.1 Interface and User Study,0,[0]
Each participant sees a simple user interface (Figure 2) with topic given as a row with two columns.,4.1 Interface and User Study,0,[0]
The left column allows users to view and edit topics’ anchor words; the right column lists the most probable words in each topic.7,4.1 Interface and User Study,0,[0]
"The user can remove an anchor word or drag words from
7While we use topics generated using harmonic mean for our final analysis, users were shown topics generated using the min combiner.",4.1 Interface and User Study,0,[0]
"However, this does not change our result.
",4.1 Interface and User Study,0,[0]
the topic word lists (right column) to become an anchor word.,4.1 Interface and User Study,0,[0]
Users can also add additional topics by clicking the “Add Anchor” to create additional anchors.,4.1 Interface and User Study,0,[0]
"If the user wants to add a word to a tandem anchor set that does not appear in the interface, they manually type the word (restricted to the model’s vocabulary).",4.1 Interface and User Study,0,[0]
"When the user wants to see the updated topics for their newly refined anchors, they click “Update Topics”.
",4.1 Interface and User Study,0,[0]
We give each a participant a high level overview of topic modeling.,4.1 Interface and User Study,0,[0]
"We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous topics.",4.1 Interface and User Study,0,[0]
Users are instructed to use their best judgement to determine if topics are useful.,4.1 Interface and User Study,0,[0]
The task is to edit the anchor words to improve the topics.,4.1 Interface and User Study,0,[0]
"We asked that users spend at least twenty minutes, but no more than thirty minutes.",4.1 Interface and User Study,0,[0]
"We repeat the task twice: once with tandem anchors, and once with single word anchors.8",4.1 Interface and User Study,0,[0]
"We now validate our main result that for interactive topic modeling, tandem anchors yields better topics than single word anchors.",4.2 Quantitative Results,0,[0]
"Like our titlebased experiments in Section 3, topics generated from users become features to train and test a classifier for the 20NEWS dataset.",4.2 Quantitative Results,0,[0]
We choose this dataset for easier comparison with the Interactive Topic Modeling result of Hu et al. (2014).,4.2 Quantitative Results,0,[0]
"Basedsie on our results with title-based anchors, we use the harmonic mean combiner in our analysis.",4.2 Quantitative Results,0,[0]
"As before, we report not only accuracy, but also multiple clustering metrics using the confusion matrix from the classification task.",4.2 Quantitative Results,0,[0]
"Finally, we report topic coherence.
",4.2 Quantitative Results,0,[0]
Figure 3 summarizes the results of our quantitative evaluation.,4.2 Quantitative Results,0,[0]
"While we only compare user generated anchors in our analysis, we include the unsupervised Gram-Schmidt anchors as a baseline.",4.2 Quantitative Results,0,[0]
Some of the data violate assumptions of normality.,4.2 Quantitative Results,0,[0]
"Therefore, we use Wilcoxon’s signed-rank test (Wilcoxon, 1945) to determine if the differences between multiword anchors and single word anchors are significant.
Topics from user generated multiword anchors yield higher classification accuracy (Figure 3).",4.2 Quantitative Results,0,[0]
"Not only is our approach more scalable than the Interactive Topic Model, but we also achieve
8The order in which users complete these tasks is counterbalanced.
higher classification accuracy than Hu et al. (2014).9 Tandem anchors also improve clustering metrics.10
While user selected tandem anchors produce better classification features than single word anchors, users selected single word anchors produce topics with similar topic coherence scores.11
To understand this phenomenon, we use quality metrics (AlSumait et al., 2009) for ranking topics by their correspondence to genuine themes in the data.",4.2 Quantitative Results,0,[0]
"Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words.",4.2 Quantitative Results,0,[0]
"Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution.",4.2 Quantitative Results,0,[0]
"Finally, background topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data.
",4.2 Quantitative Results,0,[0]
Figure 4 reports our topic significance findings.,4.2 Quantitative Results,0,[0]
"For all three significance metrics, multiword anchors produce more significant topics than single word anchors.10 Topic coherence is based solely on the top n words of a topic, while both accuracy and topic significance depend on the entire topicword distributions.",4.2 Quantitative Results,0,[0]
"With single word anchors, topics with good coherence may still be too general.",4.2 Quantitative Results,0,[0]
Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification.,4.2 Quantitative Results,0,[0]
We examine the qualitative differences between how users select multiword anchor facets versus single word anchors.,4.3 Qualitative Results,0,[0]
Table 2 gives examples of topics generated using different anchor strategies.,4.3 Qualitative Results,0,[0]
"In a follow-up survey with our users, 75% find it easier to affect individual changes in the topics using tandem anchors compared to single word anchors.",4.3 Qualitative Results,0,[0]
"Users who prefer editing multiword anchors over single word anchors often report that
9However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits.
10Significant at p < 0.01 when using Wilcoxon’s signedrank test.
",4.3 Qualitative Results,0,[0]
"11The difference between coherence scores was not statistically significant using Wilcoxon’s signed-rank test.
",4.3 Qualitative Results,0,[0]
multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors.,4.3 Qualitative Results,0,[0]
"For example, by combining multiple words related to Christianity, users were able to create a topic which is highly specific, and differentiated from general religion themes which included terms about Atheism and Judaism.
",4.3 Qualitative Results,0,[0]
"While users find that use tandem anchors is easier, only 55% of our users say that they prefer the final topics produced by tandem anchors compared to single word anchors.",4.3 Qualitative Results,0,[0]
"This is in harmony with our quantitative measurements of topic coherence, and may be the result of our stopping criteria: when users judged the topics to be useful.
",4.3 Qualitative Results,0,[0]
"However, 100% of our users feel that the topics created through interaction were better than those generated from Gram-Schmidt anchors.",4.3 Qualitative Results,0,[0]
"This was true regardless of whether we used tandem anchors or single word anchors.
",4.3 Qualitative Results,0,[0]
Our participants also produce fewer topics when using multiword anchors.,4.3 Qualitative Results,0,[0]
The mean difference between topics under single word anchors and multiple word anchors is 9.35.,4.3 Qualitative Results,0,[0]
"In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors.",4.3 Qualitative Results,0,[0]
"In contrast, fixing an ambiguous tandem anchor is simple: users just add more terms to the anchor facet.",4.3 Qualitative Results,0,[0]
Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets.,5 Conclusion,0,[0]
"For interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use.",5 Conclusion,0,[0]
"Furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large
datasets for which interactivity was previous impossible.",5 Conclusion,0,[0]
This work was supported by the collaborative NSF Grant IIS-1409287 (UMD) and,Acknowledgements,0,[0]
IIS- 1409739 (BYU).,Acknowledgements,0,[0]
Boyd-Graber is also supported by NSF grants IIS-1320538 and NCSE-1422492.,Acknowledgements,0,[0]
Interactive topic models are powerful tools for understanding large collections of text.,abstractText,0,[0]
"However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets.",abstractText,0,[0]
"Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for userfacing applications.",abstractText,0,[0]
"We propose combinations of words as anchors, going beyond existing single word anchor algorithms— an approach we call “Tandem Anchors”.",abstractText,0,[0]
We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and noninteractive approaches.,abstractText,0,[0]
Tandem anchors are faster and more intuitive than existing interactive approaches.,abstractText,0,[0]
"Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation.",abstractText,0,[0]
"In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008).",abstractText,0,[0]
"However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).",abstractText,0,[0]
"Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition.",abstractText,0,[0]
"Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.",abstractText,0,[0]
"The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009).",abstractText,0,[0]
We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.,abstractText,0,[0]
"The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1).",abstractText,0,[0]
"This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.",abstractText,0,[0]
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive.,abstractText,0,[0]
We extend the anchor algorithm to use multiple anchor words in tandem (Section 2).,abstractText,0,[0]
"Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.",abstractText,0,[0]
"For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3).",abstractText,0,[0]
Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4).,abstractText,0,[0]
"Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.",abstractText,0,[0]
Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,title,0,[0]
"Applications using machine learning techniques have exploded during the recent years, with “deep learning” techniques being applied on a wide variety of tasks that had hitherto proved challenging.",1. Introduction,0,[0]
"Training highly accurate machine learning models requires large quantities of (high quality) data, technical expertise and computational resources.",1. Introduction,0,[0]
"An important recent paradigm is prediction as a service, whereby a service provider with expertise and resources can make predictions for clients.",1. Introduction,0,[0]
"However, this approach requires trust between service provider and client; there are several instances where clients may be unwilling or unable to provide data to service providers due to privacy
1University of Oxford, Oxford, UK 2The Alan Turing Institute, London, UK 3University of Warwick, Coventry, UK.",1. Introduction,0,[0]
"Correspondence to: Amartya Sanyal <amartya.sanyal@cs.ox.ac.uk>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
concerns.",1. Introduction,0,[0]
"Examples include assisting in medical diagnoses (Kononenko, 2001; Blecker et al., 2017), detecting fraud from personal finance data (Ghosh & Reilly, 1994), and detecting online communities from user data (Fortunato, 2010).",1. Introduction,0,[0]
"The ability of a service provider to predict on encrypted data can alleviate concerns of data leakage.
",1. Introduction,0,[0]
The framework of fully homomorphic encryption (FHE) is ideal for this paradigm.,1. Introduction,0,[0]
Fully homomorphic encryption schemes support arbitrary computations to be performed directly on encrypted data without prior decryption.,1. Introduction,0,[0]
"The first fully homomorphic encryption system was developed just 10 years ago by Gentry (2009), after being an open question for 30 years (Rivest et al., 1978).",1. Introduction,0,[0]
"Since then several other schemes have been proposed (Gentry et al., 2012; 2013; Brakerski & Vaikuntanathan, 2014; Ducas & Micciancio, 2015; Chillotti et al., 2016).",1. Introduction,0,[0]
"However, without significant changes to machine learning models and improved algorithmic tools, homomorphic encryption does not scale to real-world machine learning applications.
",1. Introduction,0,[0]
"Indeed, already there have been several recent works trying to accelerate predictions of machine learning models on fully homomorphic encrypted data.",1. Introduction,0,[0]
"In general, the approach has been to approximate all or parts of a machine learning model to accommodate the restrictions of an FHE framework.",1. Introduction,0,[0]
"Often, certain kind of FHE schemes are preferred because they allow for “batched” parallel encrypted computations, called SIMD operations (Smart & Vercauteren, 2014).",1. Introduction,0,[0]
"This technique is exemplified by the CryptoNets model (Gilad-Bachrach et al., 2016).",1. Introduction,0,[0]
"While these models allow for high-throughput (via SIMD), they are not particularly suited for the prediction as a service framework for individual users, as single predictions are slow.",1. Introduction,0,[0]
"Further, because they employ a leveled homomorphic encryption scheme, they are unable to perform many nested multiplications, a requirement for state-of-the-art deep learning models (He et al., 2016; Huang et al., 2017).
",1. Introduction,0,[0]
"Our solution demonstrates that existing work on Binary Neural Networks (BNNs) (Kim & Smaragdis, 2015; Courbariaux et al., 2016) can be adapted to produce efficient and highly accurate predictions on encrypted data.",1. Introduction,0,[0]
"We show that a recent FHE encryption scheme (Chillotti et al., 2016) which only supports operations on binary data can be leveraged to compute all of the operations of BNNs.",1. Introduction,0,[0]
"To do so,
we develop specialized circuits for fully-connected, convolutional, and batch normalization layers (Ioffe & Szegedy, 2015).",1. Introduction,0,[0]
"Additionally we design tricks to sparsify encrypted computation that reduce computation time even further.
",1. Introduction,0,[0]
Most similar to our work is Bourse et al. (2017) who use neural networks with signed integer weights and binary activations to perform encrypted prediction.,1. Introduction,0,[0]
"However, this model is only evaluated on MNIST, with modest accuracy results, and the encryption scheme parameters depend on the structure of the model, potentially requiring clients to re-encrypt their data if the service provider updates their model.",1. Introduction,0,[0]
"Our framework allows the service provider to update their model at anytime, and allows one to use binary neural networks of Courbariaux et al. (2016) which, in particular, achieve high accuracy on MNIST (99.04%).",1. Introduction,0,[0]
Another closely related work is Meehan et al. (2018) who design encrypted adder and multiplier circuits so that they can implement machine learning models on integers.,1. Introduction,0,[0]
"This can be seen as complementary to our work on binary networks: while they achieve improved accuracy because of greater precision, they are less efficient than our methods (however on MNIST we achieve the same accuracy with a 29× speedup, via our sparsification and parallelization tricks).
",1. Introduction,0,[0]
Private training.,1. Introduction,0,[0]
"In this work, we do not address the question of training machine learning models with encrypted data.",1. Introduction,0,[0]
"There has been some recent work in this area (Hardy et al., 2017; Aono et al., 2017).",1. Introduction,0,[0]
"However, as of now it appears possible only to train very small models using fully homomorphic encryption.",1. Introduction,0,[0]
We leave this for future work.,1. Introduction,0,[0]
"In this work, our focus is on achieving speed-ups when using complex models on fully homomorphic encrypted data.",1.1. Our contributions,0,[0]
"In order to achieve these speed-ups, we propose several methods to modify the training and design of neural networks, as well as algorithmic tricks to parallelize and accelerate computation on encrypted data:
• We propose two types of circuits for performing inner products between unencrypted and encrypted data: reduce tree circuits and sorting networks.",1.1. Our contributions,0,[0]
"We give a runtime comparison of each method.
",1.1. Our contributions,0,[0]
"• We introduce an easy trick, which we call the +1 trick to sparsify encrypted computations.
",1.1. Our contributions,0,[0]
"• We demonstrate that our techniques are easily parallelizable and we report timing for a variety of computation settings on real world datasets, alongside classification accuracies.",1.1. Our contributions,0,[0]
In this section we describe our Encrypted Prediction as a Service (EPAAS) paradigm.,2. Encrypted Prediction as a Service,0,[0]
We then detail our privacy and computational guarantees.,2. Encrypted Prediction as a Service,0,[0]
"Finally, we discuss how different related work is suited to this paradigm and propose a solution.
",2. Encrypted Prediction as a Service,0,[0]
"In the EPAAS setting we have any number of clients, say C1, . . .",2. Encrypted Prediction as a Service,0,[0]
", Cn that have data x1, . . .",2. Encrypted Prediction as a Service,0,[0]
",xn.",2. Encrypted Prediction as a Service,0,[0]
The clients would like to use a highly-accurate model f provided by a server S to predict some outcome.,2. Encrypted Prediction as a Service,0,[0]
"In cases where data x is not sensitive there are already many solutions for this such as BigML, Wise.io, Google Cloud AI, Amazon Machine Learning, among others.",2. Encrypted Prediction as a Service,0,[0]
"However, if the data is sensitive so that the clients would be uncomfortable giving the raw data to the server, none of these systems can offer the client a prediction.",2. Encrypted Prediction as a Service,0,[0]
"If data x is sensitive (e.g., x may be the health record of client C, and f(x) may be the likelihood of heart disease), then we would like to have the following privacy guarantees:
P1.",2.1. Privacy and computational guarantees,0,[0]
"Neither the server S, or any other party, learn anything about client data x, other than its size (privacy of the data).
P2.",2.1. Privacy and computational guarantees,0,[0]
"Neither the client C, or any other party, learn anything about model f , other than the prediction f(x) given client data x (and whatever can be deduced from it) (privacy of the model).
",2.1. Privacy and computational guarantees,0,[0]
"Further, the main attraction of EPAAS is that the client is involved as little as possible.",2.1. Privacy and computational guarantees,0,[0]
"More concretely, we wish to have the following computational guarantees:
C1.",2.1. Privacy and computational guarantees,0,[0]
"No external party is involved in the computation.
",2.1. Privacy and computational guarantees,0,[0]
C2.,2.1. Privacy and computational guarantees,0,[0]
"The rounds of communication between client and server should be limited to 2 (send data & receive prediction).
",2.1. Privacy and computational guarantees,0,[0]
C3.,2.1. Privacy and computational guarantees,0,[0]
Communication and computation at the client side should be independent of model f .,2.1. Privacy and computational guarantees,0,[0]
"In particular, (i) the server should be able to update f without communicating with any client, and (ii) clients should not need to be online during the computation of f(x).
",2.1. Privacy and computational guarantees,0,[0]
Note that these requirements rule out protocols with preprocessing stages or that involve third parties.,2.1. Privacy and computational guarantees,0,[0]
"Generally speaking, a satisfactory solution based on FHE would proceed as follows: (1) a client generates encryption parameters, encrypts their data x using the private key, and sends the resulting encryption x̃, as well as the public key to the server.",2.1. Privacy and computational guarantees,0,[0]
"(2) The server evaluates f on x̃ leveraging the homomorphic properties of the encryption, to obtain an encryption f̃(x) without learning anything whatsoever about x, and sends f̃(x) to the client.",2.1. Privacy and computational guarantees,0,[0]
"(3) Finally, the client decrypts and recovers the prediction f(x) in the clear.",2.1. Privacy and computational guarantees,0,[0]
A high level depiction of these steps is shown in Figure 1.,2.1. Privacy and computational guarantees,0,[0]
Table 1 describes whether prior work satisfy the above privacy and computational guarantees.,2.2. Existing approaches,0,[0]
"First, note that Cryptonets (Gilad-Bachrach et al., 2016) violates C3(i) and P2.",2.2. Existing approaches,0,[0]
"This is because the clients would have to generate parameters for the encryption according to the structure of f , so we are able to make inferences about the model (violating P2) and the client is not allowed to change the model f without telling the client (violating C3(i)).",2.2. Existing approaches,0,[0]
The same holds for the work of Chabanne et al. (2017).,2.2. Existing approaches,0,[0]
"The approach of Bourse et al. (2017) requires the server to calibrate the parameters of the encryption scheme according to the magnitude of intermediate values, thus C3(i) is not necessarily satisfied.",2.2. Existing approaches,0,[0]
"Closely related to our work is that of Meehan et al. (2018)
which satisfies our privacy and computational requirements.",2.2. Existing approaches,0,[0]
"We will show that our method is significantly faster than this method, with very little sacrifice in accuracy.
",2.2. Existing approaches,0,[0]
Multi-Party Computation (MPC).,2.2. Existing approaches,0,[0]
"It is important to distinguish between approaches purely based on homomorphic encryption (described above), and those involving MultiParty Computation (MPC) techniques, such as (Mohassel & Zhang, 2017; Liu et al., 2017; Rouhani et al., 2017; Riazi et al., 2017; Chase et al.;",2.2. Existing approaches,0,[0]
"Juvekar et al., 2018).",2.2. Existing approaches,0,[0]
"While generally MPC approaches are faster, they crucially rely on all parties being involved in the whole computation, which is in conflict with requirement C3(ii).",2.2. Existing approaches,0,[0]
"Additionally, in MPC the structure of the computation is public to both parties, which means that the server would have to communicate basic information such as the number of layers of f .",2.2. Existing approaches,0,[0]
"This is conflict with requirements P1, C2, and C3(i).
",2.2. Existing approaches,0,[0]
"In this work, we propose to use a very tailored homomorphic encryption technique to guarantee all privacy and computational requirements.",2.2. Existing approaches,0,[0]
In the next section we give background on homomorphic encryption.,2.2. Existing approaches,0,[0]
"Further, we motivate the encryption protocol and the machine learning model class we use to satisfy all guarantees.",2.2. Existing approaches,0,[0]
All cryptosystems define two functions: 1. an encryption function E(·) that maps data (often called plaintexts) to encrypted data (ciphertexts); 2.,3. Background,0,[0]
a decryption function D(·) that maps ciphertexts back to plaintexts.,3. Background,0,[0]
"In public-key cryptosystems, to evaluate the encryption function E , one needs to hold a public key kPUB, so the encryption of data x is E(x, kPUB).",3. Background,0,[0]
"Similarly, to compute the decryption function D(·) one needs to hold a secret key kSEC which allows us to recover: D(E(x, kPUB), kSEC)",3. Background,0,[0]
"= x.
A cryptosystem is homomorphic in some operation if it is possible to perform another (possibly different) operation such that: E(x, kPUB) E(x, kPUB) = E(x y, kPUB).",3. Background,0,[0]
"Finally, in this work we assume all data to be binary ∈ {0, 1}.",3. Background,0,[0]
"For more detailed background on FHE beyond what is described below, see the excellent tutorial of Halevi (2017).",3. Background,0,[0]
"In 1978, cryptographers posed the question: Does an encryption scheme exist that allows one to perform arbitrary computations on encrypted data?",3.1. Fully Homomorphic Encryption,0,[0]
"The implications of this, called a Fully homomorphic encryption (FHE) scheme, would enable clients to send computations to the cloud while retaining control over the secrecy of their data.",3.1. Fully Homomorphic Encryption,0,[0]
This was still an open problem however 30 years later.,3.1. Fully Homomorphic Encryption,0,[0]
"Then, in 2009, a cryptosystem (Gentry, 2009) was devised that could, in principle, perform such computations on encrypted data.",3.1. Fully Homomorphic Encryption,0,[0]
"Similar to previous approaches, in each computation, noise is introduced into the encrypted data.",3.1. Fully Homomorphic Encryption,0,[0]
"And after a certain number of computations, the noise grows too large so that the encryptions can no longer be decrypted.",3.1. Fully Homomorphic Encryption,0,[0]
"The key innovation was a technique called bootstrapping, which allows one to reduce the noise to its original level without decrypting.",3.1. Fully Homomorphic Encryption,0,[0]
"That result constituted a massive breakthrough, as it established, for the first time, a fully homomorphic encryption scheme (Gentry, 2009).",3.1. Fully Homomorphic Encryption,0,[0]
"Unfortunately, the original bootstrapping procedure was highly impractical.",3.1. Fully Homomorphic Encryption,0,[0]
"Consequently, much of the research since the first FHE scheme has been devoted to reducing the growth of noise so that the scheme never has to perform bootstrapping.",3.1. Fully Homomorphic Encryption,0,[0]
"Indeed, even in recent FHE schemes bootstrapping is slow (roughly six minutes in a highly-optimized implementation of a recent popular scheme (Halevi & Shoup, 2015)) and bootstrapping many times increases the memory requirements of encrypted data.",3.1. Fully Homomorphic Encryption,0,[0]
"Thus, one common technique to implement encrypted prediction was to take an existing ML algorithm and approximate it with as few operations as possible, in order to never have to bootstrap.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,0,[0]
"This involved careful parameter tuning to ensure that the security of the encryption scheme was sufficient, that it didn’t require too much memory, and that it ran in a reasonable amount of time.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,0,[0]
"One prominent example of this is Cryptonets (Gilad-Bachrach et al., 2016).",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,0,[0]
Recent developments in cryptography call for rethinking this approach.,3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
Ducas & Micciancio (2015) devised a scheme that that could bootstrap a single Boolean gate in under one second with reduced memory.,3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
"Recently, Chillotti et al. (2016) introduced optimizations implemented in the TFHE library, which further reduced bootstrapping of to under 0.1 seconds.",3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
"In this paper, we demonstrate that this change has a huge impact on designing encrypted machine learning algorithms.",3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
"Specifically, encrypted computation is now modular: the cost of adding a few layers to an encrypted neural network is simply the added cost of each layer in isolation.",3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
"This is particularly important as recent developments in deep learning such as Residual Networks (He
et al., 2016) and Dense Networks (Huang et al., 2017) have shown that networks with many layers are crucial to achieve state-of-the-art accuracy.",3.1.2. ENCRYPTED PREDICTION WITH FHE,0,[0]
"The cryptosystem that we will use in this paper, TFHE, is however restricted to computing binary operations.",3.2. Binary Neural Networks,0,[0]
"We note that, concurrent to the work that led to TFHE, was the development of neural network models that perform binary operations between binary weights and binary activations.",3.2. Binary Neural Networks,0,[0]
"These models, called Binary Neural Networks (BNNs), were first devised by Kim & Smaragdis (2015); Courbariaux et al. (2016), and were motivated by the prospect of training and testing deep models on limited memory and limited compute devices, such as mobile phones.
",3.2. Binary Neural Networks,0,[0]
Technical details.,3.2. Binary Neural Networks,0,[0]
We now describe the technical details of binary networks that we will aim to replicate on encrypted data.,3.2. Binary Neural Networks,0,[0]
"In a Binary Neural Network (BNN) every layer maps a binary input x ∈ {−1, 1}d to a binary output z ∈ {−1, 1}p using a set of binary weights W ∈ {−1, 1}(p,d) and a binary activation function sign(·) that is 1 if x ≥ 0 and −1 otherwise.",3.2. Binary Neural Networks,0,[0]
"Although binary nets don’t typically use a bias term, applying batch-normalization (Ioffe & Szegedy, 2015) when evaluating the model it means that a bias term b ∈",3.2. Binary Neural Networks,0,[0]
Zp may need to be added before applying the activation function (cf. Sec. 4.1.2).,3.2. Binary Neural Networks,0,[0]
"Thus, when evaluating the model, a fully connected layer in a BNN implements the following transformation z",3.2. Binary Neural Networks,0,[0]
:= sign(Wx + b).,3.2. Binary Neural Networks,0,[0]
"From now on we will call all data represented as {−1, 1} non-standard binary and data represented as {0, 1} as binary.",3.2. Binary Neural Networks,0,[0]
"Kim & Smaragdis (2015); Courbariaux et al. (2016) were the first to note that the above inner product nonlinearity in BNNs could be implemented using the following steps:
1.",3.2. Binary Neural Networks,0,[0]
"Transform data and weights from non-standard binary to binary: w,x→ w,x by replacing −1 with 0. n
2.",3.2. Binary Neural Networks,0,[0]
"Element-wise multiply by applying the logical operator XNOR(w,x) for each element of w and x.
3.",3.2. Binary Neural Networks,0,[0]
"Sum result of previous step by using popcount operation (which counts the number of 1s), call this S.
4.",3.2. Binary Neural Networks,0,[0]
"If the bias term is b, check if 2S ≥",3.2. Binary Neural Networks,0,[0]
d,3.2. Binary Neural Networks,0,[0]
"− b, if so the activation is positive and return 1, otherwise return −1.
",3.2. Binary Neural Networks,0,[0]
"Thus we have that,
zi = sign(2 · popcount(XNOR(wi,x))− d + b)
",3.2. Binary Neural Networks,0,[0]
Related binary models.,3.2. Binary Neural Networks,0,[0]
"Since the initial work on BNNs there has been a wealth of work on binarizing, ternarizing, and quantizing neural networks Chen et al. (2015); Courbariaux et al. (2015); Han et al. (2016); Hubara et al. (2016);
Zhu et al. (2016); Chabanne et al. (2017); Chen et al. (2017).",3.2. Binary Neural Networks,0,[0]
Our approach is currently tailored to methods that have binary activations and we leave the implementation of these methods on encrypted data for future work.,3.2. Binary Neural Networks,0,[0]
"In this work, we make the observation that BNNs can be run on encrypted data by designing circuits in TFHE for computing their operations.",4. Methods,0,[0]
In this section we consider Boolean circuits that operate on encrypted data and unencrypted weights and biases.,4. Methods,0,[0]
"We show how these circuits allow us to efficiently implement the three main layers of binary neural networks: fully connected, convolutional, and batch-normalization.",4. Methods,0,[0]
We then show how a simple trick allows us to sparsify our computations.,4. Methods,0,[0]
Our techniques can be easily parallelized.,4. Methods,0,[0]
"During the evaluation of a circuit, gates at the same level in the tree representation of the circuit can be evaluated in parallel.",4. Methods,0,[0]
"Hence, when implementing a function, “shallow” circuits are preferred in terms of parallelization.",4. Methods,0,[0]
While parallel computation was often used to justify employing second generation FHE techniques— where parallelization comes from ciphertext packing—we show in the following section that our techniques create dramatic speedups for a state-of-the-art FHE technique.,4. Methods,0,[0]
We emphasize that a key challenge is that we need to use data oblivious algorithms (circuits) when dealing with encrypted data as the algorithm never discovers the actual value of any query made on the data.,4. Methods,0,[0]
The three primary circuits we need are for the following tasks: 1. computing the inner product; 2. computing the binary activation function (described in the previous section) and; 3. dealing with the bias.,4.1. Binary OPs,0,[0]
"As described in the previous section, BNNs can speed up an inner product by computing XNORs (for element-wise multiplication) followed by a POPCOUNT (for summing).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"In our case, we compute an inner product of size d by computing XNORs element-wise between d bits of encrypted data and d bits of unencrypted data, which results in an encrypted d bit output.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"To sum this output, the POPCOUNT operation is useful when weights and data are unencrypted because POPCOUNT is implemented in the instruction set of Intel and AMD processors, but when dealing with encrypted data we simply resort to using shallow circuits.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"We consider two circuits for summation, both with sublinear depth: a reduce tree adder and a sorting network.
",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
Reduce tree adder.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"We implement the sum using a binary tree of half and ripple-carry (RC) adders organized into a reduction tree, as shown in Figure 2 (Left).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"All these structures can be implemented to run on encrypted data because TFHE allows us to compute XNOR, AND, and OR on encrypted data.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"The final number returned by the reduction tree S̃ is the binary representation of the number of 1s resulting from the XNOR, just like POPCOUNT.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Thus, to compute the BNN activation function sign(·) we need to check whether 2S̃ ≥ d− b, where d is the number of bits in S̃ and b is the bias.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
Note that if the bias is zero we simply need to check if S̃ ≥ d/2.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
To do so we can simply return the second-to-last bit of S̃. If it is 1 then S̃ is at least d/2.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"If the bias b is non-zero (because of batch-normalization, described in Section 4.1.2), we can implement a circuit to perform the check 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"The bias b (which is available in the clear) may be an integer as large as S̃. Let B[(d − b)/2], B[S̃] be the binary representations of b and S̃. Algorithm 1 describes a comparator circuit that returns an encrypted value of 1 if the above condition holds and (encrypted) 0",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"otherwise (where MUX(s, a, b) returns a if s = 1 and b otherwise).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"As encrypted operations dominate
the running time of our computation, in practice this computation essentially corresponds to evaluating d MUX gates.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"This gate has a dedicated implementation in TFHE, which results in a very efficient comparator in our setting.
",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Algorithm 1 Comparator Inputs: Encrypted B[S̃], unencrypted B[(d− b)/2], size d of B[(d− b)/2],B[S̃]",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Output: Result of 2S̃ ≥ d− b
1: o = 0 2: for i = 1, . . .",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
", d do 3: if B[(d− b)/2]i = 0",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"then 4: o = MUX(B[S̃]i, 1̃, o) 5: else 6: o = MUX(B[S̃]i, o, 0̃) 7: end if 8: end for 9: Return: o
Sorting network.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
We do not technically care about the sum of the result of the element-wise XNOR between w̄ and x̄.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"In fact, all we care about is if the result of the comparison: 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Thus, another idea is to take the output of the (bitwise) XNOR and sort it.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Although this sorting needs to be performed over encrypted data, the rest of the computation does not require any homomorphic operations; after sorting we hold a sequence of encrypted 1s, followed by encrypted 0s.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"To output the correct value, we only need to select one the (encrypted) bit in the correct position and return it.",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"If b = 0 we can simply return the encryption of the central bit in the sequence; indeed, if the central bit is 1, then there are more 1s than 0s and thus 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
and we return 1.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
If b 6= 0,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
we need to offset the returned index by b in the correct direction depending on the sign of b.,4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"In order to sort the initial array we implement a sorting network, shown in Figure 2 (Right).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"The sorting network is a sequence of swap gates between individuals bits, where SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"(OR(a, b), AND(a, b)).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"Note that if a ≥ b then SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"(a, b), and otherwise is (b, a).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
"More specifically, we implement Batcher’s sorting network (Batcher, 1968), which consists of O(n log2(n)) swap gates, and has depth O(log2(n)).",4.1.1. ENCRYPTED INNER PRODUCT,0,[0]
Batch normalization is mainly used during training; however during evaluating a model this requires us scale and translate and scale the input (which is the output of the previous layer).,4.1.2. BATCH NORMALIZATION,0,[0]
"In practice, when our activation function is the sign function, this only means that we need to update the bias term (the actual change to the bias term is an elementary calculation).",4.1.2. BATCH NORMALIZATION,0,[0]
"As our circuits are designed to work with a bias term, and the scaling and translation factors are available as
plaintext (as they are part of the model), this operation is easily implemented during test time.",4.1.2. BATCH NORMALIZATION,0,[0]
"Since we have access to W ∈ {−1, 1}p×d and the bias term b ∈ Zp in the clear (only data x and subsequent activations are encrypted), we can exploit the fact that W always has values±1 to roughly halve the cost computation.",4.2. Sparsification via “+1”-trick,0,[0]
"We consider w ∈ {−1, 1}d which is a single row of W and observe that:
w>x = (1 + w)>(1 + x)− ∑ i wi − (1 + x)>1,
where 1 denotes the vector in which every entry is 1.",4.2. Sparsification via “+1”-trick,0,[0]
"Further note that (1 + w) ∈ {0, 2}d which means that the product (1+w)>(1+x) is simply the quantity 4 ∑ i:wi=1
x̄i, where x̄ refers to the standard binary representation of the nonstandard binary x.",4.2. Sparsification via “+1”-trick,0,[0]
"Assuming at most half of the wis were originally +1, if w ∈ {−1, 1}d, only d/2 encrypted values need be added.",4.2. Sparsification via “+1”-trick,0,[0]
"We also need to compute the encrypted sum ∑ i xi; however, this latter sum need only be computed once, no matter how many output units the layer has.",4.2. Sparsification via “+1”-trick,0,[0]
"Thus, this small bit of extra overhead roughly halves the amount of computation required.",4.2. Sparsification via “+1”-trick,0,[0]
"We note that if w has more −1s than +1s, w>x can be computed using (1−w) and (1− x) instead.",4.2. Sparsification via “+1”-trick,0,[0]
This guarantees that we never need to sum more than half the inputs for any output unit.,4.2. Sparsification via “+1”-trick,0,[0]
The sums of encrypted binary values can be calculated as described in Sec. 4.1.,4.2. Sparsification via “+1”-trick,0,[0]
"The overheads are two additions required to compute (1+x)>1 and (1 − x)>1, and then a subtraction of two log(d)-bit long encrypted numbers.",4.2. Sparsification via “+1”-trick,0,[0]
"(The multiplication by 2 or 4 as may be sometimes required is essentially free, as bit shifts correspond to dropping bits, and hence do not require homomorphic operations).",4.2. Sparsification via “+1”-trick,0,[0]
"As our experimental results show this simple trick roughly halves the computation time of one
layer; the actual savings appear to be even more than half as in many instances the number of elements we need to sum over is significantly smaller than half.
",4.2. Sparsification via “+1”-trick,0,[0]
It is worth emphasizing the advantage for binarizing and then using the above approach to making the sums sparse.,4.2. Sparsification via “+1”-trick,0,[0]
"By default, units in a neural network compute an affine function to which an activation function is subsequently applied.",4.2. Sparsification via “+1”-trick,0,[0]
The affine map involves an inner product which involves d multiplications.,4.2. Sparsification via “+1”-trick,0,[0]
Multiplication under fully homomorphic encryption schemes is however significantly more expensive than addition.,4.2. Sparsification via “+1”-trick,0,[0]
"By binarizing and applying the above calculation, we’ve replaced the inner product operation by selection (which is done in the clear as W is available in plaintext) and (encrypted) addition.",4.2. Sparsification via “+1”-trick,0,[0]
"Ternary neural networks use weights in {−1, 0, 1} rather than {−1, 1}; this can alternatively be viewed as dropping connections from a BNN.",4.3. Ternarization (Weight Dropping),0,[0]
"Using ternary neural networks rather than binary reduces the computation time as encrypted inputs for which the corresponding wi is 0 can be safely dropped from the computation, before the method explained in section 4.2 is applied to the remaining elements.",4.3. Ternarization (Weight Dropping),0,[0]
Our experimental results show that a binary network can be ternarized to maintain the same level of test accuracy with roughly a quarter of the weights being 0,4.3. Ternarization (Weight Dropping),0,[0]
(cf. Sec. 5.4).,4.3. Ternarization (Weight Dropping),0,[0]
In this section we report encrypted binary neural network prediction experiments on a number of real-world datasets.,5. Experimental Results,0,[0]
"We begin by comparing the efficiency of the two circuits used for inner product, the reduce tree and the sorting network.",5. Experimental Results,0,[0]
We then describe the datasets and the architecture of the BNNs used for classification.,5. Experimental Results,0,[0]
"We report the classification timings of these BNNs for each dataset, for different computational settings.",5. Experimental Results,0,[0]
"Finally, we give accuracies of the BNNs compared to floating point networks.",5. Experimental Results,0,[0]
"Our code is freely available at (tap, 2018).",5. Experimental Results,0,[0]
"We show timings of reduce tree and sorting network for different number of input bits, with and without parallelization in Figure 3 (parallelization is over 16 CPUs).",5.1. Reduce tree vs. sorting network,0,[0]
We notice that the reduce tree is strictly better when comparing parallel or non-parallel timings of the circuits.,5.1. Reduce tree vs. sorting network,0,[0]
"As such, from now on we use the reduce tree circuit for inner product.
",5.1. Reduce tree vs. sorting network,0,[0]
"It should be mentioned that at the outset this result was not obvious because while sorting networks have more levels of computation, they have fewer gates.",5.1. Reduce tree vs. sorting network,0,[0]
"Specifically, the sorting network used for encrypted sorting is the bitonic sorting network which for n bits has O(log2 n) levels of computa-
tion whereas the reduce tree only has O(log n) levels.",5.1. Reduce tree vs. sorting network,0,[0]
"On the other hand, the reduce tree requires 2 gates for each half adder and 5k gates for each k-bit RC adder, whereas a sorting network only requires 2 gates per SWAP operation.",5.1. Reduce tree vs. sorting network,0,[0]
"Another factor that may slow down sorting networks is that is that our implementation of sorting networks is recursive, whereas the reduce tree is iterative.",5.1. Reduce tree vs. sorting network,0,[0]
"We evaluate on four datasets, three of which have privacy implications due to health care information (datasets Cancer and Diabetes) or applications in surveillance (dataset Faces).",5.2. Datasets,0,[0]
"We also evaluate on the standard benchmark MNIST dataset.
Cancer.",5.2. Datasets,0,[0]
The Cancer dataset1 contains 569 data points where each point has 30 real-valued features.,5.2. Datasets,0,[0]
The task is to predict whether a tumor is malignant (cancerous) or benign.,5.2. Datasets,0,[0]
Similar to Meehan et al. (2018) we divide the dataset into a training set and a test in a 70 : 30 ratio.,5.2. Datasets,0,[0]
"For every real-valued feature, we divide the range of each feature into three equal-spaced bins and one-hot encode each feature by its bin-membership.",5.2. Datasets,0,[0]
This creates a 90-dimensional binary vector for each example.,5.2. Datasets,0,[0]
"We use a single fully connected layer 90→ 1 followed by a batch normalization layer, as is common practice for BNNs (Courbariaux et al., 2016).
",5.2. Datasets,0,[0]
Diabetes.,5.2. Datasets,0,[0]
This dataset2 contains data on 100000 patients with diabetes.,5.2. Datasets,0,[0]
The task is to predict one of three possible labels regarding hospital readmission after release.,5.2. Datasets,0,[0]
We divide patients into a 80/20 train/test split.,5.2. Datasets,0,[0]
"As this dataset contains real and categorical features, we bin them as in the Cancer dataset.",5.2. Datasets,0,[0]
We obtain a 1704 dimensional binary data point for each entry.,5.2. Datasets,0,[0]
"Our network (selected by cross validation) consists of a fully connected layer 1704→ 10, a batch normalization layer, a SIGN activation function, followed by another fully connected layer 10→ 3, and a batch normalization layer.
",5.2. Datasets,0,[0]
Faces.,5.2. Datasets,0,[0]
The Labeled Faces in the Wild-a dataset contains 13233 gray-scale face images.,5.2. Datasets,0,[0]
We use the binary classification task of gender identification from the images.,5.2. Datasets,0,[0]
We resize the images to size 50 × 50.,5.2. Datasets,0,[0]
"Our network architecture (selected by cross-validation) contains 5 convolutional layers, each of which is followed by a batch normalization layer and a SIGN activation function (except the last which has no activation).",5.2. Datasets,0,[0]
All convolutional layers have unit stride and filter dimensions 10 × 10.,5.2. Datasets,0,[0]
All layers except the last layer have 32 output channels (the last has a single output channel).,5.2. Datasets,0,[0]
"The output is flattened and passed through a fully connected layer 25→ 1 and a batch normalization layer.
",5.2. Datasets,0,[0]
1https://tinyurl.com/gl3yhzb 2https://tinyurl.com/m6upj7y,5.2. Datasets,0,[0]
"in Table 2 (computed with Intel Xeon CPUs @ 2.40GHz, processor number E5-2673V3).",5.3. Timing,0,[0]
"We notice that without parallelization over BNN outputs, the predictions on datasets which use fully connected layers: Cancer and Diabetes, finish within seconds or minutes.",5.3. Timing,0,[0]
"While the for the datasets that use convolutional layers: Faces and MNIST, predictions require multiple days.",5.3. Timing,0,[0]
The +1-trick cuts the time of MNIST prediction by half and reduces the time of Faces prediction by 200 hours.,5.3. Timing,0,[0]
"With only a bit of parallelism over outputs (Out 16-Parallel) prediction on the Faces dataset now requires less than 1.5 days and MNIST can be done
3https://tinyurl.com/yc8d79oe
in 2 hours.",5.3. Timing,0,[0]
With complete parallelism (Out N-Parallel) all methods reduce to under 2 hours.,5.3. Timing,0,[0]
We wanted to ensure that BNNs can still achieve similar test set accuracies to floating point networks.,5.4. Accuracy,0,[0]
"To do so, for each dataset we construct similar floating point networks.",5.4. Accuracy,0,[0]
"For the Cancer dataset we use the same network except we use the original 30 real-valued features, so the fully connected layer is 30 → 1, as was used in Meehan et al. (2018).",5.4. Accuracy,0,[0]
"For Diabetes and Faces, just like for our BNNs we cross validate to find the best networks (for Faces: 4 convolutional layers, with filter sizes of 5× 5 and 64 output channels; for Diabetes the best network is the same as used in the BNN).",5.4. Accuracy,0,[0]
"For MNIST we report the accuracy of the best performing method (Wan et al., 2013) as reported4.",5.4. Accuracy,0,[0]
"Additionally, we report the accuracy of the weight-dropping method described in Section 4.",5.4. Accuracy,0,[0]
"The results are shown in
Table 3.",5.4. Accuracy,0,[0]
"We notice that apart from the Faces dataset, the accuracies differ between the floating point networks and BNNs by at most 1.2% (on MNIST).",5.4. Accuracy,0,[0]
The face dataset uses a different network in floating point which seems to be able to exploit the increased precision to increase accuracy by 5.1%.,5.4. Accuracy,0,[0]
We also observe that weight dropping by 10% reduces the accuracy by at most 1.2% (on Faces).,5.4. Accuracy,0,[0]
"Dropping 20% of the weights seem to have small effect on all datasets except Cancer, which has only a single layer and so likely relies more on every individual weight.",5.4. Accuracy,0,[0]
"In this work, we devised a set of techniques that allow for practical Encrypted Prediction as a Service.",6. Conclusion,0,[0]
"In future work, we aim to develop techniques for encrypting non-binary quantized neural networks, and well as design methods for encrypted model training.
4https://tinyurl.com/knn2434",6. Conclusion,0,[0]
The authors would like to thank Nick Barlow and Oliver Strickson for their support in using the SHEEP platform.,Acknowledgments,0,[0]
"AS acknowledges support from The Alan Turing Institute under the Turing Doctoral Studentship grant TU/C/000023. AG, MK, and VK were supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.",Acknowledgments,0,[0]
Machine learning methods are widely used for a variety of prediction problems.,abstractText,0,[0]
Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients.,abstractText,0,[0]
"However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed.",abstractText,0,[0]
Equally important is to minimize the amount of computation and communication required between client and server.,abstractText,0,[0]
"Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations.",abstractText,0,[0]
The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data.,abstractText,0,[0]
"We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.",abstractText,0,[0]
TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 957–967 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
957",text,0,[0]
"Aspect sentiment classification (ASC) is a core problem of sentiment analysis (Liu, 2012).",1 Introduction,0,[0]
"Given an aspect and a sentence containing the aspect, ASC classifies the sentiment polarity expressed in the sentence about the aspect, namely, positive, neutral, or negative.",1 Introduction,0,[0]
"Aspects are also called opinion targets (or simply targets), which are usually product/service features in customer reviews.",1 Introduction,0,[0]
"In this paper, we use aspect and target interchangeably.",1 Introduction,0,[0]
"In practice, aspects can be specified by the user or extracted automatically using an aspect extraction technique (Liu, 2012).",1 Introduction,0,[0]
"In this work, we assume the aspect terms are given and only focus on the classification task.
",1 Introduction,0,[0]
"Due to their impressive results in many NLP tasks (Deng et al., 2014), neural networks have been applied to ASC (see the survey (Zhang et al., 2018)).",1 Introduction,0,[0]
"Memory networks (MNs), a type of neural networks which were first proposed for question answering (Weston et al., 2015; Sukhbaatar et al., 2015), have achieved the state-of-the-art results in ASC (Tang et al., 2016).",1 Introduction,0,[0]
A key factor for their success is the attention mechanism.,1 Introduction,0,[0]
"However, we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling cannot solve it.",1 Introduction,0,[0]
"That is, their performance degrades when the sentiment of a context word is sensitive to the given target.
",1 Introduction,0,[0]
"Let us consider the following sentences:
(1)",1 Introduction,0,[0]
The screen resolution is excellent but the price is ridiculous.,1 Introduction,0,[0]
(2) The screen resolution is excellent but the price is high.,1 Introduction,0,[0]
(3) The price is high.,1 Introduction,0,[0]
"(4) The screen resolution is high.
",1 Introduction,0,[0]
"In sentence (1), the sentiment expressed on aspect screen resolution (or resolution for short) is positive, whereas the sentiment on aspect price is negative.",1 Introduction,0,[0]
"For the sake of predicting correct sentiment, a crucial step is to first detect the sentiment context about the given aspect/target.",1 Introduction,0,[0]
We call this step targeted-context detection.,1 Introduction,0,[0]
Memory networks (MNs) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs.,1 Introduction,0,[0]
"Concretely, in sentence (1) the word “excellent” can be identified as the sentiment context when resolution is specified.",1 Introduction,0,[0]
"Likewise, the context word “ridiculous” will be placed with a high attention when price is the target.",1 Introduction,0,[0]
"With the correct targeted-context detected, a trained MN, which recognizes “excellent” as positive sentiment and “ridiculous” as negative sentiment, will infer correct sentiment polarity for the given target.",1 Introduction,0,[0]
"This
is relatively easy as “excellent” and “ridiculous” are both target-independent sentiment words, i.e., the words themselves already indicate clear sentiments.
",1 Introduction,0,[0]
"As illustrated above, the attention mechanism addressing the targeted-context detection problem is very useful for ASC, and it helps classify many sentences like sentence (1) accurately.",1 Introduction,0,[0]
This also led to existing and potential research in improving attention modeling (discussed in Section 5).,1 Introduction,0,[0]
"However, we observed that simply focusing on tackling the target-context detection problem and learning better attention are not sufficient to solve the problem found in sentences (2), (3) and (4).
",1 Introduction,0,[0]
Sentence (2) is similar to sentence (1) except that the (sentiment) context modifying aspect/target price is “high”.,1 Introduction,0,[0]
"In this case, when “high” is assigned the correct attention for the aspect price, the model also needs to capture the sentiment interaction between “high” and price in order to identify the correct sentiment polarity.",1 Introduction,0,[0]
This is not as easy as sentence (1) because “high” itself indicates no clear sentiment.,1 Introduction,0,[0]
"Instead, its sentiment polarity is dependent on the given target.
",1 Introduction,0,[0]
"Looking at sentences (3) and (4), we further see the importance of this problem and also why relying on attention mechanism alone is insufficient.",1 Introduction,0,[0]
"In these two sentences, sentiment contexts are both “high” (i.e., same attention), but sentence (3) is negative and sentence (4) is positive simply because their target aspects are different.",1 Introduction,0,[0]
"Therefore, focusing on improving attention will not help in these cases.",1 Introduction,0,[0]
"We will give a theoretical insight about this problem with MNs in Section 3.
",1 Introduction,0,[0]
"In this work, we aim to solve this problem.",1 Introduction,0,[0]
"To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence (1), we refer to the problem in (2), (3) and (4) as the target-sensitive sentiment (or target-dependent sentiment) problem, which means that the sentiment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike “excellent” and “ridiculous”.",1 Introduction,0,[0]
"To address this problem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interaction between targets and contexts.",1 Introduction,0,[0]
We present several approaches to implementing TMNs and experimentally evaluate their effectiveness.,1 Introduction,0,[0]
"This section describes our basic memory network for ASC, also as a background knowledge.",2 Memory Network for ASC,0,[0]
"It does not include the proposed target-sensitive sentiment solutions, which are introduced in Section 4.",2 Memory Network for ASC,0,[0]
"The model design follows previous studies (Sukhbaatar et al., 2015; Tang et al., 2016) except that a different attention alignment function is used (shown in Eq. 1).",2 Memory Network for ASC,0,[0]
Their original models will be compared in our experiments as well.,2 Memory Network for ASC,0,[0]
"The definitions of related notations are given in Table 1.
",2 Memory Network for ASC,0,[0]
Input Representation:,2 Memory Network for ASC,0,[0]
"Given a target aspect t, an embedding matrix A is used to convert t into a vector representation, vt (vt = At).",2 Memory Network for ASC,0,[0]
"Similarly, each context word (non-aspect word in a sentence) xi ∈ {x1, x2, ...xn} is also projected to the continuous space stored in memory, denoted by mi (mi = Axi) ∈ {m1,m2, ...mn}.",2 Memory Network for ASC,0,[0]
Here n is the number of words in a sentence and i is the word position/index.,2 Memory Network for ASC,0,[0]
Both t and xi are one-hot vectors.,2 Memory Network for ASC,0,[0]
"For an aspect expression with multiple words, its aspect representation vt is the averaged vector of those words (Tang et al., 2016).
",2 Memory Network for ASC,0,[0]
Attention:,2 Memory Network for ASC,0,[0]
Attention can be obtained based on the above input representation.,2 Memory Network for ASC,0,[0]
"Specifically, an attention weight αi for the context word xi is computed based on the alignment function:
αi = softmax(v T t Mmi) (1)
where M ∈ Rd×d is the general learning matrix suggested by Luong et al. (2015).",2 Memory Network for ASC,0,[0]
"In this manner, attention α = {α1, α2, ..αn} is represented as a vector of probabilities, indicating the weight/importance of context words towards a given target.",2 Memory Network for ASC,0,[0]
Note that αi ∈,2 Memory Network for ASC,0,[0]
"(0, 1) and
∑ i αi = 1.
",2 Memory Network for ASC,0,[0]
Output Representation: Another embedding matrixC is used for generating the individual (output) continuous vector ci (ci = Cxi) for each context word xi.,2 Memory Network for ASC,0,[0]
"A final response/output vector o is produced by summing over these vectors weighted with the attention α, i.e., o =
∑ i αici.
",2 Memory Network for ASC,0,[0]
"Sentiment Score (or Logit): The aspect sentiment scores (also called logits) for positive, neutral, and negative classes are then calculated, where a sentiment-specific weight matrix W ∈ RK×d is used.",2 Memory Network for ASC,0,[0]
"The sentiment scores are represented in a vector s ∈ RK×1, whereK is the number of (sentiment) classes, which is 3 in ASC.
s =W (o+ vt) (2)
",2 Memory Network for ASC,0,[0]
"The final sentiment probability y is produced with a softmax operation, i.e., y = softmax(s).",2 Memory Network for ASC,0,[0]
This section analyzes the problem of targetsensitive sentiment in the above model.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
The analysis can be generalized to many existing MNs as long as their improvements are on attention α only.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"We first expand the sentiment score calculation from Eq. 2 to its individual terms:
s =W (o+ vt) =W ( ∑ i αici + vt)
",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"= α1Wc1 + α2Wc2 + ...αnWcn +Wvt
(3)
where “+” denotes element-wise summation.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"In Eq. 3, αiWci can be viewed as the individual sentiment logit for a context word and Wvt is the sentiment logit of an aspect.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
They are linearly combined to determine the final sentiment score s. This can be problematic in ASC.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"First, an aspect word often expresses no sentiment, for example, “screen”.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"However, if the aspect term vt is simply removed from Eq. 3, it also causes the problem that the model cannot handle target-dependent sentiment.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"For instance, the sentences (3) and (4) in Section 1 will then be treated as identical if their aspect words are not considered.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"Second, if an aspect word is considered and it directly bears some positive or negative sentiment, then when an aspect word occurs with different context words for expressing opposite sentiments, a contradiction can be resulted from them, especially in the case that the context word is a target-sensitive sentiment word.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"We explain it as follows.
",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
Let us say we have two target words price and resolution (denoted as p and r).,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
We also have two possible context words “high” and “low” (denoted as h and l).,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"As these two sentiment words can modify both aspects, we can construct four snippets “high price”, “low price”, “high resolution” and “low resolution”.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"Their sentiments are negative, positive, positive, and negative respectively.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
Let us set W to R1×d so that s becomes a 1-dimensional sentiment score indicator.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
s > 0 indicates a positive sentiment and s < 0 indicates a negative sentiment.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"Based on the above example snippets or phrases we have four corresponding inequalities: (a) W (αhch + vp) < 0, (b) W (αlcl+ vp) > 0, (c) W (αhch+ vr) > 0",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
and (d) W (αlcl + vr) < 0.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"We can drop all α terms here as they all equal to 1, i.e., they are the only context word in the snippets to attend to (the target words are not contexts).",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
From (a) and (b) we can infer (e),3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
Wch < −Wvp,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
<,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
Wcl.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
From (c) and (d) we can infer (f),3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
Wcl < −Wvr < Wch.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
From (e) and (f) we have (g),3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"Wch < Wcl < Wch, which is a contradiction.
",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
This contradiction means that MNs cannot learn a set of parameters W and C to correctly classify the above four snippets/sentences at the same time.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
This contradiction also generalizes to realworld sentences.,3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"That is, although real-world review sentences are usually longer and contain more words, since the attention mechanism makes MNs focus on the most important sentiment context (the context with high αi scores), the problem is essentially the same.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"For example, in sentences (2) and (3) in Section 1, when price is targeted, the main attention will be placed on “high”.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"For MNs, these situations are nearly the same as that for classifying the snippet “high price”.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"We will also show real examples in the experiment section.
",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"One may then ask whether improving attention can help address the problem, as αi can affect the final results by adjusting the sentiment effect of the context word via αiWci.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"This is unlikely, if not impossible.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"First, notice that αi is a scalar ranging in (0,1), which means it essentially assigns higher or lower weight to increase or decrease the sentiment effect of a context word.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"It cannot change the intrinsic sentiment orientation/polarity of the context, which is determined by Wci.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"For example, if Wci assigns the context word “high” a positive sentiment (Wci > 0), αi will not make it negative (i.e., αiWci < 0 cannot be achieved by chang-
ing αi).",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"Second, other irrelevant/unimportant context words often carry no or little sentiment information, so increasing or decreasing their weights does not help.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"For example, in the sentence “the price is high”, adjusting the weights of context words “the” and “is” will neither help solve the problem nor be intuitive to do so.",3 Problem of the above Model for Target-Sensitive Sentiment,0,[0]
"This section introduces six (6) alternative targetsensitive memory networks (TMNs), which all can deal with the target-sensitive sentiment problem.",4 The Proposed Approaches,0,[0]
"Each of them has its characteristics.
",4 The Proposed Approaches,0,[0]
Non-linear Projection (NP): This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context.,4 The Proposed Approaches,0,[0]
"Instead of directly following the common linear combination as shown in Eq. 3, we use a non-linear projection (tanh) as the replacement to calculate the aspect-specific sentiment score.
",4 The Proposed Approaches,0,[0]
s =W · tanh( ∑,4 The Proposed Approaches,0,[0]
"i αici + vt) (4)
",4 The Proposed Approaches,0,[0]
"As shown in Eq. 4, by applying a non-linear projection over attention-weighted ci and vt, the context and aspect information are coupled in a way that the final sentiment score cannot be obtained by simply summing their individual contributions (compared with Eq. 3).",4 The Proposed Approaches,0,[0]
This technique is also intuitive in neural networks.,4 The Proposed Approaches,0,[0]
"However, notice that by using the non-linear projection (or adding more sophisticated hidden layers) over them in this way, we sacrifice some interpretability.",4 The Proposed Approaches,0,[0]
"For example, we may have difficulty in tracking how each individual context word (ci) affects the final sentiment score s, as all context and target representations are coupled.",4 The Proposed Approaches,0,[0]
"To avoid this, we can use the following five alternative techniques.
",4 The Proposed Approaches,0,[0]
"Contextual Non-linear Projection (CNP): Despite the fact that it also uses the non-linear projection, this approach incorporates the interplay between a context word and the given target into its (output) context representation.",4 The Proposed Approaches,0,[0]
"We thus name it Contextual Non-linear Projection (CNP).
",4 The Proposed Approaches,0,[0]
s,4 The Proposed Approaches,0,[0]
"=W ∑ i αi · tanh(ci + vt) (5)
",4 The Proposed Approaches,0,[0]
"From Eq. 5, we can see that this approach can keep the linearity of attention-weighted context aggregation while taking into account the aspect information with non-linear projection, which works
in a different way compared to NP.",4 The Proposed Approaches,0,[0]
"If we define c̃i = tanh(ci + vt), c̃i can be viewed as the target-aware context representation of context xi and the final sentiment score is calculated based on the aggregation of such c̃i.",4 The Proposed Approaches,0,[0]
"This could be a more reasonable way to carry the aspect information rather than simply summing the aspect representation (Eq. 3).
",4 The Proposed Approaches,0,[0]
"However, one potential disadvantage is that this setting uses the same set of vector representations (learned by embeddings C) for multiple purposes, i.e., to learn output (context) representations and to capture the interplay between contexts and aspects.",4 The Proposed Approaches,0,[0]
"This may degenerate its model performance when the computational layers in memory networks (called “hops”) are deep, because too much information is required to be encoded in such cases and a sole set of vectors may fail to capture all of it.
",4 The Proposed Approaches,0,[0]
"To overcome this, we suggest the involvement of an additional new set of embeddings/vectors, which is exclusively designed for modeling the sentiment interaction between an aspect and its context.",4 The Proposed Approaches,0,[0]
"The key idea is to decouple different functioning components with different representations, but still make them work jointly.",4 The Proposed Approaches,0,[0]
"The following four techniques are based on this idea.
",4 The Proposed Approaches,0,[0]
Interaction Term (IT): The third approach is to formulate explicit target-context sentiment interaction terms.,4 The Proposed Approaches,0,[0]
"Different from the targeted-context detection problem which is captured by attention (discussed in Section 1), here the targetcontext sentiment (TCS) interaction measures the sentiment-oriented interaction effect between targets and contexts, which we refer to as TCS interaction (or sentiment interaction) for short in the rest of this paper.",4 The Proposed Approaches,0,[0]
"Such sentiment interaction is captured by a new set of vectors, and we thus also call such vectors TCS vectors.
s = ∑ i αi(Wsci + wI〈di, dt〉) (6)
",4 The Proposed Approaches,0,[0]
"In Eq. 6, Ws ∈ RK×d and wI ∈ RK×1 are used instead of W in Eq. 3.",4 The Proposed Approaches,0,[0]
Ws models the direct sentiment effect from ci while wI works with di and dt together for learning the TCS interaction.,4 The Proposed Approaches,0,[0]
"di and dt are TCS vector representations of context xi and aspect t, produced from a new embedding matrix D,",4 The Proposed Approaches,0,[0]
"i.e., di = Dxi, dt = Dt (D ∈ Rd×V and di, dt ∈ Rd×1).
",4 The Proposed Approaches,0,[0]
"Unlike input and output embeddings A and C, D is designed to capture the sentiment interac-
tion.",4 The Proposed Approaches,0,[0]
"The vectors fromD affect the final sentiment score through wI〈di, dt〉, where wI is a sentimentspecific vector and 〈di, dt〉 ∈ R denotes the dot product of the two TCS vectors di and dt.",4 The Proposed Approaches,0,[0]
"Compared to the basic MNs, this model can better capture target-sensitive sentiment because the interactions between a context word h and different aspect words (say, p and r) can be different, i.e., 〈dh, dp〉 6=",4 The Proposed Approaches,0,[0]
"〈dh, dr〉.
",4 The Proposed Approaches,0,[0]
The key advantage is that now the sentiment effect is explicitly dependent on its target and context.,4 The Proposed Approaches,0,[0]
"For example, 〈dh, dp〉 can help shift the final sentiment to negative and 〈dh, dr〉 can help shift it to positive.",4 The Proposed Approaches,0,[0]
Note that α is still needed to control the importance of different contexts.,4 The Proposed Approaches,0,[0]
"In this manner, targeted-context detection (attention) and TCS interaction are jointly modeled and work together for sentiment inference.",4 The Proposed Approaches,0,[0]
The proposed techniques introduced below also follow this core idea but with different implementations or properties.,4 The Proposed Approaches,0,[0]
"We thus will not repeat similar discussions.
",4 The Proposed Approaches,0,[0]
Coupled Interaction (CI): This proposed technique associates the TCS interaction with an additional set of context representation.,4 The Proposed Approaches,0,[0]
"This representation is for capturing the global correlation between context and different sentiment classes.
",4 The Proposed Approaches,0,[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ei) (7)
",4 The Proposed Approaches,0,[0]
"Specifically, ei is another output representation for xi, which is coupled with the sentiment interaction factor 〈di, dt〉.",4 The Proposed Approaches,0,[0]
"For each context word xi, ei is generated as ei = Exi whereE ∈ Rd×V is an embedding matrix.",4 The Proposed Approaches,0,[0]
"〈di, dt〉 and ei function together as a target-sensitive context vector and are used to produce sentiment scores with WI (WI ∈ RK×d).
",4 The Proposed Approaches,0,[0]
"Joint Coupled Interaction (JCI): A natural variant of the above model is to replace ei with ci, which means to learn a joint output representation.",4 The Proposed Approaches,0,[0]
"This can also reduce the number of learning parameters and simplify the CI model.
",4 The Proposed Approaches,0,[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ci) (8)
Joint Projected Interaction (JPI): This model also employs a unified output representation like JCI, but a context output vector ci will be projected to two different continuous spaces before sentiment score calculation.",4 The Proposed Approaches,0,[0]
"To achieve the goal, two projection matrices W1, W2 and the non-linear projection function tanh are used.",4 The Proposed Approaches,0,[0]
"The intuition is
that, when we want to reduce the (embedding) parameters and still learn a joint representation, two different sentiment effects need to be separated in different vector spaces.",4 The Proposed Approaches,1,"['However, the literature is severely lacking in algorithm-independent lower bounds, and without these, it is impossible to know to what extent the upper bounds, including (2), can be improved.']"
"The two sentiment effects are modeled as two terms:
s = ∑ i αiWJ tanh(W1ci)
+ ∑",4 The Proposed Approaches,0,[0]
"i αiWJ〈di, dt〉 tanh(W2ci) (9)
where the first term can be viewed as learning target-independent sentiment effect while the second term captures the TCS interaction.",4 The Proposed Approaches,0,[0]
"A joint sentiment-specific weight matrix WJ(WJ ∈ RK×d) is used to control/balance the interplay between these two effects.
",4 The Proposed Approaches,0,[0]
"Discussions: (a) In IT, CI, JCI, and JPI, their first-order terms are still needed, because not in all cases sentiment inference needs TCS interaction.",4 The Proposed Approaches,0,[0]
"For some simple examples like “the battery is good”, the context word “good” simply indicates clear sentiment, which can be captured by their first-order term.",4 The Proposed Approaches,0,[0]
"However, notice that the modeling of second-order terms offers additional help in both general and target-sensitive scenarios.",4 The Proposed Approaches,0,[0]
(b) TCS interaction can be calculated by other modeling functions.,4 The Proposed Approaches,0,[0]
"We have tried several methods and found that using the dot product 〈di, dt〉 or dTi Wdt (with a projection matrix W ) generally produces good results.",4 The Proposed Approaches,0,[0]
"(c) One may ask whether we can use fewer embeddings or just use one universal embedding to replace A, C and D (the definition of D can be found in the introduction of IT).",4 The Proposed Approaches,0,[0]
We have investigated them as well.,4 The Proposed Approaches,0,[0]
We found that merging A and C is basically workable.,4 The Proposed Approaches,0,[0]
But merging D and A/C produces poor results because they essentially function with different purposes.,4 The Proposed Approaches,0,[0]
"While A and C handle targeted-context detection (attention), D captures the TCS interaction.",4 The Proposed Approaches,0,[0]
(d),4 The Proposed Approaches,0,[0]
"Except NP, we do not apply non-linear projection to the sentiment score layer.",4 The Proposed Approaches,0,[0]
"Although adding non-linear transformation to it may further improve model performance, the individual sentiment effect from each context will become untraceable, i.e., losing some interpretability.",4 The Proposed Approaches,0,[0]
"In order to show the effectiveness of learning TCS interaction and for analysis purpose, we do not use it in this work.",4 The Proposed Approaches,0,[0]
"But it can be flexibly added for specific tasks/analyses that do not require strong interpretability.
",4 The Proposed Approaches,0,[0]
Loss function: The proposed models are all trained in an end-to-end manner by minimizing the cross entropy loss.,4 The Proposed Approaches,0,[0]
"Let us denote a sentence and a
target aspect as x and t respectively.",4 The Proposed Approaches,0,[0]
"They appear together in a pair format (x, t) as input and all such pairs construct the dataset H .",4 The Proposed Approaches,0,[0]
"g(x,t) is a one-hot vector and gk(x,t) ∈ {0, 1} denotes a gold sentiment label, i.e., whether (x, t) shows sentiment k. yx,t is the model-predicted sentiment distribution for (x, t).",4 The Proposed Approaches,0,[0]
"ykx,t denotes its probability in class k.",4 The Proposed Approaches,0,[0]
"Based on them, the training loss is constructed as:
loss = − ∑
(x,t)∈H ∑ k∈K gk(x,t) log y k",4 The Proposed Approaches,0,[0]
"(x,t) (10)",4 The Proposed Approaches,0,[0]
"Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)).",5 Related Work,0,[0]
"Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC.
Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC.",5 Related Work,0,[0]
Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance.,5 Related Work,0,[0]
"Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016).
",5 Related Work,0,[0]
"As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem.",5 Related Work,0,[0]
"Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017).",5 Related Work,0,[0]
Chen et al. (2017) proposed to use a recurrent attention mechanism.,5 Related Work,0,[0]
"Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words.",5 Related Work,0,[0]
"Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target.",5 Related Work,0,[0]
Notice that our work does not lie in this direction.,5 Related Work,0,[0]
"Our goal is to solve the target-sensitive sen-
timent and to capture the TCS interaction, which is a different problem.",5 Related Work,0,[0]
"This direction is also finergrained, and none of the above works addresses this problem.",5 Related Work,0,[0]
"Certainly, both directions can improve the ASC task.",5 Related Work,0,[0]
"We will also show in our experiments that our work can be integrated with an improved attention mechanism.
",5 Related Work,0,[0]
"To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting.",5 Related Work,0,[0]
"Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012).",5 Related Work,0,[0]
"Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013).",5 Related Work,0,[0]
"We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al., 2014), which contain online reviews from domain Laptop and Restaurant.",6 Experiments,0,[0]
"In these datasets, aspect sentiment polarities are labeled.",6 Experiments,0,[0]
The training and test sets have also been provided.,6 Experiments,0,[0]
Full statistics of the datasets are given in Table 2.,6 Experiments,0,[0]
MN:,6.1 Candidate Models for Comparison,0,[0]
"The classic end-to-end memory network (Sukhbaatar et al., 2015).",6.1 Candidate Models for Comparison,0,[0]
"AMN: A state-of-the-art memory network used for ASC (Tang et al., 2016).",6.1 Candidate Models for Comparison,0,[0]
"The main difference from MN is in its attention alignment function, which concatenates the distributed representations of the context and aspect, and uses an additional weight matrix for attention calculation, following the method introduced in (Bahdanau et al., 2015).",6.1 Candidate Models for Comparison,0,[0]
"BL-MN: Our basic memory network presented in Section 2, which does not use the proposed techniques for capturing target-sensitive sentiments.",6.1 Candidate Models for Comparison,0,[0]
AE-LSTM: RNN/LSTM is another popular attention based neural model.,6.1 Candidate Models for Comparison,0,[0]
"Here we compare
with a state-of-the-art attention-based LSTM for ASC, AE-LSTM (Wang et al., 2016).",6.1 Candidate Models for Comparison,0,[0]
ATAE-LSTM:,6.1 Candidate Models for Comparison,0,[0]
"Another attention-based LSTM for ASC reported in (Wang et al., 2016).",6.1 Candidate Models for Comparison,0,[0]
"Target-sensitive Memory Networks (TMNs): The six proposed techniques, NP, CNP, IT, CI, JCI, and JPI give six target-sensitive memory networks.
",6.1 Candidate Models for Comparison,0,[0]
"Note that other non-neural network based models like SVM and neural models without attention mechanism like traditional LSTMs have been compared and reported with inferior performance in the ASC task (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016), so they are excluded from comparisons here.",6.1 Candidate Models for Comparison,0,[0]
"Also, note that non-neural models like SVMs require feature engineering to manually encode aspect information, while this work aims to improve the aspect representation learning based approaches.",6.1 Candidate Models for Comparison,0,[0]
"Since we have a three-class classification task (positive, negative and neutral) and the classes are imbalanced as shown in Table 2, we use F1-score as our evaluation measure.",6.2 Evaluation Measure,0,[0]
We report both F1Macro over all classes and all individual classbased F1 scores.,6.2 Evaluation Measure,0,[0]
"As our problem requires finegrained sentiment interaction, the class-based F1 provides more indicative information.",6.2 Evaluation Measure,0,[0]
"In addition, we report the accuracy (same as F1-Micro), as it is used in previous studies.",6.2 Evaluation Measure,0,[0]
"However, we suggest using F1-score because accuracy biases towards the majority class.",6.2 Evaluation Measure,0,[0]
We use the open-domain word embeddings1 for the initialization of word vectors.,6.3 Training Details,0,[0]
"We initialize other model parameters from a uniform distribution U (-0.05, 0.05).",6.3 Training Details,0,[0]
The dimension of the word embedding and the size of the hidden layers are 300.,6.3 Training Details,0,[0]
The learning rate is set to 0.01 and the dropout rate is set to 0.1.,6.3 Training Details,0,[0]
Stochastic gradient descent is used as our optimizer.,6.3 Training Details,0,[0]
"The position encoding is also used (Tang et al., 2016).",6.3 Training Details,0,[0]
"We also compare the memory networks in their multiple computational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the mentioned previous studies.",6.3 Training Details,0,[0]
"We implemented all models in the TensorFlow environment using same input, embedding size, dropout rate, optimizer, etc.
",6.3 Training Details,0,[0]
"1https://github.com/mmihaltz/word2vec-GoogleNewsvectors
so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from elsewhere.",6.3 Training Details,0,[0]
"Meanwhile, we can also report all evaluation measures discussed above2.",6.3 Training Details,0,[0]
10% of the training data is used as the development set.,6.3 Training Details,0,[0]
We report the best results for all models based on their F-1 Macro scores.,6.3 Training Details,0,[0]
The classification results are shown in Table 3.,6.3.1 Result Analysis,0,[0]
"Note that the candidate models are all based on classic/standard attention mechanism, i.e., without sophisticated or multiple attentions involved.",6.3.1 Result Analysis,0,[0]
We compare the 1-hop and 3-hop memory networks as two different settings.,6.3.1 Result Analysis,0,[0]
The top three F1-Macro scores are marked in bold.,6.3.1 Result Analysis,0,[0]
"Based on them, we have the following observations:
1.",6.3.1 Result Analysis,0,[0]
"Comparing the 1-hop memory networks (first nine rows), we see significant performance gains achieved by CNP, CI, JCI, and JPI on both datasets, where each of them has p < 0.01 over the strongest baseline (BL-MN) from paired t-test using F1-Macro.",6.3.1 Result Analysis,0,[0]
IT also outperforms the other baselines while NP has similar performance to BL-MN.,6.3.1 Result Analysis,0,[0]
"This indicates that TCS interaction is very useful, as BL-MN and NP do not model it.",6.3.1 Result Analysis,0,[0]
2,6.3.1 Result Analysis,0,[0]
"In the 3-hop setting, TMNs achieve much better results on Restaurant.",6.3.1 Result Analysis,0,[0]
"JCI, IT, and CI achieve the best scores, outperforming the strongest baseline AMN by 2.38%, 2.18%, and 2.03%.",6.3.1 Result Analysis,0,[0]
"On Laptop, BL-MN and most TMNs (except CNP and JPI) perform similarly.",6.3.1 Result Analysis,0,[0]
"However, BL-MN performs poorly on Restaurant (only better than two models) while TMNs show more stable performance.",6.3.1 Result Analysis,0,[0]
3,6.3.1 Result Analysis,0,[0]
"Comparing all TMNs, we see that JCI works the best as it always obtains the top-three scores on two datasets and in two settings.",6.3.1 Result Analysis,0,[0]
CI and JPI also perform well in most cases.,6.3.1 Result Analysis,0,[0]
"IT, NP, and CNP can achieve very good scores in some cases but are less stable.",6.3.1 Result Analysis,0,[0]
We also analyzed their potential issues in Section 4. 4.,6.3.1 Result Analysis,0,[0]
It is important to note that these improvements are quite large because in many cases sentiment interactions may not be necessary (like sentence (1) in Section 1).,6.3.1 Result Analysis,0,[0]
"The overall good results obtained by TMNs demonstrate their capability of handling both general and target-sensitive sentiments, i.e., the proposed
2Most related studies report accuracy only.
",6.3.1 Result Analysis,0,[0]
techniques do not bring harm while capturing additional target-sensitive signals.,6.3.1 Result Analysis,0,[0]
5,6.3.1 Result Analysis,0,[0]
"Micro-F1/accuracy is greatly affected by the majority class, as we can see the scores from Pos. and Micro are very consistent.",6.3.1 Result Analysis,0,[0]
"TMNs, in fact, effectively improve the minority classes, which are reflected in Neg.",6.3.1 Result Analysis,0,[0]
"and Neu., for example, JCI improves BL-MN by 3.78% in Neg.",6.3.1 Result Analysis,0,[0]
on Restaurant,6.3.1 Result Analysis,0,[0]
.,6.3.1 Result Analysis,0,[0]
This indicates their usefulness of capturing fine-grained sentiment signals.,6.3.1 Result Analysis,0,[0]
"We will give qualitative examples in next section to show their modeling superiority for identifying target-sensitive sentiments.
Integration with Improved Attention: As discussed, the goal of this work is not for learning better attention but addressing the targetsensitive sentiment.",6.3.1 Result Analysis,0,[0]
"In fact, solely improving attention does not solve our problem (see Sections 1 and 3).",6.3.1 Result Analysis,0,[0]
"However, better attention can certainly help achieve an overall better performance for the ASC task, as it makes the targeted-context detection more accurate.",6.3.1 Result Analysis,0,[0]
"Here we integrate our pro-
posed technique JCI with a state-of-the-art sophisticated attention mechanism, namely, the recurrent attention framework, which involves multiple attentions learned iteratively (Kumar et al., 2016; Chen et al., 2017).",6.3.1 Result Analysis,0,[0]
We name our model with this integration as Target-sensitive Recurrent-attention Memory Network (TRMN) and the basic memory network with the recurrent attention as Recurrentattention Memory Network (RMN).,6.3.1 Result Analysis,0,[0]
Their results are given in Table 4.,6.3.1 Result Analysis,0,[0]
TRMN achieves significant performance gain with p < 0.05 in paired t-test.,6.3.1 Result Analysis,0,[0]
"We now give some real examples to show the effectiveness of modeling TCS interaction for identifying target-sensitive sentiments, by comparing a regular MN and a TMN.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Specifically, BL-MN and JPI are used.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Other MNs/TMNs have similar performances to BL-MN/JPI qualitatively, so we do not list all of them here.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"For BL-MN and JPI, their sentiment scores of a single context word i are calculated by αiWci (from Eq. 3) and αiWJ tanh(W1ci) + αiWJ〈di, dt〉tanh(W2ci) (from Eq. 9), each of which results in a 3-dimensional vector.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
Illustrative Examples: Table 5 shows two records in Laptop.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"In record 1, to identify the sentiment of target price in the presented sentence, the sentiment interaction between the context word “higher” and the target word price is the key.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"The
specific sentiment scores of the word “higher” towards negative, neutral and positive classes in both models are reported.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
We can see both models accurately assign the highest sentiment scores to the negative class.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"We also observe that in MN the negative score (0.3641) in the 3-dimension vector {0.3641,−0.3275,−0.0750} calculated by αiWci is greater than neutral (−0.3275) and positive (−0.0750) scores.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Notice that αi is always positive (ranging in (0, 1)), so it can be inferred that the first value in vector Wci is greater than the other two values.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Here ci denotes the vector representation of “higher” so we use chigher to highlight it and we have {Wchigher}Negative > {Wchigher}Neutral/Positive as an inference.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"In record 2, the target is resolution and its sentiment is positive in the presented sentence.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Although we have the same context word “higher”, different from record 1, it requires a positive sentiment interaction with the current target.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Looking at the results, we see TMN assigns the highest sentiment score of word “higher” to positive class correctly, whereas MN assigns it to negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
This error is expected if we consider the above inference {Wchigher}Negative > {Wchigher}Neutral/Positive in MN.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
The cause of this unavoidable error is that Wci is not conditioned on the target.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"In contrast, WJ〈di, ·dt〉tanh(W2ci) can change the sentiment polarity with the aspect vector dt encoded.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Other TMNs also achieve it (like WI〈di, dt〉ci in JCI).
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
One may notice that the aspect information (vt) is actually also considered in the form of αiWci+ Wvt in MNs and wonder whether Wvt may help address the problem given different vt.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Let us assume it helps, which means in the above example an MN makes Wvresolution favor the positive class and Wvprice favor the negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"But then we will have trouble when the context word is “lower”, where it requires Wvresolution to favor the negative class and Wvprice to favor the positive class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"This contradiction reflects the theoretical problem discussed in Section 3.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"Other Examples: We also found other interesting target-sensitive sentiment expressions like “large bill” and “large portion”, “small tip” and “small portion” from Restaurant.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
Notice that TMNs can also improve the neutral sentiment (see Table 3).,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"For instance, TMN generates a sentiment score vector of the context “over” for target aspect price: {0.1373, 0.0066, -0.1433} (negative) and for target aspect dinner: {0.0496, 0.0591, - 0.1128} (neutral) accurately.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"But MN produces both negative scores {0.0069, 0.0025, -0.0090} (negative) and {0.0078, 0.0028, -0.0102} (negative) for the two different targets.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
The latter one in MN is incorrect.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,0,[0]
"In this paper, we first introduced the targetsensitive sentiment problem in ASC.",7 Conclusion and Future Work,0,[0]
"After that, we discussed the basic memory network for ASC and analyzed the reason why it is incapable of capturing such sentiment from a theoretical perspective.",7 Conclusion and Future Work,0,[0]
We then presented six techniques to construct target-sensitive memory networks.,7 Conclusion and Future Work,0,[0]
"Finally, we reported the experimental results quantitatively and qualitatively to show their effectiveness.
",7 Conclusion and Future Work,0,[0]
"Since ASC is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sentiment composition, and learning better attention.",7 Conclusion and Future Work,0,[0]
We believe all these can help improve the ASC task.,7 Conclusion and Future Work,0,[0]
"The work presented in this paper lies in the direction of addressing target-sensitive sentiment, and we have demonstrated the usefulness of capturing this signal.",7 Conclusion and Future Work,0,[0]
We believe that there will be more effective solutions coming in the near future.,7 Conclusion and Future Work,0,[0]
This work was partially supported by National Science Foundation (NSF) under grant nos.,Acknowledgments,0,[0]
"IIS1407927 and IIS-1650900, and by Huawei Technologies Co. Ltd with a research gift.",Acknowledgments,0,[0]
Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis.,abstractText,0,[0]
"Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence.",abstractText,0,[0]
Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results.,abstractText,0,[0]
"In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target.",abstractText,0,[0]
"However, we found an important problem with the current MNs in performing the ASC task.",abstractText,0,[0]
Simply improving the attention mechanism will not solve it.,abstractText,0,[0]
"The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone.",abstractText,0,[0]
"To tackle this problem, we propose the targetsensitive memory networks (TMNs).",abstractText,0,[0]
Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.,abstractText,0,[0]
Target-Sensitive Memory Networks for Aspect Sentiment Classification,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1192",text,0,[0]
A language model (LM) defines a probability distribution over sequences of words.,1 Introduction,0,[0]
Recent technological advances have led to an explosion of neural network-based LM architectures.,1 Introduction,0,[0]
"The most popular ones are based on recurrent neural networks (RNNs) (Elman, 1990; Mikolov et al., 2010), in particular Long Short-Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997).",1 Introduction,0,[0]
"While a large number of alternative architectures have been proposed in the past few years, LSTMs are still highly competitive (Melis et al., 2018).
",1 Introduction,0,[0]
Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data.,1 Introduction,0,[0]
"This measure conflates multiple sources of success (or failure) in predicting the next word: common collo-
cations, semantics, pragmatics, syntax, and so on.",1 Introduction,0,[0]
"The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions.
",1 Introduction,0,[0]
We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language.,1 Introduction,0,[0]
"Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one.
",1 Introduction,0,[0]
"The value of this approach can be illustrated with a recent study by Tran et al. (2018), where a standard LSTM language model was compared to an attention-only LM without recurrence (Vaswani et al., 2017).",1 Introduction,0,[0]
"Although the attention-only model had somewhat better perplexity on the validation set, when the models were tested specifically on challenging subject-verb agreement dependencies, the attention-only model made three times as many errors as the LSTM.",1 Introduction,0,[0]
"In other words, the LSTM learned more robust syntactic representations, but this advantage was not reflected in its average perplexity on the corpus, since syntactically challenging sentences are relatively infrequent.
",1 Introduction,0,[0]
"Previous work on targeted syntactic evaluation of language models has identified syntactically challenging sentences in corpora (Linzen et al., 2016; Gulordava et al., 2018).",1 Introduction,0,[0]
"While evaluation on naturally occurring examples is appealing, this approach has its limitations (see Section 2).",1 Introduction,0,[0]
"In particular, syntactically challenging examples are sparsely represented in a corpus, their identifica-
tion requires a clean parsed corpus, and naturally occurring sentences are difficult to control for confounds.",1 Introduction,0,[0]
"We contrast the naturalistic approach with a constructed dataset, which allows us to examine a much larger range of specific grammatical phenomena than has been possible before.",1 Introduction,0,[0]
"We use templates to automatically create our test sentences, making it possible to generate a large test set while maintaining experimental control over our materials as well as a balanced number of examples of each phenomenon.
",1 Introduction,0,[0]
"We test three LMs on the data set we develop: an n-gram baseline, an RNN LM trained on an unannotated corpus, and an RNN LM trained on a multitask objective: language modeling and Combinatory Categorial Grammar (CCG) supertagging (Bangalore and Joshi, 1999).",1 Introduction,0,[0]
We also conduct a human experiment using the same materials.,1 Introduction,0,[0]
"The n-gram baseline largely performed at chance, suggesting that good performance on the task requires syntactic representations.",1 Introduction,0,[0]
"The RNN LMs performed well on simple cases, but struggled on more complex ones.",1 Introduction,0,[0]
"Multi-task training with a supervised syntactic objective improved the performance of the RNN, but it was still much weaker than humans.",1 Introduction,0,[0]
"This suggests that our data set is challenging, especially when explicit syntactic supervision is not available, and can therefore motivate richer language modeling architectures.",1 Introduction,0,[0]
How should grammaticality be captured in the probability distribution defined by an LM?,2.1 Grammaticality and LM probability,0,[0]
The most extreme position would be that a language model should assign a probability of zero to ungrammatical sentences.,2.1 Grammaticality and LM probability,0,[0]
"For most applications, some degree of error tolerance is desirable, and it is not practical to assign a sentence a probability of exactly zero.1 Following Linzen et al. (2016) and Gulordava et al. (2018), our desideratum for the language model is more modest: if two closely matched sentence differ only in their grammaticality, the probability of the grammatical sentence should be higher than the probability of the ungrammatical one.",2.1 Grammaticality and LM probability,0,[0]
"For example, the following minimal pair illustrates the fact that third-
1Nor is it possible to have a threshold such that all grammatical sentences have probability higher than and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017).
person present English verbs agree with the number of their subject:
(1) Simple agreement:",2.1 Grammaticality and LM probability,0,[0]
a.,2.1 Grammaticality and LM probability,0,[0]
The author laughs.,2.1 Grammaticality and LM probability,0,[0]
b. *,2.1 Grammaticality and LM probability,0,[0]
"The author laugh.
",2.1 Grammaticality and LM probability,0,[0]
We expect the probability of (1a) to be higher than the probability of (1b).,2.1 Grammaticality and LM probability,0,[0]
Previous work has simplified this setting further by comparing the probability that the LM assigns to a single word that is the locus of ungrammaticality.,2.1 Grammaticality and LM probability,0,[0]
"In (1), for example, the LM would be fed the first two words of the sentence, and would be considered successful on the task if it predicts P (laughs) >",2.1 Grammaticality and LM probability,0,[0]
"P (laugh).
",2.1 Grammaticality and LM probability,0,[0]
"The prediction setting is only applicable when the locus of ungrammaticality is a single word, rather than, say, the interaction between two words; moreover, the information needed to make the grammaticality decision needs to be available in the left context of the locus of grammaticality.",2.1 Grammaticality and LM probability,0,[0]
These conditions do not always hold.,2.1 Grammaticality and LM probability,0,[0]
"Negative polarity items (NPIs), for example, are words like any and ever that can only be used in the scope of negation.2 The grammaticality of placing a particular quantifier in the beginning of the sentences in (2) depends on whether the sentence contains an NPI later on:
(2) Simple NPI:",2.1 Grammaticality and LM probability,0,[0]
a. No students have ever lived here.,2.1 Grammaticality and LM probability,0,[0]
b. *,2.1 Grammaticality and LM probability,0,[0]
"Most students have ever lived here.
",2.1 Grammaticality and LM probability,0,[0]
It would not be possible to compare these two sentences using the prediction task.,2.1 Grammaticality and LM probability,0,[0]
"In the current paper, we use the more general setting and compare the probability of the two complete sentences.",2.1 Grammaticality and LM probability,0,[0]
Previous work has used syntactically complex sentences identified from a parsed corpus.,2.2 Data set construction,0,[0]
This approach has several limitations.,2.2 Data set construction,0,[0]
"If the corpus is automatically parsed, the risk of a parse error increases with the complexity of the construction (Bender et al., 2011).",2.2 Data set construction,0,[0]
"If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of examples of syntactically challenging cases.",2.2 Data set construction,0,[0]
"Moreover, using naturally occurring sentences can introduce
2In practice, the conditions that govern the distribution of NPIs are much more complicated, but this first approximation will suffice for the present purposes.",2.2 Data set construction,0,[0]
"For a review, see Giannakidou (2011).
",2.2 Data set construction,0,[0]
"confounds that may complicate the interpretation of the experiments (Ettinger et al., 2018).
",2.2 Data set construction,0,[0]
"To circumvent these issues, we use templates to automatically construct a large number of English sentence pairs (∼350,000).",2.2 Data set construction,0,[0]
"Our data set includes three phenomena that linguists consider to be sensitive to hierarchical syntactic structure (Everaert et al., 2015; Xiang et al., 2009): subjectverb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and negative polarity items (Section 4.4).
",2.2 Data set construction,0,[0]
The templates can be described using nonrecursive context-free grammars.,2.2 Data set construction,0,[0]
We specify the preterminal symbols that make up a syntactic construction and have different terminal symbols that those preterminals could be mapped to.,2.2 Data set construction,0,[0]
"For example, the template for the simple agreement construction illustrated in (1) consists of the following rules:
(3) a. Simple agreement→ D MS MV b. D→ the c. MS→ {author, pilot, . . .}",2.2 Data set construction,0,[0]
"d. MV→ {laughs, smiles, . . .",2.2 Data set construction,0,[0]
"}
We generate all possible combinations of the terminals.",2.2 Data set construction,0,[0]
"The Supplementary Materials provide a full description of all our templates.3
While these examples are somewhat artificial, our goal is to isolate the syntactic capabilities of the model; it is in fact beneficial to minimize the semantic or collocational cues that can be used to identify the grammatical sentence.",2.2 Data set construction,0,[0]
Gulordava et al. took this approach further and constructed “colorless green ideas” test cases by substituting random content words into sentences from a corpus.,2.2 Data set construction,0,[0]
"We take a more moderate position and avoid combinations that are very implausible or violate selectional restrictions (e.g., the apple laughs).",2.2 Data set construction,0,[0]
We do this by having separate templates for animate and inanimate subjects and verbs so that the resulting sentences are always reasonably plausible.,2.2 Data set construction,0,[0]
"Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016).",3 Related work,0,[0]
"Evaluation sets consisting of chal-
3The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval.
",3 Related work,0,[0]
"lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation.
",3 Related work,0,[0]
Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments.,3 Related work,0,[0]
"The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017).",3 Related work,0,[0]
"In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality.",3 Related work,0,[0]
"Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction.",3 Related work,0,[0]
"Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”).
",3 Related work,0,[0]
"Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012).",3 Related work,0,[0]
"While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model.",3 Related work,0,[0]
"This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017).",3 Related work,0,[0]
"Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models.",3 Related work,0,[0]
"This section describes all of the types of sentence pairs included in our data set, which include examples of subject-verb agreement (Sections 4.1 and 4.2), reflexive anaphoras (Section 4.3) and negative polarity items (Section 4.4).",4 Data set composition,0,[0]
"Determining the correct number of the verb is trivial in examples such as (1) above, in which the sentence only contains a single noun.",4.1 Subject-verb agreement,0,[0]
"By contrast, in cases where there are multiple nouns in the sentence, identifying which of them is the subject of a given verb requires understanding the structure of the sentence.",4.1 Subject-verb agreement,0,[0]
"In particular, the relevant subject is not necessarily the first noun of the sentence:
(4) Agreement in a sentential complement:",4.1 Subject-verb agreement,0,[0]
a. The bankers knew the officer smiles.,4.1 Subject-verb agreement,0,[0]
b. *,4.1 Subject-verb agreement,0,[0]
"The bankers knew the officer smile.
",4.1 Subject-verb agreement,0,[0]
Here the verb smiles needs to agree with the embedded subject officer rather than the main clause subject bankers.,4.1 Subject-verb agreement,0,[0]
"The subject is also not necessarily the most recent noun before the verb: when the subject is modified by a phrase, a distracting noun (“attractor”) often intervenes in the linear order of the sentence between the head of the subject and the verb.",4.1 Subject-verb agreement,0,[0]
"Two examples of such modifiers are prepositional phrases and relative clauses (RCs):
(5) Agreement across a prepositional phrase: a. The farmer near the parents smiles.",4.1 Subject-verb agreement,0,[0]
b.,4.1 Subject-verb agreement,0,[0]
*,4.1 Subject-verb agreement,0,[0]
"The farmer near the parents smile.
",4.1 Subject-verb agreement,0,[0]
(6) Agreement across a subject relative clause:,4.1 Subject-verb agreement,0,[0]
a. The officers that love the skater smile.,4.1 Subject-verb agreement,0,[0]
b. *,4.1 Subject-verb agreement,0,[0]
"The officers that love the skater smiles.
",4.1 Subject-verb agreement,0,[0]
"We include all four possible configurations of noun number for each type of minimal pair; for (5), these would be:4
(7) a.",4.1 Subject-verb agreement,0,[0]
The farmer near the parent smiles/*smile.,4.1 Subject-verb agreement,0,[0]
b.,4.1 Subject-verb agreement,0,[0]
The farmer near the parents smiles/*smile.,4.1 Subject-verb agreement,0,[0]
c.,4.1 Subject-verb agreement,0,[0]
The farmers near the parent smile/*smiles.,4.1 Subject-verb agreement,0,[0]
d.,4.1 Subject-verb agreement,0,[0]
"The farmers near the parents
smile/*smiles.
",4.1 Subject-verb agreement,0,[0]
"Sentences where the two nouns conflict in number are expected to be more challenging, but interpretable errors may certainly occur even when they do not.",4.1 Subject-verb agreement,0,[0]
"For example, the model may use the heuristic that sentences with multiple nouns are likely to have a plural verb (a heuristic that
4The slash notation indicates the word that differs between the grammatical and ungrammatical sentence; for example, in (7a), the full sentence pair would be:
(i) a.",4.1 Subject-verb agreement,0,[0]
The farmer near the parent smiles.,4.1 Subject-verb agreement,0,[0]
b.,4.1 Subject-verb agreement,0,[0]
*,4.1 Subject-verb agreement,0,[0]
"The farmer near the parent smile.
would be effective for coordination); alternatively, it might prefer singular verbs to plural ones regardless of whether the subject is singular or plural, simply because the singular form of the verb is more frequent.
",4.1 Subject-verb agreement,0,[0]
"Next, in verb phrase (VP) coordination, both of the verbs need to agree with the subject:
(8) Short VP coordination: a.",4.1 Subject-verb agreement,0,[0]
The senator smiles and laughs.,4.1 Subject-verb agreement,0,[0]
b. *,4.1 Subject-verb agreement,0,[0]
"The senator smiles and laugh.
",4.1 Subject-verb agreement,0,[0]
We had both singular and plural subjects.,4.1 Subject-verb agreement,0,[0]
The number of the verb immediately adjacent to the subject was always grammatical.,4.1 Subject-verb agreement,0,[0]
"This problem can in principle be solved with a trigram model (smiles and laughs is likely to be a more frequent trigram than smiles and laugh); to address this potential concern, we also included a coordination condition with a longer dependency:
(9) Long VP coordination: The manager writes in a journal every day and likes/*like to watch television shows.",4.1 Subject-verb agreement,0,[0]
"We go into greater depth in object relative clauses, which most clearly require a hierarchical representation.",4.2 Agreement and object relative clauses,0,[0]
"In (10) and (11), the model needs to be able to distinguish the embedded subject (parents) from the main clause subject (farmer) when making its predictions:
(10) Agreement across an object relative clause: a. The farmer that the parents love swims.",4.2 Agreement and object relative clauses,0,[0]
b. *,4.2 Agreement and object relative clauses,0,[0]
"The farmer that the parents love swim.
",4.2 Agreement and object relative clauses,0,[0]
(11) Agreement in an object relative clause:,4.2 Agreement and object relative clauses,0,[0]
a. The farmer that the parents love swims.,4.2 Agreement and object relative clauses,0,[0]
b. *,4.2 Agreement and object relative clauses,0,[0]
"The farmer that the parents loves swims.
",4.2 Agreement and object relative clauses,0,[0]
"In keeping with the minimal pair approach, we never introduce two agreement errors at the same time: either the embedded verb or the main verb is incorrectly inflected, but not both.
",4.2 Agreement and object relative clauses,0,[0]
We include a number of variations on the pattern in (11).,4.2 Agreement and object relative clauses,0,[0]
"First, we delete the relativizer that, with the hypothesis that the absence of an overt cue to structure will make the task more difficult:
(12) The farmer the parents love/*loves swims.
",4.2 Agreement and object relative clauses,0,[0]
"In another condition, we replace the main subject with an inanimate noun and keep the embed-
ded subject animate.",4.2 Agreement and object relative clauses,0,[0]
"We base this manipulation on human experimental work showing that similar nouns (for example, two animate nouns) are more likely to cause confusion during comprehension than dissimilar nouns, such as an animate and an inanimate noun (Van Dyke, 2007):
(13) The movies that the author likes are/*is good.
",4.2 Agreement and object relative clauses,0,[0]
"For a complete list of all the types of minimal pairs we include, see the Supplementary Materials.",4.2 Agreement and object relative clauses,0,[0]
A reflexive pronoun such as himself needs to have an antecedent from which it derives its interpretation.,4.3 Reflexive anaphora,0,[0]
"The pronoun needs to agree in number (and gender) with its antecedent:
(14)",4.3 Reflexive anaphora,0,[0]
Simple reflexive:,4.3 Reflexive anaphora,0,[0]
a.,4.3 Reflexive anaphora,0,[0]
The senators embarrassed themselves.,4.3 Reflexive anaphora,0,[0]
b. *,4.3 Reflexive anaphora,0,[0]
"The senators embarrassed herself.
",4.3 Reflexive anaphora,0,[0]
There are structural conditions on the nouns to which a reflexive pronoun can be bound.,4.3 Reflexive anaphora,0,[0]
One of these conditions requires the antecedent to be in the same clause as the reflexive pronoun.,4.3 Reflexive anaphora,0,[0]
"For example, (15b) cannot refer to a context in which the pilot embarrassed the bankers:
(15) Reflexive in a sentential complement: a.",4.3 Reflexive anaphora,0,[0]
"The bankers thought the pilot embar-
rassed himself.",4.3 Reflexive anaphora,0,[0]
b. *,4.3 Reflexive anaphora,0,[0]
"The bankers thought the pilot embar-
rassed themselves.
",4.3 Reflexive anaphora,0,[0]
"Likewise, in the following minimal pair, sentence (16b) is ungrammatical, because the reflexive pronoun themselves, which is part of the main clause, cannot be bound to the noun phrase the architects, which is inside an embedded clause:
(16) Reflexive across an object relative clause:",4.3 Reflexive anaphora,0,[0]
"a. The manager that the architects like
doubted himself.",4.3 Reflexive anaphora,0,[0]
b.,4.3 Reflexive anaphora,0,[0]
*,4.3 Reflexive anaphora,0,[0]
"The manager that the architects like
doubted themselves.",4.3 Reflexive anaphora,0,[0]
"Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation.",4.4 Negative polarity items,0,[0]
"Crucially for the purposes of the present work, the scope of negation is structurally defined.",4.4 Negative polarity items,0,[0]
"In particular
the negative noun phrase needs to c-command the NPI: the syntactic non-terminal node that dominates the negative noun phrase must also dominate the NPI.",4.4 Negative polarity items,0,[0]
"This is the case in (17a), but not in (17b), where the negative noun phrase is too deep in the tree to c-command the NPI ever (Xiang et al., 2009; Everaert et al., 2015).
",4.4 Negative polarity items,0,[0]
(17) NPI across a relative clause:,4.4 Negative polarity items,0,[0]
"a. No authors that the security guards like
have ever been famous.",4.4 Negative polarity items,0,[0]
b. *,4.4 Negative polarity items,0,[0]
"The authors that no security guards like
have ever been famous.
",4.4 Negative polarity items,0,[0]
All of the nouns and verbs in the NPI cases were plural.,4.4 Negative polarity items,0,[0]
"As in some of the agreement cases, we included a variant of (17) in which the subject was inanimate.",4.4 Negative polarity items,0,[0]
"To show how our challenge set can be used to evaluate the syntactic performance of LMs, we trained three LMs with increasing levels of syntactic sophistication.",5 Experimental setup,0,[0]
"All of the LMs were trained on a 90 million word subset of Wikipedia (Gulordava et al., 2018).",5 Experimental setup,0,[0]
Our n-gram LM and LSTM LM do not require annotated data.,5 Experimental setup,0,[0]
"The third model is also an LSTM LM, but it requires syntactically annotated data (CCG supertags).
N-gram model:",5 Experimental setup,0,[0]
"We trained a 5-gram model on the same 90M word corpus using the SRILM toolkit (Stolcke, 2002) which backs off to smaller n-grams using Kneser-Ney smoothing.
",5 Experimental setup,0,[0]
Single-task RNN:,5 Experimental setup,0,[0]
"The RNN LM had two layers of 650 LSTM units, a batch size of 128, a dropout rate of 0.2, and a learning rate of 20.0, and was trained for 40 epochs (following the hyperparameters of Gulordava et al. 2018).
",5 Experimental setup,0,[0]
Multi-task RNN:,5 Experimental setup,0,[0]
"In multi-task learning, the system is trained to optimize an objective function that combines the objective functions of several tasks.",5 Experimental setup,0,[0]
"We combine language modeling with CCG supertagging, a task that predicts for each word in the sentence its CCG supertag (Bangalore and Joshi, 1999; Lewis et al., 2016).",5 Experimental setup,0,[0]
"We simply sum the two objective functions with equal weights (Enguehard et al., 2017).",5 Experimental setup,0,[0]
Early stopping in this model is based on the combined loss on language modeling and supertagging.,5 Experimental setup,0,[0]
"Supertags provide a large amount of syntactic information
about the word; the sequence of supertags of a sentence strongly constrains the possible parses of the sentence.",5 Experimental setup,0,[0]
"We use supertagging as a “scaffold” task (Swayamdipta et al., 2017): our goal is not to produce a competitive supertagger, but to induce better syntactic representations, which would then lead to improved language modeling.",5 Experimental setup,0,[0]
"We used CCG-Bank (Hockenmaier and Steedman, 2007) as our CCG corpus.
Human evaluation: We designed a human experiment on Amazon Mechanical Turk that mirrored the task that was given to the LMs: both versions of a minimal pair were shown on the screen at the same time, and participants were asked to judge which one of them was more acceptable (for details, see the Supplementary Materials).",5 Experimental setup,0,[0]
We emphasize that we do not see human performance on complex syntactic dependencies as setting an upper bound on the performance that we should expect from an LM.,5 Experimental setup,0,[0]
"There is a rich literature showing that humans make mistakes such as subject-verb agreement errors; in fact, most of the phenomena we test were inspired by work in psycholinguistics that studies these errors (Bock and Miller, 1991; Phillips et al., 2011).",5 Experimental setup,0,[0]
"At the same time, while we do not see a reason not to aspire for 100% accuracy, we are interested in comparing LM and human errors: if the errors are similar, the two systems may be using similar representations.",5 Experimental setup,0,[0]
Local agreement: The overall accuracy per condition can be seen in Table 1.,6 Results,0,[0]
"The n-gram LM’s accuracy was only 79% for simple agreement and agreement in a sentential complement, both of which can be solved entirely using local context.",6 Results,0,[0]
"This is because not all subject and verb combinations in our materials appeared verbatim in the 90M word training corpus; for those combinations, the model fell back on unigram probabilities, which in this context amounts to selecting the more frequent form of the verb.
",6 Results,0,[0]
"Both RNNs performed much better than the n-gram model on the simple agreement case (single-task: 94%; multi-task: 100%), reflecting these models’ ability to generalize beyond the specific bigrams that occurred in the corpus.",6 Results,0,[0]
Accuracy on agreement in a sentential complement was also very high (single-task: 99%; multi-task: 93%).,6 Results,0,[0]
"This indicates that the RNNs do not rely on the heuristic whereby the first noun of the sentence
is likely to be its subject.",6 Results,0,[0]
"They did slightly worse but still very well on short VP coordination (both 90%); this dependency is also local, albeit across the word and.
",6 Results,0,[0]
Non-local agreement: The accuracy of the n-gram model on non-local dependencies (long VP coordination and agreement across a phrase or a clause) was very close to 50%.,6 Results,0,[0]
This suggests that local collocational information is not useful in these conditions.,6 Results,0,[0]
"The single-task RNN also performed much more poorly on these conditions than on the local agreement conditions, though for the most part its accuracy was better than chance.",6 Results,0,[0]
"Humans did worse on these dependencies as well, but their accuracy did not drop as sharply as the RNNs’ (human accuracies ranged from 82% to 88%).",6 Results,0,[0]
"In most of these cases, multitask learning was very helpful; for example, accuracy in long VP coordination increased from 61% to 81%.",6 Results,0,[0]
"Still, both RNNs performed poorly on agreement across an object RC, especially without that, whereas humans performed comparably on all non-local dependencies.
",6 Results,0,[0]
"Agreement inside an object RC: This case is particularly interesting, because this dependency is purely local (see (11)), and the interference is from the distant sentence-initial noun.",6 Results,0,[0]
"Although this configuration is similar to the sentential complement case, performance was worse both in RNNs and humans.",6 Results,0,[0]
"However, RNNs performed better than humans, at least when the sentence included the overt relativizer that.",6 Results,0,[0]
"This suggests that interference is sensitive to proximity in RNNs but to syntactic status in humans — humans appear to be confusing the main clause subject and the embedded subject (Wagers et al., 2009).
",6 Results,0,[0]
Reflexive anaphora:,6 Results,0,[0]
"The RNNs’ performance was significantly worse on simple reflexives (83%) than on simple agreement (94%), and did not differ between the single-task and multi-task models.",6 Results,0,[0]
"By contrast, human performance did not differ between subject-verb agreement and reflexive anaphoras.",6 Results,0,[0]
"The surprisingly poor performance for this adjacent dependency seems to be due to an asymmetry in accuracy between himself and themselves on the one hand (100% accuracy in the multi-task RNN) and herself on the other hand (49% accuracy).5 Accuracy was very low for all
5This may be because himself and themselves are significantly more frequent than herself, and consequently the num-
pronouns in the structurally complex case in which the dependency was across a relative clause (55% compared to 87% in humans).
",6 Results,0,[0]
NPIs:,6 Results,0,[0]
"The dependency in simple NPIs spans only four words, so the n-gram model could in principle capture it.",6 Results,0,[0]
"In practice, the n-gram model systematically selected the wrong answer, suggesting that it backed off to comparing the bigrams no students and most students, the first of which is presumably less frequent.",6 Results,0,[0]
"Surprisingly, the n-gram model’s accuracy was higher than 50% on NPIs across a relative clause, a dependency that spans more than five words.",6 Results,0,[0]
"In this case, the bigrams that the and the chef (for example) happen to be more frequent than the that no and no chef.",6 Results,0,[0]
"This difference was apparently strong enough to make up for the low-frequency bigram at the start of the sentence.
",6 Results,0,[0]
The RNNs did poorly on this task.,6 Results,0,[0]
The accuracy of the single-task model was around 40%.,6 Results,0,[0]
The multi-task did somewhat better on the simple NPIs (48%) and much better on the NPIs across a relative clause (73%).,6 Results,0,[0]
"At the same time, an examination of the plot of log probability of each word in a sentence (Figure A.1 in the Supplementary Materials) suggests that the single-task RNN is in
ber representation learned for herself was not robust.",6 Results,0,[0]
"Another possibility is that gender bias reduces the probability of an anaphoric relation between herself and words such as surgeon (Rudinger et al., 2018).
",6 Results,0,[0]
"fact able to differentiate between the grammatical and ungrammatical sentences when it reaches the NPI, but this difference does not offset the overall probability advantage of the ungrammatical sentence (which is likely due to non-grammatical collocational factors).",6 Results,0,[0]
"In any case, the fact that the n-gram baseline did not perform at chance suggests that there are non-syntactic cues to this task, complicating the interpretation of the performance of other LMs.
",6 Results,0,[0]
"Perplexity: The perplexity of the n-gram model on the Wikipedia test data was 157.5, much higher than the perplexity of the single-task RNN (78.65) and the multi-task RNN (61.10).",6 Results,0,[0]
"In other words, perplexity tracked accuracy on our syntactic data set – an unsatisfying outcome given our goal of dissociating perplexity and our syntactic evaluation method, but an expected one given that each model was conditioned on richer information than the previous one.",6 Results,0,[0]
"In previous work, perplexity and syntactic judgment accuracy have been found to be partly dissociable (Kuncoro et al., 2018; Tran et al., 2018).
",6 Results,0,[0]
Lexical variation and frequency: There was considerable lexical variation in the results; we have mentioned the surprising asymmetry between himself and herself above.,6 Results,0,[0]
"As another case study, we examine variation in the results of the simple agreement condition in the single-
task RNN.",6 Results,0,[0]
"Accuracy varied by verb, ranging from is and are, which had 100% accuracy, to swims, where accuracy was only 60% (recall that average accuracy was 94%).",6 Results,0,[0]
"This may be a frequency effect: either the LM is learning less robust number representations for infrequent verbs, or the tail of the distribution over the vocabulary is more fragile during word prediction.",6 Results,0,[0]
Pauls and Klein (2012) propose normalizing for unigram frequency when deriving acceptability judgments from an LM.,6 Results,0,[0]
"Our preliminary experiments with this method did not significantly improve overall performance; regardless of the effectiveness of this method, such corrections should arguably not be necessary in an LM that adequately captures grammaticality.",6 Results,0,[0]
The overall results in Table 1 were averaged over all of the possible number configurations within each condition.,7 Case study: agreement and object relative clauses,0,[0]
"In this section, we take a closer look at agreement in sentences with an object RC (see Table 2).",7 Case study: agreement and object relative clauses,0,[0]
"This kind of finer-grained analysis helps explain the cases in which the LMs are failing, and might reveal some of the patterns or heuristics the LMs are using.
",7 Case study: agreement and object relative clauses,0,[0]
Performance in agreement across an object RC was poor.,7 Case study: agreement and object relative clauses,0,[0]
Both RNNs made attraction errors: they often preferred the verb that agreed in number with the irrelevant embedded subject to the verb that agreed with the correct main subject.,7 Case study: agreement and object relative clauses,0,[0]
"The multitask RNN showed greater symmetry between the simpler singular/singular and plural/plural cases, whereas the single-task RNN performed poorly even in these cases, often preferring a singular
verb when both subjects were plural.",7 Case study: agreement and object relative clauses,0,[0]
"This default preference for singular verbs matches the behavior of younger children (Franck et al., 2004).
",7 Case study: agreement and object relative clauses,0,[0]
"Performance in agreement within an object RC was better; still, the single-task RNN made the most errors when both subjects were singular, perhaps due to a heuristic in which a sentence with multiple subjects is likely to have a plural verb (as in coordination sentences).",7 Case study: agreement and object relative clauses,0,[0]
"By contrast, the multitask model seemed to have a general bias towards singular subjects in this condition.",7 Case study: agreement and object relative clauses,0,[0]
"Incidentally, the human results with object RCs were also unexpected: while attraction errors when the two subjects differ in number are to be expected (Wagers et al., 2009), our participants made a sizable number of errors even when both subjects were plural.
",7 Case study: agreement and object relative clauses,0,[0]
"Despite the generally poor performance in object RCs, Figures A.2 and A.3 in the Supplementary Materials show that the single-task RNN is typically assigning a higher probability to the grammatical word of a minimal pair than to the ungrammatical word.",7 Case study: agreement and object relative clauses,0,[0]
We have described a template-based data set for targeted syntactic evaluation of language models.,8 Discussion,0,[0]
"The data set consists of pairs of sentences that are matched except for their grammaticality; we consider a language model to capture the relevant aspects of the grammar of the language if it assigns a higher probability to the grammatical sentence than to the ungrammatical one.
",8 Discussion,0,[0]
"An RNN language model performed very well on local subject-verb agreement dependencies, significantly outperforming an n-gram baseline.
",8 Discussion,0,[0]
This suggests that the task is a viable evaluation strategy.,8 Discussion,0,[0]
"Even on simple cases, however, the RNN’s accuracy was sensitive to the particular lexical items that occurred in the sentence; this would not be expected if its syntactic representations were fully abstract.",8 Discussion,0,[0]
"The RNN’s performance degraded markedly on non-local dependencies, approaching chance levels on agreement across an object relative clause.",8 Discussion,0,[0]
Multi-task training with a syntactic objective (CCG supertagging) mitigated this drop in performance for some but not all of the dependencies we tested.,8 Discussion,0,[0]
"We conjecture that the benefits of the inductive bias conferred by multi-task learning will be amplified when the amount of training data is limited.
",8 Discussion,0,[0]
"Our results contrast with the results of Gulordava et al. (2018), who obtained a prediction accuracy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus.",8 Discussion,0,[0]
"It is likely that our sentences are more syntactically challenging than the ones they were able to find in the relatively small manually annotated treebank they used.
",8 Discussion,0,[0]
One limitation of our approach is that it is not always clear what constitutes a minimal grammaticality contrast.,8 Discussion,0,[0]
"In the subject-verb agreement case, the contrast was clear: the two present-tense forms of the verb, e.g., laugh vs. laughs.",8 Discussion,0,[0]
"Our NPI manipulations, on the other hand, were less successful: the members of the contrasts differed not only in their syntactic structure but also in low-level n-gram probabilities, making the performance on this particular contrast harder to interpret.
",8 Discussion,0,[0]
"We emphasize that the goal of this article was not to advocate for LSTMs in particular as an effective architecture for modeling syntax; indeed, our results show that LSTM language models are far from matching naive annotators’ performance on this task, let alone performing at 100% accuracy.",8 Discussion,0,[0]
"We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models.",8 Discussion,0,[0]
We would like to thank Ming Xiang for sharing materials from human experiments that inspired many of our test cases.,9 Acknowledgments,0,[0]
"We also thank Brian Roark and the JHU Computational Psycholinguistics lab
for discussion, and Brian Leonard for help conducting the human experiment.",9 Acknowledgments,0,[0]
We present a dataset for evaluating the grammaticality of the predictions of a language model.,abstractText,0,[0]
"We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence.",abstractText,0,[0]
"The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.",abstractText,0,[0]
We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one.,abstractText,0,[0]
"In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.",abstractText,0,[0]
"Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online.",abstractText,0,[0]
This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.,abstractText,0,[0]
Targeted Syntactic Evaluation of Language Models,title,0,[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 574–583 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,0,[0]
Search engines help us find what we need among the vast array of available data.,1 Introduction,0,[0]
"When we request some information using a long or inexact description of it, these systems, however, often fail to deliver relevant items.",1 Introduction,0,[0]
"In this case, what typically follows is an iterative process in which we try to express our need differently in the hope that the system will return what we want.",1 Introduction,0,[0]
This is a major issue in information retrieval.,1 Introduction,0,[0]
"For instance, Huang and Efthimiadis (2009) estimate that 28-52% of all the web queries are modifications of previous ones.
",1 Introduction,0,[0]
"To a certain extent, this problem occurs because search engines rely on matching words in the query with words in relevant documents, to
perform retrieval.",1 Introduction,0,[0]
"If there is a mismatch between them, a relevant document may be missed.
",1 Introduction,0,[0]
One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.,1 Introduction,0,[0]
This technique is known as automatic query reformulation.,1 Introduction,0,[0]
"It typically expands the original query by adding terms from, for instance, dictionaries of synonyms such as WordNet (Miller, 1995), or from the initial set of retrieved documents (Xu and Croft, 1996).",1 Introduction,0,[0]
"This latter type of reformulation is known as pseudo (or blind) relevance feedback (PRF), in which the relevance of each term of the retrieved documents is automatically inferred.
",1 Introduction,0,[0]
"The proposed method is built on top of PRF but differs from previous works as we frame the query
574
reformulation problem as a reinforcement learning (RL) problem.",1 Introduction,0,[0]
"An initial query is the natural language expression of the desired goal, and an agent (i.e. reformulator) learns to reformulate an initial query to maximize the expected return (i.e. retrieval performance) through actions (i.e. selecting terms for a new query).",1 Introduction,0,[0]
The environment is a search engine which produces a new state (i.e. retrieved documents).,1 Introduction,0,[0]
"Our framework is illustrated in Fig. 1.
",1 Introduction,0,[0]
The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.,1 Introduction,0,[0]
This opens the possibility of training an agent to use a search engine for a task other than the one it was originally intended for.,1 Introduction,0,[0]
"To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.
",1 Introduction,0,[0]
"As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.
",1 Introduction,0,[0]
"Furthermore, we present a method to estimate the upper bound performance of our RL-based model.",1 Introduction,0,[0]
"Based on the estimated upper bound, we claim that this framework has a strong potential for future improvements.
",1 Introduction,0,[0]
"Here we summarize our main contributions:
•",1 Introduction,0,[0]
"A reinforcement learning framework for automatic query reformulation.
",1 Introduction,0,[0]
"• A simple method to estimate the upper-bound performance of an RL-based model in a given environment.
",1 Introduction,0,[0]
•,1 Introduction,0,[0]
A new large dataset with hundreds of thousands of query/relevant document pairs.1,1 Introduction,0,[0]
"In this section we describe the proposed method, illustrated in Fig. 2.
",2.1 Model Description,0,[0]
"The inputs are a query q0 consisting of a sequence of words (w1, ..., wn) and a candidate term ti with some context words (ti−k, ..., ti+k), where k ≥ 0 is the context window size.",2.1 Model Description,0,[0]
"Candidate terms
1The dataset and code to run the experiments are available at https://github.com/nyu-dl/ QueryReformulator.
are from q0 ∪ D0, the union of the terms in the original query and those from the documents D0 retrieved using q0.
",2.1 Model Description,0,[0]
"We use a dictionary of pretrained word embeddings (Mikolov et al., 2013) to convert the symbolic terms wj",2.1 Model Description,0,[0]
"and ti to their vector representations vj and ei ∈ Rd, respectively.",2.1 Model Description,0,[0]
"We map outof-vocabulary terms to an additional vector that is learned during training.
",2.1 Model Description,0,[0]
"We convert the sequence {vj} to a fixed-size vector φa(v) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence (Kim, 2014) or by using the last hidden state of a Recurrent Neural Network (RNN).2
Similarly, we fed the candidate term vectors ei to a CNN or RNN to obtain a vector representation φb(ei) for each term ti.",2.1 Model Description,0,[0]
"The convolutional/recurrent layers serve an important role of capturing context information, especially for outof-vocabulary and rare terms.",2.1 Model Description,0,[0]
"CNNs can process candidate terms in parallel, and, therefore, are faster for our application than RNNs.",2.1 Model Description,0,[0]
"RNNs, on the other hand, can encode longer contexts.
",2.1 Model Description,0,[0]
"Finally, we compute the probability of selecting
2To deal with variable-length inputs in a mini-batch, we pad smaller ones with zeros on both ends so they end up as long as the largest sample in the mini-batch.
ti as:
P (ti|q0) = σ(UT tanh(W (φa(v)‖φb(ei))",2.1 Model Description,0,[0]
"+ b)), (1)
where σ is the sigmoid function, ‖ is the vector concatenation operation,",2.1 Model Description,0,[0]
"W ∈ Rd×2d and U ∈ Rd are weights, and b ∈ R is a bias.
",2.1 Model Description,0,[0]
"At test time, we define the set of terms used in the reformulated query as T = {ti | P (ti|q0) > }, where is a hyper-parameter.",2.1 Model Description,0,[0]
"At training time, we sample the terms according to their probability distribution, T = {ti | α = 1∧α ∼ P (ti|q0)}.",2.1 Model Description,0,[0]
"We concatenate the terms in T to form a reformulated query q′, which will then be used to retrieve a new set of documents D′.",2.1 Model Description,0,[0]
One problem with the method previously described is that terms are selected independently.,2.2 Sequence Generation,0,[0]
This may result in a reformulated query that contains duplicated terms since the same term can appear multiple times in the feedback documents.,2.2 Sequence Generation,0,[0]
"Another problem is that the reformulated query can be very long, resulting in a slow retrieval.
",2.2 Sequence Generation,0,[0]
"To solve these problems, we extend the model to sequentially generate a reformulated query, as proposed by Buck et al. (2017).",2.2 Sequence Generation,0,[0]
We use a Recurrent Neural Network (RNN) that selects one term at a time from the pool of candidate terms and stops when a special token is selected.,2.2 Sequence Generation,0,[0]
The advantage of this approach is that the model can remember the terms previously selected through its hidden state.,2.2 Sequence Generation,0,[0]
"It can, therefore, produce more concise queries.
",2.2 Sequence Generation,0,[0]
"We define the probability of selecting ti as the k-th term of a reformulated query as:
P (tki |q0) ∝",2.2 Sequence Generation,0,[0]
"exp(φb(ei)Thk), (2) where hk is the hidden state vector at the k-th step, computed as:
hk = tanh(Waφa(v)",2.2 Sequence Generation,0,[0]
"+Wbφb(tk−1) +Whhk−1), (3) where tk−1 is the term selected in the previous step and Wa ∈ Rd×d, Wb ∈ Rd×d, and Wh ∈ Rd×d are weight matrices.",2.2 Sequence Generation,0,[0]
"In practice, we use an LSTM (Hochreiter and Schmidhuber, 1997) to encode the hidden state as this variant is known to perform better than a vanilla RNN.
",2.2 Sequence Generation,0,[0]
We avoid normalizing over a large vocabulary by using only terms from the retrieved documents.,2.2 Sequence Generation,0,[0]
"This makes inference faster and training practical since learning to select words from the whole
vocabulary might be too slow with reinforcement learning, although we leave this experiment for a future work.",2.2 Sequence Generation,0,[0]
"We train the proposed model using REINFORCE (Williams, 1992) algorithm.",2.3 Training,0,[0]
"The perexample stochastic objective is defined as
Ca = (R− R̄) ∑ t∈T",2.3 Training,0,[0]
"− logP (t|q0), (4)
where R is the reward and R̄ is the baseline, computed by the value network as:
R̄ = σ(ST tanh(V (φa(v)‖ē) + b)), (5) where ē = 1N ∑N i=1 φb(ei), N = |q0 ∪ D0|, V ∈ Rd×2d and S ∈ Rd are weights and b ∈ R is a bias.",2.3 Training,0,[0]
"We train the value network to minimize
Cb = α||R− R̄||2, (6) where α is a small constant (e.g. 0.1) multiplied to the loss in order to stabilize learning.",2.3 Training,0,[0]
We conjecture that the stability is due to the slowly evolving value network which directly affects the learning of the policy.,2.3 Training,0,[0]
"This effectively prevents the value network to fit extreme cases (unexpectedly high or low reward.)
",2.3 Training,0,[0]
"We minimize Ca and Cb using stochastic gradient descent (SGD) with the gradient computed by backpropagation (Rumelhart et al., 1988).",2.3 Training,0,[0]
"This allows the entire model to be trained end-to-end directly to optimize the retrieval performance.
",2.3 Training,0,[0]
Entropy Regularization We observed that the probability distribution in Eq.(1) became highly peaked in preliminary experiments.,2.3 Training,0,[0]
This phenomenon led to the trained model not being able to explore new terms that could lead to a betterreformulated query.,2.3 Training,0,[0]
We address this issue by regularizing the negative entropy of the probability distribution.,2.3 Training,0,[0]
We add the following regularization term to the original cost function in Eq.,2.3 Training,0,[0]
"(4):
CH = −λ ∑
t∈q0∪D0 P (t|q0) logP (t|q0), (7)
where λ is a regularization coefficient.",2.3 Training,0,[0]
"Query reformulation techniques are either based on a global method, which ignores a set of documents returned by the original query, or a local
method, which adjusts a query relative to the documents that initially appear to match the query.",3 Related Work,0,[0]
"In this work, we focus on local methods.
",3 Related Work,0,[0]
"A popular instantiation of a local method is the relevance model, which incorporates pseudo-relevance feedback into a language model form (Lavrenko and Croft, 2001).",3 Related Work,0,[0]
The probability of adding a term to an expanded query is proportional to its probability of being generated by the language models obtained from the original query and the document the term occurs in.,3 Related Work,0,[0]
"This framework has the advantage of not requiring query/relevant documents pairs as training data since inference is based on word co-occurrence statistics.
",3 Related Work,0,[0]
"Unlike the relevance model, algorithms can be trained with supervised learning, as proposed by Cao et al. (2008).",3 Related Work,0,[0]
A training dataset is automatically created by labeling each candidate term as relevant or not based on their individual contribution to the retrieval performance.,3 Related Work,0,[0]
Then a binary classifier is trained to select expansion terms.,3 Related Work,0,[0]
"In Section 4, we present a neural network-based implementation of this supervised approach.
",3 Related Work,0,[0]
A generalization of this supervised framework is to iteratively reformulate the query by selecting one candidate term at each retrieval step.,3 Related Work,0,[0]
"This can be viewed as navigating a graph where the nodes represent queries and associated retrieved results and edges exist between nodes whose queries are simple reformulations of each other (Diaz, 2016).",3 Related Work,0,[0]
"However, it can be slow to reformulate a query this way as the search engine must be queried for each newly added term.",3 Related Work,0,[0]
"In our method, on the contrary, the search engine is queried with multiple new terms at once.
",3 Related Work,0,[0]
"An alternative technique based on supervised learning is to learn a common latent representation of queries and relevant documents terms by using a click-through dataset (Sordoni et al., 2014).",3 Related Work,0,[0]
Neighboring document terms of a query in the latent space are selected to form an expanded query.,3 Related Work,0,[0]
"Instead of using a click-through dataset, which is often proprietary, it is possible to use an alternative dataset consisting of anchor text/title pairs.",3 Related Work,0,[0]
"In contrast, our approach does not require a dataset of paired queries as it learns term selection strategies via reinforcement learning.
",3 Related Work,0,[0]
"Perhaps the closest work to ours is that by Narasimhan et al. (2016), in which a reinforcement learning based approach is used to reformu-
late queries iteratively.",3 Related Work,0,[0]
A key difference is that in their work the reformulation component uses domain-specific template queries.,3 Related Work,0,[0]
"Our method, on the other hand, assumes open-domain queries.",3 Related Work,0,[0]
"In this section we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets and implementation details.",4 Experiments,0,[0]
Raw: The original query is given to a search engine without any modification.,4.1 Baseline Methods,0,[0]
"We evaluate two search engines in their default configuration: Lucene3 (Raw-Lucene) and Google Search4 (Raw-Google).
",4.1 Baseline Methods,0,[0]
Pseudo Relevance Feedback (PRF-TFIDF):,4.1 Baseline Methods,0,[0]
A query is expanded with terms from the documents retrieved by a search engine using the original query.,4.1 Baseline Methods,0,[0]
"In this work, the top-N TF-IDF terms from each of the top-K retrieved documents are added to the original query, where N and K are selected by a grid search on the validation data.
",4.1 Baseline Methods,0,[0]
PRF-Relevance Model (PRF-RM):,4.1 Baseline Methods,0,[0]
This is a popular relevance model for query expansion by Lavrenko and Croft (2001).,4.1 Baseline Methods,0,[0]
"The probability of using a term t in an expanded query is given by:
P (t|q0) =",4.1 Baseline Methods,0,[0]
(1− λ)P ′(t|q0),4.1 Baseline Methods,0,[0]
"+ λ ∑ d∈D0 P (d)P (t|d)P (q0|d), (8)
where P (d) is the probability of retrieving the document d, assumed uniform over the set, P (t|d) and P (q0|d) are the probabilities assigned by the language model obtained from d to t and q0, respectively.",4.1 Baseline Methods,0,[0]
"P ′(t|q0) = tf(t∈q)|q| , where tf(t, d) is the term frequency of t in d. We set the interpolation parameter λ to 0.5, following Zhai and Lafferty (2001).
",4.1 Baseline Methods,0,[0]
"We use a Dirichlet smoothed language model (Zhai and Lafferty, 2001) to compute a language model from a document d ∈ D0:
P (t|d) = tf(t, d) + uP (t|C)|d|+ u , (9)
3https://lucene.apache.org/ 4https://cse.google.com/cse/
where u is a scalar constant (u = 1500 in our experiments), and P (t|C) is the probability of t occurring in the entire corpus C.
We use the N terms with the highest P (t|q0) in an expanded query, whereN is a hyper-parameter.
",4.1 Baseline Methods,0,[0]
"Embeddings Similarity: Inspired by the methods proposed by Roy et al. (2016) and Kuzi et al. (2016), the top-N terms are selected based on the cosine similarity of their embeddings against the original query embedding.",4.1 Baseline Methods,0,[0]
"Candidate terms come from documents retrieved using the original query (PRF-Emb), or from a fixed vocabulary (Vocab-Emb).",4.1 Baseline Methods,0,[0]
"We use pretrained embeddings from Mikolov et al. (2013), and it contains 374,000 words.",4.1 Baseline Methods,0,[0]
Supervised Learning (SL): Here we detail a deep learning-based variant of the method proposed by Cao et al. (2008).,4.2 Proposed Methods,0,[0]
It assumes that query terms contribute independently to the retrieval performance.,4.2 Proposed Methods,0,[0]
We thus train a binary classifier to select a term if the retrieval performance increases beyond a preset threshold when that term is added to the original query.,4.2 Proposed Methods,0,[0]
"More specifically, we mark a term as relevant if (R′ −R)/R > 0.005, where R and R′ are the retrieval performances of the original query and the query expanded with the term, respectively.
",4.2 Proposed Methods,0,[0]
"We experiment with two variants of this method: one in which we use a convolutional network for both original query and candidate terms (SL-CNN), and the other in which we replace the convolutional network with a single hidden layer feed-forward neural network (SL-FF).",4.2 Proposed Methods,0,[0]
"In this variant, we average the output vectors of the neural network to obtain a fixed size representation of q0.
",4.2 Proposed Methods,0,[0]
Reinforcement Learning (RL): We use multiple variants of the proposed RL method.,4.2 Proposed Methods,0,[0]
"RL-CNN and RL-RNN are the models described in Section 2.1, in which the former uses CNNs to encode query and term features and the latter uses RNNs (more specifically, bidirectional LSTMs).",4.2 Proposed Methods,0,[0]
RL-FF is the model in which term and query vectors are encoded by single hidden layer feed-forward neural networks.,4.2 Proposed Methods,0,[0]
"In the RL-RNN-SEQ model, we add the sequential generator described in Section 2.2 to the RL-RNN variant.",4.2 Proposed Methods,0,[0]
"We summarize in Table 1 the datasets.
",4.3 Datasets,0,[0]
TREC - Complex Answer Retrieval (TRECCAR),4.3 Datasets,0,[0]
"This is a publicly available dataset automatically created from Wikipedia whose goal is to encourage the development of methods that respond to more complex queries with longer answers (Dietz and Ben, 2017).",4.3 Datasets,0,[0]
A query is the concatenation of an article title and one of its section titles.,4.3 Datasets,0,[0]
The ground-truth documents are the paragraphs within that section.,4.3 Datasets,0,[0]
"For example, a query is “Sea Turtle, Diet” and the ground truth documents are the paragraphs in the section “Diet” of the “Sea Turtle” article.",4.3 Datasets,0,[0]
"The corpus consists of all the English Wikipedia paragraphs, except the abstracts.",4.3 Datasets,0,[0]
"The released dataset has five predefined folds, and we use the first three as the training set and the remaining two as validation and test sets, respectively.
",4.3 Datasets,0,[0]
Jeopardy This is a publicly available Q&A dataset introduced by Nogueira and Cho (2016).,4.3 Datasets,0,[0]
A query is a question from the Jeopardy! TV Show and the corresponding document is a Wikipedia article whose title is the answer.,4.3 Datasets,0,[0]
"For example, a query is “For the last eight years of his life, Galileo was under house arrest for espousing this mans theory” and the answer is the Wikipedia article titled “Nicolaus Copernicus”.",4.3 Datasets,0,[0]
"The corpus consists of all the articles in the English Wikipedia.
",4.3 Datasets,0,[0]
Microsoft Academic (MSA),4.3 Datasets,0,[0]
This dataset consists of academic papers crawled from Microsoft Academic API.5,4.3 Datasets,0,[0]
"The crawler started at the paper Silver et al. (2016) and traversed the graph of references until 500,000 papers were crawled.",4.3 Datasets,0,[0]
We then removed papers that had no reference within or whose abstract had less than 100 characters.,4.3 Datasets,0,[0]
"We ended up with 480,000 papers.
",4.3 Datasets,0,[0]
"A query is the title of a paper, and the groundtruth answer consists of the papers cited within.",4.3 Datasets,0,[0]
Each document in the corpus consists of its title and abstract.6,4.3 Datasets,0,[0]
"Three metrics are used to evaluate performance:
Recall@K:",4.4 Metrics and Reward,0,[0]
"Recall of the top-K retrieved documents:
R@K = |DK",4.4 Metrics and Reward,0,[0]
"∩D∗| |D∗| , (10)
5https://www.microsoft.com/cognitive-services/enus/academic-knowledge-api
6This was done to avoid a large computational overhead for indexing full papers.
where DK are the top-K retrieved documents and D∗ are the relevant documents.",4.4 Metrics and Reward,0,[0]
"Since one of the goals of query reformulation is to increase the proportion of relevant documents returned, recall is our main metric.
",4.4 Metrics and Reward,0,[0]
"Precision@K: Precision of the top-K retrieved documents:
P@K = |DK",4.4 Metrics and Reward,0,[0]
∩D∗| |DK,4.4 Metrics and Reward,0,[0]
"| (11)
Precision captures the proportion of relevant documents among the returned ones.",4.4 Metrics and Reward,0,[0]
"Despite not being the main goal of a reformulation method, improvements in precision are also expected with a good query reformulation method.",4.4 Metrics and Reward,0,[0]
"Therefore, we include this metric.
",4.4 Metrics and Reward,0,[0]
Mean Average Precision:,4.4 Metrics and Reward,0,[0]
"The average precision of the top-K retrieved documents is defined as:
AP@K = ∑K
k=1 P@k × rel(k) |D∗| , (12)
where
rel(k) = { 1, if the k-th document is relevant; 0, otherwise.
",4.4 Metrics and Reward,0,[0]
"(13)
The mean average precision of a set of queries Q is then:
MAP@K = 1 |Q| ∑ q∈Q AP@Kq, (14)
where AP@Kq is the average precision at K for a query q.",4.4 Metrics and Reward,0,[0]
"This metric values the position of a relevant document in a returned list and is, therefore, complementary to precision and recall.
",4.4 Metrics and Reward,0,[0]
"Reward We use R@K as a reward when training the proposed RL-based models as this metric has shown to be effective in improving the other metrics as well.
",4.4 Metrics and Reward,0,[0]
"SL-Oracle In addition to the baseline methods and proposed reinforcement learning approach, we report two oracle performance bounds.",4.4 Metrics and Reward,0,[0]
The first oracle is a supervised learning oracle (SLOracle).,4.4 Metrics and Reward,0,[0]
It is a classifier that perfectly selects terms that will increase performance according to the procedure described in Section 4.2.,4.4 Metrics and Reward,0,[0]
This measure serves as an upper-bound for the supervised methods.,4.4 Metrics and Reward,0,[0]
Notice that this heuristic assumes that each term contributes independently from all the other terms to the retrieval performance.,4.4 Metrics and Reward,0,[0]
"There may be, however, other ways to explore the dependency of terms that would lead to a higher performance.
",4.4 Metrics and Reward,0,[0]
"RL-Oracle Second, we introduce a reinforcement learning oracle (RL-Oracle) which estimates a conservative upper-bound performance for the RL models.",4.4 Metrics and Reward,0,[0]
"Unlike the SL-Oracle, it does not assume that each term contributes independently to the retrieval performance.",4.4 Metrics and Reward,0,[0]
"It works as follows: first, the validation or test set is divided into N small subsets {Ai}Ni=1 (each with 100 examples, for instance).",4.4 Metrics and Reward,0,[0]
"An RL model is trained on each subset Ai until it overfits, that is, until the reward R∗i stops increasing or an early stop mechanism ends training.7",4.4 Metrics and Reward,0,[0]
"Finally, we compute the oracle performance R∗ as the average reward over all the subsets: R∗ = 1N ∑N i=1R ∗",4.4 Metrics and Reward,0,[0]
"i .
",4.4 Metrics and Reward,0,[0]
"This upper bound by the RL-Oracle is, however, conservative since there might exist better reformulation strategies that the RL model was not able to discover.",4.4 Metrics and Reward,0,[0]
"Search engine We use Lucene and BM25 as the search engine and the ranking function, respectively, for all PRF, SL and RL methods.",4.5 Implementation Details,0,[0]
"For RawGoogle, we restrict the search to the wikipedia.org domain when evaluating its performance on the Jeopardy dataset.",4.5 Implementation Details,0,[0]
"We could not apply the same restriction to the two other datasets as Google does not index Wikipedia paragraphs, and as it is not trivial to match papers from MS Academic to the ones returned by Google Search.
Candidate terms We use Wikipedia articles as a source for candidate terms since it is a well curated, clean corpus, with diverse topics.
",4.5 Implementation Details,0,[0]
"At training and test times of SL methods, and at test time of RL methods, the candidate terms are from the first M words of the top-K Wikipedia articles retrieved.",4.5 Implementation Details,0,[0]
"We select M and K using grid search on the validation set over {50, 100, 200, 300} and {1, 3, 5, 7}, respectively.",4.5 Implementation Details,0,[0]
The best values are M = 300 and K = 7.,4.5 Implementation Details,0,[0]
"These correspond to the maximum number of terms we could fit in a single GPU.
7The subset should be small enough, or the model should be large enough so it can overfit.
",4.5 Implementation Details,0,[0]
"At training time of an RL model, we use only one document uniformly sampled from the top-K retrieved ones as a source for candidate terms, as this leads to a faster learning.
",4.5 Implementation Details,0,[0]
"For the PRF methods, the top-M terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-K retrieved documents are added to the original query.",4.5 Implementation Details,0,[0]
"We select M and K using grid search over {10, 50, 100, 200, 300, 500} and {1, 3, 5, 9, 11}, respectively.",4.5 Implementation Details,0,[0]
"The best values are M = 300 and K = 9.
",4.5 Implementation Details,0,[0]
"Multiple Reformulation Rounds Although our framework supports multiple rounds of search and reformulation, we did not find any significant improvement in reformulating a query more than once.",4.5 Implementation Details,0,[0]
"Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.
",4.5 Implementation Details,0,[0]
"Neural Network Setup For SL-CNN and RLCNN variants, we use a 2-layer convolutional network for the original query.",4.5 Implementation Details,0,[0]
Each layer has a window size of 3 and 256 filters.,4.5 Implementation Details,0,[0]
"We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer.",4.5 Implementation Details,0,[0]
"We set the dimension d of the weight matrices W,S,U , and V to 256.",4.5 Implementation Details,0,[0]
"For the optimizer, we use ADAM (Kingma and Ba, 2014) with α = 10−4, β1 = 0.9, β2 = 0.999, and = 10−8.",4.5 Implementation Details,0,[0]
"We set the entropy regularization coefficient λ to 10−3.
",4.5 Implementation Details,0,[0]
"For RL-RNN and RL-RNN-SEQ, we use a 2- layer bidirectional LSTM with 256 hidden units in each layer.",4.5 Implementation Details,0,[0]
We clip the gradients to unit norm.,4.5 Implementation Details,0,[0]
"For RL-RNN-SEQ, we set the maximum possible
number of generated terms to 50 and we use beam search of size four at test time.
",4.5 Implementation Details,0,[0]
"We fix the dictionary of pre-trained word embeddings during training, except the vector for outof-vocabulary words.",4.5 Implementation Details,0,[0]
We found that this led to faster convergence and observed no difference in the overall performance when compared to learning embeddings during training.,4.5 Implementation Details,0,[0]
Table 2 shows the main result.,5 Results and Discussion,0,[0]
"As expected, reformulation based methods work better than using the original query alone.",5 Results and Discussion,0,[0]
"Supervised methods (SL-FF and SL-CNN) have in general a better performance than unsupervised ones (PRF-TFIDF, PRF-RM, PRF-Emb, and Emb-Vocab), but perform worse than RL-based models (RL-FF, RLCNN, RL-RNN, and RL-RNN-SEQ).
",5 Results and Discussion,0,[0]
"RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs 47 words).",5 Results and Discussion,0,[0]
"Thus, RLRNN-SEQ is faster in retrieving documents and therefore might be a better candidate for a production implementation.
",5 Results and Discussion,0,[0]
"The performance gap between the oracle and best performing method (Table 2, RL-Oracle vs. RL-RNN) suggests that there is a large room for improvement.",5 Results and Discussion,0,[0]
"The cause for this gap is unknown but we suspect, for instance, an inherent difficulty in learning a good selection strategy and the partial observability from using a black box search engine.",5 Results and Discussion,0,[0]
The proportion of relevant terms selected by the SL- and RL-Oracles over the total number of candidate terms (Table 3) indicates that only a small subset of terms are useful for the reformulation.,5.1 Relevant Terms per Document,0,[0]
"Thus, we may conclude that the proposed method was able to learn an efficient term selection strategy in an environment where relevant terms are infrequent.",5.1 Relevant Terms per Document,0,[0]
Fig. 3 shows the improvement in recall as more candidate terms are provided to a reformulation method.,5.2 Scalability: Number of Terms vs Recall,0,[0]
"The RL-based model benefits from more candidate terms, whereas the classical PRF method quickly saturates.",5.2 Scalability: Number of Terms vs Recall,0,[0]
"In our experiments, the best performing RL-based model uses the maximum number of candidate terms that we could fit
on a single GPU.",5.2 Scalability: Number of Terms vs Recall,0,[0]
"We, therefore, expect further improvements with more computational resources.",5.2 Scalability: Number of Terms vs Recall,0,[0]
"We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Fig. 4.
Notice that terms that are more related to the query have higher probabilities, although common words such as ”the” are also selected.",5.3 Qualitative Analysis,0,[0]
"This is a consequence of our choice of a reward that does
not penalize the selection of neutral terms.",5.3 Qualitative Analysis,0,[0]
"In Table 4 we show an original and reformulated query examples extracted from the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents.",5.3 Qualitative Analysis,0,[0]
Notice that the reformulated query retrieves more relevant documents than the original one.,5.3 Qualitative Analysis,0,[0]
"As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents.
",5.3 Qualitative Analysis,0,[0]
"Same query, different tasks We compare in Table 5 the reformulation of a sample query made by models trained on different datasets.",5.3 Qualitative Analysis,0,[0]
"The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such as “serves” and “accreditation”.",5.3 Qualitative Analysis,0,[0]
These selections are expected for this task since similar terms can be effective in retrieving similar paragraphs.,5.3 Qualitative Analysis,0,[0]
"On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as “Tunxis”, as these have a higher chance of being an answer to the question.",5.3 Qualitative Analysis,0,[0]
"The model trained on MSA selects terms that cover different aspects of the entity being queried, such as “arts center” and “library”, since retrieving a diverse set of documents is necessary for the task the of citation recommendation.",5.3 Qualitative Analysis,0,[0]
"Our best model, RL-RNN, takes 8-10 days to train on a single K80 GPU.",5.4 Training and Inference Times,0,[0]
"At inference time, it takes
approximately one second to reformulate a batch of 64 queries.",5.4 Training and Inference Times,0,[0]
Approximately 40% of this time is to retrieve documents from the search engine.,5.4 Training and Inference Times,0,[0]
We introduced a reinforcement learning framework for task-oriented automatic query reformulation.,6 Conclusion,0,[0]
An appealing aspect of this framework is that an agent can be trained to use a search engine for a specific task.,6 Conclusion,0,[0]
The empirical evaluation has confirmed that the proposed approach outperforms strong baselines in the three separate tasks.,6 Conclusion,0,[0]
The analysis based on two oracle approaches has revealed that there is a meaningful room for further development.,6 Conclusion,0,[0]
"In the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework, (2) using information from modalities other than text, and (3) better reinforcement learning algorithms for a partially-observable environment.",6 Conclusion,0,[0]
RN is funded by Coordenao de Aperfeioamento de Pessoal de Nvel Superior (CAPES).,Acknowledgements,0,[0]
"KC thanks support by Facebook, Google and NVIDIA.",Acknowledgements,0,[0]
This work was partly funded by the Defense Advanced Research Projects Agency (DARPA) D3M program.,Acknowledgements,0,[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",Acknowledgements,0,[0]
Search engines play an important role in our everyday lives by assisting us in finding the information we need.,abstractText,0,[0]
"When we input a complex query, however, results are often far from satisfactory.",abstractText,0,[0]
"In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned.",abstractText,0,[0]
We train this neural network with reinforcement learning.,abstractText,0,[0]
"The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall.",abstractText,0,[0]
We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall.,abstractText,0,[0]
"Furthermore, we present a simple method to estimate a conservative upperbound performance of a model in a particular environment and verify that there is still large room for improvements.",abstractText,0,[0]
Task-Oriented Query Reformulation with Reinforcement Learning,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1088–1097 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1088",text,0,[0]
"Automated essay scoring (AES) utilizes natural language processing and machine learning techniques to automatically rate essays written for a target prompt (Dikli, 2006).",1 Introduction,0,[0]
"Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006).
",1 Introduction,0,[0]
"Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999).",1 Introduction,0,[0]
"While the established models are
effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; Dong et al., 2017), we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible.",1 Introduction,0,[0]
"For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data.",1 Introduction,0,[0]
"Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays written for the target prompt, as well as the rated essays for several non-target prompts.
",1 Introduction,0,[0]
"We argue that it is not straightforward, if possible, to apply the established promptdependent AES methods for the mentioned prompt-independent scenario.",1 Introduction,0,[0]
"On one hand, essays for different prompts may differ a lot in the uses of vocabulary, the structure, and the grammatic characteristics; on the other hand, however, established prompt-dependent AES models are designed to learn from these prompt-specific features, including the on/off-topic degree, the tf - idf weights of topical terms (Attali and Burstein, 2006; Dikli, 2006), and the n-gram features extracted from word semantic embeddings (Dong and Zhang, 2016; Alikaniotis et al., 2016).",1 Introduction,0,[0]
"Consequently, the prompt-dependent models can hardly learn generalized rules from rated essays for nontarget prompts, and are not suitable for the promptindependent AES.
Being aware of this difficulty, to this end, a twostage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem.",1 Introduction,0,[0]
"In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage,
a shallow model is trained on a number of rated essays for several non-target prompts; given a target prompt and a set of essays to rate, the trained model is employed to generate pseudo training data by selecting essays with the extreme quality.",1 Introduction,0,[0]
"At the second stage, a novel end-to-end hybrid deep neural network learns prompt-dependent features from these selected training data, by considering semantic, part-of-speech, and syntactic features.
",1 Introduction,0,[0]
"The contributions in this paper are threefold: 1) a two-stage learning framework is proposed to bridge the gap between the target and non-target prompts, by only consuming rated essays for nontarget prompts as training data; 2) a novel deep model is proposed to learn from pseudo labels by considering semantic, part-of-speech, and syntactic features; and most importantly, 3) to the best of our knowledge, the proposed TDNN is actually the first approach dedicated to addressing the prompt-independent AES.",1 Introduction,0,[0]
"Evaluation on the standard ASAP dataset demonstrates the effectiveness of the proposed method.
",1 Introduction,0,[0]
The rest of this paper is organized as follows.,1 Introduction,0,[0]
"In Section 2, we describe our novel TDNN model, including the two-stage framework and the proposed deep model.",1 Introduction,0,[0]
"Following that, we describe the setup of our empirical study in Section 3, thereafter present the results and provide analyzes in Section 4.",1 Introduction,0,[0]
"Section 5 recaps existing literature and put our work in context, before drawing final conclusions in Section 6.",1 Introduction,0,[0]
"In this section, the proposed two-stage deep neural network (TDNN) for prompt-independent AES is described.",2 Two-stage Deep Neural Network for AES,0,[0]
"To accurately rate an essay, on one hand, we need to consider its pertinence to the given prompt; on the other hand, the organization, the analyzes, as well as the uses of the vocabulary are all crucial for the assessment.",2 Two-stage Deep Neural Network for AES,0,[0]
"Henceforth, both prompt-dependent and -independent factors should be considered, but the latter ones actually do not require prompt-dependent training data.",2 Two-stage Deep Neural Network for AES,0,[0]
"Accordingly, in the proposed framework, a supervised ranking model is first trained to learn from prompt-independent data, hoping to roughly assess essays without considering the prompt; subsequently, given the test dataset, namely, a set of essays for a target prompt, a subset of essays are selected as positive and negative training
data based on the prediction of the trained model from the first stage; ultimately, a novel deep model is proposed to learn both prompt-dependent and -independent factors on this selected subset.",2 Two-stage Deep Neural Network for AES,0,[0]
"As indicated in Figure 1, the proposed framework includes two stages.",2 Two-stage Deep Neural Network for AES,0,[0]
Prompt-independent stage.,2.1 Overview,0,[0]
"Only the promptindependent factors are considered to train a shallow model, aiming to recognize the essays with the extreme quality in the test dataset, where the rated essays for non-target prompts are used for training.",2.1 Overview,0,[0]
"Intuitively, one could recognize essays with the highest and the lowest scores correctly by solely examining their quality of writing, e.g., the number of typos, without even understanding them, and the prompt-independent features such as the number of grammatic and spelling errors should be sufficient to fulfill this screening procedure.",2.1 Overview,0,[0]
"Accordingly, a supervised model trained solely on prompt-independent features is employed to identify the essays with the highest and lowest scores in a given set of essays for the target prompt, which are used as the positive and negative training data in the follow-up prompt-dependent learning phase.
",2.1 Overview,0,[0]
Prompt-dependent stage.,2.1 Overview,0,[0]
"Intuitively, most essays are with a quality in between the extremes, requiring a good understanding of their meaning to make an accurate assessment, e.g., whether the examples from the essay are convincing or whether the analyzes are insightful, making the consideration of prompt-dependent features crucial.",2.1 Overview,0,[0]
"To achieve that, a model is trained to learn from the comparison between essays with the highest and lowest scores for the target prompt according to the predictions from the first step.",2.1 Overview,0,[0]
"Akin to the settings in transductive transfer learning (Pan and
Yang, 2010), given essays for a particular prompt, quite a few confident essays at two extremes are selected and are used to train another model for a fine-grained content-based prompt-dependent assessment.",2.1 Overview,0,[0]
"To enable this, a powerful deep model is proposed to consider the content of the essays from different perspectives using semantic, part-of-speech (POS) and syntactic network.",2.1 Overview,0,[0]
"After being trained with the selected essays, the deep model is expected to memorize the properties of a good essay in response to the target prompt, thereafter accurately assessing all essays for it.",2.1 Overview,0,[0]
"In Section 2.2, building blocks for the selection of the training data and the proposed deep model are described in details.",2.1 Overview,0,[0]
Select confident essays as training data.,2.2 Building Blocks,0,[0]
"The identification of the extremes is relatively simple, where a RankSVM (Joachims, 2002) is trained on essays for different non-target prompts, avoiding the risks of over-fitting some particular prompts.",2.2 Building Blocks,0,[0]
"A set of established prompt-independent features are employed, which are listed in Table 2.",2.2 Building Blocks,0,[0]
"Given a prompt and a set of essays for evaluation, to begin with, the trained RankSVM is used to assign prediction scores to individual prompt-essay pairs, which are uniformly transformed into a 10- point scale.",2.2 Building Blocks,0,[0]
"Thereafter, the essays with predicted scores in [0, 4] and [8, 10] are selected as negative and positive examples respectively, serving as the bad and good templates for training in the next stage.",2.2 Building Blocks,0,[0]
"Intuitively, an essay with a score beyond eight out of a 10-point scale is considered good, while the one receiving less than or equal to four, is considered to be with a poor quality.
",2.2 Building Blocks,0,[0]
A hybrid deep model for fine-grained assessment.,2.2 Building Blocks,0,[0]
"To enable a prompt-dependent assessment, a model is desired to comprehensively capture the ways in which a prompt is described or discussed in an essay.",2.2 Building Blocks,0,[0]
"In this paper, semantic meaning, part-of-speech (POS), and the syntactic taggings of the token sequence from an essay are considered, grasping the quality of an essay for a target prompt.",2.2 Building Blocks,0,[0]
The model architecture is summarized in Figure 2.,2.2 Building Blocks,0,[0]
"Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, denoted as −→e sem, hoping to understand what the essay is about; in addition, the part-of-speech information is encoded as a sequence of POS tag-
gings, coined as −→e pos; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured via syntactic network, leading to −→e synt, where the model learns the organization of the essay.",2.2 Building Blocks,0,[0]
"Akin to (Li et al., 2015) and (Zhou and Xu, 2015), biLSTM is employed as a basic component to encode a sequence.",2.2 Building Blocks,0,[0]
"Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating.",2.2 Building Blocks,0,[0]
"In the following, the architecture of the model is described in details.
- Semantic embedding.",2.2 Building Blocks,0,[0]
"Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016), semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014), are employed.",2.2 Building Blocks,0,[0]
"On top of the word embeddings, two bi-LSTM layers are stacked, namely, the essay layer is constructed on top of the sentence layer, ending up with the semantic representation of the whole essay, which is denoted as −→e sem in Figure 2.",2.2 Building Blocks,0,[0]
-,2.2 Building Blocks,0,[0]
"Part-Of-Speech (POS) embeddings for individual terms are first generated by the Stanford Tagger (Toutanova et al., 2003), where 36 different POS tags present.",2.2 Building Blocks,0,[0]
"Accordingly, individual words are embedded with 36-dimensional one-hot representation, and is transformed to a 50-dimensional vector through a lookup layer.",2.2 Building Blocks,0,[0]
"After that, two biLSTM layers are stacked, leading to −→e pos.",2.2 Building Blocks,0,[0]
"Take Figure 3 for example, given a sentence “Attention please, here is an example.”",2.2 Building Blocks,0,[0]
", it is first converted into a POS sequence using the tagger, namely, VB, VBP, RB, VBZ, DT, NN; thereafter it is further mapped to vector space through one-hot embedding and a lookup layer.
",2.2 Building Blocks,0,[0]
#NAME?,2.2 Building Blocks,0,[0]
"The Stanford Parser (Socher et al., 2013) is employed to label the syntactic structure of words and phrases in sentences, accounting for 59 different types in total.",2.2 Building Blocks,0,[0]
"Similar to (Tai et al., 2015), we opt for three stacked bi-LSTM, aiming at encoding individual phrases, sentences, and ultimately the whole essay in sequence.",2.2 Building Blocks,0,[0]
"In particular, according to the hierarchical structure from a parsing tree, the phrase-level bi-LSTM first encodes different phrases by consuming syntactic
embeddings ( −→ Sti in Figure 2) from a lookup table of individual syntactic units in the tree; thereafter, the encoded dense layers in individual sentences are further consumed by a sentence-level bi-LSTM, ending up with sentence-level syntactic representations, which are ultimately combined by the essay-level bi-LSTM, resulting in −→e synt.",2.2 Building Blocks,0,[0]
"For example, the parsed tree for a sentence “Attention please, here is an example.” is displayed in Figure 3.",2.2 Building Blocks,0,[0]
"To start with, the sentence is parsed into ((NP VP)(NP VP NP)), and the dense embeddings are fetched from a lookup table for all tokens, namely, NP and VP; thereafter, the phraselevel bi-LSTM encodes (NP VP) and (NP VP NP) separately, which are further consumed by the sentence-level bi-LSTM.",2.2 Building Blocks,0,[0]
"Afterward, essay-level bi-LSTM further combines the representations of different sentences into −→e synt.
- Combination.",2.2 Building Blocks,0,[0]
"A feed-forward network linearly transforms the concatenated representations of an essay from the mentioned three perspectives into a scalar, which is further normalized into [0, 1] with a sigmoid function.",2.2 Building Blocks,0,[0]
Objective.,2.3 Objective and Training,0,[0]
"Mean square error (MSE) is optimized, which is widely used as a loss function in regression tasks.",2.3 Objective and Training,0,[0]
"Given N pairs of a target prompt pi and an essay ei, MSE measures the average value of square error between the normalized gold standard rating r∗(pi, ei) and the predicted rating r(pi, ei) assigned by the AES model, as summarized in Equation 1.
1
N N∑ i=1",2.3 Objective and Training,0,[0]
"( r(pi, ei)− r∗(pi, ei) )2",2.3 Objective and Training,0,[0]
(1) Optimization.,2.3 Objective and Training,0,[0]
"Adam (Kingma and Ba, 2014) is employed to minimize the loss over the training data.",2.3 Objective and Training,0,[0]
"The initial learning rate η is set to 0.01 and the gradient is clipped between [−10, 10] during training.",2.3 Objective and Training,0,[0]
"In addition, dropout (Srivastava et al., 2014) is introduced for regularization with a dropout rate of 0.5, and 64 samples are used in each batch with batch normalization (Ioffe and Szegedy, 2015).",2.3 Objective and Training,0,[0]
30% of the training data are reserved for validation.,2.3 Objective and Training,0,[0]
"In addition, early stopping (Yao et al., 2007) is employed according to the validation loss, namely, the training is terminated if no decrease of the loss is observed for ten consecutive epochs.",2.3 Objective and Training,0,[0]
"Once training is finished,
akin to (Dong et al., 2017), the model with the best quadratic weighted kappa on the validation set is selected.",2.3 Objective and Training,0,[0]
Dataset.,3 Experimental Setup,0,[0]
"The Automated Student Assessment Prize (ASAP) dataset has been widely used for AES (Alikaniotis et al., 2016; Chen and He, 2013; Dong et al., 2017), and is also employed as the prime evaluation instrument herein.",3 Experimental Setup,0,[0]
"In total, ASAP consists of eight sets of essays, each of which associates to one prompt, and is originally written by students between Grade 7 and Grade 10.",3 Experimental Setup,0,[0]
"As summarized in Table 1, essays from different sets differ in their rating criteria, length, as well as the rating distribution1.",3 Experimental Setup,0,[0]
Cross,3 Experimental Setup,0,[0]
-,3 Experimental Setup,0,[0]
validation.,3 Experimental Setup,0,[0]
"To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation.",3 Experimental Setup,0,[0]
"In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data.",3 Experimental Setup,0,[0]
Evaluation metric.,3 Experimental Setup,0,[0]
"The model outputs are first uniformly re-scaled into [0, 10], mirroring the range of ratings in practice.",3 Experimental Setup,0,[0]
"Thereafter, akin to (Yannakoudakis et al., 2011; Chen and He, 2013; Alikaniotis et al., 2016), we report our results primarily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth.",3 Experimental Setup,0,[0]
Pearson correlation coefficient (PCC) and Spearman rankorder correlation coefficient (SCC) are also reported.,3 Experimental Setup,0,[0]
"The correlations obtained from individual folds, as well as the average over all eight folds, are reported as the ultimate results.",3 Experimental Setup,0,[0]
Competing models.,3 Experimental Setup,0,[0]
"Since the promptindependent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction, serving as baselines.",3 Experimental Setup,0,[0]
"This is due to the facts that the
1Details of this dataset can be found at https://www. kaggle.com/c/asap-aes.
prompt-dependent and -independent models differ a lot in terms of problem settings and model designs, especially in their requirements for the training data, where the latter ones release the prompt-dependent requirements and thereby are accessible to more data.",3 Experimental Setup,0,[0]
"- RankSVM, using handcrafted features for AES (Yannakoudakis et al., 2011; Chen et al., 2014), is trained on a set of pre-defined promptindependent features as listed in Table 2, where the features are standardized beforehand to remove the mean and variance.",3 Experimental Setup,0,[0]
The RankSVM is also used for the prompt-independent stage in our proposed TDNN model.,3 Experimental Setup,0,[0]
"In particular, the linear kernel RankSVM2 is employed, where C is set to 5 according to our pilot experiments.",3 Experimental Setup,0,[0]
- 2L-LSTM.,3 Experimental Setup,0,[0]
"Two-layer bi-LSTM with GloVe for AES (Alikaniotis et al., 2016) is employed as another baseline.",3 Experimental Setup,0,[0]
Regularized word embeddings are dropped to avoid over-fitting the prompt-specific features.,3 Experimental Setup,0,[0]
#NAME?,3 Experimental Setup,0,[0]
"This model (Taghipour and Ng, 2016) employs a convolutional (CNN) layer over one-hot representations of words, followed by an LSTM layer to encode word sequences in a given essay.",3 Experimental Setup,0,[0]
A linear layer with sigmoid activation function is then employed to predict the essay rating.,3 Experimental Setup,0,[0]
#NAME?,3 Experimental Setup,0,[0]
"This model (Dong et al., 2017) employs a CNN layer to encode word sequences into sentences, followed by an LSTM layer to generate the essay representation.",3 Experimental Setup,0,[0]
"An attention mechanism is added to model the influence of individual sentences on the final essay representation.
",3 Experimental Setup,0,[0]
"2http://svmlight.joachims.org/
For the proposed TDNN model, as introduced in Section 2.2, different variants of TDNN are examined by using one or multiple components out of the semantic, POS and the syntactic networks.",3 Experimental Setup,0,[0]
The combinations being considered are listed in the following.,3 Experimental Setup,0,[0]
"In particular, the dimensions of POS tags and syntactic network are fixed to 50, whereas the sizes of the hidden units in LSTM, as well as the output units of the linear layers are tuned by grid search.",3 Experimental Setup,0,[0]
"- TDNN(Sem) only includes the semantic building block, which is similar to the two-layer LSTM neural network from (Alikaniotis et al., 2016) but without regularizing the word embeddings; - TDNN(Sem+POS) employs the semantic and the POS building blocks; - TDNN(Sem+Synt) uses the semantic and the syntactic network building blocks; - TDNN(POS+Synt) includes the POS and the syntactic network building blocks; - TDNN(ALL) employs all three building blocks.
",3 Experimental Setup,0,[0]
The use of POS or syntactic network alone is not presented for brevity given the facts that they perform no better than TDNN(POS+Synt) in our pilot experiments.,3 Experimental Setup,0,[0]
Source code of the TDNN model is publicly available to enable further comparison3.,3 Experimental Setup,0,[0]
"In this section, the evaluation results for different competing methods are compared and analyzed in terms of their agreements with the manual ratings using three correlation metrics, namely, QWK, PCC and SCC, where the best results for each prompt is highlighted in bold in Table 3.
",4 Results and Analyzes,0,[0]
"It can be seen that, for seven out of all eight prompts, the proposed TDNN variants outperform the baselines by a margin in terms of QWK, and the TDNN variant with semantic and syntactic features, namely, TDNN(Sem+Synt), consistently performs the best among different competing methods.",4 Results and Analyzes,0,[0]
"More precisely, as indicated in the bottom right corner in Table 3, on average, TDNN(Sem+Synt) outperforms the baselines by at least 25.52% under QWK, by 10.28% under PCC, and by 15.66% under SCC, demonstrating that the proposed model not only correlates better with the manual ratings in terms of QWK, but also linearly (PCC) and monotonically (SCC) correlates better with the manual ratings.",4 Results and Analyzes,0,[0]
"As for the
3https://github.com/ucasir/TDNN4AES
four baselines, note that, the relatively underperformed deep models suffer from larger variances of performance under different prompts, e.g., for prompts two and eight, 2L-LSTM’s QWK is lower than 0.3.",4 Results and Analyzes,0,[0]
"This actually confirms our choice of RankSVM for the first stage in TDNN, since a more complicated model (like 2L-LSTM) may end up with learning prompt-dependent signals, making it unsuitable for the prompt-independent rating prediction.",4 Results and Analyzes,0,[0]
"As a comparison, RankSVM performs more stable among different prompts.
",4 Results and Analyzes,0,[0]
"As for the different TDNN variants, it turns out that the joint uses of syntactic network with semantic or POS features can lead to better performances.",4 Results and Analyzes,0,[0]
"This indicates that, when learning the prompt-dependent signals, apart from the widelyused semantic features, POS features and the sentence structure taggings (syntactic network) are also essential in learning the structure and the arrangement of an essay in response to a particular prompt, thereby being able to improve the results.",4 Results and Analyzes,0,[0]
"It is also worth mentioning, however, when using all three features, the TDNN actually performs worse than when only using (any) two features.",4 Results and Analyzes,0,[0]
"One possible explanation is that the uses of all three features result in a more complicated model, which over-fits the training data.
",4 Results and Analyzes,0,[0]
"In addition, recall that the prompt-independent RankSVM model from the first stage enables the proposed TDNN in learning prompt-dependent information without manual ratings for the target prompt.",4 Results and Analyzes,0,[0]
"Therefore, one would like to understand how good the trained RankSVM is in feeding training data for the model in the second stage.",4 Results and Analyzes,0,[0]
"In particular, the precision, recall and F-score (P/R/F) of the essays selected by RanknSVM, namely, the negative ones rated between [0, 4], and the positive ones rated between [8, 10], are displayed in Figure 4.",4 Results and Analyzes,0,[0]
It can be seen that the P/R/F scores of both positive and negative classes differ a lot among different prompts.,4 Results and Analyzes,0,[0]
"Moreover, it turns out that the P/R/F scores do not necessarily correlate with the performance of the TDNN model.",4 Results and Analyzes,0,[0]
"Take TDNN(Sem+Synt), the best TDNN variant, as an example: as indicated in Table 4, the performance and the P/R/F scores of the pseudo examples are only weakly correlated in most cases.
",4 Results and Analyzes,0,[0]
"To gain a better understanding in how the quality of pseudo examples affects the performance of TDNN, the sanctity of the selected essays are examined.",4 Results and Analyzes,0,[0]
"In Figure 5, the relative precision of
the selected positive and negative training data by RankSVM are displayed for all eight prompts in terms of their concordance with the manual ratings, by computing the number of positive (negative) essays that are better (worse) than all negative (positive) essays.",4 Results and Analyzes,0,[0]
"It can be seen that, such relative precision is at least 80% and mostly beyond 90% on different prompts, indicating that the overlap of the selected positive and negative essays are fairly small, guaranteeing that the deep model in the second stage at least learns from correct labels, which are crucial for the success of our TDNN model.
",4 Results and Analyzes,0,[0]
"Beyond that, we further investigate the class balance of the selected training data from the first
stage, which could also influence the ultimate results.",4 Results and Analyzes,0,[0]
"The number of selected positive and negative essays are reported in Table 5, where for prompts three and eight the training data suffers from serious imbalanced problem, which may explain their lower performance (namely, the two lowest QWKs among different prompts).",4 Results and Analyzes,0,[0]
"On one hand, this is actually determined by real distribution of ratings for a particular prompt, e.g., how many essays are with an extreme quality for a given prompt in the target data.",4 Results and Analyzes,0,[0]
"On the other hand, a fine-grained tuning of the RankSVM (e.g., tuning C+ and C− for positive and negative exam-
ples separately) may partially resolve the problem, which is left for the future work.",4 Results and Analyzes,0,[0]
"Classical regression and classification algorithms are widely used for learning the rating model based on a variety of text features including lexical, syntactic, discourse and semantic features (Larkey, 1998; Rudner, 2002; Attali and Burstein, 2006; Mcnamara et al., 2015; Phandi et al., 2015).",5 Related Work,0,[0]
There are also approaches that see AES as a preference ranking problem by applying learning to ranking algorithms to learn the rating model.,5 Related Work,0,[0]
"Results show improvement of learning to rank approaches over classical regression and classification algorithms (Chen et al., 2014; Yannakoudakis et al., 2011).",5 Related Work,0,[0]
"In addition, Chen & He propose to incorporate the evaluation metric into the loss function of listwise learning to rank for AES (Chen and He, 2013).
",5 Related Work,0,[0]
"Recently, there have been efforts in developing AES approaches based on deep neural networks (DNN), for which feature engineering is not required.",5 Related Work,0,[0]
"Taghipour & Ng explore a variety of neural network model architectures based on recurrent neural networks which can effectively encode the information required for essay scoring and learn the complex connections in the data through the non-linear neural layers (Taghipour and Ng, 2016).",5 Related Work,0,[0]
"Alikaniotis et al. introduce a neural network model to learn the extent to which specific words contribute to the text’s score, which
is embedded in the word representations.",5 Related Work,0,[0]
"Then a two-layer bi-directional Long-Short Term Memory networks (bi-LSTM) is used to learn the meaning of texts, and finally the essay score is predicted through a mutli-layer feed-forward network (Alikaniotis et al., 2016).",5 Related Work,0,[0]
"Dong & Zhang employ a hierarchical convolutional neural network (CNN) model, with a lower layer representing sentence structure and an upper layer representing essay structure based on sentence representations, to learn features automatically (Dong and Zhang, 2016).",5 Related Work,0,[0]
This model is later improved by employing attention layers.,5 Related Work,0,[0]
"Specifically, the model learns text representation with LSTMs which can model the coherence and co-reference among sequences of words and sentences, and uses attention pooling to capture more relevant words and sentences that contribute to the final quality of essays (Dong et al., 2017).",5 Related Work,0,[0]
"Song et al. propose a deep model for identifying discourse modes in an essay (Song et al., 2017).
",5 Related Work,0,[0]
"While the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a promptindependent setting remains to be explored.",5 Related Work,0,[0]
"Chen & He studied the usefulness of promptindependent text features and achieved a humanmachine rating agreement slightly lower than the use of all text features (Chen and He, 2013) for prompt-dependent essay scoring prediction.",5 Related Work,0,[0]
"A constrained multi-task pairwise preference learning approach was proposed in (Cummins et al., 2016) to combine essays from multiple prompts for training.",5 Related Work,0,[0]
"However, as shown by (Dong and Zhang, 2016; Zesch et al., 2015; Phandi et al., 2015), straightforward applications of existing AES methods for prompt-independent AES lead to a poor performance.",5 Related Work,0,[0]
"This study aims at addressing the promptindependent automated essay scoring (AES), where no rated essay for the target prompt is available.",6 Conclusions & Future Work,0,[0]
"As demonstrated in the experiments, two kinds of established prompt-dependent AES models, namely, RankSVM for AES (Yannakoudakis et al., 2011; Chen et al., 2014) and the deep models for AES (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017), fail to provide satisfactory performances, justifying our arguments in Section 1 that the application of estab-
lished prompt-dependent AES models on promptindependent AES is not straightforward.",6 Conclusions & Future Work,0,[0]
"Therefore, a two-stage TDNN learning framework was proposed to utilize the prompt-independent features to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consuming semantic, part-of-speech, and syntactic signals.",6 Conclusions & Future Work,0,[0]
"Through the experiments on the ASAP dataset, the proposed TDNN model outperforms the baselines, and leads to promising improvement in the human-machine agreement.
",6 Conclusions & Future Work,0,[0]
"Given that our approach in this paper is similar to the methods for transductive transfer learning (Pan and Yang, 2010), we argue that the proposed TDNN could be further improved by migrating the non-target training data to the target prompt (Busto and Gall, 2017).",6 Conclusions & Future Work,0,[0]
Further study of the uses of transfer learning algorithms on promptindependent AES needs to be undertaken.,6 Conclusions & Future Work,0,[0]
"This work is supported in part by the National Natural Science Foundation of China (61472391), and the Project of Beijing Advanced Innovation Center for Language Resources (451122512).",Acknowledgments,0,[0]
Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data.,abstractText,0,[0]
"Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available.",abstractText,0,[0]
"To close this gap, a two-stage deep neural network (TDNN) is proposed.",abstractText,0,[0]
"In particular, in the first stage, using the rated essays for nontarget prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step.",abstractText,0,[0]
Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.,abstractText,0,[0]
TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1237–1246 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1237",text,0,[0]
"The current leading perspective on temporal information extraction regards three phases: (1) a temporal entity recognition phase, extracting events (blue boxes in Fig. 1) and their attributes, and extracting temporal expressions (green boxes), and normalizing their values to dates or durations, (2) a relation extraction phase, where temporal links (TLinks) among those entities, and between events and the document-creation time (DCT) are found (arrows in Fig. 1, left).",1 Introduction,0,[0]
"And (3), construction of a time-line (Fig. 1, right) from the extracted temporal links, if they are temporally consistent.",1 Introduction,0,[0]
"Much research concentrated on the first two steps, but very little research looks into step 3, time-line construction, which is the focus of this work.
",1 Introduction,0,[0]
"In this paper, we propose a new time-line construction paradigm that evades phase 2, the relation extraction phase, because in the classical
paradigm temporal relation extraction comes with many difficulties in training and prediction that arise from the fact that for a text with n temporal entities (events or temporal expressions) there are n2 possible entity pairs, which makes it likely for annotators to miss relations, and makes inference slow as n2 pairs need to be considered.",1 Introduction,0,[0]
"Temporal relation extraction models consistently give lower performance than those in the entity recognition phase (UzZaman et al., 2013; Bethard et al., 2016, 2017), introducing errors in the time-line construction pipe-line.
",1 Introduction,0,[0]
"The ultimate goal of our proposed paradigm is to predict from a text in which entities are already detected, for each entity: (1) a probability distribution on the entity’s starting point, and (2) another distribution on the entity’s duration.",1 Introduction,0,[0]
The probabilistic aspect is crucial for time-line based decision making.,1 Introduction,0,[0]
"Constructed time-lines allow for further quantitative reasoning with the temporal information, if this would be needed for certain applications.
",1 Introduction,0,[0]
"As a first approach towards this goal, in this paper, we propose several initial time-line models in this paradigm, that directly predict - in a linear fashion - start points and durations for each entity, using text with annotated temporal entities as input (shown in Fig. 1).",1 Introduction,0,[0]
"The predicted start points and durations constitute a relative time-line, i.e. a total order on entity start and end points.",1 Introduction,0,[0]
"The time-line is relative, as start and duration values cannot (yet) be mapped to absolute calender dates or durations expressed in seconds.",1 Introduction,0,[0]
It represents the relative temporal order and inclusions that temporal entities have with respect to each other by the quantitative start and end values of the entities.,1 Introduction,0,[0]
"Relative time-lines are a first step toward our goal, building models that predict statistical absolute time-lines.",1 Introduction,0,[0]
"To train our relative time-line models, we define novel loss functions that exploit TimeML-style an-
notations, used in most existing temporal corpora.",1 Introduction,0,[0]
"This work leads to the following contributions:
• A new method to construct a relative time-line from a set of temporal relations (TL2RTL).
",1 Introduction,0,[0]
"• Two new models that, for the first time, directly predict (relative) time-lines - in linear complexity - from entity-annotated texts without doing a form of temporal relation extraction (S-TLM & C-TLM).
",1 Introduction,0,[0]
"• Three new loss functions based on the mapping between Allen’s interval algebra and the end-point algebra to train time-line models from TimeML-style annotations.
",1 Introduction,0,[0]
In the next sections we will further discuss the related work on temporal information extraction.,1 Introduction,0,[0]
"We will describe the models and training losses in detail, and report on conducted experiments.",1 Introduction,0,[0]
The way temporal information is conveyed in language has been studied for a long time.,2.1 Temporal Information Extraction,0,[0]
"It can be conveyed directly through verb tense, explicit temporal discourse markers (e.g. during or afterwards) (Derczynski, 2017) or temporal expressions such as dates, times or duration expressions (e.g. 10-05-2010 or yesterday).",2.1 Temporal Information Extraction,0,[0]
"Temporal information is also captured in text implicitly, through background knowledge about, for example, duration of events mentioned in the text (e.g. even without context, walks are usually shorter than journeys).
",2.1 Temporal Information Extraction,0,[0]
"Most temporal corpora are annotated with TimeML-style annotations, of which an example is shown in Fig 1, indicating temporal entities, their attributes, and the TLinks among them.
",2.1 Temporal Information Extraction,0,[0]
The automatic extraction of TimeML-style temporal information from text using machine learning was first explored by Mani et al. (2006).,2.1 Temporal Information Extraction,0,[0]
They proposed a multinomial logistic regression classifier to predict the TLinks between entities.,2.1 Temporal Information Extraction,0,[0]
"They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data.
",2.1 Temporal Information Extraction,0,[0]
"Since then, much research focused on further improving the pairwise classification models, by
exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017).",2.1 Temporal Information Extraction,0,[0]
"Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components.
",2.1 Temporal Information Extraction,0,[0]
"Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs.",2.1 Temporal Information Extraction,0,[0]
"To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017).",2.1 Temporal Information Extraction,0,[0]
"In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition.",2.1 Temporal Information Extraction,0,[0]
"Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990).",2.2 Temporal Reasoning,0,[0]
"Or (2), by first mapping the temporal interval expressions to expressions about interval end-points (start and endings of entities) (Vilain et al., 1990).",2.2 Temporal Reasoning,0,[0]
"An example of such mapping is that If event X occurs before Y then the end of X should be before the start of Y. Then reasoning can be done with end-points in a point algebra, which has only three point-wise relations (=, <,>), making reasoning much more efficient compared to reasoning with Allen’s thirteen interval relations.
",2.2 Temporal Reasoning,0,[0]
"Mapping interval relations to point-wise expressions has been exploited for model inference by Denis and Muller (2011), and for evaluation by UzZaman and Allen (2011).",2.2 Temporal Reasoning,0,[0]
"In this work, we ex-
ploit it for the first time for model training, in our loss functions.",2.2 Temporal Reasoning,0,[0]
"We propose two model structures for direct time-line construction: (1) a simple contextindependent model (S-TLM), and (2) a contextual model (C-TLM).",3 Models,0,[0]
Their structures are shown in Fig. 2.,3 Models,0,[0]
"Additionally, we propose a method to construct relative time-lines from a set of (extracted) TLinks (TL2RTL).",3 Models,0,[0]
"In this section we first explain the first two direct models S-TLM and C-TLM, and afterwards the indirect method TL2RTL.",3 Models,0,[0]
"In both S-TLM and C-TLM, words are represented as a concatenation of a word embedding, a POS embedding, and a Boolean feature vector containing entity attributes such as the type, class, aspect, following (Do et al., 2012).",Word representation,0,[0]
Further details on these are given in the experiments section.,Word representation,0,[0]
"For the simple context-independent time-line model, each entity is encoded by the word representation of the last word of the entity (generally the most important).",Simple Time-line Model (S-TLM),0,[0]
"From this representation we have a linear projection to the duration d, and the start s. S-TLM is shown by the dotted edges in Fig 2.",Simple Time-line Model (S-TLM),0,[0]
"An advantage of S-TLM is that it has very few parameters, and each entity can be placed on the time-line independently of the others, allowing parallelism during prediction.",Simple Time-line Model (S-TLM),0,[0]
The downside is that S-TLM is limited in its use of contextual information.,Simple Time-line Model (S-TLM),0,[0]
"To better exploit the entity context we also propose a contextual time-line model C-TLM (solid edges in Fig 2), that first encodes the full text using two bi-directional recurrent neural networks, one for entity starts (BiRNNs), and one for entity durations (BiRNNd).1 On top of the encoded text we learn two linear mappings, one from the BiRNNd output of the last word of the entity mention to its duration d, and similarly for the start time, from the BiRNNs output to the entity’s start s.",Contextual Time-line Model (C-TLM),0,[0]
"Both proposed models use linear mappings2 to predict the start value si and duration di for the encoded entity i. By summing start si and duration di we can calculate the entity’s end-point ei.
ei = si +max(di, dmin) (1)
","Predicting Start, Duration, and End",0,[0]
"Predicting durations rather than end-points makes it easy to control that the end-point lies after the start-point by constraining the duration di by a constant minimum duration value dmin above 0, as shown in Eq. 1.","Predicting Start, Duration, and End",0,[0]
"Although the DCT is often not found explicitly in the text, it is an entity in TimeML, and has TLinks to other entities.",Modeling Document-Creation Time,0,[0]
"We model it by assigning it a text-independent start sDCT and duration dDCT.
",Modeling Document-Creation Time,0,[0]
Start sDCT is set as a constant (with value 0).,Modeling Document-Creation Time,0,[0]
"This way the model always has the same reference point, and can learn to position the entities w.r.t.",Modeling Document-Creation Time,0,[0]
"the DCT on the time-line.
",Modeling Document-Creation Time,0,[0]
1We also experimented with sharing weights among BiRNNd and BiRNNs.,Modeling Document-Creation Time,0,[0]
"In our experiments, this gave worse performance, so we propose to keep them separate.
",Modeling Document-Creation Time,0,[0]
"2Adding more layers did not improve results.
",Modeling Document-Creation Time,0,[0]
"In contrast, DCT duration dDCT is modeled as a single variable that is learned (initialized with 1).",Modeling Document-Creation Time,0,[0]
"Since multiple entities may be included in the DCT, and entities have a minimum duration dmin, a constant dDCT could possibly prevent the model from fitting all entities in the DCT.",Modeling Document-Creation Time,0,[0]
Modeling dDCT as a variable allows growth of dDCT and averts this issue.3,Modeling Document-Creation Time,0,[0]
"We propose three loss functions to train time-line models from TimeML-style annotations: a regular time-line loss Lτ , and two slightly expanded discriminative time-line losses, Lτce and Lτh.",Training Losses,0,[0]
Ground-truth TLinks can be seen as constraints on correct positions of entities on a time-line.,Regular Time-line Loss (Lτ ),0,[0]
The regular time-line loss Lτ expresses the degree to which these constraints are met for a predicted time-line.,Regular Time-line Loss (Lτ ),0,[0]
"If all TLinks are satisfied in the timeline for a certain text, Lτ will be 0 for that text.
",Regular Time-line Loss (Lτ ),0,[0]
"As TLinks relate entities (intervals), we first convert the TLinks to expressions that relate the start and end points of entities.",Regular Time-line Loss (Lτ ),0,[0]
"How each TLink is translated to its corresponding point-algebraic constraints is given in Table 1, following Allen (1990).
",Regular Time-line Loss (Lτ ),0,[0]
"As can be seen in the last column there are only two point-wise operations in the point-algebraic constraints: an order operation (<), and an equality operation (=).",Regular Time-line Loss (Lτ ),0,[0]
"To model to what degree each point-wise constraint is met, we employ hinge losses, with a margin mτ , as shown in Eq. 2.
",Regular Time-line Loss (Lτ ),0,[0]
"3Other combinations of modeling sDCT and dDCT as variable or constant decreased performance.
",Regular Time-line Loss (Lτ ),0,[0]
"4No TLink for Allen’s overlap relation is present in TimeML, also concluded by UzZaman and Allen (2011).
",Regular Time-line Loss (Lτ ),0,[0]
To explain the intuition and notation: If we have a point-wise expression ξ of the form,Regular Time-line Loss (Lτ ),0,[0]
x < y,Regular Time-line Loss (Lτ ),0,[0]
"(first case of Eq. 2), then the predicted point x̂ should be at least a distance mτ smaller (or earlier on the time-line) than predicted point ŷ",Regular Time-line Loss (Lτ ),0,[0]
in order for the loss to be 0.,Regular Time-line Loss (Lτ ),0,[0]
"Otherwise, the loss represents the distance x̂ or ŷ still has to move to make x̂ smaller than ŷ (and satisfy the constraint).",Regular Time-line Loss (Lτ ),0,[0]
"For the second case, if ξ is of the form x = y, then point x̂ and ŷ should lie very close to each other, i.e. at most a distance mτ away from each other.",Regular Time-line Loss (Lτ ),0,[0]
Any distance further than the margin mτ is counted as loss.,Regular Time-line Loss (Lτ ),0,[0]
"Notice that if we set margin mτ to 0, the second case becomes an L1 loss |x̂",Regular Time-line Loss (Lτ ),0,[0]
− ŷ|.,Regular Time-line Loss (Lτ ),0,[0]
"However, we use a small margin mτ to promote some distance between ordered points and prevent con-
fusion with equality.",Regular Time-line Loss (Lτ ),0,[0]
"Fig. 3 visualizes the loss for three TLinks.
Lp(ξ|t, θ) = { max(x̂+mτ − ŷ, 0) iff x < y max(|x̂−",Regular Time-line Loss (Lτ ),0,[0]
"ŷ| −mτ , 0) iff x = y
(2)
",Regular Time-line Loss (Lτ ),0,[0]
"The total time-line loss Lτ (t|θ) of a model with parameters θ on text t with ground-truth TLinks R(t), is the sum of the TLink-level losses of all TLinks r ∈ R(t).",Regular Time-line Loss (Lτ ),0,[0]
"Each TLink-level loss Lr(r|t, θ) for TLink r is the sum of the pointwise losses Lp(ξ|t, θ) of the corresponding pointalgebraic constraints ξ ∈ IPA(r) from Table 1.5
Lr(r|t, θ) = ∑
ξ∈IPA(r)
",Regular Time-line Loss (Lτ ),0,[0]
"Lp(ξ|t, θ) (3)
",Regular Time-line Loss (Lτ ),0,[0]
"Lτ (t, θ) = ∑ r∈R(t) Lr(r|t, θ) (4)",Regular Time-line Loss (Lτ ),0,[0]
"To promote a more explicit difference between the relations on the time-line we introduce two discriminative loss functions, Lτce and Lτh, which build on top of Lr.",Discriminative Time-line Losses,0,[0]
"Both discriminative loss functions use an intermediate score S(r|t, θ) for each TLink r based on the predicted time-line.",Discriminative Time-line Losses,0,[0]
"As scoring function, we use the negativeLr loss, as shown in Eq. 5.
S(r|t, θ) = −Lr(r|t, θ) (5) 5The TLink during and its inverse are mapped to simulta-
neous, following the evaluation of TempEval-3.
",Discriminative Time-line Losses,0,[0]
"Then, a lower time-line loss Lr(r|t, θ) results in a higher score for relation type r. Notice that the maximum score is 0, as this is the minimum Lr.",Discriminative Time-line Losses,0,[0]
Our first discriminative loss is a cross-entropy based loss.,Probabilistic Loss (Lτce),0,[0]
For this the predicted scores are normalized using a soft-max over the possible relation types (TL).,Probabilistic Loss (Lτce),0,[0]
"The resulting probabilities are used to calculate a cross-entropy loss, shown in Eq. 6.",Probabilistic Loss (Lτce),0,[0]
"This way, the loss does not just promote the correct relation type but also distantiates from the other relation types.
",Probabilistic Loss (Lτce),0,[0]
"Lτce(t|θ) = ∑ r∈R(t) r · log ( eS(r|t,θ)∑ r′∈TL e S(r′|t,θ) )",Probabilistic Loss (Lτce),0,[0]
-6,Probabilistic Loss (Lτce),0,[0]
"When interested in discriminating relations on the time-line, we want the correct relation type to have the highest score from all possible relation types TL.",Ranking Loss (Lτh),0,[0]
"To represent this perspective, we also define a ranking loss with a score margin mh in Eq. 7.
",Ranking Loss (Lτh),0,[0]
Lτh(t|θ),Ranking Loss (Lτh),0,[0]
"=∑ r∈R(t) ∑ r′∈TL\{r} max(S(r′|t, θ)−S(r|t, θ)+mh, 0)
(7)",Ranking Loss (Lτh),0,[0]
"S-TLM and C-TLM are trained by by iterating through the training texts, sampling mini-batches of 32 annotated TLinks.",Training Procedure,0,[0]
"For each batch we (1) perform a forward pass, (2) calculate the total loss (for one of the loss functions), (3) derive gradients using Adam6 (Kingma and Ba, 2014), and (4) update the model parameters θ via back-propagation.",Training Procedure,0,[0]
After each epoch we shuffle the training texts.,Training Procedure,0,[0]
"As stopping criteria we use early stopping (Morgan and Bourlard, 1990), with a patience of 100 epochs and a maximum number of 1000 epochs.",Training Procedure,0,[0]
"To model the indirect route, we construct a novel method, TL2RTL, that predicts relative time lines from a subset of TLinks, shown in Fig 1.",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"One can choose any method to obtain a set of TLinks R(t) from a text t, serving as input to TL2RTL.
",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"6Using the default parameters from the paper.
",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"TL2RTL constructs a relative time-line, by assigning start and end values to each temporal entity, such that the resulting time-line satisfies the extracted TLinksR(t) by minimizing a loss function that is 0 when the extracted TLinks are satisfied.",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
TL2RTL on itself is a method and not a model.,3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"The only variables over which it optimizes the loss are the to be assigned starts and duration values.
",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"In detail, for a text t, with annotated entities E(t), we first extract a set of TLinks R(t).",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"In this work, to extract TLinks, we use the current state-of-the-art structured TLink extraction model by Ning et al. (2017).",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"Secondly, we assign a start variable si, and duration variable di to each entity i ∈ E(t).",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"Similar to S-TLM and C-TLM, for each i ∈ E(t), di is bounded by a minimum duration dmin to ensure start si always lies before end ei.",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"Also, we model the DCT start sDCT as a constant, and its duration dDCT as a variable.",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"Then we minimize one of the loss functions Lτ , Lτce, or Lτh on the extracted TLinks R(t), obtaining three TL2RTL variants, one for each loss.",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"If the initially extracted set of TLinks R(t) is consistent, and the loss is minimized sufficiently, all si and di form a relative time-line that satisfies the TLinks R(t), but from which we can now also derive consistent TLinks for any entity pair, also the pairs that were not in R(t).",3.2 From TLinks to Time-lines (TL2RTL),0,[0]
To minimize the loss we use Adam for 10k epochs until the loss is zero for each document.7,3.2 From TLinks to Time-lines (TL2RTL),0,[0]
"Because prediction of relative time-lines trained on TimeML-style annotations is new, we cannot compare our model directly to relation extraction or classification models, as the latter do not provide completely temporally consistent TLinks for all possible entity pairs, like the relative timelines do.",4.1 Evaluation and Data,0,[0]
"Neither can we compare directly to existing absolute time-line prediction models such as Reimers et al. (2018) because they are trained on different data with a very different annotation scheme.
",4.1 Evaluation and Data,0,[0]
"To evaluate the quality of the relative time-line models in a fair way, we use TimeML-style test sets as follows: (1) We predict a time-line for each test-text, and (2) we check for all ground-truth an-
7For some documents the extracted TLinks were temporally inconsistent, resulting in a non-zero loss.",4.1 Evaluation and Data,0,[0]
"Nevertheless, > 96% of the extracted TLinks were satisfied.
",4.1 Evaluation and Data,0,[0]
"notated TLinks that are present in the data, what would be the derived relation type based on the predicted time-line, which is the relation type that gives the lowest time-line loss Lr.",4.1 Evaluation and Data,0,[0]
"This results in a TLink assignment for each annotated pair in the TimeML-style reference data, and therefor we can use similar metrics.",4.1 Evaluation and Data,0,[0]
"As evaluation metric we employ the temporal awareness metric, used in TempEval-3, which takes into account temporal closure (UzZaman et al., 2013).",4.1 Evaluation and Data,0,[0]
"Notice that although we use the same metric, comparison against relation classification systems would be unfair, as our model assigns consistent labels to all pairs, whereas relation classification systems do not.
",4.1 Evaluation and Data,0,[0]
"For training and evaluation we use two data splits, TE‡ and TD‡, exactly following Ning et al. (2017).",4.1 Evaluation and Data,0,[0]
"Some statistics about the data are shown in Table 2.8 The splits are constituted from various smaller datasets: the TimeBank (TB) (Pustejovsky et al., 2002), the AQUANT dataset (AQ), and the platinum dataset (PT) all from TempEval-3 (UzZaman et al., 2013).",4.1 Evaluation and Data,0,[0]
"And, the TimeBank Dense (Cassidy et al., 2014) , and the Verb-Clause dataset (VC) (Bethard et al., 2007).",4.1 Evaluation and Data,0,[0]
Hyper-parameters shared in all settings can be found in Table 3.,4.2 Hyper-parameters and Preprocessing,0,[0]
"The following hyper-parameters are tuned using grid search on a development set (union of TB and AQ): dmin is chosen from {1, 0.1, 0.01}, mτ from {0, 0.025, 0.05, 0.1}, αd from {0, 0.1, 0.2, 0.4, 0.8}, and αrnn from {10, 25, 50}.",4.2 Hyper-parameters and Preprocessing,0,[0]
"We use LSTM (Hochreiter and Schmidhuber, 1997) as RNN units9 and employ 50-dimensional GloVe word-embeddings pre-trained10 on 6B words (Wikipedia and NewsCrawl) to initialize the models’ word embeddings.
",4.2 Hyper-parameters and Preprocessing,0,[0]
"We use very simple tokenization and consider punctuation11 or newline tokens as individual tokens, and split on spaces.",4.2 Hyper-parameters and Preprocessing,0,[0]
"Additionally, we lowercase the text and use the Stanford POS Tagger (Toutanova et al., 2003) to obtain POS.
8We explicitly excluded all test documents from training as some corpora annotated the same documents.
",4.2 Hyper-parameters and Preprocessing,0,[0]
"9We also experimented with GRU as RNN type, obtaining similar results.
",4.2 Hyper-parameters and Preprocessing,0,[0]
"10https://nlp.stanford.edu/projects/glove 11, ./\""’=+-;:()!?<>%&$*|",4.2 Hyper-parameters and Preprocessing,0,[0]
[]{},4.2 Hyper-parameters and Preprocessing,0,[0]
"We compared our three proposed models for the three loss functions Lτ , Lτce, and Lτh, and their linear (unweighted) combination L∗, on TE3‡ and TD‡, for which the results are shown in Table 4.
",5 Results,0,[0]
"A trend that can be observed is that overall performance on TD‡ is higher than that of TE3‡, even though less documents are used for training.",5 Results,0,[0]
"We inspected why this is the case, and this is caused by a difference in class balance between both test sets.",5 Results,0,[0]
"In TE3‡ there are many more TLinks of type simultaneous (12% versus 3%), which are very
difficult to predict, resulting in lower scores for TE3‡ compared to TD‡.",5 Results,0,[0]
"The difference in performance between the datasets is probably also be related to the dense annotation scheme of TD‡ compared to the sparser annotations of TE3‡, as dense annotations give a more complete temporal view of the training texts.",5 Results,0,[0]
"For TL2RTL better TLink extraction12 is also propagated into the final timeline quality.
",5 Results,0,[0]
"If we compare loss functions Lτ , Lτce, and Lτh, and combination L∗, it can be noticed that, although all loss functions seem to give fairly similar performance, Lτ gives the most robust results (never lowest), especially noticeable for the smaller dataset TD‡.",5 Results,0,[0]
"This is convenient, because Lτ is fastest to compute during training, as it requires no score calculation for each TLink type.",5 Results,0,[0]
Lτ is also directly interpretable on the timeline.,5 Results,0,[0]
"The combination of losses L∗ shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL.",5 Results,0,[0]
"However, it is slowest to compute, and less interpretable, as it is a combined loss.
",5 Results,0,[0]
"Moreover, we can clearly see that on TE3‡, CTLM performs better than the indirect models, across all loss functions.",5 Results,0,[0]
"This is a very interesting result, as C-TLM is an order of complexity faster in prediction speed compared to the indirect models (O(n) compared to O(n2) for a text with n",5 Results,0,[0]
entities).13,5 Results,0,[0]
"We further explore why this is the case through our error analysis in the next section.
",5 Results,0,[0]
"On TD‡, the indirect models seem to perform slightly better.",5 Results,0,[0]
"We suspect that the reason for this is that C-TLM has more parameters (mostly the LSTM weights), and thus requires more data (TD‡ has much fewer documents than TE3‡) compared to the indirect methods.",5 Results,0,[0]
"Another result supporting this hypothesis is the fact that the difference between C-TLM and S-TLM is small on the smaller
12F1 of 40.3 for TE3‡ and 48.5 for TD‡ (Ning et al., 2017)",5 Results,0,[0]
"13We do not directly compare prediction speed, as it would result in unfair evaluation because of implementation differences.",5 Results,0,[0]
"However, currently, C-TLM predicts at∼100 w/s incl.",5 Results,0,[0]
"POS tagging, and ∼2000 w/s without.",5 Results,0,[0]
"When not using POS, overall performance decreases consistently with 2-4 points.
TD‡, indicating that C-TLM does not yet utilize contextual information from this dataset, whereas, in contrast, on TE3‡, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions, showing that when enough data is available C-TLM learns good LSTM weights that exploit context substantially.",5 Results,0,[0]
"We compared predictions of TL2RTL(Lτ ) with those of C-TLM (Lτ ), the best models of each paradigm.",6 Error Analysis,0,[0]
"In Table 4, we show the confusion matrices of both systems on TE3‡.
When looking at the overall pattern in errors, both models seem to make similar confusions on both datasets (TD‡ was excluded for space constraints).",6 Error Analysis,0,[0]
"Overall, we find that simultaneous is the most violated TLink for both models.",6 Error Analysis,0,[0]
This can be explained by two reasons: (1) It is the least frequent TLink in both datasets.,6 Error Analysis,0,[0]
"And (2), simultaneous entities are often co-referring events.",6 Error Analysis,0,[0]
"Event co-reference resolution is a very difficult task on its own.
",6 Error Analysis,0,[0]
We also looked at the average token-distance between arguments of correctly satisfied TLinks by the time-lines of each model.,6 Error Analysis,0,[0]
For TL2RTL (Lτ ),6 Error Analysis,0,[0]
"this is 13 tokens, and for C-TLM (Lτ ) 15.",6 Error Analysis,0,[0]
"When looking only at the TLinks that C-TLM (Lτ ) satisfied and TL2RTL (Lτ ) did not, the average distance is 21.",6 Error Analysis,0,[0]
These two observations suggest that the direct C-TLM (Lτ ) model is better at positioning entities on the time-line that lie further away from each other in the text.,6 Error Analysis,0,[0]
"An explanation for this can be error propagation of TLink extraction to the time-line construction, as the pairwise TLink extraction of the indirect paradigm extracts TLinks in a contextual window, to prune the O(n2) number of possible TLink candidates.",6 Error Analysis,0,[0]
"This
consequently prevents TL2RTL to properly position distant events with respect to each other.
",6 Error Analysis,0,[0]
To get more insight in what the model learns we calculated mean durations and mean starts of CTLM (Lτ ) predictions.,6 Error Analysis,0,[0]
"Table 5 contains examples from the top-shortest, and top-longest duration assignments and earliest and latest starting points.",6 Error Analysis,0,[0]
We observe that events that generally have more events included are assigned longer duration and vice versa.,6 Error Analysis,0,[0]
"And, events with low start values are in the past tense and events with high start values are generally in the present (or future) tense.",6 Error Analysis,0,[0]
"A characteristic of our model is that it assumes that all events can be placed on a single timeline, and that it does not assume that unlabeled pairs are temporally unrelated.",7 Discussion,0,[0]
"This has big advantages: it results in fast prediction, and missed annotation do not act as noise to the training, as they do for pairwise models.",7 Discussion,0,[0]
"Ning et al. (2018) argue that actual, negated, hypothesized, expected or opinionated events should possibly be annotated
on separate time-axis.",7 Discussion,0,[0]
We believe such multi-axis representations can be inferred from the generated single time-lines if hedging information is recognized.,7 Discussion,0,[0]
"This work leads to the following three main contributions14: (1) Three new loss functions that connect the interval-based TimeML-annotations to points on a time-line, (2) A new method, TL2RTL, to predict relative time-lines from a set of predicted temporal relations.",8 Conclusions,0,[0]
"And (3), most importantly, two new models, S-TLM and C-TLM, that – to our knowledge for the first time – predict (relative) time-lines in linear complexity from text, by evading the computationally expensive (often O(n2))",8 Conclusions,0,[0]
intermediate relation extraction phase in earlier work.,8 Conclusions,0,[0]
"From our experiments, we conclude that the proposed loss functions can be used effectively to train direct and indirect relative time-line models, and that, when provided enough data, the – much faster – direct model C-TLM outperforms the indirect method TL2RTL.
",8 Conclusions,0,[0]
"As a direction for future work, it would be very interesting to extend the current models, diving further into direct time-line models, and learn to predict absolute time-lines, i.e. making the time-lines directly mappable to calender dates and times, e.g. by exploiting complementary data sources such as the EventTimes Corpus (Reimers et al., 2016) and extending the current loss functions accordingly.",8 Conclusions,0,[0]
"The proposed models also provide a good starting point for research into probabilistic time-line models, that additionally model the (un)certainty of the predicted positions and durations of the entities.",8 Conclusions,0,[0]
The authors thank Geert Heyman and the reviewers for their constructive comments which helped us to improve the paper.,Acknowledgments,0,[0]
"This work was funded by the KU Leuven C22/15/16 project ”MAchine Reading of patient recordS (MARS)”, and by the IWT-SBO 150056 project ”ACquiring CrUcial Medical information Using LAnguage TEchnology” (ACCUMULATE).
",Acknowledgments,0,[0]
14Code is available at: liir.cs.kuleuven.be/software.php,Acknowledgments,0,[0]
"The current leading paradigm for temporal information extraction from text consists of three phases: (1) recognition of events and temporal expressions, (2) recognition of temporal relations among them, and (3) time-line construction from the temporal relations.",abstractText,0,[0]
"In contrast to the first two phases, the last phase, time-line construction, received little attention and is the focus of this work.",abstractText,0,[0]
"In this paper, we propose a new method to construct a linear time-line from a set of (extracted) temporal relations.",abstractText,0,[0]
"But more importantly, we propose a novel paradigm in which we directly predict start and end-points for events from the text, constituting a time-line without going through the intermediate step of prediction of temporal relations as in earlier work.",abstractText,0,[0]
"Within this paradigm, we propose two models that predict in linear complexity, and a new training loss using TimeML-style annotations, yielding promising results.",abstractText,0,[0]
Temporal Information Extraction by Predicting Relative Time-lines,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 162–171 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
162
is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.",text,0,[0]
We examine the task of Natural Sentence Grounding in Video (NSGV).,1 Introduction,0,[0]
"Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sentence, as shown in Figure 1 (a).",1 Introduction,0,[0]
"Comparing with the other video researches, such as bidirectional video-sentence retrieval (Xu et al., 2015b), video attractiveness prediction (Chen et al., 2018, 2016), and video captioning (Pasunuru and Bansal, 2017; Wang et al., 2018a,b), NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging.
",1 Introduction,0,[0]
"∗ Work done while Jingyuan Chen and Xinpeng Chen were Research Interns with Tencent AI Lab.
",1 Introduction,0,[0]
1,1 Introduction,0,[0]
"The project homepage is https:// jingyuanchen.github.io/archive/tgn.html.
",1 Introduction,0,[0]
"Recently, several related works (Gao et al., 2017; Hendricks et al., 2017) leverage one temporal sliding window approach over video sequences to generate video segment candidates, which are then independently combined (Gao et al., 2017) or compared (Hendricks et al., 2017) with the given sentence to make the grounding prediction.",1 Introduction,0,[0]
"Although the existing works have achieved promising performances, they are still suffering from inferior effectiveness and efficiency.",1 Introduction,0,[0]
"First, existing methods project the video segment and sentence into one common space, as shown in Figure 1 (b),
where the two generated embedding vectors are used to perform the matching between video segment and sentence.",1 Introduction,0,[0]
"Such a matching is only performed in the global segment and sentence level and thus not expressive enough, which ignores the fine-grained matching relations between video frames and the words in sentences.",1 Introduction,0,[0]
"Second, in order to handle the diverse temporal scales and locations of the candidate segments, exhaustive matching between the large amount of overlapping segments and the sentence is required.",1 Introduction,0,[0]
"As such, the sliding window methods are very computationally expensive.
",1 Introduction,0,[0]
"In order to tackle the above two limitations, we introduce a novel Temporal GroundNet (TGN) model, the first dynamic single-stream deep architecture for the NSGV task that takes full advantage of fine-grained interactions between video frames and words in a sentence, as shown in Figure 1 (c).",1 Introduction,0,[0]
"TGN sequentially processes video frames, where at each time step we rely on a novel multimodal interactor to exploit the evolving fine-grained frameby-word interactions.",1 Introduction,0,[0]
"Then, TGN works on the yielded interaction status to simultaneously score a set of temporal candidates of multiple scales and finally localize the video segment that corresponds to the sentence.",1 Introduction,0,[0]
"More importantly, our proposed TGN is able to analyze an untrimmed video frame by frame without resorting to handling overlapping temporal video segments.",1 Introduction,0,[0]
Grounding natural language in image is also known as natural language object retrieval.,2.1 Grounding Natural Language in Image,0,[0]
"The task is to localize an image region described by natural language, which involves comprehending and modeling different spatial contexts, such as spatial configurations (Hu et al., 2016), attributes (Yu et al., 2018), and relationships between objects (Hu et al., 2017).",2.1 Grounding Natural Language in Image,0,[0]
"Specifically, the task is usually formulated as a ranking problem over a set of candidate regions in a given image, where candidate spatial locations come from region proposal methods (Uijlings et al., 2013; Jie et al., 2016b,a; Ren et al., 2017) such as EdgeBox (Zitnick and Dollár, 2014).",2.1 Grounding Natural Language in Image,0,[0]
"Earlier studies (Mao et al., 2016; Rohrbach et al., 2016) score the generated candidate regions according to their appearances and spatial features along with features of the entire image.",2.1 Grounding Natural Language in Image,0,[0]
"However, these meth-
ods fail to incorporate the interactions between objects, because the scoring process of each region proposal is isolated.",2.1 Grounding Natural Language in Image,0,[0]
"More recent studies (Hu et al., 2017; Nagaraja et al., 2016) improve the performance with the aid of modeling relationships between objects.",2.1 Grounding Natural Language in Image,0,[0]
"Analogous to spatial grounding in image, this work studies a similar problem—temporal natural language grounding in video.",2.2 Grounding Natural Language in Video,0,[0]
"Earlier works (Yu and Siskind, 2013; Lin et al., 2014) learn the semantics of sentences, which are then matched to visual concepts via exploiting object appearance, motion and spatial relationships.",2.2 Grounding Natural Language in Video,0,[0]
"However, they are limited to a small set of objects.",2.2 Grounding Natural Language in Video,0,[0]
"Recently, larger datasets (Gao et al., 2017; Hendricks et al., 2017) are constructed to support more flexible groundings.",2.2 Grounding Natural Language in Video,0,[0]
"The methods proposed in (Gao et al., 2017; Hendricks et al., 2017) learn a common embedding space shared by video segment features and sentence representations, in which their similarities are measured.",2.2 Grounding Natural Language in Video,0,[0]
"Specifically, moment context network (MCN) (Hendricks et al., 2017) learns a shared embedding for video clip-level features and language features.",2.2 Grounding Natural Language in Video,0,[0]
"The video features integrate local video features, global features, and temporal endpoint features.",2.2 Grounding Natural Language in Video,0,[0]
"Cross-modal temporal regression localizer (CTRL) (Gao et al., 2017) contains four modules, specifically a visual encoder extracting clip-level features with context, a sentence encoder yielding its embedding through LSTM, a multimodal processing network generating the fused representations via element-wise operations, and a temporal regression network producing the alignment scores and location offsets.",2.2 Grounding Natural Language in Video,0,[0]
"One limitation of those common space matching methods is that the video segment generation process is computationally expensive, as they carry out overlapping sliding window matching (Gao et al., 2017) or exhaustive search (Hendricks et al., 2017).",2.2 Grounding Natural Language in Video,0,[0]
"Another weakness is that they exploit the relationships between textual and visual modalities by conducting a simple concatenation (Gao et al., 2017) or measuring a squared distance loss (Hendricks et al., 2017), which ignores the evolving fine-grained video-sentence interactions.",2.2 Grounding Natural Language in Video,0,[0]
"In this paper, a novel model TGN is proposed to deal with the aforementioned limitations for the task of natural sentence grounding in video.",2.2 Grounding Natural Language in Video,0,[0]
"Given a long and untrimmed video sequence V and a natural sentence S, the NSGV task is to localize a video segment Vs = {ft}tet=tb from V , beginning at tb and ending at te, which corresponds to and expresses the same semantic meaning as the given sentence S.",3 Approach,0,[0]
"In order to perform the grounding, each video is represented as V = {ft}Tt=1, where T is the total number of frames and ft denotes the feature representation of the t-th video frame.",3 Approach,0,[0]
"Similarly, each sentence is represented as S = {wn}Nn=1, where wn is the embedding vector of the n-th word in the sentence",3 Approach,0,[0]
"andN denotes the total number of words.
",3 Approach,0,[0]
"We propose a novel model, namely Temporal GroundNet (TGN), to tackle the NSGV problem.",3 Approach,0,[0]
"As illustrated in Figure 2, TGN consists of three modules.",3 Approach,0,[0]
"1) Encoder: visual and textual encoders are used to compose the video frame representations and word embeddings, respectively.",3 Approach,0,[0]
2) Interactor: a multimodal interactor learns the frame-byword interactions between the video and sentence.,3 Approach,0,[0]
3) Grounder: a grounder generates the temporal localization in one single pass.,3 Approach,0,[0]
"Please note that these three modules are fully coupled together, which can thus be trained in an end-to-end fashion.",3 Approach,0,[0]
"With the obtained video frame features V = {ft}Tt=1 and word embeddings of the sentence S = {wn}Nn=1, we employ two long shortterm memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to sequentially process the two different modalities, i.e., video and sentence, independently.",3.1 Encoder,0,[0]
"Specifically, one LSTM sequentially models the video V , yielding the hidden states {hvt }Tt=1, while the other LSTM processes the sequential words in the sentence S, resulting in its corresponding hidden states {hsn}Nn=1.",3.1 Encoder,0,[0]
"Owing to natural behaviors and characteristics of LSTMs, both {hvt }Tt=1 and {hsn}Nn=1 can encode and aggregate the contextual evidences (Wang and Jiang, 2016b) from the sequential video frame representations and word embeddings of the sentence, respectively, meanwhile casting aside the irrelevant information.",3.1 Encoder,0,[0]
"Based on the hidden states of the video and sentence yielded from the leveraged encoders, we de-
sign a multimodal interactor to perform the frameby-word interactions between the video and sentence.",3.2 Interactor,0,[0]
"First, the frame-specific sentence feature is generated through summarizing the sentence hidden states by considering their relationships with the specific video frame at each time step.",3.2 Interactor,0,[0]
"Afterwards, an interaction LSTM, dubbed i-LSTM, is performed to aggregate frame-by-word interactions.",3.2 Interactor,0,[0]
Directly operating on the clip-level and sentencelevel features generated by the encoders cannot well exploit the frame-by-word relationships between video and sentence that evolve over time.,3.2.1 Frame-Specific Sentence Feature,0,[0]
"Inspired by (Wang and Jiang, 2016a; Feng et al., 2018), we introduce one novel frame-specific sentence feature, which adaptively summarizes the hidden states of the sentence {hsn}Nn=1 with respect to the t-th video frame:
",3.2.1 Frame-Specific Sentence Feature,0,[0]
"Hst = N∑
n=1
αnt h s n, (1)
where Hst denotes the summarized sentence representation specified by the t-th video frame.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"At each time step t, we utilize the hidden state hvt to
selectively attend the words and summarize them accordingly.",3.2.1 Frame-Specific Sentence Feature,0,[0]
The attention weight αnt encodes the degree to which the n-th word in the sentence is aligned with the t-th video frame.,3.2.1 Frame-Specific Sentence Feature,0,[0]
"As the processing of video frames proceeds, the attention weights dynamically change regarding to the current video frame.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"As such, the generated framespecific sentence features {Hst}Tt=1 consider the frame-by-word relationships between all the video frames and all the words in the sentence.
",3.2.1 Frame-Specific Sentence Feature,0,[0]
"As the generation of frame-specific sentence feature is deeply coupled with the following interaction LSTM, we will explain the calculation of the attention weight αnt later.
",3.2.1 Frame-Specific Sentence Feature,0,[0]
3.2.2 Interaction LSTM (i-LSTM),3.2.1 Frame-Specific Sentence Feature,0,[0]
"In order to accurately ground the sentence in a video, the multimodal interation behaviors between the video and sentence need to be comprehensively modeled.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"Previous approaches on multimodal interactions were limited to concatenation (Zhu et al., 2016), element-wise product or sum (Gao et al., 2017), and bilinear pooling (Fukui et al., 2016).",3.2.1 Frame-Specific Sentence Feature,0,[0]
"These methods are not expressive enough since they ignore the evolving fine-grained interactions across video and sentence, particularly the frame-by-word interactions.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"In this paper, we propose a novel multimodal interaction model, which is realized by LSTM.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"We term it interaction LSTM (i-LSTM), which sequentially processes the video sequence frame by frame, holding deep interactions with the words in the sentence.
",3.2.1 Frame-Specific Sentence Feature,0,[0]
"In order to well capture the complicated temporal interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th video hidden state hvt and the t-th frame-specific sentence feature Hst as:",3.2.1 Frame-Specific Sentence Feature,0,[0]
rt = h v t ‖ Hst .,3.2.1 Frame-Specific Sentence Feature,0,[0]
"rt is then fed into the i-LSTM unit to yield the t-th intermediate interaction status between the video and sentence:
hrt = i-LSTM(rt,h r t−1), (2)
where hrt is the yielded hidden state, encoding the fine-grained interactions between the word and video frame.",3.2.1 Frame-Specific Sentence Feature,0,[0]
hrt will be further used to perform the grounding process.,3.2.1 Frame-Specific Sentence Feature,0,[0]
"Due to the inherent properties and characteristics of LSTMs, important cues regarding to grounding up to the current stage will be “remembered”, while non-essential ones will be “forgotten”.
",3.2.1 Frame-Specific Sentence Feature,0,[0]
Now we go back to the generation of attention weight αnt in Eq.,3.2.1 Frame-Specific Sentence Feature,0,[0]
"(1), based on the obtained vi-
sual hidden states hvt and textual hidden state h s n as well as the yielded interaction status hrt−1 in the previous step.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"The widely used soft-attention mechanism (Xu et al., 2015a; Chen et al., 2017) is used to generate the attention weights in a frameby-word manner.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"As aforementioned, the i-LSTM models the evolving frame-by-word interactions between the sentence and video.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"Therefore, the attention weight between the n-th word hsn and the t-th video frame hvt is determined by not only the content of the video and sentence but also their interaction status.",3.2.1 Frame-Specific Sentence Feature,0,[0]
"Thus, we design one network to compute the relevance score of one video frame with respect to each word:
βnt = w ᵀ tanh(WShsn+W V hvt +W Rhrt−1+b)+c, (3)
where vector w, matrices W∗, bias vector b, and bias c are the network parameters to be learned.",3.2.1 Frame-Specific Sentence Feature,0,[0]
hrt−1 is the hidden state of the i-LSTM at t − 1 time step.,3.2.1 Frame-Specific Sentence Feature,0,[0]
"The final word-level attention weights are obtained by:
αnt = exp(βnt )",3.2.1 Frame-Specific Sentence Feature,0,[0]
∑N j=1 exp(β j t ) .,3.2.1 Frame-Specific Sentence Feature,0,[0]
"(4)
The obtained attention weight αnt is thereafter to generate the frame-specific sentence feature as in Eq.",3.2.1 Frame-Specific Sentence Feature,0,[0]
(1).,3.2.1 Frame-Specific Sentence Feature,0,[0]
"In this section, we introduce the grounder, which works on the yielded interaction status hrt from i-LSTM, to localize the video segment that corresponds to the sentence.",3.3 Grounder,0,[0]
"Our proposed grounder works in one single pass without introducing overlapping sliding windows, which thus results in a fast runtime.",3.3 Grounder,0,[0]
"As shown in Figure 2, at each time step t, the grounder efficiently scores a set of K grounding candidates by considering multiple time scales (Buch et al., 2017) that end at time step t. Specifically, we use different K for different datasets, which is determined by the distribution of the lengths of all ground-truth groundings in a certain dataset.",3.3 Grounder,0,[0]
"To simplify the following discussions, the lengths of K time scales are assumed to be an arithmetic sequence with the common difference δ",3.3 Grounder,0,[0]
and all the temporal candidates are sorted by increasing lengths.,3.3 Grounder,0,[0]
"In other words, the length of the k-th candidate is kδ.",3.3 Grounder,0,[0]
"Note that all grounding candidates considered at time t have a fixed ending boundary.
",3.3 Grounder,0,[0]
"Specifically, at each time step t, the grounder will classify each temporal candidate in consideration as a positive grounding or a negative one with respect to the given sentence.",3.3 Grounder,0,[0]
"Considering multiple time scales, the grounder will generate the confidence scores Ct = (c1t , c 2 t , ..., c K t ) that correspond to the set of K visual grounding candidates, all ending at time step t. The hidden state hrt generated by i-LSTM at time t, representing the interaction status between the sentence and video sequence up to the current position, is naturally suited to yield the confidence scores for the different time scales ending at time step t.",3.3 Grounder,0,[0]
"In this paper, the confidence scores, indicating the sentence grounding, are generated by a fullyconnected layer with sigmoid nonlinearity:
Ct = σ(W Khrt + b r t ), (5)
where WK and brt are the corresponding parameters, and σ denotes the nonlinear sigmoid function.",3.3 Grounder,0,[0]
The training samples collected in X for NSGV are video-sentence pairs.,3.4 Training,0,[0]
"Specifically, each video V is temporally associated with a set of sentence annotations: A = {(Si, tbi , tei )}Mi=1, where M is the number of annotated sentences of the video, and Si is a sentence description of a video clip, with tbi and t e",3.4 Training,0,[0]
i indicating the beginning and ending time in the video.,3.4 Training,0,[0]
Each training sample corresponds to a ground-truth matrix y ∈ RT×K with binary entries.,3.4 Training,0,[0]
"We use ykt to denote the (t, k)-th entry of the ground-truth matrix.",3.4 Training,0,[0]
ykt is interpreted as whether the k-th grounding candidate at time step t corresponds to the given natural sentence.,3.4 Training,0,[0]
"Concretely, the entry ykt is set as 1, indicating that the corresponding video segment (ends at time step t with length kδ) has a temporal Intersection-over-Union (IoU) with (tb, te) larger than a threshold θ.",3.4 Training,0,[0]
"Otherwise ykt is set as 0.
",3.4 Training,0,[0]
"For a training pair (V, S) ∈ X , the objective at time step t is given by a weighted binary cross entropy loss L(t, V, S):
− K∑ k=1 wk0y",3.4 Training,0,[0]
k,3.4 Training,0,[0]
t log c k,3.4 Training,0,[0]
"t +w k 1(1−ykt ) log(1− ckt ), (6)
where the weights wk0 and w k 1 are calculated according to the frequencies of positive and negative samples in the training set with length kδ.",3.4 Training,0,[0]
"ykt is the ground-truth value and ckt denotes the prediction results by our proposed model.
",3.4 Training,0,[0]
"Our TGN backpropagates at every time step t to learn all the parameters of the fully-coupled three modules: encoder, interactor, and grounder.",3.4 Training,0,[0]
"The objective of all training video-sentence pairs X is defined as:
LX = ∑
(V,S)∈X T∑ t=1 L(t, V, S).",3.4 Training,0,[0]
-7,3.4 Training,0,[0]
"During the inference stage, given a testing video V and a sentence S, the textual and visual encoders first generate hidden states for each word and video frame, respectively.",3.5 Inference,0,[0]
"Then, the interactor sequentially goes through the video frame by frame to yield the frame-by-word interaction status.",3.5 Inference,0,[0]
"At each position t, a K-dimensional score vector Ct is generated by the grounder.",3.5 Inference,0,[0]
"Therefore, after processing the last frame in the video, a T × K score matrix is obtained for the whole video, with the (t, k)-th entry in the matrix indicating the probability that the video segment ended at position t with length kδ in video V corresponds to sentence S. Eventually, the evaluation is reduced to a ranking problem over all the grounding candidates based on the generated scores.",3.5 Inference,0,[0]
"In this section, we evaluate the effectiveness of our proposed TGN on the NSGV task.",4 Experiments,0,[0]
"We begin by describing the datasets used for evaluation, followed by the introduction of the experimental settings including the baselines, configurations, as well as the evaluation metrics.",4 Experiments,0,[0]
"Afterwards, we demonstrate the effectiveness of TGN by comparing with the state-of-the-art approaches and efficiency through a runtime test.",4 Experiments,0,[0]
"We experiment on three publicly accessible datasets: DiDeMo (Hendricks et al., 2017), TACoS",4.1 Datasets,0,[0]
"(Regneri et al., 2013), and ActivityNet Captions (Fabian Caba Heilbron and Niebles, 2015).",4.1 Datasets,0,[0]
These datasets consist of videos as well as their associated temporally annotated sentences.,4.1 Datasets,0,[0]
DiDeMo2 consists of 10464 25-50 second long videos.,4.1 Datasets,0,[0]
"The same split provided by (Hendricks et al., 2017) is used for a fair comparison, with 33008, 4180, and 4022 video-sentence pairs for training, validation, and testing, respectively.
",4.1 Datasets,0,[0]
"2https://goo.gl/JpbAhg.
",4.1 Datasets,0,[0]
"TACoS3 consists of 127 videos selected from the MPII Cooking Composite Activities video corpus (Rohrbach et al., 2012).",4.1 Datasets,0,[0]
"The same split as in (Gao et al., 2017) is used, consisting of 10146, 4589, and 4083 video-sentence pairs for training, validation, and testing, respectively.",4.1 Datasets,0,[0]
"ActivityNet Captions4 consists of 19, 209 videos amounting to 849 hours.",4.1 Datasets,0,[0]
"The public split is used for our experiments, which has 37421, 17505, and 17031 video-sentence pairs for training, validation, and testing, respectively.",4.1 Datasets,0,[0]
"We compare our proposed TGN against the following two state-of-the-art models, specifically, the MCN (Hendricks et al., 2017), CTRL (Gao et al., 2017), visual-semantic alignment with LSTM (VSA-RNN) (Karpathy and Li, 2015), and visual-semantic alignment with skip thought vector (VSA-STV) (Kiros et al., 2015).",4.2.1 Baselines,0,[0]
"For fair comparisons, we compare the results of MCN on DiDeMo and the results of CTRL, VSA-RNN, VSA-STV on TACoS reported in their papers.",4.2.1 Baselines,0,[0]
A grounding of one natural sentence in a video is considered as “correct” if its temporal IoU with the ground-truth boundary is above a threshold θ.,4.2.2 Evaluation Metrics,0,[0]
"To be consistent with the baselines, we adopt R@N , IoU=θ, and mean IoU (mIoU) as our evaluation metrics.",4.2.2 Evaluation Metrics,0,[0]
"R@N , IoU=θ represents the percentage of testing samples which have at least one of the top-N results with IoU larger than θ. mIoU means the average IoU over all testing samples.",4.2.2 Evaluation Metrics,0,[0]
"Generally, the video frame features are usually extracted with a time resolution.",4.2.3 Configurations,0,[0]
"For the videos in DiDeMo and TACoS, we sample every 5 second as done by (Hendricks et al., 2017).",4.2.3 Configurations,0,[0]
"As the videos in DiDeMo are 25-30 second long, the video feature length is reduced to 6.",4.2.3 Configurations,0,[0]
"For videos in ActivityNet Captions, we sample every second.",4.2.3 Configurations,0,[0]
"To extract visual features, we consider both appearance and optical flow features.",4.2.3 Configurations,0,[0]
"Specifically, we study four widely-used visual features: VGG16 (Simonyan and Zisserman, 2014), C3D (Tran et al., 2015), Inception-V4 (Szegedy et al., 2017), and optical flow (Wang et al., 2016).",4.2.3 Configurations,0,[0]
"Please note that when
3https://goo.gl/ajmsva.",4.2.3 Configurations,0,[0]
"4https://goo.gl/N355bG.
comparing with specific baseline methods, we use the same features as baseline methods, specifically, VGG16 and optical flow for MCN and C3D for CTRL, VSA-RNN, and VSA-STV.
",4.2.3 Configurations,0,[0]
"For sentences, we tokenize each sentence by Stanford CoreNLP",4.2.3 Configurations,0,[0]
"(Manning et al., 2014) and use the 300-D word embeddings from GloVe (Pennington et al., 2014) to initialize the models.",4.2.3 Configurations,0,[0]
The words not found in GloVe are initialized as zero vectors.,4.2.3 Configurations,0,[0]
"The hidden state dimensions of all LSTMs (including the video, sentence, and interaction LSTMs) are set as 512.",4.2.3 Configurations,0,[0]
"We use the Adam (Kingma and Ba, 2014) optimizer with β1 = 0.5 and β2 = 0.999.",4.2.3 Configurations,0,[0]
The initial learning rate is set to 0.001.,4.2.3 Configurations,0,[0]
"We train the network for 200 iterations, and the learning rate is gradually decayed over time.",4.2.3 Configurations,0,[0]
The mini-batch size is set to 64.,4.2.3 Configurations,0,[0]
Experiments on DiDeMo.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
Table 1 illustrates the performance comparisons on the DiDeMo dataset.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in (Hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"First, TGN with different features can significantly outperforms the “prior baseline” MFP, which retrieves segments corresponding to the most common start and end points in the dataset.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Second, it can be observed that with the same visual features, specifically VGG16 and optical flow, TGN significantly outperforms MCN.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
And the performance of TGN with optical flow is better than that with VGG16.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"In such a case, the action information plays
a more critical role.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"This finding is also consistent with (Hendricks et al., 2017).",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"By fusing the results obtained by VGG16 and optical flow together, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Third, MCN introduces the temporal endpoint feature (TEF) as prior knowledge, which indicates when a segment occurs in a video.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"With TEF, the performance of MCN can be significantly improved.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"However, it is still inferior to our proposed TGN.
MCN is designed as an enumeration-based approach.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
Each video in the DiDeMo dataset is split into six five-second chunks which are considered as the time unit for localization.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Therefore, in total there are only C27 = 7 × 6/2 = 21 different ways of localization for DiDeMo videos.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Therefore, although MCN can be effectively applied to videos with several chunks due to the small search space, it is not practical for untrimmed long videos.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"In the Section 4.3.3, we will evaluate and compare the efficiencies of MCN, CTRL, and our proposed TGN.
Experiments on TACoS. Table 2 illustrates the experimental results on TACoS. First, it can be observed that CTRL performs much better than VSA-RNN and VSA-STV.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"The reasons lie in twofold (Gao et al., 2017).",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"On one hand, CTRL utilizes a multilayer alignment network to learn better alignment.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"On the other hand, VSA-RNN and VSA-STV do not encode temporal context information of video.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Second, with the same visual feature, specifically C3D, TGN-C3D significantly outperforms CTRL-C3D.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
This is due to the fact that TGN exploits not only the contextual information but also the fine-grained interaction behaviors.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"More concretely, TGN considers the frameby-word correlations by introducing an attentive combinations of the words in the sentence, where each weight encodes the degree to which the word is aligned with each specific frame.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"This mechanism is beneficial to capturing the informative se-
mantics in the sentences for alignment.
",4.3.1 Comparisons with State-of-the-Arts,0,[0]
Experiments on ActivityNet Captions.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Besides the two benchmarks, we also evaluate our model on the ActivityNet Captions dataset.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
Different CNNs are used to encode video visual information.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Specifically, we consider VGG16, C3D, and Inception-V4.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
The results are included in Table 3.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"First, our proposed TGN can perform effectively on long untrimmed videos.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Second, Inception-V4 performs generally better than VGG16 and C3D, which is consistent with the finding in (Canziani et al., 2016).",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"Therefore, more powerful visual representations of video features will undoubtedly improve the the performance of our proposed TGN on the NSGV task.
",4.3.1 Comparisons with State-of-the-Arts,0,[0]
Some qualitative results of our proposed TGN on ActiveityNet Captions dataset is illustrated in Figure 3.,4.3.1 Comparisons with State-of-the-Arts,0,[0]
"It can be observed that with different visual features, different grounding results are obtained.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"For the first and second examples, TGN with VGG16 and Inception-V4 generates more accurate groundings than that with C3D, while TGN with C3D yields more accurate grounding results for the third example.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
"More specifically, our proposed TGN with VGG16 and Inception-V4 can well identify the visual information related with the sentence, i.e. “A man in a red shirt claps his hands”.",4.3.1 Comparisons with State-of-the-Arts,0,[0]
We examine the effect of the frame-by-word attention in interactor.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
We ablate TGN into two other methods.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
1) NA: There is no attention layer in this model.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"After obtaining the sequential hidden states of the sentence, mean pooling is used to generate the representation for the whole sentence.
",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"Then the generated representation is concatenated with video representation, based on which the scores for multiple grounding candidates are predicted.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
2) NM:,4.3.2 Effect of Frame-by-Word Attention,0,[0]
The idea of generating framespecific sentence feature is still reserved in the NM model.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
The difference between NM and TGN is that there is no interaction LSTM in NM.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"Specifically, when calculating the attention weight for each word as in Eq. (3), the hidden state hrt−1 indicating the interaction status is not incorporated.
",4.3.2 Effect of Frame-by-Word Attention,0,[0]
The quantitative results are displayed in Table 4.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"First, when the attention mechanism is applied (NM), the performance is improved as compared with utilizing mean pooling (NA) for sentence features.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
The better performance demonstrates that our assumption about the evolving frame-by-word correlations between two modalities is reasonable.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"This also indicates that it is necessary to discriminate the contribution of each word in a sentence
to perform the NSGV task.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"Second, utilizing the interaction LSTM module (TGN) achieves better performance than simply concatenating the video representation and the attentive sentence representation (NM).",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"This result indicates that the interaction LSTM yields better interaction status between these two modalities, which can thereby benefit the final grounding.
",4.3.2 Effect of Frame-by-Word Attention,0,[0]
We provide some qualitative examples in Figure 4 for a better understanding of the frame-byword attention.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"Meanwhile, the grounding results yielded by TGN-Fusion (considering both VGG16 and optical flow) are also illustrated.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
This experiment is designed to verify whether the frameby-word attention mechanism in interactor is useful to highlight the representative concepts in the sentence.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"The attention weights α for two testing samples in DiDeMo are illustrated in Figure 4, where the darker the color is, the larger
the attention weight is.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
It can be observed that some words well match the frames.,4.3.2 Effect of Frame-by-Word Attention,0,[0]
"For example, in Figure 4 (a), the concept “forest” appears across all the video frames presenting an evenly distributed attention weights, while the other concept “waterfall” only presents in the first two frames.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"In addition to nouns, the adjective “blue” in Figure 4 (b) also receives relatively higher attention weights in relevant frames.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"Lastly, for stop words like “a”, “the” and “in”, their attention weights, which are very small, also present an even distribution.",4.3.2 Effect of Frame-by-Word Attention,0,[0]
"We evaluate the efficiency of our proposed TGN, by comparing its runtime with MCN and CTRL on a Tesla M40 GPU.",4.3.3 Efficiency,0,[0]
The efficiency is measured by frames per second (FPS) as shown in Table 5.,4.3.3 Efficiency,0,[0]
Please not that the feature extraction time is excluded.,4.3.3 Efficiency,0,[0]
"It can be observed that our TGN model achieves much faster processing speeds, with 1,363 fps vs. 562 and 286 for CTRL and MCN, respectively.",4.3.3 Efficiency,0,[0]
The reason mainly attributes to that the proposed TGN only process each video in one single pass without processing overlapped sliding windows.,4.3.3 Efficiency,0,[0]
"In this paper, we focused on the task of natural sentence grounding in video that is believed to offer a comprehensive understanding of bridging computer vision and natural language processing.",5 Conclusion,0,[0]
"Towards this task, we proposed an end-to-end Temporal GroundNet (TGN) by incorporating the evolving fine-grained frame-by-word interactions across video-sentence modalities to generate a visual grounding tailored to each given natural sentence.",5 Conclusion,0,[0]
"Moreover, TGN performs efficiently, which only needs to process the video sequence in one single pass.",5 Conclusion,0,[0]
Extensive experiments on three realworld datasets clearly demonstrate the effectiveness and efficiency of the proposed TGN.,5 Conclusion,0,[0]
"We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences.",abstractText,0,[0]
"Specifically, a novel Temporal GroundNet (TGN)1 is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence.",abstractText,0,[0]
"TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence.",abstractText,0,[0]
"Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass.",abstractText,0,[0]
We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts.,abstractText,0,[0]
We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.,abstractText,0,[0]
Temporally Grounding Natural Sentence in Video,title,0,[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2225–2229, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,0,[0]
Understanding language requires the ability to perform basic inference– to make conclusions about what is likely true or false based on what is said.,1 Overview,0,[0]
"For example, given the sentence She fixed the bug, we should almost certainly infer that the bug is fixed.",1 Overview,0,[0]
"However, rather than stating plainly that She fixed the bug, one might instead say:
(1a)",1 Overview,0,[0]
"She managed to fix the bug before midnight.
",1 Overview,0,[0]
(1b),1 Overview,0,[0]
"She happened to fix the bug while refactoring.
",1 Overview,0,[0]
"In either case, the hearer should still infer that the bug is fixed.",1 Overview,0,[0]
But it is not as easy as always inferring that embedded clauses are true.,1 Overview,0,[0]
"By changing only one word, these sentence no longer give a clear indication as to whether or not the bug has been fixed:
(2a) She wanted to fix the bug before midnight.
",1 Overview,0,[0]
(2b),1 Overview,0,[0]
"She planned to fix the bug while refactoring.
",1 Overview,0,[0]
"Implicative verbs, like those in (1), give rise to entailments, while non-implicative verbs, like those in (2), do not.",1 Overview,0,[0]
"It is therefore vital to natural language understanding to differentiate between clauses that are embedded under implicatives, which we can often infer to be either true or false, and those which are embedded under non-implicatives, for which such inferences cannot be made.",1 Overview,0,[0]
"In this paper, we exploit a known linguistic property of implicative verbs– that their complement clause is constrained to be in the same tense as the main clause– in order to predict the tendency of verbs to behave implicatively.",1 Overview,0,[0]
"We show that our method almost perfectly separates non-implicatives from implicatives in a small hand-labeled dataset, and that it provides strong signal for predicting entailments in sentences involving implicative verbs.",1 Overview,0,[0]
"Some English verbs can take infinitival complements, meaning they can appear in constructions of the form VB∗1 to VB2, where VB ∗ 1 is the “main” verb (which can be conjugated1) and VB2 is the “complement” verb (which is in infinitive form).",2 Implicative Verbs,0,[0]
"Examples (1a)-(2b) illustrate verbs taking infinitive complements.
",2 Implicative Verbs,0,[0]
"Implicative verbs are a special subclass2 of such verbs which give rise to entailments involving their
1Here, * indicates that VB1 can match any verb form, e.g. VB, VBD, VBP, etc.",2 Implicative Verbs,0,[0]
"VB2 can only match the base form VB.
2We note that factive verbs represent another special class of verbs which can take infinitival complements.",2 Implicative Verbs,0,[0]
"Unlike implica-
2225
complement clauses.",2 Implicative Verbs,0,[0]
"Individual implicatives can differ in the entailments they generate: e.g. while manage entails the truth of its complement, fail entails the falsity of its complement (failed to solve the problem⇒ didn’t solve the problem).",2 Implicative Verbs,0,[0]
"Despite these differences, however, implicatives represent a coherent class of verbs in that they permit some inference to be made about their complements, and this inference is sensitive to the context (positive/negated) of the main clause.",2 Implicative Verbs,0,[0]
"This contrasts with non-implicative verbs, like want, which do not permit any inference regarding their complements, and for which the truth of the complement is unaffected by negation in the main clause (Table 1).
",2 Implicative Verbs,0,[0]
"The method described in this paper aims to separate implicatives from non-implicatives (manage vs. want), rather than to differentiate between types implicatives (manage vs. fail).",2 Implicative Verbs,0,[0]
"Making this implicative/non-implicative distinction is a necessary first step toward handling inferences involving embedded clauses, and one that, to date, has only been performed using manually-constructed word lists (MacCartney, 2009; Recasens et al., 2013).",2 Implicative Verbs,0,[0]
"Karttunen (1971) observed that, in sentences involving implicatives, the tense of the main verb must necessarily match the tense of the complement clause.",2.1 Tense Constraints on Complement Clauses,0,[0]
"For example, (3), in which the main clause and the complement are both in the past tense, is acceptable but (4), in which the complement is in the future, is clearly not.",2.1 Tense Constraints on Complement Clauses,0,[0]
"For non-implicatives, however,
tives, factives presuppose, rather than entail, their complements.",2.1 Tense Constraints on Complement Clauses,0,[0]
E.g. both I was/was not glad to solve the problem entail,2.1 Tense Constraints on Complement Clauses,0,[0]
I solved the problem.,2.1 Tense Constraints on Complement Clauses,0,[0]
"We do not address factives here, as factives rarely take infinitival complements: more often, they take “that” complements (e.g. know that, realize that).",2.1 Tense Constraints on Complement Clauses,0,[0]
"Factives that do take infinitival complements are mostly phrasal (e.g. be glad to).
",2.1 Tense Constraints on Complement Clauses,0,[0]
"no such constraint exists: (6) is perfectly felicitous.
",2.1 Tense Constraints on Complement Clauses,0,[0]
"(3) I managed to solve the problem last night.
",2.1 Tense Constraints on Complement Clauses,0,[0]
"(4) #I managed to solve the problem tomorrow.
",2.1 Tense Constraints on Complement Clauses,0,[0]
"(5) I planned to solve the problem last night.
",2.1 Tense Constraints on Complement Clauses,0,[0]
"(6) I planned to solve the problem tomorrow.
",2.1 Tense Constraints on Complement Clauses,0,[0]
We exploit this property to predict implicativeness– whether the truth of a verb’s complement can be inferred– by observing the verb’s usage in practice.,2.1 Tense Constraints on Complement Clauses,0,[0]
"We hypothesize that, given a large corpus, we should be able to distinguish implicative verbs from nonimplicative verbs by observing how often the main verb tense agrees/disagrees with the tense of the complement clause.",3 Method,0,[0]
"Unfortunately, verbs in infinitival complement clauses are not conjugated, and so are not necessarily marked for tense.",3 Method,0,[0]
"We therefore use the Stanford Temporal Tagger (TT) (Chang and Manning, 2012) in order to identify time-referring expressions (e.g. tomorrow or last night) and resolve them to either past, present, or future tense.
",3 Method,0,[0]
"We find all sentences containing VB∗1 to VB2 constructions in the Annotated Gigaword corpus (Napoles et al., 2012).",3 Method,0,[0]
We run the the TT over all of the sentences in order to identify time-referring expressions.,3 Method,0,[0]
We only consider sentences in which a time-referring expression appears and is in a direct dependency relationship with the complement verb (VB2).,3 Method,0,[0]
"We provide the TT with the document publication dates,3 which are used to resolve each time mention to a specific calendar date and time.",3 Method,0,[0]
"We then map these time expressions coarsely to either past, present, or future tense by comparing the
3Provided as metadata in the Annotated Gigaword.
resolved time to the document creation time.",3 Method,0,[0]
"Because of the fact that days were often not resolved correctly, or at all, we eventually throw away sentences in which the complement clause is labeled as present tense, as these are rarely true references to the present, and rather the result of incorrect time resolution, or implicit future references (e.g. I am going to solve the problem today implies the future as in later today, but this is not captured by the TT).",3 Method,0,[0]
"We also assign the main clause to past, present, or future tense by using the fine-grained POS tag and a set of heuristics (for example, to check for modals).4
We assign a tense agreement score to each verb v as follows.",3 Method,0,[0]
Let S be the set of all VB∗1 to VB2 constructions in which VB∗1 = v. Then tense agreement is simply 1|S| × |{s,3 Method,0,[0]
∈ S,3 Method,0,[0]
"| complement tense = main tense}|, i.e. the fraction of constructions in S in which the tenses of the main and complement clauses agree.",3 Method,0,[0]
"We expect implicative verbs to occur mostly in agreeing constructions, and thus have high tense agreement, while nonimplicatives may occur in both agreeing and nonagreeing constructions, and thus should have lower tense agreement.",3 Method,0,[0]
"Note that while in theory, implicatives should never appear in non-agreeing constructions, the time annotation process is very imprecise, and thus we do not expect perfect results.",3 Method,0,[0]
Recreating list from Karttunen (1971).,4 Evaluation,0,[0]
Karttunen (1971) provides a short illustrative list of 7 known implicatives5 and 8 non-implicatives (shown in Table 2).,4 Evaluation,0,[0]
"As a first evaluation, we test whether tense agreement can accurately separate the verbs in this list, such that the implicatives are assigned higher agreement scores than the non-implicatives.",4 Evaluation,0,[0]
Table 2 shows that this is indeed the case.,4 Evaluation,0,[0]
"Tense agreement almost perfectly divides the list, with implicative verbs appearing above non-implicative verbs in all cases.",4 Evaluation,0,[0]
"The one exception is decide (reportedly non-implicative), which appears above dare (reportedly implicative).",4 Evaluation,0,[0]
"This error, however,
4Full set of heuristics in supplementary material.",4 Evaluation,0,[0]
"5The original list had 8 implicatives, but we omit remember since, in our data, it occurred almost exclusively with recurring time expressions, which we were not able to map to a specific date/time and thus tense, e.g. consumers must remember to make payments every 14 days.
",4 Evaluation,0,[0]
"seems understandable: while decide is not strictly implicative in the way manage is, it is often used as an implicative.",4 Evaluation,0,[0]
"E.g. the sentence I decided to leave would likely be taken to mean I left.
",4 Evaluation,0,[0]
Predicting Entailment.,4 Evaluation,0,[0]
"Our interest is not in distinguishing implicatives from non-implicatives for its own sake, but rather to predict, based on the main verb, whether the truth of the complement can be inferred.",4 Evaluation,0,[0]
We therefore conduct a second evaluation to assess how well tense agreement predicts this entailment property.,4 Evaluation,0,[0]
"We design our evaluation following the recognizing textual entailment (RTE) task (Dagan et al., 2006), in which two sentences are given, a premise p and a hypothesis h, and the goal is to determine whether p reasonably entails h. To construct our p/h pairs, we take all the verbs extracted in Section 3 which appear in at least 50 tense-labeled sentences.",4 Evaluation,0,[0]
"For each of these verbs, we choose 3 random sentences in which the verb appears as VB∗1 in a VB∗1 to VB2 construction.
6",4 Evaluation,0,[0]
"From each sentence, we extract the complement clause by deleting VB∗1 to from the sentence, and conjugating VB2 to match the tense of VB∗1.",4 Evaluation,0,[0]
We then use the original sentence as p and the extracted complement as h: e.g. a p/h pair might look like I get to interact with fellow professors/I interact with fellow professors.,4 Evaluation,0,[0]
"We ask 5 independent annotators on Amazon Mechanical Turk to read each p and then determine whether h is true, false, or unclear given p.7",4 Evaluation,0,[0]
We take the majority answer as the true label.,4 Evaluation,0,[0]
"We expect that implicative verbs should lead to judgements which are decidedly true or false while non-implicatives should lead
6These sentences can come from anywhere in the Gigaword corpus, they are not required to contain time expressions.
",4 Evaluation,0,[0]
"7Full annotation guidelines in supplementary material.
",4 Evaluation,0,[0]
to mostly judgements of unclear.,4 Evaluation,0,[0]
Figure 1 shows that these expectations hold.,4 Evaluation,0,[0]
"When a verb with low tense agreement appeared as the main verb of a sentence, the truth of the complement could only be inferred 30% of the time.",4 Evaluation,0,[0]
"When a verb with high tense agreement appeared as the main verb, the truth of the complement could be inferred 91% of the time.",4 Evaluation,0,[0]
This difference is significant at p < 0.01.,4 Evaluation,0,[0]
"That is, tense agreement provides a strong signal for identifying non-implicative verbs, and thus can help systems avoid false-positive entailment judgements, e.g. incorrectly inferring that wanting to merge⇒ merging (Table 3).
",4 Evaluation,0,[0]
"Interestingly, tense agreement accurately models verbs that are not implicative by definition, but which nonetheless tend to behave implicatively in practice.",4 Evaluation,0,[0]
"For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen.",4 Evaluation,0,[0]
"To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through
the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method.",4 Evaluation,0,[0]
Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives.,4 Evaluation,0,[0]
"Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data.",5 Discussion and Related Work,0,[0]
"Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled.",5 Discussion and Related Work,0,[0]
"The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems.",5 Discussion and Related Work,0,[0]
"The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012).",5 Discussion and Related Work,0,[0]
Another interesting extension concerns the role of tense in word representations.,5 Discussion and Related Work,0,[0]
"While currently, tense is rarely built directly into distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014), our results suggest it may offer important insights into the semantics of individual words.",5 Discussion and Related Work,0,[0]
We leave this question as a direction for future work.,5 Discussion and Related Work,0,[0]
Differentiating between implicative and nonimplicative verbs is important for discriminating inferences that can and cannot be made in natural language.,6 Conclusion,0,[0]
"We have presented a data-driven method
that captures the implicative tendencies of verbs by exploiting the tense relationship between the verb and its complement clauses.",6 Conclusion,0,[0]
"This method effectively separates known implicatives from known non-implicatives, and, more importantly, provides good predictive signal in an entailment recognition task.",6 Conclusion,0,[0]
We would like to thank Florian Schwartz for valuable discussions.,Acknowledgments,0,[0]
"This research was supported by a Facebook Fellowship, and by gifts from the Alfred P. Sloan Foundation, Google, and Facebook.",Acknowledgments,0,[0]
This material is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program).,Acknowledgments,0,[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.,Acknowledgments,0,[0]
The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.,Acknowledgments,0,[0]
"Implicative verbs (e.g. manage) entail their complement clauses, while non-implicative verbs (e.g. want) do not.",abstractText,0,[0]
"For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem.",abstractText,0,[0]
"Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization.",abstractText,0,[0]
We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements.,abstractText,0,[0]
"We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.",abstractText,0,[0]
Tense Manages to Predict Implicative Behavior in Verbs,title,0,[0]
"Matrix balancing is the problem of rescaling a given square nonnegative matrix A ∈ Rn×n≥0 to a doubly stochastic matrix RAS, where every row and column sums to one, by multiplying two diagonal matrices R and S.",1. Introduction,0,[0]
"This is a fundamental process for analyzing and comparing matrices in a wide range of applications, including input-output analysis in economics, called the RAS approach (Parikh, 1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004), seat assignments in elections (Balinski, 2008; Akartunalı &
1National Institute of Informatics 2JST",1. Introduction,0,[0]
"PRESTO 3RIKEN Brain Science Institute 4Graduate School of Frontier Sciences, The University of Tokyo 5RIKEN AIP 6NIMS.",1. Introduction,0,[0]
"Correspondence to: Mahito Sugiyama <mahito@nii.ac.jp>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
"Every ber sums to 1
Given tensor A
Multistochastic tensor A’ Submanifold (β)
",1. Introduction,0,[0]
"Probability distribution P
Statistical manifold (dually at Riemannian manifold)
",1. Introduction,0,[0]
"ProjectionTensor balancing
Projected distribution Pβ
Figure 1.",1. Introduction,0,[0]
"Overview of our approach.
",1. Introduction,0,[0]
"Knight, 2016), Hi-C data analysis (Rao et al., 2014; Wu & Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and the optimal transportation problem (Cuturi, 2013; Frogner et al., 2015; Solomon et al., 2015).",1. Introduction,0,[0]
"An excellent review of this theory and its applications is given by Idel (2016).
",1. Introduction,0,[0]
"The standard matrix balancing algorithm is the SinkhornKnopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp, 1967; Marshall & Olkin, 1968; Knight, 2008), a special case of Bregman’s balancing method (Lamond & Stewart, 1981) that iterates rescaling of each row and column until convergence.",1. Introduction,0,[0]
The algorithm is widely used in the above applications due to its simple implementation and theoretically guaranteed convergence.,1. Introduction,0,[0]
"However, the algorithm converges linearly (Soules, 1991), which is prohibitively slow for recently emerging large and sparse matrices.",1. Introduction,0,[0]
"Although Livne & Golub (2004) and Knight & Ruiz (2013) tried to achieve faster convergence by approximating each step of Newton’s method, the exact Newton’s method with quadratic convergence has not been intensively studied yet.
",1. Introduction,0,[0]
"Another open problem is tensor balancing, which is a generalization of balancing from matrices to higher-order multidimentional arrays, or tensors.",1. Introduction,0,[0]
"The task is to rescale an N th order nonnegative tensor to a multistochastic tensor, in which every fiber sums to one, by multiplying (N −1)th order N tensors.",1. Introduction,0,[0]
"There are some results about mathematical properties of multistochastic tensors (Cui et al., 2014; Chang et al., 2016; Ahmed et al., 2003).",1. Introduction,0,[0]
"However, there is no result for tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now.
",1. Introduction,0,[0]
Here we show that Newton’s method with quadratic convergence can be applied to tensor balancing while avoiding solving a linear system on the full tensor.,1. Introduction,0,[0]
"Our strategy is to realize matrix and tensor balancing as projection onto a dually flat Riemmanian submanifold (Figure 1), which is a statistical manifold and known to be the essential structure for probability distributions in information geometry (Amari, 2016).",1. Introduction,0,[0]
"Using a partially ordered outcome space, we generalize the log-linear model (Agresti, 2012) used to model the higher-order combinations of binary variables (Amari, 2001; Ganmor et al., 2011; Nakahara & Amari, 2002; Nakahara et al., 2003), which allows us to model tensors as probability distributions in the statistical manifold.",1. Introduction,0,[0]
"The remarkable property of our model is that the gradient of the manifold can be analytically computed using the Möbius inversion formula (Rota, 1964), the heart of combinatorial mathematics (Ito, 1993), which enables us to directly obtain the Jacobian matrix in Newton’s method.",1. Introduction,0,[0]
"Moreover, we show that (n − 1)N entries for the size nN of a tensor are invariant with respect to one of the two coordinate systems of the statistical manifold.",1. Introduction,0,[0]
"Thus the number of equations in Newton’s method is O(nN−1).
",1. Introduction,0,[0]
The remainder of this paper is organized as follows: We begin with a low-level description of our matrix balancing algorithm in Section 2 and demonstrate its efficiency in numerical experiments in Section 3.,1. Introduction,0,[0]
"To guarantee the correctness of the algorithm and extend it to tensor balancing, we provide theoretical analysis in Section 4.",1. Introduction,0,[0]
"In Section 4.1, we introduce a generalized log-linear model associated with a partial order structured outcome space, followed by introducing the dually flat Riemannian structure in Section 4.2.",1. Introduction,0,[0]
"In Section 4.3, we show how to use Newton’s method to compute projection of a probability distribution onto a submanifold.",1. Introduction,0,[0]
"Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6.",1. Introduction,0,[0]
"Given a nonnegative square matrix A = (aij) ∈ Rn×n≥0 , the task of matrix balancing is to find r, s ∈ Rn that satisfy
(RAS)1 = 1, (RAS)T1 = 1, (1)
where R = diag(r) and S = diag(s).",2. The Matrix Balancing Algorithm,0,[0]
The balanced matrix A′,2. The Matrix Balancing Algorithm,0,[0]
#NAME?,2. The Matrix Balancing Algorithm,0,[0]
"The most popular algorithm is the Sinkhorn-Knopp algorithm, which repeats updating r and s as r = 1/(As) and s = 1/(ATr).",2. The Matrix Balancing Algorithm,0,[0]
"We denote by [n] = {1, 2, . . .",2. The Matrix Balancing Algorithm,0,[0]
", n} hereafter.
",2. The Matrix Balancing Algorithm,0,[0]
"In our algorithm, instead of directly updating r and s, we update two parameters θ and η defined as
log pij = ∑ i′≤i ∑ j′≤j θi′j′ , ηij = ∑ i′≥i ∑ j′≥j pi′j′ (2)
Matrix Constraints for balancing
for each i, j ∈",2. The Matrix Balancing Algorithm,0,[0]
"[n], where we normalized entries as pij = aij/ ∑",2. The Matrix Balancing Algorithm,0,[0]
ij,2. The Matrix Balancing Algorithm,0,[0]
aij,2. The Matrix Balancing Algorithm,0,[0]
so that ∑ ij pij = 1.,2. The Matrix Balancing Algorithm,0,[0]
We assume for simplicity that each entry is strictly larger than zero.,2. The Matrix Balancing Algorithm,0,[0]
"The assumption will be removed in Section 5.
",2. The Matrix Balancing Algorithm,0,[0]
"The key to our approach is that we update θ(t)ij with i = 1 or j = 1 by Newton’s method at each iteration t = 1, 2, . . .",2. The Matrix Balancing Algorithm,0,[0]
"while fixing θij with i, j ̸= 1 so that η(t)ij satisfies the following condition (Figure 2):
η (t) i1 = (n− i+ 1)/n, η (t) 1j =",2. The Matrix Balancing Algorithm,0,[0]
"(n− j + 1)/n.
Note that the rows and columns sum not to 1 but to 1/n due to the normalization.",2. The Matrix Balancing Algorithm,0,[0]
"The update formula is described as θ (t+1) 11 ... θ (t+1) 1n θ (t+1) 21
... θ (t+1) n1
 =  θ (t) 11 ...",2. The Matrix Balancing Algorithm,0,[0]
"θ (t) 1n
θ (t) 21 ...",2. The Matrix Balancing Algorithm,0,[0]
"θ (t) n1
 − J−1  η (t) 11",2. The Matrix Balancing Algorithm,0,[0]
− (n− 1 + 1)/n ...,2. The Matrix Balancing Algorithm,0,[0]
η (t),2. The Matrix Balancing Algorithm,0,[0]
1n,2. The Matrix Balancing Algorithm,0,[0]
− (n− n+ 1)/n η (t) 21,2. The Matrix Balancing Algorithm,0,[0]
"− (n− 2 + 1)/n
... η (t) n1",2. The Matrix Balancing Algorithm,0,[0]
"− (n− n+ 1)/n
 , (3)
where J is the Jacobian matrix given as
J(ij)(i′j′)= ∂η
(t) ij",2. The Matrix Balancing Algorithm,0,[0]
"∂θ (t) i′j′ = ηmax{i,i′}max{j,j′}−n2ηijηi′j′ , (4)
which is derived from our theoretical result in Theorem 3.",2. The Matrix Balancing Algorithm,0,[0]
"Since J is a (2n−1)×(2n−1) matrix, the time complexity of each update is O(n3), which is needed to compute the inverse of J .
",2. The Matrix Balancing Algorithm,0,[0]
"After updating to θ(t+1)ij , we can compute p (t+1) ij and η (t+1) ij by Equation (2).",2. The Matrix Balancing Algorithm,0,[0]
"Since this update does not ensure the condition ∑ ij p (t+1) ij = 1, we again update θ (t+1) 11 as θ (t+1) 11 = θ (t+1) 11 − log ∑ ij p (t+1) ij and recompute p (t+1) ij and η(t+1)ij for each i, j ∈",2. The Matrix Balancing Algorithm,0,[0]
"[n].
By iterating the above update process in Equation (3) until convergence, A = (aij) with aij = npij becomes doubly stochastic.",2. The Matrix Balancing Algorithm,0,[0]
"We evaluate the efficiency of our algorithm compared to the two prominent balancing methods, the standard SinkhornKnopp algorithm (Sinkhorn, 1964) and the state-of-the-art
algorithm BNEWT (Knight & Ruiz, 2013), which uses Newton’s method-like iterations with conjugate gradients.",3. Numerical Experiments,0,[0]
All experiments were conducted on Amazon Linux AMI release 2016.09 with a single core of 2.3 GHz Intel Xeon CPU E5-2686 v4 and 256 GB of memory.,3. Numerical Experiments,0,[0]
All methods were implemented in C++ with the Eigen library and compiled with gcc 4.8.31.,3. Numerical Experiments,0,[0]
"We have carefully implemented BNEWT by directly translating the MATLAB code provided in (Knight & Ruiz, 2013) into C++ with the Eigen library for fair comparison, and used the default parameters.",3. Numerical Experiments,0,[0]
"We measured the residual of a matrix A′ = (a′ij) by the squared norm ∥(A′1−1, A′T1−1)∥2, where each entry a′ij is obtained as npij in our algorithm, and ran each of three algorithms until the residual is below the tolerance threshold 10−6.
",3. Numerical Experiments,0,[0]
Hessenberg Matrix.,3. Numerical Experiments,0,[0]
"The first set of experiments used a Hessenberg matrix, which has been a standard benchmark for matrix balancing (Parlett & Landis, 1982; Knight & Ruiz, 2013).",3. Numerical Experiments,0,[0]
Each entry of an n × n Hessenberg matrix,3. Numerical Experiments,0,[0]
Hn = (hij) is given as hij = 0,3. Numerical Experiments,0,[0]
if j < i,3. Numerical Experiments,0,[0]
− 1 and hij = 1 otherwise.,3. Numerical Experiments,0,[0]
"We varied the size n from 10 to 5, 000, and measured running time (in seconds) and the number of iterations of each method.
",3. Numerical Experiments,0,[0]
Results are plotted in Figure 3.,3. Numerical Experiments,0,[0]
"Our balancing algorithm with the Newton’s method (plotted in blue in the figures)
1An implementation of algorithms for matrices and third order tensors is available at: https://github.com/ mahito-sugiyama/newton-balancing
is clearly the fastest: It is three to five orders of magnitude faster than the standard Sinkhorn-Knopp algorithm (plotted in red).",3. Numerical Experiments,0,[0]
"Although the BNEWT algorithm (plotted in green) is competitive if n is small, it suddenly fails to converge whenever n ≥ 200, which is consistent with results in the original paper (Knight & Ruiz, 2013) where there is no result for the setting n ≥ 200 on the same matrix.",3. Numerical Experiments,0,[0]
"Moreover, our method converges around 10 to 20 steps, which is about three and seven orders of magnitude smaller than BNEWT and Sinkhorn-Knopp, respectively, at n = 100.
",3. Numerical Experiments,0,[0]
"To see the behavior of the rate of convergence in detail, we plot the convergence graph in Figure 4 for n = 20, where we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT algorithm, which contrasts with our quick convergence.
",3. Numerical Experiments,0,[0]
Trefethen Matrix.,3. Numerical Experiments,0,[0]
"Next, we collected a set of Trefethen matrices from a collection website2, which are nonnegative diagonal matrices with primes.",3. Numerical Experiments,0,[0]
"Results are plotted in Figure 5, where we observe the same trend as before:",3. Numerical Experiments,0,[0]
Our algorithm is the fastest and about four orders of magnitude faster than the Sinkhorn-Knopp algorithm.,3. Numerical Experiments,0,[0]
"Note that larger matrices with n > 300 do not have total support, which is the necessary condition for matrix balancing (Knight & Ruiz, 2013), while the BNEWT algorithm fails to converge if n = 200 or n = 300.",3. Numerical Experiments,0,[0]
"In the following, we provide theoretical support to our algorithm by formulating the problem as a projection within a statistical manifold, in which a matrix corresponds to an element, that is, a probability distribution, in the manifold.
",4. Theoretical Analysis,0,[0]
"We show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold, where the Jacobian matrix in Equation (4) is derived from the gradient of the manifold.
2http://www.cise.ufl.edu/research/sparse/",4. Theoretical Analysis,0,[0]
matrices/,4. Theoretical Analysis,0,[0]
"We introduce our log-linear probabilistic model, where the outcome space is a partially ordered set, or a poset (Gierz et al., 2003).",4.1. Formulation,0,[0]
"We prepare basic notations and the key mathematical tool for posets, the Möbius inversion formula, followed by formulating the log-linear model.",4.1. Formulation,0,[0]
"A poset (S,≤), the set of elements S and a partial order ≤ on S, is a fundamental structured space in computer science.",4.1.1. MÖBIUS INVERSION,0,[0]
"A partial order “≤” is a relation between elements in S that satisfies the following three properties: For all x, y, z ∈ S, (1) x ≤ x (reflexivity), (2) x ≤ y, y ≤ x ⇒",4.1.1. MÖBIUS INVERSION,0,[0]
"x = y (antisymmetry), and (3) x",4.1.1. MÖBIUS INVERSION,0,[0]
"≤ y, y ≤ z ⇒ x ≤ z",4.1.1. MÖBIUS INVERSION,0,[0]
(transitivity).,4.1.1. MÖBIUS INVERSION,0,[0]
"In what follows, S is always finite and includes the least element (bottom) ⊥ ∈ S; that is, ⊥ ≤ x for all x ∈ S. We denote S \ {⊥} by S+.
",4.1.1. MÖBIUS INVERSION,0,[0]
Rota (1964) introduced the Möbius inversion formula on posets by generalizing the inclusion-exclusion principle.,4.1.1. MÖBIUS INVERSION,0,[0]
"Let ζ :S × S → {0, 1} be the zeta function defined as
ζ(s, x) = { 1 if s ≤ x, 0 otherwise.
",4.1.1. MÖBIUS INVERSION,0,[0]
"The Möbius function µ :S×S → Z satisfies ζµ = I , which is inductively defined for all x, y with x ≤ y as
µ(x, y) =  1 if x = y, − ∑
x≤s<y µ(x, s) if x < y, 0 otherwise.
",4.1.1. MÖBIUS INVERSION,0,[0]
"From the definition, it follows that∑ s∈S ζ(s, y)µ(x, s) = ∑ x≤s≤y
µ(x, s) = δxy,∑ s∈S ζ(x, s)µ(s, y) = ∑ x≤s≤y µ(s, y) = δxy (5)
with the Kronecker delta δ such that δxy = 1 if x = y and δxy = 0 otherwise.",4.1.1. MÖBIUS INVERSION,0,[0]
"Then for any functions f , g, and h with the domain S such that
g(x) = ∑ s∈S ζ(s, x)f(s) =",4.1.1. MÖBIUS INVERSION,0,[0]
"∑ s≤x f(s),
h(x) = ∑ s∈S ζ(x, s)f(s) = ∑ s≥x f(s)",4.1.1. MÖBIUS INVERSION,0,[0]
",
f is uniquely recovered with the Möbius function: f(x) = ∑ s∈S µ(s, x)g(s), f(x) = ∑ s∈S µ(x, s)h(s).
",4.1.1. MÖBIUS INVERSION,0,[0]
"This is called the Möbius inversion formula and is at the heart of enumerative combinatorics (Ito, 1993).",4.1.1. MÖBIUS INVERSION,0,[0]
"We consider a probability vector p on (S,≤) that gives a discrete probability distribution with the outcome space S.
A probability vector is treated as a mapping p :S → (0, 1) such that ∑ x∈S p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.
",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"Using the zeta and the Möbius functions, let us introduce two mappings θ :S → R and η :S → R as
θ(x)",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"= ∑ s∈S µ(s, x) log p(s), (6)
η(x) = ∑ s∈S ζ(x, s)p(s) = ∑ s≥x p(s).",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"(7)
From the Möbius inversion formula, we have log p(x) = ∑ s∈S ζ(s, x)θ(s) = ∑ s≤x θ(s), (8)
p(x) = ∑ s∈S µ(x, s)η(s).",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"(9)
They are generalization of the log-linear model (Agresti, 2012) that gives the probability p(x) of an n-dimensional binary vector x = (x1, . .",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
.,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
", xn) ∈ {0, 1}n as
log p(x) = ∑",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
i θixi,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
#NAME?,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
nx1x2 . . .,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
xn,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"− ψ,
where θ = (θ1, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
", θ12...n) is a parameter vector, ψ is a normalizer, and η = (η1, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
", η12...n) represents the expectation of variable combinations such that
ηi = E[xi] = Pr(xi = 1),
ηij = E[xixj ] = Pr(xi = xj = 1), i < j, . .",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
".
",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
η1...n = E[x1 . . .,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"xn] = Pr(x1 = · · · = xn = 1).
",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"They coincide with Equations (8) and (7) when we let S = 2V with V = {1, 2, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
",",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"n},",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"each x ∈ S as the set of indices of “1” of x, and the order ≤ as the inclusion relationship, that is, x ≤ y",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
if and only if x ⊆ y. Nakahara et al. (2006) have pointed out that θ can be computed from p using the inclusion-exclusion principle in the log-linear model.,4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"We exploit this combinatorial property of the loglinear model using the Möbius inversion formula on posets and extend the log-linear model from the power set 2V to any kind of posets (S,≤).",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"Sugiyama et al. (2016) studied a relevant log-linear model, but the relationship with Möbius inversion formula has not been analyzed yet.",4.1.2. LOG-LINEAR MODEL ON POSETS,0,[0]
"We theoretically analyze our log-linear model introduced in Equations (6), (7) and show that they form dual coordinate systems on a dually flat manifold, which has been mainly studied in the area of information geometry (Amari, 2001; Nakahara & Amari, 2002; Amari, 2014; 2016).",4.2. Dually Flat Riemannian Manifold,0,[0]
"Moreover, we show that the Riemannian metric and connection of our model can be analytically computed in closed forms.
",4.2. Dually Flat Riemannian Manifold,0,[0]
"In the following, we denote by ξ the function θ or η and by ∇ the gradient operator with respect to S+ = S \{⊥}, i.e., (∇f(ξ))(x) = ∂f/∂ξ(x) for x ∈ S+, and denote by S the set of probability distributions specified by probability vectors, which forms a statistical manifold.",4.2. Dually Flat Riemannian Manifold,0,[0]
"We use uppercase letters P,Q,R, . . .",4.2. Dually Flat Riemannian Manifold,0,[0]
"for points (distributions) in S and their lowercase letters p, q, r, . . .",4.2. Dually Flat Riemannian Manifold,0,[0]
for the corresponding probability vectors treated as mappings.,4.2. Dually Flat Riemannian Manifold,0,[0]
"We write θP and ηP if they are connected with p by Equations (6) and (7), respectively, and abbreviate subscripts if there is no ambiguity.",4.2. Dually Flat Riemannian Manifold,0,[0]
We show that S has the dually flat Riemannian structure induced by two functions θ and η in Equation (6) and (7).,4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"We define ψ(θ) as
ψ(θ) = −θ(⊥) =",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"− log p(⊥), (10)
which corresponds to the normalizer of p.",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"It is a convex function since we have
ψ(θ) = log ∑ x∈S exp  ∑ ⊥<s≤x θ(s)  from log p(x) = ∑ ⊥<s≤x θ(s)−ψ(θ).",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"We apply the Legendre transformation to ψ(θ) given as
φ(η) = max θ′
( θ′η − ψ(θ′) ) , θ′η = ∑ x∈S+ θ′(x)η(x).",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"(11)
Then φ(η) coincides with the negative entropy.
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"Theorem 1 (Legendre dual). φ(η) = ∑ x∈S p(x) log p(x).
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
Proof.,4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"From Equation (5), we have
θ′η = ∑ x∈S+  ∑ ⊥<s≤x µ(s, x) log p′(s) ∑ s≥x p(s)  = ∑ x∈S+ p(x) ( log p′(x)− log p′(⊥) ) .
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
Thus it holds that θ′η − ψ(θ′) = ∑ x∈S p(x) log p′(x).,4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"(12)
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"Hence it is maximized with p(x) = p′(x).
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"Since they are connected with each other by the Legendre transformation, they form a dual coordinate system ∇ψ(θ) and ∇φ(η) of S (Amari, 2016, Section 1.5), which coincides with θ and η as follows.
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"Theorem 2 (dual coordinate system).
∇ψ(θ) = η, ∇φ(η) = θ. (13)
Proof.",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"They can be directly derived from our definitions (Equations (6) and (11)) as
∂ψ(θ) ∂θ(x) =
∑ y≥x exp (∑ ⊥<s≤y θ(s) )",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"∑
y∈S exp (∑ ⊥<s≤y θ(s) )",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"=∑ s≥x p(s) = η(x),
∂φ(η) ∂η(x) = ∂ ∂η(x)
( θη − ψ(θ) )",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"= θ(x).
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"Moreover, we can confirm the orthogonality of θ and η as
E
[ ∂ log p(s)
∂θ(x)
∂ log p(s)
∂η(y) ]",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"= ∑ s∈S ζ(x, s)µ(s, y) = δxy.
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"The last equation holds from Equation (5), hence the Möbius inversion directly leads to the orthogonality.
",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"The Bregman divergence is known to be the canonical divergence (Amari, 2016, Section 6.6) to measure the difference between two distributions P and Q on a dually flat manifold, which is defined as
D",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"[P,Q] = ψ(θP )",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
+ φ(ηQ)− θP ηQ.,4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"In our case, since we have φ(ηQ) = ∑
x∈S q(x) log q(x) and θP ηQ−ψ(θP ) = ∑ x∈S q(x) log p(x) from Theorem 1 and Equation (12), it is given as
D [P,Q] = ∑ x∈S q(x) log q(x) p(x) ,
which coincides with the Kullback–Leibler divergence (KL divergence) from Q to P :",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
"D [P,Q] = DKL [Q,P ].",4.2.1. DUALLY FLAT STRUCTURE,0,[0]
Next we analyze the Riemannian structure on S and show that the Möbius inversion formula enables us to compute the Riemannian metric of S. Theorem 3 (Riemannian metric).,4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"The manifold (S, g(ξ)) is a Riemannian manifold with the Riemannian metric g(ξ) such that for all x, y ∈ S+
gxy(ξ) =  ∑ s∈S [ ζ(x, s)ζ(y, s)p(s)− η(x)η(y) ] if ξ = θ,
∑ s∈S µ(s, x)µ(s, y)p(s)−1 if ξ = η.
Proof.",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"Since the Riemannian metric is defined as
g(θ) = ∇∇ψ(θ), g(η) = ∇∇φ(η),
when ξ = θ",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"we have
gxy(θ) = ∂2
∂θ(x)∂θ(y) ψ(θ) =
∂
∂θ(x) η(y)
= ∂
∂θ(x) ∑ s∈S ζ(y, s) exp  ∑ ⊥<u≤s θ(u)− ψ(θ) 
= ∑ s∈S ζ(x, s)ζ(y, s)p(s)− |S|η(x)η(y).
",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"When ξ = η, it follows that
gxy(η) = ∂2
∂η(x)∂η(y) φ(η) =
∂
∂η(x) θ(y)
= ∂
∂η(x) ∑ s≤y µ(s, y) log ∑ u≥s µ(s, u)η(u)  = ∑ s∈S µ(s, x)µ(s, y)p(s)−1.
",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"Since g(ξ) coincides with the Fisher information matrix,
E
[ ∂
∂θ(x) log p(s)
∂
∂θ(y) log p(s)
] = gxy(θ),
E
[ ∂
∂η(x) log p(s)
∂
∂η(y) log p(s)
] = gxy(η).
",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"Then the Riemannian (Levi–Chivita) connection Γ(ξ) with respect to ξ, which is defined as
Γxyz(ξ) = 1
2
( ∂gyz(ξ)
∂ξ(x) + ∂gxz(ξ)",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
∂ξ(y) − ∂gxy(ξ) ∂ξ(z) ),4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"for all x, y, z ∈ S+, can be analytically obtained.",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
Theorem 4 (Riemannian connection).,4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"The Riemannian connection Γ(ξ) on the manifold (S, g(ξ)) is given in the following for all x, y, z ∈ S+,
Γxyz(ξ) =  1 2 ∑ s∈S ( ζ(x, s)− η(x) )",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"( ζ(y, s)− η(y) )",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"( ζ(z, s)− η(z) )",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
p(s),4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"if ξ = θ, −1 2 ∑ s∈S µ(s, x)µ(s, y)µ(s, z)p(s)−2 if ξ = η.
Proof.",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"Connections Γxyz(θ) and Γxyz(η) can be obtained by directly computing ∂gyz(θ)/∂θ(x) and ∂gyz(η)/∂η(x), respectively.",4.2.2. RIEMANNIAN STRUCTURE,0,[0]
"Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model (Amari, 2016).",4.3. The Projection Algorithm,0,[0]
Here we define projection of distributions on posets and show that Newton’s method can be applied to perform projection as the Jacobian matrix can be analytically computed.,4.3. The Projection Algorithm,0,[0]
"Let S(β) be a submanifold of S such that
S(β) = {P ∈ S | θP (x) = β(x), ∀x ∈ dom(β)} (14)
specified by a function β with dom(β) ⊆ S+.",4.3.1. DEFINITION,0,[0]
"Projection of P ∈ S onto S(β), calledm-projection, which is defined as the distribution Pβ ∈ S(β) such that{
θPβ (x) = β(x) if x ∈ dom(β), ηPβ (x) = ηP (x)",4.3.1. DEFINITION,0,[0]
"if x ∈ S+ \ dom(β),
is the minimizer of the KL divergence from P to S(β):
Pβ = argmin Q∈S(β) DKL[P,Q].
",4.3.1. DEFINITION,0,[0]
"The dually flat structure with the coordinate systems θ and η guarantees that the projected distribution Pβ always exists and is unique (Amari, 2009, Theorem 3).",4.3.1. DEFINITION,0,[0]
"Moreover, the Pythagorean theorem holds in the dually flat manifold, that is, for any Q ∈ S(β)",4.3.1. DEFINITION,0,[0]
"we have
DKL[P,Q] = DKL[P, Pβ ] +DKL[Pβ , Q].
",4.3.1. DEFINITION,0,[0]
"We can switch η and θ in the submanifold S(β) by changing DKL[P,Q] to DKL[Q,P ], where the projected distribution Pβ of P is given as{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",4.3.1. DEFINITION,0,[0]
"if x ∈ dom(β),
This projection is called e-projection.
",4.3.1. DEFINITION,0,[0]
Example 1 (Boltzmann machine).,4.3.1. DEFINITION,0,[0]
"Given a Boltzmann machine represented as an undirected graph G = (V,E) with a vertex set V and an edge set E ⊆ {{i, j} | i, j ∈ V }.",4.3.1. DEFINITION,0,[0]
"The set of probability distributions that can be modeled by a Boltzmann machine G coincides with the submanifold
SB = {P ∈ S | θP (x) = 0",4.3.1. DEFINITION,0,[0]
"if |x| > 2 or x ̸∈ E},
with S = 2V .",4.3.1. DEFINITION,0,[0]
Let P̂ be an empirical distribution estimated from a given dataset.,4.3.1. DEFINITION,0,[0]
"The learned model is the mprojection of the empirical distribution P̂ onto SB, where the resulting distribution Pβ is given as{
θPβ (x) = 0",4.3.1. DEFINITION,0,[0]
"if |x| > 2 or x ̸∈ E, ηPβ (x) = ηP̂ (x) if |x| = 1 or x ∈ E.",4.3.1. DEFINITION,0,[0]
Here we show how to compute projection of a given probability distribution.,4.3.2. COMPUTATION,0,[0]
"We show that Newton’s method can be used to efficiently compute the projected distribution Pβ by iteratively updating P (0)β = P as P (0) β , P (1) β , P (2) β , . . .",4.3.2. COMPUTATION,0,[0]
"until converging to Pβ .
Let us start with the m-projection with initializing P (0)β = P .",4.3.2. COMPUTATION,0,[0]
"In each iteration t, we update θ(t)Pβ (x) for all x ∈ domβ while fixing η(t)Pβ (x) = ηP (x) for all x ∈ S
+ \ dom(β), which is possible from the orthogonality of θ and η.",4.3.2. COMPUTATION,0,[0]
"Using Newton’s method, η(t+1)Pβ (x) should satisfy( θ (t)",4.3.2. COMPUTATION,0,[0]
Pβ (x)− β(x) ),4.3.2. COMPUTATION,0,[0]
#NAME?,4.3.2. COMPUTATION,0,[0]
Jxy ( η (t+1),4.3.2. COMPUTATION,0,[0]
Pβ (y)− η(t)Pβ (y) ),4.3.2. COMPUTATION,0,[0]
"= 0,
for every x ∈ dom(β), where Jxy is an entry of the |dom(β)| × |dom(β)| Jacobian matrix J and given as
Jxy = ∂θ
(t)",4.3.2. COMPUTATION,0,[0]
"Pβ (x)
∂η",4.3.2. COMPUTATION,0,[0]
(t),4.3.2. COMPUTATION,0,[0]
"Pβ (y) = ∑ s∈S µ(s, x)µ(s, y)p (t) β (s) −1
from Theorem 3.",4.3.2. COMPUTATION,0,[0]
"Therefore, we have the update formula for all x ∈ dom(β) as
η (t+1)",4.3.2. COMPUTATION,0,[0]
Pβ (x) = η (t),4.3.2. COMPUTATION,0,[0]
"Pβ
(x)− ∑
y∈dom(β)
J−1xy ( θ (t)",4.3.2. COMPUTATION,0,[0]
"Pβ (y)− β(y) ) .
",4.3.2. COMPUTATION,0,[0]
"In e-projection, update η(t)Pβ (x) for x ∈ dom(β) while fixing θ(t)Pβ (x) = θP (x) for all x ∈ S
+ \ dom(β).",4.3.2. COMPUTATION,0,[0]
To ensure η (t),4.3.2. COMPUTATION,0,[0]
"Pβ
(⊥) = 1, we add ⊥ to dom(β) and β(⊥) = 1.",4.3.2. COMPUTATION,0,[0]
"We update θ(t)Pβ (x) at each step t as
θ (t+1)",4.3.2. COMPUTATION,0,[0]
Pβ (x) = θ (t),4.3.2. COMPUTATION,0,[0]
"Pβ
(x)− ∑
y∈dom(β)
",4.3.2. COMPUTATION,0,[0]
J ′ −1,4.3.2. COMPUTATION,0,[0]
xy ( η (t),4.3.2. COMPUTATION,0,[0]
"Pβ (y)− β(y) ) ,
J ′xy = ∂η
(t)",4.3.2. COMPUTATION,0,[0]
"Pβ (x)
∂θ (t)",4.3.2. COMPUTATION,0,[0]
"Pβ (y) = ∑ s∈S ζ(x, s)ζ(y, s)p (t) β (s)
",4.3.2. COMPUTATION,0,[0]
− |S|η(t)Pβ (x)η (t),4.3.2. COMPUTATION,0,[0]
"Pβ (y).
",4.3.2. COMPUTATION,0,[0]
"In this case, we also need to update θ(t)Pβ (⊥) as it is not guaranteed to be fixed.",4.3.2. COMPUTATION,0,[0]
"Let us define
p ′(t+1) β",4.3.2. COMPUTATION,0,[0]
(x) = p (t),4.3.2. COMPUTATION,0,[0]
"β (x) ∏ s∈dom(β)
exp ( θ (t+1)",4.3.2. COMPUTATION,0,[0]
"Pβ (s) )
exp ( θ (t) Pβ (s) ) ζ(s, x).
",4.3.2. COMPUTATION,0,[0]
"Since we have
p (t+1) β (x) =
exp ( θ (t+1)",4.3.2. COMPUTATION,0,[0]
"Pβ (⊥) )
exp ( θ (t)",4.3.2. COMPUTATION,0,[0]
Pβ (⊥) ),4.3.2. COMPUTATION,0,[0]
"p′(t+1)β (x),
it follows that
θ (t+1)",4.3.2. COMPUTATION,0,[0]
"Pβ (⊥)− θ(t)Pβ (⊥)
=",4.3.2. COMPUTATION,0,[0]
− log ( exp ( θ (t),4.3.2. COMPUTATION,0,[0]
Pβ (⊥) ),4.3.2. COMPUTATION,0,[0]
#NAME?,4.3.2. COMPUTATION,0,[0]
"(x) ) ,
The time complexity of each iteration is O(|dom(β)|3), which is required to compute the inverse of the Jacobian matrix.
",4.3.2. COMPUTATION,0,[0]
Global convergence of the projection algorithm is always guaranteed by the convexity of a submanifold S(β) defined in Equation (14).,4.3.2. COMPUTATION,0,[0]
"Since S(β) is always convex with respect to the θ- and η-coordinates, it is straightforward to see that our e-projection is an instance of the Bregman algorithm onto a convex region, which is well known to always converge to the global solution (Censor & Lent, 1981).",4.3.2. COMPUTATION,0,[0]
Now we are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold.,5. Balancing Matrices and Tensors,0,[0]
"Recall that the task of matrix balancing is to find r, s ∈",5.1. Matrix Balancing,0,[0]
"Rn that satisfy (RAS)1 = 1 and (RAS)T1 = 1 with R = diag(r) and S = diag(s) for a given nonnegative square matrix A = (aij) ∈ Rn×n≥0 .
",5.1. Matrix Balancing,0,[0]
"Let us define S as S = {(i, j) |",5.1. Matrix Balancing,0,[0]
"i, j ∈",5.1. Matrix Balancing,0,[0]
"[n] and aij ̸= 0}, (15)
where we remove zero entries from the outcome space S as our formulation cannot treat zero probability, and give each probability as p((i, j))",5.1. Matrix Balancing,0,[0]
= aij/ ∑ ij aij .,5.1. Matrix Balancing,0,[0]
"The partial order ≤ of S is naturally introduced as x = (i, j) ≤ y",5.1. Matrix Balancing,0,[0]
=,5.1. Matrix Balancing,0,[0]
"(k, l) ⇔ i ≤ j and k ≤",5.1. Matrix Balancing,0,[0]
"l, (16)
resulting in ⊥ = (1, 1).",5.1. Matrix Balancing,0,[0]
"In addition, we define ιk,m for each k ∈",5.1. Matrix Balancing,0,[0]
"[n] and m ∈ {1, 2}",5.1. Matrix Balancing,0,[0]
"such that
ιk,m = min{x =",5.1. Matrix Balancing,0,[0]
"(i1, i2) ∈ S | im = k }, where the minimum is with respect to the order ≤.",5.1. Matrix Balancing,0,[0]
"If ιk,m does not exist, we just remove the entire kth row if m = 1 or kth column if m = 2 from A.",5.1. Matrix Balancing,0,[0]
"Then we switch rows and columns of A so that the condition
ι1,m ≤ ι2,m ≤ · · · ≤ ιn,m (17) is satisfied for each m ∈ {1, 2}, which is possible for any matrices.",5.1. Matrix Balancing,0,[0]
"Since we have
η(ιk,m)− η(ιk+1,m) = { ∑n j=1 p((k, j)) if m = 1,∑n i=1",5.1. Matrix Balancing,0,[0]
"p((i, k))",5.1. Matrix Balancing,0,[0]
"if m = 2
if the condition (17) is satisfied, the probability distribution is balanced if for all",5.1. Matrix Balancing,0,[0]
k ∈,5.1. Matrix Balancing,0,[0]
[n],5.1. Matrix Balancing,0,[0]
"and m ∈ {1, 2}
η(ιk,m) = n−k+1
n .
",5.1. Matrix Balancing,0,[0]
"Therefore, we obtain the following result.
",5.1. Matrix Balancing,0,[0]
Matrix balancing as e-projection:,5.1. Matrix Balancing,0,[0]
"Given a matrix A ∈ Rn×n with its normalized probability distribution P ∈ S such that p((i, j))",5.1. Matrix Balancing,0,[0]
= aij/ ∑ ij aij .,5.1. Matrix Balancing,0,[0]
"Define the poset (S,≤) by Equations (15) and (16) and let S(β) be the submanifold of S such that S(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},
where the function β is given as dom(β)",5.1. Matrix Balancing,0,[0]
"= {ιk,m ∈ S | k ∈",5.1. Matrix Balancing,0,[0]
"[n],m ∈ {1, 2}},
β(ιk,m) = n−k+1
n .
",5.1. Matrix Balancing,0,[0]
"Matrix balancing is the e-projection of P onto the submanifold S(β), that is, the balanced matrix (RAS)/n is the distribution Pβ such that{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",5.1. Matrix Balancing,0,[0]
"if x ∈ dom(β),
which is unique and always exists in S, thanks to its dually flat structure.",5.1. Matrix Balancing,0,[0]
"Moreover, two balancing vectors r and s are
exp
( i∑
k=1
θPβ (ιk,m)− θP (ιk,m)
)",5.1. Matrix Balancing,0,[0]
"= { ri if m = 1, ai if m = 2,
for every i ∈",5.1. Matrix Balancing,0,[0]
"[n] and r = rn/ ∑
ij aij .",5.1. Matrix Balancing,0,[0]
■,5.1. Matrix Balancing,0,[0]
"Next, we generalize our approach from matrices to tensors.",5.2. Tensor Balancing,0,[0]
"For anN th order tensorA = (ai1i2...iN ) ∈ Rn1×n2×···×nN and a vector b ∈ Rnm , the m-mode product of A and b is defined as
(A×m b)i1...im−1im+1...iN = nm∑
im=1
ai1i2...iN bim .
",5.2. Tensor Balancing,0,[0]
We define tensor balancing as follows:,5.2. Tensor Balancing,0,[0]
"Given a tensor A ∈ Rn1×n2×···×nN with n1 = · · · = nN = n, find (N − 1) order tensors R1, R2, . . .",5.2. Tensor Balancing,0,[0]
", RN such that
A′ ×m 1 = 1 (∈ Rn1×···×nm−1×nm+1×···×nN ) (18) for all m ∈",5.2. Tensor Balancing,0,[0]
"[N ], i.e., ∑n
im=1",5.2. Tensor Balancing,0,[0]
"a′i1i2...iN = 1, where each
entry a′i1i2...iN of the balanced tensor A ′ is given as a′i1i2...iN = ai1i2...iN ∏
m∈[N ]
Rmi1...im−1im+1...iN .
",5.2. Tensor Balancing,0,[0]
"A tensor A′ that satisfies Equation (18) is called multistochastic (Cui et al., 2014).",5.2. Tensor Balancing,0,[0]
"Note that this is exactly the same as the matrix balancing problem if N = 2.
",5.2. Tensor Balancing,0,[0]
It is straightforward to extend matrix balancing to tensor balancing as e-projection onto a submanifold.,5.2. Tensor Balancing,0,[0]
"Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P such that
p(x) = ai1i2...iN / ∑ j1j2...jN aj1j2...jN (19)
for all x = (i1, i2, . . .",5.2. Tensor Balancing,0,[0]
", iN ).",5.2. Tensor Balancing,0,[0]
"The objective is to obtain Pβ such that ∑n im=1 pβ((i1, . . .",5.2. Tensor Balancing,0,[0]
", iN ))",5.2. Tensor Balancing,0,[0]
=,5.2. Tensor Balancing,0,[0]
1/(n N−1) for all m ∈,5.2. Tensor Balancing,0,[0]
"[N ] and i1, . . .",5.2. Tensor Balancing,0,[0]
", iN ∈",5.2. Tensor Balancing,0,[0]
[n].,5.2. Tensor Balancing,0,[0]
"In the same way as matrix balancing, we define S as
S = { (i1, i2, . . .",5.2. Tensor Balancing,0,[0]
", iN ) ∈",5.2. Tensor Balancing,0,[0]
"[n]N ∣∣ ai1i2...iN ̸= 0 } with removing zero entries and the partial order ≤ as
x = (i1 . . .",5.2. Tensor Balancing,0,[0]
iN ),5.2. Tensor Balancing,0,[0]
≤ y = (j1 . . .,5.2. Tensor Balancing,0,[0]
jN ) ⇔ ∀m ∈,5.2. Tensor Balancing,0,[0]
"[N ], im ≤ jm.
",5.2. Tensor Balancing,0,[0]
"In addition, we introduce ιk,m as
ιk,m = min{x =",5.2. Tensor Balancing,0,[0]
"(i1, i2, . . .",5.2. Tensor Balancing,0,[0]
", iN ) ∈ S",5.2. Tensor Balancing,0,[0]
"| im = k }.
and require the condition in Equation (17).
",5.2. Tensor Balancing,0,[0]
Tensor balancing as e-projection:,5.2. Tensor Balancing,0,[0]
Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P ∈ S given in Equation (19).,5.2. Tensor Balancing,0,[0]
"The submanifold S(β) of multistochastic tensors is given as
S(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},
where the domain of the function β is given as
dom(β)",5.2. Tensor Balancing,0,[0]
"= { ιk,m | k ∈",5.2. Tensor Balancing,0,[0]
"[n],m ∈",5.2. Tensor Balancing,0,[0]
"[N ] }
and each value is described using the zeta function as β(ιk,m) = ∑ l∈[n] ζ(ιk,m, ιl,m) 1 nN−1 .
",5.2. Tensor Balancing,0,[0]
"Tensor balancing is the e-projection of P onto the submanifold S(β), that is, the multistochastic tensor is the distribution",5.2. Tensor Balancing,0,[0]
"Pβ such that{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",5.2. Tensor Balancing,0,[0]
"if x ∈ dom(β),
which is unique and always exists in S, thanks to its dually flat structure.",5.2. Tensor Balancing,0,[0]
"Moreover, each balancing tensor Rm is
Rmi1...",5.2. Tensor Balancing,0,[0]
"im−1im+1...iN
= exp  ∑ m′",5.2. Tensor Balancing,0,[0]
̸=m im′∑,5.2. Tensor Balancing,0,[0]
"k=1 θPβ (ιk,m′)− θP (ιk,m′)  for every m ∈",5.2. Tensor Balancing,0,[0]
"[N ] and R1 = R1nN−1/ ∑ j1...jN
aj1...jN to recover a multistochastic tensor.",5.2. Tensor Balancing,0,[0]
■ Our result means that the e-projection algorithm based on Newton’s method proposed in Section 4.3 converges to the unique balanced tensor whenever S(β) ̸= ∅ holds.,5.2. Tensor Balancing,0,[0]
"In this paper, we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using Newton’s method.",6. Conclusion,0,[0]
"Our algorithm quadratically converges, while the popular Sinkhorn-Knopp algorithm linearly converges.",6. Conclusion,0,[0]
"We have examined the efficiency of our algorithm in numerical experiments on matrix balancing and showed that the proposed algorithm is several orders of magnitude faster than the existing approaches.
",6. Conclusion,0,[0]
"We have analyzed theories behind the algorithm, and proved that balancing is e-projection in a special type of a statistical manifold, in particular, a dually flat Riemannian manifold studied in information geometry.",6. Conclusion,0,[0]
"Our key finding is that the gradient of the manifold, equivalent to Riemannian metric or the Fisher information matrix, can be analytically obtained using the Möbius inversion formula.
",6. Conclusion,0,[0]
Our information geometric formulation can model several machine learning applications such as statistical analysis on a DAG structure.,6. Conclusion,0,[0]
"Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study in future work.",6. Conclusion,0,[0]
The authors sincerely thank Marco Cuturi for his valuable comments.,Acknowledgements,0,[0]
"This work was supported by JSPS KAKENHI Grant Numbers JP16K16115, JP16H02870 (MS), JP26120732 and JP16H06570 (HN).",Acknowledgements,0,[0]
"The research of K.T. was supported by JST CREST JPMJCR1502, RIKEN PostK, KAKENHI Nanostructure and KAKENHI JP15H05711.",Acknowledgements,0,[0]
"We solve tensor balancing, rescaling an N th order nonnegative tensor by multiplying N tensors of order N −1",abstractText,0,[0]
so that every fiber sums to one.,abstractText,0,[0]
This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics.,abstractText,0,[0]
We present an efficient balancing algorithm with quadratic convergence using Newton’s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones.,abstractText,0,[0]
"To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold.",abstractText,0,[0]
"The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton’s method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics.",abstractText,0,[0]
"Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.",abstractText,0,[0]
Tensor Balancing on Statistical Manifold,title,0,[0]
Probabilistic graphical models provide a general framework to conveniently build probabilistic models in a modular and compact way.,1. Introduction,0,[0]
"They are commonly used in statistics, computer vision, natural language processing, machine learning and many related fields (Wainwright & Jordan, 2008).",1. Introduction,0,[0]
The success of graphical models depends largely on the availability of efficient inference algorithms.,1. Introduction,0,[0]
"Unfortunately, exact inference is intractable in general, making approximate inference an important research topic.
",1. Introduction,0,[0]
Approximate inference algorithms generally adopt a variational approach or a sampling approach.,1. Introduction,0,[0]
"The variational approach formulates the inference problem as an optimi-
1Australian National University, Canberra, Australia.",1. Introduction,0,[0]
"2National University of Singapore, Singapore.",1. Introduction,0,[0]
"3Queensland University of Technology, Brisbane, Australia.",1. Introduction,0,[0]
"Correspondence to: Andrew Wrigley <andrew.wrigley@anu.edu.au>, Wee Sun Lee <leews@comp.nus.edu.sg>, Nan Ye <n.ye@qut.edu.au>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
sation problem and constructs approximations by solving relaxations of the optimisation problem.,1. Introduction,0,[0]
"A number of wellknown inference algorithms can be seen as variational algorithms, such as loopy belief propagation, mean-field variational inference, and generalized belief propagation (Wainwright & Jordan, 2008).",1. Introduction,0,[0]
The sampling approach uses sampling to approximate either the underlying distribution or key quantities of interest.,1. Introduction,0,[0]
"Commonly used sampling methods include particle filters and Markov-chain Monte Carlo (MCMC) algorithms (Andrieu et al., 2003).
",1. Introduction,0,[0]
"Our proposed algorithm, tensor belief propagation (TBP), can be seen as a sampling-based algorithm.",1. Introduction,0,[0]
"Unlike particle filters or MCMC methods, which sample states (also known as particles), TBP samples functions in the form of rank-1 tensors.",1. Introduction,0,[0]
"Specifically, we use a data structure commonly used in exact inference, the junction tree, and perform approximate message passing on the junction tree using messages represented as mixtures of rank-1 tensors.
",1. Introduction,0,[0]
We assume that each factor in the graphical model is originally represented as a tensor decomposition (mixture of rank-1 tensors).,1. Introduction,0,[0]
"Under this assumption, all messages and intermediate factors also have the same representation.",1. Introduction,0,[0]
"Our key observation is that marginalisation can be performed efficiently for mixtures of rank-1 tensors, and multiplication can be approximated by sampling.",1. Introduction,0,[0]
"This leads to an approximate message passing algorithm where messages and intermediate factors are approximated by low-rank tensors.
",1. Introduction,0,[0]
"We provide analysis, giving conditions under which the method performs well.",1. Introduction,0,[0]
"We compare TBP experimentally with several existing approximate inference methods using Ising models, random MRFs and two real-world datasets, demonstrating promising results.",1. Introduction,0,[0]
"Exact inference on tree-structured graphical models can be performed efficiently using belief propagation (Pearl, 1982), a dynamic programming algorithm that involves passing messages between nodes containing the results of intermediate computations.",2. Related Work,0,[0]
"For arbitrary graphical models, the well-known junction tree algorithm (Shafer & Shenoy, 1990; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990) is commonly used.",2. Related Work,0,[0]
"The model is first compiled into a junc-
tion tree data structure and a similar message passing algorithm is then run over the junction tree.",2. Related Work,0,[0]
"Unfortunately, for non-tree models the time and space complexity of the junction tree algorithm grows exponentially with a property of the graph called its treewidth.",2. Related Work,0,[0]
"For high-treewidth graphical models, exact inference is intractable in general.
",2. Related Work,0,[0]
Our work approximates the messages passed in the junction tree algorithm to avoid the exponential runtime caused by exact computations in high-treewidth models.,2. Related Work,0,[0]
Various previous work has taken the same approach.,2. Related Work,0,[0]
"Expectation propagation (EP) (Minka, 2001) approximates messages by minimizing the Kullback-Leiber (KL) divergence between the actual message and its approximation.",2. Related Work,0,[0]
"Structured message passing (Gogate & Domingos, 2013) can be considered as a special case of EP where structured representations, in particular algebraic decision diagrams (ADDs) and sparse hash tables, are used so that EP can be performed efficiently.",2. Related Work,0,[0]
"In contrast to ADDs, the tensor decompositions used for TBP may provide a more compact representation for some problems.",2. Related Work,0,[0]
An ADD partitions a tensor into axis-aligned hyper-rectangles – it is possible to represent a hyper-rectangle using a rank-1 tensor but a rank-1 tensor is generally not representable as an axis-aligned rectangle.,2. Related Work,0,[0]
"Furthermore, the supports of the rank-1 tensors in the mixture may overlap.",2. Related Work,0,[0]
"However, ADD compresses hyperrectangles that share sub-structures and this may result in the methods having different strengths.",2. Related Work,0,[0]
"Sparse tables, on the other hand, work well for cases with extreme sparsity.
",2. Related Work,0,[0]
"Several methods use sampled particles to approximate messages (Koller et al., 1999; Ihler & McAllester, 2009; Sudderth et al., 2010).",2. Related Work,0,[0]
"To allow their algorithms to work well on problems with less sparsity, Koller et al. (1999) and Sudderth et al. (2010) use non-parametric methods to smooth the particle representation of messages.",2. Related Work,0,[0]
"In contrast, we decompose each tensor into a mixture of rank-1 tensors and sample the rank-1 tensors directly, instead of through the intermediate step of sampling particles.",2. Related Work,0,[0]
"Another approach, which pre-samples the particles at each node and passes messages between these pre-sampled particles was taken by Ihler & McAllester (2009).",2. Related Work,0,[0]
"The methods of Ihler & McAllester (2009) and Sudderth et al. (2010) were also applied on graphs with loops using loopy belief propagation.
",2. Related Work,0,[0]
Xue et al. (2016) use discrete Fourier representations for inference via the elimination algorithm.,2. Related Work,0,[0]
The discrete Fourier representation is a special type of tensor decomposition.,2. Related Work,0,[0]
"Instead of sampling, the authors perform approximations by truncating the Fourier coefficients, giving different approximation properties.",2. Related Work,0,[0]
"Other related works include (Darwiche, 2000; Park & Darwiche, 2002; Chavira & Darwiche, 2005), where belief networks are compiled into compact arithmetic circuits (ACs).",2. Related Work,0,[0]
"On the related problem of MAP inference, McAuley & Caetano (2011) show that junction tree
clusters that factor over subcliques or consist only of latent variables yield improved complexity properties.",2. Related Work,0,[0]
"For simplicity we limit our discussion to Markov random fields (MRFs), but our results apply equally to Bayesian networks and general factor graphs.",3. Preliminaries,0,[0]
We focus only on discrete models.,3. Preliminaries,0,[0]
"A Markov random field G is an undirected graph representing a probability distribution P (X1, . . .",3. Preliminaries,0,[0]
", XN ), such that P factorises over the maxcliques in G, i.e.
P (X1, . . .",3. Preliminaries,0,[0]
", XN ) = 1
Z ∏ c∈cl(G) φc(Xc) (1)
where cl(G) is the set of max-cliques in G and Z =∑ X ∏ c∈cl(G) φc(Xc) ensures normalisation.",3. Preliminaries,0,[0]
"We call the factors φc clique potentials or potentials.
",3. Preliminaries,0,[0]
"TBP is based on the junction tree algorithm (see e.g. (Koller & Friedman, 2009)).",3. Preliminaries,0,[0]
"A junction tree is a special type of cluster graph, i.e. an undirected graph with nodes called clusters that are associated with sets of variables rather than single variables.",3. Preliminaries,0,[0]
"Specifically, a junction tree is a cluster graph that is a tree and which also satisfies the running intersection property.",3. Preliminaries,0,[0]
"The running intersection property states that if a variable is in two clusters, it must also be in every cluster on the path that connects the two clusters.
",3. Preliminaries,0,[0]
The junction tree algorithm is essentially the well-known belief propagation algorithm applied to the junction tree after the cluster potentials have been initialized.,3. Preliminaries,0,[0]
"At initialisation, each clique potential is first associated with a cluster.",3. Preliminaries,0,[0]
Each cluster potential Φt(Xt) is computed by multiplying all the clique potentials φc(Xc) associated with the cluster Xt.,3. Preliminaries,0,[0]
"Thereafter, the algorithm is defined recursively in terms of messages passed between neighbouring clusters.",3. Preliminaries,0,[0]
"A message is always a function of the variables in the receiving cluster, and represents an intermediate marginalisation over a partial set of factors.",3. Preliminaries,0,[0]
"The messagemt→s(Xs) sent from a cluster t to a neighbouring cluster s is defined recursively by
mt→s(Xs) = ∑
Xt\Xs
Φt(Xt)",3. Preliminaries,0,[0]
"∏
u∈N(t)\{s}
mu→t(Xt), (2)
where N(t) is the set of neighbours of t. Since the junction tree is singly connected, this recursion is well-defined.",3. Preliminaries,0,[0]
"After all messages have been computed, the marginal distribution on a cluster of variables Xs is computed using
Ps(Xs) ∝ Φs(Xs)",3. Preliminaries,0,[0]
"∏
t∈N(s)
mt→s(Xs), (3)
and univariate marginals can be computed by summation over cluster marginals.",3. Preliminaries,0,[0]
"The space and time complexity of
the junction tree inference algorithm is exponential in the induced width of the graph, i.e. the number of variables in the largest tree cluster minus 1 (Koller & Friedman, 2009).",3. Preliminaries,0,[0]
The lowest possible induced width (over all possible junction trees for the graph) is defined as the treewidth of the graph1.,3. Preliminaries,0,[0]
"The TBP algorithm we propose (Algorithm 1) is the same as the junction tree algorithm except for the approximations at line 1 and line 4, which we describe below.
",4. Tensor Belief Propagation,0,[0]
"Algorithm 1 Tensor Belief Propagation input Clique potentials {φc(Xc)}, junction tree J .",4. Tensor Belief Propagation,0,[0]
"output Approximate cluster marginals {P̃s(Xs)} for all s.
1: For each cluster Xt, compute Φ̃t(Xt)",4. Tensor Belief Propagation,0,[0]
"≈ ∏ c φc(Xc),
where the product is over all cliques c associated with t. 2: while there is any unsent message do 3: Pick an unsent message mt→s(Xs) with all messages to t from neighbours other than s sent.",4. Tensor Belief Propagation,0,[0]
4: Send m̃t→s(Xs),4. Tensor Belief Propagation,0,[0]
"≈
∑ Xt\Xs Φ̃t(Xt) ∏",4. Tensor Belief Propagation,0,[0]
"u∈N(t)\{s} m̃u→t(Xt).
",4. Tensor Belief Propagation,0,[0]
"5: end while 6: return P̃s(Xs) ∝ Φ̃s(Xs) ∏ t∈N(s) m̃t→s(Xs).
",4. Tensor Belief Propagation,0,[0]
"There are two challenges in applying the junction tree algorithm to high-treewidth graphical models: representing the intermediate potential functions, and computing them.",4. Tensor Belief Propagation,0,[0]
"For representation, using tables to represent the cluster potentials and messages requires space exponential in the induced width.",4. Tensor Belief Propagation,0,[0]
"For computation, the two operations relevant to the complexity of the algorithm are marginalisation over a subset of variables in a cluster and multiplication of multiple factors.",4. Tensor Belief Propagation,0,[0]
"When clusters become large, these operations become intractable unless additional structure can be exploited.
",4. Tensor Belief Propagation,0,[0]
TBP alleviates these difficulties by representing all potential functions as mixtures of rank-1 tensors.,4. Tensor Belief Propagation,0,[0]
"We show how to perform exact marginalisation of a mixture (required in line 4) efficiently, and how to perform approximate multiplication of mixtures using sampling (used for approximation in lines 1 and 4).",4. Tensor Belief Propagation,0,[0]
"As we are concerned with discrete distributions, each potential φc can be represented by a multidimensional array, i.e. a tensor.",4.1. Mixture of Rank-1 Tensors,0,[0]
"Furthermore, a d-dimensional tensor T ∈ RN1×···×Nd can be decomposed into a weighted sum
1Finding the treewidth of a graph is NP-hard; in practice we use various heuristics to construct the junction tree.
of outer products of vectors as
T = r∑ k=1 wk a 1 k ⊗ a2k ⊗ · · · ⊗ adk, (4)
where wk ∈ R, aik ∈ RNi and ⊗ is the outer product, i.e.( a1k ⊗ a2k ⊗ · · · ⊗ adk )",4.1. Mixture of Rank-1 Tensors,0,[0]
"i1,...,id = ( a1k ) i1 · · · · · ( adk ) id
.",4.1. Mixture of Rank-1 Tensors,0,[0]
We denote the vector of weights {wk} as w. The smallest r for which an exact r-term decomposition exists is called the rank of T and a decomposition (4) using this r is a tensor rank decomposition.,4.1. Mixture of Rank-1 Tensors,0,[0]
"This decomposition is known by several names including CANDECOMP/PARAFAC (CP) decomposition and Hitchcock decomposition (Kolda & Bader, 2009).",4.1. Mixture of Rank-1 Tensors,0,[0]
"In this paper, we assume without loss of generality that the weights are non-negative and sum to 1, giving a mixture of rank-1 tensors.",4.1. Mixture of Rank-1 Tensors,0,[0]
"Such a mixture forms a probability distribution over rank-1 tensors, which we refer to as a tensor belief.",4.1. Mixture of Rank-1 Tensors,0,[0]
"We also assume that the decomposition is non-negative, i.e. the rank-1 tensors are nonnegative, although the method can be extended to allow negative values.
",4.1. Mixture of Rank-1 Tensors,0,[0]
"For a (clique or cluster) potential function φs(Xs) over |Xs| = Ns variables, (4) is equivalent to decomposing φs into a sum of fully-factorised terms, i.e.
φs(Xs) =",4.1. Mixture of Rank-1 Tensors,0,[0]
"r∑ k=1 wsk ψ s k(Xs)
=",4.1. Mixture of Rank-1 Tensors,0,[0]
"r∑ k=1 wsk ψ s k,1(Xs1) · · ·ψsk,Ns(XsNs ).",4.1. Mixture of Rank-1 Tensors,0,[0]
"(5)
",4.1. Mixture of Rank-1 Tensors,0,[0]
This observation allows us to perform marginalisation and multiplication operations efficiently.,4.1. Mixture of Rank-1 Tensors,0,[0]
"Marginalising out a variable Xi from a cluster Xs simplifies in the same manner as if the distribution was fullyfactorised, namely∑ Xi φs(Xs) = r∑ k=1 wsk (∑ Xi ψsk,j(Xi) ) ·",4.2. Marginalisation,0,[0]
"(6)
ψsk,1(Xs1)ψ s k,2(Xs2) · · ·ψsk,Ns(XsNs )︸",4.2. Marginalisation,0,[0]
"︷︷ ︸
excluding ψsk,j(Xi) where we simply push the sum ∑ Xi
inside and only evaluate it over the univariate factor ψsk,j(Xi).",4.2. Marginalisation,0,[0]
We can then absorb this sum into the weights {wsk} and the result stays in decomposed form (5).,4.2. Marginalisation,0,[0]
"To marginalise over multiple variables, we evaluate (6) for each variable in turn.",4.2. Marginalisation,0,[0]
"The key observation for multiplication is that a mixture of rank-1 tensors can be treated as a probability distribution
over the rank-1 tensors with expectation equal to the true function, by considering the weight of each rank-1 term wk as its probability.
",4.3. Multiplication,1,['The idea is to model the unknown function as a Gaussian process with a given kernel function dictating the smoothness properties.']
"To multiply two potentials φi and φj , we repeatedly sample rank-1 terms from each multiplicand to build the product.",4.3. Multiplication,0,[0]
"We draw a sample of K pairs of indices {(kr, lr)}Kr=1 independently from wi and wj respectively, and use the approximation
φi(Xi) · φj(Xj) ≈ 1
K K∑ r=1 ψikr (Xi) ·",4.3. Multiplication,0,[0]
ψ,4.3. Multiplication,0,[0]
j lr (Xj).,4.3. Multiplication,0,[0]
"(7)
The approximation is also a mixture of rank-1 tensors, with the rank-1 tensors being the ψikr (Xi) ·",4.3. Multiplication,0,[0]
ψ,4.3. Multiplication,0,[0]
"j lr
(Xj), and their weights being the frequencies of the sampled (kr, lr) pairs.",4.3. Multiplication,0,[0]
This process is equivalent to drawing a sample of the same size from the distribution representing φi(Xi) ·φj(Xj) and hence provides an unbiased estimate of the product function.,4.3. Multiplication,0,[0]
Multiplication of each pair ψikr (Xi) · ψ,4.3. Multiplication,0,[0]
"j lr
(Xj) can be performed efficiently due to their factored forms.",4.3. Multiplication,0,[0]
"It is possible to extend the method to allow multiplication of more potential functions simultaneously but we only use pairwise multiplication in this paper, repeating the process as necessary to multiply more functions.",4.3. Multiplication,0,[0]
"For simplicity, we give results for binary MRFs.",4.4. Theoretical Analysis,0,[0]
Extension to non-binary variables is straightforward.,4.4. Theoretical Analysis,0,[0]
"We assume that exact tensor decompositions are given for all initial clique potentials.
",4.4. Theoretical Analysis,0,[0]
Theorem 1.,4.4. Theoretical Analysis,0,[0]
Consider a binary MRF with C max-cliques with potential functions represented as non-negative tensor decompositions.,4.4. Theoretical Analysis,0,[0]
Consider the unnormalised distribution D(X) = ∏ c∈cl(G) φc(Xc).,4.4. Theoretical Analysis,0,[0]
Let Di(Xi) = ∑ X\Xi D(X) be the unnormalised marginal for variableXi.,4.4. Theoretical Analysis,0,[0]
"Assume that the values of the rank-1 tensors in any mixture resulting from pairwise multiplication is upper bounded by M , and that the approximation target for any cell in any multiplication operation is lower bounded by B. Let D̃i(Xi) be the estimates produced by TBP using a junction tree with an induced width T .",4.4. Theoretical Analysis,0,[0]
"With probability at least 1 − δ, for all i and xi,
(1− )Di(xi) ≤",4.4. Theoretical Analysis,0,[0]
"D̃i(xi) ≤ (1 + )Di(xi)
if the sample size used for all multiplication operations is at least
Kmin( , δ) ∈",4.4. Theoretical Analysis,0,[0]
"O ( C2 2 M
B
( logC + T + log 2
δ
)) .
",4.4. Theoretical Analysis,0,[0]
"Furthermore, D̃i(Xi) remains a consistent estimator for Di(Xi) if B = 0.
",4.4. Theoretical Analysis,0,[0]
Proof.,4.4. Theoretical Analysis,0,[0]
We give the proof for the case B 6= 0 here.,4.4. Theoretical Analysis,0,[0]
"The consistency proof for the case B = 0 can be found in the supplementary material.
",4.4. Theoretical Analysis,0,[0]
The errors in the unnormalised marginals are due to the errors in approximate pairwise multiplication defined by Equation (7).,4.4. Theoretical Analysis,0,[0]
"We first give bounds for the errors in all the pairwise multiplication by sampling operations, then derive the bounds for the errors in the unnormalised marginals.
",4.4. Theoretical Analysis,0,[0]
"Recall that by the multiplicative Chernoff bound (see e.g. (Koller & Friedman, 2009)), for K i.i.d.",4.4. Theoretical Analysis,0,[0]
"random variables Y1, . . .",4.4. Theoretical Analysis,0,[0]
", YK in range [0,M ] with expected value µ, we have
P
( 1
K K∑ i=1",4.4. Theoretical Analysis,0,[0]
Yi /∈,4.4. Theoretical Analysis,0,[0]
"[(1− ζ)µ, (1 + ζ)µ]
) ≤ 2 exp ( −ζ 2µK
3M
) .
",4.4. Theoretical Analysis,0,[0]
The number of pairwise multiplications required to initialize the cluster potentials {Φ̃t(Xt)} is at most C (line 1).,4.4. Theoretical Analysis,0,[0]
"The junction tree has at most C clusters, and thus at most 2(C − 1) messages need to be computed.",4.4. Theoretical Analysis,0,[0]
Each message requires at most C pairwise multiplications (line 4).,4.4. Theoretical Analysis,0,[0]
Hence the total number of pairwise multiplications needed is at most 2C2.,4.4. Theoretical Analysis,0,[0]
"Each pairwise multiplication involves functions defined on at most T binary variables, and thus it simultaneously estimates at most 2T values.",4.4. Theoretical Analysis,0,[0]
"Hence at most 2T+1C2 values are estimated by sampling in the algorithm.
",4.4. Theoretical Analysis,0,[0]
"Since each estimate satisfies the Chernoff bound, we can apply a union bound to bound the probability that the estimate for any µj is outside [(1− ζ)µj , (1 + ζ)µj ], giving
Pζ
( 1
K K∑ i=1",4.4. Theoretical Analysis,0,[0]
Xji /∈,4.4. Theoretical Analysis,0,[0]
"[(1− ζ)µj , (1 + ζ)µj ] for
any estimate j ) ≤ 2T+2C2 exp ( −ζ 2BK
3M
) ,
whereB is a lower bound on the minimum value of all cells estimated during the algorithm.",4.4. Theoretical Analysis,0,[0]
"If we set an upper-bound on this error probability of δ and rearrange for K, we have that with probability at least δ, all estimates are within a factor of (1± ζ) of their true values when
K ≥ 3 ζ2 M B ln
2T+2C2
δ .",4.4. Theoretical Analysis,0,[0]
"(8)
We now seek an expression for the sample size required for the unnormalised marginals to have small errors.",4.4. Theoretical Analysis,0,[0]
First we argue that at most C + Q,4.4. Theoretical Analysis,0,[0]
"− 1 pairwise multiplications are used in the process of constructing the marginals at any node u, where Q is the number of clusters.",4.4. Theoretical Analysis,0,[0]
This is true for the base case of a tree of size 1 as no messages need to be passed.,4.4. Theoretical Analysis,0,[0]
"As the inductive hypothesis, assume that the statement is true for trees of size less than n. The node u is
considered as the root of the tree and by the inductive hypothesis the number of multiplications used in each subtree i is at most Ci + Qi",4.4. Theoretical Analysis,0,[0]
"− 1 where Ci and Qi are the number of clique potentials and clusters associated with subtree i. Summing them up and noting that we need to perform at most one additional multiplication for each clique potential associated with u for initialisation, and one additional multiplication for each subtree, gives us the required result.",4.4. Theoretical Analysis,0,[0]
"To simplify analysis, we bound C + Q",4.4. Theoretical Analysis,0,[0]
"− 1 by 2C from here onwards.
",4.4. Theoretical Analysis,0,[0]
"At worst, each pairwise multiplication results in an extra (1 ± ζ) factor in the bound.",4.4. Theoretical Analysis,0,[0]
"Since we are using a multiplicative bound, marginalisation operations have no effect on the bound.",4.4. Theoretical Analysis,0,[0]
"As we do no more than 2C multiplications, the final marginal estimates are all within a factor (1±ζ)2C of their true value.
",4.4. Theoretical Analysis,0,[0]
"To bound the marginal so that it is within a factor (1 ± ) of its true value for a chosen > 0, we note that choosing ζ = ln(1+ )",4.4. Theoretical Analysis,0,[0]
"2C implies (1 − ζ)
2C ≥ 1 − and (1 + ζ)2C ≤",4.4. Theoretical Analysis,0,[0]
"(1 + ):
(1− ζ)2C = (
1− ln(1 + )",4.4. Theoretical Analysis,0,[0]
"2C
)2C (9)
≥ (
1− 2C
)",4.4. Theoretical Analysis,0,[0]
"2C ≥ 1−
(1 + ζ)2C =",4.4. Theoretical Analysis,0,[0]
"( 1 + ln(1 + )
2C
)2C (10)
≤ exp (ln(1 + ))",4.4. Theoretical Analysis,0,[0]
"= 1 + .
",4.4. Theoretical Analysis,0,[0]
"In (9) we use Bernoulli’s inequality together with ln(1 + ) ≤ , and in (10) we use (1 + x)r ≤ exp(rx).
",4.4. Theoretical Analysis,0,[0]
"Substituting this ζ into (8), we have that all marginal estimates are accurate within factor (1± ) with probability at least 1− δ, when
K ≥ 12C 2 (ln(1 + ))",4.4. Theoretical Analysis,0,[0]
"2 M B ln 2T+2C2 δ (11)
",4.4. Theoretical Analysis,0,[0]
Using the fact that ln(1 + ),4.4. Theoretical Analysis,0,[0]
"≥ · ln 2 for 0 ≤ ≤ 1, and 24 ln 2 < 35, we can relax this bound to
K ≥ 35C 2 2 M
B
( logC + T + log 2
δ
) .
",4.4. Theoretical Analysis,0,[0]
Corollary 1.,4.4. Theoretical Analysis,0,[0]
"Under the same conditions as Theorem 1, with probability at least 1− δ, the normalised marginal estimates p̃i(xi) satisfy
(1− γ)pi(xi) ≤ p̃i(xi) ≤ (1 + γ)pi(xi)
for all i and xi, if the sample size used for all multiplication operations is at least
K ′min(γ, δ) ∈",4.4. Theoretical Analysis,0,[0]
"O ( C2
γ2 M B
( logC + T + log 1
δ
)) .
",4.4. Theoretical Analysis,0,[0]
Proof.,4.4. Theoretical Analysis,0,[0]
"Suppose the unnormalised marginals have relative error bounded by (1± ), i.e.
(1− )Di(xi) ≤",4.4. Theoretical Analysis,0,[0]
"D̃i(xi) ≤ (1 + )Di(xi)
for all i and xi.",4.4. Theoretical Analysis,0,[0]
"Then we have
p̃i(xi) = D̃i(xi)∑ xi D̃i(xi) ≤ (1 + )Di(xi)∑",4.4. Theoretical Analysis,0,[0]
xi (1− )Di(xi),4.4. Theoretical Analysis,0,[0]
1,4.4. Theoretical Analysis,0,[0]
"+ 1− pi(xi).
",4.4. Theoretical Analysis,0,[0]
"To bound the relative error on p̃i(xi) to (1 + γ), we set
1 + 1− = 1 + γ =⇒ =",4.4. Theoretical Analysis,0,[0]
"γ γ + 2 .
",4.4. Theoretical Analysis,0,[0]
Since 1 2 = ( γ+2 γ )2 <,4.4. Theoretical Analysis,0,[0]
"( 3 γ )2 = O ( 1 γ2 ) for γ < 1, the increase in Kmin required to bound the normalised estimates rather than the unnormalised estimates is at most a constant.",4.4. Theoretical Analysis,0,[0]
"Thus,
K ′min(γ, δ) ∈ O ( C2
γ2 M B
( logC + T + log 1
δ
)) .
",4.4. Theoretical Analysis,0,[0]
as required.,4.4. Theoretical Analysis,0,[0]
"The negative side of the bound is similar.
",4.4. Theoretical Analysis,0,[0]
"Interestingly, the sample size does not grow exponentially with the induced width, and hence the treewidth of the graph.",4.4. Theoretical Analysis,0,[0]
"As the inference problem is NP-hard, we expect the ratio M/B to be large in difficult problems.",4.4. Theoretical Analysis,0,[0]
The M/B ratio comes from bounding the relative error when sampling a mixture φ(x) =,4.4. Theoretical Analysis,0,[0]
∑r k=1 wkψk(x).,4.4. Theoretical Analysis,0,[0]
A more refined bound can be obtained by bounding maxk maxx ψk(x)/φ(x) instead; this bound would not grow as quickly if ψk(x) is always small whenever φ(x) is small.,4.4. Theoretical Analysis,0,[0]
"Understanding the properties of function classes where these bounds are small may help us understand when TBP works well.
",4.4. Theoretical Analysis,0,[0]
Theorem 1 suggests that that it may be useful to reweight the rank-1 tensors in a multiplicand to give a smaller M/B ratio.,4.4. Theoretical Analysis,0,[0]
"The following proposition gives a reweighting scheme that minimises the maximum value of the rank-1 tensors in a multiplicand, which leads to a smaller M with B fixed.",4.4. Theoretical Analysis,0,[0]
"Theorem 1 still holds with this reweighting.
Proposition 1.",4.4. Theoretical Analysis,0,[0]
Let φ(x) =,4.4. Theoretical Analysis,0,[0]
"∑r k=1 wkψk(x) where wk ≥
0, ∑r k=1 wk = 1 and ψk(x) ≥ 0 for all x. Con-
sider a reweighted representation ∑r",4.4. Theoretical Analysis,0,[0]
k=1 w ′,4.4. Theoretical Analysis,0,[0]
"kψ ′ k(x), where ψ′k(x) = wk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k",4.4. Theoretical Analysis,0,[0]
1,4.4. Theoretical Analysis,0,[0]
Then maxk maxx ψ′k(x) is minimized when w ′,4.4. Theoretical Analysis,0,[0]
"k ∝ wk maxx ψk(x).
",4.4. Theoretical Analysis,0,[0]
Proof.,4.4. Theoretical Analysis,0,[0]
"Let ak = wk maxx ψk(x), and let v =",4.4. Theoretical Analysis,0,[0]
maxk,4.4. Theoretical Analysis,0,[0]
"maxx ψ ′ k(x) = maxk
ak w′k .",4.4. Theoretical Analysis,0,[0]
"For any choice of w′, we have v ≥ ∑
k",4.4. Theoretical Analysis,0,[0]
ak∑ k w ′,4.4. Theoretical Analysis,0,[0]
k = ∑ k ak.,4.4. Theoretical Analysis,0,[0]
"The first inequality holds be-
cause vw′k ≥ ak for any k by the definition of v. Since
Tensor Belief Propagation∑ k ak is a constant lower bound for v, and this is clearly achieved by setting w′k ∝",4.4. Theoretical Analysis,0,[0]
"ak, we have the claimed result.
",4.4. Theoretical Analysis,0,[0]
"Note that with ψk(x) represented as a rank-1 tensor, the maximum value over x can be computed quickly with the help of the factored structure.
",4.4. Theoretical Analysis,0,[0]
Reweighting the rank-1 tensors as described in Proposition 1 and then sampling gives a form of importance sampling.,4.4. Theoretical Analysis,0,[0]
Importance sampling is often formulated instead with the objective of minimizing the variance of the estimates.,4.4. Theoretical Analysis,0,[0]
"Since we expect lowering the variance of intermediate estimates to lead to lower variance on the final marginal estimates, we also examine the following alternative reweighting scheme.",4.4. Theoretical Analysis,0,[0]
Proposition 2.,4.4. Theoretical Analysis,0,[0]
Let φ(x) =,4.4. Theoretical Analysis,0,[0]
"∑r k=1 wkψk(x) where wk ≥
0, ∑r k=1 wk = 1.",4.4. Theoretical Analysis,0,[0]
"Consider a reweighted representa-
tion ∑r k=1",4.4. Theoretical Analysis,0,[0]
w ′,4.4. Theoretical Analysis,0,[0]
"kψ ′ k(x), where ψ ′ k(x) =
wk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k",4.4. Theoretical Analysis,0,[0]
1,4.4. Theoretical Analysis,0,[0]
Let φ̃(x) = 1 K ∑K i=1 ψ ′,4.4. Theoretical Analysis,0,[0]
"Ki(x) be an estimator for φ(x), where each Ki is drawn independently from the categorical distribution with parameters {w′1, . . .",4.4. Theoretical Analysis,0,[0]
", w′r}.",4.4. Theoretical Analysis,0,[0]
"Then φ̃(x) is unbiased, and the total variance ∑ x Var[φ̃
′(x)] is minimized when w′k ∝",4.4. Theoretical Analysis,0,[0]
"wk √∑ x ψk(x) 2.
",4.4. Theoretical Analysis,0,[0]
Proof.,4.4. Theoretical Analysis,0,[0]
Clearly φ̃(x) is an unbiased estimator for φ(x).,4.4. Theoretical Analysis,0,[0]
"The variance of this estimator is
Var[φ̃(x)]",4.4. Theoretical Analysis,0,[0]
"= 1
K Var[ψ′K′i(x)]
= 1
K
( E[ψ′Ki(x) 2]− E[ψ′Ki(x)] 2 )
",4.4. Theoretical Analysis,0,[0]
"= 1
K (∑ k w′kψ ′ k(x) 2 − φ(x)2 )
",4.4. Theoretical Analysis,0,[0]
"= 1
K (∑ k w′k",4.4. Theoretical Analysis,0,[0]
w2k,4.4. Theoretical Analysis,0,[0]
"w′2k ψk(x) 2 − φ(x)2 )
",4.4. Theoretical Analysis,0,[0]
"= 1
K (∑ k w2k w′k ψk(x) 2",4.4. Theoretical Analysis,0,[0]
"− φ(x)2 ) .
",4.4. Theoretical Analysis,0,[0]
"Since K and φ(x) are constant, we have
{w′k}∗ = argmin {w′k} ∑ x Var[φ̃′(x)]
= argmin {w′k} ∑ x ∑ k 1 w′k (wkψk(x)) 2,
where each w′k ≥ 0, ∑ k w ′ k = 1.",4.4. Theoretical Analysis,0,[0]
"A straightforward application of the method of Lagrange multipliers yields
w′k = wk √∑ x ψk(x)",4.4. Theoretical Analysis,0,[0]
"2∑
l wl √∑ x ψl(x) 2 .
",4.4. Theoretical Analysis,0,[0]
"The factored forms of rank-1 tensors again allows the importance reweighting to be computed efficiently.
",4.4. Theoretical Analysis,0,[0]
Propositions 1 and 2 could be applied directly to the algorithm by multiplying two potential functions fully before sampling.,4.4. Theoretical Analysis,0,[0]
"If each potential function has K terms, the multiplication would result in K2 terms which is computationally expensive.",4.4. Theoretical Analysis,0,[0]
We only partially exploit the results of Propositions 1 and 2 in our algorithm.,4.4. Theoretical Analysis,0,[0]
"When multiplying two potential functions, we draw K pairs of rank-1 tensors from their respective distributions and multiply them as described earlier but re-weight the resulting mixture using either Proposition 1 or 2.",4.4. Theoretical Analysis,0,[0]
"We call the former max-norm reweighting, and the latter min-variance reweighting.",4.4. Theoretical Analysis,0,[0]
"We present experiments on grid-structured Ising models, random graphs with pairwise Ising potentials, and two real-world datasets from the UAI 2014 Inference Competition (Gogate, 2014).",5. Experiments,0,[0]
"We test our algorithm against commonly used approximate inference methods, namely loopy belief propagation (labelled BP), mean-field (MF), treereweighted BP (TRW) and Gibbs sampling using existing implementations from the libDAI package (Mooij, 2010).",5. Experiments,0,[0]
"TBP was implemented in C++ inside the libDAI framework using Eigen (Guennebaud et al., 2010) and all tests were executed on a single core of a 1.4 GHz Intel Core i5 processor.",5. Experiments,0,[0]
"In each experiment, we time the execution of TBP and allow Gibbs sampling the same wall-clock time.",5. Experiments,0,[0]
"We run the other algorithms until convergence, which occurs quickly in each case (hence only the final performance is shown).",5. Experiments,0,[0]
"Parameters used for BP, MF, TRW and Gibbs are given in the supplementary material.",5. Experiments,0,[0]
"To build the junction tree, we use the min-fill heuristic2 implemented in libDAI.",5. Experiments,0,[0]
Figure 1 gives results for 10×10 Ising models.,5.1. Grid-structured Ising Models,0,[0]
TheN×N,5.1. Grid-structured Ising Models,0,[0]
"Ising model is a planar grid-structured MRF described by the joint distribution
p(x1, . . .",5.1. Grid-structured Ising Models,0,[0]
", xN2) = 1
Z exp ∑ (i,j) wijxixj + ∑ i bixi  where (i, j) runs over all edges in the grid.",5.1. Grid-structured Ising Models,0,[0]
Each variable Xi takes value either 1 or −1.,5.1. Grid-structured Ising Models,0,[0]
"In our experiments, we choose the wij uniformly from [−2, 2] (mixed interactions) or [0, 2] (attractive interactions), and the b uniformly from [−1, 1].",5.1. Grid-structured Ising Models,0,[0]
We use a symmetric rank-2 tensor decomposition for the pairwise potentials.,5.1. Grid-structured Ising Models,0,[0]
"We measure performance
2Min-fill repeatedly eliminates variables that result in the lowest number of additional edges in the graph (Koller & Friedman, 2009).
",5.1. Grid-structured Ising Models,0,[0]
"by the mean L1 error of marginal estimates over the N2 variables,
1
N2 N2∑ i=1 |Pexact(Xi",5.1. Grid-structured Ising Models,0,[0]
"= 1)− Papprox(Xi = 1)|.
",5.1. Grid-structured Ising Models,0,[0]
Exact marginals were obtained using the exact junction tree algorithm (possible because the grid size is small).,5.1. Grid-structured Ising Models,0,[0]
Results are all averages over 100 model instances generated randomly with the specified parameters; error bars show standard error.,5.1. Grid-structured Ising Models,0,[0]
"We give a description of the tensor decomposition and additional results for a range of grid sizes N and interaction strengths wij in the supplementary material.
",5.1. Grid-structured Ising Models,0,[0]
"As we increase the number of samples used for multiplication K, and hence running time, the performance of TBP improves as expected.",5.1. Grid-structured Ising Models,0,[0]
"On both attractive and mixed interaction models, loopy BP, mean-field and tree-reweighted BP perform poorly.",5.1. Grid-structured Ising Models,0,[0]
Gibbs sampling performs poorly on models with attractive interactions but performs better on models with mixed interactions.,5.1. Grid-structured Ising Models,0,[0]
This is expected since models with mixed interactions mix faster.,5.1. Grid-structured Ising Models,0,[0]
"Reweighting gives a noticeable improvement over not using reweighting; both max-norm reweighting and min-variance reweighting give very similar results (in Figure 1, they not easily differentiated).",5.1. Grid-structured Ising Models,0,[0]
"For the remainder of our experiments, we show results for max-norm reweighting only.",5.1. Grid-structured Ising Models,0,[0]
We also test TBP on random binary N -node MRFs with pairwise Ising potentials.,5.2. Random Pairwise MRFs,0,[0]
"We construct the graphs by independently adding each edge (i, j) with probability 0.5, with the requirement that the resulting graph is connected.",5.2. Random Pairwise MRFs,0,[0]
"Pairwise potentials are of the same form as Ising models (i.e. exp[wijxixj ]), where interaction strengths wij are chosen uniformly from [0, 2] (attractive) or [−2, 2] (mixed) and the
bi are chosen uniformly from [−1, 1].
",5.2. Random Pairwise MRFs,0,[0]
Figure 2a shows the performance of TBP with increasing sample size K on 15-node models.,5.2. Random Pairwise MRFs,0,[0]
"Similar to gridstructured Ising models, TBP performs well as the sample size is increased.",5.2. Random Pairwise MRFs,0,[0]
"Notably, TBP performs very well on attractive models where other methods perform poorly.
",5.2. Random Pairwise MRFs,0,[0]
Figure 2b shows the performance of the algorithms as the graph size is increased to 30 nodes.,5.2. Random Pairwise MRFs,0,[0]
"Interestingly, TBP starts to fail for mixed interaction models as the graph size is increased but remains very effective for attractive interaction models while other methods perform poorly.",5.2. Random Pairwise MRFs,0,[0]
"Finally, we show results of TBP on two real-world datasets from the UAI 2014 Inference Competition, namely the Promedus and Linkage datasets.",5.3. UAI Competition Problems,0,[0]
We chose these two problems following Zhu & Ermon (2015).,5.3. UAI Competition Problems,0,[0]
"To compute the initial tensor rank decompositions, we use the non-negative cp nmu method from Bader et al. (2015), an iterative optimisation method based on (Lee & Seung, 2001).",5.3. UAI Competition Problems,0,[0]
The initial potential functions are decomposed into mixtures with r components.,5.3. UAI Competition Problems,0,[0]
We show results for r = 2 and r = 4.,5.3. UAI Competition Problems,0,[0]
"We measure performance by the mean L1 error over all states and all variables
1
NSi N∑ i=1",5.3. UAI Competition Problems,0,[0]
∑ xi |Pexact(Xi,5.3. UAI Competition Problems,0,[0]
= xi)− Papprox(Xi,5.3. UAI Competition Problems,0,[0]
"= xi)|,
where Si is the number of states of variableXi.",5.3. UAI Competition Problems,0,[0]
"Results are averages over all problem instances in (Gogate, 2014).
",5.3. UAI Competition Problems,0,[0]
We see that TBP outperforms Gibbs sampling on both problems3.,5.3. UAI Competition Problems,0,[0]
"On the Linkage dataset, using r = 2 mix-
3We omit results for BP, MF and TRW for these problems because the libDAI package was unable to run them.
ture components for the initial decompositions performs best and increasing r does not improve performance.",5.3. UAI Competition Problems,0,[0]
"On the Promedus dataset r = 4 performs better; in this case, increasing r may improve the accuracy of the decomposition of the initial potential functions.",5.3. UAI Competition Problems,0,[0]
"For reference, the marginal error achieved by the random projection method of Zhu & Ermon (2015) is 0.09 ± 0.02 for Linkage and 0.21 ± 0.06 for Promedus.",5.3. UAI Competition Problems,0,[0]
TBP with K = 105 achieved error of 0.08±0.01 for Linkage and 0.12±0.01 for Promedus despite using substantially less running time 4.,5.3. UAI Competition Problems,0,[0]
"We proposed a new sampling-based approximate inference algorithm, tensor belief propagation, which uses a mixtureof-rank-1-tensors representation to approximate cluster potentials and messages in the junction tree algorithm.",6. Conclusion and Future Work,0,[0]
"It
4TBP with K = 105 takes an average across all problem instances of approximately 10 minutes (Linkage) and 3 minutes (Promedus) per problem on our machine, and does not differ substantially for r = 2 vs r = 4.",6. Conclusion and Future Work,0,[0]
"The results described by Zhu & Ermon (2015) were comparable in running time to 10 million iterations of Gibbs sampling, which we estimate would take several hours on our machine without parallelism.
",6. Conclusion and Future Work,0,[0]
"gives consistent estimators, and performs well on a range of problems against well-known algorithms.
",6. Conclusion and Future Work,0,[0]
"In this paper, we have not addressed how to perform the initial tensor decomposition.",6. Conclusion and Future Work,0,[0]
"There exist well-known optimisation algorithms for this problem to minimize Euclidean error (Kolda & Bader, 2009), though it is not clear that this is the best objective for the purpose of TBP.",6. Conclusion and Future Work,0,[0]
We are currently investigating the best way of formulating and solving this optimisation problem to yield accurate results during inference.,6. Conclusion and Future Work,0,[0]
"Further, when constructing the junction tree, it is not clear whether min-fill and other well-known heuristics remain appropriate for TBP.",6. Conclusion and Future Work,0,[0]
"In particular, these cost functions aim to minimise the induced width of the junction tree which is no longer directly relevant to the performance of the algorithm.",6. Conclusion and Future Work,0,[0]
"Further work may investigate other heuristics, for example with the goal of minimising cluster variance.
",6. Conclusion and Future Work,0,[0]
"Finally, approximate inference algorithms have applications in learning (for example, in the gradient computations when training restricted Boltzmann machines) and thus TBP may improve learning as well as inference in some applications.",6. Conclusion and Future Work,0,[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgements,0,[0]
This work is supported by NUS AcRF Tier 1 grant R-252-000-639-114 and a QUT Vice-Chancellor’s Research Fellowship Grant.,Acknowledgements,0,[0]
"We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm.",abstractText,0,[0]
Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors.,abstractText,0,[0]
"Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling.",abstractText,0,[0]
"Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable.",abstractText,0,[0]
We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.,abstractText,0,[0]
Tensor Belief Propagation,title,0,[0]
"Tensors have long been successfully used in several disciplines, including neuroscience, phylogenetics, statistics, signal processing, computer vision, and data mining.",1. Introduction,0,[0]
"They are used to model multi-relational or multi-modal data, and their decompositions often reveal some underlying structures behind the observed data.",1. Introduction,0,[0]
"See (Kolda & Bader, 2009) for a survey of such results.",1. Introduction,0,[0]
"Recently, they have found applications in machine learning, particularly for learning various latent variable models (Anandkumar et al., 2012; Chaganty & Liang, 2014; Anandkumar et al., 2014a).
",1. Introduction,0,[0]
"One popular decomposition method in such applications is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components.",1. Introduction,0,[0]
"This is similar to the singular value decomposition (SVD) of matrices, and a popular approach for SVD is the power method, which is well-understood and has nice theoretical guarantee.",1. Introduction,0,[0]
"As tensors can be seen as generalization of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could inherit the success from the matrix case.",1. Introduction,0,[0]
"However, the situation turns out to be much more complicated for tensors (see e.g. the discussion in (Anandkumar et al., 2014a)),
1Academia Sinica, Taiwan.",1. Introduction,0,[0]
"Correspondence to: Po-An Wang <poanwang@iis.sinica.edu.tw>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
"and in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013).",1. Introduction,0,[0]
"Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable again.",1. Introduction,0,[0]
"In particular, for tensors having orthogonal decomposition, Anandkumar et al. (2014a) provided an efficient algorithm based on the tensor power method with theoretical guarantee.",1. Introduction,0,[0]
"Still, as we will discuss later in Section 2, the seemingly subtle change of going from matrices to tensors makes some significant differences for the power method.
",1. Introduction,0,[0]
"The first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely converge to the top singular vector, we have much less control of where the convergence goes in the tensor case.",1. Introduction,0,[0]
"Consequently, most previous works based on the tensor power method with theoretical guarantee, such as (Anandkumar et al., 2014a;b; Wang & Anandkumar, 2016), require much more complicated procedures.",1. Introduction,0,[0]
"In particular, they can only find the top k eigenvectors one by one, each time with the power method applied to a modified tensor, deflated from the original tensor according to the previously found vectors.",1. Introduction,0,[0]
"Moreover, to find each vector, they need to sample several initial vectors and apply the power method on all of them, before selecting just one from them.",1. Introduction,0,[0]
"In contrast, algorithms for matrices such as (Mitliagkas et al., 2013; Hardt & Price, 2014) are much simpler, as they can find the k vectors simultaneously by applying the power method only on k random initial vectors.",1. Introduction,0,[0]
"The second difference, on the other hand, has a beneficial effect, which allows the tensor power method to converge exponentially faster than the matrix one when starting from good initial vectors.",1. Introduction,0,[0]
Then a natural question is: can we inherit the best of both worlds?,1. Introduction,0,[0]
"Namely, is it possible to have a simple algorithm which can find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?
",1. Introduction,0,[0]
Our Results.,1. Introduction,0,[0]
"As in previous works, we consider the slightly harder scenario in which we only have access to a noisy version of the tensor we want to decompose.",1. Introduction,0,[0]
"This arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from some empirical average of the observed data.",1. Introduction,0,[0]
"Our main contribution is to answer the above question affirmatively.
",1. Introduction,0,[0]
"First, we consider the batch setting in which we assume
that the given noisy tensor is stored somewhere and can be accessed whenever we want to.",1. Introduction,0,[0]
"In this setting, we identify a sufficient condition such that if we have k initial vectors satisfying this condition, then we can apply the tensor power method on them simultaneously, which will come within some distance ε to the eigenvectors in O(log log 1ε ) iterations, with parameters related to eigenvalues considered as constant.",1. Introduction,0,[0]
"To apply such a result, we need an efficient way to find such initial vectors.",1. Introduction,0,[0]
"We show how to do this by choosing a good direction to project the tensor down to a matrix while preserving the eigengaps, and then applying the matrix power method for only a few iterations just to obtain vectors meeting that sufficient condition.",1. Introduction,0,[0]
"The number of iterations needed here is only O(log d), independent of ε, where d is the dimension of the eigenvectors.
",1. Introduction,0,[0]
The result stated above is for orthogonal tensors.,1. Introduction,0,[0]
"On the other hand, it is known that an nonorthogonal tensor with linearly independent eigenvectors can be converted into an orthogonal one with the help of some whitening matrix.",1. Introduction,0,[0]
"However, previous works usually pay little attention on how to find such a whitening matrix efficiently.",1. Introduction,0,[0]
"According to (Anandkumar et al., 2014a), one way is via SVD on some second moment matrix, but doing this using the matrix power method would take longer to converge compared to the tensor power method which would then be applied on the whitened tensor.",1. Introduction,0,[0]
"Our second contribution is to provide an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.
",1. Introduction,0,[0]
"While most previous works on tensor decomposition focus on the batch setting, storing even a tensor of order three requires Ω(d3) space, which is infeasible for a large d. We show to avoid this in the streaming setting, with a stream of data arriving one at a time, which is the only source of information about the tensor.",1. Introduction,0,[0]
"We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension d. To achieve an approximation error ε, the total number of samples we need is O(kd log d+",1. Introduction,0,[0]
"1ε2 log(d log 1 ε )).
",1. Introduction,0,[0]
"Related Works There is a huge literature on tensor decomposition, and it is beyond the scope of this paper to give a comprehensive survey.",1. Introduction,0,[0]
"Thus, we only compare our results to the most related ones, particularly those based on the power method.",1. Introduction,0,[0]
"While different works may focus on different aspects, we are most interested in understanding how the error parameter ε affects various performance measures, having in mind a small ε.
",1. Introduction,0,[0]
"First, the batch algorithm of Anandkumar et al. (2014a), using a better analysis in (Wang & Anandkumar, 2016), runs in time about O((k2 log k)(log log 1ε )), which can be made to run in O(k(log log 1ε ))",1. Introduction,0,[0]
"iterations in parallel, while ours are O(k log log 1ε ) and O(log log 1 ε ), respectively.",1. Introduction,0,[0]
"On
the other hand, one advantage of their algorithm is that its running time does not depend on the eigengaps, while ours has the dependence hidden above as some constant.
",1. Introduction,0,[0]
"In the streaming setting, Wang & Anandkumar (2016) provided an algorithm using O(dk log k) memory and O( kε2 log( d ε )) samples, while ours only uses O(dk) memory and O( 1ε2 log(d log log 1 ε ))",1. Introduction,0,[0]
"samples.
",1. Introduction,0,[0]
1,1. Introduction,0,[0]
"Nevertheless, the sample complexity of Wang & Anandkumar (2016) is also independent of the eigengaps, while ours has the dependence hidden above as a constant factor.
",1. Introduction,0,[0]
"As one can see, our algorithms, which find the k eigenvectors simultaneously, allow us to save a factor of k in the time complexity and the sample complexity, although our bounds may become worse when the eigengaps are small.",1. Introduction,0,[0]
"Thus, our algorithms can be seen as new options for users to choose from, depending on the data they are given.
",1. Introduction,0,[0]
"Although not directed related, let us also compare to previous works on SVD.",1. Introduction,0,[0]
"Two related ones, both based on the simultaneous matrix power method, are the batch algorithm of (Hardt & Price, 2014) which converges in O(log dε ) iterations, and the streaming algorithm of (Li et al., 2016) which requires O( 1ε2 log(d log 1 ε ))",1. Introduction,0,[0]
samples.,1. Introduction,0,[0]
Both bounds are worse than ours and also depend on the eigengaps.,1. Introduction,0,[0]
"Thus, although one approach for orthogonal tensor decomposition is to reduce it to a matrix SVD problem, this does not appear to result in better performance than ours.
",1. Introduction,0,[0]
"Finally, comparisons of the tensor power method with other approaches can be found in works such as (Anandkumar et al., 2014a; Wang & Anandkumar, 2016).",1. Introduction,0,[0]
"For example, the online SGD approach of (Ge et al., 2015) works only for tensors of even orders and its sample complexity has a poor dependency on the dimension d.
Organization of the paper.",1. Introduction,0,[0]
"First, we provide some preliminaries in Section 2.",1. Introduction,0,[0]
"Then we present our batch algorithm for orthogonal and symmetric tensors of order three in Section 3, and then for general orthogonal tensors in Section 4.",1. Introduction,0,[0]
"In Section 5, we introduce our whitening procedure for nonorthogonal but symmetry tensors.",1. Introduction,0,[0]
"Finally, in Section 6, we present our algorithm for the streaming setting.",1. Introduction,0,[0]
"Due to the space limitation, we will move all our proofs to the appendix in the supplementary material.",1. Introduction,0,[0]
Let us first introduce some notations and definitions which we will use later.,2. Preliminaries,0,[0]
Let R denote the set of real numbers and N the set of positive integers.,2. Preliminaries,0,[0]
"Let N (0, 1) denote the standard normal distribution with mean 0 and variance 1,
1We use a different input distribution from theirs.",2. Preliminaries,0,[0]
"The bound listed here is modified from theirs according to our distribution.
and let N d(0, 1), for d ∈ N, denote the d-variate one which has each of its d dimensions sampled independently from N (0, 1).",2. Preliminaries,0,[0]
"For d ∈ N, let [d] denote the set {1, . . .",2. Preliminaries,0,[0]
", d}.",2. Preliminaries,0,[0]
"For a vector x, let ‖x‖ denote its L2 norm.",2. Preliminaries,0,[0]
"For d ∈ N, let Id denote the d × d identity matrix.",2. Preliminaries,0,[0]
"For a matrix A ∈ Rd×k, let Ai, for i ∈",2. Preliminaries,0,[0]
"[k], denote its i-th column, and let Ai,j , for j ∈",2. Preliminaries,0,[0]
"[d], be the j-th entry of Ai.",2. Preliminaries,0,[0]
"Moreover, for a matrix A, let A> denote its transpose, and define its norm as ‖A‖ = maxx∈Rk ‖A·x‖ ‖x‖ , using the convention that 0 0 = 0.
",2. Preliminaries,0,[0]
"Tensors are the focus of our paper, which can be seen as generalization of matrices to higher orders.",2. Preliminaries,0,[0]
"For simplicity of presentation, we will use symmetric tensors of order three as examples in the following definitions.",2. Preliminaries,0,[0]
"A real tensor T of order three can be seen as an three-dimensional array in Rd×d×d, for some d ∈ N, with its (i, j, k)-th entry denoted as Ti,j,k.",2. Preliminaries,0,[0]
"For such a tensor T and three matrices A ∈ Rd×m1 , B ∈ Rd×m2 , C ∈ Rd×m3 , let T (A,B,C) be the tensor in Rm1×m2×m3 , with its (a, b, c)th entry defined as ∑ i,j,k∈[d]",2. Preliminaries,0,[0]
"Ti,j,kAa,iBb,jCc,k.",2. Preliminaries,0,[0]
"The norm of a tensor T we will use is the operator norm: ‖T‖ = maxx,y,z∈Rd |T (x,y,z)| ‖x‖‖y‖‖z‖ .
",2. Preliminaries,0,[0]
The tensor decomposition problem.,2. Preliminaries,0,[0]
"In this problem, there is a tensor T with some unknown decomposition
T = ∑ i∈[d] λi · ui ⊗ ui ⊗ ui,
with λi ≥ 0 and ui ∈ Rd for any",2. Preliminaries,0,[0]
i ∈,2. Preliminaries,0,[0]
[d].,2. Preliminaries,0,[0]
Then given some k ∈,2. Preliminaries,0,[0]
"[d] and ε ∈ (0, 1), our goal is to find λ̂i and ûi with
|λ̂i − λi| ≤ ε and ‖ûi",2. Preliminaries,0,[0]
"− ui‖ ≤ ε, for every i ∈",2. Preliminaries,0,[0]
"[k].
We will assume that∑ i∈[d]",2. Preliminaries,0,[0]
λi ≤ 1 and ∀i ∈,2. Preliminaries,0,[0]
[k] : λi > λi+1.,2. Preliminaries,0,[0]
"(1)
As in previous works, we consider a slightly harder version of the problem, in which we only have access to some noisy version of T , instead of the noiseless T .",2. Preliminaries,0,[0]
We will consider the following two settings.,2. Preliminaries,0,[0]
"In the batch setting, we have access to some T̄ = T + Φ for the whole time, for some perturbation tensor Φ. In the streaming setting, we have a stream of data points",2. Preliminaries,0,[0]
"x1, x2, . . .",2. Preliminaries,0,[0]
"arriving one by one, which provide the only information we have about T , with each xτ ∈ Rd allowing us to compute some T̄τ with mean E[T̄ ] = T .",2. Preliminaries,0,[0]
"In this streaming setting, we are particularly interested in the case of a large d which prohibits one to store a tensor of size d3 in memory.
",2. Preliminaries,0,[0]
Power Method: Matrices versus Tensors.,2. Preliminaries,0,[0]
"Note that a tensor of order two is just a matrix, and a popular approach for decomposing matrices is the so-called power
method, which works as follows.",2. Preliminaries,0,[0]
"Suppose we are given a d × d matrix M = ∑ i∈[d] λi · ui ⊗ ui, with nonnegative λ1 > λ2 ≥ · · · and orthonormal vectors u1, . . .",2. Preliminaries,0,[0]
", ud.",2. Preliminaries,0,[0]
"The power method starts with some q(0) = ∑ i∈[d] ci · ui, usually chosen randomly, and then repeatedly performs the update q(t) = M · q(t−1), which results in
q(t) = ∑ i∈[d]",2. Preliminaries,0,[0]
λi ( u>i q,2. Preliminaries,0,[0]
(t−1) ) · ui = ∑ i∈[d],2. Preliminaries,0,[0]
"( λtici ) · ui.
Note that for any i 6= 1, as λi < λ1, the coefficient λtici will soon become much smaller than the coefficient λt1c1 if c1 is not too small, which is likely to happen for a randomly chosen q(0).",2. Preliminaries,0,[0]
"This has the effect that after normalization, q(t)/‖q(t)‖ approaches u1 quickly.
",2. Preliminaries,0,[0]
"Now consider a tensor T = ∑
i∈[d] λi · ui ⊗ ui ⊗ ui, with nonnegative λ1 > λ2 ≥ · · · and orthonormal vectors u1, . . .",2. Preliminaries,0,[0]
", ud.",2. Preliminaries,0,[0]
"The tensor version of the power method again starts from a randomly chosen q(0) =∑
i∈[d] ci · ui, but now repeatedly performs the update q(t)",2. Preliminaries,0,[0]
"= T (Id, q (t−1), q(t−1)), which in turn results in
q(t) = ∑ i∈[d] λi ( u>i q",2. Preliminaries,0,[0]
(,2. Preliminaries,0,[0]
t−1) ),2. Preliminaries,0,[0]
2 · ui = ∑ i∈[d],2. Preliminaries,0,[0]
"λ−1i (λici) 2t · ui.
",2. Preliminaries,0,[0]
"The coefficient of each ui now has a different form from the matrix case, and this leads to the following two effects.
",2. Preliminaries,0,[0]
"First, one now has much less control on what q(t)/‖q(t)‖ converges to.",2. Preliminaries,0,[0]
"In fact, it can converge to any ui 6= u1 if ui has the largest value of λi|ci|, which happens with a good probability if λi is not much smaller than λ1.",2. Preliminaries,0,[0]
"Consequently, to find the top k vectors u1, . .",2. Preliminaries,0,[0]
.,2. Preliminaries,0,[0]
", uk, previous works based on the power method all need much more complicated procedures (Anandkumar et al., 2014a), compared to those for matrices, as discussed in the introduction.
",2. Preliminaries,0,[0]
"On the other hand, the different form of q(t) has the beneficial effect that the convergence is now exponentially faster than in the matrix case.",2. Preliminaries,0,[0]
"More precisely, if λi|ci| <",2. Preliminaries,0,[0]
"λj |cj |, than the gap between the coefficients (λici)2 t and (λjcj)2 t is now amplified much faster.",2. Preliminaries,0,[0]
We will show how to inherit this nice property of faster convergence but at the same time avoid the difficulty discussed above.,2. Preliminaries,0,[0]
"In this section, we focus on the special case in which the tensors to be decomposed are orthogonal, symmetric, and of order three.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Formally, there is an underlying tensor
T = ∑ i∈[d] λi · ui ⊗ ui ⊗ ui,
Algorithm 1 Robust tensor power method Input: Tensor T̄ ∈",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Rd×d×d and parameters k, L, S,N .",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Initialization Phase: Sample w1, . . .",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
", wL, Y (0) 1 , . . .",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
", Y (0) k ∼ N d(0, 1).
",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Compute w̄ =,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"1L ∑
j∈[L] T̄ (Id, wj , wj).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Compute M̄ = T̄,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"(Id, Id, w̄).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Factorize Y (0) as Z(0) ·R(0) by QR decomposition.,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"for s = 1 to S do
Compute Y (s) = M̄ · Z(s−1).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Factorize Y (s) as Z(s) ·R(s) by QR decomposition.
end for Tensor Power Phase: Let Q(0) = Z(S).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"for t = 1 to N do
Compute Y (t)j = T̄",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"(Id, Q (t−1) j , Q (t−1) j ), ∀j ∈",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"[k].
",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Factorize Y (t) as Q(t) ·R(t) by QR decomposition.,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"end for Output: ûj = Q (N) j and λ̂j = T̄ (ûj , ûj , ûj), ∀j ∈",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"[k].
with orthonormal vectors ui’s and real λi’s satisfying the condition (1).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Then given k ∈,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"[d] and ε ∈ (0, 1), our goal is to find approximates to those λi and ui within distance ε, but we only have access to some noisy tensor T̄ = T",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
+,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Φ for some symmetric perturbation tensor Φ.
",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Our algorithm is given in Algorithm 1, which consists of two phases: the initialization phase and the tensor power phase.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"The main phase is the tensor power phase, which we will discuss in detail in Subsection 3.1.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"For our tensor power phase to work, it needs to have a good starting point.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"This is provided by the initialization phase, which we will discuss in detail in Subsection 3.2.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Through these two subsections, we will prove Theorem 1 below, which summarizes the performance of our algorithm, according to the following parameters of the tensor:
γ = min i∈[k] λ2i",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"− λ2i+1 λ2i and ∆ = min i∈[k] λi − λi+1 4 .
",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Theorem 1.,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
Suppose ε ≤,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
λk2,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
and the perturbation tensor has the bound ‖Φ‖ ≤,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"min{ ∆ε
2 √ k , ∆3d ,
α0∆ 2
√ dk } for a small enough constant α0.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Then for some L = O( 1γ2 log d), S =",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"O( 1γ log d), and N = O(log( 1 γ log 1 ε )), our Algorithm 1 with high probability will output ûi and λ̂i with ‖ûi−ui‖ ≤ ε",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
and |λ̂i − λi| ≤ ε for every,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
i ∈,3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"[k].
Let us make some remarks about the theorem.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"First, the L samples are used to compute w̄ and M̄ , which can be done in a parallel way.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Second, our parameter γ is related to a parameter γ′ = mini∈[k",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
],3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"λi−λi+1 λi
used in (Hardt & Price, 2014), and it is easy to verify that γ ≥ γ′. Thus, our algorithm for tensors converges in O( 1γ log d + log( 1 γ log 1 ε )) rounds, which is faster than the O( 1γ′ log d + 1 γ′ log 1 ε )
rounds of (Hardt & Price, 2014) for matrices.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Note that our dependence on the error parameter ε is exponentially smaller than that of (Hardt & Price, 2014), which means that for a small ε, we can decompose tensors much faster than matrices.",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"Finally, compared to previous works on tensors, our convergence time, for a small ε is about O(log log 1ε ) while those in (Anandkumar et al., 2014a; Wang & Anandkumar, 2016) are at least Ω(k log log 1ε ).",3. Orthogonal and Symmetric Tensors of Order Three,0,[0]
"The tensor power phase of our Algorithm 1 is based on our version of the tensor power method, which works as follows.",3.1. Our Robust Tensor Power Method,0,[0]
"At each step t, we maintain a d × k matrix Q(t) with columns Q(t)1 , . . .",3.1. Our Robust Tensor Power Method,0,[0]
", Q (t) k",3.1. Our Robust Tensor Power Method,0,[0]
"as our current estimators for u1, . .",3.1. Our Robust Tensor Power Method,0,[0]
.,3.1. Our Robust Tensor Power Method,0,[0]
", uk, which is obtained by updating the previous estimators with the following two operations.
",3.1. Our Robust Tensor Power Method,0,[0]
"The main operation is to apply the noisy tensor T̄ on them simultaneously to get a d× k matrix Y (t) with its j-th column computed as Y (t)j = T̄ (Id, Q (t−1) j , Q (t−1) j ), which equals ∑ i∈[d]",3.1. Our Robust Tensor Power Method,0,[0]
"λi ( u>i Q (t−1) j )2 · ui + Φ̂(t)j ,
for Φ̂(t)j = Φ(Id, Q (t−1) j , Q (t−1) j ).",3.1. Our Robust Tensor Power Method,0,[0]
"This implies that
∀i ∈",3.1. Our Robust Tensor Power Method,0,[0]
[d] : u>i Y (t) j = λi,3.1. Our Robust Tensor Power Method,0,[0]
"( u>i Q (t−1) j )2 + u>i Φ̂ (t) j , (2)
which shows the progress made by this operation.
",3.1. Our Robust Tensor Power Method,0,[0]
"The second operation is to orthogonalize Y (t) as
Y (t) = Q(t) ·R(t),
by the QR decomposition via the Gram-Schmidt process, to obtain a d× k matrix Q(t) with columns Q(t)1 , . . .",3.1. Our Robust Tensor Power Method,0,[0]
", Q (t) k , which then become our new estimators.",3.1. Our Robust Tensor Power Method,0,[0]
"As we will show in Lemma 1 below, given a small enough ‖Φ‖, if we start with a full-rank Q(0), then each Q(t) also has full rank and consists of orthonormal columns, and each R(t) is invertible.",3.1. Our Robust Tensor Power Method,0,[0]
"Moreover, although we apply the QR decomposition on the whole matrix Y (t) to obtain the matrix Q(t), it has the effect that for any m ∈",3.1. Our Robust Tensor Power Method,0,[0]
"[k], the first m columns of Q(t) can be seen as obtained from the first m columns of Y (t) by a QR decomposition.",3.1. Our Robust Tensor Power Method,0,[0]
This property is needed in our Lemma 1 and Theorem 2 below to guarantee the simultaneous convergence of Q(t)i to ui for every,3.1. Our Robust Tensor Power Method,0,[0]
i ∈,3.1. Our Robust Tensor Power Method,0,[0]
"[k].
Before stating Lemma 1 which guarantees the progress we make at each step, let us prepare some notations first.",3.1. Our Robust Tensor Power Method,0,[0]
For a d × k matrix A and some m ∈,3.1. Our Robust Tensor Power Method,0,[0]
"[k], let A[m] denote the d ×m matrix containing the first m columns of A.",3.1. Our Robust Tensor Power Method,0,[0]
Let U denote the d× k matrix with the target vector ui as its i’th column.,3.1. Our Robust Tensor Power Method,0,[0]
For a d× k matrix Q and some m ∈,3.1. Our Robust Tensor Power Method,0,[0]
"[k], define
cosm(Q) =",3.1. Our Robust Tensor Power Method,0,[0]
"min y∈Rm
∥∥∥U>[m] ·Q[m] · y∥∥∥ /‖Q[m] ·",3.1. Our Robust Tensor Power Method,0,[0]
"y‖,
which equals the cosine of the m’th principal angle between the column spaces of U[m] and Q[m], let sinm(Q) =√
1− cos2m(Q), and let us use as the error measure
tanm(Q) = sinm(Q)/ cosm(Q).
",3.1. Our Robust Tensor Power Method,0,[0]
"More information about the principal angles can be found in, e.g., (Golub & Van Loan, 1996).",3.1. Our Robust Tensor Power Method,0,[0]
"Then we have the following lemma, which we prove in Appendix B.1.
Lemma 1.",3.1. Our Robust Tensor Power Method,0,[0]
Fix any m ∈,3.1. Our Robust Tensor Power Method,0,[0]
[k] and t ≥ 0.,3.1. Our Robust Tensor Power Method,0,[0]
"Let Φ̂(t)[m] denote the d×m matrix with Φ̂(t)j = Φ(Id, Q (t−1) j , Q (t−1) j ) as its j’th column, and suppose∥∥∥Φ̂(t)[m]∥∥∥ < ∆ ·min{β, cos2m(Q(t−1))} , (3) for some β > 0.",3.1. Our Robust Tensor Power Method,0,[0]
"Then for ρ = maxi∈[k](
λi+1 λi ) 1 4 , we have
tanm(Q (t))",3.1. Our Robust Tensor Power Method,0,[0]
"≤ max { β,max{β, ρ} · tan2m(Q(t−1)) } .
",3.1. Our Robust Tensor Power Method,0,[0]
"Observe that the guarantee provided by the lemma above has a similar form as that in (Hardt & Price, 2014) for matrices.",3.1. Our Robust Tensor Power Method,0,[0]
"The main difference is that here in the tensor case, we have the error measure essentially squared after each step, which has the following two implications.",3.1. Our Robust Tensor Power Method,0,[0]
"First, to guarantee that the error is indeed reduced, we need tanm(Q
(t−1)) to be small enough (say, less than one), unlike in the matrix case.",3.1. Our Robust Tensor Power Method,0,[0]
"Next, if we indeed have a small enough tanm(Q(t−1)), then the error can be reduced in a much faster rate than in the matrix case.",3.1. Our Robust Tensor Power Method,0,[0]
"Another difference is that here we provide the guarantee for all the k submatrices Q(t)[m], for m ∈",3.1. Our Robust Tensor Power Method,0,[0]
"[k], instead of just one matrix Q
(t).",3.1. Our Robust Tensor Power Method,0,[0]
This allows us to show the simultaneous convergence of each column Q(t)i to the target vector ui for every i ∈,3.1. Our Robust Tensor Power Method,0,[0]
"[k], as given in the following, which we prove in Appendix B.2.
",3.1. Our Robust Tensor Power Method,0,[0]
Theorem 2.,3.1. Our Robust Tensor Power Method,0,[0]
"For any ε ∈ (0, λk2 ), there exists some N ≤",3.1. Our Robust Tensor Power Method,0,[0]
O(log( 1γ log 1 ε )),3.1. Our Robust Tensor Power Method,0,[0]
such that the following holds.,3.1. Our Robust Tensor Power Method,0,[0]
Suppose the perturbation is bounded by ‖Φ‖ ≤,3.1. Our Robust Tensor Power Method,0,[0]
∆ε 2 √ k,3.1. Our Robust Tensor Power Method,0,[0]
and we start from some initial Q(0) with tanm Q(0) ≤ 1 for every m ∈,3.1. Our Robust Tensor Power Method,0,[0]
[k].,3.1. Our Robust Tensor Power Method,0,[0]
"Then for any t ≥ N , with ûi = Q(t)i and λ̂i = T̄ (ûi, ûi, ûi), we have ‖ui − ûi‖ ≤ ε",3.1. Our Robust Tensor Power Method,0,[0]
"and |λi − λ̂i| ≤ ε, for every i ∈",3.1. Our Robust Tensor Power Method,0,[0]
"[k].
Note that the convergence rate guaranteed by the theorem above is exponentially faster than that in (Hardt & Price, 2014) for matrices, assuming that we indeed can have such a good initial Q(0) to start with.",3.1. Our Robust Tensor Power Method,0,[0]
"In the next subsection, we show how it can be found efficiently.",3.1. Our Robust Tensor Power Method,0,[0]
"Our approach for finding a good initialization is to project the tensor down to a matrix and apply the matrix power
method for only a few steps just to make the tangents less than one.",3.2. Initialization Procedure,0,[0]
"Although we could continue applying the matrix power method till reaching the much smaller target bound ε, this would take exponentially longer than by switching to the tensor power method as we actually do.
",3.2. Initialization Procedure,0,[0]
"As mentioned above, we would first like to project the tensor T̄ down to a matrix.",3.2. Initialization Procedure,0,[0]
"A naive approach is to sample a random vector w̄ and take the matrix T̄ (Id, Id, w̄)",3.2. Initialization Procedure,0,[0]
"≈ T (Id, Id, w̄) = ∑ i∈[d]",3.2. Initialization Procedure,0,[0]
λi(u > i w̄) · ui ⊗ ui.,3.2. Initialization Procedure,0,[0]
"However, this may mess up the gaps between eigenvalues, which are needed to guarantee the convergence rate of the matrix power method.",3.2. Initialization Procedure,0,[0]
"The reason is that as each u>i w̄ has mean zero, the coefficient λi(u>i w̄) also has mean zero and thus has a good chance of coming very close to others.",3.2. Initialization Procedure,0,[0]
"To preserve the gaps, we would like to have u>i w̄ ≥ u>i+1w̄ for each i with high probability.",3.2. Initialization Procedure,0,[0]
"To achieve this, let us first imagine sampling a random w ∈ Rd from N d(0, 1), and computing the vector w̄ = T̄ (Id, w, w), which is close to
T (Id, w, w) = ∑ i∈[d]",3.2. Initialization Procedure,0,[0]
"λi(u > i w) 2 · ui.
Then one can show that for every i, E[(u>i w)2] = 1, so that E[w̄] ≈ E[T (Id, w, w)]",3.2. Initialization Procedure,0,[0]
"= ∑ i∈[d] λiui, and
u>i",3.2. Initialization Procedure,0,[0]
E[w̄] ≈ λi >,3.2. Initialization Procedure,0,[0]
λi+1,3.2. Initialization Procedure,0,[0]
"≈ u>i+1 E[w̄].
",3.2. Initialization Procedure,0,[0]
"However, we want the gap-preserving guarantee to be in high probability, instead in expectation.",3.2. Initialization Procedure,0,[0]
"Thus we go further by sampling not just one, but some number L of vectors w1, . . .",3.2. Initialization Procedure,0,[0]
", wL independently from the distribution N d(0, 1), and then taking the average
w̄",3.2. Initialization Procedure,0,[0]
"= 1
L ∑ j∈[L] T̄ (Id, wj , wj).",3.2. Initialization Procedure,0,[0]
"(4)
The following lemma shows that such a w̄ is likely to have u>i w̄",3.2. Initialization Procedure,0,[0]
"≈ λi, which we prove in Appendix B.3.",3.2. Initialization Procedure,0,[0]
Lemma 2.,3.2. Initialization Procedure,0,[0]
Suppose we have T̄ =,3.2. Initialization Procedure,0,[0]
T + Φ with ‖Φ‖ ≤ ∆3d .,3.2. Initialization Procedure,0,[0]
Then for some L ≤,3.2. Initialization Procedure,0,[0]
"O( 1γ2 log d), the vector w̄ computed according to (4) with high probability satisfies∣∣u>i w̄",3.2. Initialization Procedure,0,[0]
− λi∣∣ ≤ 14 (λiγ + 2∆) for every i ∈,3.2. Initialization Procedure,0,[0]
"[k]. (5) With this w̄, we compute the matrix M̄ = T̄",3.2. Initialization Procedure,0,[0]
"(Id, Id, w̄).",3.2. Initialization Procedure,0,[0]
"As shown by the following lemma, which we prove in Appendix B.4, M̄ is close to a matrix with ui’s as eigenvectors and good gaps between eigenvalues.
",3.2. Initialization Procedure,0,[0]
Lemma 3.,3.2. Initialization Procedure,0,[0]
Suppose we have T̄ =,3.2. Initialization Procedure,0,[0]
T + Φ.,3.2. Initialization Procedure,0,[0]
"Then for any w̄ satisfying the condition (5) in Lemma 2, the matrix M̄ = T̄",3.2. Initialization Procedure,0,[0]
"(Id, Id, w̄) can be decomposed as
M̄ = ∑ i∈[d]",3.2. Initialization Procedure,0,[0]
"λ̄i · ui ⊗ ui + Φ̄,
for some λ̄i’s with λ̄i",3.2. Initialization Procedure,0,[0]
"− λ̄i+1 ≥ ∆2, for i ∈",3.2. Initialization Procedure,0,[0]
"[k], and Φ̄ = Φ(Id, Id, w̄) with ‖Φ̄‖ ≤ 2‖Φ‖.
With such a matrix M̄ , we next apply the matrix power method of (Hardt & Price, 2014) to find good approximates to its eigenvectors.",3.2. Initialization Procedure,0,[0]
"More precisely, we sample an initial matrix Y (0) ∈ Rd×k by choosing each of its column independently according to the distribution N d(0, 1), and factorize it as Y (0) = Z(0) · R(0) by QR decomposition via the Gram-Schmidt process.",3.2. Initialization Procedure,0,[0]
"Then at step s ≥ 1, we multiply the previous estimate Z(s−1) by M̄ to obtain Y (s) = M̄ · Z(s−1), factorize it as Y (s) = Z(s) ·",3.2. Initialization Procedure,0,[0]
"R(s) by QR decomposition via the Gram-Schmidt process, and then take the orthonormal Z(s) as the new estimate.",3.2. Initialization Procedure,0,[0]
The following lemma shows the number of steps needed to find a good enough Z(s).,3.2. Initialization Procedure,0,[0]
Lemma 4.,3.2. Initialization Procedure,0,[0]
Suppose we are given a matrix M̄,3.2. Initialization Procedure,0,[0]
"having the decomposition described in Lemma 3, with ‖Φ̄‖ ≤ 2α0∆
2 √ dk
for a small enough constant α0.",3.2. Initialization Procedure,0,[0]
Then there exists some S ≤,3.2. Initialization Procedure,0,[0]
"O( 1γ log d) such that with high probability, we have tanm(Z (s))",3.2. Initialization Procedure,0,[0]
≤ 1 for every m ∈,3.2. Initialization Procedure,0,[0]
"[k] whenever s ≥ S.
This together with the previous two lemmas guarantee that given T̄ = T +Φ, with ‖Φ‖ ≤ min{ ∆3d , α0∆ 2",3.2. Initialization Procedure,0,[0]
"√ dk
}, for a small enough constant α0, we can obtain with high probability a good Z(S) which can be used as the initial Q(0) for our tensor power phase.",3.2. Initialization Procedure,0,[0]
"Combining this with Theorem 2 in the previous subsection, we then have our Theorem 1 given at the beginning of the section.",3.2. Initialization Procedure,0,[0]
"In the previous section, we consider tensors which are orthogonal, symmetric, and of order three.",4. General Orthogonal Tensors,0,[0]
"In this section, we show how to extend our results for general orthogonal tensors, to deal with higher orders first and then asymmetry.",4. General Orthogonal Tensors,0,[0]
"To handle orthogonal and symmetric tensors of any order, only the initialization procedure needs to be modified.",4.1. Higher-Order Tensors,0,[0]
"First, for tensors of any odd order, a straightforward modification is as follows.",4.1. Higher-Order Tensors,0,[0]
Take for example a tensor of order 2r + 1.,4.1. Higher-Order Tensors,0,[0]
"Now we simply compute
w̄",4.1. Higher-Order Tensors,0,[0]
"= 1
L ∑ j∈[L] T̄ (Id, wj , . . .",4.1. Higher-Order Tensors,0,[0]
", wj),
with 2r copies of wj , and similarly to Lemma 2, one can show that w̄ is likely to be close to the vector ∑ i∈[d] λi ·ui.",4.1. Higher-Order Tensors,0,[0]
"With such a vector w̄, one can show that the matrix
M̄ = T̄ (Id, Id, w̄, . . .",4.1. Higher-Order Tensors,0,[0]
", w̄), with 2r − 1 copies of w̄, is close to the matrix ∑
i∈[d]",4.1. Higher-Order Tensors,0,[0]
"λ 2r i ·
ui ⊗ ui, similarly to Lemma 3.",4.1. Higher-Order Tensors,0,[0]
"Then the rest is the same
as that in the previous section.",4.1. Higher-Order Tensors,0,[0]
Note that this approach has the eigenvalues decreased exponentially in r.,4.1. Higher-Order Tensors,0,[0]
"A different approach avoiding this is to compute M̄ directly as
M̄ = 1
L ∑ j∈[L] ( T̄ (Id, Id, wj , . .",4.1. Higher-Order Tensors,0,[0]
.,4.1. Higher-Order Tensors,0,[0]
", wj) )2 ,
which is close to
1
L ∑ j∈[L] (T (Id, Id, wj , . . .",4.1. Higher-Order Tensors,0,[0]
", wj))",4.1. Higher-Order Tensors,0,[0]
"2
= 1
L ∑ j∈[L] ∑ i∈[d]",4.1. Higher-Order Tensors,0,[0]
"λ2i ( u>i wj )4r−2 · ui ⊗ ui =
∑ i∈[d]",4.1. Higher-Order Tensors,0,[0]
λ2i 1 L ∑ j∈[L] ( u>i wj )4r−2 · ui ⊗ ui.,4.1. Higher-Order Tensors,0,[0]
Then one can again show that such a matrix M̄ is likely to be close to the matrix ∑ i∈[d],4.1. Higher-Order Tensors,0,[0]
"λ 2 i · ui ⊗ ui.
To handle tensors of even orders, the initialization is slightly different but the idea is similar.",4.1. Higher-Order Tensors,0,[0]
"Given a tensor of order 2r, we again sample vectors w1, . . .",4.1. Higher-Order Tensors,0,[0]
", wL as before, but now we compute the matrix directly as
M̄ = 1
L ∑ j∈[L] T̄ (Id, Id, wj , . . .",4.1. Higher-Order Tensors,0,[0]
", wj),
with 2r − 2 copies of wj .",4.1. Higher-Order Tensors,0,[0]
"As before, one can show that the matrix M̄ is likely to be close to the matrix ∑ i∈[d] λi · ui⊗ui.",4.1. Higher-Order Tensors,0,[0]
Then again we can apply the matrix power method on M̄ and obtain a good initialization for the tensor power method as before.,4.1. Higher-Order Tensors,0,[0]
"Note that now the eigenvalues are no longer squared, and the previous requirement on ‖Φ‖ can be slightly relaxed, with the dependence on ∆2 being replaced by ∆.",4.1. Higher-Order Tensors,0,[0]
"For simplicity of presentation, let us focus on the third order case; the extension to higher orders is straightforward.",4.2. Asymmetric Tensors,0,[0]
"That is, now the underlying tensor has the form
T = ∑ i∈[d] λi · ai ⊗ bi ⊗ ci,
with nonnegative λi’s satisfying the condition (1), together with three sets of orthonormal vectors of ai’s, bi’s, and ci’s.",4.2. Asymmetric Tensors,0,[0]
"As before, we only have access to a noisy version T̄ of T .
",4.2. Asymmetric Tensors,0,[0]
"The main modification of our algorithm is again to the initialization procedure, but the idea is also similar.",4.2. Asymmetric Tensors,0,[0]
"To find a good initial matrix A for ai’s, we sample w1, . . .",4.2. Asymmetric Tensors,0,[0]
", wL independently from N d(0, 1), and now compute the matrix
M̄",4.2. Asymmetric Tensors,0,[0]
"= 1
L ∑ j∈[L] ( T̄ (Id, Id, wj) )",4.2. Asymmetric Tensors,0,[0]
"( T̄ (Id, Id, wj) )> .
",4.2. Asymmetric Tensors,0,[0]
"As before, it is not hard to show that
M̄",4.2. Asymmetric Tensors,0,[0]
≈ ∑ i∈[d],4.2. Asymmetric Tensors,0,[0]
1 L ∑ j∈[L] λ2i,4.2. Asymmetric Tensors,0,[0]
"( c>i wj )2 · ai ⊗ ai, which is close to the matrix ∑ i∈[d]",4.2. Asymmetric Tensors,0,[0]
λ 2 i · ai ⊗ ai with high probability.,4.2. Asymmetric Tensors,0,[0]
"From the matrix M̄ , we can again apply the matrix power method to find a good initial matrix A. Similarly, we can find good initial matrices B and C for bi’s and ci’s, respectively.
",4.2. Asymmetric Tensors,0,[0]
"Next, with such matrices, we would like to apply the tensor power method, which we modify as follows.",4.2. Asymmetric Tensors,0,[0]
"Now at each step t, we take previous estimates A(t−1), B(t−1), C(t−1), and compute X(t)i = T̄ (Id, B (t−1) i , C (t−1) i ), Y (t)",4.2. Asymmetric Tensors,0,[0]
i = T̄,4.2. Asymmetric Tensors,0,[0]
(A (t−1),4.2. Asymmetric Tensors,0,[0]
"i , Id, C (t−1) i ), Z (t) i = T̄",4.2. Asymmetric Tensors,0,[0]
"(A (t−1) i , B (t−1)",4.2. Asymmetric Tensors,0,[0]
"i , Id), for i ∈",4.2. Asymmetric Tensors,0,[0]
"[k], followed by orthonormalizing X(t), Y (t), Z(t) to obtain the new estimates A(t), B(t), C(t) via QRdecomposition.",4.2. Asymmetric Tensors,0,[0]
It is not hard to show that the resulting algorithm has a similar convergence rate as our Algorithm 1.,4.2. Asymmetric Tensors,0,[0]
"In the previous section, we consider general orthogonal tensors, which can be asymmetric.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"In this section, we consider non-orthogonal tensors which are symmetric.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"We remark that for some latent variable models such as the multi-view model, the corresponding asymmetric tensors can be converted into symmetric ones (Anandkumar et al., 2014a), so that our result here can still be applied.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"For simplicity of exposition, let us again focus on the case of order three, so that the given tensor has the form T =∑
i∈[d] λi ·vi⊗vi⊗vi, but the vectors vi’s are no longer assumed to be orthogonal to each other.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Still we assume them to be linearly independent, and we again assume without loss of generality that ‖T‖ ≤ 1 and ‖vi‖",5. Nonorthogonal but Symmetric Tensors,0,[0]
"= 1 for each i. In addition, let us assume, as in previous works, that λj = 0 for j ≥ k + 1.2
Following (Anandkumar et al., 2014a), we would like to whiten such a tensor T into an orthogonal one, so that we can then apply our Algorithm 1.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"More precisely, our goal is to find a d× k matrix W such that the tensor T (W,W,W ) becomes orthogonal.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"As in (Anandkumar et al., 2014a), assume that we also have available a matrix
M = ∑ i∈[k] λi · vi ⊗ vi.3
Then for a whitening matrix, it suffices to find some W
2This assumption is not necessary.",5. Nonorthogonal but Symmetric Tensors,0,[0]
We assume it just to simplify the first step of our algorithm given below.,5. Nonorthogonal but Symmetric Tensors,0,[0]
"Without it, we can simply replace that step by the matrix power method used in our Algorithm 1, which takes more steps but can still do the job.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"3More generally, the weights λi in M are allowed to differ from those in T , but for simplicity we assume they are the same.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
such that W>MW = Ik.,5. Nonorthogonal but Symmetric Tensors,0,[0]
"The reason is that
Ik = W >MW = ∑ i∈[k] λi · ( W>vi ) ⊗",5. Nonorthogonal but Symmetric Tensors,0,[0]
"( W>vi ) ,
which implies that the vectors √ λiW
>vi, for i ∈",5. Nonorthogonal but Symmetric Tensors,0,[0]
"[k], are orthonormal.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Then the tensor T (W,W,W ) equals∑
i∈[k] λi · (W>vi)⊗3 = ∑ i∈[k] 1√ λi · (√ λiW >vi )⊗3 ,
which has an orthogonal decomposition.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"According to (Anandkumar et al., 2014a), one way to find such a W is to do the spectral decomposition of M as UΛU>, with eigenvectors as columns of U , and let W = UΛ− 1 2 .",5. Nonorthogonal but Symmetric Tensors,0,[0]
"However, we will not take this approach, because finding a good approximate to U by the matrix power method would take longer to converge than the tensor power method which we will later apply to the whitened tensor.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Our key observation is that it suffices to find a d×k matrix Q such that the matrix P = Q>MQ is invertible, since we can then let W = QP− 1 2 and have
W>MW = P− 1 2Q>MQP− 1 2 = Ik.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"With such a W , the tensor T (W,W,W ) becomes orthogonal, so that we can decompose it4 to obtain σi = 1√λi and ui = √ λiW
>vi, from which we can recover λi = 1σ2i and
vi = σiQP 1 2ui if Q has orthonormal columns.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"As before, we consider a similar setting in which we only have access to a noisy M̄",5. Nonorthogonal but Symmetric Tensors,0,[0]
"= M + Φ̄, for some symmetric perturbation matrix Φ̄, in addition to the noisy tensor T̄ =",5. Nonorthogonal but Symmetric Tensors,0,[0]
"T+Φ. Then our algorithm for finding the whitening matrix consists of the following two steps:
1.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Sample a random matrix Z ∈ Rd×k with orthonormal columns, compute Ȳ = M̄Z, and factorize it as Ȳ = QR̄ by a QR decomposition.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
2,5. Nonorthogonal but Symmetric Tensors,0,[0]
"Compute P̄ = Q>M̄Q and output W̄ = QP̄− 1 2 as
the whitening matrix.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
We analyze our algorithm in the following.,5. Nonorthogonal but Symmetric Tensors,0,[0]
"First note that Q is computed in the same way as we compute Z(1) in Algorithm 1, and with λk+1 = 0",5. Nonorthogonal but Symmetric Tensors,0,[0]
we are likely to have tank(Q),5. Nonorthogonal but Symmetric Tensors,0,[0]
≈ 0,5. Nonorthogonal but Symmetric Tensors,0,[0]
so that the matrix P = Q>MQ is invertible.,5. Nonorthogonal but Symmetric Tensors,0,[0]
"Formally, we have the following, which we prove in Appendix C.1.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
Lemma 5.,5. Nonorthogonal but Symmetric Tensors,0,[0]
Suppose ‖Φ̄‖ ≤ α0λk√ dk for a small enough constant α0.,5. Nonorthogonal but Symmetric Tensors,0,[0]
Then with high probability we have σmax(P ),5. Nonorthogonal but Symmetric Tensors,0,[0]
≤,5. Nonorthogonal but Symmetric Tensors,0,[0]
λ1 and σmin(P ) ≥,5. Nonorthogonal but Symmetric Tensors,0,[0]
"λk2 .
4To apply our Algorithm 1, we need to scale it properly, say by a factor of √ λk/k to make its norm at most one.
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Next, with a small enough ‖Φ̄‖, if P is invertible, then so is P̄ , and moreover, we have P̄− 1 2 ≈ P− 12 .",5. Nonorthogonal but Symmetric Tensors,0,[0]
"This is shown in the following, which we prove in Appendix C.2.
Lemma 6.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Fix any ∈ (0, 1) and suppose we have σmin(P ) ≥ 2 and ‖Φ̄‖ ≤ .",5. Nonorthogonal but Symmetric Tensors,0,[0]
Then P̄ is invertible and ‖P̄− 12 − P− 12 ‖ ≤ 2 (σmin(P ))−2(σmax(P )),5. Nonorthogonal but Symmetric Tensors,0,[0]
"1 2 .
",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Then, with a good P̄− 1 2 , we can obtain a good W̄ and have T̄ (W̄ , W̄ , W̄ )",5. Nonorthogonal but Symmetric Tensors,0,[0]
"close to T (W,W,W ) which has an orthogonal decomposition.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"This is shown in the following, which we prove in Appendix C.3.
Theorem 3.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Fix any ε ∈ (0, λk4 ) and suppose we have ‖Φ‖ ≤ α0λ 3 2
k ε and ‖Φ̄‖ ≤ α0εmin{ λk√ dk , λ3k√ λ1 }, for a
small enough constant α0.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"Then with high probability we have ‖T̄ (W̄ , W̄ , W̄ )",5. Nonorthogonal but Symmetric Tensors,0,[0]
"− T (W,W,W )‖ ≤ ε.",5. Nonorthogonal but Symmetric Tensors,0,[0]
"In the previous sections, we consider the batch setting in which the tensor T̄ is assumed to be stored somewhere which can be accessed whenever we want to.",6. Streaming setting,0,[0]
"However, storing such a tensor, say of order three, requires a space complexity of Ω(d3), which becomes impractical even for a moderate value of d. In this section, we study the possibility of achieving a space complexity of O(kd), which is the least amount of memory needed just to store the k vectors in Rd.",6. Streaming setting,0,[0]
"More precisely, we consider the streaming setting, in which there is a stream of vectors x1, x2, . . .",6. Streaming setting,0,[0]
arriving one at a time.,6. Streaming setting,0,[0]
"We assume that each vector x is sampled independently from some distribution over Rd, with ‖x‖ ≤ 1 and some function g : Rd → Rd×d×d such that
• E[g(x)]",6. Streaming setting,0,[0]
#NAME?,6. Streaming setting,0,[0]
"Such a function g is known to exist for some latent variable models (Ge et al., 2015; Wang & Anandkumar, 2016).",6. Streaming setting,0,[0]
"Given such a function, our algorithms in previous sections can all be converted to work in the streaming setting using O(kd) space.",6. Streaming setting,0,[0]
"This is because all our operations involving tensors have the form T̄ (Id, u, v), for some u, v ∈ Rd, which can be realized as(
1 |J | ∑ t∈J g(xt)
)",6. Streaming setting,0,[0]
"(Id, u, v)",6. Streaming setting,0,[0]
"= 1
|J | ∑ t∈J (g(xt) (Id, u, v)) ,
for a collection J of samples, with the righthand side above clearly computable in O(kd) space.5",6. Streaming setting,0,[0]
"Then depending on the distance we want between T̄ = 1|J| ∑ t∈J g(xt) and T ,
5This also includes the initialization phase in which we now do not store the matrix M̄ explicitly but instead replace the operation M̄Zi by T̄ (Id, Zi, w̄).
",6. Streaming setting,0,[0]
we can choose a proper size for J .,6. Streaming setting,0,[0]
"In fact, to save the total number of samples, we can follow the approach of (Li et al., 2016) by choosing different sizes in different iterations of the matrix or tensor power method.
",6. Streaming setting,0,[0]
"Following (Wang & Anandkumar, 2016), let us take the specific case with g(x) = x⊗ x⊗ x",6. Streaming setting,0,[0]
as a concrete example and focus on the orthogonal case studied in Section 3; it is not hard to convert other algorithms of ours to the streaming setting.,6. Streaming setting,0,[0]
"One can show that in this specific case, we have E[x] = ∑ i∈[d] λiui so that there is a more efficient way to find a vector w̄ for producing the matrix M̄ in the initialization phase.",6. Streaming setting,0,[0]
"Formally, we have the following lemma, which we prove in Appendix D.1.
",6. Streaming setting,0,[0]
Lemma 7.,6. Streaming setting,0,[0]
"There is an algorithm using O(d) space and O( log k∆2 ) samples to find some w̄ ∈ R
d satisfying the condition (5) in Lemma 2 with high probability.
",6. Streaming setting,0,[0]
"With such a vector w̄, we can then use the streaming algorithm of (Li et al., 2016) to find a good initial matrix Z for the later tensor power phase.",6. Streaming setting,0,[0]
"Formally, we have the following lemma, which we prove in Appendix D.2.
",6. Streaming setting,0,[0]
Lemma 8.,6. Streaming setting,0,[0]
"Given w̄ from Lemma 7, we can use O(kd) space and O(kd log d γ
∆4γ ) samples to find some Z ∈ R d×k
with tanm(Z) < 1, for any m ∈",6. Streaming setting,0,[0]
"[k], with high probability.
",6. Streaming setting,0,[0]
"Having such a matrix Z, we can proceed to the tensor power phase.",6. Streaming setting,0,[0]
"Borrowing again the idea from (Li et al., 2016), let us partition the incoming data into blocks of increasing sizes, with the t’th block Jt used to carry out one tensor power iteration Y (t)i = T̄ (t)(Id, Q (t−1) i , Q (t−1) i ), for
i ∈",6. Streaming setting,0,[0]
"[k], of Algorithm 1, with T̄ (t) = 1|Jt| ∑ τ∈Jt g(xτ ).",6. Streaming setting,0,[0]
"Instead of preparing this T̄ (t) and then computing each Y (t)i , we now go through |Jt| steps of updates:
•",6. Streaming setting,0,[0]
For τ ∈,6. Streaming setting,0,[0]
Jt do: Y (t)i = Y (t),6. Streaming setting,0,[0]
i + 1 |Jt| (x > τ,6. Streaming setting,0,[0]
"Q (t−1) i ) 2xτ .
",6. Streaming setting,0,[0]
The block sizes are chosen carefully to keep ‖T̄ (t),6. Streaming setting,0,[0]
− T‖ small enough so that we can have tanm(Q(t)) decreased in a desirable rate.,6. Streaming setting,0,[0]
"Here, we choose the parameters
βt = max { ρ2 t−1, ε
2
} and |Jt| = c0 log(dt)
∆2β2t , (6)
for a large enough constant c0, to make the condition (3) in Lemma 1 hold with high probability so that we have tanm(Q
(t))",6. Streaming setting,0,[0]
≤ βt.,6. Streaming setting,0,[0]
"In Appendix D.3, we summarize our algorithm and prove the following theorem.
",6. Streaming setting,0,[0]
Theorem 4.,6. Streaming setting,0,[0]
"Given ε ∈ (0, λk2 ), with high probability we can find λ̂i, ûi with |λ̂i − λi|, ‖ûi",6. Streaming setting,0,[0]
"− ui‖ ≤ ε, for any i ∈",6. Streaming setting,0,[0]
"[k], using O(kd) space and O(kd log d γ
∆4γ + log(d log( 1γ log 1 ε ))
∆2γε2 ) samples.",6. Streaming setting,0,[0]
"Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem is the tensor power method.",abstractText,0,[0]
"However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices.",abstractText,0,[0]
"In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure.",abstractText,0,[0]
"This allows us to achieve a better running time in the batch setting, as well as a lower sample complexity in the streaming setting.",abstractText,0,[0]
Tensor Decomposition via Simultaneous Power Iteration,title,0,[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1103–1114 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,0,[0]
"Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).
",1 Introduction,0,[0]
"This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.).",1 Introduction,0,[0]
"The central challenge in multimodal sentiment analysis is to model the inter-modality dynamics: the interactions between
† means equal contribution
language, visual and acoustic behaviors that change the perception of the expressed sentiment.
",1 Introduction,0,[0]
Figure 1 illustrates these complex inter-modality dynamics.,1 Introduction,0,[0]
"The utterance “This movie is sick” can be ambiguous (either positive or negative) by itself, but if the speaker is also smiling at the same time, then it will be perceived as positive.",1 Introduction,0,[0]
"On the other hand, the same utterance with a frown would be perceived negatively.",1 Introduction,0,[0]
A person speaking loudly “This movie is sick” would still be ambiguous.,1 Introduction,0,[0]
These examples are illustrating bimodal interactions.,1 Introduction,0,[0]
Examples of trimodal interactions are shown in Figure 1 when loud voice increases the sentiment to strongly positive.,1 Introduction,0,[0]
"The complexity of inter-modality dynamics is shown in the second trimodal example where the utterance “This movie is fair” is still weakly positive, given the strong influence of the word “fair”.
",1 Introduction,0,[0]
A second challenge in multimodal sentiment analysis is efficiently exploring intra-modality dynamics of a specific modality (unimodal interaction).,1 Introduction,0,[0]
"Intra-modality dynamics are particularly
1103
challenging for the language analysis since multimodal sentiment analysis is performed on spoken language.",1 Introduction,0,[0]
A spoken opinion such as “I think it was alright . . .,1 Introduction,0,[0]
Hmmm . . .,1 Introduction,0,[0]
let me think . . .,1 Introduction,0,[0]
yeah . . .,1 Introduction,0,[0]
no . . .,1 Introduction,0,[0]
ok yeah”,1 Introduction,0,[0]
almost never happens in written text.,1 Introduction,0,[0]
"This volatile nature of spoken opinions, where proper language structure is often ignored, complicates sentiment analysis.",1 Introduction,0,[0]
"Visual and acoustic modalities also contain their own intra-modality dynamics which are expressed through both space and time.
",1 Introduction,0,[0]
"Previous works in multimodal sentiment analysis does not account for both intra-modality and intermodality dynamics directly, instead they either perform early fusion (a.k.a., feature-level fusion) or late fusion (a.k.a., decision-level fusion).",1 Introduction,0,[0]
"Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016).",1 Introduction,0,[0]
This fusion approach does not allow the intra-modality dynamics to be efficiently modeled.,1 Introduction,0,[0]
This is due to the fact that inter-modality dynamics can be more complex at input level and can dominate the learning process or result in overfitting.,1 Introduction,0,[0]
"Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).",1 Introduction,0,[0]
"This prevents the model from learning inter-modality dynamics in an efficient way by assuming that simple weighted averaging is a proper fusion approach.
",1 Introduction,0,[0]
"In this paper, we introduce a new model, termed Tensor Fusion Network (TFN), which learns both the intra-modality and inter-modality dynamics end-to-end.",1 Introduction,0,[0]
"Inter-modality dynamics are modeled with a new multimodal fusion approach, named Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions.",1 Introduction,0,[0]
"Intramodality dynamics are modeled through three Modality Embedding Subnetworks, for language, visual and acoustic modalities, respectively.
",1 Introduction,0,[0]
"In our extensive set of experiments, we show (a) that TFN outperforms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches.",1 Introduction,0,[0]
"Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008).",2 Related Work,0,[0]
"Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015).
",2 Related Work,0,[0]
Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment.,2 Related Work,0,[0]
"There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (Wöllmer et al., 2013), YouTube (Morency et al., 2011), and MOUD (Pérez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels.",2 Related Work,0,[0]
"The newest multimodal sentiment analysis approaches have used deep neural networks, including convolutional neural networks (CNNs) with multiple-kernel learning (Poria et al., 2015), SAL-CNN (Wang et al., 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).
",2 Related Work,0,[0]
"Audio-Visual Emotion Recognition is closely tied to multimodal sentiment analysis (Poria et al., 2017).",2 Related Work,0,[0]
"Both audio and visual features have been shown to be useful in the recognition of emotions (Ghosh et al., 2016a).",2 Related Work,0,[0]
"Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).
",2 Related Work,0,[0]
Multimodal Machine Learning has been a growing trend in machine learning research that is closely tied to the studies in this paper.,2 Related Work,0,[0]
"Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",2 Related Work,0,[0]
"Multimodal Opinion Sentiment Intensity (CMUMOSI) dataset is an annotated dataset of video
0 100
200
300
400
500
600
Highly Negative Negative Weakly Negative Neutral Weakly Positive Positive Highly Positive
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
100%
5 10 15 20 25 30 35
Pe rc
en ta
ge o
f S en
tim en
t D eg
re es
Utterance Size
Highly Positive
Positive
Weakly Positive
Neutral
Weakly Negative
Negative
Highly Negative
N um
be r o
f O pi
ni on
S eg
m en
ts
Figure 2: Distribution of sentiment across different opinions (left) and opinion sizes (right) in CMU-MOSI.
opinions from YouTube movie reviews (Zadeh et al., 2016a).",3 CMU-MOSI Dataset,0,[0]
"Annotation of sentiment has closely followed the annotation scheme of the Stanford Sentiment Treebank (Socher et al., 2013), where sentiment is annotated on a seven-step Likert scale from very negative to very positive.",3 CMU-MOSI Dataset,0,[0]
"However, whereas the Stanford Sentiment Treebank is segmented by sentence, the CMU-MOSI dataset is segmented by opinion utterances to accommodate spoken language where sentence boundaries are not as clear as text.",3 CMU-MOSI Dataset,0,[0]
There are 2199 opinion utterances for 93 distinct speakers in CMU-MOSI.,3 CMU-MOSI Dataset,0,[0]
There are an average 23.2 opinion segments in each video.,3 CMU-MOSI Dataset,0,[0]
Each video has an average length of 4.2 seconds.,3 CMU-MOSI Dataset,0,[0]
"There are a total of 26,295 words in the opinion utterances.",3 CMU-MOSI Dataset,0,[0]
These utterance are annotated by five Mechanical Turk annotators for sentiment.,3 CMU-MOSI Dataset,0,[0]
The final agreement between the annotators is high in terms of Krippendorf’s alpha α = 0.77.,3 CMU-MOSI Dataset,0,[0]
Figure 2 shows the distribution of sentiment across different opinions and different opinion sizes.,3 CMU-MOSI Dataset,0,[0]
"CMU-MOSI dataset facilitates three prediction tasks, each of which we address in our experiments: 1) Binary Sentiment Classification 2) Five-Class Sentiment Classification (similar to Stanford Sentiment Treebank fine-grained classification with seven scale being mapped to five) and 3) Sentiment Regression in range [−3, 3].",3 CMU-MOSI Dataset,0,[0]
"For sentiment regression, we report Mean-Absolute Error (lower is better) and correlation (higher is better) between the model predictions and regression ground truth.",3 CMU-MOSI Dataset,0,[0]
"Our proposed TFN consists of three major components: 1) Modality Embedding Subnetworks take as input unimodal features, and output a rich modality embedding.",4 Tensor Fusion Network,0,[0]
2),4 Tensor Fusion Network,0,[0]
"Tensor Fusion Layer explicitly models the unimodal, bimodal and trimodal interactions using a 3-fold Cartesian product from modality embeddings.",4 Tensor Fusion Network,0,[0]
"3) Sentiment Inference Subnetwork is a
network conditioned on the output of the Tensor Fusion Layer and performs sentiment inference.",4 Tensor Fusion Network,0,[0]
"Depending on the task from Section 3 the network output changes to accommodate binary classification, 5-class classification or regression.",4 Tensor Fusion Network,0,[0]
"Input to the TFN is an opinion utterance which includes three modalities of language, visual and acoustic.",4 Tensor Fusion Network,0,[0]
The following three subsections describe the TFN subnetworks and their inputs in detail.,4 Tensor Fusion Network,0,[0]
"Spoken Language Embedding Subnetwork: Spoken text is different than written text (reviews, tweets) in compositionality and grammar.",4.1 Modality Embedding Subnetworks,0,[0]
We revisit the spoken opinion: “I think it was alright . . .,4.1 Modality Embedding Subnetworks,0,[0]
Hmmm . . .,4.1 Modality Embedding Subnetworks,0,[0]
let me think . . .,4.1 Modality Embedding Subnetworks,0,[0]
yeah . . .,4.1 Modality Embedding Subnetworks,0,[0]
no . . .,4.1 Modality Embedding Subnetworks,0,[0]
ok yeah”.,4.1 Modality Embedding Subnetworks,0,[0]
This form of opinion rarely happens in written language but variants of it are very common in spoken language.,4.1 Modality Embedding Subnetworks,0,[0]
The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part.,4.1 Modality Embedding Subnetworks,0,[0]
"The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech.
",4.1 Modality Embedding Subnetworks,0,[0]
Our proposed approach to deal with challenges of spoken language is to learn a rich representation of spoken words at each word interval and use it as input to a fully connected deep network (Figure 3).,4.1 Modality Embedding Subnetworks,0,[0]
"This rich representation for ith word contains information from beginning of utterance through time, as well as ith word.",4.1 Modality Embedding Subnetworks,0,[0]
"This way as the model is discovering the meaning of the utterance through time, if it encounters unusable information in word i+ 1 and arbitrary number of words after, the representation up until i is not diluted or lost.",4.1 Modality Embedding Subnetworks,0,[0]
"Also, if the model encounters usable information again, it can recover by embedding those in the long short-term memory (LSTM).",4.1 Modality Embedding Subnetworks,0,[0]
"The time-dependent
encodings are usable by the rest of the pipeline by simply focusing on relevant parts using the nonlinear affine transformation of time-dependent embeddings which can act as a dimension reducing attention mechanism.",4.1 Modality Embedding Subnetworks,0,[0]
"To formally define our proposed Spoken Language Embedding Subnetwork (Ul), let l = {l1, l2, l3, . . .",4.1 Modality Embedding Subnetworks,0,[0]
", lTl ; lt ∈ R300}, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014).
",4.1 Modality Embedding Subnetworks,0,[0]
"A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations",4.1 Modality Embedding Subnetworks,0,[0]
"hl = {h1, h2, h3, . . .",4.1 Modality Embedding Subnetworks,0,[0]
", hTl ;ht ∈ R128} for words according to the following LSTM formulation.
",4.1 Modality Embedding Subnetworks,0,[0]
i f o,4.1 Modality Embedding Subnetworks,0,[0]
m  =  sigmoid sigmoid sigmoid tanh,4.1 Modality Embedding Subnetworks,0,[0]
"Wld (XtWleht−1 )
ct = f ct−1",4.1 Modality Embedding Subnetworks,0,[0]
+,4.1 Modality Embedding Subnetworks,0,[0]
i m ht = o⊗ tanh(ct) hl,4.1 Modality Embedding Subnetworks,0,[0]
=,4.1 Modality Embedding Subnetworks,0,[0]
[h1;h2;h3; . . .,4.1 Modality Embedding Subnetworks,0,[0]
";hTl ]
hl is a matrix of language representations formed from concatenation of h1, h2, h3, . . .",4.1 Modality Embedding Subnetworks,0,[0]
hTl .,4.1 Modality Embedding Subnetworks,0,[0]
"hl is then used as input to a fully-connected network that generates language embedding zl:
zl = Ul(l; Wl) ∈ R128
where Wl is the set of all weights in the Ul network (including Wld , Wle ,Wlfc , and blfc), σ is the sigmoid function.
",4.1 Modality Embedding Subnetworks,0,[0]
"Visual Embedding Subnetwork: Since opinion videos consist mostly of speakers talking to the audience through close-up camera, face is the most important source of visual information.",4.1 Modality Embedding Subnetworks,0,[0]
"The speaker’s face is detected for each frame (sampled at 30Hz) and indicators of the seven basic emotions
(anger, contempt, disgust, fear, joy, sadness, and surprise) and two advanced emotions (frustration and confusion) (Ekman, 1992) are extracted using FACET facial expression analysis framework1.",4.1 Modality Embedding Subnetworks,0,[0]
"A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET.",4.1 Modality Embedding Subnetworks,0,[0]
"Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltrušaitis et al., 2016; Zadeh et al., 2017).
",4.1 Modality Embedding Subnetworks,0,[0]
Let the visual features v̂j =,4.1 Modality Embedding Subnetworks,0,[0]
"[v1j , v 2 j , v 3 j , . . .",4.1 Modality Embedding Subnetworks,0,[0]
", v p j ] for frame j of utterance video contain the set of p visual features, with Tv the number of total video frames in utterance.",4.1 Modality Embedding Subnetworks,0,[0]
We perform mean pooling over the frames to obtain the expected visual features v =,4.1 Modality Embedding Subnetworks,0,[0]
"[E[v1],E[v2],E[v3], . . .",4.1 Modality Embedding Subnetworks,0,[0]
",E[vl]].",4.1 Modality Embedding Subnetworks,0,[0]
v is then used as input to the Visual Embedding Subnetwork Uv.,4.1 Modality Embedding Subnetworks,0,[0]
"Since information extracted using FACET from videos is rich, using a deep neural network would be sufficient to produce meaningful embeddings of visual modality.",4.1 Modality Embedding Subnetworks,0,[0]
We use a deep neural network with three hidden layers of 32 ReLU units and weights Wv.,4.1 Modality Embedding Subnetworks,0,[0]
Empirically we observed that making the model deeper or increasing the number of neurons in each layer does not lead to better visual performance.,4.1 Modality Embedding Subnetworks,0,[0]
"The subnetwork output provides the visual embedding zv:
zv = Uv(v; Wv) ∈",4.1 Modality Embedding Subnetworks,0,[0]
"R32
Acoustic Embedding Subnetwork: For each opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework (Degottex et al., 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al., 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).",4.1 Modality Embedding Subnetworks,0,[0]
"These extracted features capture different characteristics of human voice and have been shown to be related to emotions (Ghosh et al., 2016b).
",4.1 Modality Embedding Subnetworks,0,[0]
"1http://goo.gl/1rh1JN
For each opinion segment with Ta audio frames (sampled at 100Hz; i.e., 10ms), we extract the set of q acoustic features âj",4.1 Modality Embedding Subnetworks,0,[0]
=,4.1 Modality Embedding Subnetworks,0,[0]
"[a1j , a 2 j , a 3 j , . . .",4.1 Modality Embedding Subnetworks,0,[0]
", a q j ] for audio frame j in utterance.",4.1 Modality Embedding Subnetworks,0,[0]
"We perform mean pooling per utterance on these extracted acoustic features to obtain the expected acoustic features a = [E[a1],E[a2],E[a3], . . .",4.1 Modality Embedding Subnetworks,0,[0]
",E[q]].",4.1 Modality Embedding Subnetworks,0,[0]
"Here, a is the input to the Audio Embedding Subnetwork Ua.",4.1 Modality Embedding Subnetworks,0,[0]
"Since COVAREP also extracts rich features from audio, using a deep neural network is sufficient to model the acoustic modality.",4.1 Modality Embedding Subnetworks,0,[0]
"Similar to Uv, Ua is a network with 3 layers of 32 ReLU units with weights Wa.
",4.1 Modality Embedding Subnetworks,0,[0]
"Here, we also empirically observed that making the model deeper or increasing the number of neurons in each layer does not lead to better performance.",4.1 Modality Embedding Subnetworks,0,[0]
"The subnetwork produces the audio embedding za:
za = Ua(a;Wa) ∈",4.1 Modality Embedding Subnetworks,0,[0]
R32,4.1 Modality Embedding Subnetworks,0,[0]
"While previous works in multimodal research has used feature concatenation as an approach for multimodal fusion, we aim to build a fusion layer in TFN that disentangles unimodal, bimodal and trimodal dynamics by modeling each of them explicitly.",4.2 Tensor Fusion Layer,0,[0]
"We call this layer Tensor Fusion, which is defined as the following vector field using three-fold Cartesian product:{
(zl, zv, za) | zl ∈",4.2 Tensor Fusion Layer,0,[0]
"[ zl
1
] , zv ∈",4.2 Tensor Fusion Layer,0,[0]
"[ zv
1
] , za ∈",4.2 Tensor Fusion Layer,0,[0]
"[ za
1
]}
The extra constant dimension with value 1 generates the unimodal and bimodal dynamics.",4.2 Tensor Fusion Layer,0,[0]
"Each neural coordinate (zl, zv, za) can be seen as a 3-D point in the 3-fold Cartesian space defined by the language, visual, and acoustic embeddings dimensions [zl1]T , [zv1]T , and [za1]T .
",4.2 Tensor Fusion Layer,0,[0]
"This definition is mathematically equivalent to a differentiable outer product between zl, the visual representation zv, and the acoustic representation za.
",4.2 Tensor Fusion Layer,0,[0]
zm =,4.2 Tensor Fusion Layer,0,[0]
"[ zl
1
] ⊗",4.2 Tensor Fusion Layer,0,[0]
"[ zv
1
] ⊗ [ za
1 ] Here⊗ indicates the outer product between vectors and zm ∈ R129×33×33 is the 3D cube of all possible combination of unimodal embeddings with seven semantically distinct subregions in Figure 4.",4.2 Tensor Fusion Layer,0,[0]
"The first three subregions zl, zv, and za are unimodal embeddings from Modality Embedding Subnetworks forming unimodal interactions in Tensor Fusion.",4.2 Tensor Fusion Layer,0,[0]
"Three subregions zl ⊗ zv, zl ⊗ za, and zv ⊗ za capture bimodal interactions in Tensor Fusion.",4.2 Tensor Fusion Layer,0,[0]
"Finally, zl ⊗ zv ⊗ za captures trimodal interactions.
",4.2 Tensor Fusion Layer,0,[0]
"Early fusion commonly used in multimodal research dealing with language, vision and audio, can be seen as a special case of Tensor Fusion with only unimodal interactions.",4.2 Tensor Fusion Layer,0,[0]
"Since Tensor Fusion is mathematically formed by an outer product, it has no learnable parameters and we empirically observed that although the output tensor is high dimensional, chances of overfitting are low.
",4.2 Tensor Fusion Layer,0,[0]
"We argue that this is due to the fact that the output neurons of Tensor Fusion are easy to interpret and semantically very meaningful (i.e., the manifold that they lie on is not complex but just high dimensional).",4.2 Tensor Fusion Layer,0,[0]
"Thus, it is easy for the subsequent layers of the network to decode the meaningful information.",4.2 Tensor Fusion Layer,0,[0]
"After Tensor Fusion layer, each opinion utterance can be represented as a multimodal tensor zm.",4.3 Sentiment Inference Subnetwork,0,[0]
We use a fully connected deep neural network called Sentiment Inference Subnetwork Us with weights,4.3 Sentiment Inference Subnetwork,0,[0]
Ws conditioned on zm.,4.3 Sentiment Inference Subnetwork,0,[0]
The architecture of the network consists of two layers of 128 ReLU activation units connected to decision layer.,4.3 Sentiment Inference Subnetwork,0,[0]
"The likelihood function of the Sentiment Inference Subnetwork is defined as follows, where φ is the sentiment prediction:
arg max φ p(φ | zm;Ws) = arg max φ Us(zm;Ws)
",4.3 Sentiment Inference Subnetwork,0,[0]
"In our experiments, we use three variations of the Us network.",4.3 Sentiment Inference Subnetwork,0,[0]
"The first network is trained for binary sentiment classification, with a single sigmoid output neuron using binary cross-entropy loss.",4.3 Sentiment Inference Subnetwork,0,[0]
"The second network is designed for five-class sentiment classification, and uses a softmax probability function using categorical cross-entropy loss.",4.3 Sentiment Inference Subnetwork,0,[0]
"The third network uses a single sigmoid output, using meansquarred error loss to perform sentiment regression.",4.3 Sentiment Inference Subnetwork,0,[0]
"In this paper, we devise three sets of experiments each addressing a different research question:
Experiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.
",5 Experiments,0,[0]
Experiment 2: We study the importance of the TFN subtensors and the impact of each individual modality (see Figure 4).,5 Experiments,0,[0]
"We also compare with the commonly-used early fusion approach.
",5 Experiments,0,[0]
"Experiment 3: We compare the performance of our three modality-specific networks (language, visual and acoustic) with state-of-the-art unimodal approaches.
",5 Experiments,0,[0]
Section 5.4 describes our experimental methodology which is kept constant across all experiments.,5 Experiments,0,[0]
Section 6 will discuss our results in more details with a qualitative analysis.,5 Experiments,0,[0]
"In this section, we compare the performance of TFN model with previously proposed multimodal sentiment analysis models.",5.1 E1: Multimodal Sentiment Analysis,0,[0]
"We compare to the following baselines:
C-MKL (Poria et al., 2015)",5.1 E1: Multimodal Sentiment Analysis,0,[0]
Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis.,5.1 E1: Multimodal Sentiment Analysis,0,[0]
"It is current SOTA (state of the art) on CMU-MOSI.
SAL-CNN (Wang et al., 2016)",5.1 E1: Multimodal Sentiment Analysis,0,[0]
Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network.,5.1 E1: Multimodal Sentiment Analysis,0,[0]
"We retrain the model for 5-fold cross-validation using the code provided by the authors on github.
",5.1 E1: Multimodal Sentiment Analysis,0,[0]
"SVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion.",5.1 E1: Multimodal Sentiment Analysis,0,[0]
"The model used in (Morency et al., 2011) and (Pérez-Rosas et al., 2013) also similarly use SVM on multimodal concatenated features.",5.1 E1: Multimodal Sentiment Analysis,0,[0]
"We also present the results of Random Forest RF-MD to compare to another non-neural approach.
",5.1 E1: Multimodal Sentiment Analysis,0,[0]
The results first experiment are reported in Table 1.,5.1 E1: Multimodal Sentiment Analysis,0,[0]
TFN outperforms previously proposed neural and non-neural approaches.,5.1 E1: Multimodal Sentiment Analysis,0,[0]
This difference is specifically visible in the case of 5-class classification.,5.1 E1: Multimodal Sentiment Analysis,0,[0]
Table 4 shows the results of our ablation study.,5.2 E2: Tensor Fusion Evaluation,0,[0]
"The first three rows are showing the performance of each modality, when no intermodality dynamics are modeled.",5.2 E2: Tensor Fusion Evaluation,0,[0]
"From this first experiment, we observe that the language modality is the most predictive.
",5.2 E2: Tensor Fusion Evaluation,0,[0]
"As a second set of ablation experiments, we test our TFN approach when only the bimodal subtensors are used (TFNbimodal) or when only the trimodal subtensor is used (TFNbimodal).",5.2 E2: Tensor Fusion Evaluation,0,[0]
We observe that bimodal subtensors are more informative when used without other subtensors.,5.2 E2: Tensor Fusion Evaluation,0,[0]
The most interesting comparison is between our full TFN model and a variant (TFNnotrimodal) where the trimodal subtensor is removed (but all the unimodal and bimodal subtensors are present).,5.2 E2: Tensor Fusion Evaluation,0,[0]
"We observe a big improvement for the full TFN model, confirming the importance of the trimodal dynamics and the need for all components of the full tensor.
",5.2 E2: Tensor Fusion Evaluation,0,[0]
"We also perform a comparison with the early fusion approach (TFNearly) by simply concatenating all three modality embeddings < zl, za, zv > and passing it directly as input to Us.",5.2 E2: Tensor Fusion Evaluation,0,[0]
This approach was depicted on the left side of Figure 4.,5.2 E2: Tensor Fusion Evaluation,0,[0]
"When looking at Table 4 results, we see that our TFN approach outperforms the early fusion approach2.",5.2 E2: Tensor Fusion Evaluation,0,[0]
"In this experiment, we compare the performance of our Modality Embedding Networks with stateof-the-art approaches for language-based, visualbased and acoustic-based sentiment analysis.",5.3 E3: Modality Embedding Subnetworks Evaluation,0,[0]
"We selected the following state-of-the-art approaches to include variety in their techniques,
2We also performed other comparisons with variants of the early fusion model TFNearly where we increased the number of parameters and neurons to replicate the numbers from our TFN model.",5.3.1 Language Sentiment Analysis,0,[0]
"In all cases, the performances were similar to TFNearly (and lower than our TFN model).",5.3.1 Language Sentiment Analysis,0,[0]
"Because of space constraints, we could not include them in this paper.
",5.3.1 Language Sentiment Analysis,0,[0]
"based on dependency parsing (RNTN), distributional representation of text (DAN), and convolutional approaches (DynamicCNN).",5.3.1 Language Sentiment Analysis,0,[0]
"When possible, we retrain them on the CMU-MOSI dataset (performances of the original pre-trained models are shown in parenthesis in Table 3) and compare them to our language only TFNlanguage.
",5.3.1 Language Sentiment Analysis,0,[0]
"RNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure.
",5.3.1 Language Sentiment Analysis,0,[0]
"DAN (Iyyer et al., 2015)",5.3.1 Language Sentiment Analysis,0,[0]
"The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences.
",5.3.1 Language Sentiment Analysis,0,[0]
"DynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences.
",5.3.1 Language Sentiment Analysis,0,[0]
"CMK-L, SAL-CNN-L and SVM-MD-L are multimodal models from section using only language modality 5.1.
",5.3.1 Language Sentiment Analysis,0,[0]
Results in Table 3 show that our model using only language modality outperforms state-of-theart approaches for the CMU-MOSI dataset.,5.3.1 Language Sentiment Analysis,0,[0]
"While previous models are well-studied and suitable models for sentiment analysis in written language, they underperform in modeling the sentiment in spoken language.",5.3.1 Language Sentiment Analysis,0,[0]
"We suspect that this underperformance is due to: RNTN and similar approaches rely heavily on dependency structure, which may not be present
in spoken language; DAN and similar sentence embeddings approaches can easily be diluted by words that may not relate directly to sentiment or meaning; D-CNN and similar convolutional approaches rely on spatial proximity of related words, which may not always be present in spoken language.",5.3.1 Language Sentiment Analysis,0,[0]
"We compare the performance of our models using visual information (TFNvisual) with the following well-known approaches in visual sentiment analysis and emotion recognition (retrained for sentiment analysis):
3DCNN (Byeon and Kwak, 2014)",5.3.2 Visual Sentiment Analysis,0,[0]
a network using 3D CNN is trained using the face of the speaker.,5.3.2 Visual Sentiment Analysis,0,[0]
"Face of the speaker is extracted in every 6 frames and resized to 64× 64 and used as the input to the proposed network.
",5.3.2 Visual Sentiment Analysis,0,[0]
"CNN-LSTM (Ebrahimi Kahou et al., 2015) is a recurrent model that at each timestamp performs convolutions over facial region and uses output to an LSTM.",5.3.2 Visual Sentiment Analysis,0,[0]
"Face processing is similar to 3DCNN.
",5.3.2 Visual Sentiment Analysis,0,[0]
"LSTM-FA similar to both baselines above, information extracted by FACET is used every 6 frames as input to an LSTM with a memory dimension of 100 neurons.
SAL-CNN-V, SVM-MD-V, CMKL-V, RF-V use only visual modality in multimodal baselines from Section 5.1.
",5.3.2 Visual Sentiment Analysis,0,[0]
The results in Table 5 show that Uv is able to outperform state-of-the-art approaches on visual sentiment analysis.,5.3.2 Visual Sentiment Analysis,0,[0]
"We compare the performance of our models using visual information (TFNacoustic) with the following well-known approaches in audio sentiment analysis
and emotion recognition (retrained for sentiment analysis):
HL-RNN (Lee and Tashev, 2015) uses an LSTM on high-level audio features.",5.3.3 Acoustic Sentiment Analysis,0,[0]
"We use the same features extracted for Ua averaged over time slices of every 200 intervals.
",5.3.3 Acoustic Sentiment Analysis,0,[0]
"Adieu-Net (Trigeorgis et al., 2016) is an endto-end approach for emotion recognition in audio using directly PCM features.
",5.3.3 Acoustic Sentiment Analysis,0,[0]
"SER-LSTM (Lim et al., 2016) is a model that uses recurrent neural networks on top of convolution operations on spectrogram of audio.
SAL-CNN-A, SVM-MD-A, CMKL-A, RF-A use only acoustic modality in multimodal baselines from Section 5.1.",5.3.3 Acoustic Sentiment Analysis,0,[0]
"All the models in this paper are tested using five-fold cross-validation proposed by CMUMOSI (Zadeh et al., 2016a).",5.4 Methodology,0,[0]
"All of our experiments are performed independent of speaker identity, as no speaker is shared between train and test sets for generalizability of the model to unseen speakers in real-world.",5.4 Methodology,0,[0]
The best hyperparameters are chosen using grid search based on model performance on a validation set (using last 4 videos in train fold).,5.4 Methodology,0,[0]
"The TFN model is trained using the Adam optimizer (Kingma and Ba, 2014) with the learning rate 5e4.",5.4 Methodology,0,[0]
"Uv and Ua, Us subnetworks are regularized using dropout on all hidden layers with p = 0.15 and L2 norm coefficient 0.01.",5.4 Methodology,0,[0]
"The train, test and validation folds are exactly the same for all baselines.",5.4 Methodology,0,[0]
"We analyze the impact of our proposed TFN multimodal fusion approach by comparing it with the
early fusion approach TFNearly and the three unimodal models.",6 Qualitative Analysis,0,[0]
Table 6 shows examples taken from the CMU-MOSI dataset.,6 Qualitative Analysis,0,[0]
Each example is described with the spoken words as well as the acoustic and visual behaviors.,6 Qualitative Analysis,0,[0]
"The sentiment predictions and the ground truth labels range between strongly negative (-3) and strongly positive (+3).
",6 Qualitative Analysis,0,[0]
"As a first general observation, we observe that the early fusion model TFNearly shows a strong preference for the language modality and seems to be neglecting the intermodality dynamics.",6 Qualitative Analysis,0,[0]
We can see this trend by comparing it with the language unimodal model TFNlanguage.,6 Qualitative Analysis,0,[0]
"In comparison, our TFN approach seems to capture more complex interaction through bimodal and trimodal dynamics and thus performs better.",6 Qualitative Analysis,0,[0]
"Specifically, in the first example, the utterance is weakly negative where the speaker is referring to lack of funny jokes in the movie.",6 Qualitative Analysis,0,[0]
"This example contains a bimodal interaction where the visual modality shows a negative expression (frowning) which is correctly captured by our TFN approach.
",6 Qualitative Analysis,0,[0]
"In the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences.",6 Qualitative Analysis,0,[0]
Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment.,6 Qualitative Analysis,0,[0]
"The third example is interesting since it shows an interaction where language predicts a positive sentiment
but the strong negative visual behaviors bring the final prediction of our TFN approach almost to a neutral sentiment.",6 Qualitative Analysis,0,[0]
The fourth example shows how the acoustic modality is also influencing our TFN predictions.,6 Qualitative Analysis,0,[0]
"We introduced a new end-to-end fusion method for sentiment analysis which explicitly represents unimodal, bimodal, and trimodal interactions between behaviors.",7 Conclusion,0,[0]
Our experiments on the publiclyavailable CMU-MOSI dataset produced state-ofthe-art performance when compared against both multimodal approaches.,7 Conclusion,0,[0]
"Furthermore, our approach brings state-of-the-art results for languageonly, visual-only and acoustic-only multimodal sentiment analysis on CMU-MOSI.",7 Conclusion,0,[0]
This project was partially supported by Oculus research grant.,Acknowledgments,0,[0]
We would like to thank the reviewers for their valuable feedback.,Acknowledgments,0,[0]
"Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language.",abstractText,0,[0]
"In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics.",abstractText,0,[0]
"We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end.",abstractText,0,[0]
The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice.,abstractText,0,[0]
"In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.",abstractText,0,[0]
Tensor Fusion Network for Multimodal Sentiment Analysis,title,0,[0]
"Nowadays, the Recurrent Neural Network (RNN), especially its more advanced variants such as the LSTM and the GRU, belong to the most successful machine learning approaches when it comes to sequence modeling.",1. Introduction,0,[0]
"Especially in Natural Language Processing (NLP), great improvements have been achieved by exploiting these Neu-
1Ludwig Maximilian University of Munich, Germany 2Siemens AG, Corporate Technology, Germany.",1. Introduction,0,[0]
Correspondence to:,1. Introduction,0,[0]
"Yinchong Yang <yinchong.yang@siemens.com>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
ral Network architectures.,1. Introduction,0,[0]
"This success motivates efforts to also apply these RNNs to video data, since a video clip could be seen as a sequence of image frames.",1. Introduction,0,[0]
"However, plain RNN models turn out to be impractical and difficult to train directly on video data due to the fact that each image frame typically forms a relatively high-dimensional input, which makes the weight matrix mapping from the input to the hidden layer in RNNs extremely large.",1. Introduction,0,[0]
"For instance, in case of an RGB video clip with a frame size of say 160×120×3, the input vector for the RNN would already be 57, 600 at each time step.",1. Introduction,0,[0]
"In this case, even a small hidden layer consisting of only 100 hidden nodes would lead to 5,760,000 free parameters, only considering the inputto-hidden mapping in the model.
",1. Introduction,0,[0]
"In order to circumvent this problem, state-of-the-art approaches often involve pre-processing each frame using Convolution Neural Networks (CNN), a Neural Network model proven to be most successful in image modeling.",1. Introduction,0,[0]
"The CNNs do not only reduce the input dimension, but can also generate more compact and informative representations that serve as input to the RNN.",1. Introduction,0,[0]
"Intuitive and tempting as it is, training such a model from scratch in an endto-end fashion turns out to be impractical for large video datasets.",1. Introduction,0,[0]
"Thus, many current works following this concept focus on the CNN part and reduce the size of RNN in term of sequence length (Donahue et al., 2015; Srivastava et al., 2015), while other works exploit pre-trained deep CNNs as pre-processor to generate static features as input to RNNs (Yue-Hei Ng et al., 2015; Donahue et al., 2015; Sharma et al., 2015).",1. Introduction,0,[0]
"The former approach neglects the capability of RNNs to handle sequences of variable lengths and therefore does not scale to larger, more realistic video data.",1. Introduction,0,[0]
"The second approach might suffer from suboptimal weight parameters by not being trained end-to-end (Fernando & Gould, 2016).",1. Introduction,0,[0]
"Furthermore, since these CNNs are pretrained on existing image datasets, it remains unclear how well the CNNs can generalize to video frames that could be of totally different nature from the image training sets.
",1. Introduction,0,[0]
"Alternative approaches were earlier applied to generate image representations using dimension reductions such as PCA (Zhang et al., 1997; Kambhatla & Leen, 1997; Ye et al., 2004) and Random Projection (Bingham & Mannila, 2001).",1. Introduction,0,[0]
Classifiers were built on such features to perform object and face recognition tasks.,1. Introduction,0,[0]
"These models, however,
are often restricted to be linear and cannot be trained jointly with the classifier.
",1. Introduction,0,[0]
"In this work, we pursue a new direction where the RNN is exposed to the raw pixels on each frame without any CNN being involved.",1. Introduction,0,[0]
"At each time step, the RNN first maps the large pixel input to a latent vector in a typically much lower dimensional space.",1. Introduction,0,[0]
"Recurrently, each latent vector is then enriched by its predecessor at the last time step with a hidden-to-hidden mapping.",1. Introduction,0,[0]
"In this way, the RNN is expected to capture the inter-frame transition patterns to extract the representation for the entire sequence of frames, analogous to RNNs generating a sentence representation based on word embeddings in NLP (Sutskever et al., 2014).",1. Introduction,0,[0]
"In comparison with other mapping techniques, a direct input-to-hidden mapping in an RNN has several advantages.",1. Introduction,0,[0]
First it is much simpler to train than deep CNNs in an end-to-end fashion.,1. Introduction,0,[0]
Secondly it is exposed to the complete pixel input without the linear limitation as PCA and Random Projection.,1. Introduction,0,[0]
"Thirdly and most importantly, since the input-to-hidden and hidden-to-hidden mappings are trained jointly, the RNN is expected to capture the correlation between spatial and temporal patterns.
",1. Introduction,0,[0]
"To address the issue of having too large of a weight matrix for the input-to-hidden mapping in RNN models, we propose to factorize the matrix with the Tensor-Train decomposition (Oseledets, 2011).",1. Introduction,0,[0]
"In (Novikov et al., 2015) the Tensor-Train has been applied to factorize a fullyconnected feed-forward layer that can consume image pixels as well as latent features.",1. Introduction,0,[0]
"We conducted experiments on three large-scale video datasets that are popular benchmarks in the community, and give empirical proof that the proposed approach makes very simple RNN architectures competitive with the state-of-the-art models, even though they are of several orders of magnitude lower complexity.
",1. Introduction,0,[0]
"The rest of the paper is organized as follows: In Section 2 we summarize the state-of-the-art works, especially in video classification using Neural Network models and the tensorization of weight matrices.",1. Introduction,0,[0]
In Section 3 we first introduce the Tensor-Train model and then provide a detailed derivation of our proposed Tensor-Train RNNs.,1. Introduction,0,[0]
In Section 4 we present our experimental results on three large scale video datasets.,1. Introduction,0,[0]
"Finally, Section 5 serves as a wrap-up of our current contribution and provides an outlook of future work.
",1. Introduction,0,[0]
"Notation We index an entry in a d-dimensional tensor A ∈ Rp1×p2×...×pd using round parentheses such as A(l1, l2, ..., ld) ∈ R and A(l1) ∈ Rp2×p3×...×pd , when we only write the first index.",1. Introduction,0,[0]
"Similarly, we also use A(l1, l2) ∈",1. Introduction,0,[0]
Rp3×p4×...×pd to refer to the sub-tensor specified by two indices l1 and l2.,1. Introduction,0,[0]
The current approaches to model video data are closely related to models for image data.,2. Related Works,0,[0]
"A large majority of these works use deep CNNs to process each frame as image, and aggregate the CNN outputs.",2. Related Works,0,[0]
"(Karpathy et al., 2014) proposes multiple fusion techniques such as Early, Late and Slow Fusions, covering different aspects of the video.",2. Related Works,0,[0]
"This approach, however, does not fully take the order of frames into account.",2. Related Works,0,[0]
"(Yue-Hei Ng et al., 2015) and (Fernando & Gould, 2016) apply global pooling of frame-wise CNNs, before feeding the aggregated information to the final classifier.",2. Related Works,0,[0]
An intuitive and appealing idea is to fuse these frame-wise spatial representations learned by CNNs using RNNs.,2. Related Works,0,[0]
"The major challenge, however, is the computation complexity; and for this reason multiple compromises in the model design have to be made: (Srivastava et al., 2015) restricts the length of the sequences to be 16, while (Sharma et al., 2015) and (Donahue et al., 2015) use pre-trained CNNs.",2. Related Works,0,[0]
"(Shi et al., 2015) proposed a more compact solution that applies convolutional layers as input-tohidden and hidden-to-hidden mapping in LSTM.",2. Related Works,0,[0]
"However, they did not show its performance on large-scale video data.",2. Related Works,0,[0]
"(Simonyan & Zisserman, 2014) applied two stacked CNNs, one for spatial features and the other for temporal ones, and fused the outcomes of both using averaging and a Support-Vector Machine as classifier.",2. Related Works,0,[0]
"This approach is further enhanced with Residual Networks in (Feichtenhofer et al., 2016).",2. Related Works,0,[0]
"To the best of our knowledge, there has been no published work on applying pure RNN models to video classification or related tasks.
",2. Related Works,0,[0]
"The Tensor-Train was first introduced by (Oseledets, 2011) as a tensor factorization model with the advantage of being capable of scaling to an arbitrary number of dimensions.",2. Related Works,0,[0]
"(Novikov et al., 2015) showed that one could reshape a fully connected layer into a high-dimensional tensor and then factorize this tensor using Tensor-Train.",2. Related Works,0,[0]
This was applied to compress very large weight matrices in deep Neural Networks where the entire model was trained end-toend.,2. Related Works,0,[0]
"In these experiments they compressed fully connected layers on top of convolution layers, and also proved that a Tensor-Train Layer can directly consume pixels of image data such as CIFAR-10, achieving the best result among all known non-convolutional models.",2. Related Works,0,[0]
"Then in (Garipov et al., 2016) it was shown that even the convolutional layers themselves can be compressed with Tensor-Train Layers.",2. Related Works,0,[0]
"Actually, in an earlier work by (Lebedev et al., 2014)",2. Related Works,0,[0]
"a similar approach had also been introduced, but their CP factorization is calculated in a pre-processing step and is only fine tuned with error back propagation as a post processing step.
",2. Related Works,0,[0]
"(Koutnik et al., 2014) performed two sequence classification tasks using multiple RNN architectures of relatively low dimensionality:",2. Related Works,0,[0]
"The first task was to classify spoken
words where the input sequence had a dimension of 13 channels.",2. Related Works,0,[0]
"In the second task, RNNs were trained to classify handwriting based on the time-stamped 4D spatial features.",2. Related Works,0,[0]
"RNNs have been also applied to classify the sentiment of a sentence such as in the IMDB reviews dataset (Maas et al., 2011).",2. Related Works,0,[0]
"In this case, the word embeddings form the input to RNN models and they may have a dimension of a few hundreds.",2. Related Works,0,[0]
"The sequence classification model can be seen as a special case of the Encoder-Decoder-Framework (Sutskever et al., 2014) in the sense that a classifier decodes the learned representation for the entire sequence into a probabilistic distribution over all classes.",2. Related Works,0,[0]
"In this section, we first give an introduction to the core ingredient of our proposed approach, i.e., the Tensor-Train Factorization, and then use this to formulate a so-called Tensor-Train Layer (Novikov et al., 2015) which replaces the weight matrix mapping from the input vector to the hidden layer in RNN models.",3. Tensor-Train RNN,0,[0]
"We emphasize that such a layer is learned end-to-end, together with the rest of the RNN in a very efficient way.",3. Tensor-Train RNN,0,[0]
A Tensor-Train Factorization (TTF) is a tensor factorization model that can scale to an arbitrary number of dimensions.,3.1. Tensor-Train Factorization,0,[0]
"Assuming a d-dimensional target tensor of the form A ∈ Rp1×p2×...×pd , it can be factorized in form of:
Â(l1, l2, ..., ld) TTF = G1(l1) G2(l2) ... Gd(ld)",3.1. Tensor-Train Factorization,0,[0]
"(1)
where
Gk ∈ Rpk×rk−1×rk , lk ∈",3.1. Tensor-Train Factorization,0,[0]
"[1, pk] ∀k ∈",3.1. Tensor-Train Factorization,0,[0]
"[1, d] and r0 = rd = 1.
(2)
As Eq. 1 suggests, each entry in the target tensor is represented as a sequence of matrix multiplications.",3.1. Tensor-Train Factorization,0,[0]
The set of tensors {Gk}dk=1 are usually called core-tensors.,3.1. Tensor-Train Factorization,0,[0]
"The complexity of the TTF is determined by the ranks [r0, r1, ..., rd].",3.1. Tensor-Train Factorization,0,[0]
We demonstrate this calculation also in Fig. 1.,3.1. Tensor-Train Factorization,0,[0]
"Please note that the dimensions and core-tensors are indexed from 1 to d while the rank index starts from 0; also note that the first and last ranks are both restricted to be 1, which implies that the first and last core tensors can be seen as matrices so that the outcome of the chain of multiplications in Eq. 1 is always a scalar.
",3.1. Tensor-Train Factorization,0,[0]
If one imposes the constraint that each integer pk as in Eq.,3.1. Tensor-Train Factorization,0,[0]
(1) can be factorized as pk = mk · nk ∀k ∈,3.1. Tensor-Train Factorization,0,[0]
"[1, d], and consequently reshapes each Gk into G∗k ∈ Rmk×nk×rk−1×rk , then each index lk in Eq.",3.1. Tensor-Train Factorization,0,[0]
"(1) and (2) can be uniquely rep-
resented with two indices (ik, jk), i.e.
ik = b lk nk c, jk",3.1. Tensor-Train Factorization,0,[0]
"= lk − nkb lk nk c, (3) so that Gk(lk) =",3.1. Tensor-Train Factorization,0,[0]
"G∗k(ik, jk) ∈ Rrk−1×rk .",3.1. Tensor-Train Factorization,0,[0]
"(4)
Correspondingly, the factorization for the tensor A ∈ R(m1·n1)×(m2·n2)×...×(md·nd) can be rewritten equivalently to Eq.(1):
Â((i1, j1), (i2, j2), ..., (id, jd))",3.1. Tensor-Train Factorization,0,[0]
"TTF = G∗1(i1, j1) G ∗ 2(i2, j2) ...",3.1. Tensor-Train Factorization,0,[0]
"G ∗ d(id, jd).
",3.1. Tensor-Train Factorization,0,[0]
"(5)
This double index trick (Novikov et al., 2015) enables the factorizing of weight matrices in a feed-forward layer as described next.",3.1. Tensor-Train Factorization,0,[0]
Here we factorize the weight matrix W of a fullyconnected feed-forward layer denoted in ŷ = Wx+,3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"b.
First we rewrite this layer in an equivalent way with scalars as:
ŷ(j) = M∑ i=1",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"W (i, j) · x(i) + b(j) ∀j ∈",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"[1, N ] and with x ∈ RM , y ∈ RN .
(6)
Then, if we assume that M = ∏d
k=1mk, N =∏d k=1 nk i.e. both M and N can be factorized into two integer arrays of the same length, then we can reshape the input vector x and the output vector ŷ into two tensors with the same number of dimensions: X ∈ Rm1×m2×...×md ,Y ∈ Rn1×n2×...×nd , and the mapping function Rm1×m2×...×md → Rn1×n2×...×nd can be written as:
Ŷ(j1, j2, ..., jd)
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
= m1∑ i1=1 m2∑ i2=1 ...,3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"md∑ id=1 W((i1, j1), (i2, j2), ..., (id, jd))·
X (i1, i2, ..., id) +B(j1, j2, ..., jd).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"(7)
Note that Eq. (6) can be seen as a special case of Eq.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
(7) with d = 1.,3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"The d-dimensional double-indexed tensor of weights W in Eq.(7) can be replaced by its TTF representation:
Ŵ((i1, j1), (i2, j2), ..., (id, jd))",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"TTF = G∗1(i1, j1) G ∗ 2(i2, j2) ...",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"G ∗ d(id, jd).
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"(8)
Now instead of explicitly storing the full tensor W of size∏d k=1mk·nk =M ·N , we only store its TT-format, i.e., the
set of low-rank core tensors {Gk}dk=1 of size ∑d
k=1mk · nk · rk−1 · rk, which can approximately reconstruct W .
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"The forward pass complexity (Novikov et al., 2015) for one scalar in the output vector indexed by (j1, j2, ..., jd) turns out to beO(d ·m̃ · r̃2).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"Since one needs an iteration through all such tuples, yielding O(ñd), the total complexity for one Feed-Forward-Pass can be expressed as O(d · m̃ · r̃2 · ñd), where m̃ = maxk∈[1,d]mk, ñ = maxk∈[1,d] nk, r̃ = maxk∈[1,d] rk.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"This, however, would be O(M · N) for a fully-connected layer.
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"One could also compute the compression rate as the ratio between the number of weights in a fully connected layer and that in its compressed form as:
r = ∑d",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"k=1mknkrk−1rk∏d
k=1mknk .",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"(9)
For instance, an RGB frame of size 160 × 120 × 3 implies an input vector of length 57,600.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"With a hidden layer of size, say, 256 one would need a weight matrix consisting of 14,745,600 free parameters.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"On the other hand, a TTL that factorizes the input dimension with 8×20×20×18 is able to represent this matrix using 2,976 parameters with a TT-rank of 4, or 4,520 parameters with a TT-rank of 5 (Tab. 1), yielding compression rates of 2.0e-4 and 3.1e-4, respectively.
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"For the rest of the paper, we term a fully-connected layer in form of ŷ = Wx + b, whose weight matrix W is factorized with TTF, a Tensor-Train Layer (TTL) and use the notation
ŷ =",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"TTL(W , b,x), or TTL(W ,x) (10)
where in the second case no bias is required.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"Please also note that, in contrast to (Lebedev et al., 2014) where the weight tensor is firstly factorized using non-linear LeastSquare method and then fine-tuned with Back-Propagation, the TTL is always trained end-to-end.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
"For details on the gradients calculations please refer to Section 5 in (Novikov et al., 2015).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,0,[0]
In this work we investigate the challenge of modeling highdimensional sequential data with RNNs.,3.3. Tensor-Train RNN,0,[0]
"For this reason,
we factorize the matrix mapping from the input to the hidden layer with a TTL.",3.3. Tensor-Train RNN,0,[0]
"For an Simple RNN (SRNN), which is also known as the Elman Network, this mapping is realized as a vector-matrix multiplication, whilst in case of LSTM and GRU, we consider the matrices that map from the input vector to the gating units:",3.3. Tensor-Train RNN,0,[0]
r[t],TT-GRU:,0,[0]
"= σ(TTL(W r,x[t])",TT-GRU:,0,[0]
"+U rh[t−1] + br)
z[t] = σ(TTL(W z,x[t])",TT-GRU:,0,[0]
"+Uzh[t−1] + bz)
d[t] = tanh(TTL(W d,x[t])",TT-GRU:,0,[0]
"+Ud(r[t] ◦ h[t−1]))
h[t] = (1− z[t]) ◦",TT-GRU:,0,[0]
"h[t−1] + z[t] ◦ d[t],
(11)",TT-GRU:,0,[0]
"k[t] = σ(TTL(W k,x[t])",TT-LSTM:,0,[0]
"+Ukh[t−1] + bk)
",TT-LSTM:,0,[0]
f,TT-LSTM:,0,[0]
"[t] = σ(TTL(W f ,x[t]) +Ufh[t−1] + bf )
",TT-LSTM:,0,[0]
"o[t] = σ(TTL(W o,x[t])",TT-LSTM:,0,[0]
"+Uoh[t−1] + bo)
g[t] = tanh(TTL(W g,x[t])",TT-LSTM:,0,[0]
"+Ugh[t−1] + bg)
c[t] = f",TT-LSTM:,0,[0]
"[t] ◦ c[t−1] + k[t] ◦ g[t]
h[t] = o[t] ◦ tanh(c[t]).
(12)
One can see that LSTM and GRU require 4 and 3 TTLs, respectively, one for each of the gating units.",TT-LSTM:,0,[0]
"Instead of calculating these TTLs successively (which we call vanilla TT-LSTM and vanilla TT-GRU), we increase n1 —the first 1 of the factors that form the output size N = ∏d k=1 nk in a TTL— by a factor of 4 or 3, and concatenate all the gates as one output tensor, thus parallelizing the computation.",TT-LSTM:,0,[0]
"This trick, inspired by the implementation of standard LSTM and GRU in (Chollet, 2015), can further reduce the number of parameters, where the concatenation is actually participating in the tensorization.",TT-LSTM:,0,[0]
"The compression rate for the input-to-hidden weight matrix W now becomes
r∗ =
∑d k=1mknkrk−1rk",TT-LSTM:,0,[0]
"+ (c− 1)(m1n1r0r1)
",TT-LSTM:,0,[0]
"c · ∏d k=1mknk (13)
where c = 4 in case of LSTM and 3 in case of GRU,
and one can show that r∗ is always smaller than r as in Eq. 9.",TT-LSTM:,0,[0]
"For the former numerical example of a input frame size 160×120×3, a vanilla TT-LSTM would simply require 4 times as many parameters as a TTL, which would be 11,904 for rank 4 and 18,080 for rank 5.",TT-LSTM:,0,[0]
"Applying this trick would, however, yield only 3,360 and 5,000 parameters for both ranks, respectively.",TT-LSTM:,0,[0]
"We cover other possible settings of this numerical example in Tab. 1.
",TT-LSTM:,0,[0]
"Finally to construct the classification model, we denote the i-th sequence of variable length Ti as a set of vectors
1Though in theory one could of course choose any nk.
{x[t]i } Ti t=1 with x",TT-LSTM:,0,[0]
[t],TT-LSTM:,0,[0]
i ∈ RM∀t.,TT-LSTM:,0,[0]
For video data each x [t] i would be an RGB frame of 3 dimensions.,TT-LSTM:,0,[0]
"For the sake of simplicity we denote an RNN model, either with or without TTL, with a function f(·):
h",TT-LSTM:,0,[0]
[Ti] i = f({x,TT-LSTM:,0,[0]
"[t] i } Ti t=1),",TT-LSTM:,0,[0]
where h,TT-LSTM:,0,[0]
"[Ti] i ∈ R N , (14)
which outputs the last hidden layer vector h[Ti]i out of a sequential input of variable length.",TT-LSTM:,0,[0]
"This vector can be interpreted as a latent representation of the whole sequence, on top of which a parameterized classifier φ(·) with either softmax or logistic activation produces the distribution over all J classes:
P(yi = 1|{x",TT-LSTM:,0,[0]
[t] i } Ti t=1) = φ(h,TT-LSTM:,0,[0]
"[Ti] i )
",TT-LSTM:,0,[0]
= φ(f(x [t] i } Ti t=1)) ∈,TT-LSTM:,0,[0]
"[0, 1]J ,
(15)
The model is also illustrated in Fig. 2:",TT-LSTM:,0,[0]
"In the following, we present our experiments conducted on three large video datasets.",4. Experiments,0,[0]
"These empirical results demonstrate that the integration of the Tensor-Train Layer in plain RNN architectures such as a tensorized LSTM or GRU boosts the classification quality of these models tremendously when directly exposed to high-dimensional input data, such as video data.",4. Experiments,0,[0]
"In addition, even though the plain architectures are of very simple nature and very low complexity opposed to the state-of-the-art solutions on these datasets, it turns out that the integration of the Tensor-Train Layer alone makes these simple networks very competitive to the state-of-the-art, reaching second best results in all cases.
",4. Experiments,0,[0]
"UCF11 Data (Liu et al., 2009)",4. Experiments,0,[0]
We first conduct experiments on the UCF11 – earlier known as the YouTube Action Dataset.,4. Experiments,0,[0]
"It contains in total 1600 video clips belonging to 11 classes that summarize the human action visible in each video clip such as basketball shooting, biking, diving etc..",4. Experiments,0,[0]
"These videos originate from YouTube and have natural background (’in the
wild’) and a resolution of 320 × 240.",4. Experiments,0,[0]
"We generate a sequence of RGB frames of size 160 × 120 from each clip at an fps(frame per second) of 24, corresponding to the standard value in film and television production.",4. Experiments,0,[0]
"The lengths of frame sequences vary therefore between 204 to 1492 with an average of 483.7.
",4. Experiments,0,[0]
"For both the TT-GRUs and TT-LSTMs the input dimension at each time step is 160 × 120 × 3 = 57600 which is factorized as 8 × 20 × 20 × 18, the hidden layer is chosen to be 4 × 4 × 4 × 4 = 256 and the Tensor-Train ranks are [1, 4, 4, 4, 1].",4. Experiments,0,[0]
"A fully-connected layer for such a mapping would have required 14,745,600 parameters to learn, while the input-to-hidden layer in TT-GRU and TT-LSTM consist of only 3,360 and 3,232, respectively.
",4. Experiments,0,[0]
As the first baseline model we sample 6 random frames in ascending order.,4. Experiments,0,[0]
"The model is a simple Multilayer Perceptron (MLP) with two layers of weight matrices, the first of which being a TTL.",4. Experiments,0,[0]
The input is the concatenation of all 6 flattened frames and the hidden layer is of the same size as the hidden layer in TT-RNNs.,4. Experiments,0,[0]
We term this model as Tensor-Train Multilayer Perceptron (TT-MLP) for the rest of the paper.,4. Experiments,0,[0]
As the second baseline model we use plain GRUs and LSTMs that have the same size of hidden layer as their TT pendants.,4. Experiments,0,[0]
"We follow (Liu et al., 2013) and perform for each experimental setting a 5-fold cross validation with mutual exclusive data splits.",4. Experiments,0,[0]
"The mean and standard deviation of the prediction accuracy scores are reported in Tab. 2.
",4. Experiments,0,[0]
The standard LSTM and GRU do not show large improvements compared with the TT-MLP model.,4. Experiments,0,[0]
"The TT-LSTM and TT-GRU, however, do not only compress the weight matrix from over 40 millions to 3 thousands, but also significantly improve the classification accuracy.",4. Experiments,0,[0]
It seems that plain LSTM and GRU are not adequate to model such high-dimensional sequential data because of the large weight matrix from input to hidden layer.,4. Experiments,0,[0]
"Compared to some latest state-of-the-art performances in Tab. 3, our model —simple as it is— shows accuracy scores second to (Sharma et al., 2015), which uses pre-trained GoogLeNet CNNs plus 3-fold stacked LSTM with attention mechanism.",4. Experiments,0,[0]
"Please note that a GoogLeNet CNN alone consists of over 6 million parameters (Szegedy et al., 2015).",4. Experiments,0,[0]
"In term of runtime, the plain GRU and LSTM took on average more than 8 and 10 days to train, respectively; while the TTGRU and TT-LSTM both approximately 2 days.",4. Experiments,0,[0]
Therefore please note the TTL reduces the training time by a factor of 4 to 5 on these commodity hardwares.,4. Experiments,0,[0]
"The Hollywood2 dataset contains video clips from 69 movies, from which 33 movies serve as training set and 36 movies as test set.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"From these movies 823 training clips and 884 test clips are generated and each clip is assigned one or multiple of 12 action labels such as answering the phone, driving a car, eating or fighting a person.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
This data set is much more realistic and challenging since the same action could be performed in totally different style in front of different background in different movies.,"Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"Furthermore, there are often montages, camera movements and zooming within a single clip.
","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"The original frame sizes of the videos vary, but based on the majority of the clips we generate frames of size 234 × 100, which corresponds to the Anamorphic Format, at fps of 12.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"The length of training sequences varies from 29 to 1079 with an average of 134.8; while the length of test sequences varies from 30 to 1496 frames with an average of 143.3.
","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"The input dimension at each time step, being 234× 100× 3 = 70200, is factorized as 10× 18× 13× 30.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"The hidden layer is still 4 × 4 × 4 × 4 = 256 and the Tensor-Train ranks are [1, 4, 4, 4, 1].","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"Since each clip might have more
than one label (multi-class multi-label problem) we implement a logistic activated classifier for each class on top of the last hidden layer.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"Following (Marszałek et al., 2009)","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"we measure the performances using Mean Average Precision across all classes, which corresponds to the Area-UnderPrecision-Recall-Curve.
","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"As before we conduct experiments on this dataset using the plain LSTM, GRU and their respective TT modifications.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"The results are presented in in Tab. 4 and state-of-the-art in Tab. 5.
","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"(Fernando et al., 2015) and (Jain et al., 2013) use improved trajectory features with Fisher encoding (Wang & Schmid, 2013) and Histogram of Optical Flow (HOF) features (Laptev et al., 2008), respectively, and achieve so far the best score.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"(Sharma et al., 2015) and (Fernando & Gould, 2016) provide best scores achieved with Neural Network models but only the latter applies end-toend training.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"To this end, the TT-LSTM model provides the second best score in general and the best score with Neural Network models, even though it merely replaces the input-to-hidden mapping with a TTL.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"Please note the large difference between the plain LSTM/GRU and the TT-
LSTM/GRU, which highlights the significant performance improvements the Tensor-Train Layer contributes to the RNN models.
","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"It is also to note that, although the plain LSTM and GRU consist of up to approximately 23K as many parameters as their TT modifications do, the training time does not reflect such discrepancy due to the good parallelization power of GPUs.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"However, the obvious difference in their training qualities confirms that training larger models may require larger amounts of data.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
"In such cases, powerful hardwares are no guarantee for successful training.","Hollywood2 Data (Marszałek et al., 2009)",0,[0]
This dataset consists of 1910 Youtube video clips of 47 prominent individuals such as movie stars and politicians.,"Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"In the simplest cases, where the face of the subject is visible as a long take, a mere frame level classification would suffice.","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"The major challenge, however, is posed by the fact that some videos involve zooming and/or changing the angle of view.","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"In such cases a single frame may not provide enough information for the classification task and we believe it is advantageous to apply RNN models that can aggregate frame level information over time.
","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
The original frame sizes of the videos vary but based on the majority of the clips we generate frames of size 160 × 120 at fps of 12.,"Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
The retrieved sequences have lengths varying from 2 to 85 with an average of 39.9.,"Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"The input dimension at each time step is 160 × 120 × 3 = 57600 which is factorized as 4× 20× 20× 36, the hidden layer is again 4× 4× 4× 4 = 256 and the Tensor-Train ranks are
[1, 4, 4, 4, 1].
","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"As expected, the baseline of TT-MLP model tends to perform well on the simpler video clips where the position of the face remains less changed over time, and can even outperform the plain GRU and LSTM.","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"The TT-GRU and TT-LSTM, on the other hand, provide accuracy very close to the best state-of-the-art model (Tab. 7) using Mean Sequence Sparse Representation-based Classification (Ortiz et al., 2013) as feature extraction.","Youtube Celebrities Face Data (Kim et al., 2008)",0,[0]
"We applied 0.25 Dropout (Srivastava et al., 2014) for both input-to-hidden and hidden-to-hidden mappings in plain GRU and LSTM as well as their respective TT modifications; and 0.01 ridge regularization for the single-layered classifier.",Experimental Settings,0,[0]
"The models were implemented in Theano (Bastien et al., 2012) and deployed in Keras (Chollet, 2015).",Experimental Settings,0,[0]
"We used the Adam (Kingma & Ba, 2014) step rule for the updates with an initial learning rate 0.001.",Experimental Settings,0,[0]
"We proposed to integrate Tensor-Train Layers into Recurrent Neural Network models including LSTM and GRU, which enables them to be trained end-to-end on highdimensional sequential data.",5. Conclusions and Future Work,0,[0]
We tested such integration on three large-scale realistic video datasets.,5. Conclusions and Future Work,0,[0]
"In comparison to the plain RNNs, which performed very poorly on these video datasets, we could empirically show that the integration of the Tensor-Train Layer alone significantly improves
the modeling performances.",5. Conclusions and Future Work,0,[0]
"In contrast to related works that heavily rely on deep and large CNNs, one advantage of our classification model is that it is simple and lightweight, reducing the number of free parameters from tens of millions to thousands.",5. Conclusions and Future Work,0,[0]
This would make it possible to train and deploy such models on commodity hardware and mobile devices.,5. Conclusions and Future Work,0,[0]
"On the other hand, with significantly less free parameters, such tensorized models can be expected to be trained with much less labeled data, which are quite expensive in the video domain.
",5. Conclusions and Future Work,0,[0]
"More importantly, we believe that our approach opens up a large number of possibilities to model high-dimensional sequential data such as videos using RNNs directly.",5. Conclusions and Future Work,0,[0]
"In spite of its success in modeling other sequential data such as natural language, music data etc., RNNs have not been applied to video data in a fully end-to-end fashion, presumably due to the large input-to-hidden weight mapping.",5. Conclusions and Future Work,0,[0]
"With TTRNNs that can directly consume video clips on the pixel level, many RNN-based architectures that are successful in other applications, such as NLP, can be transferred to modeling video data: one could implement an RNN autoencoder that can learn video representations similar to (Srivastava et al., 2015), an Encoder-Decoder Network (Cho et al., 2014) that can generate captions for videos similar to (Donahue et al., 2015), or an attention-based model that can learn on which frame to allocate the attention in order to improve the classification.
",5. Conclusions and Future Work,0,[0]
"We believe that the TT-RNN provides a fundamental building block that would enable the transfer of techniques from fields, where RNNs have been very successful, to fields that deal with very high-dimensional sequence data –where RNNs have failed in the past.
",5. Conclusions and Future Work,0,[0]
The source codes of our TT-RNN implementations and all the experiments in Sec. 4 are publicly available at https: //github.com/Tuyki/TT_RNN.,5. Conclusions and Future Work,0,[0]
"In addition, we also provide codes of unit tests, simulation studies as well as experiments performed on the HMDB51 dataset (Kuehne et al., 2011).",5. Conclusions and Future Work,0,[0]
The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing.,abstractText,0,[0]
"These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix.",abstractText,0,[0]
This may have prevented RNNs’ large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors.,abstractText,0,[0]
"To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves.",abstractText,0,[0]
"We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex.",abstractText,0,[0]
We believe that the proposed approach provides a novel and fundamental building block for modeling highdimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling highdimensional sequential data.,abstractText,0,[0]
Tensor-Train Recurrent Neural Networks for Video Classification,title,0,[0]
Property testing is the study of algorithms that query their input a small number of times and distinguish between whether their input satisfies a given property or is “far” from satisfying that property.,1. Introduction,0,[0]
"The quest for efficient testing algorithms was initiated by (Blum et al., 1993) and (Babai et al., 1991) and later explicitly formulated by (Rubinfeld & Sudan, 1996) and (Goldreich et al., 1998).",1. Introduction,0,[0]
"Property testing can be viewed as a relaxation of the traditional notion of a decision problem, where the relaxation is quantified in terms of a distance parameter.",1. Introduction,0,[0]
"There has been extensive work in this area over the last couple of decades; see, for instance, the surveys (Ron, 2008) and (Rubinfeld & Shapira, 2006) for some different perspectives.
",1. Introduction,0,[0]
"*Equal contribution 1Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India.",1. Introduction,0,[0]
Correspondence to:,1. Introduction,0,[0]
"Siddharth Barman <barman@iisc.ac.in>, Arnab Bhattacharyya <arnabb@iisc.ac.in>, Suprovat Ghoshal <suprovat@iisc.ac.in>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
",1. Introduction,0,[0]
"As evident from these surveys, research in property testing has largely focused on properties of combinatorial and algebraic structures, such as bipartiteness of graphs, linearity of Boolean functions on the hypercube, membership in errorcorrecting codes or representability of functions as concise Boolean formulae.",1. Introduction,0,[0]
"In this work, we study the question of testing properties of continuous structures, specifically properties of vectors and matrices over the reals.
",1. Introduction,0,[0]
Our computational model is a natural extension of the standard property testing framework by allowing queries to be linear measurements of the input.,1. Introduction,0,[0]
Let,1. Introduction,0,[0]
P ⊂,1. Introduction,0,[0]
Rd be a property of real vectors.,1. Introduction,0,[0]
Let dist :,1. Introduction,0,[0]
Rd → R>0 be a “distance” function such that dist(x) = 0 for all x ∈ P .,1. Introduction,0,[0]
"We say that an algorithm A is a tester for P with respect to dist and with parameters ε, δ > 0",1. Introduction,0,[0]
if for any input,1. Introduction,0,[0]
"y ∈ Rn, the algorithm",1. Introduction,0,[0]
"A observes My where M ∈ Rq×d is a randomized matrix and has the following guarantee:
(i)",1. Introduction,0,[0]
"If y ∈ P , PrM[A(My) accepts] > 1− δ.
(ii) If dist(y) > ε, PrM[A(My) accepts] 6 δ.
",1. Introduction,0,[0]
"We call each inner product between the rows of M and y a (linear) query, and the number of rows q = q(ε, δ) is the query complexity of the tester.",1. Introduction,0,[0]
The running time of the tester A is its running time on the outcome of its queries.,1. Introduction,0,[0]
"As typical in property testing, we do not count the time needed to evaluate the queries.",1. Introduction,0,[0]
If P ⊂ Rd×p is a property of real matrices with an associated distance function dist :,1. Introduction,0,[0]
"Rd×p → R>0, testing is defined similarly: given an input matrix Y ∈ Rd×p, the algorithm observes MY for a random matrix M ∈ Rq×d with analogous completeness and soundness properties.",1. Introduction,0,[0]
A linear projection of an input vector or matrix to a low-dimensional space is also called a linear sketch or a linear measurement.,1. Introduction,0,[0]
"The technique of obtaining small linear sketches of high-dimensional vectors has been used to great effect in algorithms for streaming (e.g., (Alon et al., 1996; McGregor, 2014)) and numerical linear algebra (see (Woodruff, 2014) for an excellent survey).",1. Introduction,0,[0]
"Because GPUs are specially designed to optimize matrix-vector computation, many modern optimization and learning algorithms work with linear sketches of their input.
",1. Introduction,0,[0]
"We focus on testing whether a vector is sparse with respect
to some basis.1",1. Introduction,0,[0]
A vector x is said to be k-sparse if it has at most k nonzero coordinates.,1. Introduction,0,[0]
Sparsity is a structural characteristic of signals of interest in a diverse range of applications.,1. Introduction,0,[0]
"It is a pervasive concept throughout modern statistics and machine learning, and algorithms to solve inverse problems under sparsity constraints are among the most successful stories of the optimization community (see the book (Hastie et al., 2015)).",1. Introduction,0,[0]
"The natural property testing question we consider is whether there exists a solution to a linear inverse problem under a sparsity constraint.
",1. Introduction,0,[0]
"There are two settings in which we investigate the sparsity testing problem.
",1. Introduction,0,[0]
"(a) In the first setting, the basis is not known in advance.",1. Introduction,0,[0]
"For input vectors y1,y2, . . .",1. Introduction,0,[0]
",yp ∈ Rd, the property to test is whether there exists a matrix A ∈ Rd×m and k-sparse unit vectors x1,x2, . .",1. Introduction,0,[0]
.xp,1. Introduction,0,[0]
∈,1. Introduction,0,[0]
Rm such that yi = Axi for all,1. Introduction,0,[0]
i ∈,1. Introduction,0,[0]
[p].,1. Introduction,0,[0]
Note that m is specified as a parameter and could be much larger than d (the overcomplete case).,1. Introduction,0,[0]
"In this setting, we restrict the unknown A to be a (ε, k)-RIP matrix which means that (1 − ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ for any ksparse x.",1. Introduction,0,[0]
"This is a standard assumption made in many related works (see Section 1.2 for details).
",1. Introduction,0,[0]
"In this setting, we design an efficient tester for this property that projects the inputs toO(ε−2 log p) dimensions and, informally speaking, rejects if for all (ε, k)-RIP matrices",1. Introduction,0,[0]
"A, there is some yi such that yi −Axi has large norm for all “approximately sparse” xi.
(b)",1. Introduction,0,[0]
"In the second setting, a design matrix A ∈ Rd×m is known explicitly, and the property to test is whether a given input vector y ∈ Rd equals Ax for a k-sparse vector x ∈ Rm.",1. Introduction,0,[0]
"For instance, A can be the Fourier basis or an overcomplete dictionary in an image processing application.",1. Introduction,0,[0]
"We approach this problem in full generality, without putting any restriction on the structure of A.
Informally, our main result in this setting is that for any design matrix A, there exists a tester projecting the input y toO(k logm) dimensions that rejects if y−Ax has large norm for any O(k)-sparse x.",1. Introduction,0,[0]
The running time of the tester is polynomial inm.,1. Introduction,0,[0]
"As we describe in Section 1.2, previous work in numerical linear algebra yields a tester with the same query complexity and with qualitatively similar soundness guarantees but which requires running time exponential in m or assumptions about the matrix A.
Remark 1.1 (Problem Formulation).",1. Introduction,0,[0]
"Note that the settings considered in the known and unknown design matrix settings
1With slight abuse of notation, we use the term basis to denote the set of columns of a design matrix.",1. Introduction,0,[0]
"The columns might not be linearly independent.
are quite different from each other.",1. Introduction,0,[0]
"In particular, for the known design setting, the input is a single vector.",1. Introduction,0,[0]
"However, given a single input vector y ∈ Rd, the analogous unknown design testing question would be moot, since one can always consider the vector y to be the design matrix A, in which it trivially admits a 1-sparse representation.",1. Introduction,0,[0]
"For the same reason, unknown design testing is interesting only when the number of vectors p exceeds m.
In both of the above tests, the measurement matrix is a random matrix with iid gaussian entries, chosen so as to preserve norms and certain other geometric properties upon dimensionality reduction.2 In particular, our testers are oblivious to the input.",1. Introduction,0,[0]
It is a very interesting open question as to whether non-oblivious testers can strengthen the above results.,1. Introduction,0,[0]
We now present our results more formally.,1.1. Our Results,0,[0]
"For integer m > 0, let Sm−1 = {x ∈",1.1. Our Results,0,[0]
"Rm : ‖x‖ = 1}, and let Spmk = {x ∈ Sm−1 : ‖x‖0 6 k}.3
Theorem 1.2 (Unknown Design Matrix).",1.1. Our Results,0,[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and p, such that (",1.1. Our Results,0,[0]
k/m)1/8,1.1. Our Results,0,[0]
<,1.1. Our Results,0,[0]
ε,1.1. Our Results,0,[0]
< 1100 and k > 10 log 1 ε .,1.1. Our Results,0,[0]
There exists a tester with query complexity O(ε−2 log (p/δ)),1.1. Our Results,0,[0]
"which, given as input vectors y1,y2, . . .",1.1. Our Results,0,[0]
",yp ∈ Rd, has the following behavior (where Y is the matrix having y1,y2, . . .",1.1. Our Results,0,[0]
",yp as columns):
– Completeness: If Y admits a decomposition Y = AX, where A ∈ Rd×m satisfies (ε, k)-RIP and X ∈ Rm×p with each column of X in Spmk , then the tester accepts with probability > 1− δ.
– Soundness:",1.1. Our Results,0,[0]
Suppose Y does not admit a decomposition Y = A(X + Z) +,1.1. Our Results,0,[0]
"W with
1.",1.1. Our Results,0,[0]
"The design matrix A ∈ Rd×m being (ε, k)-RIP, with ‖ai‖ = 1 for every",1.1. Our Results,0,[0]
i ∈,1.1. Our Results,0,[0]
[m].,1.1. Our Results,0,[0]
2,1.1. Our Results,0,[0]
"The coefficient matrix X ∈ Rm×p being column wise `-sparse, where ` = O(k/ε4).
",1.1. Our Results,0,[0]
3,1.1. Our Results,0,[0]
The error matrices Z ∈,1.1. Our Results,0,[0]
Rm×p and W ∈,1.1. Our Results,0,[0]
Rd×p,1.1. Our Results,0,[0]
"satisfying
‖zi‖∞ 6 ε2, ‖wi‖2 6 O(ε1/4) for all i ∈",1.1. Our Results,0,[0]
"[p].
Then the tester rejects with probability > 1− δ.",1.1. Our Results,0,[0]
"2If evaluating the queries efficiently was an objective, one could also use sparse dimension reduction matrices (Dasgupta et al., 2010; Kane & Nelson, 2014; Bourgain et al., 2015), but we do not pursue this direction here.
3Here, ‖x‖0 denotes the the sparsity of the vector, ‖x‖0 := |{i ∈",1.1. Our Results,0,[0]
[m] | xi 6= 0}|.,1.1. Our Results,0,[0]
"Without any subscript, ‖ · ‖ denotes the `2-norm: ‖x‖ := √∑ i x 2 i .
",1.1. Our Results,0,[0]
"The contrapositive of the soundness guarantee from the above theorem states that if the tester accepts, then matrix Y admits a factorization of the form Y = A(X+Z)+W, with error matrices Z and W having `∞ and `2 error bounds.",1.1. Our Results,0,[0]
"The matrix X+Z is a sparse matrix with `∞-based thresholding, and W is an additive `2-error term.4
Theorem 1.3 (Known Design Matrix).",1.1. Our Results,0,[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m such that ‖ai‖ = 1 for every i ∈",1.1. Our Results,0,[0]
[m].,1.1. Our Results,0,[0]
There exists a tester with query complexity O(kε−2 log(m/δ)) that behaves as follows for an input vector y ∈,1.1. Our Results,0,[0]
"Rd:
– Completeness:",1.1. Our Results,0,[0]
"If y = Ax for some x ∈ Spmk , then the tester accepts with probability 1.
– Soundness: If ‖Ax− y‖2 > ε for every x : ‖x‖0 6 K, then the tester rejects with probability > 1",1.1. Our Results,0,[0]
− δ.,1.1. Our Results,0,[0]
"Here, K = O(k/ε2).
",1.1. Our Results,0,[0]
"The running time of the tester is poly(m, k, 1/ε).
",1.1. Our Results,0,[0]
"A different way of stating the result is that the tester, using O(kε−2 log(m/δ)) linear queries, accepts with probability 1 if y = Ax for a k-sparse x ∈",1.1. Our Results,0,[0]
Rm and rejects with probability 1− δ,1.1. Our Results,0,[0]
"if ‖Ax− y‖ > ε‖x‖ for every O(k/ε2)sparse x. To complement this result, we show that a better tradeoff between the sparsity and reconstruction error is likely to be impossible.
",1.1. Our Results,0,[0]
Theorem 1.4 (Hardness).,1.1. Our Results,0,[0]
"Assume SAT does not have nO(log logn)-time algorithms, and let η be any constant less than 1.",1.1. Our Results,0,[0]
"Then, there does not exist a polynomial time algorithm that, given input A ∈ Rd×m (where ‖ai‖ = 1 for every i ∈",1.1. Our Results,0,[0]
"[m]), y ∈ Rd and ε > 0, distinguishes with constant probability between the following two cases: (i) y",1.1. Our Results,0,[0]
#NAME?,1.1. Our Results,0,[0]
"−Ax‖ > ε‖x‖η for every (k/ε2)-sparse x.
Note that the above hardness applies to any polynomial time algorithm, not just sketching algorithms.
",1.1. Our Results,0,[0]
We also give tolerant variants of these testers (Theorems H.1 and H.2) which can handle bounded noise for the completeness case.,1.1. Our Results,0,[0]
"Moreover, the tester for the known design case can be converted into a new sketching algorithm for sparse recovery (Theorem D.1).
",1.1. Our Results,0,[0]
"Finally, we also give an algorithm for testing dimensionality, which is based on similar techniques.
",1.1. Our Results,0,[0]
Theorem 1.5 (Testing Dimensionality).,1.1. Our Results,0,[0]
"Fix ε, δ ∈ (0, 1), positive integers d, k and p, where k > 10ε2 log d. There exists a tester with query complexity O(p log δ−1), which
4Theorem 1.2 can be restated in terms of incoherent (instead of RIP) design matrices as well.",1.1. Our Results,0,[0]
This follows from the fact that the incoherence and RIP constants of a matrix are order-wise equivalent.,1.1. Our Results,0,[0]
"This observation is formalized in Appendix F.
gives as input vectors y1, . . .",1.1. Our Results,0,[0]
",yp ⊂",1.1. Our Results,0,[0]
"Sd−1, has the following behavior:
– Completeness: If rank(Y ) 6 k, then the tester accepts with probability > 1− δ.
– Soundness: If rankε(Y )",1.1. Our Results,0,[0]
"> k′, then the tester rejects with probability > 1− δ.",1.1. Our Results,0,[0]
"Here, k′ = 20k/ε2
The soundness criteria in the above Theorem is stated in terms of the ε-approximate rank of a matrix (see Definition E.1).",1.1. Our Results,0,[0]
"This is a well-studied relaxation of the standard definition of rank, and has applications in approximation algorithms, communication complexity and learning theory (see (Alon et al., 2013) and references therein).",1.1. Our Results,0,[0]
"Although, to the best of our knowledge, the testing problems we consider have not been explicitly investigated before, there are several related areas of study that frame our results in their proper context.
",1.2. Related Work,0,[0]
Unknown Design setting.,1.2. Related Work,0,[0]
"In the setting of the unknown design matrix, the question of recovering the design matrix and the sparse representation (as opposed to our problem of testing their existence) is called the dictionary learning or sparse coding problem.",1.2. Related Work,0,[0]
"The first work to give a dictionary learning algorithm with provable guarantees was (Spielman et al., 2012) where the dictionary was restricted to be square.",1.2. Related Work,0,[0]
"For the more common overcomplete setting, (Arora et al., 2014) and (Agarwal et al., 2014) independently gave algorithms with provable guarantees for dictionaries satisfying incoherence and RIP respectively.",1.2. Related Work,0,[0]
All of these (as well as other more recent) works assume distributions from which the input samples are generated in an i.i.d fashion.,1.2. Related Work,0,[0]
"In contrast, our work is in the agnostic setting and hence, is incomparable with these results.
",1.2. Related Work,0,[0]
"It is known that the dictionary learning problem is NP-hard, even for square dictionaries (Razaviyayn et al., 2014; Tillmann, 2015).",1.2. Related Work,0,[0]
"In fact, (Tillmann, 2015) shows that unless SAT has a quasi-polynomial time algorithm, it is impossible, given Y ∈ Rd×p, to approximate in polynomial time the minimum k upto a factor 2log
1−ε d (for any ε > 0) such that Y = AX where each column of X ∈ Rd×p is k-sparse.",1.2. Related Work,0,[0]
"This motivates our bicriteria relaxation of both the sparsity as well as the additive error in Theorem 1.2.
",1.2. Related Work,0,[0]
Known Design setting.,1.2. Related Work,0,[0]
Some results about testing sparsity in the known design setting are implicit in recent work on streaming algorithms and oblivious subspace embeddings.,1.2. Related Work,0,[0]
"Of particular interest are the following results:
Theorem 1.6 (Implicit in (Kane et al., 2010)).",1.2. Related Work,0,[0]
"Fix ε ∈ (0, 1), positive integers m, k and an invertible matrix A ∈
Rm×m.",1.2. Related Work,0,[0]
"Then, there is a tester with query complexity O(ε−2 log(m)) that, for an input y ∈ Rm, accepts with probability at least 2/3 if y = Ax for some k-sparse x ∈ Zm, and rejects with probability 2/3 if y 6=",1.2. Related Work,0,[0]
Ax for all (1 + ε)k-sparse x ∈ Zm.,1.2. Related Work,0,[0]
"The running time of the algorithm is poly(m, 1/ε).",1.2. Related Work,0,[0]
"Theorem 1.7 (Implicit in prior work, see (Woodruff, 2014)).",1.2. Related Work,0,[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m.",1.2. Related Work,0,[0]
"Then, there is a tester with query complexity O(kε−2 log(m/δ))",1.2. Related Work,0,[0]
"that, for an input vector y ∈ Rd, accepts with probability 1 if y = Ax for some k-sparse x and rejects with probability at least 1− δ",1.2. Related Work,0,[0]
if ‖y −Ax‖,1.2. Related Work,0,[0]
> ε for all k-sparse x.,1.2. Related Work,0,[0]
"The running time of the tester is the time required to solve the following optimization problem:
x̂ = arg min x′∈K ‖SAx′",1.2. Related Work,0,[0]
− Sy‖ = arg min x′∈K ‖S(Ax′,1.2. Related Work,0,[0]
"− y)‖
(1) where S ∈ Rq×d is a random sketch matrix (where q d) and K = {x : ‖x‖0 6 k}
Detailed descriptions of the algorithms and proof sketches for the above Theorems are given in Section B.4.",1.2. Related Work,0,[0]
The algorithms from the above theorems come with significant limitations.,1.2. Related Work,0,[0]
"In particular, the guarantees for Theorem 1.6 hold only when the design matrix is invertible.",1.2. Related Work,0,[0]
"On the other hand, the running time for the algorithm in Theorem 1.7 is the cost of solving the optimization problem in Equation (1), which is known to be NP-hard for general matrices.
",1.2. Related Work,0,[0]
"The problem of testing sparsity has also been studied in non-sketching settings as well, where the algorithm is allowed access to the entire input.",1.2. Related Work,0,[0]
"In particular, (Natarajan, 1995) gave a bicriteria-approximation algorithm, where the blowup in the sparsity is proportional to ‖A†‖22 (which can be large if A is ill conditioned).
",1.2. Related Work,0,[0]
Testing Dimensionality.,1.2. Related Work,0,[0]
"In (Czumaj et al., 2000), some problems in computational geometry were studied from the property testing perspective, but the problems involved only discrete structures.",1.2. Related Work,0,[0]
"(Krauthgamer & Sasson, 2003) studied the problem of testing dimensionality, but their notion of farness from being low-dimensional is different from ours5.",1.2. Related Work,0,[0]
"(Chierichetti et al., 2017) gave approximation algorithms for computing approximate rank of the matrix, in the setting where the algorithms have full access to the input.",1.2. Related Work,0,[0]
A standard approach to designing a testing algorithm for a property P is the following: we identify an alternative property P ′,1.3. Discussion,0,[0]
"which can be tested efficiently and exactly, while satisfying the following:
5In their setup, a sequence of vectors y1, . . .",1.3. Discussion,0,[0]
",yp is ε-far from being d-dimensional if at least εp vectors need to be removed to make it be of dimension d
(i) Completeness:",1.3. Discussion,0,[0]
"If an instance satisfies P , then it satisfies P ′.
(ii) Soundness:",1.3. Discussion,0,[0]
"If an instance satisfies P ′, the it is close to satisfying P .
",1.3. Discussion,0,[0]
"In other words, we reduce the property testing problem to that of finding a efficiently testable property P ′, which can be interpreted as a surrogate for property P .",1.3. Discussion,0,[0]
"The inherent geometric nature of the problems looked at in this paper motivate us to look for P ′s which are based around convex geometry and high dimensional probability.
",1.3. Discussion,0,[0]
"For the unknown design setting, we are intuitively looking for a P ′",1.3. Discussion,0,[0]
"based on a quantity ω that robustly captures sparsity and is easily computable using linear queries, in the sense that ω is small when the input vectors have a sparse coding and large when they are “far” from any sparse coding.",1.3. Discussion,0,[0]
"Moreover, ω needs to be invariant with respect to isometries and nearly invariant with respect to near-isometries.",1.3. Discussion,0,[0]
"A natural and widely-used measure of structure that satisfies the above mentioned properties is the gaussian width.
",1.3. Discussion,0,[0]
Definition 1.8.,1.3. Discussion,0,[0]
"The gaussian width of a set S ⊆ Rd is: ω(S) = Eg[supv∈S〈g,v〉] where g ∈ Rd is a random vector drawn from N(0, 1)d, i.e., a vector of independent standard normal variables.
",1.3. Discussion,0,[0]
The gaussian width of S measures how well on average the vectors in S correlate with a randomly chosen direction.,1.3. Discussion,0,[0]
It is invariant under orthogonal transformations of S as the distribution of g is spherically symmetric.,1.3. Discussion,0,[0]
"It is a well-studied quantity in high-dimensional geometry ((Vershynin, 2015; Mendelson & Vershynin, 2002)), optimization ((Chandrasekaran et al., 2012; Amelunxen et al., 2013)) and statistical learning theory ((Bartlett & Mendelson, 2002)).",1.3. Discussion,0,[0]
"The following bounds are well-known.
",1.3. Discussion,0,[0]
"Lemma 1.9 (See, for example, (Rudelson & Vershynin, 2008; Vershynin, 2015)).
",1.3. Discussion,0,[0]
(i),1.3. Discussion,0,[0]
"If S is a finite subset of Sd−1, then ω(S) 6 √ 2 log |S|.
(ii) ω(Sd−1) 6 √ d
(iii) If S ⊆ Sd−1 is of dimension k, then ω(S) 6 √ k.
(iv) ω(Spdk) 6 2 √ 3k log(d/k) when d/k > 2",1.3. Discussion,0,[0]
"and k > 4.
",1.3. Discussion,0,[0]
"In the context of Theorems 1.2 and 1.5, one can observe that whenever a given set satisfies sparsity or dimensionality constraints, the gaussian width of such sets are small (points (iii) and (iv) from the above Lemma).",1.3. Discussion,0,[0]
"Therefore, one can hope to test dimensionality or sparsity by computing an empirical estimate of the gaussian width and comparing the estimate to the results in Lemma 1.9.",1.3. Discussion,0,[0]
"While completeness of such testers would follow directly from concentration of measure, establishing soundness would require us to show
that approximate converses of points (iii) and (iv) hold as well i.e., whenever the gaussian width of the set S is small, it can be approximated by sets which are approximately sparse in some design matrix (or have low rank).
",1.3. Discussion,0,[0]
"For the soundness direction of Theorem 1.2, the above arguments are made precise using Lemma 3.3 and Theorem 3.2, which show that small gaussian width sets can be approximated by random projections of sparse vectors and vectors with small `∞-norm.",1.3. Discussion,0,[0]
"For Theorem 1.5, we use lemma E.2 which shows that sets with small gaussian width have small approximate rank.
",1.3. Discussion,0,[0]
"For the known design setting, we are looking for aP ′, which would ensure that if a given point y ∈ Rd satisfies P ′, then it is close to having a sparse representation in the matrix A. Towards this end, the approximate Carathéodory’s theorem states that if a point y ∈ Rd belonging to the convex-hull of A, then it is close to another point which admits a sparse representation.",1.3. Discussion,0,[0]
"On the other hand, if a unit vector x ∈ Sd−1 ∩Rd+ were k-sparse to begin with , then it can be seen that the corresponding y = Ax would belong to the convex hull of √ k · A.",1.3. Discussion,0,[0]
"These observations taken together, seem to suggest that one can take P ′ to be membership in the convex-hull of √",1.3. Discussion,0,[0]
k ·A. This intuition is made precise in the analysis of the tester in Section 4.,1.3. Discussion,0,[0]
Section 2 introduces notations and preliminaries used in the rest of the paper.,1.4. Organization,0,[0]
"In Sections 3 and 4, we design and analyze the testers for the unknown and known basis setting respectively.",1.4. Organization,0,[0]
Section 5 contains empirical results which supplement Section 3.,1.4. Organization,0,[0]
"In Section B we prove additional lemmas used in the proof of Theorem 3.2, and in Section A we prove Theorem 3.2.",1.4. Organization,0,[0]
"In Section C, we prove Theorem C.1, a stronger version of Theorem 1.4.",1.4. Organization,0,[0]
"In Section D, we show that Theorem 1.3 yields a sketching algorithm for sparse recovery.",1.4. Organization,0,[0]
"In Section E, we design and analyze the dimensionality tester.",1.4. Organization,0,[0]
"In Section G, we describe the results for testing sparsity in the known case implicit in previous work.",1.4. Organization,0,[0]
"Finally, in Section H, we give noise tolerant testers for the known and unknown basis settings.",1.4. Organization,0,[0]
"Given S ⊂ Rd, we shall use conv(S) to denote the convex hull of S. For a vector x ∈ Rd, we use ‖ · ‖p to denote its `p-norm, and we will drop the indexing when p = 2.",2. Preliminaries,0,[0]
"We denote the `2-distance of the point x to the set S by dist(x, S).",2. Preliminaries,0,[0]
"We recall the definition of ε-isometry:
Definition 2.1.",2. Preliminaries,0,[0]
Given sets S ⊂ Rm and S′ ⊂,2. Preliminaries,0,[0]
"Rn (for some m,n ∈ N), we say that S′ is an ε-isometry of S, if there exists a mapping ψ",2. Preliminaries,0,[0]
:,2. Preliminaries,0,[0]
"S 7→ S′ which satisfies the following
property:
∀x,y ∈ S : (1−ε)‖x−y‖ 6 ‖ψ(x)−ψ(y)‖ 6 (1+ε)‖x−y‖
For the unknown design setting, we shall require the notion of Restricted Isometry Property, which is defined as follows:
Definition 2.2 ((ε, k)-RIP).",2. Preliminaries,0,[0]
"A matrix A ∈ Rd×m satisfies (ε, k)-RIP, if for every x ∈ Spmk the following holds:
(1− ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ (2)
We use the following version of Gordon’s Theorem repeatedly in this work.
",2. Preliminaries,0,[0]
"Theorem 2.3 (Gordon’s Theorem (Gordon, 1985)).",2. Preliminaries,0,[0]
"Given S ⊂ SD−1 and a random gaussian matrix G ∼ 1√ d′ N(0, 1)d ′×D, we have
E G [ max x∈S ‖Gx‖2 ] 6 1 + ω(S)√ d′
It directly implies the following generalization of the Johnson-Lindenstrauss lemma.
",2. Preliminaries,0,[0]
Theorem 2.4 (Generalized Johnson-Lindenstrauss lemma).,2. Preliminaries,0,[0]
Let S ⊆ Sn−1.,2. Preliminaries,0,[0]
Then there exists linear transformation Φ :,2. Preliminaries,0,[0]
"Rn 7→ Rd′ , for d′ = O ( ω(S)2
ε2
) , such that Φ is an
ε-isometry on S. Moreover, Φ ∼ 1√ d′ N(0, 1)d ′×n is an ε-isometry on S with high probability.
",2. Preliminaries,0,[0]
"It can be easily verified that the quantity maxx∈S ‖Gx‖2 is 1-Lipschitz with respect to G. Therefore, using Gaussian concentration for Lipschitz functions, we get the following corollary :
Corollary 2.5.",2. Preliminaries,0,[0]
Let S and G be as in Theorem 2.3.,2. Preliminaries,0,[0]
"Then for all ε > 0, we have
Pr G ( max x∈S ‖Gx‖2 > 1 + ( 1 + ε )",2. Preliminaries,0,[0]
"ω(S)√ d′ ) 6 exp ( −O(εω(S))2
)",2. Preliminaries,0,[0]
"The following lemma gives concentration for the gaussian width:
Lemma 2.6 (Concentration on the gaussian width (Boucheron et al., 2013)).",2. Preliminaries,0,[0]
Let S ⊂ Rd.,2. Preliminaries,0,[0]
"Let W = supv∈S〈g,v〉 where g is drawn from N(0, 1)d.",2. Preliminaries,0,[0]
"Then:
Pr[|W −EW | > u] < 2e− u2 2σ2
",2. Preliminaries,0,[0]
where σ2 = supv∈S ( ‖v‖22 ),2. Preliminaries,0,[0]
.,2. Preliminaries,0,[0]
"Notice that the bound is dimension independent.
",2. Preliminaries,0,[0]
"Lastly, we shall use the `2-variant of the approximate Carathéodory’s Theorem:
Theorem 2.7.",2. Preliminaries,0,[0]
"(Theorem 0.1.2 (Vershynin, 2016) )",2. Preliminaries,0,[0]
"Given X = {w1, . . .",2. Preliminaries,0,[0]
",wp} where ‖wi‖ 6 1 for every i ∈",2. Preliminaries,0,[0]
[p].,2. Preliminaries,0,[0]
"Then for every choice z ∈ conv ( X )
and k ∈ N, there exists wi1 ,wi2 , . . .",2. Preliminaries,0,[0]
",wik such that∥∥∥∥1k ∑
j∈[k]
",2. Preliminaries,0,[0]
wij,2. Preliminaries,0,[0]
− z ∥∥∥∥ 6 2√k (3),2. Preliminaries,0,[0]
"We record here simple lemmas bounding the number of linear queries needed to estimate the gaussian width of a set and the length of a vector.
",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
Lemma 2.8 (Estimating Gaussian Width using linear queries).,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"For any u > 4, ε ∈ (0, 1/2) and δ > 0, there is a randomized algorithm that given a set S ⊆ Rd and ‖v‖ ∈",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"[1 ± ε] for all v ∈ S, computes ω̂ such that ω(S)− u 6 ω̂ 6 ω(S)",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
+ u with probability at least 1− δ.,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"The algorithm makes O(log(1/δ) · |S|) linear queries to S.
Proof.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"By Lemma 2.6, for a random g ∼ N(0, 1)d, supv∈S〈g,v〉 is away from ω(S) by u with probability at most 2e−16/4.5 < 0.1.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"By the Chernoff bound, the median of O(log δ−1) trials will satisfy the conditions required of ω̂ with probability at least 1− δ.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
Lemma 2.9 (Estimating norm using linear queries).,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"Given ε ∈ (0, 1/2) and δ > 0, for any vector x ∈ Rd , only O(ε−2 log δ−1) linear queries to x suffice to decide whether ‖x‖ ∈",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"[1− ε, 1 + ε] with success probability 1− δ.
",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
Proof.,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"It is easy to verify that Eg∼N(0,1)d [〈g,x〉2] = ‖x‖2.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"Therefore, it can be estimated to a multiplicative error of (1 ± ε/2) by taking the average of the squares of linear measurements using O ( 1 ε2 log 1 δ ) -queries.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"For the case ‖x‖2 6 2, a multiplicative error (1± ε/2) implies an additive error of ε.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"Furthermore, when ‖x‖2 > 2, a multiplicative error of (1± ε/2) implies that L > 2(1− ε/2)",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
> 1,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
#NAME?,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,0,[0]
"In this section, we prove Theorem 1.2.",3. Analysis for Unknown Design setting,0,[0]
"Let S denote the set {y1, . . .",3. Analysis for Unknown Design setting,0,[0]
",yp}.",3. Analysis for Unknown Design setting,0,[0]
"Our testing algorithm is shown in Algorithm 1.
",3. Analysis for Unknown Design setting,0,[0]
The number of linear queries made by the tester is O(pε−2 log(p/δ)),3. Analysis for Unknown Design setting,0,[0]
in Line 1 and O(p log δ−1) in Line 2.,3. Analysis for Unknown Design setting,0,[0]
Assume that for each i ∈,3.1. Completeness,0,[0]
"[p], yi = Axi for a matrix A ∈ Rd×m satisfying (ε, k)-RIP and xi ∈ Spmk .",3.1. Completeness,0,[0]
"By definition
Algorithm 1 SparseTestUnknown 1: Use Lemma 2.9 to decide with probability at least 1− δ/2 if there exists yi such that ‖yi‖ 6∈",3.1. Completeness,0,[0]
"[1− 2ε, 1 + 2ε].",3.1. Completeness,0,[0]
"Reject if so.
2: Use Lemma 2.8 to obtain ω̂, an estimate of ω(S) within additive error √ 3k",3.1. Completeness,0,[0]
"log(m/k) with probability at least
1− δ/2.",3.1. Completeness,0,[0]
"3: Accept if ω̂ 6 4 √ 3k log(m/k), else reject.
of RIP, we know that 1− ε 6 ‖yi‖ 6 1 + ε, so that Line 1 of the algorithm will pass with probability at least 1− δ/2.",3.1. Completeness,0,[0]
"From Lemma 1.9, we know that ω({x1, . .",3.1. Completeness,0,[0]
.xp}),3.1. Completeness,0,[0]
"6 2 √
3k log(m/k).",3.1. Completeness,0,[0]
"Lemma 3.1 shows that the gaussian width of S is approximately the same; its proof, deferred to the appendix (Section B.4), uses Slepian’s Lemma (Lemma B.3).
",3.1. Completeness,0,[0]
Lemma 3.1.,3.1. Completeness,0,[0]
LetX ⊂,3.1. Completeness,0,[0]
"Sm−1 be a finite set, and let S ⊂ Rd be an ε-isometric embedding of X .",3.1. Completeness,0,[0]
"Then
(1− ε)ω(X) 6 ω(S) 6 (1 + ε)ω(X) (4)
Hence, the gaussian width of y1, . .",3.1. Completeness,0,[0]
.,3.1. Completeness,0,[0]
",yp is at most 2(1 + ε) √
3k log(m/k).",3.1. Completeness,0,[0]
"Taking into account the additive error in Line 2, we see that with probability at least 1 − δ/2, ω̂ 6 (3 + 2ε) √ 3k",3.1. Completeness,0,[0]
log(m/k) 6 4 √ 3k,3.1. Completeness,0,[0]
log(m/k).,3.1. Completeness,0,[0]
"Hence, the tester accepts with probability at least 1− δ.",3.1. Completeness,0,[0]
"As mentioned before, in order to prove soundness we need to show that whenever the gaussian width of the set S is small, it is close to some sparse point-set.",3.2. Soundness,0,[0]
Let ω∗ = 4 √ 3k log mk .,3.2. Soundness,0,[0]
We shall break the analysis into two cases:,3.2. Soundness,0,[0]
Case (i) { ω∗ >,3.2. Soundness,0,[0]
"(ε/C)2 √ d } : For this case, we use the
fact random projection of discretized sparse point-sets (Definition A.1) form an appropriated cover of S.",3.2. Soundness,0,[0]
"This is formalized in the following theorem, which in a sense shows an approximate inverse of Gordon’s Theorem for sparse vectors:
Theorem 3.2.",3.2. Soundness,0,[0]
"Given ε > 0 and integers C, d, k and m, let n =",3.2. Soundness,0,[0]
O ( k ε2 log(m/k) ) .,3.2. Soundness,0,[0]
Suppose m > k/ε8.,3.2. Soundness,0,[0]
Let Φ :,3.2. Soundness,0,[0]
"Rm 7→ Rn be drawn from 1√ n N(0, 1)n×m.",3.2. Soundness,0,[0]
"Then, for ` = O(kε−4), with high probability, the set Φnorm(Ŝp m
` ) is an O(ε 1/4)-cover of Sn−1, where
Φnorm(x) = Φ(x)/‖Φ(x)‖2.
",3.2. Soundness,0,[0]
"The proof of the above Theorem is deferred to Section A. From the choice of parameters we have d 6 C
′k ε2 log m k
Therefore, using the above Theorem we know that there
exists (ε, k)-RIP matrix Φ ∈ Rd×m such that Φnorm ( Spm` ) is an O(ε1/4)-cover of Sd−1 (and therefore it is a ε1/4cover of S).",3.2. Soundness,0,[0]
"Therefore, there exists X ∈ Rm×p such that Y = Φ(X) +",3.2. Soundness,0,[0]
E where the columns of X and E satisfy the respective ‖ · ‖0 and ‖ · ‖2-upper bounds respectively.,3.2. Soundness,0,[0]
Case (ii) { ω∗ 6 (ε/C)2 √ d } :,3.2. Soundness,0,[0]
"For this case, we use the
following result on the concentration of `∞-norm: Lemma 3.3.",3.2. Soundness,0,[0]
Given S ⊂,3.2. Soundness,0,[0]
"Sd−1, we have
Pr R∼Od
[ max
y∈R(S) ‖y‖∞ 6 C
ω(S)
d1/2
] > 1
2
where Od is the orthogonal group in Rd i.e., R is a uniform random rotation.
",3.2. Soundness,0,[0]
"Although this concentration bound is known, for completeness we give a proof in the appendix (Section B.7).",3.2. Soundness,0,[0]
"From the above lemma, it follows that there exists R ∈ Od such that for any z ∈ Z := R(S) we have ‖z‖∞ 6 ε2 and therefore Y = R−1Z.",3.2. Soundness,0,[0]
"Furthermore, since R is orthogonal, therefore the matrix R−1 is also orthogonal, and therefore it satisfies (ε, k)-RIP.
",3.2. Soundness,0,[0]
"To complete the proof, we observe that even though the given factorization has inner dimension d, we can trivially extend it to one with inner dimension m. This can be done by constructing Φ =",3.2. Soundness,0,[0]
"[ R−1 G ] with G ∼ 1√ d N(0, 1)d×m−d.",3.2. Soundness,0,[0]
"Since ω∗ d, from Theorem 2.4 it follows that with high probability G (and consequently Φ) will satisfy (ε, k)-RIP.",3.2. Soundness,0,[0]
"Finally, we construct Ẑ ∈ Rm×n by padding Z with m − d rows of zeros.",3.2. Soundness,0,[0]
"Therefore, by construction Y = Φ · Ẑ, where for every i ∈",3.2. Soundness,0,[0]
[p] we have ‖zi‖∞ 6 ε2.,3.2. Soundness,0,[0]
Hence the claim follows.,3.2. Soundness,0,[0]
"In this section, we describe and analyze the tester for the known design matrix case.",4. Analysis for the Known Design setting,0,[0]
"The algorithm itself is a simple convex-hull membership test, which can be solved using a linear program.
",4. Analysis for the Known Design setting,0,[0]
"Algorithm 2 SparseTest-KnownDesign 1: Set n = 100klog mδ , sample projection matrix Φ ∼
1√ n",4. Analysis for the Known Design setting,0,[0]
"N(0, 1)n×d
2: Observe linear sketch ỹ = Φ(y) 3: Let A±",4. Analysis for the Known Design setting,0,[0]
= A ∪ −A 4: Accept iff ỹ ∈ √ k · conv ( Φ(A±) ),4. Analysis for the Known Design setting,0,[0]
We shall now prove the completeness and soundness guarantees of the above tester.,4. Analysis for the Known Design setting,0,[0]
"The running time bound follows because convex hull membership reduces to linear programming.
",4. Analysis for the Known Design setting,0,[0]
Completeness Let y = Ax where A ∈ Rd×m is an arbitrary matrix with ‖ai‖ = 1 for every,4. Analysis for the Known Design setting,0,[0]
i ∈,4. Analysis for the Known Design setting,0,[0]
[m].,4. Analysis for the Known Design setting,0,[0]
"Furthermore ‖x‖2 = 1 and ‖x‖0 6 k. Therefore, by Cauchy-Schwartz we have ‖x‖1 6",4. Analysis for the Known Design setting,0,[0]
√ k‖x‖2 = √ k.,4. Analysis for the Known Design setting,0,[0]
"Hence, it follows that
y ∈ √ k · conv(A±).",4. Analysis for the Known Design setting,0,[0]
"Since Φ : Rm 7→ Rd is a linear trans-
formation, we have Φ(y) ∈",4. Analysis for the Known Design setting,0,[0]
√ k · conv(Φ(A±)).,4. Analysis for the Known Design setting,0,[0]
"Therefore, the tester accepts with probability 1.
",4. Analysis for the Known Design setting,0,[0]
"Soundness Consider the set Aε/√k which is the set of all (2k/ε2)-uniform convex combinations of √ k(A±) i.e.,
Aε/ √ k = { ∑ vi∈Ω ε2 2k vi : multiset Ω ∈ (√ k.A± )2k/ε2} (5)
",4. Analysis for the Known Design setting,0,[0]
"Then, from the approximate Carathéodory theorem, it follows that Aε/√k is an ε-cover of √ k · conv ( A± ) .",4. Analysis for the Known Design setting,0,[0]
"Furthermore, |Aε/√k| 6 (2m) 2k/ε2 .",4. Analysis for the Known Design setting,0,[0]
"By our choice of n, with
probability at least 1 − δ/2, the set Φ ( {y} ∪ Aε/√k ) is ε-isometric to {y} ∪Aε/√k.
",4. Analysis for the Known Design setting,0,[0]
Let Ãε/√k = Φ ( Aε/ √ k ) .,4. Analysis for the Known Design setting,0,[0]
"Again, by the approximate Carathéodory’s theorem, the set Ãε/√k is an ε-cover of
Φ (√ k · conv(A±) ) .",4. Analysis for the Known Design setting,0,[0]
Now suppose the test accepts y with probability at least δ.,4. Analysis for the Known Design setting,0,[0]
"Then, with probability at least δ/2, the test accepts and the above ε-isometry conditions hold simultaneously.",4. Analysis for the Known Design setting,0,[0]
"Then,
ỹ ∈ √ k · conv ( Φ(A±) )",4. Analysis for the Known Design setting,0,[0]
"1⇒ dist ( ỹ, Ãε/ √ k ) 6 ε
2⇒ dist ( y, Aε/ √ k ) 6 ε(1− ε)−1 6 2ε
⇒ dist",4. Analysis for the Known Design setting,0,[0]
(,4. Analysis for the Known Design setting,0,[0]
"y, √ k · conv(A±) )",4. Analysis for the Known Design setting,0,[0]
"6 2ε
where step 1 follows from the ε-cover guarantee of Ãε/√k, step 2 follows from the ε-isometry guarantee.",4. Analysis for the Known Design setting,0,[0]
"Invoking the approximate Carathéodory theorem, we get that there exists ŷ",4. Analysis for the Known Design setting,0,[0]
= Ax̂ ∈,4. Analysis for the Known Design setting,0,[0]
√ k · conv(±A) such that ‖x̂‖0 6 O(k/ε2) and ‖ŷ − y‖ 6 O(ε).,4. Analysis for the Known Design setting,0,[0]
This completes the soundness direction.,4. Analysis for the Known Design setting,0,[0]
Our algorithm for the unknown design setting is based on the principle that the property of sparse representability in some basis admits an approximate characterization in terms of gaussian width.,5. Experimental Results,0,[0]
This section provides experimental evidence which supplements our theoretical results.,5. Experimental Results,0,[0]
"For the empirical study, we use the classic Barbara image (which is of size 512× 512 pixels).",5. Experimental Results,0,[0]
"Specifically, we consider 9 subimages of size 100 × 100 pixels each (see Figure 1).",5. Experimental Results,0,[0]
"For each such sub-image, we compute a matrix representation (by the standard technique of subdividing the images into patches, see, e.g., (Elad & Aharon, 2006)).",5. Experimental Results,0,[0]
"In particular,
each sub-image is represented as a matrix Y of dimension 64 × 8649.",5. Experimental Results,0,[0]
"Then, for each matrix Y corresponding to a sub-image, we estimate the gaussian width of the `2-column normalized matrix.",5. Experimental Results,0,[0]
"In addition, setting the number of atoms m = 100 and sparsity k = 10, we run the k-SVD algorithm for 50 iterations and record the reconstruction error.6
Figure 2 shows the comparison between gaussian width and reconstruction error, in which we observe that there is an approximate correlation between the two quantities.",5. Experimental Results,0,[0]
"In particular, for sub-images 2,7 and 8—which mostly consist of background—both the gaussian width and the reconstruction error is small.",5. Experimental Results,0,[0]
"On the other hand, images 3, 6 and 9, which consist of intricate patterns and objects, have large gaussian width as well as large reconstruction error.",5. Experimental Results,0,[0]
"Consequently, we can deduce that for sub-images with large gaussian width, in order to achieve low reconstruction error, one would have consider a larger number of atoms m or larger sparsity k.",5. Experimental Results,0,[0]
"In this paper, we studied the problem of testing sparsity with respect to unknown and known bases.",6. Conclusion and Open Questions,0,[0]
"While the optimization variants of these problems (namely Dictionary Learning and Sparse Recovery) are known to be NP-hard in the worst case, our results show that under appropriate relaxations, these problems admit efficient property testing algorithms.",6. Conclusion and Open Questions,0,[0]
"Future work include designing testing algorithms for sparsity over an unknown basis with stronger
6For a matrix Y ∈ Rd×n approximated by overcomplete basis A and coefficient matrix X, the reconstruction error is equal to ‖Y −AX‖2F /(n.d).
guarantees or developing impossibility results.",6. Conclusion and Open Questions,0,[0]
We also hope that this paper leads to study of property testing of other widely studied hypotheses in machine learning such as nonnegative rank and VC-dimension.,6. Conclusion and Open Questions,0,[0]
We would like to thank David Woodruff for showing us the sketching-based tester described in Section 1.2.,Acknowledgements,0,[0]
Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications.,abstractText,0,[0]
"In this work, we describe property testing algorithms for sparsity that observe a lowdimensional projection of the input.",abstractText,0,[0]
We consider two settings.,abstractText,0,[0]
"In the first setting, we test sparsity with respect to an unknown basis: given input vectors y1, . . .",abstractText,0,[0]
",yp ∈ R whose concatenation as columns forms Y ∈ Rd×p, does Y = AX for matrices A ∈ Rd×m",abstractText,0,[0]
"and X ∈ Rm×p such that each column of X is k-sparse, or is Y “far” from having such a decomposition?",abstractText,0,[0]
"In the second setting, we test sparsity with respect to a known basis: for a fixed design matrix A ∈ Rd×m, given input vector y ∈ R, is y = Ax for some ksparse vector x or is y “far” from having such a decomposition?",abstractText,0,[0]
We analyze our algorithms using tools from high-dimensional geometry and probability.,abstractText,0,[0]
Testing Sparsity over Known and Unknown Bases,title,0,[0]
"Proceedings of NAACL-HLT 2013, pages 201–210, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,0,[0]
"Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television.",1 Introduction,0,[0]
"To maintain consistency between the captions being read and other visual cues, the latency between when a word was said and when it is displayed must be under five seconds.",1 Introduction,0,[0]
"The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text within approximately 5 seconds.",1 Introduction,0,[0]
"Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short notice.",1 Introduction,0,[0]
"Automatic speech recognition (ASR) (Saraclar et al., 2002) attempts to solve this
problem by converting speech to text completely automatically.",1 Introduction,0,[0]
"However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b).
",1 Introduction,0,[0]
"An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013).",1 Introduction,0,[0]
"In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1).",1 Introduction,0,[0]
"This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk.",1 Introduction,0,[0]
"Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer.",1 Introduction,0,[0]
"However, combining these captions involves real-time alignment of partial captions that may be incomplete and that often have spelling errors and inconsistent timestamps.",1 Introduction,0,[0]
"In this paper, we present a more accurate combiner that leverages
201
Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance.
",1 Introduction,0,[0]
Gauging the quality of captions is not easy.,1 Introduction,0,[0]
"Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability.",1 Introduction,0,[0]
"As a result, a lower WER does not always result in better understanding (Wang et al., 2003).",1 Introduction,0,[0]
"We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators.
",1 Introduction,0,[0]
"The key contributions of this paper are as follows:
• We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters.",1 Introduction,0,[0]
"We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure.
",1 Introduction,0,[0]
"• We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently.
",1 Introduction,0,[0]
"• We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions.",1 Introduction,0,[0]
"Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Pražák et al., 2012).",2 Related Work,0,[0]
"However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001).",2 Related Work,0,[0]
"To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012).",2.1 Crowd Captioning,0,[0]
"The system incrementally
builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words.",2.1 Crowd Captioning,0,[0]
"A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram language model.",2.1 Crowd Captioning,0,[0]
"The algorithm is designed to be used online, and hence has high speed and low latency.",2.1 Crowd Captioning,0,[0]
"However, due to the incremental nature of the algorithm and due to the lack of a principled objective function, it is not guaranteed to find the globally optimal alignment for the captions.",2.1 Crowd Captioning,0,[0]
"The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006).",2.2 Multiple Sequence Alignment,0,[0]
"MSA is an important problem in computational biology (Durbin et al., 1998).",2.2 Multiple Sequence Alignment,0,[0]
The goal is to find an optimal alignment from a given set of biological sequences.,2.2 Multiple Sequence Alignment,0,[0]
"The pairwise alignment problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length.",2.2 Multiple Sequence Alignment,0,[0]
"The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994).",2.2 Multiple Sequence Alignment,0,[0]
"Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time.
",2.2 Multiple Sequence Alignment,0,[0]
"Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994).",2.2 Multiple Sequence Alignment,0,[0]
"Finally, the input sequences are aligned according to the order specified by the guide tree.",2.2 Multiple Sequence Alignment,0,[0]
"While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000).
",2.2 Multiple Sequence Alignment,0,[0]
"Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012).",2.2 Multiple Sequence Alignment,0,[0]
"The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layout of a keyboard and based on common spelling
errors.",2.2 Multiple Sequence Alignment,0,[0]
"However, MUSCLE relies on a progressive alignment strategy and may result in suboptimal solutions.",2.2 Multiple Sequence Alignment,0,[0]
"Moreover, it uses characters as atomic symbols instead of words.",2.2 Multiple Sequence Alignment,0,[0]
Our approach operates on a per-word basis and is able to arrive at a solution that is within a selectable error-bound of optimal.,2.2 Multiple Sequence Alignment,0,[0]
We start with an overview of the MSA problem using standard notations as described by Lermen and Reinert (2000).,3 Multiple Sequence Alignment,0,[0]
"Let S1, . . .",3 Multiple Sequence Alignment,0,[0]
", SK , K ≥ 2, be the K sequences over an alphabet Σ, and having length N1, . . .",3 Multiple Sequence Alignment,0,[0]
", NK .",3 Multiple Sequence Alignment,0,[0]
The special gap symbol is denoted by ‘−’ and does not belong to Σ. Let,3 Multiple Sequence Alignment,0,[0]
"A = (aij) be a K × Nf matrix, where aij ∈ Σ ∪ {−}, and the ith row has exactly (Nf − Ni) gaps and is identical to Si if we ignore the gaps.",3 Multiple Sequence Alignment,0,[0]
Every column of A must have at least one non-gap symbol.,3 Multiple Sequence Alignment,0,[0]
"Therefore, the jth column of A indicates an alignment state for the jth position, where the state can have one of the 2K − 1 possible combinations.",3 Multiple Sequence Alignment,0,[0]
"Our goal is to find the optimum alignment matrix AOPT that minimizes the sum of pairs (SOP) cost function:
c(A) = ∑
1≤i≤j≤K
c(Aij) (1)
where c(Aij) is the cost of the pairwise alignment between Si and Sj according to A. Formally, c(Aij) = ∑Nf
l=1 sub(ail, ajl), where sub(ail, ajl) denotes the cost of substituting ajl for ail.",3 Multiple Sequence Alignment,0,[0]
"If ail and ajl are identical, the substitution cost is usually zero.",3 Multiple Sequence Alignment,0,[0]
"For the caption alignment task, we treat each individual word as a symbol in our alphabet Σ.",3 Multiple Sequence Alignment,0,[0]
The substitution cost for two words is estimated based on the edit distance between two words.,3 Multiple Sequence Alignment,0,[0]
"The exact solution to the SOP optimization problem is NP-Complete, but many methods solve it approximately.",3 Multiple Sequence Alignment,0,[0]
"In this paper, we adapt weighted A∗ search for approximately solving the MSA problem.",3 Multiple Sequence Alignment,0,[0]
The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source and single sink node in a K-dimensional lattice.,3.1 A∗ Search for MSA,0,[0]
The total number of nodes in the lattice is (N1 + 1) ×,3.1 A∗ Search for MSA,0,[0]
"(N2 +
Algorithm 1 MSA-A∗ Algorithm Require: K input sequences S = {S1, . . .",3.1 A∗ Search for MSA,0,[0]
", SK} having
length N1, . . .",3.1 A∗ Search for MSA,0,[0]
", NK , heuristic weight w, beam size b
1: start← 0K , goal←",3.1 A∗ Search for MSA,0,[0]
"[N1, . . .",3.1 A∗ Search for MSA,0,[0]
", NK ] 2: g(start)← 0, f(start)← w × h(start).",3.1 A∗ Search for MSA,0,[0]
3: Q← {start} 4: while Q 6= ∅,3.1 A∗ Search for MSA,0,[0]
do 5: n←,3.1 A∗ Search for MSA,0,[0]
"EXTRACT-MIN(Q) 6: for all s ∈ {0, 1}K",3.1 A∗ Search for MSA,0,[0]
"− {0K} do 7: ni ← n + s 8: if ni = goal then 9: Return the alignment matrix for the reconstructed
path from start to ni 10: else if ni 6∈ Beam(b) then 11: continue; 12: else 13: g(ni)← g(n) + c(n, ni) 14: f(ni)← g(ni) +",3.1 A∗ Search for MSA,0,[0]
"w × h(ni) 15: INSERT-ITEM(Q, ni, f(ni))",3.1 A∗ Search for MSA,0,[0]
"16: end if 17: end for 18: end while
1) × · · · × (NK + 1), each corresponding to a distinct position in K sequences.",3.1 A∗ Search for MSA,0,[0]
"The source node is [0, . . .",3.1 A∗ Search for MSA,0,[0]
", 0] and the sink node is [N1, . . .",3.1 A∗ Search for MSA,0,[0]
", NK ].",3.1 A∗ Search for MSA,0,[0]
"The dynamic programming algorithm for estimating the shortest path from source to sink treats each node position [n1, . . .",3.1 A∗ Search for MSA,0,[0]
", nK ] as a state and calculates a matrix that has one entry for each node.",3.1 A∗ Search for MSA,0,[0]
"Assuming the sequences have roughly same length N , the size of the dynamic programming matrix is O(NK).",3.1 A∗ Search for MSA,0,[0]
"At each vertex, we need to minimize the cost over all its 2K − 1 predecessor nodes, and, for each such transition, we need to estimate the SOP objective function that requires O(K2) operations.",3.1 A∗ Search for MSA,0,[0]
"Therefore, the dynamic programming algorithm has time complexity of O(K22KNK) and space complexity of O(NK), which is infeasible for most practical problem instances.",3.1 A∗ Search for MSA,0,[0]
"However, we can efficiently solve it via heuristic A∗ search (Lermen and Reinert, 2000).
",3.1 A∗ Search for MSA,0,[0]
"We use A∗ search based MSA (shown in Algorithm 1, illustrated in Figure 2) that uses a priority queue Q to store dynamic programming states corresponding to node positions in the K dimensional lattice.",3.1 A∗ Search for MSA,0,[0]
Let n =,3.1 A∗ Search for MSA,0,[0]
"[n1, . . .",3.1 A∗ Search for MSA,0,[0]
", nK ] be any node in the lattice, s be the source, and t be the sink.",3.1 A∗ Search for MSA,0,[0]
"The A∗ search can find the shortest path using a greedy Best First Search according to an evaluation function f(n), which is the summation of the cost func-
tions g(n) and the heuristic function h(n) for node n. The cost function g(n) denotes the cost of the shortest path from the source s to the current node n.",3.1 A∗ Search for MSA,0,[0]
"The heuristic function h(n) is the approximate estimated cost of the shortest path from n to the destination t. At each step of the A∗ search algorithm, we extract the node with the smallest f(n) value from the priority queue Q and expand it by one edge.",3.1 A∗ Search for MSA,0,[0]
The heuristic function h(n) is admissible if it never overestimates the cost of the cheapest solution from n to the destination.,3.1 A∗ Search for MSA,0,[0]
An admissible heuristic function guarantees that A∗ will explore the minimum number of nodes and will always find the optimal solution.,3.1 A∗ Search for MSA,0,[0]
"One commonly used admissible heuristic function is hpair(n):
hpair(n) = L(n → t) = ∑
1≤i<j≤K
c(A∗p(σ n i , σ n j ))
(2) where L(n → t) denotes the lower bound on the cost of the shortest path from n to destination t, A∗p is the optimal pairwise alignment, and σni is the suffix of node n in the i-th sequence.",3.1 A∗ Search for MSA,0,[0]
A∗ search using the pairwise heuristic function hpair significantly reduces the search space and also guarantees finding the optimal solution.,3.1 A∗ Search for MSA,0,[0]
We must be able to estimate hpair(n) efficiently.,3.1 A∗ Search for MSA,0,[0]
It may appear that we need to estimate the optimal pairwise alignment for all the pairs of suffix sequences at every node.,3.1 A∗ Search for MSA,0,[0]
"However, we can precompute the dynamic programming matrix over all the pair of sequences (Si, Sj) once from the backward direction, and then reuse these values at each node.",3.1 A∗ Search for MSA,0,[0]
"This simple trick significantly speeds up the computation of hpair(n).
",3.1 A∗ Search for MSA,0,[0]
"Despite the significant reduction in the search space, the A∗ search may still need to explore a large number of nodes, and may become too slow for real-time captioning.",3.1 A∗ Search for MSA,0,[0]
"However, we can further improve the speed by following the idea of weighted A∗ search (Pohl, 1970).",3.1 A∗ Search for MSA,0,[0]
"We modify the evaluation
function f(n) = g(n)+hpair(n) to a weighted evaluation function f ′(n) = g(n) + whpair(n), where w ≥ 1 is a weight parameter.",3.1 A∗ Search for MSA,0,[0]
"By setting the value of w to be greater than 1, we increase the relative weight of the estimated cost to reach the destination.",3.1 A∗ Search for MSA,0,[0]
"Therefore, the search prefers the nodes that are closer to the destination, and thus reaches the goal faster.",3.1 A∗ Search for MSA,0,[0]
"Weighted A∗ search can significantly reduce the number of nodes to be examined, but it also loses the optimality guarantee of the admissible heuristic function.",3.1 A∗ Search for MSA,0,[0]
We can trade-off between accuracy and speed by tuning the weight parameter w.,3.1 A∗ Search for MSA,0,[0]
The computational cost of the A∗ search algorithm grows exponentially with increase in the number of sequences.,3.2 Beam Search using Time-stamps,0,[0]
"However, in order to keep the crowdsourced captioning system cost-effective, only a small number of workers are generally recruited at a time (typically K ≤ 10).",3.2 Beam Search using Time-stamps,0,[0]
"We, therefore, are more concerned about the growth in computational cost as the sequence length increases.
",3.2 Beam Search using Time-stamps,0,[0]
"In practice, we break down the sequences into smaller chunks by maintaining a window of a given time interval, and we apply MSA only to the smaller chunks of captions entered by the workers during that time window.",3.2 Beam Search using Time-stamps,0,[0]
"As the window size increases, the accuracy of our MSA based combining system increases, but so does the computational cost and latency.",3.2 Beam Search using Time-stamps,0,[0]
"Therefore, it is important to apply MSA with a relatively small window size for real-time captioning applications.",3.2 Beam Search using Time-stamps,0,[0]
"Another interesting application can be the offline captioning, for example, captioning an entire lecture and uploading the captions later.
",3.2 Beam Search using Time-stamps,0,[0]
"For the offline captioning problem, we can focus less on latency and more on accuracy by aligning longer sequences.",3.2 Beam Search using Time-stamps,0,[0]
"To restrict the search space from exploding with sequence length (N ), we apply a beam constraint on our search space using the time stamps of each captioned words.",3.2 Beam Search using Time-stamps,0,[0]
"For example, if we
set the beam size to be 20 seconds, then we ignore any state in our search space that aligns two words having more than 20 seconds time lag.",3.2 Beam Search using Time-stamps,0,[0]
"Given a fixed beam size b, we can restrict the number of priority queue removals by the A∗ algorithm to O(NbK).",3.2 Beam Search using Time-stamps,0,[0]
The maximum size of the priority queue is O(NbK).,3.2 Beam Search using Time-stamps,0,[0]
"For each node in the priority queue, for each of the O(2K) successor states, the objective function and heuristic estimation requires O(K2) operations and each priority queue insertion requires O(log(NbK)) i.e. O(log N + K log b) operations.",3.2 Beam Search using Time-stamps,0,[0]
"Therefore, the overall worst case computational complexity is O ( NbK2K(K2 + log N + K log b) )
.",3.2 Beam Search using Time-stamps,0,[0]
"Note that for fixed beam size b and number of sequences K, the computational cost grows as O(N log N) with the increase in N .",3.2 Beam Search using Time-stamps,0,[0]
"However, in practice, weighted A∗ search explores much smaller number of states compared to this beam-restricted space.",3.2 Beam Search using Time-stamps,0,[0]
"After aligning the captions by multiple workers in a given chunk, we need to combine them to obtain the final caption.",3.3 Majority Voting after Alignment,0,[0]
We do that via majority voting at each position of the alignment matrix containing a nongap symbol.,3.3 Majority Voting after Alignment,0,[0]
"In case of tie, we apply the language model to choose the most likely word.
",3.3 Majority Voting after Alignment,0,[0]
"Often workers type in nonstandard symbols, abbreviations, or misspelled words that do not match with any other workers’ input and end up as a single word aligned to gaps in all the other sequences.",3.3 Majority Voting after Alignment,0,[0]
"To filter out such spurious words, we apply a voting threshold (tv) during majority voting and filter out words having less than tv votes.",3.3 Majority Voting after Alignment,0,[0]
Typically we set tv = 2 (see the example in Figure 3).,3.3 Majority Voting after Alignment,0,[0]
"While applying the voting threshold improves the word error rate and readability, it runs the risk of loosing correct words if they are covered by only a single worker.",3.3 Majority Voting after Alignment,0,[0]
We also experimented with a version of our system designed to incorporate the score from an n-gram language model into the search.,3.4 Incorporating an N-gram Language Model,0,[0]
"For this purpose, we modified the alignment algorithm to produce a hypothesized output string as it moves through the input strings, as opposed to using voting to produce the final string as a post-processing step.",3.4 Incorporating an N-gram Language Model,0,[0]
"The states for our dynamic programming are extended to include not only the current position in each input string, but also the last two words of the hypothesis string (i.e. [n1, . . .",3.4 Incorporating an N-gram Language Model,0,[0]
", nK , wi−1, wi−2]) for use in computing the next trigram language model probability.",3.4 Incorporating an N-gram Language Model,0,[0]
"We replace our sum-of-all-pairs objective function with the sum of the alignment cost of each input with the hypothesis string, to which we add the log of the language model probability and a feature for the total number of words in the hypothesis.",3.4 Incorporating an N-gram Language Model,0,[0]
"Mathematically, we consider the hypothesis string to be the 0th row of the alignment matrix, making our objective function:
c(A) = ∑
1≤i≤K
c(A0,i) +",3.4 Incorporating an N-gram Language Model,0,[0]
"wlen
Nf ∑
l=1
I(a0,l 6=",3.4 Incorporating an N-gram Language Model,0,[0]
"−)
",3.4 Incorporating an N-gram Language Model,0,[0]
#NAME?,3.4 Incorporating an N-gram Language Model,0,[0]
"Nf ∑
l=1
log P (a0,l|a0,l−2, a0,l−1)
where wlm and wlen are negative constants indicating the relative weights of the language model probability and the length penalty.
",3.4 Incorporating an N-gram Language Model,0,[0]
Extending states with two previous words results in a larger computational complexity.,3.4 Incorporating an N-gram Language Model,0,[0]
"Given K sequences of length N each, we can have O(NK) distinct words.",3.4 Incorporating an N-gram Language Model,0,[0]
"Therefore, the number distinct states is O(NbK(NK)2) i.e. O(N3K2bK).",3.4 Incorporating an N-gram Language Model,0,[0]
"Each state can have O(K2K) successors, giving an overall computational complexity of O(N3K3bK2K(K2 + log N + log K + K log b)).",3.4 Incorporating an N-gram Language Model,0,[0]
"Alternatively, if the vo-
cabulary size |V | is smaller than NK, the number of distinct states is bounded by O(NbK |V |2).",3.4 Incorporating an N-gram Language Model,0,[0]
"Automated evaluation of speech to text captioning is known to be a challenging task (Wang et al., 2003).",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
Word Error Rate (WER) is the most commonly used metric that finds the best pairwise alignment between the candidate caption and the ground truth reference sentence.,3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"WER is estimated as S+I+D
N ,
where S, I , and D is the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"However, WER does not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011).
",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002).",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"However, unlike WER, BLEU imposes no explicit constraints on the word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure.
",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outputs can be equally correct.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"On the other hand, unlike with ASR, out-of-order output is frequently produced by our alignment system when there is not enough overlap between the partial captions to derive the correct ordering for all words.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"It may be the case that even such out-of-order output can be of value to the user, and should receive some sort of partial credit that is not possible using WER.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"For this reason, we wished to systematically compare BLEU, F-measure, and WER as metrics for our task.
",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
We performed a study to evaluate the agreement of the three metrics with human judgment.,3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"We ran-
domly extracted one-minute long audio clips from four MIT OpenCourseWare lectures.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Each clip was transcribed by 7 human workers, and then aligned and combined using four different systems: the graph-based system, and three different versions of our weighted A∗ algorithm with different values of tuning parameters.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
Fifty people participated in the study and were split in two equal sized groups.,3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Each group was assigned two of the four audio clips, and each person evaluated all four captions for both clips.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Each participant assigned a score between 1 to 10 to these captions, based on two criteria: 1) the overall estimated agreement of the captions with the ground truth text, and 2) the readability and understandability of the captions.
",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Finally, we estimated the correlation coefficients (both Spearman and Pearson) for the three metrics discussed above with respect to the average score assigned by the human participants.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
The results are presented in Table 1.,3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"Among the three metrics, WER had the highest agreement with the human participants.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"This indicates that reconstructing the correct word order is in fact important to the users, and that, in this aspect, our task has more of the flavor of speech recognition than of machine translation.",3.5 Evaluation Metric for Speech to Text Captioning,0,[0]
"We experiment with the MSA-A∗ algorithm for captioning different audio clips, and compare the results with two existing techniques.",4 Experimental Results,0,[0]
Our experimental set up is similar to the experiments by Lasecki et al. (2012).,4 Experimental Results,0,[0]
Our dataset consists of four 5-minute long audio clips extracted from lectures available on MIT OpenCourseWare.,4 Experimental Results,0,[0]
The audio clips contain speech from electrical engineering and chemistry lectures.,4 Experimental Results,0,[0]
Each audio clip is transcribed by ten non-expert human workers in real-time.,4 Experimental Results,0,[0]
"We then combine these inputs using our MSA-A∗ algorithm, and also compare with the existing graph-based system and mul-
tiple sequence alignment using MUSCLE.
",4 Experimental Results,0,[0]
"As explained earlier, we vary the four key parameters of the algorithm: the chunk size (c), the heuristic weight (w), the voting threshold (tv), and the beam size (b).",4 Experimental Results,0,[0]
"The heuristic weight and chunk size parameters help us to trade-off between speed versus accuracy; the voting threshold tv helps improve precision by pruning words having less than tv votes, and beam size reduces the search space by restricting states to be inside a time window/beam.",4 Experimental Results,0,[0]
"We use affine gap penalty (Edgar, 2004) with different gap opening and gap extension penalty.",4 Experimental Results,0,[0]
We set gap opening penalty to 0.125 and gap extension penalty to 0.05.,4 Experimental Results,0,[0]
"We evaluate the performance using the three standard metrics: Word Error Rate (WER), BLEU, and F-measure.",4 Experimental Results,0,[0]
"The performance in terms of these metrics using different systems is presented in Figure 4.
",4 Experimental Results,0,[0]
"Out of the five systems in Figure 4, the first three are different versions of our A∗ search based MSA algorithm with different parameter settings: 1) A∗10-t system (c = 10 seconds, tv = 2), 2) A∗-15-t (c = 15 seconds, tv = 2), and 3)",4 Experimental Results,0,[0]
"A∗-15 (c = 15 seconds, tv = 1 i.e. no pruning while voting).",4 Experimental Results,0,[0]
"For all three systems, the heuristic weight parameter w is set to 2.5 and beam size b = 20 seconds.",4 Experimental Results,0,[0]
The other two systems are the existing graph-based system and multiple sequence alignment using MUSCLE.,4 Experimental Results,0,[0]
"Among the three A∗ based algorithms, both A∗-15-t and A∗10-t produce better quality transcripts and outperform the existing algorithms.",4 Experimental Results,0,[0]
Both systems apply the voting threshold that improves precision.,4 Experimental Results,0,[0]
"The system A∗-15 applies no threshold and ends up producing many spurious words having poor agreement among the workers, and hence it scores worse in all the three metrics.",4 Experimental Results,0,[0]
"The A∗-15-t achieves 57.4% average accuracy in terms of (1-WER), providing 29.6% improvement with respect to the graph-based system (average accuracy 42.6%), and 35.4% improvement with respect to the MUSCLE-based MSA system (average accuracy 41.9%).",4 Experimental Results,0,[0]
"On the same set of audio clips, Lasecki et al. (2012) reported 36.6% accuracy using ASR (Dragon Naturally Speaking, version 11.5 for Windows), which is worse than all the crowd-based based systems used in this experiment.",4 Experimental Results,0,[0]
"To measure the statistical significance of this improvement, we performed a t-test at both the dataset level (n = 4 clips) and the word level (n = 2862 words).",4 Experimental Results,0,[0]
The improvement over the graph-based model was statistically significant with dataset level p-value 0.001 and word level p-value smaller than 0.0001.,4 Experimental Results,0,[0]
"The average time to align each 15 second chunk with 10 input captions is ∼400 milliseconds.
",4 Experimental Results,0,[0]
"We have also experimented with a trigram language model, trained on the British National Corpus (Burnard, 1995) having ∼122 million words.",4 Experimental Results,0,[0]
The language-model-integrated A∗ search provided a negligible 0.21% improvement in WER over the A∗-15-t system on average.,4 Experimental Results,0,[0]
The task of combining captions does not require recognizing words; it only requires aligning them in the correct order.,4 Experimental Results,0,[0]
"This could explain why language model did not improve accuracy, as it does for speech recognition.",4 Experimental Results,0,[0]
"Since the standard MSA-A∗ algorithm (without language model) produced comparable accuracy and faster running time, we used that version in the rest of the
experiments.
",4 Experimental Results,0,[0]
"Next, we look at the critical speed versus accuracy trade-off for different values of the heuristic weight (w) and the chunk size (c) parameters.",4 Experimental Results,0,[0]
"Since WER has been shown to correlate most with human judgment, we show the next results only with respect to WER.",4 Experimental Results,0,[0]
"First, we fix the chunk size at different values, and then vary the heuristic weight parameter: w = 1.8, 2, 2.5, 3, 4, 6, and 8.",4 Experimental Results,0,[0]
"The results are shown in Figure 5(a), where each curve represents how time and accuracy changed over the range of values of w and a fixed value of c. We observe that for smaller values of w, the algorithm is more accurate, but comparatively slower.",4 Experimental Results,0,[0]
"As w increases, the search reaches the goal faster, but the quality of the solution degrades as well.",4 Experimental Results,0,[0]
"Next, we fix w and vary chunk size c = 5, 10, 15, 20, 40, 60 second.",4 Experimental Results,0,[0]
We repeat this experiment for a range of values of w and the results are shown in Figure 5(b).,4 Experimental Results,0,[0]
"We can see that the accuracy improves steeply up to c = 20 seconds, and does not improve much beyond c = 40 seconds.",4 Experimental Results,0,[0]
"For all these benchmarks, we set the beam size (b) to 20 seconds and voting threshold (tv) to 2.
",4 Experimental Results,0,[0]
"In our tests, the beam size parameter (b) did not play a significant role in performance, and setting it to any reasonably large value (usually≥ 15 seconds) resulted in similar accuracy and running time.",4 Experimental Results,0,[0]
"This is because the A∗ search with hpair heuristic already reduces the the search space significantly, and usually reaches the goal in a number of steps smaller than the state space size after the beam restriction.
",4 Experimental Results,0,[0]
"Finally, we investigate how the accuracy of our algorithm varies with the number of inputs/workers.",4 Experimental Results,0,[0]
We start with a pool of 10 input captions for one of the audio clips.,4 Experimental Results,0,[0]
We vary the number of input captions (K) to the MSA-A∗ algorithm from 2 up to 10.,4 Experimental Results,0,[0]
The quality of input captions differs greatly among the workers.,4 Experimental Results,0,[0]
"Therefore, for each value of K, we repeat the experiment min ( 20, ( 10
K
))
times; each time we randomly select K input captions out of the total pool of 10.",4 Experimental Results,0,[0]
"Figure 6 shows that accuracy steeply increases as the number of inputs increases to 7, and after that adding more workers does not provide much improvement in accuracy, but increases running time.",4 Experimental Results,0,[0]
"In this paper, we show that the A∗ search based MSA algorithm performs better than existing algorithms for combining multiple captions.",5 Discussion and Future Work,0,[0]
"The existing graph-based model has low latency, but it usually can not find a near optimal alignment because of its incremental alignment.",5 Discussion and Future Work,0,[0]
"Weighted A∗ search on the other hand performs joint multiple sequence alignment, and is guaranteed to produce a solution having cost no more than (1 + ǫ) times the cost of the optimal solution, given a heuristic weight of (1+ ǫ).",5 Discussion and Future Work,0,[0]
"Moreover, A∗ search allows for straightforward integration of an n-gram language model during the search.
",5 Discussion and Future Work,0,[0]
"Another key advantage of the proposed algorithm is the ease with which we can trade-off between
speed and accuracy.",5 Discussion and Future Work,0,[0]
The algorithm can be tailored to real-time by using a larger heuristic weight.,5 Discussion and Future Work,0,[0]
"On the other hand, we can produce better transcripts for offline tasks by choosing a smaller weight.
",5 Discussion and Future Work,0,[0]
It is interesting to compare our results with those achieved using the MUSCLE MSA tool of Edgar (2004).,5 Discussion and Future Work,0,[0]
"One difference is that our system takes a hierarchical approach in that it aligns at the word level, but also uses string edit distance at the letter level as a substitution cost for words.",5 Discussion and Future Work,0,[0]
"Thus, it is able to take advantage of the fact that individual transcriptions do not generally contain arbitrary fragments of words.",5 Discussion and Future Work,0,[0]
"More fundamentally, it is interesting to note that MUSCLE and most other commonly used MSA tools for biological sequences make use of a guide tree formed by a hierarchical clustering of the input sequences.",5 Discussion and Future Work,0,[0]
"The guide tree produced by the algorithms may or may not match the evolutionary tree of the organisms whose genomes are being aligned, but, nevertheless, in the biological application, such an underlying evolutionary tree generally exists.",5 Discussion and Future Work,0,[0]
"In aligning transcriptions, there is no particular reason to expect individual pairs of transcriptions to be especially similar to one another, which may make the guide tree approach less appropriate.
",5 Discussion and Future Work,0,[0]
"In order to get competitive results, the A∗ search based algorithm aligns sequences that are at least 7- 10 seconds long.",5 Discussion and Future Work,0,[0]
"The delay for collecting the captions within a chunk can introduce latency, however,
each alignment usually takes less than 300 milliseconds, allowing us to repeatedly align the stream of words, even before the window is filled.",5 Discussion and Future Work,0,[0]
This provides less accurate but immediate response to users.,5 Discussion and Future Work,0,[0]
"Finally, when we have all the words entered in a chunk, we perform the final alignment and show the caption to users for the entire chunk.
",5 Discussion and Future Work,0,[0]
"After aligning the input sequences, we obtain the final transcript by majority voting at each alignment position, which treats each worker equally and does not take individual quality into account.",5 Discussion and Future Work,0,[0]
"Recently, some work has been done for automatically estimating individual worker’s quality for crowd-based data labeling tasks (Karger et al., 2011; Liu et al., 2012).",5 Discussion and Future Work,0,[0]
Extending these methods for crowd-based text captioning could be an interesting future direction.,5 Discussion and Future Work,0,[0]
"In this paper, we have introduced a new A∗ search based MSA algorithm for aligning partial captions into a final output stream in real-time.",6 Conclusion,0,[0]
This method has advantages over prior approaches both in formal guarantees of optimality and the ability to trade off speed and accuracy.,6 Conclusion,0,[0]
Our experiments on real captioning data show that it outperforms prior approaches based on a dependency graph model and a standard MSA implementation (MUSCLE).,6 Conclusion,0,[0]
"An experiment with 50 participants explored whether exiting automatic metrics of quality matched human evaluations of readability, showing WER did best.
",6 Conclusion,0,[0]
Acknowledgments Funded by NSF awards IIS1218209 and IIS-0910611.,6 Conclusion,0,[0]
The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates.,abstractText,0,[0]
"Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear.",abstractText,0,[0]
"In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA).",abstractText,0,[0]
"In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds.",abstractText,0,[0]
"Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%).",abstractText,0,[0]
"The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.",abstractText,0,[0]
Text Alignment for Real-Time Crowd Captioning,title,0,[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 763–772 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1071",text,0,[0]
The number of pages required to print the content of the World Wide Web was estimated to 305 billion in a 2015 article1.,1 Background,0,[0]
"While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace.",1 Background,0,[0]
"A recent study shows that the average webpage weights 1,200 KB with plain text accounting for up to 16% of that size2.
",1 Background,0,[0]
"While efficient distribution of textual data and computations are the key to deal with the increas-
1http://goo.gl/p9lt7V 2http://goo.gl/c41wpa
ing scale of textual search, similarity measures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information.",1 Background,0,[0]
"These tasks are also often performed on dedicated document collections for domain-specific applications where text similarity measures can be directly applied.
",1 Background,0,[0]
"Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015).",1 Background,0,[0]
"However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora.
",1 Background,0,[0]
"For instance, Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words.",1 Background,0,[0]
"The proposed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs.",1 Background,0,[0]
"In a more general study, Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task.",1 Background,0,[0]
"In their experiments the best performance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in (Li et al., 2006).",1 Background,0,[0]
"This word order similarity is computed by constructing first two vectors representing the common words between two given sentences and using their respective positions in the sentences as term
763
weights.",1 Background,0,[0]
The similarity value is then obtained by subtracting the two vectors and taking the absolute value.,1 Background,0,[0]
"While such representation takes into account the actual positions of the words, it does not allow detecting sub-sequence matches and takes into account missing words only by omission.
",1 Background,0,[0]
"More generally, existing standalone (or traditional) text similarity measures rely on the intersections between token sets and/or text sizes and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein (Sankoff and Kruskal, 1983), Jaccard (Jain and Dubes, 1988) and Jaro (Jaro, 1989).",1 Background,0,[0]
"The sequential nature of natural language is taken into account mostly through word n-grams and skipgrams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.
",1 Background,0,[0]
"In this paper, we use intuitions from a common representation in DNA sequence alignment to design a new standalone similarity measure called TextFlow (XF).",1 Background,0,[0]
The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches.,1 Background,0,[0]
"Our contributions can be detailed further as follows:
•",1 Background,0,[0]
"A novel standalone similarity measure which:
– exploits the full sequence of words in the compared texts.
– is asymmetric in a way that allows it to provide the best performance on different tasks (e.g., paraphrase detection, textual entailment and ranking).
– when required, it can be trained with a small set of parameters controlling the impact of sub-sequence matching, position gaps and unmatched words.
– provides consistent high performance across tasks and datasets compared to traditional similarity measures.
",1 Background,0,[0]
"• A neural network architecture to train TextFlow parameters for specific tasks.
",1 Background,0,[0]
"• An empirical study on both performance consistency and standard evaluation measures, performed with eight datasets from three different tasks.
•",1 Background,0,[0]
"A new evaluation measure, called CORE, used to better show the consistency of a system at high performance using both its rank average and rank variance when compared to competing systems over a set of datasets.
",1 Background,0,[0]
"2 The TextFlow Similarity
XF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf. figure 1).",1 Background,0,[0]
We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf. figure 2).,1 Background,0,[0]
The area under the curve is considered to be the distance between the two text pairs which is then normalized with the matrix surface.,1 Background,0,[0]
"For practical computation, we transform this first intuitive representation using the delta of positions as in figure 3.",1 Background,0,[0]
"In this setting, the Y axis is the delta of positions of a word occurring in the two texts being compared.",1 Background,0,[0]
"If the word does not occur in the target text, the delta is considered to be a maximum reference value (l in figure 2).
",1 Background,0,[0]
"The semantics are: the bigger the area under curve is, the lower the similarity between the compared texts.",1 Background,0,[0]
"XF values are real numbers in the [0,1] interval, with 1 indicating a perfect match, and 0 indicating that the compared texts do not have any common tokens.",1 Background,0,[0]
"With this representation, we are able to take into account all matched words and sub-sequences at the same time.",1 Background,0,[0]
"The exact value for the XF similarity between two texts X = {x1, x2, .., xn} and Y = {y1, y2, .., ym} is therefore computed as:
XF (X,Y ) =",1 Background,0,[0]
"1− 1 nm
n∑
i=2
1
Si Ti,i−1(X,Y )
",1 Background,0,[0]
"− 1 nm
n∑
i=2
1
Si Ri,i−1(X,Y )
(1)
With Ti,i−1(X,Y ) corresponding to the triangular area in the [i − 1, i] step (cf. figure 3) and Ri,i−1(X,Y ) corresponding to the rectangular component.",1 Background,0,[0]
"They are expressed as:
Ti,i−1(X,Y ) = |∆P",1 Background,0,[0]
"(xi, X, Y )−∆P (xi−1, X, Y )",1 Background,0,[0]
"|
2 (2)
and:
Ri,i−1(X,Y ) =",1 Background,0,[0]
Min(∆P,1 Background,0,[0]
"(xi, X, Y ),∆P (xi−1, X, Y ))",1 Background,0,[0]
"(3)
With:
• ∆P (xi, X, Y ) the minimum difference between xi positions in X and Y .",1 Background,0,[0]
xi position in X is multiplied by the factor |Y ||X| for normalization.,1 Background,0,[0]
"If xi /∈ X ∩ Y , ∆P (xi, X, Y )
is set to the same reference value equal to m, (i.e., the cost of a missing word is set by default to the length of the target text), and:
• Si is the length of the longest matching sequence between X and Y including the word xi,",1 Background,0,[0]
"if xi ∈ X ∩ Y , or 1 otherwise.
",1 Background,0,[0]
XF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input textX .,1 Background,0,[0]
XF is an asymmetric similarity measure.,1 Background,0,[0]
Its asymmetric aspect has interesting semantic applications as we show in the example below (cf. figure 2).,1 Background,0,[0]
"The minimum value of XF provided the best differentiation between positive and negative text pairs when looking for semantic equivalence (i.e., paraphrases), the maximum value was among the top three for the textual entailment example.",1 Background,0,[0]
"We conduct this comparison at a larger scale in the evaluation section.
",1 Background,0,[0]
"We add 3 parameters to XF in order to represent the importance that should be given to position deltas (Position factor α), missing words (sensitivity factor β), and sub-sequence matching (sequence factor γ), such that:
XFα,β,γ(X,Y ) = 1− 1
βnm
n∑
i=2
α
Sγi T βi,i−1(X,Y )
",1 Background,0,[0]
"− 1 βnm
n∑
i=2
α
Sγi Rβi,i−1(X,Y )
(4)
",1 Background,0,[0]
"With:
T βi,i−1(X,Y ) = |∆βP",1 Background,0,[0]
"(xi, X, Y )−∆βP (xi−1, X, Y )",1 Background,0,[0]
"|
2 (5)
",1 Background,0,[0]
"Rβi,i−1(X,Y ) =",1 Background,0,[0]
"Min(∆βP (xi, X, Y ),∆βP (xi−1, X, Y )) (6) and:
• ∆βP (xi, X, Y ) = βm, if xi /∈ X ∩ Y
• α",1 Background,0,[0]
<,1 Background,0,[0]
β: forces missing words to always cost more than matched words.,1 Background,0,[0]
•,1 Background,0,[0]
"Sγi = {
1ifSi = 1orxi /∈",1 Background,0,[0]
X ∩ Y γ SiforSi,1 Background,0,[0]
>,1 Background,0,[0]
"1
The γ factor increases or decreases the impact of sub-sequence matching, α applies to individual token matches whether inside or outside a sequence, and β increases or decreases the impact of
missing tokens as well as the normalization quantity βnm in equation 4 to keep the similarity values in the [0,1] range.",1 Background,0,[0]
By default XF has canonical parameters set to 1.,2.1 Parameter Training,0,[0]
"However, when needed, α, β, and γ can be learned on training data for a specific task.",2.1 Parameter Training,0,[0]
"We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value.",2.1 Parameter Training,0,[0]
"To do so we compute, for each input text pair, the coefficients vector that would lead exactly to the XF value when multiplied by the vector<",2.1 Parameter Training,0,[0]
"αβ , α βγ , 1 >.",2.1 Parameter Training,0,[0]
"Figure 5) presents the training neural network considering several types of sequences (or translations) of the input text pairs (e.g., lemmas, words, synsets).
",2.1 Parameter Training,0,[0]
"We use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf. figure 6).",2.1 Parameter Training,0,[0]
Datasets.,3 Evaluation,0,[0]
"This evaluation was performed on 8 datasets from 3 different classification tasks: Tex-
tual Entailment Recognition, Paraphrase Detection, and ranking relevance.",3 Evaluation,0,[0]
"The datasets are as follows:
• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006).",3 Evaluation,0,[0]
"Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment.",3 Evaluation,0,[0]
"They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.
",3 Evaluation,0,[0]
"• Guardian: an RTE dataset collected from 78,696 Guardian articles5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets.",3 Evaluation,0,[0]
Positive examples were collected from the titles and first sentences.,3 Evaluation,0,[0]
"Negative examples were collected from the same source by selecting consecutive sentences and random sentences.
",3 Evaluation,0,[0]
"• SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with
5https://github.com/daoudclarke/ rte-experiment
3 labels: entailment, neutral and contradiction (Bowman et al., 2015).",3 Evaluation,0,[0]
We discarded the contradiction pairs as they do not necessarily represent dissimilar sentences and are therefore a random noise w.r.t.,3 Evaluation,0,[0]
"our similarity measure evaluation.
",3 Evaluation,0,[0]
"• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.
•",3 Evaluation,0,[0]
"Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow (Nakov et al., 2016).",3 Evaluation,0,[0]
"The dataset contains 3,169 training pairs and 700 test pairs.",3 Evaluation,0,[0]
"Three labels are considered: ”Perfect Match”, ”Relevant” or ”Irrelevant”.",3 Evaluation,0,[0]
"We combined the first two into the same positive category for our evaluation.
",3 Evaluation,0,[0]
"• Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels.
",3 Evaluation,0,[0]
Features.,3 Evaluation,0,[0]
"After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token:
• Word (plain text value)
• Lemma
• Part-Of-Speech (POS) tag
• WordNet Synset6",3 Evaluation,0,[0]
"OR Lemma
• WordNet Synset OR Lemma for Nouns
• WordNet Synset OR Lemma for Verbs
• WordNet Synset OR Lemma for Nouns and Verbs.
",3 Evaluation,0,[0]
In the last 4 types of sequences the lemma is used when there is no corresponding WordNet synset.,3 Evaluation,0,[0]
"In a first experiment we compare different aggregation functions on top of XF (minimum, maximum and average) in table 1.",3 Evaluation,0,[0]
"We used the LibLinear7 SVM classifier for this task.
",3 Evaluation,0,[0]
"In the second part of the evaluation, we use neural networks to compare the efficiency of XFc, XFt and other similarity measures with in the same setting.",3 Evaluation,0,[0]
We use the neural net described in figure 5 for the trained versionXFt and the equivalent architecture presented in figure 6 for all other similarity measures.,3 Evaluation,0,[0]
"For canonical XFc we use by default the best aggregation for the task as observed in table 3.
6https://wordnet.princeton.edu/ 7https://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
Similarity Measures.",3 Evaluation,0,[0]
"We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).",3 Evaluation,0,[0]
Implementation.,3 Evaluation,0,[0]
"XF was implemented in Java as an extension of the Simmetrics package, made available at this address9.",3 Evaluation,0,[0]
The neural networks were implemented in Python with TensorFlow10.,3 Evaluation,0,[0]
We also share the training sets used for both parameter training and evaluation.,3 Evaluation,0,[0]
The evaluation was performed on a 4-core laptop with 32GB of RAM.,3 Evaluation,0,[0]
The initial parameters for XFt were chosen with a random function.,3 Evaluation,0,[0]
Evaluation Measures.,3 Evaluation,0,[0]
"We use the standard accuracy values and F1, precision and recall for the
8https://github.com/Simmetrics/ simmetrics
9https://github.com/ymrabet/TextFlow 10https://www.tensorflow.org/
positive class (i.e., entailment, paraphrase, and ranking relevance).",3 Evaluation,0,[0]
"We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called Consistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}:
CORE D,S,E (m) =
MIN p∈S ( AVG d∈D (RS(Ed(p))",3 Evaluation,0,[0]
"+ Vd∈D(RS(Ed(p))) )
",3 Evaluation,0,[0]
"AVG d∈D
( RS(Ed(m)) )",3 Evaluation,0,[0]
#NAME?,3 Evaluation,0,[0]
"(7)
With RS(Ed(m))",3 Evaluation,0,[0]
the rank of m according to the evaluation measure E on dataset d w.r.t.,3 Evaluation,0,[0]
competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets.,3 Evaluation,0,[0]
"The results in tables 2, 3, and 4 demonstrate the intuition.",3 Evaluation,0,[0]
"Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems",3 Evaluation,0,[0]
S. The maximum value of CORE is 1 for the best performing system according to its rank.,3 Evaluation,0,[0]
"It also allows quantifying how consistently a system achieves high performance for the remaining systems.
",3 Evaluation,0,[0]
"TextFlow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in (Achananuparp et al., 2008), with gaps of +1.0 in F1 and +6.1 accuracy on MSRP and +4.2 F1 and +2.7% accuracy on RTE 3.",3 Evaluation,0,[0]
"4.1 Canonical Text Flow TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures.",4 Discussion,0,[0]
It also reached the best precision average with a gap of +1.8 to +6.3.,4 Discussion,0,[0]
"On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3 (cf. table 3).",4 Discussion,0,[0]
"On a rank level, XFc had the best consistent rank for
accuracy, F1 and precision, and the second best for recall.",4 Discussion,0,[0]
"When compared to state-of-the-art measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8.",4.2 Trained Text Flow,0,[0]
"XFt also obtained the second best F1 average with a -1.0 gap, but with clear inconsistencies according to the dataset.",4.2 Trained Text Flow,0,[0]
XFt obtained the best precision with a gap ranging from +0.8 to +7.1.,4.2 Trained Text Flow,0,[0]
XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%.,4.2 Trained Text Flow,0,[0]
"Both its recall and F1 performance can be explained by the fact that the measure was trained to optimize accuracy, and not the F1 score for the positive class; which also suggests that the approach could be adapted to F1 optimization if needed.",4.2 Trained Text Flow,0,[0]
"Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized.",4.3 Synthesis,0,[0]
"We argue that this consistency was made possible through the asymmetry of XF which allowed it to adapt to different kinds of similarities (i.e., equivalence/paraphrase, inference/entailment, and mutual distance/ranking).",4.3 Synthesis,0,[0]
These results also show that the actual position difference is a relevant factor for text similarity.,4.3 Synthesis,0,[0]
"We explain it mainly by the natural flow of language where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are
equivalent in meaning.",4.3 Synthesis,0,[0]
"The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.",4.3 Synthesis,0,[0]
"In additional experiments, we compared TFc and TFt with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens.",4.4 Additional Experiments,0,[0]
The results were significantly lower on all datasets (between 3 and 10 points loss in accuracy) for both the soa measures and TextFlow variants.,4.4 Additional Experiments,0,[0]
"This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of a new word inside a 3-words sequence leads to a tri-gram mismatch and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence.",4.4 Additional Experiments,0,[0]
"This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words.",4.4 Additional Experiments,0,[0]
"It must be noted here that not all languages grant the same level of importance to sequences and that additional multilingual tests have to be carried out.
",4.4 Additional Experiments,0,[0]
"In addition to binary classification output such as textual entailment and paraphrase recognition, text similarity measures can be evaluated more precisely when we consider the correlation of their values for ranking purposes.
",4.4 Additional Experiments,0,[0]
"We conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs.",4.4 Additional Experiments,0,[0]
"The datasets have 750 sentence pairs each, and are extracted from
the Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl11.",4.4 Additional Experiments,0,[0]
"When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correlation on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, noting that the Cosine similarity uses word frequencies when the evaluated version of TextFlow did not use word-level weights.
",4.4 Additional Experiments,0,[0]
Including word weights is one of the promising perspectives in line with this work as it could be done simply by making the deltas vary according to the weight/importance of the (un)matched word.,4.4 Additional Experiments,0,[0]
"Also, in such a setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. figure 3).",4.4 Additional Experiments,0,[0]
"We conducted a preliminary test using the inverse document frequency of the words as extracted from Wikipedia with Gensim12, which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them.",4.4 Additional Experiments,0,[0]
"Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances (Pedersen et al., 2004).",4.4 Additional Experiments,0,[0]
We presented a novel standalone similarity measure that takes into account continuous word sequences.,5 Conclusion,0,[0]
"An evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking.",5 Conclusion,0,[0]
"Among the potential extensions of this work are the inclusion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness.",5 Conclusion,0,[0]
"We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words.",5 Conclusion,0,[0]
"This work was supported in part by the Intramural Research Program of the NIH, National Library of Medicine.
",Acknowledgements,0,[0]
11goo.gl/NVnybD 12https://radimrehurek.com/gensim/,Acknowledgements,0,[0]
"Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment.",abstractText,0,[0]
"While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language.",abstractText,0,[0]
Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts.,abstractText,0,[0]
In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms.,abstractText,0,[0]
"The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value.",abstractText,0,[0]
"Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",abstractText,0,[0]
TextFlow: A Text Similarity Measure based on Continuous Sequences,title,0,[0]
"Proceedings of NAACL-HLT 2018, pages 177–184 New Orleans, Louisiana, June 1 - 6, 2018. c©2017 Association for Computational Linguistics",text,0,[0]
"Amazon has developed Alexa, a voice assistant that has been deployed across millions of devices and processes voice requests in multiple languages.",1 Introduction,0,[0]
"This paper addresses improvements to the Alexa voice service, whose core capabilities (as measured by the number of supported intents and slots) has expanded more than four-fold over the last two years.",1 Introduction,0,[0]
In addition more than ten thousand voice skills have been created by third-party developers using the Alexa Skills Kit (ASK).,1 Introduction,0,[0]
"In order to continue this expansion, new voice experiences must be both accurate and capable of supporting complex interactions.
",1 Introduction,0,[0]
"However, as the number of features has expanded, adding new features has become increasingly difficult for four primary reasons.",1 Introduction,0,[0]
"First, requests with a similar surface form may belong to different domains, which makes it challenging to add features without degrading the accuracy of existing domains.",1 Introduction,0,[0]
"For example, similar linguistic phrases such as “order me an echo dot” (e.g., for Shopping) have a similar form to phrases used for a ride-hailing feature such as, “Alexa, order me
a taxi”.",1 Introduction,0,[0]
"The second challenge is that a fixed flat structure is unable to easily support certain features (Gupta et al., 2006b), such as cross-domain queries or complex utterances, which cannot be clearly categorized into a given domain.",1 Introduction,0,[0]
"For example, “Find me a restaurant near the sharks game” contains both local businesses and sporting events and “Play hunger games and turn the lights down to 3” requires a representation that supports assigning an utterance to two intents.",1 Introduction,0,[0]
"The third challenge is that there is no mechanism to represent ambiguity, forcing the choice of a fixed interpretation for ambiguous utterances.",1 Introduction,0,[0]
"For example, “Play Hunger Games” could refer to an audiobook, a movie, or a soundtrack.",1 Introduction,0,[0]
"Finally, representations are not reused between skills, leading to the need for each developer to create a custom data and representations for their voice experiences.
",1 Introduction,0,[0]
"In order to address these challenges and make Alexa more capable and accurate, we have developed two key components.",1 Introduction,0,[0]
"The first is the Alexa ontology, a large hierarchical ontology that contains fine-grained types, properties, actions and roles.",1 Introduction,0,[0]
"Actions represent a predicate that determines what the agent should do, roles express the arguments to an action, types categorize textual mentions and properties are relations between type mentions.",1 Introduction,0,[0]
"The second component is the Alexa Meaning Representation Language (AMRL), a graph-based domain and language independent meaning representation that can capture the meaning of spoken language utterances to intelligent assistants.",1 Introduction,0,[0]
"AMRL is a rooted graph where action, operators, relations and classes are labeled vertices and properties and roles are labeled edges.",1 Introduction,0,[0]
"Unlike typical representations for spoken language understanding (SLU), which factors language understanding into the prediction of intents (nonoverlapping actions) and slots (e.g., named entities) (Gupta et al., 2006a), our representation is
177
grounded in the Alexa ontology, which provides a common semantic representation for spoken language understanding and can directly represent ambiguity, complex nested utterances and crossdomain queries.",1 Introduction,0,[0]
"Unlike similar meaning representations such as AMR (Banarescu et al., 2013), AMRL is designed to be cross-lingual, explicitly represent fine-grained entity types, logical statements, spatial prepositions and relationships and support type mentions.",1 Introduction,0,[0]
"Examples of AMRL and the SLU representations can be seen in Figure 1.
",1 Introduction,0,[0]
"The AMRL has been released via Alexa Skills Kit (ASK) built-in intents and slots in 2016 at a developers conference, offering coverage for eight of the ∼20 SLU domains 1.",1 Introduction,0,[0]
"In addition to these domains, we have demonstrated that the AMRL can cover a wide range of additional utterances by annotating a sample from all first and thirdparty applications.",1 Introduction,0,[0]
We have manually annotated data for 20k examples using the Alexa ontology.,1 Introduction,0,[0]
"This data includes the annotation of∼100 actions, ∼500 types, ∼20 roles and ∼172 properties.",1 Introduction,0,[0]
"This paper describes a common representation for SLU, consisting of two primary components: • The Alexa ontology - A large-scale hierarchi-
cal ontology developed to cover all spoken language usage.",2 Approach,0,[0]
•,2 Approach,0,[0]
"The Alexa meaning representation language
(AMRL) - A rooted graph that provides a common semantic representation, is compositional and can support complex user requests.
",2 Approach,0,[0]
These two components are described in the following sections.,2 Approach,0,[0]
The Alexa ontology provides a common semantics for SLU.,2.1 The Alexa ontology,0,[0]
"The Alexa ontology is developed in RDF and consists of five primary components: • Classes A hierarchy of Classes, also re-
ferred to as types, is defined in the ontology.",2.1 The Alexa ontology,0,[0]
"This hierarchy is a rooted tree, with finergrained types at deeper levels.",2.1 The Alexa ontology,0,[0]
"Coarse types that are children of THING include PERSON, PLACE, INTANGIBLE, ACTION, PRODUCT, CREATIVEWORK, EVENT and ORGANIZATION.",2.1 The Alexa ontology,0,[0]
"Fine-grained types include MUSICRECORDING and RESTAURANT.
",2.1 The Alexa ontology,0,[0]
"1https://amzn.to/2qDjNcJ
• Properties A given class contains a list of properties, which relate that class to other classes.",2.1 The Alexa ontology,0,[0]
"Properties are defined in a hierarchy, with finer-grained classes inheriting the properties of its parent.",2.1 The Alexa ontology,0,[0]
There are range restrictions on the available types for both the domain and range of the property.,2.1 The Alexa ontology,0,[0]
•,2.1 The Alexa ontology,0,[0]
"Actions A hierarchy of actions are defined as
classes within the ontology.",2.1 The Alexa ontology,0,[0]
ACTIONS cover the core functionality of Alexa.,2.1 The Alexa ontology,0,[0]
•,2.1 The Alexa ontology,0,[0]
"Roles ACTIONS operate on entities via roles.
",2.1 The Alexa ontology,0,[0]
"The most common role for an ACTION is the .object role, which is defined to be the entity on which the ACTION operates.",2.1 The Alexa ontology,0,[0]
"• Operators and Relations A hierarchy of op-
erators and relations represent complex relationships that cannot be expressed easily as properties.",2.1 The Alexa ontology,0,[0]
"Represented as classes, these include ComparativeOperator, Equals and Coordinator (Figure 2).
",2.1 The Alexa ontology,0,[0]
The Alexa ontology utilized schema.org as its base and has been updated to include support for spoken language.,2.1 The Alexa ontology,0,[0]
"In addition, using schema.org as the base of the Alexa Ontology means that it shares a vocabulary used by more than 10 million websites, which can be linked to the Alexa ontology.",2.1 The Alexa ontology,0,[0]
"AMRL leverages classes, properties, actions, roles and operators in the main ontology to create a compositional, graph-based representation of the meaning of an utterance.",2.2 Alexa meaning representation language,0,[0]
The graph-based representation conceptualizes each arc as a property and each node as an instance of a type; each type can have multiple parents.,2.2 Alexa meaning representation language,0,[0]
Conventions have been developed to annotate the AMRL for an utterance accurately and consistently.,2.2 Alexa meaning representation language,0,[0]
"These conventions focus primarily on linguistic annotation, and only consider filled pauses, edits, and repairs in limited contexts.",2.2 Alexa meaning representation language,0,[0]
"The conventions include: • Fine-grained type mentions When an entity
type appears in an utterance, the most finegrained type will be annotated.",2.2 Alexa meaning representation language,0,[0]
"For “turn on the light”, the mention ‘light’ could be annotated as a DEVICE.",2.2 Alexa meaning representation language,0,[0]
"However, there is a more appropriate finer-grained type, LIGHTING which will be selected instead.",2.2 Alexa meaning representation language,0,[0]
"• Ambiguous type mentions When more than
one fine-grained type is possible, then the annotator will utilize a more coarse-grained
type in the hierarchy.",2.2 Alexa meaning representation language,0,[0]
This type should be the finest-grained type that still captures the ambiguity.,2.2 Alexa meaning representation language,0,[0]
"For example, in the utterance “play thriller’, “thriller” can either be a MUSICALBUM or a MUSICRECORDING.",2.2 Alexa meaning representation language,0,[0]
Instead of selecting one of these a more coarse-grained type of MUSICCREATIVEWORK will be chosen.,2.2 Alexa meaning representation language,0,[0]
"When the ambiguity would force fallback to the root class of the ontology THING, AMRL annotation chooses a sub-class and marks the usage of it as uncertain.",2.2 Alexa meaning representation language,0,[0]
"• Properties Properties are annotated when
they are unambiguous.",2.2 Alexa meaning representation language,0,[0]
"For example, “find books by truman capote”, the use of the .author property on the BOOK class is unambiguous.",2.2 Alexa meaning representation language,0,[0]
"Similarly, for “find books about truman capote” the use of the .about property on the BOOK class is unambiguous.",2.2 Alexa meaning representation language,0,[0]
• Ambiguous property usage,2.2 Alexa meaning representation language,0,[0]
"When there is
uncertainty in the property that should be selected for the representation, the annotator may fall back to a more generic property.",2.2 Alexa meaning representation language,0,[0]
• Property inverses,2.2 Alexa meaning representation language,0,[0]
"When a property can
be annotated in two different directions, a canonical property is defined in the ontology and used for all annotations.",2.2 Alexa meaning representation language,0,[0]
"For example, .parentOrganization has an inverse of .subOrganization.",2.2 Alexa meaning representation language,0,[0]
"The former is selected as canonical for annotation flexibility and to
eliminate cycles in the graph.
",2.2 Alexa meaning representation language,0,[0]
A few of these properties have special meaning at annotation time.,2.2 Alexa meaning representation language,0,[0]
"Specifically, for the annotation of textual mentions there exist three primary properties: .name, .value and .type.",2.2 Alexa meaning representation language,0,[0]
"The conventions for these properties are as follows:
• .name",2.2 Alexa meaning representation language,0,[0]
"This is a nominal mention in the utterance, the .name property links the text to an instance of a class.",2.2 Alexa meaning representation language,0,[0]
.name is only used for mentions that are not a numeric quantity or enumeration.,2.2 Alexa meaning representation language,0,[0]
An example of .name for a MUSICIAN class would be “madonna”.,2.2 Alexa meaning representation language,0,[0]
• .value,2.2 Alexa meaning representation language,0,[0]
"This is defined in the same way as
.name but is used for mentions that are numeric quantities or enumerations.",2.2 Alexa meaning representation language,0,[0]
"For instance, “two” would be a .value of an INTEGER class.",2.2 Alexa meaning representation language,0,[0]
• .type,2.2 Alexa meaning representation language,0,[0]
"This is a generic mention of an entity
type.",2.2 Alexa meaning representation language,0,[0]
"For example, “musician” is a .type mention of the MUSICIAN class.
",2.2 Alexa meaning representation language,0,[0]
One action (NULLACTION) has a special meaning.,2.2 Alexa meaning representation language,0,[0]
This is annotated whenever a SLU query does not have an associated action or the action is unclear.,2.2 Alexa meaning representation language,0,[0]
"This happens, for example, when someone says, “temperature”.",2.2 Alexa meaning representation language,0,[0]
"In contrast, “show me the temperature” is annotated with the more specific DISPLAYACTION.",2.2 Alexa meaning representation language,0,[0]
AMRL has been used to represent utterances that are either not supported or challenging to support using standard SLU representations.,2.3 Expanded Language Support,0,[0]
"The following section describes support for anaphora, complex and cross-domain utterances, referring expressions for locations and composition.",2.3 Expanded Language Support,0,[0]
AMRL can natively support pronominal anaphora resolution both within the same utterance or across utterances.,2.3.1 Anaphora,0,[0]
"For example: • Within utterance: “Find the highest-rated
toaster and show me its reviews’’ •",2.3.1 Anaphora,0,[0]
"Across utterances: “What is Madonna’s lat-
est album” “Play it.”",2.3.1 Anaphora,0,[0]
Terminal nodes refer back to the same (unique) entity.,2.3.1 Anaphora,0,[0]
An example annotation across multiple utterances can be seen in Figures 3a and 3b.,2.3.1 Anaphora,0,[0]
"Similar to the above, it can handle bridges within discourse, such as, “find me an italian restaurant” and “what’s on its menu.”",2.3.1 Anaphora,0,[0]
AMRL contains nodes that are not grounded in the text.,2.3.2 Inferred nodes,0,[0]
"For example, for the utterance, in Figure 2a there are two inferred nodes, one for the address of the restaurant and another for the address of the sports event.",2.3.2 Inferred nodes,0,[0]
Not explicitly representing types has two primary benefits.,2.3.2 Inferred nodes,0,[0]
"First, certain linguistic phenomena such as anaphora are easier to support.",2.3.2 Inferred nodes,0,[0]
"Second, the representation is aligned to the ontology, which enables direct queries against the knowledge base.",2.3.2 Inferred nodes,0,[0]
Inferred nodes are the AMRL way to perform reification.,2.3.2 Inferred nodes,0,[0]
Using the common semantics of AMRL means that parses do not need to obey domain boundaries.,2.3.3 Cross-domain utterances,0,[0]
"For example, these utterances would belong to two domains (e.g., sports and local search): “Where is the nearest restaurant” and “What is happening at the Sharks game”.",2.3.3 Cross-domain utterances,0,[0]
"AMRL, as in Figure 2a, can handle utterances that span multiple domains, such as the one shown in Figure 2a.",2.3.3 Cross-domain utterances,0,[0]
"AMRL can cover logical expressions, where there can be an arbitrary combination of conjunctions, disjunctions, or conditional statements.","2.3.4 Conjunctions, disjunctions and negations",0,[0]
"Some examples of object-level or clause-level conjunctions
include: • Object-level conjunction: “Add milk, bread,
and eggs to my shopping list” • Clause-level conjunction: “Restart this song
and turn the volume up to seven” Conjunctions and disjunctions are represented using a Coordinator class.","2.3.4 Conjunctions, disjunctions and negations",0,[0]
The “.value” property defined which logical operation is to be performed.,"2.3.4 Conjunctions, disjunctions and negations",0,[0]
Examples of the AMRL representation for these is shown in Figure 2b and 2c.,"2.3.4 Conjunctions, disjunctions and negations",0,[0]
Conditional statements are not usually represented in other formalisms.,2.3.5 Conditional statements,0,[0]
"An example of a conditional statement is, “when its raining, turn off the sprinklers”.",2.3.5 Conditional statements,0,[0]
Time-based conditional statements are special cased due to their frequency in spoken language.,2.3.5 Conditional statements,0,[0]
"For time-based expressions (e.g., “when it is three p.m., turn on the lights”), a startTime (or endTime) property is used on the action to denote the condition of when the action should start (or stop).",2.3.5 Conditional statements,0,[0]
"For all other expressions, we use the ConditionalOperator, which has a “condition” property as well as a “result” property.",2.3.5 Conditional statements,0,[0]
"When the condition is true, then the result would apply.",2.3.5 Conditional statements,0,[0]
The constrained properties are defining the arguments of the Equals operator.,2.3.5 Conditional statements,0,[0]
An example can be seen in Figure 4.,2.3.5 Conditional statements,0,[0]
A deterministic transformation from the simplified time-based scheme to ConditionalOperator form when greater consistency is desired.,2.3.5 Conditional statements,0,[0]
AMRL can represent locations and their relationships.,2.3.6 Referring expressions for locations,0,[0]
"For simpler expressions that are common, such as “on” or “in,” properties are used to represent the relationship between two entity mentions.",2.3.6 Referring expressions for locations,0,[0]
"For other spatial relations, such as “between” or “around,” an operator is introduced.",2.3.6 Referring expressions for locations,0,[0]
Two examples of spatial relationships can be seen in Figure 2d.,2.3.6 Referring expressions for locations,0,[0]
"In this example “beside” grounds to the relation being used (e.g., “beside”) and uses two properties (e.g., constrained and target), which are the the first and second arguments to the spatial preposition.",2.3.6 Referring expressions for locations,0,[0]
"AMRL supports composition, which enables reuse of types and subgraphs to represent utterances with similar meanings.",2.3.7 Composition,0,[0]
"For example, Figures 2e and 2f show the ability to create significantly different actions only by changing the type of the object of the utterance.",2.3.7 Composition,0,[0]
"Such substitution can occur
anywhere in the annotation graph.",2.3.7 Composition,0,[0]
PlaybackAction is used to denote playing of the entity referred to by the object role.,2.3.7 Composition,0,[0]
"Although many linguistic phenomena can be supported in AMRL, there are a few that have not been explicitly supported and are left for future work.",2.3.8 Unsupported features,0,[0]
These include existential and universal quantification and scoping and conventions for agency (most requests are imperative).,2.3.8 Unsupported features,0,[0]
"In addition, there is currently no easy way to convert to first order logic (e.g., lambda calculus), due to conventions that simplify annotation, but lose information about operators such as spatial relationships.",2.3.8 Unsupported features,0,[0]
Data has been collected for the AMRL across many spoken language use-cases.,3 Dataset,0,[0]
"The current domains that are supported include music, books, video, local search, weather and calendar.",3 Dataset,0,[0]
"We have prototyped mechanisms to speed up annotation via paraphrasing (Berant and Liang, 2014) and conversion from our current SLU representation, in order to leverage the much larger data available.",3 Dataset,0,[0]
The primary mechanism we have for data-acquisition is via manual annotation.,3 Dataset,0,[0]
"Tools have been developed in order to acquire the full graph annotated with all the properties, classes, actions and operators.
",3 Dataset,0,[0]
AMRL manual annotation is performed by data annotators in four stages.,3 Dataset,0,[0]
"In the first stage an action is selected, for example ACTIVATEACTION in Figure 1b.",3 Dataset,0,[0]
"The second stage defines the text spans in an utterance that link to a class in the ontology (e.g., “michael jackson” is a Musician type and “thriller” and “song” are MusicRecording types, the first is a .name mention, while the latter is a .type mention.",3 Dataset,0,[0]
The third stage creates connections between the classes and defines any missing nodes in the graph.,3 Dataset,0,[0]
In the final stage a skilled annotator reviews the graph for mistakes and and re-annotates it if necessary.,3 Dataset,0,[0]
"There is a visualization of the semantic annotation available, enabling an annotator to verify that they have built the graph in a semantically accurate manner.",3 Dataset,0,[0]
Manual annotation happens at the rate of 40 per hour.,3 Dataset,0,[0]
"The manually annotated dataset contains∼20k annotated utterances and contains 93 unique actions,
448 types, 172 properties and 23 roles.",3 Dataset,0,[0]
Any graph parsing method can be used to predict AMRL given a natural language utterance.,4 Parsing,0,[0]
"One approach is to use hyperedge replacement grammars (Chiang et al., 2013) (Peng et al., 2015), though these require large datasets in order to train accurate parsers.",4 Parsing,0,[0]
"Alternatively, the graph can be linearized, as in (Gildea et al., 2017) and sequence to sequence or sequential models can be used to predict AMRL (Perera and Strubell, 2018).",4 Parsing,0,[0]
"We have shown that AMRL full-parse accuracy is at 78%, though the serialization, use of embeddings from related tasks can improve parser accuracy.",4 Parsing,0,[0]
"More details can be found in (Perera and Strubell, 2018).",4 Parsing,0,[0]
"FreeBase (Bollacker et al., 2008) (now WikiData) and schema.org (Guha et al., 2016) are two common ontologies.",5 Related Work,0,[0]
"Schema.org is widely used on the web and contains actions, types and properties.",5 Related Work,0,[0]
"The Alexa ontology expands schema.org to cover types, properties and roles used in spoken language.
",5 Related Work,0,[0]
"Semantic parsing has been investigated in the content of small domain-specific datasets such as GeoQuery (Wong and Mooney, 2006) and in the context of larger broad-coverage representations such as the Groningen Meaning Bank (GMB) (Bos et al., 2017), the Abstract Meaning Representation (AMR) (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013), PropBank (Kingsbury and Palmer, 2002), Raiment (Baker et al., 1998) and lambda-DCS (Kingsbury and Palmer, 2002).",5 Related Work,0,[0]
"OntoNotes (Hovy et al., 2006), lambdaDCS s (Liang, 2013) (Baker et al., 1998), FrameNet (Baker et al., 1998), combinatory categorial grammars (CCG) (Steedman and
Baldridge, 2011) (Hockenmaier and Steedman, 2007), universal dependencies (Nivre et al., 2016) are all related representations.",5 Related Work,0,[0]
A comparison of semantic representations for natural language semantics is described in Abend and Rappoport.,5 Related Work,0,[0]
"Unlike these meaning representations for written language, AMRL covers question answering, imperative actions, and a wide range of new types and properties (e.g., smart home, timers, etc.).
",5 Related Work,0,[0]
"AMR and AMRL are both rooted, directed, leaf-labeled and edge-labeled graphs.",5 Related Work,0,[0]
"AMRL does not reuse PropBank frame arguments, covers predicate-argument relations, including a wide variety of semantic roles, modifiers, co-reference, named entities and time expressions (Banarescu et al., 2013).",5 Related Work,0,[0]
There are more than 1000 namedentity types in AMRL (AMR has around 80).,5 Related Work,0,[0]
Reentrancy is not used in AMRL notation.,5 Related Work,0,[0]
"In addition to the AMR “name” property, AMRL contains a “type” property for mentions of a type (or class) and a “value” property for the mention of numeric values.",5 Related Work,0,[0]
"Anaphora is handled in AMRL for spoken dialog Poesio and Artstein (Gross et al., 1993).",5 Related Work,0,[0]
"Unlike representations used for spoken language understanding (SLU) (Gupta et al., 2006b), AMRL represents both entity spans, complex natural language expressions, and fine-grained named-entity types.",5 Related Work,0,[0]
"This paper develops AMRL, a meaning representation for spoken language.",6 Conclusions and Future Work,0,[0]
"We have shown how it can be used to expand the set of supported usecases to complex and cross-domain utterances, while leveraging a single compositional semantics.",6 Conclusions and Future Work,0,[0]
The representation has been released at AWS Re:Invent 2016 2.,6 Conclusions and Future Work,0,[0]
"It is also being used as a representation for expanded support for complex utterances, such as those with sequential composi-
2https://amzn.to/2qDjNcJ
tion.",6 Conclusions and Future Work,0,[0]
"Continued development of a common meaning representation for spoken language will enable Alexa to become capable and accurate, expanding the set of functionality for all Alexa users.",6 Conclusions and Future Work,0,[0]
This paper introduces a meaning representation for spoken language understanding.,abstractText,0,[0]
"The Alexa meaning representation language (AMRL), unlike previous approaches, which factor spoken utterances into domains, provides a common representation for how people communicate in spoken language.",abstractText,0,[0]
"AMRL is a rooted graph, links to a large-scale ontology, supports cross-domain queries, finegrained types, complex utterances and composition.",abstractText,0,[0]
"A spoken language dataset has been collected for Alexa, which contains ∼ 20k examples across eight domains.",abstractText,0,[0]
A version of this meaning representation was released to developers at a trade show in 2016.,abstractText,0,[0]
The Alexa Meaning Representation Language,title,0,[0]
"Proceedings of NAACL-HLT 2018, pages 1930–1940 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,0,[0]
Most house cats face enemies.,1 Introduction,0,[0]
Russia has the opposite objectives of the US.,1 Introduction,0,[0]
"There is much innovation in 3-d printing and it is sustainable.
",1 Introduction,0,[0]
What do the three propositions have in common?,1 Introduction,0,[0]
They were never uttered but solely presupposed in arguments made by the participants of online discussions.,1 Introduction,0,[0]
Presuppositions are a fundamental pragmatic instrument of natural language argumentation in which parts of arguments are left unstated.,1 Introduction,0,[0]
"This phenomenon is also referred to as
1Available at https://github.com/UKPLab/ argumentreasoning-comprehension-task/, including source codes and supplementary materials.
common knowledge (Macagno and Walton, 2014, p. 218), enthymemes (Walton, 2007b, p. 12), tacit major premises (Amossy, 2009, p. 319), or implicit warrants (Newman and Marshall, 1991, p. 8).",1 Introduction,0,[0]
"Wilson and Sperber (2004) suggest that, when we comprehend arguments, we reconstruct their warrants driven by the cognitive principle of relevance.",1 Introduction,0,[0]
"In other words, we go straight for the interpretation that seems most relevant and logical within the given context (Hobbs et al., 1993).",1 Introduction,0,[0]
"Although any incomplete argument can be completed in different ways (Plumer, 2016), it is assumed that certain knowledge is shared between the arguing parties (Macagno and Walton, 2014, p. 180).
",1 Introduction,0,[0]
"Filling the gap between the claim and premises (aka reasons) of a natural language argument empirically remains an open issue, due to the inherent difficulty of reconstructing the world knowledge and reasoning patterns in arguments.",1 Introduction,0,[0]
"In a direct fashion, Boltužić and Šnajder (2016) let annotators write down implicit warrants, but they concluded only with a preliminary analysis due to large variance in the responses.",1 Introduction,0,[0]
"In an indirect fashion, implicit warrants correspond to major premises in argumentation schemes; a concept heavily referenced in argumentation theory (Walton, 2012).",1 Introduction,0,[0]
"However, mapping schemes to realworld arguments has turned out difficult even for the author himself.
",1 Introduction,0,[0]
"Our main hypothesis is that, even if there is no limit to the tacit length of the reasoning chain between claims and premises, it is possible to systematically reconstruct a meaningful warrant, depending only on what we take as granted and what needs to be explicit.",1 Introduction,0,[0]
"As warrants encode our current presupposed world knowledge and connect the reason with the claim in a given argument, we expect that other warrants can be found which connect the reason with a different claim.",1 Introduction,0,[0]
"In the ex-
1930
Title: Is Marijuana a Gateway Drug?",1 Introduction,0,[0]
"Description: Does using marijuana lead to the use of more dangerous drugs, making it too dangerous to legalize?",1 Introduction,0,[0]
Reason:,1 Introduction,0,[0]
Milk isn’t a gateway drug even though most people drink it as children.,1 Introduction,0,[0]
"And since {Warrant 1 | Warrant 2}, Claim: Marijuana is not a gateway drug.",1 Introduction,0,[0]
"4 Warrant 1: milk is similar to marijuana 7 Warrant 2: milk is not marijuana
Figure 1: Instance of the argument reasoning comprehension task.",1 Introduction,0,[0]
The correct warrant has to be identified.,1 Introduction,0,[0]
"Notice the fallacious presupposed false analogy used by the author to make the argument.
",1 Introduction,0,[0]
"treme case, there may exist an alternative warrant in which the same reason is connected to the opposite claim.
",1 Introduction,0,[0]
The intuition of alternative warrants is key to the systematic methodology that we develop in this paper for reconstructing a warrant for the original claim of an argument.,1 Introduction,0,[0]
"In particular, we first ‘twist’ the stance of a given argument, trying to plausibly explain its reasoning towards the opposite claim.",1 Introduction,0,[0]
"Then, we twist the stance back and use a similar reasoning chain to come up with a warrant for the original argument.",1 Introduction,0,[0]
"As we discuss further below, this works for real-world arguments with a missing piece of information that is taken for granted and considered as common knowledge, yet, would lead to the opposite stance if twisted.
",1 Introduction,0,[0]
We demonstrate the applicability of our methodology in a large crowdsourcing study.,1 Introduction,0,[0]
"The study results in 1,970 high-quality instances for a new task that we call argument reasoning comprehension:",1 Introduction,0,[0]
"Given a reason and a claim, identify the correct warrant from two opposing options.",1 Introduction,0,[0]
An example is given in Figure 1.,1 Introduction,0,[0]
A solution to this task will represent a substantial step towards automatic warrant reconstruction.,1 Introduction,0,[0]
"However, we present experiments with several neural attention and language models which reveal that current approaches based on the words and phrases in arguments and warrants do not suffice to solve the task.
",1 Introduction,0,[0]
The main contributions of this paper are (1) a methodology for obtaining implicit warrants realized by means of scalable crowdsourcing and (2) a new task along with a high-quality dataset.,1 Introduction,0,[0]
"In addition, we provide (a) 2,884 user-generated arguments annotated for their stance, covering 50+ controversial topics, (b) 2,026 arguments with annotated reasons supporting the stance, (c) 4,235 rephrased reason gists, useful for argument summarization and sentence compression, and (d) a
method for checking the reliability of crowdworkers in document and span labeling using traditional inter-annotator agreement measures.",1 Introduction,0,[0]
"It is widely accepted that an argument consists of a claim and one or more premises (reasons) (Damer, 2013).",2 Related Work,0,[0]
Toulmin (1958) elaborated on a model of argument in which the reason supports the claim on behalf of a warrant.,2 Related Work,0,[0]
The abstract structure of an argument then is Reason→ (since) Warrant→ (therefore) Claim.,2 Related Work,0,[0]
"The warrant takes the role of an inference rule, similar to the major premise in Walton’s terminology (Walton, 2007a).
",2 Related Work,0,[0]
"In principle, the chain Reason → Warrant → Claim is applicable to deductive arguments and syllogisms, which allows us to validate arguments properly formalized in propositional logic.",2 Related Work,0,[0]
"However, most natural language arguments are in fact inductive (Govier, 2010, p. 255) or defeasible (Walton, 2007b, p. 29).2",2 Related Work,0,[0]
"Accordingly, the unsuitability of formal logic for natural language arguments has been discussed by argumentation scholars since the 1950’s (Toulmin, 1958).",2 Related Work,0,[0]
"To be clear, we do not claim that arguments cannot be represented logically (e.g., in predicate logic), however the drift to informal logic in the 20th century makes a strong case that natural language argumentation is more than modus ponens (van Eemeren et al., 2014).
",2 Related Work,0,[0]
"In argumentation theory, the notion of a warrant has also been contentious.",2 Related Work,0,[0]
"Some argue that the distinction of warrants from premises is clear only in Toulmin’s examples but fails in practice, i.e., it is hard to tell whether the reason of a given argument is a premise or a warrant (van Eemeren et al., 1987, p. 205).",2 Related Work,0,[0]
"However, Freeman (2011) provides alternative views on modeling an argument.",2 Related Work,0,[0]
"Given a claim and two or more premises, the argument structure is linked if the reasoning step involves the logical conjunction of the premises.",2 Related Work,0,[0]
"If we treat a warrant as a simple premise, then the linked structure fits the intuition behind Toulmin’s model, such that premise and warrant combined give support to the claim.",2 Related Work,0,[0]
"For details, see (Freeman, 2011, Chap. 4).
",2 Related Work,0,[0]
2A recent empirical example is provided by Walker et al. (2014) who propose possible approaches to identify patterns of inference from premises to claims in vaccine court cases.,2 Related Work,0,[0]
"The authors conclude that it is extremely rare that a reasoning is explicitly laid out in a deductively valid format.
",2 Related Work,0,[0]
"What makes comprehending and analyzing arguments hard is that claims and warrants are usually implicit (Freeman, 2011, p. 82).",2 Related Work,0,[0]
"As they are ‘taken for granted’ by the arguer, the reader has to infer the contextually most relevant content that she believes the arguer intended to use.",2 Related Work,0,[0]
"To this end, the reader relies on common sense knowledge (Oswald, 2016; Wilson and Sperber, 2004).
",2 Related Work,0,[0]
The reconstruction of implicit premises has already been faced in computational approaches.,2 Related Work,0,[0]
"In light of the design of their argument diagramming tool, Reed and Rowe (2004) pointed out that the automatic reconstruction is a task that skilled analysts find both taxing and hard to explain.",2 Related Work,0,[0]
"More recently, Feng and Hirst (2011) as well as Green (2014) outlined the reconstruction of missing enthymemes or warrants as future work, but they never approached it since.",2 Related Work,0,[0]
"To date, the most advanced attempt in this regard is from Boltužić and Šnajder (2016).",2 Related Work,0,[0]
The authors let annotators ‘reconstruct’ several propositions between premises and claims and investigated whether the number of propositions correlates with the semantic distance between the claim and the premises.,2 Related Work,0,[0]
"However, they conclude that the written warrants heavily vary both in depth and in content.",2 Related Work,0,[0]
"By contrast, we explore cases with a missing single piece of information that is considered as common knowledge, yet leading to the opposite conclusion if twisted.",2 Related Work,0,[0]
"Recently, Becker et al. (2017) also experimented with reconstructing implicit knowledge in short German argumentative essays.",2 Related Work,0,[0]
"In contrast to our work, they used expert annotators who iteratively converged to a single proposition.
",2 Related Work,0,[0]
"As the task we propose involves natural language comprehension, we also review relevant work outside argumentation here.",2 Related Work,0,[0]
"In particular, the goal of the semantic inference task textual entailment is to classify whether a proposition entails or contradicts a hypothesis (Dagan et al., 2009).",2 Related Work,0,[0]
"A similar task, natural language inference, was boosted by releasing the large SNLI dataset (Bowman et al., 2015) containing 0.5M entailment pairs crowdsourced by describing pictures.",2 Related Work,0,[0]
"While the understanding of semantic inference is crucial in language comprehension, argumentation also requires coping with phenomena beyond semantics.",2 Related Work,0,[0]
Rajpurkar et al. (2016) presented a large dataset for reading comprehension by answering questions over Wikipedia articles (SQuAD).,2 Related Work,0,[0]
"In an analysis of this dataset Sugawara and Aizawa (2016)
found, though, that only 6.2% of the questions require causal reasoning, 1.2% logical reasoning, and 0% analogy.",2 Related Work,0,[0]
"In contrast, these reasoning types often make up the core of argumentation (Walton, 2007a).",2 Related Work,0,[0]
"Mostafazadeh et al. (2016) introduced the cloze story test, in which the appropriate ending of a narrative has to be selected automatically.",2 Related Work,0,[0]
The overall context of this task is completely different to ours.,2 Related Work,0,[0]
"Moreover, the narratives were written from scratch by explicitly instructing crowd workers, whereas our data come from genuine argumentative comments.",2 Related Work,0,[0]
Common-sense reasoning was also approached by Angeli and Manning (2014) who targeted the inference of commonsense facts from a large knowledge base.,2 Related Work,0,[0]
"Since their logical formalism builds upon an enhanced version of Aristotle’s syllogisms, its applicability to natural language argumentation remains limited (see our discussion above).",2 Related Work,0,[0]
"In contrast to our data source, a few synthetic datasets for general natural language reasoning have been recently introduced, such as answers to questions over a described physical world (Weston et al., 2016) or an evaluation set of 100 questions in the Winograd Schema Challenge (Levesque et al., 2012).
",2 Related Work,0,[0]
"Finally, we note that, although being related, research on argument mining, argumentation quality, and stance classification is not in the immediate scope of this paper.",2 Related Work,0,[0]
"For details on these, we therefore refer to recent papers from Lippi and Torroni (2016); Habernal and Gurevych (2017) or Mohammad et al. (2016).",2 Related Work,0,[0]
"Let R be a reason for a claim C, both of which being propositions extracted from a natural language argument.",3 Argument Reasoning Comprehension,0,[0]
"Then there is a warrant W that justifies the use of R as support for C, but W is left implicit.
",3 Argument Reasoning Comprehension,0,[0]
"For example, in a discussion about whether declawing a cat should be illegal, an author takes the following position (which is her claim C): ‘It should be illegal to declaw your cat’.",3 Argument Reasoning Comprehension,0,[0]
She gives the following reason (R): ‘They need to use their claws for defense and instinct’.3,3 Argument Reasoning Comprehension,0,[0]
"The warrant W could then be ‘If cat needs claws for instincts, declawing would be against nature’ or similar.",3 Argument Reasoning Comprehension,0,[0]
"W remains implicit, because R already implies C quite obviously and so, according to common sense, any further explanation seems superfluous.
",3 Argument Reasoning Comprehension,0,[0]
"3The example is taken from our dataset introduced below.
",3 Argument Reasoning Comprehension,0,[0]
"Now, the question is how to find the warrant W for a given reason R and claim C.",3 Argument Reasoning Comprehension,0,[0]
"Our key hypothesis in the definition of the argument reasoning comprehension task is the existence of an alternative warrant AW that justifies the use of R as support for the opposite ¬C of the claim C (regardless of the question of how strong this justification is).
",3 Argument Reasoning Comprehension,0,[0]
"For the example above, assume that we ‘twist’ C to ‘It should be legal to declaw your cat’ (¬C) but use the same reason R. Is it possible to come up with an alternative warrant AW that justifies R?",3 Argument Reasoning Comprehension,0,[0]
"In the given case, ‘most house cats don’t face enemies’ would bridge R to ¬C quite plausibly.",3 Argument Reasoning Comprehension,0,[0]
"If we now use a reasoning based on AW but twist AW again such that it leads to the claim C, we get ‘most house cats face enemies’, which is a plausible warrant W for the original argument containing R and C. 4
Constructing an alternative warrant is not possible for all reason/claim pairs; in some reasons the arguer’s position is deeply embedded.",3 Argument Reasoning Comprehension,0,[0]
"As a result, trying to give a plausible reasoning for the opposite claim ¬C either leads to nonsense or to a proposition that resembles a rebuttal rather than a warrant (Toulmin, 1958).",3 Argument Reasoning Comprehension,0,[0]
"However, if both W and AW are available, they usually capture the core of a reason’s relevance and reveal the implicit presuppositions (examples follow further below).
",3 Argument Reasoning Comprehension,0,[0]
"Based on our key hypothesis, we define the argument reasoning comprehension task as:
Given a reason R and a claim C along with the title and a short description of the debate they occur in, identify the correct warrant W from two candidates: the correct warrant W and an incorrect alternative warrant AW.
",3 Argument Reasoning Comprehension,0,[0]
"An instance of the task is thus basically given by a tuple (R,C,W,AW ).",3 Argument Reasoning Comprehension,0,[0]
"The debate title and description serve as the context of R and C. As it is binary, we propose to evaluate the task using accuracy.",3 Argument Reasoning Comprehension,0,[0]
"We now describe our methodology to systematically reconstruct implicit warrants, along with the scalable crowdsourcing process that operationalizes the methodology.",4 Reconstruction of Implicit Warrants,0,[0]
"The result of the process is
4This way, we also reveal the weakness of the original argument that was hidden in the implicit premise.",4 Reconstruction of Implicit Warrants,0,[0]
"It can be challenged by asking the arguer whether house cats really face enemies.
",4 Reconstruction of Implicit Warrants,0,[0]
"a dataset with authentic instances (R,C,W,AW ) of the argument reasoning comprehension task.",4 Reconstruction of Implicit Warrants,0,[0]
"Instead of extending an existing dataset, we decided to create a new one from scratch, because we aimed to study a variety of controversial issues in user-generated web comments and because we sought for a dataset with a permissive license.
",4.1 Source Data,0,[0]
"As a source, we opted for the Room for Debate section of the New York Times.5 It provides authentic argumentation on contemporary issues with good editorial work and moderation — as opposed to debate portals such as createdebate.com, where classroom assignments, silly topics, and bad writing prevail.",4.1 Source Data,0,[0]
We manually selected 188 debates with polar questions in the title.,4.1 Source Data,0,[0]
"These questions are controversial and provoking, giving a stimulus for stance-taking and argumentation.6",4.1 Source Data,0,[0]
"For each debate we created two explicit opposing claims, e.g., ‘It should be illegal to declaw your cat’ and ‘It should be legal to declaw your cat’.",4.1 Source Data,0,[0]
"We crawled all comments from each debate and sampled about 11k high-ranked, root-level comments.7",4.1 Source Data,0,[0]
The methodology we propose consists of eight consecutive steps that are illustrated in Figure 2 and detailed below.,4.2 Methodology and Crowdsourcing Process,0,[0]
Each step can be operationalized with crowdsourcing.,4.2 Methodology and Crowdsourcing Process,0,[0]
"For our dataset, we performed crowdsourcing on 5,000 randomly sampled comments using Amazon Mechanical Turk (AMT) from December 2016 to April 2017.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Before, each comment was split into elementary discourse units (EDUs) using SistaNLP (Surdeanu et al., 2015).
1.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Stance Annotation For each comment, we first classify what stance it is taking (recall that we always have two explicit claims with opposing stance).",4.2 Methodology and Crowdsourcing Process,0,[0]
"Alternatively, it may be neutral (consider-
5https://www.nytimes.com/roomfordebate 6Detailed theoretical research on polar and alternative questions can be found in (van Rooy and Šafářová, 2003); Asher and Reese (2005) analyze bias and presupposition in polar questions.
",4.2 Methodology and Crowdsourcing Process,0,[0]
"7To remove ‘noisy’ candidates, we applied several criteria, such as the absence of quotations or URLs and certain lengths.",4.2 Methodology and Crowdsourcing Process,0,[0]
"For details, see the source code we provide.",4.2 Methodology and Crowdsourcing Process,0,[0]
"We did not check any quality criteria of arguments, as this was not our focus; see, e.g., (Wachsmuth et al., 2017) for argumentation quality.
",4.2 Methodology and Crowdsourcing Process,0,[0]
"ing both sides) or may not take any stance.8
All 2,884 comments in our dataset classified as stance-taking by the crowdworkers were then also annotated as to whether being sarcastic or ironic; both pose challenges in analyzing argumentation not solved so far (Habernal and Gurevych, 2017).
2.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Reason Span Annotation For all comments taking a stance, the next step is to select those spans that give a reason for the claim (with a single EDU as the minimal unit).
",4.2 Methodology and Crowdsourcing Process,0,[0]
"In our dataset, the workers found 5,119 reason spans, of which 2,026 lay within arguments.",4.2 Methodology and Crowdsourcing Process,0,[0]
"About 40 comments lacked any explicit reason.
3.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Reason Gist Summarization This new task is, in our view, crucial for downstream annotations.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Each reason from the previous step is rewritten, such that the reason’s gist in the argument remains the same but the clutter is removed (examples are given in the supplementary material which is available both in the ACL Anthology and the project GitHub site).",4.2 Methodology and Crowdsourcing Process,0,[0]
"Besides, wrongly annotated reasons are removed in this step.",4.2 Methodology and Crowdsourcing Process,0,[0]
"The result is pairs of reason R and claim C.
All 4,294 gists in our dataset were summarized under Creative Commons Zero license (CC-0).
4.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Reason Disambiguation Within our methodology, we need to be able to identify to what extent a reason itself implies a stance: While ‘C because R’ allows for many plausible interpretations (as discussed above), whether R→ C or R→ ¬C depends on how much presupposition is encoded in R.",4.2 Methodology and Crowdsourcing Process,0,[0]
"In this step, we decide which claim (C or ¬C) is most plausible for R, or whether both are
8We also experimented with approaching the annotations top-down starting by annotating explicit claims, but the results were unsatisfying.",4.2 Methodology and Crowdsourcing Process,0,[0]
"This is in line with empirical observations made by Habernal and Gurevych (2017) who showed that the majority of claims in user-generated arguments are implicit.
similarly plausible (in the given data, respective reasons turned out to be rather irrelevant though).
",4.2 Methodology and Crowdsourcing Process,0,[0]
"We used only those 1,955 instances where R indeed implied C according to the workers, as this suggests at least some implicit presupposition in R.
5.",4.2 Methodology and Crowdsourcing Process,0,[0]
Alternative Warrant,4.2 Methodology and Crowdsourcing Process,0,[0]
"This step is the trickiest, since it requires both creativity and ‘brain twisting’.",4.2 Methodology and Crowdsourcing Process,0,[0]
"As exemplified in Section 3, a plausible explanation needs to be given why R supports ¬C (i.e., the alternative warrant AW ).",4.2 Methodology and Crowdsourcing Process,0,[0]
"Alternatively, this may be classified as being impossible.
Exact instructions for our workers can be found in the provided sources.",4.2 Methodology and Crowdsourcing Process,0,[0]
"All 5,342 alternative warrants in our dataset are written under CC-0 license.
6.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Alternative Warrant Validation As the previous step produces largely uncontrolled writings, we validate each fabricated alternative warrant AW as to whether it actually relates to the reason R. To this end, we show AW and ¬C together with two alternatives: R itself and a distracting reason.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Only instances with correctly validated R are kept.
",4.2 Methodology and Crowdsourcing Process,0,[0]
"For our dataset, we sampled the distracting reason from the same debate topic, using the most dissimilar to R in terms of skip-thought vectors (Kiros et al., 2015) and cosine similarity.",4.2 Methodology and Crowdsourcing Process,0,[0]
"We kept 3,791 instances, for which the workers also rated how ‘logical’ the explanation of AW was (0– 2 scale).
7.",4.2 Methodology and Crowdsourcing Process,0,[0]
Warrant For Original Claim This step refers to the second task in the example from Section 3:,4.2 Methodology and Crowdsourcing Process,0,[0]
"Given R and C, make minimal modifications to the alternative warrant AW , such that it becomes an actual warrant W (i.e., such that R→W →C).
",4.2 Methodology and Crowdsourcing Process,0,[0]
"For our dataset, we restricted this step to those 2,613 instances that had a ‘logic score’ of at least 0.68 (obtained from the annotations mentioned
above), in order to filter out nonsense alternative warrants.",4.2 Methodology and Crowdsourcing Process,0,[0]
"All resulting 2,447 warrants were written by the workers again under CC0 license.
8.",4.2 Methodology and Crowdsourcing Process,0,[0]
"Warrant Validation To ensure that each tuple (R,C,W,AW ) allows only one logical explanation (i.e., either R→W→C or R→AW→C is correct, not both), all instances are validated again.
",4.2 Methodology and Crowdsourcing Process,0,[0]
Disputed cases in the dataset (according to our workers) were fixed by an expert to ensure quality.,4.2 Methodology and Crowdsourcing Process,0,[0]
"We ended up with 1,970 instances to be used for the argument reasoning comprehension task.",4.2 Methodology and Crowdsourcing Process,0,[0]
"To strictly assess quality in the entire crowdsourcing process, we propose an evaluation method that enables ‘classic’ inter-annotator agreement measures for crowdsourcing, such as Fleiss’ κ or Krippendorff’s α .",4.3 Agreement and Dataset Statistics,0,[0]
"Applying κ and α directly to crowdsourced data has been disputed (Passonneau and Carpenter, 2014).",4.3 Agreement and Dataset Statistics,0,[0]
"For estimating gold labels from the crowd, several models have been proposed; we rely on MACE (Hovy et al., 2013).",4.3 Agreement and Dataset Statistics,0,[0]
"Given a number of noisy workers, MACE outputs best estimates, outperforming simple majority votes.",4.3 Agreement and Dataset Statistics,0,[0]
"At least five workers are recommended for a crowdsourcing task, but how reliable is the output really?
We hence collected 18 assignments per item and split them into two groups (9+9) based on their submission time.",4.3 Agreement and Dataset Statistics,0,[0]
"We then considered each group as an independent crowdsourcing experiment and estimated gold labels using MACE for each group, thus yielding two ‘experts from the crowd.’",4.3 Agreement and Dataset Statistics,0,[0]
Having two independent ‘experts’ from the crowd allowed us to compute standard agreement scores.,4.3 Agreement and Dataset Statistics,0,[0]
We also varied the size of the sub-sample from each group from 1 to 9 by repeated random sampling of assignments.,4.3 Agreement and Dataset Statistics,0,[0]
"This revealed how the score varies with respect to the crowd size per ‘expert’.
",4.3 Agreement and Dataset Statistics,0,[0]
Figure 3 shows the Cohen’s κ agreement for stance annotation with respect to the crowd size computed by our method.,4.3 Agreement and Dataset Statistics,0,[0]
"As MACE also includes a threshold for keeping only the most confident predictions in order to benefit precision, we tuned this parameter, too.",4.3 Agreement and Dataset Statistics,0,[0]
Deciding on the number of workers per task is a trade-off between the desired quality and the budget.,4.3 Agreement and Dataset Statistics,0,[0]
"For example, reason span annotation is a harder task; however, the results for six workers are comparable to those for the expert annotations of Habernal and Gurevych (2017).9
9The supplementary material contains a detailed figure;
Table 1 lists statistics of the entire crowdsourcing process carried out for our dataset, including tasks for which we created data as a by-product.",4.3 Agreement and Dataset Statistics,0,[0]
"Below, we show three examples in which implicit common-sense presuppositions were revealed during the construction of the alternative warrant AW and the original warrant W .",4.4 Examples,0,[0]
"For brevity, we omit the debate title and description here.",4.4 Examples,0,[0]
"A full walkthrough example is found in the supplementary material.
",4.4 Examples,0,[0]
"R: Cooperating with Russia on terrorism ignores Russia’s overall objectives.
",4.4 Examples,0,[0]
C: Russia cannot be a partner.,4.4 Examples,0,[0]
AW :,4.4 Examples,0,[0]
Russia has the same objectives of the US.,4.4 Examples,0,[0]
W :,4.4 Examples,0,[0]
"Russia has the opposite objectives of the US.
",4.4 Examples,0,[0]
R: Economic growth needs innovation.,4.4 Examples,0,[0]
"C: 3-D printing will change the world.
",4.4 Examples,0,[0]
AW :,4.4 Examples,0,[0]
"There is no innovation in 3-d printing since it’s unsustainable.
W : There is much innovation in 3-d printing and it is sustainable.
",4.4 Examples,0,[0]
"R: College students have the best chance of knowing history.
",4.4 Examples,0,[0]
C: College students’ votes do matter in an election.,4.4 Examples,0,[0]
AW :,4.4 Examples,0,[0]
Knowing history doesn’t mean that we will repeat it.,4.4 Examples,0,[0]
W :,4.4 Examples,0,[0]
Knowing history means that we won’t repeat it.,4.4 Examples,0,[0]
"Given the dataset, we performed first experiments to assess the complexity of argument reasoning comprehension.",5 Experiments,0,[0]
"To this end, we split the 1,970 instances into three sets based on the year of the de-
not to be confused with Figure 3 which refers to stance annotation.
",5 Experiments,0,[0]
"bate they were taken from: 2011–2015 became the training set (1,210 instances), 2016 the development set (316 instances), and 2017 the test set (444 instances).",5 Experiments,0,[0]
This follows the paradigm of learning on past data and predicting on new ones.,5 Experiments,0,[0]
"In addition, it removes much lexical and topical overlap.",5 Experiments,0,[0]
"To evaluate human upper bounds for the task, we sampled 100 random questions (such as those presented in Section 4.4) from the test set and distributed them among 173 participants of an AMT survey.",5.1 Human Upper Bounds,0,[0]
Every participant had to answer 10 questions.,5.1 Human Upper Bounds,0,[0]
"We also asked the participants about their highest completed education (six categories) and the amount of formal training they have in reasoning, logic, or argumentation (no training, some, or extensive).",5.1 Human Upper Bounds,0,[0]
"In addition, they specified for each question how familiar they were with the topic (3- point scale).
",5.1 Human Upper Bounds,0,[0]
How Hard is the Task for Humans?,5.1 Human Upper Bounds,0,[0]
"It depends, as shown in Figure 4.",5.1 Human Upper Bounds,0,[0]
"Whereas education had almost negligible influence on the performance, the more extensive formal training in reasoning the participants had, the higher their score was.",5.1 Human Upper Bounds,0,[0]
"Overall, 30 of the 173 participants scored 100%.",5.1 Human Upper Bounds,0,[0]
"The mean score for those with extensive
formal training was 90.9%.",5.1 Human Upper Bounds,0,[0]
"For all participants, the mean was 79.8%.",5.1 Human Upper Bounds,0,[0]
"However, we have to note that some of the questions are more difficult than others, for which we could not control explicitly.
",5.1 Human Upper Bounds,0,[0]
Does Topic Familiarity Affect Human Performance?,5.1 Human Upper Bounds,0,[0]
"Not really, i.e., we found no significant (Spearman) correlation between the mean score and familiarity of a participant in almost all education/training configurations.",5.1 Human Upper Bounds,0,[0]
"This suggests that ar-
gument reasoning comprehension skills are likely to be independent of topic-specific knowledge.",5.1 Human Upper Bounds,0,[0]
"To assess the complexity of computationally approaching argument reasoning comprehension, we carried out first experiments with systems based on the following models.
",5.2 Computational Models,0,[0]
"The simplest considered model was the random baseline, which chooses either of the candidate warrants of an instance by chance.",5.2 Computational Models,0,[0]
"As another baseline, we used a 4-gram Modified Kneser-Ney language model trained on 500M tokens (100k vocabulary) from the C4Corpus (Habernal et al., 2016).",5.2 Computational Models,0,[0]
The effectiveness of language models was demonstrated by Rudinger et al. (2015) for the narrative cloze test where they achieved state-ofthe-art results.,5.2 Computational Models,0,[0]
"We computed log-likelihood of the candidate warrants and picked the one with lower score.10
To specifically appoach the given task, we implemented two neural models based on a bidirectional LSTM.",5.2 Computational Models,0,[0]
"In the standard attention version, we encoded the reason and claim using a BiLSTM and provided it as an attention vector after max-pooling to LSTM layers from the two available warrants W0 and W1 (corresponding to W and AW , see below).",5.2 Computational Models,0,[0]
"Our more elaborated version used intra-warrant attention, as shown in Figure 5.",5.2 Computational Models,0,[0]
Both versions were also extended with the debate title and description added as context to the attention layer (w/ context).,5.2 Computational Models,0,[0]
"We trained the resulting four models using the ADAM optimizer, with heavy dropout (0.9) and early stopping (5 epochs), tuned on the development set.",5.2 Computational Models,0,[0]
"Input embeddings were pre-trained word2vec’s (Mikolov et al., 2013).",5.2 Computational Models,0,[0]
"We ran each model three times with random initializations.
",5.2 Computational Models,0,[0]
"To evaluate all systems, each instance in our dataset is represented as a tuple (R,C,W0,W1) with a label (0 or 1).",5.2 Computational Models,0,[0]
"If the label is 0, W0 is the correct warrant, otherwise W1.",5.2 Computational Models,0,[0]
"Recall that we have two warrants W and AW whose correctness depends on the claim: W is correct for R and the original claim C, whereas AW would be correct for R and the opposite claim ¬C.",5.2 Computational Models,0,[0]
"We thus doubled the training data by adding a permuted instance (R,C,W1,W0) with the respective correct label; this led to increased performance.",5.2 Computational Models,0,[0]
"The overall
10This might seem counterintuitive, but since W is created by rewriting AW , it may suffer from some dis-coherency, which is then caught by the language model.
results of all approaches (humans and systems) are shown in Table 2.",5.2 Computational Models,0,[0]
"Intra-warrant attention with rich context outperforms standard neural models with a simple attention, but it only slightly beats the language model on the dev set.",5.2 Computational Models,0,[0]
"The language model is basically random on the test set.
",5.2 Computational Models,0,[0]
A manual error analysis of 50 random wrong predictions (a single run of the best-performing system on the dev set) revealed no explicit pattern of encountered errors.,5.2 Computational Models,0,[0]
Drawing any conclusions is hard given the diversity of included topics and the variety of reasoning patterns.,5.2 Computational Models,0,[0]
"A possible approach would be to categorize warrants using, e.g., argumentation schemes (Walton et al., 2008) and break down errors accordingly.",5.2 Computational Models,0,[0]
"However, this is beyond the scope here and thus left for future work.
",5.2 Computational Models,0,[0]
Can We Benefit from Alternative Warrants and Opposite Claims?,5.2 Computational Models,0,[0]
"Since the reasoning chain R→AW→¬C is correct, too, we also tried adding respective instances to the training set (thus doubling the size).",5.2 Computational Models,0,[0]
"In this configuration, however, the neural models failed to learn anything better than a random guess.",5.2 Computational Models,0,[0]
"The reason behind is probably that the opposing claims are lexically very close, usually negated, and the models cannot pick this up.",5.2 Computational Models,0,[0]
This underlines that argument reasoning comprehension cannot be solved by simply looking at the occurring words or phrases.,5.2 Computational Models,0,[0]
"We presented a new task called argument reasoning comprehension that tackles the core of reasoning in natural language argumentation — im-
plicit warrants.",6 Conclusion and Outlook,0,[0]
"Moreover, we proposed a methodology to systematically reconstruct implicit warrants in eight consecutive steps.",6 Conclusion and Outlook,0,[0]
"So far, we implemented the methodology in a manual crowdsourcing process, along with a strategy that enables standard inter-annotator agreement measures in crowdsourcing.
",6 Conclusion and Outlook,0,[0]
"Following the process, we constructed a new dataset with 1,970 instances for the task.",6 Conclusion and Outlook,0,[0]
"This number might not seem large (e.g., compared to 0.5M from SNLI), but tasks with hand-crafted data are of a similar size (e.g., 3,744 Story Cloze Test instances).",6 Conclusion and Outlook,0,[0]
"Also, the crowdsourcing process is scalable and is limited only by the budget.11 Moreover, we created several data ‘byproducts’ that are valuable for argumentation research: 5,000 comments annotated with stance, which outnumbers the 4,163 tweets for stance detection of Mohammad et al. (2016); 2,026 arguments with 4,235 annotated reasons, which is six times larger than the 340 documents of Habernal and Gurevych (2017); and 4,235 summarized reason gists — we are not aware of any other handcrafted dataset for abstractive argument summarization built upon authentic arguments.
",6 Conclusion and Outlook,0,[0]
"Based on the dataset, we evaluated human performance in argument reasoning comprehension.",6 Conclusion and Outlook,0,[0]
"Our findings suggest that the task is harder for people without formal argumentation training, while being solvable without knowing the topic.",6 Conclusion and Outlook,0,[0]
"We also found that neural attention models outperform language models on the task.
",6 Conclusion and Outlook,0,[0]
"In the short run, we plan to draw more attention to this topic by running a SemEval 2018 shared task.12 A deep qualitative analysis of the warrants from the theoretical perspective of reasoning
11In our case, the total costs were about $6,000 including bonuses and experiments with the workflow set-up.
",6 Conclusion and Outlook,0,[0]
"12https://competitions.codalab.org/ competitions/17327
patterns or argumentation schemes is also necessary.",6 Conclusion and Outlook,0,[0]
"In the long run, an automatic generation and validation warrants can be understood as the ultimate goal in argument evaluation.",6 Conclusion and Outlook,0,[0]
"It has been claimed that for reconstructing and evaluating natural language arguments, one has to fully ‘roll out’ their implicit premises (van Eemeren et al., 2014, Chap. 3.2) and leverage knowledge bases (Wyner et al., 2016).",6 Conclusion and Outlook,0,[0]
"We believe that a system that can distinguish between the wrong and the right warrant given its context will be helpful in filtering out good candidates in argument reconstruction.
",6 Conclusion and Outlook,0,[0]
"For the moment, we just made a first empirical step towards exploring how much common-sense reasoning is necessary in argumentation and how much common sense there might be at all.",6 Conclusion and Outlook,0,[0]
"This work has been supported by the ArguAna Project GU 798/20-1 (DFG), and by the DFGfunded research training group “Adaptive Preparation of Information form Heterogeneous Sources” (AIPHES, GRK 1994/1).",Acknowledgments,0,[0]
Reasoning is a crucial part of natural language argumentation.,abstractText,0,[0]
"To comprehend an argument, one must analyze its warrant, which explains why its claim follows from its premises.",abstractText,0,[0]
"As arguments are highly contextualized, warrants are usually presupposed and left implicit.",abstractText,0,[0]
"Thus, the comprehension does not only require language understanding and logic skills, but also depends on common sense.",abstractText,0,[0]
In this paper we develop a methodology for reconstructing warrants systematically.,abstractText,0,[0]
"We operationalize it in a scalable crowdsourcing process, resulting in a freely licensed dataset with warrants for 2k authentic arguments from news comments.1",abstractText,0,[0]
"On this basis, we present a new challenging task, the argument reasoning comprehension task.",abstractText,0,[0]
"Given an argument with a claim and a premise, the goal is to choose the correct implicit warrant from two options.",abstractText,0,[0]
"Both warrants are plausible and lexically close, but lead to contradicting claims.",abstractText,0,[0]
A solution to this task will define a substantial step towards automatic warrant reconstruction.,abstractText,0,[0]
"However, experiments with several neural attention and language models reveal that current approaches do not suffice.",abstractText,0,[0]
The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 76–86 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
76",text,0,[0]
"In recent years, the emergence of seq2seq models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) has revolutionized the field of MT by replacing traditional phrasebased approaches with neural machine translation (NMT) systems based on the encoder-decoder paradigm.",1 Introduction,0,[0]
"In the first architectures that surpassed
∗ Equal contribution.
",1 Introduction,0,[0]
"the quality of phrase-based MT, both the encoder and decoder were implemented as Recurrent Neural Networks (RNNs), interacting via a soft-attention mechanism (Bahdanau et al., 2015).",1 Introduction,0,[0]
"The RNN-based NMT approach, or RNMT, was quickly established as the de-facto standard for NMT, and gained rapid adoption into large-scale systems in industry, e.g. Baidu (Zhou et al., 2016), Google (Wu et al., 2016), and Systran (Crego et al., 2016).
",1 Introduction,0,[0]
"Following RNMT, convolutional neural network based approaches (LeCun and Bengio, 1998) to NMT have recently drawn research attention due to their ability to fully parallelize training to take advantage of modern fast computing devices.",1 Introduction,0,[0]
"such as GPUs and Tensor Processing Units (TPUs) (Jouppi et al., 2017).",1 Introduction,0,[0]
"Well known examples are ByteNet (Kalchbrenner et al., 2016) and ConvS2S (Gehring et al., 2017).",1 Introduction,0,[0]
"The ConvS2S model was shown to outperform the original RNMT architecture in terms of quality, while also providing greater training speed.
",1 Introduction,0,[0]
"Most recently, the Transformer model (Vaswani et al., 2017), which is based solely on a selfattention mechanism (Parikh et al., 2016) and feed-forward connections, has further advanced the field of NMT, both in terms of translation quality and speed of convergence.
",1 Introduction,1,"['We build on the ideas of (de Freitas et al., 2012) for the noiseless setting, while addressing highly non-trivial challenges arising in the presence of noise.']"
"In many instances, new architectures are accompanied by a novel set of techniques for performing training and inference that have been carefully optimized to work in concert.",1 Introduction,0,[0]
"This ‘bag of tricks’ can be crucial to the performance of a proposed architecture, yet it is typically under-documented and left for the enterprising researcher to discover in publicly released code (if any) or through anecdotal evidence.",1 Introduction,0,[0]
"This is not simply a problem for reproducibility; it obscures the central scientific question of how much of the observed gains come from the new architecture
and how much can be attributed to the associated training and inference techniques.",1 Introduction,0,[0]
"In some cases, these new techniques may be broadly applicable to other architectures and thus constitute a major, though implicit, contribution of an architecture paper.",1 Introduction,0,[0]
"Clearly, they need to be considered in order to ensure a fair comparison across different model architectures.
",1 Introduction,0,[0]
"In this paper, we therefore take a step back and look at which techniques and methods contribute significantly to the success of recent architectures, namely ConvS2S and Transformer, and explore applying these methods to other architectures, including RNMT models.",1 Introduction,0,[0]
"In doing so, we come up with an enhanced version of RNMT, referred to as RNMT+, that significantly outperforms all individual architectures in our setup.",1 Introduction,0,[0]
"We further introduce new architectures built with different components borrowed from RNMT+, ConvS2S and Transformer.",1 Introduction,0,[0]
"In order to ensure a fair setting for comparison, all architectures were implemented in the same framework, use the same pre-processed data and apply no further post-processing as this may confound bare model performance.
",1 Introduction,0,[0]
"Our contributions are three-fold:
1.",1 Introduction,0,[0]
"In ablation studies, we quantify the effect of several modeling improvements (including multi-head attention and layer normalization) as well as optimization techniques (such as synchronous replica training and labelsmoothing), which are used in recent architectures.",1 Introduction,0,[0]
"We demonstrate that these techniques are applicable across different model architectures.
",1 Introduction,0,[0]
2,1 Introduction,0,[0]
"Combining these improvements with the RNMT model, we propose the new RNMT+ model, which significantly outperforms all fundamental architectures on the widely-used WMT’14 En→Fr and En→De benchmark datasets.",1 Introduction,0,[0]
"We provide a detailed model analysis and comparison of RNMT+, ConvS2S and Transformer in terms of model quality, model size, and training and inference speed.
3.",1 Introduction,0,[0]
"Inspired by our understanding of the relative strengths and weaknesses of individual model architectures, we propose new model architectures that combine components from the RNMT+ and the Transformer model, and achieve better results than both individual architectures.
",1 Introduction,0,[0]
We quickly note two prior works that provided empirical solutions to the difficulty of training NMT architectures (specifically RNMT).,1 Introduction,0,[0]
"In (Britz et al., 2017)",1 Introduction,0,[0]
the authors systematically explore which elements of NMT architectures have a significant impact on translation quality.,1 Introduction,0,[0]
"In (Denkowski and Neubig, 2017)",1 Introduction,0,[0]
the authors recommend three specific techniques for strengthening NMT systems and empirically demonstrated how incorporating those techniques improves the reliability of the experimental results.,1 Introduction,0,[0]
"In this section, we briefly discuss the commmonly used NMT architectures.",2 Background,0,[0]
"RNMT models are composed of an encoder RNN and a decoder RNN, coupled with an attention network.",2.1 RNN-based NMT Models - RNMT,0,[0]
"The encoder summarizes the input sequence into a set of vectors while the decoder conditions on the encoded input sequence through an attention mechanism, and generates the output sequence one token at a time.
",2.1 RNN-based NMT Models - RNMT,0,[0]
"The most successful RNMT models consist of stacked RNN encoders with one or more bidirectional RNNs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005), and stacked decoders with unidirectional RNNs.",2.1 RNN-based NMT Models - RNMT,0,[0]
"Both encoder and decoder RNNs consist of either LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) or GRU units (Cho et al., 2014), and make extensive use of residual (He et al., 2015) or highway (Srivastava et al., 2015) connections.
",2.1 RNN-based NMT Models - RNMT,0,[0]
In Google-NMT (GNMT),2.1 RNN-based NMT Models - RNMT,0,[0]
"(Wu et al., 2016), the best performing RNMT model on the datasets we consider, the encoder network consists of one bi-directional LSTM layer, followed by 7 uni-directional LSTM layers.",2.1 RNN-based NMT Models - RNMT,0,[0]
The decoder is equipped with a single attention network and 8 uni-directional LSTM layers.,2.1 RNN-based NMT Models - RNMT,0,[0]
"Both the encoder and the decoder use residual skip connections between consecutive layers.
",2.1 RNN-based NMT Models - RNMT,0,[0]
"In this paper, we adopt GNMT as the starting point for our proposed RNMT+ architecture.",2.1 RNN-based NMT Models - RNMT,0,[0]
"In the most successful convolutional sequence-tosequence model (Gehring et al., 2017), both the encoder and decoder are constructed by stacking multiple convolutional layers, where each layer
contains 1-dimensional convolutions followed by a gated linear units (GLU) (Dauphin et al., 2016).",2.2 Convolutional NMT Models - ConvS2S,0,[0]
Each decoder layer computes a separate dotproduct attention by using the current decoder layer output and the final encoder layer outputs.,2.2 Convolutional NMT Models - ConvS2S,0,[0]
Positional embeddings are used to provide explicit positional information to the model.,2.2 Convolutional NMT Models - ConvS2S,0,[0]
"Following the practice in (Gehring et al., 2017), we scale the gradients of the encoder layers to stabilize training.",2.2 Convolutional NMT Models - ConvS2S,0,[0]
"We also use residual connections across each convolutional layer and apply weight normalization (Salimans and Kingma, 2016) to speed up convergence.",2.2 Convolutional NMT Models - ConvS2S,0,[0]
We follow the public ConvS2S codebase1 in our experiments.,2.2 Convolutional NMT Models - ConvS2S,0,[0]
"The Transformer model (Vaswani et al., 2017) is motivated by two major design choices that aim to address deficiencies in the former two model families: (1) Unlike RNMT, but similar to the ConvS2S, the Transformer model avoids any sequential dependencies in both the encoder and decoder networks to maximally parallelize training.",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"(2) To address the limited context problem (limited receptive field) present in ConvS2S, the Transformer model makes pervasive use of selfattention networks (Parikh et al., 2016) so that each position in the current layer has access to information from all other positions in the previous layer.
",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
The Transformer model still follows the encoder-decoder paradigm.,2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
Encoder transformer layers are built with two sub-modules: (1) a selfattention network and (2) a feed-forward network.,2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"Decoder transformer layers have an additional cross-attention layer sandwiched between the selfattention and feed-forward layers to attend to the encoder outputs.
",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"There are two details which we found very important to the model’s performance: (1) Each sublayer in the transformer (i.e. self-attention, crossattention, and the feed-forward sub-layer) follows a strict computation sequence: normalize→ transform→ dropout→ residual-add.",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"(2) In addition to per-layer normalization, the final encoder output is again normalized to prevent a blow up after consecutive residual additions.
",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"In this paper, we follow the latest version of the
1https://github.com/facebookresearch/fairseq-py
Transformer model in the Tensor2Tensor2 codebase.",2.3 Conditional Transformation-based NMT Models - Transformer,0,[0]
"From a theoretical point of view, RNNs belong to the most expressive members of the neural network family (Siegelmann and Sontag, 1995)3.",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"Possessing an infinite Markovian structure (and thus an infinite receptive fields) equips them to model sequential data (Elman, 1990), especially natural language (Grefenstette et al., 2015) effectively.",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"In practice, RNNs are notoriously hard to train (Hochreiter, 1991; Bengio et al., 1994; Hochreiter et al., 2001), confirming the well known dilemma of trainability versus expressivity.
",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
Convolutional layers are adept at capturing local context and local correlations by design.,2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
A fixed and narrow receptive field for each convolutional layer limits their capacity when the architecture is shallow.,2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"In practice, this weakness is mitigated by stacking more convolutional layers (e.g. 15 layers as in the ConvS2S model), which makes the model harder to train and demands meticulous initialization schemes and carefully designed regularization techniques.
",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"The transformer network is capable of approximating arbitrary squashing functions (Hornik et al., 1989), and can be considered a strong feature extractor with extended receptive fields capable of linking salient features from the entire sequence.",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"On the other hand, lacking a memory component (as present in the RNN models) prevents the network from modeling a state space, reducing its theoretical strength as a sequence model, thus it requires additional positional information (e.g. sinusoidal positional encodings).
",2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
Above theoretical characterizations will drive our explorations in the following sections.,2.4 A Theory-Based Characterization of NMT Architectures,0,[0]
"We train our models on the standard WMT’14 En→Fr and En→De datasets that comprise 36.3M and 4.5M sentence pairs, respectively.",3 Experiment Setup,0,[0]
"Each sentence was encoded into a sequence of sub-word units obtained by first tokenizing the sentence with the Moses tokenizer, then splitting tokens into subword units (also known as “wordpieces”) using the approach described in (Schuster and Nakajima, 2012).
2https://github.com/tensorflow/tensor2tensor 3Assuming that data complexity is satisfied.
",3 Experiment Setup,0,[0]
We use a shared vocabulary of 32K sub-word units for each source-target language pair.,3 Experiment Setup,0,[0]
No further manual or rule-based post processing of the output was performed beyond combining the subword units to generate the targets.,3 Experiment Setup,0,[0]
"We report all our results on newstest 2014, which serves as the test set.",3 Experiment Setup,0,[0]
"A combination of newstest 2012 and newstest 2013 is used for validation.
",3 Experiment Setup,0,[0]
"To evaluate the models, we compute the BLEU metric on tokenized, true-case output.4 For each training run, we evaluate the model every 30 minutes on the dev set.",3 Experiment Setup,0,[0]
"Once the model converges, we determine the best window based on the average dev-set BLEU score over 21 consecutive evaluations.",3 Experiment Setup,0,[0]
We report the mean test score and standard deviation over the selected window.,3 Experiment Setup,0,[0]
"This allows us to compare model architectures based on their mean performance after convergence rather than individual checkpoint evaluations, as the latter can be quite noisy for some models.
",3 Experiment Setup,0,[0]
"To enable a fair comparison of architectures, we use the same pre-processing and evaluation methodology for all our experiments.",3 Experiment Setup,0,[0]
"We refrain from using checkpoint averaging (exponential moving averages of parameters) (JunczysDowmunt et al., 2016) or checkpoint ensembles (Jean et al., 2015; Chen et al., 2017) to focus on
4This procedure is used in the literature to which we compare (Gehring et al., 2017; Wu et al., 2016).
evaluating the performance of individual models.",3 Experiment Setup,0,[0]
The newly proposed RNMT+ model architecture is shown in Figure 1.,4.1 Model Architecture of RNMT+,0,[0]
Here we highlight the key architectural choices that are different between the RNMT+ model and the GNMT model.,4.1 Model Architecture of RNMT+,0,[0]
There are 6 bidirectional LSTM layers in the encoder instead of 1 bidirectional LSTM layer followed by 7 unidirectional layers as in GNMT.,4.1 Model Architecture of RNMT+,0,[0]
"For each bidirectional layer, the outputs of the forward layer and the backward layer are concatenated before being fed into the next layer.",4.1 Model Architecture of RNMT+,0,[0]
The decoder network consists of 8 unidirectional LSTM layers similar to the GNMT model.,4.1 Model Architecture of RNMT+,0,[0]
Residual connections are added to the third layer and above for both the encoder and decoder.,4.1 Model Architecture of RNMT+,0,[0]
"Inspired by the Transformer model, pergate layer normalization (Ba et al., 2016) is applied within each LSTM cell.",4.1 Model Architecture of RNMT+,0,[0]
Our empirical results show that layer normalization greatly stabilizes training.,4.1 Model Architecture of RNMT+,0,[0]
No non-linearity is applied to the LSTM output.,4.1 Model Architecture of RNMT+,0,[0]
A projection layer is added to the encoder final output.5 Multi-head additive attention is used instead of the single-head attention in the GNMT model.,4.1 Model Architecture of RNMT+,0,[0]
"Similar to GNMT, we use the
5Additional projection aims to reduce the dimensionality of the encoder output representations to match the decoder stack dimension.
bottom decoder layer and the final encoder layer output after projection for obtaining the recurrent attention context.",4.1 Model Architecture of RNMT+,0,[0]
"In addition to feeding the attention context to all decoder LSTM layers, we also feed it to the softmax by concatenating it with the layer input.",4.1 Model Architecture of RNMT+,0,[0]
"This is important for both the quality of the models with multi-head attention and the stability of the training process.
",4.1 Model Architecture of RNMT+,0,[0]
"Since the encoder network in RNMT+ consists solely of bi-directional LSTM layers, model parallelism is not used during training.",4.1 Model Architecture of RNMT+,0,[0]
"We compensate for the resulting longer per-step time with increased data parallelism (more model replicas), so that the overall time to reach convergence of the RNMT+ model is still comparable to that of GNMT.
",4.1 Model Architecture of RNMT+,0,[0]
"We apply the following regularization techniques during training.
",4.1 Model Architecture of RNMT+,0,[0]
• Dropout: We apply dropout to both embedding layers and each LSTM layer output before it is added to the next layer’s input.,4.1 Model Architecture of RNMT+,0,[0]
"Attention dropout is also applied.
",4.1 Model Architecture of RNMT+,0,[0]
"• Label Smoothing: We use uniform label smoothing with an uncertainty=0.1 (Szegedy et al., 2015).",4.1 Model Architecture of RNMT+,0,[0]
"Label smoothing was shown to have a positive impact on both Transformer and RNMT+ models, especially in the case of RNMT+ with multi-head attention.",4.1 Model Architecture of RNMT+,0,[0]
"Similar to the observations in (Chorowski and Jaitly, 2016), we found it beneficial to use a larger beam size (e.g. 16, 20, etc.)",4.1 Model Architecture of RNMT+,0,[0]
"during decoding when models are trained with label smoothing.
• Weight Decay: For the WMT’14 En→De task, we apply L2 regularization to the weights with λ = 10−5.",4.1 Model Architecture of RNMT+,0,[0]
"Weight decay is only applied to the En→De task as the corpus is smaller and thus more regularization is required.
",4.1 Model Architecture of RNMT+,0,[0]
"We use the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.999, = 10−6 and vary the learning rate according to this schedule: lr = 10−4 ·min ( 1+
t · (n− 1) np , n, n · (2n) s−nt e−s ) (1)
Here, t is the current step, n is the number of concurrent model replicas used in training, p is the number of warmup steps, s is the start step of the exponential decay, and e is the end step of the decay.",4.1 Model Architecture of RNMT+,0,[0]
"Specifically, we first increase the learning rate linearly during the number of warmup steps, keep
it a constant until the decay start step s, then exponentially decay until the decay end step e, and keep it at 5 · 10−5 after the decay ends.",4.1 Model Architecture of RNMT+,0,[0]
"This learning rate schedule is motivated by a similar schedule that was successfully applied in training the Resnet-50 model with a very large batch size (Goyal et al., 2017).
",4.1 Model Architecture of RNMT+,0,[0]
"In contrast to the asynchronous training used for GNMT (Dean et al., 2012), we train RNMT+ models with synchronous training (Chen et al., 2016).",4.1 Model Architecture of RNMT+,0,[0]
"Our empirical results suggest that when hyper-parameters are tuned properly, synchronous training often leads to improved convergence speed and superior model quality.
",4.1 Model Architecture of RNMT+,0,[0]
"To further stabilize training, we also use adaptive gradient clipping.",4.1 Model Architecture of RNMT+,0,[0]
"We discard a training step completely if an anomaly in the gradient norm value is detected, which is usually an indication of an imminent gradient explosion.",4.1 Model Architecture of RNMT+,0,[0]
"More specifically, we keep track of a moving average and a moving standard deviation of the log of the gradient norm values, and we abort a step if the norm of the gradient exceeds four standard deviations of the moving average.",4.1 Model Architecture of RNMT+,0,[0]
"In this section, we compare the results of RNMT+ with ConvS2S and Transformer.
",4.2 Model Analysis and Comparison,0,[0]
All models were trained with synchronous training.,4.2 Model Analysis and Comparison,0,[0]
"RNMT+ and ConvS2S were trained with 32 NVIDIA P100 GPUs while the Transformer Base and Big models were trained using 16 GPUs.
",4.2 Model Analysis and Comparison,0,[0]
"For RNMT+, we use sentence-level crossentropy loss.",4.2 Model Analysis and Comparison,0,[0]
Each training batch contained 4096 sentence pairs (4096 source sequences and 4096 target sequences).,4.2 Model Analysis and Comparison,0,[0]
"For ConvS2S and Transformer models, we use token-level cross-entropy loss.",4.2 Model Analysis and Comparison,0,[0]
Each training batch contained 65536 source tokens and 65536 target tokens.,4.2 Model Analysis and Comparison,0,[0]
"For the GNMT baselines on both tasks, we cite the largest BLEU score reported in (Wu et al., 2016) without reinforcement learning.
",4.2 Model Analysis and Comparison,0,[0]
Table 1 shows our results on the WMT’14 En→Fr task.,4.2 Model Analysis and Comparison,0,[0]
Both the Transformer Big model and RNMT+ outperform GNMT and ConvS2S by about 2 BLEU points.,4.2 Model Analysis and Comparison,0,[0]
RNMT+ is slightly better than the Transformer Big model in terms of its mean BLEU score.,4.2 Model Analysis and Comparison,0,[0]
"RNMT+ also yields a much lower standard deviation, and hence we observed much less fluctuation in the training curve.",4.2 Model Analysis and Comparison,0,[0]
"It takes approximately 3 days for the Transformer
Base model to converge, while both RNMT+ and the Transformer Big model require about 5 days to converge.",4.2 Model Analysis and Comparison,0,[0]
"Although the batching schemes are quite different between the Transformer Big and the RNMT+ model, they have processed about the same amount of training samples upon convergence.
",4.2 Model Analysis and Comparison,0,[0]
Table 2 shows our results on the WMT’14 En→De task.,4.2 Model Analysis and Comparison,0,[0]
The Transformer Base model improves over GNMT and ConvS2S by more than 2 BLEU points while the Big model improves by over 3 BLEU points.,4.2 Model Analysis and Comparison,0,[0]
RNMT+ further outperforms the Transformer Big model and establishes a new state of the art with an averaged value of 28.49.,4.2 Model Analysis and Comparison,0,[0]
"In this case, RNMT+ converged slightly faster than the Transformer Big model and maintained much more stable performance after convergence with a very small standard deviation, which is similar to what we observed on the En-Fr task.
",4.2 Model Analysis and Comparison,0,[0]
Table 3 summarizes training performance and model statistics.,4.2 Model Analysis and Comparison,0,[0]
"The Transformer Base model
6Since the ConvS2S model convergence is very slow we did not explore further tuning on En→Fr, and validated our implementation on En→De.
",4.2 Model Analysis and Comparison,0,[0]
"7The BLEU scores for Transformer model are slightly lower than those reported in (Vaswani et al., 2017) due to four differences:
1)",4.2 Model Analysis and Comparison,0,[0]
"We report the mean test BLEU score using the strategy described in section 3.
2)",4.2 Model Analysis and Comparison,0,[0]
"We did not perform checkpoint averaging since it would be inconsistent with our evaluation for other models.
",4.2 Model Analysis and Comparison,0,[0]
3),4.2 Model Analysis and Comparison,0,[0]
"We avoided any manual post-processing, like unicode normalization using Moses replace-unicode-punctuation.perl or output tokenization using Moses tokenizer.perl, to rule out its effect on the evaluation.",4.2 Model Analysis and Comparison,0,[0]
"We observed a significant BLEU increase (about 0.6) on applying these post processing techniques.
4)",4.2 Model Analysis and Comparison,0,[0]
"In (Vaswani et al., 2017), reported BLEU scores are calculated using mteval-v13a.pl from Moses, which re-tokenizes its input.
is the fastest model in terms of training speed.",4.2 Model Analysis and Comparison,0,[0]
RNMT+ is slower to train than the Transformer Big model on a per-GPU basis.,4.2 Model Analysis and Comparison,0,[0]
"However, since the RNMT+ model is quite stable, we were able to offset the lower per-GPU throughput with higher concurrency by increasing the number of model replicas, and hence the overall time to convergence was not slowed down much.",4.2 Model Analysis and Comparison,0,[0]
We also computed the number of floating point operations (FLOPs) in the model’s forward path as well as the number of total parameters for all architectures (cf. Table 3).,4.2 Model Analysis and Comparison,0,[0]
"RNMT+ requires fewer FLOPs than the Transformer Big model, even though both models have a comparable number of parameters.",4.2 Model Analysis and Comparison,0,[0]
"In this section, we evaluate the importance of four main techniques for both the RNMT+ and the Transformer Big models.",5 Ablation Experiments,0,[0]
"We believe that these techniques are universally applicable across different model architectures, and should always be employed by NMT practitioners for best performance.
",5 Ablation Experiments,0,[0]
We take our best RNMT+ and Transformer Big models and remove each one of these techniques independently.,5 Ablation Experiments,0,[0]
"By doing this we hope to learn two things about each technique: (1) How much does
it affect the model performance?",5 Ablation Experiments,0,[0]
"(2) How useful is it for stable training of other techniques and hence the final model?
",5 Ablation Experiments,0,[0]
"From Table 4 we draw the following conclusions about the four techniques:
• Label Smoothing",5 Ablation Experiments,0,[0]
"We observed that label smoothing improves both models, leading to an average increase of 0.7 BLEU for RNMT+ and 0.2 BLEU for Transformer Big models.
",5 Ablation Experiments,0,[0]
"• Multi-head Attention Multi-head attention contributes significantly to the quality of both models, resulting in an average increase of 0.6 BLEU for RNMT+ and 0.9 BLEU for Transformer Big models.
",5 Ablation Experiments,0,[0]
"• Layer Normalization Layer normalization is most critical to stabilize the training process of either model, especially when multi-head attention is used.",5 Ablation Experiments,0,[0]
Removing layer normalization results in unstable training runs for both models.,5 Ablation Experiments,0,[0]
"Since by design, we remove one technique at a time in our ablation experiments, we were unable to quantify how much layer normalization helped in either case.",5 Ablation Experiments,0,[0]
"To be able to successfully train a model without layer normalization, we would have to adjust other parts of the model and retune its hyper-parameters.
",5 Ablation Experiments,0,[0]
• Synchronous training Removing synchronous training has different effects on RNMT+ and Transformer.,5 Ablation Experiments,0,[0]
"For RNMT+, it results in a significant quality drop, while for the Transformer Big model, it causes the model to become unstable.",5 Ablation Experiments,0,[0]
"We also notice that synchronous training is only successful when coupled with a tailored learning rate schedule that has a warmup stage at the beginning (cf. Eq. 1 for RNMT+ and
Eq. 2 for Transformer).",5 Ablation Experiments,0,[0]
"For RNMT+, removing this warmup stage during synchronous training causes the model to become unstable.",5 Ablation Experiments,0,[0]
"In this section, we explore hybrid architectures that shed some light on the salient behavior of each model family.",6 Hybrid NMT Models,0,[0]
These hybrid models outperform the individual architectures on both benchmark datasets and provide a better understanding of the capabilities and limitations of each model family.,6 Hybrid NMT Models,0,[0]
"In an encoder-decoder architecture, a natural assumption is that the role of an encoder is to build feature representations that can best encode the meaning of the source sequence, while a decoder should be able to process and interpret the representations from the encoder and, at the same time, track the current target history.",6.1 Assessing Individual Encoders and Decoders,0,[0]
"Decoding is inherently auto-regressive, and keeping track of the state information should therefore be intuitively beneficial for conditional generation.
",6.1 Assessing Individual Encoders and Decoders,0,[0]
"We set out to study which family of encoders is more suitable to extract rich representations from a given input sequence, and which family of decoders can make the best of such rich representations.",6.1 Assessing Individual Encoders and Decoders,0,[0]
We start by combining the encoder and decoder from different model families.,6.1 Assessing Individual Encoders and Decoders,0,[0]
"Since it takes a significant amount of time for a ConvS2S model to converge, and because the final translation quality was not on par with the other models, we focus on two types of hybrids only: Transformer encoder with RNMT+ decoder and RNMT+ encoder with Transformer decoder.
",6.1 Assessing Individual Encoders and Decoders,0,[0]
"From Table 5, it is clear that the Transformer encoder is better at encoding or feature extraction than the RNMT+ encoder, whereas RNMT+ is better at decoding or conditional language modeling, confirming our intuition that a stateful de-
coder is beneficial for conditional language generation.",6.1 Assessing Individual Encoders and Decoders,0,[0]
"Next, we explore how the features extracted by an encoder can be further enhanced by incorporating additional information.",6.2 Assessing Encoder Combinations,0,[0]
"Specifically, we investigate the combination of transformer layers with RNMT+ layers in the same encoder block to build even richer feature representations.",6.2 Assessing Encoder Combinations,0,[0]
"We exclusively use RNMT+ decoders in the following architectures since stateful decoders show better performance according to Table 5.
",6.2 Assessing Encoder Combinations,0,[0]
"We study two mixing schemes in the encoder (see Fig. 2):
(1) Cascaded Encoder: The cascaded encoder aims at combining the representational power of RNNs and self-attention.",6.2 Assessing Encoder Combinations,0,[0]
"The idea is to enrich a set of stateful representations by cascading a feature extractor with a focus on vertical mapping, similar to (Pascanu et al., 2013; Devlin, 2017).",6.2 Assessing Encoder Combinations,0,[0]
Our best performing cascaded encoder involves fine tuning transformer layers stacked on top of a pre-trained frozen RNMT+ encoder.,6.2 Assessing Encoder Combinations,0,[0]
Using a pre-trained encoder avoids optimization difficulties while significantly enhancing encoder capacity.,6.2 Assessing Encoder Combinations,0,[0]
"As shown in Table 6, the cascaded encoder improves over the Transformer encoder by more than 0.5 BLEU points on the WMT’14 En→Fr task.",6.2 Assessing Encoder Combinations,0,[0]
"This suggests that the Transformer encoder is able to extract richer representations if the input is augmented with sequential context.
(2) Multi-Column Encoder: As illustrated in Fig.",6.2 Assessing Encoder Combinations,0,[0]
"2b, a multi-column encoder merges the outputs of several independent encoders into a single combined representation.",6.2 Assessing Encoder Combinations,0,[0]
"Unlike a cascaded encoder, the multi-column encoder enables us to investigate whether an RNMT+ decoder can distinguish information received from two different channels and benefit from its combination.",6.2 Assessing Encoder Combinations,0,[0]
A crucial operation in a multi-column encoder is therefore how different sources of information are merged into a unified representation.,6.2 Assessing Encoder Combinations,0,[0]
"Our best multi-column encoder performs a simple concatenation of individual column outputs.
",6.2 Assessing Encoder Combinations,0,[0]
The model details and hyperparameters of the above two encoders are described in Appendix A.5 and A.6.,6.2 Assessing Encoder Combinations,0,[0]
"As shown in Table 6, the multi-column encoder followed by an RNMT+ decoder achieves better results than the Transformer and the RNMT model on both WMT’14 benchmark tasks.",6.2 Assessing Encoder Combinations,0,[0]
In this work we explored the efficacy of several architectural and training techniques proposed in recent studies on seq2seq models for NMT.,7 Conclusion,0,[0]
We demonstrated that many of these techniques are broadly applicable to multiple model architectures.,7 Conclusion,0,[0]
"Applying these new techniques to RNMT models yields RNMT+, an enhanced RNMT model that significantly outperforms the three fundamental architectures on WMT’14 En→Fr and En→De tasks.",7 Conclusion,0,[0]
"We further presented several hybrid models developed by combining encoders and decoders from the Transformer and RNMT+ models, and empirically demonstrated the superiority of the Transformer encoder and the RNMT+ decoder in comparison with their counterparts.",7 Conclusion,0,[0]
"We then enhanced the encoder architecture by horizontally and vertically mixing components borrowed from these architectures, leading to hybrid architectures that obtain further improvements over RNMT+.
",7 Conclusion,0,[0]
"We hope that our work will motivate NMT researchers to further investigate generally applicable training and optimization techniques, and that our exploration of hybrid architectures will open paths for new architecture search efforts for NMT.
",7 Conclusion,0,[0]
"Our focus on a standard single-language-pair translation task leaves important open questions to be answered: How do our new architectures compare in multilingual settings, i.e., modeling an interlingua?",7 Conclusion,0,[0]
"Which architecture is more efficient and powerful in processing finer grained inputs and outputs, e.g., characters or bytes?",7 Conclusion,0,[0]
How transferable are the representations learned by the different architectures to other tasks?,7 Conclusion,0,[0]
"And what are the characteristic errors that each architecture makes, e.g., linguistic plausibility?",7 Conclusion,0,[0]
We would like to thank the entire Google Brain Team and Google Translate Team for their foundational contributions to this project.,Acknowledgments,0,[0]
We would also like to thank the entire Tensor2Tensor development team for their useful inputs and discussions.,Acknowledgments,0,[0]
The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT).,abstractText,0,[0]
"The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then outperformed by the more recent Transformer model.",abstractText,0,[0]
Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures.,abstractText,0,[0]
"In this paper, we tease apart the new architectures and their accompanying techniques in two ways.",abstractText,0,[0]
"First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT’14 English→French and English→German tasks.",abstractText,0,[0]
"Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths.",abstractText,0,[0]
"Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.",abstractText,0,[0]
The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation,title,0,[0]
"Deep neural networks trained with backpropagation have commonly attained superhuman performance in applications of computer vision (Krizhevsky et al., 2012) and many others (Schmidhuber, 2015) and are thus receiving an unprecedented research interest.",1. Introduction,0,[0]
"Despite the rapid growth of the list of successful applications with these gradientbased methods, our theoretical understanding, however, is progressing at a more modest pace.
",1. Introduction,0,[0]
"One of the salient features of deep networks today is that they often have far more model parameters than the number of training samples that they are trained on, but meanwhile some of the models still exhibit remarkably good generalization performance when applied to unseen data of similar nature, while others generalize poorly in exactly the same setting.",1. Introduction,0,[0]
"A satisfying explanation of this phenomenon would be the key to more powerful and reliable network structures.
",1. Introduction,0,[0]
"1Laboratoire des Signaux et Systèmes (L2S), CentraleSupélec, Université Paris-Saclay, France; 2G-STATS Data Science Chair, GIPSA-lab, University Grenobles-Alpes, France.",1. Introduction,0,[0]
Correspondence to:,1. Introduction,0,[0]
"Zhenyu Liao <zhenyu.liao@l2s.centralesupelec.fr>, Romain Couillet <romain.couillet@centralesupelec.fr>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
",1. Introduction,0,[0]
"To answer such a question, statistical learning theory has proposed interpretations from the viewpoint of system complexity (Vapnik, 2013; Bartlett & Mendelson, 2002; Poggio et al., 2004).",1. Introduction,0,[0]
"In the case of large numbers of parameters, it is suggested to apply some form of regularization to ensure good generalization performance.",1. Introduction,0,[0]
"Regularizations can be explicit, such as the dropout technique (Srivastava et al., 2014) or the l2-penalization (weight decay) as reported in (Krizhevsky et al., 2012); or implicit, as in the case of the early stopping strategy (Yao et al., 2007) or the stochastic gradient descent algorithm itself (Zhang et al., 2016).
",1. Introduction,0,[0]
"Inspired by the recent line of works (Saxe et al., 2013; Advani & Saxe, 2017), in this article we introduce a random matrix framework to analyze the training and, more importantly, the generalization performance of neural networks, trained by gradient descent.",1. Introduction,0,[0]
"Preliminary results established from a toy model of two-class classification on a single-layer linear network are presented, which, despite their simplicity, shed new light on the understanding of many important aspects in training neural nets.",1. Introduction,0,[0]
"In particular, we demonstrate how early stopping can naturally protect the network against overfitting, which becomes more severe as the number of training sample approaches the dimension of the data.",1. Introduction,0,[0]
We also provide a strict lower bound on the training sample size for a given classification task in this simple setting.,1. Introduction,0,[0]
"A byproduct of our analysis implies that random initialization, although commonly used in practice in training deep networks (Glorot & Bengio, 2010; Krizhevsky et al., 2012), may lead to a degradation of the network performance.
",1. Introduction,0,[0]
"From a more theoretical point of view, our analyses allow one to evaluate any functional of the eigenvalues of the sample covariance matrix of the data (or of the data representation learned from previous layers in a deep model), which is at the core of understanding many experimental observations in today’s deep networks (Glorot & Bengio, 2010; Ioffe & Szegedy, 2015).",1. Introduction,0,[0]
"Our results are envisioned to generalize to more elaborate settings, notably to deeper models that are trained with the stochastic gradient descent algorithm, which is of more practical interest today due to the tremendous size of the data.
",1. Introduction,0,[0]
"Notations: Boldface lowercase (uppercase) characters stand for vectors (matrices), and non-boldface for scalars respectively.",1. Introduction,0,[0]
"0p is the column vector of zeros of size p, and Ip
the p ⇥ p identity matrix.",1. Introduction,0,[0]
The notation (·)T denotes the transpose operator.,1. Introduction,0,[0]
The norm k · k is the Euclidean norm for vectors and the operator norm for matrices.,1. Introduction,0,[0]
#NAME?,1. Introduction,0,[0]
"For x 2 R, we denote for simplicity (x)+ ⌘ max(x, 0).
",1. Introduction,0,[0]
"In the remainder of the article, we introduce the problem of interest and recall the results of (Saxe et al., 2013) in Section 2.",1. Introduction,0,[0]
"After a brief overview of basic concepts and methods to be used throughout the article in Section 3, our main results on the training and generalization performance of the network are presented in Section 4, followed by a thorough discussion in Section 5 and experiments on the popular MNIST database (LeCun et al., 1998) in Section 6.",1. Introduction,0,[0]
Section 7 concludes the article by summarizing the main results and outlining future research directions.,1. Introduction,0,[0]
"Let the training data x1, . . .",2. Problem Statement,0,[0]
",xn 2",2. Problem Statement,0,[0]
"Rp be independent vectors drawn from two distribution classes C1 and C2 of cardinality n1 and n2 (thus n1 + n2 = n), respectively.",2. Problem Statement,0,[0]
"We assume that the data vector xi of class Ca can be written as
xi =",2. Problem Statement,0,[0]
"( 1)aµ+ zi
for a = {1, 2}, with µ 2 Rp and zi a Gaussian random vector zi ⇠ N (0p, Ip).",2. Problem Statement,0,[0]
"In the context of a binary classification problem, one takes the label yi = 1 for xi 2 C1 and yj = 1 for xj 2 C2 to distinguish the two classes.",2. Problem Statement,0,[0]
We denote the training data matrix X = ⇥,2. Problem Statement,0,[0]
"x1, . . .",2. Problem Statement,0,[0]
",xn ⇤ 2 Rp⇥n by cascading all xi’s as column vectors and associated label vector y 2 Rn.",2. Problem Statement,0,[0]
"With the pair {X,y}, a classifier is trained using “full-batch” gradient descent to minimize the loss function L(w) given by
L(w) = 1
2n kyT wTXk2
so that for a new datum x̂, the output of the classifier is ŷ =",2. Problem Statement,0,[0]
"wTx̂, the sign of which is then used to decide the class of x̂.",2. Problem Statement,0,[0]
"The derivative of L with respective to w is given by
@L(w)",2. Problem Statement,0,[0]
"@w = 1 n X(y XTw).
",2. Problem Statement,0,[0]
"The gradient descent algorithm (Boyd & Vandenberghe, 2004) takes small steps of size ↵ along the opposite direction of the associated gradient, i.e., wt+1 = wt ↵@L(w)@w
w=wt
.
",2. Problem Statement,0,[0]
"Following the previous works of (Saxe et al., 2013; Advani & Saxe, 2017), when the learning rate ↵ is small, wt+1 and wt are close to each other so that by performing a continuous-time approximation, one obtains the following differential equation
@w(t)",2. Problem Statement,0,[0]
@t =,2. Problem Statement,0,[0]
"↵@L(w) @w = ↵ n X y XTw(t)
",2. Problem Statement,0,[0]
"the solution of which is given explicitly by
w(t) =",2. Problem Statement,0,[0]
"e ↵t n XX T w0 + ⇣ Ip e ↵t n XX T ⌘ (XXT) 1Xy
(1) if one assumes that XXT is invertible (only possible in the case p < n), with w0 ⌘ w(t = 0)",2. Problem Statement,0,[0]
"the initialization of the weight vector; we recall the definition of the exponential of a matrix 1nXX
T given by the power series e 1nXX T = P1
k=0 1 k!",2. Problem Statement,0,[0]
( 1 nXX T)k,2. Problem Statement,0,[0]
"= Ve⇤VT, with the eigendecomposition of 1nXX
T = V⇤VT and e⇤ is a diagonal matrix with elements equal to the exponential of the elements of ⇤.",2. Problem Statement,0,[0]
As t !,2. Problem Statement,0,[0]
"1 the network “forgets” the initialization w0 and results in the least-square solution wLS ⌘ (XXT) 1Xy.
",2. Problem Statement,0,[0]
"When p > n, XXT is no longer invertible.",2. Problem Statement,0,[0]
"Assuming XTX is invertible and writing Xy = XXT X XTX 1 y, the solution is similarly given by
w(t) =",2. Problem Statement,0,[0]
e ↵t n XX T w0 +X ⇣,2. Problem Statement,0,[0]
"In e ↵t n X TX ⌘ (XTX) 1y
with the least-square solution wLS ⌘ X(XTX) 1y.
",2. Problem Statement,0,[0]
"In the work of (Advani & Saxe, 2017) it is assumed that X has i.i.d. entries and that there is no linking structure between the data and associated targets in such a way that the “true” weight vector w̄ to be learned is independent of X so as to simplify the analysis.",2. Problem Statement,0,[0]
"In the present work we aim instead at exploring the capacity of the network to retrieve the (mixture modeled) data structure and position ourselves in a more realistic setting where w captures the different statistical structures (between classes) of the pair (X,y).",2. Problem Statement,0,[0]
"Our results are thus of more guiding significance for practical interests.
",2. Problem Statement,0,[0]
"From (1) note that both e ↵tn XX T and Ip e ↵t n XX T share the same eigenvectors with the sample covariance matrix 1 nXX
T, which thus plays a pivotal role in the network learning dynamics.",2. Problem Statement,0,[0]
"More concretely, the projections of w0 and wLS onto the eigenspace of 1nXX
T, weighted by functions (exp( ↵t i) or 1 exp( ↵t i)) of the associated eigenvalue",2. Problem Statement,0,[0]
"i, give the temporal evolution of w(t) and consequently the training and generalization performance of the network.",2. Problem Statement,0,[0]
"The core of our study therefore consists in deeply understanding of the eigenpairs of this sample covariance matrix, which has been largely investigated in the random matrix literature (Bai & Silverstein, 2010).",2. Problem Statement,0,[0]
"Throughout this paper, we will be relying on some basic yet powerful concepts and methods from random matrix theory, which shall be briefly highlighted in this section.",3. Preliminaries,0,[0]
Consider an n⇥ n,3.1. Resolvent and deterministic equivalents,0,[0]
"Hermitian random matrix M. We define its resolvent QM(z), for z 2 C not an eigenvalue of M, as
QM(z)",3.1. Resolvent and deterministic equivalents,0,[0]
"= (M zIn) 1 .
",3.1. Resolvent and deterministic equivalents,0,[0]
"Through the Cauchy integral formula discussed in the following subsection, as well as its central importance in random matrix theory, Q 1
nXX T(z) is the key object investigated
in this article.
",3.1. Resolvent and deterministic equivalents,0,[0]
"For certain simple distributions of M, one may define a so-called deterministic equivalent (Hachem et al., 2007; Couillet & Debbah, 2011) Q̄M for QM, which is a deterministic matrix such that for all A 2 Rn⇥n and all a,b 2 Rn of bounded (spectral and Euclidean, respectively) norms, 1n tr (AQM) 1 n tr AQ̄M ! 0 and aT QM Q̄M b ! 0",3.1. Resolvent and deterministic equivalents,0,[0]
almost surely as n !,3.1. Resolvent and deterministic equivalents,0,[0]
1,3.1. Resolvent and deterministic equivalents,0,[0]
"As such, deterministic equivalents allow to transfer random spectral properties of M in the form of deterministic limiting quantities and thus allows for a more detailed investigation.",3.1. Resolvent and deterministic equivalents,0,[0]
"First note that the resolvent QM(z) has the same eigenspace as M, with associated eigenvalue i replaced by 1 i z .",3.2. Cauchy’s integral formula,0,[0]
"As discussed at the end of Section 2, our objective is to evaluate functions of these eigenvalues, which reminds us of the fundamental Cauchy’s integral formula, stating that for any function f holomorphic on an open subset U of the complex plane, one can compute f( ) by contour integration.",3.2. Cauchy’s integral formula,0,[0]
"More concretely, for a closed positively (counter-clockwise) oriented path in U with winding number one (i.e., describing a 360 rotation), one has, for contained in the surface described by , 12⇡i H f(z) z dz = f( ) and 1 2⇡i H f(z) z",3.2. Cauchy’s integral formula,0,[0]
dz = 0,3.2. Cauchy’s integral formula,0,[0]
"if lies outside the contour of .
",3.2. Cauchy’s integral formula,0,[0]
"With Cauchy’s integral formula, one is able to evaluate more sophisticated functionals of the random matrix M. For example, for f(M) ⌘ aTeMb one has
f(M) = 1 2⇡i
I
exp(z)aTQM(z)b dz
with a positively oriented path circling around all the eigenvalues of M. Moreover, from the previous subsection one knows that the bilinear form aTQM(z)b is asymptotically close to a non-random quantity aTQ̄M(z)b.",3.2. Cauchy’s integral formula,0,[0]
"One thus deduces that the functional aTeMb has an asymptotically deterministic behavior that can be expressed as 12⇡i H exp(z)a TQ̄M(z)b dz.
",3.2. Cauchy’s integral formula,0,[0]
"This observation serves in the present article as the foundation for the performance analysis of the gradient-based classifier, as described in the following section.",3.2. Cauchy’s integral formula,0,[0]
"With the explicit expression of w(t) in (1), we now turn our attention to the training and generalization performances of the classifier as a function of the training time t. To this end, we shall be working under the following assumptions.",4. Temporal Evolution of Training and Generalization Performance,0,[0]
Assumption 1 (Growth Rate).,4. Temporal Evolution of Training and Generalization Performance,0,[0]
As n !,4. Temporal Evolution of Training and Generalization Performance,0,[0]
"1,
1. p n !",4. Temporal Evolution of Training and Generalization Performance,0,[0]
"c 2 (0,1).
",4. Temporal Evolution of Training and Generalization Performance,0,[0]
2,4. Temporal Evolution of Training and Generalization Performance,0,[0]
"For a = {1, 2}, nan !",4. Temporal Evolution of Training and Generalization Performance,0,[0]
"ca 2 (0, 1).
",4. Temporal Evolution of Training and Generalization Performance,0,[0]
3,4. Temporal Evolution of Training and Generalization Performance,0,[0]
"kµk = O(1).
",4. Temporal Evolution of Training and Generalization Performance,0,[0]
"The above assumption ensures that the matrix 1nXX T is of bounded operator norm for all large n, p with probability one (Bai & Silverstein, 1998).",4. Temporal Evolution of Training and Generalization Performance,0,[0]
Assumption 2 (Random Initialization).,4. Temporal Evolution of Training and Generalization Performance,0,[0]
"Let w0 ⌘ w(t = 0) be a random vector with i.i.d. entries of zero mean, variance 2 /p for some > 0 and finite fourth moment.
",4. Temporal Evolution of Training and Generalization Performance,0,[0]
"We first focus on the generalization performance, i.e., the average performance of the trained classifier taking as input an unseen new datum x̂ drawn from class C1 or C2.",4. Temporal Evolution of Training and Generalization Performance,0,[0]
"To evaluate the generalization performance of the classifier, we are interested in two types of misclassification rates, for a new datum x̂ drawn from class C1 or C2, as
P(w(t)Tx̂ > 0",4.1. Generalization Performance,0,[0]
"| x̂ 2 C1), P(w(t)Tx̂ < 0",4.1. Generalization Performance,0,[0]
"| x̂ 2 C2).
",4.1. Generalization Performance,0,[0]
"Since the new datum x̂ is independent of w(t), w(t)Tx̂ is a Gaussian random variable of mean ±w(t)Tµ and variance kw(t)k2.",4.1. Generalization Performance,0,[0]
The above probabilities can therefore be given via the Q-function: Q(x) ⌘,4.1. Generalization Performance,0,[0]
"1p
2⇡
R1 x exp
⇣ u 2
2
⌘ du.",4.1. Generalization Performance,0,[0]
"We thus
resort to the computation of w(t)Tµ as well as w(t)Tw(t) to evaluate the aforementioned classification error.
",4.1. Generalization Performance,0,[0]
"For µTw(t), with Cauchy’s integral formula we have
µTw(t) = µTe ↵t n XX T w0 + µ T ⇣ Ip e ↵t n XX T ⌘ wLS
= 1 2⇡i
I
ft(z)µ
T
✓ 1
n XXT zIp
◆ 1 w0 dz
1 2⇡i
I
1 ft(z) z
µT ✓ 1
n XXT zIp ◆ 1 1 n",4.1. Generalization Performance,0,[0]
"Xy dz
with ft(z) ⌘ exp( ↵tz), for a positive closed path circling around all eigenvalues of 1nXX
T. Note that the data matrix X can be rewritten as
X = µjT1 + µjT2 + Z = µyT + Z
with Z ⌘ ⇥",4.1. Generalization Performance,0,[0]
"z1, . . .",4.1. Generalization Performance,0,[0]
", zn ⇤ 2",4.1. Generalization Performance,0,[0]
Rp⇥n of i.i.d.,4.1. Generalization Performance,0,[0]
"N (0, 1) entries and ja 2 Rn the canonical vectors of class",4.1. Generalization Performance,0,[0]
Ca such that (ja)i = xi2Ca .,4.1. Generalization Performance,0,[0]
"To isolate the deterministic vectors µ and ja’s from the random Z in the expression of µTw(t), we exploit Woodbury’s identity to obtain
✓ 1
n XXT zIp
◆ 1 = Q(z) Q(z) ⇥ µ 1nZy ⇤
 µTQ(z)µ 1 + 1nµ
TQ(z)Zy ⇤ 1 + 1ny TZTQ(z) 1nZy
1  µT
1 ny TZT
Q(z)
where we denote the resolvent Q(z) ⌘ 1 nZZ T zIp 1, a deterministic equivalent of which is given by
Q(z) $ Q̄(z) ⌘",4.1. Generalization Performance,0,[0]
"m(z)Ip
with m(z) determined by the popular Marčenko–Pastur equation (Marčenko & Pastur, 1967)
m(z)",4.1. Generalization Performance,0,[0]
"= 1 c z
2cz",4.1. Generalization Performance,0,[0]
"+
p (1 c z)2 4cz
2cz (2)
where the branch of the square root is selected in such a way that =(z) ·=m(z) > 0, i.e., for a given z there exists a unique corresponding m(z).
",4.1. Generalization Performance,0,[0]
"Substituting Q(z) by the simple form deterministic equivalent m(z)Ip, we are able to estimate the random variable µTw(t) with a contour integral of some deterministic quantities as n, p !",4.1. Generalization Performance,0,[0]
1,4.1. Generalization Performance,0,[0]
"Similar arguments also hold for w(t)Tw(t), together leading to the following theorem.
",4.1. Generalization Performance,0,[0]
Theorem 1 (Generalization Performance).,4.1. Generalization Performance,0,[0]
Let Assumptions 1 and 2 hold.,4.1. Generalization Performance,0,[0]
As n !,4.1. Generalization Performance,0,[0]
"1, with probability one
P(w(t)Tx̂ > 0",4.1. Generalization Performance,0,[0]
"| x̂ 2 C1) Q ✓
Ep V
◆ ! 0
",4.1. Generalization Performance,0,[0]
P(w(t)Tx̂ < 0,4.1. Generalization Performance,0,[0]
"| x̂ 2 C2) Q ✓
Ep V
◆ !",4.1. Generalization Performance,0,[0]
"0
where
E ⌘ 1 2⇡i
I
1 ft(z) z kµk2m(z) dz (kµk2 + c)m(z)",4.1. Generalization Performance,0,[0]
"+ 1
V ⌘ 1 2⇡i
I "" 1 z2 (1 ft(z)) 2
(kµk2 + c)m(z) + 1 2 f 2 t (z)m(z)
# dz
with a closed positively oriented path that contains all eigenvalues of 1 nXX T and the origin, ft(z) ⌘ exp( ↵tz) and m(z) given by Equation (2).
",4.1. Generalization Performance,0,[0]
"Although derived from the case p < n, Theorem 1 also applies when p >",4.1. Generalization Performance,0,[0]
n.,4.1. Generalization Performance,0,[0]
"To see this, note that with Cauchy’s integral formula, for z 6= 0",4.1. Generalization Performance,0,[0]
"not an eigenvalue of 1nXX T (thus not of 1nX TX), one has X 1 nX TX zIn 1 y =
1 nXX T zIp 1
Xy, which further leads to the same expressions as in Theorem 1.",4.1. Generalization Performance,0,[0]
"Since 1nXX T and 1nX TX have the same eigenvalues except for additional zero eigenvalues for the larger matrix, the path remains unchanged (as we demand that contains the origin) and hence Theorem 1 holds true for both p < n and p > n.",4.1. Generalization Performance,0,[0]
The case p = n can be obtained by continuity arguments.,4.1. Generalization Performance,0,[0]
"To compare generalization versus training performance, we are now interested in the behavior of the classifier when applied to the training set X. To this end, we consider the random vector XTw(t) given by
XTw(t) = XTe ↵t n XX T w0+X T ⇣ Ip e ↵t n XX T ⌘ wLS .
",4.2. Training performance,0,[0]
"Note that the i-th entry of XTw(t) is given by the bilinear form eTi XTw(t), with ei the canonical vector with unique non-zero entry [ei]i = 1.",4.2. Training performance,0,[0]
"With previous notations we have
eTi X Tw(t)
",4.2. Training performance,0,[0]
"= 1 2⇡i
I
ft(z, t)e T i X T
✓ 1
n XXT zIp
◆ 1 w0 dz
1 2⇡i
",4.2. Training performance,0,[0]
"I
1 ft(z) z",4.2. Training performance,0,[0]
"eTi 1 n XT
✓ 1
n XXT zIp
◆ 1",4.2. Training performance,0,[0]
"Xy dz
which yields the following results.
",4.2. Training performance,0,[0]
Theorem 2 (Training Performance).,4.2. Training performance,0,[0]
"Under the assumptions and notations of Theorem 1, as n !",4.2. Training performance,0,[0]
"1,
P(w(t)Txi",4.2. Training performance,0,[0]
> 0,4.2. Training performance,0,[0]
"| xi 2 C1) Q E⇤p V⇤ E2⇤
! !",4.2. Training performance,0,[0]
"0
P(w(t)Txi",4.2. Training performance,0,[0]
< 0,4.2. Training performance,0,[0]
"| xi 2 C2) Q E⇤p V⇤ E2⇤
! !",4.2. Training performance,0,[0]
"0
almost surely, with
E⇤ ⌘ 1
2⇡i
I
1 ft(z) z
dz
(kµk2 + c)m(z) + 1
V⇤ ⌘ 1
2⇡i
I "" 1 z (1 ft(z)) 2
(kµk2 + c)m(z) + 1 2 f 2 t (z)zm(z)
# dz.
",4.2. Training performance,0,[0]
"In Figure 1 we compare finite dimensional simulations with theoretical results obtained from Theorem 1 and 2 and observe a very close match, already for not too large n, p.",4.2. Training performance,0,[0]
"As t grows large, the generalization error first drops rapidly with the training error, then goes up, although slightly, while the training error continues to decrease to zero.",4.2. Training performance,0,[0]
"This is because the classifier starts to over-fit the training data
The Dynamics of Learning: A Random Matrix Approach
0 50 100 150 200 250 300 0
0.1
0.2
0.3
0.4
0.5
Training time (t)
M is
cl as
si fic
at io
n ra
te
Simulation: training performance Theory: training performance Simulation: generalization performance Theory: generalization performance
0 50 100 150 200 250 300 0
0.1
0.2
0.3
0.4 0.5
Training time (t)
",4.2. Training performance,0,[0]
"M is
cl as
si fic
at io
n ra
te
Simulation: training performance Theory: training performance Simulation: generalization performance Theory: generalization performance
Figure 1.",4.2. Training performance,0,[0]
"Training and generalization performance for µ = [2;0p 1], p = 256, n = 512, 2 = 0.1, ↵ = 0.01 and c1 = c2 = 1/2.",4.2. Training performance,0,[0]
"Results obtained by averaging over 50 runs.
X and performs badly on unseen ones.",4.2. Training performance,0,[0]
"To avoid overfitting, one effectual approach is to apply regularization strategies (Bishop, 2007), for example, to “early stop” (at t = 100 for instance in the setting of Figure 1) in the training process.",4.2. Training performance,0,[0]
"However, this introduces new hyperparameters such as the optimal stopping time topt that is of crucial importance for the network performance and is often tuned through cross-validation in practice.",4.2. Training performance,0,[0]
"Theorem 1 and 2 tell us that the training and generalization performances, although being random themselves, have asymptotically deterministic behaviors described by (E⇤, V⇤) and (E, V ), respectively, which allows for a deeper understanding on the choice of topt, since E, V are in fact functions of t via ft(z) ⌘ exp( ↵tz).
",4.2. Training performance,0,[0]
"Nonetheless, the expressions in Theorem 1 and 2 of contour integrations are not easily analyzable nor interpretable.",4.2. Training performance,0,[0]
"To gain more insight, we shall rewrite (E, V ) and (E⇤, V⇤) in a more readable way.",4.2. Training performance,0,[0]
"First, note from Figure 2 that the matrix 1nXX
T has (possibly) two types of eigenvalues: those inside the main bulk (between ⌘ (1 p c)2 and
+ ⌘",4.2. Training performance,0,[0]
"(1 + p c)2) of the Marčenko–Pastur distribution
⌫(dx) =
p (x )",4.2. Training performance,0,[0]
"+( + x)+
2⇡cx dx+
✓ 1 1
c
◆+ (x)
(3) and a (possibly) isolated one1 lying away from [ , +], that shall be treated separately.",4.2. Training performance,0,[0]
"We rewrite the path (that contains all eigenvalues of 1nXX T) as the sum of two paths
1The existence (or absence) of outlying eigenvalues for the sample covariance matrix has been largely investigated in the random matrix literature and is related to the so-called “spiked random matrix model”.",4.2. Training performance,0,[0]
"We refer the reader to (Benaych-Georges & Nadakuditi, 2011) for an introduction.",4.2. Training performance,0,[0]
"The information carried by these “isolated” eigenpairs also marks an important technical difference to (Advani & Saxe, 2017) in which X is only composed of noise terms.
b and s, that circle around the main bulk and the isolated eigenvalue (if any), respectively.",4.2. Training performance,0,[0]
"To handle the first integral of b, we use the fact that for any nonzero 2 R, the limit limz2Z! m(z) ⌘",4.2. Training performance,0,[0]
"m̌( ) exists (Silverstein & Choi, 1995) and follow the idea in (Bai & Silverstein, 2008) by choosing the contour b to be a rectangle with sides parallel to the axes, intersecting the real axis at 0 and + and the horizontal sides being a distance "" ! 0",4.2. Training performance,0,[0]
"away from the real axis, to split the contour integral into four single ones of m̌(x).",4.2. Training performance,0,[0]
The second integral circling around s can be computed with the residue theorem.,4.2. Training performance,0,[0]
"This together leads to the expressions of (E, V ) and (E⇤, V⇤) as follows2
E =
Z 1 ft(x)
x µ(dx) (4)
V = kµk2 + c kµk2
Z (1 ft(x))2µ(dx)
",4.2. Training performance,0,[0]
"x2 + 2
Z f 2 t (x)⌫(dx)
(5)
",4.2. Training performance,0,[0]
"E⇤ = kµk2 + c kµk2
Z 1 ft(x)
x µ(dx)",4.2. Training performance,0,[0]
"(6)
V⇤ = kµk2 + c kµk2
Z (1 ft(x))2µ(dx)
x + 2
Z xf
2 t (x)⌫(dx)
(7)
where we recall ft(x) = exp( ↵tx), ⌫(x) given by (3) and denote the measure µ(dx) ⌘",4.2. Training performance,0,[0]
"p
(x )+( + x)+ 2⇡( s x)",4.2. Training performance,0,[0]
"dx+ (kµk4 c)+
kµk2 s(x) (8)
as well as
s = c+ 1 + kµk2 + c",4.2. Training performance,0,[0]
"kµk2 ( p c+ 1)2 (9)
with equality if and only if kµk2 = p c.
A first remark on the expressions of (4)-(7) is that E⇤ differs from E only by a factor of kµk
2+c kµk2 .",4.2. Training performance,0,[0]
"Also, both V and V⇤
are the sum of two parts: the first part that strongly depends on µ and the second one that is independent of µ. One thus deduces for kµk ! 0",4.2. Training performance,0,[0]
that E ! 0,4.2. Training performance,0,[0]
"and
V !",4.2. Training performance,0,[0]
"Z (1 ft(x))2
x2 ⇢(dx) + 2
Z f 2 t (x)⌫(dx) > 0
with ⇢(dx) ⌘",4.2. Training performance,0,[0]
"p
(x )+( + x)+ 2⇡(c+1) dx and therefore the gen-
eralization performance goes to Q(0) = 0.5.",4.2. Training performance,0,[0]
"On the other hand, for kµk ! 1, one has Ep
V !",4.2. Training performance,0,[0]
"1 and hence the
classifier makes perfect predictions.
",4.2. Training performance,0,[0]
"In a more general context (i.e., for Gaussian mixture models with generic means and covariances as investigated in
2We defer the readers to Section A in Supplementary Material for a detailed exposition of Theorem 1 and 2, as well as (4)-(7).
",4.2. Training performance,0,[0]
"The Dynamics of Learning: A Random Matrix Approach
(Benaych-Georges & Couillet, 2016), and obviously for practical datasets), there may be more than one eigenvalue of 1nXX
T lying outside the main bulk, which may not be limited to the interval [ , +].",4.2. Training performance,0,[0]
"In this case, the expression of m(z), instead of being explicitly given by (2), may be determined through more elaborate (often implicit) formulations.",4.2. Training performance,0,[0]
"While handling more generic models is technically reachable within the present analysis scheme, the results are much less intuitive.",4.2. Training performance,0,[0]
"Similar objectives cannot be achieved within the framework presented in (Advani & Saxe, 2017); this conveys more practical interest to our results and the proposed analysis framework.
",4.2. Training performance,0,[0]
"0 1 2 3 4
Eigenvalues of 1nXX T Marčenko–Pastur distribution Theory: s given in (9)
0 1 2 3 4
Eigenvalues of 1nXX T Marčenko–Pastur distribution Theory: s given in (9)
Figure 2.",4.2. Training performance,0,[0]
Eigenvalue distribution of 1nXX T for µ =,4.2. Training performance,0,[0]
"[1.5;0p 1], p = 512, n = 1024 and c1 = c2 = 1/2.",4.2. Training performance,0,[0]
"In this section, with a careful inspection of (4) and (5), discussions will be made from several different aspects.",5. Discussions,0,[0]
"First of all, recall that the generalization performance is simply given by Q ⇣ µTw(t) kw(t)k ⌘ , with the term µ Tw(t) kw(t)k describing the alignment between w(t) and µ, therefore the best possible generalization performance is simply Q(kµk).",5. Discussions,0,[0]
"Nonetheless, this “best” performance can never be achieved as long as p/n !",5. Discussions,0,[0]
"c > 0, as described in the following remark.",5. Discussions,0,[0]
Remark 1 (Optimal Generalization Performance).,5. Discussions,0,[0]
"Note that, with Cauchy–Schwarz inequality and the fact thatR µ(dx) =",5. Discussions,0,[0]
"kµk2 from (8), one has
E 2 
Z (1 ft(x))2
x2 dµ(x) ·
Z dµ(x)  ",5. Discussions,0,[0]
"kµk 4
kµk2 + cV
with equality in the right-most inequality if and only if the variance 2 = 0.",5. Discussions,0,[0]
"One thus concludes that E/ p V  kµk2/ p kµk2 + c and the best generalization performance (lowest misclassification rate) is Q(kµk2/ p kµk2 + c) and can be attained only when 2 = 0.
",5. Discussions,0,[0]
"The above remark is of particular interest because, for a given task (thus p,µ fixed)",5. Discussions,0,[0]
"it allows one to compute the
minimum training data number n to fulfill a certain request of classification accuracy.",5. Discussions,0,[0]
"As a side remark, note that in the expression of E/ p V the initialization variance 2 only appears in V , meaning that random initializations impair the generalization performance of the network.",5. Discussions,0,[0]
"As such, one should initialize with 2 very close, but not equal, to zero, to obtain symmetry breaking between hidden units (Goodfellow et al., 2016) as well as to mitigate the drop of performance due to large 2.
",5. Discussions,0,[0]
"In Figure 3 we plot the optimal generalization performance with the corresponding optimal stopping time as functions of 2, showing that small initialization helps training in terms of both accuracy and efficiency.
",5. Discussions,0,[0]
"Although the integrals in (4) and (5) do not have nice closed forms, note that, for t close to 0, with a Taylor expansion of ft(x) ⌘ exp( ↵tx) around ↵tx = 0, one gets more interpretable forms of E and V without integrals, as presented in the following subsection.",5. Discussions,0,[0]
"Taking t = 0, one has ft(x) = 1 and therefore E = 0, V = 2 R ⌫(dx) = 2, with ⌫(dx) the Marčenko–Pastur distribution given in (3).",5.1. Approximation for t close to 0,0,[0]
"As a consequence, at the beginning stage of training, the generalization performance is Q(0) = 0.5 for 2 6= 0 and the classifier makes random guesses.
",5.1. Approximation for t close to 0,0,[0]
"For t not equal but close to 0, the Taylor expansion of ft(x) ⌘ exp( ↵tx) around ↵tx = 0 gives
ft(x) ⌘ exp( ↵tx) ⇡ 1 ↵tx+O(↵2t2x2).
",5.1. Approximation for t close to 0,0,[0]
"Making the substitution x = 1 + c 2 p c cos ✓ and with the fact that R ⇡ 0 sin2 ✓ p+q cos ✓d✓ = p⇡ q2 ⇣ 1 p 1 q2/p2 ⌘ (see for example 3.644-5 in (Gradshteyn & Ryzhik, 2014)), one gets E = Ẽ +O(↵2t2) and V = Ṽ +O(↵2t2), where
Ẽ ⌘ ↵t 2 g(µ, c)",5.1. Approximation for t close to 0,0,[0]
"+
(kµk4 c)+
kµk2 ↵t = kµk 2 ↵t
Ṽ ⌘",5.1. Approximation for t close to 0,0,[0]
kµk 2 + c,5.1. Approximation for t close to 0,0,[0]
kµk2 (kµk4 c)+,5.1. Approximation for t close to 0,0,[0]
kµk2 ↵ 2 t 2 +,5.1. Approximation for t close to 0,0,[0]
kµk2 + c,5.1. Approximation for t close to 0,0,[0]
"kµk2 ↵ 2 t 2 2 g(µ, c)
+ 2(1 + c)↵2t2 2 2↵t+ ✓ 1 1
c
◆+ 2
+ 2
2c
1 + c",5.1. Approximation for t close to 0,0,[0]
"(1 + p c)|1 p c|
= (kµk2 + c+ c 2)↵2t2 + 2(↵t 1)2
with g(µ, c) ⌘",5.1. Approximation for t close to 0,0,[0]
"kµk2+ ckµk2 ⇣ kµk+ p c kµk ⌘ kµk p c kµk and consequently 12g(µ, c) +",5.1. Approximation for t close to 0,0,[0]
(kµk4 c)+,5.1. Approximation for t close to 0,0,[0]
kµk2 = kµk 2.,5.1. Approximation for t close to 0,0,[0]
"It is interesting to note from the above calculation that, although E and V seem to have different behaviors3 for kµk2 > p c or c > 1, it is in fact not the case and the extra part of kµk2 > p c (or c > 1) compensates for the singularity of the integral, so that the generalization performance of the classifier is a smooth function of both kµk2 and c.
Taking the derivative of Ẽp Ṽ with respect to t, one has
@
@t Ẽp Ṽ
= ↵(1 ↵t) 2
Ṽ 3/2
which implies that the maximum of Ẽp Ṽ
is kµk 2p
kµk2+c+c 2
and can be attained with t = 1/↵.",5.1. Approximation for t close to 0,0,[0]
"Moreover, taking t = 0 in the above equation one gets @@t
Ẽp Ṽ t=0 = ↵ .",5.1. Approximation for t close to 0,0,[0]
"Therefore,
large is harmful to the training efficiency, which coincides with the conclusion from Remark 1.
",5.1. Approximation for t close to 0,0,[0]
"The approximation error arising from Taylor expansion can be large for t away from 0, e.g., at t = 1/↵ the difference E Ẽ is of order O(1) and thus cannot be neglected.",5.1. Approximation for t close to 0,0,[0]
As t !,5.2. As t ! 1: least-squares solution,0,[0]
"1, one has ft(x) ! 0",5.2. As t ! 1: least-squares solution,0,[0]
"which results in the least-square solution wLS = (XXT) 1Xy or wLS = X(XTX) 1y and consequently
µTwLS kwLSk = kµk2p",5.2. As t ! 1: least-squares solution,0,[0]
"kµk2 + c
s
1 min ✓ c, 1
c
◆ .",5.2. As t ! 1: least-squares solution,0,[0]
"(10)
Comparing (10) with the expression in Remark 1, one observes that when t !",5.2. As t ! 1: least-squares solution,0,[0]
"1 the network becomes “over-trained” and the performance drops by a factor of p 1 min(c, c 1).",5.2. As t ! 1: least-squares solution,0,[0]
"This becomes even worse when c gets close to 1, as is consistent with the empirical findings in (Advani & Saxe, 2017).",5.2. As t ! 1: least-squares solution,0,[0]
"However, the point c = 1 is a singularity for (10), but not for Ep V
as in (4) and (5).",5.2. As t ! 1: least-squares solution,0,[0]
"One may thus expect to have a smooth and reliable behavior of the well-trained network for c close
3This phenomenon has been largely observed in random matrix theory and is referred to as “phase transition”(Baik et al., 2005).
",5.2. As t ! 1: least-squares solution,0,[0]
"to 1, which is a noticeable advantage of gradient-based training compared to simple least-square method.",5.2. As t ! 1: least-squares solution,0,[0]
"This coincides with the conclusion of (Yao et al., 2007) in which the asymptotic behavior of solely n !",5.2. As t ! 1: least-squares solution,0,[0]
"1 is considered.
",5.2. As t ! 1: least-squares solution,0,[0]
"In Figure 4 we plot the generalization performance from simulation (blue line), the approximation from Taylor expansion of ft(x) as described in Section 5.1 (red dashed line), together with the performance of wLS (cyan dashed line).",5.2. As t ! 1: least-squares solution,0,[0]
"One observes a close match between the result from Taylor expansion and the true performance for t small, with the former being optimal at t = 100 and the latter slowly approaching the performance of wLS as t goes to infinity.
",5.2. As t ! 1: least-squares solution,0,[0]
In Figure 5 we underline the case c = 1 by taking p = n = 512 with all other parameters unchanged from Figure 4.,5.2. As t ! 1: least-squares solution,0,[0]
"One observes that the simulation curve (blue line) increases much faster compared to Figure 4 and is supposed to end up at 0.5, which is the performance of wLS (cyan dashed line).",5.2. As t ! 1: least-squares solution,0,[0]
"This confirms a serious degradation of performance for c close to 1 of the classical least-squares solution.
",5.2. As t ! 1: least-squares solution,0,[0]
"0 200 400 600 800 1,000 0
0.1
0.2
0.3
0.4
0.5
Training time (t)
M is
cl as
si fic
at io
n ra
te
Simulation Approximation via Taylor expansion Performance of wLS
0 200 400 600 800 1,000 0
0.1
0.2
0.3
0.4
0.5
Training time (t)
",5.2. As t ! 1: least-squares solution,0,[0]
"M is
cl as
si fic
at io
n ra
te
Simulation Approximation via Taylor expansion Performance of wLS
Figure 4.",5.2. As t ! 1: least-squares solution,0,[0]
"Generalization performance for µ = ⇥ 2;0p 1 ⇤ , p = 256, n = 512, c1 = c2 = 1/2, 2 = 0.1 and ↵ = 0.01.",5.2. As t ! 1: least-squares solution,0,[0]
Simulation results obtained by averaging over 50 runs.,5.2. As t ! 1: least-squares solution,0,[0]
One major interest of random matrix analysis is that the ratio c appears constantly in the analysis.,5.3. Special case for c = 0,0,[0]
Taking c = 0 signifies that we have far more training data than their dimension.,5.3. Special case for c = 0,0,[0]
"This results in both , + !",5.3. Special case for c = 0,0,[0]
"1, s !",5.3. Special case for c = 0,0,[0]
"1 + kµk2 and
E !",5.3. Special case for c = 0,0,[0]
kµk2 1 ft(1 +,5.3. Special case for c = 0,0,[0]
"kµk 2)
1 + kµk2
V !",5.3. Special case for c = 0,0,[0]
"kµk2 ✓ 1 ft(1 + kµk2)
1 + kµk2
◆2 + 2f2t (1).
",5.3. Special case for c = 0,0,[0]
"As a consequence, Ep V !",5.3. Special case for c = 0,0,[0]
kµk if 2 = 0.,5.3. Special case for c = 0,0,[0]
"This can be explained by the fact that with sufficient training data the
classifier learns to align perfectly to µ so that µ Tw(t)
kw(t)k = kµk.",5.3. Special case for c = 0,0,[0]
"On the other hand, with initialization 2 6= 0, one always has Ep
V < kµk.",5.3. Special case for c = 0,0,[0]
"But still, as t goes large, the network
forgets the initialization exponentially fast and converges to the optimal w(t) that aligns to µ.
In particular, for 2 6= 0, we are interested in the optimal stopping time by taking the derivative with respect to t,
@
@t Ep V = ↵
2kµk2
V 3/2 kµk2ft(1 + kµk2)",5.3. Special case for c = 0,0,[0]
"+ 1 1 + kµk2 f 2 t (1) > 0
showing that when c = 0, the generalization performance continues to increase as t grows and there is in fact no “over-training” in this case.
",5.3. Special case for c = 0,0,[0]
"0 50 100 150 200 250 300 0
0.1
0.2
0.3
0.4
0.5
Training time (t)
M is
cl as
si fic
at io
n ra
te
Simulation: training performance Theory: training performance Simulation: generalization performance Theory: generalization performance
0 50 100 150 200 250 300 0
0.1
0.2
0.3
0.4
0.5
Training time (t)
",5.3. Special case for c = 0,0,[0]
"M is
cl as
si fic
at io
n ra
te
Simulation: training performance Theory: training performance Simulation: generalization performance Theory: generalization performance
Figure 6.",5.3. Special case for c = 0,0,[0]
"Training and generalization performance for MNIST data (number 1 and 7) with n = p = 784, c1 = c2 = 1/2, ↵ = 0.01 and 2 = 0.1.",5.3. Special case for c = 0,0,[0]
Results obtained by averaging over 100 runs.,5.3. Special case for c = 0,0,[0]
"We close this article with experiments on the popular MNIST dataset (LeCun et al., 1998) (number 1 and 7).",6. Numerical Validations,0,[0]
We randomly select training sets of size n = 784 vectorized images of dimension p = 784 and add artificially a Gaussian white noise of 10dB in order to be more compliant with our toy model setting.,6. Numerical Validations,0,[0]
Empirical means and covariances of each class are estimated from the full set of 13 007 MNIST images (6 742 images of number 1 and 6 265 of number 7).,6. Numerical Validations,0,[0]
"The image vectors in each class are whitened by pre-multiplying C 1/2a and re-centered to have means of ±µ, with µ half of the difference between means from the two classes.",6. Numerical Validations,0,[0]
"We observe an extremely close fit between our results and the empirical simulations, as shown in Figure 6.",6. Numerical Validations,0,[0]
"In this article, we established a random matrix approach to the analysis of learning dynamics for gradient-based algorithms on data of simultaneously large dimension and size.",7. Conclusion,0,[0]
"With a toy model of Gaussian mixture data with ±µ means and identity covariance, we have shown that the training and generalization performances of the network have asymptotically deterministic behaviors that can be evaluated via so-called deterministic equivalents and computed with complex contour integrals (and even under the form of real integrals in the present setting).",7. Conclusion,0,[0]
"The article can be generalized in many ways: with more generic mixture models (with the Gaussian assumption relaxed), on more appropriate loss functions (logistic regression for example), and more advanced optimization methods.
",7. Conclusion,0,[0]
"In the present work, the analysis has been performed on the “full-batch” gradient descent system.",7. Conclusion,0,[0]
"However, the most popular method used today is in fact its “stochastic” version (Bottou, 2010) where only a fixed-size (nbatch) randomly selected subset (called a mini-batch) of the training data is used to compute the gradient and descend one step along with the opposite direction of this gradient in each iteration.",7. Conclusion,0,[0]
"In this scenario, one of major concern in practice lies in determining the optimal size of the mini-batch and its influence on the generalization performance of the network (Keskar et al., 2016).",7. Conclusion,0,[0]
"This can be naturally linked to the ratio nbatch/p in the random matrix analysis.
",7. Conclusion,0,[0]
"Deep networks that are of more practical interests, however, need more efforts.",7. Conclusion,0,[0]
"As mentioned in (Saxe et al., 2013; Advani & Saxe, 2017), in the case of multi-layer networks, the learning dynamics depend, instead of each eigenmode separately, on the coupling of different eigenmodes from different layers.",7. Conclusion,0,[0]
"To handle this difficulty, one may add extra assumptions of independence between layers as in (Choromanska et al., 2015) so as to study each layer separately and then reassemble to retrieve the results of the whole network.",7. Conclusion,0,[0]
We thank the anonymous reviewers for their comments and constructive suggestions.,Acknowledgments,0,[0]
We would like to acknowledge this work is supported by the ANR Project RMT4GRAPH (ANR-14-CE28-0006) and the Project DeepRMT of La Fondation Supélec.,Acknowledgments,0,[0]
Understanding the learning dynamics of neural networks is one of the key issues for the improvement of optimization algorithms as well as for the theoretical comprehension of why deep neural nets work so well today.,abstractText,0,[0]
"In this paper, we introduce a random matrix-based framework to analyze the learning dynamics of a single-layer linear network on a binary classification problem, for data of simultaneously large dimension and size, trained by gradient descent.",abstractText,0,[0]
"Our results provide rich insights into common questions in neural nets, such as overfitting, early stopping and the initialization of training, thereby opening the door for future studies of more elaborate structures and models appearing in today’s neural networks.",abstractText,0,[0]
The Dynamics of Learning: A Random Matrix Approach,title,0,[0]
"Some potential NLP tasks have very sparse data by machine learning standards, as each of the IID training examples is an entire language.",1 Motivation,0,[0]
"For instance:
• typological classification of a language on various dimensions; • adaptation of any existing NLP system to new,
low-resource languages; • induction of a syntactic grammar from text;",1 Motivation,0,[0]
"• discovery of a morphological lexicon from text; • other types of unsupervised discovery of lin-
guistic structure.
",1 Motivation,0,[0]
"Given a corpus or other data about a language, we might aim to predict whether it is an SVO language, or to learn to pick out its noun phrases.",1 Motivation,0,[0]
"For such problems, a single training or test example corresponds to an entire human language.
",1 Motivation,0,[0]
"Unfortunately, we usually have only from 1 to 40 languages to work with.",1 Motivation,0,[0]
"In contrast, machine learning methods thrive on data, and recent AI successes have mainly been on tasks where one can train richly parameterized predictors on a huge set of IID (input, output) examples.",1 Motivation,0,[0]
"Even 7,000 training examples— one for each language or dialect on Earth—would be a small dataset by contemporary standards.
",1 Motivation,0,[0]
"As a result, it is challenging to develop systems that will discover structure in new languages in the same way that an image segmentation method, for example, will discover structure in new images.",1 Motivation,0,[0]
"The limited resources even make it challenging to develop methods that handle new languages by unsupervised, semi-supervised, or transfer learning.",1 Motivation,0,[0]
Some such projects evaluate their methods on new sentences of the same languages that were used to develop the methods in the first place—which leaves one worried that the methods may be inadvertently tuned to the development languages and may not be able to discover correct structure in other languages.,1 Motivation,0,[0]
"Other projects take care to hold out languages for evaluation (Spitkovsky, 2013; Cotterell et al., 2015), but then are left with only a few development languages on which to experiment with different unsupervised methods and their hyperparameters.
",1 Motivation,0,[0]
"If we had many languages, then we could develop better unsupervised language learners.",1 Motivation,0,[0]
"Even better, we could treat linguistic structure discovery as a supervised learning problem.",1 Motivation,0,[0]
"That is, we could train a system to extract features from the surface of a language that are predictive of its deeper structure.",1 Motivation,0,[0]
"Principles & Parameters theory (Chomsky, 1981) conjectures that such features exist and that the juvenile human brain is adapted to extract them.
",1 Motivation,0,[0]
"Our goal in this paper is to release a set of about 50,000 high-resource languages that could be used to train supervised learners, or to evaluate lesssupervised learners during development.",1 Motivation,0,[0]
"These “unearthly” languages are intended to be at least sim-
ar X
iv :1
71 0.
03 83
8v 1
[ cs
.C",1 Motivation,0,[0]
"L
] 1
0",1 Motivation,0,[0]
"O
ct 2
01 7
ilar to possible human languages.",1 Motivation,0,[0]
"As such, they provide useful additional training and development data that is slightly out of domain (reducing the variance of a system’s learned parameters at the cost of introducing some bias).",1 Motivation,0,[0]
The initial release as described in this paper (version 1.0) is available at https://github.com/gdtreebank/ gdtreebank.,1 Motivation,0,[0]
"We plan to augment this dataset in future work (§8).
",1 Motivation,0,[0]
"In addition to releasing thousands of treebanks, we provide scripts that can be used to “translate” other annotated resources into these synthetic languages.",1 Motivation,0,[0]
"E.g., given a corpus of English sentences labeled with sentiment, a researcher could reorder the words in each English sentence according to one of our English-based synthetic languages, thereby obtaining labeled sentences in the synthetic language.",1 Motivation,0,[0]
Synthetic data generation is a well-known trick for effectively training a large model on a small dataset.,2 Related Work,0,[0]
Abu-Mostafa (1995) reviews early work that provided “hints” to a learning system in the form of virtual training examples.,2 Related Work,0,[0]
"While datasets have grown in recent years, so have models: e.g., neural networks have many parameters to train.",2 Related Work,0,[0]
"Thus, it is still common to create synthetic training examples—often by adding noise to real inputs or otherwise transforming them in ways that are expected to preserve their labels.",2 Related Work,0,[0]
"Domains where it is easy to exploit these invariances include image recognition (Simard et al., 2003; Krizhevsky et al., 2012), speech recognition (Jaitly and Hinton, 2013; Cui et al., 2015), information retrieval (Vilares et al., 2011), and grammatical error correction (Rozovskaya and Roth, 2010).
",2 Related Work,0,[0]
Synthetic datasets have also arisen recently for semantic tasks in natural language processing.,2 Related Work,0,[0]
"bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2016).",2 Related Work,0,[0]
"Hermann et al. (2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers.",2 Related Work,0,[0]
"Serban et al. (2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems.",2 Related Work,0,[0]
"Wang et al.
(2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language.
",2 Related Work,0,[0]
"In speech recognition, morphology-based “vocabulary expansion” creates synthetic word forms (Rasooli et al., 2014; Varjokallio and Klakow, 2016).
",2 Related Work,0,[0]
"Machine translation researchers have often tried to automatically preprocess parse trees of a source language to more closely resemble those of the target language, using either hand-crafted or automatically extracted rules (Dorr et al., 2002; Collins et al., 2005, etc.; see review by Howlett and Dras, 2011).",2 Related Work,0,[0]
A treebank is a corpus of parsed sentences of some language.,3 Synthetic Language Generation,0,[0]
We propose to derive each synthetic treebank from some real treebank.,3 Synthetic Language Generation,0,[0]
"By manipulating the existing parse trees, we obtain a useful corpus for our synthetic language—a corpus that is already tagged, parsed, and partitioned into training/development/test sets.",3 Synthetic Language Generation,0,[0]
"Additional data in the synthetic language can be obtained, if desired, by automatically parsing additional real-language sentences and manipulating these trees in the same way.",3 Synthetic Language Generation,0,[0]
"We begin with the Universal Dependencies collection version 1.2 (Nivre et al., 2015, 2016),1 or UD.",3.1 Method,0,[0]
"This provides manually edge-labeled dependency treebanks in 37 real languages, in a consistent style and format—the Universal Dependencies format.",3.1 Method,0,[0]
"An example appears in Figure 1.
",3.1 Method,0,[0]
"In this paper, we select a substrate language S represented in the UD treebanks, and systematically reorder the dependents of some nodes in the S trees, to obtain trees of a synthetic language S′.
Specifically, we choose a superstrate language RV, and write S′ = S[RV/V] to denote a (projective) synthetic language obtained from S by permuting the dependents of verbs (V) to match the ordering statistics of the RV treebanks.",3.1 Method,0,[0]
"We can similarly permute the dependents of nouns (N).2 This permutes
1http://universaldependencies.org 2In practice, this means applying a single permutation model to permute the dependents of every word tagged as NOUN (common noun), PROPN (proper noun), or PRON (pronoun).
about 93% of S’s nodes (Table 2), as UD treats adpositions and conjunctions as childless dependents.
",3.1 Method,0,[0]
"For example, English[French/N, Hindi/V] is a synthetic language based on an English substrate, but which adopts subject-object-verb (SOV) word order from the Hindi superstrate and noun-adjective word order from the French superstrate (Figure 1).",3.1 Method,0,[0]
"Note that it still uses English lexical items.
",3.1 Method,0,[0]
"Our terms “substrate” and “superstrate” are borrowed from the terminology of creoles, although our synthetic languages are unlike naturally occurring creoles.",3.1 Method,0,[0]
"Our substitution notation S′ = S[RN/N, RV/V] is borrowed from the logic and programming languages communities.",3.1 Method,0,[0]
There may be more adventurous ways to manufacture synthetic languages (see §8 for some options).,3.2 Discussion,0,[0]
"However, we emphasize that our current method is designed to produce fairly realistic languages.
",3.2 Discussion,0,[0]
"First, we retain the immediate dominance structure and lexical items of the substrate trees, altering only their linear precedence relations.",3.2 Discussion,0,[0]
"Thus each sentence remains topically coherent; nouns continue to be distinguished by case according to their role in the clause structure; wh-words continue to ccommand gaps; different verbs (e.g., transitive vs. intransitive) continue to be associated with different subcategorization frames; and so on.",3.2 Discussion,0,[0]
"These im-
portant properties would not be captured by a simple context-free model of dependency trees, which is why we modify real sentences rather than generating new sentences from such a model.",3.2 Discussion,0,[0]
"In addition, our method obviously preserves the basic context-free properties, such as the fact that verbs typically subcategorize for one or two nominal arguments (Naseem et al., 2010).
",3.2 Discussion,0,[0]
"Second, by drawing on real superstrate languages, we ensure that our synthetic languages use plausible word orders.",3.2 Discussion,0,[0]
"For example, if RV is a V2 language that favors SVO word order but also allows OVS, then S′ will match these proportions.",3.2 Discussion,0,[0]
"Similarly, S′ will place adverbs in reasonable positions with respect to the verb.
",3.2 Discussion,0,[0]
"We note, however, that our synthetic languages might violate some typological universals or typological tendencies.",3.2 Discussion,0,[0]
"For example, RV might prescribe head-initial verb orderings while RN prescribes head-final noun orderings, yielding an unusual language.",3.2 Discussion,0,[0]
"Worse, we could synthesize a language that uses free word order (from RV) even though nouns (from S) are not marked for case.",3.2 Discussion,0,[0]
"Such languages are rare, presumably for the functionalist reason that sentences would be too ambiguous.",3.2 Discussion,0,[0]
"One could automatically filter out such an implausible language S′, or downweight it, upon discovering that a parser for S′ was much less accurate on held-out data than a comparable parser for S.
We also note that our reordering method (§4) does ignore some linguistic structure.",3.2 Discussion,0,[0]
"For example, we do not currently condition the order of the dependent subtrees on their heaviness or on the length of resulting dependencies, and thus we will not faithfully model phenomena like heavy-shift (Hawkins, 1994; Eisner and Smith, 2010).",3.2 Discussion,0,[0]
Nor will we model the relative order of adjectives.,3.2 Discussion,0,[0]
"We also treat all verbs interchangeably, and thus use the same word orders—drawn fromRV—for both main clauses and embedded clauses.",3.2 Discussion,0,[0]
"This means that we will never produce a language like German (which uses V2 order in main clauses and SOV order in embedded clauses), even if RV = German.",3.2 Discussion,0,[0]
All of these problems could be addressed by enriching the features that are described in the next section.,3.2 Discussion,0,[0]
"Let X be a part-of-speech tag, such as Verb.",4 Modeling Dependent Order,0,[0]
"To produce a dependency tree in language S′ = S[RX/X], we start with a projective dependency tree in language S.3 For each node x in the tree that is tagged with X , we stochastically select a new ordering for its dependent nodes, including a position in this ordering for the head x itself.",4 Modeling Dependent Order,0,[0]
"Thus, if node x has n − 1 dependents, then we must sample from a probability distribution over n!",4 Modeling Dependent Order,0,[0]
"orderings.
",4 Modeling Dependent Order,0,[0]
Our job in this section is to define this probability distribution.,4 Modeling Dependent Order,0,[0]
"Using π = (π1, . . .",4 Modeling Dependent Order,0,[0]
", πn) to denote an ordering of these n nodes, we define a log-linear model over the possible values of π:
pθ(π | x) = 1
Z(x) exp ∑ 1≤i<j≤n θ · f(π, i, j) (1)
Here Z(x) is the normalizing constant for node x. θ is the parameter vector of the model.",4 Modeling Dependent Order,0,[0]
"f extracts a sparse feature vector that describes the ordered pair of nodes πi, πj , where the ordering π would place πi to the left of πj .",4 Modeling Dependent Order,0,[0]
"To sample exactly from the distribution pθ,4 we must explicitly compute all n!",4.1 Efficient sampling,0,[0]
"unnormalized prob-
3Our method can only produce projective trees.",4.1 Efficient sampling,0,[0]
"This is because it recursively generates a node’s dependent subtrees, one at a time, in some chosen order.",4.1 Efficient sampling,0,[0]
"Thus, to be safe, we only apply our method to trees that were originally projective.",4.1 Efficient sampling,0,[0]
"See §8.
4We could alternatively have used MCMC sampling.
abilities and their sum Z(x).",4.1 Efficient sampling,0,[0]
"Fortunately, we can compute each unnormalized probability in just O(1) amortized time, if we enumerate the n!",4.1 Efficient sampling,0,[0]
"orderings π using the SteinhausJohnson-Trotter algorithm (Sedgewick, 1977).",4.1 Efficient sampling,0,[0]
"This enumeration sequence has the property that any two consecutive permutations π, π′ differ by only a single swap of some pair of adjacent nodes.",4.1 Efficient sampling,0,[0]
"Thus their probabilities are closely related: the sum in equation (1) can be updated in O(1) time by subtracting θ · f(π, i, i+1) and adding θ · f(π′, i, i+1) for some i.",4.1 Efficient sampling,0,[0]
"The other O(n2) summands are unchanged.
",4.1 Efficient sampling,0,[0]
"In addition, if n ≥ 8, we avoid this computation by omitting the entire tree from our treebank; so we have at most 7! =",4.1 Efficient sampling,0,[0]
5040 summands.,4.1 Efficient sampling,0,[0]
Our feature functions (§4.4) are fixed over all languages.,4.2 Training parameters on a real language,0,[0]
"They refer to the 17 node labels (POS tags) and 40 edge labels (dependency relations) that are used consistently throughout the UD treebanks.
",4.2 Training parameters on a real language,0,[0]
"For each UD language L and each POS tag X , we find parameters θLX that globally maximize the unregularized log-likelihood:
θLX = argmax θ ∑ x log pθ(πx | x) (2)
",4.2 Training parameters on a real language,0,[0]
"Here x ranges over all nodes tagged with X in the projective training trees of the L treebank, omitting nodes with n ≥ 7 for speed.
",4.2 Training parameters on a real language,0,[0]
"The expensive part of this computation is the gradient of logZ(x), which is an expected feature vector.",4.2 Training parameters on a real language,0,[0]
"To compute this expectation efficiently, we again take care to loop over the permutations in Steinhaus-Johnson-Trotter order.
",4.2 Training parameters on a real language,0,[0]
A given language L may not use all of the tags and relations.,4.2 Training parameters on a real language,0,[0]
"Universal features that mention unused tags or relations do not affect (2), and their weights remain at 0 during training.",4.2 Training parameters on a real language,0,[0]
We use (1) to permute the X nodes of substrate language S into an order resembling superstrate language RX .,4.3 Setting parameters of a synthetic language,0,[0]
"In essence, this applies the RX ordering model to out-of-domain data, since the X nodes may have rather different sets of dependents in the S treebank than in the RX treebank.",4.3 Setting parameters of a synthetic language,0,[0]
"We mitigate this issue in two ways.
",4.3 Setting parameters of a synthetic language,0,[0]
"First, our ordering model (1) is designed to be more robust to transfer than, say, a Markov model.",4.3 Setting parameters of a synthetic language,0,[0]
"The position of each node is influenced by all n− 1 other nodes, not just by the two adjacent nodes.",4.3 Setting parameters of a synthetic language,0,[0]
"As a result, the burden of explaining the ordering is distributed over more features, and we hope some of these features will transfer to S.",4.3 Setting parameters of a synthetic language,0,[0]
"For example, suppose RX lacks adverbs",4.3 Setting parameters of a synthetic language,0,[0]
and yet we wish to use θ RX X to permute a sequence of S that contains adverbs.,4.3 Setting parameters of a synthetic language,0,[0]
"Even though the resulting order must disrupt some familiar non-adverb bigrams by inserting adverbs, other features—which consider non-adjacent tags— will still favor anRX -like order for the non-adverbs.
",4.3 Setting parameters of a synthetic language,0,[0]
"Second, we actually sample the reordering from a distribution pθ with an interpolated parameter vector
θ = θS ′
X =",4.3 Setting parameters of a synthetic language,0,[0]
"(1− λ)θ RX X + λθ S X ,
where λ = 0.05.",4.3 Setting parameters of a synthetic language,0,[0]
"This gives a weighted product of experts, in which ties are weakly broken in favor of the substrate ordering.",4.3 Setting parameters of a synthetic language,0,[0]
"(Ties arise when RX is unfamiliar with some tags that appear in S, e.g., adverb.)",4.3 Setting parameters of a synthetic language,0,[0]
"We write ti for the POS tag of node πi, and ri for the dependency relation of πi to the head node.",4.4 Feature Templates,0,[0]
"If πi is itself the head, then necessarily ti = X ,5 and we specially define ri = head.
",4.4 Feature Templates,0,[0]
"In our feature vector f(π, i, j), the features with the following names have value 1, while all others have value 0:
• L.ti.ri and L.ti and L.ri, provided that rj = head.",4.4 Feature Templates,0,[0]
"For example, L.ADJ will fire on each ADJ node to the left of the head.
",4.4 Feature Templates,0,[0]
"• L.ti.ri.tj.rj and L.ti.tj and L.ri.rj , provided that ri 6= head, rj 6= head.",4.4 Feature Templates,0,[0]
"These features detect the relative order of two siblings.
",4.4 Feature Templates,0,[0]
"• d.ti.ri.tj .rj ,",4.4 Feature Templates,0,[0]
"d.ti.tj , and d.ri.rj , where d is l (left), m (middle), or r (right) according to whether the head position h satisfies i < j < h, i < h < j, or h <",4.4 Feature Templates,0,[0]
"i < j. For example, l.nsubj.dobj will fire on SOV clauses.",4.4 Feature Templates,0,[0]
"This is a specialization of the previous feature, and is skipped if i = h or j = h.
5Recall that for each head POS X of language L, we learn a separate ordering model with parameter vector θLX .
",4.4 Feature Templates,0,[0]
"• A.ti.ri.tj.rj and A.ti.tj and A.ri.rj , provided that j = i + 1.",4.4 Feature Templates,0,[0]
These “bigram features” detect two adjacent nodes.,4.4 Feature Templates,0,[0]
"For this feature and the next one, we extend the summation in (1) to allow 0 ≤",4.4 Feature Templates,0,[0]
i < j ≤ n,4.4 Feature Templates,0,[0]
"+ 1, taking t0 = r0 = BOS (“beginning of sequence”) and tn+1 = rn+1 = EOS (“end of sequence”).",4.4 Feature Templates,0,[0]
"Thus, a bigram feature such as A.DET.EOS would fire on DET when it falls at the end of the sequence.
",4.4 Feature Templates,0,[0]
"• H.ti.ri.ti+1.ri+1.....tj.rj , provided that i+2 ≤ j ≤ i+4.",4.4 Feature Templates,0,[0]
"Among features of this form, we keep only the 10% that fire most frequently in the training data.",4.4 Feature Templates,0,[0]
"These “higher-order kgram” features memorize sequences of lengths 3 to 5 that are common in the language.
",4.4 Feature Templates,0,[0]
"Notice that for each non-H feature that mentions both tags t and relations r, we also defined two backoff features, omitting the t fields or r fields respectively.
",4.4 Feature Templates,0,[0]
"Using the example from Figure 1, for subtree
DET ADJ NOUN this particular future
det
amod
the features that fire are
Template Features L.ti.ri L.DET.det, L.ADJ.amod L.ti.ri.tj.rj L.DET.det.ADJ.amod d.ti.ri.tj.rj l.DET.det.ADJ.amod A.t1.r1.t2.r2 A.BOS.BOS.DET.det,
A.DET.det.ADJ.amod, A.ADJ.amod.NOUN.head, A.NOUN.head.",4.4 Feature Templates,0,[0]
"EOS.EOS
plus backoff features and H features (not shown).",4.4 Feature Templates,0,[0]
"In Galactic Dependencies v1.0, or GD, we release real and synthetic treebanks based on UD v1.2.",5 The Resource,0,[0]
Each synthetic treebank is a modified work that is freely licensed under the same CC or GPL license as its substrate treebank.,5 The Resource,0,[0]
"We provide all languages of the form S, S[RV/N], S[RN/V], and S[RN/N, RV/V], where the substrate S and the superstrates RN and
RV each range over the 37 available languages.",5 The Resource,0,[0]
(RN = S orRV = S gives “self-permutation”).,5 The Resource,0,[0]
"This yields 37× 38× 38 = 53, 428 languages in total.
",5 The Resource,0,[0]
"Each language is provided as a directory of 3 files: training, development, and test treebanks.",5 The Resource,0,[0]
"The directories are systematically named: for example, English[French/N, Hindi/V] can be found in directory en∼fr@N∼hi@V.
Our treebanks provide alignment information, to facilitate error analysis as well as work on machine translation.",5 The Resource,0,[0]
Each word in a synthetic sentence is annotated with its original position in the substrate sentence.,5 The Resource,0,[0]
"Thus, all synthetic treebanks derived from the same substrate treebank are node-to-node aligned to the substrate treebank and hence to one another.
",5 The Resource,0,[0]
"In addition to the generated data, we also provide the parameters θLX of our ordering models; code for training new ordering models; and code for producing new synthetic trees and synthetic languages.",5 The Resource,0,[0]
"Our code should produce reproducible results across platforms, thanks to Java’s portability and our standard random number seed of 0.",5 The Resource,0,[0]
How do the synthetic languages compare to the real ones?,6 Exploratory Data Analysis,0,[0]
"For analysis and experimentation, we partition the real UD languages into train/dev/test (Table 1).",6 Exploratory Data Analysis,0,[0]
(This is orthogonal to the train/dev/test split of each language’s treebank.),6 Exploratory Data Analysis,0,[0]
"Table 2 shows some properties of the real training languages.
",6 Exploratory Data Analysis,0,[0]
"In this section and the next, we use the Yara
meanx[− log2 1/n(x)!",6 Exploratory Data Analysis,0,[0]
"]
, where x ranges over all N and V
tokens in the dev sentences, n(x) is 1 + the number of dependents of x, and π∗(x) is the observed ordering at x.
parser (Rasooli and Tetreault, 2015), a fast arc-eager transition-based projective dependency parser, with beam size of 8.",6 Exploratory Data Analysis,0,[0]
"We train only delexicalized parsers, whose input is the sequence of POS tags.",6 Exploratory Data Analysis,0,[0]
"Parsing accuracy is evaluated by the unlabeled attachment score (UAS), that is, the fraction of word tokens in held-out (dev) data that are assigned their correct parent.",6 Exploratory Data Analysis,0,[0]
"For language modeling, we train simple trigram backoff language models with add-1 smoothing, and we measure predictive accuracy as the perplexity of held-out (dev) data.
",6 Exploratory Data Analysis,0,[0]
Figures 2–3 show how the parsability and perplexity of a real training language usually get worse when we permute it.,6 Exploratory Data Analysis,0,[0]
"We could have discarded lowparsability synthetic languages, on the functionalist grounds that they would be unlikely to survive as natural languages anywhere in the galaxy.",6 Exploratory Data Analysis,0,[0]
"However, the curves in these figures show that most synthetic languages have parsability and perplexity within the plausible range of natural languages, so we elected to simply keep all of them in our collection.
",6 Exploratory Data Analysis,0,[0]
"An interesting exception in Figure 2 is Latin
1 2 3 4 5 6 7 8
fr
pt",6 Exploratory Data Analysis,0,[0]
"no
de
it la_itt
ar cs
hi
es
real pos synthetic pos real word synthetic word
(la itt), whose poor parsability—at least by a delexicalized parser that does not look at word endings— may be due to its especially free word order (Table 2).",6 Exploratory Data Analysis,0,[0]
"When we impose another language’s more consistent word order on Latin, it becomes more parsable.",6 Exploratory Data Analysis,0,[0]
"Elsewhere, permutation generally hurts, perhaps because a real language’s word order is globally optimized to enhance parsability.",6 Exploratory Data Analysis,0,[0]
It even hurts slightly when we randomly “self-permute” S trees to use other word orders that are common in S itself!,6 Exploratory Data Analysis,0,[0]
"Presumably this is because the authors of the original S sentences chose, or were required, to order each constituent in a way that would enhance its parsability in context: see the last paragraph of §3.2.
",6 Exploratory Data Analysis,0,[0]
Synthesizing languages is a balancing act.,6 Exploratory Data Analysis,0,[0]
"The synthetic languages are not useful if all of them are too conservatively close to their real sources to add
diversity—or too radically different to belong in the galaxy of natural languages.",6 Exploratory Data Analysis,0,[0]
"Fortunately, we are at neither extreme.",6 Exploratory Data Analysis,0,[0]
"Figure 4 visualizes a small sample of 110 languages from our collection.6 For each ordered pair of languages (S, T ), we defined the dissimilarity d(S, T ) as the decrease in UAS when we parse the dev data of T using a parser trained on S instead of one trained on T .",6 Exploratory Data Analysis,0,[0]
"Small dissimilarity (i.e., good parsing transfer) translates to small distance in the figure.",6 Exploratory Data Analysis,0,[0]
"The figure shows that the permutations of a substrate language (which share its color) can be radically different from it, as we already saw above.",6 Exploratory Data Analysis,0,[0]
"Some may be unnatural, but others are similar to other real languages, including held-out dev
6For each of the 10 real training languages, we sampled 9 synthetic languages: 3 N-permuted, 3 V-permuted and 3 {N,V}permuted.",6 Exploratory Data Analysis,0,[0]
"We also included all 10 training + 10 dev languages.
languages.",6 Exploratory Data Analysis,0,[0]
"Thus Dutch (nl) and Estonian (et) have close synthetic neighbors within this small sample, although they have no close real neighbors.",6 Exploratory Data Analysis,0,[0]
"We now illustrate the use of GD by studying how expanding the set of available treebanks can improve a simple NLP method, related to Figure 4.",7 An Experiment,0,[0]
Dependency parsing of low-resource languages has been intensively studied for years.,7.1 Single-source transfer,0,[0]
"A simple method is called “single-source transfer”: parsing a target language T with a parser that was trained on a source language S, where the two languages are syntactically similar.",7.1 Single-source transfer,0,[0]
"Such single-source transfer parsers (Ganchev et al., 2010; McDonald et al., 2011; Ma and Xia, 2014; Guo et al., 2015; Duong et al., 2015; Rasooli and Collins, 2015) are not state-ofthe-art, but they have shown substantial improvements over fully unsupervised grammar induction systems (Klein and Manning, 2004; Smith and Eisner, 2006; Spitkovsky et al., 2013).
",7.1 Single-source transfer,0,[0]
It is permitted for S and T to have different vocabularies.,7.1 Single-source transfer,0,[0]
The S parser can nonetheless parse T (as in Figure 4)—provided that it is a “delexicalized” parser that only cares about the POS tags of the input words.,7.1 Single-source transfer,0,[0]
"In this case, we require only that the target sentences have already been POS tagged using the same tagset as S: in our case, the UD tagset.",7.1 Single-source transfer,0,[0]
"We evaluate single-source transfer when the pool of m source languages consists of n real UD languages, plus m− n synthetic GD languages derived by “remixing” just these real languages.7 We try various values of n and m, where n can be as large as 10 (training languages from Table 1) and m can be as large as n× (n+ 1)× (n+ 1) ≤ 1210 (see §5).
",7.2 Experimental Setup,0,[0]
"Given a real target language T from outside the pool, we select a single source language S from the pool, and try to parse UD sentences of T with a parser trained on S. We evaluate the results on T by measuring the unlabeled attachment score (UAS),
7The m− n GD treebanks are comparatively impoverished because—in the current GD release—they include only projective sentences (Table 2).",7.2 Experimental Setup,0,[0]
"The n UD treebanks are unfiltered.
",7.2 Experimental Setup,0,[0]
"that is, the fraction of word tokens that were assigned their correct parent.",7.2 Experimental Setup,0,[0]
"In these experiments (unlike those of §6), we always evaluate fairly on T ’s full dev or test set from UD—not just the sentences we kept for its GD version (cf. Table 2).8
",7.2 Experimental Setup,0,[0]
The hope is that a large pool will contain at least one language—real or synthetic—that is “close” to T .,7.2 Experimental Setup,0,[0]
"We have two ways of trying to select a source S with this property:
Supervised selection selects the S whose parser achieves the highest UAS on 100 training sentences of language T .",7.2 Experimental Setup,0,[0]
"This requires 100 good trees for T , which could be obtained with a modest investment—a single annotator attempting to follow the UD annotation standards in a consistent way on 100 sentences of T , without writing out formal T - specific guidelines.",7.2 Experimental Setup,0,[0]
(There is no guarantee that selecting a parser on training data will choose well for the test sentences of T .,7.2 Experimental Setup,0,[0]
"We are using a small amount of data to select among many dubious parsers, many of which achieve similar results on the training sentences of T .",7.2 Experimental Setup,0,[0]
"Furthermore, in the UD treebanks, the test sentences of T are sometimes drawn from a different distribution than the training sentences.)
",7.2 Experimental Setup,0,[0]
Unsupervised selection selects the S whose training sentences had the best “coverage” of the POS tag sequences in the actual data from T that we aim to parse.,7.2 Experimental Setup,0,[0]
"More precisely, we choose the S that maximizes pS(tag sequences from T )—in other words, the maximum-likelihood S—where pS is our trigram language model for the tag sequences of S. This approach is loosely inspired by Søgaard (2011).",7.2 Experimental Setup,0,[0]
"Our most complete visualization is Figure 5, which we like to call the “kite graph” for its appearance.",7.3 Results,0,[0]
"We plot the UAS on the development treebank of T as a function of n, m, and the selection method.",7.3 Results,0,[0]
"As Appendix A details, each point on this graph is actually an average over 10,000 experiments that make random choices of T (from the UD development languages), the n real languages (from the UD training languages), and the m − n synthetic languages (from the GD languages derived from the n real lan-
8The Yara parser can only produce projective parses.",7.3 Results,0,[0]
"It attempts to parse all test sentences of T projectively, but sadly ignores non-projective training sentences of S (as can occur for real S).
",7.3 Results,0,[0]
"Each point is the mean dev UAS over 10,000 experiments.",7.3 Results,0,[0]
We use paler lines in the same color and style to show the considerable variance of these UAS scores.,7.3 Results,0,[0]
These essentially delimit the interdecile range from the 10th to the 90th percentile of UAS score.,7.3 Results,0,[0]
"However, if the plot shows a mean of 57, an interdecile range from 53 to 61 actually means that the middle 80% of experiments were within ±4 percentage points of the mean UAS for their target language.",7.3 Results,0,[0]
"(In other words, before computing this range, we adjust each UAS score for target T by subtracting the mean UAS from the experiments with target T , and adding back the mean UAS from all 10,000 experiments (e.g., 57).)
",7.3 Results,0,[0]
"Notice that on the n = 10 curve, there is no variation among experiments either at the minimum m (where the pool always consists of all 10 real languages) or at the maximum m (where the pool always consists of all 1210 galactic languages).
guages).",7.3 Results,0,[0]
We see from the black lines that increasing the number of real languages n is most beneficial.,7.3 Results,0,[0]
"But crucially, when n is fixed in practice, gradually increasing m by remixing the real languages does lead to meaningful improvements.",7.3 Results,0,[0]
This is true for both selection methods.,7.3 Results,0,[0]
"Supervised selection is markedly better than unsupervised.
",7.3 Results,0,[0]
The “selection graph” in Figure 6 visualizes the same experiments in a different way.,7.3 Results,0,[0]
Here we ask about the fraction of experiments in which using the full pool of m source languages was strictly better than using only the n real languages.,7.3 Results,0,[0]
"We find that when m has increased to its maximum, the full pool nearly always contains a synthetic source language that gets better results than anything in the real pool.",7.3 Results,0,[0]
"After all, our generation of “random” languages is a scattershot attempt to hit the target: the more languages we generate, the higher our chances of coming close.",7.3 Results,0,[0]
"However, our selection methods only manage to pick a better language in about 60% of those experiments.
",7.3 Results,0,[0]
Figure 7 offers a fine-grained look at which real and synthetic source languages S succeeded best when T = English.,7.3 Results,0,[0]
"Each curve shows a different superstrate, with the x-axis ranging over substrates.",7.3 Results,0,[0]
"(The figure omits the hundreds of synthetic source languages that use two distinct superstrates, RV 6= RN.)",7.3 Results,0,[0]
"Real languages are shown as solid black dots, and are often beaten by synthetic languages.",7.3 Results,0,[0]
"For comparison, this graph also plots results that “cheat” by using English supervision.
",7.3 Results,0,[0]
The above graphs are evaluated on development sentences in development languages.,7.3 Results,0,[0]
"For our final results, Table 3, we finally allow ourselves to try transferring to the UD test languages, and we eval-
uate on test sentences.",7.3 Results,0,[0]
The comparison is similar to the comparison in the selection graph: do the synthetic treebanks add value?,7.3 Results,0,[0]
"We use our largest source pools, n = 10 and m = 1210.",7.3 Results,0,[0]
"With supervised selection, selecting the source language from the full pool of m options (not just the n real languages) tends to achieve significantly better UAS on the target language, often dramatically so.",7.3 Results,0,[0]
"On average, the UAS on the test languages increases by 2.3 percentage points, and this increase is statistically significant across these 17 data points.",7.3 Results,0,[0]
"Even with unsupervised selection, UAS still increases by 1.2 points on average, but this difference could be a chance effect.
",7.3 Results,0,[0]
The results above use gold POS tag sequences for T .,7.3 Results,0,[0]
These may not be available if T is a low-resource language; see Appendix B for a further experiment.,7.3 Results,0,[0]
"Many of the curves in Figures 5–6 still seem to be increasing steadily at maximum m, which suggests
that we would benefit from finding ways to generate even more synthetic languages.",7.4 Discussion,0,[0]
"Diversity of languages seems to be crucial, since adding new real languages improves performance much faster than remixing existing languages.",7.4 Discussion,0,[0]
"This suggests that we should explore making more extensive changes to
the UD treebanks (see §8).",7.4 Discussion,0,[0]
"Surprisingly, Figures 5–6 show improvements even when n = 1.",7.4 Discussion,0,[0]
"Evidently, self-permutation of a single language introduces some useful variety, perhaps by transporting specialized word orders (e.g., English still allows some limited V2 constructions) into contexts where the source language would not ordinarily allow them but the target language does.
",7.4 Discussion,0,[0]
Figure 5 shows why unsupervised selection is considerably worse on average than supervised selection.,7.4 Discussion,0,[0]
"Its 90th percentile is comparable, but at the 10th percentile—presumably representing experiments where no good sources are available—the unsupervised heuristic has more trouble at choosing among the mediocre options.",7.4 Discussion,0,[0]
"The supervised method can actually test these options using the true loss function.
",7.4 Discussion,0,[0]
Figure 7 is interesting to inspect.,7.4 Discussion,0,[0]
"English is essentially a Germanic language with French influence due to the Norman conquest, so it is reassuring that German and French substrates can each be improved by using the other as a superstrate.",7.4 Discussion,0,[0]
"We also see that Arabic and Hindi are the worst source languages for English, but that Hindi[Arabic/V] is considerably better.",7.4 Discussion,0,[0]
This is because Hindi is reasonably similar to English once we correct its SOV word order to SVO (via almost any superstrate).,7.4 Discussion,0,[0]
"This paper is the first release of a novel resource, the Galactic Dependencies treebank collection, that may unlock a wide variety of research opportunities (discussed in §1).",8 Conclusions and Future Work,0,[0]
Our empirical studies show that the synthetic languages in this collection remain somewhat natural while improving the diversity of the collection.,8 Conclusions and Future Work,0,[0]
"As a simplistic but illustrative use of the resource, we carefully evaluated its impact on the naive technique of single-source transfer parsing.",8 Conclusions and Future Work,0,[0]
"We found that performance could consistently be improved by adding synthetic languages to the pool of sources, assuming gold POS tags.
",8 Conclusions and Future Work,0,[0]
"There are several non-trivial opportunities for improving and extending our treebank collection in future releases.
1.",8 Conclusions and Future Work,0,[0]
"Our current method is fairly conservative, only synthesizing languages with word orders already attested in our small collection of real languages.",8 Conclusions and Future Work,0,[0]
"This
does not increase the diversity of the pool as much as when we add new real languages.",8 Conclusions and Future Work,0,[0]
"Thus, we are particularly interested in generating a wider range of synthetic languages.",8 Conclusions and Future Work,0,[0]
"We could condition reorderings on the surrounding tree structure, as noted in §3.2.",8 Conclusions and Future Work,0,[0]
We could choose reordering parameters θX more adventurously than by drawing them from a single known superstrate language.,8 Conclusions and Future Work,0,[0]
"We could go beyond reordering, to systematically choose what function words (determiners, prepositions, particles), function morphemes, or punctuation symbols 9 should appear in the synthetic tree, or to otherwise alter the structure of the tree (Dorr, 1993).",8 Conclusions and Future Work,0,[0]
These options may produce implausible languages.,8 Conclusions and Future Work,0,[0]
"To mitigate this, we could filter or reweight our sample of synthetic languages—via rejection sampling or importance sampling—so that they are distributed more like real languages, as measured by their parsabilities, dependency lengths, and estimated WALS features (Dryer and Haspelmath, 2013).
2.",8 Conclusions and Future Work,0,[0]
"Currently, our reordering method only generates projective dependency trees.",8 Conclusions and Future Work,0,[0]
"We should extend it to allow non-projective trees as well—for example, by pseudo-projectivizing the substrate treebank (Nivre and Nilsson, 2005) and then deprojectivizing it after reordering.
3.",8 Conclusions and Future Work,0,[0]
"The treebanks of real languages can typically be augmented with larger unannotated corpora in those languages (Majliš, 2011), which can be used to train word embeddings and language models, and can also be used for self-training and bootstrapping methods.",8 Conclusions and Future Work,0,[0]
"We plan to release comparable unannotated corpora for our synthetic languages, by au-
9Our current handling of punctuation produces unnatural results, and not merely because we treat all tokens with tag PUNCT as interchangeable.",8 Conclusions and Future Work,0,[0]
Proper handling of punctuation and capitalization would require more than just reordering.,8 Conclusions and Future Work,0,[0]
"For example, “Jane loves her dog, Lexie.”",8 Conclusions and Future Work,0,[0]
"should reorder into “Her dog, Lexie, Jane loves.”, which has an extra comma and an extra capital.",8 Conclusions and Future Work,0,[0]
"Accomplishing this would require first recovering a richer tree for the original sentence, in which the appositive Lexie is bracketed by a pair of commas and the name Jane is doubly capitalized.",8 Conclusions and Future Work,0,[0]
"These extra tokens were not apparent in the original sentence’s surface form because the final comma was absorbed into the adjacent period, and the startof-sentence capitalization was absorbed into the intrinsic capitalization of Jane (Nunberg, 1990).",8 Conclusions and Future Work,0,[0]
"The tokenization provided by the UD treebanks unfortunately does not attempt to undo these orthographic processes, even though it undoes some morphological processes such as contraction.
tomatically parsing and permuting the unnanotated corpora of their substrate languages.
4.",8 Conclusions and Future Work,0,[0]
"At present, all languages derived from an English substrate use the English vocabulary.",8 Conclusions and Future Work,0,[0]
"In the future, we plan to encipher that vocabulary separately for each synthetic language, perhaps choosing a cipher so that the result loosely conforms to the realistic phonotactics and/or orthography of some superstrate language.",8 Conclusions and Future Work,0,[0]
This would let multilingual methods exploit lexical features without danger of overfitting to specific lexical items that appear in many synthetic training languages.,8 Conclusions and Future Work,0,[0]
"Alphabetic ciphers can preserve features of words that are potentially informative for linguistic structure discovery: their cooccurrence statistics, their length and phonological shape, and the sharing of substrings among morphologically related words.
5.",8 Conclusions and Future Work,0,[0]
"Finally, we note that this paper has focused on generating a broadly reusable collection of synthetic treebanks.",8 Conclusions and Future Work,0,[0]
"For some applications (including singlesource transfer), one might wish to tailor a synthetic language on demand, e.g., starting with one of our treebanks but modifying it further to more closely match the surface statistics of a given target language (Dorr et al., 2002).",8 Conclusions and Future Work,0,[0]
"In our setup, this would involve actively searching the space of reordering parameters, using algorithms such as gradient ascent or simulated annealing.
",8 Conclusions and Future Work,0,[0]
We conclude by revisiting our opening point.,8 Conclusions and Future Work,0,[0]
Unsupervised discovery of linguistic structure is difficult.,8 Conclusions and Future Work,0,[0]
"We often do not know quite what function to maximize, or how to globally maximize it.",8 Conclusions and Future Work,0,[0]
"If we could make labeled languages as plentiful as labeled images, then we could treat linguistic structure discovery as a problem of supervised prediction—one that need not succeed on all formal languages, but which should generalize at least to the domain of possible human languages.",8 Conclusions and Future Work,0,[0]
"The mean lines in the “kite graph” (Figure 5) are actually obtained by averaging 10,000 graphs.",A Constructing the Kite Graph,0,[0]
Each of these graphs is “smooth” because it incrementally adds new languages as n or m increases.,A Constructing the Kite Graph,0,[0]
"Pseudocode to generate one such graph is given as Algorithm 1; all random choices are made uniformly.
",A Constructing the Kite Graph,0,[0]
"Algorithm 1 Data collection for one graph Input: Sets T (target languages), S (real source lan-",A Constructing the Kite Graph,0,[0]
Acknowledgements This work was funded by the U.S. National Science Foundation under Grant No. 1423276.,B Experiment with Noisy Tags,0,[0]
"Our data release is derived from the Universal Dependencies project, whose many selfless contributors have our gratitude.",B Experiment with Noisy Tags,0,[0]
"We would also like to thank Matt Gormley and Sharon Li for early discussions and code prototypes, Mohammad Sadegh Rasooli for guidance on working with the Yara parser, and Jiang Guo, Tim Vieira, Adam Teichert, and Nathaniel Filardo for additional useful discussion.",B Experiment with Noisy Tags,0,[0]
"Finally, we thank TACL editors Joakim Nivre and Lillian Lee and the anonymous reviewers for several suggestions that improved the paper.",B Experiment with Noisy Tags,0,[0]
"We release Galactic Dependencies 1.0—a large set of synthetic languages not found on Earth, but annotated in Universal Dependencies format.",abstractText,0,[0]
This new resource aims to provide training and development data for NLP methods that aim to adapt to unfamiliar languages.,abstractText,0,[0]
Each synthetic treebank is produced from a real treebank by stochastically permuting the dependents of nouns and/or verbs to match the word order of other real languages.,abstractText,0,[0]
"We discuss the usefulness, realism, parsability, perplexity, and diversity of the synthetic languages.",abstractText,0,[0]
"As a simple demonstration of the use of Galactic Dependencies, we consider single-source transfer, which attempts to parse a real target language using a parser trained on a “nearby” source language.",abstractText,0,[0]
"We find that including synthetic source languages somewhat increases the diversity of the source pool, which significantly improves results for most target languages.",abstractText,0,[0]
The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages,title,0,[0]
"Φ(x) = inf z∈Rm
||x− z||22 + h(||z||2)
and h is an even and univariate function on the real line. Connections are drawn between Φ and the Moreau envelope of h. A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works. Finally comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O( √ log n/n).",text,0,[0]
"The dictionary learning problem, also known as sparse coding, was initially studied in the context of Neuroscience (Olshausen & Field, 1997); the relevant literature has grown enormously since; see (Zhang et al., 2015) and references therein.",1. Introduction,0,[0]
The problem is described as follows: given set {Xi}ni=1 ⊂,1. Introduction,0,[0]
"Rm with n points sampled from an unknown fixed probability measure µ, a dictionary matrixD ∈ Rm×d is to be constructed so that any sample from µ can be approximated well by linear combinations of columns of D. The quality of approximation, for a given dictionary D, is measured by some function fD while D usually belongs
1School of Electrical and Computer Engineering, Technical University of Crete, Greece.",1. Introduction,0,[0]
"Correspondence to: Alexandros Georgogiannis <alexandrosgeorgogiannis@gmail.com>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
to some predefined family of matrices.",1. Introduction,0,[0]
"From the statistical learning theory perspective, the aim is to minimize the population risk
R(D)",1. Introduction,0,[0]
":= ∫ fD(X)dµ = ∫ fDdµ, (1)
when the only accessible information is a set of n training samples, say {Xi}ni=1, usually independent and identically distributed.",1. Introduction,0,[0]
"Notation X is used for random vectors sampled from µ and notation x for real vectors in Rm.
",1. Introduction,0,[0]
"The empirical risk minimization principle (ERM) is a natural approach in search of the best dictionary (Vapnik, 1998).",1. Introduction,0,[0]
"It suggests that since the only availiable information is the set of training samples, one should search for the matrix D̂n that minimizes the empirical risk
Rn(D) := 1
n n∑ i=1 fD(Xi).",1. Introduction,0,[0]
"(2)
The empirical estimate D̂n is not of much use unless |Rn(D̂n)",1. Introduction,0,[0]
− R(D̂n)| decreases as the number of samples n increases.,1. Introduction,0,[0]
"Subsuming all computational difficulties on computing the global minimizing argument of (2), the problem addressed here is a “generalization problem”.",1. Introduction,0,[0]
"Given the family D of all m× d matrices with unit-norm columns, we design a loss function fD that measures the quality of approximation x ' Da and ask: Does the difference
|R(D̂n)− inf D∈D R(D)| = ∣∣∣∣∫ fD̂ndµ− infD∈D ∫ fDdµ ∣∣∣∣ (3) decrease as the number of samples n increases, and if so, at what rate?",1. Introduction,0,[0]
"Or even further, if R(D̂n) is close to infD∈DR(D), is D̂n close to the global minimizing argument of R(D)?",1. Introduction,0,[0]
"Intuitively, the decrement of the absolute difference in expression (3) guarantees that by increasing the amount of data the population risk, with high probability, is within a very small distance of the optimal achievable gets arbitrarily close to one.",1. Introduction,0,[0]
"The answers to the previous questions of course depend on the number of samples, the predefined family of dictionaries and the loss function.
",1. Introduction,0,[0]
"The proposed loss functions in the literature of dictionary learning vary according to the application but it would not be an exaggeration to say that almost all of them may be
described by a function of the form:
fD(x) := inf a∈Rd
Φ(x−Da) + g(a), (4)
with Φ : Rm → [0,+∞) and g : Rd → [0,+∞].",1. Introduction,0,[0]
"This article focuses on the generalization properties of dictionary learning when Φ has the form:
Φ(x−Da) :",1. Introduction,0,[0]
"= inf z∈Rm
1 2 ||x−Da− z||22 + h(||z||2).",1. Introduction,0,[0]
"(5)
Function h takes values on [0,+∞] and is described in further detail later on.",1. Introduction,0,[0]
"Definition (5) is not novel and has been used in many applications of sparse coding, robust linear regression and dictionary learning (Adler et al., 2015; Amini et al., 2014; Forero et al., 2015; 2017; Jiang et al., 2015; Liu et al., 2015; Zhao & Tan, 2017).",1. Introduction,0,[0]
"Although there is no formal robustness analysis yet to justify the superiority of (5) over the common square Euclidean loss ||x−Da||22, experimental evaluations in the previous applications suggest that this modification is a computationally “cheap” alternative, achieving better reconstruction error in some cases.
",1. Introduction,0,[0]
"As can be seen from (4), if g is a sparsity promoting penalty then approximations that are linear combinations of a few columns of D are favored.",1. Introduction,0,[0]
"The rationale behind the choice of h in (5) is not so obvious but if h satisfies a set of assumptions, then the following simplification holds true:
Φ(x−Da) :",1. Introduction,0,[0]
= eh(||x−Da||2).,1. Introduction,0,[0]
"(6)
Here, eh is a univariate continuous function with special name and properties, the so called Moreau envelope of h (Rockafellar & Wets, 2009).",1. Introduction,0,[0]
"Interestingly enough, the epigraphical form of eh is completely determined by the generating function h.",1. Introduction,0,[0]
"Roughly speaking, with a suitably chosen h, we can design loss functions fD able to ignore the influence of points x, the distance of which from their approximation Da is above a predefined threshold.",1. Introduction,0,[0]
"The consistency results of this study should be regarded as complementary extensions−and in some cases refinements−of the generalization bounds in (Gribonval et al., 2015b) and (Vainsencher et al., 2011).",1. Introduction,0,[0]
"Contrary to previous works, all bounds presented here are valid for the whole of space of dictionaries with unit-norm columns.
",1. Introduction,0,[0]
"In Section 3 is considered the case where g is a separable function, that is, g is of the form g(a)",1. Introduction,0,[0]
"= ∑d i=1 ĝ(ai), and ĝ : R→ [0,+∞) is univariate, continuous, even, and strictly increasing with minimum value ĝ(0)",1. Introduction,0,[0]
0,1. Introduction,0,[0]
"These assumptions are valid for many coordinate-separable regularizers, e.g., the lp-norms and variants of the logarithmic function.",1. Introduction,0,[0]
"Let us point out here that if h = 0, using the results of Section 3 we revert to previously known bounds for the penalized squared Euclidean loss fD(x) = infa∈Rd (1/2)‖x−Da‖22 + g(a).
",1. Introduction,0,[0]
"Section 4 is an attempt to cover, beyond the class of strictly increasing penalties ĝ of Section 3, continuous and bounded
penalties from robust statistics, such as the MCP or SCAD.",1. Introduction,0,[0]
"This type of penalty functions have achieved widespread use, and to the best of our knowledge, the bounds presented here are among the first that consider them.
",1. Introduction,0,[0]
"However, the extended bounds of Section 4 turn out to be of limited applicability and do not work when g is the indicator function of all k-sparse vectors.",1. Introduction,0,[0]
"To overcome this difficulty, in Section 5, we remove the continuity assumption from g and rely on combinatorial tools from VapnikChervonenkis (VC) theory in order to present bounds valid for any bounded, lower semicontinuous function g.",1. Introduction,0,[0]
"Whenever possible, the sample complexity bounds presented here are compared to similar ones in literature.",1. Introduction,0,[0]
Next follows a brief overview of the relevant literature.,1. Introduction,0,[0]
"The authors in (Gribonval et al., 2015b; Vainsencher et al., 2011) derive sample complexity bounds for the rate of convergence towards 0 of the absolute difference in (3) when Φ(x) = ||x||22, D is a general constraint set, and g(a) ranges from the lp-norms and characteristic functions of compact sets to the indicator function of non-negative vectors or k-sparse vectors.",1.1. Related Work and Contribution,1,"['The assumption of a unique maximizer holds with probability one in most non-trivial cases (de Freitas et al., 2012), and (7) simply formally defines the gap to the second-highest peak.']"
"The results in (Maurer & Pontil, 2010) are independent of dimension m, as well as some results in (Vainsencher et al., 2011).
",1.1. Related Work and Contribution,0,[0]
"A closer look on results of (Gribonval et al., 2015b) and (Vainsencher et al., 2011) concerning the finite case for dimension d, reveals that those are valid under joint assumptions on g and D. For instance, if g is the indicator function of k-sparse vectors in Rd, then the generalization bounds in (Vainsencher et al., 2011) are valid under an incoherence assumption on D while in (Gribonval et al., 2015b) under a “restricted isometry”-like property.",1.1. Related Work and Contribution,0,[0]
"General non-asymptotic results can be extracted from the previous analyses, as the case Φ(x) = ω(||x||), for any convex function ω",1.1. Related Work and Contribution,0,[0]
:,1.1. Related Work and Contribution,0,[0]
"R→ [0,+∞) and any norm || ·",1.1. Related Work and Contribution,0,[0]
|| on Rm.,1.1. Related Work and Contribution,0,[0]
"In (Liu & Tao, 2016) authors focus on the l1-non-negative matrix factorization problem where Φ(x) = ||x||1",1.1. Related Work and Contribution,0,[0]
"and g is the indicator function of the non-negative orthant in Rd.
",1.1. Related Work and Contribution,0,[0]
The main contribution of our work is the addition of generalization bounds concerning loss functions that are combinations of Moreau envelopes with bounded and lower semicontinuous regularizers.,1.1. Related Work and Contribution,0,[0]
"Some results are refinements of previously known ones, meaning that a spurious assumption on dictionary D has been removed.",1.1. Related Work and Contribution,0,[0]
This is mainly a technical section where we take a closer look at the loss function fD and describe the statistical framework for the analysis.,2. Preliminaries and some Technical Remarks,0,[0]
"The value of fD at point x ∈ Rm, in light of equations (4) and (5), is expressed through
the solution of the minimization problem:
inf a∈Rd  := Φ(x−Da)︷",2. Preliminaries and some Technical Remarks,0,[0]
︸︸,2. Preliminaries and some Technical Remarks,0,[0]
︷ inf z∈Rm { 1 2 ||x−Da− z||22 + h(||z||2) },2. Preliminaries and some Technical Remarks,0,[0]
#NAME?,2. Preliminaries and some Technical Remarks,0,[0]
︷︷ ︸ fD(x) .,2. Preliminaries and some Technical Remarks,0,[0]
"(7) The close connection between Φ and h is captured in Lemma 1 that, among others, gives a description of the set of points z ∈ Rm that achieve the minimum in (5).",2. Preliminaries and some Technical Remarks,0,[0]
Lemma 1.,2. Preliminaries and some Technical Remarks,0,[0]
"Let h : R → [0,+∞] be a lower semicontinuous (lsc) and even function with its restriction on [0,+∞) non-decreasing and h(0) = 0.",2. Preliminaries and some Technical Remarks,0,[0]
"Assume that the multivalued map Ph : R→→ R, defined as
Ph(t) := argminu∈R 1
2 (t− u)2 + h(u), (8)
(H1) is odd, i.e., Ph(−t) = −Ph(t), (H2) compact-valued, (H3) non-decreasing, (H4) has a closed graph and (H5) satisfies Ph(t) ≤ t for all t ∈ R. Then function Φ in (5) becomes
Φ(x−Da) = eh(||x−Da||2), (9)
where eh : R→ [0,+∞) is defined as
eh(t) := inf u∈R
1 2 (t− u)2 + h(u), t ∈ R (10)
and is continuous with its restriction on [0,+∞) being nondecreasing.",2. Preliminaries and some Technical Remarks,0,[0]
"Furthermore, map Ph : Rm →→ Rm,
Ph(x−Da) := argminz∈Rm 1
2 ||x−Da−z||22 +h(||z||2),
(11) is equivalently represented as
Ph(x−Da) = x−Da ||x−Da||2 Ph(||x−Da||2).",2. Preliminaries and some Technical Remarks,0,[0]
"(12)
",2. Preliminaries and some Technical Remarks,0,[0]
"According to Lemma 1, if h satisfies a certain set of assumptions, then Φ is equal to the composition of the Moreau envelope of h with the Euclidean norm.",2. Preliminaries and some Technical Remarks,0,[0]
"Although the restrictions surrounding h and its proximal map seem to be strict, the lemma is valid for a large number of h and Ph pairs; see Section 3.1 in (Antoniadis, 2007) for various examples.",2. Preliminaries and some Technical Remarks,0,[0]
"Hereafter, any univariate function h in this article satisfies assumptions (H1) through (H5).
",2. Preliminaries and some Technical Remarks,0,[0]
Example 1.,2. Preliminaries and some Technical Remarks,0,[0]
"The case of the l0-norm on R is an example that clearly describes the influence of h on the boundedness properties of Φ. Let h be the l0-(pseudo)norm on the real line defined as l0(t;λ) = λ 2
2 1{t 6=0} for some λ > 0.",2. Preliminaries and some Technical Remarks,0,[0]
The values of 1{t 6=0} alternate between zero and one according to whether t 6= 0 or not.,2. Preliminaries and some Technical Remarks,0,[0]
"The l0-norm satisfies all
assumptions of Lemma 1: it is even, non-decreasing and lower semicontinuous while its proximal map Pl0 equals Pl0(t) = argminu∈R 1 2 (t−u) 2 + l0(u;λ) and is defined as
Pl0(t) =  0, |t|",2. Preliminaries and some Technical Remarks,0,[0]
"< λ, {0, t}, |t| = λ, t, |t| > λ.
(13)
Now function fD :",2. Preliminaries and some Technical Remarks,0,[0]
"Rm → [0,+∞) reads as
fD(x) = inf a∈Rd  12 min{||x−Da||22, λ2}︸ ︷︷ ︸ := el0 (||x−Da||2) +g(a)  .",2. Preliminaries and some Technical Remarks,0,[0]
"(14) Boundedness of el0 implies that whenever the distance ||x−Da∗D(x)||2 between a point x and its best linear approximation Da∗D(x) is greater than the predefined value λ, then el0(||x − Da∗D(x)||2) = λ2/2; here a∗D(x) is the (possibly multivalued) map
a∗D(x) := argmin a∈Rd
{ el0(||x−Da||22) + g(a) } .",2. Preliminaries and some Technical Remarks,0,[0]
"(15)
As long as g is globally upper bounded by some M > 0, if ||x−Da∗D(x)||2 > λ and a∗D(x) is sufficiently large, then fD(x) = λ
2/2+M .",2. Preliminaries and some Technical Remarks,0,[0]
"Since the empirical optimal dictionary D̂n is defined through the minimization of the empirical risk Rn(D) in (2), and Rn is solely a function of D, points x for which f(x) =",2. Preliminaries and some Technical Remarks,0,[0]
"λ2/2 + M have no influence on the estimation of D̂n, and in that sense are “outliers”.
",2. Preliminaries and some Technical Remarks,0,[0]
The previous example is merely used to build some intuition behind the popularity of fidelity term (9) in the presence of “outliers”.,2. Preliminaries and some Technical Remarks,0,[0]
"As “outliers” are considered points, the distance of which from their approximation Da∗D(x) is larger than a predefined threshold, say γ > 0.",2. Preliminaries and some Technical Remarks,0,[0]
Note that any function h with proximal map satisfying Ph(t) = t when |t| >,2. Preliminaries and some Technical Remarks,0,[0]
"γ behaves like the l0-norm in Example 1.
",2. Preliminaries and some Technical Remarks,0,[0]
Remark 1.,2. Preliminaries and some Technical Remarks,0,[0]
"The simple example described above may serve to anchor intuition, but it should be kept in mind that although we use the term “outlier”, this is rather a study that focuses on the generalization error of dictionary learning.",2. Preliminaries and some Technical Remarks,0,[0]
"We do not provide robustness analysis of dictionary learning, since this would require a detailed mathematical definition of the notion “outlier”.",2. Preliminaries and some Technical Remarks,0,[0]
"Robustness analysis results for Moreau envelope losses using notions from robust statistics, as the breakdown value, are provided in (Georgogiannis, 2016) for the generalized k-means problem; k-means is an unstructured dictionary learning problem−as Dm×d does not have unit-norm columns−with m d, h(t) = 0, and g(·) the indicator of the basis vectors in Rd.
",2. Preliminaries and some Technical Remarks,0,[0]
"A robustness analysis different from the previous one has already been developed in (Gribonval et al., 2015a); the authors show that under coherence-based assumptions on D,
it is highly probable that the empirical risk",2. Preliminaries and some Technical Remarks,0,[0]
1n,2. Preliminaries and some Technical Remarks,0,[0]
∑n i=1,2. Preliminaries and some Technical Remarks,0,[0]
"fD(Xi), when fD(x) = infa∈Rd 12 ||x − Da|| 2 2 + g(a), has a guaranteed empirical local minimum around the neighborhood of a population global minimum dictionary.",2. Preliminaries and some Technical Remarks,0,[0]
"A study motivated by the above references is of great interest and would fill the gap between theoretical and actual performance of dictionary learning algorithms using Moreau envelopes.
",2. Preliminaries and some Technical Remarks,0,[0]
Next is introduced the statistical learning framework.,2. Preliminaries and some Technical Remarks,0,[0]
"Denote as X , X1, X2, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", independent and identically distributed random vectors with values in a closed ball in Rm, say BRm(T ) with radius T centered at the origin, and denote as P̄ the set of all probability distributions µ on the Borel σ-algebra B(BRm(T )) generated by this ball.1",2. Preliminaries and some Technical Remarks,0,[0]
"The aim is to show that the family of functions
FD = { fD(x) : Rd → R; D ∈ D } (16)
has the uniform convergence of empirical means property on the measure space (BRm(T ),B(BRm(T )), µ), µ ∈",2. Preliminaries and some Technical Remarks,0,[0]
P̄ .,2. Preliminaries and some Technical Remarks,0,[0]
Here D is the set of all m× d real matrices with unit Euclideannorm columns and fD is of the form (4).,2. Preliminaries and some Technical Remarks,0,[0]
"The collection of functions FD has the uniform convergence of empirical means (UCEM) property if the following convergence
P  supfD∈FD︸ ︷︷ ︸",2. Preliminaries and some Technical Remarks,0,[0]
supD∈D ∣∣∣∣∣∣∣∣∣∣ 1 n n∑ i=1 fD(Xi)︸ ︷︷ ︸ Rn(D),2. Preliminaries and some Technical Remarks,0,[0]
− ∫ fDdµ︸ ︷︷ ︸ R(D) ∣∣∣∣∣∣∣∣∣∣,2. Preliminaries and some Technical Remarks,0,[0]
"> ε  n→∞−→ 0
(17) is valid for every positive number ε and probability measure µ ∈ P̄ on BRm(T )",2. Preliminaries and some Technical Remarks,0,[0]
"(Vidyasagar, 2002).2 This asymptotic result immediately answers the question raised in the introduction: if (17) holds true, then an application of inequality
R(D̂n)− inf D∈D R(D) .",2. Preliminaries and some Technical Remarks,0,[0]
"sup D∈D |Rn(D)−R(D)|
assures that R(D̂n) tends to the optimal value infD∈DR(D) as the number of samples increases.
",2. Preliminaries and some Technical Remarks,0,[0]
"In most of our proofs, standard arguments from empirical processes theory are followed.",2. Preliminaries and some Technical Remarks,0,[0]
"In Sections 3 and 4 an appropriate form for h and g is chosen and then are used techniques based on either deterministic (Kolmogorov & Širjaev, 1993) or random ε-covers of the function class FD (Györfi et al., 2006); let us recall their definitions.
",2. Preliminaries and some Technical Remarks,0,[0]
Definition 1 (ε-cover).,2. Preliminaries and some Technical Remarks,0,[0]
Let ε > 0,2. Preliminaries and some Technical Remarks,0,[0]
"and let F be a class of functions from A ⊆ Rm to R. Every finite collection
1",2. Preliminaries and some Technical Remarks,0,[0]
The Borel σ-algebra B(Y ) of a subset Y of a metric space S is the one generated by B(Y ),2. Preliminaries and some Technical Remarks,0,[0]
= {Y ∩ E : E ∈ B(S)}.,2. Preliminaries and some Technical Remarks,0,[0]
"Thus the Borel σ-algebra B(BRm(T )) is precisely the class of all subsets of BRm(T ) which are Borel sets in Rm (Folland, 2013).
",2. Preliminaries and some Technical Remarks,0,[0]
2Symbol P in (17) denotes the product measure µ×∞1 on the product σ-algebra ⊗∞ 1 B(BRm(T )),2. Preliminaries and some Technical Remarks,0,[0]
"(Folland, 2013).
of functions f̃1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", f̃N : Rm → R , for which for each f ∈ F there is a j(f) ∈ {1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", N} such that
||f − f̃j ||∞ := sup x∈A",2. Preliminaries and some Technical Remarks,0,[0]
"|f(x)− f̃j(x)| < ε, (18)
is called ε-cover of F under the supremum norm.
",2. Preliminaries and some Technical Remarks,0,[0]
"Let FD,ε = {f1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", fN} be a ε-cover of FD with respect to || · ||∞. As intuitively expected, the fewer the balls needed to cover FD, the smaller the FD.",2. Preliminaries and some Technical Remarks,0,[0]
Definition 2 (ε-covering number).,2. Preliminaries and some Technical Remarks,0,[0]
Let ε > 0,2. Preliminaries and some Technical Remarks,0,[0]
"and let F be a class of functions from a set A ⊆ Rm to R. Let N (ε,F , || · ||∞) be the size of the smallest ε-cover of F under the supremum norm in (18).",2. Preliminaries and some Technical Remarks,0,[0]
"If no finite ε-cover exists, takeN (ε,F , ||·||∞) =∞. ThenN (ε,F , ||·||∞) is named the ε-covering number of F , abbreviated to N∞(ε,F ).
",2. Preliminaries and some Technical Remarks,0,[0]
"The method of proof used in Sections 3 and 4 to establish the UCEM property for FD when g is continuous is based on deterministic ε-covers, basic exponential inequalities and the Borel-Cantelli lemma.",2. Preliminaries and some Technical Remarks,0,[0]
"Unfortunately, this approach does not work when g is the indicator function of all k-sparse vectors; see Section 5.",2. Preliminaries and some Technical Remarks,0,[0]
"To overcome this difficulty, we rely on tools from VC theory, such as the shatter coefficient of the family of subgraphs of a function class.",2. Preliminaries and some Technical Remarks,0,[0]
Definition 3 (subgraphs of a function class).,2. Preliminaries and some Technical Remarks,0,[0]
Consider a function class F with functions f :,2. Preliminaries and some Technical Remarks,0,[0]
Rm → R+.,2. Preliminaries and some Technical Remarks,0,[0]
"The set
F+ := { {(x, t) ∈",2. Preliminaries and some Technical Remarks,0,[0]
"Rm+1 : f(x) ≥ t}; f ∈ F } (19)
is the collection of all subgraphs of functions f in F .
",2. Preliminaries and some Technical Remarks,0,[0]
A family of subgraphs is a family of sets for which the shatter coefficient and VC dimension are defined as follows.,2. Preliminaries and some Technical Remarks,0,[0]
Definition 4 (shatter coefficient).,2. Preliminaries and some Technical Remarks,0,[0]
Let A be a family of sets.,2. Preliminaries and some Technical Remarks,0,[0]
"For {x1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", xn} ⊂ Rm, let NA(x1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", xn) be the number of different sets in {{x1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", xn} ∩ A; A ∈ A} .",2. Preliminaries and some Technical Remarks,0,[0]
"The n-th shatter coefficient s(A, n) of A is
s(A, n)",2. Preliminaries and some Technical Remarks,0,[0]
":= max x1,...,xn NA(x1, . . .",2. Preliminaries and some Technical Remarks,0,[0]
", xn).
",2. Preliminaries and some Technical Remarks,0,[0]
The shatter coefficient is the maximal number of different subsets of n points that can be picked out by sets of A. Definition 5 (VC dimension).,2. Preliminaries and some Technical Remarks,0,[0]
Let A be a collection of sets with |A| ≥ 2.,2. Preliminaries and some Technical Remarks,0,[0]
"The largest integer k ≥ 1 for which s(A, k) = 2k is denoted by VA and is called the VC dimension of the class A.
If for some hypothetical function class F the corresponding shatter coefficient s(F+, n) is a polynomial of degree b with respect to n, i.e., s(F+, n)",2. Preliminaries and some Technical Remarks,0,[0]
"= O(nb), then the popular Vapnik-Chervonenkis’s inequality (Theorem 12.5 in (Devroye et al., 1997)) implies UCEM for F .",2. Preliminaries and some Technical Remarks,0,[0]
"Later on, in Section 5, we show that this is the case for s(F+D , n) as well, where F+D denotes the collection of all subgraphs of functions in FD with g the indicator of k-sparse vectors in Rd−recall the definitions of fD and FD in (4) and (16).",2. Preliminaries and some Technical Remarks,0,[0]
"In this section we prove the UCEM property for the function class FD in (16) when fD is defined as
fD(x) :","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"= inf a∈Rd
{eh(||x−Da||2) + g(a)} (20)
and g has the following form:
g(a) = d∑ i=1 ĝ(ai).","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"(21)
Here is assumed that ĝ : R→ [0,+∞) is a univariate, continuous, even, and strictly increasing function on [0,+∞) with minimum value ĝ(0) = 0.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"The aforementioned assumptions on g are valid for many coordinate-separable regularizers, e.g., the lp norms on Rd, g(a) = λ||a||p, 0","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
< p,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
<,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"+∞ for some λ > 0, and the log penalty function g(a)","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
#NAME?,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
λ,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"log(γ+1) log(γ|ai| + 1), γ > 0.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"From now on, a separable function of the previous form is called (strictly) increasing if for all i, ĝ(ai) is (strictly) increasing as |ai| → +∞. The main result is the following theorem.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Theorem 1.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Let ε > 0,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
and consider the function class FD in (16) with fD :,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
BRm(T ),"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"→ [0, eh(T )] and g : Rd → [0,+∞) defined as in (20) and (21) respectively.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Then
P { sup
fD∈FD ∣∣∣∣∣","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
1n n∑ i=1 fD(Xi)−,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
∫ fDdµ ∣∣∣∣∣,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"> ε }
≤ 2 ( 9dĝ−1(eh(T ))
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"2ε
)md e − 2nε2 9eh(T ) 2 .
(22)","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Furthermore,
sup fD∈FD ∣∣∣∣∣","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
1n n∑ i=1 fD(Xi)−,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"∫ fDdµ ∣∣∣∣∣→ 0 (n→∞) (23) almost surely, for any µ ∈ P̄ .","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Hence, the function class FD has the UCEM property with respect to P̄ .
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"An outline of Theorem’s 1 proof is the following:
1.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
We define map F that maps any m× p matrix to some function of the form (20).,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Using appropriate metrics, F is shown to be globally Lipschitz.
2.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"The Lipschitz continuity of F and the covering number of D generate an upper bound for N∞(ε,FD).
3.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Standard theorems from the empirical process theory imply the concentration result in (22) and finally prove the UCEM property for the function class FD.
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
The above outline makes clear that the main difficulty in proving Theorem 1 is the verification of the Lipschitz continuity of map F .,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Let us mention that factor dĝ
−1(eh(T ))","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"2
appearing on the right hand side (rhs) of (22) is an upper bound for the Lipschitz constant of the aforementioned map.
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
There exist other approaches that do not require any form of continuity on F to prove the UCEM property for FD.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"However, theoretical questions regarding the existence of the optimal dictionary are answered quite easily if we manage to construct such a map.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"For example, as well known, a continuous map maps compact sets to compact sets.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"If F is continuous, the compactness of D implies the compactness of FD.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"This in turn implies the existence of the optimal solution f∗D of minimization problem inffD∈FD ∫ fDdµ; indeed, the integral is a linear operator and FD is compact.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Remark 2.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Another theoretical question, of great importance for the measure theory enthusiasts, concerns the measurability of the supremum appearing on the left hand side (lhs) of (22).","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
This is a random variable of which the measurability stems from total boundedness of FD with respect to the supremum norm ||f ||∞ := sup{x:||x||2≤T} |f(x)|.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Proposition 1.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Assume a set up as the one in Theorem 1.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Then for any δ > 0,
sup fD∈FD ∣∣∣∣∣","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
1n n∑ i=1 fD(Xi)− ∫,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
fDdµ ∣∣∣∣∣ ≤,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
O (√ log(nd) n ),"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"(24)
with probability at least 1− δ.
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"The term log(d) in (24), responsible for the sub-optimality of the bound in case of convex Moreau envelopes, results from our proof method; similar bounds in the literature are of order","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
O( √ log n/n),"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"(Gribonval et al., 2015b; Vainsencher et al., 2011).","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
This term is eliminated in Lemma 2 below to end up with a same order upper bound.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"The latter is in alignment with the sample complexity results presented in (Gribonval et al., 2015b) and (Liu & Tao, 2016) for the cases where fD(x) equals infa∈Rd 12 ||x −Da|| 2 2 + g(a) and infa∈Rd 12 ||x−Da||1 + g(a) respectively.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Lemma 2.,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Let L > dĝ −1(eh(T ))
2 and define β > 0","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"as β := mdmax{log(6L √ 8), 1}.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Assume that n satisfies condition
n
log(n) ≥","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"max
{ 8, ( 1
2 √ 8L
)2 β } (25)
and consider the same set up as in Theorem 1.","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Then,
sup fD∈FD ∣∣∣∣∣","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
1n n∑ i=1 fD(Xi)−,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
∫ fDdµ ∣∣∣∣∣ ≤,"3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"2√8 √ β log n n
+ 1√ 8
√ β + t
n ,
(26) with probability at least 1− 2e−t.
","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"The rationale behind this lemma is to find conditions, under which for large values of the sample size n, an exponential tail for the error kicks in but without the term log(d) of inequality (24).","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
"Although the analysis seems finer, the result is valid only if the sample size satisfies the quite strict and complex inequality (25).","3. The case of a separable, continuous, even, and strictly increasing g : Rd → [0,+∞)",0,[0]
Analysis of Section 3 covers a broad range of regularizers g,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"but it does not cover popular penalty functions from robust statistics, like SCAD, gscad(a) = ∑d i=1 ĝscad(ai) or MCP,
gmcp(a) = ∑d i=1","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"ĝmcp(ai) (Mazumder et al., 2012):
ĝscad(t;λ, γ) =  λt, t ≤ λ λγt− 12 (t 2+λ2)
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"γ−1 , λ < t ≤ γλ λ2(γ2−1) 2(γ−1) , t > λγ, (27)
and
ĝmcp(t;λ, γ) =
{ λt− t 2
2γ , t ≤","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
λ,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"1 2γλ 2, t > γλ.","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"(28)
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Although the previous univariate functions are continuous, even, and satisfy the assumptions of Lemma 1, they fail to satisfy the assumptions of Theorem 1 because they are bounded above and thus not strictly increasing.
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
This section is an attempt to extend the results of Section 3 and handle a very special case of coordinate-separable regularizers: those g(a) = ∑d i=1,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"ĝ(ai), where ĝ : R → [0,+∞) is not only continuous and symmetric around zero, but also strictly increasing up to some point in [0,+∞) and then constant.","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"For this purpose, we require that ĝ satisfies the additional (strict) inequality
eh(T )","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
< sup t∈R ĝ(t).,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"(29)
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Under assumption (29), all results presented in Section 3 remain valid; see the relevant discussion in Appendix A.5.","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Example 2 describes the impact of this assumption on penalty function ĝmcp while the same applies to ĝscad.
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
Example 2.,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Let h be the l0-norm on the real line and ĝ(a) = ĝmcp(a; γ, λ2), λ2 > 0; recall the definition of the l0-norm on the real line: l0(t;λ1) = λ12 1{t 6=0}, λ1 > 0 .","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"In this case, the Moreau envelope is
el0(t;λ1) = 1
2 min{t2, λ21}
and gmcp(a) = ∑d i=1","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"ĝmcp(ai; γ, λ2).","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Now assumption (29) reads as
sup t∈R
ĝmcp(t;λ2, γ) > 1
2 min{T 2, λ21} (30)
or after some simple algebraic calculations,
1 2 λ22γ","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"> 1 2 min{T 2, λ21} ⇔ λ2 >
√ 1
γ min{T 2, λ21}.
(31)","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"Thus, function class FD in (16) with fD(x) defined as
fD(x) = inf a∈Rd
{ el0(||x−Da||2) + d∑ i=1","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"ĝmcp(ai; γ, λ2) }
has the UCEM property only for pairs of values (λ1, λ2) with λ2 >","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"√ 1 γ min{T 2, λ 2 1}.
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
Example 2 reveals that the ease with which we extend the results of Section 3 has great impact on the diversity of functions ĝ that we could handle.,"4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"In order to use the upper bounds in Proposition 1 or Lemma 2, our focus needs to be restricted on families FD where the rightmost inequality in (31) holds.","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"This artificial restriction on the available pair of values (λ1, λ2) makes this extension quite useless; in many applications, when setting up λ1 and λ2, we search on a wider grid of values.
","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"In the next section, we remove the continuity assumption from g and derive generalization bounds valid for any bounded lsc function, such as SCAD, MCP or the indicator function of all k-sparse vectors in Rd.","4. The case of a separable, continuous, even, and bounded g : Rd → [0,+∞)",0,[0]
"k-sparse vectors in Rd and its extension
Denote as Σk = {a ∈ Rd : |{i : ai 6= 0}|",5. The case of the indicator function of all,0,[0]
= k} the set of all k-sparse vectors in Rd.,5. The case of the indicator function of all,0,[0]
The approach followed in Sections 3 and 4 to prove the UCEM property for FD heavily relies on the assumption that g is continuous.,5. The case of the indicator function of all,0,[0]
"Consequently, it does not work for the function
g(a) =",5. The case of the indicator function of all,0,[0]
"{ 0, if a ∈ Σk +∞, otherwise,
(32)
the non-separable and lsc indicator function of all k-sparse vectors in Rd.",5. The case of the indicator function of all,0,[0]
"Using combinatorial tools from VC theory, we remove the spurious condition on the coherence of D ∈ D appearing in previous works (Gribonval et al., 2015b; Vainsencher et al., 2011) and prove the UCEM property when g is bounded and lsc.",5. The case of the indicator function of all,0,[0]
"Starting the analysis with function (32), the results are then extended to cover any bounded lsc function on Rd with range in [0,+∞).
",5. The case of the indicator function of all,0,[0]
"Next is presented Proposition 2, a modification of Theorem 20 in (Vainsencher et al., 2011): it states that map F from metric space (D, || · ||1,2) to metric space (FD, || · ||∞),
FD := { min a∈Σk eh(||x−Da||2); D ∈ D } , (33)
is not uniformly Lipschitz for any Lipschitz constant.3 This is the main reason we resign (ourselves) from previous proof techniques.",5. The case of the indicator function of all,0,[0]
"Without an explicit upper bound for the Lipschitz constant of map F , we cannot infer a bound for the covering number of FD in terms of the one of D.
Proposition 2.",5. The case of the indicator function of all,0,[0]
Consider the family of functions FD in (33).,5. The case of the indicator function of all,0,[0]
"Then, there exist γ > 0 and q ∈ BRm(T )",5. The case of the indicator function of all,0,[0]
"such that for every ε > 0, there exist D,D′ ∈ D such that
max 1≤j≤d
||D·,j −D′·,j ||2 ≤ ε",5. The case of the indicator function of all,0,[0]
but |fD(q)−,5. The case of the indicator function of all,0,[0]
"fD′(q)| > γ.
",5. The case of the indicator function of all,0,[0]
"In other words, map F from D to FD with D ∈ D 7→ F (D) ∈ FD is not globally Lipschitz.
",5. The case of the indicator function of all,0,[0]
Proposition 2 suggests that there are two ways to overcome the limitations when dealing with k-sparse vectors: either more restrictions shall be imposed on the class of dictionaries D or a different proof method has to be followed.,5. The case of the indicator function of all,0,[0]
"The former approach was adopted by (Vainsencher et al., 2011) and (Gribonval et al., 2015b), who both use deterministic ε-net arguments under an incoherence assumption on D and a lower RIP-property, respectively.",5. The case of the indicator function of all,0,[0]
"In such a way, the authors restrict their analysis on a subspace of original space of all unit-norm column dictionaries.
",5. The case of the indicator function of all,0,[0]
"Here the latter approach is adopted: without additional assumptions on the dictionaries, standard tools from VC theory verify the UCEM property of FD.",5. The case of the indicator function of all,0,[0]
"The main result is Proposition 3 which delivers an upper bound for s(F+D , n), the shatter coefficient of
F+D := { {(x, t) ∈ Rm+1 : fD(x) ≥ t}; fD ∈ FD } ;
(34) the previous set collection is the family of all subgraphs of functions fD which belong to FD (as defined in (33)).
",5. The case of the indicator function of all,0,[0]
Proposition 3.,5. The case of the indicator function of all,0,[0]
"The shatter coefficient s(F+D , n) of the collection of sets F+D , as defined in (34), is bounded above as
s(F+D , n) ≤",5. The case of the indicator function of all,0,[0]
"( en α(m,d) )",5. The case of the indicator function of all,0,[0]
"α(m,d) with α(m, d) independent of n and α(m, d) =",5. The case of the indicator function of all,0,[0]
"((m+ d)2 + 3(m+ d))/2 + 1.
",5. The case of the indicator function of all,0,[0]
"A direct use of Proposition’s 3 bound in the popular VapnikChervonenkis’s theorem (Theorem 12.5, (Devroye et al., 1997)) generates Theorem 2 and its byproduct Proposition 4.",5. The case of the indicator function of all,0,[0]
The latter characterizes the rate of convergence to zero of the difference of the sample average from the true mean of fD(X).,5. The case of the indicator function of all,0,[0]
"All random variables appearing in Theorems 2, 3 and Proposition 4 below are assumed measurable.
",5. The case of the indicator function of all,0,[0]
Theorem 2.,5. The case of the indicator function of all,0,[0]
Let fD :,5. The case of the indicator function of all,0,[0]
BRm(T ),5. The case of the indicator function of all,0,[0]
"→ [0, eh(T )] for each fD 3",5. The case of the indicator function of all,0,[0]
"Although Proposition 2 has the same formulation as Theorem 20 of (Vainsencher et al., 2011), the latter cannot apply directly in our case except for k = 2.",5. The case of the indicator function of all,0,[0]
"Proposition 2 clarifies through minor modifications what happens when k > 2.
in the function class FD in (33) and let ε > 0.",5. The case of the indicator function of all,0,[0]
"Then
P { sup
fD∈FD ∣∣∣∣∣",5. The case of the indicator function of all,0,[0]
1n n∑ i=1 fD(Xi)− ∫,5. The case of the indicator function of all,0,[0]
fDdµ ∣∣∣∣∣ >,5. The case of the indicator function of all,0,[0]
"}
≤",5. The case of the indicator function of all,0,[0]
"8s(F+D , n)e − nε2 32eh(T ) 2 .
(35)",5. The case of the indicator function of all,0,[0]
"Furthermore,
∞∑ n=1 ( en α(m, d) )",5. The case of the indicator function of all,0,[0]
"α(m,d) e",5. The case of the indicator function of all,0,[0]
"− 2nε2 32eh(T ) 2 <∞ (36)
for all ε > 0, and by the Borel-Cantelli lemma,
sup fD∈FD ∣∣∣∣∣",5. The case of the indicator function of all,0,[0]
1n n∑ i=1 fD(Xi)−,5. The case of the indicator function of all,0,[0]
∫ fDdµ ∣∣∣∣∣→ 0 (almost surely).,5. The case of the indicator function of all,0,[0]
"(37)
Hence, function class FD has the UCEM property.",5. The case of the indicator function of all,0,[0]
Proposition 4.,5. The case of the indicator function of all,0,[0]
Assume the same setup as in Theorem 2 and let δ > 0.,5. The case of the indicator function of all,0,[0]
"With probability at least 1− δ, holds true that
sup fD∈FD ∣∣∣∣∣",5. The case of the indicator function of all,0,[0]
1n n∑ i=1 fD(Xi)− ∫,5. The case of the indicator function of all,0,[0]
fDdµ ∣∣∣∣∣ ≤,5. The case of the indicator function of all,0,[0]
"O (√ log n n ) .
",5. The case of the indicator function of all,0,[0]
"(38)
When fD(x) = eh(||x||2) and eh(t) = t2, the bounds for the absolute difference in the rhs of (38) in (Gribonval et al., 2015b) and (Vainsencher et al., 2011) are of order
O (√
logn n
) and O (√ log( √ n)
n
) respectively.
",5. The case of the indicator function of all,0,[0]
"Although Proposition 4 is suboptimal compared to the latter, let us recall that Proposition 4 is valid for all dictionaries with unit-norm columns, in contrast to the last referenced bounds that do not cover the whole of space D. With slight modifications, Theorem 2 extends to Theorem 3 which covers any bounded lsc function g, including MCP or SCAD.",5. The case of the indicator function of all,0,[0]
Theorem 3.,5. The case of the indicator function of all,0,[0]
"Let g : Rd → [0,+∞) bounded and lsc, and FD the function class with functions
fD(x) := inf a∈Rd
eh(||x−Da||2) + g(a).",5. The case of the indicator function of all,0,[0]
"(39)
Let ε > 0.",5. The case of the indicator function of all,0,[0]
"Then
P { sup
fD∈FD ∣∣∣∣∣",5. The case of the indicator function of all,0,[0]
1n n∑ i=1 fD(Xi)− ∫,5. The case of the indicator function of all,0,[0]
fDdµ ∣∣∣∣∣ >,5. The case of the indicator function of all,0,[0]
"}
≤",5. The case of the indicator function of all,0,[0]
"8s(F+D , n)e − nε2 32eh(T ) 2 ,
(40) where s(F+D , n) ≤",5. The case of the indicator function of all,0,[0]
"( en α(m,d) )",5. The case of the indicator function of all,0,[0]
"α(m,d) and α(m, d) := ((m+ d)2 + 3(m+ d))/2 + 1.",5. The case of the indicator function of all,0,[0]
"Furthermore, with probability at least 1− δ, holds true that
sup fD∈FD ∣∣∣∣∣",5. The case of the indicator function of all,0,[0]
1n n∑ i=1 fD(Xi)− ∫,5. The case of the indicator function of all,0,[0]
fDdµ ∣∣∣∣∣ ≤,5. The case of the indicator function of all,0,[0]
"O (√ log n n ) .
",5. The case of the indicator function of all,0,[0]
-41,5. The case of the indicator function of all,0,[0]
"As already mentioned, our aim is to analyze the expected reconstruction error of the learned bases D̂n, R(D̂n) :=∫ fD̂ndµ, when D̂n is the (ERM)-estimator D̂n := argminD∈DRn(D).",6. On the approximation error when m d,0,[0]
"This reconstruction error decomposes into the estimation error est and the approximation error app as follows:
R(D̂n) = R(D̂n)−R(D∗)︸ ︷︷ ︸ := est +R(D∗)︸ ︷︷ ︸ := app , (42)
where D∗",6. On the approximation error when m d,0,[0]
":= argminD∈DR(D) is the optimal dictionary, the global minimizer of the population risk.",6. On the approximation error when m d,0,[0]
The estimation error exists because D̂n is just an estimate for D∗. The approximation error measures the risk of restricting ourselves to D rather than to a larger family of matrices.,6. On the approximation error when m d,0,[0]
The optimal choice for D̂n guarantees that both est and app are the smallest possible.,6. On the approximation error when m d,0,[0]
"The estimation error is bounded as
est := R(D̂n)−R(D∗) ≤ 2 sup D∈D |Rn(D)−R(D)|.
",6. On the approximation error when m d,0,[0]
"(43) In previous sections was proven that the rhs of (43) approaches zero as n → +∞ and that, in view of (42), the reconstruction errorR(D̂n) is asymptotically equal to app.",6. On the approximation error when m d,0,[0]
The approximation error does not depend on the sample size n; it is determined by the family of losses under study and the probability distribution of the data.,6. On the approximation error when m d,0,[0]
"In the k-sparse case, app is rarely zero, even for well behaved probability measures µ. The authors in (Vovk, 2016), Section 24, show that the two objectives, of good data approximation and of sparsity of the combination vector a, are incompatible if the data distribution puts its mass far from any low dimensional subspace and in such cases app 6= 0.
",6. On the approximation error when m d,0,[0]
"In this section, assuming m d, app is considered a function of d. An upper bound for app as d→ m, valid for any probability measure µ ∈",6. On the approximation error when m d,0,[0]
"P̄ , gives insights to the problem of approximating points in Rm with combinations of points lying on subspaces of dimension d.",6. On the approximation error when m d,0,[0]
"Following the approach in (Liu & Tao, 2016), we relate the optimal population risk R(D∗) to the quantization error of probability measure µ.
",6. On the approximation error when m d,0,[0]
"Next proposition is meaningful only in the case where g is the indicator function of special compact subsets of Rd, i.e., g(a) = χK(a) withK ⊂ Rd; χK(a) alternates between zero and infinity according to whether its actual argument belongs in K or not.",6. On the approximation error when m d,0,[0]
"Specifically, K is assumed to contain the basis vectors of the positive orthant.",6. On the approximation error when m d,0,[0]
"Assumptions (H1) to (H5) in Lemma 1 regarding h and its proximal map Ph remain valid, but is also required that
(H6) Ph(t) = 0, when t ∈",6. On the approximation error when m d,0,[0]
"[−τ, τ ], (44)
for some predefined value τ > 0.",6. On the approximation error when m d,0,[0]
"These assumptions simplify the proof of Proposition 5: if they are true, then the
Moreau envelope behaves like the quadratic function t2 in a neighborhood around zero.",6. On the approximation error when m d,0,[0]
"Although assumptions (H1) through (H6) may seem strict, they are valid for many univariate penalty functions and compact sets, such as the closed unit-norm balls in Rd.",6. On the approximation error when m d,0,[0]
Proposition 5.,6. On the approximation error when m d,0,[0]
Assume m d.,6. On the approximation error when m d,0,[0]
Let the family of losses FD be defined as FD := {fD(x);,6. On the approximation error when m d,0,[0]
"D ∈ D} with
fD(x) := inf a∈Rd
eh(||x−Da||2) + χK(a), (45)
where χK is the indicator function of some compact setK ⊂ Rd that contains all basis vectors of the positive orthant, i.e., {ej}d1 ∈ K and ej is the j-th column of the identity matrix.
",6. On the approximation error when m d,0,[0]
"If h satisfies the assumptions of Lemma 1 while its proximal map Ph satisfies assumptions (H1)-(H6), then for the approximation error it holds true that
R(D∗) := inf D∈D
∫ fD(x)dµ ≤ O(d−2/m).",6. On the approximation error when m d,0,[0]
"(46)
",6. On the approximation error when m d,0,[0]
"The bound in (46) depends on m and d. Despite being “weak”, as m−2/m → 1, this upper bound provides an insight to the problem: when m is fixed, but sufficiently large, and d → m, the approximation error decreases as d increases, at rate O(d−2/m).",6. On the approximation error when m d,0,[0]
Let us note here that the UCEM property for the family of risk functions FD as defined in Proposition 5 can be proved using elements from the proof of Theorem 1 in Section 3.,6. On the approximation error when m d,0,[0]
This article is a theoretical analysis on the sample complexity of dictionary learning when the loss function to be minimized is the sum of the Moreau envelope of some univariate lsc function h on the real line and a regularization function,7. Conclusions,0,[0]
"g. We derive generalization bounds for a wide range of g, including the case of the indicator function of all k-sparse vectors.",7. Conclusions,0,[0]
"As a byproduct of this analysis is provided some intuition behind the popularity of loss functions under study in the context of “gross outliers”, that is, samples with arbitrary “large” values.",7. Conclusions,0,[0]
"Finally, we comment on the approximation error of an ideal family of losses when the dimension m d, where d is the size of the dictionary.",7. Conclusions,0,[0]
"In the future, it would be interesting to characterize the differentiability properties of the losses under study.",7. Conclusions,0,[0]
Such an analysis would have direct practical applications on the design of numerical optimization algorithms.,7. Conclusions,0,[0]
The author thanks Athanasios P. Liavas and the anonymous reviewers for helpful comments and suggestions that improved the quality of the article.,Acknowledgements,0,[0]
This is a theoretical study on the sample complexity of dictionary learning with general type of reconstruction losses.,abstractText,0,[0]
The goal is to estimate a m × d matrix D of unit-norm columns when the only available information is a set of training samples.,abstractText,0,[0]
Points x in R are subsequently approximated by the linear combination Da after solving the problem mina∈Rd Φ(x−Da),abstractText,0,[0]
#NAME?,abstractText,0,[0]
Here is considered the case where Φ(x) = inf z∈Rm ||x− z||2 + h(||z||2) and h is an even and univariate function on the real line.,abstractText,0,[0]
Connections are drawn between Φ and the Moreau envelope of h.,abstractText,0,[0]
A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works.,abstractText,0,[0]
Finally comments are made on the approximation error of certain families of losses.,abstractText,0,[0]
The derived generalization bounds are of order O( √ log n/n).,abstractText,0,[0]
The Generalization Error of Dictionary Learning with Moreau Envelopes,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2793–2798 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2793",text,0,[0]
"The glass ceiling is a powerful metaphor for the unethical, invisible, and yet virtually impenetrable barrier that prevents highly achieving women and minorities from obtaining equal access to senior career opportunities.",1 Introduction,0,[0]
"The existence of a glass ceiling is well documented both in STEM1 and specifically in Computer Science (Moss-Racusin et al., 2012; Shen, 2013; Larivière et al., 2013; Van der Lee and Ellemers, 2015; Way et al., 2016, for example).",1 Introduction,0,[0]
"To date there has been no published study on this topic for the field of NLP.
",1 Introduction,0,[0]
"In most countries, Computer Science has long been struggling to support female researchers sufficiently: female representation in Computer Science is not only disproportional to the population, but it is lower than the average STEM field.",1 Introduction,0,[0]
"Moreover, as opposed to STEM fields in general, the proportion of women in Computer Science has been on a marked decline for the past two decades (Sax et al., 2017; Williams et al., 2017), placing the entire the tech field in a diversity crisis today.
",1 Introduction,0,[0]
"The discussion of gender representation or even the existence of a glass ceiling is rather more complex for NLP due to its fundamental interdisciplinarity especially across the fields of Linguistics, Computer Science, and Statistics.",1 Introduction,0,[0]
"That is, much mainstream research in NLP follows trends that are heavily situated in one of the main subdisciplines.",1 Introduction,0,[0]
"Can we witness any emergent glass
1Science, technology, engineering and mathematics fields.
",1 Introduction,0,[0]
ceiling for female researchers in the wake of an increasing concentration on deep learning engineering techniques applied to NLP problems?,1 Introduction,0,[0]
What about the preceding Machine Learning wave from the mid 2000s?,1 Introduction,0,[0]
"In this paper we answer this question in the affirmative.
",1 Introduction,0,[0]
We acquired a gender-annotated co-author dataset covering arguably the most central ACL publication venues for the past 52 years.,1 Introduction,0,[0]
We carry out basic data analysis over this dataset and the bipopulated (female and male researcher) mentormentee network derived from it.,1 Introduction,0,[0]
"We make the following concerning empirical observations:
1.",1 Introduction,0,[0]
There is a growing mentor gender gap.,1 Introduction,0,[0]
"There is a growing disparity between the proportions of female and male NLP researchers who achieve mentor status, with a higher proportion of male researchers becoming mentors, especially since the mid 2000s.",1 Introduction,0,[0]
2,1 Introduction,0,[0]
There is a significant time gap to mentor status across genders.,1 Introduction,0,[0]
Female NLP researchers must wait a considerable time longer to achieve mentor status than their male colleagues.,1 Introduction,0,[0]
3,1 Introduction,0,[0]
In-gender mentorship correlates with future success.,1 Introduction,0,[0]
"Female NLP researchers who take a male supervisor will have greater difficulty in becoming a mentor than if they take a female supervisor, on average.",1 Introduction,0,[0]
4,1 Introduction,0,[0]
Homophily is on the rise.,1 Introduction,0,[0]
"There is consistently increasing homophily in our field– the preference to establish in-gender mentormentee relationships.
",1 Introduction,0,[0]
"Following this analysis, we employ Avin et al. (2015)’s rigorously studied conditions for power inequality and the glass-ceiling effect for complex systems data structured like ours to show that these empirical observations indicate quite precisely the existence of a glass ceiling effect for the field of NLP.",1 Introduction,0,[0]
We scraped all meta-information available from the ACL Anthology2 for arguably the most central publication venues in NLP.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"This includes all papers from CoNLL, EACL, TACL, CL, ACL, EMNLP, COLING, ANLP, NAACL, *Sem/SemEval from 1965 to 2017: 19,552 papers in total.
",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"We carried out some normalisation of the author names scraped by lower-casing, normalising for order (first name then last name), removing middle initials and title abbreviations, and removing accents and punctuation, collapsing the extracted list of 18,437 author names to a list of 17,232 author names.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"Following this, we applied several gendered first-name lists to automatically annotate a large portion of the author names with gender.3 This resulted in 13,435 automatically annotated author names.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"Of the remaining 3797 unannotated names, we automatically label as ’unknown’ all author names with only an initial standing for the first name, effectively filtering out a further 565 author names.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"The remaining 3232 author names
2http://www.aclweb.org/anthology 3The lists are discussed in the appendix.
were annotated by the current authors by manually inspecting the results of Google Image queries for the full name.
",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"The resulting dataset spans 52 years and includes 17,232 authors, of which we labeled 10,382 as male, 5,227 as female and 1,623 whose gender we could not identify.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"In what remains of our study, we discard these latter authors.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"This leaves a total of 15,609 researchers.
",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
Power in academia.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"In our study, we need to account for mentor status–a type of seniority and power.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"As in many other fields, in NLP it is customary for mentors to take the last-authorship position of papers.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"Though there can be exceptions to this custom, the assumption of mentor lastauthorship is simple, and with this large dataset, we believe it provides a robust approximation of mentorship in the absence of other more precise indications like centralised supervision logs.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"This method was also adopted by Avin et al. (2015).
",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
We use the assumption of mentor lastauthorship to provide an empirical definition of a mentor in our dataset.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"We say that after t last-authored papers for some threshold t ∈ {1, . . .",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
", 10}, and excluding all sole-author papers,
a researcher is considered to hold mentor standing with seniority threshold t.
We model the interactions between researchers by creating the bi-populated (for female and male populations) mentor-mentee network.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
The network’s nodes therefore are researchers and there is an edge between two co-authors of a paper in our dataset if and only if one of the co-authors is the last author.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
"This leaves a mentor-mentee network with 14248 nodes, 25211 edges, and average degree 3.539.",2 Acquiring a gender-annotated mentor-mentee network,0,[0]
This network allows us to observe whether the current system of mentor-mentee relationships entails a glass ceiling effect in the modeled community.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
We now present the results of this analysis.,2 Acquiring a gender-annotated mentor-mentee network,0,[0]
We provide some basic empirical evidence which could be indicative of the presence of a glass ceiling effect in NLP.,3 Evidence of a rising gender gap,0,[0]
"In Section 4, we then prove that there is indeed a glass ceiling.",3 Evidence of a rising gender gap,0,[0]
A researcher who has achieved some seniority is generally eligible to become a mentor and supervise students.,3.1 Growing mentor standing disparity,0,[0]
"As such, the rise to becoming a mentor is a measurable criterion of success for a researcher in academia.",3.1 Growing mentor standing disparity,0,[0]
"Concretely, in some countries, the mentor role is reserved for permanent/tenured faculty (for example, in Denmark).",3.1 Growing mentor standing disparity,0,[0]
Therefore a barrier to mentor standing for females can lead to an important under-representation of women.,3.1 Growing mentor standing disparity,0,[0]
"This under-representation in turn may perpetuate itself through the lower availability of same-gender advisors for female students, which we show to be of central importance for rising NLP researchers (in Section 3.3).
",3.1 Growing mentor standing disparity,0,[0]
"For thresholds of “mentor seniority” t ∈ {2, . . .",3.1 Growing mentor standing disparity,0,[0]
", 10} we examined the proportion of mentors with respect to the pool of researchers of the same gender over time from 1966 to today.",3.1 Growing mentor standing disparity,0,[0]
Figure 1 shows the resulting time series.,3.1 Growing mentor standing disparity,0,[0]
"Across all thresholds, we observe that the proportion of male supervisors with respect to the total number of male researchers is increasing faster than the proportion of female supervisors within the general pool of female researchers.",3.1 Growing mentor standing disparity,0,[0]
"In fact, the discrepancy between these two proportions seems to slowly close until the early-to-mid 2000s after which it steadily increases again.",3.1 Growing mentor standing disparity,0,[0]
"And in almost all cases this difference in proportions develops into a statistically significant difference (with a 1-sided z-test for proportions, and p-value 0.05).",3.1 Growing mentor standing disparity,0,[0]
This is despite there being no corresponding development in mentor-mentee proportions as shown in Figure 2.,3.1 Growing mentor standing disparity,0,[0]
"We further investigate the subset of female researchers who achieved mentor standing, and compare their difficulty in doing so with that of the respective pool of male researchers.",3.2 Time to seniority gap,0,[0]
One measurable factor from our dataset is time.,3.2 Time to seniority gap,0,[0]
Isolating a substantially larger delay to achieving mentor standing for female researchers is one way to use our dataset to measure the difficulty in transitioning female researchers from mentee to mentor standing.,3.2 Time to seniority gap,0,[0]
We consider the average time it takes to achieve mentor standing between the two populations.,3.2 Time to seniority gap,0,[0]
"For consecutive periods of two years, we compute the average number of years for researchers to achieve mentor standing at threshold t ∈ {2, . . .",3.2 Time to seniority gap,0,[0]
", 10}.",3.2 Time to seniority gap,0,[0]
We provide a visualisation of the results in Figure 3.,3.2 Time to seniority gap,0,[0]
We do a two sample t-test to expose the statistical significance in the non-equality of the respective means.,3.2 Time to seniority gap,0,[0]
"We observe that across all thresholds for mentor standing, female researchers are substantially more delayed than male researchers in becoming mentors.",3.2 Time to seniority gap,0,[0]
"For the most recent numbers, the result is most significant where there is the most data, at seniority t = 3, with p-level 0.04.",3.2 Time to seniority gap,0,[0]
However we note the general whitening of the plots (indicating statistical significance) after the mid-2000s.,3.2 Time to seniority gap,0,[0]
"The availability of female mentors been has shown to correlate with mentees’ future success–in particular, females in Chemistry who are mentored by
female supervisors are considerably more likely to become faculty themselves (Gaule and Piacentini, 2018).",3.3 The effects of in-gender supervision,0,[0]
"In Table 1 we observe a similar trend
for in-gender mentorship.",3.3 The effects of in-gender supervision,0,[0]
"In particular, female researchers who have female mentors are much more likely to become mentors themselves.",3.3 The effects of in-gender supervision,0,[0]
"This is a particular problem if, as Sections 3.1 and 3.2 show, the proportion of female NLP mentors is not increasing at the same rate as that of male NLP researchers, possibly due in part to the added delay in achieving mentor status for women.",3.3 The effects of in-gender supervision,0,[0]
"Indeed this delay in access, perpetuated due to the lack of in-gender supervision, can be the result of a glass ceiling in NLP.",3.3 The effects of in-gender supervision,0,[0]
In the next section we investigate the likelihood of such a glass ceiling.,3.3 The effects of in-gender supervision,0,[0]
"In order to understand better how the population of female researchers in NLP can be increasing, but the growth level of seniority/mentor standing still falls significantly below that of the male population and that this gap is widening, we turn to an investigation of power inequality and the glass ceiling effect.
",4 The glass ceiling effect in NLP,0,[0]
"First three key observations can be made of the mentor-mentee network introduced in Section 2 vis-à-vis three well-accepted mechanisms of observed human behavior.
",4 The glass ceiling effect in NLP,0,[0]
(O1) Minority-majority partition.,4 The glass ceiling effect in NLP,0,[0]
Figure 2 shows the resulting proportion of male and female researchers in NLP through the 52 years.,4 The glass ceiling effect in NLP,0,[0]
"Our network displays a minority-majority partition: the proportion of females has hovered around 33.5% for the past decade now.
",4 The glass ceiling effect in NLP,0,[0]
(O2) Homophily is the is the tendency of individuals to associate with people similar to them.,4 The glass ceiling effect in NLP,0,[0]
Easley and Kleinberg (2010) provide the following test for homophily.,4 The glass ceiling effect in NLP,0,[0]
"Given the proportions of male and female ended edges in the network, we should be able to calculate the approximate proportion of mixed edges (the probability that we select a mixed-gendered edge at random).",4 The glass ceiling effect in NLP,0,[0]
"If the true fraction is significantly below the expected amount, the network is exhibiting homophily.",4 The glass ceiling effect in NLP,0,[0]
Figure 4 shows that homophily is a consistently worsening problem in the NLP community.,4 The glass ceiling effect in NLP,0,[0]
All numbers are significant with p-value virtually 0 (1-sided z-test for proportions).,4 The glass ceiling effect in NLP,0,[0]
"Note that the plot includes error bars, which are so small they are not visible.
",4 The glass ceiling effect in NLP,0,[0]
(O3),4 The glass ceiling effect in NLP,0,[0]
"The “rich-get-richer” feedback mechanism describes and explains the process of wealth concentration, by which the future distribution of wealth is predictable from empirical data
based on the current wealth distribution.",4 The glass ceiling effect in NLP,0,[0]
"In our network, the degree of a node captures its level of social wealth: people may try to connect more often to people who already have many connections, either in order to profit from their social wealth or because they are more visible in the network.",4 The glass ceiling effect in NLP,0,[0]
"In our NLP mentormentee network, the average degree for male researcher nodes is 3.356, while for females it is 3.186.",4 The glass ceiling effect in NLP,0,[0]
"Hence our mentor-mentee network exhibits a “rich-get-richer” mechanism in favour of male researchers.
",4 The glass ceiling effect in NLP,0,[0]
The biased preferential attachment model.,4 The glass ceiling effect in NLP,0,[0]
"Avin et al. (2015) extend Barabási and Albert (1999)’s preferential attachment model that was originally based on the “rich-get-richer” feedback mechanism to a biased preferential attachment model of mentor-mentee dynamics, G(n, f, p), where there further is (1) a minority-majority partition (the proportion of female nodes is less than half, f < 12 ) and (2) homophily.",4 The glass ceiling effect in NLP,0,[0]
"The model works as follows, instantiated to our context.",4 The glass ceiling effect in NLP,0,[0]
"Over time, a sequence of bi-populated mentor-mentee networks is constructed, Gt = (Vt, Et), like the one described in Section 2.",4 The glass ceiling effect in NLP,0,[0]
"Vt = Ft ∪Mt is the set of Gt’s nodes, and Et its edges, where Ft(Mt) is the set of female (male) nodes.",4 The glass ceiling effect in NLP,0,[0]
G0 is the empty graph.,4 The glass ceiling effect in NLP,0,[0]
At each time t > 0,4 The glass ceiling effect in NLP,0,[0]
a mentee enters the network.,4 The glass ceiling effect in NLP,0,[0]
The mentee is a female with probability f and a male with probabilitym = 1−f .,4 The glass ceiling effect in NLP,0,[0]
"Assuming a rich-get-richer mechanism, the mentee chooses a potential mentor according to that mentor’s importance in the network: with probability δt(u)∑
v∈Vt δt(v)
where δt(v) is the degree of v ∈ Vt.",4 The glass ceiling effect in NLP,0,[0]
"If this supervision is in-gender, then a relation (edge) is established.",4 The glass ceiling effect in NLP,0,[0]
"However if genders differ, then the relation (edge) is established according to the probability of homophily (p); otherwise (with proba-
bility (1 − p))",4 The glass ceiling effect in NLP,0,[0]
it is rejected and the mentee must restart the process of finding a mentor.,4 The glass ceiling effect in NLP,0,[0]
"Once an advisor for the mentee has been found, t increments to the next time step.
",4 The glass ceiling effect in NLP,0,[0]
"We now introduce definitions and the main theorem established by (Avin et al., 2015) for conditions of the existence of power inequality and a glass ceiling effect in bi-populated networks.",4 The glass ceiling effect in NLP,0,[0]
Then we empirically check for these conditions in our NLP mentor-mentee network for the main result.,4 The glass ceiling effect in NLP,0,[0]
Power inequality definition.,4 The glass ceiling effect in NLP,0,[0]
"The sequence of mentor-mentee networks Gt is said to exhibit a power inequality effect for females if the average power of a female node is strictly bounded by the power of a male node: i.e., limt→∞ 1 |Ft| ∑ v∈Ft δt(v)
1 |Mt| ∑ v∈Mt δt(v) < 1.
",4 The glass ceiling effect in NLP,0,[0]
Tail and moment glass ceiling definitions.,4 The glass ceiling effect in NLP,0,[0]
Let topk(Ft) (topk(Mt)),4 The glass ceiling effect in NLP,0,[0]
denote the number of female (male) nodes that have degree of at least k in Gt– this is the group of scholars whose wealth in relations in the network is at level at least k; this wealth of relations is a form of power.,4 The glass ceiling effect in NLP,0,[0]
The glass ceiling effect for the minority of females describes a process by which the proportion of access to this wealth of relations is limited for females but not for males.,4 The glass ceiling effect in NLP,0,[0]
"Formally, the sequence Gt is said to exhibit a tail glass ceiling effect for the female nodes (the minority) if there exists an increasing sequence kt such that limt→∞ topkt(Mt) = ∞ and limt→∞
topkt (Ft) topkt (Mt)
= 0.",4 The glass ceiling effect in NLP,0,[0]
"Gt exhibits a moment glass ceiling g for the female nodes, if g = limt→∞ 1 |Ft| ∑ v∈Ft δt(v)2
1 |Mt| ∑ v∈Mt δt(v)2 .",4 The glass ceiling effect in NLP,0,[0]
"And if g = 0, Gt has a
strong glass ceiling effect.
",4 The glass ceiling effect in NLP,0,[0]
The main result: Power inequality and glass ceiling.,4 The glass ceiling effect in NLP,0,[0]
"Avin et al. (2015) proved that if 0 < f < 12 and 0 < p < 1, then for G(n, f, p) produced by the biased preferential attachment model, G(n, f, p) exhibits both power inequality and a tail and strong glass ceiling effects.",4 The glass ceiling effect in NLP,0,[0]
"In observations (O2) and (O3), we identified the conditions f = 0.335 < 0.5 and the existence of homophily (i.e., 0 < p < 1) in our NLP mentormentee network.",4 The glass ceiling effect in NLP,0,[0]
We have therefore shown there to exist power inequality and a glass ceiling in NLP.,4 The glass ceiling effect in NLP,0,[0]
"Given our study of the mentee-mentor network for NLP, we have shown that there is a glass ceiling for female researchers in NLP that has taken a hold of the field since the mid-2000s.",5 Concluding remarks,0,[0]
"In this paper, we provide empirical evidence based on a rigourously studied mathematical model for bi-populated networks, that a glass ceiling within the field of NLP has developed since the mid",abstractText,0,[0]
The glass ceiling in NLP,title,0,[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2379–2389 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,0,[0]
"Argumentation theory has established a number of major argument models focusing on different aspects, such as the roles of an argument’s units (Toulmin, 1958), the inference scheme of an argument (Walton et al., 2008), or the support and attack relations between arguments (Freeman, 2011).",1 Introduction,0,[0]
"The common ground of these models is that they conceptualize an argument as a conclusion (in terms of a claim) inferred from a set of pro and con premises (reasons), which in turn may be the conclusions of other arguments.",1 Introduction,0,[0]
"For the overall argumentation of a monological argumentative text such as the one in Figure 1(a), this results in an implicit hierarchical structure with the text’s main claim at the lowest depth.",1 Introduction,0,[0]
"In addition, the text has an explicit linguistic structure that can be seen as a regulated sequence of speech acts (van Eemeren and Grootendorst, 2004).
",1 Introduction,0,[0]
"Figure 1(b) illustrates the interplay of the two types of overall structure in form of a tree-like graph.
",1 Introduction,0,[0]
"Natural language processing research has largely adopted the outlined hierarchical models for mining arguments from text (Stab and Gurevych, 2014; Habernal and Gurevych, 2015; Peldszus and Stede, 2016).",1 Introduction,0,[0]
"However, the adequacy of the resulting overall structure for downstream analysis tasks of computational argumentation has rarely been evaluated (see Section 2 for details).",1 Introduction,0,[0]
"In fact, a computational approach that can capture patterns in hierarchical overall argumentation is missing so far.",1 Introduction,0,[0]
"Even more, our previous work indicates that a sequential model of overall structure is preferable for analysis tasks such as stance classification or quality assessment (Wachsmuth and Stein, 2017).
",1 Introduction,0,[0]
"In this paper, we ask and investigate what model of (monological) overall argumentation is important to tackle argumentation-related analysis tasks.",1 Introduction,0,[0]
"To this end, we consider three corpora with fully
2379
annotated argument structure (Section 3).",1 Introduction,0,[0]
"Each corpus allows studying one text classification task, two of which we hypothesize to benefit from modeling argumentation (myside bias, stance), the third not (genre).",1 Introduction,0,[0]
An empirical analysis of the corpora reveals class-specific patterns of how people argue (Section 4).,1 Introduction,0,[0]
"In order to combine the explicit sequential and the implicit hierarchical structure of an argumentative text for the first time, we then adapt the approach of route kernels (Aiolli et al., 2009), modeling overall argumentation in form of a positional tree (Section 5).
",1 Introduction,0,[0]
"On this basis, we design an experiment to evaluate the impact of the different types of argumentative structure (Section 6).",1 Introduction,0,[0]
"In particular, we decompose our approach into four complementary modeling steps, both for a general model of overall argumentation and for the specific models of the given corpora.",1 Introduction,0,[0]
"Using the structure annotated in the corpora, we systematically compare the effectiveness of all eight resulting models and two standard baselines in the three classification tasks.
",1 Introduction,0,[0]
Our results provide strong evidence that both sequential and hierarchical structure are important.,1 Introduction,0,[0]
"As indicated by related work, sequential structure nearly competes with hierarchical structure, at least based on the specific argument models.",1 Introduction,0,[0]
"Even more impressively, modeling hierarchical structure practically solves the task of identifying argumentation with myside bias, achieving an outstanding accuracy of 97.1%.",1 Introduction,0,[0]
"For stance classification, the combination captured by positional trees works best.",1 Introduction,0,[0]
"In contrast, all types of structure fail in distinguishing genres, suggesting that they indeed capture properties of argumentation.",1 Introduction,0,[0]
"We conclude that the impact of modeling overall structure on downstream analysis tasks is high, while the required type may vary.
",1 Introduction,0,[0]
"Contributions To summarize, the main contributions of this paper are the following:
1.",1 Introduction,0,[0]
"Empirical insights into how people structure argumentative texts in overall terms.
",1 Introduction,0,[0]
2,1 Introduction,0,[0]
"The first approach to model and analyze the sequential and hierarchical overall structure of argumentative texts in combination.
3.",1 Introduction,0,[0]
Evidence that modeling overall structure impacts argumentation-related analysis tasks.,1 Introduction,0,[0]
"The study of overall argumentation traces back to Aristotle (2007) who outlined the impact of the
sequential arrangement of the different parts of a speech.",2 Related Work,0,[0]
"Conceptually, theory agrees that a monological argumentative text has an implicit tree-like hierarchical structure: Toulmin (1958) defines an argument as a claim supported by data that is reasoned by a warrant, which in turn may have a backing.",2 Related Work,0,[0]
"In addition, a rebuttal may be given showing exceptions to the claim.",2 Related Work,0,[0]
The role of support and attack relations is investigated by Freeman (2011) who models dialectical arguments that discuss both a proponent’s and an opponent’s view on the main claim argued for.,2 Related Work,0,[0]
"Walton et al. (2008) put the focus on the inference scheme that describes how an argument’s conclusion follows from its premises, which may themselves be conclusions of arguments.
",2 Related Work,0,[0]
"In natural language processing, argumentation research deals with the mining of argument units and their relations from text (Mochales and Moens, 2011).",2 Related Work,0,[0]
Several corpora with annotated argument structure have been published in the last years.,2 Related Work,0,[0]
"Many of the corpora adapt the hierarchical models from theory (Reed and Rowe, 2004; Habernal and Gurevych, 2015; Peldszus and Stede, 2016) or propose comparable models (Stab and Gurevych, 2014).",2 Related Work,0,[0]
"Since we target monological overall argumentation, we use those that capture the complete structure of texts, as detailed in Section 3.",2 Related Work,0,[0]
"Corpora focusing on dialogical argumentation (Walker et al., 2012), topic-related arguments (Rinott et al., 2015), or sequential structure (Wachsmuth et al., 2014b; Al Khatib et al., 2016) are out of scope.
",2 Related Work,0,[0]
"We do not mine the structure of argumentative texts, but we exploit the previously mined structure to tackle downstream tasks of computational argumentation, namely, to classify the myside bias and stance of texts.",2 Related Work,0,[0]
"For myside bias, Stab and Gurevych (2016) use features derived from discourse structure, whereas Faulkner (2014) and Sobhani et al. (2015) model arguments to classify stance.",2 Related Work,0,[0]
"Ong et al. (2014) and we ourselves (Wachsmuth et al., 2016) do similar to assess the quality of persuasive essays, and Beigman Klebanov et al. (2016) examine how an essay’s content and structure influence quality.",2 Related Work,0,[0]
"Other works predict the outcome of legal cases based on the applied types of reasoning (Brüninghaus and Ashley, 2003) or analyze inference schemes for given arguments (Feng and Hirst, 2011).",2 Related Work,0,[0]
"In contrast to the local structure of single arguments employed by all these approaches, we study the impact of the global overall structure of complete monological argumentative texts.
",2 Related Work,0,[0]
"In (Wachsmuth et al., 2017), we point out that the argumentation quality of a text is affected by interactions of its content at different levels of granularity, from single argument units over arguments to overall argumentation.",2 Related Work,0,[0]
"Stede (2016) explores how different depths of overall argumentation can be identified, observing differences across genres.",2 Related Work,0,[0]
"Unlike in our experiments, however, the genres considered there reflect diverging types of argumentation.",2 Related Work,0,[0]
"Related to argumentation, Feng et al. (2014) build upon rhetorical structure theory (Mann and Thompson, 1988) to assess the coherence of texts, while Persing et al. (2010) score the organization of persuasive essays based on sequences of sentence and paragraph functions.
",2 Related Work,0,[0]
"We introduced the first explicit computational model of overall argumentation in (Wachsmuth et al., 2014a).",2 Related Work,0,[0]
"There, we compared the flow of local sentiment in a review to a set of learned flow patterns in order to classify global sentiment.",2 Related Work,0,[0]
"Recently, we generalized the model in order to make flows applicable to any type of information relevant for argumentation-related analysis tasks (Wachsmuth and Stein, 2017).",2 Related Work,0,[0]
"However, flows capture only sequential structure, whereas here we also model the hierarchical structure of overall argumentation.",2 Related Work,0,[0]
"To this end, we make use of kernel methods.
",2 Related Work,0,[0]
"Kernel methods are a popular approach for learning on structured data, with several applications in natural language processing (Moschitti, 2006b) including argument mining (Rooney et al., 2012).",2 Related Work,0,[0]
They employ a similarity function defined between any two input objects that are represented in a taskspecific implicit feature space.,2 Related Work,0,[0]
"The evaluation of such a kernel function relies on the common features of the input objects (Cristianini and ShaweTaylor, 2000).",2 Related Work,0,[0]
"The kernel function encodes knowledge of the task in the form of these features.
",2 Related Work,0,[0]
Several kernel functions have been defined for structured data.,2 Related Work,0,[0]
"To assess the impact of sequential argumentation, we refer to the function of Mooney and Bunescu (2006), which computes common subsequences of two input sequences.",2 Related Work,0,[0]
"For trees, most existing approaches count common subtrees of a certain type (Collins and Duffy, 2001; Moschitti, 2006a; Kimura and Kashima, 2012), but they do not take the ordering of the nodes in the subtrees into account.",2 Related Work,0,[0]
"In contrast, Aiolli et al. (2009) developed a kernel that combines the content of substructures with the relative positions inside trees, called the route kernel.",2 Related Work,0,[0]
"Similarly, the tree kernel of Daumé III
and Marcu (2004) includes positional information for document compression.",2 Related Work,0,[0]
"For overall argumentation, we decided to use the route kernel in Section 5, as it makes the modeling of the sequential positions of an argument unit in a text straightforward.",2 Related Work,0,[0]
This allows us to capture both the sequential and the hierarchical structure at the same time.,2 Related Work,0,[0]
"To our knowledge, no work has done this before.1
Neural networks denote an alternative for learning on structured data.",2 Related Work,0,[0]
"They become particularly effective when few prior knowledge about what is important to address a task at hand is available, because they can learn any feature representation in principle (Goodfellow et al., 2016).",2 Related Work,0,[0]
"Due to this flexibility, however, large amounts of data are required for training an effective model, making neural networks inadequate for the small datasets that allow studying overall argumentation.",2 Related Work,0,[0]
We seek to study the impact of modeling overall argumentation on downstream tasks without the noise from argument mining errors.,3 Tasks and Datasets,0,[0]
"To this end, we rely on three ground-truth argument corpora.",3 Tasks and Datasets,0,[0]
"Each corpus is suitable for evaluating one text classification task and comes with a specific model of overall argumentation, as detailed in the following.
",3 Tasks and Datasets,0,[0]
Myside Bias on AAE-v2,3 Tasks and Datasets,0,[0]
The Argument Annotated Essays corpus was originally been presented by Stab and Gurevych (2014).,3 Tasks and Datasets,0,[0]
"We use version 2 of the corpus (available on the website of the authors), which consists of 402 persuasive student essays.",3 Tasks and Datasets,0,[0]
"In each essay, all main claims, claims, and premises are annotated as such.",3 Tasks and Datasets,0,[0]
"Each claim has a pro or con stance towards each instance of the main claim, whereas each premise supports or attacks a claim or another premise.",3 Tasks and Datasets,0,[0]
"Thereby, argumentation is modeled as one tree structure for each major claim.
",3 Tasks and Datasets,0,[0]
"Stab and Gurevych (2016) added a myside bias class to each essay, reflecting whether its argumentation is one-sided considering only arguments for the own stance (251 cases) or not (151 cases).
",3 Tasks and Datasets,0,[0]
Stance on Arg-Microtexts The Arg-Microtexts corpus of Peldszus and Stede (2016) contains 112 short argumentative texts.,3 Tasks and Datasets,0,[0]
They cover 18 different controversial topics and are annotated according to Freeman (2011): Each argument unit takes the role of the proponent or opponent of a main claim.,3 Tasks and Datasets,0,[0]
"What
1While extensions of the route kernel idea have been published later on (Aiolli et al., 2011, 2015), we resort to the original version in this paper for simplicity.
",3 Tasks and Datasets,0,[0]
"the main claim is follows from a tree-like overall structure emerging from four types of relations: normal or example support from one unit to another, a rebuttal of units by other units, and undercutters where a relation is attacked by another unit.
",3 Tasks and Datasets,0,[0]
"For 88 texts, the stance towards a specified topic is labeled as pro (46) or con (42).",3 Tasks and Datasets,0,[0]
"We use these labels for classification, but we do not access the topic.",3 Tasks and Datasets,0,[0]
"This way, stance needs to be identified only based on a text itself — a very challenging task.2
Genre on Web Discourse Finally, we consider the Argument Annotated User-Generated Web Discourse corpus of Habernal and Gurevych (2015).",3 Tasks and Datasets,0,[0]
"There, 340 texts are annotated according to a modified version of the specific model of Toulmin (1958) where claims are supported by premises or attacked by rebuttals.",3 Tasks and Datasets,0,[0]
Rebuttals in turn may be attacked by refutations.,3 Tasks and Datasets,0,[0]
"Besides, emotional units not participating in the actual arguments are marked as pathos.",3 Tasks and Datasets,0,[0]
"The support and attack relations build up the overall argumentation of a text.
",3 Tasks and Datasets,0,[0]
"The corpus composes argumentative texts of four genres, namely, 5 articles, 216 comments to articles, 46 blog posts, and 73 forum posts.",3 Tasks and Datasets,0,[0]
The genre is specified in form of a label for each text.,3 Tasks and Datasets,0,[0]
"Due to the low number, we ignore the articles below.
",3 Tasks and Datasets,0,[0]
"To give an idea of the sequential and hierarchical overall structure in each corpus, Table 1 presents statistics of the argument units, the arguments (in terms of relations between two or more units), and the depth of the resulting argumentation.
",3 Tasks and Datasets,0,[0]
"While the size of the given corpora and the variety of tasks are limited, the only other available corpus with fully annotated argument structure that we are aware of is AraucariaDB",3 Tasks and Datasets,0,[0]
"(Reed and Rowe,
2We do not include the topic, in order not to conflate the impact of modeling argumentation with the influence of the topic.",3 Tasks and Datasets,0,[0]
"The corpus is too small to analyze topic differences.
2004).",3 Tasks and Datasets,0,[0]
No downstream task can be tackled on AraucariaDB,3 Tasks and Datasets,0,[0]
"besides inference scheme classification (Feng and Hirst, 2011).",3 Tasks and Datasets,0,[0]
"As all schemes compose a conclusion and a set of premises (without more specific roles), analyzing overall structure hardly makes sense, which is why we omit the corpus.",3 Tasks and Datasets,0,[0]
"Before we approach overall argumentation computationally, this section analyzes the three given corpora empirically to provide insights into how people argue in overall terms.",4 Insights into Overall Argumentation,0,[0]
"For this, we unify the specific corpus models of overall argumentation outlined above in one general model.",4 Insights into Overall Argumentation,0,[0]
"The texts in all corpora are segmented into argument units, partly with non-argumentative spans in between that we ignore here for lack of relevance.",4.1 A Unified View of Overall Argumentation,0,[0]
"To capture the sequential ordering of the segmentation, we assign a global index to each unit.
",4.1 A Unified View of Overall Argumentation,0,[0]
"As described in Section 3, the specific models of all three corpora in the end consider an argument as a composition of one unit serving as the conclusion with one or more units that support or attack the conclusion (the premises).",4.1 A Unified View of Overall Argumentation,0,[0]
This composition is defined through multiple relations from one premise to one conclusion each.,4.1 A Unified View of Overall Argumentation,0,[0]
"There is one exception, namely, the undercutter relations in the Arg-Microtexts corpus have a relation as their target.",4.1 A Unified View of Overall Argumentation,0,[0]
"To obtain a unified form in the general model, we modify the undercutters such that they target the premise of the undercutted relation.
",4.1 A Unified View of Overall Argumentation,0,[0]
"In all corpora, a premise may be the conclusion of another argument, while no argument unit serves as a premise in multiple arguments.",4.1 A Unified View of Overall Argumentation,0,[0]
This leads to a tree structure for each main claim of the associated text.,4.1 A Unified View of Overall Argumentation,0,[0]
A main claim corresponds to a unit that is not a premise.,4.1 A Unified View of Overall Argumentation,0,[0]
"In AAE-v2 and in Web Discourse, more than one such unit may exist per text.
",4.1 A Unified View of Overall Argumentation,0,[0]
"Depending on the corpus, the distinction of support and attack is encoded through a specified relation type, a unit’s stance, or both.",4.1 A Unified View of Overall Argumentation,0,[0]
We unify these alternatives by modeling the stance of each unit towards its parent in the associated tree.,4.1 A Unified View of Overall Argumentation,0,[0]
This stance can be derived in all corpora.3,4.1 A Unified View of Overall Argumentation,0,[0]
"All other unit and relation types from the specific models are ignored, since there is no clear mapping between them.
",4.1 A Unified View of Overall Argumentation,0,[0]
"3Alternatively, the stance towards the main claim could be modeled.",4.1 A Unified View of Overall Argumentation,0,[0]
"We decided against this alternative to avoid possibly wrong reinterpretations, e.g., it is unclear whether a unit that attacks its parent always supports a unit attacked by the parent.
",4.1 A Unified View of Overall Argumentation,0,[0]
"All texts
Texts with myside bias
Texts without myside bias
All texts
Texts with pro stance
All texts
Comments
Blog posts
(a) AAE-v2 (b) Arg-Microtexts (c) Web Discourse
0 1 2 3
0 1 2 3
0 1 2 3
0 1 2 3
0 1
0 1
0 1
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
1 2 3 4 5 6 7
1 2 3 4 5 6
1 2 3 4 5 6 7 8
1 2 3 4 5 6 7
1 2 3 4 5 6 7 8720 21 22
20 21 22
General Model As a result, we model the overall argumentation of an argumentative text as a forest of trees.",4.1 A Unified View of Overall Argumentation,0,[0]
Each node in a tree corresponds to an argument unit.,4.1 A Unified View of Overall Argumentation,0,[0]
It has an assigned stance (pro or con) as well as a global index that defines its position in the text.,4.1 A Unified View of Overall Argumentation,0,[0]
Each edge defines a relation from a premise (the child node) to a conclusion (the parent node).,4.1 A Unified View of Overall Argumentation,0,[0]
"Each main claim defines the root of a tree.
",4.1 A Unified View of Overall Argumentation,0,[0]
Figure 1(b) has already illustrated an instance of the general model.,4.1 A Unified View of Overall Argumentation,0,[0]
The general model is slightly less expressive than the specific models.,4.1 A Unified View of Overall Argumentation,0,[0]
We evaluate in Section 6 to what extent this reduces its use for tackling argumentation-related analysis tasks.,4.1 A Unified View of Overall Argumentation,0,[0]
"The advantage of the general model is that it allows a comparison of patterns of overall argumentation across corpora, as we do in the following.4",4.1 A Unified View of Overall Argumentation,0,[0]
"Based on the general model, we empirically analyze class-specific patterns of overall argumentation on the three corpora.",4.2 Visualization of Argumentation Patterns,0,[0]
"To this end, we compute one “average graph” for all texts in each complete corpus and one such graph for all texts with a particular class (e.g., for all “no myside bias” texts in case of AAE-v2).",4.2 Visualization of Argumentation Patterns,0,[0]
"In an average graph, each node is labeled with the relative frequency of the associated combination of position and depth in all texts (edges accordingly).",4.2 Visualization of Argumentation Patterns,0,[0]
"We align positions
4Besides, although not in the focus here, we also assume stance to be easier to detect in practice than fine-grained roles.
of different texts based on their start node, due to our observation that the first argument unit overproportionally often represents the main claim.5",4.2 Visualization of Argumentation Patterns,0,[0]
"In addition the relative frequency, we determine the proportion of con to pro stance for each node.
",4.2 Visualization of Argumentation Patterns,0,[0]
"As we aim to provide intuitive insights into how people argue in overall terms, we discuss the graphs in an informal visual way instead of listing exact numbers.6 In the visualizations in Figure 2, brightness captures (inverse) frequency, so darker nodes represent more frequent argument units.",4.2 Visualization of Argumentation Patterns,0,[0]
The diameter of the inner light-red part of each node reflects its proportion of con stance.,4.2 Visualization of Argumentation Patterns,0,[0]
"Nodes with a relative frequency below 0.3% and/or an absolute frequency below 3 are pruned, along with all their associated edges.
",4.2 Visualization of Argumentation Patterns,0,[0]
"AAE-v2 Figure 2(a) stresses that most students state the main claim (depth 0, position 1) in a persuasive essay first.",4.2 Visualization of Argumentation Patterns,0,[0]
"When the first argument unit is a premise of the main claim instead, it often attacks the main claim, as the large light-red proportion of the node at depth 1 and position 1 conveys.",4.2 Visualization of Argumentation Patterns,0,[0]
"While, on average, texts with myside bias do not differ in length from those without, the latter show more con stance, especially at depth 1.",4.2 Visualization of Argumentation Patterns,0,[0]
"Also, argumenta-
5We also considered using the main claim as the fix point, but the resulting graphs would be much wider than the longest argumentation, which may be misleading.
",4.2 Visualization of Argumentation Patterns,0,[0]
"6We provide files with the exact frequencies of all nodes and edges at: http://www.arguana.com/software.html
tion without myside bias shows more variance, as indicated, for instance, by the nodes at depth 0 and position 12 and 13 respectively.",4.2 Visualization of Argumentation Patterns,0,[0]
"In contrast, clear patterns in the sequential ordering of pro and con stance are not recognizable in AAE-v2.
Arg-Microtexts",4.2 Visualization of Argumentation Patterns,0,[0]
"According to the graphs in Figure 2(b), the position of the main claim varies in the microtexts.",4.2 Visualization of Argumentation Patterns,0,[0]
"While the proportion of con stance seems rather similar between pro and con texts, our visualization reveals that their overall structure is “mirror-inverted” to a limited extent: Most pro texts start with the main claim (depth 0, position 1), discuss con stance later (red proportions increase to the right), and deepen the argumentation in a topdown fashion (most edges from top left to bottom right).",4.2 Visualization of Argumentation Patterns,0,[0]
"Vice versa, con texts more often present the main claim later, attack it earlier, and seem to argue more bottom-up.",4.2 Visualization of Argumentation Patterns,0,[0]
"This suggests that both sequential and hierarchical structure play a role here.
",4.2 Visualization of Argumentation Patterns,0,[0]
Web Discourse,4.2 Visualization of Argumentation Patterns,0,[0]
"The web discourse texts, finally, comprise rather shallow argumentation across all genres.",4.2 Visualization of Argumentation Patterns,0,[0]
"Slight structural differences can be seen, especially, the comments appear a little shorter and richer of pro stance on average.",4.2 Visualization of Argumentation Patterns,0,[0]
"Besides, the blog posts have more con stance later.",4.2 Visualization of Argumentation Patterns,0,[0]
"Still, the darker and thus more frequent nodes are at similar positions in all graphs.",4.2 Visualization of Argumentation Patterns,0,[0]
"So, if at all, differences may be reflected in a sequential model of argumentation, which implicitly covers length.",4.2 Visualization of Argumentation Patterns,0,[0]
"In terms of the hierarchical structure of the frequent nodes, the graphs of all genres are rather indistinguishable.
",4.2 Visualization of Argumentation Patterns,0,[0]
"Altogether, the visualizations give first support for the impact of modeling overall argumentation.",4.2 Visualization of Argumentation Patterns,0,[0]
"In particular, we hypothesize that hierarchical overall structure is decisive for myside bias, whereas a combination of sequential and hierarchical structure helps to distinguish pro-stance from con-stance texts.",4.2 Visualization of Argumentation Patterns,0,[0]
"In contrast, we expect that the impact on classifying genres in the Web Discourse corpus is low.",4.2 Visualization of Argumentation Patterns,0,[0]
This section presents our kernel-based approaches for argumentation-related analysis tasks.,5 Modeling Overall Argumentation,0,[0]
They rely on a tree representation of overall argumentation.,5 Modeling Overall Argumentation,0,[0]
"We model the overall structure of an argumentative text in form of a positional tree T = (V,E) that, in principle, equals those exemplified in Figure 1 and analyzed above.",5.1 Representation of Overall Argumentation,0,[0]
"Each node v ∈ V represents an argument unit and each edge e = (v1, v2) ∈ E a
relation between two units.",5.1 Representation of Overall Argumentation,0,[0]
"Technically, we therefor map the forest of trees representing a text (see Section 4) to a single tree by adding a “virtual” root node v0 to V that is the parent of all tree roots.
",5.1 Representation of Overall Argumentation,0,[0]
"In analysis tasks, we seek to compare sequential and hierarchical structures irrespective of the actual texts and the size of the associated trees.",5.1 Representation of Overall Argumentation,0,[0]
"To this end, we represent labels and positions as follows:
",5.1 Representation of Overall Argumentation,0,[0]
Labels The tree kernel approaches in natural language processing discussed in Section 2 include text (usually words) in the leaf nodes.,5.1 Representation of Overall Argumentation,0,[0]
"In contrast, we label each node v ∈ V with the type of the associated argument unit only.",5.1 Representation of Overall Argumentation,0,[0]
"Thereby, we almost fully abstract from the content of texts, which benefits the identification of common structures.",5.1 Representation of Overall Argumentation,0,[0]
"In case of the general model, the only two labels are pro and con.",5.1 Representation of Overall Argumentation,0,[0]
"In case of the specific models, we combine the role of a unit with the type of the relation the unit is the source of (if any).",5.1 Representation of Overall Argumentation,0,[0]
"On Arg-Microtexts, for instance, this creates labels such as opponent-support or opponent-undercutter.
Positions As we adapt the route kernels of Aiolli et al. (2009) below, we follow their representation of sequential structure with one exception.",5.1 Representation of Overall Argumentation,0,[0]
"In particular, the authors assigned an index to each edge that numbers the child nodes of each node ascending from 1.",5.1 Representation of Overall Argumentation,0,[0]
"Thereby, they encoded the relative positions of sibling nodes to each other.",5.1 Representation of Overall Argumentation,0,[0]
"To capture the ordering of argument units in a text from left to right, we also model positions as indices of the edges in E. Unlike Aiolli et al. (2009), however, we use indices decreasing from -1 in the left direction of the parent node and ascending from 1 to the right (derived from the nodes’ global indices).",5.1 Representation of Overall Argumentation,0,[0]
"While such a simple relabeling allows us to reuse their algorithm for computing kernels, it makes a decisive difference, namely, it encodes the relative positions of child nodes to their parent.",5.1 Representation of Overall Argumentation,0,[0]
"This in turn implies the sequential structure of the whole tree.
",5.1 Representation of Overall Argumentation,0,[0]
"Figure 3(a) exemplifies the tree representation for the argument unit types of the general model, omitting the virtual root v0 for simplicity.",5.1 Representation of Overall Argumentation,0,[0]
"Analogously, the types of the specific models of the three considered corpora could be used.",5.1 Representation of Overall Argumentation,0,[0]
"Based on the tree representation, we now introduce four approaches for modeling overall argumentation.",5.2 Kernel-based Modeling Approaches,0,[0]
Figure 3(b) illustrates the kernel representations of each approach.,5.2 Kernel-based Modeling Approaches,0,[0]
"As discussed in Section 2,
the associated kernel function compares the representations of the trees T, T ′ of any two texts.
",5.2 Kernel-based Modeling Approaches,0,[0]
Label Frequencies (a1),5.2 Kernel-based Modeling Approaches,0,[0]
Our simplest model of overall argumentation does not encode structure at all.,5.2 Kernel-based Modeling Approaches,0,[0]
"Instead, it compares only the frequencies of each node label in T and T ′. We represent the model with a linear kernel, which in the end corresponds to a standard feature representation.
",5.2 Kernel-based Modeling Approaches,0,[0]
"Label Sequences (a2) To encode sequential overall structure, we refer to the kernel of Mooney and Bunescu (2006), representing the sequential ordering of node labels in a tree by all contiguous subsequences.",5.2 Kernel-based Modeling Approaches,0,[0]
"The similarity of two trees T and T ′ follows from the proportion of common subsequences, but longer subsequences are penalized by a decay factor.",5.2 Kernel-based Modeling Approaches,0,[0]
"This approach can be seen as an imitation of our flow model (Wachsmuth and Stein, 2017).7
Label Tree Paths (a3)",5.2 Kernel-based Modeling Approaches,0,[0]
"We capture hierarchical overall structure adapting the non-positional part of the route kernel of Aiolli et al. (2009), label paths.
",5.2 Kernel-based Modeling Approaches,0,[0]
7We use a sequence kernel instead of flows in order to obtain a uniform setting.,5.2 Kernel-based Modeling Approaches,0,[0]
"In Wachsmuth and Stein (2017), we also analyze flow abstractions (e.g., collapsing sequences of the same label).",5.2 Kernel-based Modeling Approaches,0,[0]
"Here, we resort only to the original sequence.
",5.2 Kernel-based Modeling Approaches,0,[0]
"A label path ξ(vi, vj) denotes the sequence of labels of the nodes in the shortest path between vi, vj in a tree (including vi, vj).",5.2 Kernel-based Modeling Approaches,0,[0]
"Following Aiolli et al. (2009), we consider only label paths starting at the root vi = v0, abbreviated here as ξ(vj).",5.2 Kernel-based Modeling Approaches,0,[0]
"Implicitly, other paths may still be considered through the use of polynomial kernels with degree d > 1.",5.2 Kernel-based Modeling Approaches,0,[0]
"As the authors, we compare any two paths with a function δ whose values is 1 when the paths are identical and 0 otherwise.",5.2 Kernel-based Modeling Approaches,0,[0]
"Given two trees T = (V,E) and T ′ =",5.2 Kernel-based Modeling Approaches,0,[0]
"(V ′, E′), we then define a normalized polynomial kernel Kξ(T, T ′) over all label paths as:(∑
v∈V ∑ v′∈V ′",5.2 Kernel-based Modeling Approaches,0,[0]
"δ(ξ(v), ξ(v′)) |V",5.2 Kernel-based Modeling Approaches,0,[0]
| · |V ′| )d Positional Tree Paths (a4),5.2 Kernel-based Modeling Approaches,0,[0]
"In addition to label paths, Aiolli et al. (2009) define a route π(vi, vj) as the sequence of edge indices on the shortest path between any two nodes vi, vj in a tree, i.e., the sequence of local positions.",5.2 Kernel-based Modeling Approaches,0,[0]
"As above, they restrict their view to routes starting at the root, which we denote as π(vj), and compare them using δ.",5.2 Kernel-based Modeling Approaches,0,[0]
"To combine positional information with label information, the authors build the product of a kernel based on the label paths and a kernel based on routes.",5.2 Kernel-based Modeling Approaches,0,[0]
"As a result, sequential and hierarchical overall structure are compared at the same time.",5.2 Kernel-based Modeling Approaches,0,[0]
"For overall argumentation, we define the resulting normalized polynomial product kernel Kξπ(T, T ′)",5.2 Kernel-based Modeling Approaches,0,[0]
"as:(∑
v∈V ∑ v′∈V ′",5.2 Kernel-based Modeling Approaches,0,[0]
"δ(ξ(v), ξ(v′)) · δ(π(v), π(v′)) (|V | · |V ′|)2 )",5.2 Kernel-based Modeling Approaches,0,[0]
d,5.2 Kernel-based Modeling Approaches,0,[0]
"Each approach, a1–a4, can be seen as representing one particular step of modeling overall argumentation; a4 combines the complementary steps of a2 and a3, both of which implicitly include a1.",5.2 Kernel-based Modeling Approaches,0,[0]
"Finally, we evaluate all four approaches to model overall argumentation from Section 5 on the three tasks associated to the corpora from Section 3.8",6 Evaluation,0,[0]
Our goal is to assess the theoretical impact of each introduced step of modeling overall argumentation as far as possible.,6.1 Experimental Set-up,0,[0]
"To this end, we conduct a systematic experiment where we use the ground-truth argument structure in each corpus for the associated downstream task based on the following set-up:
8The Java source code for reproducing the experiment results is available at: http://www.arguana.com/software.html
Approaches The modeling steps are reflected by the approaches a1–a4 from Section 5.",6.1 Experimental Set-up,0,[0]
"For each task, we measure the accuracy of all four approaches.",6.1 Experimental Set-up,0,[0]
"We do this once for our general model of overall argumentation from Section 4 and once for the specific model annotated in the respective corpus, in order to assess the loss of resorting to our always applicable general model.
",6.1 Experimental Set-up,0,[0]
Baselines,6.1 Experimental Set-up,0,[0]
"As a basic task-intrinsic measure, we compare a1–a4 to the majority baseline that always predicts the majority class in the given corpus.",6.1 Experimental Set-up,0,[0]
"In addition, we employ two standard feature types and combine them with a1–a4, in order to roughly assess the need for modeling argumentation:
b1 POS n-grams.",6.1 Experimental Set-up,0,[0]
The frequency of each partof-speech 1- to 3-gram found in ≥ 5% of all texts.,6.1 Experimental Set-up,0,[0]
"This style feature has been effective in argumentation-related analysis tasks (Persing and Ng, 2015; Wachsmuth et al., 2016).
",6.1 Experimental Set-up,0,[0]
b2,6.1 Experimental Set-up,0,[0]
Token n-grams.,6.1 Experimental Set-up,0,[0]
The frequency of each token 1- to 3-gram found in ≥ 5% of all texts.,6.1 Experimental Set-up,0,[0]
"This content feature is strong in many text analysis tasks (Joachims, 1998; Pang et al., 2002).
",6.1 Experimental Set-up,0,[0]
"From the tackled tasks, only myside bias has been approached on the given datasets in previous work.",6.1 Experimental Set-up,0,[0]
"While we mention the respective results for completeness below, a comparison is in fact unfair due to our resort to ground-truth argument structure.
",6.1 Experimental Set-up,0,[0]
"Experiments The evaluation of all approaches and baselines was done using the kernel-based machine learning platform KeLP (Filice et al., 2015), performing classification with the available implementation of LibSVM (Chang and Lin, 2011).",6.1 Experimental Set-up,0,[0]
"As
we target the theoretically possible impact of modeling overall argumentation, we tested a number of hyperparameter configurations.9 We performed 10-fold cross-validation on the complete corpora and repeated each experiment 10 times, with instance shuffling in between.",6.1 Experimental Set-up,0,[0]
"Then, we averaged the accuracy of each configuration over all folds and repetitions.",6.1 Experimental Set-up,0,[0]
"To prevent the classifiers from using knowledge about the class distributions, we used fairness during training, i.e., each class was given an equal weight (Filice et al., 2014).",6.1 Experimental Set-up,0,[0]
"Thus, the majority baseline is not a trivial competitor.",6.1 Experimental Set-up,0,[0]
Table 2 presents the best obtained results of each evaluated approach for each task/corpus combination.,6.2 Results,0,[0]
"To clarify the reliability of the differences between the results, the table includes the confidence level (starting at 95%) at which each approach is significantly better than all weaker approaches according to a two-tailed paired student’s t-test.10
Myside Bias on AAE-v2 The highest accuracy reported for classifying myside bias is 77.0 (Stab and Gurevych, 2016).",6.2 Results,0,[0]
"While the comparability is limited (see above), we see that label frequencies (a1) already achieve 83.4 and 85.7 for the general and specific model respectively, outperforming all baselines with 99.9% confidence.",6.2 Results,0,[0]
"Matching the insights from Section 4, the sole proportion of attacks thus seems a good predictor of myside bias.
",6.2 Results,0,[0]
"9SVM C parameter: 0.01, 0.1, 1, 10, 100; sequence kernel decay factor: 0, 0.5, 1; polynomial tree kernel degree: 1, 2, 3.
",6.2 Results,0,[0]
"10While selecting the best result a posteriori gives an upper bound on the true effectiveness, we do this to assess to what extent each approach captures task-relevant information.
",6.2 Results,0,[0]
"Label sequences (a2) further improve over a1, which underlines that also the sequential position of con stance and attack relations has an impact.",6.2 Results,0,[0]
a2 is particular strong under the specific model (94.7).,6.2 Results,0,[0]
"Unlike the general model, this model reflects some hierarchical information via the roles of argument units, such as premise.",6.2 Results,0,[0]
"a2 performs only slightly worse than the label tree paths (a3), indicating that an adequate sequential model can compete with a hierarchical model, as we hypothesized in previous work (Wachsmuth and Stein, 2017).
",6.2 Results,0,[0]
"Nevertheless, a3 turns out best on AAE-v2, most likely due to its capability to capture the depth at which con stance occurs.",6.2 Results,0,[0]
"Considering that no corpus annotation is perfect, the outstanding accuracy of 97.1 conveys an important finding: Modeling the tree structure of an argumentation basically solves the myside bias task without requiring other features.",6.2 Results,0,[0]
Neither the positional tree paths (a4) nor the combination with token n-grams (ba) can add to that.,6.2 Results,0,[0]
"Also, there is no difference between the general and the specific model, underlining that the unit roles in AAE-v2 are implicitly covered by the hierarchical structure in the general model.
",6.2 Results,0,[0]
Stance on Arg-Microtexts The accuracy results for the given challenging variant of stance classification (see Section 3) are much lower.,6.2 Results,0,[0]
"Under the general model, the label frequencies (49.7) do not even compete with the majority baseline (52.3).",6.2 Results,0,[0]
"Notable gains are achieved by the label sequences under the specific model (62.3), slightly beating the label tree paths (61.9).",6.2 Results,0,[0]
"Putting them together in the positional tree paths (a4) yields an accuracy of 66.7 and 67.8 respectively; more than the token n-grams (b2, 65.2).",6.2 Results,0,[0]
"Combining a4 and b2 in ba in turn results in the best observed accuracy value (71.0 on the specific model).
",6.2 Results,0,[0]
"We conclude that both sequential and hierarchical overall structure are important for the distinction of pro from con argumentation, supporting our hypothesis from Section 4.",6.2 Results,0,[0]
"They complement content-oriented approaches, such as b2.",6.2 Results,0,[0]
"Moreover, the fine-grained unit and relation types of the specific model annotated in Arg-Microtexts seem useful, consistently obtaining higher accuracy than the general model.",6.2 Results,0,[0]
"Notice, though, that due to the small size of the corpus, only few reported gains are statistically significant, as shown in Table 2.
",6.2 Results,0,[0]
"Genre on Web Discourse Although Section 4 has made minor structural differences in Web Discourse visible, Table 2 shows that a1–a4 all fail in
genre classification: None of them beats the majority baseline (64.5), suggesting that no decisive discriminative patterns are learned.",6.2 Results,0,[0]
Both POS and token n-grams (b1–b2) significantly outperform a1– a4 at 99.9% confidence.,6.2 Results,0,[0]
"While combining b2 with a2 (ba) minimally increases accuracy from 75.6 to 75.9, the results reveal that overall argumentation hardly impacts genre — as hypothesized.",6.2 Results,0,[0]
This paper provides answers to the question of how the overall structure of a monological argumentative text should be modeled in order to tackle downstream tasks of computational argumentation.,7 Conclusion,0,[0]
We have adopted the idea of including positional information in tree kernels in order to capture the explicit sequential and the implicit hierarchical overall structure of the text at the same time.,7 Conclusion,0,[0]
"In systematic experiments, we have demonstrated the strong impact of modeling overall argumentation.",7 Conclusion,0,[0]
"Most impressively, we have found that hierarchical structure decides about myside bias alone, while the combination of sequential and hierarchical structure has turned out beneficial for classifying stance.",7 Conclusion,0,[0]
"The missing impact on genre supports that the presented approaches actually capture argumentationrelated properties of a text.
",7 Conclusion,0,[0]
"So far, however, we have restricted our view to ground-truth argument structure, leaving the integration of computational argument mining approaches to future work.",7 Conclusion,0,[0]
"While the noise from mining errors might qualify some of our findings, we also expect that larger corpora will allow us to discover more reliable and discriminative patterns.",7 Conclusion,0,[0]
"After all, our results underline the general importance of modeling overall argumentation.",7 Conclusion,0,[0]
Several approaches have been proposed to model either the explicit sequential structure of an argumentative text or its implicit hierarchical structure.,abstractText,0,[0]
"So far, the adequacy of these models of overall argumentation remains unclear.",abstractText,0,[0]
This paper asks what type of structure is actually important to tackle downstream tasks in computational argumentation.,abstractText,0,[0]
We analyze patterns in the overall argumentation of texts from three corpora.,abstractText,0,[0]
"Then, we adapt the idea of positional tree kernels in order to capture sequential and hierarchical argumentative structure together for the first time.",abstractText,0,[0]
"In systematic experiments for three text classification tasks, we find strong evidence for the impact of both types of structure.",abstractText,0,[0]
Our results suggest that either of them is necessary while their combination may be beneficial.,abstractText,0,[0]
The Impact of Modeling Overall Argumentation with Tree Kernels,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4731–4736 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4731",text,0,[0]
"Recurrent neural networks (RNNs), in particular Long Short-Term Memory networks (LSTMs), have become a dominant tool in natural language processing.",1 Introduction,0,[0]
"While LSTMs appear to be a natural choice for modeling sequential data, recently a class of non-recurrent models (Gehring et al., 2017; Vaswani et al., 2017) have shown competitive performance on sequence modeling.",1 Introduction,0,[0]
Gehring et al. (2017) propose a fully convolutional sequence-tosequence model that achieves state-of-the-art performance in machine translation.,1 Introduction,0,[0]
Vaswani et al. (2017) introduce Transformer networks that do not use any convolution or recurrent connections while obtaining the best translation performance.,1 Introduction,0,[0]
These non-recurrent models are appealing due to their highly parallelizable computations on modern GPUs.,1 Introduction,0,[0]
But do they have the same ability to exploit hierarchical structures implicitly in comparison to RNNs?,1 Introduction,0,[0]
"In this work, we provide a first answer to this question.
",1 Introduction,0,[0]
"Our interest here is the ability of capturing hierarchical structure without being equipped with explicit structural representations (Bowman et al., 2015b; Tran et al., 2016; Linzen et al., 2016).",1 Introduction,0,[0]
We choose Transformer as a non-recurrent model to study in this paper.,1 Introduction,0,[0]
We refer to Transformer as Fully Attentional Network (FAN) to emphasize this characteristic.,1 Introduction,0,[0]
"Our motivation to favor FANs over convolutional neural networks (CNNs) is that FANs always have full access to the sequence history, making them more suited for modeling long distance dependencies than CNNs.",1 Introduction,0,[0]
"Additionally, FANs promise to be more interpretable than LSTMs by visualizing attention weights.
",1 Introduction,0,[0]
The rest of the paper is organized as follows: We first highlight the differences between the two architectures (§2) and introduce the two tasks (§3).,1 Introduction,0,[0]
Then we provide setup and results for each task (§4 and §5) and discuss our findings (§6).,1 Introduction,0,[0]
"Conceptually, FANs differ from LSTMs in the way they utilize the previous input to predict the next output.",2 FAN versus LSTM,0,[0]
Figure 1 depicts the main difference in terms of computation when each model is making predictions.,2 FAN versus LSTM,0,[0]
"At time step t, a FAN can access information from all previous time steps directly with O(1) computational operations.",2 FAN versus LSTM,0,[0]
FANs do so by employing a self-attention mechanism to compute the weighted average of all previous input representations.,2 FAN versus LSTM,0,[0]
"In contrast, LSTMs compress at each time step all previous information into a single vector recursively based on the current input and the previous compressed vector.",2 FAN versus LSTM,0,[0]
"By their definition, LSTMs require O(d) computational operations to access the information at time step t− d.
For the details of self-attention mechanics in FANs, we refer to the work of Vaswani et al. (2017).",2 FAN versus LSTM,0,[0]
"We now proceed to measure both models’ ability to
learn hierarchical structure with a set of controlled experiments.",2 FAN versus LSTM,0,[0]
"We choose two tasks to study in this work: (1) subject-verb agreement, and (2) logical inference.",3 Tasks,0,[0]
The first task was proposed by Linzen et al. (2016) to test the ability of recurrent neural networks to capture syntactic dependencies in natural language.,3 Tasks,0,[0]
The second task was introduced by Bowman et al. (2015b) to compare tree-based recursive neural networks against sequence-based recurrent networks with respect to their ability to exploit hierarchical structures to make accurate inferences.,3 Tasks,0,[0]
"The choice of tasks here is important to ensure that both models have to exploit hierarchical structural features (Jia and Liang, 2017).",3 Tasks,0,[0]
Linzen et al. (2016) propose the task of predicting number agreement between subject and verb in naturally occurring English sentences as a proxy for the ability of LSTMs to capture hierarchical structure in natural language.,4 Subject-Verb Agreement,0,[0]
"We use the dataset provided by Linzen et al. (2016) and follow their experimental protocol of training each model using either (a) a general language model, i.e., next word prediction objective, and (b) an explicit supervision objective, i.e., predicting the number of the verb given its sentence history.",4 Subject-Verb Agreement,0,[0]
Table 1 illustrates the training and testing conditions of the task.,4 Subject-Verb Agreement,0,[0]
Data:,4 Subject-Verb Agreement,0,[0]
"Following the original setting, we take 10% of the data for training, 1% for validation, and the rest for testing.",4 Subject-Verb Agreement,0,[0]
"The vocabulary consists of the 10k most frequent words, while the remaining words are replaced by their part-of-speech.
",4 Subject-Verb Agreement,0,[0]
"Hyperparameters: To allow for a fair comparison, we find the best configuration for each model by running a grid search over the following hyperparameters: number of layers in {2, 3, 4}, dropout rate in {0.2, 0.3, 0.5}, embedding size and number of hidden units in {128, 256, 512}, number of heads (for FAN) in {2, 4}, and learning rate in {0.00001, 0.0001, 0.001}.",4 Subject-Verb Agreement,0,[0]
"The weights of the word embeddings and output layer are shared (Inan et al., 2017; Press and Wolf, 2017).",4 Subject-Verb Agreement,0,[0]
"Models are optimized by Adam (Kingma and Ba, 2015).
",4 Subject-Verb Agreement,0,[0]
We first assess whether the LSTM and FAN models trained with respect to the language model objective assign higher probabilities to the correctly inflected verbs.,4 Subject-Verb Agreement,0,[0]
"As shown in Figures 2a and 2b, both models achieve high accuracies for this task, but LSTMs consistently outperform FANs.",4 Subject-Verb Agreement,0,[0]
"Moreover, LSTMs are clearly more robust than FANs with respect to task difficulty, measured both in terms of word distance and number of agreement attractors1 between subject and verb.",4 Subject-Verb Agreement,0,[0]
"Christiansen and Chater (2016); Cornish et al. (2017) have argued that human memory limitations give rise to important characteristics of natural language, including its hierarchical structure.",4 Subject-Verb Agreement,0,[0]
"Similarly, our experiments suggest that, by compressing the history into a single vector before making predictions, LSTMs are forced to better learn the input structure.",4 Subject-Verb Agreement,0,[0]
"On the other hand, despite having direct access to all words in their history, FANs are less capable of detecting the verb’s subject.",4 Subject-Verb Agreement,0,[0]
"We note that the validation perplexities of the LSTM and FAN are 67.06 and 69.14, respectively.
",4 Subject-Verb Agreement,0,[0]
"Secondly, we evaluate FAN and LSTM models explicitly trained to predict the verb number (Figures 2c and 2d).",4 Subject-Verb Agreement,0,[0]
"Again, we observe that LSTMs consistently outperform FANs.",4 Subject-Verb Agreement,0,[0]
"This is a particularly interesting result since the self-attention mechanism in FANs connects two words in any po-
1Agreement attractors are intervening nouns with the opposite number from the subject.
sition with a O(1) number of executed operations, whereas RNNs require more recurrent operations.",4 Subject-Verb Agreement,0,[0]
"Despite this apparent advantage of FANs, the performance gap between FANs and LSTMs increases with the distance and number of attractors.2
To gain further insights into our results, we examine the attention weights computed by FANs during verb-number prediction (supervised objective).",4 Subject-Verb Agreement,0,[0]
"Specifically, for each attention head at each layer of the FAN, we compute the percentage of
2We note that our LSTM results are better than those in Linzen et al. (2016).",4 Subject-Verb Agreement,0,[0]
Also surprising is that the language model objective yields higher accuracies than the number prediction objective.,4 Subject-Verb Agreement,0,[0]
"We believe this may be due to better model optimization and to the embedding-output layer weight sharing, but we leave a thorough investigation to future work.
",4 Subject-Verb Agreement,0,[0]
times the subject is the most attended word among all words in the history.,4 Subject-Verb Agreement,0,[0]
Figure 3 shows the results for all cases where the model made the correct prediction.,4 Subject-Verb Agreement,0,[0]
"While it is hard to interpret the exact role of attention for different heads and at different layers, we find that some of the attention heads at the higher layers (`2 h1, `3 h0) frequently point to the subject with an accuracy that decreases linearly with the distance between subject and verb.",4 Subject-Verb Agreement,0,[0]
"In this task, we choose the artificial language introduced by Bowman et al. (2015b).",5 Logical inference,0,[0]
"The vocabulary of this language includes six word types {a, b, c, d, e, f } and three logical operators {or, and, not}.",5 Logical inference,0,[0]
"The task consists of predicting one of seven mutually exclusive logical relations that describe the relationship between a pair of sentences: entailment (@, A), equivalence (≡), exhaustive and non-exhaustive contradiction (∧, |), and two types of semantic independence (#, `).",5 Logical inference,0,[0]
"We generate 60,000 samples3 with the number of logical operations ranging from 1 to 12.",5 Logical inference,0,[0]
The train/dev/test dataset ratios are set to 0.8/0.1/0.1.,5 Logical inference,0,[0]
"Here are some samples of the training data:
( d ( or f ) )",5 Logical inference,0,[0]
A ( f ( and a ) ) ( d ( and ( c ( or d ) ) ) ),5 Logical inference,0,[0]
"# ( not f )
( not ( d ( or ( f ( or c ) ) ) ) )",5 Logical inference,0,[0]
"@ ( not ( c ( and ( not d ) ) ) )
",5 Logical inference,0,[0]
Why artificial data?,5 Logical inference,0,[0]
"Despite the simplicity of the 3https://github.com/sleepinyourhat/ vector-entailment
language, this task is not trivial.",5 Logical inference,0,[0]
"To correctly classify logical relations, the model must learn nested structures as well as the scope of logical operators.",5 Logical inference,0,[0]
We verify the difficulty of the task by training three bag-of-words models followed by sum/average/max-pooling.,5 Logical inference,0,[0]
"The best of the three models achieve less than 59% accuracy on the logical inference versus 77% on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015a).",5 Logical inference,0,[0]
"This shows that the SNLI task can be largely solved by exploiting shallow features without understanding the underlying linguistic structures, which has also been pointed out by recent work (Glockner et al., 2018; Gururangan et al., 2018).
",5 Logical inference,0,[0]
Concurrently to our work Evans et al. (2018) proposed an alternative data set for logical inference and also found that a FAN model underperformed various other architectures including LSTMs.,5 Logical inference,0,[0]
"We follow the general architecture proposed in (Bowman et al., 2015b): Premise and hypothesis sentences are encoded by fixed-size vectors.",5.1 Models,0,[0]
"These two vectors are then concatenated and fed to a 3- layer feed-forward neural network with ReLU nonlinearities to perform 7-way classification of the logical relation.
",5.1 Models,0,[0]
The LSTM architecture used in this experiment is similar to that of Bowman et al. (2015b).,5.1 Models,0,[0]
We simply take the last hidden state of the top LSTM layer as a fixed-size vector representation of the sentence.,5.1 Models,0,[0]
"Here, we use a 2-layer LSTM with skip connections.",5.1 Models,0,[0]
The FAN maps a sentence x of length n to H =,5.1 Models,0,[0]
"[h1, . . .",5.1 Models,0,[0]
",hn] ∈ Rd×n.",5.1 Models,0,[0]
"To obtain a fixedsize representation z, we use a self-attention layer with two trainable queries q1,q2 ∈ R1×d:
zi = softmax ( qiH√
d
)",5.1 Models,0,[0]
"H> i ∈ {1, 2}
z =",5.1 Models,0,[0]
"[z1, z2]
We find the best hyperparameters for each model by running a grid search as explained in §4.",5.1 Models,0,[0]
"Following the experimental protocol of Bowman et al. (2015b), the data is divided into 13 bins based on the number of logical operators.",5.2 Results,0,[0]
Both FANs and LSTMs are trained on samples with at most n logical operators and tested on all bins.,5.2 Results,0,[0]
"Figure 4 shows the result of the experiments with n ≤ 6 and
n ≤ 12.",5.2 Results,0,[0]
We see that FANs and LSTMs perform similarly when trained on the whole dataset (Figure 4a).,5.2 Results,0,[0]
"However when trained on a subset of the data (Figure 4b), LSTMs obtain better accuracies on similar examples (n ≤ 6) and generalize better on longer examples (6 < n ≤ 12).",5.2 Results,0,[0]
We have compared a recurrent architecture (LSTM) to a non-recurrent one (FAN) with respect to the ability of capturing the underlying hierarchical structure of sequential data.,6 Discussion and Conclusion,0,[0]
Our experiments show that LSTMs slightly but consistently outperform FANs.,6 Discussion and Conclusion,0,[0]
"We found that LSTMs are notably more robust with respect to the presence of misleading features in the agreement task, whether trained with explicit supervision or with a general language model objective.",6 Discussion and Conclusion,0,[0]
"Secondly, we found that LSTMs generalize better than FANs to longer sequences in a logical inference task.",6 Discussion and Conclusion,0,[0]
"These findings suggest that recurrency is a key model property which should not be sacrificed for efficiency when hierarchical structure matters for the task.
",6 Discussion and Conclusion,0,[0]
This does not imply that LSTMs should always be preferred over non-recurrent architectures.,6 Discussion and Conclusion,0,[0]
"In fact, both FAN- and CNN-based networks have proved to perform comparably or better than LSTM-based ones on a very complex task like machine translation (Gehring et al., 2017; Vaswani et al., 2017).",6 Discussion and Conclusion,0,[0]
"Nevertheless, we believe that the ability of capturing hierarchical information in sequen-
tial data remains a fundamental need for building intelligent systems that can understand and process language.",6 Discussion and Conclusion,0,[0]
Thus we hope that our insights will be useful towards building the next generation of neural networks.,6 Discussion and Conclusion,0,[0]
"This research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project numbers 639.022.213, 612.001.218, and 639.021.646.",Acknowledgments,0,[0]
"Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016).",abstractText,0,[0]
"In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017).",abstractText,0,[0]
"In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose.",abstractText,0,[0]
The code and data used in our experiments is available at https://github.com/,abstractText,0,[0]
The Importance of Being Recurrent for Modeling Hierarchical Structure,title,0,[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 456–461 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
456",text,0,[0]
Sentence acceptability is defined as the extent to which a sentence is well formed or natural to native speakers of a language.,1 Introduction,0,[0]
"It encompasses semantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory limitation.",1 Introduction,0,[0]
"Grammaticality, by contrast, is the syntactic well-formedness of a sentence.",1 Introduction,0,[0]
Grammaticality as characterised by formal linguists is a theoretical concept that is difficult to elicit from non-expert assessors.,1 Introduction,0,[0]
"In the research presented here we are interested in predicting acceptability judgements.2
Lau et al. (2015, 2016) present unsupervised probabilistic methods to predict sentence acceptability, where sentences were judged independently of context.",1 Introduction,0,[0]
"In this paper we extend this
1Annotated data (with acceptability ratings) is available at: https://github.com/GU-CLASP/BLL2018.
",1 Introduction,0,[0]
2See Lau et al. (2016) for a detailed discussion of the relationship between acceptability and grammaticality.,1 Introduction,0,[0]
"They provide motivation for measuring acceptability rather than grammaticality in their crowd source surveys and modelling experiments.
research to investigate the impact of context on human acceptability judgements, where context is defined as the full document environment surrounding a sentence.",1 Introduction,0,[0]
"We also test the accuracy of more sophisticated language models — one which incorporates document context during training — to predict human acceptability judgements.
",1 Introduction,0,[0]
We believe that understanding how context influences acceptability is crucial to success in modelling human acceptability judgements.,1 Introduction,0,[0]
It has implications for tasks such as style/coherence assessment and language generation.,1 Introduction,0,[0]
Showing a strong correlation between unsupervised language model sentence probability and acceptability supports the view that linguistic knowledge can be represented as a probabilistic system.,1 Introduction,0,[0]
"This result addresses foundational questions concerning the nature of grammatical knowledge (Lau et al., 2016).
",1 Introduction,0,[0]
Our work is guided by 3 hypotheses:,1 Introduction,0,[0]
H1: Document context boosts sentence acceptability judgements.,1 Introduction,0,[0]
H2: Document context helps language models to model acceptability.,1 Introduction,0,[0]
H3:,1 Introduction,0,[0]
"A language model predicts acceptability more accurately when it is tested on sentences within document context than when it is tested on the sentences alone.
",1 Introduction,0,[0]
We sample sentences and their document contexts from English Wikipedia articles.,1 Introduction,0,[0]
"We perform round-trip machine translation to generate sentences of varying degrees of well-formedness and ask crowdsourced workers to judge the acceptability of these sentences, presenting the sentences with and without their document environments.",1 Introduction,0,[0]
"We describe this experiment and address H1 in Section 2.
",1 Introduction,0,[0]
"In Section 3, we experiment with two types of language models to predict acceptability: a standard language model and a topically-driven model.",1 Introduction,0,[0]
"The latter extends the language model by incorporating document context as a conditioning
variable.",1 Introduction,0,[0]
The model comparison allows us to understand the impact of incorporating context during training for acceptability prediction.,1 Introduction,0,[0]
We also experiment with adding context as input at test time for both models.,1 Introduction,0,[0]
"These experiments collectively address H2, by investigating the impact of using context during training and testing for modelling acceptability.",1 Introduction,0,[0]
We evaluate the models against crowd-sourced annotated sentences judged both in context and out of context.,1 Introduction,0,[0]
"This tests H3.
",1 Introduction,0,[0]
In Section 4 we briefly consider related work.,1 Introduction,0,[0]
We indicate the issues to be addressed in future research and summarise our conclusions in Section 5.,1 Introduction,0,[0]
"Our goal is to construct a dataset of sentences annotated with acceptability ratings, judged with and without document context.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To obtain sentences and their document context, we extracted 100 random articles from the English Wikipedia and sampled a sentence from each article.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To generate a set of sentences with varying degrees of acceptability we used the Moses MT system (Koehn et al., 2007) to translate each sentence from English to 4 target languages — Czech, Spanish, German and French — and then back to English.3 We chose these 4 languages because preliminary experiments found that they produce sentences with different sorts of grammatical, semantic, and lexical infelicities.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"Note that we only translate the sentences; the document context is not modified.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To gather acceptability judgements we used Amazon Mechanical Turk and asked workers to judge acceptability using a 4-point scale.4 We ran the annotation task twice: first where we presented sentences without context, and second within their document context.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"For the in-context experiment, the target sentence was highlighted in boldface, with one preceding and one succeeding sentence included as additional context.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
Workers had the option of revealing the full document context by clicking on the preceding and succeeding sentences.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We did not check whether subjects viewed
3We use the pre-trained Moses models for translation: http://www.statmt.org/moses/RELEASE-4.0/",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"models/.
4We ask workers to judge how “natural” they find a sentence.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"For more details on the AMT protocol and our use of a four category naturalness rating system, see Lau et al. (2015, 2016).
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
the full context when recording their ratings.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
Henceforth human judgements made without context are denoted as h− and judgements with context as h+.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We collected 20 judgements per sentence, giving us a total of a 20,000 annotations (100 sentences× 5 languages× 2 presentations× 20 judgements).
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To ensure annotation reliability, sentences were presented in groups of five, one from the original English set, and four from the round-trip translations, one per target language, with no sentence type (English original or its translated variant) appearing more than once in a HIT.5",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We assume that the original English sentences are generally acceptable, and we filtered out workers who fail to consistently rate these sentences as such.6 Postfiltering, we aggregate the multiple ratings and compute the mean.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
We first look at the correlation between withoutcontext (h−) and with-context (h+) mean ratings.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
Figure 1 is a scatter plot of this relation.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We found a strong correlation of Pearson’s r = 0.80 between the two sets of ratings.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We see that adding context generally improves acceptability (evidenced by points above the diagonal), but the pattern reverses as acceptability increases, suggesting that context boosts sentence ratings most for ill-formed sentences.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"The trend persists throughout the whole range of acceptability, so that for the most acceptable sentences, adding context actually diminishes their rated acceptability.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We can see this trend clearly in Figure 1, where the average difference between h− and h+ is represented by the distance between the linear regression and the diagonal.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"These lines cross at h+ = h− = 3.28, the point where context no longer boosts acceptability.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To understand the spread of individual judgements on a sentence, we compute the standard deviation of ratings for each sentence and then take the mean over all sentences.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
We found a small difference: 0.71 for h− and 0.76 for h+.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We also calculate one-vs-rest correlation, where for each
5A HIT is a “human intelligence task”.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"It constitutes a unit of work for crowdworkers.
6Control sentence rating threshold = 3.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
Minimum accuracy for control sentences = 0.70.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To prevent workers from gaming this system (by giving all perfect ratings), we also removed workers whose average rating ≥ 3.5.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"Using these rules we filtered out on average, for each sentence, 7.5125 answers for h+ and 3.9725 for h−. This gave us approximately 13 and 16 annotators for each h+ and h− sentence respectively.
sentence we randomly single out an annotator rating and compute the Pearson correlation between these judgements against the mean ratings for the rest of the annotators.7 This number can be interpreted as a performance upper bound on a single annotator for predicting the mean acceptability of a group of annotators.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
We found a big gap in the one-vs-rest correlations: 0.628 for h− and 0.293 for h+.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"We were initially surprised as to why the correlation is so different, even though the standard deviation is similar.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"Further investigation reveals that this dif-
7Trials are repeated 1000 times and the average correlation is computed, to insure that we obtain robust results and avoid outlier ratings skewing our Pearson coefficient value.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"See Lau et al. (2016) for the details of this and an alternative method for simulating an individual annotator.
ference is explained by the pattern shown in Figure 1.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"Adding context “compressess” the distribution of (mean) ratings, pushing the extremes to the middle (i.e. very ill/well-formed sentences are now less ill/well-formed).",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"The net effect is that it lowers correlation, as the good and bad sentences are now less separable.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
One possible explanation for this compression is that workers focus more on global semantic and pragmatic coherence when context is supplied.,2 The Influence of Document Context on Acceptability Ratings,0,[0]
"If this is the case, then the syntactic mistakes introduced by MT have less effect on ratings than for the out-of-context sentences, where global coherence is not a factor.
",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"To give a sense how context influences ratings, we present a sample of sentences with their without-context (h−) and with-context (h+) ratings in Table 1.",2 The Influence of Document Context on Acceptability Ratings,0,[0]
"Lau et al. (2015, 2016) explored a number of unsupervised models for predicting acceptability, including n-gram language models, Bayesian HMMs, LDA-based models, and a simple recurrent network language model.",3 Modelling Sentence Acceptability with Enriched LMs,1,"['Bayesian optimization (BO) (Shahriari et al., 2016) is a powerful and versatile tool for black-box function optimization, with applications including parameter tuning, robotics, molecular design, sensor networks, and more.']"
"They found that the neural model outperforms the others consistently over multiple domains, in several languages.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"In light of this, we experiment with neural models in this paper.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"We use: (1) a LSTM language model (lstm: Hochreiter and Schmidhuber (1997); Mikolov et al. (2010)), and (2) a topically driven neural language model (tdlm: Lau et al. (2017)).8
lstm is a standard LSTM language model, trained over a corpus to predict word sequences.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"8We use the following tdlm implementation: https://github.com/jhlau/ topically-driven-language-model.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
tdlm is a joint model of topic and language.,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
The topic model component produces topics by processing documents through a convolutional layer and aligning it with trainable topic embeddings.,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"The language model component incorporates context by combining its topic vector (produced by the topic model component) with the LSTM’s hidden state, to generate the probability distribution for the next word.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"After training, given a sentence both lstm and tdlm produce a sentence probability (aggregated using the sequence of conditional word probabilities).",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"In our case, we also have the document context, information which both models can leverage.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Therefore we have 4 variants at test time: models that use only the sentence as input, lstm− and tdlm−, and models that use both sentence and context, lstm+ and tdlm+.9 lstm+ incorporates context by feeding it to the LSTM network and taking its final state10 as the initial state for the current sentence.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"tdlm− ignores the context by converting the topic vector into a vector of zeros.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"To map sentence probability to acceptability, we compute several acceptability measures (Lau et al., 2016), which are designed to normalise sentence length and word frequency.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"These are given in Table 2.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"We train tdlm and lstm on a sample of 100K English Wikipedia articles, which has no over-
9There are only two trained models: lstm and tdlm.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"The four variants are generated by varying the type of input provided at test time when computing the sentence probability.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"10The final state is the hidden state produced by the last word of the context.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
lap with the 100 documents used for the annotation described in Section 2.,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"The training data has approximately 40M tokens and a vocabulary size of 66K.11 Training details and all model hyperparameter settings are detailed in the supplementary material.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"To assess the performance of the acceptability measures, we compute Pearson’s r against mean human ratings (Table 3).",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"We also experimented with Spearman’s rank correlation, but found similar trends and so present only the Pearson results.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"The first observation is that we replicate the performance of the original experiment setting (Lau et al., 2015).",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"We achieved a correlation of 0.584 when we compared lstm− against h−, which is similar to the previously reported performance (0.570).12 SLOR outperforms all other measures, which is consistent with the findings in Lau et al. (2015).",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"We will focus on SLOR for the remainder of the discussion.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Across all models (lstm and tdlm) and human ratings (h− and h+), using context at test time improves model performance.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"This suggests that taking context into account helps in modelling acceptability, regardless of whether it is tested against judgements made with (h+) or without context (h−).13 We also see that tdlm consis-
11We filter word types that occur less than 10 times, lowercase all words, and use a special unkown token to represent unseen words.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
12We note two differences.,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"First, we use a different set of Wikipedia training and testing articles.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Second, we employ a LSTM instead of a simple RNN for the language model.
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
13We believe incorporating context at test time for lstm improves performance because context puts the starting state of the current sentence in the right “semantic” space when predicting its words.,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Without context, the initial state for the current sentence is defaulted to a vector of zeros, and the
tently outperforms lstm over both types of human ratings and test input variants, showing that tdlm is a better model at predicting acceptability.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"In fact, if we look at tdlm− vs. lstm+ (h−: 0.640 vs. 0.633; h+: 0.557 vs. 0.546), tdlm still performs better without context than lstm with context.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"These observations confirm that context helps in the modelling of acceptability, whether it is incorporated during training (lstm vs. tdlm) or at test time (lstm−/tdlm− vs. lstm+/tdlm+).
",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Interestingly, we see a lower correlation when we are predicting sentence acceptability that is judged with context.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
The SLOR correlation of lstm+/tdlm+ vs. h+ (0.546/568) is lower than that of lstm−/tdlm− vs. h− (0.584/0.640).,3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"This result corresponds to the low one-vs-rest human performance of h+ compared to h− (0.299 vs. 0.636, see Section 2).",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"It suggests that h+ ratings are more difficult to predict than h−. With human performance taken into account, both models substantially outperform the average single-annotator correlation, which is encouraging for the prospect of accurate model prediction on this task.",3 Modelling Sentence Acceptability with Enriched LMs,0,[0]
"Nagata (1988) reports a small scale experiment with 12 Japanese speakers on the effect of repetition of sentences, and embedding them in context.",4 Related Work,0,[0]
He notes that both repetition and context cause acceptability judgements for ill formed sentences to be more lenient.,4 Related Work,0,[0]
"Gradience in acceptability judgements are studied in the works of Sorace and Keller (2005) and Sprouse (2007).
",4 Related Work,0,[0]
"There is an extensive literature on automatic detection of grammatical errors (Atwell, 1987; Chodorow and Leacock, 2000; Bigert and Knutsson, 2002; Sjöbergh, 2005; Wagner et al., 2007), but limited work on acceptability prediction.",4 Related Work,0,[0]
"Heilman et al. (2014) trained a linear regression model that uses features such as spelling errors, sentence scores from n-gram models and parsers.",4 Related Work,0,[0]
"Lau et al. (2015, 2016) experimented with unsupervised learners and found that a simple RNN was the best performing model.",4 Related Work,0,[0]
"Both works predict acceptability independently of any contextual factors outside the target sentence.
model has no information as to what words will be relevant.",4 Related Work,0,[0]
"We found that (i) context positively influences acceptability, particularly for ill-formed sentences, but it also has the reverse effect for well-formed sentences (H1); (ii) incorporating context (during training or testing) when modelling acceptability improves model performance (H2); and (iii) prediction performance declines when tested on judgements collected with context, overturning our original hypothesis (H3).",5 Future Work and Conclusions,0,[0]
"We discovered that human agreement decreases when context is introduced, suggesting that ratings are less predictable in this case.
",5 Future Work and Conclusions,0,[0]
"While it is intuitive that context should improve acceptability for ill-formed sentences, it is less obvious why it reduces acceptability for well-formed sentences.",5 Future Work and Conclusions,0,[0]
We will investigate this question in future work.,5 Future Work and Conclusions,0,[0]
"We will also experiment with a wider range of models, including sentence embedding methodologies such as Skip-Thought (Kiros et al., 2015).",5 Future Work and Conclusions,0,[0]
We are grateful to the anonymous ACL reviewers for their helpful comments.,Acknowledgments,0,[0]
"Versions of this paper were presented to the Queen Mary University of London NLP Seminar in March 2018, and the Saarland University SFB Language Sciences Colloquium in May 2018.",Acknowledgments,0,[0]
"We are grateful to the audiences of both forums, and to colleagues at CLASP for useful discussion of the ideas presented here.
",Acknowledgments,0,[0]
"The research of the first two authors was supported by grant 2014-39 from the Swedish Research Council, which funds the Centre for Linguistic Theory and Studies in Probability in FLoV at the University of Gothenburg.",Acknowledgments,0,[0]
"We investigate the influence that document context exerts on human acceptability judgements for English sentences, via two sets of experiments.",abstractText,0,[0]
The first compares ratings for sentences presented on their own with ratings for the same set of sentences given in their document contexts.,abstractText,0,[0]
The second assesses the accuracy with which two types of neural models — one that incorporates context during training and one that does not — predict these judgements.,abstractText,0,[0]
"Our results indicate that: (1) context improves acceptability ratings for ill-formed sentences, but also reduces them for well-formed sentences; and (2) context helps unsupervised systems to model acceptability.1",abstractText,0,[0]
The Influence of Context on Sentence Acceptability Judgements,title,0,[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2871–2876 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2871",text,0,[0]
"The advent of Neural Machine Translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) has led to remarkable improvements in machine translation quality (Bentivogli et al., 2016) but has also produced models that are much less interpretable.",1 Introduction,0,[0]
"In particular, the role played by linguistic features in the process of understanding the source text and rendering it in the target language remains hard to gauge.",1 Introduction,0,[0]
"Acquiring this knowledge is important to inform future research in NMT, especially regarding the usefulness of injecting linguistic information into the NMT model, e.g. by using supervised annotation (Sennrich and Haddow, 2016).
",1 Introduction,0,[0]
"Hill et al. (2014) gave a first answer to this question, reporting high accuracies by source-side NMT word embeddings on the well-known analogy task by Mikolov et al. (2013) which also includes a number of derivational and inflectional transformations in the morphologically poor English language.",1 Introduction,0,[0]
"More recent work (Shi et al., 2016) has shown that source sentence representations produced by NMT encoders contain a great
deal of syntactic information.",1 Introduction,0,[0]
Belinkov et al. (2017a) focused on the word level and examined to what extent part-of-speech and morphological information can be extracted from various NMT word representations.,1 Introduction,0,[0]
The latter study found that source-side morphology is captured slightly better by the first recurrent layer than by the word embedding and the final recurrent layer.,1 Introduction,0,[0]
"Another, somewhat surprising finding was that source-side morphology is learned better when translating into an ‘easier’ target language than into a related one, even if the ’easier’ language is morphologically poor.
",1 Introduction,0,[0]
"In this paper, we also focus on source-side morphology but perform a finer-grained analysis of how morphological features are captured by different components of the NMT encoder while varying the target language.",1 Introduction,0,[0]
"We argue that predicting generic morphological tags where all features are mixed, as done by Belinkov et al. (2017a), can only give us a limited insight into the linguistic competence of the model.",1 Introduction,0,[0]
"Hence, we predict morphological features independently from one another and ask the following questions:
• Are different morphological features captured by the NMT encoder to substantially different extents and, if yes, why?
• Are morphological features captured as a word type property (i.e. at the word embedding level) or are they mostly computed in context (i.e. at the recurrent state level)?
",1 Introduction,0,[0]
"• How does source-target language relatedness affect the morphological competence of the NMT encoder?
More specifically, we look at whether the NMT encoder only learns those morphological features that can be directly transferred to the target words
(such as number) or whether it also learns features that are not directly transferable but can still be useful to correctly parse and infer the meaning of a sentence (such as gender).",1 Introduction,0,[0]
"See example in Fig. 1.
",1 Introduction,0,[0]
"We focus on French and similarly to previous work (Shi et al., 2016; Belinkov et al., 2017a)",1 Introduction,0,[0]
we use the continuous word representations produced by a trained NMT system to build and evaluate a number of linguistic feature classifiers.,1 Introduction,0,[0]
Classifier accuracy represents the extent to which a given feature is captured by the NMT encoder.,1 Introduction,0,[0]
"We train NMT systems on the following language pairs: French-Italian (FRIT ), French-German (FRDE), and French-English (FREN ).",2 Methods,0,[0]
We chose these language pairs for their different levels of language relatedness and morphological feature correspondence.,2 Methods,0,[0]
"Grammatical gender is especially interesting as it is marked in French, Italian and German, but not in English (except for a few pronouns).",2 Methods,0,[0]
"The gender of Italian nouns often corresponds to that of French because of their common language ancestor, whereas German gender is mostly unrelated from French gender (see example in Fig. 1).
",2 Methods,0,[0]
The continuous word representations produced by the three NMT systems while encoding a corpus of French sentences are used to build and evaluate several specialized classifiers: one per morphological feature.,2 Methods,0,[0]
"If a classifier significantly outperforms the majority baseline, we conclude that the corresponding feature is captured by the NMT encoder.",2 Methods,0,[0]
"While this methodology is similar to that of previous work (Köhn, 2015; Belinkov et al., 2017a,b; Dalvi et al., 2017) we make sure that our results are not affected by overfitting by eliminat-
ing any vocabulary overlap between the classifier’s training and test sets.",2 Methods,0,[0]
We find this step crucial to ensure that the redundancy in this type of data does not lead to over-optimistic conclusions.,2 Methods,0,[0]
"We now provide more details on the experimental setup.
",2 Methods,0,[0]
Parallel corpora.,2 Methods,0,[0]
"For a fair comparison among target languages, we extract the intersection of the Europarl corpus (Koehn, 2005) in our three language pairs so that the source side data is identical for all NMT systems.",2 Methods,0,[0]
Sentences longer than 50 tokens are ignored.,2 Methods,0,[0]
"This data is then split into an NMT training, validation, and test set of 1.3M, 2.5K, and 2.5K sentence pairs respectively.
NMT model.",2 Methods,0,[0]
"The NMT architecture is an attentional encoder-decoder model similar to (Luong et al., 2015) and uses a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) as the recurrent cell.",2 Methods,0,[0]
The models have 3 stacked LSTM layers and are trained for 15 epochs.,2 Methods,0,[0]
Embedding and hidden state sizes are set to 1000.,2 Methods,0,[0]
"Source and target vocabularies are limited to the 30,000 most frequent words on each side of the training data.1",2 Methods,0,[0]
"The NMT models achieve a test BLEU score of 32.6, 25.4 and 39.4 for FrenchItalian, French-German and French-English respectively.
",2 Methods,0,[0]
Continuous word representations.,2 Methods,0,[0]
"Given a source sentence, the NMT system first encodes it into a sequence of word embeddings (contextindependent representations), and then into a sequence of recurrent states (context-dependent representations).",2 Methods,0,[0]
"As we are mostly interested in the impact of context on word representations, we compare the word embeddings against the final layer of the stacked LSTMs (corresponding to layers 0 and 3 in Belinkov et al. (2017a)’s terms) while disregarding the intermediate layers.
",2 Methods,0,[0]
Morphological classification.,2 Methods,0,[0]
"The continuous word representations are used to train a logistic regression classifier2 for each morphological feature: gender and number for noun and adjectives; tense for verbs (with labels: present, fu-
1Subword/character-level representations are not included in this study since we are interested in the models’ ability to learn morphology from word usage, rather than word form.
",2 Methods,0,[0]
"2We use linear classifiers since their accuracies can be interpreted as a measure of supervised clustering accuracy, which gives a better insight on the structure of the vector space (Köhn, 2015).",2 Methods,0,[0]
"Results with a simple multi-layer perceptron were consistent with the findings by the linear classifier, with slightly better performance overall.
",2 Methods,0,[0]
"ture, imperfect, or simple past).",2 Methods,0,[0]
"Word labels are taken from the Lefff French morphological lexicon (Sagot, 2010)3.",2 Methods,0,[0]
"To ensure a fair comparison between context-independent and context-dependent embedding classification, words that are ambiguous with respect to a given feature are excluded from the respective classifier’s training and test data.
",2 Methods,0,[0]
Classifiers’ training/test data.,2 Methods,0,[0]
The classifiers are trained on a 50K-sentence subset of the NMT training data and tested on the NMT test sets (2.5K).,2 Methods,0,[0]
"For each experiment, we extract one vector per token from the NMT encoder.",2 Methods,0,[0]
"While this is the only possible setup for context-dependent representations, it leads to a problematic training/test overlap in the word embedding experiment because all occurrences of the same word are associated to exactly the same vector.",2 Methods,0,[0]
"We find that, due to this overlap, a dummy binary feature assigned to a random half of the vocabulary can be predicted from the word embeddings with very high accuracy (86% for a linear, 98% for a non-linear classifier) leading to over-optimistic conclusions on the linguistic regularities of these representations.",2 Methods,0,[0]
"To avoid this, we split the vocabulary in two parts of 15K types: the first is used to filter the training samples and the second to filter the test samples.",2 Methods,0,[0]
We repeat each experiment five times using five different random vocabulary splits and report mean accuracies.,2 Methods,0,[0]
This process is applied to all experiments (including those on hidden states) to allow for a fair comparison of the results.,2 Methods,0,[0]
"This section presents our results along three dimensions: context-dependency of the word representations (§3.1), different morphological features (§3.2), and target language impact (§3.3).",3 Results and Discussion,0,[0]
"Unless explicitly stated, all discussed results are statistically significant (computed using a t-test for a onetailed hypothesis and independent means).",3 Results and Discussion,0,[0]
One of our goals was to discover whether morphological features are captured as a word type property or in context.,3.1 Word embeddings vs recurrent states,0,[0]
"Fig. 2 shows the extent to which the NMT encoder captures different features at the word level (word embeddings) compared to the recurrent state level (LSTM state), averaged over
3Lexique des Formes Fléchies du Français: http:// alpage.inria.fr/˜sagot/lefff-en.html
all target languages.",3.1 Word embeddings vs recurrent states,0,[0]
"We can see that each feature is clearly captured at the recurrent state level, confirming that source-side morphology is indeed successfully exploited by NMT models.",3.1 Word embeddings vs recurrent states,0,[0]
"However, at the word embedding level, accuracies are comparable to the majority class baseline (these differences are not significant), which implies that the source-side lexicon of our NMT systems does not encode morphology in a systematic way.",3.1 Word embeddings vs recurrent states,0,[0]
This might be partly explained by the fact that learning morphological features at the word level is difficult due to data sparsity – indeed the rarest French words in our dataset are observed only 10 times in the training data.,3.1 Word embeddings vs recurrent states,0,[0]
"However, additional experiments showed that our finding is consistent across different word frequency bins: that is, even the embeddings of frequent words do not encode morphological features better than the majority baseline.
",3.1 Word embeddings vs recurrent states,0,[0]
"This result is surprising, considering that our morphological features are usually easy to infer from the immediate context of French words (see examples in Fig.1) and that morphology was shown to be well captured by monolingual word embeddings in various European languages including French (Köhn, 2015).",3.1 Word embeddings vs recurrent states,0,[0]
"By contrast, our NMT encoders choose not to store morphology at the word type level, perhaps in order to allocate more capacity to semantic information.",3.1 Word embeddings vs recurrent states,0,[0]
"Secondly, we asked whether the NMT encoder captured different morphological features to different extents.",3.2 Different morphological features,0,[0]
"For this question, we disregard the word embedding results because none of the features are significantly captured at this level.
",3.2 Different morphological features,0,[0]
"Fig. 2 shows that the mean accuracy of number is the highest, followed by tense and then by gender.",3.2 Different morphological features,0,[0]
"However, it should be noted that the majority baselines for number and tense are much higher than the one for gender.",3.2 Different morphological features,0,[0]
"In both absolute and rela-
tive terms, the best performing feature is number.",3.2 Different morphological features,0,[0]
"This can be explained by the fact that number remains most often unchanged through translation, and is marked in all target languages – albeit to different extents.",3.2 Different morphological features,0,[0]
"On the other hand, tense is determined by the semantics but also by languagespecific usage, while gender has little semantic value and is mostly assigned to nouns arbitrarily.
",3.2 Different morphological features,0,[0]
The fact that the results of different morphological features are so variable confirms the setup of examining each feature independently.,3.2 Different morphological features,0,[0]
Fig. 3 shows the impact of the target language on the encoded morphology accuracy.,3.3 Source-target language relatedness,0,[0]
"We again focus our analysis on the LSTM state level since embedding level results are mostly near the baseline.
Differently from Belinkov et al. (2017a)",3.3 Source-target language relatedness,0,[0]
"we do not find that source-side morphology is captured better when translating into the ‘easiest’ language, which in our case is English, both in terms of morphological complexity and BLEU performance.",3.3 Source-target language relatedness,0,[0]
"We note that their findings were based on very small, possibly not significant differences, and on the prediction of all morphological features simultaneously.",3.3 Source-target language relatedness,0,[0]
"By contrast, our fine-grained analysis reveals that the impact of target language is significant and even major on only one feature, namely gender, where it agrees with our linguistic intuition.",3.3 Source-target language relatedness,0,[0]
"Indeed this feature differs from the others because it varies largely among languages and, when present, is semantically determined only to a very limited extent.",3.3 Source-target language relatedness,0,[0]
"FRIT , where source gender is a good predictor of target gender, shows the highest accuracy; FREN , where target gender is not marked, shows the lowest; FRDE , where source gender is often confounding for target gender, lies in-between.
",3.3 Source-target language relatedness,0,[0]
"Is language relatedness the main explaining
variable?",3.3 Source-target language relatedness,0,[0]
"To find that out, we experiment with a modified Italian target language without gender marking, i.e. all gender-marked words are replaced by their masculine form (FRIT ∗).",3.3 Source-target language relatedness,0,[0]
"This language pair achieves a slightly higher BLEU score than FRIT (33.2 vs 32.6), which can be attributed to the smaller target vocabulary.",3.3 Source-target language relatedness,0,[0]
"However its source gender accuracy is much worse (see Fig. 3), which indicates that the high performance of the FRIT encoder is mostly due to the ubiquitous gender marking in the target language, rather than to language relatedness.",3.3 Source-target language relatedness,0,[0]
"All this suggests that source morphological features contribute to sentence understanding to some degree, but the incentive to learn them mostly depends on how directly they can be transferred to the target sentence.
",3.3 Source-target language relatedness,0,[0]
"Finally, we look at what happens when a single NMT system is trained in a multitarget fashion on our three language pairs.",3.3 Source-target language relatedness,0,[0]
"Following the setup of Johnson et al. (2017), we prepend a totarget-language tag {2it,2de,2en} to the source side of each sentence pair and mix all language pairs in the NMT training data.",3.3 Source-target language relatedness,0,[0]
"Results are presented for gender in Fig. 3 (right).4 Note that, while word embeddings are identical for the three language pairs, recurrent states change according to the language tag.",3.3 Source-target language relatedness,0,[0]
In this setup the target language impact is less visible and gender accuracy at the LSTM state level is overall much higher than that of the mono-target systems (0.77 vs 0.68 on average) whereas BLEU scores are slightly lower (−0.9% on average).,3.3 Source-target language relatedness,0,[0]
"While this is only an initial exploration of multilingual NMT systems, our results suggest that this kind of multi-task objective pushes the model to learn linguistic features in a more consistent way (Bjerva, 2017; Enguehard et al., 2017).
",3.3 Source-target language relatedness,0,[0]
4Multitarget results for tense and number did not differ significantly from the corresponding monotarget results.,3.3 Source-target language relatedness,0,[0]
We have confirmed previous findings that morphological features are significantly captured by word-level NMT encoders.,4 Conclusion,0,[0]
"However, the features are not captured at the word type level but only at the recurrent state level where word representations are context-dependent.",4 Conclusion,0,[0]
"Secondly, there is a visible difference in the extent to which different morphological features are learned: Semantic categories like number and verb tense are well captured in all language pairs, whereas grammatical gender with its only agreement-triggering function, is dramatically affected by the target language.",4 Conclusion,0,[0]
"Source-side gender is encoded well only when it is a good predictor of target gender and when target-side marking is extensive, i.e. when translating from French to Italian.
",4 Conclusion,0,[0]
Our findings indicate that the importance of linguistic structure for the neural translation process is very variable and language-dependent.,4 Conclusion,0,[0]
"They also suggest that the NMT encoder is rather ‘lazy’ when it comes to learning grammatical features of the source words, unless these are directly transferable to their target equivalents.",4 Conclusion,0,[0]
This research was partly funded by the Netherlands Organization for Scientific Research (NWO) under project number 639.021.646.,Acknowledgments,0,[0]
"Part of the work was carried out on the DAS computing system (Bal et al., 2016) while the authors were affiliated at the Informatics Institute of the University of Amsterdam.",Acknowledgments,0,[0]
We thank Ke Tran for providing feedback on the early stages of this research.,Acknowledgments,0,[0]
"Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability.",abstractText,0,[0]
"To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language.",abstractText,0,[0]
"Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality.",abstractText,0,[0]
We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.,abstractText,0,[0]
The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation,title,0,[0]
Maximum selection (maxing) and sorting (ranking) are fundamental problems in Computer Science with numerous important applications.,1.1. Background and motivation,0,[0]
"Deterministic versions of these problems are well studied.
",1.1. Background and motivation,0,[0]
"In practical applications, comparisons are rarely deterministic.",1.1. Background and motivation,0,[0]
"For example in soccer, when Real Madrid plays Barcelona the outcome is not always the same.",1.1. Background and motivation,0,[0]
"Similarly, individual preferences in restaurants vary a lot.",1.1. Background and motivation,0,[0]
"Other practical applications are in areas such as social choice (Caplin & Nalebuff, 1991; Soufiani et al., 2013), web search and information retrieval (Radlinski & Joachims, 2007; Radlinski et al., 2008), crowdsourcing (Chen et al., 2013; gif), recommender systems (Baltrunas et al., 2010) and several others.
",1.1. Background and motivation,0,[0]
"These practical applications and the intrinsic theoretical interest, has led to significant work on the probabilistic version of maxing and ranking.",1.1. Background and motivation,0,[0]
Yet the most general model for which maxing can be done using near-linear comparisons is not known.,1.1. Background and motivation,0,[0]
"We consider the most general transitive model
1University of California, San Diego.",1.1. Background and motivation,0,[0]
"Correspondence to: Venkatadheeraj Pichapati <dheerajpv7@ucsd.edu>.
",1.1. Background and motivation,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1.1. Background and motivation,0,[0]
"Copyright 2018 by the author(s).
that guarantees the existence of maximum and show that under this model any maxing algorithm requires quadratic many comparisons.",1.1. Background and motivation,0,[0]
"We also consider a slightly more restrictive transitive model and propose a linear complexity maxing algorithm, making it the most general model known for which linear complexity maxing is possible.",1.1. Background and motivation,0,[0]
"Also, for the most general known model with sub-quadratic complexity for ranking, we improve the complexity, making it orderwise optimal.",1.1. Background and motivation,0,[0]
We also propose an optimal algorithm that can simulate all pairwise comparisons.,1.1. Background and motivation,0,[0]
"Without loss of generality, let [n] def= {1, 2, ..., n} be the set of n elements.",1.2. Notation and problem formulation,0,[0]
"We consider probabilistic noisy comparisons i.e., whenever two elements i and j are compared, i is returned with an unknown probability pi,j .",1.2. Notation and problem formulation,0,[0]
"There are no “ties” i.e., pj,i = 1 − pi,j .",1.2. Notation and problem formulation,0,[0]
"Let p̃i,j def = pi,j",1.2. Notation and problem formulation,0,[0]
"− 12 be the centered preference probability.
",1.2. Notation and problem formulation,0,[0]
"A maximal is an element i that is preferable to every other element i.e., p̃i,j ≥ 0 ∀j.",1.2. Notation and problem formulation,0,[0]
"A ranking is a permutation σ1, σ2, ..., σn of [n] such that p̃σi,σj ≥ 0",1.2. Notation and problem formulation,0,[0]
"whenever i > j.
But sometimes maximal and ranking might not even exist.",1.2. Notation and problem formulation,0,[0]
"For example, consider the popular Rock-Paper-Scissor game i.e., p1,2 = p2,3 = p3,1 = 1.",1.2. Notation and problem formulation,0,[0]
Notice that under this model there is neither a maximal nor a ranking.,1.2. Notation and problem formulation,0,[0]
"Hence we need additional constraints on pairwise probabilities pi,j .
Notice that for ranking to exist, there must exist an ordering ( ) among elements s.t.",1.2. Notation and problem formulation,0,[0]
"whenever i j, p̃i,j ≥ 0.",1.2. Notation and problem formulation,0,[0]
The models that have such an ordering are said to satisfy Weak Stochastic Transitivity (WST).,1.2. Notation and problem formulation,0,[0]
"Observe that WST is sufficient for existence of both maximal and ranking.
",1.2. Notation and problem formulation,0,[0]
More restrictive notions of transitivity are motivated and used in different contexts.,1.2. Notation and problem formulation,0,[0]
"Strong Stochastic Transitivity (SST) which assumes that whenever i j k, p̃i,k ≥",1.2. Notation and problem formulation,0,[0]
"max(p̃i,j , p̃j,k), as its name suggests is a stronger notion of transitivity that confines the model more than WST, hence less general.",1.2. Notation and problem formulation,0,[0]
"Medium Stochastic Transitivity (MST) (Skorepa, 2010) sitting in between WST and SST, assumes that whenever i j k, p̃i,k ≥",1.2. Notation and problem formulation,0,[0]
"min(p̃i,j , p̃j,k).",1.2. Notation and problem formulation,0,[0]
"From WST to MST to SST, the model becomes more restrictive.
",1.2. Notation and problem formulation,0,[0]
"Another model restriction used in some of the previous
works Stochastic Triangle Inequality (STI), assumes that whenever i j k, p̃i,k ≤ p̃i,j + p̃j,k.",1.2. Notation and problem formulation,0,[0]
"In this paper we propose maxing and ranking algorithms for models under various set of constraints.
",1.2. Notation and problem formulation,0,[0]
There is also a concern with finding an exact maximal and ranking.,1.2. Notation and problem formulation,0,[0]
"Consider the case of n = 2 and p̃1,2 ≈ 0.",1.2. Notation and problem formulation,0,[0]
"Notice that in this case where n is just 2, finding maximal and ranking could take arbitrarily many comparisons.",1.2. Notation and problem formulation,0,[0]
"Easy fix to alleviate this problem is to consider Probably Approximately Correct (PAC) formulation which we also adopt.
",1.2. Notation and problem formulation,0,[0]
"An element i is said to be -preferable to j if p̃i,j ≥ − .",1.2. Notation and problem formulation,0,[0]
"For ∈ (0, 1/2), an -maximal is an element i that is - preferable to all elements i.e., p̃i,j ≥ − ∀j.",1.2. Notation and problem formulation,0,[0]
"Given 0 < < 1/2, 0 < δ ≤ 1/2, a PAC maxing algorithm must output an -maximal with probability≥ 1−δ.",1.2. Notation and problem formulation,0,[0]
"Similarly, an -ranking is a permutation σ1, σ2, ..., σn of [n] such that σi is -preferable to σj whenever i > j.",1.2. Notation and problem formulation,0,[0]
"Given 0 < < 1/2, 0 < δ ≤ 1/2, a PAC ranking algorithm must output an -ranking with probability ≥ 1− δ.",1.2. Notation and problem formulation,0,[0]
Researchers initially considered more restrictive models.,1.3. Related work,0,[0]
"(Feige et al., 1994) considered constant noise model i.e., p̃i,j = α > 0",1.3. Related work,0,[0]
if i j and presented a maxing algorithm,1.3. Related work,0,[0]
that usesO ( n α2 log 1 δ ) comparisons and outputs maximal with probability ≥ 1 − δ.,1.3. Related work,0,[0]
"It also presented a ranking algorithm that uses O ( n logn α2 ) comparisons and outputs ranking with probability ≥ 1− 1/n.
Another set of widely-studied restrictive models are parametric ones.",1.3. Related work,0,[0]
"(Szörényi et al., 2015) considered one of the most popular parametric models, Plackett-Luce (Plackett, 1975; Luce, 2005) and presented PAC maxing and ranking algorithms that useO ( n 2 log n δ ) andO",1.3. Related work,0,[0]
"( n logn 2 log n δ
) comparisons respectively.
",1.3. Related work,0,[0]
"Researchers also considered models that are more general than parametric models, yet still more restrictive than WST.",1.3. Related work,0,[0]
"(Yue & Joachims, 2011) considered models that satisfy both SST and STI and derived a PAC maxing algorithm that uses O ( n 2 log n δ ) comparisons.",1.3. Related work,0,[0]
"Later (Falahatgar et al., 2017b) considered same model and proposed an optimal PAC maxing algorithm that uses O ( n 2 log 1 δ
) comparisons.",1.3. Related work,0,[0]
"It also proposed a PAC ranking algorithm that with probability ≥ 1 − 1/n, outputs an -ranking using O ( n logn(log logn)3
2
) comparisons, (log log n)3 times
the known lower bound.",1.3. Related work,0,[0]
"Until now, it was not known if the additional (log log n)3 factor is necessary for PAC ranking.
",1.3. Related work,0,[0]
"(Falahatgar et al., 2017a) considered models that satisfy only SST but not necessarily STI and proposed an optimal PAC maxing algorithm that uses O ( n 2 log 1 δ ) compar-
isons.",1.3. Related work,0,[0]
"They also showed that there exists a model which satisfies SST and yet no algorithm can find an -ranking for this model using o(n2) comparisons, establishing a lower bound of Ω(n2) comparisons once STI property is dropped.
",1.3. Related work,0,[0]
"Among other related works we can point out (BusaFekete et al., 2014b; Lee et al., 2014; Dudı́k et al., 2015; Hüllermeier et al., 2008), who considered models more general than WST under different definitions of maximum and ranking.",1.3. Related work,0,[0]
"More discussion about these models can be found in Appendix G. (Busa-Fekete et al., 2014a; Mohajer et al., 2017) considered the non-PAC version and (Rajkumar & Agarwal, 2014; Negahban et al., 2012; 2016; Jang et al., 2016) considered the non-adaptive version of this problem.",1.3. Related work,0,[0]
"Also (Acharya et al., 2016; Ajtai et al., 2015) considered the deterministic adversarial version of maxing and ranking.",1.3. Related work,0,[0]
"(Shah et al., 2016b; Chatterjee et al., 2015; Shah et al., 2016a) studied the problem of estimating pairwise probabilities in non-adaptive setting.",1.3. Related work,0,[0]
"Maxing Linear-complexity maxing algorithm under SST by (Falahatgar et al., 2017a) encourages the search for a linear-complexity maxing algorithm for models with only WST properties.",2. New results and Outline,0,[0]
Two questions then arise: 1a) Is a linear complexity PAC maxing algorithm possible for models with only WST property?,2. New results and Outline,0,[0]
1b),2. New results and Outline,0,[0]
"If not, does there exist a model more general than SST and less general than WST for which a linear complexity PAC maxing is possible?
",2. New results and Outline,0,[0]
We resolve both questions in this paper: 1a),2. New results and Outline,0,[0]
No.,2. New results and Outline,0,[0]
Theorem 1 in Section 3 shows that there are WST models for which any PAC maxing algorithm requires Ω(n2) comparisons.,2. New results and Outline,0,[0]
1b),2. New results and Outline,0,[0]
Yes.,2. New results and Outline,0,[0]
"In Theorem 8 in Section 4, we derive a PAC maxing algorithm for MST model that uses O ( n 2 log 1 δ
) comparisons for δ ≥ min(1/n, e−n1/4).
",2. New results and Outline,0,[0]
"Ranking Motivated by the previous results of ranking under SST + STI, three questions arise: 2a) For models with SST + STI, is the additional (log log n)3 factor necessary for PAC ranking algorithms?",2. New results and Outline,0,[0]
2b),2. New results and Outline,0,[0]
"Since the nearlinear complexity of ranking under SST + STI changes to quadratic complexity by dropping STI (Falahatgar et al., 2017a), is there a sub-quadratic algorithm for ranking under MST + STI? 2c) For models with SST + STI, since PAC ranking is possible with near linear complexity, is it also possible to approximate all pairwise probabilities to accuracy of using near linear number of comparisons?
",2. New results and Outline,0,[0]
We essentially resolve all three questions.,2. New results and Outline,0,[0]
2a),2. New results and Outline,0,[0]
No.,2. New results and Outline,0,[0]
"In Theorem 9 in Section 5, we improve the PAC ranking algorithm for models with SST + STI removing additional (log log n)3 factor and hence making it optimal.",2. New results and Outline,0,[0]
2b),2. New results and Outline,0,[0]
No.,2. New results and Outline,0,[0]
"Theorem 10 in Section 6 shows that there is a model with MST+STI, for which any PAC ranking algorithm requires
Ω(n2) comparisons.",2. New results and Outline,0,[0]
2c),2. New results and Outline,0,[0]
Yes.,2. New results and Outline,0,[0]
"For models with SST + STI, in Theorems 11 and 12 in Sections 7, we present an optimal algorithm that uses O ( nmin(n,1/ ) logn
2
) comparisons
and approximates all pairwise probabilities to accuracy of with probability ≥ 1− 1/n.
We present experiments over simulated data in Section 8 and end with our conclusions in Section 9.
",2. New results and Outline,0,[0]
"Interpretation Table 1 summarizes all known results for problems of maxing, ranking, and finding pairwise probabilities under different transitive properties.",2. New results and Outline,0,[0]
"Notice that under the most general model WST, all these problems require quadratic many comparisons and under the most restrictive model SST + STI, all problems have optimal algorithms with near-linear complexity.",2. New results and Outline,0,[0]
For MST and WST models adding STI property does not influence complexity for any problem.,2. New results and Outline,0,[0]
"But for SST model adding STI property facilitates near-linear complexity algorithms for PAC ranking and approximating pairwise probabilities.
",2. New results and Outline,0,[0]
"It is easy to see that once all pairwise probabilities are approximated to accuracy of /2, one can find an -maximum and an -ranking.",2. New results and Outline,0,[0]
Hence approximating pairwise probabilities is harder than PAC ranking and lower bound for PAC ranking implies a lower bound for problem of approximating pairwise probabilities.,2. New results and Outline,0,[0]
Therefore in Table 1 lower bounds for finding pij follow from lower bounds for ranking.,2. New results and Outline,0,[0]
"Further in Appendix B.1, under WST model, we present a trivial algorithm that with probability ≥ 1 − δ, estimates all pairwise probabilities to accuracy of using O ( n2
2 log n δ
) comparisons.",2. New results and Outline,0,[0]
"Hence upper bound of
O ( n2
2 log n δ
) follows for all problems.",2. New results and Outline,0,[0]
"We show the lower bound of Ω(n2) for maxing under WST by presenting an example for which any algorithm requires
Ω(n2) comparisons to output a 1/4-maximum for δ ≤ 1/8.
",3. PAC maxing for WST,0,[0]
"To establish the lower bound, we reduce the problem of finding a 1/4-maximum to finding the left most piece of a linear jigsaw puzzle.",3. PAC maxing for WST,0,[0]
"We consider the following model with n elements S = {a1, a2, . . .",3. PAC maxing for WST,0,[0]
", an} : p̃ai,ai+1 =",3. PAC maxing for WST,0,[0]
"12∀i < n, and p̃ai,aj = µ(0 < µ < 1/n
",3. PAC maxing for WST,0,[0]
"10),∀j >",3. PAC maxing for WST,0,[0]
i + 1.,3. PAC maxing for WST,0,[0]
"This model satisfies WST since there exists an underlying order , ai aj if i < j",3. PAC maxing for WST,0,[0]
"(because p̃ai,aj > 0) and a1 is the only 1/4-maximum under this model.
",3. PAC maxing for WST,0,[0]
"Observe that ai is always preferred to ai+1, but for every non consecutive pair, comparison output is almost a fair coin flip.",3. PAC maxing for WST,0,[0]
We make the problem simpler by giving the extra information of whether two non consecutive elements are being compared.,3. PAC maxing for WST,0,[0]
"Notice that this only makes the problem easier, namely, complexity for modified problem is smaller than that of original problem.
",3. PAC maxing for WST,0,[0]
"The modified problem is similar to a linear jigsaw puzzle where if we compare two pieces we will know if pieces are adjacent or not and if adjacent, which piece is on the left, the goal is to find the left most piece.",3. PAC maxing for WST,0,[0]
"We show that w.h.p., any algorithm neither finds more than n/32 connections (a set of neighbors) nor asks Ω(n) comparisons for the left most piece.",3. PAC maxing for WST,0,[0]
We use this to show the lower bound.,3. PAC maxing for WST,0,[0]
"The proof is in Appendix A.
Theorem 1.",3. PAC maxing for WST,0,[0]
There exists a model that satisfies WST for which any algorithm requires Ω(n2) comparisons to find a 1/4-maximum with probability ≥ 7/8.,3. PAC maxing for WST,0,[0]
"Outline In this section, we propose OPT-MAX, a linear complexity maxing algorithm for MST.",4. PAC maxing for MST,0,[0]
"In the process, we present two other suboptimal maxing algorithms SOFTSEQ-ELIM, NEAR-OPT-MAX and use them as building blocks in OPT-MAX.",4. PAC maxing for MST,0,[0]
SOFT-SEQ-ELIM finds an - maximum with quadratic complexity.,4. PAC maxing for MST,0,[0]
Its performance depends on the starting element (anchor).,4. PAC maxing for MST,0,[0]
"NEAR-OPT-MAX
first finds a good anchor and then uses SOFT-SEQ-ELIM, guaranteeing near linear comparison complexity.",4. PAC maxing for MST,0,[0]
"OPTMAX builds on NEAR-OPT-MAX and finds an -maximum in linear-complexity for δ ≥ min(1/n, e−n1/4).",4. PAC maxing for MST,0,[0]
"Before presenting SOFT-SEQ-ELIM, we first present the subroutine COMPARE we use to compare two elements.
",4.1. SOFT-SEQ-ELIM,0,[0]
"COMPARE COMPARE takes 5 parameters : two elements i, j that need to be compared, lower bias l, upper bias u, confidence δ and deems if p̃i,j < l or p̃i,j > u.",4.1. SOFT-SEQ-ELIM,0,[0]
It compares i and j for 8( u− l)2 log 2 δ times.,4.1. SOFT-SEQ-ELIM,0,[0]
"Let p̂i,j be the fraction of times i won and ˆ̃pi,j = p̂i,j − 1/2.",4.1. SOFT-SEQ-ELIM,0,[0]
"If ˆ̃pi,j < 3 l 4 + u 4 , then COMPARE deems p̃i,j < l (returns 1), if ˆ̃pi,j >",4.1. SOFT-SEQ-ELIM,0,[0]
"l 4 + 3 u 4 , then COMPARE deems p̃i,j > u (returns 3) and for other ranges of ˆ̃pi,j , COMPARE not able to take a decision, returns 2.
",4.1. SOFT-SEQ-ELIM,0,[0]
Lemma 2 bounds comparisons used by COMPARE and proves its correctness.,4.1. SOFT-SEQ-ELIM,0,[0]
"COMPARE and its analysis is presented in Appendix C.2.
",4.1. SOFT-SEQ-ELIM,0,[0]
Lemma 2.,4.1. SOFT-SEQ-ELIM,0,[0]
"For u > l, COMPARE(i, j, l, u, δ) uses ≤ 8 ( u− l)2 log 2 δ comparisons and if p̃i,j < l, then w.p.≥ 1− δ, it returns 1, else if p̃i,j > u, w.p.≥ 1− δ, it returns 3.",4.1. SOFT-SEQ-ELIM,0,[0]
"Further if p̃i,j ≤ ( l + u)/2, w.p.≥ 1 − δ, it does not return 3 and if p̃i,j >",4.1. SOFT-SEQ-ELIM,0,[0]
"( l + u)/2, w.p.≥ 1− δ, it does not return 1.
SOFT-SEQ-ELIM SOFT-SEQ-ELIM takes 5 parameters: input set S, starting anchor element r, lower bias l, upper bias u and confidence δ.",4.1. SOFT-SEQ-ELIM,0,[0]
SOFT-SEQ-ELIM happens in rounds.,4.1. SOFT-SEQ-ELIM,0,[0]
"In each round, it compares the current anchor a with remaining elements one by one using COMPARE.",4.1. SOFT-SEQ-ELIM,0,[0]
"Due to probabilistic nature, we cannot exactly compare if p̃e,a > u vs p̃e,a ≤ u.",4.1. SOFT-SEQ-ELIM,0,[0]
"Hence we compare if p̃e,a > u vs p̃e,a < l. For an element e, if COMPARE deems p̃e,a < l, then SOFT-SEQ-ELIM eliminates that element and if COMPARE deems p̃e,a > u, then SOFT-SEQ-ELIM updates current anchor element to e and eliminates a.",4.1. SOFT-SEQ-ELIM,0,[0]
"This process is continued until the current anchor element is not updated after comparing with all remaining elements and then SOFT-SEQ-ELIM outputs final anchor element.
",4.1. SOFT-SEQ-ELIM,0,[0]
"If p̃e,a < l or p̃e,a > u, COMPARE deems correctly.",4.1. SOFT-SEQ-ELIM,0,[0]
"If l ≤ p̃e,a ≤ u, then COMPARE can sometimes fail to output any decision and in that case, SOFT-SEQ-ELIM neither eliminates that element nor updates the anchor element, it just moves to next remaining element in S.
Theoretically, performance of SOFT-SEQ-ELIM strongly depends on the starting anchor element r. To define a good anchor element, similar to (Falahatgar et al., 2017a), an element a is called an ( ,m)-good anchor if a is
Algorithm 1 SOFT-SEQ-ELIM 1: inputs 2: Set S, element r, lower bias l, upper bias u, confi-
dence δ 3: Q = S \ {r} 4: while Q 6= ∅",4.1. SOFT-SEQ-ELIM,0,[0]
"do 5: r′ = r, Q′ = ∅ 6: for c ∈ Q do 7: k = COMPARE(c, r, l, u, 2δ|S|2 ) 8: if k == 1 then 9: Q′ = Q′",4.1. SOFT-SEQ-ELIM,0,[0]
"⋃ {c}.
10: else if k == 3 then 11: r ← c 12: Q′ = Q′",4.1. SOFT-SEQ-ELIM,0,[0]
"⋃ {c} 13: break 14: end if 15: end for 16: if r == r′ then 17: break 18: end if 19: Q = Q \Q′ 20: end while 21: return r
not -preferable to at most m elements, i.e., |{e : e ∈ S and p̃e,a > }| ≤ m.",4.1. SOFT-SEQ-ELIM,0,[0]
We show that every element for which initial anchor r is l-preferable is deemed bad and gets eliminated after its first comparison round and hence comparisons spent on all such elements is O(|S|).,4.1. SOFT-SEQ-ELIM,0,[0]
"Since initial anchor r is an ( l,m)-good anchor element, there are only m elements for which r is not l-preferable.",4.1. SOFT-SEQ-ELIM,0,[0]
"We later show that only these elements can become anchors, leading to at most m changes of anchors.",4.1. SOFT-SEQ-ELIM,0,[0]
Therefore each such element gets compared in at mostm rounds and hence we can bound total comparison rounds by O(|S| + m2).,4.1. SOFT-SEQ-ELIM,0,[0]
Lemma 3 bounds comparisons used by SOFT-SEQ-ELIM and proves its correctness.,4.1. SOFT-SEQ-ELIM,0,[0]
"Proof is in Appendix C.3.
",4.1. SOFT-SEQ-ELIM,0,[0]
Lemma 3.,4.1. SOFT-SEQ-ELIM,0,[0]
"If r is an ( l,m)-good anchor element, w.p.≥ 1 − δ, SOFT-SEQ-ELIM(S, r, l, u, δ) uses O ( |S|+m2 ( u− l)2 log |S| δ ) comparisons and outputs r̂, an u maximum of S, such that either r̂ = r or p̃r̂,r >",4.1. SOFT-SEQ-ELIM,0,[0]
"l+ u2 .
",4.1. SOFT-SEQ-ELIM,0,[0]
Corollary 4 bounds comparisons used by SOFT-SEQ-ELIM for any starting anchor.,4.1. SOFT-SEQ-ELIM,0,[0]
"Proof follows from Lemma 3
Corollary 4.",4.1. SOFT-SEQ-ELIM,0,[0]
"For any r, w.p.≥ 1 − δ, SOFT-SEQ-ELIM(S, r, l, u, δ) uses O
( |S|2
( u− l)2 log |S| δ ) comparisons and outputs r̂, an u maximum of S, such that either r̂ = r or p̃r̂,r >",4.1. SOFT-SEQ-ELIM,0,[0]
"l+ u2 .
",4.1. SOFT-SEQ-ELIM,0,[0]
Now we build on SOFT-SEQ-ELIM and propose a near linear algorithm NEAR-OPT-MAX.,4.1. SOFT-SEQ-ELIM,0,[0]
"NEAR-OPT-MAX(S, , δ) w.p.≥ 1 − δ, uses O ( |S| 2 ( log |S|δ )2) comparisons and outputs an -
maximum of S.
Since complexity of SOFT-SEQ-ELIM depends on the initial anchor element, if we can pick a good initial anchor element, then we can reduce the number of comparisons.",4.2. NEAR-OPT-MAX,0,[0]
"One way to pick a good initial anchor element is to find an /2-maximum of a randomly picked subset.
",4.2. NEAR-OPT-MAX,0,[0]
Lemma 5 shows that an -maximum of a randomly picked subset is a good anchor element.,4.2. NEAR-OPT-MAX,0,[0]
"Proof in Appendix C.4.
",4.2. NEAR-OPT-MAX,0,[0]
Lemma 5.,4.2. NEAR-OPT-MAX,0,[0]
"If r is an -maximum of a set Q, formed by picking m elements randomly from S, then w.p.≥ 1 − δ, r is an ( , |S|m log |S| δ )",4.2. NEAR-OPT-MAX,0,[0]
"-good anchor element of S.
NEAR-OPT-MAX(S, , δ) first picks a random subset Q of size √ |S| log 4|S|δ and uses SOFT-SEQ-ELIM to find an
/2-maximum of Q.
By Lemma 5, w.p.≥ 1 − δ/4, an /2-maximum of Q will be an ( /2, √ |S| log 4|S|δ )-good anchor element.",4.2. NEAR-OPT-MAX,0,[0]
"NEAROPT-MAX then uses SOFT-SEQ-ELIM with /2-maximum of Q as initial anchor to find an -maximum of S. Since the initial anchor is provably good, we are able to bound the comparisons.
",4.2. NEAR-OPT-MAX,0,[0]
"Algorithm 2 NEAR-OPT-MAX 1: inputs 2: Set S, bias , confidence δ
3: Form a set Q by selecting √ |S| log 4|S|δ random ele-
ments from S without replacement.",4.2. NEAR-OPT-MAX,0,[0]
"4: a← random element from Q, Q = Q \ {a} 5: r ← SOFT-SEQ-ELIM ( Q, a, 0, 2 , δ 4 ) , S = S \ {r} 6: return SOFT-SEQ-ELIM(S, r, /2, , δ/2)
",4.2. NEAR-OPT-MAX,0,[0]
"Lemma 6 bounds the comparisons used by NEAR-OPT-MAX and proves its correctness.
",4.2. NEAR-OPT-MAX,0,[0]
Lemma 6.,4.2. NEAR-OPT-MAX,0,[0]
"With probability ≥ 1 − δ, NEAR-OPT-MAX(S, , δ) uses O ( |S| 2 ( log |S|δ )2) comparisons and outputs an -maximum of S.
We build on NEAR-OPT-MAX and derive an optimal algorithm for δ ≥ min(1/|S|, e−|S|1/4).",4.2. NEAR-OPT-MAX,0,[0]
"We first present an algorithm that is optimal for low ranges of δ i.e., min(e−|S| 1/4
, 1/|S|) ≤ δ ≤ 1|S|1/3 .
",4.3. Optimal linear Algorithm,0,[0]
4.3.1.,4.3. Optimal linear Algorithm,0,[0]
"LOW RANGES OF δ
We first find a good anchor, this time using NEAR-OPTMAX and then use SOFT-SEQ-ELIM with NEAR-OPTMAX output as initial anchor.
",4.3. Optimal linear Algorithm,0,[0]
OPT-MAX-LOW picks a random subset of size |S|3/4 and finds an /2-maximum of this set using NEAR-OPT-MAX.,4.3. Optimal linear Algorithm,0,[0]
"We later show that output is an ( /2,O( √ |S|))-good anchor element of S. OPT-MAX-LOW then uses SOFT-SEQELIM with the previous output as initial anchor to find an -maximum of S. Since initial anchor is good, we are able to bound comparisons used by OPT-MAX-LOW.
",4.3. Optimal linear Algorithm,0,[0]
"Observe that in OPT-MAX-LOW, we call SOFT-SEQ-ELIM three times in total: two times during NEAR-OPT-MAX and once to produce the final output.",4.3. Optimal linear Algorithm,0,[0]
"Each successive call of SOFT-SEQ-ELIM acts on higher size, namely first we find /4-maximum in a small set and using this element as anchor, then we find /2-maximum in a larger set and finally using this new element as anchor, we find an - maximum of the whole set S.
Algorithm 3 OPT-MAX-LOW 1: inputs 2: Set S, bias , confidence δ 3: Form a set Q by selecting |S|3/4 random elements
from S without replacement 4: r ← NEAR-OPT-MAX(Q, 2 , δ 3 ) 5: return SOFT-SEQ-ELIM(S, r, 2 , , δ 3 )
",4.3. Optimal linear Algorithm,0,[0]
Lemma 7 bounds comparisons used by OPT-MAX-LOW and proves its correctness.,4.3. Optimal linear Algorithm,0,[0]
"Proof is in Appendix C.6.
",4.3. Optimal linear Algorithm,0,[0]
Lemma 7.,4.3. Optimal linear Algorithm,0,[0]
For 1|S|1/3 ≥ δ ≥,4.3. Optimal linear Algorithm,0,[0]
"min(1/|S|, e −|S|1/4), w.p.≥ 1− δ, OPT-MAX-LOW(S, , δ) uses O( |S| 2 log 1 δ ) comparisons and outputs r, an -maximum
4.3.2.",4.3. Optimal linear Algorithm,0,[0]
HIGHER RANGES OF CONFIDENCE δ,4.3. Optimal linear Algorithm,0,[0]
For low ranges of confidence δ,4.3. Optimal linear Algorithm,0,[0]
"( δ ≤ 1|S|1/3 ) , notice that log 1δ and log |S| δ are of same order and hence if we use SOFT-SEQ-ELIM with a good anchor, we can guarantee complexity of O ( |S| 2 log |S| δ ) =",4.3. Optimal linear Algorithm,0,[0]
"O ( |S| 2 log 1 δ ) .
",4.3. Optimal linear Algorithm,0,[0]
"However, for high values of δ, this is not the case.",4.3. Optimal linear Algorithm,0,[0]
We solve this problem by pruning S to a smaller set of size |S|/ log |S| such that it contains all good elements and then use SOFT-SEQ-ELIM.,4.3. Optimal linear Algorithm,0,[0]
"Due to space constraint, we present PRUNE, the pruning algorithm, OPT-MAX-MEDIUM, and OPT-MAX-HIGH, linear complexity maxing algorithms for higher ranges of confidence in Appendix C.8.",4.3. Optimal linear Algorithm,0,[0]
In Theorem 8 we bound comparisons used by OPT-MAX and prove its correctness.,4.4. Full Algorithm,0,[0]
"Proof follows from Lemmas 7
Algorithm 4 OPT-MAX inputs
Set S, bias , confidence δ",4.4. Full Algorithm,0,[0]
"if δ ≤ 1|S|1/3 then
return OPT-MAX-LOW(S, , δ) end if if δ ≤",4.4. Full Algorithm,0,[0]
"1log |S| then
return OPT-MAX-MEDIUM(S, , δ) end if return OPT-MAX-HIGH(S, , δ)
and corresponding Lemmas 19 and 20 for OPT-MAXMEDIUM and OPT-MAX-HIGH given in Appendix C.8.
Theorem 8.",4.4. Full Algorithm,0,[0]
"For δ ≥ min(1/|S|, e−|S|1/4), w.p.≥ 1 − δ, OPT-MAX(S, , δ) uses O ( |S| 2 log 1 δ ) comparisons and outputs an -maximum of S.",4.4. Full Algorithm,0,[0]
"(Falahatgar et al., 2017b) provides a ranking algorithm that w.p.≥ 1 − 1/|S|, uses O ( |S| 2 log |S|(log log |S|) 3 )
comparisons and outputs an -ranking of input set S.
We build on their algorithm BINARY-SEARCH-RANKING, improving two components which lead to additional (log log |S|)3 factor, thereby proposing an optimal - ranking algorithm that uses O ( |S| 2 log |S| ) comparisons.
",5. Ranking for SST+STI,0,[0]
"In Appendix 5, we outline the algorithm proposed in (Falahatgar et al., 2017b), pointing out the two components that lead to additional factor, and present ideas that improve over these components.",5. Ranking for SST+STI,0,[0]
"For detailed explanation of BINARY-SEARCH-RANKING we refer readers to (Falahatgar et al., 2017b).",5. Ranking for SST+STI,0,[0]
"Now we explain the high-level idea of how we improve over these components.
",5. Ranking for SST+STI,0,[0]
"The two components that we improve upon share the property that each is being called for Ω(|S|/(log |S|)3) times and at each time finds a correct output w.p.≥ 1− 1/|S|5.
",5. Ranking for SST+STI,0,[0]
"Instead of finding a correct output w.p.≥ 1− 1/|S|5 in one shot, and incurring high complexity, we propose the following.",5. Ranking for SST+STI,0,[0]
"First use the component to find a correct output w.p.≥ 1 − 1/ log |S|, then check if the output is correct or not.",5. Ranking for SST+STI,0,[0]
"If the output is deemed to be not correct, run the component again, finding a correct output w.p.≥ 1− 1/|S|6.
",5. Ranking for SST+STI,0,[0]
"Thus to show the potency of this idea, it suffices to show: One, the second run is only invoked a few times and two, the complexity of checking whether an output is correct is not high.",5. Ranking for SST+STI,0,[0]
Our main contribution is RANK-CHECK algorithm that checks if an ordered set is -ranked or not 3 - ranked.,5. Ranking for SST+STI,0,[0]
"We present RANK-CHECK in Appendix D.3
Theorem 9.",5. Ranking for SST+STI,0,[0]
"BINARY-SEARCH-RANKING(S, ) (Falahatgar et al., 2017b) with new improved components presented here, w.p.≥ 1−1/|S|, usesO ( |S| log |S|
2
) comparisons and
outputs an -ranking of S.",5. Ranking for SST+STI,0,[0]
In this section we show that there exists a model with both MST and STI properties under which any PAC ranking algorithm requires quadratic many comparisons.,6. Lower bound for ranking for MST+STI,0,[0]
"Consider the model S = {a1, a2, ..., an} s.t.",6. Lower bound for ranking for MST+STI,0,[0]
"a1 is preferable to a2 i.e., p̃a1,a2 = 1/2 and comparison between any other pair is almost a fair coin flip i.e., p̃ai,aj = µ ∀i < j and {i, j} 6= {1, 2} for some µ < 1/n10.",6. Lower bound for ranking for MST+STI,0,[0]
This model satisfies both MST and STI.,6. Lower bound for ranking for MST+STI,0,[0]
Any permutation which has a1 coming after a2 is a 1/4-ranking.,6. Lower bound for ranking for MST+STI,0,[0]
"But since comparison between any pair other than (a1, a2) is essentially a fair coin toss, any strategy that does not compare a1 and a2 will not have them in correct order in the output w.p.≈ 1/2 and hence won’t be a 1/4-ranking.",6. Lower bound for ranking for MST+STI,0,[0]
"Therefore this problem is similar to finding a single biased coin among ( n 2 ) coins which needs Ω(n2) comparisons.
",6. Lower bound for ranking for MST+STI,0,[0]
Theorem 10 bounds the complexity required for -ranking of models with MST and STI.,6. Lower bound for ranking for MST+STI,0,[0]
"Proof is in Appendix E.
Theorem 10.",6. Lower bound for ranking for MST+STI,0,[0]
There exists a model with MST and STI properties for which any algorithm requires Ω(n2) comparisons to output a 1/4-ranking w.p.≥ 7/8.,6. Lower bound for ranking for MST+STI,0,[0]
"Theorem 9 shows that for a model satisfying both SST and STI, an -ranking can be found using O ( |S| log |S|
2
) com-
parisons.",7. Finding pairwise probabilities for SST+STI,0,[0]
"In this section we answer the question whether under same model we can approximate all pairwise probabilities to accuracy of using almost same complexity.
",7. Finding pairwise probabilities for SST+STI,0,[0]
"We first show a lower bound of Ω ( |S|min(|S|,1/ ) 2 log |S| )
utilizing a model for which Ω(|S|min(|S|, 1/ ))",7. Finding pairwise probabilities for SST+STI,0,[0]
pairwise probabilities need to be approximated using comparisons.,7. Finding pairwise probabilities for SST+STI,0,[0]
"Later we present APPROX-PROB that uses comparisons only for O(|S|min(|S|, 1/ ))",7. Finding pairwise probabilities for SST+STI,0,[0]
pairs and hence obtain orderwise same upper bound as lower bound.,7. Finding pairwise probabilities for SST+STI,0,[0]
"We show that any algorithm requires Ω ( |S|min(|S|,1/ ) log |S|
2
) comparisons to approximate
all pairwise probabilities to accuracy.
",7.1. Lower Bound,0,[0]
We prove the lower bound by using the model: (4k+4) ≤,7.1. Lower Bound,0,[0]
"p̃ai+k,ai ≤ (4k + 8) for 1 ≤ k ≤",7.1. Lower Bound,0,[0]
"min(n − i, b 116 − 2c) and p̃ai+k,ai = 1/4 for k > min(n−",7.1. Lower Bound,0,[0]
"i, b 116 − 2c).
",7.1. Lower Bound,0,[0]
"It can be shown that this model satisfies both SST and STI.
",7.1. Lower Bound,0,[0]
"Under this model, the only way to approximate unfixed pairwise probabilities is by comparing those pairs.",7.1. Lower Bound,0,[0]
"Since pairwise probabilities are not fixed for Ω(nmin(n, 1/ ))",7.1. Lower Bound,0,[0]
"pairs, any algorithm needs to approximate those many probabilities to accuracy of , hence the lower bound.
",7.1. Lower Bound,0,[0]
Theorem 11 bounds the required complexity to approximate all pairwise probabilities.,7.1. Lower Bound,0,[0]
Proof is in Appendix F.1 Theorem 11.,7.1. Lower Bound,0,[0]
"For < 1/48, there exists a model that satisfies both SST and STI for which any algorithm requires Ω ( |S|min(|S|,1/ ) 2 log |S| ) comparisons to approximate all
pairwise probabilities to accuracy w.p. ≥ 3/4.",7.1. Lower Bound,0,[0]
"Here we propose an algorithm to approximate all pairwise probabilities to an accuracy of .
",7.2. Upper Bound,0,[0]
"The proposed algorithm, first finds an /8-ranking of the input set S and then approximates pairwise probabilities.",7.2. Upper Bound,0,[0]
"By Theorem 9, w.p.≥ 1− 1|S|2 we can find an /8-ranking
of the input set S using O ( |S| log |S|
2
) comparisons.",7.2. Upper Bound,0,[0]
"We
present APPROX-PROB that given an /8-ranked set, approximates all pairwise probabilities to an accuracy of .
",7.2. Upper Bound,0,[0]
"APPROX-PROB APPROX-PROB takes an /8-ranked ordered set S i.e., p̃S(i),S(j) ≤ /8",7.2. Upper Bound,0,[0]
"∀i < j and bias and approximates all pairwise probabilities to an accuracy of .
",7.2. Upper Bound,0,[0]
"Note that it is enough to approximate p̃S(j),S(i) for j ≥ i since p̃S(i),S(j) = −p̃S(j),S(i).",7.2. Upper Bound,0,[0]
"For all i > 1, APPROXPROB compares S(i) and S(1), 16 log |S| 4
2 times and approximates p̃S(i),S(1) by ˆ̃pS(i),S(1), the fraction of times S(i) won rounded off to the nearest multiple of .",7.2. Upper Bound,0,[0]
"Since for perfectly ranked ordered set p̃S(i+1),S(1) ≥ p̃S(i),S(1), if ˆ̃pS(i+1),S(1) < ˆ̃pS(i),S(1), then APPROX-PROB corrects ˆ̃pS(i+1),S(1), setting it equal to ˆ̃pS(i),S(1).",7.2. Upper Bound,0,[0]
"It can be shown that p̃S(i),S(1) is approximated to an accuracy of 7 8 .
",7.2. Upper Bound,0,[0]
"APPROX-PROB continues this process by approximating p̃S(i),S(2) for i ≥ 2 by increasing i one at a time.",7.2. Upper Bound,0,[0]
"For a perfectly ranked set, p̃S(i−1),S(2) ≤ p̃S(i),S(2) ≤ p̃S(i),S(1) and hence if ˆ̃pS(i−1),S(2) = p̃S(i),S(1), APPROXPROB does not use comparisons to approximate p̃S(i),S(2), instead assigns ˆ̃pS(i),S(2) = ˆ̃pS(i−1),S(2).",7.2. Upper Bound,0,[0]
"Whenever ˆ̃pS(i−1),S(2) 6= p̃S(i),S(1), APPROX-PROB approximates p̃S(i),S(2) by comparing S(i) and S(2).",7.2. Upper Bound,0,[0]
"It can be shown that p̃S(i),S(2) is approximated to accuracy of .
",7.2. Upper Bound,0,[0]
"APPROX-PROB continues this process for S(3), then S(4) and so on until S(n).",7.2. Upper Bound,0,[0]
"Notice that whenever ˆ̃pS(i−1),S(j) = ˆ̃pS(i),S(j−1), APPROX-PROB does not use comparisons to approximate p̃S(i),S(j) but simply assigns ˆ̃pS(i),S(j) = ˆ̃pS(i−1),S(j).",7.2. Upper Bound,0,[0]
"We show this in fact happens at many places
and only O(|S|min(|S|, 1/ ))",7.2. Upper Bound,0,[0]
pairwise probabilities are approximated using comparisons.,7.2. Upper Bound,0,[0]
"This enables obtaining orderwise same upper bound as the lower bound.
",7.2. Upper Bound,0,[0]
"Algorithm 5 APPROX-PROB 1: inputs 2: Ordered Set S, bias 3: ˆ̃pS(1),S(1) = 0 4: for i from 2 to |S| do 5: Compare S(1) and S(i) for 16 2 log |S| 4 times
6: ˆ̃pS(i),S(1) =",7.2. Upper Bound,0,[0]
"[ fraction of times S(i) won − 1 2 ] 7: if ˆ̃pS(i),S(1) < ˆ̃pS(i−1),S(1) then 8: ˆ̃pS(i),S(1) = ˆ̃pS(i−1),S(1) 9: end if
10: end for 11: for j from 2 to |S| do 12: ˆ̃pS(j),S(j) = 0 13: for k from j + 1 to |S| do 14: if ˆ̃pS(k−1),S(j) = ˆ̃pS(k),S(j−1) then 15: ˆ̃pS(k),S(j) = ˆ̃pS(k−1),S(j) 16: else 17: Compare S(j) and S(k) for 16 2 log |S| 4 times
18: ˆ̃pS(k),S(j) =",7.2. Upper Bound,0,[0]
"[ fraction of times S(k) won − 1 2 ] 19: end if 20: end for 21: end for
Theorem 12 shows the correctness of APPROX-PROB and bounds its comparisons.",7.2. Upper Bound,0,[0]
"Proof is in Appendix F.3
Theorem 12.",7.2. Upper Bound,0,[0]
"Given an /8-ranked ordered set S i.e., p̃S(i),S(j) ≤ /8",7.2. Upper Bound,0,[0]
"∀i < j, APPROX-PROB(S, ) uses O( |S|min(|S|,1/ ) 2 log |S|) comparisons and w.p.≥ 1− 1 |S|2 approximates all pairwise probabilities to accuracy of .",7.2. Upper Bound,0,[0]
"In this section, we compare the performance of our maxing algorithms with previous work on synthetic data.",8. Experiments,0,[0]
"All results presented here are averaged over 1000 runs.
",8. Experiments,0,[0]
"We compare our maxing algorithms SOFT-SEQELIM, NEAR-OPT-MAX, and OPT-MAX with SEQELIMINATE (Falahatgar et al., 2017a), KNOCKOUT (Falahatgar et al., 2017b), MallowsMPI (BusaFekete et al., 2014a), AR (Heckel et al., 2016) and BTM-PAC (Yue & Joachims, 2011).",8. Experiments,0,[0]
KNOCKOUT and BTM-PAC are PAC maxing algorithms for models with both SST and STI properties.,8. Experiments,0,[0]
SEQ-ELIMINATE is a PAC maxing algorithm for SST model.,8. Experiments,0,[0]
"MallowsMPI, originally designed for Mallows model, finds a condorcet winner which exists under WST.",8. Experiments,0,[0]
"AR is a maxing algorithm that finds Borda winner that is same as condorcet winner
under WST.",8. Experiments,0,[0]
"In all experiments, we use maxing algorithms to find a 0.05-maximum with δ = 0.1.
",8. Experiments,0,[0]
"We first consider the model pi,j = 0.6 ∀i < j same as in (Yue & Joachims, 2011; Falahatgar et al., 2017b;a) that satisfies both SST and STI properties.",8. Experiments,0,[0]
Note that i = 1 is the only 0.05-maximum under this model.,8. Experiments,0,[0]
Figure 1 presents number of comparisons used by each maxing algorithm.,8. Experiments,0,[0]
"Observe that compared to other algorithms, BTMPAC uses too many comparisons even for n = 15.",8. Experiments,0,[0]
The reason might be BTM-PAC is mainly intended for reducing regret in the conventional bandits setting.,8. Experiments,0,[0]
The bar for BTM-PAC complexity for n = 100 is not fully shown in the figure to better scale the other complexity bars.,8. Experiments,0,[0]
Comparison complexity of AR is high for n = 100 mainly because AR eliminates elements based on Borda scores and Borda scores are very close to each other for large n.,8. Experiments,0,[0]
"We drop BTM-PAC and AR henceforth.
",8. Experiments,0,[0]
"Now we consider a model that satisfies MST but not SST, i.e., p5i+l,5i+k = 0.6 ∀i < n/5",8. Experiments,0,[0]
"− 1, 1 ≤",8. Experiments,0,[0]
"l < k ≤ 5 and p5i+l,5j+k = 0.52 ∀i",8. Experiments,0,[0]
< j < n/5,8. Experiments,0,[0]
"− 1, 0 < l, k ≤ 5.",8. Experiments,0,[0]
"Notice that under this model elements are divided into groups of five where within each group |p̃i,j | = 0.1 and for elements in two different groups |p̃i,j | = 0.02, hence there is a 0.05-maximum in each group.",8. Experiments,0,[0]
Figure 2 demonstrates comparison complexity of algorithms under this model.,8. Experiments,0,[0]
"SEQELIMINATE uses fewer comparisons, but it fails to output a 0.05-maximum with probability 0.21 for n = 25 and 0.19 for n = 100.",8. Experiments,0,[0]
Hence SEQ-ELIMINATE fails once SST is not satisfied.,8. Experiments,0,[0]
"This is because when you compare a 0.05-maximum of a group with an element in other group, 0.05-maximum can get eliminated with probability ≈ 0.5.",8. Experiments,0,[0]
Hence with lots of groups SEQ-ELIMINATE fails.,8. Experiments,0,[0]
Other algorithms find a 0.05-maximum in all runs.,8. Experiments,0,[0]
"We drop SEQELIMINATE henceforth.
",8. Experiments,0,[0]
"Now we consider a model that does not satisfy STI but satisfies MST i.e., n = 10 and p1,j = 1/2 + q̃ ∀j ≤ n/2, p1,j = 1 ∀j > n/2 and pi,j = 1/2 + q̃ ∀1",8. Experiments,0,[0]
<,8. Experiments,0,[0]
"i < j, q̃ < 0.05.",8. Experiments,0,[0]
Under this model any i ≤ 5 is a 0.05-maximum.,8. Experiments,0,[0]
Figure 3 shows the average comparison complexity of algorithms under this model.,8. Experiments,0,[0]
"KNOCKOUT uses fewer com-
parisons, but fails to output a 0.05-maximum with probability 0.12 for q̃ = 0.001 and 0.25 for q̃ = 0.0001, hence fails to meet the confidence requirement once STI is dropped.",8. Experiments,0,[0]
"Other algorithms find a 0.05-maximum in all runs.
",8. Experiments,0,[0]
"It is interesting to note that MallowsMPI uses more comparisons as q̃ decreases, whereas the complexity of other algorithms remains almost same.",8. Experiments,0,[0]
This is because MallowsMPI tries to find absolute maximum which is not always practical.,8. Experiments,0,[0]
"Further note that the performance of SOFTSEQ-ELIM is better than NEAR-OPT-MAX, and NEAROPT-MAX is better than OPT-MAX.",8. Experiments,0,[0]
"This is because the bias gap for SOFT-SEQ-ELIM, NEAR-OPT-MAX and OPTMAX is , /2 and /4 respectively, resulting in higher constants for NEAR-OPT-MAX and OPT-MAX.",8. Experiments,0,[0]
"While the theoretical order complexity is higher for SOFT-SEQ-ELIM, in practice it can find a good anchor quickly and seems to have near-linear order complexity.",8. Experiments,0,[0]
"We studied the problem of maxing, ranking, and estimating comparison probabilities under different stochastic transitivity constraints.",9. Conclusion,0,[0]
"We showed that under WST, maxing needs quadratic comparisons.",9. Conclusion,0,[0]
We also presented a linearcomplexity algorithm for maxing under MST.,9. Conclusion,0,[0]
"We also proposed an optimal ranking algorithm for SST models with Stochastic Triangle Inequality, closing (log log n)3 gap.",9. Conclusion,0,[0]
"For the same model, we proposed an optimal algorithm for estimating the comparison probabilities.",9. Conclusion,0,[0]
"We thank NSF for supporting this work through grants CIF1564355 and CIF-1619448.
References http://www.gif.gf/.
Acharya, J., Falahatgar, M., Jafarpour, A., Orlitsky, A., and Suresh, A. T. Maximum selection and sorting with adversarial comparators and an application to density estimation.",ACKNOWLEDGMENTS,0,[0]
"arXiv preprint arXiv:1606.02786, 2016.
",ACKNOWLEDGMENTS,0,[0]
"Ajtai, M., Feldman, V., Hassidim, A., and Nelson, J. Sorting and selection with imprecise comparisons.",ACKNOWLEDGMENTS,0,[0]
"ACM Transactions on Algorithms (TALG), 12(2):19, 2015.
",ACKNOWLEDGMENTS,0,[0]
"Baltrunas, L., Makcinskas, T., and Ricci, F. Group recommendations with rank aggregation and collaborative filtering.",ACKNOWLEDGMENTS,0,[0]
"In Proceedings of the fourth ACM conference on Recommender systems, pp. 119–126.",ACKNOWLEDGMENTS,0,[0]
"ACM, 2010.
",ACKNOWLEDGMENTS,0,[0]
"Busa-Fekete, R., Hüllermeier, E., and Szörényi, B. Preference-based rank elicitation using statistical models:",ACKNOWLEDGMENTS,0,[0]
The case of mallows.,ACKNOWLEDGMENTS,0,[0]
"In Proc. of the ICML, pp. 1071–1079, 2014a.
",ACKNOWLEDGMENTS,0,[0]
"Busa-Fekete, R., Szörényi, B., and Hüllermeier, E. Pac rank elicitation through adaptive sampling of stochastic pairwise preferences.",ACKNOWLEDGMENTS,0,[0]
"In AAAI, 2014b.
Caplin, A. and Nalebuff, B. Aggregation and social choice: a mean voter theorem.",ACKNOWLEDGMENTS,0,[0]
"Econometrica: Journal of the Econometric Society, pp.",ACKNOWLEDGMENTS,0,[0]
"1–23, 1991.
",ACKNOWLEDGMENTS,0,[0]
"Chatterjee, S. et al. Matrix estimation by universal singular value thresholding.",ACKNOWLEDGMENTS,0,[0]
"The Annals of Statistics, 43(1):177– 214, 2015.
",ACKNOWLEDGMENTS,0,[0]
"Chen, X., Bennett, P. N., Collins-Thompson, K., and Horvitz, E. Pairwise ranking aggregation in a crowdsourced setting.",ACKNOWLEDGMENTS,0,[0]
"In Proceedings of the sixth ACM international conference on Web search and data mining, pp.",ACKNOWLEDGMENTS,0,[0]
"193–202. ACM, 2013.
Dudı́k, M., Hofmann, K., Schapire, R. E., Slivkins, A., and Zoghi, M. Contextual dueling bandits.",ACKNOWLEDGMENTS,0,[0]
"arXiv preprint arXiv:1502.06362, 2015.
",ACKNOWLEDGMENTS,0,[0]
"Falahatgar, M., Hao, Y., Orlitsky, A., Pichapati, V., and Ravindrakumar, V. Maxing and ranking with few assumptions.",ACKNOWLEDGMENTS,0,[0]
"In Advances in Neural Information Processing Systems, pp.",ACKNOWLEDGMENTS,0,[0]
"7063–7073, 2017a.
",ACKNOWLEDGMENTS,0,[0]
"Falahatgar, M., Orlitsky, A., Pichapati, V., and Suresh, A. T. Maximum selection and ranking under noisy comparisons.",ACKNOWLEDGMENTS,0,[0]
"In International Conference on Machine Learning, pp. 1088–1096, 2017b.
Feige, U., Raghavan, P., Peleg, D., and Upfal, E. Computing with noisy information.",ACKNOWLEDGMENTS,0,[0]
"SIAM Journal on Computing, 23(5):1001–1018, 1994.
",ACKNOWLEDGMENTS,0,[0]
"Heckel, R., Shah, N. B., Ramchandran, K., and Wainwright, M. J. Active ranking from pairwise comparisons and when parametric assumptions don’t help.",ACKNOWLEDGMENTS,0,[0]
"arXiv preprint arXiv:1606.08842, 2016.
",ACKNOWLEDGMENTS,0,[0]
"Hüllermeier, E., Fürnkranz, J., Cheng, W., and Brinker, K. Label ranking by learning pairwise preferences.",ACKNOWLEDGMENTS,0,[0]
"Artificial Intelligence, 172(16-17):1897–1916, 2008.
",ACKNOWLEDGMENTS,0,[0]
"Jang, M., Kim, S., Suh, C., and Oh, S. Top-k ranking from pairwise comparisons: When spectral ranking is optimal.",ACKNOWLEDGMENTS,0,[0]
"arXiv preprint arXiv:1603.04153, 2016.
",ACKNOWLEDGMENTS,0,[0]
"Lee, D. T., Goel, A., Aitamurto, T., and Landemore, H. Crowdsourcing for participatory democracies: Efficient elicitation of social choice functions.",ACKNOWLEDGMENTS,0,[0]
"In Second AAAI Conference on Human Computation and Crowdsourcing, 2014.
Luce, R. D. Individual choice behavior: A theoretical analysis.",ACKNOWLEDGMENTS,0,[0]
"Courier Corporation, 2005.
Mohajer, S., Suh, C., and Elmahdy, A. Active learning for top-k rank aggregation from noisy comparisons.",ACKNOWLEDGMENTS,0,[0]
"In International Conference on Machine Learning, pp. 2488– 2497, 2017.
Negahban, S., Oh, S., and Shah, D. Iterative ranking from pair-wise comparisons.",ACKNOWLEDGMENTS,0,[0]
"In NIPS, pp. 2474–2482, 2012.
Negahban, S., Oh, S., and Shah, D. Rank centrality:",ACKNOWLEDGMENTS,0,[0]
Ranking from pairwise comparisons.,ACKNOWLEDGMENTS,0,[0]
"Operations Research, 2016.
",ACKNOWLEDGMENTS,0,[0]
"Plackett, R. L.",ACKNOWLEDGMENTS,0,[0]
The analysis of permutations.,ACKNOWLEDGMENTS,0,[0]
"Applied Statistics, pp.",ACKNOWLEDGMENTS,0,[0]
"193–202, 1975.
",ACKNOWLEDGMENTS,0,[0]
"Radlinski, F. and Joachims, T. Active exploration for learning rankings from clickthrough data.",ACKNOWLEDGMENTS,0,[0]
"In Proceedings of the 13th ACM SIGKDD, pp.",ACKNOWLEDGMENTS,0,[0]
"570–579. ACM, 2007.
",ACKNOWLEDGMENTS,0,[0]
"Radlinski, F., Kurup, M., and Joachims, T. How does clickthrough data reflect retrieval quality?",ACKNOWLEDGMENTS,0,[0]
"In Proceedings of the 17th ACM conference on Information and knowledge management, pp. 43–52. ACM, 2008.
",ACKNOWLEDGMENTS,0,[0]
"Rajkumar, A. and Agarwal, S. A statistical convergence perspective of algorithms for rank aggregation from pairwise data.",ACKNOWLEDGMENTS,0,[0]
"In Proc. of the ICML, pp. 118–126, 2014.
",ACKNOWLEDGMENTS,0,[0]
"Shah, N., Balakrishnan, S., Guntuboyina, A., and Wainwright, M. Stochastically transitive models for pairwise comparisons: Statistical and computational issues.",ACKNOWLEDGMENTS,0,[0]
"In International Conference on Machine Learning, pp. 11– 20, 2016a.
Shah, N. B., Balakrishnan, S., and Wainwright, M. J. Feeling the bern: Adaptive estimators for bernoulli probabilities of pairwise comparisons.",ACKNOWLEDGMENTS,0,[0]
"arXiv preprint arXiv:1603.06881, 2016b.
",ACKNOWLEDGMENTS,0,[0]
"Skorepa, M. Decision making: a behavioral economic approach.",ACKNOWLEDGMENTS,0,[0]
"Palgrave Macmillan, 2010.
",ACKNOWLEDGMENTS,0,[0]
"Soufiani, H. A., Chen, W., Parkes, D. C., and Xia, L. Generalized method-of-moments for rank aggregation.",ACKNOWLEDGMENTS,0,[0]
"In Advances in Neural Information Processing Systems, pp. 2706–2714, 2013.
Szörényi, B., Busa-Fekete, R., Paul, A., and Hüllermeier, E. Online rank elicitation for plackett-luce: A dueling bandits approach.",ACKNOWLEDGMENTS,0,[0]
"In NIPS, pp.",ACKNOWLEDGMENTS,0,[0]
"604–612, 2015.
",ACKNOWLEDGMENTS,0,[0]
"Yue, Y. and Joachims, T. Beat the mean bandit.",ACKNOWLEDGMENTS,0,[0]
"In Proc. of the ICML, pp.",ACKNOWLEDGMENTS,0,[0]
"241–248, 2011.",ACKNOWLEDGMENTS,0,[0]
"We present a comprehensive understanding of three important problems in PAC preference learning: maximum selection (maxing), ranking, and estimating all pairwise preference probabilities, in the adaptive setting.",abstractText,0,[0]
"With just Weak Stochastic Transitivity, we show that maxing requires Ω(n) comparisons and with slightly more restrictive Medium Stochastic Transitivity, we present a linear complexity maxing algorithm.",abstractText,0,[0]
"With Strong Stochastic Transitivity and Stochastic Triangle Inequality, we derive a ranking algorithm with optimalO(n log n) complexity and an optimal algorithm that estimates all pairwise preference probabilities.",abstractText,0,[0]
"The Limits of Maxing, Ranking, and Preference Learning",title,0,[0]
"The application of deep learning (LeCun et al., 2015) has in recent years lead to a dramatic boost in performance in many areas such as computer vision, speech recognition or natural language processing.",1. Introduction,0,[0]
"Despite this huge empirical success, the theoretical understanding of deep learning is still limited.",1. Introduction,0,[0]
In this paper we address the non-convex optimization problem of training a feedforward neural network.,1. Introduction,0,[0]
"This problem turns out to be very difficult as there can be exponentially many distinct local minima (Auer et al., 1996; Safran & Shamir, 2016).",1. Introduction,0,[0]
"It has been shown that the training of a network with a single neuron with a variety of activation functions turns out to be NP-hard (Sima, 2002).
",1. Introduction,0,[0]
In practice local search techniques like stochastic gradient descent or variants are used for training deep neural networks.,1. Introduction,0,[0]
"Surprisingly, it has been observed (Dauphin et al., 2014; Goodfellow et al., 2015) that in the training of stateof-the-art feedforward neural networks with sparse connectivity like convolutional neural networks (LeCun et al., 1990; Krizhevsky et al., 2012) or fully connected ones one
1Department of Mathematics and Computer Science, Saarland University, Germany.",1. Introduction,0,[0]
"Correspondence to: Quynh Nguyen <quynh@cs.uni-saarland.de>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
does not encounter problems with suboptimal local minima.",1. Introduction,0,[0]
"However, as the authors admit themselves in (Goodfellow et al., 2015), the reason for this might be that there is a connection between the fact that these networks have good performance and that they are easy to train.
",1. Introduction,0,[0]
"On the theoretical side there have been several interesting developments recently, see e.g. (Brutzkus & Globerson, 2017; Lee et al., 2016; Poggio & Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2017; Zhou & Feng, 2017).",1. Introduction,0,[0]
For some class of networks one can show that one can train them globally optimal efficiently.,1. Introduction,0,[0]
"However, it turns out that these approaches are either not practical (Janzamin et al., 2016; Haeffele & Vidal, 2015; Soltanolkotabi, 2017) as they require e.g. knowledge about the data generating measure, or they modify the neural network structure and objective (Gautier et al., 2016).",1. Introduction,0,[0]
"One class of networks which are simpler to analyze are deep linear networks for which it has been shown that every local minimum is a global minimum (Baldi & Hornik, 1988; Kawaguchi, 2016).",1. Introduction,0,[0]
"While this is a highly non-trivial result as the optimization problem is non-convex, deep linear networks are not interesting in practice as one efficiently just learns a linear function.",1. Introduction,0,[0]
"In order to characterize the loss surface for general networks, an interesting approach has been taken by (Choromanska et al., 2015a).",1. Introduction,0,[0]
"By randomizing the nonlinear part of a feedforward network with ReLU activation function and making some additional simplifying assumptions, they can relate it to a certain spin glass model which one can analyze.",1. Introduction,0,[0]
In this model the objective of local minima is close to the global optimum and the number of bad local minima decreases quickly with the distance to the global optimum.,1. Introduction,0,[0]
"This is a very interesting result but is based on a number of unrealistic assumptions (Choromanska et al., 2015b).",1. Introduction,0,[0]
"It has recently been shown (Kawaguchi, 2016) that if some of these assumptions are dropped one basically recovers the result of the linear case, but the model is still unrealistic.
",1. Introduction,0,[0]
"In this paper we analyze the case of overspecified neural networks, that is the network is larger than what is required to achieve minimum training error.",1. Introduction,0,[0]
"Under overspecification (Safran & Shamir, 2016) have recently analyzed under which conditions it is possible to generate an initialization so that it is in principle possible to reach the global optimum with descent methods.",1. Introduction,0,[0]
"However, they can only deal with one hidden layer networks and have to make strong
assumptions on the data such as linear independence or cluster structure.",1. Introduction,0,[0]
"In this paper overspecification means that there exists a very wide layer, where the number of hidden units is larger than the number of training points.",1. Introduction,0,[0]
"For this case, we can show that a large class of local minima is globally optimal.",1. Introduction,0,[0]
"In fact, we will argue that almost every critical point is globally optimal.",1. Introduction,0,[0]
"Our results generalize previous work of (Yu & Chen, 1995), who have analyzed a similar setting for one hidden layer networks, to networks of arbitrary depth.",1. Introduction,0,[0]
"Moreover, it extends results of (Gori & Tesi, 1992; Frasconi et al., 1997) who have shown that for certain deep feedforward neural networks almost all local minima are globally optimal whenever the training data is linearly independent.",1. Introduction,0,[0]
"While it is clear that our assumption on the number of hidden units is quite strong, there are several recent neural network structures which contain a quite wide hidden layer relative to the number of training points e.g. in (Lin et al., 2016)",1. Introduction,0,[0]
"they have 50,000 training samples and the network has one hidden layer with 10,000 hidden units and (Ba & Caruana, 2014) have 1.1 million training samples and a layer with 400,000 hidden units.",1. Introduction,0,[0]
"We refer to (Ciresan et al., 2010; Neyshabur et al., 2015; Vincent et al., 2010; Caruana et al., 2001) for other examples where the number of hidden units of one layer is on the order of the number of training samples.",1. Introduction,0,[0]
We conjecture that for these kind of wide networks it still holds that almost all local minima are globally optimal.,1. Introduction,0,[0]
The reason is that one can expect linear separability of the training data in the wide layer.,1. Introduction,0,[0]
We provide supporting evidence for this conjecture by showing that basically every critical point for which the training data is linearly separable in the wide layer is globally optimal.,1. Introduction,0,[0]
"Moreover, we want to emphasize that all of our results hold for neural networks used in practice.",1. Introduction,0,[0]
There are no simplifying assumptions as in previous work.,1. Introduction,0,[0]
We are mainly concerned with multi-class problems but our results also apply to multivariate regression problems.,2. Feedforward Neural Networks and Backpropagation,0,[0]
Let N be the number of training samples and denote by X =,2. Feedforward Neural Networks and Backpropagation,0,[0]
"[x1, . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", xN ]
T ∈ RN×d, Y =",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[y1, . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", yN",2. Feedforward Neural Networks and Backpropagation,0,[0]
]T ∈ RN×m the input resp.,2. Feedforward Neural Networks and Backpropagation,0,[0]
"output matrix for the training data (xi, yi) N i=1, where d is the input dimension and m the number of classes.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"We consider fully-connected feedforward networks with L layers, indexed from 0, 1, 2, . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", L, which correspond to the input layer, 1st hidden layer, etc, and output layer.",2. Feedforward Neural Networks and Backpropagation,0,[0]
The network structure is determined by the weight matrices (,2. Feedforward Neural Networks and Backpropagation,0,[0]
Wk)Lk=1 ∈ W := Rd×n1 × . . .,2. Feedforward Neural Networks and Backpropagation,0,[0]
× Rnk−1×nk×. .,2. Feedforward Neural Networks and Backpropagation,0,[0]
".×RnL−1×m; where nk is the number of hidden units of layer k (for consistency, we set n0 = d, nL = m), and the bias vectors (bk)Lk=1 ∈ B := Rn1 × . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
.×RnL .,2. Feedforward Neural Networks and Backpropagation,0,[0]
We denote by P = W × B the space of all possible parameters of the network.,2. Feedforward Neural Networks and Backpropagation,0,[0]
"In this paper, [a] denotes the set
of integers {1, 2, . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", a} and [a, b] the set of integers from a to b.",2. Feedforward Neural Networks and Backpropagation,0,[0]
The activation function σ :,2. Feedforward Neural Networks and Backpropagation,0,[0]
"R → R is assumed at least to be continuously differentiable, that is σ ∈ C1(R).",2. Feedforward Neural Networks and Backpropagation,0,[0]
"In this paper, we assume that all the functions are applied componentwise.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"Let fk, gk : Rd → Rnk be the mappings from the input space to the feature space at layer k, which are defined as
f0(x) = x, fk(x) = σ(gk(x)), gk(x) = W T k fk−1(x) + bk
for every k ∈",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[L], x ∈ Rd.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"In the following, let Fk = [fk(x1), fk(x2), . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", fk(xN )]
",2. Feedforward Neural Networks and Backpropagation,0,[0]
T ∈ RN×nk and,2. Feedforward Neural Networks and Backpropagation,0,[0]
Gk =,2. Feedforward Neural Networks and Backpropagation,0,[0]
"[gk(x1), gk(x2), . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
.,2. Feedforward Neural Networks and Backpropagation,0,[0]
", gk(xN )]
T ∈ RN×nk be the matrices that store the feature vectors of layer k after and before applying the activation function.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"One can easily check that
F1 = σ(XW1 + 1Nb T 1 ), Fk = σ(Fk−1Wk + 1Nb T k ), for k ∈",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[2, L].
",2. Feedforward Neural Networks and Backpropagation,0,[0]
"In this paper we analyze the behavior of the loss of the network without any form of regularization, that is the final objective Φ : P → R of the network is defined as
Φ (
(Wk, bk) L k=1
)",2. Feedforward Neural Networks and Backpropagation,0,[0]
#NAME?,2. Feedforward Neural Networks and Backpropagation,0,[0]
"m∑ j=1 l(fLj(xi)− yij) (1)
where l : R → R is assumed to be a continuously differentiable loss function, that is l ∈ C1(R).",2. Feedforward Neural Networks and Backpropagation,0,[0]
"The prototype loss which we consider in this paper is the squared loss, l(α) = α2, which is one of the standard loss functions in the neural network literature.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"We assume throughout this paper that the minimum of (1) is attained.
",2. Feedforward Neural Networks and Backpropagation,0,[0]
The idea of backpropagation is the core of our theoretical analysis.,2. Feedforward Neural Networks and Backpropagation,0,[0]
"Lemma 2.1 below shows well-known relations for feed-forward neural networks, which are used throughout the paper.",2. Feedforward Neural Networks and Backpropagation,0,[0]
The derivative of the loss w.r.t.,2. Feedforward Neural Networks and Backpropagation,0,[0]
the value of unit j at layer k evaluated at a single training sample xi is denoted as δkj(xi) = ∂Φ∂gkj(xi) .,2. Feedforward Neural Networks and Backpropagation,0,[0]
"We arrange these vectors for all training samples into a single matrix ∆k, defined as
∆k =",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[δk:(x1), . . .",2. Feedforward Neural Networks and Backpropagation,0,[0]
", δk:(xN )]",2. Feedforward Neural Networks and Backpropagation,0,[0]
"T ∈ RN×nk .
",2. Feedforward Neural Networks and Backpropagation,0,[0]
"In the following, A ◦B denotes the Hadamard product between two matrices, i.e. (A ◦B)ij = AijBij .
",2. Feedforward Neural Networks and Backpropagation,0,[0]
"Lemma 2.1 Let σ, l ∈ C1(R).",2. Feedforward Neural Networks and Backpropagation,0,[0]
"Then it holds
1.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"∆k = { l′(FL − Y ) ◦ σ′(GL), k = L (∆k+1W T k+1) ◦ σ′(Gk), k ∈",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[L− 1]
2. ∇WkΦ",2. Feedforward Neural Networks and Backpropagation,0,[0]
"=
{ XT∆1, k = 1
FTk−1∆k, k ∈",2. Feedforward Neural Networks and Backpropagation,0,[0]
"[2, L]
3. ∇bkΦ",2. Feedforward Neural Networks and Backpropagation,0,[0]
= ∆Tk 1N ∀ k ∈,2. Feedforward Neural Networks and Backpropagation,0,[0]
"[L]
Note that Lemma 2.1 does not apply to non-differentiable activation functions like the ReLU function, σReLU(x) = max{0, x}.",2. Feedforward Neural Networks and Backpropagation,0,[0]
"However, it is known that one can approximate this activation function arbitrarily well by a smooth function e.g. σα(x) = 1α log(1 + e
αx) (a.k.a. softplus) satisfies limα→∞ σα(x) = σReLU(x) for any x ∈ R.",2. Feedforward Neural Networks and Backpropagation,0,[0]
We first discuss some prior work and present then our main result together with extensive discussion.,3. Main Result,0,[0]
For improved readability we postpone the proof of the main result to the next section which contains several intermediate results which are of independent interest.,3. Main Result,0,[0]
"Our work can be seen as a generalization of the work of (Gori & Tesi, 1992; Yu & Chen, 1995).",3.1. Previous Work,0,[0]
"While (Yu & Chen, 1995) has shown that for a one-hidden layer network, that if n1 = N",3.1. Previous Work,0,[0]
"− 1, then every local minimum is a global minimum, the work of (Gori & Tesi, 1992) considered also multi-layer networks.",3.1. Previous Work,0,[0]
"For the convenience of the reader, we first restate Theorem 1 of (Gori & Tesi, 1992) using our previously introduced notation.",3.1. Previous Work,0,[0]
"The critical points of a continuously differentiable function f : Rd → R are the points where the gradient vanishes, that is ∇f(x) = 0.",3.1. Previous Work,0,[0]
"Note that this is a necessary condition for a local minimum.
",3.1. Previous Work,0,[0]
"Theorem 3.1 (Gori & Tesi, 1992) Let Φ : P → R be defined as in (1) with least squares loss l(a) = a2.",3.1. Previous Work,0,[0]
Assume σ,3.1. Previous Work,0,[0]
:,3.1. Previous Work,0,[0]
R →,3.1. Previous Work,0,[0]
"[d, d̄] to be continuously differentiable with strictly positive derivative and
lim a→∞
σ′(a) d̄−σ(a) > 0, lima→∞ −σ′′(a) d̄−σ(a)",3.1. Previous Work,0,[0]
"> 0
lima→−∞ σ′(a)",3.1. Previous Work,0,[0]
"σ(a)−d > 0, lima→−∞ σ′′(a)",3.1. Previous Work,0,[0]
"σ(a)−d > 0
Then every critical point (Wl, bl)Ll=1 of Φ which satisfies the conditions
1. rank(Wl) = nl for all l ∈",3.1. Previous Work,0,[0]
"[2, L],
2.",3.1. Previous Work,0,[0]
"[X,1N ]T∆1 = 0 implies ∆1 = 0
is a global minimum.
",3.1. Previous Work,0,[0]
"While this result is already for general multi-layer networks, the condition “[X,1N ]T∆1 = 0 implies ∆1 = 0” is the main caveat.",3.1. Previous Work,0,[0]
"It is already noted in (Gori & Tesi, 1992), that “it is quite hard to understand its practical meaning” as it requires prior knowledge of ∆1 at every critical point.",3.1. Previous Work,0,[0]
Note that this is almost impossible as ∆1 depends on all the weights of the network.,3.1. Previous Work,0,[0]
"For a particular case, when the training samples (biases added) are linearly independent, i.e. rank([X,1N ])",3.1. Previous Work,0,[0]
#NAME?,3.1. Previous Work,0,[0]
"This case is discussed in the following Theorem
3.4, where we consider a more general class of loss and activation functions.",3.1. Previous Work,0,[0]
"A function f : Rd → R is real analytic if the corresponding multivariate Taylor series converges to f(x) on an open subset of Rd (Krantz & Parks, 2002).",3.2. First Main Result and Discussion,0,[0]
"All results in this section are proven under the following assumptions on the loss/activation function and training data.
",3.2. First Main Result and Discussion,0,[0]
Assumptions 3.2 1.,3.2. First Main Result and Discussion,0,[0]
"There are no identical training samples, i.e. xi 6= xj for all i 6= j,
2.",3.2. First Main Result and Discussion,0,[0]
"σ is analytic on R, strictly monotonically increasing and
(a) σ is bounded or (b) there are positive ρ1, ρ2, ρ3, ρ4, s.t. |σ(t)| ≤
ρ1e ρ2t for t < 0 and |σ(t)| ≤ ρ3t+ ρ4 for t ≥ 0
3.",3.2. First Main Result and Discussion,0,[0]
l ∈ C2(R),3.2. First Main Result and Discussion,0,[0]
"and if l′(a) = 0 then a is a global minimum
These conditions are not always necessary to prove some of the intermediate results presented below, but we decided to provide the proof under the above strong assumptions for better readability.",3.2. First Main Result and Discussion,0,[0]
"For instance, all of our results also hold for strictly monotonically decreasing activation functions.",3.2. First Main Result and Discussion,0,[0]
"Note that the above conditions are not restrictive as many standard activation functions satisfy them.
",3.2. First Main Result and Discussion,0,[0]
Lemma 3.3,3.2. First Main Result and Discussion,0,[0]
The sigmoid activation function σ1(t),3.2. First Main Result and Discussion,0,[0]
"= 1 1+e−t , the tangent hyperbolic σ2(t) = tanh(t) and the softplus function σ3(t) = 1α log(1 + e
αt) for α > 0 satisfy Assumption 3.2.
",3.2. First Main Result and Discussion,0,[0]
The conditions on l are satisfied for any twice continuously differentiable convex loss function.,3.2. First Main Result and Discussion,0,[0]
"A typical example is the squared loss l(a) = a2 or the PseudoHuber loss (Hartley & Zisserman, 2004) given as lδ(a) = 2δ2( √ 1 + a2/δ2 − 1) which approximates a2 for small a and is linear with slope 2δ for large a.",3.2. First Main Result and Discussion,0,[0]
"But also nonconvex loss functions satisfy this requirement, e.g. the Blake-Zisserman, corrupted Gaussian and Cauchy functions (Hartley & Zisserman, 2004)",3.2. First Main Result and Discussion,0,[0]
"(p.617-p.619).
",3.2. First Main Result and Discussion,0,[0]
"As a motivation for our main result, we first analyze the case when the training samples are linearly independent, which requiresN ≤ d+1.",3.2. First Main Result and Discussion,0,[0]
"It can be seen as a generalization of Corollary 1 in (Gori & Tesi, 1992).
",3.2. First Main Result and Discussion,0,[0]
Theorem 3.4 Let Φ : P → R be defined as in (1) and let the Assumptions 3.2 hold.,3.2. First Main Result and Discussion,0,[0]
"If the training samples are linearly independent, that is rank([X,1N ])",3.2. First Main Result and Discussion,0,[0]
"= N , then every critical point (Wl, bl)Ll=1 of Φ for which the weight matrices (Wl)Ll=2 have full column rank, that is rank(Wl) = nl for l ∈",3.2. First Main Result and Discussion,0,[0]
"[2, L], is a global minimum.
",3.2. First Main Result and Discussion,0,[0]
Theorem 3.4 implies that the weight matrices of potential saddle points or suboptimal local minima need to have low rank for one particular layer.,3.2. First Main Result and Discussion,0,[0]
Note however that the set of low rank weight matrices in W has measure zero.,3.2. First Main Result and Discussion,0,[0]
At the moment we cannot prove that suboptimal low rank local minima cannot exist.,3.2. First Main Result and Discussion,0,[0]
"However, it seems implausible that such suboptimal low rank local minima exist as every neighborhood of such points contains full rank matrices which increase the expressiveness of the network.",3.2. First Main Result and Discussion,0,[0]
"Thus it should be possible to use this degree of freedom to further reduce the loss, which contradicts the definition of a local minimum.",3.2. First Main Result and Discussion,0,[0]
"Thus we conjecture that all local minima are indeed globally optimal.
",3.2. First Main Result and Discussion,0,[0]
"The main restriction in the assumptions of Theorem 3.4 is the linear independence of the training samples as it requires N ≤ d+ 1, which is very restrictive in practice.",3.2. First Main Result and Discussion,0,[0]
We prove in this section a similar guarantee in our main Theorem 3.8 by implicitly transporting this condition to some higher layer.,3.2. First Main Result and Discussion,0,[0]
"A similar guarantee has been proven by (Yu & Chen, 1995) for a single hidden layer network, whereas we consider general multi-layer networks.",3.2. First Main Result and Discussion,0,[0]
"The main ingredient of the proof of our main result is the observation in the following lemma.
",3.2. First Main Result and Discussion,0,[0]
Lemma 3.5 Let Φ : P → R be defined as in (1) and let the Assumptions 3.2 hold.,3.2. First Main Result and Discussion,0,[0]
"Let (Wl, bl)Ll=1 ∈ P be given.",3.2. First Main Result and Discussion,0,[0]
Assume there is some k ∈,3.2. First Main Result and Discussion,0,[0]
[L− 1] s.t.,3.2. First Main Result and Discussion,0,[0]
"the following holds
1. rank([Fk,1N",3.2. First Main Result and Discussion,0,[0]
]),3.2. First Main Result and Discussion,0,[0]
"= N
2. rank(Wl) =",3.2. First Main Result and Discussion,0,[0]
"nl, l ∈",3.2. First Main Result and Discussion,0,[0]
"[k + 2, L] 3. ∇Wk+1Φ",3.2. First Main Result and Discussion,0,[0]
"( (Wl, bl) L l=1 )",3.2. First Main Result and Discussion,0,[0]
"= 0
∇bk+1Φ",3.2. First Main Result and Discussion,0,[0]
"( (Wl, bl) L l=1 )",3.2. First Main Result and Discussion,0,[0]
"= 0
then (Wl, bl)Ll=1 is a global minimum.
",3.2. First Main Result and Discussion,0,[0]
The first condition of Lemma 3.5 can be seen as a generalization of the requirement of linearly independent training inputs in Theorem 3.4 to a condition of linear independence of the feature vectors at a hidden layer.,3.2. First Main Result and Discussion,0,[0]
"Lemma 3.5 suggests that if we want to make statements about the global optimality of critical points, it is sufficient to know when and which critical points fulfill these conditions.",3.2. First Main Result and Discussion,0,[0]
The third condition is trivially satisfied by a critical point and the requirement of full column rank of the weight matrices is similar to Theorem 3.4.,3.2. First Main Result and Discussion,0,[0]
"However, the first one may not be fulfilled since rank([Fk,1N ]) is dependent not only on the weights but also on the architecture.",3.2. First Main Result and Discussion,0,[0]
The main difficulty of the proof of our following main theorem is to prove that this first condition holds under the rather simple requirement that nk ≥ N,3.2. First Main Result and Discussion,0,[0]
"− 1 for a subset of all critical points.
",3.2. First Main Result and Discussion,0,[0]
"But before we state the theorem we have to discuss a particular notion of non-degenerate critical point.
",3.2. First Main Result and Discussion,0,[0]
Definition 3.6 (Block Hessian) Let f : D → R be a twice-continuously differentiable function defined on some open domain D ⊆ Rn.,3.2. First Main Result and Discussion,0,[0]
The Hessian w.r.t.,3.2. First Main Result and Discussion,0,[0]
"a subset of variables S ⊆ {x1, . .",3.2. First Main Result and Discussion,0,[0]
.,3.2. First Main Result and Discussion,0,[0]
", xn} is denoted as∇2Sf(x) ∈",3.2. First Main Result and Discussion,0,[0]
R|S|×|S|.,3.2. First Main Result and Discussion,0,[0]
"When |S| = n, we write∇2f(x) ∈ Rn×n to denote the full Hessian matrix.
",3.2. First Main Result and Discussion,0,[0]
"We use this to introduce a slightly more general notion of non-degenerate critical point.
",3.2. First Main Result and Discussion,0,[0]
Definition 3.7 (Non-degenerate critical point) Let f : D → R be a twice-continuously differentiable function defined on some open domain D ⊆ Rn.,3.2. First Main Result and Discussion,0,[0]
"Let x ∈ D be a critical point, i.e. ∇f(x) = 0, then
• x is non-degenerate for a subset of variables S ⊆ {x1, . . .",3.2. First Main Result and Discussion,0,[0]
", xn} if∇2Sf(x) is non-singular.
",3.2. First Main Result and Discussion,0,[0]
"• x is non-degenerate if∇2f(x) is non-singular.
",3.2. First Main Result and Discussion,0,[0]
"Note that a non-degenerate critical point might not be nondegenerate for a subset of variables, and vice versa, if it is non-degenerate on a subset of variables it does not necessarily imply non-degeneracy on the whole set.",3.2. First Main Result and Discussion,0,[0]
"For instance,
∇2f(x) = 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 , ∇2f(y) = 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0
Clearly, det∇2f(x) = 0 but det∇2{x1,x2}f(x) 6= 0, and det∇2f(y) 6= 0",3.2. First Main Result and Discussion,0,[0]
"but det∇2{y3,y4}f(y) = 0.",3.2. First Main Result and Discussion,0,[0]
"The concept of non-degeneracy on a subset of variables is crucial for the following statement of our main result.
",3.2. First Main Result and Discussion,0,[0]
Theorem 3.8 Let Φ : P → R be defined as in (1) and let the Assumptions 3.2 hold.,3.2. First Main Result and Discussion,0,[0]
Suppose nk ≥ N,3.2. First Main Result and Discussion,0,[0]
− 1 for some k ∈,3.2. First Main Result and Discussion,0,[0]
[L − 1].,3.2. First Main Result and Discussion,0,[0]
"Then every critical point (W ∗l , b∗l )Ll=1 of Φ which satisfies the following conditions
1.",3.2. First Main Result and Discussion,0,[0]
"(W ∗l , b ∗ l )",3.2. First Main Result and Discussion,0,[0]
"L l=1 is non-degenerate on {(Wl, bl)",3.2. First Main Result and Discussion,0,[0]
| l ∈,3.2. First Main Result and Discussion,0,[0]
"I},
for some subset I ⊆ {k + 1, . . .",3.2. First Main Result and Discussion,0,[0]
", L} satisfying {k + 1} ∈ I,
2.",3.2. First Main Result and Discussion,0,[0]
(W ∗l ),3.2. First Main Result and Discussion,0,[0]
"L l=k+2 has full column rank, that is, rank(W ∗",3.2. First Main Result and Discussion,0,[0]
l ),3.2. First Main Result and Discussion,0,[0]
#NAME?,3.2. First Main Result and Discussion,0,[0]
"[k + 2, L],
is a global minimum of Φ.
First of all we note that the full column rank condition of (Wl) L l=k+2 in Theorem 3.4, and 3.8 implicitly requires that nk+1 ≥ nk+2 ≥ . . .",3.2. First Main Result and Discussion,0,[0]
≥ nL.,3.2. First Main Result and Discussion,0,[0]
This means the network needs to have a pyramidal structure from layer k + 2 to L.,3.2. First Main Result and Discussion,0,[0]
"It is interesting to note that most modern neural network architectures have a pyramidal structure from some layer, typically the first hidden layer, on.",3.2. First Main Result and Discussion,0,[0]
"Thus this is not a restrictive
requirement.",3.2. First Main Result and Discussion,0,[0]
"Indeed, one can even argue that Theorem 3.8 gives an implicit justification as it hints on the fact that such networks are easy to train if one layer is sufficiently wide.
",3.2. First Main Result and Discussion,0,[0]
Note that Theorem 3.8 does not require fully nondegenerate critical points but non-degeneracy is only needed for some subset of variables that includes layer k + 1.,3.2. First Main Result and Discussion,0,[0]
"As a consequence of Theorem 3.8, we get directly a stronger result for non-degenerate local minima.
",3.2. First Main Result and Discussion,0,[0]
Corollary 3.9 Let Φ : P → R be defined as in (1) and let the Assumptions 3.2 hold.,3.2. First Main Result and Discussion,0,[0]
Suppose nk ≥ N,3.2. First Main Result and Discussion,0,[0]
− 1 for some k ∈,3.2. First Main Result and Discussion,0,[0]
[L − 1].,3.2. First Main Result and Discussion,0,[0]
"Then every non-degenerate local minimum (W ∗l , b ∗",3.2. First Main Result and Discussion,0,[0]
l ),3.2. First Main Result and Discussion,0,[0]
L l=1 of Φ for which (W ∗ l ),3.2. First Main Result and Discussion,0,[0]
"L l=k+2 has full column rank, that is rank(W ∗l )",3.2. First Main Result and Discussion,0,[0]
"= nl, is a global minimum of Φ.
Let us discuss the implications of these results.",3.2. First Main Result and Discussion,0,[0]
"First, note that Theorem 3.8 is slightly weaker than Theorem 3.4 as it requires also non-degeneracy wrt to a set of variables including layer k + 1.",3.2. First Main Result and Discussion,0,[0]
"Moreover, similar to Theorem 3.4 it does not exclude the possibility of suboptimal local minima of low rank in the layers “above” layer k + 1.",3.2. First Main Result and Discussion,0,[0]
On the other hand it makes also very strong statements.,3.2. First Main Result and Discussion,0,[0]
"In fact, if nk ≥ N",3.2. First Main Result and Discussion,0,[0]
− 1 for some k ∈,3.2. First Main Result and Discussion,0,[0]
[L − 1] then even degenerate saddle points/local maxima are excluded as long as they are non-degenerate with respect to any subset of parameters of upper layers that include layer k + 1 and the rank condition holds.,3.2. First Main Result and Discussion,0,[0]
"Thus given that the weight matrices of the upper layers have full column rank , there is not much room left for degenerate saddle points/local maxima.",3.2. First Main Result and Discussion,0,[0]
"Moreover, for a one-hidden-layer network for which n1 ≥ N",3.2. First Main Result and Discussion,0,[0]
"− 1, every non-degenerate critical point with respect to the output layer parameters is a global minimum, as the full rank condition is not active for one-hidden layer networks.
",3.2. First Main Result and Discussion,0,[0]
"Concerning the non-degeneracy condition of main Theorem 3.8, one might ask how likely it is to encounter degenerate points of a smooth function.",3.2. First Main Result and Discussion,0,[0]
"This is answered by an application of Sard’s/Morse theorem in (Milnor, 1965).
",3.2. First Main Result and Discussion,0,[0]
"Theorem 3.10 (A. Morse, p.11)",3.2. First Main Result and Discussion,0,[0]
If f : U ⊂ Rd → R is twice continuously differentiable.,3.2. First Main Result and Discussion,0,[0]
Then for almost all w ∈ Rd with respect to the Lebesgue measure it holds that f ′ defined as f ′(x) = f(x) +,3.2. First Main Result and Discussion,0,[0]
"〈w, x〉 has only non-degenerate critical points.
",3.2. First Main Result and Discussion,0,[0]
Note that the theorem would still hold if one would draw w uniformly at random from the set {z ∈ Rd | ‖z‖2 ≤ } for any > 0.,3.2. First Main Result and Discussion,0,[0]
Thus almost every linear perturbation f ′ of a function f will lead to the fact all of its critical points are non-degenerate.,3.2. First Main Result and Discussion,0,[0]
"Thus, this result indicates that exact degenerate points might be rare.",3.2. First Main Result and Discussion,0,[0]
"Note however that in practice the Hessian at critical points can be close to singular (at least up to numerical precision), which might affect the training of neural networks negatively (Sagun et al., 2016).
",3.2. First Main Result and Discussion,0,[0]
"As we argued for Theorem 3.4 our main Theorem 3.8 does
not exclude the possibility of suboptimal degenerate local minima or suboptimal local minima of low rank.",3.2. First Main Result and Discussion,0,[0]
"However, we conjecture that the second case cannot happen as every neighborhood of the local minima contains full rank matrices which increase the expressiveness of the network and this additional flexibility can be used to reduce the loss which contradicts the definition of a local minimum.
",3.2. First Main Result and Discussion,0,[0]
As mentioned in the introduction the condition nk,3.2. First Main Result and Discussion,0,[0]
≥ N−1 looks at first sight very strong.,3.2. First Main Result and Discussion,0,[0]
"However, as mentioned in the introduction, in practice often networks are used where one hidden layer is rather wide, that is nk is on the order of N (typically it is the first layer of the network).",3.2. First Main Result and Discussion,0,[0]
"As the condition of Theorem 3.8 is sufficient and not necessary, one can expect out of continuity reasons that the loss surface of networks where the condition is approximately true, is still rather well behaved, in the sense that still most local minima are indeed globally optimal and the suboptimal ones are not far away from the globally optimal ones.",3.2. First Main Result and Discussion,0,[0]
"For better readability, we first prove our main Theorem 3.8 for a special case where I is the whole set of upper layers, i.e. I = {k + 1, . . .",4. Proof of Main Result,0,[0]
", L} , and then show how to extend the proof to the general case where I ⊆ {k + 1, . . .",4. Proof of Main Result,0,[0]
", L} .",4. Proof of Main Result,0,[0]
Our proof strategy is as follows.,4. Proof of Main Result,0,[0]
We first show that the output of each layer are real analytic functions of network parameters.,4. Proof of Main Result,0,[0]
"Then we prove that there exists a set of parameters such that rank([Fk,1N ]) = N.",4. Proof of Main Result,0,[0]
"Using properties of real analytic functions, we conclude that the set of parameters where rank([Fk,1N ])",4. Proof of Main Result,0,[0]
< N has measure zero.,4. Proof of Main Result,0,[0]
"Then with the non-degeneracy condition, we can apply the implicit-function theorem to conclude that even if rank([Fk,1N ])",4. Proof of Main Result,0,[0]
"= N is not true at a critical point, then still in any neighborhood of it there exists a point where the conditions of Lemma 3.5 are true and the loss is minimal.",4. Proof of Main Result,0,[0]
"By continuity of Φ, this implies that the loss must also be minimal at the critical point.
",4. Proof of Main Result,0,[0]
We introduce some notation frequently used in the proofs.,4. Proof of Main Result,0,[0]
"Let B(x, r) =",4. Proof of Main Result,0,[0]
"{z ∈ Rd | ‖x− z‖2 < r} be the open ball in Rd of radius r around x.
Lemma 4.1",4. Proof of Main Result,0,[0]
"If the Assumptions 3.2 hold, then the output of each layer fl for every l ∈",4. Proof of Main Result,0,[0]
"[L] are real analytic functions of the network parameters on P.
Proof: Any linear function is real analytic and the set of real analytic functions is closed under addition, multiplication and composition, see e.g. Prop. 2.2.2 and Prop. 2.2.8 in (Krantz & Parks, 2002).",4. Proof of Main Result,0,[0]
"As we assume that the activation function is real analytic, we get that all the output functions of the neural network fk are real analytic functions of the parameters as compositions of real analytic functions.",4. Proof of Main Result,0,[0]
"2
The concept of real analytic functions is important in our proofs as these functions can never be “constant” in a set of the parameter space which has positive measure unless they are constant everywhere.",4. Proof of Main Result,0,[0]
"This is captured by the following lemma.
",4. Proof of Main Result,0,[0]
"Lemma 4.2 (Nguyen, 2015; Mityagin, 2015)",4. Proof of Main Result,0,[0]
"If f : Rn → R is a real analytic function which is not identically zero then the set {x ∈ Rn | f(x) = 0} has Lebesgue measure zero.
",4. Proof of Main Result,0,[0]
"In the next lemma we show that there exist network parameters such that rank([Fk,1N ])",4. Proof of Main Result,0,[0]
= N holds if nk ≥ N,4. Proof of Main Result,0,[0]
− 1.,4. Proof of Main Result,0,[0]
Note that this is only possible due to the fact that one uses non-linear activation functions.,4. Proof of Main Result,0,[0]
"For deep linear networks, it is not possible for Fk to achieve maximum rank if the layers below it are not sufficiently wide.",4. Proof of Main Result,0,[0]
"To see this, one considers Fk = Fk−1Wk + 1NbTk for a linear network, then rank(Fk) ≤ min{rank(Fk−1), rank(Wk)}",4. Proof of Main Result,0,[0]
+ 1 since the addition of a rank-one term does not increase the rank of a matrix by more than one.,4. Proof of Main Result,0,[0]
"By using induction, one gets rank(Fk) ≤ rank(Wl) +",4. Proof of Main Result,0,[0]
k,4. Proof of Main Result,0,[0]
− l + 1 for every l ∈,4. Proof of Main Result,0,[0]
"[k].
",4. Proof of Main Result,0,[0]
"The existence of network parameters where rank([Fk,1N ])",4. Proof of Main Result,0,[0]
"= N together with the previous lemma will then be used to show that the set of network parameters where rank([Fk,1N ])",4. Proof of Main Result,0,[0]
"< N has measure zero.
",4. Proof of Main Result,0,[0]
Lemma 4.3 If the Assumptions 3.2 hold and nk ≥ N,4. Proof of Main Result,0,[0]
− 1 for some k ∈,4. Proof of Main Result,0,[0]
"[L − 1], then there exists at least one set of parameters (Wl, bl)kl=1 such that rank([Fk,1N",4. Proof of Main Result,0,[0]
]),4. Proof of Main Result,0,[0]
"= N.
Now we combine the previous lemma with Lemma 4.2 to conclude the following.
",4. Proof of Main Result,0,[0]
Lemma 4.4 If the Assumptions 3.2 hold and nk ≥ N,4. Proof of Main Result,0,[0]
− 1 for some k ∈,4. Proof of Main Result,0,[0]
"[L − 1] then the set S :={( Wl, bl )k l=1 ∣∣∣ rank([Fk,1N",4. Proof of Main Result,0,[0]
"]) < N} has Lebesgue measure zero.
",4. Proof of Main Result,0,[0]
"We conclude that for nk ≥ N − 1 even if there are network parameters such that rank([Fk,1N",4. Proof of Main Result,0,[0]
]),4. Proof of Main Result,0,[0]
"< N , then every neighborhood of these parameters contains network parameters such that rank([Fk,1N ])",4. Proof of Main Result,0,[0]
"= N.
Corollary 4.5 If the Assumptions 3.2 hold and nk ≥ N−1 for some k ∈",4. Proof of Main Result,0,[0]
"[L − 1], then for any given (W 0l , b0l )kl=1 and for every > 0, there exists at least one ( Wl, bl )k l=1 ∈
B (( W 0l , b 0",4. Proof of Main Result,0,[0]
l )k,4. Proof of Main Result,0,[0]
"l=1 , ) s.t. rank([Fk,1N",4. Proof of Main Result,0,[0]
]),4. Proof of Main Result,0,[0]
"= N.
Proof: Let S := {( Wl, bl )",4. Proof of Main Result,0,[0]
"k l=1 ∣∣∣ rank([Fk,1N",4. Proof of Main Result,0,[0]
]) < N} .,4. Proof of Main Result,0,[0]
"The ball B (( Wl, bl )k l=1 , )
has positive Lebesgue measure while S has measure zero due to Lemma 4.4.",4. Proof of Main Result,0,[0]
"Thus,
for every ( Wl, bl )k l=1 ∈ B (( W 0l , b 0 l )k l=1 , )",4. Proof of Main Result,0,[0]
"\ S it holds rank([Fk,1N ])",4. Proof of Main Result,0,[0]
"= N. 2
The final proof of our main Theorem 3.8 is heavily based on the implicit function theorem, see e.g. (Marsden, 1974).
",4. Proof of Main Result,0,[0]
Theorem 4.6 Let Ψ :,4. Proof of Main Result,0,[0]
Rs × Rt → Rt be a continuously differentiable function.,4. Proof of Main Result,0,[0]
"Suppose (u0, v0) ∈ Rs × Rt and Ψ(u0, v0) = 0.",4. Proof of Main Result,0,[0]
"If the Jacobian matrix w.r.t. v,
JvΨ(u0, v0) =  ∂Ψ1 ∂v1 · · · ∂Ψ1∂vt ...
... ∂Ψt",4. Proof of Main Result,0,[0]
"∂v1 · · · ∂Ψt∂vt  ∈ Rt×t is non-singular at (u0, v0), then there is an open ball B(u0, ) for some > 0 and a unique function α : B(u0, )",4. Proof of Main Result,0,[0]
"→ Rt such that Ψ(u, α(u))",4. Proof of Main Result,0,[0]
"= 0 for all u ∈ B(u0, ).",4. Proof of Main Result,0,[0]
"Furthermore, α is continuously differentiable.
",4. Proof of Main Result,0,[0]
"With all the intermediate results proven above, we are finally ready for the proof of the main result.
",4. Proof of Main Result,0,[0]
"Proof of Theorem 3.8 for case I = {k + 1, . . .",4. Proof of Main Result,0,[0]
", L} Let us divide the set of all parameters of the network into two subsets where one corresponds to all parameters of all layers up to k, for that we denote u =",4. Proof of Main Result,0,[0]
"[vec(W1)T , bT1 , . . .",4. Proof of Main Result,0,[0]
", vec(Wk) T , bTk ] T , and the other corresponds to the remaining parameters, for that we denote v =",4. Proof of Main Result,0,[0]
"[vec(Wk+1)T , bTk+1, . .",4. Proof of Main Result,0,[0]
.,4. Proof of Main Result,0,[0]
", vec(WL) T , bTL] T .",4. Proof of Main Result,0,[0]
"By abuse
of notation, we write Φ(u, v) to denote Φ (
(Wl, bl) L l=1
) .
",4. Proof of Main Result,0,[0]
"Let s = dim(u), t = dim(v) and (u∗, v∗) ∈ Rs×Rt be the corresponding vectors for the critical point (W ∗l , b ∗",4. Proof of Main Result,0,[0]
l ),4. Proof of Main Result,0,[0]
L l=1.,4. Proof of Main Result,0,[0]
Let Ψ :,4. Proof of Main Result,0,[0]
"Rs × Rt → Rt be a map defined as Ψ(u, v) = ∇vΦ(u, v) ∈",4. Proof of Main Result,0,[0]
"Rt, which is the gradient mapping of Φ w.r.t.",4. Proof of Main Result,0,[0]
"all parameters of the upper layers from (k + 1) to L. Since the gradient vanishes at a critical point, it holds that Ψ(u∗, v∗) = ∇vΦ(u∗, v∗) = 0.",4. Proof of Main Result,0,[0]
The Jacobian of Ψ w.r.t.,4. Proof of Main Result,0,[0]
v is the principal submatrix of the Hessian of Φ w.r.t.,4. Proof of Main Result,0,[0]
"v, that is, JvΨ(u, v) = ∇2vΦ(u, v) ∈ Rt×t.",4. Proof of Main Result,0,[0]
"As the critical point is assumed to be non-degenerate with respect to v, it holds that JvΨ(u∗, v∗) = ∇2vΦ(u∗, v∗) is nonsingular.",4. Proof of Main Result,0,[0]
"Moreover, Ψ is continuously differentiable since Φ ∈ C2(P) due to Assumption 3.2.",4. Proof of Main Result,0,[0]
"Therefore, Ψ and (u∗, v∗) satisfy the conditions of the implicit function theorem 4.6.",4. Proof of Main Result,0,[0]
"Thus there exists an open ball B(u∗, δ1) ⊂ Rs for some δ1 > 0 and a continuously differentiable function α : B(u∗, δ1)→",4. Proof of Main Result,0,[0]
"Rt such that{
Ψ(u, α(u))",4. Proof of Main Result,0,[0]
"= 0, ∀u ∈ B(u∗, δ1) α(u∗)",4. Proof of Main Result,0,[0]
"= v∗
By assumption we have rank(W ∗l )",4. Proof of Main Result,0,[0]
#NAME?,4. Proof of Main Result,0,[0]
"[k+2, L], that is the weight matrices of the “upper” layers have full column rank.",4. Proof of Main Result,0,[0]
Note that (W ∗l ),4. Proof of Main Result,0,[0]
"L l=k+2 corresponds to the weight
matrix part of v∗ where one leaves out W ∗k+1.",4. Proof of Main Result,0,[0]
"Thus there exists a sufficiently small such that for any v ∈ B(v∗, ), the weight matrix part (Wl)Ll=k+2 of v has full column rank.",4. Proof of Main Result,0,[0]
"In particular, this, combined with the continuity of α, implies that for a potentially smaller 0 < δ2 ≤ δ1, it holds for all u ∈ B(u∗, δ2) that
Ψ(u, α(u))",4. Proof of Main Result,0,[0]
"= 0, α(u∗) = v∗,
and that the weight matrix part (Wl)Ll=k+2 of α(u) ∈",4. Proof of Main Result,0,[0]
"Rt has full column rank.
",4. Proof of Main Result,0,[0]
"Now, by Corollary 4.5 for any 0 < δ3 ≤ δ2 there exists a ũ ∈ B(u∗, δ3) such that the generated output matrix F̃k at layer k of the corresponding network parameters of ũ satisfies rank([F̃k,1N ])",4. Proof of Main Result,0,[0]
#NAME?,4. Proof of Main Result,0,[0]
"that Ψ(ũ, ṽ) = 0 and the weight matrix part (W̃l)Ll=k+2 of ṽ has full column rank.",4. Proof of Main Result,0,[0]
"Assume (ũ, ṽ) corresponds to the following representation{ ũ = [vec(W̃1)T , b̃T1 , . . .",4. Proof of Main Result,0,[0]
", vec(W̃k) T , b̃Tk ] T ∈",4. Proof of Main Result,0,[0]
"Rs
ṽ =",4. Proof of Main Result,0,[0]
"[vec(W̃k+1)T , b̃Tk+1, . .",4. Proof of Main Result,0,[0]
.,4. Proof of Main Result,0,[0]
", vec(W̃L) T , b̃TL] T ∈",4. Proof of Main Result,0,[0]
"Rt
We obtain the following Ψ(ũ, ṽ) = 0⇒ ∇Wk+1Φ",4. Proof of Main Result,0,[0]
"( (W̃l, b̃l) k l=1 ) = 0 Ψ(ũ, ṽ) = 0⇒ ∇bk+1Φ",4. Proof of Main Result,0,[0]
"( (W̃l, b̃l) k l=1 ) = 0
rank(W̃l) =",4. Proof of Main Result,0,[0]
"nl,∀ l ∈",4. Proof of Main Result,0,[0]
"[k + 2, L] rank([F̃k,1N ])",4. Proof of Main Result,0,[0]
"= N
Thus, Lemma 3.5 implies that (W̃l, b̃l)Ll=1 is a global minimum of Φ. Let p∗ = Φ (
(W̃l, b̃l) L l=1
)",4. Proof of Main Result,0,[0]
"= Φ(ũ, ṽ).",4. Proof of Main Result,0,[0]
"Note that
this construction can be done for any δ3 ∈ (0, δ2].",4. Proof of Main Result,0,[0]
"In particular, let (γr)∞r=1 be a strictly monotonically decreasing sequence such that γ1 = δ3 and limr→∞ γr = 0.",4. Proof of Main Result,0,[0]
"By Corollary 4.5 and the previous argument, we can choose for any γr > 0",4. Proof of Main Result,0,[0]
"a point ũr ∈ B(u∗, γr) such that ṽr = α(ũr) has full rank and Φ(ũr, ṽr) = p∗.",4. Proof of Main Result,0,[0]
"Moreover, as limr→∞ γr = 0, it follows that limr→∞ ũr = u∗ and as α is a continuous function, it holds with ṽr = α(ũr) that limr→∞ ṽr = limr→∞ α(ũr) = α(limr→∞ ũr) = α(u
∗) =",4. Proof of Main Result,0,[0]
v∗.,4. Proof of Main Result,0,[0]
"Thus we get limr→∞(ũr, ṽr) =",4. Proof of Main Result,0,[0]
"(u∗, v∗) and as Φ is a continuous function it holds
lim r→∞
Φ ( (ũr, ṽr) )",4. Proof of Main Result,0,[0]
"= Φ(u∗, v∗) = p∗,
as Φ attains the global minimum for the whole sequence (ũr, ṽr).
",4. Proof of Main Result,0,[0]
"Proof of Theorem 3.8 for general case In the general case I ⊆ {k + 1, . . .",4. Proof of Main Result,0,[0]
", L}, the previous proof can be easily adapted.",4. Proof of Main Result,0,[0]
"The idea is that we fix all layers in {k + 1, . . .",4. Proof of Main Result,0,[0]
", L} \ I.",4. Proof of Main Result,0,[0]
"In particular, let{ u =",4. Proof of Main Result,0,[0]
"[vec(W1)T , bT1 , . . .",4. Proof of Main Result,0,[0]
", vec(Wk) T , bTk ] T
v =",4. Proof of Main Result,0,[0]
"[vec(WI(1))T , bTI(1), . .",4. Proof of Main Result,0,[0]
.,4. Proof of Main Result,0,[0]
", vec(WI(|I|))",4. Proof of Main Result,0,[0]
"T , bTI(|I|)] T .
",4. Proof of Main Result,0,[0]
"Let s = dim(u), t = dim(v) and (u∗, v∗) ∈ Rs×Rt be the corresponding vectors at (W ∗l ,",4. Proof of Main Result,0,[0]
b ∗,4. Proof of Main Result,0,[0]
l ),4. Proof of Main Result,0,[0]
L l=1.,4. Proof of Main Result,0,[0]
Let Ψ :,4. Proof of Main Result,0,[0]
"Rs × Rt →
Rt be a map defined as Ψ(u, v) = ∇vΦ ( (Wl, bl) L l=1 ) with
Ψ(u∗, v∗) = ∇vΦ ( (W ∗l , b ∗",4. Proof of Main Result,0,[0]
l ),4. Proof of Main Result,0,[0]
L l=1 ),4. Proof of Main Result,0,[0]
0,4. Proof of Main Result,0,[0]
"The only difference is that all the layers from {k + 1, . . .",4. Proof of Main Result,0,[0]
", L} \",4. Proof of Main Result,0,[0]
I are hold fixed.,4. Proof of Main Result,0,[0]
"They are not contained in the arguments of Ψ, thus will not be involved in our perturbation analysis.",4. Proof of Main Result,0,[0]
"In this way, the full rank property of the weight matrices of these layers are preserved, which is needed to obtain the global minimum.",4. Proof of Main Result,0,[0]
We have seen that nk ≥ N,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"− 1 is a sufficient condition which leads to a rather simple structure of the critical points, in the sense that all local minima which have full rank in the layers k + 2 to L and for which the Hessian is non-degenerate on any subset of upper layers that includes layer k + 1 are automatically globally optimal.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
This suggests that suboptimal locally optimal points are either completely absent or relatively rare.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"We have motivated before that networks with a certain wide layer are used in practice, which shows that the condition nk ≥ N",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
− 1 is not completely unrealistic.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
On the other hand we want to discuss in this section how it could be potentially relaxed.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
The following result will provide some intuition about the case nk < N,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"− 1, but will not be as strong as our main result 3.8 which makes statements about a large class of critical points.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
The main idea is that with the condition nk ≥,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
N−1,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
the data is linearly separable at layer k.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"As modern neural networks are expressive enough to represent any function, see (Zhang et al., 2017) for an interesting discussion on this, one can expect that in some layer the training data becomes linearly separable.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"We prove that any critical point, for which the “learned” network outputs at any layer are linearly separable (see Definition 5.1) is a global minimum of the training error.
",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Definition 5.1 (Linearly separable vectors),5. Relaxing the Condition on the Number of Hidden Units,0,[0]
A set of vectors (xi)Ni=1 ∈,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Rd from m classes (Cj)mj=1 is called linearly separable if there exist m vectors (aj)mj=1 ∈ Rd and m scalars (bj)mj=1 ∈ R so that aTj xi + bj > 0,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
for xi ∈ Cj and aTj xi + bj < 0 for xi /∈,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Cj for every i ∈,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"[N ], j ∈",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"[m].
In this section, we use a slightly different loss function than in the previous section.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
The reason is that the standard least squares loss is not necessarily small when the data is linearly separable.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Let C1, . . .",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
", Cm denote m classes.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"We
consider the objective function Φ : P → R from (1)
Φ (
(Wl, bl) L l=1
) =",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
N∑ i=1,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
m∑ j=1,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"l ( fLj(xi)− yij ) (2)
where the loss function now takes the new form
l ( fLj(xi)− yij ) =
{ l1 ( fLj(xi)− yij )",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
xi ∈,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Cj
l2 ( fLj(xi)− yij )",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
xi /∈,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Cj
where l1, l2 penalize the deviation from the label encoding for the true class resp.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
wrong classes.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
We assume that the minimum of Φ is attained over P. Note that Φ is bounded from below by zero as l1 and l2 are non-negative loss functions.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"The results of this section are made under the following assumptions on the activation and loss function.
",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Assumptions 5.2 1.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
σ ∈ C1(R) and,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"strictly monotonically increasing.
2.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"l1 : R → R+, l1 ∈ C1, l1(a) = 0 ⇔ a ≥ 0, l′1(a) = 0⇔",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
a ≥ 0 and l′1(a),5. Relaxing the Condition on the Number of Hidden Units,0,[0]
< 0 ∀,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"a < 0
3. l2 :",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"R → R+, l2 ∈ C1, l2(a) = 0",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"⇔ a ≤ 0, l′2(a) = 0⇔ a ≤ 0 and l′2(a) > 0 ∀",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"a > 0
In classification tasks, this loss function encourages higher values for the true class and lower values for wrong classes.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"An example of the loss function that satisfies Assumption 5.2 is given as (see Figure 1):
l1(a)",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
= { a2 a ≤ 0 0,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"a ≥ 0
l2(a) = { 0 a ≤ 0",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"a2 a ≥ 0
Note that for a {+1,−1}-label encoding, +1 for the true class and −1 for all wrong classes, one can rewrite (2) as
Φ (
(Wl, bl) L l=1
) =",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
N∑ i=1,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"m∑ j=1 max{0, 1− yijfLj(xi)}2,
which is similar to the truncated squared loss (also called squared hinge loss) used in the SVM for binary classification.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Since σ and l are continuously differentiable, all the results from Lemma 2.1 still hold.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Our main result in this section is stated as follows.
",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Theorem 5.3 Let Φ : P → R+ be defined as in (2) and let the Assumptions 5.2 hold.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Then it follows:
1.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Every critical point of Φ for which the feature vectors contained in the rows of Fk are linearly separable and all the weight matrices (Wl)Ll=k+2 have full column rank is a global minimum.
2.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"If the training inputs are linearly separable then every critical point of Φ for which all the weight matrices (Wl) L l=2 have full column rank is a global minimum.
",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Note that the second statement of Theorem 5.3 can be considered as a special case of the first statement.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"In the case where L = 2 and training inputs are linearly separable, the second statement of our Theorem 5.3 recovers the similar result of (Gori & Tesi, 1992; Frasconi et al., 1997) for onehidden layer networks.
",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"Even though the assumptions of Theorem 3.4 and Theorem 5.3 are different in terms of class of activation and loss functions, their results are related.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"In fact, it is well known that if a set of vectors is linearly independent then they are linearly separable, see e.g. p.340 (Barber, 2012).",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Thus Theorem 5.3 can be seen as a direct generalization of Theorem 3.4.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"The caveat, which is also the main difference to Theorem 3.8, is that Theorem 5.3 makes only statements for all the critical points for which the problem has become separable at some layer, whereas there is no such condition in Theorem 3.8.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
"However, we still think that the result is of practical relevance, as one can expect for a sufficiently large network that stochastic gradient descent will lead to a network structure where the data becomes separable at a particular layer.",5. Relaxing the Condition on the Number of Hidden Units,0,[0]
When this happens all the associated critical points are globally optimal.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
It is an interesting question for further research if one can show directly under some architecture condition that the network outputs become linearly separable at some layer for any local minimum and thus every local minimum is a global minimum.,5. Relaxing the Condition on the Number of Hidden Units,0,[0]
Our results show that the loss surface becomes wellbehaved when there is a wide layer in the network.,6. Discussion,0,[0]
"Implicitly, such a wide layer is often present in convolutional neural networks used in computer vision.",6. Discussion,0,[0]
It is thus an interesting future research question how and if our result can be generalized to neural networks with sparse connectivity.,6. Discussion,0,[0]
We think that the results presented in this paper are a significant addition to the recent understanding why deep learning works so efficiently.,6. Discussion,0,[0]
"In particular, since in this paper we are directly working with the neural networks used in practice without any modifications or simplifications.",6. Discussion,0,[0]
The authors acknowledge support by the ERC starting grant NOLEPRO 307793.,Acknowledgment,0,[0]
"While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points.",abstractText,0,[0]
It has been argued that this is the case as all local minima are close to being globally optimal.,abstractText,0,[0]
"We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.",abstractText,0,[0]
The Loss Surface of Deep and Wide Neural Networks,title,0,[0]
"Empirical practice tends to show that modern neural networks have relatively benign loss surfaces, in the sense that training a deep network proves less challenging than the nonconvex and non-smooth nature of the optimization would naı̈vely suggest.",1. Introduction,0,[0]
"Many theoretical efforts, especially in recent years, have attempted to explain this phenomenon and, more broadly, the successful optimization of deep networks in general (Gori & Tesi, 1992; Choromanska et al., 2015; Kawaguchi, 2016; Safran & Shamir, 2016; Mei et al., 2016; Soltanolkotabi, 2017; Soudry & Hoffer, 2017; Du et al., 2017; Zhong et al., 2017; Tian, 2017; Li & Yuan, 2017; Zhou & Feng, 2017; Brutzkus et al., 2017).",1. Introduction,0,[0]
The properties of the loss surface of neural networks remain poorly understood despite these many efforts.,1. Introduction,0,[0]
"Developing of a coherent mathematical understanding of them is therefore one of the
*Equal contribution 1Department of Mathematics, Loyola Marymount University, Los Angeles, CA 90045, USA 2Department of Mathematics and Statistics, California State University, Long Beach, Long Beach, CA 90840, USA.",1. Introduction,0,[0]
"Correspondence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von Brecht <james.vonbrecht@csulb.edu>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
",1. Introduction,0,[0]
"major open problems in deep learning.
",1. Introduction,0,[0]
"We focus on investigating the loss surfaces that arise from feed-forward neural networks where rectified linear units (ReLUs) σ(x) := max(x, 0) or leaky ReLUs σα(x) := αmin(x, 0) + max(x, 0) account for all nonlinearities present in the network.",1. Introduction,0,[0]
We allow the transformations defining the hidden-layers of the network to take the form of fully connected affine transformations or convolutional transformations.,1. Introduction,0,[0]
"By employing a ReLU-based criterion we then obtain a loss with a consistent, homogeneous structure for the nonlinearities in the network.",1. Introduction,0,[0]
"We elect to use the binary hinge loss
`(ŷ, y) :",1. Introduction,0,[0]
#NAME?,1. Introduction,0,[0]
"( 1− yŷ ) (1)
for binary classification, where ŷ denote the scalar output of the network and y ∈ {−1, 1} denotes the target.",1. Introduction,0,[0]
"Similarly, for multiclass classification we use the multiclass hinge loss,
`(ŷ, r0) = ∑ r 6=r0 σ",1. Introduction,0,[0]
( 1 + ŷr − ŷr0 ),1. Introduction,0,[0]
"(2)
where ŷ = (ŷ1, . . .",1. Introduction,0,[0]
", ŷR) ∈ RR denotes the vectorial output of the network and r0 ∈ {1, . . .",1. Introduction,0,[0]
", R} denotes the target class.
",1. Introduction,0,[0]
"To see the type of structure that emerges in these networks, let Ω denote the space of network parameters and let L(ω) denote the loss.",1. Introduction,0,[0]
"Due to the choices (1,2) of network criteria, all nonlinearities involved in L(ω) are piecewise linear.
",1. Introduction,0,[0]
These nonlinearities encode a partition of parameter space Ω = Ω1 ∪ · · · ∪ΩM ∪N into a finite number of open cells,1. Introduction,0,[0]
Ωu and a closed set N of cell boundaries (c.f. figure 1).,1. Introduction,0,[0]
"A cell Ωu corresponds to a given activation pattern of the nonlinearities, and so L(ω) is smooth in the interior of cells and (potentially) non-differentiable on cell boundaries.",1. Introduction,0,[0]
"This decomposition provides a description of the smooth (i.e. Ω\N ) and non-smooth (i.e. N ) parts of parameter space.
",1. Introduction,0,[0]
We begin by showing that the loss restricted to a cell Ωu is a multilinear form.,1. Introduction,0,[0]
"As multilinear forms are harmonic functions, an appeal to the strong maximum principle shows that non-trivial optima of the loss must happen on cell boundaries (i.e. the non-differentiable region N of the parameter space).",1. Introduction,0,[0]
"In other words, ReLU networks with hinge loss criteria do not have differentiable local minima, except for those trivial ones that occur in regions of parameter space where the loss surface is perfectly flat.",1. Introduction,0,[0]
"Figure 1b) provides a visual example of such a loss.
",1. Introduction,0,[0]
As a consequence the loss function has only two types of local minima.,1. Introduction,0,[0]
"They are
• Type I (Flat): Local minima that occur in a flat (i.e. constant loss) cell or on the boundary of a flat cell.",1. Introduction,0,[0]
"• Type II (Sharp): Local minima on N that are not on the boundary of any flat cell.
",1. Introduction,0,[0]
We then investigate type I and type II local minima in more detail.,1. Introduction,0,[0]
The investigation reveals a clean dichotomy.,1. Introduction,0,[0]
"First and foremost,
Main Result 1. L(ω) > 0",1. Introduction,0,[0]
"at any type II local minimum.
",1. Introduction,0,[0]
"Importantly, if zero loss minimizers exist (which happens for most modern deep networks) then sharp local minima are always sub-optimal.",1. Introduction,0,[0]
This result applies to a quite general class of deep neural networks with fully connected or convolutional layers equipped with either ReLU or leaky ReLU nonlinearities.,1. Introduction,0,[0]
To obtain a converse we restrict our attention to fully connected networks with leaky ReLU nonlinearities.,1. Introduction,0,[0]
"Under mild assumptions on the data we have
Main Result 2. L(ω) = 0",1. Introduction,0,[0]
"at any type I local minimum, while L(ω) > 0",1. Introduction,0,[0]
"at any type II local minimum.
",1. Introduction,0,[0]
Thus flat local minima are always optimal whereas sharp minima are always sub-optimal in the case where zero loss minimizers exist.,1. Introduction,0,[0]
"Conversely, if zero loss minimizers do not exist then all local minima are sharp.",1. Introduction,0,[0]
"See figure 2 for an
illustration of such a loss surface.
",1. Introduction,0,[0]
All in all these results paint a striking picture.,1. Introduction,0,[0]
Networks with ReLU or leaky ReLU nonlinearities and hinge loss criteria have only two types of local minima.,1. Introduction,0,[0]
Sharp minima always have non-zero loss; they are undesirable.,1. Introduction,0,[0]
"Conversely, flat minima are always optimal for certain classes of networks.",1. Introduction,0,[0]
"In this case the structure of the loss (flat v.s. sharp) provides a perfect characterization of their quality (optimal v.s. suboptimal).
",1. Introduction,0,[0]
This analysis also shows that local minima generically occur in the non-smooth region of parameter space.,1. Introduction,0,[0]
"Analyzing them requires an invocation of machinery from non-smooth, non-convex analysis.",1. Introduction,0,[0]
We show how to apply these techniques to study non-smooth networks in the context of binary classification.,1. Introduction,0,[0]
"We consider three specific scenarios to illustrate how nonlinearity and data complexity affect the loss surface of multilinear networks —
• Scenario 1: A deep linear network with arbitrary data.
",1. Introduction,0,[0]
"• Scenario 2: A network with one hidden layer, leaky ReLUs and linearly separable data.
•",1. Introduction,0,[0]
"Scenario 3: A network with one hidden layer, ReLUs and linearly separable data.
",1. Introduction,0,[0]
The nonlinearities σα(x) vary from the linear regime (α = 1) to the leaky regime (0 < α < 1) and finally to the ReLU regime (α = 0) as we pass from the first to the third scenario.,1. Introduction,0,[0]
We show that no sub-optimal local minimizers exist in the first two scenarios.,1. Introduction,0,[0]
"When passing to the case of paramount interest, i.e. the third scenario, a bifurcation occurs.",1. Introduction,0,[0]
Degeneracy in the nonlinearities (i.e. α = 0) induces sub-optimal local minima in the loss surface.,1. Introduction,0,[0]
We also provide an explicit description of all such sub-optimal local optima.,1. Introduction,0,[0]
"They correspond to the occurence of dead data points, i.e. when some data points do not activate any of the neurons of the hidden layer and are therefore ignored by the network.",1. Introduction,0,[0]
Our results for the second and third scenarios provide a mathematically precise formulation of a commonplace intuitive picture.,1. Introduction,0,[0]
"A ReLU can completely “turn off,” and sub-optimal minima correspond precisely to situations in which a data point turns off all ReLUs in the hidden layer.",1. Introduction,0,[0]
"As leaky ReLUs have no completely “off” state, such networks therefore have no sub-optimal minima.
",1. Introduction,0,[0]
"Finally, in section 4 we conclude by investigating the extent to which these phenomena do, or do not, persist when passing to the multiclass context.",1. Introduction,0,[0]
The loss surface of a multilinear network with the multiclass hinge loss (2) is fundamentally different than that of a binary classification problem.,1. Introduction,0,[0]
"In particular, the picture that emerges from our two-class results does not extend to the multiclass hinge loss.",1. Introduction,0,[0]
"Nevertheless, we show how to obtain a similar picture of critical points by modifying the training strategy applied to multiclass problems.
",1. Introduction,0,[0]
Many recent works theoretically investigate the loss surface of ReLU networks.,1. Introduction,0,[0]
"The closest to ours is (Safran & Shamir, 2016), which uses ReLU nonlinearities to partition the parameter space into basins that, while similar in spirit, differ from our notion of cells.",1. Introduction,0,[0]
"Works such as (Keskar et al., 2016; Chaudhari et al., 2017) have empirically investigated the notion of “width” of a local minimizer.",1. Introduction,0,[0]
"Conjecturally, a “wide” local minimum should generalize better than a “narrow” one and might be more likely to attract the solution generated by a stochastic gradient descent algorithm.",1. Introduction,0,[0]
Our flat and sharp local minima are reminiscent of these notions.,1. Introduction,0,[0]
"Finally, some prior works have proved variants of our results in smooth situations.",1. Introduction,0,[0]
"For instance, (Brutzkus et al., 2017) derives results about the smooth local minima occurring in scenarios 2 and 3, but they do not investigate non-differentiable local minima.",1. Introduction,0,[0]
"Additionally, (Kawaguchi, 2016) considers our first scenario with a mean squared error loss instead of the hinge loss, while (Frasconi et al., 1997) considers our second scenario with a smooth version of the hinge loss and with sigmoid nonlinearities.",1. Introduction,0,[0]
Our non-smooth analogues of these results require fundamentally different techniques.,1. Introduction,0,[0]
"We prove all lemmas, theorems and corollaries in the appendix.",1. Introduction,0,[0]
We begin by describing the global structure of ReLU networks with hinge loss that arises due to their piecewise multilinear form.,2. Global Structure of the Loss,0,[0]
"Let us start by rewriting (2) as
`(ŷ,y) = −1 + R∑ r=1 σ",2. Global Structure of the Loss,0,[0]
( 1 + ŷr,2. Global Structure of the Loss,0,[0]
"− 〈y, ŷ〉 )",2. Global Structure of the Loss,0,[0]
#NAME?,2. Global Structure of the Loss,0,[0]
"+ 〈 1 , σ",2. Global Structure of the Loss,0,[0]
"( (Id− 1⊗ y)ŷ + 1 )〉 (3)
where we now view the target y ∈ {0, 1}R as a one-hot vector that encodes for the desired class.",2. Global Structure of the Loss,0,[0]
"The term 1 ⊗ y denotes the outer product between the constant vector 1 = (1, . . .",2. Global Structure of the Loss,0,[0]
", 1)T and the target, while 〈y, ŷ〉 refers to the usual Euclidean inner product.",2. Global Structure of the Loss,0,[0]
"We consider a collection (x(i),y(i)) of N labeled data points fed through a neural network with L hidden layers,
x(i,`) = σα(W (`)x(i,`−1) + b(`)) for ` ∈",2. Global Structure of the Loss,0,[0]
"[L]
ŷ(i) = V x(i,L) +",2. Global Structure of the Loss,0,[0]
"c, (4)
",2. Global Structure of the Loss,0,[0]
so that for ` ∈,2. Global Structure of the Loss,0,[0]
"[L] := {1, . . .",2. Global Structure of the Loss,0,[0]
", L} each x(i,`) refers to feature vector of the ith data point at the `th layer (with the convention that x(i,0) = x(i)) and ŷ(i) refers to the output of the network for the ith datum.",2. Global Structure of the Loss,0,[0]
"By (3) we obtain
L(ω) = −1+ ∑ i µ(i) 〈 1, σ ( (Id−1⊗y(i))ŷ(i) +1 )〉 (5)
for the loss L(ω).",2. Global Structure of the Loss,0,[0]
The positive weights µ(i) > 0,2. Global Structure of the Loss,0,[0]
"sum to one, say µ(i) = 1/N in the simplest case, but we allow for other
choices to handle those situations, such as an unbalanced training set, in which non-homogeneous weights could be beneficial.",2. Global Structure of the Loss,0,[0]
"The matrices W (`) and vector b(`) appearing in (4) define the affine transformation at layer ` of the network, and V and c in (4) denote the weights and bias of the output layer.",2. Global Structure of the Loss,0,[0]
"We allow for fully-connected as well as structured models, such as convolutional networks, by imposing the assumption that each W (`) is a matrix-valued function that depends linearly on some set of parameters ω(`) —
W (`) ( cω(`) + dω̂(`) )",2. Global Structure of the Loss,0,[0]
= cW (`) ( ω(`) ),2. Global Structure of the Loss,0,[0]
"+ dW (`) ( ω̂(`) ) ;
thus the collection
ω =",2. Global Structure of the Loss,0,[0]
"(ω(1), . . .",2. Global Structure of the Loss,0,[0]
", ω(L), V,b(1), . . .",2. Global Structure of the Loss,0,[0]
",b(L), c) ∈ Ω
represents the parameters of the network and Ω denotes parameter space.",2. Global Structure of the Loss,0,[0]
As the slope α of the nonlinearity decreases from α = 1 to α = 0,2. Global Structure of the Loss,0,[0]
the network transitions from a deep linear architecture to a standard ReLU network.,2. Global Structure of the Loss,0,[0]
"Finally, we let d` denote the dimension of the features at layer ` of the network, with the convention that d0 = d (dimension of the input data) and dL+1 = R (number of classes).",2. Global Structure of the Loss,0,[0]
We use D = d1 + . .,2. Global Structure of the Loss,0,[0]
.+,2. Global Structure of the Loss,0,[0]
dL+1,2. Global Structure of the Loss,0,[0]
for the total number of neurons.,2. Global Structure of the Loss,0,[0]
The nonlinearities σα(x) and σ(x) account for the only sources of nondifferentiabilty in the loss of a ReLU network.,2.1. Partitioning Ω into Cells,0,[0]
"To track these potential sources of nondifferentiability, for a given a data point x(i) we define the functions
s(i,`)(ω) := sign(W (`)x(i,`−1) + b`) for ` ∈",2.1. Partitioning Ω into Cells,0,[0]
"[L] s(i,L+1)(ω)",2.1. Partitioning Ω into Cells,0,[0]
:= sign ( (Id− 1⊗ y(i)) ŷ(i),2.1. Partitioning Ω into Cells,0,[0]
"+ 1 ) , (6)
where sign(x) stands for the signum function that vanishes at zero.",2.1. Partitioning Ω into Cells,0,[0]
"The function s(i,`) describes how data point x(i) activates the d` neurons at the `th layer, while s(i,L+1)(ω) describes the corresponding “activation” of the loss.",2.1. Partitioning Ω into Cells,0,[0]
"These activations take one of three possible states, the fully active state (encoded by a one), the fully inactive state (encoded by a minus one), or an in-between state (encoded by a zero).",2.1. Partitioning Ω into Cells,0,[0]
"We then collect all of these functions into a single signature function
S(ω) = ( s(1,1)(ω), . . .",2.1. Partitioning Ω into Cells,0,[0]
", s(1,L+1)(ω); . . . . .",2.1. Partitioning Ω into Cells,0,[0]
.,2.1. Partitioning Ω into Cells,0,[0]
";
s(N,1)(ω), . . .",2.1. Partitioning Ω into Cells,0,[0]
", s(N,L+1)(ω) )
to obtain a function S : Ω 7→ {−1, 0, 1}ND since there are a total of D neurons and N data points.",2.1. Partitioning Ω into Cells,0,[0]
"If S(ω) belongs to the subset {−1, 1}ND of {−1, 0, 1}ND then none of the ND entries of S(ω) vanish, and as a consequence, all of the nonlinearities are differentiable near ω; the loss L is smooth near such points.",2.1. Partitioning Ω into Cells,0,[0]
"With this in mind, for a given
u ∈ {−1, 1}ND we define the cell Ωu as the (possibly empty) set
Ωu := S−1(u) := {ω ∈ Ω : S(ω) = u}
of parameter space.",2.1. Partitioning Ω into Cells,0,[0]
"By choice L is smooth on each nonempty cell Ωu, and so the cells Ωu provide us with a partition of the parameter space
Ω =  ⋃ u∈{−1,1}ND Ωu  ⋃ N into smooth and potentially non-smooth regions.",2.1. Partitioning Ω into Cells,0,[0]
"The set N contains those ω for which at least one of the ND entries of S(ω) takes the value 0, which implies that at least one of the nonlinearities is non-differentiable at such a point.",2.1. Partitioning Ω into Cells,0,[0]
Thus N consists of points at which the loss is potentially nondifferentiable.,2.1. Partitioning Ω into Cells,0,[0]
The following lemma collects the various properties of the cells,2.1. Partitioning Ω into Cells,0,[0]
Ωu and of N that we will need.,2.1. Partitioning Ω into Cells,0,[0]
Lemma 1.,2.1. Partitioning Ω into Cells,0,[0]
"For each u ∈ {−1, 1}ND the cell Ωu is an open set.",2.1. Partitioning Ω into Cells,0,[0]
If u 6= u′ then Ωu and Ωu′ are disjoint.,2.1. Partitioning Ω into Cells,0,[0]
The set N is closed and has Lebesgue measure 0.,2.1. Partitioning Ω into Cells,0,[0]
Recall that a function φ : Rd1 × . . .,2.2. Flat and Sharp Minima,0,[0]
× Rdn → R is a multilinear form if it is linear with respect to each of its inputs when the other inputs are fixed.,2.2. Flat and Sharp Minima,0,[0]
"That is,
φ(v1, . . .",2.2. Flat and Sharp Minima,0,[0]
", cvk + dwk, . .",2.2. Flat and Sharp Minima,0,[0]
.,2.2. Flat and Sharp Minima,0,[0]
",vn) = cφ(v1, . . .",2.2. Flat and Sharp Minima,0,[0]
",vk, . . .",2.2. Flat and Sharp Minima,0,[0]
",vn)
+ dφ(v1, . . .",2.2. Flat and Sharp Minima,0,[0]
",wk, . . .",2.2. Flat and Sharp Minima,0,[0]
",vn).
",2.2. Flat and Sharp Minima,0,[0]
Our first theorem forms the basis for our analytical results.,2.2. Flat and Sharp Minima,0,[0]
"It states that, up to a constant, the loss restricted to a fixed cell Ωu is a sum of multilinear forms.
",2.2. Flat and Sharp Minima,0,[0]
Theorem 1 (Multilinear Structure of the Loss).,2.2. Flat and Sharp Minima,0,[0]
"For each cell Ωu there exist multilinear forms φu0 , . . .",2.2. Flat and Sharp Minima,0,[0]
", φ u L+1 and a constant φuL+2 such that
L|Ωu(ω(1), . . .",2.2. Flat and Sharp Minima,0,[0]
", ω(L), V,b(1), . . .",2.2. Flat and Sharp Minima,0,[0]
",b(L), c) = φu0 (ω (1), ω(2), ω(3), ω(4) .",2.2. Flat and Sharp Minima,0,[0]
. .,2.2. Flat and Sharp Minima,0,[0]
", ω(L), V )
+φu1 (b (1), ω(2), ω(3), ω(4) . . .",2.2. Flat and Sharp Minima,0,[0]
", ω(L), V )
+φu2 (b (2), ω(3), ω(4) . . .",2.2. Flat and Sharp Minima,0,[0]
", ω(L), V )
...
+φuL−1(b (L−1), ω(L), V )
+φuL(b (L), V )
+φuL+1(c)
",2.2. Flat and Sharp Minima,0,[0]
#NAME?,2.2. Flat and Sharp Minima,0,[0]
"The proof relies on the fact that the signature function S(ω) is constant inside a fixed cell Ωu, and so the network reduces
to a succession of affine transformations.",2.2. Flat and Sharp Minima,0,[0]
These combine to produce a sum of multilinear forms.,2.2. Flat and Sharp Minima,0,[0]
Appealing to properties of multilinear forms then gives two important corollaries.,2.2. Flat and Sharp Minima,0,[0]
Multilinear forms are harmonic functions.,2.2. Flat and Sharp Minima,0,[0]
"Using the strong maximum principle for harmonic functions1 we show that L does not have differentiable optima, except for the trivial flat ones.
",2.2. Flat and Sharp Minima,0,[0]
Corollary 1 (No Differentiable Extrema).,2.2. Flat and Sharp Minima,0,[0]
Local minima and maxima of the loss (5) occur only on the boundary set N or on those cells Ωu where the loss is constant.,2.2. Flat and Sharp Minima,0,[0]
"In the latter case, L|Ωu(ω) = φuL+2.
",2.2. Flat and Sharp Minima,0,[0]
"Our second corollary reveals the saddle-like structure of the loss.
",2.2. Flat and Sharp Minima,0,[0]
Corollary 2 (Saddle-like Structure of the Loss).,2.2. Flat and Sharp Minima,0,[0]
If ω ∈ Ω,2.2. Flat and Sharp Minima,0,[0]
\,2.2. Flat and Sharp Minima,0,[0]
"N and the Hessian matrix D2L(ω) does not vanish, then it must have at least one strictly positive and one strictly negative eigenvalue.
",2.2. Flat and Sharp Minima,0,[0]
These corollaries have implications for various optimization algorithms.,2.2. Flat and Sharp Minima,0,[0]
At a local minimum D2L either vanishes (flat local minima) or does not exist (sharp local minima).,2.2. Flat and Sharp Minima,0,[0]
Therefore local minima do not carry any second order information.,2.2. Flat and Sharp Minima,0,[0]
"Moreover, away from minima the Hessian is never positive definite and is typically indefinite.",2.2. Flat and Sharp Minima,0,[0]
"Thus an optimization algorithm using second-order (i.e. Hessian) information must pay close attention to both the indefinite and non-differentiable nature of the loss.
",2.2. Flat and Sharp Minima,0,[0]
To investigate type I/II minima in greater depth we must there exploit the multilinear structure of L itself.,2.2. Flat and Sharp Minima,0,[0]
"Our first result along these lines concerns type II local minima.
",2.2. Flat and Sharp Minima,0,[0]
Theorem 2.,2.2. Flat and Sharp Minima,0,[0]
"If ω is a type II local minimum then L(ω) > 0.
",2.2. Flat and Sharp Minima,0,[0]
Modern networks of the form (5) typically have zero loss global minimizers.,2.2. Flat and Sharp Minima,0,[0]
For any such network type II (i.e. sharp) local minimizers are therefore always sub-optimal.,2.2. Flat and Sharp Minima,0,[0]
A converse of theorem 2 holds for a restricted class of networks.,2.2. Flat and Sharp Minima,0,[0]
"That is, type I (i.e. flat) local minimizers are always optimal.",2.2. Flat and Sharp Minima,0,[0]
"To make this precise we need a mild assumption on the data.
",2.2. Flat and Sharp Minima,0,[0]
Definition 1.,2.2. Flat and Sharp Minima,0,[0]
"Fix α > 0 and a collection of weighted data points (µ(i),x(i),y(i)).",2.2. Flat and Sharp Minima,0,[0]
"The weighted data are rare if there exist N coeffecients λ(i) ∈ {1, α, . . .",2.2. Flat and Sharp Minima,0,[0]
", αL} and a non-zero collection of NR scalars ε(i,r) ∈ {0, 1} so that the system
ε(i) = ∑
r:y (i) r",2.2. Flat and Sharp Minima,0,[0]
"=0
ε(i,r)
∑",2.2. Flat and Sharp Minima,0,[0]
i:y (i) r =1 λ(i)µ(i)ε(i)x(i) = ∑,2.2. Flat and Sharp Minima,0,[0]
"i:y (i) r =0 λ(i)µ(i)ε(i,r)x(i)
1The strong maximum principle states that a non-constant harmonic function cannot attain a local minimum or a local maximum at an interior point of an open, connected set.
",2.2. Flat and Sharp Minima,0,[0]
The Multilinear Structure of ReLU Networks∑,2.2. Flat and Sharp Minima,0,[0]
i:y (i) r =1 λ(i)µ(i)ε(i) =,2.2. Flat and Sharp Minima,0,[0]
"∑ i:y (i) r =0 λ(i)µ(i)ε(i,r) (7)
holds ∀r ∈",2.2. Flat and Sharp Minima,0,[0]
[R].,2.2. Flat and Sharp Minima,0,[0]
"The data are generic if they are not rare.
",2.2. Flat and Sharp Minima,0,[0]
"As the possible choices of λ(i), ε(i,r) take on at most a finite set of values, rare data points (µ(i),x(i),y(i)) must satisfy one of a given finite set of linear combinations.",2.2. Flat and Sharp Minima,0,[0]
"Thus (7) represents the exceptional case, and most data are generic.",2.2. Flat and Sharp Minima,0,[0]
"For example, if the x(i) ∼ X(i) come from indepenendent samples of atomless random variables X(i) they are generic with probability one.",2.2. Flat and Sharp Minima,0,[0]
"Similarly, a small perturbation in the weights µ(i) will usually transform data from rare to generic.
Theorem 3.",2.2. Flat and Sharp Minima,0,[0]
Consider the loss (5) for a fully connected network.,2.2. Flat and Sharp Minima,0,[0]
Assume that α > 0,2.2. Flat and Sharp Minima,0,[0]
"and that the data points (x(i),y(i)) are generic.",2.2. Flat and Sharp Minima,0,[0]
Then L(ω) = 0,2.2. Flat and Sharp Minima,0,[0]
"at any type I local minimum.
",2.2. Flat and Sharp Minima,0,[0]
For most data we may pair this result with its counterpart for fully connected networks and obtain a clear picture.,2.2. Flat and Sharp Minima,0,[0]
"Desirable (zero loss) minima are always flat, while undesireable (positive loss) minima are always sharp.",2.2. Flat and Sharp Minima,0,[0]
"Analyzing suboptimal minima therefore requires handling the non-smooth case, and we now turn to this task.",2.2. Flat and Sharp Minima,0,[0]
"In this section we use machinery from non-smooth analysis (see chapter 6 of (Borwein & Lewis, 2010) for a good reference) to study critical points of the loss surface of such piecewise multilinear networks.",3. Critical Point Analysis,0,[0]
We consider three scenarios by traveling from the deep linear case (α = 1) and passing through the leaky ReLU case (0 < α < 1) before arriving at the most common case (α = 0) of ReLU networks.,3. Critical Point Analysis,0,[0]
We intend this journey to highlight how the loss surface changes as the level of nonlinearity increases.,3. Critical Point Analysis,0,[0]
"A deep linear network has a trivial loss surface, in that local and global minima coincide (see theorem 100 in the appendix for a precise statement and its proof).",3. Critical Point Analysis,0,[0]
"If we impose further assumptions, namely linearly separable data in a one-hidden layer network, this benign structure persists into the leaky ReLU regime.",3. Critical Point Analysis,0,[0]
When we arrive at α = 0,3. Critical Point Analysis,0,[0]
"a bifurcation occurs, and sub-optimal local minima suddenly appear in classical ReLU networks.
",3. Critical Point Analysis,0,[0]
"To begin, we recall that for a Lipschitz but non-differentiable function f(ω) the Clarke subdifferential ∂0f(ω) of f at a point ω ∈ Ω provides a generalization of both the gradient ∇f(ω) and the usual subdifferential ∂f(ω) of a convex function.",3. Critical Point Analysis,0,[0]
"The Clarke subdifferential is defined as follow (c.f. page 133 of (Borwein & Lewis, 2010)):
",3. Critical Point Analysis,0,[0]
Definition 2 (Clarke Subdifferential and Critical Points).,3. Critical Point Analysis,0,[0]
"Assume that a function f : Ω 7→ R is locally Lipschitz around ω ∈ Ω, and differentiable on Ω \M whereM is a
Figure 3.",3. Critical Point Analysis,0,[0]
"Illustration of the Clarke Subdifferential
set of Lebesgue measure zero.",3. Critical Point Analysis,0,[0]
"Then the convex hull
∂0f(ω) := c.h. { lim k ∇f(ωk) : ωk → ω,ωk /∈M } is the Clarke Subdifferential of f at ω.",3. Critical Point Analysis,0,[0]
"In addition, if
0 ∈ ∂0f(ω), (8)
then ω is a critical point of f in the Clarke sense.
",3. Critical Point Analysis,0,[0]
"The definition of critical point is a consistent one, in that (8) must hold whenever ω is a local minimum (c.f. page 125 of (Borwein & Lewis, 2010)).",3. Critical Point Analysis,0,[0]
Thus the set of all critical points contains the set of all local minima.,3. Critical Point Analysis,0,[0]
Figure 3 provides an illustration of the Clarke Subdifferential.,3. Critical Point Analysis,0,[0]
"It depicts a function f : R2 7→ R with global minimum at the origin, which therefore defines a critical point in the Clarke sense.",3. Critical Point Analysis,0,[0]
"While the gradient of f(x) itself does not exist at 0, its restrictions fk := f |Ωk to the four cells Ωk neighboring 0 have well-defined gradients ∇fk(0) (shown in red) at the critical point.",3. Critical Point Analysis,0,[0]
"By definition the Clarke subdifferential ∂0f(0) of f at 0 consists of all convex combinations
θ1∇f1(0) + θ2∇f2(0) + θ3∇f3(0) + θ4∇f4(0)
of these gradients; that some such combination vanishes (say, 12∇f1(0)",3. Critical Point Analysis,0,[0]
+ 1 2∇f3(0) = 0) means that 0 satisfies the definition of a critical point.,3. Critical Point Analysis,0,[0]
"Moreover, an element of the subdifferential ∂0f naturally arises from gradient descent.",3. Critical Point Analysis,0,[0]
"A gradient-based optimization path x(j+1) = x(j)−dt(j)∇f(x(j)) (shown in blue) asymptotically builds, by successive accumulation at each step, a convex combination of the∇fk whose corresponding weights θk represent the fraction of time the optimization spends in each cell.
",3. Critical Point Analysis,0,[0]
We may now show how to apply these tools in the study of ReLU Networks.,3. Critical Point Analysis,0,[0]
"We first analyze the leaky regime (0 < α < 1) and then analyze the ordinary ReLU case (α = 0).
",3. Critical Point Analysis,0,[0]
Leaky Networks (0 < α < 1): Take 0 <,3. Critical Point Analysis,0,[0]
α,3. Critical Point Analysis,0,[0]
< 1,3. Critical Point Analysis,0,[0]
"and consider the corresponding loss L(W,v,b, c) =∑
µ(i) σ",3. Critical Point Analysis,0,[0]
[ 1− y(i) {,3. Critical Point Analysis,0,[0]
"vTσα(Wx (i) + b) + c }] (9)
associated to a fully connected network with one hidden layer.",3. Critical Point Analysis,0,[0]
We shall also assume the data {x(i)} are linearly separable.,3. Critical Point Analysis,0,[0]
"In this setting we have
Theorem 4 (Leaky ReLU Networks).",3. Critical Point Analysis,0,[0]
"Consider the loss (9) with α > 0 and data x(i), i ∈",3. Critical Point Analysis,0,[0]
[N ] that are linearly separable.,3. Critical Point Analysis,0,[0]
"Assume that ω = (W,v,b, c) is any critical point of the loss in the Clarke sense.",3. Critical Point Analysis,0,[0]
"Then either v = 0 or ω is a global minimum.
",3. Critical Point Analysis,0,[0]
The loss in this scenario has two type of critical points.,3. Critical Point Analysis,0,[0]
Critical points with v = 0 correspond to a trivial network in which all data points are mapped to a constant; all other critical points are global minima.,3. Critical Point Analysis,0,[0]
"If we further assume equally weighted classes∑
i:y(i)=1
µ(i) =",3. Critical Point Analysis,0,[0]
"∑
i:y(i)=−1
µ(i)
then all local minima are global minima —
Theorem 5 (Leaky ReLU Networks with Equal Weight).",3. Critical Point Analysis,0,[0]
"Consider the loss (9) with α > 0 and data x(i), i ∈",3. Critical Point Analysis,0,[0]
[N ] that are linearly separable.,3. Critical Point Analysis,0,[0]
Assume that the µ(i) weight both classes equally.,3. Critical Point Analysis,0,[0]
"Then every local minimum of L(ω) is a global minimum.
",3. Critical Point Analysis,0,[0]
"In other words, the loss surface is trivial when 0 < α ≤ 1.
ReLU Networks (α = 0):",3. Critical Point Analysis,0,[0]
This is the case of paramount interest.,3. Critical Point Analysis,0,[0]
When passing from α > 0,3. Critical Point Analysis,0,[0]
to α = 0,3. Critical Point Analysis,0,[0]
a structural bifurcation occurs in the loss surface — ReLU nonlinearities generate non-optimal local minima even in a one hidden layer network with separable data.,3. Critical Point Analysis,0,[0]
"Our analysis provides an explicit description of all the critical points of such loss surfaces, which allows us to precisely understand the way in which sub-optimality occurs.
",3. Critical Point Analysis,0,[0]
"In order to describe this structure let us briefly assume that we have a simplified model with two hidden neurons, no output bias and uniform weights.",3. Critical Point Analysis,0,[0]
"If wk denotes the kth row of W then we have the loss
L(W,v,b) = 1 N
∑ σ(1− y(i)ŷ(i)), where
ŷ(i)",3. Critical Point Analysis,0,[0]
"= 2∑ k=1 vkσ ( 〈wk,x(i)〉+ bk ) (10)
for such a network.",3. Critical Point Analysis,0,[0]
"Each hidden neuron has an associated hyperplane 〈wk, ·〉+ bk as well as a scalar weight vk used to form the output.",3. Critical Point Analysis,0,[0]
Figure 4 shows three different local minima of such a network.,3. Critical Point Analysis,0,[0]
"The first panel, figure 4(a), shows a global minimum where all the data points have zero loss.",3. Critical Point Analysis,0,[0]
Figure 4(b) shows a sub-optimal local minimum.,3. Critical Point Analysis,0,[0]
"All unsolved data points, namely those that contribute a non-zero value to the loss, lie on the “blind side” of the two hyperplanes.",3. Critical Point Analysis,0,[0]
For each of these data points the corresponding network output ŷ(i) vanishes and so the loss is σ( 1 − y(i)ŷ(i)),3. Critical Point Analysis,0,[0]
= 1 for these unsolved points.,3. Critical Point Analysis,0,[0]
"Small perturbations of the hyperplanes or of the values of the vk do not change the fact that these data points lie on the blind side of the
two hyperplanes.",3. Critical Point Analysis,0,[0]
"Their loss will not decrease under small perturbations, and so the configuration is, in fact, a local minimum.",3. Critical Point Analysis,0,[0]
"The same reasoning shows that the configuration in figure 4(c), in which no data point is classified correctly, is also a local minimum.
",3. Critical Point Analysis,0,[0]
"Despite the presence of sub-optimal local minimizers, the local minima depicted in figure 4 are somehow trivial cases.",3. Critical Point Analysis,0,[0]
"They simply come from the fact that, due to inactive ReLUs, some data points are completely ignored by the network, and this fact cannot be changed by small perturbations.",3. Critical Point Analysis,0,[0]
The next theorem essentially shows that these are the only possible sub-optimal local minima that occur.,3. Critical Point Analysis,0,[0]
"Moreover, the result holds for the case (9) of interest and not just the simplified model.
",3. Critical Point Analysis,0,[0]
Theorem 6 (ReLU networks).,3. Critical Point Analysis,0,[0]
"Consider the loss (9) with α = 0 and data x(i), i ∈",3. Critical Point Analysis,0,[0]
[N ] that are linearly separable.,3. Critical Point Analysis,0,[0]
"Assume that ω = (W,v,b, c) is a critical point in the Clarke sense, and that x(i) is any data point that contributes a nonzero value to the loss.",3. Critical Point Analysis,0,[0]
Then for each hidden neuron k ∈,3. Critical Point Analysis,0,[0]
"[K] either
(i) 〈wk,x(i)〉+ bk ≤ 0, or (ii) vk = 0.
",3. Critical Point Analysis,0,[0]
If vk = 0 then the kth hidden neuron is unused when forming network predictions.,3. Critical Point Analysis,0,[0]
"In this case we may say the kth hyperplane is inactive, while if vk 6= 0",3. Critical Point Analysis,0,[0]
the corresponding hyperplane is active.,3. Critical Point Analysis,0,[0]
Theorem 6 therefore states that if a data point x(i) is unsolved it must lie on the blind side of every active hyperplane.,3. Critical Point Analysis,0,[0]
"So all critical points, including local minima, obey the property sketched in figure 4.
",3. Critical Point Analysis,0,[0]
"When taken together, theorems 5 and 6 provide rigorous mathematical ground for the common view that dead or inactive neurons can cause difficulties in optimizing neural networks, and that using leaky ReLU networks can overcome these difficulties.",3. Critical Point Analysis,0,[0]
"The former have sub-optimal local minimizers exactly when a data point does not activate any of the ReLUs in the hidden layer, but this situation never occurs with leaky ReLUs",3. Critical Point Analysis,0,[0]
and so neither do sub-optima minima.,3. Critical Point Analysis,0,[0]
These results give a clear illustration of how nonlinearity and data complexity combine to produce local minimizers in the loss surface for binary classification tasks.,4. Exact Penalties and Multi-Class Structure,0,[0]
"While we might try to analyze multi-class tasks by following down the same path, such an effort would unfotunately bring us to a quite different destination.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Specifically, the conclusion of theorem 6 fails for multi-class case; in the presence of three or more classes a critical point may exhibit active yet unsolved data points (c.f. figure 5).",4. Exact Penalties and Multi-Class Structure,0,[0]
"This phenomenon is inherent to multiclass tasks in a certain sense, for if we use the same features x(i,`) (c.f. (4)) in a multi-layer ReLU network but apply a different network criterion ¯̀(y, ŷ)",4. Exact Penalties and Multi-Class Structure,0,[0]
then the phenomenon persists.,4. Exact Penalties and Multi-Class Structure,0,[0]
"For example, using the one-versus-all criterion
¯̀(ŷ,y) := ∑ r µ (i,r)σ ( 1 + ŷ",4. Exact Penalties and Multi-Class Structure,0,[0]
"(i) r (−1)y (i) r ) , (11)
in place of the hinge loss (2) still gives rise to a network with non-trivial critical points (similar to figure 5) despite its more “binary” structure.",4. Exact Penalties and Multi-Class Structure,0,[0]
"In this way, the emergence of non-trivial critical points reflects the nature of multi-class tasks rather than some pathology of the hinge-loss network criterion itself.
",4. Exact Penalties and Multi-Class Structure,0,[0]
To arrive at the same destination our analysis must therefore take a more circumlocuitous route.,4. Exact Penalties and Multi-Class Structure,0,[0]
"As these counterexamples suggest, if the loss L(ω) has non-trivial critical points then we must avoid non-trivial critical points by modifing the training strategy instead.",4. Exact Penalties and Multi-Class Structure,0,[0]
"We shall employ the one-versus-all criterion (11) for this task, as this choice will allows us to directly leverage our binary analyses.
",4. Exact Penalties and Multi-Class Structure,0,[0]
"Let us begin this process by recalling that x(i,L) ( ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
", ω(L),b(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",b(L) )",4. Exact Penalties and Multi-Class Structure,0,[0]
and ŷ(i),4. Exact Penalties and Multi-Class Structure,0,[0]
"= V x(i,L) + c denote the features and predictions of the network with L hidden layers, respectively.",4. Exact Penalties and Multi-Class Structure,0,[0]
The sub,4. Exact Penalties and Multi-Class Structure,0,[0]
#NAME?,4. Exact Penalties and Multi-Class Structure,0,[0]
":= ( ω(1), . .",4. Exact Penalties and Multi-Class Structure,0,[0]
.,4. Exact Penalties and Multi-Class Structure,0,[0]
", ω(L),b(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",b(L) ) therefore determine a set of features x(i,L) for the network while the parameters V, c determine a set of one-versus-all
classifiers utilizing these features.",4. Exact Penalties and Multi-Class Structure,0,[0]
"We may write the loss for the rth class as
L(r)(ω̆,vr, cr) = ∑ µ(i,r)σ ( 1 + ŷ(i)r (−1)y (i) r ) (12)
and then form the sum over classes
L̄(ω) := (L(1) + · · ·+ L(R))(ω)
to recover the total objective.",4. Exact Penalties and Multi-Class Structure,0,[0]
We then seek to minimize L̄ by applying a soft-penalty approach.,4. Exact Penalties and Multi-Class Structure,0,[0]
"We introduce the R replicates
ω̆(r) =",4. Exact Penalties and Multi-Class Structure,0,[0]
"( ω(1,r), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
", ω(L,r),b(1,r), . .",4. Exact Penalties and Multi-Class Structure,0,[0]
.,4. Exact Penalties and Multi-Class Structure,0,[0]
",b(L,r) )",4. Exact Penalties and Multi-Class Structure,0,[0]
r ∈,4. Exact Penalties and Multi-Class Structure,0,[0]
"[R]
of the hidden-layer parameters ω̆ and include a soft `2- penaltyR ( ω̆(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
", ω̆(R) )",4. Exact Penalties and Multi-Class Structure,0,[0]
":=
R
R− 1 L∑ `=1 R∑ r=1 ‖ω(`,r)",4. Exact Penalties and Multi-Class Structure,0,[0]
"− ω̄(`)‖2 + ‖b(`,r) − b̄(`)‖2
to enforce that the replicated parameters ω(`,r),b(`,r) remain close to their corresponding means (ω̄(`), b̄(`)) across classes.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Our training strategy then proceeds to minimize the penalized loss Eγ ( ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",ω(R)
)",4. Exact Penalties and Multi-Class Structure,0,[0]
":=∑
r L(r) ( ω(r) )",4. Exact Penalties and Multi-Class Structure,0,[0]
"+ γR ( ω̆(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
", ω̆(R) ) (13)
for γ > 0",4. Exact Penalties and Multi-Class Structure,0,[0]
some parameter controlling the strength of the penalty.,4. Exact Penalties and Multi-Class Structure,0,[0]
"Remarkably, utilizing this strategy yields
Theorem 7 (Exact Penalty and Recovery of Two-Class Structure).",4. Exact Penalties and Multi-Class Structure,0,[0]
If γ > 0 then the following hold for (13) — (i),4. Exact Penalties and Multi-Class Structure,0,[0]
"The penalty is exact, that is, at any critical point( ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",ω(R) )",4. Exact Penalties and Multi-Class Structure,0,[0]
"of Eγ the equalities
ω(`,1) = · · · = ω(`,R) = ω̄(`) := 1 R R∑ r=1 ω(`,r)
b(`,1) = · · · = b(`,R) = b̄(`) := 1 R R∑ r=1 b(`,r)
",4. Exact Penalties and Multi-Class Structure,0,[0]
hold for all ` ∈,4. Exact Penalties and Multi-Class Structure,0,[0]
[L].,4. Exact Penalties and Multi-Class Structure,0,[0]
(ii),4. Exact Penalties and Multi-Class Structure,0,[0]
"At any critical point of Eγ the two-class critical point
relations 0 ∈ ∂0L(r)(ω̆,vr, cr) hold for all r ∈",4. Exact Penalties and Multi-Class Structure,0,[0]
"[R].
",4. Exact Penalties and Multi-Class Structure,0,[0]
"In other words, applying a soft-penalty approach to minimizing the original problem (12) actually yields an exact penalty method.",4. Exact Penalties and Multi-Class Structure,0,[0]
"By (i), at critical points we obtain a common set of features x(i,L) for each of the R binary classification problems.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Moreover, by (ii) these features simultaneously yield critical points
0 ∈ ∂0L(r) ( ω̆,vr, cr ) (14)
for all of these binary classification problems.",4. Exact Penalties and Multi-Class Structure,0,[0]
"The fact that (14) may fail for critical points of L̄ is responsible
for the presence of non-trivial critical points in the context of a network with one hidden layer.",4. Exact Penalties and Multi-Class Structure,0,[0]
We may therefore interpret (ii) as saying that a training strategy that uses the penalty approach will avoid pathological critical points where 0 ∈ ∂0L̄(ω) holds but (14) does not.,4. Exact Penalties and Multi-Class Structure,0,[0]
In this way the penalty approach provides a path forward for studying multi-class problems.,4. Exact Penalties and Multi-Class Structure,0,[0]
"Regardless of the number L of hidden layers, it allows us to form an understanding of the family of critical points (14) by reducing to a study of critical points of binary classification problems.",4. Exact Penalties and Multi-Class Structure,0,[0]
"This allows us to extend the analyses of the previous section to the multi-class context.
",4. Exact Penalties and Multi-Class Structure,0,[0]
We may now pursue an analysis of multi-class problems by traveling along the same path that we followed for binary classification.,4. Exact Penalties and Multi-Class Structure,0,[0]
"That is, a deep linear network (α = 1) once again has a trivial loss surface (see corollaries 100 and 101 in the appendix for precise statements and proofs).",4. Exact Penalties and Multi-Class Structure,0,[0]
"By imposing the same further assumptions, namely linearly separable data in a one-hidden layer network, we may extend this benign structure into the leaky ReLU regime.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Finally, when α = 0 sub-optimal local minima appear; we may characterize them in a manner analogous to the binary case.
",4. Exact Penalties and Multi-Class Structure,0,[0]
"To be precise, recall the loss
L(ω) = R∑ r=1",4. Exact Penalties and Multi-Class Structure,0,[0]
"L(r)(ω ) for (15)
L(r)(ω) := ∑ µ(i,r)σ ( 1− y(i,r)(〈vr,x(i,1)〉+ cr) )",4. Exact Penalties and Multi-Class Structure,0,[0]
"that results from the features x(i,1)",4. Exact Penalties and Multi-Class Structure,0,[0]
#NAME?,4. Exact Penalties and Multi-Class Structure,0,[0]
"If the positive weights µ(i,r) > 0",4. Exact Penalties and Multi-Class Structure,0,[0]
"satisfy∑
y(i,r)=1
µ(i,r) = ∑
y(i,r)=−1
µ(i,r) = 1
2
then we say that the µ(i,r) give equal weight to all classes.",4. Exact Penalties and Multi-Class Structure,0,[0]
Appealing to the critical point relations (14) yields the following corollary.,4. Exact Penalties and Multi-Class Structure,0,[0]
It gives the precise structure that emerges from the leaky regime 0,4. Exact Penalties and Multi-Class Structure,0,[0]
"< α < 1 with separable data —
Corollary 3 (Multiclass with 0 < α < 1).",4. Exact Penalties and Multi-Class Structure,0,[0]
"Consider the loss (15) and its corresponding penalty (13) with γ > 0, 0",4. Exact Penalties and Multi-Class Structure,0,[0]
"< α < 1 and data x(i), i ∈",4. Exact Penalties and Multi-Class Structure,0,[0]
[N ] that are linearly separable.,4. Exact Penalties and Multi-Class Structure,0,[0]
"(i) Assume that ω = (ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",ω(R)) is a critical point of Eγ in the Clarke sense.",4. Exact Penalties and Multi-Class Structure,0,[0]
If v(r) 6= 0 for all r ∈,4. Exact Penalties and Multi-Class Structure,0,[0]
[R] then ω is a global minimum of L and of Eγ .,4. Exact Penalties and Multi-Class Structure,0,[0]
"(ii) Assume that the µ(i,r) give equal weight to all classes.",4. Exact Penalties and Multi-Class Structure,0,[0]
"If ω = (ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",ω(R)) is a local minimum of Eγ and vr = 0 for some r ∈",4. Exact Penalties and Multi-Class Structure,0,[0]
"[R] then ω is a global minimum of L and of Eγ .
",4. Exact Penalties and Multi-Class Structure,0,[0]
"Finally, when arriving at the standard ReLU nonlinearity α = 0 a bifurcation occurs.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Sub-optimal local minimizers of
Eγ can exist, but once again the manner in which these suboptimal solutions appear is easy to describe.",4. Exact Penalties and Multi-Class Structure,0,[0]
"We let `(i,r)(ω) denote the contribution of the ith data point x(i) to the loss L(r) for the rth class, so that L(r)(ω) = ∑ i µ
(i,r)`(i,r)(ω) gives the total loss.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Appealing directly to the family of critical point relations 0 ∈ ∂0L(r) ( ω̆,vr, cr ) furnished by theorem 7 yields our final corollary in the multiclass setting.",4. Exact Penalties and Multi-Class Structure,0,[0]
Corollary 4 (Multiclass with α = 0).,4. Exact Penalties and Multi-Class Structure,0,[0]
"Consider the loss (15) and its corresponding penalty (13) with γ > 0, α = 0 and data x(i), i ∈",4. Exact Penalties and Multi-Class Structure,0,[0]
[N ] that are linearly separable.,4. Exact Penalties and Multi-Class Structure,0,[0]
"Assume that ω = (ω(1), . . .",4. Exact Penalties and Multi-Class Structure,0,[0]
",ω(R)) is any critical point of Eγ in the Clarke sense.",4. Exact Penalties and Multi-Class Structure,0,[0]
"Then `(i,r) > 0
=⇒ (vr)k σ(〈wk,x(i)〉+ bk) = 0 for all k ∈",4. Exact Penalties and Multi-Class Structure,0,[0]
[K].,4. Exact Penalties and Multi-Class Structure,0,[0]
We conclude by painting the overall picture that emerges from our analyses.,5. Conclusion,0,[0]
The loss of a ReLU network is a multilinear form inside each cell.,5. Conclusion,0,[0]
"Multilinear forms are harmonic functions, and so maxima or minima simply cannot occur in the interior of a cell unless the loss is constant on the entire cell.",5. Conclusion,0,[0]
This simple harmonic analysis reasoning leads to the following striking fact.,5. Conclusion,0,[0]
"ReLU networks do not have differentiable minima, except for trivial cases.",5. Conclusion,0,[0]
"This reasoning is valid for any convolutional or fully connected network, with plain or leaky ReLUs, and with binary or multiclass hinge loss.",5. Conclusion,0,[0]
"Dealing with non-differentiable minima is therefore not a technicality; it is the heart of the matter.
",5. Conclusion,0,[0]
"Given this dichotomy between trivial, differentiable minima on one hand and nontrivial, nondifferentiable minima on the other, it is natural to try and characterise these two classes of minima more precisely.",5. Conclusion,0,[0]
"We show that global minima with zero loss must be trivial, while minima with nonzero loss are necessarily nondifferentiable for many fully connected networks.",5. Conclusion,0,[0]
"In particular, if a network has no zero loss minimizers then all minima are nondifferentiable.
",5. Conclusion,0,[0]
"Finally, our analysis clearly shows that local minima of ReLU networks are generically nondifferentiable.",5. Conclusion,0,[0]
"They cannot be waved away as a technicality, so any study of the loss surface of such network must invoke nonsmooth analysis.",5. Conclusion,0,[0]
We show how to properly use this machinery (e.g. Clark subdifferentials) to study ReLU networks.,5. Conclusion,0,[0]
Our goal is twofold.,5. Conclusion,0,[0]
"First, we prove that a bifurcation occurs when passing from leaky ReLU to ReLU nonlinearities, as suboptimal minima suddenly appear in the latter case.",5. Conclusion,0,[0]
"Secondly, and perhaps more importantly, we show how to apply nonsmooth analysis in familiar settings so that future researchers can adapt and extend our techniques.",5. Conclusion,0,[0]
We study the loss surface of neural networks equipped with a hinge loss criterion and ReLU or leaky ReLU nonlinearities.,abstractText,0,[0]
Any such network defines a piecewise multilinear form in parameter space.,abstractText,0,[0]
"By appealing to harmonic analysis we show that all local minima of such network are non-differentiable, except for those minima that occur in a region of parameter space where the loss surface is perfectly flat.",abstractText,0,[0]
Non-differentiable minima are therefore not technicalities or pathologies; they are heart of the problem when investigating the loss of ReLU networks.,abstractText,0,[0]
"As a consequence, we must employ techniques from nonsmooth analysis to study these loss surfaces.",abstractText,0,[0]
We show how to apply these techniques in some illustrative cases.,abstractText,0,[0]
The Multilinear Structure of ReLU Networks,title,0,[0]
Natural language understanding seeks to create models that read and comprehend text.,1 Introduction,0,[0]
"A common strategy for assessing the language understanding capabilities of comprehension models is to demonstrate that they can answer questions about documents they read, akin to how reading comprehension is tested in children when they are learning to read.",1 Introduction,0,[0]
"After reading a document, a reader usually can not reproduce
the entire text from memory, but often can answer questions about underlying narrative elements of the document: the salient entities, events, places, and the relations between them.",1 Introduction,0,[0]
"Thus, testing understanding requires creation of questions that examine high-level abstractions instead of just facts occurring in one sentence at a time.
",1 Introduction,0,[0]
"Unfortunately, superficial questions about a document may often be answered successfully (by both humans and machines) using a shallow pattern match-
ar X
iv :1
71 2.
07 04
0v 1
[ cs
.C",1 Introduction,0,[0]
"L
] 1
9 D
ec 2
ing strategies or guessing based on global salience.",1 Introduction,0,[0]
"In the following section, we survey existing QA datasets, showing that they are either too small or answerable by shallow heuristics (Section 2).",1 Introduction,0,[0]
"On the other hand, questions which are not about the surface form of the text, but rather about the underlying narrative, require the formation of more abstract representations about the events and relations expressed in the course of the document.",1 Introduction,0,[0]
"Answering such questions requires that readers integrate information which may be distributed across several statements throughout the document, and generate a cogent answer on the basis of this integrated information.",1 Introduction,0,[0]
"That is, they test that the reader comprehends language, not just that it can pattern match.",1 Introduction,0,[0]
"We present a new task and dataset, which we call NarrativeQA, which will test and reward artificial agents approaching this level of competence (Section 3).
",1 Introduction,0,[0]
"The dataset consists of stories, which are books and movie scripts, with human written questions and answers based solely on human-generated abstractive summaries.",1 Introduction,0,[0]
"For the RC tasks, questions may be answered using just the summaries or the full story text.",1 Introduction,0,[0]
We give a short example of a sample movie script from this dataset in Figure 1.,1 Introduction,0,[0]
Fictional stories have a number of advantages as a domain.,1 Introduction,0,[0]
"First, they are largely self-contained: beyond the basic fundamental vocabulary of English, all the information about salient entities and concepts required to understand the narrative is present in the document, with the expectation that a reasonably competent language user would be able to understand it.1 Second, story summaries are abstractive and generally written by independent authors who know the work only as a reader.",1 Introduction,0,[0]
We make the dataset available online.2,1 Introduction,0,[0]
"There are a large number of datasets and associated tasks available for the training and evaluation of read-
1For example, new names and words may be coined by the author (e.g. “muggle” in Harry Potter novels) but the reader need only appeal to the book itself to understand the meaning of these concepts, and their place in the narrative.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension, and is in part tested as part of the question answering tasks we present.
2http://deepmind.com/publications
ing comprehension models.",2 Review of Reading Comprehension Datasets and Models,0,[0]
We summarize the key features of a collection of popular recent datasets in Table 1.,2 Review of Reading Comprehension Datasets and Models,0,[0]
"In this section, we briefly discuss the nature and limitations of these datasets and their associated tasks.
",2 Review of Reading Comprehension Datasets and Models,0,[0]
"MCTest (Richardson et al., 2013) is a collection of short stories, each with multiple questions.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Each such question has set of possible answers, one of which is labelled as correct.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"While this could be used as a QA task, the MCTest corpus is in fact intended as an answer selection corpus.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"The data is human generated, and the answers can be phrases or sentences.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size.
",2 Review of Reading Comprehension Datasets and Models,0,[0]
"In contrast, CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al., 2016), and BookTest (Bajgar et al., 2016) each provide large amounts of question–answer pairs.",2 Review of Reading Comprehension Datasets and Models,0,[0]
Questions are Cloze-form (predict the missing word) and are produced from either short abstractive summaries (CNN/Daily Mail) or from next sentence in the document the context was taken from (CBT and BookTest).,2 Review of Reading Comprehension Datasets and Models,0,[0]
"The tasks associated with these datasets are all selecting an answer from a set of options, which is explicitly provided for CBT and BookTest, and is implicit for CNN/Daily Mail, as the answers are always entities from the document.",2 Review of Reading Comprehension Datasets and Models,0,[0]
This significantly favors models that operate by pointing to a particular token (or type).,2 Review of Reading Comprehension Datasets and Models,0,[0]
"Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) (Kadlec et al., 2016), exploit precisely this bias in the data.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"However, these models are inappropriate for answers requiring synthesis of a new answer.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event.
",2 Review of Reading Comprehension Datasets and Models,0,[0]
"SQuAD (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"A large number of a questions and answers are provided for a set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words from the document.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Although the answers are not just single word/entity answers, many plausible questions for assessing RC cannot be asked
because no document span would contain its answer.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"While they provide a large number of questions, these are from a relatively small number of documents, which are themselves fairly short, thereby limiting the lexical and topical diversity models trained on this data can cope with.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"While the answers are multiword phrases, the spans are generally short and rarely cross sentence boundaries.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Simple models scoring and/or extracting candidate spans conditioned on the question and superficial signal from the rest of the document do well (Seo et al., 2016, e.g.).",2 Review of Reading Comprehension Datasets and Models,0,[0]
"These models will not trivially generalize to problems where the answers are not spans in the document, supervision for spans is not provided, or several discontinuous spans are needed to generate a correct answer.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"This restricts the scalability and applicability of models doing well on SQuAD or NewsQA to more complex problems.
",2 Review of Reading Comprehension Datasets and Models,0,[0]
MS MARCO,2 Review of Reading Comprehension Datasets and Models,0,[0]
"(Nguyen et al., 2016) presents a bolder challenge: questions are paired with sets of snippets (“context passages”) that contain the information necessary to answer the question, and answers are free-form human generated text.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"However, as no restriction was placed on annotators preventing them from copying answers from source documents, many answers are in fact verbatim copies of short spans from the context passages.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Models which do well on SQuAD (e.g. Wang and Jiang (2016), Weissenborn et al. (2017)), extracting spans or pointing, do well here too, and the same concerns as above about the general applicability of solutions to this dataset to
larger reading comprehension problems applies.",2 Review of Reading Comprehension Datasets and Models,0,[0]
SearchQA,2 Review of Reading Comprehension Datasets and Models,0,[0]
"(Dunn et al., 2017) is a recent dataset in which the context for each question is a set of documents retrieved by a search engine using the question as the query.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"However, in contrast with previous datasets neither questions nor answers were produced by annotating the context documents, but rather the context documents were retrieved after collecting pre-existing question–answer pairs.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"As such, it is not open to same annotation bias as the datasets discussed above.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"However, upon examining answers in the Jeopardy data used to construct this dataset, one finds that 80% of answers are bigrams or unigrams, and 99% are 5 tokens or fewer.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Of a sample of 100 answers, 72% are named entities, all are short noun-phrases.
",2 Review of Reading Comprehension Datasets and Models,0,[0]
Summary of Limitations.,2 Review of Reading Comprehension Datasets and Models,0,[0]
We see several limitations of the scope and depth of the RC problems in existing datasets.,2 Review of Reading Comprehension Datasets and Models,0,[0]
"First, several datasets are small (MCTest) or not overly naturalistic (bAbI; Weston et al. (2015)).",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Second, in more naturalistic documents, a majority of questions require only a single sentence to locate supporting information for answering (Chen et al., 2016; Rajpurkar et al., 2016).",2 Review of Reading Comprehension Datasets and Models,0,[0]
"This, we suspect, is largely an artifact of the question generation methodology, in which annotators have created questions from a context document, or where context documents that explicitly answer a question are identified using a search engine.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Although the factoidlike Jeopardy questions of SearchQA also appears
to favor questions answerable with local context.",2 Review of Reading Comprehension Datasets and Models,0,[0]
"Finally, we see further evidence of the superficiality of the questions in the architectures that have evolved to solve them, which tend to exploit span selection based on representations derived from local context and the query (Seo et al., 2016; Wang et al., 2017).",2 Review of Reading Comprehension Datasets and Models,0,[0]
"In this section, we introduce our new dataset, NarrativeQA, which addresses many of the limitations identified in existing datasets.",3 NarrativeQA: A New Dataset,0,[0]
From the above discussed features and limitations,3.1 Desiderata,0,[0]
", we define our desiderata as follows.",3.1 Desiderata,0,[0]
We wish to construct a dataset with a large number of question– answer pairs based on either a large number of supporting documents or from a smaller collection of large documents.,3.1 Desiderata,0,[0]
This permits the training of neural network-based models over word embeddings and provide decent lexical coverage and diversity.,3.1 Desiderata,0,[0]
"The questions and answers should be natural, unconstrained, and human generated, and answering questions should frequently require reference to several parts or a larger span of the context document rather than superficial representations of local context.",3.1 Desiderata,0,[0]
"Furthermore, we want annotators to privilege writing answers expressed in their own words, and consider higher-level relations between entities, places, and events, rather than copy short spans of the document.
",3.1 Desiderata,0,[0]
"Furthermore, we want to evaluate models both on the fluency and correctness of generated free-form answers, and as an answer selection problem, which requires the provision of sensible distractors to the correct answer.",3.1 Desiderata,0,[0]
"Finally, the scope and complexity of the QA problem should be such that current models struggle, while humans are capable of solving the task correctly, so as to motivate further research into the development of models seeking human reading comprehension ability.",3.1 Desiderata,0,[0]
"We will consider complex, self-contained narratives as our documents/stories.",3.2 Data Collection Method,0,[0]
"To make the annotation tractable and lead annotators towards asking nonlocalized questions, we will only provide them human written summaries of the stories for generating the question–answer pairs.
",3.2 Data Collection Method,0,[0]
We present both books and movie scripts as stories in our dataset.,3.2 Data Collection Method,0,[0]
Books were collected from Project Gutenberg3 and movie scripts scraped from the web.4,3.2 Data Collection Method,0,[0]
We matched our stories with plot summaries from Wikipedia using titles and verified the matching with help from human annotators.,3.2 Data Collection Method,0,[0]
"The annotators were asked to determine if both the story and the summary refer to a movie or a book (as some books are made into movies), or if they are the same part in a series produced in the same year.",3.2 Data Collection Method,0,[0]
"In this way we obtained 1,567 stories.",3.2 Data Collection Method,0,[0]
"This provides with a smaller set of documents, compared to the other datasets, but the documents are long which provides us with good lexical coverage and diversity.",3.2 Data Collection Method,0,[0]
"The bottleneck for obtaining a larger number of publicly available stories was finding corresponding summaries.
",3.2 Data Collection Method,0,[0]
Annotators on Amazon Mechanical Turk were instructed to write 10 question–answer pairs each based solely on a given summary.,3.2 Data Collection Method,0,[0]
"Reading and annotating summaries is tractable unlike writing questions and answers based on the full stories, and moreover, as the annotators never see the full stories we are much less likely to get questions and answers which are extracted from a localized context.
",3.2 Data Collection Method,0,[0]
Annotators were instructed to imagine that they are writing questions to test students who have read the full stories but not the summaries.,3.2 Data Collection Method,0,[0]
"We required questions that are specific enough, given the length and complexity of the narratives, and to provide a diverse set of questions about characters, events, why this happened, and so on.",3.2 Data Collection Method,0,[0]
"Annotators were encouraged to use their own words and we prevented them from copying.5 We asked for answers that are grammatical, complete sentences, and explicitly allowed short answers (one word, or a few-word phrase, or a short sentence) as we think that answering with a full sentence is frequently perceived as artificial when asking about factual information.",3.2 Data Collection Method,0,[0]
"Annotators were asked to avoid extra, unnecessary information in the question or the answer, and to avoid yes/no questions or questions about the author or the actors.
",3.2 Data Collection Method,0,[0]
"About 30 question–answer pairs per summary
3http://www.gutenberg.org/ 4Mainly from http://www.imsdb.com/, but
also http://www.dailyscript.com/, http: //www.awesomefilm.com/.
5This was done both through instructions and Javascript hard limitations on the annotation site.
were obtained.",3.2 Data Collection Method,0,[0]
The result is a collection of human written natural questions and answers.,3.2 Data Collection Method,0,[0]
"As we have multiple questions per summary/story, this allows us to consider answer selection (from among the 30) as a simpler version of the QA than answer generation from scratch.",3.2 Data Collection Method,0,[0]
"Answer selection (Hewlett et al., 2016) and multiple-choice question answering (Richardson et al., 2013; Hill et al., 2016) are frequently used.
",3.2 Data Collection Method,0,[0]
"We additionally collected a second reference answer for each question by asking annotators to judge whether a question is answerable, given the summary, and provide an answer if it was.",3.2 Data Collection Method,0,[0]
All but 2.3% of the questions were judged as answerable.,3.2 Data Collection Method,0,[0]
"We collected 1,567 stories, evenly split between books and movie scripts.",3.3 Core Statistics,0,[0]
"We partitioned the dataset into non-overlapping training, validation, and test portions, along stories/summaries.",3.3 Core Statistics,0,[0]
"See Table 2 for detailed statistics.
",3.3 Core Statistics,0,[0]
"The dataset contains 46,765 question–answer pairs.",3.3 Core Statistics,0,[0]
"The questions are grammatical questions written by human annotators, average 9.8 tokens in length, and are mostly formed as ‘WH’-questions (see Table 3).",3.3 Core Statistics,0,[0]
We categorized a sample of 300 questions in Table 4.,3.3 Core Statistics,0,[0]
We observe a good variety of question types.,3.3 Core Statistics,0,[0]
"An interesting category are questions which ask for something related to or occurring together/before/after with an event, of which there are about 15%.
",3.3 Core Statistics,0,[0]
"Answers in the dataset are human written, short, averaging 4.73 tokens, but not restricted to spans from the documents.",3.3 Core Statistics,0,[0]
"There are 44.05% and 29.57% answers that appear as spans of the summaries and the stories, respectively; as expected, lower proportion of answers are spans on stories compared to summaries on which they were constructed.",3.3 Core Statistics,0,[0]
"We present tasks varying in their scope and complexity: we consider either the summary or the story as context, and for each we evaluate answer generation and answer selection.
",3.4 Tasks,0,[0]
The task of answering questions based on summaries is similar in scope to previous datasets.,3.4 Tasks,0,[0]
"However, summaries contain more complex relationships and timelines than news articles or short paragraphs from the web and thus provide a task different in nature.",3.4 Tasks,0,[0]
"We hope that NarrativeQA will motivate the
design of architectures capable of modeling such relationships.",3.4 Tasks,0,[0]
"This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents.
",3.4 Tasks,0,[0]
"The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts).",3.4 Tasks,0,[0]
This task is at present intractable for existing neural models out of the box.,3.4 Tasks,0,[0]
"We further discuss the challenges and possible approaches in the following sections.
",3.4 Tasks,0,[0]
We require the use of metrics for generated text.,3.4 Tasks,0,[0]
"We evaluate using Bleu-1, Bleu-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and Rouge-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other.",3.4 Tasks,0,[0]
We also evaluate our models using a ranking metric.,3.4 Tasks,0,[0]
This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers.,3.4 Tasks,0,[0]
We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7,3.4 Tasks,0,[0]
"In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating several baselines based on information retrieval (IR) techniques and neural models.",4 Baselines and Oracles,0,[0]
"Since neural models use quite different processes for generating answers (e.g., predicting a single word or entity, selecting a span of the document context, or open generation of the answer sequence), we present results on each.",4 Baselines and Oracles,0,[0]
We also report the human performance by scoring the second reference answer against the first.,4 Baselines and Oracles,0,[0]
We consider basic IR baselines which retrieve an answer by selecting a span of tokens from the context document based on a similarity measure between the candidate span and a query.,4.1 Simple IR Baselines,0,[0]
We compare two queries: the question and (as an oracle) the gold standard answer.,4.1 Simple IR Baselines,0,[0]
"The answer oracle provides an upper bound
6We lowercase both the candidates and the references and remove the end of sentence marker and the final full stop.
",4.1 Simple IR Baselines,0,[0]
"7MRR is the mean over examples of 1/r, where r ∈ {1, 2, . . .",4.1 Simple IR Baselines,0,[0]
"} is the rank of the correct answer among candidates.
",4.1 Simple IR Baselines,0,[0]
"First token Frequency
What 38.04% Who 23.37% Why 9.78% How 8.85% Where 7.53% Which 2.21% How many/much 1.80% When 1.67% In 1.19% OTHER 5.57%
Table 3: Frequency of first token of the question in the training set.
on the performance of span retrieval models, including the neural models discussed below.",4.1 Simple IR Baselines,0,[0]
"When using the question as the query, we obtain generalization results of IR methods.",4.1 Simple IR Baselines,0,[0]
"Test set results are computed by extracting either 4-gram, 8-gram, or full-sentence spans according to the best performance on the validation set.8
We consider three similarity metrics for extracting spans: Bleu-1, Rouge-L, and the cosine similarity between bag-of-words embedding of the query and the candidate span using pre-trained GloVe word embeddings (Pennington et al., 2014).",4.1 Simple IR Baselines,0,[0]
"As a first benchmark we consider a simple bidirectional LSTM sequence to sequence (Seq2Seq) model (Sutskever et al., 2014) predicting the answer directly from the query.",4.2 Neural Benchmarks,0,[0]
"Importantly, we provide no context information from either summary or story.",4.2 Neural Benchmarks,0,[0]
"Such a model might classify the question and predict an answer of similar topic or category.
",4.2 Neural Benchmarks,0,[0]
Previous reading comprehension tasks such as CNN/Daily Mail motivated models constrained to predicting a single token from the input sequence.,4.2 Neural Benchmarks,0,[0]
"The AS Reader (Kadlec et al., 2016) considers the entire context and predicts a distribution over unique word types.",4.2 Neural Benchmarks,0,[0]
"We adapt the model for sequence prediction by using an LSTM sequence decoder and choosing a token from the input at each step of the
8Note that we do not consider the span’s context when computing the MRR for IR baselines, as the candidate spans (i.e. all answers to questions on the story) are given and simply ranked by their similarity to the query.
output sequence.",4.2 Neural Benchmarks,0,[0]
"As a span-prediction model we consider a simplified version of the Bi-Directional Attention Flow network (Seo et al., 2016).",4.2 Neural Benchmarks,0,[0]
We omit the character embedding layer and learn a mapping from words to a vector space rather than making use of pre-trained embeddings; and we use a single layer bi-directional LSTM to model interactions among context words conditioned on the query (modelling layer).,4.2 Neural Benchmarks,0,[0]
"As proposed, we adopt the output-layer tailored for spanprediction and leave the rest unchanged.",4.2 Neural Benchmarks,0,[0]
"It was not our aim to use the state-of-the-art model for other datasets but rather to provide a strong benchmark.
",4.2 Neural Benchmarks,0,[0]
Span prediction models can be trained by obtaining supervision on the training set from the oracle IR model.,4.2 Neural Benchmarks,0,[0]
We use start and end indices of the span achieving the highest Rouge-L score with respect to the reference answers as labels on the training set.,4.2 Neural Benchmarks,0,[0]
The model is then trained to predict these spans by maximizing the probability of the indices.,4.2 Neural Benchmarks,0,[0]
"The design of the NarrativeQA dataset makes the straight-forward application of the existing neural architectures computationally infeasible, as this would require running an recurrent neural network on sequences of hundreds of thousands of time steps or computing a distribution over the entire input for attention, as is common.
",4.3 Neural Benchmarks on Stories,0,[0]
"We split the task into two steps: first, we retrieve a small number of relevant passages from the story using an IR system, and subsequently, apply one of
the neural models above on the resulting document.",4.3 Neural Benchmarks on Stories,0,[0]
The question becomes the query for retrieval.,4.3 Neural Benchmarks on Stories,0,[0]
"This IR problem is much harder that traditional document retrieval, as the documents, the passages here, are very similar, and the question is short and entities mentioned likely occur many times in the story.
",4.3 Neural Benchmarks on Stories,0,[0]
Our retrieval system considers chunks of 200 words from story and computes representations for all chunks and the query.,4.3 Neural Benchmarks on Stories,0,[0]
We then select a varying number of such chunks based on their similarity to the query.,4.3 Neural Benchmarks on Stories,0,[0]
We experiment with different representations and similarity measures in Section 5.,4.3 Neural Benchmarks on Stories,0,[0]
"Finally, we concatenate the selected chunks in the correct temporal order and insert delimiters between them to obtain a much shorter document.",4.3 Neural Benchmarks on Stories,0,[0]
"For span prediction models, we then further select a span from the retrieved chunks as described in Section 4.2.",4.3 Neural Benchmarks on Stories,0,[0]
"In this section, we describe the data prepraration methodology we used, and experimental results on the summary-reading task as well as the full story task.",5 Experiments,0,[0]
The provided narratives contain a large number of named entities (such as names of characters or places).,5.1 Data Preparation,0,[0]
"Inspired by Hermann et al. (2015), we replace such entities with markers, such as
@entity42.",5.1 Data Preparation,0,[0]
These markers are permuted during training and testing so that none of their embeddings learn a specific entity’s representation.,5.1 Data Preparation,0,[0]
"This allows us to build representations for entities from stories that were never seen in training, since they are given a specific identifier (to differentiate them from other entities in the document) from a set of generic identifiers re-used across documents.",5.1 Data Preparation,0,[0]
Entities are replaced according to a simple heuristic based on capital first character and the respective word not appearing in lowercase.,5.1 Data Preparation,0,[0]
Reading comprehension of summaries is similar to a number of previous reading comprehension tasks where questions were constructed based on the context document.,5.2 Reading Summaries Only,0,[0]
"However, plot summaries tend to contain more intricate event time lines and a larger number of characters, and in this sense, are more complex to follow than news articles or paragraphs from Wikipedia.",5.2 Reading Summaries Only,0,[0]
"See Table 5 for the results.
",5.2 Reading Summaries Only,0,[0]
"Given that questions were constructed based on the summaries, we expected that both neural models and span-selection models would perform well.",5.2 Reading Summaries Only,0,[0]
"This is indeed the case, with the neural span prediction model significantly outperforming all other proposed methods.",5.2 Reading Summaries Only,0,[0]
"However, there remains a significant room for improvement when compared with the oracle and human scores.
",5.2 Reading Summaries Only,0,[0]
"Both the plain sequence to sequence model and the AS Reader, successfully applied to the CNN/DailyMail reading comprehension task, also perform well on this task.",5.2 Reading Summaries Only,0,[0]
"We observe that the AS Reader tends to copy subsequent tokens from the context, thus behaving like a span prediction model.",5.2 Reading Summaries Only,0,[0]
An additional inductive bias results in higher performance for the span prediction model.,5.2 Reading Summaries Only,0,[0]
"Similar observations between AS Reader and span models have also been made by Wang and Jiang (2016).
",5.2 Reading Summaries Only,0,[0]
"Note that we have tuned each model separately on the development set twice, once selecting the best model based on Rouge-L and report the first four metrics, and a second time selecting based on MRR.",5.2 Reading Summaries Only,0,[0]
"Table 6 summarizes the results on the full NarrativeQA task, where the context documents are full stories.",5.3 Reading Full Stories Only,0,[0]
"As expected (and desired), we observe a decline in performance of the span-selection oracle IR model, compared with the results on summaries.",5.3 Reading Full Stories Only,0,[0]
This is unsurprising as the questions were constructed on summaries and confirms the initial motivation for designing this task.,5.3 Reading Full Stories Only,0,[0]
"As previously, we considered all spans of a given length across the entire story for
this model.",5.3 Reading Full Stories Only,0,[0]
"For short answers of one or two words— typically main characters in a story—the candidate, i.e. the closest span to the reference answer, is easily found due to being mentioned throughout the text.",5.3 Reading Full Stories Only,0,[0]
"For longer answers it becomes much less likely, compared to the summaries, that a high-scoring span can be found in the story.",5.3 Reading Full Stories Only,0,[0]
"Note that this distinguishes NarrativeQA from many of the reviewed datasets.
",5.3 Reading Full Stories Only,0,[0]
"In our IR plus neural two-step approach to the task, we first retrieve relevant chunks of the stories and then apply existing reading comprehension models.",5.3 Reading Full Stories Only,0,[0]
"We use the questions to guide the IR system for chunk extraction, with the results of the standalone IR baselines giving an indication of the difficulty of this aspect of the task.",5.3 Reading Full Stories Only,0,[0]
The retrieval quality has a direct effect on the performance of all neural models; a challenge which models on summaries are not presented with.,5.3 Reading Full Stories Only,0,[0]
"We considered several approaches to chunk selection: we retrieve chunks based on the highest Rouge-L or Bleu-1 scoring span with respect to the question in the story; comparing topic distributions from an LDA model (Blei et al., 2003) between questions and chunks according to their symmetric Kullback–Leibler divergence.",5.3 Reading Full Stories Only,0,[0]
"Finally, we also consider the cosine similarity of TF-IDF representations.
",5.3 Reading Full Stories Only,0,[0]
"We found that this approach lead to the best performance of the subsequently applied model on the validation set, irrespective of the number of chunks.",5.3 Reading Full Stories Only,0,[0]
"Note that we used the answer as the query on the training, and the question for validation and test.
",5.3 Reading Full Stories Only,0,[0]
"Given the retrieved chunks, we experimented with several neural models using them as context.",5.3 Reading Full Stories Only,0,[0]
"The AS Reader, which was the better-performing model on the summaries task, underperforms the simple no-context Seq2Seq baseline (shown in Table 5) in terms of MRR.",5.3 Reading Full Stories Only,0,[0]
"While is does slightly better on the other metrics, it clearly fails to make use of the retrieved context to gain a distinctive margin over the no-context Seq2Seq model.",5.3 Reading Full Stories Only,0,[0]
"Increasing the number of retrieved chunks, and thereby recall of possibly relevant parts of the story, had only a minor positive effect.",5.3 Reading Full Stories Only,0,[0]
The span prediction model—which here also uses selected chunks for context—does especially poorly in this setup.,5.3 Reading Full Stories Only,0,[0]
"While this model provided the best neural results on the summaries task, we suspect that its performance was particularly badly hurt by the fact that there is so little lexical and grammatical overlap between the source of the questions (summaries) and the context provided (stories).",5.3 Reading Full Stories Only,0,[0]
"As with the AS Reader, we observed no significant differences for varying number of chunks.
",5.3 Reading Full Stories Only,0,[0]
"These results leave us a large gap to human performance, highlighting the success of our design objective to build a task that is realistic and straightforward for humans while very difficult for current reading comprehension models.",5.3 Reading Full Stories Only,0,[0]
We find that the proposed dataset meets the desiderata we set out in Section 3.1.,6 Qualitative Analysis and Challenges,0,[0]
"In particular, we constructed a dataset with a number of long documents, characterised by good lexical coverage and diversity.",6 Qualitative Analysis and Challenges,0,[0]
The questions and answers are human generated and natural sounding.,6 Qualitative Analysis and Challenges,0,[0]
"And, based on a small manual examination (of ‘Ghostbusters II’, ‘Airplane’, ‘Jacob’s Ladder’), only a small number of questions and answers are shallow paraphrases of sentences in the full document.",6 Qualitative Analysis and Challenges,0,[0]
"Most questions require reading segments at least several paragraphs long, and in some cases even multiple segments spread throughout the story.
",6 Qualitative Analysis and Challenges,0,[0]
"Computational challenges identified in Section 5.3 naturally suggest a retrieval procedure as the first step.
",6 Qualitative Analysis and Challenges,0,[0]
We found that the retrieval is challenging even for humans not familiar with the presented narrative.,6 Qualitative Analysis and Challenges,0,[0]
"In particular, the task often requires referring to larger parts of the story, in addition to knowing at least some background about entities.",6 Qualitative Analysis and Challenges,0,[0]
"This makes the search procedure, based on only a short question, a challenging and interesting task in itself.
",6 Qualitative Analysis and Challenges,0,[0]
"We show example question–answer pairs in Figures 1, 2, 3.",6 Qualitative Analysis and Challenges,0,[0]
These examples were chosen from a small set of manually annotated question–answer pairs to be representative of this collection.,6 Qualitative Analysis and Challenges,0,[0]
"In particular, the examples show that larger parts of the story are required to answer questions.",6 Qualitative Analysis and Challenges,0,[0]
Consider Figure 3.,6 Qualitative Analysis and Challenges,0,[0]
"While the relevant paragraph depicting the injury appears early on, it is not until the next snippet (which appears at the end of the narrative) that the lethal consequences of the injury are revealed.",6 Qualitative Analysis and Challenges,0,[0]
This illustrates an iterative reasoning process as well as extremely long temporal dependencies we encountered during manual annotation.,6 Qualitative Analysis and Challenges,0,[0]
"As shown in Figure 1, reading comprehension on movie scripts requires understanding of written dialogue.",6 Qualitative Analysis and Challenges,0,[0]
"This is a challenge as dialogue is typically non-descriptive, whereas the questions were asked based on descriptive summaries, requiring models to “read between the lines”.
",6 Qualitative Analysis and Challenges,0,[0]
"We expect that understanding narratives as complex as those presented in NarrativeQA will require
transferring text understanding capability from other supervised learning tasks.",6 Qualitative Analysis and Challenges,0,[0]
This paper is the first large-scale question answering dataset on full-length books and movie scripts.,7 Related Work,0,[0]
"However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP.",7 Related Work,0,[0]
"These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009).",7 Related Work,0,[0]
"In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA.",7 Related Work,0,[0]
"It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts.",7 Related Work,0,[0]
"We have introduced a new dataset and a set of tasks for training and evaluating reading comprehension systems, born from an analysis of the limitations of existing datasets and tasks.",8 Conclusion,0,[0]
"While our QA task resembles tasks provided by existing datasets, it exposes new challenges because of its domain: fiction.",8 Conclusion,0,[0]
"Fictional stories—in contrast to news stories—are selfcontained and describe richer set of entities, events, and the relations between them.",8 Conclusion,0,[0]
"We have a range of tasks, from simple (which requires models to read summaries of books and movie scripts, and generate or rank fluent English answers to human-generated questions) to more complex (which requires models to read the full stories to answer the questions, with no access to the summaries).
",8 Conclusion,0,[0]
"In addition to the issue of scaling neural models to large documents, the larger tasks are significantly more difficult as questions formulated based on one or two sentences of a summary might require appealing to possibly discontiguous sentences or paragraphs
from the source text.",8 Conclusion,0,[0]
"This requires potential solutions to these tasks to jointly model the process of searching for information (possibly in several steps) to serve as support for generating an answer, alongside the process of generating the answer entailed by said support.",8 Conclusion,0,[0]
"End-to-end mechanisms for both searching for information, such as attention, do not scale beyond selecting words or n-grams in short contexts such as sentences and small documents.",8 Conclusion,0,[0]
"Likewise, neural models for mapping documents to answers, or determining entailment between supporting evidence and a hypothesis, typically operate on the scale of sentences rather than sets of paragraphs.
",8 Conclusion,0,[0]
"We have provided baseline and benchmark results for both sets of tasks, demonstrating that while existing models give sensible results out of the box on summaries, they do not get any traction on the book-scale tasks.",8 Conclusion,0,[0]
"Having given a quantitative and qualitative analysis of the difficulty of the more complex tasks, we suggest research directions that may help bridge the gap between existing models and hu-
man performance.",8 Conclusion,0,[0]
"Our hope is that this dataset will serve not only as a challenge for the machine reading community, but as a driver for the development of a new class of neural models which will take a significant step beyond the level of complexity which existing datasets and tasks permit.",8 Conclusion,0,[0]
"Reading comprehension (RC)—in contrast to information retrieval—requires integrating information and reasoning about events, entities, and their relations across a full document.",abstractText,0,[0]
"Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read.",abstractText,0,[0]
"However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC.",abstractText,0,[0]
"To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts.",abstractText,0,[0]
These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience.,abstractText,0,[0]
"We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here.",abstractText,0,[0]
We provide an analysis of the dataset and the challenges it presents.,abstractText,0,[0]
The NarrativeQA Reading Comprehension Challenge,title,0,[0]
"The central idea of model-based reinforcement learning is to decompose the RL problem into two subproblems: learning a model of the environment, and then planning with this model.",1. Introduction,0,[0]
The model is typically represented by a Markov reward process (MRP) or decision process (MDP).,1. Introduction,0,[0]
The planning component uses this model to evaluate and select among possible strategies.,1. Introduction,0,[0]
This is typically achieved by rolling forward the model to construct a value function that estimates cumulative reward.,1. Introduction,0,[0]
"In prior work, the model is trained essentially independently of its use within the planner.",1. Introduction,0,[0]
"As a result, the model is not well-matched with the overall objective of the agent.",1. Introduction,0,[0]
"Prior deep reinforcement learning methods have successfully constructed models that can unroll near pixel-perfect reconstructions
*Equal contribution 1DeepMind, London.",1. Introduction,0,[0]
"Correspondence to: David Silver <davidsilver@google.com>, Hado van Hasselt <hado@google.com>, Matteo Hessel <mtthss@google.com>, Tom Schaul <schaul@google.com>, Arthur Guez <aguez@google.com>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
"(Oh et al., 2015; Chiappa et al., 2016); but are yet to surpass state-of-the-art model-free methods in challenging RL domains with raw inputs (e.g., Mnih et al., 2015; 2016; Lillicrap et al., 2016).
",1. Introduction,0,[0]
"In this paper we introduce a new architecture, which we call the predictron, that integrates learning and planning into one end-to-end training procedure.",1. Introduction,0,[0]
"At every step, a model is applied to an internal state, to produce a next state, reward, discount, and value estimate.",1. Introduction,0,[0]
This model is completely abstract and its only goal is to facilitate accurate value prediction.,1. Introduction,0,[0]
"For example, to plan effectively in a game, an agent must be able to predict the score.",1. Introduction,0,[0]
"If our model makes accurate predictions, then an optimal plan with respect to our model will also be optimal for the underlying game – even if the model uses a different state space (e.g., abstract representations of enemy positions, ignoring their shapes and colours), action space (e.g., highlevel actions to move away from an enemy), rewards (e.g., a single abstract step could have a higher value than any real reward), or even time-step (e.g., a single abstract step could “jump” the agent to the end of a corridor).",1. Introduction,0,[0]
All we require is that trajectories through the abstract model produce scores that are consistent with trajectories through the real environment.,1. Introduction,0,[0]
"This is achieved by training the predictron end-to-end, so as to make its value estimates as accurate as possible.
",1. Introduction,0,[0]
"An ideal model could generalise to many different prediction tasks, rather than overfitting to a single task; and could learn from a rich variety of feedback signals, not just a single extrinsic reward.",1. Introduction,0,[0]
We therefore train the predictron to predict a host of different value functions for a variety of pseudo-reward functions and discount factors.,1. Introduction,0,[0]
"These pseudo-rewards can encode any event or aspect of the environment that the agent may care about, e.g., staying alive or reaching the next room.
",1. Introduction,0,[0]
We focus upon the prediction task: estimating value functions in MRP environments with uncontrolled dynamics.,1. Introduction,0,[0]
"In this case, the predictron can be implemented as a deep neural network with an MRP as a recurrent core.",1. Introduction,0,[0]
"The predictron unrolls this core multiple steps and accumulates rewards into an overall estimate of value.
",1. Introduction,0,[0]
"We applied the predictron to procedurally generated ran-
dom mazes, and a simulated pool domain, directly from pixel inputs.",1. Introduction,0,[0]
"In both cases, the predictron significantly outperformed model-free algorithms with conventional deep network architectures; and was much more robust to architectural choices such as depth.",1. Introduction,0,[0]
We consider environments defined by an MRP with states s ∈ S .,2. Background,0,[0]
"The MRP is defined by a function, s′, r, γ = p(s, α), where s′ is the next state, r is the reward, and γ is the discount factor, which can for instance represent the non-termination probability for this transition.",2. Background,0,[0]
"The process may be stochastic, given IID noise α.
",2. Background,0,[0]
"The return of an MRP is the cumulative discounted reward over a single trajectory, gt = rt+1 + γt+1rt+2 + γt+1γt+2rt+3 + ... , where γt can vary per time-step.",2. Background,0,[0]
"We consider a generalisation of the MRP setting that includes vector-valued rewards r, diagonal-matrix discounts γ , and vector-valued returns g; definitions are otherwise identical to the above.",2. Background,0,[0]
"We use this bold font notation to closely match the more familiar scalar MRP case; the majority of the paper can be comfortably understood by reading all rewards as scalars, and all discount factors as scalar and constant, i.e., γt = γ.
",2. Background,0,[0]
"The value function of an MRP p is the expected return from state s, vp(s) =",2. Background,0,[0]
Ep [gt | st = s].,2. Background,0,[0]
"In the vector case, these are known as general value functions (Sutton et al., 2011).",2. Background,0,[0]
"We will say that a (general) value function v(·) is consistent with environment p if and only if v = vp which satisfies the following Bellman equation (Bellman, 1957),
vp(s) =",2. Background,0,[0]
Ep [r + γvp(s′),2. Background,0,[0]
| s] .,2. Background,0,[0]
"(1)
In model-based reinforcement learning (Sutton & Barto, 1998), an approximation m ≈ p to the environment is learned.",2. Background,0,[0]
"In the uncontrolled setting this model is normally an MRP s′, r, γ = m(s, β) that maps from state s to subsequent state s′ and additionally outputs rewards r and discounts γ ; the model may be stochastic given an IID source of noise β.",2. Background,0,[0]
"A (general) value function vm(·) is consistent with model m (or valid, (Sutton, 1995)), if and only if it satisfies a Bellman equation vm(s) =",2. Background,0,[0]
Em,2. Background,0,[0]
"[r + γvm(s′) | s] with respect to model m. Conventionally, model-based RL methods focus on finding a value function v that is consistent with a separately learned model m.",2. Background,0,[0]
The predictron is composed of four main components.,3. Predictron architecture,0,[0]
"First, a state representation s = f(s) that encodes raw input s (this could be a history of observations, in partially observed settings, for example when f is a recurrent network) into an internal (abstract, hidden) state s. Second, a
model s′, r, γ = m(s, β) that maps from internal state s to subsequent internal state s′, internal rewards r, and internal discounts γ .",3. Predictron architecture,0,[0]
"Third, a value function v that outputs internal values v = v(s) representing the remaining internal return from internal state s onwards.",3. Predictron architecture,0,[0]
"The predictron is applied by unrolling its modelmmultiple “planning” steps to produce internal rewards, discounts and values.",3. Predictron architecture,0,[0]
We use superscripts •k to indicate internal steps of the model (which have no necessary connection to time steps •t of the environment).,3. Predictron architecture,0,[0]
"Finally, these internal rewards, discounts and values are combined together by an accumulator into an overall estimate of value g. The whole predictron, from input state s to output, may be viewed as a value function approximator for external targets (i.e., the returns in the real environment).",3. Predictron architecture,0,[0]
"We consider both k-step and λ-weighted accumulators.
",3. Predictron architecture,0,[0]
The k-step predictron rolls its internal model forward k steps (Figure 1a).,3. Predictron architecture,0,[0]
"The 0-step predictron return (henceforth abbreviated as preturn) is simply the first value g0 = v0, the 1-step preturn is g1 = r1+γ1v1.",3. Predictron architecture,0,[0]
"More generally, the kstep predictron return gk is the internal return obtained by accumulating k model steps, plus a discounted final value vk from the kth step:
gk = r1 + γ1(r2 + γ2(. .",3. Predictron architecture,0,[0]
.+ γk−1(rk,3. Predictron architecture,0,[0]
"+ γkvk) . . .))
",3. Predictron architecture,0,[0]
The λ-predictron combines together many k-step preturns.,3. Predictron architecture,0,[0]
"Specifically, it computes a diagonal weight matrix λk from each internal state sk.",3. Predictron architecture,0,[0]
"The accumulator uses weights λ0, ...,λK to aggregate over k-step preturns g0, ...,gK and output a combined value that we call the λ-preturn gλ,
gλ = K∑ k=0 wkgk (2)
wk =  (1− λk) ∏k−1 j=0",3. Predictron architecture,0,[0]
"λ j if k < K
∏K−1",3. Predictron architecture,0,[0]
j=0,3. Predictron architecture,0,[0]
λ j otherwise.,3. Predictron architecture,0,[0]
"(3)
where 1 is the identity matrix.",3. Predictron architecture,0,[0]
"This λ-preturn is analogous to the λ-return in the forward-view TD(λ) algorithm (Sutton, 1988; Sutton & Barto, 1998).",3. Predictron architecture,0,[0]
"It may also be computed by a backward accumulation through intermediate steps gk,λ,
gk,λ = (1− λk)vk + λk",3. Predictron architecture,0,[0]
"( rk+1 + γk+1gk+1,λ ) , (4)
where gK,λ = vK , and then using gλ = g0,λ.",3. Predictron architecture,0,[0]
"Computation in the λ-predictron operates in a sweep, iterating first through the model from k = 0 . .",3. Predictron architecture,0,[0]
.K,3. Predictron architecture,0,[0]
and then back through the accumulator from k = K . . .,3. Predictron architecture,0,[0]
0,3. Predictron architecture,0,[0]
in a single “forward” pass of the network (see Figure 1b).,3. Predictron architecture,0,[0]
"Each λk weight
acts as a gate on the computation of the λ-preturn: a value of λk = 0 will truncate the λ-preturn at layer k, while a value of λk = 1 will utilise deeper layers based on additional steps of the model m; the final weight is always λK = 0.",3. Predictron architecture,0,[0]
The individual λk weights may depend on the corresponding abstract state sk and can differ per prediction.,3. Predictron architecture,0,[0]
"This enables the predictron to compute to an adaptive depth (Graves, 2016) depending on the internal state and learning dynamics of the network.",3. Predictron architecture,0,[0]
"We first consider updates that optimise the joint parameters θ of the state representation, model, and value function.",4. Predictron learning updates,0,[0]
We begin with the k-step predictron.,4. Predictron learning updates,0,[0]
We update the k-step preturn,4. Predictron learning updates,0,[0]
gk towards a target outcome,4. Predictron learning updates,0,[0]
"g, e.g. the MonteCarlo return from the real environment, by minimising a mean-squared error loss,
Lk = 1
2 ∥∥Ep",4. Predictron learning updates,0,[0]
[g | s]−,4. Predictron learning updates,0,[0]
Em,4. Predictron learning updates,0,[0]
[gk | s]∥∥2 .,4. Predictron learning updates,0,[0]
"∂lk
∂θ =",4. Predictron learning updates,0,[0]
( g − gk ) ∂gk ∂θ .,4. Predictron learning updates,0,[0]
"(5)
where lk = 12 ∥∥g − gk∥∥2 is the sample loss.",4. Predictron learning updates,0,[0]
"We can use the gradient of the sample loss to update parameters, e.g., by stochastic gradient descent.",4. Predictron learning updates,0,[0]
"For stochastic models, independent samples of gk and ∂g k
∂θ are required for unbiased samples of the gradient of Lk.
",4. Predictron learning updates,0,[0]
The λ-predictron combines many k-step preturns.,4. Predictron learning updates,0,[0]
"To up-
date the joint parameters θ, we can uniformly average the losses on the individual preturns gk,
L0:K = 1
2K K∑ k=0 ∥∥Ep",4. Predictron learning updates,0,[0]
[g | s]−,4. Predictron learning updates,0,[0]
"Em [gk | s]∥∥2 , ∂l0:K
∂θ =
1
K K∑ k=0",4. Predictron learning updates,0,[0]
( g − gk ) ∂gk ∂θ .,4. Predictron learning updates,0,[0]
"(6)
Alternatively, we could weight each loss by the usage wk of the corresponding preturn, such that the gradient is∑K k=0w k",4. Predictron learning updates,0,[0]
"( g − gk ) ∂gk ∂θ .
",4. Predictron learning updates,0,[0]
"In the λ-predictron, the λk weights (that determine the relative weightingwk of the k-step preturns) depend on additional parameters η, which are updated so as to minimise a mean-squared error loss Lλ,
Lλ = 1
2 ∥∥Ep",4. Predictron learning updates,0,[0]
[g | s]−,4. Predictron learning updates,0,[0]
Em [gλ | s]∥∥2 .,4. Predictron learning updates,0,[0]
"∂lλ
∂η =",4. Predictron learning updates,0,[0]
( g − gλ ) ∂gλ ∂η .,4. Predictron learning updates,0,[0]
"(7)
In summary, the joint parameters θ of the state representation f , the model m, and the value function v are updated to make each of the k-step preturns gk more similar to the target g, and the parameters η of the λ-accumulator are updated to learn the weights wk so that the aggregate λpreturn gλ becomes more similar to the target g.",4. Predictron learning updates,0,[0]
"In model-based reinforcement learning architectures such as Dyna (Sutton, 1990), value functions may be updated using both real and imagined trajectories.",4.1. Consistency updates,0,[0]
The refinement of value estimates based on these imagined trajectories is often referred to as planning.,4.1. Consistency updates,0,[0]
A similar opportunity arises in the context of the predictron.,4.1. Consistency updates,0,[0]
"Each rollout of the predictron generates a trajectory in abstract space, alongside with rewards, discounts and values.",4.1. Consistency updates,0,[0]
"Furthermore, the predictron aggregates these components in multiple value estimates (g0, ..., gk, gλ).
",4.1. Consistency updates,0,[0]
We may therefore update each individual value estimate towards the best aggregated estimate.,4.1. Consistency updates,0,[0]
"This corresponds to adjusting each preturn gk towards the λ-preturn gλ, by minimizing:
L = 1
2 K∑ k=0 ∥∥Em",4.1. Consistency updates,0,[0]
[gλ | s]−,4.1. Consistency updates,0,[0]
Em,4.1. Consistency updates,0,[0]
[gk | s]∥∥2 .,4.1. Consistency updates,0,[0]
∂l ∂θ,4.1. Consistency updates,0,[0]
= K∑ k=0 ( gλ − gk ) ∂gk ∂θ .,4.1. Consistency updates,0,[0]
"(8)
Here gλ is considered fixed; the parameters θ are only updated to make gk more similar to gλ, not vice versa.
",4.1. Consistency updates,0,[0]
These consistency updates do not require any labels g or samples from the environment.,4.1. Consistency updates,0,[0]
"As a result, it can be applied to (potentially hypothetical) states that have no associated ‘real’ (e.g. Monte-Carlo) outcome: we update the value estimates to be self-consistent with each other.",4.1. Consistency updates,0,[0]
"This is especially relevant in the semi-supervised setting, where these consistency updates allow us to exploit the unlabelled inputs.",4.1. Consistency updates,0,[0]
We conducted experiments in two domains.,5. Experiments,0,[0]
The first domain consists of randomly generated mazes.,5. Experiments,0,[0]
Each location either is empty or contains a wall.,5. Experiments,0,[0]
"In these mazes, we considered two tasks.",5. Experiments,0,[0]
"In the first task, the input was a 13× 13 maze and a random initial position and the goal is to predict a trajectory generated by a simple fixed deterministic policy.",5. Experiments,0,[0]
"The target g was a vector with an element for each cell of the maze which is either one, if that cell was reached by the policy, or zero.",5. Experiments,0,[0]
In the second random-maze task the goal was to predict for each of the cells on the diagonal of a 20 × 20 maze (top-left to bottom-right) whether it is connected to the bottom-right corner.,5. Experiments,0,[0]
Two locations in a maze are considered connected if they are both empty and we can reach one from the other by moving horizontally or vertically through adjacent empty cells.,5. Experiments,0,[0]
"In both cases some predictions would seem to be easier if we could learn a simple algorithm, such as some form of search or flood fill; our hypothesis is that an internal model can learn to
emulate such algorithms, where naive approximation may struggle.",5. Experiments,0,[0]
"A few example mazes are shown in Figure 2.
",5. Experiments,0,[0]
"Our second domain is a simulation of the game of pool, using four balls and four pockets.",5. Experiments,0,[0]
"The simulator is implemented in the physics engine Mujoco (Todorov et al., 2012).",5. Experiments,0,[0]
We generate sequences of RGB frames starting from a random arrangement of balls on the table.,5. Experiments,0,[0]
"The goal is to simultaneously learn to predict future events for each of the four balls, given 5 RGB frames as input.",5. Experiments,0,[0]
"These events include: collision with any other ball, collision with any boundary of the table, entering a quadrant (×4, for each quadrant), being located in a quadrant (×4, for each quadrant), and entering a pocket (×4, for each pocket).",5. Experiments,0,[0]
"Each of these 14 × 4 events provides a binary pseudo-reward that we combine with 5 different discount factors {0, 0.5, 0.9, 0.98, 1} and predict their cumulative discounted sum over various time spans.",5. Experiments,0,[0]
This yields a total of 280 general value functions.,5. Experiments,0,[0]
An example trajectory is shown in Figure 2.,5. Experiments,0,[0]
"In both domains, inputs are presented as minibatches of i.i.d.",5. Experiments,0,[0]
samples with their regression targets.,5. Experiments,0,[0]
Additional domain details are provided in the appendix.,5. Experiments,0,[0]
In the first experiment we trained a predictron to predict trajectories generated by a simple deterministic policy in 13×13 random mazes with random starting positions.,5.1. Learning sequential plans,0,[0]
"Figure 3 shows the weighted preturns wkgk and the resulting prediction gλ = ∑ kw
kgk for six example inputs and targets.",5.1. Learning sequential plans,0,[0]
The predictions are almost perfect—the training error was very close to zero.,5.1. Learning sequential plans,0,[0]
"The full prediction is composed from weighted preturns which decompose the trajectory piece by piece, starting at the start position in the first step k = 1, and where often multiple policy steps are added per planning step.",5.1. Learning sequential plans,0,[0]
"The predictron was not informed about the sequential build up of the targets—it never sees a policy
walking through the maze, only the resulting trajectories— and yet sequential plans emerged spontaneously.",5.1. Learning sequential plans,0,[0]
"Notice also that the easier trajectory on the right was predicted in only two steps, while more thinking steps are used for more complex trajectories.",5.1. Learning sequential plans,0,[0]
"In the next set of experiments, we tackle the problem of predicting connectivity of multiple pairs of locations in a random maze, and the problem of learning many different value functions from our simulator of the game of pool.",5.2. Exploring the predictron architecture,0,[0]
We use these more challenging domains to examine three binary dimensions that differentiate the predictron from standard deep networks.,5.2. Exploring the predictron architecture,0,[0]
"We compare eight predictron variants corresponding to the corners of the cube on the left in Figure 4.
",5.2. Exploring the predictron architecture,0,[0]
"The first dimension, labelled r, γ, corresponds to whether
or not we use the structure of an MRP model.",5.2. Exploring the predictron architecture,0,[0]
In the MRP case internal rewards and discounts are both learned.,5.2. Exploring the predictron architecture,0,[0]
"In the non-(r, γ) case, which corresponds to a vanilla hidden-tohidden neural network module, internal rewards and discounts are ignored by fixing their values to rk = 0 and γk = 1.
",5.2. Exploring the predictron architecture,0,[0]
The second dimension is whether a K-step accumulator or λ-accumulator is used to aggregate preturns.,5.2. Exploring the predictron architecture,0,[0]
"When a λaccumulator is used, a λ-preturn is computed as described in Section 3.",5.2. Exploring the predictron architecture,0,[0]
"Otherwise, intermediate preturns are ignored by fixingλk = 1 for k < K.",5.2. Exploring the predictron architecture,0,[0]
"In this case, the overall output of the predictron is the maximum-depth preturn gK .
",5.2. Exploring the predictron architecture,0,[0]
"The third dimension, labelled usage weighting, defines the loss that is used to update the parameters θ.",5.2. Exploring the predictron architecture,0,[0]
"We consider two options: the preturn losses can either be weighted uniformly (see Equation 6), or the update for each preturn gk can be weighted according to the weight wk that determines how much it is used in the λ-predictron’s overall output.",5.2. Exploring the predictron architecture,0,[0]
We call the latter loss ‘usage weighted’.,5.2. Exploring the predictron architecture,0,[0]
"Note that for architectures without a λ-accumulator, wk = 0 for k < K, and wK = 1, thus usage weighting then implies backpropagating only the loss on the final preturn gK .
",5.2. Exploring the predictron architecture,0,[0]
All variants utilise a convolutional core with 2 intermediate hidden layers; parameters were updated by supervised learning (see appendix for more details).,5.2. Exploring the predictron architecture,0,[0]
"Root mean squared prediction errors for each architecture, aggregated over all predictions, are shown in Figure 4.",5.2. Exploring the predictron architecture,0,[0]
The top row corresponds to the random mazes and the bottom row to the pool domain.,5.2. Exploring the predictron architecture,0,[0]
The main conclusion is that learning an MRP model improved performance greatly.,5.2. Exploring the predictron architecture,0,[0]
"The inclusion of λ weights helped as well, especially on pool.",5.2. Exploring the predictron architecture,0,[0]
Usage weighting further improved performance.,5.2. Exploring the predictron architecture,0,[0]
"Our third set of experiments compares the predictron to feedforward and recurrent deep learning architectures, with and without skip connections.",5.3. Comparing to other architecture,0,[0]
"We compare the corners of a new cube, as depicted on the left in Figure 5, based on three different binary dimensions.
",5.3. Comparing to other architecture,0,[0]
"The first dimension of this second cube is whether we use a predictron, or a (non-λ, non-(r, γ))",5.3. Comparing to other architecture,0,[0]
deep network that does not have an internal model and does not output or learn from intermediate predictions.,5.3. Comparing to other architecture,0,[0]
"We use the most effective predictron from the previous section, i.e., the (r, γ, λ)predictron with usage weighting.
",5.3. Comparing to other architecture,0,[0]
"The second dimension is whether all cores share weights (as in a recurrent network), or each core uses separate weights (as in a feedforward network).",5.3. Comparing to other architecture,0,[0]
"The non-λ, non(r, γ) variants of the predictron then correspond to standard (convolutional) feedforward and (unrolled) recurrent neural networks respectively.
",5.3. Comparing to other architecture,0,[0]
The third dimension is whether we include skip connections.,5.3. Comparing to other architecture,0,[0]
"This is equivalent to defining the model step to output a change to the current state, ∆s, and then defining sk+1 = h(sk + ∆sk), where h is the non-linear function— in our case a ReLU, h(x) = max(0, x).",5.3. Comparing to other architecture,0,[0]
"The deep network with skip connections is a variant of ResNet (He et al., 2015).
",5.3. Comparing to other architecture,0,[0]
Root mean squared prediction errors for each architecture are shown in Figure 5.,5.3. Comparing to other architecture,0,[0]
"All (r, γ, λ)-predictrons (red lines) outperformed the corresponding feedforward or recurrent baselines (black lines) both in the random mazes and in pool.",5.3. Comparing to other architecture,0,[0]
"We also investigated the effect of changing the depth of the networks (see appendix); the predictron outperformed the corresponding feedforward or recurrent baselines for all depths, with and without skip connections.",5.3. Comparing to other architecture,0,[0]
"We now consider how to use the predictron for semisupervised learning, training the model on a combination of labelled and unlabelled random mazes.",5.4. Semi-supervised learning by consistency,0,[0]
"Semi-supervised learning is important because a common bottleneck in applying machine learning in the real world is the difficulty of collecting labelled data, whereas often large quantities of unlabelled data exist.
",5.4. Semi-supervised learning by consistency,0,[0]
"We trained a full (r, γ, λ)-predictron by alternating standard supervised updates with consistency updates, obtained by stochastically minimizing the consistency loss (8), on additional unlabelled samples drawn from the same distribution.",5.4. Semi-supervised learning by consistency,0,[0]
"For each supervised update we apply either 0, 1, or 9 consistency updates.",5.4. Semi-supervised learning by consistency,0,[0]
"Figure 6 shows that the perfor-
mance improved monotonically with the number of consistency updates, measured as a function of the number of labelled samples consumed.",5.4. Semi-supervised learning by consistency,0,[0]
"In principle, the predictron can adapt its depth to ‘think more’ about some predictions than others, perhaps depending on the complexity of the underlying target.",5.5. Analysis of adaptive depth,0,[0]
We saw indications of this in Figure 3.,5.5. Analysis of adaptive depth,0,[0]
"We investigate this further by looking at qualitatively different prediction types in pool: ball collisions, rail collisions, pocketing balls, and entering or staying in quadrants.",5.5. Analysis of adaptive depth,0,[0]
For each prediction type we consider several different time-spans (determined by the real-world discount factors associated with each pseudoreward).,5.5. Analysis of adaptive depth,0,[0]
Figure 7 shows distributions of depth for each type of prediction.,5.5. Analysis of adaptive depth,0,[0]
The ‘depth’ of a predictron is here defined as the effective number of model steps.,5.5. Analysis of adaptive depth,0,[0]
"If the predictron relies fully on the very first value (i.e., λ0 = 0), this counts as 0 steps.",5.5. Analysis of adaptive depth,0,[0]
"If, instead, it learns to place equal weight on all rewards and on the final value, this counts as 16 steps.",5.5. Analysis of adaptive depth,0,[0]
"Concretely, the depth d can be defined recursively as d = d0 where dk = λk(1 + γkdk+1) and dK = 0.",5.5. Analysis of adaptive depth,0,[0]
"Note that even for the same input state, each prediction has a separate depth.
",5.5. Analysis of adaptive depth,0,[0]
The depth distributions exhibit three properties.,5.5. Analysis of adaptive depth,0,[0]
"First, different types of predictions used different depths.",5.5. Analysis of adaptive depth,0,[0]
"Second, depth was correlated with the real-world discount for the first four prediction types.",5.5. Analysis of adaptive depth,0,[0]
"Third, the distributions are not strongly peaked, which implies that the depth can differ per input even for a single real-world discount and prediction type.",5.5. Analysis of adaptive depth,0,[0]
"In a control experiment (not shown) we used a
r,
w e ig h t sh arin g
skip connections (r, , )-predictron
ConvNet
recurrent ConvNet
ResNet
recurrent ResNet
usage w eighting
0 1M 2M 3M 4M 5M
0.0001
0.001
0.01
M S E",5.5. Analysis of adaptive depth,0,[0]
"o
n",5.5. Analysis of adaptive depth,0,[0]
"r
a n",5.5. Analysis of adaptive depth,0,[0]
"d o m
m a ze s",5.5. Analysis of adaptive depth,0,[0]
(l o,5.5. Analysis of adaptive depth,0,[0]
"g s ca le )
Shared core
deep net deep net with skips (r, γ, λ)-predictron (r, γ, λ)-predictron with skips
0 1M 2M 3M 4M 5M
Unshared cores
0 500K 1M
Updates
0.2
0.3
0.4
M S E o
n p
o",5.5. Analysis of adaptive depth,0,[0]
o,5.5. Analysis of adaptive depth,0,[0]
"l
0 500K 1M
Updates
Figure 5.",5.5. Analysis of adaptive depth,0,[0]
Comparing predictron to baselines.,5.5. Analysis of adaptive depth,0,[0]
Aggregated prediction errors on random mazes (top) and pool (bottom) over all predictions for the eight architectures corresponding to the cube on the left.,5.5. Analysis of adaptive depth,0,[0]
Each line is the median of RMSE over five seeds; shaded regions encompass all seeds.,5.5. Analysis of adaptive depth,0,[0]
"The full (r, γ, λ)-predictron (red), consistently outperformed conventional deep network architectures (black), with and without skips and with and without weight sharing.
",5.5. Analysis of adaptive depth,0,[0]
"0 100K 200K 300K 400K 500K
Number of labels
0.001
0.003
0.01
0.03
M S E o
n",5.5. Analysis of adaptive depth,0,[0]
"r
a n",5.5. Analysis of adaptive depth,0,[0]
"d o m
m a ze s",5.5. Analysis of adaptive depth,0,[0]
(l o,5.5. Analysis of adaptive depth,0,[0]
"g s ca le )
Shared core
0 consistency updates 1 consistency update 9 consistency updates
0 100K 200K 300K 400K 500K
Number of labels
Unshared cores
Figure 6.",5.5. Analysis of adaptive depth,0,[0]
Semi-supervised learning.,5.5. Analysis of adaptive depth,0,[0]
"Prediction errors of the (r, γ, λ)-predictrons (shared core, no skips) using 0, 1, or 9 consistency updates for every update with labelled data, plotted as function of the number of labels consumed.",5.5. Analysis of adaptive depth,0,[0]
"Learning performance improves with more consistency updates.
",5.5. Analysis of adaptive depth,0,[0]
"scalar λ shared among all predictions, which reduced performance in all scenarios, indicating that the heterogeneous depth is a valuable form of flexibility.",5.5. Analysis of adaptive depth,0,[0]
We test the quality of the predictions in the pool domain to evaluate whether they are well-suited to making decisions.,5.6. Using predictions to make decisions,0,[0]
"For each sampled pool position, we consider a set I of different initial conditions (different angles and velocity of the white ball), and ask which is more likely to lead to pocketing coloured balls.",5.6. Using predictions to make decisions,0,[0]
For each initial condition s ∈,5.6. Using predictions to make decisions,0,[0]
"I , we apply the (r, γ, λ)-predictron (shared cores, 16 model steps, no skip connections) to obtain predictions gλ.",5.6. Using predictions to make decisions,0,[0]
We ensemble the predictions associated to pocketing any ball (except the white one) with discounts γ = 0.98 and γ = 1.,5.6. Using predictions to make decisions,0,[0]
"We select the condition s∗ that maximises this sum.
",5.6. Using predictions to make decisions,0,[0]
We then roll forward the pool simulator from s∗ and log the number of pocketing events.,5.6. Using predictions to make decisions,0,[0]
"Figure 2 shows a sam-
pled rollout, using the predictron to pick s∗.",5.6. Using predictions to make decisions,0,[0]
"When providing the choice of 128 angles and two velocities for initial conditions (|I| = 256), this procedure resulted in pocketing 27 coloured balls in 50 episodes.",5.6. Using predictions to make decisions,0,[0]
Using the same procedure with an equally deep convolutional network only resulted in 10 pocketing events.,5.6. Using predictions to make decisions,0,[0]
"These results suggest that the lower loss of the learned (r, γ, λ)-predictron translated into meaningful improvements when informing decisions.",5.6. Using predictions to make decisions,0,[0]
A video of the rollouts selected by the predictron is available at the following url: https://youtu.be/ BeaLdaN2C3Q.,5.6. Using predictions to make decisions,0,[0]
Lee et al. (2015) introduced a neural network architecture where classifications branch off intermediate hidden layers.,6. Related work,0,[0]
"An important difference with respect to the λ-predictron is that the weights are hand-tuned as hyper-parameters, whereas in the predictron the λweights are learnt and, more
importantly, conditional on the input.",6. Related work,0,[0]
"Another difference is that the loss on the auxiliary classifications is used to speed up learning, but the classifications themselves are not combined into an aggregate prediction; the output of the model itself is the deepest prediction.
",6. Related work,0,[0]
"Graves (2016) introduced an architecture with adaptive computation time (ACT), with a discrete (but differentiable) decision on when to halt, and aggregating the outputs at each pondering step.",6. Related work,0,[0]
"This is related to our λ weights, but obtains depth in a different way; one notable difference is that the λ-predictron can use different pondering depths for each of its predictions.
",6. Related work,0,[0]
"Value iteration networks (VINs) (Tamar et al., 2016) also learn value functions end-to-end using an internal model, similar to the (non-λ) predictron.",6. Related work,0,[0]
"However, VINs plan via convolutional operations over the full input state space; whereas the predictron plans via imagined trajectories through an abstract state space.",6. Related work,0,[0]
"This may allow the predictron architecture to scale much more effectively in domains that do not have a natural two-dimensional encoding of the state space.
",6. Related work,0,[0]
"The notion of learning about many predictions of the future relates to work on predictive state representations (PSRs; Littman et al., 2001), general value functions (GVFs; Sutton et al., 2011), and nexting (Modayil et al., 2012).",6. Related work,0,[0]
"Such predictions have been shown to be useful as representations (Schaul & Ring, 2013) and for transfer (Schaul et al., 2015).",6. Related work,0,[0]
"So far, however, none of these have been considered for learning abstract models.
",6. Related work,0,[0]
"Schmidhuber (2015) discusses learning abstract models, but maintains separate losses for the model and a controller, and suggests training the model unsupervised to compactly encode the entire history of observations, through predictive coding.",6. Related work,0,[0]
The predictron’s abstract model is instead trained end-to-end to obtain accurate values.,6. Related work,0,[0]
The predictron is a single differentiable architecture that rolls forward an internal model to estimate external values.,7. Conclusion,0,[0]
This internal model may be given both the structure and the semantics of traditional reinforcement learning models.,7. Conclusion,0,[0]
"But, unlike most approaches to model-based reinforcement learning, the model is fully abstract: it need not correspond to the real environment in any human understandable fashion, so long as its rolled-forward “plans” accurately predict outcomes in the true environment.
",7. Conclusion,0,[0]
The predictron may be viewed as a novel network architecture that incorporates several separable ideas.,7. Conclusion,0,[0]
"First, the predictron outputs a value by accumulating rewards over a series of internal planning steps.",7. Conclusion,0,[0]
"Second, each forward pass of the predictron outputs values at multiple planning depths.",7. Conclusion,0,[0]
"Third, these values may be combined together, also within a single forward pass, to output an overall ensemble value.",7. Conclusion,0,[0]
"Finally, the different values output by the predictron may be encouraged to be self-consistent with each other, to provide an additional signal during learning.",7. Conclusion,0,[0]
"Our experiments demonstrate that these differences result in more accurate predictions of value, in reinforcement learning environments, than more conventional network architectures.
",7. Conclusion,0,[0]
We have focused on value prediction tasks in uncontrolled environments.,7. Conclusion,0,[0]
"However, these ideas may transfer to the control setting, for example by using the predictron as a Qnetwork (Mnih et al., 2015).",7. Conclusion,0,[0]
"Even more intriguing is the possibility of learning an internal MDP with abstract internal actions, rather than the MRP considered in this paper.",7. Conclusion,0,[0]
We aim to explore these ideas in future work.,7. Conclusion,0,[0]
One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning.,abstractText,0,[0]
In this document we introduce the predictron architecture.,abstractText,0,[0]
"The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps.",abstractText,0,[0]
Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths.,abstractText,0,[0]
The predictron is trained end-toend so as to make these accumulated values accurately approximate the true value function.,abstractText,0,[0]
We applied the predictron to procedurally generated random mazes and a simulator for the game of pool.,abstractText,0,[0]
The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.,abstractText,0,[0]
The Predictron: End-To-End Learning and Planning,title,0,[0]
"setting, our results demonstrate that ""-differential privacy may be ensured for free – in particular, the regret bounds scale as O( p T ) + ˜O 1
""
. For
bandit linear optimization, and as a special case, for non-stochastic multi-armed bandits, the proposed algorithm achieves a regret of ˜O ⇣ 1
""
p T ⌘
, while the previously known best regret bound was ˜O ⇣ 1
""
T 2 3 ⌘ .",text,0,[0]
"In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries.",1. Introduction,0,[0]
"In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees.",1. Introduction,0,[0]
"Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements.",1. Introduction,0,[0]
"In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms.",1. Introduction,0,[0]
"A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.
",1. Introduction,0,[0]
"In this paper, we design differentially private algorithms for online linear optimization with near-optimal regret, both in
*Equal contribution 1Computer Science, Princeton University, Princeton, NJ, USA.",1. Introduction,0,[0]
"Correspondence to: Naman Agarwal <namana@cs.princeton.edu>, Karan Singh <karans@cs.princeton.edu>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
1Here the ˜O(·) notation hides polylog(T ),1. Introduction,0,[0]
"factors.
",1. Introduction,0,[0]
the full information and partial information (bandit) settings.,1. Introduction,0,[0]
This result improves the known best regret bounds for a number of important online learning problems – including prediction from expert advice and non-stochastic multi-armed bandits.,1. Introduction,0,[0]
"For the full-information setting where the algorithm gets to see the complete loss vector every round, we design ""-differentially private algorithms with regret bounds that scale as O ⇣p T ⌘ + ˜O 1
""
(Theorem 3.1), partially resolv-
ing an open question to improve the previously best known bound of O ⇣ 1
""
p T ⌘
posed in (Smith & Thakurta, 2013).",1.1. Full-Information Setting: Privacy for Free,0,[0]
"A decomposition of the bound on the regret bound of this form implies that when "" 1p
T , the regret incurred by the differentially private algorithm matches the optimal regret in the non-private setting, i.e. differential privacy is free.",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Moreover even when ""  1p
T , our results guarantee a sub-constant regret per round in contrast to the vacuous constant regret per round guaranteed by existing results.
",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Concretely, consider the case of online linear optimization over the cube, with unit l1-norm-bounded loss vectors.",1.1. Full-Information Setting: Privacy for Free,0,[0]
"In this setting, (Smith & Thakurta, 2013) achieves a regret bound of O( 1
""
p NT ), which is meaningful only if T N
"" 2 .",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Our theorems imply a regret bound of ˜O( p NT + N
"" ).",1.1. Full-Information Setting: Privacy for Free,0,[0]
"This is an improvement on the previous bound regardless of the value of "".",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Furthermore, when T is between N
"" and N "" 2 , the previous bounds are vacuous whereas our results are still meaningful.",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Note that the above arguments show an improvement over existing results even for moderate value of "".",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Indeed, when "" is very small, the magnitude of improvements are more pronounced.
",1.1. Full-Information Setting: Privacy for Free,0,[0]
"Beyond the separation between T and "", the key point of our analysis is that we obtain bounds for general regularization based algorithms which adapt to the geometry of the underlying problem optimally, unlike the previous algorithms (Smith & Thakurta, 2013) which utilizes euclidean regularization.",1.1. Full-Information Setting: Privacy for Free,0,[0]
This allows our results to get rid of a polynomial dependence on N (in the p T term) in some cases.,1.1. Full-Information Setting: Privacy for Free,0,[0]
"Online linear optimization over the sphere and prediction with expert advice are notable examples.
",1.1. Full-Information Setting: Privacy for Free,0,[0]
We summarize our results in Table 1.1.,1.1. Full-Information Setting: Privacy for Free,0,[0]
"In the partial-information (bandit) setting, the online learning algorithm only gets to observe the loss of the prediction it prescribed.",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"We outline a reduction technique that translates a non-private bandit algorithm to a differentially private bandit algorithm, while retaining the ˜O( p T ) dependency of the regret bound on the number of rounds of play (Theorem 4.5).",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"This allows us to derive the first ""- differentially private algorithm for bandit linear optimization achieving ˜O( p T ) regret, using the algorithm for the non-private setting from (Abernethy et al., 2012).",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"This answers a question from (Smith & Thakurta, 2013) asking if ˜O( p T ) regret is attainable for differentially private linear bandits .
",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"An important case of the general bandit linear optimization framework is the non-stochastic multi-armed bandits problem(Bubeck et al., 2012b), with applications for website optimization, personalized medicine, advertisement placement and recommendation systems.",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"Here, we propose an ""-differentially private algorithm which enjoys a regret of ˜O( 1
""
p NT logN) (Theorem 4.1), improving on the
previously best attainable regret of ˜O( 1 ""
NT 2 3 )(Smith &
Thakurta, 2013).
",1.2. Bandits: Reduction to the Non-private Setting,0,[0]
We summarize our results in Table 1.2.,1.2. Bandits: Reduction to the Non-private Setting,0,[0]
"The problem of differentially private online learning was first considered in (Dwork et al., 2010), albeit guaranteeing privacy in a weaker setting – ensuring the privacy of the individual entries of the loss vectors.",1.3. Related Work,0,[0]
"(Dwork et al., 2010) also introduced the tree-based aggregation scheme for releasing the cumulative sums of vectors in a differentially private manner, while ensuring that the total amount of noise added for each cumulative sum is only polylogarithmically dependent on the number of vectors.",1.3. Related Work,0,[0]
"The stronger notion of privacy protecting entire loss vectors was first studied in (Jain et al., 2012), where gradient-based algorithms were proposed that achieve ("", )-differntial privacy and regret bounds of ˜O ⇣ 1
""
p T log 1 ⌘ .",1.3. Related Work,0,[0]
"(Smith &
Thakurta, 2013) proposed a modification of Follow-theApproximate-Leader template to achieve ˜O 1
""
log 2.5 T
regret for strongly convex loss functions, implying a regret bound of ˜O ⇣ 1
""
p T ⌘
for general convex functions.",1.3. Related Work,0,[0]
"In addition, they also demonstrated that under bandit feedback, it is possible to obtain regret bounds that scale as ˜O ⇣ 1
""
T 2 3 ⌘ .
(Dwork et al., 2014a; Jain & Thakurta, 2014) proved that in the special case of prediction with expert advice setting, it is possible to achieve a regret of O 1
""
p T logN .",1.3. Related Work,0,[0]
"While
most algorithms for differentially private online learning are based on the regularization template, (Dwork et al., 2014b) used a perturbation-based algorithm to guarantee ("", )-differential privacy for the problem of online PCA.",1.3. Related Work,0,[0]
"(Tossou & Dimitrakakis, 2016) showed that it is possible to design ""-differentially private algorithms for the stochastic multi-armed bandit problem with a separation of "", T for the regret bound.",1.3. Related Work,0,[0]
"Recently, an independent work due to (Tossou & Dimitrakakis, 2017), which we were made aware of after the first manuscript, also demonstrated a ˜O ⇣ 1
""
p T ⌘
regret bound in the non-stochastic multi-armed bandits setting.",1.3. Related Work,0,[0]
"We match their results (Theorem 4.1), as well as provide a generalization to arbitrary convex sets (Theorem 4.5).",1.3. Related Work,0,[0]
"Full Information Setting: We consider the two well known paradigms for online learning, Folllow-theRegularized-Leader (FTRL) and Folllow-the-PerturbedLeader (FTPL).",1.4. Overview of Our Techniques,0,[0]
"In both cases, we ensure differential privacy by restricting the mode of access to the inputs (the loss vectors).",1.4. Overview of Our Techniques,0,[0]
"In particular, the algorithm can only retrieve estimates of the loss vectors released by a tree based aggregation protocol (Algorithm 2) which is a slight modification of the protocol used in (Jain et al., 2012; Smith & Thakurta, 2013).",1.4. Overview of Our Techniques,0,[0]
"We outline a tighter analysis of the regret minimization framework by crucially observing that in case of linear losses, the expected regret of an algorithm that injects identically (though not necessarily independently) distributed noise per step is the same as one that injects a single copy of the noise at the very start of the algorithm.
",1.4. Overview of Our Techniques,0,[0]
"The regret analysis of Follow-the-Leader based algorithm involves two components, a bias term due to the regularization and a stability term which bounds the change in the output of the algorithm per step.",1.4. Overview of Our Techniques,0,[0]
"In the analysis due to (Smith & Thakurta, 2013), the stability term is affected by the variance of the noise as it changes from step to step.",1.4. Overview of Our Techniques,0,[0]
"However in our analysis, since we treat the noise to have been sampled just once, the stability analysis does not factor in the variance and the magnitude of the noise essentially appears as an additive term in the bias.
",1.4. Overview of Our Techniques,0,[0]
Bandit Feedback:,1.4. Overview of Our Techniques,0,[0]
"In the bandit feedback setting, we show a general reduction that takes a non-private algorithm and outputs a private algorithm (Algorithm 4).",1.4. Overview of Our Techniques,0,[0]
"Our key observation here (presented as Lemma 4.3) is that on linear functions, in expectation the regret of an algorithm on a noisy sequence of loss vectors is the same as its regret on the original loss sequence as long as noise is zero mean.",1.4. Overview of Our Techniques,0,[0]
"We now bound the regret on the noisy sequence by conditioning out the case when the noise can be large and using exploration techniques from (Bubeck et al., 2012a) and (Abernethy et al., 2008).",1.4. Overview of Our Techniques,0,[0]
"This section introduces the model of online (linear) learning, the distinction between full and partial feedback scenarios, and the notion of differential privacy in this model.
",2. Model and Preliminaries,0,[0]
"Full-Information Setting: Online linear optimization (Hazan et al., 2016; Shalev-Shwartz, 2011) involves repeated decision making over T rounds of play.",2. Model and Preliminaries,0,[0]
"At the beginning of every round (say round t), the algorithm chooses a point in x
t 2 X , where X ✓ RN is a (compact) convex set.",2. Model and Preliminaries,0,[0]
"Subsequently, it observes the loss l
t 2 Y ✓ RN and suffers a loss of hl
t , x t i.",2. Model and Preliminaries,0,[0]
"The success of such an algorithm, across T rounds of play, is measured though regret, which is defined as
Regret = E  TX
t=1
hl t ,",2. Model and Preliminaries,0,[0]
x t,2. Model and Preliminaries,0,[0]
i min,2. Model and Preliminaries,0,[0]
"x2K
TX
t=1
hl t , xi
where the expectation is over the randomness of the algorithm.",2. Model and Preliminaries,0,[0]
"In particular, achieving a sub-linear regret (o(T )) corresponds to doing almost as good (averaging across T rounds) as the fixed decision with the least loss in hindsight.",2. Model and Preliminaries,0,[0]
"In the non-private setting, a number of algorithms have been devised to achieve O( p T ) regret, with additional dependencies on other parameters dependent on the properties of the specific decision set X and loss set Y .",2. Model and Preliminaries,0,[0]
"(See (Hazan et al., 2016) for a survey of results.)
",2. Model and Preliminaries,0,[0]
"Following are three important instantiations of the above
framework.
",2. Model and Preliminaries,0,[0]
"• Prediction with Expert Advice: Here the underlying decision set is the simplex X =
N = {x 2",2. Model and Preliminaries,0,[0]
"Rn : x i 0, P n
i=1
x i = 1} and the loss vectors are constrained to the unit cube Y = {l
t 2 RN : kl t k1  1}.
",2. Model and Preliminaries,0,[0]
"• OLO over the Sphere: Here the underlying decision is the euclidean ball X = {x 2 Rn : kxk
2  1} and the loss vectors are constrained to the unit euclidean ball Y = {l
t 2 RN : kl t k 2  1}.
",2. Model and Preliminaries,0,[0]
• OLO over the Cube: The decision is the unit cube X = {x 2,2. Model and Preliminaries,0,[0]
"Rn : kxk1  1}, while the loss vectors are constrained to the set Y = {l
t 2 RN : kl t k 1  1}.
",2. Model and Preliminaries,0,[0]
"Partial-Information Setting: In the setting of bandit feedback, the critical difference is that the algorithm only gets to observe the value hl
t , x t i, in contrast to the complete loss vector l
t 2 RN as in the full information scenario.",2. Model and Preliminaries,0,[0]
"Therefore, the only feedback the algorithm receives is the value of the loss it incurs for the decision it takes.",2. Model and Preliminaries,0,[0]
This makes designing algorithms for this feedback model challenging.,2. Model and Preliminaries,0,[0]
"Nevertheless for the general problem of bandit linear optimization, (Abernethy et al., 2008) introduced a computationally efficient algorithm that achieves an optimal dependence of the incurred regret of O( p T ) on the number of rounds of play.",2. Model and Preliminaries,0,[0]
"The non-stochastic multi-armed
bandit (Auer et al., 2002) problem is the bandit version of the prediction with expert advice framework.
",2. Model and Preliminaries,0,[0]
"Differential Privacy: Differential Privacy (Dwork et al., 2006) is a rigorous framework for establishing guarantees on privacy loss, that admits a number of desirable properties such as graceful degradation of guarantees under composition and robustness to linkage acts (Dwork et al., 2014a).
",2. Model and Preliminaries,0,[0]
"Definition 2.1 (("", )-Differential Privacy).",2. Model and Preliminaries,0,[0]
A randomized online learning algorithm,2. Model and Preliminaries,0,[0]
"A on the action set X and the loss set Y is ("", )-differentially private if for any two sequence of loss vectors L = (l
1 , . . .",2. Model and Preliminaries,0,[0]
l T ),2. Model and Preliminaries,0,[0]
"✓ YT and L0 = (l0 1 , . . .",2. Model and Preliminaries,0,[0]
"l0 T
)",2. Model and Preliminaries,0,[0]
"✓ YT differing in at most one vector – that is to say 9t
0 2",2. Model and Preliminaries,0,[0]
"[T ], 8t 2",2. Model and Preliminaries,0,[0]
"[T ] {t 0 }, l t = l0 t – for all S ✓ X T , it holds that
P(A(L) 2 S)  e""P(A(L0) 2 S) +
Remark 2.2.",2. Model and Preliminaries,0,[0]
The above definition of Differential Privacy is specific to the online learning scenario in the sense that it assumes the change of a complete loss vector.,2. Model and Preliminaries,0,[0]
"This has been the standard notion considered earlier in (Jain et al., 2012; Smith & Thakurta, 2013).",2. Model and Preliminaries,0,[0]
"Note that the definition entails that the entire sequence of predictions produced by the algorithm is differentially private.
",2. Model and Preliminaries,0,[0]
Notation: We define kYk p = max{kl t k p :,2. Model and Preliminaries,0,[0]
"l t 2 Y}, kXk
p = max{kxk p :",2. Model and Preliminaries,0,[0]
"x 2 X}, and M = max
l2Y,x2X |hl, xi|, where k · kp is the lp norm.",2. Model and Preliminaries,0,[0]
"By Holder’s inequality, it is easy to see that M  kYk
p kXk q
for all p, q 1 with 1 p + 1 q = 1.",2. Model and Preliminaries,0,[0]
We define the distribution LapN ( ) to be the distribution over RN such that each coordinate is drawn independently from the Laplace distribution with parameter .,2. Model and Preliminaries,0,[0]
"In this section, we describe an algorithmic template (Algorithm 1) for differentially private online linear optimization, based on Follow-the-Regularized-Leader scheme.",3. Full-Information Setting: Privacy for Free,0,[0]
"Subsequently, we outline the noise injection scheme (Algorithm 2), based on the Tree-based Aggregation Protocol (Dwork et al., 2010), used as a subroutine by Algorithm 1 to ensure input differential privacy.",3. Full-Information Setting: Privacy for Free,0,[0]
"The following is our main theorem in this setting.
",3. Full-Information Setting: Privacy for Free,0,[0]
Theorem 3.1.,3. Full-Information Setting: Privacy for Free,0,[0]
"Algorithm 1 when run with D = LapN ( ) where = kYk1 log T
"" , regularization R(x), decision set X and loss vectors l
1 , . . .",3. Full-Information Setting: Privacy for Free,0,[0]
"l t , the regret of Algorithm 1 is bounded by
Regret  vuutD R TX
t=1
max x2X (kl t k⇤r2R(x))2 +DLap
where
D Lap",3. Full-Information Setting: Privacy for Free,0,[0]
"= E Z⇠D0  max
x2X hZ, xi min x2X hZ, xi
D R = max x2X R(x) min x2X R(x)
and D0 is the distribution induced by the sum of dlog T e independent samples from D, k ·",3. Full-Information Setting: Privacy for Free,0,[0]
"k⇤r2R(x) represents the dual of the norm with respect to the hessian of R. Moreover, the algorithm is ""-differentially private, i.e. the sequence of predictions produced (x
t : t 2",3. Full-Information Setting: Privacy for Free,0,[0]
"[T ]) is ""-differentially private.
",3. Full-Information Setting: Privacy for Free,0,[0]
"Algorithm 1 FTRL Template for OLO input Noise distribution D, Regularization R(x)
1: Initialize an empty binary tree B to compute differentially private estimates of P t
s=1
l s .",3. Full-Information Setting: Privacy for Free,0,[0]
"2: Sample n1
0 , . . .",3. Full-Information Setting: Privacy for Free,0,[0]
ndlog,3. Full-Information Setting: Privacy for Free,0,[0]
"Te 0
independently from D. 3: ˜L
0
Pdlog Te
i=1
ni 0 .",3. Full-Information Setting: Privacy for Free,0,[0]
"4: for t = 1 to T do 5: Choose x
t = argmin x2X
⇣ ⌘hx, ˜L t 1i+R(x) ⌘
.",3. Full-Information Setting: Privacy for Free,0,[0]
"6: Observe l
t 2 Y , and suffer a loss of hl t , x t i. 7: (˜L
t , B) TreeBasedAgg(l",3. Full-Information Setting: Privacy for Free,0,[0]
"t , B, t,D, T ).",3. Full-Information Setting: Privacy for Free,0,[0]
"8: end for
The above theorem leads to following corollary where we show the bounds obtained in specific instantiations of online linear optimization.
",3. Full-Information Setting: Privacy for Free,0,[0]
Corollary 3.2.,3. Full-Information Setting: Privacy for Free,0,[0]
"Substituting the choices of , R(x) listed below, we specify the regret bounds in each case.
1.",3. Full-Information Setting: Privacy for Free,0,[0]
"Prediction with Expert Advice: Choosing = N log T
""
and R(x) = P N
i=1
x",3. Full-Information Setting: Privacy for Free,0,[0]
"i log(x i ),
Regret  O p T logN + N log2 T logN
""
!
2. OLO over the Sphere Choosing = p N log T
"" and R(x) = kxk2
2
Regret  O p T + N log2 T
""
!
3.",3. Full-Information Setting: Privacy for Free,0,[0]
"OLO over the Cube With = log T "" and R(x) =",3. Full-Information Setting: Privacy for Free,0,[0]
"kxk2 2
Regret  ",3. Full-Information Setting: Privacy for Free,0,[0]
"O p NT + N log2 T
""
!
Algorithm 2",3. Full-Information Setting: Privacy for Free,0,[0]
"TreeBasedAgg(l t , B, t,D, T ) input Loss vector l
t , Binary tree B, Round t, Noise distribution D, Time horizon T
1: ( ˜L0 t , B) PrivateSum(l",3. Full-Information Setting: Privacy for Free,0,[0]
"t , B, t,D, T ) – Algorithm 5 ((Jain et al., 2012))",3. Full-Information Setting: Privacy for Free,0,[0]
"with the noise added at each node – be it internal or leaf – sampled independently from the distribution D.
2: s t the binary representation of t as a string.",3. Full-Information Setting: Privacy for Free,0,[0]
"3: Find the minimum set S of already populated nodes in
B that can compute P t
s=1
l s .",3. Full-Information Setting: Privacy for Free,0,[0]
"4: Define Q = |S|  dlog T e. Define r
t = dlog T e Q. 5:",3. Full-Information Setting: Privacy for Free,0,[0]
"Sample n1
t
, . . .",3. Full-Information Setting: Privacy for Free,0,[0]
"nrt t independently from D. 6: ˜L
t
˜L0 t +
P rt
i=1
ni t .",3. Full-Information Setting: Privacy for Free,0,[0]
"output ( ˜L
t
, B).",3. Full-Information Setting: Privacy for Free,0,[0]
"We first prove the privacy guarantee, and then prove the claimed bound on the regret.",3.1. Proof of Theorem 3.1,0,[0]
"For the analysis, we define the random variable Z
t to be the net amount of noise injected by the TreeBasedAggregation (Algorithm 2) on the true partial sums.",3.1. Proof of Theorem 3.1,0,[0]
"Formally, Z
t is the difference between cumulative sum of loss vectors and its differentially private estimate used as input to the arg-min oracle.
",3.1. Proof of Theorem 3.1,0,[0]
"Z t = ˜L t
tX
i=1
",3.1. Proof of Theorem 3.1,0,[0]
"l i
",3.1. Proof of Theorem 3.1,0,[0]
"Further, let D0 be the distribution induced by summing of dlog T e independent samples from D.
Privacy : To make formal claims about the quality of privacy, we ensure input differential privacy for the algorithm – that is, we ensure that the entire sequence of partial sums of the loss vectors ( P t
s=1
l s : t 2",3.1. Proof of Theorem 3.1,0,[0]
"[T ]) is ""- differentially private.",3.1. Proof of Theorem 3.1,0,[0]
"Since the outputs of Algorithm 1 are strictly determined by the prefix sum estimates produced by TreeBasedAgg, by the post-processing theorem, this certifies that the entire sequence of choices made by the algorithm (across all T rounds of play) (x
t :",3.1. Proof of Theorem 3.1,0,[0]
t 2,3.1. Proof of Theorem 3.1,0,[0]
"[T ]) is ""- differentially private.",3.1. Proof of Theorem 3.1,0,[0]
We modify the standard Tree-based Aggregation protocol to make sure that the noise on each output (partial sum) is distributed identically (though not necessarily independently) across time.,3.1. Proof of Theorem 3.1,0,[0]
"While this modification is not essential for ensuring privacy, it simplifies the regret analysis.",3.1. Proof of Theorem 3.1,0,[0]
Lemma 3.3 (Privacy Guarantees with Laplacian Noise).,3.1. Proof of Theorem 3.1,0,[0]
"Choose any kYk1 log T
"" .",3.1. Proof of Theorem 3.1,0,[0]
"When Algorithm 2 A(D, T ) is run with D = LapN ( ), the following claims hold true:
• Privacy:",3.1. Proof of Theorem 3.1,0,[0]
The sequence (˜L t : t 2,3.1. Proof of Theorem 3.1,0,[0]
"[T ]) is ""- differentially private.
",3.1. Proof of Theorem 3.1,0,[0]
• Distribution: 8t 2,3.1. Proof of Theorem 3.1,0,[0]
"[T ], Z t ⇠ Pdlog Te
i=1
n",3.1. Proof of Theorem 3.1,0,[0]
"i , where
each n",3.1. Proof of Theorem 3.1,0,[0]
"i is independently sampled from LapN ( ).
",3.1. Proof of Theorem 3.1,0,[0]
Proof.,3.1. Proof of Theorem 3.1,0,[0]
"By Theorem 9 ((Jain et al., 2012)), we have that the sequence ( ˜L0
t : t 2",3.1. Proof of Theorem 3.1,0,[0]
"[T ]) is ""-differentially private.",3.1. Proof of Theorem 3.1,0,[0]
"Now the sequence (˜L
t : t 2",3.1. Proof of Theorem 3.1,0,[0]
"[T ]) is ""-differentially private because differential privacy is immune to post-processing(Dwork et al., 2014a).
",3.1. Proof of Theorem 3.1,0,[0]
"Note that the PrivateSum algorithm adds exactly |S| independent draws from the distribution D to P t
s=1
l s , where S is the minimum set of already populated nodes in the tree that can compute the required prefix sum.",3.1. Proof of Theorem 3.1,0,[0]
"Due to Line 6 in Algorithm 2, it is made certain that every prefix sum released is a sum of the true prefix sum and dlog T e independent draws from D.
Regret Analysis: In this section, we show that for linear loss functions any instantiation of the Follow-theRegularized-Leader algorithm can be made differentially private with an additive loss in regret.",3.1. Proof of Theorem 3.1,0,[0]
Theorem 3.4.,3.1. Proof of Theorem 3.1,0,[0]
"For any noise distribution D, regularization R(x), decision set X and loss vectors l
1 , . . .",3.1. Proof of Theorem 3.1,0,[0]
"l t , the regret of Algorithm 1 is bounded by
Regret  vuutD R TX
t=1
max x2X (kl t k⇤r2R(x))2 +DD0
where DD0 = EZ⇠D0 [maxx2X hZ, xi minx2X",3.1. Proof of Theorem 3.1,0,[0]
"hZ, xi], D
R
= max x2X R(x) minx2X R(x), and k · k⇤r2R(x) represents the dual of the norm with respect to the hessian of R.
Proof.",3.1. Proof of Theorem 3.1,0,[0]
"To analyze the regret suffered by Algorithm 1, we consider an alternative algorithm that performs a one-shot noise injection – this alternate algorithm may not be differentially private.",3.1. Proof of Theorem 3.1,0,[0]
"The observation here is that the alternate algorithm and Algorithm 1 suffer the same loss in expectation and therefore the same expected regret which we bound in the analysis below.
",3.1. Proof of Theorem 3.1,0,[0]
"Consider the following alternate algorithm which instead of sampling noise Z
t at each step instead samples noise at the beginning of the algorithm and plays with respect to that.",3.1. Proof of Theorem 3.1,0,[0]
"Formally consider the sequence of iterates x̂
t defined as follows.",3.1. Proof of Theorem 3.1,0,[0]
"Let Z ⇠ D.
x̂ 1 , x 1 , x̂ t , argmin x2X ⌘hx, Z +
X
i
l i
i+R(x)
",3.1. Proof of Theorem 3.1,0,[0]
"We have that
E Z1...ZT⇠D
"" TX
t=1
hl t , x t i # = E Z⇠D "" TX
t=1
hl t , x̂",3.1. Proof of Theorem 3.1,0,[0]
t,3.1. Proof of Theorem 3.1,0,[0]
"i # (1)
To see the above equation note that E Zt⇠D [hlt, x̂ti] = E Z⇠D [hlt, xti] since x, x̂t have the same distribution.
",3.1. Proof of Theorem 3.1,0,[0]
Therefore it is sufficient to bound the regret of the sequence x̂ 1 . . .,3.1. Proof of Theorem 3.1,0,[0]
"x̂ t
.",3.1. Proof of Theorem 3.1,0,[0]
The key idea now is to notice that the addition of one shot noise does not affect the stability term of the FTRL analysis and therefore the effect of the noise need not be paid at every time step.,3.1. Proof of Theorem 3.1,0,[0]
"Our proof will follow the standard template of using the FTL-BTL (Kalai & Vempala, 2005) lemma and then bounding the stability term in the standard way.",3.1. Proof of Theorem 3.1,0,[0]
"Formally define the augmented series of loss functions by defining
l 0
(x) = 1 ⌘ R(x) + hZ, xi
where Z ⇠ D is a sample.",3.1. Proof of Theorem 3.1,0,[0]
"Now invoking the Follow the Leader, Be the Leader Lemma (Lemma 5.3, (Hazan et al., 2016))",3.1. Proof of Theorem 3.1,0,[0]
"we get that for any fixed u 2 X
TX
t=0
l t
(u) TX
t=0
l t (x̂ t+1 )
",3.1. Proof of Theorem 3.1,0,[0]
"Therefore we can conclude that TX
t=1
[l t (x̂ t )",3.1. Proof of Theorem 3.1,0,[0]
"l t (u)] (2)
 TX
t=1
",3.1. Proof of Theorem 3.1,0,[0]
[l t (x̂ t ),3.1. Proof of Theorem 3.1,0,[0]
l t (x̂ t+1 )],3.1. Proof of Theorem 3.1,0,[0]
+,3.1. Proof of Theorem 3.1,0,[0]
l 0,3.1. Proof of Theorem 3.1,0,[0]
(u) l 0,3.1. Proof of Theorem 3.1,0,[0]
"(x̂ 1 )
 ",3.1. Proof of Theorem 3.1,0,[0]
"TX
t=1
[l t (x̂ t )",3.1. Proof of Theorem 3.1,0,[0]
l t (x̂ t+1 )],3.1. Proof of Theorem 3.1,0,[0]
"+
1 ⌘",3.1. Proof of Theorem 3.1,0,[0]
"D R +D Z
(3)
where D Z , max x2X(hZ, xi) minx2X(hZ, xi)",3.1. Proof of Theorem 3.1,0,[0]
Therefore we now need to bound the stability term l t (x̂ t ),3.1. Proof of Theorem 3.1,0,[0]
"l t (x̂ t+1
).",3.1. Proof of Theorem 3.1,0,[0]
"Now, the regret bound follows from the standard analysis for the stability term in the FTRL scheme (see for instance (Hazan et al., 2016)).",3.1. Proof of Theorem 3.1,0,[0]
"Notice that the bound only depends on the change in the cumulative loss per step i.e. ⌘ ( P
t
l t + Z), for which the change is the loss vector ⌘l t+1
across time steps.",3.1. Proof of Theorem 3.1,0,[0]
"Therefore we get that
l t (x̂ t )",3.1. Proof of Theorem 3.1,0,[0]
"l t (x̂ t+1 )  max x2X k⌘l t k2 ⌘r 2R(x) (4)
Combining Equations (1), (3), (4) we get the regret bound in Theorem 3.4.",3.1. Proof of Theorem 3.1,0,[0]
"In this section, we outline algorithms based on the Followthe-Perturbed-Leader template(Kalai & Vempala, 2005).",3.2. Regret Bounds for FTPL,0,[0]
FTPL-based algorithms ensure low-regret by perturbing the cumulative sum of loss vectors with noise from a suitably chosen distribution.,3.2. Regret Bounds for FTPL,0,[0]
We show that the noise added in the process of FTPL is sufficient to ensure differential privacy.,3.2. Regret Bounds for FTPL,0,[0]
"More concretely, using the regret guarantees due to
(Abernethy et al., 2014), for the full-information setting, we establish that the regret guarantees obtained scale as O( p T )+ ˜O( 1
""
log
1 ).",3.2. Regret Bounds for FTPL,0,[0]
"While Theorem 3.5 is valid for all instances of online linear optimization and achieves O( p T ) regret, it yields sub-optimal dependence on the dimension of the problem.",3.2. Regret Bounds for FTPL,0,[0]
"The advantage of FTPL-based approaches over FTRL is that FTPL performs linear optimization over the decision set every round, which is possibly computationally less expensive than solving a convex program every round, as FTRL requires.
",3.2. Regret Bounds for FTPL,0,[0]
"Algorithm 3 FTPL Template for OLO – A(D, T ) on the action set X , the loss set Y .
1: Initialize an empty binary tree B to compute differentially private estimates of P t
s=1
l s .",3.2. Regret Bounds for FTPL,0,[0]
"2: Sample n1
0 , . . .",3.2. Regret Bounds for FTPL,0,[0]
ndlog,3.2. Regret Bounds for FTPL,0,[0]
"Te 0
independently from D. 3: ˜L
0
Pdlog Te
i=1
ni 0 .",3.2. Regret Bounds for FTPL,0,[0]
"4: for t = 1 to T do 5: Choose x
t = argmin x2X hx, ˜Lt 1i.
6: Observe the loss vector l t 2 Y , and suffer hl t ,",3.2. Regret Bounds for FTPL,0,[0]
"x t i. 7: (˜L
t , B) TreeBasedAgg(l",3.2. Regret Bounds for FTPL,0,[0]
"t , B, t,D, T ).",3.2. Regret Bounds for FTPL,0,[0]
"8: end for
Theorem 3.5 (FTPL: Online Linear Optimization).",3.2. Regret Bounds for FTPL,0,[0]
"Let kXk
2
= sup x2X kxk2 and kYk2 = suplt2Y kltk2.",3.2. Regret Bounds for FTPL,0,[0]
"Choosing = max{kYk
2
q Tp
N log T
, p N
""
log T log log T } and D = N (0, 2I
N ), we have that RegretA(D,T )(T ) is
O N 1 4 kXk
2 kYk 2 p T +",3.2. Regret Bounds for FTPL,0,[0]
"NkXk 2
"" log
1.5 T log log T
!
",3.2. Regret Bounds for FTPL,0,[0]
"Moreover the algorithm is ""-differentially private.
",3.2. Regret Bounds for FTPL,0,[0]
The proof of the theorem is deferred to the appendix.,3.2. Regret Bounds for FTPL,0,[0]
"In this section, we state our main results regarding bandit linear optimization, the algorithms that achieve it and prove the associated regret bounds.",4. Differentially Private Multi-Armed Bandits,0,[0]
The following is our main theorem concerning non-stochastic multi-armed bandits.,4. Differentially Private Multi-Armed Bandits,0,[0]
Theorem 4.1 (Differentially Private Multi-Armed Bandits).,4. Differentially Private Multi-Armed Bandits,0,[0]
"Fix loss vectors (l
1 . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
l T ) such that kl t k1  1.,4. Differentially Private Multi-Armed Bandits,0,[0]
"When Algorithm 4 is run with parameters D = LapN ( ) where = 1
"" and algorithm",4. Differentially Private Multi-Armed Bandits,0,[0]
"A = Algorithm 5 with the following parameters: ⌘ = q logN
2NT (1+2 2 logNT )
,
= ⌘N p
1 + 2 2 logNT and the exploration distribution µ(i) = 1
N
.",4. Differentially Private Multi-Armed Bandits,0,[0]
"The regret of the Algorithm 4 is
O
✓p NT log T logN
""
◆
Moreover, Algorithm 4 is ""-differentially private
Bandit Feedback: Reduction to the Non-private Setting
We begin by describing an algorithmic reduction that takes as input a non-private bandit algorithm and translates it into an ""-differentially private bandit algorithm.",4. Differentially Private Multi-Armed Bandits,0,[0]
The reduction works in a straight-forward manner by adding the requisite magnitude of Laplace noise to ensure differential privacy.,4. Differentially Private Multi-Armed Bandits,0,[0]
"For the rest of this section, for ease of exposition we will assume that both T and N are sufficiently large.
",4. Differentially Private Multi-Armed Bandits,0,[0]
"Algorithm 4 A0(A,D) – Reduction to the Non-private Setting for Bandit Feedback Input: Online Algorithm A, Noise Distribution D.
1: for t = 0 to T do 2: Receive x̃
t 2 X from A and output x̃ t .",4. Differentially Private Multi-Armed Bandits,0,[0]
"3: Receive a loss value hl
t , x̃ t i from the adversary.",4. Differentially Private Multi-Armed Bandits,0,[0]
"4: Sample Z
t ⇠ D. 5:",4. Differentially Private Multi-Armed Bandits,0,[0]
"Forward hl
t x̃ t i+ hZ t , x̃ t i as input to A. 6: end for
Algorithm 5 EXP2 with exploration µ Input: learning rate ⌘; mixing coefficient ; distribution µ
1: q 1 = 1
N . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
"1 N
2 RN .
2: for t = 1,2 . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
"T do 3: Let p
t = (1 )q t + µ and play i t ⇠ p t
4: Estimate loss vector l t by ˜l t = P+",4. Differentially Private Multi-Armed Bandits,0,[0]
t e ite T,4. Differentially Private Multi-Armed Bandits,0,[0]
"it l t , with P t = E",4. Differentially Private Multi-Armed Bandits,0,[0]
i⇠pt ⇥,4. Differentially Private Multi-Armed Bandits,0,[0]
e,4. Differentially Private Multi-Armed Bandits,0,[0]
i eT,4. Differentially Private Multi-Armed Bandits,0,[0]
"i ⇤ 5: Update the exponential weights,
q t+1
(i) = e ⌘hei, ˜",4. Differentially Private Multi-Armed Bandits,0,[0]
ltiq,4. Differentially Private Multi-Armed Bandits,0,[0]
"t (i) P
i
0 e ⌘hei0 , ˜ ltiq t (i0)
6: end for
The following Lemma characterizes the conditions under which Algorithm 4 is "" differentially private Lemma 4.2 (Privacy Guarantees).",4. Differentially Private Multi-Armed Bandits,0,[0]
"Assume that each loss vector l
t is in the set Y ✓ RN , such that max
t,l2Y | hl,x̃tikx̃tk1",4. Differentially Private Multi-Armed Bandits,0,[0]
"|  B. For D = Lap N ( ) where = B ""
, the sequence of outputs (x̃
t : t 2",4. Differentially Private Multi-Armed Bandits,0,[0]
"[T ]) produced by the Algorithm A0(A,D) is ""-differentially private.
",4. Differentially Private Multi-Armed Bandits,0,[0]
The following lemma charaterizes the regret of Algorithm 4.,4. Differentially Private Multi-Armed Bandits,0,[0]
"In particular we show that the regret of Algorithm 4 is, in expectation, same as that of the regret of the input algorithm A on a perturbed version of loss vectors.",4. Differentially Private Multi-Armed Bandits,0,[0]
Lemma 4.3 (Noisy Online Optimization).,4. Differentially Private Multi-Armed Bandits,0,[0]
"Consider a loss sequence (l
1 . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
l T ) and a convex set X .,4. Differentially Private Multi-Armed Bandits,0,[0]
"Define a perturbed version of the sequence as random vectors (˜l
t : t 2",4. Differentially Private Multi-Armed Bandits,0,[0]
[T ]),4. Differentially Private Multi-Armed Bandits,0,[0]
"as ˜l
t = l t + Z t where Z t is a random vector such that {Z
1 , . .",4. Differentially Private Multi-Armed Bandits,0,[0]
.,4. Differentially Private Multi-Armed Bandits,0,[0]
Z t } are independent and E[Z t ],4. Differentially Private Multi-Armed Bandits,0,[0]
0,4. Differentially Private Multi-Armed Bandits,0,[0]
for all t 2,4. Differentially Private Multi-Armed Bandits,0,[0]
"[T ].
Let A be a full information (or bandit) online algorithm which outputs a sequence (x̃
t
2 X : t 2",4. Differentially Private Multi-Armed Bandits,0,[0]
"[T ]) and takes as
input ˜l t (respectively h˜l t , x̃ t i) at time t. Let x⇤ 2 K be a fixed point in the convex set.",4. Differentially Private Multi-Armed Bandits,0,[0]
"Then we have that
E{Zt} "" EA "" TX
t=1
(hl t ,",4. Differentially Private Multi-Armed Bandits,0,[0]
x̃ t,4. Differentially Private Multi-Armed Bandits,0,[0]
"i hl t , x⇤i) ##
= E{Zt} "" EA "" TX
t=1
⇣ h˜l",4. Differentially Private Multi-Armed Bandits,0,[0]
t .x̃,4. Differentially Private Multi-Armed Bandits,0,[0]
t,4. Differentially Private Multi-Armed Bandits,0,[0]
i h˜l,4. Differentially Private Multi-Armed Bandits,0,[0]
"t , x⇤i ⌘##
We provide the proof of Lemma 4.2 and defer the proof of Lemma 4.3 to the Appendix Section B.
Proof of Lemma 4.2.",4. Differentially Private Multi-Armed Bandits,0,[0]
"Consider a pair of sequence of loss vectors that differ at exactly one time step – say L = (l 1 , . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
l t0 . .,4. Differentially Private Multi-Armed Bandits,0,[0]
.,4. Differentially Private Multi-Armed Bandits,0,[0]
", lT ) and L0 = (l1, . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
", l0t0 , . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
lT ).,4. Differentially Private Multi-Armed Bandits,0,[0]
"Since the prediction of produced by the algorithm at time step any time t can only depend on the loss vectors in the past (l 1 , . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
"l t 1), it is clear that the distribution of the output of the algorithm for the first t 0 rounds (x̃ 1 , . . .",4. Differentially Private Multi-Armed Bandits,0,[0]
x̃ t0) is unaltered.,4. Differentially Private Multi-Armed Bandits,0,[0]
"We claim that 8I ✓ R, it holds that
P(hl t0 + Zt0 , x̃t0i 2",4. Differentially Private Multi-Armed Bandits,0,[0]
"I)  e""P(hl0t0 + Zt0 , x̃t0i 2",4. Differentially Private Multi-Armed Bandits,0,[0]
"I)
",4. Differentially Private Multi-Armed Bandits,0,[0]
"Before we justify the claim, let us see how this implies that desired statement.",4. Differentially Private Multi-Armed Bandits,0,[0]
"To see this, note that conditioned on the value fed to the inner algorithm A at time t
0 , the distribution of all outputs produced by the algorithm are completely determined since the feedback to the algorithm at other time steps (discounting t
0 ) stays the same (in distribution).",4. Differentially Private Multi-Armed Bandits,0,[0]
"By the above discussion, it is sufficient to demonstrate ""-differential privacy for each input fed (as feedback) to the algorithm A.
For the sake of analysis, define lFict t as follows.",4. Differentially Private Multi-Armed Bandits,0,[0]
"If x̃ t = 0, define lFict
t
= 0 2 RN .",4. Differentially Private Multi-Armed Bandits,0,[0]
"Else, define lFict t 2 RN to be such that (lFict
t
)
",4. Differentially Private Multi-Armed Bandits,0,[0]
"i
= hlt,x̃ti x̃i",4. Differentially Private Multi-Armed Bandits,0,[0]
"if and only if i = argmax i2[d]|x̃i|
and 0 otherwise, where argmax breaks ties arbitrarily.",4. Differentially Private Multi-Armed Bandits,0,[0]
"Define ˜lFict
t
= lFict t + Z t .",4. Differentially Private Multi-Armed Bandits,0,[0]
"Now note that h˜lFict t , x̃ t",4. Differentially Private Multi-Armed Bandits,0,[0]
i,4. Differentially Private Multi-Armed Bandits,0,[0]
#NAME?,4. Differentially Private Multi-Armed Bandits,0,[0]
"t x̃ t i+ hZ t , x̃ t i.
",4. Differentially Private Multi-Armed Bandits,0,[0]
"It suffices to establish that each ˜lFict t is ""-differentially private.",4. Differentially Private Multi-Armed Bandits,0,[0]
"To argue for this, note that Laplace mechanism (Dwork et al., 2014a) ensures the same, since the l
1 norm of ˜lFict
t
is bounded by B.",4. Differentially Private Multi-Armed Bandits,0,[0]
Privacy:,4.1. Proof of Theorem 4.1,0,[0]
"Note that since max t,l2Y | hl,x̃tikx̃tk1",4.1. Proof of Theorem 4.1,0,[0]
"|  kYk1  1 as x̃ t 2 {e i
: i 2",4.1. Proof of Theorem 4.1,0,[0]
[N ]}.,4.1. Proof of Theorem 4.1,0,[0]
"Therefore by Lemma 4.2, setting = 1
""
is sufficient to ensure ""-differential privacy.
",4.1. Proof of Theorem 4.1,0,[0]
"Regret Analysis: For the purpose of analysis we define the following pseudo loss vectors.
",4.1. Proof of Theorem 4.1,0,[0]
"˜l t = l t + Z t
where by definition Z t ⇠ LapN ( ).",4.1. Proof of Theorem 4.1,0,[0]
"The following follows from Fact C.1 proved in the appendix.
P(kZ t k21 10 2 log 2 NT )  1
T 2
Taking a union bound, we have
P(9t kZ",4.1. Proof of Theorem 4.1,0,[0]
"t k21 10 2 log 2 NT )  1 T (5)
",4.1. Proof of Theorem 4.1,0,[0]
"To bound the norm of the loss we define the event F , {9t : kZ
t k21 10 2 log 2 NT}.",4.1. Proof of Theorem 4.1,0,[0]
"We have from (5) that
P(F )  1 T .",4.1. Proof of Theorem 4.1,0,[0]
"We now have that
E[Regret]  E[Regret| ¯F ] + P(F )E[Regret|F",4.1. Proof of Theorem 4.1,0,[0]
"]
Since the regret is always bounded by T we get that the second term above is at most 1.",4.1. Proof of Theorem 4.1,0,[0]
Therefore we will concern ourselves with bounding the first term above.,4.1. Proof of Theorem 4.1,0,[0]
"Note that Z t
remains independent and symmetric even when conditioned on the event ¯F .",4.1. Proof of Theorem 4.1,0,[0]
"Moreover the following statements also hold.
",4.1. Proof of Theorem 4.1,0,[0]
8t E[Z t | ¯F ],4.1. Proof of Theorem 4.1,0,[0]
"= 0 (6)
8t E[kZ t k21| ¯F ]  ",4.1. Proof of Theorem 4.1,0,[0]
"10 2 log 2 NT (7)
",4.1. Proof of Theorem 4.1,0,[0]
Equation (6) follows by noting that Z t remains symmetric around the origin even after conditioning.,4.1. Proof of Theorem 4.1,0,[0]
It can now be seen that Lemma 4.3 still applies even when the noise is sampled from LapN ( ) conditioned under the event ¯F,4.1. Proof of Theorem 4.1,0,[0]
(due to Equation 6).,4.1. Proof of Theorem 4.1,0,[0]
"Therefore we have that
E[Regret| ¯F ]",4.1. Proof of Theorem 4.1,0,[0]
"= E{Zt} "" EA "" TX
t=1
⇣ h˜l",4.1. Proof of Theorem 4.1,0,[0]
"t , x̃ t",4.1. Proof of Theorem 4.1,0,[0]
i h˜l,4.1. Proof of Theorem 4.1,0,[0]
"t , x⇤i ⌘# ¯F #
(8)
To bound the above quantity we make use of the following lemma which is a specialization of Theorem 1 in (Bubeck et al., 2012a)",4.1. Proof of Theorem 4.1,0,[0]
to the case of multi-armed bandits.,4.1. Proof of Theorem 4.1,0,[0]
Lemma 4.4 (Regret Guarantee for Algorithm 5).,4.1. Proof of Theorem 4.1,0,[0]
"If ⌘ is such that ⌘|he
i , ˜l t i|  1, we have that the regret of Algorithm 5 is bounded by
Regret  2 T + logN ⌘
+ ⌘E X
t
X
i
p t",4.1. Proof of Theorem 4.1,0,[0]
"(i)he i , ˜l t i2
Now note that due to the conditioning kZ t k21  10 2 log2 NT and therefore we have that
max t,x2 N |hZt, xi|  4 logNT.
",4.1. Proof of Theorem 4.1,0,[0]
"It can be seen that the condition ⌘|he i , ˜l t i|  1 in Theorem 4.4 is satisfied for exploration µ(i) = 1
N and under the condition ¯F as long as
⌘N(1 + 4 logNT ) 
which holds by the choice of these parameters.",4.1. Proof of Theorem 4.1,0,[0]
"Finally
E[Regret| ¯F ]
= E{Zt} "" EA "" TX
t=1
⇣ h˜l",4.1. Proof of Theorem 4.1,0,[0]
"t , x̃ t",4.1. Proof of Theorem 4.1,0,[0]
i h˜l,4.1. Proof of Theorem 4.1,0,[0]
"t , x⇤i ⌘# ¯F",4.1. Proof of Theorem 4.1,0,[0]
"#
 E{Zt}
"" logN
⌘ + ⌘
TX
t=1
Nk˜l t k21 + 2T ¯F",4.1. Proof of Theorem 4.1,0,[0]
"#
 E{Zt}
"" logN
⌘ + 2⌘
TX
t=1
N(kl t k21 + kZtk21)",4.1. Proof of Theorem 4.1,0,[0]
+ 2T ¯F,4.1. Proof of Theorem 4.1,0,[0]
"#
 logN ⌘",4.1. Proof of Theorem 4.1,0,[0]
+ 2⌘TN(1 + 2 log2 NT ) + 2T  O,4.1. Proof of Theorem 4.1,0,[0]
"✓q TN logN(1 + 2 log2 NT ) ◆
 O",4.1. Proof of Theorem 4.1,0,[0]
"✓p NT log T logN
""
◆",4.1. Proof of Theorem 4.1,0,[0]
"In this section we prove a general result about bandit linear optimization over general convex sets, the proof of which is deferred to the appendix.
",4.2. Differentially Private Bandit Linear Optimization,0,[0]
Theorem 4.5 (Bandit Linear Optimization).,4.2. Differentially Private Bandit Linear Optimization,0,[0]
Let X ✓ RN be a convex set.,4.2. Differentially Private Bandit Linear Optimization,0,[0]
"Fix loss vectors (l
1 , . . .",4.2. Differentially Private Bandit Linear Optimization,0,[0]
"l T ) such that max
t,x2X |hlt, xi|  M .",4.2. Differentially Private Bandit Linear Optimization,0,[0]
"We have that Algorithm 4 when run with parameters D = LapN ( ) (with = kYk1
"" ) and algorithm",4.2. Differentially Private Bandit Linear Optimization,0,[0]
"A = SCRiBLe(Abernethy et al., 2012) with step parameter ⌘ = q ⌫ log T
2N 2 T (M 2 + 2 NkXk22) we have the following guarantees that the regret of the algorithm is bounded by
O
p
T log T
s
N2⌫ ✓ M2 + NkXk2 2 kYk2 1
""2
◆!
where ⌫ is the self-concordance parameter of the convex body X .",4.2. Differentially Private Bandit Linear Optimization,0,[0]
"Moreover the algorithm is ""-differentially private.",4.2. Differentially Private Bandit Linear Optimization,0,[0]
"In this work, we demonstrate that ensuring differential privacy leads to only a constant additive increase in the incurred regret for online linear optimization in the full feedback setting.",5. Conclusion,0,[0]
We also show nearly optimal bounds (in terms of T) in the bandit feedback setting.,5. Conclusion,0,[0]
"Multiple avenues for future research arise, including extending our bandit results to other challenging partial-information models such as semi-bandit, combinatorial bandit and contextual bandits.",5. Conclusion,0,[0]
"Another important unresolved question is whether it is possible to achieve an additive separation in "", T in the adversarial bandit setting.",5. Conclusion,0,[0]
"In the paradigm of online learning, a learning algorithm makes a sequence of predictions given the (possibly incomplete) knowledge of the correct answers for the past queries.",abstractText,0,[0]
"In contrast to statistical learning, online learning algorithms typically offer distribution-free guarantees.",abstractText,0,[0]
"Consequently, online learning algorithms are well suited to dynamic and adversarial environments, where real-time learning from changing data is essential making them ubiquitous in practical applications such as servicing search advertisements.",abstractText,0,[0]
"In these settings often these algorithms interact with sensitive user data, making privacy a natural concern for these algorithms.",abstractText,0,[0]
"A natural notion of privacy in such settings is differential privacy (Dwork et al., 2006) which ensures that the outputs of an algorithm are indistinguishable in the case when a user’s data is present as opposed to when it is absent in a dataset.",abstractText,0,[0]
The Price of Differential Privacy for Online Learning,title,0,[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 209–216, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,0,[0]
"This paper describes the REAL Challenge (REAL), including the motivations for the challenge and preliminary results from the first year and prospects for the near future.",1 Introduction,0,[0]
"The ultimate goal of REAL is to bring about a steady stream of data from real users talking to spoken dialogue systems, that can be used for academic research.",1 Introduction,0,[0]
"The immediate goal of the first year of REAL is to bring together high school and undergraduate students, who have fresh ideas of how people will
talk to things in the future and what the constraints may be, and seasoned researchers, who know how to create the systems and could work with the students to realize a Wizard of Oz (WOZ) study or a proof-of-concept prototype to try out the idea.
",1 Introduction,0,[0]
"At SLT 2012, panelists stated that there was no publicly available, significant stream of spoken dialog data coming from real users other than the Lets Go data (Raux et al., 2006).",1 Introduction,0,[0]
"Although Lets Go can be used to create statistical models for some information-giving systems, with the wide variety of community needs, it cannot satisfy applications that are not two-way and information giving.",1 Introduction,0,[0]
"In answer to this, REAL was created to spark ideas for speech applications that are needed on a regular basis (fulfilling some real need) by real users.",1 Introduction,0,[0]
"Observing present applications in the commercial and academic community and how little use that they are getting, it was apparent, at least to the authors of this paper, that new minds were needed to devise the right kind of applications.",1 Introduction,0,[0]
"This led the REAL organizers to reach out to high school and undergraduate students.
",1 Introduction,0,[0]
"From announcements in late summer 2013 to the REAL workshop on June 21, 2014, and beyond, this paper traces how REAL was managed, the proposals we received, what happened at the workshop, what follow up we have had and how we measure success.",1 Introduction,0,[0]
"Speech and spoken dialog researchers often note that whereas industry has access to a wealth of ecologically valid speech data, the academic community lags far behind.",2 Motivation,0,[0]
The lag in quantity of data can impede research on system evaluation and in training the machine learning (ML) system components.,2 Motivation,0,[0]
This chasm can be filled by using recruited subjects.,2 Motivation,0,[0]
"But studies (Ai et al., 2007) have found that the resulting data does not resemble real user data.",2 Motivation,0,[0]
"Paid users follow the rules, but are usu-
209
ally just going through the motions.",2 Motivation,0,[0]
They do not create and follow their own personal goals.,2 Motivation,0,[0]
"Without personal goals, they are not overly concerned about satisfying the problem they were asked to solve.",2 Motivation,0,[0]
"For example, if they asked for a specific flight booking, they won’t change their mind opportunistically when a better plan becomes available.",2 Motivation,0,[0]
Yet this ability to find alternative ways to accomplish a goal is present in real user behavior and poses interesting challenges to spoken dialog systems.,2 Motivation,0,[0]
Paid users are not bothered by system results that are not what they had requested.,2 Motivation,0,[0]
They often want to finish the task as rapidly as possible while real users will usually take a little more time to get what they want.,2 Motivation,0,[0]
"And, they don’t quit or curse the system at same rate if things are not going well.",2 Motivation,0,[0]
"Thus, at evaluation time, the feedback from the paid user does not reflect the quality of system performance on real users.
",2 Motivation,0,[0]
"Although simulated users can be another datagenerating possibility, there are still several good reasons to pursue direct learning from human users.",2 Motivation,0,[0]
Usually conventional methods to build a user simulator follow a cycle of operations: data collection; annotation; model training and evaluation; and deployment for policy training.,2 Motivation,0,[0]
"The whole development cycle takes quite a long time, and so user behavior can change by the time it is done.",2 Motivation,0,[0]
"Moreover, it is highly likely that the new dialog policy, trained with the user simulator, will cause different user behavior patterns.",2 Motivation,0,[0]
"Additionally, there are always discrepancies between real and simulated user behavior due to many simplifying assumptions in the user model.",2 Motivation,0,[0]
"Thus, training on data from a simulated user can make dialog policies lag behind the ones that are optimal for real users.
",2 Motivation,0,[0]
"While there are significant real user speech databases in industry, that data and the platforms that collected it are not available for release to researchers due to a variety of issues including intellectual property (IP), monetization, customer loyalty and information privacy concerns.",2 Motivation,0,[0]
"So while industry can forge ahead (Halevy et al., 2009), academia is unable to show comparable performances, not due to poor research quality, but simply because of the lack of data.
",2 Motivation,0,[0]
Thus the community needs new streams of speech data that are available to academia.,2 Motivation,0,[0]
"For this, we must find new applications that real users actually need and will use often.",2 Motivation,0,[0]
"Although as-
sistant applications like Siri, Cortana et al. have sparked the interest and imagination of the public, many people dont use them.",2 Motivation,0,[0]
"The speech and spoken dialog communities must find something else, embracing novel interfaces and applications.",2 Motivation,0,[0]
And the research community may not be the place where these new ideas should come from.,2 Motivation,0,[0]
They might better originate with people who are: completely comfortable with the new technologies; not influenced by rigid ideas of what can and can’t be done; and not limited by an agenda of what they need to do next.,2 Motivation,0,[0]
This leads us to believe that the community needs the input of young students who have always lived with the technology and know how they would use it in the future.,2 Motivation,0,[0]
"Biased as the research community is by its knowledge of the science behind the systems, researchers also sometimes overlook some of the basic issues that must be dealt with, going forward.",2 Motivation,0,[0]
Younger students may also be able to identify the red flags that are keeping speech from being an interface of choice.,2 Motivation,0,[0]
"An important side-benefit of this approach is that this challenge serves as an additional vehicle to bring new practitioners into the spoken dialogue community, by having early access to top researchers and training materials.",2 Motivation,0,[0]
There is a significant leap from a young student’s idea to a data-generating system.,3 THE REAL CHALLENGE PROCESS,0,[0]
"The process that REAL put in place breaks this leap into small, achievable steps.",3 THE REAL CHALLENGE PROCESS,0,[0]
"First, the organizers of REAL formed an international scientific committee, shown in Table 1.",3 THE REAL CHALLENGE PROCESS,0,[0]
"The scientific committee consisted of people who had espoused the spirit of REAL and were willing to work to make it a success.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"A webpage (https://dialrc.org/realchallenge/) was created, including a timeline through the June 21st, 2014 workshop, a separate page with details of REAL for students and their teachers, contact information and an application form.",3 THE REAL CHALLENGE PROCESS,0,[0]
Researchers around the world were contacted and asked to recruit students.,3 THE REAL CHALLENGE PROCESS,0,[0]
"Six countries began recruitment and four, China, Ireland, Korea and the US, had applicants for the 2014 challenge.",3 THE REAL CHALLENGE PROCESS,0,[0]
One experienced researcher headed each country’s efforts and was responsible for recruiting and organizing their students and for sending them to the workshop.,3 THE REAL CHALLENGE PROCESS,0,[0]
"The international Coordination Committee members are shown in Table 2.
",3 THE REAL CHALLENGE PROCESS,0,[0]
The students were encouraged to contact the organizers at any time for more information and/or for guidance in proposal writing.,3 THE REAL CHALLENGE PROCESS,0,[0]
"The proposals were submitted by April 1, 2014.",3 THE REAL CHALLENGE PROCESS,0,[0]
"They were sent to the scientific committee for review, with two reviewers per proposal.",3 THE REAL CHALLENGE PROCESS,0,[0]
"The reviewers, taking into account the age of the participants (from 13 to 23 years old), were asked to evaluate the proposals according the following criteria:
novelty: the proposal could not be exactly the same as an existing application.",3 THE REAL CHALLENGE PROCESS,0,[0]
"While existing applications could have the same subject, like cooking, the user interaction and/or function had to be novel.
speech is clearly necessary: the students needed to show that the application solves an issue thanks to its use of speech communication.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"practical: this idea could be implemented either with current technology or with clearly definable extensions.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"viable: this application is likely to attract real users — while it is not evident at present how best to measure viability, at this stage we could poll potential users.",3 THE REAL CHALLENGE PROCESS,0,[0]
"We also believe that the students are well aware of their peers habits and needs.
",3 THE REAL CHALLENGE PROCESS,0,[0]
The reviews were edited to take into account the age of the students.,3 THE REAL CHALLENGE PROCESS,0,[0]
"They included feedback on shaping the ideas (focusing the application, getting rid of spurious activities) and requiring more details about the application (how would someone use it, under what conditions would someone use it.",3 THE REAL CHALLENGE PROCESS,0,[0]
who would want to use it).,3 THE REAL CHALLENGE PROCESS,0,[0]
"After the students received their feedback, they were told what they would need to prepare for the workshop: a oneminute presentation of their idea, a poster and a presentation in front of the poster.",3 THE REAL CHALLENGE PROCESS,0,[0]
"Some students (China, Ireland) had exams at the time of the workshop and participated via Skype.",3 THE REAL CHALLENGE PROCESS,0,[0]
These students were asked to record their in-front-of-poster presentations in case Skype was not working (in the end it worked very well!).,3 THE REAL CHALLENGE PROCESS,0,[0]
"Then the students were given some training:
• a class on speech and spoken dialog for the high school students (undergrads had had this in one of their regular classes);
• a video on how to make a poster – ensuring smooth communication between students and researchers on the day of the workshop: the poster included the goal, a comparison to what presently exists, why their idea was better, and an illustration of the use of their idea showing why it is needed, how someone would use it and how it solves the problem.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"The workshop was held on June 21, 2014.",3 THE REAL CHALLENGE PROCESS,0,[0]
"After the one-minute presentations, the students stood in front of their posters for 90 minutes.",3 THE REAL CHALLENGE PROCESS,0,[0]
In the following 30 minutes they could go around to see one another’s posters.,3 THE REAL CHALLENGE PROCESS,0,[0]
"Then groups of researchers and
students formed to discuss the ideas.",3 THE REAL CHALLENGE PROCESS,0,[0]
All of the students found at least two researchers interested in having a discussion with them.,3 THE REAL CHALLENGE PROCESS,0,[0]
Each group created a few slides summarizing their discussion and reported back to everyone.,3 THE REAL CHALLENGE PROCESS,0,[0]
"Most of the reports contained ways to focus ideas, to make them doable and most importantly, to define the next steps.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"After the workshop, the organizers followed up with the researcher participants to find out their plans going forward.",3 THE REAL CHALLENGE PROCESS,0,[0]
"They were also asked whether they would be encouraging high school or undergraduates to join REAL in the next round.
",3 THE REAL CHALLENGE PROCESS,0,[0]
"Going forward, the organizers plan to have yearly REAL meetings.",3 THE REAL CHALLENGE PROCESS,0,[0]
"While the first workshop saw only proposals, the second and following ones should see both new proposals and results of WOZ studies and proof-of-concept demos from the proposals presented the previous year.",3 THE REAL CHALLENGE PROCESS,0,[0]
"This rolling participation enables new students and researchers to join at any time and puts less pressure on past participants – the successful projects will have something to show, but aren’t expected to have a fully working system, within just one year.",3 THE REAL CHALLENGE PROCESS,0,[0]
"The intended cycle for successful proposals is the following:
1.",3 THE REAL CHALLENGE PROCESS,0,[0]
"find technical partners
2.",3 THE REAL CHALLENGE PROCESS,0,[0]
"for limitations that must be dealt with: work on why this is a limitation and what the possible fixes are
3.",3 THE REAL CHALLENGE PROCESS,0,[0]
"for applications or systems: work on the design then on the prototype or WOZ system
4.",3 THE REAL CHALLENGE PROCESS,0,[0]
"conduct a study (testing the prototype or WOZ system)
5.",3 THE REAL CHALLENGE PROCESS,0,[0]
"show study results (and possibly demo of system or propose a major design change for speech systems)
6.",3 THE REAL CHALLENGE PROCESS,0,[0]
write a proposal for future funding to continue the work,3 THE REAL CHALLENGE PROCESS,0,[0]
"The first year of REAL enabled the organizers to assess how well its goals were fulfilled, what outcomes there were and what lessons were learned.",4 Year One Winning Proposals,0,[0]
The main outcome of REAL can first be shown in the quality of the proposals.,4 Year One Winning Proposals,0,[0]
"Here are summaries of the 11 successful proposals from 2014 (note
that all participants from outside the US are undergrads, the US participants are high school students):
Bocal (Jude Rosen, Joe Flot, US)",4 Year One Winning Proposals,0,[0]
How can we protect the privacy of the user at the same time as offering a high quality of speech commanding and response?,4 Year One Winning Proposals,0,[0]
Boneconducting devices can answer this question by capturing sounds emanating through skulls.,4 Year One Winning Proposals,0,[0]
"The next step includes finding out a specific set of scenarios where the device will be useful and conducting Wizard of Oz experiments to collect data about how users would behave with the device on.
",4 Year One Winning Proposals,0,[0]
"Daily Journaling (Keun Woo Park, Jungkook Park, Korea)",4 Year One Winning Proposals,0,[0]
This system will help users record events in their everyday life.,4 Year One Winning Proposals,0,[0]
"Lightweight and multimodal, it uses many sensors to determine what is going on around the user.",4 Year One Winning Proposals,0,[0]
"To interpret what it captures, it asks the user questions.",4 Year One Winning Proposals,0,[0]
"With the information gleaned from the questions, it updates its information about the user.
",4 Year One Winning Proposals,0,[0]
"Fashion Advisor (Jung-eun Kim, Korea)",4 Year One Winning Proposals,0,[0]
This advisor knows what clothing a person possesses and carries on a dialog in the morning to help the user choose what to wear.,4 Year One Winning Proposals,0,[0]
It would have a camera to capture the user and show them how they would look when wearing its suggestions (like a mirror).,4 Year One Winning Proposals,0,[0]
It also knows what the weather will be and will suggest appropriate clothing.,4 Year One Winning Proposals,0,[0]
"It can also search sources such as Pinterest for clothes to purchase that would work with what the use has and their body type.
",4 Year One Winning Proposals,0,[0]
"Gourmet (Jaichen Shi, China)",4 Year One Winning Proposals,0,[0]
The Gourmet helps people choose a restaurant.,4 Year One Winning Proposals,0,[0]
Many people have dietary restrictions and the Gourmet would suggest restaurants where the user can be assured of finding something they can eat.,4 Year One Winning Proposals,0,[0]
It also tells the user what other diners have thought of a restaurant and can find specific feedback from diners who were at the restaurant on the present day.,4 Year One Winning Proposals,0,[0]
"When a choice is made, it can call the restaurant for reservations.
",4 Year One Winning Proposals,0,[0]
"Human Chatting System (Yunqi Guo, China)",4 Year One Winning Proposals,0,[0]
"This is a system that allows people to chat
with it.",4 Year One Winning Proposals,0,[0]
"It is aimed at helping people rehearse discussions they would have with real people, either helping in how to deal with a difficult social situation (asking a girl for a date, for example), or speaking a new language (with a tutor that detects speaking errors and tells the student how to correct them).
",4 Year One Winning Proposals,0,[0]
"Lecture Trainer (Qizhe Xie, China)",4 Year One Winning Proposals,0,[0]
This application would listen to a user preparing a presentation and help them out.,4 Year One Winning Proposals,0,[0]
"It could help with word choice, but also with grammar, intonation, and fluency.",4 Year One Winning Proposals,0,[0]
"The user could choose a topic and also listen to recorded speeches from famous people so that the user could imitate the latter.
",4 Year One Winning Proposals,0,[0]
"Mobile Cooking App (BongJin Sohn, JongWoo Choi, DongHyun Kim, Korea)",4 Year One Winning Proposals,0,[0]
Modern-day appliances continue to evolve based on communication with users to identify and meet their needs.,4 Year One Winning Proposals,0,[0]
"The cooking app will offer a cooking guide in the form of audio or video, voice control for oven and alarm setting, and provide a grocery list, etc.",4 Year One Winning Proposals,0,[0]
"This app traces interaction history and each step of a recipe to make a dialog intelligent and efficient by being context-aware.
",4 Year One Winning Proposals,0,[0]
"Neeloid (Neeloy Chakraborty, US)",4 Year One Winning Proposals,0,[0]
The invention connects people with their surroundings.,4 Year One Winning Proposals,0,[0]
Camera and other sensors can also work together to create an accurate description of the audience’s surroundings.,4 Year One Winning Proposals,0,[0]
It also understands gestures pointing at certain things for inquiry and looks into connected wiki to retrieve relevant information.,4 Year One Winning Proposals,0,[0]
"This invention may give the visually impaired the confidence of knowing what is around them without the use of a white cane, hoople, guide, etc.",4 Year One Winning Proposals,0,[0]
"Another application of this idea is as an educational tool that can be used by a wide variety of people, in particular, children full of curiosity.
",4 Year One Winning Proposals,0,[0]
"Sam the Kitchen Assistant (Enno Hermann, Ireland)",4 Year One Winning Proposals,0,[0]
Sam comes to the aid of the cook who has hands occupied and full of food and eyes also busy.,4 Year One Winning Proposals,0,[0]
"Sam can tell a cook what to do next in a recipe, but also has information about how to adapt a recipe to any one of many dietary restrictions.",4 Year One Winning Proposals,0,[0]
"Sam can suggest a recipe, on the
way home, given what is in the house and list what needs to be bought.
",4 Year One Winning Proposals,0,[0]
"SmartCID (Zachary McAlexander, David Donehue, US) Millions of consumers today use smart technology in everyday life, including smartphones, tablets, and desktop computers.",4 Year One Winning Proposals,0,[0]
"However, none of these technologies are truly easy-to-use.",4 Year One Winning Proposals,0,[0]
The user must always issue some command before the aid begins to operate.,4 Year One Winning Proposals,0,[0]
SmartCID solves this problem by automatically detecting external activity and instantaneously capturing content.,4 Year One Winning Proposals,0,[0]
"For example, SmartCID can detect things like people posing for a picture, the word cheese said by a group, or a laugh from the user, to prompt the device to begin recording a video or audio file, allowing the user to review the funny moment at a later date.
",4 Year One Winning Proposals,0,[0]
"Smart Watch (So Hyeon Jung, Korea)",4 Year One Winning Proposals,0,[0]
This is a patient health care system.,4 Year One Winning Proposals,0,[0]
Elderly users (some with poor eyesight) can be told when to take their medication.,4 Year One Winning Proposals,0,[0]
They can also find out when their supply of medication is about to run out and get help ordering more.,4 Year One Winning Proposals,0,[0]
The system can also guide its users in healthy eating choices for the specific nutrients that the individual needs.,4 Year One Winning Proposals,0,[0]
"And since it can suggest good foods, it can also help with calorie counts.",4 Year One Winning Proposals,0,[0]
"The first outcome of the workshop was the proposals for new ideas, described in the previous section.",5 Outcomes from the First Year,0,[0]
"All of them met the desired criteria of novelty, use of speech, with potential for practicality and viability.",5 Outcomes from the First Year,0,[0]
"One of the ideas has already led to a peer-reviewed publication (Jung et al., 2015).
",5 Outcomes from the First Year,0,[0]
Another outcome of REAL is the set of issues in the ubiquitous use of speech that the students raised.,5 Outcomes from the First Year,0,[0]
"First, the Bocal proposal raised the issue of privacy.",5 Outcomes from the First Year,0,[0]
"Although we generally think that speech should be used in any setting, it is possible that privacy may restrict its frequent use in environments where there are other people in close proximity to the speaker.",5 Outcomes from the First Year,0,[0]
"In this situation, it may indeed be necessary to either whisper or use a boneconducting microphone.",5 Outcomes from the First Year,0,[0]
"Second, several proposals, such as Mobile Cooking, Lecture Trainer, and Human Chatting System show that the most com-
pelling applications for a user may not be for general use, but rather suites of applications that are important to individuals.",5 Outcomes from the First Year,0,[0]
"Finally, we see that many of the proposals, without being prompted by organizers or teachers, were in a context of busy hands and eyes.
",5 Outcomes from the First Year,0,[0]
A third outcome of REAL is what took place the day of the workshop (described in Section 3).,5 Outcomes from the First Year,0,[0]
Students described their ideas to technologists/researchers.,5 Outcomes from the First Year,0,[0]
The participants met with students in the afternoon.,5 Outcomes from the First Year,0,[0]
The breakout reports from these meetings were given by both the researchers and the students.,5 Outcomes from the First Year,0,[0]
All had made slides and the one common element was the next steps points that all displayed.,5 Outcomes from the First Year,0,[0]
"For many of the projects, the students got help in:
focus: concentrating on just one thing, deciding which thing was worth it, not trying to solve all of the worlds problems.
deciding what to do next: e.g., Is there hard-
ware to concentrate on?",5 Outcomes from the First Year,0,[0]
Should a scenario be defined?,5 Outcomes from the First Year,0,[0]
What software is involved?,5 Outcomes from the First Year,0,[0]
"What software modules exist and which ones must be built?
",5 Outcomes from the First Year,0,[0]
"Finally, there is the promise of what is to come.",5 Outcomes from the First Year,0,[0]
Table 3 shows the post-meeting feedback from participants concerning their plans.,5 Outcomes from the First Year,0,[0]
"For example, one academic participant is proposing internships to two of the students (from two different proposals).",5 Outcomes from the First Year,0,[0]
The first year of REAL can be assessed using several metrics.,6 Assessing REAL,0,[0]
"But before the metrics are used, some perspective is needed.",6 Assessing REAL,0,[0]
It is very difficult in one year to get a large part of the speech and spoken dialog community actively interested.,6 Assessing REAL,0,[0]
"It is hard to plan the venue of the workshop so that it coincides with a major meeting, while not taking place at the same time.",6 Assessing REAL,0,[0]
"It is also hard to organize students
in many different countries, including the funding for the students.",6 Assessing REAL,0,[0]
And finding support for REAL is also difficult.,6 Assessing REAL,0,[0]
Industry is not yet sure what a company can get from this meeting.,6 Assessing REAL,0,[0]
One measure is researcher participation.,6 Assessing REAL,0,[0]
"There were 21 researchers at the Workshop, 17 people were from academia, and the rest were from industry.",6 Assessing REAL,0,[0]
Another metric is the depth and breadth of what is being proposed to the students to take their work forward.,6 Assessing REAL,0,[0]
Yet another metric is whether colleagues plan to get more students involved in the coming year.,6 Assessing REAL,0,[0]
This is also shown in Table 3.,6 Assessing REAL,0,[0]
"Three colleagues from three different countries proposed either to:
• increase the numbers of their participants next year
• bring in a new high school class • bring in new undergraduate students The use of Skype is considered to have been very helpful this year.",6 Assessing REAL,0,[0]
"If a student worked on a proposal during the year and could not, for some reason, attend the workshop (including exams, lack of travel funding, etc), then they were still able to make a presentation and get feedback.",6 Assessing REAL,0,[0]
Another way to assess REAL is to observe the results of the interaction between the students and the researchers at the workshop breakout sessions.,6 Assessing REAL,0,[0]
"Some examples of the changes in the projects:
• Smart Watch project: there were four functions proposed: calorie-store, alarm, food recommendation, exercise recommendation.",6 Assessing REAL,0,[0]
"Issues that arose: hardware could become multiple devices; calorie store might be difficult for users; it should be multimodal, combining both spoken dialog and images for the users.",6 Assessing REAL,0,[0]
Plan of action: break project into individual functions; examine existing apps to get a sense of range of interaction; do a WOZ data collection with diet expert function to observe dialogs and users reactions; use WOZ data to finalize design and train ASR/NLU.,6 Assessing REAL,0,[0]
"Subsequent to the workshop, this action plan was followed, and the food and exercise recommendation functions were implemented and tested, resulting in a peer review publication (Jung et al., 2015).
",6 Assessing REAL,0,[0]
• Bocal project: focusing ideas into a platform for allowing system-user communication when privacy is important.,6 Assessing REAL,0,[0]
"Noting that
the key technology will be transferring input from skull microphones to text, the main challenges were gaining an understanding of the differences between speech through skull and standard microphones and understanding how this technology will influence users’ behavior.",6 Assessing REAL,0,[0]
"The action items were: choosing application domains that will necessitate privacy, like banking; collecting data with a WOZ setup; analyzing the data to find features for encoding the output of the skull microphone; developing models for transforming the output of the skull microphone to text; developing a spoken dialog system for exhibiting the feasibility of the approach.
",6 Assessing REAL,0,[0]
"Thus, the students got a considerable amount of help in focusing their ideas, in breaking down the steps that they need to take in the upcoming year to find out how feasible their projects are, and in understanding what the hardware and usage issues were.",6 Assessing REAL,0,[0]
"As seen on Table 3, several of the students have found mentors and they will be going forward with their projects.",6 Assessing REAL,0,[0]
Although we have had a successful first year we are interested in the long term continued success of this challenge.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
As it grows in stability year to year it will be easier to get students to be aware of and take part in it.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
Even since out first year we have seen more standardized SDKs for developing speech based systems on more platforms.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Microsoft’s Cortana, and Amazon’s Echo offer SDKs that we would like to utilize to aid student’s proposals and eventual development.
",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
The REAL Challenge is a bold step for researchers.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
Its stated goal was to find new applications that would create streams of spoken dialog data from real users.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
It has achieved this goal — students have proposed novel systems that have the potential to be very useful and thus to attract real users.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Beyond the stated goals of the Challenge, the students have brought to the forefront issues that must be dealt with:
• The issue of privacy must be addressed.",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"For example, real users would not dictate email or text messages if they feel that their messages are not secure.
",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"• It is probable that the most successful speech applications will not be the general ones (like SIRI and Cortana), but may be the ones that are highly personalized to specific tasks.
",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
Plans going forward concern both this year’s projects and those to come in the future.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
REAL is seen as a regularly occurring event where there are multiple levels of presentation.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
There will be students who have proposed an idea (like all of the 2014 participants) who are looking for feedback and mentorship.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
There will be students who proposed their ideas the preceding year and are presenting either WOZ study results or a prototype.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"And ultimately there will be students (and researchers) who proposed one year, presented preliminary results the next year and are presenting a working system and real user data.
",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
The REAL Challenge continues in its second year with renewed support from the National Science Foundation.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Year two proposals for the REAL challenge are under development, with an intended participant workshop in Fall 2015.",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
So far there are six proposals for 2015: three undergraduates and three high school students.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"The undergrad proposals are all new, while two of the ones from the high school students are updates of last years proposal and one is new.",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Table 4 shows this years proposals.
",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Due to the differences in academic schedules around the world, to the success of virtual participation and to cost, the second year will see the
students all participate remotely.",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
Experts in the field will be brought in to the Workshop in person.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
Individual presentations will be given and group breakouts will be organized.,7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
"Given that last year this Challenge not only proposed novel applications, but also unearthed interesting issues, part of the Workshop will address some of the issues (such as privacy) that are being brought to light.",7 CONCLUSIONS AND FUTURE DIRECTIONS,0,[0]
The REAL Challenge has been sponsored by NSF grant no. CNS-1405644 and 1406000.,ACKNOWLEDGEMENTS,0,[0]
The student participants in REAL were mentioned in the description of their projects.,ACKNOWLEDGEMENTS,0,[0]
"We would like to thank Ann Gollapudi, the teacher of the US highschool students.",ACKNOWLEDGEMENTS,0,[0]
"The persons who ran the Challenge in their own countries were:
• Gary Lee, Korea • Kai Yu, China • Emer Gilmartin, Ireland.
",ACKNOWLEDGEMENTS,0,[0]
"The authors would like to thank the researchers who took part in the Workshop, many of whom have made plans to follow up on projects and/or future versions of the Challenge.",ACKNOWLEDGEMENTS,0,[0]
"The REAL Challenge took place for the first time in 2014, with a long term goal of creating streams of real data that the research community can use, by fostering the creation of systems that are capable of attracting real users.",abstractText,0,[0]
A novel approach is to have high school and undergraduate students devise the types of applications that would attract many real users and that need spoken interaction.,abstractText,0,[0]
The projects are presented to researchers from the spoken dialog research community and the researchers and students work together to refine and develop the ideas.,abstractText,0,[0]
Eleven projects were presented at the first workshop.,abstractText,0,[0]
Many of them have found mentors to help in the next stages of the projects.,abstractText,0,[0]
The students have also brought out issues in the use of speech for real applications.,abstractText,0,[0]
Those issues involve privacy and significant personalization of the applications.,abstractText,0,[0]
"While long-term impact of the challenge remains to be seen, the challenge has already been a success at its immediate aims of bringing new ideas and new researchers into the community, and serves as a model for related outreach efforts.",abstractText,0,[0]
THE REAL CHALLENGE 2014: PROGRESS AND PROSPECTS,title,0,[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 186–196, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,0,[0]
"It has been argued that sarcasm, or verbal irony, is a type of interactional phenomenon with specific perlocutionary effects on the hearer (Haverkate, 1990), such as to break their pattern of expectation.",1 Introduction,0,[0]
"Thus, to be able to detect speakers’ sarcastic intent it is necessary (even if maybe not sufficient) to consider their utterances in the larger conversation context.",1 Introduction,0,[0]
Consider the Twitter conversation example in Table 1.,1 Introduction,0,[0]
"Without the context of UserA’s
1We use response and reply interchangeably.
statement, the sarcastic intent of UserB’s response might not be detected.
",1 Introduction,0,[0]
"Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; González-Ibáñez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016).",1 Introduction,0,[0]
"In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014).
",1 Introduction,0,[0]
"In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums).",1 Introduction,0,[0]
"Table 1 shows some examples of sarcastic replies taken from two media platforms (userB
186
and userD’s posts, respectively) and a minimum unit of conversation context given by the prior turn (userA and userC’s posts, respectively).
",1 Introduction,0,[0]
"We address two specific issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply (e.g., which sentence(s) from userC’s comment triggered userD’s sarcastic reply).",1 Introduction,0,[0]
"To address the first issue, we investigate both SVM models with linguistically-motivated discrete features and several types of Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) that can model both the context and the sarcastic reply (Section 3).",1 Introduction,0,[0]
"We show that the conditional LSTM network (Rocktäschel et al., 2015) and LSTM networks with sentence level attention on context and reply outperform the LSTM model that reads only the reply (Section 4).",1 Introduction,0,[0]
"To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention, and discuss the results compared with human performance on the task (Section 4.1).",1 Introduction,0,[0]
We make all datasets and code available.2,1 Introduction,0,[0]
One goal of our investigation is to comparatively study two types of social media platforms that have been considered individually for sarcasm detection: discussion forums and Twitter.,2 Data,0,[0]
"We first discuss the two datasets and then point out some differences between them that could impact results and modeling choices.
",2 Data,0,[0]
Discussion Forums.,2 Data,0,[0]
"Oraby et al. (2016) have introduced the Sarcasm Corpus V2, a subset of the Internet Argument Corpus that consists of discussion forum data.",2 Data,0,[0]
This corpus consists of sarcastic responses and their context (quotes to which the posts are replies to).,2 Data,0,[0]
"The annotation of sarcastic vs. non-sarcastic replies was done using crowdsourcing, where annotators were asked to label a reply as sarcastic if any part of the reply contained sarcasm (thus annotation is done at the reply/comment level and not sentence level).",2 Data,0,[0]
The final gold sarcastic label was assigned only if a majority of the annotators labeled the reply as sarcastic.,2 Data,0,[0]
"Although the dataset described by Oraby et al. (2016) consists of 9,400 post, only
2https://github.com/debanjanghosh/sarcasm context
50% (4,692 altogether; balanced between sarcastic and non-sarcastic categories) of that corpus is currently available for research.3
An example from this dataset is given in Table 1, where userD’s reply has been labeled as sarcastic by annotators, in the context of userC’s post/comment.
",2 Data,0,[0]
"Twitter: To collect sarcastic and non-sarcastic tweets, we adopt the methodology proposed in related work (González-Ibáñez et al., 2011; Riloff et al., 2013; Bamman and Smith, 2015; Muresan et al., 2016).",2 Data,0,[0]
"The sarcastic tweets were collected using hashtags such as, #sarcasm, #sarcastic, #irony, while the non-sarcastic tweets were the ones that do not contain these hashtags, but they might contain sentiment hashtags such as #happy, #love, #sad, #hate.",2 Data,0,[0]
"We exclude the retweets, duplicates, quotes, tweets that contain only hashtags and URLs or are shorter than three words.",2 Data,0,[0]
"Also, we eliminate all tweets where the hashtags of interest were not positioned at the very end of the message.",2 Data,0,[0]
"Thus, we removed utterances such as “#sarcasm is something that I love”.",2 Data,0,[0]
"To built the conversation context, for each sarcastic and nonsarcastic utterance we used the “reply to status” parameter in the tweet to determine whether it was in reply to a previous tweet: if so, we downloaded the last tweet (i.e., “local conversation context”) to which the original tweet was replying to (Bamman and Smith, 2015).",2 Data,0,[0]
"In addition, we also collected the entire threaded conversation when available (Wang et al., 2015).",2 Data,0,[0]
"Although we have collected over 200K tweets in the first step, around 13% of them were a reply to another tweet and thus our final Twitter conversations set contains 25,991 instances (12,215 instances for sarcastic class and 13,776 instances for the non-sarcastic class).",2 Data,0,[0]
"We observe that 30% of the tweets have more than one tweet in the conversation context.
",2 Data,0,[0]
There are two main differences between these two datasets that need to be acknowledged.,2 Data,0,[0]
"First, discussion forum posts are much longer than Twitter messages.",2 Data,0,[0]
"Second, the way the gold labels for the sarcastic class are obtained is different.",2 Data,0,[0]
"In the discussion forum dataset the gold label is obtained via crowdsourcing, thus the gold label emphasizes whether the sarcastic intent is perceived by hearers (we do not know if the speaker intended to be sarcastic or not).",2 Data,0,[0]
"In Twitter dataset the gold label
3This reduction in the training size will have obvious effects in the classification performance.
is given directly by the #hashtag the speaker used, signaling clearly the speaker’s sarcastic intent.",2 Data,0,[0]
A third difference should be made: the size of the forum dataset is much smaller than the size of the Twitter dataset.,2 Data,0,[0]
"To assess the effect of conversation context (c) on labeling a reply (r) as sarcastic or not sarcastic, we consider two binary classification tasks.",3 Computational Models and Experimental Setup,0,[0]
We refer to sarcastic instances as S and non-sarcastic instances as NS.,3 Computational Models and Experimental Setup,0,[0]
"In the first task, classification is performed using the reply in isolation (Sr vs. NSr task).",3 Computational Models and Experimental Setup,0,[0]
"In the second, the classification considers both the reply and its context (Sc+r vs. NSc+r task).",3 Computational Models and Experimental Setup,0,[0]
"We experiment with two types of computational models: Support Vector Machines (SVM) with linguistically-motivated discrete features (used as baseline; SVMbl), and approaches using distributed representations.",3 Computational Models and Experimental Setup,0,[0]
"For the latter we use the Long short-term Memory (LSTM) Networks (Hochreiter and Schmidhuber, 1997) that have been shown to be successful in various NLP tasks, such as constituency parsing (Vinyals et al., 2015), language modeling (Zaremba et al., 2014), machine translation (Sutskever et al., 2014) and textual entailment (Bowman et al., 2015; Rocktäschel et al., 2015; Parikh et al., 2016).",3 Computational Models and Experimental Setup,0,[0]
"We present these models in the next subsections.
3.1 SVM with discrete features (SVMbl)
For features, we used n-grams, lexicon-based features, and sarcasm indicators that are commonly used in the existing sarcasm detection approaches (Tchokni et al., 2014; González-Ibáñez et al., 2011; Riloff et al., 2013; Joshi et al., 2015; Ghosh et al., 2015; Muresan et al., 2016).",3 Computational Models and Experimental Setup,0,[0]
"Below is a short description of the features.
",3 Computational Models and Experimental Setup,0,[0]
"• BoW: Features are derived from unigram, bigram, and trigram representation of words.
",3 Computational Models and Experimental Setup,0,[0]
"• Sentiment and Pragmatic features: We use the Linguistic Inquiry and Word Count (LIWC) lexicon (Pennebaker et al., 2001) to identify the pragmatic features.",3 Computational Models and Experimental Setup,0,[0]
Each category in this dictionary is treated as a separate feature and we define a Boolean feature that indicates if a context or a reply contains a LIWC category.,3 Computational Models and Experimental Setup,0,[0]
"Two sentiment lexicons are also used to model the utterance sentiment:
“MPQA” (Wilson et al., 2005) and “Opinion Lexicon” (Hu and Liu, 2004).",3 Computational Models and Experimental Setup,0,[0]
"To capture sentiment, we count the number of positive and negative sentiment tokens, negations, and use a boolean feature that represents whether a reply contains both positive and negative sentiment tokens.",3 Computational Models and Experimental Setup,0,[0]
"For the Sc+r vs. NSc+r classification task, we check whether the reply r has a different sentiment than the context c (similar to Joshi et al. (2015)).",3 Computational Models and Experimental Setup,0,[0]
"Given that sarcastic utterances often contain a positive sentiment towards a negative situation, we hypothesize that this feature will capture this type of sentiment incongruity.
",3 Computational Models and Experimental Setup,0,[0]
• Sarcasm Indicators: Burgers et al. (2012) introduce a set of sarcasm indicators that explicitly signal if an utterance is sarcastic.,3 Computational Models and Experimental Setup,0,[0]
"We use morpho-syntactic features such as interjections (e.g., “uh”, “oh”, “yeah”), tag questions (e.g., “is not it?”, “don’t they”), exclamation marks (e.g., “!”, “?”); typographic features such as capitalization of words, quotation marks, emoticons; tropes such as superlative and intensifiers words (e.g., “greatest”, “best”, “really”) that often occur in sarcastic utterances (Camp, 2012).
",3 Computational Models and Experimental Setup,0,[0]
"When building the features, we lowercased the utterances, except the words where all the characters are uppercased (i.e., we did not lowercased “GREAT”, “SO”, and “WONDERFUL” in “GREAT i’m SO happy; shattered phone on this WONDERFUL day!!!”).",3 Computational Models and Experimental Setup,0,[0]
"Tokenization is conducted via CMU’s Tweeboparser (Gimpel et al., 2011).",3 Computational Models and Experimental Setup,0,[0]
"For the discussion forum dataset we use the NLTK tool (Bird et al., 2009) for sentence boundary detection and tokenization.",3 Computational Models and Experimental Setup,0,[0]
"We used libSVM toolkit with Linear Kernel (Chang and Lin, 2011) with weights inversely proportional to the number of instances in each class.",3 Computational Models and Experimental Setup,0,[0]
"LSTMs are a type of recurrent neural networks (RNNs) able to learn long-term dependencies (Hochreiter and Schmidhuber, 1997).",3.2 Long Short-Term Memory Networks,0,[0]
"Recently, LSTMs have been shown to be effective in Natural Language Inference (NLI) research, where the task is to establish the relationship between multiple inputs (i.e., a pair of premise and hypothesis as in the case of Recognizing Textual Entailment task (Bowman et al., 2015; Rocktäschel et al., 2015;
Parikh et al., 2016)).",3.2 Long Short-Term Memory Networks,0,[0]
"Since our goal is to explore the role of contextual information (our first input) for recognizing whether the reply (our second input) is sarcastic or not, we argue that using LSTM networks that read the context and reply are a natural modeling choice.
",3.2 Long Short-Term Memory Networks,0,[0]
Attention-based LSTM Networks:,3.2 Long Short-Term Memory Networks,0,[0]
"Attentive neural networks have been shown to perform well on a variety of NLP tasks (Yang et al., 2016; Yin et al., 2015; Xu et al., 2015).",3.2 Long Short-Term Memory Networks,0,[0]
"Using attentionbased LSTM will accomplish two goals: (1) test whether they achieve higher performance than simple LSTM models and (2) use the attention weights produced by the LSTM models to perform a qualitative analysis to determine which portions of context triggers the sarcastic reply.
",3.2 Long Short-Term Memory Networks,0,[0]
Although Yang et al. (2016) have included two levels of attention mechanisms – one at the word level and another at the sentence level – we primarily focus on sentence level attention for two specific reasons.,3.2 Long Short-Term Memory Networks,0,[0]
"First, sentence level attentions can show the exact sentence in the context that is most informative to trigger sarcasm.",3.2 Long Short-Term Memory Networks,0,[0]
"In the discussion forum dataset, context posts are usually three or four sentences long and it could be helpful to identify the exact text that triggers the sarcastic reply.",3.2 Long Short-Term Memory Networks,0,[0]
"Second, attention over both the words and sentences seek to learn a large number of model parameters and given the moderate size of the discussion forum corpus they might overfit.",3.2 Long Short-Term Memory Networks,0,[0]
"For tweets, we treat each individual tweet as a sentence.",3.2 Long Short-Term Memory Networks,0,[0]
"The majority of tweets consist of a single sentence and even if there are multiple sentences in a tweet, often one sentence contains only hashtags, URLs, and emoticons making them uninformative if treated in isolation.
",3.2 Long Short-Term Memory Networks,0,[0]
Figure 1 shows the high-level structure of the model.,3.2 Long Short-Term Memory Networks,0,[0]
The context (left) is read by an LSTM (LSTMc) whereas the response (right) is read by another LSTM (LSTMr).,3.2 Long Short-Term Memory Networks,0,[0]
"We represent each sentence by the average of its word embeddings.
",3.2 Long Short-Term Memory Networks,0,[0]
Let the context c contain d sentences and each sentence sci contain Tci words.,3.2 Long Short-Term Memory Networks,0,[0]
"Similar to the notation of Yang et al. (2016), we first feed the sentence annotation hci through a one layer MLP to get uci as a hidden representation of hci , then we weight the sentence uci by measuring similarity with a sentence level context vector ucs .",3.2 Long Short-Term Memory Networks,0,[0]
This gives a normalized importance weight αci through a softmax function.,3.2 Long Short-Term Memory Networks,0,[0]
"vc is the vector that summarize all the information of sentences in the context
(LSTMc).
",3.2 Long Short-Term Memory Networks,0,[0]
"vc = ∑
i∈[1,d] αichic (1)
where attention is calculated as:
αic = exp(uTciucs)∑
i∈[1,d] exp(uTciucs) (2)
",3.2 Long Short-Term Memory Networks,0,[0]
Likewise we compute vr for the response r via LSTMr (similar to eq. 1 and 2; also shown in Figure 1).,3.2 Long Short-Term Memory Networks,0,[0]
"Finally, we concatenate the vector vc and vr from the two LSTMs for the final softmax decision (i.e., predicting the S or NS class).
",3.2 Long Short-Term Memory Networks,0,[0]
We also experiment with both word and sentence level attentions in a hierarchical fashion similarly to the approach proposed by Yang et al. (2016).,3.2 Long Short-Term Memory Networks,0,[0]
"As we show in Section 4 however, we achieve best performance for both datasets using just the sentence-level attention.
",3.2 Long Short-Term Memory Networks,0,[0]
Conditional LSTM Networks:,3.2 Long Short-Term Memory Networks,0,[0]
We also experiment with the conditional encoding model as introduced by Rocktäschel et al. (2015) for the task of recognizing textual entailment.,3.2 Long Short-Term Memory Networks,0,[0]
"In this architecture, two separate LSTMs are used – LSTMc and LSTMr – similar to the previous architecture without any attention, but for LSTMr, its memory state is initialized with the last cell state of LSTMc.",3.2 Long Short-Term Memory Networks,0,[0]
"In other words, LSTMr is conditioned on the representation of LSTMc that is built on the context.
",3.2 Long Short-Term Memory Networks,0,[0]
Parameters and pre-trained word vectors.,3.2 Long Short-Term Memory Networks,0,[0]
"For both discussion forum and Twitter, we split randomly the corpus into training (80%), development (10%), and test (10%), maintaining the same distribution of sarcastic vs. non-sarcastic data in training, development and test.",3.2 Long Short-Term Memory Networks,0,[0]
"For Twitter we used the skip-gram word-embeddings (100-dimension) used in (Ghosh et al., 2015) that was built using over 2.5 million tweets.4 For discussion forums, we use the standard Google n-gramword2vec pre-trained model (300- dimension) (Mikolov et al., 2013).",3.2 Long Short-Term Memory Networks,0,[0]
We do not optimize the word embedding during training.,3.2 Long Short-Term Memory Networks,0,[0]
"Out-ofvocabulary words in the training set are randomly initialized via sampling values uniformly from (- 0.05,0.05).",3.2 Long Short-Term Memory Networks,0,[0]
"We use the development data to tune the parameters and selected dropout rate of 0.5 (from [.25,0.5, 0.75]), L2 regularization strength and evaluate only that configuration on the test set.",3.2 Long Short-Term Memory Networks,0,[0]
For both datasets mini-batch size of 16 is employed.,3.2 Long Short-Term Memory Networks,0,[0]
"We report Precision (P), Recall (R), and F1 scores on S and NS classes.",4 Results and Discussion,0,[0]
SVMrbl and SVM c+r bl respectively represent the performance of the SVM model using discrete features when using only the reply and the reply together with context.,4 Results and Discussion,0,[0]
"LSTMca and LSTMra are the attention-based LSTM models of context and reply, where the w, s and w + s subscripts denote the word-level, sentence-level or word and sentence level attentions.",4 Results and Discussion,0,[0]
"LSTMconditional is the conditional encoding model (no attention).
",4 Results and Discussion,0,[0]
Discussion Forums: Table 2 shows the classification results on the discussion forum dataset.,4 Results and Discussion,0,[0]
"Although a vast majority of the context posts contain 3-4 sentences, around 100 context posts have more than ten sentences and thus we set a cutoff to a maximum of ten sentences for context modeling.",4 Results and Discussion,0,[0]
"For the reply r we considered the entire reply.
",4 Results and Discussion,0,[0]
"The SVMbl models that are based on discrete features did not perform very well, and adding context actually hurt the performance.",4 Results and Discussion,0,[0]
"Regarding the performance of the neural network models, we observe that modeling context improves the performance using all types of LSTM architectures that read both context (c) and reply (r) (results are statistically significant when compared
4https://github.com/debanjanghosh/sarcasm wsd
to LSTMr).",4 Results and Discussion,0,[0]
"The highest performance when considering both the S and NS classes is achieved by the LSTMconditional model (73.32% F1 for S class and 70.56% F1 for NS, showing a 6% and 3% improvement over LSTMr for S and NS classes, respectively).",4 Results and Discussion,0,[0]
The LSTM model with sentence-level attentions on both context and reply (LSTMcas +LSTMras ) gives the best F1 score of 73.7% for the S class.,4 Results and Discussion,0,[0]
"For the NS class, while we notice an improvement in precision we notice a drop in recall when compared to the LSTM model with sentence level attention only on reply (LSTMras ).",4 Results and Discussion,0,[0]
Remember that sentence-level attentions are based on average word embeddings.,4 Results and Discussion,0,[0]
We also experimented with the hierarchical attention model where each sentence is represented by a weighted average of its word embeddings.,4 Results and Discussion,0,[0]
"In this case, attentions are based on words and sentences and we follow the architecture of hierarchical attention network (Yang et al., 2016).",4 Results and Discussion,0,[0]
"We observe the performance (69.88% F1 for S category) deteriorates, probably due to the lack of enough training data.",4 Results and Discussion,0,[0]
"Since attention over both the words and sentences seek to learn a lot more model parameters, adding more training data will be helpful.",4 Results and Discussion,0,[0]
"With the full release of the Sarcasm Corpus used by Oraby et al. (2016), we expect to achieve better accuracy for these models.
",4 Results and Discussion,0,[0]
Twitter:,4 Results and Discussion,0,[0]
Table 3 shows the results on the Twitter dataset.,4 Results and Discussion,0,[0]
"As for discussion forums, adding context using the SVM models does not show a statistically significant improvement.",4 Results and Discussion,0,[0]
"For the neural networks model, similar to the results on discussion forums, the LSTM models that read both context and reply outperform the LSTM model that reads only the reply (LSTMr).",4 Results and Discussion,0,[0]
The best performing architectures are again the LSTMconditional and LSTM with sentence-level attentions (LSTMcas +LSTMras ).,4 Results and Discussion,0,[0]
"LSTMconditional model shows an improvement of 11% F1 on the S class and 4-5%F1 on the NS class, compared to LSTMr.",4 Results and Discussion,0,[0]
"For the attentionbased models, the improvement using context is smaller (∼2% F1).",4 Results and Discussion,0,[0]
"We kept the maximum length of context to the last five tweets in the conversation context, when available.",4 Results and Discussion,0,[0]
"We also conducted experiments with only word-level attentions, however, we obtain lower accuracy in comparison to sentence level attention models.",4 Results and Discussion,0,[0]
Wallace et al. (2014) showed that by providing contextual information humans are able to identify sarcastic utterances which they were unable without the context.,4.1 Qualitative Analysis,0,[0]
"However, it will be useful to understand whether a specific part of the context triggers the sarcastic reply.
",4.1 Qualitative Analysis,0,[0]
"To begin to address this issue, we conducted a qualitative study to understand whether (a) human annotators are able to identify parts of context that trigger the sarcastic reply and (b) attention weights are able to signal similar information.",4.1 Qualitative Analysis,0,[0]
For (a) we designed a crowdsourcing experiment and for (b) we looked at the attention weights of the LSTM networks.,4.1 Qualitative Analysis,0,[0]
Below is a short description of the crowdsourcing task.,4.1 Qualitative Analysis,0,[0]
"We designed an Amazon Mechanical Turk task (for brevity, MTurk) framed as follow:",4.1.1 Crowdsourcing Experiment.,0,[0]
"Given a pair of context c and a sarcastic reply r from the discussion forum dataset, identify one or more sentences in c that may trigger the sarcastic reply r. Turkers could select one or more sentences
from the context c, including the entire context.",4.1.1 Crowdsourcing Experiment.,0,[0]
"From the test data, we select examples with context length between three to seven sentences since for longer posts the task will be too complicated for the Turkers.
",4.1.1 Crowdsourcing Experiment.,0,[0]
We provided a definition of sarcasm and a few examples to the Turkers.,4.1.1 Crowdsourcing Experiment.,0,[0]
We also explained how to carry out the task with the help of a few context/reply pairs.,4.1.1 Crowdsourcing Experiment.,0,[0]
Each HIT contains only one task and five Turkers were allowed to attempt each HIT (a total of 85 HITS).,4.1.1 Crowdsourcing Experiment.,0,[0]
"Turkers with reasonable quality (i.e., more than 95% of acceptance rate with experience of over 8,000 HITs) were selected and paid seven cents per task.",4.1.1 Crowdsourcing Experiment.,0,[0]
We visualize and compare the sentence-level attention weights of the LSTM models on context with Turkers’ annotations (Figure 2).,4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
We first measure the overlap of Turkers choice with the attention weights.,4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
"For the sentence-based attention model (i.e., LSTMcas +LSTMras model for the discussion forum), we selected the sentence
with highest attention weight and matched it to the sentence selected by Turkers using majority voting.",4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
We found that 41% of times the sentence with the highest attention weight is also the one picked by Turkers.,4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
"Figure 2 shows side by side the heat maps of the attention weights of LSTM models (LHS) and Turkers’ choices when picking up sentences from context that they thought triggered the sarcastic reply (RHS).
",4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
Here the obvious question that we need to answer is why these sentences are selected by the models (and humans).,4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
In the next section we conduct a qualitative analysis to try answering this question.,4.1.2 Comparing Turkers’ answers with attention models.,0,[0]
Semantic coherence between context and reply.,4.1.3 Interpretation of selected context via attention weights,0,[0]
Figure 2(a) depicts a case where the context contains three sentences and the attention weights given to the sentences are similar to the Turkers’ choice.,4.1.3 Interpretation of selected context via attention weights,0,[0]
Looking at this example it seems the model pays attention to output vectors that are semantically coherent between c and r.,4.1.3 Interpretation of selected context via attention weights,0,[0]
The sarcastic response of this example contains a single sentence – “. . .,4.1.3 Interpretation of selected context via attention weights,0,[0]
hold your tongue . . .,4.1.3 Interpretation of selected context via attention weights,0,[0]
in support of an anti-gay argument”.,4.1.3 Interpretation of selected context via attention weights,0,[0]
The context contains the sentence S3 “. . .,4.1.3 Interpretation of selected context via attention weights,0,[0]
I’ve held my tongue on this as long as I can”.,4.1.3 Interpretation of selected context via attention weights,0,[0]
The attention-based LSTM architecture is learning the attention weights simultaneously for the context c and the response r.,4.1.3 Interpretation of selected context via attention weights,0,[0]
Thus the model is showing contextual understanding by setting high weights to semantically coherent parts of the c and r.,4.1.3 Interpretation of selected context via attention weights,0,[0]
"In Figure 2(b), attention weights is given to the most informative sentence –“rationally explain these creatures existence so recently in our human history if they were extinct for millions of years?”.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Here, the sarcastic reply mocks by claiming the author of the context is reading a lot more religious script (“ you’re reading waaaaay too much into your precious bible”).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"We also observe similar behavior in Tweets (highest attention to words –retain and gerrymadering in context: “breaking: republicans retain majority control of house” and reply: “hooray for gerrymandering” (Figure 3).
",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Incongruity between context and reply The meaning incongruity is an inherent characteristic of irony and sarcasm and have been extensively studied in linguistics, philosophy, communication science (Grice et al., 1975; Attardo, 2000; Burgers et al., 2012) as well as recently in NLP (Riloff
et al., 2013; Joshi et al., 2015).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"For instance, Riloff et al. (2013) pointed out that identifying the incongruity between positive sentiment towards a negative situation is a key characteristic of sarcasm detection in social media.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"We observe in discussion forums and in Tweets that the attention-based models have frequently identified sentences and words from c and r that are semantically incongruous (i.e., opposite sentiment words).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"For instance, in Figure 2(c), the attention model has chosen sentence S1, which contains strong negative sentiment word (“disgusting sickening . . . ”).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Interestingly, in contrast, the attention model on the reply, has given the highest weight to sentence that contain opposite sentiment (“I love you”).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Thus, the model seems to learn the context incongruity of opposite sentiment for detecting sarcasm.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"However, it seems the Turkers prefer the second sentence S2 (“how can you tell a man that about his mum?”) as the most instructive sentence instead of the first sentence.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Looking at the sarcastic reply we observe that the reply contains remarks about “mothers” and apparently that commonality assisted the Turkers to chose the second sentence.
",4.1.3 Interpretation of selected context via attention weights,0,[0]
"In Twitter dataset, we observe often the attention models have selected utterance(s) from the context which have opposite sentiment (Figure 4, Figure 5, and Figure 6).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Here, the word and sentence-level attention model have chosen the particular utterance from the context (i.e., the top heatmap for the context) and the words with high attention (e.g., “mediocre”, “gutsy”).These words again show examples of meaning incongruity which is useful for sarcasm detection.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Word-models seem to also work well when words in the context/reply are semantically incongruous but connected via deeper semantics (“bums” and “welfare” in context: “someone needs to remind these bums they work for the people” and reply: “feels like we are paying them welfare” (Figure 6).
",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Attention weights and sarcasm markers Looking just at attention weights in reply, we notice the models are giving highest weight to sentences that contain sarcasm markers, such as emoticons (i.e., “:p”, “:)”) and interjections (i.e., “ah”, “hmm”).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Sarcasm markers are explicit indicators of sarcasm that signal that an utterance is sarcastic, such as the use of emoticons, uppercase spelling of words, or interjections.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"(Attardo, 2000; Burgers et al., 2012).",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Use of such markers in
social media (particularly in Twitter) is extensive.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"While we have started to understand the semantic of attention weights in this task, more studies need to be carry out.",4.1.3 Interpretation of selected context via attention weights,0,[0]
Rocktäschel et al. (2015) have argued that interpretations based on attentions weights have to be taken with care since the classification task is not forced to solely rely on the attentions weights.,4.1.3 Interpretation of selected context via attention weights,0,[0]
"Thus in future work, we plan to analyze utterances that are more subtle and do not consist of sarcasm markers or explicit incongruence of opposite sentiment between context and response.",4.1.3 Interpretation of selected context via attention weights,0,[0]
"Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; González-Ibáñez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016).",5 Related Work,0,[0]
"However, even humans have difficulty sometimes in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014).",5 Related Work,0,[0]
"Thus, recent work on sarcasm and irony detection have started to exploit contextual information.",5 Related Work,0,[0]
"In par-
ticular, (Khattri et al., 2015) analyzed authors’ prior sentiment towards certain entities and if a new tweet deviates from the author’s estimated sentiment the tweet is predicted to be sarcastic.",5 Related Work,0,[0]
"Similar to this approach, several models have been introduced; some relied on extensive feature engineering to capture contextual information about authors, topics or conversation context whereas the rest are using deep learning techniques to embed authors’ information (Rajadesingan et al., 2015).",5 Related Work,0,[0]
"The two studies that have considered conversation context among other contextual information have shown minimal improvement when modeling conversation context using Twitter data (Bamman and Smith, 2015; Wang et al., 2015).",5 Related Work,0,[0]
"Our work show that using better models, such as LSTM networks show a clear benefit of using context for sarcasm detection.",5 Related Work,0,[0]
"As stated earlier in Section 3, LSTM’s have been shown to be effective in NLI tasks, especially where the task is to establish the relationship between multiple inputs (i.e., in our case, between the context and the response).",5 Related Work,0,[0]
We observe that the LSTMconditional model and the sentence level attention-based models using both context and reply present the best results.,5 Related Work,0,[0]
"This research makes a complementary contribution to existing work of modeling context for sarcasm/irony detection by looking at a particular type of context, conversation context.",6 Conclusion,0,[0]
We have addressed two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we determine what part of the conversation context triggered the sarcastic reply.,6 Conclusion,0,[0]
"To answer the first question, we show that Long Short-Term Memory (LSTM) networks that can model both the context and the sarcastic reply achieve better performance than LSTM networks that read only the reply.",6 Conclusion,0,[0]
"In particular, conditional LSTM networks (Rocktäschel et al., 2015) and LSTM networks with sentence level attention achieved significant improvement (e.g., 6-11% F1 for discussion forums and Twitter messages).",6 Conclusion,0,[0]
"To address the second issue, we presented a qualitative analysis of attention weights produced by the LSTM models with attention, and discussed the results compared with human annotators.",6 Conclusion,0,[0]
"We also showed that attention-based models are able to identify inherent characteristics of sarcasm (i.e., sarcasm markers and sarcasm factors such as context in-
congruity).",6 Conclusion,0,[0]
"In future, we plan to study larger context, such as the full thread in a discussion forum that consider also the responses to the sarcastic comment, when available.",6 Conclusion,0,[0]
"We are also interested in analyzing sarcastic replies that do not contain sarcasm markers or explicit incongruence (i.e., opposing sentiment between the context and the reply).",6 Conclusion,0,[0]
This paper is based on work supported by the DARPA-DEFT program.,Acknowledgements,0,[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,Acknowledgements,0,[0]
The authors thank Christopher Hidey for the discussions and resources on LSTM and the anonymous reviewers for helpful comments.,Acknowledgements,0,[0]
Computational models for sarcasm detection have often relied on the content of utterances in isolation.,abstractText,0,[0]
"However, speaker’s sarcastic intent is not always obvious without additional context.",abstractText,0,[0]
"Focusing on social media discussions, we investigate two issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply.",abstractText,0,[0]
"To address the first issue, we investigate several types of Long Short-Term Memory (LSTM) networks that can model both the conversation context and the sarcastic response.1",abstractText,0,[0]
"We show that the conditional LSTM network (Rocktäschel et al., 2015) and LSTM networks with sentence level attention on context and response outperform the LSTM model that reads only the response.",abstractText,0,[0]
"To address the second issue, we present a qualitative analysis of attention weights produced by the LSTM models with attention and discuss the results compared with human performance on the task.",abstractText,0,[0]
The Role of Conversation Context for Sarcasm Detection in Online Interactions,title,0,[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 178–183 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2028",text,0,[0]
"Infants start learning their native language even before birth and, already during their first year of life, they succeed in acquiring linguistic structure at several levels, including phonetic and lexical knowledge.",1 Introduction,0,[0]
"One extraordinary aspect of the learning process is infants’ ability to segment continuous speech into words, while having little or no knowledge of the sounds of their native language.
",1 Introduction,0,[0]
Several hypotheses have been proposed in the experimental literature to explain how they achieve this feat.,1 Introduction,0,[0]
"Among the main classes of cues put forward, prosodic cues (e.g. stress, prosodic boundaries) have been shown to be particularly useful in early-stage word segmentation (Christophe et al., 2003; Curtin et al., 2005; Seidl and Johnson, 2006).",1 Introduction,0,[0]
"Previous work suggests that
these cues may be emphasized in the speech register often used when addressing infants (infantdirected speech; IDS).",1 Introduction,0,[0]
"This register is characterized by shorter utterances, repeated words and exaggerated prosody (see (Cristia, 2013) for a review).",1 Introduction,0,[0]
"It has been shown that IDS can facilitate segmentation performance in infants (Thiessen et al., 2005), when compared to the register that parents use when talking to adults (adult-directed speech; ADS).
",1 Introduction,0,[0]
"The process of word segmentation has received considerable attention also from the computational linguistics community, where various computational models have been proposed (e.g. (Brent and Cartwright, 1996; Goldwater et al., 2009)).",1 Introduction,0,[0]
"Yet, despite the role that prosodic cues play in early word segmentation, only lexical stress has been addressed in detail, in the computational modelling literature (e.g. (Börschinger and Johnson, 2014; Doyle and Levy, 2013; Lignos, 2011)).",1 Introduction,0,[0]
"As for prosodic boundary information, it was investigated in only one previous study (Ludusan et al., 2015).",1 Introduction,0,[0]
"That study found that that an Adaptor Grammar model (Johnson et al., 2007) performed better on both English and Japanese corpora when prosodic boundary information was added to its grammar.",1 Introduction,0,[0]
"These previous studies investigated the effect of prosodic cues while keeping register constant, investigating either IDS (e.g. (Börschinger and Johnson, 2014)) or ADS (Ludusan et al., 2015).",1 Introduction,0,[0]
Other work focuses on register only.,1 Introduction,0,[0]
"For instance, (Fourtassi et al., 2013) used the Adaptor Grammar framework to examine English and Japanese corpora of infant- and adult-directed speech, concluding that IDS was easier to segment
178
than ADS.",1 Introduction,0,[0]
"However, the corpora were not parallel or necessarily directly comparable, as, the ADS in Japanese was transcribed from academic presentation speeches, whereas the IDS came from spontaneous conversational speech.
",1 Introduction,0,[0]
"We aim to put together these two lines of research, by conducting the first computational study of word segmentation that takes into account both variables: speech register and prosodic boundary information.",1 Introduction,0,[0]
"This investigation extends the previously mentioned studies, by allowing us to observe not only the effect of each individual variable, but also any interaction between the two.",1 Introduction,0,[0]
"More importantly, it is performed in a more controlled manner as it makes use of a large corpus of spontaneous verbal interactions, containing both IDS and ADS uttered by the same speakers.",1 Introduction,0,[0]
"Furthermore, we do not limit ourselves to a specific model, but test several, different, unsupervised segmentation models in order to increase the generalizability of the findings.",1 Introduction,0,[0]
Several unsupervised segmentation algorithms were employed.,2 Methods,0,[0]
"We selected 2 sub-lexical and 2 lexical models, all of which are made freely available through the CDSwordSeg package1.
",2 Methods,0,[0]
The first model performs transition-probabilitybased segmentation (TP) employing the relative algorithm of Saksida et al. (2016).,2 Methods,0,[0]
"It takes in input transcribed utterances, segmented at the syllable level and computes the forward transitional probabilities between every pair of syllables in the corpus.",2 Methods,0,[0]
"The transition probability between two syllables X and Y is defined as the frequency of the pair (X,Y) divided by the frequency of the syllable X. Once probabilities are computed, word boundaries are posited using local minima of the probability function.",2 Methods,0,[0]
"As this algorithm only attempts to posit boundaries based on phonological information it is called a ‘sub-lexical’ model.
",2 Methods,0,[0]
"Diphone-based segmentation (DiBS) is another sub-lexical model, which uses diphones instead of syllables pairs (Daland and Pierrehumbert, 2011).",2 Methods,0,[0]
The input is represented as a sequence of phonemes and the model tries to place boundaries based on the identity of each consecutive sequence of two phonemes.,2 Methods,0,[0]
"The goal is accomplished by computing the probability of a word boundary falling within such a sequence, with the
1https://github.com/alecristia/CDSwordSeg
probability being rewritten using Bayes’ rule.",2 Methods,0,[0]
"The information needed for the computation of the word boundary probability is estimated on a small subset of the corpus, using the gold word boundaries.",2 Methods,0,[0]
"Thereafter, a boundary is placed between every diphone whose probability is above a predetermined threshold.
",2 Methods,0,[0]
Monaghan and Christiansen (2010)’s PUDDLE is a lexical model which utilizes previously seen utterances to extract lexical and phonotactic information knowledge later used to “chunk” sequences.,2 Methods,0,[0]
"In a nutshell, it is an incremental algorithm that initially memorizes whole utterances into its long-term lexical storage, from which possible word-final and word-initial diphones are extracted.",2 Methods,0,[0]
"The model continues to consider each utterance as a lexical unit, unless sub-sequences of the given utterance have already been stored in the word list.",2 Methods,0,[0]
"In that case, it cuts the utterance based on the words which it already knows and considers the newly segmented chunks as word candidates.",2 Methods,0,[0]
"In order for the word candidates to be added to the lexical list, they have to respect two rules: 1) the final diphones of the left chunk and the beginning diphones of the right chunk must be on the list of permissible final diphones; and 2) both chunks have to contain at least one vowel.",2 Methods,0,[0]
"Once a candidate is added to the lexical list, its beginning and final diphones are included into the list of permissible diphones.
",2 Methods,0,[0]
"The last model was a unigram implementation of Adaptor Grammar (AG) (Johnson et al., 2007).",2 Methods,0,[0]
AG is a hierarchical Bayesian model based on an extension of probabilistic context free grammars.,2 Methods,0,[0]
"It alternates between using the previously learned grammar to parse an utterance into a hierarchical tree structure made up of words and phonemes, and updating the grammar by learning probabilities associated to rules and entire tree fragments, called adapted non-terminals.",2 Methods,0,[0]
"The unigram model is the simplest grammar, considering utterances as being composed of words, which are represented as a sequence of phonemes.",2 Methods,0,[0]
"The RIKEN corpus (Mazuka et al., 2006) contains recordings of 22 Japanese mothers interacting with their 18 to 24-month old infants, while playing with toys or reading a book.",3 Materials,0,[0]
The same mothers were then recorded while talking to an experimenter.,3 Materials,0,[0]
"Out of the total 14.5 hours of recordings,
about 11 hours represent infant-directed speech, while the rest adult-directed speech.
",3 Materials,0,[0]
The corpus was annotated at both segmental and prosodic levels.,3 Materials,0,[0]
"We made use in this study of the prosodic boundary annotation, labelled using the X-JToBI standard (Maekawa et al., 2002).",3 Materials,0,[0]
"XJToBI defines prosodic breaks based on the degree of their perceived disjuncture, ranging from level 0 (the weakest) to level 3 (the strongest).",3 Materials,0,[0]
"We use here level 2 and level 3 prosodic breaks, which in the Japanese prosodic organization (Venditti, 2005) correspond, respectively, to accentual phrases and intonational phrases.",3 Materials,0,[0]
"Accentual phrases are sequences of words that carry at most one pitch accent; for instance, a noun with a postposition will typically only have one accent.",3 Materials,0,[0]
"Intonational phrases are made up of sequences of accentual phrases, and constitute the domain where pitch range is defined such that, for instance, the onset of an intonational phrase will be marked by a reset the pitch level.
",3 Materials,0,[0]
"An additional dataset, part of the Corpus of Spontaneous Japanese (CSJ) (Maekawa, 2003), was considered as control.",3 Materials,0,[0]
"It contains academic speech and was previously used to investigate either the effect of speech register (Fourtassi et al., 2013) or that of prosodic boundaries (Ludusan et al., 2015) on unsupervised word segmentation.",3 Materials,0,[0]
The same levels of annotations are available as for the RIKEN corpus.,3 Materials,1,['The main results of this paper are informally summarized as follows.']
"Statistics about the number of utterances and word token and types, for all three corpora, can be found in Table 1.",3 Materials,0,[0]
"The transitional probabilities used by TP were computed on the entire input dataset, while the estimation of the probabilities needed by DiBS was performed on the first 200 utterances of the corpus.",4 Experimental settings,0,[0]
"PUDDLE, being an incremental algorithm, was evaluated using a five-fold cross-validation.",4 Experimental settings,0,[0]
"For AG, the process was repeated five times for each register and prosodic boundary condition, and the average across the five runs was reported.
",4 Experimental settings,0,[0]
"Each run had 2000 iterations and Minimum Bayes Risk (Johnson and Goldwater, 2009) decoding was used for the evaluation.
",4 Experimental settings,0,[0]
"Each algorithm was run on the ADS, IDS and CSJ datasets for each of the 3 cases considered: no prosody (base), level 3 prosodic breaks (brk3) and level 2 and level 3 prosodic breaks (brk23).",4 Experimental settings,0,[0]
"For the base case, the system had in input a file containing on each line an utterance, defined as being an intonational phrase or a filler phrase followed by a pause longer than 200 ms.",4 Experimental settings,0,[0]
"In the brk3 and brk23 cases, each prosodic phrase was considered as a standalone utterance, and thus was written on a separate line.",4 Experimental settings,0,[0]
"During the evaluation of the brk3 and brk23 cases, the original utterances were rebuilt by concatenating all the prosodic phrases contained in them, after which they were compared against the reference.
",4 Experimental settings,0,[0]
"Additionally, we checked whether the size difference between the ADS and IDS datasets might have an effect on the results obtained.",4 Experimental settings,0,[0]
"For this, we created two additional, balanced, subsets of the IDS data.",4 Experimental settings,0,[0]
"The first one contained an equal number of words from each speaker as in their ADS data, while the second one an equal number of utterances, for each speaker, as in their ADS production.",4 Experimental settings,0,[0]
"As there was no significant difference between the results with the two balanced subsets and the entire IDS corpus, we will present here only the latter results.",4 Experimental settings,0,[0]
"The segmentation evaluation was performed against the gold word segmentation, provided with the corpus.",5 Results and discussion,0,[0]
"A classical metric, the token F-score, was used as evaluation measure.",5 Results and discussion,0,[0]
"It is defined as the harmonic average between the token precision (how many word tokens, out of the total number of segmented words, were correct) and token recall (how many word tokens, out of the total number of words in the reference data, were found).
",5 Results and discussion,0,[0]
"Next, we illustrate the obtained token F-score for the three corpora (IDS, ADS and CSJ) in Figure 1, for the three cases (base, brk3 and brk23) and for the four algorithms investigated (TP, DiBS, PUDDLE and AG).",5 Results and discussion,0,[0]
We observe that the largest differences are between algorithms.,5 Results and discussion,0,[0]
It appears that models employing sub-lexical information fare worse than the ones working at the lexical level.,5 Results and discussion,0,[0]
"DiBS gives the lowest performance (.132 token Fscore for CSJ base), followed by TP, PUDDLE
and AG giving the best performance (.567 token F-score for ADS brk23).",5 Results and discussion,0,[0]
"The goal of the present study, however, is not to pit algorithms against each other, but rather to sample from plausible segmentation strategies that infants could potentially use so as to provide more representative and generalizable results.
",5 Results and discussion,0,[0]
"Register effects found in the comparison between IDS and CSJ with the AG model replicate previous work (Fourtassi et al., 2013).",5 Results and discussion,0,[0]
"We considerably extend knowledge by additionally including a casual ADS sample matched to the IDS, and investigating 3 additional algorithms.",5 Results and discussion,0,[0]
This allows us to conclude that differences between IDS and ADS are considerably smaller than previous work could have suggested.,5 Results and discussion,0,[0]
"This is expected in view of previous reports that using un-matched materials leads to an overestimation of the differences between IDS and ADS (Batchelder, 2002).",5 Results and discussion,0,[0]
"Interestingly, we also found that the size and direction of this difference was dependent on the algorithm used.",5 Results and discussion,0,[0]
"An important advantage can be observed in the IDS-ADS comparison for the sub-lexical algorithms (maximally 9% for TP and 10.3% for DiBS), which decreases for PUDDLE and AG (maximally 1-1.1%), and can sometimes reverse when prosodic information is taken into account (DiBS brk23, AG brk3 and brk23).
",5 Results and discussion,0,[0]
"Turning to prosodic boundaries, breaking utterances using internal prosodic breaks seems to help to a different degree the two classes of segmentation models and the three corpora, in ways that re-
semble a crossed interaction.",5 Results and discussion,0,[0]
"The performance of sub-lexical models improves more with the use of prosodic information than that of lexical models, and this for all corpora.",5 Results and discussion,0,[0]
"By and large, performance is boosted by additional prosodic breaks more for CSJ and ADS than IDS.",5 Results and discussion,0,[0]
"This boost is, however, rather variable for PUDDLE, with apparent declines when, for instance, type 3 breaks are added for ADS.",5 Results and discussion,0,[0]
"These results only partially replicate those reported in (Ludusan et al., 2015).",5 Results and discussion,0,[0]
"Overall, the improvement brought by prosodic boundaries is smaller.",5 Results and discussion,0,[0]
"TP brk23 brings an absolute improvement of 17.3% over TP base, for CSJ, but the improvement brought for AG (3.6%) is modest compared to what was previously reported (12.3%).2
Overall, we observe that some of our conclusions are dependent on the actual corpus being used.",5 Results and discussion,0,[0]
"For this reason, we further analysed several measures which could play a role in the segmentation process.",5 Results and discussion,0,[0]
"The first one, the average number of words per utterance was highest for CSJ, followed by ADS and the lowest for IDS.",5 Results and discussion,0,[0]
"This would be expected taken into account the characteristics of IDS (Cristia, 2013).",5 Results and discussion,0,[0]
"It is important to note that the smallest difference with respect to utterance length
2These differences might stem from the model used (we used here a unigram model, while a colloc3-syll model was previously used) or from the way in which the prosodic information was integrated (at the input level, in the current study, compared to at the grammar level, before).",5 Results and discussion,0,[0]
"Indeed, a model that makes explicit in its grammar the prosodic boundaries and, thus, learns word boundaries jointly with prosodic boundaries could be more powerful.",5 Results and discussion,0,[0]
"These aspects will have to be investigated in a future study.
between the base and brk23 was obtained for IDS, the same register that seems to take advantage the least by the information on prosodic boundaries.
",5 Results and discussion,0,[0]
"Besides the length of the utterance, the length of the words plays an important role in the segmentation task.",5 Results and discussion,0,[0]
"Longer words would increase the possibility of having substrings which are words on their own, thus decreasing the segmentation performance.",5 Results and discussion,0,[0]
"As expected, CSJ has the highest average word length, but IDS was found to have a very similar word length, followed by ADS.",5 Results and discussion,0,[0]
The unexpected value obtained for IDS might be due to the high number of long onomatopoeia present in the corpus.,5 Results and discussion,0,[0]
"Thus, any IDS advantage due to having shorter utterances might be reversed by having longer words.",5 Results and discussion,0,[0]
"We computed also the average number of types per token, which can give information about the distribution of the words in the corpora.",5 Results and discussion,0,[0]
"In order not to have a measure biased by the size of the corpus, we computed it as a moving average over a window of 100 words.",5 Results and discussion,0,[0]
"It shows a slightly higher vocabulary diversity for CSJ and ADS, than IDS, suggesting a more difficult segmentation.
",5 Results and discussion,0,[0]
"The segmental ambiguity score (Fourtassi et al., 2013) measures the number of different parses of a sentence given the gold lexicon, by computing the average entropy in parses, taken into account the probability of each parse.",5 Results and discussion,0,[0]
Fourtassi and colleagues argue that this measure captures the intrinsic difficulty of the segmentation problem and predicts segmentation scores across languages (but see Phillips and Pearl (2014)).,5 Results and discussion,0,[0]
"Here, we found that segmentation ambiguity decreases with the use of prosodic information (by preventing segmentations that would straddle a prosodic break).",5 Results and discussion,0,[0]
"In
contrast, there is not much difference between registers; if anything, IDS is more ambiguous than the two adult corpora; we speculate that this may be due to the presence of many onomatopoeia in IDS (over 8% of the total word tokens) some of which contain a lot of reduplications, which would increase segmentation ambiguity.",5 Results and discussion,0,[0]
"This may explain why, when prosody equates sentence lengths, the advantage of IDS over ADS becomes small or even reverts to a detrimental effect.",5 Results and discussion,0,[0]
"We examined the performance of 4 different word segmentation algorithms on two matched corpora of spontaneous ADS and IDS, and a control corpus of more formal ADS, all of them with and without prosodic breaks.",6 Conclusions,0,[0]
"We found that, overall, sub-lexical algorithms perform less well than lexical algorithms, that IDS was overall slightly easier or equal to informal ADS, itself easier than formal ADS.",6 Conclusions,0,[0]
"In addition, across all algorithms and registers, we observed that prosody helped word segmentation.",6 Conclusions,0,[0]
"However, the impact of prosody was unequal and showed an interaction with register: It helped more ADS than IDS to the point that, with prosody taken into account, spontaneous ADS and IDS yield somewhat similar scores.
",6 Conclusions,0,[0]
"This has impact for theories of language acquisition, since IDS has been assumed to provide infants with ‘hyperspeech’, i.e. a simplified kind of input that facilitates language acquisition.",6 Conclusions,0,[0]
"If our observations are true, as far as word segmentation goes, it is not the case that IDS is massively easier to segment than ADS, at least at the stage when infants have acquired the ability to use prosodic breaks to constrain word segmentation.",6 Conclusions,0,[0]
"Of course, our observations would need to be confirmed and replicated with other languages and recording procedures.",6 Conclusions,0,[0]
"To conclude, our study illustrates the interest of testing theories of language acquisition using quantitative tools.",6 Conclusions,0,[0]
"BL, MB and ED’s research was funded by the European Research Council (ERC-2011-AdG295810 BOOTPHON), and AC by the Agence Nationale pour la Recherche (ANR-14-CE30-0003 MechELex).",Acknowledgments,0,[0]
"It was also supported by the Canon Foundation in Europe and the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL*).",Acknowledgments,0,[0]
This study explores the role of speech register and prosody for the task of word segmentation.,abstractText,0,[0]
"Since these two factors are thought to play an important role in early language acquisition, we aim to quantify their contribution for this task.",abstractText,0,[0]
"We study a Japanese corpus containing both infantand adult-directed speech and we apply four different word segmentation models, with and without knowledge of prosodic boundaries.",abstractText,0,[0]
The results showed that the difference between registers is smaller than previously reported and that prosodic boundary information helps more adultthan infant-directed speech.,abstractText,0,[0]
The Role of Prosody and Speech Register in Word Segmentation: A Computational Modelling Perspective,title,0,[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 42–52, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics
Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in reader comments in on-line news. To date, however, there has been little discussion of what these summaries should be like and a lack of humanauthored exemplars, quite likely because writing summaries of this kind of interchange is so difficult. In this paper we propose one type of reader comment summary – the conversation overview summary – that aims to capture the key argumentative content of a reader comment conversation. We describe a method we have developed to support humans in authoring conversation overview summaries and present a publicly available corpus – the first of its kind – of news articles plus comment sets, each multiply annotated, according to our method, with conversation overview summaries.",text,0,[0]
"In the past fifteen years there has been a tremendous growth in on-line news and, associated with it, the new social media phenomenon of on-line reader comments.",1 Introduction,0,[0]
"Virtually all major newspapers and news broadcasters now support a reader comment facility, which allows readers to participate in multi-party conversations in which they exchange views and opinion on issues in the news.
",1 Introduction,0,[0]
One problem with such conversations is that they can rapidly grow to hundreds or even thousands of comments.,1 Introduction,0,[0]
Few readers have the patience to wade through this much content.,1 Introduction,0,[0]
"One potential solution is to develop methods to summarize
comment automatically, allowing readers to gain an overview of the conversation.
",1 Introduction,0,[0]
In recent years researchers have begun to address the problem of summarising reader comment.,1 Introduction,0,[0]
"Broadly speaking, two main approaches to the problem have been pursued.",1 Introduction,0,[0]
"In the first approach, which might be described as technologydriven, researchers have proposed methods to automatically generate summaries of reader comment based on combining existing technologies (Khabiri et al., 2011; Ma et al., 2012; Llewellyn et al., 2014).",1 Introduction,0,[0]
"These authors adopt broadly similar approaches: first reader comments are topically clustered, then comments within clusters are ranked and finally one or more top-ranked comments are selected from each cluster, yielding an extractive summary.",1 Introduction,0,[0]
"A significant weakness of such summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation.
",1 Introduction,0,[0]
"In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015).",1 Introduction,0,[0]
If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment.,1 Introduction,0,[0]
"Indeed, several of these authors have cited summarization as a motivating application for their work.",1 Introduction,0,[0]
"To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set.
42
In our view, what has been lacking so far is a discussion of and proposed answer to the fundamental question of what a summary of reader comments should be like and human-generated exemplars of such summaries for real sets of reader comments.",1 Introduction,0,[0]
"A better idea of the target for summarisation and a resource exemplifying it would put the community in a better position to choose methods for summarisation of reader comment and to develop and evaluate their systems.
",1 Introduction,0,[0]
In this paper we make three principal contributions.,1 Introduction,0,[0]
"First, after a brief discussion of the nature of reader comment we make a proposal about one type of informative reader comment summary that we believe would have wide utility.",1 Introduction,0,[0]
"Second, we present a three stage method for manually creating reference summaries of the sort we propose.",1 Introduction,0,[0]
"This method is significant since the absence to date of human-authored reader comment summaries is no doubt due to the very serious challenge of producing them, something our method alleviates to no small degree.",1 Introduction,0,[0]
"Third, we report the construction and analysis of a corpus of human-authored reference summaries, built using our method – the first publicly available corpus of human-authored reader comment summaries.",1 Introduction,0,[0]
What should a summary of reader comment contain?,2 Summaries of Reader Comments,0,[0]
"As Spärck-Jones (2007) has observed, what a summary should contain is primarily dependent on the nature of the content to be summarised and
the use to which the summary is to be put.",2 Summaries of Reader Comments,0,[0]
In this section we first make a number of observations about the character of reader comments and offer a specification for a general informative summary.,2 Summaries of Reader Comments,0,[0]
"Figure 1 shows a fragment of a typical comment stream, taken from reader comment responses to a Guardian article announcing the decision by Bury town council to reduce bin collection to once every three weeks.",2.1 The Character of Reader Comments,0,[0]
"While not illustrating all aspects of reader comment interchanges, it serves as a good example of many of their core features.
",2.1 The Character of Reader Comments,0,[0]
Comment sets are typically organised into threads.,2.1 The Character of Reader Comments,0,[0]
Every comment is in exactly one thread and either initiates a new thread or replies to exactly one comment earlier in a thread.,2.1 The Character of Reader Comments,0,[0]
"This gives the conversations the formal character of a set of trees, with each thread-initial comment being the root node of a separate tree and all other comments being either intermediate or leaf nodes, whose parent is the comment to which they reply.",2.1 The Character of Reader Comments,0,[0]
"While threads may be topically cohesive, in practice they rarely are, with the same topic appearing in multiple threads and threads drifting from one topic onto another (see, e.g. comments 5 and 6 in Figure 1 both of which cite plague as a likely outcome of the new policy but are in different threads).
",2.1 The Character of Reader Comments,0,[0]
"Our view, based on an analysis of scores of comment sets, is that reader comments are primarily argumentative in nature, with readers making assertions that either (1) express a viewpoint (or
stance) on an issue raised in the original article or by an earlier commenter, or (2) provide evidence or grounds for believing a viewpoint or assertion already expressed.",2.1 The Character of Reader Comments,0,[0]
"Issues are questions on which multiple viewpoints are possible; e.g., the issue of whether reducing bin collection to once every three weeks is a good idea, or whether reducing bin collection will lead to an increase in vermin.",2.1 The Character of Reader Comments,0,[0]
"Issues are very often implicit, i.e not directly expressed in the comments (e.g., the issue of whether reducing bin collection will lead to an increase in vermin is never explicitly mentioned yet this is clearly what comments 1-4 are addressing).",2.1 The Character of Reader Comments,0,[0]
"A fuller account of this issue-based framework for analysing reader comment is given in Barker and Gaizauskas (2016).
",2.1 The Character of Reader Comments,0,[0]
"Aside from argumentative content, reader comments exhibit other features as well.",2.1 The Character of Reader Comments,0,[0]
"For example, commenters may seek clarification about facts (e.g. comment 4 where the commenter asks Is Bury going to provide larger bins for families . . . ?).",2.1 The Character of Reader Comments,0,[0]
"But these clarifications are typically carried out in the broader context of making an argument, i.e. advancing evidence to support a viewpoint.",2.1 The Character of Reader Comments,0,[0]
"Comments may also express jokes or emotion, though these too are often in the service of advancing some viewpoint (e.g. sarcasm or as in comments 4 and 6 emotive terms like lamebrained and crazy clearly indicating the commenters’ stances, as well as their emotional attitude).",2.1 The Character of Reader Comments,0,[0]
"Given the fundamentally argumentative nature of reader comments as sketched above, one type of summary of wide potential use is a generic informative summary that aims to provide an overview of the argument in the comments.",2.2 A Conversation Overview Summary,0,[0]
"Ideally, such a summary should:",2.2 A Conversation Overview Summary,0,[0]
the comments.,1. Identify and articulate the main issues in,0,[0]
Main issues are those receiving proportionally the most comments.,1. Identify and articulate the main issues in,0,[0]
They should be prioritized for inclusion in a spacelimited summary.,1. Identify and articulate the main issues in,0,[0]
"characterise opinion on an issue typically involves: identifying alternative viewpoints; indicating the grounds given to support viewpoints; aggregating – indicating how opinion was distributed across different issues, viewpoints and
grounds, using quantifiers or qualitative expressions e.g. “the majority discussed x”; indicating where there was consensus or agreement among the comment; indicating where there was disagreement among the comment.
",2. Characterise opinion on the main issues. To,0,[0]
"We presented this proposed summary type to a range of reader comment users, including comment readers, posters, journalists and news editors and received very positive feedback via a questionnaire1.",2. Characterise opinion on the main issues. To,0,[0]
"Based on this, we developed a set of guidelines to inform the process of summary authoring.",2. Characterise opinion on the main issues. To,0,[0]
"Whilst clear about what the general nature of the target summary should be, the guidelines avoid being too prescriptive, leaving authors some freedom to include what feels intuitively correct to include in the summary for any given conversation.",2. Characterise opinion on the main issues. To,0,[0]
"To help people write overview summaries of reader comments, we have developed a 4-stage method, which is described below2.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Summary writers are provided with an interface, which guides annotators through the 4-stage process, presenting texts in a form convenient for annotation, and collecting the annotations.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"The interface has been designed to be easily configurable for different languages, with versions for English, French and Italian already in issue.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Key details of the methodology, guidelines and example annotations follow.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Screenshots of the interfaces supporting stages 1 and 3 can be found in the Appendix.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Stage 1: Comment Labeling In this stage, annotators are shown an article in the interface, plus its comments (including the online name of the
1Further details on the summary specification and the end-user survey on it can be found in SENSEI deliverable D1.2 “Report on Use Case Design and User Requirements” at: http://www.sensei-conversation.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"eu/deliverables/.
2The method described here is not unlike the general method of thematic coding widely used in qualitative research, where a researcher manually assigns codes (either pre-specified and/or “discovered” as the coding process unfolds) to textual units, then groups the units by code and finally seeks to gain insights from the data so organised (Saldana, 2015).",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Our method differs in that: (1) our “codes” are propositional paraphrases of viewpoints expressed in comments rather than the broad thematic codes, commonly used in social science research, and (2) we aim to support an annotator in writing a summary that captures the main things people are saying as opposed to a researcher developing a thesis, though both rely on an understanding of the data that the coding and grouping process promotes.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"www.theguardian.com/commentisfree/2016/ apr/07/robots-replacing-jobs-ludditeseconomics-labor).
poster, and reply-to information).",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Annotators are asked to write a ‘label’ for each comment, which is a short, free text annotation, capturing its essential content.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"A label should record the main “points, arguments or propositions” expressed in a comment, in effect providing a mini-summary.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Two example labels are shown in Figure 2.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"We do not insist on a precise notation for labels, but we advise annotators to:
1.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"record when a comment agrees or disagrees with something/someone
2. note grounds given in support of a position 3.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"note jokes, strong feeling, emotional content 4.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"use common keywords/abbreviations to de-
scribe similar content in different comments 5. return regularly to review/revise previous la-
bels, when proceeding through the comments 6.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"make explicit any implicit content that is im-
portant to the meaning, e.g. “unemployment” in the second label of the figure (note: this process can yield labels that are longer than the original comment).
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"The label annotation process helps annotators to gain a good understanding of key content of the comments, whilst the labels themselves facilitate the grouping task of the next stage.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Stage 2: Label Grouping In stage 2, we ask annotators to sort through the Stage 1 labels, and to group together those which are similar or related.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Annotators then provide a “Group Label” to describe the common theme of the group in terms of e.g. topic, propositions, contradicting viewpoints, humour, etc.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
Annotators may also split the labels in a group into “Sub-Groups” and assign a “SubGroup Label”.,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"This exercise helps annotators to
make better sense of the broad content of the comments, before writing a summary.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"The annotation interface re-displays the labels created in Stage 1 in an edit window, so the annotator can cut/paste the labels (each with its comment id and poster name) into their groups, add Group Labels, and so on.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Here, annotators work mainly with the label text, but can refer to the source comment text (shown in context in the comment stream) if they so wish.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"When the annotator feels they have sorted and characterised the data sufficiently, they can proceed to stage 3.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
Stage 3:,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
Summary Generation Annotators write summaries based on their Label-Grouping analysis.,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"The interface (Figure 5) displays the Grouping annotation from Stage 2, alongside a text box where the summary is written in two phases.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Annotators first write an ‘unconstrained summary’, with no word-length requirement, and then (with the first summary still visible) write a ‘constrained-length summary’ of 150–250 words.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
Further analysis may take place as a person decides on what sentences to include in the summary.,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"For example, an annotator may:
• develop a group label, e.g. producing a polished or complete sentence; • carry out further abstraction over the groups, e.g. using a new high-level statement to summarise content from two separate groups; • exemplify, clarify or provide grounds for a summary sentence, using details from labels or comments within a group, etc.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
We encourage the use of phrases such as “many/ several/few comments said. . .,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"”, “opinion was divided on. . .",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"”, “the consensus was. . .",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"”, etc, to quantify the proportion of comments/posters addressing various topics/issues, and the strength/ polarisation of opinion/feeling on different issues.
",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Stage 4: Back-Linking In this stage, annotators link sentences of the constrained-length summary back to the groups (or sub-groups) that informed their creation.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
Such links imply that at least some of the labels in a group (or sub-group) played a part supporting the sentence.,3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"The interface displays the summary sentences alongside the Label Grouping from Stage 2, allowing the annotator to select a sentence and a group (or sub-group — the more specific correct option is preferred) to assert a link between them, until all links have been added.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"Note that while back-links are to groups
of labels, the labels have associated comment ids, so indirectly summary sentences are linked back to the source comments that support them.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
"This last stage goes beyond the summary creation process, but captures information valuable for system development and evaluation.",3 A Method for Human Authoring of Reader Comment Summaries,0,[0]
We recruited 15 annotators to carry out the summary writing task.,4.1 Annotators and training,0,[0]
"They included: final year journalism students, graduates with expertise in language and writing, and academics.",4.1 Annotators and training,0,[0]
The majority of annotators were native English speakers; all had excellent skills in written English.,4.1 Annotators and training,0,[0]
We provided a training session taking 1.5-2 hours for all annotators.,4.1 Annotators and training,0,[0]
This included an introduction to our guidelines for writing summaries.,4.1 Annotators and training,0,[0]
"From an initial collection of 3,362 Guardian news articles published in June-July 2014 and associated comment sets, we selected a small subset for use in the summary corpus.",4.2 Source Data,0,[0]
"Articles were drawn from the Guardian-designated topicdomains: politics, sport, health, environment, business, Scotland-news and science.",4.2 Source Data,0,[0]
Table 1 shows the summary statistics for the 18 selected sets of source texts (articles and comments).,4.2 Source Data,0,[0]
The average article length is 772 words.,4.2 Source Data,0,[0]
"The comment sets ranged in size from 100 to 1,076 comments.",4.2 Source Data,0,[0]
"For the annotation task, we selected a subset of each full comment set, by first ordering threads into chronological order (i.e. oldest first), and then selecting the first 100 comments.",4.2 Source Data,0,[0]
"If the thread containing the 100th comment had further comments, we continued including comments until the last comment in that thread.",4.2 Source Data,0,[0]
"This produced a collection of reduced comment sets totalling 87,559 words in 1,845 comments.",4.2 Source Data,0,[0]
"Reduced summary comment sets vary in length from 2,384 words to 8,663 words.",4.2 Source Data,0,[0]
"The SENSEI Social Media Corpus, comprising the full text of the original Guardian articles and reader comments as well as all annotations generated in the four stage summary writing method described in Section 3 above – comment labels, groups, summaries and backlinks – is freely available at: nlp.shef.ac.uk/sensei/.",5 Results and Analysis,0,[0]
"There were 18 articles and comment sets, of which 15 were double annotated and 3 were triple annotated, giving a total of 39 sets of complete annotations.",5.1 Overview of Corpus Annotations,0,[0]
"Annotators took 3.5-6 hours to complete the task for an article and comment set.
",5.1 Overview of Corpus Annotations,0,[0]
Table 2 shows a summary of corpus annotations counts.,5.1 Overview of Corpus Annotations,0,[0]
"The corpus includes 3,879 comment labels, an average of 99.46 per annotation set (av. 99.46/AS).",5.1 Overview of Corpus Annotations,0,[0]
"There are, in total, 329 group annotations (av. 8.44/AS) and 218 subgroups (av. 5.59/AS).",5.1 Overview of Corpus Annotations,0,[0]
Each of the 547 groups/subgroups has a short group label to characterise its content.,5.1 Overview of Corpus Annotations,0,[0]
"Such labels range from keywords (“midges”, “UK climate”, “fining directors”, “Air conditioning/fans”) to full propositions/questions (“Not fair that SE gets the investment”, “Why use the fine on wifi?”).",5.1 Overview of Corpus Annotations,0,[0]
"Each of the 39 annotation sets has two summaries, of which the unconstrained summaries have average length 321.41 words, and the constrained summaries, 237.74 (a 26% decrease).",5.1 Overview of Corpus Annotations,0,[0]
Each summary sentence is back-linked to one or more groups comment labels that informed it.,5.1 Overview of Corpus Annotations,0,[0]
Variation in Grouping There is considerable variation between annotators in use of the option to group/sub-group comment labels.,5.2 Observations,0,[0]
"Whilst the average of groups per annotation set was 9.0, for the annotator who grouped the least this was 4.0, and the maximum average 14.5.",5.2 Observations,0,[0]
"For sub-groups, the average per annotation set was 5.0.",5.2 Observations,0,[0]
"14 of 15 annotators used the sub-group option in at least one annotation set, and only 5 of the 39 sets included no sub-groups.",5.2 Observations,0,[0]
"A closer look shows a divide between annotators who use sub-groups quite frequently (7 having an average of ≥6.5/AS) and those who do not (with av. ≤2/AS).
",5.2 Observations,0,[0]
Other variations in annotator style include the fact that around a third of them did most of their grouping at the sub-group level (4 of the 6 who frequently used subgroups were amongst those having the lowest average number of groups).,5.2 Observations,0,[0]
"Also, whilst a fifth of annotators preferred to use mainly a single level of grouping (i.e. had a high average of groups, and a low average of sub-groups, per annotation set), another fifth of annotators liked to create both a high number of groups and of subgroups, i.e. used a more fine-grained analysis.
",5.2 Observations,0,[0]
"We also investigated whether the word-length of a comment set influenced the number of
groups/subgroups created by the annotators, but surprisingly, there was no obvious correlation.
",5.2 Observations,0,[0]
Reader Comment Summaries,5.2 Observations,0,[0]
"We carried out a preliminary qualitative analysis to establish the character of the summaries produced, which shows that they are in general all coherent and grammatical, and that the majority of summary sentences characterise views on issues.",5.2 Observations,0,[0]
"Some observations on summary content follow:
1.",5.2 Observations,0,[0]
All summaries contain sentences reporting different views on issues.,5.2 Observations,0,[0]
"Figure 2 shows two typical summaries, which describe a range of views on two main issues: “whether or not citizens can cope with reductions in bin collection” (Summary 1), and “whether or not new taxes on the rich should be introduced to pay for the NHS” (Summary 2).
2.",5.2 Observations,0,[0]
"Summaries frequently indicate points of contention or counter arguments, e.g. sentences (S2) and (S5) of Summary 2.
3.",5.2 Observations,0,[0]
"Summaries often provide examples of the reasons people gave in support of a viewpoint: e.g. (S2) of Summary 1 explains that people thought a reduced bin collection would attract vermin because the bins will overflow with rubbish.
4.",5.2 Observations,0,[0]
"Annotators often indicate the proportion/amount of comment addressing a particular topic/issue or supporting a particular viewpoint, e.g. see (S6) of Summary 2; (S3) of Summary 1. 5.",5.2 Observations,0,[0]
"While the majority of annotators abstracted across groups of comments to describe views on issues, there were a few outliers who did not.",5.2 Observations,0,[0]
"For example, for an article about a heatwave in the UK, the two annotators grouped the same 8 comments, but summarised the content very differently.",5.2 Observations,0,[0]
Annotator 1 generalised over the comments: “A small group of comments discussed how the heat brings about the nuisance of midges and how to deal with them”.,5.2 Observations,0,[0]
"Annotator 2 listed the points made in successive comments: “One person said how midges were a problem in this weather, another said they should shut the windows or get a screen.",5.2 Observations,0,[0]
One person told an anecdote about the use of a citronella candle . . .,5.2 Observations,0,[0]
another said they were surprised the candle worked as they had been severely bitten after using citronella oil”.,5.2 Observations,0,[0]
6,5.2 Observations,0,[0]
"Very few summary sentences describe a discussion topic without indicating views on it (e.g. “Many comments discuss the disposal of fat”).
",5.2 Observations,0,[0]
"Analysis revealed that summaries also include examples of: Background about, e.g., an event,
practice or person, to clarify an aspect of the debate, e.g. see (S5) of Summary 1, Humour; Feelings and Complaints, about e.g. commenters and reporters.",5.2 Observations,0,[0]
"We investigated the extent to which summaries of the same set of comments by different annotators have the same summary content, by performing a content comparison assessment on 10 randomly selected summary pairs, using a method similar to the manual evaluation method of DUC 2001 (Lin and Hovy, 2002).
",5.3 Similarity of Summary Content,0,[0]
"Given summaries A and B, for each sentence s in A, a subject judges the extent to which the meaning of s is evidenced (anywhere) in B, assigning a score on a 5-point scale (5=all meaning evidenced; 1=none is).",5.3 Similarity of Summary Content,0,[0]
"Any score above 1 requires evidence of common propositional content (i.e., a common entity reference alone would not suffice).",5.3 Similarity of Summary Content,0,[0]
"After A is compared to B, B is compared to A.
Comparison of the 10 random summary pairs required 300 sentence judgements, which were each done twice by two judges and averaged.",5.3 Similarity of Summary Content,0,[0]
"In these results, 17% of summary sentences received a score of 5 (indicating all meaning evidenced) and 40% a score between 3 and 4.5 (suggesting some or most of their meaning was evidenced).",5.3 Similarity of Summary Content,0,[0]
"Only 15% of sentences received a score of 1.
",5.3 Similarity of Summary Content,0,[0]
"Looking at the content overlap per individual summary pair (by averaging the sentence overlap
scores for that pair), we find values for the 10 pairs that range from 2.56 up to 3.65 (with overall average 3.06).",5.3 Similarity of Summary Content,0,[0]
"Scores may be affected by the length of comment sets (as longer sets give more scope for variation and complexity), and we observe that the two lowest scores are for long comment sets.
",5.3 Similarity of Summary Content,0,[0]
"We assessed the agreement between judges on this task, by comparing their scores for each sentence.",5.3 Similarity of Summary Content,0,[0]
"Scores differ by 0 in 46% of cases, and by 1 in 33%, giving a combined 79% with ‘near agreement’.",5.3 Similarity of Summary Content,0,[0]
Scores differ by >2 in only 6% of cases.,5.3 Similarity of Summary Content,0,[0]
These results suggest that average sentence similarity is a reliable measure of summary overlap.,5.3 Similarity of Summary Content,0,[0]
Creating abstractive reference summaries of extended dialogues is hard.,6 Related Work,0,[0]
"A more common approach involves humans assessing source units (e.g., comments in comment streams, turns in email exchanges) based on their perceived importance (aka “salience”) for inclusion in an end summary.",6 Related Work,0,[0]
"See, e.g., Khabiri et al.’s (2011) work on comments on YouTube videos; Murray and Carenini’s (2008) work on summarizing email discussions.",6 Related Work,0,[0]
"The result is a “gold standard” set of units, each with a value based on multiple human annotations.",6 Related Work,0,[0]
A system generated extractive summary is then scored against this gold standard.,6 Related Work,0,[0]
"The underlying assumption is that a good summary of length n is one that has a high score when compared against the top-ranked n gold standard units.
",6 Related Work,0,[0]
Such an approach is straightforward and provides useful feedback for extractive summarization systems.,6 Related Work,0,[0]
"While the gold standard is extractive, the selected content may have an abstractive flavour if annotators are instructed to favour “meta-level” source units that contain overview content.",6 Related Work,0,[0]
"But the comment domain has few obvious examples of meta-level sentences; explicit references to the issues under discussion are few, as are reflective comments that sum up a preceding series of comments.",6 Related Work,0,[0]
"Moreover, extractive approaches to writing comment summaries will almost certainly fall short of indicating aggregation over views and opinion.",6 Related Work,0,[0]
"In sum, this is not an ideal approach to creating reference summaries from comment.
",6 Related Work,0,[0]
"A more abstractive approach to writing summaries of multi-party conversations was used in the creation of the AMI corpus annotations, based on 100 hours of recorded meetings dialogues (Carletta et al., 2006).",6 Related Work,0,[0]
There are some similarities and differences between the AMI approach and our own.,6 Related Work,0,[0]
"First, AMI summary writers first completed a topic segmentation task to prepare them for the task of writing a summary.",6 Related Work,0,[0]
"While segmentation might appear to resemble our grouping stage, these are very different tasks.",6 Related Work,0,[0]
Key differences are that segmentation was carried on AMI dialogues using a pre-specified list of topic descriptions.,6 Related Work,0,[0]
"This would be difficult to provide for comment summary writers, since we cannot predict everything the comments will talk about.",6 Related Work,0,[0]
"Secondly, the AMI abstractive summaries are linked to dialogue acts (DAs) in their manual extractive summaries (a link is made if a DA is judged to “support” a sentence in the abstractive summary).",6 Related Work,0,[0]
"Similar to our back-links, their links provide indices from the abstractive summary to source text units.",6 Related Work,0,[0]
"However, our back-links are from a summary sentence to groups of comment labels that the summary author has judged to have informed his sentence.",6 Related Work,0,[0]
"Finally, the AMI abstractive summaries comprise an overview summary of the meeting, and list “decisions”, “problems/issues” and “actions”.",6 Related Work,0,[0]
"However, while a very small number of non-scenario corpus summaries included reports of alternative views in a meeting (e.g. on which film to choose for a film club), the AMI scenario summaries include very few examples of differences in opinion.
",6 Related Work,0,[0]
"Misra et al. (2015) have created manual summaries of short dialogue sequences, extracted from different conversations on similar issues on debat-
ing websites.",6 Related Work,0,[0]
"They then collected summaries together, and applied the Pyramid method (Nenkova et al., 2007) to identify common, central propositions, which, they describe as “abstract objects” that represent facets of an argument on an issue, e.g. gay marriage.",6 Related Work,0,[0]
Indeed the task of identifying central propositions across multiple conversations is a key aim in their work and one they point out is central to others working in argumentation mining.,6 Related Work,0,[0]
"They use the Pyramid annotations to provide indices from the central proposition to the summary and underlying comment, with a view to learning how to recognize similar argument facets automatically.",6 Related Work,0,[0]
"Note their task differs from ours in that we aim to generate a summary of a single reader comment conversation, while they aim to identify (and then possibly summarize) all facets of a single argument, gleaned from multiple distinct conversations.
",6 Related Work,0,[0]
Barker and Gaizauskas (2016) elaborate the issue-viewpoint-evidence framework introduced in Section 2.1 above and show how an argument graph representing an analysis in this framework may be created for a set of comments.,6 Related Work,0,[0]
"They show how the content in a single reference summary, created using the informal label and group method described above, corresponds closely to a subgraph in the more formally specified argument graph for the article and comment set.",6 Related Work,0,[0]
"We have presented a proposal for a form of informative summary that aims to capture the key content of multi-party, argument-oriented conversations, such as those found in reader comment.",7 Concluding Remarks and Future Work,0,[0]
"We have developed a method to help humans author such summaries, and used it to build a corpus of reader comment multiply annotated with summaries and other information.",7 Concluding Remarks and Future Work,0,[0]
"We believe the method of labeling and grouping has wide application, i.e. in creating reference summaries of complex, multi-party dialogues in other domains.
",7 Concluding Remarks and Future Work,0,[0]
"The summaries produced correspond closely to the target specification given in Sec. 2.2, and exhibit a high degree of consistency, as shown by the content similarity assessment of Sec.",7 Concluding Remarks and Future Work,0,[0]
5.3.,7 Concluding Remarks and Future Work,0,[0]
"Informal feedback from media professionals (at the Guardian and elsewhere) suggests that the summaries are viewed very positively as a summary of comments in themselves, and as a target for what an automated system might deliver online.
",7 Concluding Remarks and Future Work,0,[0]
"Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation.",7 Concluding Remarks and Future Work,0,[0]
"We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016).
",7 Concluding Remarks and Future Work,0,[0]
"Even so, there are limitations to the work done which give pointers to further work.",7 Concluding Remarks and Future Work,0,[0]
"The current corpus is limited in size, and would ideally contain annotations for more comment sets, with more annotations per set.",7 Concluding Remarks and Future Work,0,[0]
One possibility is to break the summary creation method into smaller tasks suitable for crowd-sourcing.,7 Concluding Remarks and Future Work,0,[0]
"Another issue is scalability: annotators can write summaries for ∼100 comments, but this is time-consuming and taxing, casting doubt on whether the method could scale to 1000 comments.",7 Concluding Remarks and Future Work,0,[0]
"Results from a pilot suggest annotators find it much easier to work on sets of 30–50 comments, so we are investigating how annotations for smaller subsets of a comment set might be merged into a single annotation.
",7 Concluding Remarks and Future Work,0,[0]
"Many of our annotators found the option to have groups and sub-groups useful, but this feature presents problems for some practical uses of the annotations, such as evaluation of some clustering methods.",7 Concluding Remarks and Future Work,0,[0]
"Hence, we have investigated methods to flatten the group-subgroup structure into one level, including the following two methods: (1) simple flattening, where all sub-groups merge into their parent groups (but this loses much of the analysis of some annotators), and (2) promoting subgroups to full group status (which has proved useful for generating useful group labels).",7 Concluding Remarks and Future Work,0,[0]
"More research is needed to establish the most effective flattening to best capture the consensus between annotators.
",7 Concluding Remarks and Future Work,0,[0]
"Finally, there is the open question of how to automatically evaluate system-generated summaries against the reference summaries proposed here.",7 Concluding Remarks and Future Work,0,[0]
"In particular, is ROUGE (Lin, 2004), the most widely used metric for automatic summary evaluation, an appropriate metric for use in this context?",7 Concluding Remarks and Future Work,0,[0]
"ROUGE, which calculates n-gram overlap between system and reference summaries, may not deal well with the abstractive nature of our summaries, and in particular with statements quantifying the distribution of support for various viewpoints.",7 Concluding Remarks and Future Work,0,[0]
"Its utility needs to be established by cor-
relating it with human judgements on system output quality.",7 Concluding Remarks and Future Work,0,[0]
"If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need.",7 Concluding Remarks and Future Work,0,[0]
"The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916.",Acknowledgments,0,[0]
"We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruiting annotators and The Guardian for allowing us access to their materials.",Acknowledgments,0,[0]
"Finally, thanks to our anonymous reviewers whose comments and suggestions have helped us to improve the paper.",Acknowledgments,0,[0]
"Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in reader comments in on-line news.",abstractText,0,[0]
"To date, however, there has been little discussion of what these summaries should be like and a lack of humanauthored exemplars, quite likely because writing summaries of this kind of interchange is so difficult.",abstractText,0,[0]
In this paper we propose one type of reader comment summary – the conversation overview summary – that aims to capture the key argumentative content of a reader comment conversation.,abstractText,0,[0]
"We describe a method we have developed to support humans in authoring conversation overview summaries and present a publicly available corpus – the first of its kind – of news articles plus comment sets, each multiply annotated, according to our method, with conversation overview summaries.",abstractText,0,[0]
The SENSEI Annotated Corpus: Human Summaries of Reader Comment Conversations in On-line News,title,0,[0]
"Deep neural networks have achieved outstanding performance (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016b).",1. Introduction,0,[0]
"Reducing the tendency of gradients to vanish or explode with depth (Hochreiter, 1991; Bengio et al., 1994) has been essential to this progress.
",1. Introduction,0,[0]
"Combining careful initialization (Glorot & Bengio, 2010; *Equal contribution 1Victoria University of Wellington, New Zealand 2SEED, Electronic Arts 3Disney Research, Zürich, Switzerland.",1. Introduction,0,[0]
"Correspondence to: David Balduzzi <dbalduzzi@gmail.com>, Brian McWilliams <brian@disneyresearch.com>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
"He et al., 2015) with batch normalization (Ioffe & Szegedy, 2015) bakes two solutions to the vanishing/exploding gradient problem into a single architecture.",1. Introduction,0,[0]
"The He initialization ensures variance is preserved across rectifier layers, and batch normalization ensures that backpropagation through layers is unaffected by the scale of the weights (Ioffe & Szegedy, 2015).
",1. Introduction,0,[0]
"It is perhaps surprising then that residual networks (resnets) still perform so much better than standard architectures when networks are sufficiently deep (He et al., 2016a;b).",1. Introduction,0,[0]
"This raises the question: If resnets are the solution, then what is the problem?",1. Introduction,0,[0]
We identify the shattered gradient problem: a previously unnoticed difficulty with gradients in deep rectifier networks that is orthogonal to vanishing and exploding gradients.,1. Introduction,0,[0]
"The shattering gradients problem is that, as depth increases, gradients in standard feedforward networks increasingly resemble white noise.",1. Introduction,0,[0]
"Resnets dramatically reduce the tendency of gradients to shatter.
",1. Introduction,0,[0]
Our analysis applies at initialization.,1. Introduction,0,[0]
Shattering should decrease during training.,1. Introduction,0,[0]
"Understanding how shattering affects training is an important open problem.
",1. Introduction,0,[0]
Terminology.,1. Introduction,0,[0]
We refer to networks without skip connections as feedforward nets—in contrast to residual nets (resnets) and highway nets.,1. Introduction,0,[0]
We distinguish between the real-valued output of a rectifier and its binary activation: the activation is 1 if the output is positive and 0 otherwise.,1. Introduction,0,[0]
The first step is to simply look at the gradients of neural networks.,1.1. The Shattered Gradients Problem,0,[0]
"Gradients are averaged over minibatches, depend on both the loss and the random sample from the data, and are extremely high-dimensional, which introduces multiple confounding factors and makes visualization difficult (but see section 4).",1.1. The Shattered Gradients Problem,0,[0]
We therefore construct a minimal model designed to eliminate these confounding factors.,1.1. The Shattered Gradients Problem,0,[0]
The minimal model is a neural network fW :,1.1. The Shattered Gradients Problem,0,[0]
R !,1.1. The Shattered Gradients Problem,0,[0]
R taking scalars to scalars; each hidden layer contains N = 200 rectifier neurons.,1.1. The Shattered Gradients Problem,0,[0]
The model is not intended to be applied to real data.,1.1. The Shattered Gradients Problem,0,[0]
"Rather, it is a laboratory where gradients can be isolated and investigated.
",1.1. The Shattered Gradients Problem,0,[0]
"We are interested in how the gradient varies, at initializa-
tion, as a function of the input:
dfW dx (x(i)) where x(i) 2 [ 2, 2] is in a (1)
1-dim grid of M = 256 “data points”.
",1.1. The Shattered Gradients Problem,0,[0]
"Updates during training depend on derivatives with respect to weights, not inputs.",1.1. The Shattered Gradients Problem,0,[0]
"Our results are relevant because, by the chain rule, @fW@wij = @fW @nj @nj @wij
.",1.1. The Shattered Gradients Problem,0,[0]
"Weight updates thus depend on @fW@nj —i.e. how the output of the network varies with the output of neurons in one layer (which are just inputs to the next layer).
",1.1. The Shattered Gradients Problem,0,[0]
The top row of figure 1 plots dfWdx (x (i) ),1.1. The Shattered Gradients Problem,0,[0]
for each point x(i) in the 1-dim grid.,1.1. The Shattered Gradients Problem,0,[0]
"The bottom row shows the (absolute value) of the covariance matrix: |(g ¯g)(g ¯g)>|/ 2g where g is the 256-vector of gradients, ¯g the mean, and 2g the variance.
",1.1. The Shattered Gradients Problem,0,[0]
If all the neurons were linear then the gradient would be a horizontal line (i.e. the gradient would be constant as a function of x).,1.1. The Shattered Gradients Problem,0,[0]
"Rectifiers are not smooth, so the gradients are discontinuous.
",1.1. The Shattered Gradients Problem,0,[0]
Gradients of shallow networks resemble brown noise.,1.1. The Shattered Gradients Problem,0,[0]
"Suppose the network has a single hidden layer: fw,b(x) = w
>⇢(x · v b).",1.1. The Shattered Gradients Problem,0,[0]
"Following Glorot & Bengio (2010), weights w and biases b are sampled from N (0, 2) with 2 = 1N .",1.1. The Shattered Gradients Problem,0,[0]
"Set v = (1, . . .",1.1. The Shattered Gradients Problem,0,[0]
", 1).
",1.1. The Shattered Gradients Problem,0,[0]
Figure 1a shows the gradient of the network for inputs x 2,1.1. The Shattered Gradients Problem,0,[0]
"[ 2, 2] and its covariance matrix.",1.1. The Shattered Gradients Problem,0,[0]
"Figure 1d shows a discrete approximation to brownian motion: BN (t) =Pt
s=1 Ws where Ws ⇠ N (0, 1 N ).",1.1. The Shattered Gradients Problem,0,[0]
The plots are strikingly similar: both clearly exhibit spatial covariance structure.,1.1. The Shattered Gradients Problem,0,[0]
"The resemblance is not coincidental: section A1 applies
Donsker’s theorem to show the gradient converges to brownian motion as N !",1.1. The Shattered Gradients Problem,0,[0]
"1.
Gradients of deep networks resemble white noise.",1.1. The Shattered Gradients Problem,0,[0]
Figure 1b shows the gradient of a 24-layer fully-connected rectifier network.,1.1. The Shattered Gradients Problem,0,[0]
"Figure 1e shows white noise given by samples Wk ⇠ N (0, 1).",1.1. The Shattered Gradients Problem,0,[0]
"Again, the plots are strikingly similar.
",1.1. The Shattered Gradients Problem,0,[0]
"Since the inputs lie on a 1-dim grid, it makes sense to compute the autocorrelation function (ACF) of the gradient.",1.1. The Shattered Gradients Problem,0,[0]
Figures 2a and 2d compare this function for feedforward networks of different depth with white and brown noise.,1.1. The Shattered Gradients Problem,0,[0]
The ACF for shallow networks resembles the ACF of brown noise.,1.1. The Shattered Gradients Problem,0,[0]
"As the network gets deeper, the ACF quickly comes to resemble that of white noise.
",1.1. The Shattered Gradients Problem,0,[0]
Theorem 1 explains this phenomenon.,1.1. The Shattered Gradients Problem,0,[0]
"We show that correlations between gradients decrease exponentially 12L with depth in feedforward rectifier networks.
",1.1. The Shattered Gradients Problem,0,[0]
Training is difficult when gradients behave like white noise.,1.1. The Shattered Gradients Problem,0,[0]
The shattered gradient problem is that the spatial structure of gradients is progressively obliterated as neural nets deepen.,1.1. The Shattered Gradients Problem,0,[0]
"The problem is clearly visible when inputs are taken from a one-dimensional grid, but is difficult to observe when inputs are randomly sampled from a highdimensional dataset.
",1.1. The Shattered Gradients Problem,0,[0]
"Shattered gradients undermine the effectiveness of algorithms that assume gradients at nearby points are similar such as momentum-based and accelerated methods (Sutskever et al., 2013; Balduzzi et al., 2017).",1.1. The Shattered Gradients Problem,0,[0]
"If dfWdnj behaves like white noise, then a neuron’s effect on the output of the network (whether increasing weights causes the network to output more or less) becomes extremely unstable
making learning difficult.
",1.1. The Shattered Gradients Problem,0,[0]
Gradients of deep resnets lie in between brown and white noise.,1.1. The Shattered Gradients Problem,0,[0]
"Introducing skip-connections allows much deeper networks to be trained (Srivastava et al., 2015; He et al., 2016b;a; Greff et al., 2017).",1.1. The Shattered Gradients Problem,0,[0]
Skip-connections significantly change the correlation structure of gradients.,1.1. The Shattered Gradients Problem,0,[0]
Figure 1c shows the concrete example of a 50-layer resnet which has markedly more structure than the equivalent feedforward net (figure 1b).,1.1. The Shattered Gradients Problem,0,[0]
Figure 2b shows the ACF of resnets of different depths.,1.1. The Shattered Gradients Problem,0,[0]
"Although the gradients become progressively less structured, they do not whiten to the extent of the gradients in standard feedforward networks— there are still correlations in the 50-layer resnet whereas in the equivalent feedforward net, the gradients are indistinguishable from white noise.",1.1. The Shattered Gradients Problem,0,[0]
"Figure 2c shows the dramatic effect of recently proposed -rescaling (Szegedy et al., 2016): the ACF of even the 50 layer network resemble brown-noise.
",1.1. The Shattered Gradients Problem,0,[0]
"Theorem 3 shows that correlations between gradients decay sublinearly with depth 1p
L for resnets with batch normal-
ization.",1.1. The Shattered Gradients Problem,0,[0]
"We also show, corollary 1, that modified highway networks (where the gates are scalars) can achieve a depth independent correlation structure on gradients.",1.1. The Shattered Gradients Problem,0,[0]
"The analysis explains why skip-connections, combined with suitable rescaling, preserve the structure of gradients.",1.1. The Shattered Gradients Problem,0,[0]
Section 2 shows that batch normalization increases neural efficiency.,1.2. Outline,0,[0]
"We explore how batch normalization behaves differently in feedforward and resnets, and draw out facts that are relevant to the main results.
",1.2. Outline,0,[0]
The main results are in section 3.,1.2. Outline,0,[0]
They explain why gradients shatter and how skip-connections reduce shattering.,1.2. Outline,0,[0]
The proofs are for a mathematically amenable model: fully-connected rectifier networks with the same number of hidden neurons in each layer.,1.2. Outline,0,[0]
Section 4 presents empirical results which show gradients similarly shatter in convnets for real data.,1.2. Outline,0,[0]
"It also shows that shattering causes average
gradients over minibatches to decrease with depth (relative to the average variance of gradients).
",1.2. Outline,0,[0]
"Finally, section 5 proposes the LL-init (“looks linear initialization”) which eliminates shattering.",1.2. Outline,0,[0]
Preliminary experiments show the LL-init allows training of extremely deep networks (⇠200 layers) without skip-connections.,1.2. Outline,0,[0]
Carefully initializing neural networks has led to a series of performance breakthroughs dating back (at least) to the unsupervised pretraining in Hinton et al. (2006); Bengio et al. (2006).,1.3. Related work,0,[0]
The insight of Glorot & Bengio (2010) is that controlling the variance of the distributions from which weights are sampled allows to control how layers progressively amplify or dampen the variance of activations and error signals.,1.3. Related work,0,[0]
"More recently, He et al. (2015) refined the approach to take rectifiers into account.",1.3. Related work,0,[0]
"Rectifiers effectively halve the variance since, at initialization and on average, they are active for half their inputs.",1.3. Related work,0,[0]
"Orthogonalizing weight matrices can yield further improvements albeit at a computational cost (Saxe et al., 2014; Mishkin & Matas, 2016).",1.3. Related work,0,[0]
"The observation that the norms of weights form a random walk was used by Sussillo & Abbott (2015) to tune the gains of neurons.
",1.3. Related work,0,[0]
"In short, it has proven useful to treat weights and gradients as random variables, and carefully examine their effect on the variance of the signals propagated through the network.",1.3. Related work,0,[0]
"This paper presents a more detailed analysis that considers correlations between gradients at different datapoints.
",1.3. Related work,0,[0]
"The closest work to ours is Veit et al. (2016), which shows resnets behave like ensembles of shallow networks.",1.3. Related work,0,[0]
We provide a more detailed analysis of the effect of skipconnections on gradients.,1.3. Related work,0,[0]
"A recent paper showed resnets have universal finite-sample expressivity and may lack spurious local optima (Hardt & Ma, 2017) but does not explain why deep feedforward nets are harder to train than resnets.",1.3. Related work,0,[0]
"An interesting hypothesis is that skip-connections improve performance by breaking symmetries (Orhan, 2017).
",1.3. Related work,0,[0]
The Shattered Gradients Problem,1.3. Related work,0,[0]
"Batch normalization was introduced to reduce covariate shift (Ioffe & Szegedy, 2015).",2. Observations on batch normalization,0,[0]
"However, it has other effects that are less well-known – and directly impact the correlation structure of gradients.",2. Observations on batch normalization,0,[0]
"We investigate the effect of batch normalization on neuronal activity at initialization (i.e. when it mean-centers and rescales to unit variance).
",2. Observations on batch normalization,0,[0]
We first investigate batch normalization’s effect on neural activations.,2. Observations on batch normalization,0,[0]
"Neurons are active for half their inputs on average, figure 3, with or without batch normalization.",2. Observations on batch normalization,0,[0]
Figure 3 also shows how often neurons are co-active for two inputs.,2. Observations on batch normalization,0,[0]
"With batch normalization, neurons are co-active for 14 of distinct pairs of inputs, which is what would happen if activations were decided by unbiased coin flips.",2. Observations on batch normalization,0,[0]
"Without batch normalization, the co-active proportion climbs with depth, suggesting neuronal responses are increasingly redundant.",2. Observations on batch normalization,0,[0]
"Resnets with batch normalization behave the same as feedforward nets (not shown).
",2. Observations on batch normalization,0,[0]
Figure 4 takes a closer look.,2. Observations on batch normalization,0,[0]
It turns out that computing the proportion of inputs causing neurons to be active on average is misleading.,2. Observations on batch normalization,0,[0]
The distribution becomes increasingly bimodal with depth.,2. Observations on batch normalization,0,[0]
"In particular, neurons are either always active or always inactive for layer 50 in the feedforward net without batch normalization (blue histogram in figure 4a).",2. Observations on batch normalization,0,[0]
"Batch normalization causes most neurons to be active for half the inputs, blue histograms in figures 4b,c.
Neurons that are always active may as well be linear.",2. Observations on batch normalization,0,[0]
Neurons that are always inactive may as well not exist.,2. Observations on batch normalization,0,[0]
"It follows that batch normalization increases the efficiency with which rectifier nonlinearities are utilized.
",2. Observations on batch normalization,0,[0]
The increased efficiency comes at a price.,2. Observations on batch normalization,0,[0]
The raster plot for feedforward networks resembles static television noise: the spatial structure is obliterated.,2. Observations on batch normalization,0,[0]
Resnets (Figure 4c) exhibit a compromise where neurons are utilized efficiently but the spatial structure is also somewhat preserved.,2. Observations on batch normalization,0,[0]
The preservation of spatial structure is quantified via the contiguity histograms which counts long runs of consistent activation.,2. Observations on batch normalization,0,[0]
Resnets maintain a broad distribution of contiguity even with deep networks whereas batch normalization on feedforward nets shatters these into small sections.,2. Observations on batch normalization,0,[0]
This section analyzes the correlation structure of gradients in neural nets at initialization.,3. Analysis,0,[0]
"The main ideas and results are presented; the details provided in section A3.
",3. Analysis,0,[0]
"Perhaps the simplest way to probe the structure of a random process is to measure the first few moments: the mean, variance and covariance.",3. Analysis,0,[0]
We investigate how the correlation between typical datapoints (defined below) changes with network structure and depth.,3. Analysis,0,[0]
Weaker correlations correspond to whiter gradients.,3. Analysis,0,[0]
The analysis is for fullyconnected networks.,3. Analysis,0,[0]
"Extending to convnets involves (significant) additional bookkeeping.
",3. Analysis,0,[0]
Proof strategy.,3. Analysis,0,[0]
The covariance defines an inner product on the vector space of real-valued random variables with mean zero and finite second moment.,3. Analysis,0,[0]
"It was shown in Balduzzi et al. (2015); Balduzzi (2016) that the gradients in neural nets are sums of path-weights over active paths, see section A3.",3. Analysis,0,[0]
The first step is to observe that path-weights are orthogonal with respect to the variance inner product.,3. Analysis,0,[0]
"To express gradients as linear combinations of path-weights is thus to express them over an orthogonal basis.
",3. Analysis,0,[0]
Working in the path-weight basis reduces computing the covariance between gradients at different datapoints to counting the number of co-active paths through the network.,3. Analysis,0,[0]
"The second step is to count co-active paths and adjust for rescaling factors (e.g. due to batch normalization).
",3. Analysis,0,[0]
The following assumption is crucial to the analysis: Assumption 1 (typical datapoints).,3. Analysis,0,[0]
We say x(i) and x(j) are typical datapoints if half of neurons per layer are active for each and a quarter per layer are co-active for both.,3. Analysis,0,[0]
"We assume all pairs of datapoints are typical.
",3. Analysis,0,[0]
The assumption will not hold for every pair of datapoints.,3. Analysis,0,[0]
"Figure 3 shows the assumption holds, on average, under batch normalization for both activations and coactivations.",3. Analysis,0,[0]
The initialization in He et al. (2015) assumes datapoints activate half the neurons per layer.,3. Analysis,0,[0]
"The assumption on co-activations is implied by (and so weaker than) the assumption in Choromanska et al. (2015) that activations are Bernoulli random variables independent of the inputs.
",3. Analysis,0,[0]
Correlations between gradients.,3. Analysis,0,[0]
"Weight updates in a neural network are proportional to
wjk / #mbX
i=1
PX
p=1
@`
@fp @fp",3. Analysis,0,[0]
@nk @nk @wjk x,3. Analysis,0,[0]
"(i) .
",3. Analysis,0,[0]
where fp is the pth coordinate of the output of the network and nk is the output of the kth neuron.,3. Analysis,0,[0]
The derivatives @`@fp and @nk@wjk do not depend on the network’s internal structure.,3. Analysis,0,[0]
"We are interested in the middle term @fp@nk , which does.",3. Analysis,0,[0]
"It is mathematically convenient to work with the sumPP
p=1 fp over output coordinates of the network.",3. Analysis,0,[0]
Section 4 shows that our results hold for convnets on real-data with the cross-entropy loss.,3. Analysis,0,[0]
See also remark A2.,3. Analysis,0,[0]
Definition 1.,3. Analysis,0,[0]
Let ri := PP p=1 @fp,3. Analysis,0,[0]
"@n (x (i) ) be the derivative with respect to neuron n given input x(i) 2 D. For each input x(i), the derivative ri is a real-valued random variable.",3. Analysis,0,[0]
It has mean zero since weights are sampled from distributions with mean zero.,3. Analysis,0,[0]
"Denote the covariance and correlation of gradients by
C(i, j) = E[ri rj ] and R(i, j) = E[ri rj ]q",3. Analysis,0,[0]
"E[r2i ] · E[r2j ] ,
where the expectations are w.r.t the distribution on weights.",3. Analysis,0,[0]
"Without loss of generality, pick a neuron n separated from the output by L layers.",3.1. Feedforward networks,0,[0]
The first major result is Theorem 1 (covariance of gradients in feedforward nets).,3.1. Feedforward networks,0,[0]
Suppose weights are initialized with variance 2 = 2N following He et al. (2015).,3.1. Feedforward networks,0,[0]
"Then
a) The variance of the gradient at x(i) is C fnn(i)",3.1. Feedforward networks,0,[0]
1,3.1. Feedforward networks,0,[0]
b),3.1. Feedforward networks,0,[0]
"The covariance is C fnn(i, j) = 12L .
",3.1. Feedforward networks,0,[0]
Part (a) recovers the observation in He et al. (2015) that setting 2 = 2N preserves the variance across layers in rectifier networks.,3.1. Feedforward networks,0,[0]
Part (b) is new.,3.1. Feedforward networks,0,[0]
"It explains the empirical observation, figure 2a, that gradients in feedforward nets whiten with depth.",3.1. Feedforward networks,0,[0]
"Intuitively, gradients whiten because the number of paths through the network grows exponentially faster with depth than the fraction of co-active paths, see section A3 for details.",3.1. Feedforward networks,0,[0]
"The residual modules introduced in He et al. (2016a) are
xl = xl 1 +W l⇢BN ⇣ V l⇢BN (xl 1) ⌘
where ⇢BN (a) = ⇢(BN(a)) and ⇢(a) = max(0, a) is the rectifier.",3.2. Residual networks,0,[0]
"We analyse the stripped-down variant
xl = ↵ · xl 1 + ·Wl⇢BN (xl 1)
(2)
where ↵ and are rescaling factors.",3.2. Residual networks,0,[0]
Dropping Vl⇢BN makes no essential difference to the analysis.,3.2. Residual networks,0,[0]
"The - rescaling was introduced in Szegedy et al. (2016) where it was observed setting 2 [0.1, 0.3] reduces instability.",3.2. Residual networks,0,[0]
We include ↵ for reasons of symmetry.,3.2. Residual networks,0,[0]
Theorem 2 (covariance of gradients in resnets).,3.2. Residual networks,0,[0]
Consider a resnet with batch normalization disabled and ↵ = = 1.,3.2. Residual networks,0,[0]
Suppose 2 = 2N as above.,3.2. Residual networks,0,[0]
"Then
a) The variance of the gradient at x(i) is Cres(i) = 2L.
The Shattered Gradients Problem
b)",3.2. Residual networks,0,[0]
"The covariance is Cres(i, j) = 3 2 L.
",3.2. Residual networks,0,[0]
"The correlation is Rres(i, j) =",3.2. Residual networks,0,[0]
"3 4 L.
The theorem implies there are two problems in resnets without batch normalization: (i) the variance of gradients grows and (ii) their correlation decays exponentially with depth.",3.2. Residual networks,0,[0]
Both problems are visible empirically.,3.2. Residual networks,0,[0]
"A solution to the exploding variance of resnets is to rescale layers by ↵ = 1p
2 which yields
Cres ↵= p 2 (i) = 1 and Rres ↵= p 2 (i, j) =
✓ 3
4
◆L
and so controls the variance but the correlation between gradients still decays exponentially with depth.",3.3. Rescaling in Resnets,0,[0]
"Both theoretical predictions hold empirically.
",3.3. Rescaling in Resnets,0,[0]
"In practice, ↵-rescaling is not used.",3.3. Rescaling in Resnets,0,[0]
"Instead, activations are rescaled by batch normalization (Ioffe & Szegedy, 2015) and, more recently, setting 2 [0.1, 0.3] per Szegedy et al. (2016).",3.3. Rescaling in Resnets,0,[0]
The effect is dramatic: Theorem 3 (covariance of gradients in resnets with BN and rescaling).,3.3. Rescaling in Resnets,0,[0]
"Under the assumptions above, for resnets with batch normalization and -rescaling,
a) the variance is Cres ,BN(i) = 2(L 1) + 1; b) the covariance1 is Cres ,BN(i, j) ⇠ p L; and
the correlation is Rres ,BN(i, j) ⇠ 1 pL .
",3.3. Rescaling in Resnets,0,[0]
"The theorem explains the empirical observation, figure 2a, that gradients in resnets whiten much more slowly with depth than feedforward nets.",3.3. Rescaling in Resnets,0,[0]
"It also explains why setting near zero further reduces whitening.
",3.3. Rescaling in Resnets,0,[0]
"Batch normalization changes the decay of the correlations from 12L to 1p L
.",3.3. Rescaling in Resnets,0,[0]
"Intuitively, the reason is that the variance of the outputs of layers grows linearly, so batch normalization rescales them by different amounts.",3.3. Rescaling in Resnets,0,[0]
Rescaling by introduces a constant factor.,3.3. Rescaling in Resnets,0,[0]
"Concretely, the model predicts using batch normalization with = 0.1 on a 100- layer resnet gives typical correlation Rres0.1,BN(i, j) = 0.7.",3.3. Rescaling in Resnets,0,[0]
"Setting = 1.0 gives Rres1.0,BN(i, j) = 0.1.",3.3. Rescaling in Resnets,0,[0]
"By contrast, a 100-layer feedforward net has correlation indistinguishable from zero.",3.3. Rescaling in Resnets,0,[0]
"Highway networks can be thought of as a generalization of resnets, that were in fact introduced slightly earlier (Srivas-
1See section A3.4 for exact computations.
",3.4. Highway networks,0,[0]
"tava et al., 2015; Greff et al., 2017).",3.4. Highway networks,0,[0]
"The standard highway network has layers of the form
xl = 1 T (xl 1) · xl 1 + T (xl 1) ·H(xl 1)
where T (·) and H(·) are learned gates and features respectively.",3.4. Highway networks,0,[0]
"Consider the following modification where 1 and 2 are scalars satisfying 21 + 22 = 1:
xl = 1 · xl 1 + 2 ·Wl⇢(xl 1)
",3.4. Highway networks,0,[0]
The module can be recovered by judiciously choosing ↵ and in equation (2).,3.4. Highway networks,0,[0]
"However, it is worth studying in its own right:
Corollary 1 (covariance of gradients in highway networks).",3.4. Highway networks,0,[0]
"Under the assumptions above, for modified highway networks with -rescaling,
a) the variance of gradients is CHN (i) = 1; and
b) the correlation is RHN (i, j) = 21 + 1 2 2 2 L.
In particular, if 1 = q 1 1L and 2 = q 1 L then the correlation between gradients does not decay with depth
lim L!1 RHN (i, j) = 1p e .
",3.4. Highway networks,0,[0]
The tradeoff is that the contributions of the layers becomes increasingly trivial (i.e. close to the identity) as L ! 1.,3.4. Highway networks,0,[0]
In this section we provide empirical evidence that the main results also hold for deep convnets using the CIFAR-10 dataset.,4. Gradients shatter in convnets,0,[0]
"We instantiate feedforward and resnets with 2, 4, 10, 24 and 50 layers of equivalent size.",4. Gradients shatter in convnets,0,[0]
"Using a slight modification of the “bottleneck” architecture in He et al. (2016a), we introduce one skip-connection for every two convolutional layers and both network architectures use batch normalization.
",4. Gradients shatter in convnets,0,[0]
Figures 5a and b compare the covariance of gradients in the first layer of feedforward and resnets ( = 0.1) with a minibatch of 256 random samples from CIFAR-10 for networks of depth 2 and 50.,4. Gradients shatter in convnets,0,[0]
"To highlight the spatial structure of the gradients, the indices of the minibatches were reordered according to a k-means clustering (k = 10) applied to the gradients of the two-layer networks.",4. Gradients shatter in convnets,0,[0]
The same permutation is used for all networks within a row.,4. Gradients shatter in convnets,0,[0]
"The spatial structure is visible in both two-layer networks, although it is more apparent in the resnet.",4. Gradients shatter in convnets,0,[0]
In the feedforward network the structure quickly disappears with depth.,4. Gradients shatter in convnets,0,[0]
"In the resnet, the structure remains apparent at 50 layers.
",4. Gradients shatter in convnets,0,[0]
To quantify this effect we consider the “whiteness” of the gradient using relative effective rank.,4. Gradients shatter in convnets,0,[0]
"Let be the matrix whose columns are the gradients with respect to the input, for each datapoint x(i) in a minibatch.",4. Gradients shatter in convnets,0,[0]
The effective rank is r( ) =,4. Gradients shatter in convnets,0,[0]
tr( > )/k,4. Gradients shatter in convnets,0,[0]
"k22 and measures the intrinsic dimension of a matrix (Vershynin, 2012).",4. Gradients shatter in convnets,0,[0]
It is bounded above by the rank of —a matrix with highly correlated columns and therefore more structure will have a lower effective rank.,4. Gradients shatter in convnets,0,[0]
We are interested in the effective rank of the covariance matrix of the gradients relative to a “white” matrix Y of the same dimensions with i.i.d.,4. Gradients shatter in convnets,0,[0]
Gaussian entries.,4. Gradients shatter in convnets,0,[0]
"The relative effective rank r( )/r(Y) measures the similarity between the second moments of and Y.
Figure",4. Gradients shatter in convnets,0,[0]
5c shows that the relative effective rank (averaged over 30 minibatches) grows much faster as a function of depth for networks without skip-connections.,4. Gradients shatter in convnets,0,[0]
"For resnets, the parameter slows down the rate of growth of the effective rank as predicted by theorem 3.
",4. Gradients shatter in convnets,0,[0]
Figure 5d shows the average `2-norm of the gradient in each coordinate (normalized by the standard deviation computed per minibatch).,4. Gradients shatter in convnets,0,[0]
We observe that this quantity decays much more rapidly as a function of depth for feedforward networks.,4. Gradients shatter in convnets,0,[0]
This is due to the effect of averaging increasingly whitening gradients within each minibatch.,4. Gradients shatter in convnets,0,[0]
"In other words, the noise within minibatches overwhelms the signal.",4. Gradients shatter in convnets,0,[0]
"The phenomenon is much less pronounced in resnets.
",4. Gradients shatter in convnets,0,[0]
Taken together these results confirm the results in section 3 for networks with convolutional layers and show that the gradients in resnets are indeed more structured than those in feedforward nets and therefore do not vanish when averaged within a minibatch.,4. Gradients shatter in convnets,0,[0]
This phenomena allows for the training of very deep resnets.,4. Gradients shatter in convnets,0,[0]
"Shattering gradients are not a problem for linear networks, see remark after equation (1).",5. The “looks linear” initialization,0,[0]
"Unfortunately, linear networks are not useful since they lack expressivity.
",5. The “looks linear” initialization,0,[0]
The LL-init combines the best of linear and rectifier nets by initializing rectifiers to look linear.,5. The “looks linear” initialization,0,[0]
Several implementations are possible; see Zagoruyko & Komodakis (2017) for related architectures yielding good empirical results.,5. The “looks linear” initialization,0,[0]
"We use concatenated rectifiers or CReLUs (Shang et al., 2016):
x 7!",5. The “looks linear” initialization,0,[0]
"✓
⇢(x) ⇢( x)
◆
",5. The “looks linear” initialization,0,[0]
"The key observation is that initializing weights with a mirrored block structure yields linear outputs
W W · ✓
⇢(x) ⇢( x)
◆ = W⇢(x) W⇢( x) = Wx.
",5. The “looks linear” initialization,0,[0]
"The output will cease to be linear as soon as weight updates cause the two blocks to diverge.
",5. The “looks linear” initialization,0,[0]
"An alternative architecture is based on the PReLU introduced in He et al. (2015):
PReLU: ⇢p(x) =",5. The “looks linear” initialization,0,[0]
"( x if x > 0 ax else.
",5. The “looks linear” initialization,0,[0]
Setting a = 1 at initialization obtains a different kind of LL-init.,5. The “looks linear” initialization,0,[0]
"Preliminary experiments, not shown, suggest that the LL-init is more effective on the CReLU-based architecture than PReLU.",5. The “looks linear” initialization,0,[0]
"The reason is unclear.
",5. The “looks linear” initialization,0,[0]
Orthogonal convolutions.,5. The “looks linear” initialization,0,[0]
"A detailed analysis of learning in linear neural networks by Saxe et al. (2014) showed, theoretically and experimentally, that arbitrarily deep linear networks can be trained when initialized with orthogonal weights.",5. The “looks linear” initialization,0,[0]
"Motivated by these results, we use the LL-init in conjunction with orthogonal weights.
",5. The “looks linear” initialization,0,[0]
"The Shattered Gradients Problem
Depth
We briefly describe how we orthogonally initialize a kernel K of size A ⇥ B ⇥ 3 ⇥ 3 where A B. First, set all the entries of K to zero.",5. The “looks linear” initialization,0,[0]
"Second, sample a random matrix W of size (A ⇥ B) with orthonormal columns.",5. The “looks linear” initialization,0,[0]
"Finally, set K[:, :, 2, 2] := W. The kernel is used in conjunction with strides of one and zero-padding.",5. The “looks linear” initialization,0,[0]
"We investigated the performance of the LL-init on very deep networks, evaluated on CIFAR-10.",5.1. Experiments,0,[0]
"The aim was not to match the state-of-the-art, but rather to test the hypothesis that shattered gradients adversely affect training in very deep rectifier nets.",5.1. Experiments,0,[0]
We therefore designed an experiment where (concatenated) rectifier nets are and are not shattered at initialization.,5.1. Experiments,0,[0]
"We find that the LL-init allows to train significantly deeper nets, which confirms the hypothesis.
",5.1. Experiments,0,[0]
"We compared a CReLU architecture with an orthogonal LL-init against an equivalent CReLU network, resnet, and a standard feedforward ReLU network.",5.1. Experiments,0,[0]
The other networks were initialized according to He et al. (2015).,5.1. Experiments,0,[0]
"The architectures are thin with the number of filters per layer in the ReLU networks ranging from 8 at the input layer to 64, see section A4.",5.1. Experiments,0,[0]
Doubling with each spatial extent reduction.,5.1. Experiments,0,[0]
The thinness of the architecture makes it particularly difficult for gradients to propagate at high depth.,5.1. Experiments,0,[0]
"The reduction is performed by convolutional layers with strides of 2, and following the last reduction the representation is passed to a fully connected layer with 10 neurons for classification.",5.1. Experiments,0,[0]
The numbers of filters per layer of the CReLU models were adjusted by a factor of 1/ p 2 to achieve parameter parity with the ReLU models.,5.1. Experiments,0,[0]
"The Resnet version of the model is the same as the basic ReLU model with skip-connections after every two modules following He et al. (2016a).
",5.1. Experiments,0,[0]
"Updates were performed with Adam (Kingma & Ba, 2015).",5.1. Experiments,0,[0]
"Training schedules were automatically determined by an auto-scheduler that measures how quickly the loss on the training set has been decreasing over the last ten epochs, and drops the learning rate if a threshold remains crossed for five measurements in a row.",5.1. Experiments,0,[0]
"Standard data augmentation was performed; translating up to 4 pixels in any direction and flipping horizontally with p = 0.5.
",5.1. Experiments,0,[0]
Results are shown in figure 6.,5.1. Experiments,0,[0]
Each point is the mean of 10 trained models.,5.1. Experiments,0,[0]
The ReLU and CReLU nets performed steadily worse with depth; the ReLU net performing worse than the linear baseline of 40% at the maximum depth of 198.,5.1. Experiments,0,[0]
"The feedforward net with LL-init performs comparably to a resnet, suggesting that shattered gradients are a large part of the problem in training very deep networks.",5.1. Experiments,0,[0]
The representational power of rectifier networks depends on the number of linear regions into which it splits the input space.,6. Conclusion,0,[0]
It was shown in Montufar et al. (2014) that the number of linear regions can grow exponentially with depth (but only polynomially with width).,6. Conclusion,0,[0]
"Hence deep neural networks are capable of far richer mappings than shallow ones (Telgarsky, 2016).",6. Conclusion,0,[0]
"An underappreciated consequence of the exponential growth in linear regions is the proliferation of discontinuities in the gradients of rectifier nets.
",6. Conclusion,0,[0]
"This paper has identified and analyzed a previously unnoticed problem with gradients in deep networks: in a randomly initialized network, the gradients of deeper layers are increasingly uncorrelated.",6. Conclusion,0,[0]
Shattered gradients play havoc with the optimization methods currently in use2 and may explain the difficulty in training deep feedforward networks even when effective initialization and batch normalization are employed.,6. Conclusion,0,[0]
Averaging gradients over minibatches becomes analogous to integrating over white noise – there is no clear trend that can be summarized in a single average direction.,6. Conclusion,0,[0]
"Shattered gradients can also introduce numerical instabilities, since small differences in the input can lead to large differences in gradients.
",6. Conclusion,0,[0]
Skip-connections in combination with suitable rescaling reduce shattering.,6. Conclusion,0,[0]
"Specifically, we show that the rate at which correlations between gradients decays changes from exponential for feedforward architectures to sublinear for resnets.",6. Conclusion,0,[0]
The analysis uncovers a surprising and (to us at least) unexpected side-effect of batch normalization.,6. Conclusion,0,[0]
An alternate solution to the shattering gradient problem is to design initializations that do not shatter such as the LLinit.,6. Conclusion,0,[0]
"An interesting future direction is to investigate hybrid architectures combining the LL-init with skip connections.
",6. Conclusion,0,[0]
"2Note that even the choice of a step size in SGD typically reflects an assumption about the correlation scale of the gradients.
",6. Conclusion,0,[0]
The Shattered Gradients Problem,6. Conclusion,0,[0]
A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients.,abstractText,0,[0]
"Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite wellchosen initialization and batch normalization.",abstractText,0,[0]
"In this paper, we identify the shattered gradients problem.",abstractText,0,[0]
"Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly.",abstractText,0,[0]
"Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets.",abstractText,0,[0]
"Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.",abstractText,0,[0]
"The Shattered Gradients Problem: If resnets are the answer, then what is the question?",title,0,[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 77–89 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1008
Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field.",text,0,[0]
Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way.,1 Introduction,0,[0]
"There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016).",1 Introduction,0,[0]
"Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones.",1 Introduction,0,[0]
"An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects.
",1 Introduction,0,[0]
In this paper we begin to chart the various proposals for semantic schemes according to the content they support.,1 Introduction,0,[0]
"As not many semantic queries on texts can at present be answered with near human-like reliability without using manual symbolic annotation, we will mostly focus on schemes
that represent semantic distinctions explicitly.1
We begin by discussing the goals of SRT in Section 2.",1 Introduction,0,[0]
"Section 3 surveys major represented meaning components, including predicate-argument relations, discourse relations and logical structure.",1 Introduction,0,[0]
"Section 4 details the various concrete proposals for SRT schemes and annotated resources, while Sections 5 and 6 discuss criteria for their evaluation and their relation to syntax, respectively.
",1 Introduction,0,[0]
"We find that despite the major differences in terms of formalism and interface with syntax, in terms of their content there is a great deal of convergence of SRT schemes.",1 Introduction,0,[0]
"Principal differences between schemes are mostly related to their ability to abstract away from formal and syntactic variation, namely to assign similar structures to different constructions that have a similar meaning, and to assign different structures to constructions that have different meanings, despite their surface similarity.",1 Introduction,0,[0]
"Other important differences are in the level of training they require from their annotators (e.g., expert annotators vs. crowd-sourcing) and in their cross-linguistic generality.",1 Introduction,0,[0]
"We discuss the complementary strengths of different schemes, and suggest paths for future integration.",1 Introduction,0,[0]
The term semantics is used differently in different contexts.,2 Defining Semantic Representation,0,[0]
For the purposes of this paper we define a semantic representation as one that reflects the meaning of the text as it is understood by a language speaker.,2 Defining Semantic Representation,0,[0]
A semantic representation should thus be paired with a method for extracting information from it that can be directly evaluated by humans.,2 Defining Semantic Representation,0,[0]
"The extraction process should be reliable and computationally efficient.
",2 Defining Semantic Representation,0,[0]
"1Note that even a string representation of text can be regarded as semantic given a reliable enough parser.
",2 Defining Semantic Representation,0,[0]
"77
",2 Defining Semantic Representation,0,[0]
"We stipulate that a fundamental component of the content conveyed by SRTs is argument structure – who did what to whom, where, when and why, i.e., events, their participants and the relations between them.",2 Defining Semantic Representation,0,[0]
"Indeed, the fundamental status of argument structure has been recognized by essentially all approaches to semantics both in theoretical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013).",2 Defining Semantic Representation,0,[0]
"Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3.
",2 Defining Semantic Representation,0,[0]
Another approach to defining an SRT is through external (extra-textual) criteria or applications.,2 Defining Semantic Representation,0,[0]
"For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014).",2 Defining Semantic Representation,0,[0]
"Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), or defining semantics through a different modality, for instance interpreting text in terms of images that correspond to it (Kiros et al., 2014), or in terms of embodied motor and perceptual schemas (Feldman et al., 2010).
",2 Defining Semantic Representation,0,[0]
"A different approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences.",2 Defining Semantic Representation,0,[0]
"Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016).",2 Defining Semantic Representation,0,[0]
"VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013).",2 Defining Semantic Representation,0,[0]
"However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture.",2 Defining Semantic Representation,0,[0]
"We therefore only lightly touch on VSMs in this survey.
",2 Defining Semantic Representation,0,[0]
"Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality.",2 Defining Semantic Representation,0,[0]
"While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as-
sumed to be much closer in terms of their semantic content (Bar-Hillel, 1960; Fodor, 1975).",2 Defining Semantic Representation,0,[0]
"See Section 5 for further discussion.
",2 Defining Semantic Representation,0,[0]
"A terminological note: within formal linguistics, semantics is often the study of the relation between symbols (e.g., words, syntactic constructions) and what they signify.",2 Defining Semantic Representation,0,[0]
"In this sense, semantics is the study of the aspects of meaning that are overtly expressed by the lexicon and grammar of a language, and is thus tightly associated with a theory of the syntax-semantics interface.",2 Defining Semantic Representation,0,[0]
"We note that this definition of semantics is somewhat different from the one intended here, which defines semantic schemes as theories of meaning.",2 Defining Semantic Representation,0,[0]
We turn to discussing the main content types encoded by semantic representation schemes.,3 Semantic Content,0,[0]
"Due to space limitations, we focus only on text semantics, which studies the meaning relationships between lexical items, rather than the meaning of the lexical items themselves.2 We also defer discussion of more targeted semantic distinctions, such as sentiment, to future work.
",3 Semantic Content,0,[0]
"We will use the following as a running example:
(1) Although Ann was leaving, she gave the present to John.
Events.",3 Semantic Content,0,[0]
"Events (sometimes called frames, propositions or scenes) are the basic building blocks of argument structure representations.",3 Semantic Content,0,[0]
"An event includes a predicate (main relation, frame-evoking element), which is the main determinant of what the event is about.",3 Semantic Content,0,[0]
"It also includes arguments (participants, core elements) and secondary relations (modifiers, non-core elements).",3 Semantic Content,0,[0]
"Example 1 is usually viewed as having two events, evoked by “leaving” and “gave”.
",3 Semantic Content,0,[0]
"Schemes commonly provide an ontology or a lexicon of event types (also a predicate lexicon), which categorizes semantically similar events evoked by different lexical items.",3 Semantic Content,0,[0]
"For instance, FrameNet defines frames as schematized story fragments evoked by a set of conceptually similar predicates.",3 Semantic Content,0,[0]
"In (1), the frames evoked by “leaving” and “gave” are DEPARTING and GIVING, but DEPARTING may also be evoked by “depart” and “exit”, and GIVING by “donate” and “gift”.
",3 Semantic Content,0,[0]
"2 We use the term “Text Semantics”, rather than the commonly used “Sentence Semantics” to include inter-sentence semantic relations as well.
",3 Semantic Content,0,[0]
"The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here.",3 Semantic Content,0,[0]
"The representation of such events is recently receiving considerable interest within NLP, e.g. the Richer Event Descriptions framework (RED; Ikuta et al., 2014).
",3 Semantic Content,0,[0]
Predicates and Arguments.,3 Semantic Content,0,[0]
"While predicateargument relations are universally recognized as fundamental to semantic representation, the interpretation of the terms varies across schemes.",3 Semantic Content,0,[0]
"Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered.",3 Semantic Content,0,[0]
"For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives.",3 Semantic Content,0,[0]
"FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”.",3 Semantic Content,0,[0]
"Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015).
",3 Semantic Content,0,[0]
Core and Non-core Arguments.,3 Semantic Content,0,[0]
"Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003).",3 Semantic Content,0,[0]
"While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core).",3 Semantic Content,0,[0]
"For example, FrameNet defines core arguments as conceptually necessary components of a frame, that make the frame unique and different from other frames, and peripheral arguments as those that introduce additional, independent or distinct relations from that of the frame such as time, place, manner, means and degree (Ruppenhofer et al., 2016, pp. 23-24).
",3 Semantic Content,0,[0]
Semantic Roles.,3 Semantic Content,0,[0]
Semantic roles are categories of arguments.,3 Semantic Content,0,[0]
"Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that
evoke the same frame type, such as “leave” and “depart”), and PropBank (where roles are verbspecific).",3 Semantic Content,0,[0]
PropBank’s role sets were extended by subsequent projects such as AMR.,3 Semantic Content,0,[0]
"Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and INSTRUMENT) that apply to all predicate arguments.
",3 Semantic Content,0,[0]
Co-reference and Anaphora.,3 Semantic Content,0,[0]
"Co-reference allows to abstract away from the different ways to refer to the same entity, and is commonly included in semantic resources.",3 Semantic Content,0,[0]
"Coreference interacts with argument structure annotation, as in its absence each argument is arbitrarily linked to one of its textual instances.",3 Semantic Content,0,[0]
"Most SRL schemes would mark “Ann” in (1) as an argument of “leaving” and “she” as an argument of “gave”, although on semantic grounds “Ann” is an argument of both.
",3 Semantic Content,0,[0]
"Some SRTs distinguish between the cases of argument sharing which is encoded by the syntax and is thus explicit (e.g., in “John went home and took a shower”, “John” is both an argument of “went home” and of “took a shower”), and cases where the sharing of arguments is inferred (as in (1)).",3 Semantic Content,0,[0]
"This distinction may be important for text understanding, as the inferred cases tend to be more ambiguous (“she” in (1) might not refer to “Ann”).",3 Semantic Content,0,[0]
"Other schemes, such as AMR, eschew this distinction and use the same terms to represent all cases of coreference.
",3 Semantic Content,0,[0]
Temporal Relations.,3 Semantic Content,0,[0]
"Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time.",3 Semantic Content,0,[0]
"Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013).",3 Semantic Content,0,[0]
"A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010).",3 Semantic Content,0,[0]
"For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order.
",3 Semantic Content,0,[0]
"Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications,
including planning and entailment.",3 Semantic Content,0,[0]
"See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types.",3 Semantic Content,0,[0]
"Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation.
",3 Semantic Content,0,[0]
The internal temporal structure of events has been less frequently tackled.,3 Semantic Content,0,[0]
"Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”).",3 Semantic Content,0,[0]
"Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010).",3 Semantic Content,0,[0]
"Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP.
",3 Semantic Content,0,[0]
Spatial Relations.,3 Semantic Content,0,[0]
"The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or robotic navigation.",3 Semantic Content,0,[0]
"Important tasks in this field include Spatial Role Labeling (Kordjamshidi et al., 2012) and the more recent SpaceEval (Pustejovsky et al., 2015).",3 Semantic Content,0,[0]
"The tasks include the identification and classification of spatial elements and relations, such as places, paths, directions and motions, and their relative configuration.
",3 Semantic Content,0,[0]
Discourse Relations encompass any semantic relation between events or larger semantic units.,3 Semantic Content,0,[0]
"For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type CONCESSION, evoked by “although”.",3 Semantic Content,0,[0]
"Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012).
",3 Semantic Content,0,[0]
"The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like TEMPORAL, COMPARISON and CONTINGENCY and finer-grained ones such as JUSTIFICATION and EXCEPTION.",3 Semantic Content,0,[0]
"Another commonly used resource is the RST Discourse Tree-
bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure.
",3 Semantic Content,0,[0]
"Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units categorized either according to their topic or according to their function within the text.",3 Semantic Content,0,[0]
"An example is the segmentation of scientific papers into functional segments and their labeling with categories such as BACKGROUND and DISCUSSION (Liakata et al., 2010).",3 Semantic Content,0,[0]
"See (Webber et al., 2011) for a survey of discourse structure in NLP.
Discourse relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them.",3 Semantic Content,0,[0]
"This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3
Logical Structure.",3 Semantic Content,0,[0]
"Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well.",3 Semantic Content,0,[0]
"Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013).",3 Semantic Content,0,[0]
"Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013).
",3 Semantic Content,0,[0]
Inference and Entailment.,3 Semantic Content,0,[0]
A primary motivation for many semantic schemes is their ability to support inference and entailment.,3 Semantic Content,0,[0]
"Indeed, means for predicting logical entailment are built into many forms of semantic representations.",3 Semantic Content,0,[0]
"A different approach was taken in the tasks of Recognizing Textual Entailment (Dagan et al., 2013), and Natural Logic (van Eijck, 2005), which considers an inference valid if a reasonable annotator would find the hypothesis likely to hold given
3AMR will also support discourse structure in its future versions (N. Schneider; personal communication).
",3 Semantic Content,0,[0]
"the premise, even if it cannot be deduced from it.",3 Semantic Content,0,[0]
"See (Manning, 2006) for a discussion of this point.",3 Semantic Content,0,[0]
"Such inference relations are usually not included in semantic treebanks, but annotated in specialized resources (e.g., Dagan et al., 2006; Bowman et al., 2015).",3 Semantic Content,0,[0]
This section briefly surveys the different schemes and resources for SRT.,4 Semantic Schemes and Resources,0,[0]
"We focus on design principles rather than specific features, as the latter are likely to change as the schemes undergo continuous development.",4 Semantic Schemes and Resources,0,[0]
"In general, schemes discussed in Section 3 are not repeated here.
",4 Semantic Schemes and Resources,0,[0]
Semantic Role Labeling.,4 Semantic Schemes and Resources,0,[0]
"SRL schemes diverge in their event types, the type of predicates they cover, their granularity, their cross-linguistic applicability, their organizing principles and their relation with syntax.",4 Semantic Schemes and Resources,0,[0]
"Most SRL schemes define their annotation relative to some syntactic structure, such as parse trees of the PTB in the case of PropBank, or specialized syntactic categories defined for SRL purposes in the case of FrameNet.",4 Semantic Schemes and Resources,0,[0]
"Other than PropBank, FrameNet and VerbNet discussed above, other notable resources include Semlink (Loper et al., 2007) that links corresponding entries in different resources such as PropBank, FrameNet, VerbNet and WordNet, and the Preposition Supersenses project (Schneider et al., 2015), which focuses on roles evoked by prepositions.",4 Semantic Schemes and Resources,0,[0]
"See (Palmer et al., 2010, 2013) for a review of SRL schemes and resources.",4 Semantic Schemes and Resources,0,[0]
"SRL schemes are often termed “shallow semantic analysis” due to their focus on argument structure, leaving out other relations such as discourse events, or how predicates and arguments are internally structured.
AMR.",4 Semantic Schemes and Resources,0,[0]
"AMR covers predicate-argument relations, including semantic roles (adapted from PropBank) that apply to a wide variety of predicates (including verbal, nominal and adjectival predicates), modifiers, co-reference, named entities and some time expressions.
",4 Semantic Schemes and Resources,0,[0]
"AMR does not currently support relations above the sentence level, and is admittedly Englishcentric, which results in an occasional conflation of semantic phenomena that happen to be similarly realized in English, into a single semantic category.",4 Semantic Schemes and Resources,0,[0]
"AMR thus faces difficulties when assessing the invariance of its structures across translations (Xue et al., 2014).",4 Semantic Schemes and Resources,0,[0]
"As an example,
consider the sentences “I happened to meet Jack in the office”, and “I asked to meet Jack in the office”.",4 Semantic Schemes and Resources,0,[0]
"While the two have similar syntactic forms, the first describes a single “meeting” event, where “happened” is a modifier, while the second describes two distinct events: asking and meeting.",4 Semantic Schemes and Resources,0,[0]
"AMR annotates both in similar terms, which may be suitable for English, where aspectual relations are predominantly expressed as subordinating verbs (e.g., “begin”, “want”), and are syntactically similar to primary verbs that take an infinitival complement (such as “ask to meet” or “learn to swim”).",4 Semantic Schemes and Resources,0,[0]
"However, this approach is less suitable cross-linguistically.",4 Semantic Schemes and Resources,0,[0]
"For instance, when translating the sentences to German, the divergence between the semantics of the two sentences is clear: in the first “happened” is translated to an adverb: “Ich habe Jack im Büro zufällig getroffen” (lit.",4 Semantic Schemes and Resources,0,[0]
"“I have Jack in-the office by-chance met”), and in the second “asked” is translated to a verb: “Ich habe gebeten, Jack im Büro zu treffen” (lit.",4 Semantic Schemes and Resources,0,[0]
"“I have asked, Jack in-the office to meet”).
",4 Semantic Schemes and Resources,0,[0]
UCCA.,4 Semantic Schemes and Resources,0,[0]
"UCCA (Universal Conceptual Cognitive Annotation) (Abend and Rappoport, 2013a,b) is a cross-linguistically applicable scheme for semantic annotation, building on typological theory, primarily on Basic Linguistic Theory (Dixon, 2010).",4 Semantic Schemes and Resources,0,[0]
UCCA’s foundational layer of categories focuses on argument structures of various types and relations between them.,4 Semantic Schemes and Resources,0,[0]
"In its current state, UCCA is considerably more coarse-grained than the above mentioned schemes (e.g., it does not include semantic role information).",4 Semantic Schemes and Resources,0,[0]
"However, its distinctions tend to generalize well across languages (Sulem et al., 2015).",4 Semantic Schemes and Resources,0,[0]
"For example, unlike AMR, it distinguishes between primary and aspectual verbs, so cases such as “happened to meet” are annotated similarly to cases such as “met by chance”, and differently from “asked to meet”.
",4 Semantic Schemes and Resources,0,[0]
Another design principle UCCA evokes is support for annotation by non-experts.,4 Semantic Schemes and Resources,0,[0]
To do so the scheme reformulates some of the harder distinctions into more intuitive ones.,4 Semantic Schemes and Resources,0,[0]
"For instance, the core/non-core distinction is replaced in UCCA with the distinction between pure relations (Adverbials) and those evoking an object (Participants), which has been found easier for annotators to apply.
",4 Semantic Schemes and Resources,0,[0]
UDS.,4 Semantic Schemes and Resources,0,[0]
"Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno-
tation, word senses and aspectual classes (e.g., realis/irrealis).",4 Semantic Schemes and Resources,0,[0]
"UDS emphasizes accessible distinctions, which can be collected through crowd-sourcing.",4 Semantic Schemes and Resources,0,[0]
"However, the skeletal structure of UDS representations is derived from syntactic dependencies, and only includes verbal argument structures that can be so extracted.",4 Semantic Schemes and Resources,0,[0]
"Notably, many of the distinctions in UDS are defined using feature bundles, rather than mutually exclusive categories.",4 Semantic Schemes and Resources,0,[0]
"For instance, a semantic role may be represented as having the features +VOLITION",4 Semantic Schemes and Resources,0,[0]
and,4 Semantic Schemes and Resources,0,[0]
#NAME?,4 Semantic Schemes and Resources,0,[0]
"The Prague Dependency Treebank (PDT) Tectogrammatical Layer (PDT-TL) (Sgall, 1992; Böhmová et al., 2003) covers a rich variety of functional and semantic distinctions, such as argument structure (including semantic roles), tense, ellipsis, topic/focus, co-reference, word sense disambiguation and local discourse information.",4 Semantic Schemes and Resources,0,[0]
"The PDT-TL results from an abstraction over PDT’s syntactic layers, and its close relation with syntax is apparent.",4 Semantic Schemes and Resources,0,[0]
"For instance, the PDT-TL encodes the distinction between a governing clause and a dependent clause, which is primarily syntactic in nature, so in the clauses “John came just as we were leaving” and “We were leaving just as John came” the governing and dependent clause are swapped, despite their semantic similarity.
",4 Semantic Schemes and Resources,0,[0]
CCG-based Schemes.,4 Semantic Schemes and Resources,0,[0]
"CCG (Steedman, 2000) is a lexicalized grammar (i.e., nearly all semantic content is encoded in the lexicon), which defines a theory of how lexical information is composed to form the meaning of phrases and sentences (see Section 6.2), and has proven effective in a variety of semantic tasks (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2013, inter alia).",4 Semantic Schemes and Resources,0,[0]
Several projects have constructed logical representations by associating CCG with semantic forms (by assigning logical forms to the leaves).,4 Semantic Schemes and Resources,0,[0]
"For example, Boxer (Bos, 2008) and GMB, which builds on Boxer, use Discourse Representation Structures (Kamp and Reyle, 1993), while Lewis and Steedman (2013) used Davidsonian-style λ-expressions, accompanied by lexical categorization of the predicates.",4 Semantic Schemes and Resources,0,[0]
"These schemes encode events with their argument structures, and include an elaborate logical structure, as well as lexical and discourse information.
",4 Semantic Schemes and Resources,0,[0]
HPSG-based Schemes.,4 Semantic Schemes and Resources,0,[0]
"Related to CCG-based schemes are SRTs based on Head-driven Phrase
Structure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units.",4 Semantic Schemes and Resources,0,[0]
"HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism.",4 Semantic Schemes and Resources,0,[0]
"Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger, 2005, inter alia), and generally focus on argument structural and logical semantic phenomena.",4 Semantic Schemes and Resources,0,[0]
"The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepen et al., 2014, 2015) include corpora annotated with the PDT-TL, and dependencies extracted from the HPSG grammars Enju (Miyao, 2006) and the LinGO English Reference Grammar (ERG; Flickinger, 2002).
",4 Semantic Schemes and Resources,0,[0]
"Like the PDT-TL, projects based on CCG, HPSG, and other expressive grammars such as LTAG (Joshi and Vijay-Shanker, 1999) and LFG (Kaplan and Bresnan, 1982) (e.g., GlueTag (Frank and van Genabith, 2001)), yield semantic representations that are coupled with syntactic ones.",4 Semantic Schemes and Resources,0,[0]
"While this approach provides powerful tools for inference, type checking, and mapping into external formal languages, it also often results in difficulties in abstracting away from some syntactic details.",4 Semantic Schemes and Resources,0,[0]
"For instance, the dependencies derived from ERG in the SDP corpus use the same label for different senses of the English possessive construction, regardless of whether they correspond to ownership (e.g., “John’s dog”) or to a different meaning, such as marking an argument of a nominal predicate (e.g., “John’s kick”).",4 Semantic Schemes and Resources,0,[0]
"See Section 6.
OntoNotes is a useful resource with multiple inter-linked layers of annotation, borrowed from different schemes.",4 Semantic Schemes and Resources,0,[0]
"The layers include syntactic, SRL, co-reference and word sense disambiguation content.",4 Semantic Schemes and Resources,0,[0]
"Some properties of the predicate, such as which nouns are eventive, are encoded as well.
",4 Semantic Schemes and Resources,0,[0]
"To summarize, while SRT schemes differ in the types of content they support, schemes evolve to continuously add new content types, making these differences less consequential.",4 Semantic Schemes and Resources,0,[0]
The fundamental difference between the schemes is the extent that they abstract away from syntax.,4 Semantic Schemes and Resources,0,[0]
"For instance, AMR and UCCA abstract away from syntax as part of their design, while in most other schemes syntax and semantics are more tightly coupled.
",4 Semantic Schemes and Resources,0,[0]
Schemes also differ in other aspects discussed in Sections 5 and 6.,4 Semantic Schemes and Resources,0,[0]
Human evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker.,5 Evaluation,0,[0]
"Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT.",5 Evaluation,0,[0]
"Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation.",5 Evaluation,0,[0]
"Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets.
",5 Evaluation,0,[0]
Another evaluation approach is task-based evaluation.,5 Evaluation,0,[0]
"Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural.",5 Evaluation,0,[0]
"For instance, a major motivation for AMR is its applicability to machine translation, making MT a natural (albeit hitherto unexplored) testbed for AMR evaluation.",5 Evaluation,0,[0]
"Another example is using question answering to evaluate semantic parsing into knowledge-base queries.
",5 Evaluation,0,[0]
"Another common criterion for evaluating a semantic scheme is invariance, where semantic analysis should be similar across paraphrases or translation pairs (Xue et al., 2014; Sulem et al., 2015).",5 Evaluation,0,[0]
"For instance, most SRL schemes abstract away from the syntactic divergence between the sentences (1) “He gave a present to John” and (2) “It was John who was given a present” (although a complete analysis would reflect the difference of focus between them).
",5 Evaluation,0,[0]
"Importantly, these evaluation criteria also apply in cases where the representation is automatically induced, rather than manually defined.",5 Evaluation,0,[0]
"For instance, vector space representations are generally evaluated either through task-based evaluation, or in terms of semantic features computed from them, whose validity is established by human annotators (e.g., Agirre et al., 2013, 2014).
",5 Evaluation,0,[0]
"Finally, where semantic schemes are induced through manual annotation (and not through au-
tomated procedures), a common criterion for determining whether the guidelines are sufficiently clear, and whether the categories are well-defined is to measure agreement between annotators, by assigning them the same texts and measuring the similarity of the resulting structures.",5 Evaluation,0,[0]
"Measures include the SMATCH measure for AMR (Cai and Knight, 2013), and the PARSEVAL F-score (Black et al., 1991) adapted for DAGs for UCCA.
",5 Evaluation,0,[0]
SRT schemes diverge in the background and training they require from their annotators.,5 Evaluation,0,[0]
"Some schemes require extensive training (e.g., AMR), while others can be (at least partially) collected by crowdsourcing (e.g., UDS).",5 Evaluation,0,[0]
"Other examples include FrameNet, which requires expert annotators for creating new frames, but employs less trained in-house annotators for applying existing frames to texts; QASRL, which employs non-expert annotators remotely; and UCCA, which uses inhouse non-experts, demonstrating no advantage to expert over non-expert annotators after an initial training period.",5 Evaluation,0,[0]
"Another approach is taken by GMB, which uses online collaboration where expert collaborators participate in manually correcting automatically created representations.",5 Evaluation,0,[0]
"They further employ gamification strategies for collecting some aspects of the annotation.
Universality.",5 Evaluation,0,[0]
One of the great promises of semantic analysis (over more surface forms of analysis) is its cross-linguistic potential.,5 Evaluation,0,[0]
"However, while the theoretical and applicative importance of universality in semantics has long been recognized (Goddard, 2011), the nature of universal semantics remains unknown.",5 Evaluation,0,[0]
"Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4, constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP.",5 Evaluation,0,[0]
"However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics.
",5 Evaluation,0,[0]
"Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Padó and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages.",5 Evaluation,0,[0]
"Translated versions of PropBank and FrameNet have been constructed for multiple languages (e.g., Akbik et al., 2016; Hartmann and Gurevych, 2013).",5 Evaluation,0,[0]
"How-
",5 Evaluation,0,[0]
"4http://compling.hss.ntu.edu.sg/omw/
ever, as both PropBank and FrameNet are lexicalized schemes, and as lexicons diverge wildly across languages, these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013).",5 Evaluation,0,[0]
"Ongoing research tackles the generalization of VerbNet’s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015).",5 Evaluation,0,[0]
"Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory.
",5 Evaluation,0,[0]
"Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016).",5 Evaluation,0,[0]
"However, further evaluation is required in order to determine what aspects of meaning these representations reflect reliably.",5 Evaluation,0,[0]
"Syntactic distinctions are generally guided by a combination of semantic and distributional considerations, where emphasis varies across schemes.
",6.1 Syntactic and Semantic Generalization,0,[0]
"Consider phrase-based syntactic structures, common examples of which, such as the Penn Treebank for English (Marcus et al., 1993) and the Penn Chinese Treebank (Xue et al., 2005), are adaptations of X-bar theory.",6.1 Syntactic and Semantic Generalization,0,[0]
"Constituents are commonly defined in terms of distributional criteria, such as whether they can serve as conjuncts, be passivized, elided or fronted (Carnie, 2002, pp. 50-53).",6.1 Syntactic and Semantic Generalization,0,[0]
"Moreover, phrase categories are defined according to the POS category of their headword, such as Noun Phrase, Verb Phrase or Preposition Phrase, which are also at least partly distributional, motivated by their similar morphological and syntactic distribution.",6.1 Syntactic and Semantic Generalization,0,[0]
"In contrast, SRT schemes tend to abstract away from these realizational differences and directly reflect the argument structure of the sentence using the same set of categories, irrespective of the POS of the predicate, or the case marking of its arguments.
",6.1 Syntactic and Semantic Generalization,0,[0]
"Distributional considerations are also apparent with functional syntactic schemes (the most commonly used form of which in NLP are lexicalist dependency structures), albeit to a lesser extent.",6.1 Syntactic and Semantic Generalization,0,[0]
"A prominent example is Universal Dependencies (UD; Nivre et al., 2016), which aims at produc-
ing a cross-linguistically consistent dependencybased annotation, and whose categories are motivated by a combination of distributional and semantic considerations.",6.1 Syntactic and Semantic Generalization,0,[0]
"For example, UD would distinguish between the dependency type between “John” and “brother” in “John, my brother, arrived” and “John, who is my brother, arrived”, despite their similar semantics.",6.1 Syntactic and Semantic Generalization,0,[0]
"This is due to the former invoking an apposition, and the latter a relative clause, which are different in their distribution.
",6.1 Syntactic and Semantic Generalization,0,[0]
"As an example of the different categorization employed by UD and by purely semantic schemes such as AMR and UCCA consider (1) “founding of the school”, (2) “president of the United States” and (3) “United States president”.",6.1 Syntactic and Semantic Generalization,0,[0]
"UD is faithful to the syntactic structure and represents (1) and (2) similarly, while assigning a different structure to (3).",6.1 Syntactic and Semantic Generalization,0,[0]
"In contrast, AMR and UCCA perform a semantic generalization and represents examples (2) and (3) similarly and differently from (1).",6.1 Syntactic and Semantic Generalization,0,[0]
"A common assumption on the interface between syntax and semantics is that semantics of phrases and sentences is compositional – it is determined recursively by the meaning of its immediate constituents and their syntactic relationships, which are generally assumed to form a closed set (Montague, 1970, and much subsequent work).",6.2 The Syntax-Semantics Interface,0,[0]
"Thus, the interpretation of a sentence can be computed bottom-up, by establishing the meaning of individual words, and recursively composing them, to obtain the full sentential semantics.",6.2 The Syntax-Semantics Interface,0,[0]
"The order and type of these compositions are determined by the syntactic structure.
",6.2 The Syntax-Semantics Interface,0,[0]
"Compositionality is employed by linguistically expressive grammars, such as those based on CCG and HPSG, and has proven to be a powerful method for various applications.",6.2 The Syntax-Semantics Interface,0,[0]
"See (Bender et al., 2015) for a recent discussion of the advantages of compositional SRTs.",6.2 The Syntax-Semantics Interface,0,[0]
"Nevertheless, a compositional account meets difficulties when faced with multi-word expressions and in accounting for cases like “he sneezed the napkin off the table”, where it is difficult to determine whether “sneezed” or “off” account for the constructional meaning.",6.2 The Syntax-Semantics Interface,0,[0]
"Construction Grammar (Fillmore et al., 1988; Goldberg, 1995) answers these issues by using an open set of construction-specific compositional operators, and supporting lexical en-
tries of varying lengths.",6.2 The Syntax-Semantics Interface,0,[0]
"Several ongoing projects address the implementation of the principles of Construction Grammar into explicit grammars, including Sign-based Construction Grammar (Fillmore et al., 2012), Embodied Construction Grammar (Feldman et al., 2010) and Fluid Construction Grammar (Steels and de Beules, 2006).
",6.2 The Syntax-Semantics Interface,0,[0]
"The achievements of machine learning methods in many areas, and optimism as to its prospects, have enabled the approaches to semantics discussed in this paper.",6.2 The Syntax-Semantics Interface,0,[0]
Machine learning allows to define semantic structures on purely semantic grounds and to let algorithms identify how these distinctions are mapped to surface/distributional forms.,6.2 The Syntax-Semantics Interface,0,[0]
"Some of the schemes discussed in this paper take this approach in its pure form (e.g., AMR and UCCA).",6.2 The Syntax-Semantics Interface,0,[0]
Semantic representation in NLP is undergoing rapid changes.,7 Conclusion,0,[0]
"Traditional semantic work has either used shallow methods that focus on specific semantic phenomena, or adopted formal semantic theories which are coupled with a syntactic scheme through a theory of the syntax-semantics interface.",7 Conclusion,0,[0]
"Recent years have seen increasing interest in an alternative approach that defines semantic structures independently from any syntactic or distributional criteria, much due to the availability of semantic treebanks that implement this approach.
",7 Conclusion,0,[0]
"Semantic schemes diverge in whether they are anchored in the words and phrases of the text (e.g., all types of semantic dependencies and UCCA) or not (e.g., AMR and logic-based representations).",7 Conclusion,0,[0]
"We do not view this as a major difference, because most unanchored representations (including AMR) retain their close affinity with the words of the sentence, possibly because of the absence of a workable scheme for lexical decomposition, while dependency structures can be converted into logic-based representations (Reddy et al., 2016).",7 Conclusion,0,[0]
"In practice, anchoring facilitates parsing, while unanchored representations are more flexible to use where words and semantic components are not in a one-to-one correspondence.
",7 Conclusion,0,[0]
"Our survey concludes that the main distinguishing factors between schemes are their relation to syntax, their degree of universality, and the expertise and training they require from annotators, an important factor in addressing the annotation bottleneck.",7 Conclusion,0,[0]
"We hope this survey of the state of the art in semantic representation will promote discus-
sion, expose more researchers to the most pressing questions in semantic representation, and lead to the wide adoption of the best components from each scheme.
Acknowledgements.",7 Conclusion,0,[0]
We thank Nathan Schneider for his helpful comments.,7 Conclusion,0,[0]
The work was support by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).,7 Conclusion,0,[0]
"Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth.",abstractText,0,[0]
"Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation.",abstractText,0,[0]
We address these gaps by critically surveying the state of the art in the field.,abstractText,0,[0]
The State of the Art in Semantic Representation,title,0,[0]
The analysis of sequential data has long been a staple in machine learning.,1. Introduction,0,[0]
"Domain areas like natural language (Zaremba et al., 2014; Vinyals et al., 2015), speech (Graves et al., 2013; Graves & Jaitly, 2014), music (Chung et al., 2014), and video (Donahue et al., 2015) processing have recently garnered much attention.",1. Introduction,0,[0]
"While the study of sequences itself is broad and may be extended to general functional analysis (Ramsay & Silverman, 2002), most recent success has been from neural network based models, especially from recurrent architectures.
",1. Introduction,0,[0]
Recurrent networks are dynamical systems that represent time recursively.,1. Introduction,0,[0]
"For example, the simple recurrent unit (Elman, 1990) contains a hidden state that itself depends on the previous hidden state.",1. Introduction,0,[0]
"However, training such networks has been observed to be difficult in practice due to exploding and vanishing gradients when propagating error gradients through time (Hochreiter et al., 2001).",1. Introduction,0,[0]
"While explod-
1Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA.",1. Introduction,0,[0]
"Correspondence to: Junier B. Oliva <joliva@cs.cmu.edu>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
",1. Introduction,0,[0]
"ing gradients can be mitigated with techniques like gradient clipping and normalization (Pascanu et al., 2013), vanishing gradients may be harder to deal with.",1. Introduction,0,[0]
"As a result, sophisticated gated architectures like Long-Short Term Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Unit (GRU) networks (Cho et al., 2014) have been developed.",1. Introduction,0,[0]
"These gated architectures contain “memory cells” along with gates to control how much they decay through time thereby aiding the networks’ ability to learn long term dependencies in sequences.
",1. Introduction,0,[0]
"Notwithstanding, there are still challenges in capturing long term dependencies in gated architectures (Le et al., 2015).",1. Introduction,0,[0]
"In this paper we present a simple un-gated architecture, the Statistical Recurrent Unit, that often outperforms these more complicated alternatives.",1. Introduction,0,[0]
"Although the SRU keeps only simple moving averages of summary statistics, its novel architecture makes it more adept than previous gated units for capturing long term information in sequences and comparing them across different windows of time.",1. Introduction,0,[0]
"For instance, the SRU, unlike traditional recurrent units, can obtain a multitude of viewpoints of the past by simple linear combinations of only a few averages.",1. Introduction,0,[0]
"We shall illustrate the efficacy of the SRU below using both real-world and synthetic sequential data tasks.
",1. Introduction,0,[0]
"The structure of the paper is as follows: first we detail the architecture of the SRU as well as provide several key intuitions and insights for its design; after, we describe our experiments comparing the SRU to popular gated alternatives, and we perform a “dissective” study of the SRU, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, we discuss conclusions from our study.",1. Introduction,0,[0]
The SRU maintains long term sequential dependencies in a rather intuitive fashion–through summary statistics.,2. Model,0,[0]
"As the name implies, statisticians often employ summary statistics when trying to represent a dataset.",2. Model,0,[0]
"Quite naturally then, we look to an algorithm that itself learns to represent data seen previously in much the same vein as a neural statistician (Edwards & Storkey, 2016).
",2. Model,0,[0]
"Of course, unlike with unordered i.i.d.",2. Model,0,[0]
"samples, simply averaging statistics of sequential points will lose valuable
temporal information.",2. Model,0,[0]
"The SRU maintains sequential information in two ways: first, we generate recurrent statistics that depend on a context of previously seen data; second, we generate moving averages at several scales, allowing the model to distinguish the type of data seen at different points in the past.",2. Model,0,[0]
"We expound on these methods for creating temporally-aware statistics below.
",2. Model,0,[0]
"We shall see that the statistical design of the SRU yields a powerful yet simple model that is able to analyze sequential data and, on the fly, create summary statistics for learning over sequences.",2. Model,0,[0]
"Furthermore, through the use of ReLUs and exponential moving averages, the SRU is able to mitigate vanishing gradient issues that are common to many recurrent units.",2. Model,0,[0]
"We consider an input sequence of real valued points x1, x2, . . .",2.1. Recurrent Statistics,0,[0]
", xT ∈",2.1. Recurrent Statistics,0,[0]
Rd.,2.1. Recurrent Statistics,0,[0]
"As seen in the second row of Table 1, we can compute a vector of statistics φ(xi) ∈ RD for each point.",2.1. Recurrent Statistics,0,[0]
"Here, each vector φ(xi) is independent of other points xj for j 6=",2.1. Recurrent Statistics,0,[0]
i. One may then average these vectors as µ = 1T ∑T i=1 φ(xi) to produce summary statistics of the sequence.,2.1. Recurrent Statistics,0,[0]
This approach amounts to treating the sequence as a set of i.i.d.,2.1. Recurrent Statistics,0,[0]
points drawn form some distribution and marginalizing out time.,2.1. Recurrent Statistics,0,[0]
"Clearly, here one will lose temporal information that will be useful for many sequence related ML tasks.",2.1. Recurrent Statistics,0,[0]
"It is interesting to note that global average pooling operations have gained a lot of recent traction in convolutional networks (Lin et al., 2013; Iandola et al., 2016).",2.1. Recurrent Statistics,0,[0]
"Analogously to the i.i.d. statistic approach, global averaging will lose spatial information, yet the high-level summary statistics provide an effective representation.",2.1. Recurrent Statistics,0,[0]
"Still, not marginalizing out time should provide a more robust approach for sequence tasks, thus we consider the following methods for producing statistics.
",2.1. Recurrent Statistics,0,[0]
"First, we provide temporal information whilst still utilizing averages through recurrent statistics that also depend on the values of previous points (see third row of Table 1).",2.1. Recurrent Statistics,0,[0]
"That is, we compute our statistics on the ith point xi not only as a function of xi, but also as a function of the previous statistics of xi−1, ~γi−1",2.1. Recurrent Statistics,0,[0]
"(which itself depends on ~γi−2, etc.):
~γ1",2.1. Recurrent Statistics,0,[0]
"= γ(x1, ~γ0), ~γ2 = γ(x2, ~γ1), . . .",2.1. Recurrent Statistics,0,[0]
"(1)
where γ(·, ·) is a function for producing statistics given the current point and previous statistics, and ~γ0 is a constant initial vector for convention.",2.1. Recurrent Statistics,0,[0]
"We note that from a general standpoint if given a flexible model and enough dimensions, then recurrent summary statistics like (1) can perfectly encode ones sequence.",2.1. Recurrent Statistics,0,[0]
"Take for instance the follow-
ing illustrative example where xi ∈ R+ and statistics
~γi = (0, . . .",2.1. Recurrent Statistics,0,[0]
", 0, Txi, 0, . . .)",2.1. Recurrent Statistics,0,[0]
"(2) ~γi+1 = (0, . . .",2.1. Recurrent Statistics,0,[0]
", 0, 0, Txi+1, 0, . . .)",2.1. Recurrent Statistics,0,[0]
.,2.1. Recurrent Statistics,0,[0]
"(3)
That is, one records the ith input in the ith index.",2.1. Recurrent Statistics,0,[0]
When averaged the statistics will be 1T ∑T i=1,2.1. Recurrent Statistics,0,[0]
~γi,2.1. Recurrent Statistics,0,[0]
"= (x1, x2, . . .), i.e. the complete sequence.",2.1. Recurrent Statistics,0,[0]
Such recurrent statistics will undoubtedly suffer from the curse of dimensionality.,2.1. Recurrent Statistics,0,[0]
"Hence, we consider a more restrictive model of recurrent statistics which we expound on below (6).
",2.1. Recurrent Statistics,0,[0]
"Second, we provide even more temporal information by considering summary statistics at multiple scales.",2.1. Recurrent Statistics,0,[0]
"As a simple hypothetical example, consider taking multiple means across separate time windows (for instance taking means over indices 1-10, then over indices 11-20, etc.).",2.1. Recurrent Statistics,0,[0]
"Such an approach (4) will illustrate how summary statistics evolve through time.
φ1, . . .",2.1. Recurrent Statistics,0,[0]
", φ10︸ ︷︷ ︸ µ1:10 , φ11, . . .",2.1. Recurrent Statistics,0,[0]
", φ20︸ ︷︷ ︸ µ11:20 , . . . .",2.1. Recurrent Statistics,0,[0]
"(4)
In practice, we shed light on the dynamics of statistics through time by using several averages of the same summary statistics.",2.1. Recurrent Statistics,0,[0]
"The SRU will use exponential moving averages µi = α~γi+(1−α)µi−1 to compute means; hence, we consider multiple weights by taking the exponential means at various scales α1, . . .",2.1. Recurrent Statistics,0,[0]
", αm as shown in the last row of Table 1.",2.1. Recurrent Statistics,0,[0]
Later we show that this multi-scaled approach is capable of a combinatorial number of viewpoints of past statistics through simple linear combinations.,2.1. Recurrent Statistics,0,[0]
We have discussed in broad terms how one may create temporally-aware summary statistics through multi-scaled recurrent statistics.,2.2. Update Equations,0,[0]
"Below, we cover specifically how the SRU creates and uses summary statistics for sequences.
",2.2. Update Equations,0,[0]
"Recall that our input is a sequence of ordered points: x1, x2, . . .",2.2. Update Equations,0,[0]
", xt ∈ Rd.",2.2. Update Equations,0,[0]
"Throughout, we apply an element-
wise non-linearity f (·), which we take to be the ReLU (Jarrett et al., 2009; Nair & Hinton, 2010): f (·) = max(·, 0).",2.2. Update Equations,0,[0]
"The SRU operates via exponential moving averages, µ(α) ∈ Rs (7), kept at various scales α ∈",2.2. Update Equations,0,[0]
"A = {α1, . . .",2.2. Update Equations,0,[0]
", αm}, where αi ∈",2.2. Update Equations,0,[0]
"[0, 1).",2.2. Update Equations,0,[0]
"These moving averages, µ(α), are of recurrent statistics ϕ",2.2. Update Equations,0,[0]
"(6) that are dependent not only on the current input but also on features of averages, r (5).",2.2. Update Equations,0,[0]
"The moving averages are then concatenated as µ = (µ(α1), . . .",2.2. Update Equations,0,[0]
", µ(αm))",2.2. Update Equations,0,[0]
"and used to create an output o (8) that is fed upwards in the network.
",2.2. Update Equations,0,[0]
"We detail the update equations for the SRU below (and in Figure 1):
rt = f",2.2. Update Equations,0,[0]
( W (r)µt−1 + b,2.2. Update Equations,0,[0]
"(r) )
(5) ϕt",2.2. Update Equations,0,[0]
"= f ( W (ϕ)rt +W (x)xt + b (ϕ) ) (6)
∀α ∈ A, µ(α)t = αµ (α) t−1 + (1− α)ϕt (7) ot = f ( W (o)µt + b (o) ) .",2.2. Update Equations,0,[0]
"(8)
In practiced we noted that it suffices to use only a few α’s such as A = {0, 0.25, 0.5, 0.9, 0.99}.
",2.2. Update Equations,0,[0]
It is worth noting that previous work has considered capturing recurrent information at various timescales in the past.,2.2. Update Equations,0,[0]
"For instance, Koutnik et al. (2014) considers an RNN scheme that divides the hidden state into different modules for use at different frequencies.",2.2. Update Equations,0,[0]
"Furthermore, exponential averages in recurrent units have been considered previously, e.g. (Mikolov et al., 2015; Bengio et al., 2013; Jaeger et al., 2007).",2.2. Update Equations,0,[0]
"However, such works are more akin to un-gated GRUs since they consider only one scale per feature, limiting the views available per statistic to just one.",2.2. Update Equations,0,[0]
"The use of ReLUs in recurrent units has also been recently explored by Le et al. (2015), however there no statistics are kept and their use is limited to the simple RNN when initialized in a special manner.",2.2. Update Equations,0,[0]
The design of the SRU is deliberately chosen to allow for long term dependencies to be learned.,2.3. Intuitions from Mean Map Embeddings,0,[0]
"To better elucidate the design and its intuition, let us take a brief excursion to another use of (summary) statistics in machine learning for the representation of data: mean map embeddings (MMEs) of distributions (Smola et al., 2007).",2.3. Intuitions from Mean Map Embeddings,0,[0]
"At its core, the concept of MMEs is that one may embed, and thereby represent, a distribution through statistics (such as moments).",2.3. Intuitions from Mean Map Embeddings,0,[0]
"The MME for a distributionD given a positive semidefinite kernel k is:
µ[D] = EX∼D",2.3. Intuitions from Mean Map Embeddings,0,[0]
"[φk(X)] , (9)
where φk are the reproducing kernel Hilbert space (RKHS) features of k, which may be infinite dimensional.",2.3. Intuitions from Mean Map Embeddings,0,[0]
"To represent a set Y = {y1, . . .",2.3. Intuitions from Mean Map Embeddings,0,[0]
", yn}
iid∼ D one would use an empirical mean version of the MME:
µ[Y ] = 1
n n∑ i=1 φk(yi).",2.3. Intuitions from Mean Map Embeddings,0,[0]
"(10)
Numerous works have shown success in representing distributions and sets through MMEs (Muandet et al., 2016).",2.3. Intuitions from Mean Map Embeddings,0,[0]
One interpretation for the design of SRUs is that we are modifying MME’s for use on sequences.,2.3. Intuitions from Mean Map Embeddings,0,[0]
"Of course, one way of applying MMEs directly on sequences is to simply ignore the non-i.i.d. nature of sequences and treat points as comprising a set.",2.3. Intuitions from Mean Map Embeddings,0,[0]
"This however loses important sequential information, as previously mentioned.",2.3. Intuitions from Mean Map Embeddings,0,[0]
Below we discuss the specific modifications we make from traditional MMEs and the benefits they yield.,2.3. Intuitions from Mean Map Embeddings,0,[0]
"First, we note the clear analogue between the mean embedding of a set Y , µ[Y ] (10), and the moving average µ(α) (7).",2.3.1. DATA-DRIVEN STATISTICS,0,[0]
The moving averages µ(α) are clearly serving as summary statistics of previously seen data.,2.3.1. DATA-DRIVEN STATISTICS,0,[0]
"However, the statistics we are averaging for µ(α), ϕ (6), are not comprised of apriori RKHS features as is typical with MMEs, but rather are learned non-linear features.",2.3.1. DATA-DRIVEN STATISTICS,0,[0]
"This has the benefit of using data-driven statistics, and may be interpreted as using a linear kernel in the learned features.",2.3.1. DATA-DRIVEN STATISTICS,0,[0]
"Second, recall that typical MMEs use statistics that depend only on a single point x, φk(x).",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
As aforementioned this is fine for i.i.d.,2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"data, however it loses sequential information when averaged.",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"Instead, we wish to assign statistics that depend on the data we have seen so far, since it provides context for one’s current point in the sequence.",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"For instance, one may want to have a statistic that keeps track of the difference between the current point and the mean
of previous data.",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"We provide a context based on previous data by making the statistics considered at time t, ϕt (6), a function not only of xt but also of {x1, . . .",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
", xt−1} through rt (5).",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"rt may be interpreted as a condensation of the sequence seen so far, and allows us to keep sequential information even through an averaging operation.",2.3.2. RECURSIVE STATISTICS FROM THE PAST,0,[0]
"Third, the use of multi-scaled moving averages of statistics gives the SRU a simple and powerful rich view of past data that is unique to this recurrent unit.",2.3.3. MULTI-SCALED STATISTICS,0,[0]
"In short, by keeping moving averages at different scales {α1, . . .",2.3.3. MULTI-SCALED STATISTICS,0,[0]
", αm}, we are able to uncover differences in statistics at various times in the past.",2.3.3. MULTI-SCALED STATISTICS,0,[0]
"Note that we may unroll moving averages as:
µ (α) t = (1− α) ( ϕt + αϕt−1 + α 2ϕt−2 + . . . )",2.3.3. MULTI-SCALED STATISTICS,0,[0]
"(11)
Thus, a smaller α weighs current statistics more than older statistics; hence, a concatenated vector µ = (µ(α1), . .",2.3.3. MULTI-SCALED STATISTICS,0,[0]
.,2.3.3. MULTI-SCALED STATISTICS,0,[0]
", µ(αm))",2.3.3. MULTI-SCALED STATISTICS,0,[0]
itself provides a multi-scale view of statistics through time (see Figure 2).,2.3.3. MULTI-SCALED STATISTICS,0,[0]
"For instance, keeping statistics for short and long terms pasts already yields information on the evolution of the sequence through time.",2.3.3. MULTI-SCALED STATISTICS,0,[0]
An interesting and useful property of keeping multiple scales for each statistic is that one can obtain a combinatorial number of viewpoints of the past through simple linear combinations of ones statistics.,2.4. Viewpoints of the Past,0,[0]
"For instance, for properly chosen wj , wk ∈ R, wjµ(αj)−wkµ(αk) provides an aggregate of statistics from the past for αj > αk (Figure 3).",2.4. Viewpoints of the Past,0,[0]
"Of course, more complicated linear combinations may be performed to obtain richer viewpoints that are comprised of multiple windows.",2.4. Viewpoints of the Past,0,[0]
"Furthermore, by using a linear projection of our statistics µt, as we do with ot (8), we are able to compute output features of combined viewpoints of several statistics.
",2.4. Viewpoints of the Past,0,[0]
"This kind of multi-viewpoint perspective of previously seen data is difficult to produce in traditional gated recurrent
units since they must encode where in the sequence they currently are and then store an activation on separate nodes per each viewpoint for future use.",2.4. Viewpoints of the Past,0,[0]
"SRUs, on the other hand, only need to take simple linear combinations to capture various viewpoints in the past.",2.4. Viewpoints of the Past,0,[0]
"For example, as shown above, statistics from just the distant past are available via a simple subtraction of two moving averages (Figure 3, row 1).",2.4. Viewpoints of the Past,0,[0]
"Such a windowed view would require a gated unit to learn to stop averaging after a certain point in the sequence, and the corresponding statistic would not yield any information outside of this window.",2.4. Viewpoints of the Past,0,[0]
"In contrast, each statistic kept by the SRU provides a combinatorial number of varying perspectives in the past through linear combinations and their multi-scaled nature.",2.4. Viewpoints of the Past,0,[0]
"As previously mentioned, it has been shown that vanishing gradients make learning recurrent units difficult due to an inability to propagate error gradients through time.",2.5. Vanishing Gradients,0,[0]
"Notwithstanding its simple un-gated structure, the SRU features several safeguards to alleviate vanishing gradients.",2.5. Vanishing Gradients,0,[0]
"First, units and statistics are comprised of ReLUs.",2.5. Vanishing Gradients,0,[0]
"ReLUs have been observed to be easier to train for general deep networks (Nair & Hinton, 2010) and have had success in recurrent units (Le et al., 2015).",2.5. Vanishing Gradients,0,[0]
"Intuitively, ReLUs allow for the propagation on error on positive inputs without saturation and vanishing gradients as with traditional sigmoid units.",2.5. Vanishing Gradients,0,[0]
"The ability of the SRU to use ReLUs (without any special initialization) makes it especially adept at learning long term dependencies through time.
",2.5. Vanishing Gradients,0,[0]
"Furthermore, the explicit moving average of statistics al-
lows for longer term learning.",2.5. Vanishing Gradients,0,[0]
Consider the following derivative of the error signal E w.r.t.,2.5. Vanishing Gradients,0,[0]
"an element [ µ (α) t−1 ] k of the unit’s moving averages when [ϕt]k = 0:
∂E ∂",2.5. Vanishing Gradients,0,[0]
[ µ (α) t−1 ],2.5. Vanishing Gradients,0,[0]
"k
= ∂",2.5. Vanishing Gradients,0,[0]
"[ µ (α) t ] k
∂ [ µ (α) t−1 ] k
∂E ∂",2.5. Vanishing Gradients,0,[0]
[ µ (α) t ] k = α ∂E ∂,2.5. Vanishing Gradients,0,[0]
"[ µ (α) t ] k .
",2.5. Vanishing Gradients,0,[0]
"That is, the factor α directly controls the decay of the error signal through time.",2.5. Vanishing Gradients,0,[0]
"Thus, by including an α explicitly near 1 (i.e. 0.999), the decay for that moving average can be made minuscule for the lengths of sequences in ones data.",2.5. Vanishing Gradients,0,[0]
"Also, it is interesting to note that, with a large α near 1, SRUs with ReLUs can implement part of the functionality of a gate (“remembering”) by carrying through the previous moving average [µ(α)t−1]k when the corresponding statistic [ϕt]k has be zeroed out (7).",2.5. Vanishing Gradients,0,[0]
"The other functionality of a gate (forgetting) can be had by including an α near 0; if the ReLU statistic is not zeroed out, then the moving average for a small α will “forget” the previous value.",2.5. Vanishing Gradients,0,[0]
"We compared the performance of the SRU1 to two popular gated recurrent units, the GRU and LSTM unit.",3. Experiments,0,[0]
"All experiments were performed in Tensorflow (Abadi et al., 2016) and used the standard implementations of GRUCell and BasicLSTMCell for GRUs and LSTMs respectively.",3. Experiments,0,[0]
"In order to perform a fair, unbiased comparison of the recurrent units and their hyper-parameters, which greatly affect performance (Bergstra & Bengio, 2012), we used the Hyperopt (Bergstra et al., 2015) hyper-parameter optimization package.",3. Experiments,0,[0]
"We believe that such an approach gives each algorithm a fair shot to succeed without injecting biases from experimenters or imposing gross restrictions on architectures considered.
",3. Experiments,0,[0]
"In all experiments we used SGD for optimization using gradient clipping (Pascanu et al., 2013) with a norm of 1 on all algorithms.",3. Experiments,0,[0]
"Unless otherwise specified 100 trials were performed to search over the following hyper-parameters on a validation set: one, initial learning rate the initial learning rate used for SGD, in range of [exp(−10), 1]; two, lr decay the multiplier to multiply the learning rate by every 1k iterations, in range of [0.8, 0.999]; three, dropout keep rate, percent of output units that are kept during dropout, in range (0, 1]; four, num units number of units for recurrent unit, in {1, . . .",3. Experiments,0,[0]
", 256}.",3. Experiments,0,[0]
"In addition, the following two parameters were searched over for the SRU: num stats, the dimensionality of ϕ (6), in {1, . . .",3. Experiments,0,[0]
", 256}; summary dims, the dimensionality of r (5), in {1, . . .",3. Experiments,0,[0]
", 64}.
1See https://github.com/junieroliva/ recurrent for code.",3. Experiments,0,[0]
First we provide evidence that traditional gated units have difficulties capturing the same type of multi-scale recurrent statistic based dependencies that the SRU offers.,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"We show the relative inefficiency of traditional gated units at learning long term dependencies of statistics by considering 1d synthetic data from a ground truth SRU.
",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"We begin the sequences with x1 iid∼ N (0, 1002), and xt is the results of a projection of ot.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"We generate a total of 176 points per sequence for 3200 training sequences, 400 validation sequences, and 400 testing sequences.
",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"The ground truth statistical recurrent unit has three statistics φt (6): the positive part of inputs (x)+, the negative part of inputs (x)−, and an internal statistic, z. We use α ∈ {αi}5i=1 = {0.0, 0.5, 0.9, 0.99, 0.999}.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"Denote µ (α) + , µ (α) − , µ (α) z",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
as the moving averages using α for each respective statistic,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
.,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"The internal statistic z does not get used (through rt (5)) in updating the statistics for (x)+ or (x)−. z is itself updated as:
zt = (zt−1)+ +",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
( µ (α4),3.1. Synthetic Recurrent Unit Generated Data,0,[0]
+,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
− µ (α5) + − 0.01 ) +,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
− ( −µ(α4)− + µ (α5) − − 0.01 ),3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"+
− ( −µ(α4)+ + µ (α5) + − 0.05 )",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
+,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
+ ( µ (α4),3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"− − µ (α5) − − 0.05 ) + ,
where each of the summands are rt features.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"Furthermore we have ot ∈ R15 (8):
ot = ( (xt)+, −(xt)−, vT1 µt, . . .",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
", vT13µt ) ,
where vj’s where initialized and fixed as (vj)k iid∼ N (0, ( 1100 ) 2).",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"Finally the next point is generated as:
xt+1 =",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"(xt)+ − (xt)− + wT ot,3:,
where w was initialized and fixed as (w)k iid∼ N (0, 1), and ot,3: are the last 13 dimensions of ot.
",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"After the ground truth SRU was constructed we generated the training, validation, and testing sequences.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"As can be seen in Figure 4, the sequences follow a simple pattern: at the start negative values are quickly pushed to zero and positive values follow a parabolic line until hitting zero, at which point they slope downward depending on initial values.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"While simple, it is clear that trained recurrent units must be able to hold long-term information since all sequences converge at one point and future behaviour depends on initial values.
",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"We look to minimize the mean of squared errors (MSE); that is, the loss we consider per sequence is 1 175 ∑175 t=1 |xt+1",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"− pt|2, where pt is the output of the network after being fed xt.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"We conducted 100 trials of hyperparameter optimization as described above and obtained the following results in Table 2.
",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"Not surprisingly, the SRU performs far better than traditional gated recurrent units.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
This suggests that the types of long-term statistical relationships captured by the SRU are indeed different than those of traditional recurrent units.,3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"As previously mentioned, the SRU is able to obtain a multitude of different views from its statistics, a task that traditional units achieve less efficiently since they must devote one whole memory cell per viewpoint and statistic.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
"As we show below, the SRU is able to outperform traditional gated units in long term problems even for real data that is not generated from its model class.",3.1. Synthetic Recurrent Unit Generated Data,0,[0]
Next we explore the ability of recurrent units to use longterm dependencies in ones data with a synthetic task using a real dataset.,3.2. MNIST Image Classification,0,[0]
"It has been observed that LSTMs perform poorly in classifying a long pixel-by-pixel sequence of MNIST digits (Le et al., 2015).",3.2. MNIST Image Classification,0,[0]
"In this synthetic task, each 28×28 gray-scale MNIST digit image is flattened and observed as a sequence {x1, . . .",3.2. MNIST Image Classification,0,[0]
", x784}, where xi ∈",3.2. MNIST Image Classification,0,[0]
"[0, 1] (see Figure 5).",3.2. MNIST Image Classification,0,[0]
"The task is, based on the output observed after feeding x784 through the network, to classify the digit of the corresponding image in {0, . . .",3.2. MNIST Image Classification,0,[0]
", 9}.",3.2. MNIST Image Classification,0,[0]
"Hence, we project the output after x784 of each recurrent unit to 10 dimensions and use a softmax activation.
",3.2. MNIST Image Classification,0,[0]
We report the hyper-parameter optimized results below in Table 3; due to resource constraints each trial consisted only of 10K training iterations.,3.2. MNIST Image Classification,0,[0]
We see that the SRU is able to out-perform both GRUs and LSTMs.,3.2. MNIST Image Classification,0,[0]
"Given the long length and dependencies of pixel sequences in this experiment, it is not surprising that SRUs’ abilities to capture long-term dependencies are aiding it to achieve a much lower error.",3.2. MNIST Image Classification,0,[0]
"Next, we study the behavior of the statistical recurrent unit with a dissective study where we vary several parameters of the architecture.",3.2.1. DISSECTIVE STUDY,0,[0]
We consider variants to a base model with: num stats=200; r dims=60; num units=200.,3.2.1. DISSECTIVE STUDY,0,[0]
"We keep the parameters initial learning rate, lr decay fixed at the the optimal values found (0.1, 0.99 respectively) unless we find no learning, in which case we also try learning rates of 0.01 and 0.001.
",3.2.1. DISSECTIVE STUDY,0,[0]
The need for multi-scaled recurrent statistics.,3.2.1. DISSECTIVE STUDY,0,[0]
Recall that we designed the statistics used by the SRU expressly to capture long term time dependencies in sequences.,3.2.1. DISSECTIVE STUDY,0,[0]
"We did so both with recurrent statistics, i.e. statistics that themselves depend on previous points’ statistics, and with multiscaled averages.",3.2.1. DISSECTIVE STUDY,0,[0]
We show below that both of these timedependent design choices are vital to capturing long term dependencies in data.,3.2.1. DISSECTIVE STUDY,0,[0]
"Furthermore, we show that the use of ReLU statistics lends itself to better learning.
",3.2.1. DISSECTIVE STUDY,0,[0]
We explored the impact that time-dependent statistics had on learning by first considering naive i.i.d. summary statistics for sequences.,3.2.1. DISSECTIVE STUDY,0,[0]
This was achieved by using r dims=0 and α ∈,3.2.1. DISSECTIVE STUDY,0,[0]
A = {0.99999}.,3.2.1. DISSECTIVE STUDY,0,[0]
"Here no past-dependent context is used for statistics, i.e. we used i.i.d.-type statistics as is typical for unordered sets.",3.2.1. DISSECTIVE STUDY,0,[0]
"Furthermore, the use of a single scale α near 1 means that all of the points’ statistics will be weighted nearly identically (11) regardless of index.",3.2.1. DISSECTIVE STUDY,0,[0]
"We optimized the SRU when using no recurrent statistics and a single scale (iid), when using recurrent statistics with a single scale (recur), and when using no recurrent statistics with multiple scales (multi).",3.2.1. DISSECTIVE STUDY,0,[0]
"We report errors below in Table 4.
",3.2.1. DISSECTIVE STUDY,0,[0]
"Predictably, we cannot learn by simply keeping i.i.d. type statistics of pixel values at a single scale.",3.2.1. DISSECTIVE STUDY,0,[0]
"Furthermore, we find that only using recurrent statistics (recur) in the SRU is not enough.",3.2.1. DISSECTIVE STUDY,0,[0]
"It is interesting to note, however, that keeping i.i.d.",3.2.1. DISSECTIVE STUDY,0,[0]
statistics at multiple scales is able to predict digits with limited success.,3.2.1. DISSECTIVE STUDY,0,[0]
"This lends evidence for the need of both recurrent statistics and multiple scales.
",3.2.1. DISSECTIVE STUDY,0,[0]
"Next, we explored the effects of the scales at which we keep our statistics by varying from α ∈",3.2.1. DISSECTIVE STUDY,0,[0]
"A = {0.0, 0.5, 0.9, 0.99, 0.999} considering α ∈",3.2.1. DISSECTIVE STUDY,0,[0]
"A = {0.0, 0.5, 0.9}, α ∈",3.2.1. DISSECTIVE STUDY,0,[0]
"A = {0.0, 0.5, 0.9, 0.99}.",3.2.1. DISSECTIVE STUDY,0,[0]
"We see in Table 5 that additional, longer scales aid our learning for this dataset.",3.2.1. DISSECTIVE STUDY,0,[0]
"This is not very surprising given the long term nature of the pixel sequences.
",3.2.1. DISSECTIVE STUDY,0,[0]
"Lastly, we considered the use of non-ReLU statistics by changing the element-wise non-linearity f(·) (5)-(8) to be the hyperbolic tangent f(·) = tanh(·).",3.2.1. DISSECTIVE STUDY,0,[0]
We postulated that the use of ReLUs would help our learning since they have been observed to better handle the problem of vanishing gradients.,3.2.1. DISSECTIVE STUDY,0,[0]
We find evidence of this when swapping ReLUs for hyperbolic tangent units in SRUs: we get an error rate of 0.18 when using hyperbolic tangent units.,3.2.1. DISSECTIVE STUDY,0,[0]
"Although the previous uses of ReLUs in RNN required careful initialization (Le et al., 2015), SRUs are able to use ReLUs for better learning without any special considerations.
",3.2.1. DISSECTIVE STUDY,0,[0]
Dimension of recurrent summary.,3.2.1. DISSECTIVE STUDY,0,[0]
Next we explore the effect of varying the number of dimensions used for the recurrent summary of statistics rt (5).,3.2.1. DISSECTIVE STUDY,0,[0]
"We consider r dims in {5, 20, 240}.",3.2.1. DISSECTIVE STUDY,0,[0]
As previously discussed rt provides a context based on past data so that the SRU may produce noni.i.d.,3.2.1. DISSECTIVE STUDY,0,[0]
statistics as it moves along a sequences.,3.2.1. DISSECTIVE STUDY,0,[0]
As one would expect the dimensionality of rt will limit the information flow from the past and values that are too small will hinder performance.,3.2.1. DISSECTIVE STUDY,0,[0]
"It is also interesting to see that after enough dimensions, there are diminishing returns to adding more.
",3.2.1. DISSECTIVE STUDY,0,[0]
Number of statistics and outputs.,3.2.1. DISSECTIVE STUDY,0,[0]
"Finally, we vary the number of statistics num stats, and outputs units.",3.2.1. DISSECTIVE STUDY,0,[0]
Interestingly the SRU seems robust to the number of outputs propagated in the network.,3.2.1. DISSECTIVE STUDY,0,[0]
"However, performance is considerably affected by the number of statistics considered.",3.2.1. DISSECTIVE STUDY,0,[0]
Henceforth we consider real data and sequence learning tasks.,3.3. Polyphonic Music Modeling,0,[0]
"First, we used the polyphonic music datasets from Boulanger-Lewandowski et al. (2012).",3.3. Polyphonic Music Modeling,0,[0]
Each time-step is a binary vector representing the notes played at the respective time-step.,3.3. Polyphonic Music Modeling,0,[0]
Since we were required to predict binary vectors we used the element-wise sigmoid σ.,3.3. Polyphonic Music Modeling,0,[0]
"I.e., the binary vector of notes xt+1 was modeled as σ (pt), where pt is the output after feeding xt (and previous values x1, . . .",3.3. Polyphonic Music Modeling,0,[0]
", xt−1) through the recurrent network.
",3.3. Polyphonic Music Modeling,0,[0]
It is interesting to note in Table 8 that the SRU is able to outperform one of the traditional gated units in every dataset and it outperforms both in two datasets.,3.3. Polyphonic Music Modeling,0,[0]
In the following experiment we modeled the Mel frequency cepstrum coefficients (MFCCs) in a dataset of nearly 18 000 scraped 30s sound clips of electronica-genre songs.,3.4. Electronica-Genre Music MFCC,0,[0]
"MFCCs are perceptually based spectral features positioned logarithmically on the mel scale, which approximates the human auditory system’s response (Müller, 2007).",3.4. Electronica-Genre Music MFCC,0,[0]
"We looked to model the 13 real-valued coefficients using the recurrent units, by modeling xt+1 as a projection of the output of a recurrent unit after being fed x1, . . .",3.4. Electronica-Genre Music MFCC,0,[0]
", xt.
",3.4. Electronica-Genre Music MFCC,0,[0]
"As can be seen in Table 9, SRUs again are outperforming gated architectures and are especially beating GRUs by a wider margin.",3.4. Electronica-Genre Music MFCC,0,[0]
Next we consider weather data prediction using the North America Regional Reanalysis (NARR) Project (NAR).,3.5. Climate Data,0,[0]
The dataset provides a long-term set of consistent climate data on a regional scale for the North American domain.,3.5. Climate Data,0,[0]
"The
period of the reanalyses is from October 1978 to the present and analyses were made 8 times daily (3 hour intervals).
",3.5. Climate Data,0,[0]
We take our input sequences to be year-long sequences of weather variables in a location for the year 2006.,3.5. Climate Data,0,[0]
I.e. an input sequence will be a 2920 length sequence of weather variables at a given lat/lon coordinate.,3.5. Climate Data,0,[0]
"We considered the following 7 variables: pres10m, 10 m pressure (pa); tcdc, total cloud cover (%); rh2m, relative humidity 2m (%); tmpsfc, surface temperature (k); snod, snow depth surface (m); ugrd10m, u component of wind 10m above ground; vgrd10m, v component of wind 10m above ground.",3.5. Climate Data,0,[0]
"The variables were standardized, see Figure 6 for example sequences.
",3.5. Climate Data,0,[0]
Below we see results using 51 200 training location sequences and 6 400 validation and testing instances.,3.5. Climate Data,0,[0]
"Again, we look to model the next point in a sequence as a projection of the output of the recurrent unit after feeding the previous points.",3.5. Climate Data,0,[0]
One may see in Table 10 that SRUs and LSTMs perform nearly identically; perhaps the cyclical nature of climate data was beneficial to the gated units.,3.5. Climate Data,0,[0]
"Finally, we look to predict the positions of National Basketball Association (NBA) players based on previous court positions during a play.",3.6. SportVu NBA Tracking data,0,[0]
Optical tracking data for this project were provided by STATS LLC from their SportVU product and obtained from (NBA).,3.6. SportVu NBA Tracking data,0,[0]
The data are composed of x and y coordinates for each of the ten players and the ball.,3.6. SportVu NBA Tracking data,0,[0]
"We again minimize the squared norm of errors for predictions.
",3.6. SportVu NBA Tracking data,0,[0]
We observed a large margin of improvement for SRUs over gated architectures in Table 11 that is reminiscent of the synthetic data experiment in §3.1.,3.6. SportVu NBA Tracking data,0,[0]
"This suggests that this
dataset contains long term dependencies that the SRU is able to exploit.",3.6. SportVu NBA Tracking data,0,[0]
We believe that the use of summary statistics has been under-explored in modern recurrent units.,4. Discussion,0,[0]
"Although recent studies in convolutional networks have considered global average pooling, which is essentially using high-level summary statistics to represent images, there has been little exploration of summary statistics for modern recurrent networks.",4. Discussion,0,[0]
"To this end we introduce the Statistical Recurrent Unit, a novel architecture that seeks to capture long term dependencies in data using only simple moving averages and rectified-linear units.
",4. Discussion,0,[0]
"The SRU was motivated by the success of mean-map embeddings for representing unordered datasets, and may be interpreted as an alteration of MMEs for sequential data.",4. Discussion,0,[0]
"The main modifications are as follows: first, the SRU uses data-driven statistics unlike typical MMEs, which will use RKHS features from an a-priori selected class of kernels; second, SRUs will use recurrent statistics that are dependent not only on a current point, but on previous points’ statistics through a condensation of kept moving averages; third, the SRU will keep moving averages at various scales.",4. Discussion,0,[0]
"We provide evidence that the combination of these modifications yield much better results than any one of them in isolation.
",4. Discussion,0,[0]
The resulting recurrent unit is especially adept for capturing long term dependencies in data and readily has access to a combinatorial number of viewpoints of past windows through simple linear combinations.,4. Discussion,0,[0]
"Moreover, it is interesting to note that even though the SRU is gate-less, it may implement part of both “remembering” and “forgetting” functionalities through ReLUs and moving averages.
",4. Discussion,0,[0]
We showed empirically that the SRU is better equipped that traditional gated units for long term dependencies via synthetic and real-world data experiments.,4. Discussion,0,[0]
This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887.,Acknowledgements,0,[0]
Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications.,abstractText,0,[0]
"We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics.",abstractText,0,[0]
"The SRU’s architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks.",abstractText,0,[0]
We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures’ hyperparameters for both synthetic and real-world tasks.,abstractText,0,[0]
The Statistical Recurrent Unit,title,0,[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2873–2878 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,0,[0]
It is generally assumed that the geometry of word embeddings is determined by semantic relatedness.,1 Introduction,0,[0]
"Vectors are assumed to be distributed throughout a K-dimensional space, with specific regions devoted to specific concepts.",1 Introduction,0,[0]
"We find that vectors trained with the skip-gram with negative sampling (SGNS) algorithm (Mikolov et al., 2013) are not only influenced by semantics but are also strongly influenced by the negative sampling objective.",1 Introduction,0,[0]
"In fact, far from spanning the possible space, they exist only in a narrow cone in RK .",1 Introduction,0,[0]
"Nevertheless, SGNS vectors have become a foundational tool in NLP and perform as well or better than numerous methods with similar objectives (Turian et al., 2010; Dhillon et al., 2012; Pennington et al., 2014; Luo et al., 2015) with respect to evaluations of intrinsic and extrinsic quality (Schnabel et al., 2015).
",1 Introduction,0,[0]
"SGNS works by training two sets of embeddings: the “official” word embeddings and a second set of context embeddings, with one K-dimensional vector in each set for each word in the vocabulary.",1 Introduction,0,[0]
The objective tries to make the word vector and context vector closer for a pair of words that actually occur together than for randomly sampled “negative” words.,1 Introduction,0,[0]
"Following training, the word vectors
are typically saved; the context vectors are deleted.",1 Introduction,0,[0]
"Any difference between these two sets of vectors is puzzling, since the sliding window used in training is symmetrical: a word and its context word reverse roles almost immediately.",1 Introduction,0,[0]
"Indeed, the superficially similar GloVe algorithm (Pennington et al., 2014) also defines word and context vectors and by default returns the mean of these two vectors.
",1 Introduction,0,[0]
"Previous work has analyzed what the algorithm might be doing in theory, as an approximation to a matrix factorization (Levy and Goldberg, 2014).",1 Introduction,0,[0]
"Other work has considered the empirical effects of some of the more arbitrary-seeming algorithmic choices (Levy et al., 2015).",1 Introduction,0,[0]
"But we still have relatively little understanding of how the algorithm actually determines parameter values.
",1 Introduction,0,[0]
In this work we measure geometric properties of SGNS-trained word vectors and their context vectors.,1 Introduction,0,[0]
"Although the word vectors appear to span K-dimensional space, we find that the SGNS objective results in vectors that are narrowly clustered in a single orthant, and can be made non-negative without significant loss.",1 Introduction,0,[0]
Figure 1 shows two visualizations of SGNS vectors and context vectors.,1 Introduction,0,[0]
"The context vectors mirror the “official” word vectors, with the angle between vectors effectively controlled by the number of negative samples.",1 Introduction,0,[0]
"We
2873
show that this effect is due to negative sampling and not the general embedding objective.",1 Introduction,0,[0]
"We note that this relationship between vectors is effectively hidden by the commonly-used t-SNE projection (van der Maaten and Hinton, 2008).",1 Introduction,0,[0]
"The SGNS algorithm defines two sets of parameters, K-dimensional word vectors wi and context vectors ci for each word i. We define a weight between a word i and a context word j as σij = exp(wTi cj)
1+exp(wTi cj) .",2 Word embeddings with SGNS,0,[0]
"For each observed pair i, j
we sample S “negative” context words from a modified unigram distribution p(w)0.75.",2 Word embeddings with SGNS,0,[0]
"The stochastic gradient update for one parameter wik is then
d`
dwik =",2 Word embeddings with SGNS,0,[0]
"(1− σij)cjk + S∑ s=1 −σiscsk, (1)
suppressing for clarity a learning rate parameter λ.",2 Word embeddings with SGNS,0,[0]
"A symmetrical update is performed for the context word parameters cj and cs, substituting wi for c. This update has been shown to be equivalent to the gradient of a factorization of a pointwise mutual information matrix (Levy and Goldberg, 2014).
",2 Word embeddings with SGNS,0,[0]
The impact of the update is to push the vector wi closer to the context vector of the observed context word cj and away from the context vectors of the negatively sampled words.,2 Word embeddings with SGNS,0,[0]
"The amount of change at any given update is dependent on the degree to which the current model predicts the “correct” source of the context word, whether from the real data distribution or the negative sampling distribution.",2 Word embeddings with SGNS,0,[0]
"If the model is infinitely certain that the real word is real (σij = 1.0) and the fake words are fake (σis = 0.0 ∀s), it will make no change to the current parameters.",2 Word embeddings with SGNS,0,[0]
We first present a series of empirical observations based on vectors trained from a corpus of Wikipedia articles that is commonly distributed with word embedding implementations.1 We then evaluate the sensitivity of these properties to different algorithmic parameters.,3 Results,0,[0]
"We make no assertion that these are optimal (or even particularly good) vectors, only that they are representative of the properties of the algorithm.
",3 Results,0,[0]
"1http://mattmahoney.net/dc/text8.zip
To determine whether observed properties are due to SGNS specifically or to embeddings in general, we compare SGNS-trained vectors to vectors trained by the GloVe algorithm (Pennington et al., 2014).",3 Results,0,[0]
"The choice of GloVe as a comparison is due to its popularity and superficial similarity to SGNS.2 We begin by examining one set of embeddings from each algorithm, both with K = 50 dimensions, a vocabulary of ≈ 70k words, and context window 5.",3 Results,0,[0]
"We then evaluate sensitivity to negative samples, window size, and dimension.
",3 Results,0,[0]
"Embeddings are sensitive to word frequency (Hellrich and Hahn, 2016).",3 Results,0,[0]
"Following Zipf’s law, words in natural language tend to sort into ranges of frequent words (the majority of tokens) and rare words (the majority of types), with a large class of intermediate-frequency terms in the middle.",3 Results,0,[0]
"As a result, the large majority of interactions are between frequent terms or between frequent and infrequent terms.",3 Results,0,[0]
"Interactions between infrequent terms are rare, no matter how large the corpus.",3 Results,0,[0]
"We define four categories of words by ranked frequency: the top 100 words (ultra-high frequency), the 100–500th ranked words (high frequency), the 500–5000th ranked words (moderate frequency) and the remaining (low frequency) words.
",3 Results,0,[0]
SGNS vectors are arranged along a primary axis.,3 Results,0,[0]
Our first observation is that SGNS-trained vectors all point in roughly the same direction.,3 Results,0,[0]
We can define a mean vector w̄ by averaging the vectors of the complete vocabulary w. We sample a balanced set of 400 total words with 100 each from the four frequency categories.,3 Results,0,[0]
"Figure 2 shows the
2We make no attempt in this work to compare the quality of SGNS and GloVe vectors, nor should the omission of other algorithms be attributed to anything but space constraints.
distribution of inner products between these 400 sampled words and their mean vector ŵ. All vectors have a large, positive inner product with the mean, indicating that they are not evenly dispersed through the space.",3 Results,0,[0]
"Furthermore, the frequency category of words has relatively little effect on the inner product, with the exception of the rare words, which have slightly less positive inner products.",3 Results,0,[0]
"As a comparison, the vectors trained by GloVe show a clear relationship with word frequency, with lowfrequency words opposing the frequency-balanced mean vector.
",3 Results,0,[0]
This result does not depend on a specific mean vector.,3 Results,0,[0]
"Using the global mean vector rather than the frequency-balanced mean vector reverses the order of frequency categories within each plot, but does not change their overall shape.",3 Results,0,[0]
"SGNS vector inner products are all positive, with low-frequency words the most positive.",3 Results,0,[0]
"GloVe inner products become positive for low-frequency words and negative for high-frequency words.
",3 Results,0,[0]
"The inner product between vectors is used by the algorithm during training, but in practice vectors are often normalized to have unit length before use.",3 Results,0,[0]
It is possible that the apparent pattern shown in Figure 2 may be an artifact of differing average lengths between words of different frequencies.,3 Results,0,[0]
"After normalizing SGNS vectors to length 1.0, the lowest and highest frequency words are most similar to the
mean vector, with the moderate-frequency words showing the greatest deviation.",3 Results,0,[0]
"Normalization does not change the relative order for GloVe vectors.
",3 Results,0,[0]
SGNS vectors point away from context vectors.,3 Results,0,[0]
It is possible that vectors could have a positive inner product with the mean vector but be mutually orthogonal.,3 Results,0,[0]
Figure 3 shows the distribution of inner products wTi cj for pairs of words divided by frequency for SGNS and GloVe.,3 Results,0,[0]
"Almost all interactions have similar, negative inner products for SGNS, while GloVe interactions are sensitive to frequency and vary more widely.",3 Results,0,[0]
"We note that the high-frequency words in GloVe appear to form a cohesive cluster between themselves (positive inner products) that points away from the lower frequency words (negative inner products), while infrequent words are more dispersed and have no clear pattern relative to each other.
SGNS vectors are mostly non-negative.",3 Results,0,[0]
"Not only do SGNS vectors occupy a narrow region of embedding space, it appears that the vectors can be rotated to fall mostly within the positive orthant.",3 Results,0,[0]
For each column of the matrix of vectors w we can compute the dimension-wise mean w̄k.,3 Results,0,[0]
Multiplying w by a diagonal matrix of the signs of the means diag(sign(w̄k)) flips each dimension so that its mean is positive.,3 Results,0,[0]
Figure 4 shows the resulting positive-mean histogram for 12 of the 50 dimensions trained by SGNS (the remaining dimensions are similar).,3 Results,0,[0]
"Some dimensions have medians close to 0.0, but most skew positive.
",3 Results,0,[0]
"Indeed, it is possible to simply drop all remaining negative values without radically changing the properties of the vectors.",3 Results,0,[0]
"Embeddings are often evaluated based on word similarity prediction (Schnabel et al., 2015).",3 Results,0,[0]
"Using only positive entries, Spearman rank correlation drops from 0.283 to .276 on the SIMLEX word similarity task and from 0.556 to 0.542 on the MEN task.",3 Results,0,[0]
"Subtracting the global mean vector has similarly little impact, reducing SIMLEX correlation to 0.271 and increasing MEN correlation to 0.575.",3 Results,0,[0]
"This property may help explain why sparse (Faruqui et al., 2015) and non-negative (Luo et al., 2015) embeddings do not lose significant performance.
",3 Results,0,[0]
SGNS context vectors point away from the word vectors.,3 Results,0,[0]
What then is the geometry of the context vectors c?,3 Results,0,[0]
The two sets of vectors appear to present a noisy mirror image of each other.,3 Results,0,[0]
"Figure 5 shows the distribution of inner products between
the context vectors and the same mean vector ŵ used in Figure 2.",3 Results,0,[0]
"These inner products are negative, indicating that the context vectors point in the opposite direction from the word vectors.",3 Results,0,[0]
"In contrast, the GloVe context vectors have essentially the same relationship to the mean of the word vectors as the word vectors themselves.",3 Results,0,[0]
"This property explains why it is common to output the mean of wi and ci for each word for GloVe but not for SGNS: in Glove these two vectors are essentially noisy copies of one another, while in SGNS the two vectors are pointing almost in the opposite direction.
",3 Results,0,[0]
Positive and negative weights come to equilibrium.,3 Results,0,[0]
"Eq. 1 balances two terms, a positive interaction term 1.0 − σij between a word and a context word and negative interaction terms 0.0− σis between a word and one of S randomly sampled words.",3 Results,0,[0]
"These terms can be viewed as a “label” minus an expectation, as in the gradient for logistic regression.",3 Results,0,[0]
"Since there is no 1/S term to balance the number of random samples, one might
expect that the “power” of the sampled context terms might overwhelm the true interaction term.",3 Results,0,[0]
"In practice, these samples appear to find an equilibrium that effectively balances out the number of random samples after a short burn-in phase.",3 Results,0,[0]
We recorded a moving average of positive and negative weights for an ultra-frequent word (the) and a moderately frequent word (tuesday).,3 Results,0,[0]
"In both cases, the mean of the values for positive samples starts at 0.5 and for the negative samples at -0.5.",3 Results,0,[0]
"The positive values converge toward S = 5 times the mean of the values for negative samples: 0.581 vs. -0.182 for the and 0.693 vs. -0.138 for tuesday.
",3 Results,0,[0]
The negative objective is optimized when each model vector points away from the context vectors.,3 Results,0,[0]
"The positive objective, in contrast, is maximized when word and context vectors for related words are pointing in the same direction.",3 Results,0,[0]
"The negative force acts to repel the vectors, the positive force acts to pull them together.
",3 Results,0,[0]
"During the crucial early phases of the algorithm, negative samples have more weight than positive samples: when inner products are near zero, both types of samples will have values of σij and σis close to 0.5, so negative samples will “count” S times more than positive.",3 Results,0,[0]
"The early phases of the
algorithm will focus on pushing the two sets of vectors apart into separate regions of the latent space.",3 Results,0,[0]
"Once vectors and context vectors separate, inner products will become negative, so σij and σis will move closer to 0.0.
",3 Results,0,[0]
"The balance between positive and negative samples consistently affects the geometry of the vectors, and is not sensitive to random initialization.",3 Results,0,[0]
"We varied the number of negative samples from S = 1 to S = 15, and ran 10 trials for each value with different random initializations.",3 Results,0,[0]
"As shown in Figure 6, as we increase S, the average inner product between vectors and the mean vector within each model increases.
SGNS vectors are concentrated and point away from their context vectors, and changing the number of negative samples appears to affect this property.",3 Results,0,[0]
"We now consider whether other factors could also cause this behavior.
",3 Results,0,[0]
Effect of window size Both SGNS and GloVe operate over word co-occurrences within a sliding window centered around each token in the corpus.,3 Results,0,[0]
"This window size parameter has an effect on the semantics of vectors, so it is important to consider whether it has an effect on the geometry of vectors.",3 Results,0,[0]
"Simply setting an equal window size for SGNS and GloVe does not, however, guarantee that the two algorithms are seeing equivalent data, because each pair is weighted linearly by token distance in SGNS and by 1/distance in GloVe.",3 Results,0,[0]
"Figure 7 shows average inner products for each frequency with the global mean vector for 10 trials each at window size 5, 10, 15, 20 with K = 50.",3 Results,0,[0]
"Increasing window size leads to greater divergence between high- and low-frequency words for word and context vectors, but does not change their pattern.",3 Results,0,[0]
"GloVe results are similarly unchanged.
",3 Results,0,[0]
"Effect of vector size As with window size, the dimensionality K of the word vectors can affect their ability to represent semantic relationships.",3 Results,0,[0]
"Figure 8 shows an increase in inner product with the global mean as we increase K (10 trials each, window size 15), but the effect is small relative to that of the number of negative samples S. GloVe inner products change by less than 0.05.",3 Results,0,[0]
"SGNS vectors encode semantic relatedness, but their arrangement is much more strongly influenced by the negative sampling objective than is usually
assumed.",4 Conclusion,0,[0]
We find that vectors lie on a narrow primary axis that is effectively non-negative.,4 Conclusion,0,[0]
"Users should not interpret relationships between vectors without recognizing this geometric context.
",4 Conclusion,0,[0]
In this work we have deliberately restricted ourselves to describing the geometric properties of vectors.,4 Conclusion,0,[0]
We see several areas for further work.,4 Conclusion,0,[0]
"First, there are likely to be theoretical reasons why the observed concentration of SGNS vectors in a narrow cone does not appear to affect performance relative to algorithms that do not have this property.",4 Conclusion,0,[0]
"Second, measuring the interplay between positive and negative objectives may provide insight into algorithmic choices that are now poorly understood, such as the effect of reducing the occurrence of frequent words in the corpus and the sampling distribution of negative examples.",4 Conclusion,0,[0]
"Finally, we suggest that in addition to theoretical analysis, more work should be done to understand the actual working of algorithms on real data.",4 Conclusion,0,[0]
Anonymous reviewers helped make this paper much better.,Acknowledgements,0,[0]
"This work was supported by NSF #1526155, #1652536, and #DGE-1144153; and the Alfred P. Sloan Foundation.",Acknowledgements,0,[0]
"Despite their ubiquity, word embeddings trained with skip-gram negative sampling (SGNS) remain poorly understood.",abstractText,0,[0]
"We find that vector positions are not simply determined by semantic similarity, but rather occupy a narrow cone, diametrically opposed to the context vectors.",abstractText,0,[0]
"We show that this geometric concentration depends on the ratio of positive to negative examples, and that it is neither theoretically nor empirically inherent in related embedding algorithms.",abstractText,0,[0]
The strange geometry of skip-gram with negative sampling,title,0,[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 285–294, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics
This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.",text,0,[0]
The ability for a computer to converse in a natural and coherent manner with a human has long been held as one of the primary objectives of artificial intelligence (AI).,1 Introduction,0,[0]
In this paper we consider the problem of building dialogue agents that have the ability to interact in one-on-one multi-turn conversations on a diverse set of topics.,1 Introduction,0,[0]
"We primarily target unstructured dialogues, where there is no a priori logical representation for the information exchanged during the conversation.",1 Introduction,0,[0]
"This is in contrast to recent systems which focus on structured dialogue tasks, using a slot-filling representation [10, 27, 32].
",1 Introduction,0,[0]
"We observe that in several subfields of AI— computer vision, speech recognition, machine translation—fundamental break-throughs were achieved in recent years using machine learning
∗The first two authors contributed equally.
methods, more specifically with neural architectures [1]; however, it is worth noting that many of the most successful approaches, in particular convolutional and recurrent neural networks, were known for many years prior.",1 Introduction,0,[0]
"It is therefore reasonable to attribute this progress to three major factors: 1) the public distribution of very large rich datasets [5], 2) the availability of substantial computing power, and 3) the development of new training methods for neural architectures, in particular leveraging unlabeled data.",1 Introduction,0,[0]
Similar progress has not yet been observed in the development of dialogue systems.,1 Introduction,0,[0]
"We hypothesize that this is due to the lack of sufficiently large datasets, and aim to overcome this barrier by providing a new large corpus for research in multi-turn conversation.
",1 Introduction,0,[0]
"The new Ubuntu Dialogue Corpus consists of almost one million two-person conversations extracted from the Ubuntu chat logs1, used to receive technical support for various Ubuntu-related problems.",1 Introduction,0,[0]
"The conversations have an average of 8 turns each, with a minimum of 3 turns.",1 Introduction,0,[0]
All conversations are carried out in text form (not audio).,1 Introduction,0,[0]
The dataset is orders of magnitude larger than structured corpuses such as those of the Dialogue State Tracking Challenge [32].,1 Introduction,0,[0]
"It is on the same scale as recent datasets for solving problems such as question answering and analysis of microblog services, such as Twitter [22, 25, 28, 33], but each conversation in our dataset includes several more turns, as well as longer utterances.",1 Introduction,0,[0]
"Furthermore, because it targets a specific domain, namely technical support, it can be used as a case study for the development of AI agents in targeted applications, in contrast to chatbox agents that often lack a welldefined goal [26].
",1 Introduction,0,[0]
"In addition to the corpus, we present learning architectures suitable for analyzing this dataset, ranging from the simple frequency-inverse docu-
1These logs are available from 2004 to 2015 at http: //irclogs.ubuntu.com/
285
ment frequency (TF-IDF) approach, to more sophisticated neural models including a Recurrent Neural Network (RNN) and a Long Short-Term Memory (LSTM) architecture.",1 Introduction,0,[0]
"We provide benchmark performance of these algorithms, trained with our new corpus, on the task of selecting the best next response, which can be achieved without requiring any human labeling.",1 Introduction,0,[0]
The dataset is ready for public release2.,1 Introduction,0,[0]
The code developed for the empirical results is also available3.,1 Introduction,0,[0]
"We briefly review existing dialogue datasets, and some of the more recent learning architectures used for both structured and unstructured dialogues.",2 Related Work,0,[0]
"This is by no means an exhaustive list (due to space constraints), but surveys resources most related to our contribution.",2 Related Work,0,[0]
A list of datasets discussed is provided in Table 1.,2 Related Work,0,[0]
"The Switchboard dataset [8], and the Dialogue State Tracking Challenge (DSTC) datasets [32] have been used to train and validate dialogue management systems for interactive information retrieval.",2.1 Dialogue Datasets,0,[0]
"The problem is typically formalized as a slot filling task, where agents attempt to predict the goal of a user during the conversation.",2.1 Dialogue Datasets,0,[0]
"These datasets have been significant resources for structured dialogues, and have allowed major progress in this field, though they are quite small compared to datasets currently used for training neural architectures.
",2.1 Dialogue Datasets,0,[0]
"Recently, a few datasets have been used containing unstructured dialogues extracted from Twitter4.",2.1 Dialogue Datasets,0,[0]
Ritter et al. [21] collected 1.3 million conversations; this was extended in [28] to take advantage of longer contexts by using A-B-A triples.,2.1 Dialogue Datasets,0,[0]
Shang et al. [25] used data from a similar Chinese website called Weibo5.,2.1 Dialogue Datasets,0,[0]
"However to our knowledge, these datasets have not been made public, and furthermore, the post-reply format of such microblogging services is perhaps not as representative of natural dialogue between humans as the continuous stream of messages in a chat room.",2.1 Dialogue Datasets,0,[0]
"In fact, Ritter et al. estimate that only 37% of posts on Twitter are ‘conversational in nature’, and 69%
2http://www.cs.mcgill.ca/~jpineau/ datasets/ubuntu-corpus-1.0
3http://github.com/npow/ubottu 4https://twitter.com/ 5http://www.weibo.com/
of their collected data contained exchanges of only length 2 [21].",2.1 Dialogue Datasets,0,[0]
"We hypothesize that chat-room style messaging is more closely correlated to human-tohuman dialogue than micro-blogging websites, or forum-based sites such as Reddit.
",2.1 Dialogue Datasets,0,[0]
"Part of the Ubuntu chat logs have previously been aggregated into a dataset, called the Ubuntu Chat Corpus [30].",2.1 Dialogue Datasets,0,[0]
"However that resource preserves the multi-participant structure and thus is less amenable to the investigation of more traditional two-party conversations.
",2.1 Dialogue Datasets,0,[0]
Also weakly related to our contribution is the problem of question-answer systems.,2.1 Dialogue Datasets,0,[0]
"Several datasets of question-answer pairs are available [3], however these interactions are much shorter than what we seek to study.",2.1 Dialogue Datasets,0,[0]
Most dialogue research has historically focused on structured slot-filling tasks [24].,2.2 Learning Architectures,0,[0]
"Various approaches were proposed, yet few attempts leverage more recent developments in neural learning architectures.",2.2 Learning Architectures,0,[0]
"A notable exception is the work of Henderson et al. [11], which proposes an RNN structure, initialized with a denoising autoencoder, to tackle the DSTC 3 domain.
",2.2 Learning Architectures,0,[0]
"Work on unstructured dialogues, recently pioneered by Ritter et al. [22], proposed a response generation model for Twitter data based on ideas from Statistical Machine Translation.",2.2 Learning Architectures,0,[0]
This is shown to give superior performance to previous information retrieval (e.g. nearest neighbour) approaches [14].,2.2 Learning Architectures,0,[0]
"This idea was further developed by Sordoni et al. [28] to exploit information from a longer context, using a structure similar to the Recurrent Neural Network Encoder-Decoder model [4].",2.2 Learning Architectures,0,[0]
"This achieves rather poor performance on A-B-A Twitter triples when measured by the BLEU score (a standard for machine translation), yet performs comparatively better than the model of Ritter et al. [22].",2.2 Learning Architectures,0,[0]
Their results are also verified with a human-subject study.,2.2 Learning Architectures,0,[0]
A similar encoderdecoder framework is presented in [25].,2.2 Learning Architectures,0,[0]
"This model uses one RNN to transform the input to some vector representation, and another RNN to ‘decode’ this representation to a response by generating one word at a time.",2.2 Learning Architectures,0,[0]
"This model is also evaluated in a human-subject study, although much smaller in size than in [28].",2.2 Learning Architectures,0,[0]
"Overall, these models highlight the potential of neural learning architectures for interactive systems, yet so far they have
been limited to very short conversations.",2.2 Learning Architectures,0,[0]
"We seek a large dataset for research in dialogue systems with the following properties: • Two-way (or dyadic) conversation, as op-
posed to multi-participant chat, preferably human-human.",3 The Ubuntu Dialogue Corpus,0,[0]
•,3 The Ubuntu Dialogue Corpus,0,[0]
"Large number of conversations; 105 − 106
is typical of datasets used for neural-network learning in other areas of AI.",3 The Ubuntu Dialogue Corpus,0,[0]
"• Many conversations with several turns (more
than 3).",3 The Ubuntu Dialogue Corpus,0,[0]
"• Task-specific domain, as opposed to chatbot
systems.",3 The Ubuntu Dialogue Corpus,0,[0]
All of these requirements are satisfied by the Ubuntu Dialogue Corpus presented in this paper.,3 The Ubuntu Dialogue Corpus,0,[0]
The Ubuntu Chat Logs refer to a collection of logs from Ubuntu-related chat rooms on the Freenode Internet Relay Chat (IRC) network.,3.1 Ubuntu Chat Logs,0,[0]
This protocol allows for real-time chat between a large number of participants.,3.1 Ubuntu Chat Logs,0,[0]
"Each chat room, or channel, has a particular topic, and every channel participant can see all the messages posted in a given channel.",3.1 Ubuntu Chat Logs,0,[0]
"Many of these channels are used for obtaining technical support with various Ubuntu issues.
",3.1 Ubuntu Chat Logs,0,[0]
"As the contents of each channel are moderated, most interactions follow a similar pattern.",3.1 Ubuntu Chat Logs,0,[0]
"A new user joins the channel, and asks a general question about a problem they are having with Ubuntu.",3.1 Ubuntu Chat Logs,0,[0]
"Then, another more experienced user replies with a potential solution, after first addressing the ’username’ of the first user.",3.1 Ubuntu Chat Logs,0,[0]
"This is called a name mention [29], and is done to avoid confusion in the
channel — at any given time during the day, there can be between 1 and 20 simultaneous conversations happening in some channels.",3.1 Ubuntu Chat Logs,0,[0]
"In the most popular channels, there is almost never a time when only one conversation is occurring; this renders it particularly problematic to extract dyadic dialogues.",3.1 Ubuntu Chat Logs,0,[0]
"A conversation between a pair of users generally stops when the problem has been solved, though some users occasionally continue to discuss a topic not related to Ubuntu.
",3.1 Ubuntu Chat Logs,0,[0]
"Despite the nature of the chat room being a constant stream of messages from multiple users, it is through the fairly rigid structure in the messages that we can extract the dialogues between users.",3.1 Ubuntu Chat Logs,0,[0]
"Figure 4 shows an example chat room conversation from the #ubuntu channel as well as the extracted dialogues, which illustrates how users usually state the username of the intended message recipient before writing their reply (we refer to all replies and initial questions as ‘utterances’).",3.1 Ubuntu Chat Logs,0,[0]
"For example, it is clear that users ‘Taru’ and ‘kuja’ are engaged in a dialogue, as are users ‘Old’ and ‘bur[n]er’, while user ‘_pm’ is asking an initial question, and ‘LiveCD’ is perhaps elaborating on a previous comment.",3.1 Ubuntu Chat Logs,0,[0]
"In order to create the Ubuntu Dialogue Corpus, first a method had to be devised to extract dyadic dialogues from the chat room multi-party conversations.",3.2 Dataset Creation,0,[0]
"The first step was to separate every message into 4-tuples of (time, sender, recipient, utterance).",3.2 Dataset Creation,0,[0]
"Given these 4-tuples, it is straightforward to group all tuples where there is a matching sender and recipient.",3.2 Dataset Creation,0,[0]
"Although it is easy to separate the time and the sender from the rest, finding the in-
tended recipient of the message is not always trivial.",3.2 Dataset Creation,0,[0]
"While in most cases the recipient is the first word of the utterance, it is sometimes located at the end, or not at all in the case of initial questions.",3.2.1 Recipient Identification,0,[0]
"Furthermore, some users choose names corresponding to common English words, such as ‘the’ or ‘stop’, which could lead to many false positives.",3.2.1 Recipient Identification,0,[0]
"In order to solve this issue, we create a dictionary of usernames from the current and previous days, and compare the first word of each utterance to its entries.",3.2.1 Recipient Identification,0,[0]
"If a match is found, and the word does not correspond to a very common English word6, it is assumed that this user was the intended recipient of the message.",3.2.1 Recipient Identification,0,[0]
"If no matches are found, it is assumed that the message was an initial question, and the recipient value is left empty.",3.2.1 Recipient Identification,0,[0]
"The dialogue extraction algorithm works backwards from the first response to find the initial question that was replied to, within a time frame of 3 minutes.",3.2.2 Utterance Creation,0,[0]
A first response is identified by the presence of a recipient name (someone from the recent conversation history).,3.2.2 Utterance Creation,0,[0]
"The initial question is identified to be the most recent utterance by the recipient identified in the first response.
",3.2.2 Utterance Creation,0,[0]
All utterances that do not qualify as a first response or an initial question are discarded; initial questions that do not generate any response are also discarded.,3.2.2 Utterance Creation,0,[0]
"We additionally discard conversations longer than five utterances where one user says more than 80% of the utterances, as these are typically not representative of real chat dialogues.",3.2.2 Utterance Creation,0,[0]
"Finally, we consider only extracted dialogues that consist of 3 turns or more to encourage the modeling of longer-term dependencies.
",3.2.2 Utterance Creation,0,[0]
"To alleviate the problem of ‘holes’ in the dialogue, where one user does not address the other explicitly, as in Figure 5, we check whether each user talks to someone else for the duration of their conversation.",3.2.2 Utterance Creation,0,[0]
"If not, all non-addressed utterances are added to the dialogue.",3.2.2 Utterance Creation,0,[0]
An example conversation along with the extracted dialogues is shown in Figure 5.,3.2.2 Utterance Creation,0,[0]
"Note that we also concatenate all consecutive utterances from a given user.
",3.2.2 Utterance Creation,0,[0]
"We do not apply any further pre-processing (e.g. tokenization, stemming) to the data as released in the Ubuntu Dialogue Corpus.",3.2.2 Utterance Creation,0,[0]
"However the use of
6We use the GNU Aspell spell checking dictionary.
pre-processing is standard for most NLP systems, and was also used in our analysis (see Section 4.)",3.2.2 Utterance Creation,0,[0]
"It is often the case that a user will post an initial question, and multiple people will respond to it with different answers.",3.2.3 Special Cases and Limitations,0,[0]
"In this instance, each conversation between the first user and the user who replied is treated as a separate dialogue.",3.2.3 Special Cases and Limitations,0,[0]
This has the unfortunate side-effect of having the initial question appear multiple times in several dialogues.,3.2.3 Special Cases and Limitations,0,[0]
"However the number of such cases is sufficiently small compared to the size of the dataset.
",3.2.3 Special Cases and Limitations,0,[0]
Another issue to note is that the utterance posting time is not considered for segmenting conversations between two users.,3.2.3 Special Cases and Limitations,0,[0]
"Even if two users have a conversation that spans multiple hours, or even days, this is treated as a single dialogue.",3.2.3 Special Cases and Limitations,0,[0]
"However, such dialogues are rare.",3.2.3 Special Cases and Limitations,0,[0]
We include the posting time in the corpus so that other researchers may filter as desired.,3.2.3 Special Cases and Limitations,0,[0]
Table 2 summarizes properties of the Ubuntu Dialogue Corpus.,3.3 Dataset Statistics,0,[0]
One of the most important features of the Ubuntu chat logs is its size.,3.3 Dataset Statistics,0,[0]
This is crucial for research into building dialogue managers based on neural architectures.,3.3 Dataset Statistics,0,[0]
"Another important
characteristic is the number of turns in these dialogues.",3.3 Dataset Statistics,0,[0]
The distribution of the number of turns is shown in Figure 1.,3.3 Dataset Statistics,0,[0]
It can be seen that the number of dialogues and turns per dialogue follow an approximate power law relationship.,3.3 Dataset Statistics,0,[0]
We set aside 2% of the Ubuntu Dialogue Corpus conversations (randomly selected) to form a test set that can be used for evaluation of response selection algorithms.,3.4 Test Set Generation,0,[0]
"Compared to the rest of the corpus, this test set has been further processed to extract a pair of (context, response, flag) triples from each dialogue.",3.4 Test Set Generation,0,[0]
The flag is a Boolean variable indicating whether or not the response was the actual next utterance after the given context.,3.4 Test Set Generation,0,[0]
The response is a target (output) utterance which we aim to correctly identify.,3.4 Test Set Generation,0,[0]
The context consists of the sequence of utterances appearing in dialogue prior to the response.,3.4 Test Set Generation,0,[0]
"We create a pair of triples, where one triple contains the correct response (i.e. the actual next utterance in the dialogue), and the other triple contains a false response, sampled randomly from elsewhere within the test set.",3.4 Test Set Generation,0,[0]
The flag is set to 1 in the first case and to 0 in the second case.,3.4 Test Set Generation,0,[0]
An example pair is shown in Table 3.,3.4 Test Set Generation,0,[0]
"To make the task harder, we can move from pairs of responses (one correct, one incorrect) to a larger set of wrong responses (all with flag=0).",3.4 Test Set Generation,0,[0]
"In our experiments below, we consider both the case of 1 wrong response and 10 wrong responses.
",3.4 Test Set Generation,0,[0]
"Since we want to learn to predict all parts of a conversation, as opposed to only the closing statement, we consider various portions of context for the conversations in the test set.",3.4 Test Set Generation,0,[0]
"The context size is determined stochastically using a simple formula:
c = min(t− 1, n− 1),
where n = 10C η + 2, η ∼ Unif(C/2, 10C)
Here, C denotes the maximum desired context size, which we set to C = 20.",3.4 Test Set Generation,0,[0]
"The last term is
the desired minimum context size, which we set to be 2.",3.4 Test Set Generation,0,[0]
"Parameter t is the actual length of that dialogue (thus the constraint that c ≤ t − 1), and n is a random number corresponding to the randomly sampled context length, that is selected to be inversely proportional to C.
In practice, this leads to short test dialogues having short contexts, while longer dialogues are often broken into short or medium-length segments, with the occasional long context of 10 or more turns.",3.4 Test Set Generation,0,[0]
We consider the task of best response selection.,3.5 Evaluation Metric,0,[0]
"This can be achieved by processing the data as described in Section 3.4, without requiring any human labels.",3.5 Evaluation Metric,0,[0]
"This classification task is an adaptation of the recall and precision metrics previously applied to dialogue datasets [24].
",3.5 Evaluation Metric,0,[0]
"A family of metrics often used in language tasks is Recall@k (denoted R@1 R@2, R@5 below).",3.5 Evaluation Metric,0,[0]
"Here the agent is asked to select the k most likely responses, and it is correct if the true response is among these k candidates.",3.5 Evaluation Metric,0,[0]
"Only the R@1 metric is relevant in the case of binary classification (as in the Table 3 example).
",3.5 Evaluation Metric,0,[0]
"Although a language model that performs well on response classification is not a gauge of good performance on next utterance generation, we hypothesize that improvements on a model with regards to the classification task will eventually lead to improvements for the generation task.",3.5 Evaluation Metric,0,[0]
See Section 6 for further discussion of this point.,3.5 Evaluation Metric,0,[0]
"To provide further evidence of the value of our dataset for research into neural architectures for dialogue managers, we provide performance benchmarks for two neural learning algorithms, as well as one naive baseline.",4 Learning Architectures for Unstructured Dialogues,0,[0]
"The approaches considered are: TF-IDF, Recurrent Neural networks (RNN), and Long Short-Term Memory (LSTM).",4 Learning Architectures for Unstructured Dialogues,0,[0]
"Prior to applying each method, we perform standard pre-processing of the data using the NLTK7 library and Twitter tokenizer8 to parse each utterance.",4 Learning Architectures for Unstructured Dialogues,0,[0]
"We use generic tags for various word categories, such as names, locations, organizations, URLs, and system paths.
7www.nltk.org/ 8http://www.ark.cs.cmu.edu/TweetNLP/
To train the RNN and LSTM architectures, we process the full training Ubuntu Dialogue Corpus into the same format as the test set described in Section 3.4, extracting (context, response, flag) triples from dialogues.",4 Learning Architectures for Unstructured Dialogues,0,[0]
"For the training set, we do not sample the context length, but instead consider each utterance (starting at the 3rd one) as a potential response, with the previous utterances as its context.",4 Learning Architectures for Unstructured Dialogues,0,[0]
So a dialogue of length 10 yields 8 training examples.,4 Learning Architectures for Unstructured Dialogues,0,[0]
"Since these are overlapping, they are clearly not independent, but we consider this a minor issue given the size of the dataset (we further alleviate the issue by shuffling the training examples).",4 Learning Architectures for Unstructured Dialogues,0,[0]
Negative responses are selected at random from the rest of the training data.,4 Learning Architectures for Unstructured Dialogues,0,[0]
"Term frequency-inverse document frequency is a statistic that intends to capture how important a given word is to some document, which in our case is the context [20].",4.1 TF-IDF,0,[0]
It is a technique often used in document classification and information retrieval.,4.1 TF-IDF,0,[0]
"The ‘term-frequency’ term is simply a count of the number of times a word appears in a given context, while the ‘inverse document frequency’ term puts a penalty on how often the word appears elsewhere in the corpus.",4.1 TF-IDF,0,[0]
"The final score is calculated as the product of these two terms, and has the form:
tfidf(w, d,D) = f(w, d)×log N|{d ∈ D :",4.1 TF-IDF,0,[0]
"w ∈ d}| ,
where f(w, d) indicates the number of times word w appeared in context d, N is the total number of dialogues, and the denominator represents the number of dialogues in which the word w appears.
",4.1 TF-IDF,0,[0]
"For classification, the TF-IDF vectors are first calculated for the context and each of the candidate responses.",4.1 TF-IDF,0,[0]
"Given a set of candidate response vectors, the one with the highest cosine similarity to the context vector is selected as the output.",4.1 TF-IDF,0,[0]
"For Recall@k, the top k responses are returned.",4.1 TF-IDF,0,[0]
Recurrent neural networks are a variant of neural networks that allows for time-delayed directed cycles between units [17].,4.2 RNN,0,[0]
"This leads to the formation of an internal state of the network, ht, which allows it to model time-dependent data.",4.2 RNN,0,[0]
"The internal state is updated at each time step as some function of the observed variables xt, and the hidden state at the previous time step ht−1.",4.2 RNN,0,[0]
"Wx and
Wh are matrices associated with the input and hidden state.
",4.2 RNN,0,[0]
"ht = f(Whht−1 +Wxxt).
",4.2 RNN,0,[0]
A diagram of an RNN can be seen in Figure 2.,4.2 RNN,0,[0]
"RNNs have been the primary building block of many current neural language models [22, 28], which use RNNs for an encoder and decoder.",4.2 RNN,0,[0]
"The first RNN is used to encode the given context, and the second RNN generates a response by using beam-search, where its initial hidden state is biased using the final hidden state from the first RNN.",4.2 RNN,0,[0]
"In our work, we are concerned with classification of responses, instead of generation.",4.2 RNN,0,[0]
"We build upon the approach in [2], which has also been recently applied to the problem of question answering [33].
",4.2 RNN,0,[0]
We utilize a siamese network consisting of two RNNs with tied weights to produce the embeddings for the context and response.,4.2 RNN,0,[0]
"Given some input context and response, we compute their embeddings — c, r ∈ Rd, respectively — by feeding the word embeddings one at a time into its respective RNN.",4.2 RNN,0,[0]
"Word embeddings are initialized using the pre-trained vectors (Common Crawl, 840B tokens from [19]), and fine-tuned during training.",4.2 RNN,0,[0]
"The hidden state of the RNN is updated at each step, and the final hidden state represents a summary of the input utterance.",4.2 RNN,0,[0]
"Using the final hidden states from both RNNs, we then calculate the probability that this is a valid pair:
p(flag = 1|c, r) = σ(cTMr + b),
where the bias b and the matrix M ∈ Rd×d are
learned model parameters.",4.2 RNN,0,[0]
"This can be thought of as a generative approach; given some input response, we generate a context with the product c′ = Mr, and measure the similarity to the actual context using the dot product.",4.2 RNN,0,[0]
This is converted to a probability with the sigmoid function.,4.2 RNN,0,[0]
"The model is trained by minimizing the cross entropy of all labeled (context, response) pairs [33]:
L = − log ∏ n",4.2 RNN,0,[0]
"p(flagn|cn, rn) + λ 2 ||θ||F2
where ||θ||F2 is the Frobenius norm of θ = {M, b}.",4.2 RNN,0,[0]
"In our experiments, we use λ = 0 for computational simplicity.
",4.2 RNN,0,[0]
"For training, we used a 1:1 ratio between true responses (flag = 1), and negative responses (flag=0) drawn randomly from elsewhere in the training set.",4.2 RNN,0,[0]
The RNN architecture is set to 1 hidden layer with 50 neurons.,4.2 RNN,0,[0]
"The Wh matrix is initialized using orthogonal weights [23], while Wx is initialized using a uniform distribution with values between -0.01 and 0.01.",4.2 RNN,0,[0]
"We use Adam as our optimizer [15], with gradients clipped to 10.",4.2 RNN,0,[0]
We found that weight initialization as well as the choice of optimizer were critical for training the RNNs.,4.2 RNN,0,[0]
"In addition to the RNN model, we consider the same architecture but changed the hidden units to long-short term memory (LSTM) units [12].",4.3 LSTM,0,[0]
LSTMs were introduced in order to model longerterm dependencies.,4.3 LSTM,0,[0]
"This is accomplished using a series of gates that determine whether a new input should be remembered, forgotten (and the old value retained), or used as output.",4.3 LSTM,0,[0]
The error signal can now be fed back indefinitely into the gates of the LSTM unit.,4.3 LSTM,0,[0]
"This helps overcome the vanishing and exploding gradient problems in standard RNNs, where the error gradients would otherwise decrease or increase at an exponential rate.",4.3 LSTM,0,[0]
"In training, we used 1 hidden layer with 200 neurons.",4.3 LSTM,0,[0]
The hyper-parameter configuration (including number of neurons) was optimized independently for RNNs and LSTMs using a validation set extracted from the training data.,4.3 LSTM,0,[0]
"The results for the TF-IDF, RNN, and LSTM models are shown in Table 4.",5 Empirical Results,0,[0]
"The models were evaluated using both 1 (1 in 2) and 9 (1 in 10) false
examples.",5 Empirical Results,0,[0]
"Of course, the Recall@2 and Recall@5 are not relevant in the binary classification case.
",5 Empirical Results,0,[0]
We observe that the LSTM outperforms both the RNN and TF-IDF on all evaluation metrics.,5 Empirical Results,0,[0]
It is interesting to note that TF-IDF actually outperforms the RNN on the Recall@1 case for the 1 in 10 classification.,5 Empirical Results,0,[0]
"This is most likely due to the limited ability of the RNN to take into account long contexts, which can be overcome by using the LSTM.",5 Empirical Results,0,[0]
"An example output of the LSTM where the response is correctly classified is shown in Table 5.
",5 Empirical Results,0,[0]
"We also show, in Figure 3, the increase in performance of the LSTM as the amount of data used for training increases.",5 Empirical Results,0,[0]
This confirms the importance of having a large training set.,5 Empirical Results,0,[0]
"This paper presents the Ubuntu Dialogue Corpus, a large dataset for research in unstructured multiturn dialogue systems.",6 Discussion,0,[0]
We describe the construction of the dataset and its properties.,6 Discussion,0,[0]
The availability of a dataset of this size opens up several interesting possibilities for research into dialogue systems based on rich neural-network architectures.,6 Discussion,0,[0]
We present preliminary results demonstrating use of this dataset to train an RNN and an LSTM for the task of selecting the next best response in a conversation; we obtain significantly better results with the LSTM architecture.,6 Discussion,0,[0]
There are several interesting directions for future work.,6 Discussion,0,[0]
Our approach to conversation disentanglement consists of a small set of rules.,6.1 Conversation Disentanglement,0,[0]
"More sophisticated techniques have been proposed, such as training a maximum-entropy classifier to cluster utterances into separate dialogues [6].",6.1 Conversation Disentanglement,0,[0]
"However, since we are not trying to replicate the exact conversation between two users, but only to retrieve plausible natural dialogues, the heuristic method presented in this paper may be sufficient.",6.1 Conversation Disentanglement,0,[0]
"This seems supported through qualitative examination of the data, but could be the subject of more formal evaluation.",6.1 Conversation Disentanglement,0,[0]
One of the interesting properties of the response selection task is the ability to alter the task difficulty in a controlled manner.,6.2 Altering Test Set Difficulty,0,[0]
"We demonstrated this by moving from 1 to 9 false responses, and by varying the Recall@k parameter.",6.2 Altering Test Set Difficulty,0,[0]
"In the future, instead of choosing false responses randomly, we will consider selecting false responses that are similar to the actual response (e.g. as measured by cosine similarity).",6.2 Altering Test Set Difficulty,0,[0]
"A dialogue model that performs well on this more difficult task should also manage to capture a more fine-grained semantic meaning of sentences, as compared to a model that naively picks replies with the most words in common with the context such as TF-IDF.",6.2 Altering Test Set Difficulty,0,[0]
The work described here focuses on the task of response selection.,6.3 State Tracking and Utterance Generation,0,[0]
This can be seen as an intermediate step between slot filling and utterance generation.,6.3 State Tracking and Utterance Generation,0,[0]
"In slot filling, the set of candidate outputs (states) is identified a priori through knowledge
engineering, and is typically smaller than the set of responses considered in our work.",6.3 State Tracking and Utterance Generation,0,[0]
"When the set of candidate responses is close to the size of the dataset (e.g. all utterances ever recorded), then we are quite close to the response generation case.
",6.3 State Tracking and Utterance Generation,0,[0]
There are several reasons not to proceed directly to response generation.,6.3 State Tracking and Utterance Generation,0,[0]
"First, it is likely that current algorithms are not yet able to generate good results for this task, and it is preferable to tackle metrics for which we can make progress.",6.3 State Tracking and Utterance Generation,0,[0]
"Second, we do not yet have a suitable metric for evaluating performance in the response generation case.",6.3 State Tracking and Utterance Generation,0,[0]
One option is to use the BLEU [18] or METEOR,6.3 State Tracking and Utterance Generation,0,[0]
[16] scores from machine translation.,6.3 State Tracking and Utterance Generation,0,[0]
"However, using BLEU to evaluate dialogue systems has been shown to give extremely low scores [28], due to the large space of potential sensible responses [7].",6.3 State Tracking and Utterance Generation,0,[0]
"Further, since the BLEU score is calculated using N-grams [18], it would provide a very low score for reasonable responses that do not have any words in common with the ground-truth next utterance.
",6.3 State Tracking and Utterance Generation,0,[0]
"Alternatively, one could measure the difference between the generated utterance and the actual sentence by comparing their representations in some embedding (or semantic) space.",6.3 State Tracking and Utterance Generation,0,[0]
"However, different models inevitably use different embeddings, necessitating a standardized embedding for evaluation purposes.",6.3 State Tracking and Utterance Generation,0,[0]
"Such a standardized embeddings has yet to be created.
",6.3 State Tracking and Utterance Generation,0,[0]
"Another possibility is to use human subjects to score automatically generated responses, but time and expense make this a highly impractical option.
",6.3 State Tracking and Utterance Generation,0,[0]
"In summary, while it is possible that current language models have outgrown the use of slot filling as a metric, we are currently unable to measure their ability in next utterance generation in a standardized, meaningful and inexpensive way.",6.3 State Tracking and Utterance Generation,0,[0]
This motivates our choice of response selection as a useful metric for the time being.,6.3 State Tracking and Utterance Generation,0,[0]
The authors gratefully acknowledge financial support for this work by the Samsung Advanced Institute of Technology (SAIT) and the Natural Sciences and Engineering Research Council of Canada (NSERC).,Acknowledgments,0,[0]
"We would like to thank Laurent Charlin for his input into this paper, as well as Gabriel Forgues and Eric Crawford for interesting discussions.",Acknowledgments,0,[0]
"This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words.",abstractText,0,[0]
This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data.,abstractText,0,[0]
"The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter.",abstractText,0,[0]
"We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.",abstractText,0,[0]
The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems,title,0,[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 250–259, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,0,[0]
"Currently, the amount of on-line information generated per week reaches the same quantity of data that the one produced in the Internet between its inception and 2003, time of the Social Network emergency (Cambria and White, 2014).",1 Introduction,0,[0]
"Moreover, the production of such volume of data is delivered in multiple languages, and accessing the relevant content of information or extracting the main features of documents in a competitive time is more and more challenging.",1 Introduction,0,[0]
"Therefore, automatic tasks that can help processing all this information, such as multilingual text summarization techniques, are now becoming essential.
",1 Introduction,0,[0]
"Back in 2011, the Text Analysis Conference MultiLing Pilot task1 was first introduced as an effort of the community to promote and support the development of multilingual document summarization research.",1 Introduction,0,[0]
"Considering the impact of this shared tasks in the progress of natural language processing technologies, a mutlilingual summarization workshop was also organized in 20132.
",1 Introduction,0,[0]
"Nowadays, in 2015, we take part in the 3rd MultiLing event3.",1 Introduction,0,[0]
"In this edition, new tasks have been added in order to adapt to social requirements.",1 Introduction,0,[0]
"There were the traditional Multilingual Multi-document and Single-document Summarization (MMS and MSS), coming from previous events, but also new summarization tasks related to Online Fora (OnForumS) - on how to deal with reader comments- and Call Center Conversation (CCCS) - from spoken conversations to textual synopses.
",1 Introduction,0,[0]
"Taking into consideration the interest that multilingual summarization approaches is gaining among the research community, and the positive impact and benefits it may have for the society, the objective of this paper is to present a multilingual summarization approach within the MultiLing 2015 competition, discussing its potentials and limitations, and providing some insights of the future of this type of summarization based on the average results obtained by us and other participants as well.
",1 Introduction,0,[0]
The remaining of the paper is organized as follows.,1 Introduction,0,[0]
"In Section 2 we review the most relevant multilingual summarization approaches, some of them participating in previous MultiLing events.",1 Introduction,0,[0]
"In Section 3, we explain our multilingual summarization approach and the required languagedependent knowledge.",1 Introduction,0,[0]
"Section 4 describes the
1http://www.nist.gov/tac/2011/Summarization/ 2http://multiling.iit.demokritos.gr/pages/view/662/multiling-
2013 3http://multiling.iit.demokritos.gr/pages/view/1516/multiling2015
250
task in which we participated, and the experiments performed.",1 Introduction,0,[0]
"Furthermore, the results together with their discussion and comparison to other participants are provided in Section 5, followed by an analysis of the potentials and limitations of our approach in Section 6.",1 Introduction,0,[0]
"Finally, the main conclusions are outlined in Section 7.",1 Introduction,0,[0]
"Eight teams participated in the Multilingual Pilot task in 2011, five of them testing their approaches for all the proposed languages (Arabic, Czech, English, French, Greek, Hebrew, and Hindi) (Giannakopoulos et al., 2011).",2 Related work,0,[0]
Two systems are worth mentioning.,2 Related work,0,[0]
"On the one hand, the CLASSY system (Conroy et al., 2011) that ranked 2nd or 3rd in 5 out 7 languages.",2 Related work,0,[0]
"The main feature of this approach was that a model was first trained on a corpus of newswire taken from Wikinews, and then term scoring was limited to the naive Bayes term weighting.",2 Related work,0,[0]
The final process of sentence selection was performed using non-negative matrix factorization and integer programming techniques.,2 Related work,0,[0]
"On the other hand, the best system on average was the one in (Steinberger et al., 2011), performing the 1st in five of the seven languages, and 4th in the two remaining ones.",2 Related work,0,[0]
"This approach did not used any language-dependent resources, apart from a stopword list for each language, and it relied on Latent Semantic Analysis and Singular Value Decomposition.
",2 Related work,0,[0]
"In the 2013 MultiLing edition, four teams participated submitting six systems to the task (Giannakopoulos, 2013).",2 Related work,0,[0]
"For their assessment in (Kubina et al., 2013), they were denoted as MUSE, MD, AIS and LAN.",2 Related work,0,[0]
We briefly reviewed these approaches.,2 Related work,0,[0]
"MUSE (Litvak and Last, 2013), is a supervised learning approach that scores sets of sentences by means of a genetic algorithm.",2 Related work,0,[0]
"MD (Conroy et al., 2013) developed techniques both for MMS and MSS, examining the impact of dimensionality reduction and offering different weighting methods in the experiments: either considering the frequency of terms or applying a variant of TextRank, among others.",2 Related work,0,[0]
"Adapting their techniques to Arabic and English languages, the LAN team (El-Haj and Rayson, 2013) implements a system that recovers the most significant sentences for the summary using word frequency and keyness score, introducing a statistic approach that extracts those sentences with
the maximum sum of log likelihood.",2 Related work,0,[0]
"Contrary to the previously described systems, mostly based in frequency of terms, AIS (Anechitei and Ignat, 2013) presented an approach based on the analysis of the discourse structure, exploiting, therefore, cohesion and coherence properties from the source articles.",2 Related work,0,[0]
"Although some of these participants performed well, achieving similar results as the ones obtained by human summaries, the WBU approach (Steinberger, 2013), was again the best performing summarization system in this MultiLing edition, reaching the first position in 5 of the 10 languages.",2 Related work,0,[0]
"Specifically, it was an improved version of the best-performing approach in MultiLing 2011 (Steinberger et al., 2011).
",2 Related work,0,[0]
"Outside the MultiLing competitions, other research works have been recently proposed, obtaining better results than existing commercial multilingual summarizers.",2 Related work,0,[0]
"An example of this can be found in (Lloret and Palomar, 2011) were three different approaches were analyzed and tested: i) one using language-independent techniques; ii) one with language-dependent resources; and iii) one using machine translation to monolingual summarization.",2 Related work,0,[0]
"The results obtained showed that having high-quality language specific resources often led to the best results; however, a simple language-independent approach based on term frequency was competitive enough, avoiding the effort needed to develop and/or obtain the particular resources for each language, when they were not available.
",2 Related work,0,[0]
"Having revised different multilingual summarization approaches, the main contribution of our paper is to propose a novel approach based on the Principal Component Analysis (PCA) technique, studying the influence of lexical-semantic knowledge to the base approach.",2 Related work,0,[0]
"To the best of our knowledge, although PCA has been already used for text summarization (for instance, in (Lee et al., 2003)), it has never been tested with the addition of semantic knowledge, nor in the context of multilingual summarization.",2 Related work,0,[0]
"Given that it bears some relation to LSA and SVD techniques, and it has been shown that such techniques are very competitive, MultiLing 2015 is the perfect context to test it.",2 Related work,0,[0]
"In this Section, we present our proposed multilingual summarization approach (i.e., UA-DLSI ap-
proach).",3 The UA-DLSI Approach,0,[0]
"As it was previously mentioned, the main technique that characterise the UA-DLSI approach is the Principal Component Analysis (PCA).",3 The UA-DLSI Approach,0,[0]
"PCA is a statistical technique focused on the synthesis of information to compress and interpret the data (Estellés Arolas et al., 2010).
",3 The UA-DLSI Approach,0,[0]
"As a method for developing summarization systems, PCA provides a way to determine the most relevant key terms of a document.",3 The UA-DLSI Approach,0,[0]
"It has been often employed in conjunction with other data mining techniques, such as Semantic Vector Space model (Vikas et al., 2008) or Singular Value Decomposition (Lee et al., 2005), using term-based frequency methods.",3 The UA-DLSI Approach,0,[0]
"Our main difference with respect to other summarization PCA-based approaches is the incorporation of lexical-semantic knowledge into the PCA technique, since it is necessary to go beyond the terms, and determine the meaningful sentences.",3 The UA-DLSI Approach,0,[0]
"Moreover, to finish the process, some strategies for selecting relevant information (in our case, choosing the most relevant sentences) needs to be defined as well.
",3 The UA-DLSI Approach,0,[0]
"For developing our UA-DLSI approach, we relied on the summary process stages outlined in (Sparck-Jones, 1999): 1) interpretation, 2) transformation and, finally, 3) the summary generation.
",3 The UA-DLSI Approach,0,[0]
Interpretation.,3 The UA-DLSI Approach,0,[0]
The first stage of our approach includes a linguistic and lexical-semantic processing (this latter part is optional).,3 The UA-DLSI Approach,0,[0]
"For the linguistic processing, sentence segmentation, tokenization and stopwords removal is applied.",3 The UA-DLSI Approach,0,[0]
"For the lexicalsemantic processing, a named entity recognizer (Standford Named Entity Recognizer4) and semantic resources, such as WordNet (Miller, 1995) and EuroWordNet (Vossen, 2004) are employed.",3 The UA-DLSI Approach,0,[0]
"Whereas named entity recognizers mainly provide the identification of person, organization and place names in a document (Tjong et al., 2003), the semantic resources used comprises a set of synonyms grouped by means of the synsets that allow us to work with concept better than just with terms.",3 The UA-DLSI Approach,0,[0]
"In this manner, we group a set of synonyms under the same concept.",3 The UA-DLSI Approach,0,[0]
"For instance, detonation and explosion are different words but their share the same synset (07323181), so we would keep them as a single concept.",3 The UA-DLSI Approach,0,[0]
"For identifying concepts, we relied on the most frequent sense approach, and therefore, the process searches for the first synset
4http://nlp.stanford.edu/software/ CRF-NER.shtml
of each word in the document, which corresponds to its most probable meaning.",3 The UA-DLSI Approach,0,[0]
"If two words have the same first synset, we will assume that they are synonyms and their occurrences will be added together.
",3 The UA-DLSI Approach,0,[0]
"The result of this stage is to build an initial lexical-semantic matrix, where for each sentence (rows in our matrix), we identify the units that will be later taken into account (i.e., terms, named entities, and/or concepts) which will correspond to the columns.
",3 The UA-DLSI Approach,0,[0]
Transformation.,3 The UA-DLSI Approach,0,[0]
It is in the transformation stage that we use the PCA method.,3 The UA-DLSI Approach,0,[0]
"In our approach, PCA is applied using the PCA transform Java library5 to process the covariance matrix that is computed from the lexical-semantic matrix obtained in the previous stage.",3 The UA-DLSI Approach,0,[0]
"Once PCA has been applied over the covariance matrix, the principal components (eigenvectors) and its corresponding weight (eigenvalue) are obtained.",3 The UA-DLSI Approach,0,[0]
"The eigenvectors are composed by the contribution of each variable, which determines the importance of the variable in the eigenvector.",3 The UA-DLSI Approach,0,[0]
"Moreover, the eigenvectors are derived in decreasing order of importance.",3 The UA-DLSI Approach,0,[0]
"In this manner, an eigenvector with high eigenvalue carries a great amount of information.",3 The UA-DLSI Approach,0,[0]
"Therefore, the first eigenvectors collect the major part of the information extracted from the covariance matrix, and they will be used for determining the most important sentences in the document, as it will be next shown.
",3 The UA-DLSI Approach,0,[0]
Summary generation.,3 The UA-DLSI Approach,0,[0]
"In this final stage, the relevant sentences are selected and extracted, thus producing an extractive summary.",3 The UA-DLSI Approach,0,[0]
"Since from the previous stage, only the key elements (e.g., concepts) were determined, it is necessary to define some strategies for deciding which sentences containing these elements will be finally taking part in the summary.
",3 The UA-DLSI Approach,0,[0]
"Two strategies were proposed for selecting and ordering the most relevant sentences from the document, leading to two types of summaries: one generic and one topic-focused.",3 The UA-DLSI Approach,0,[0]
"In this manner, taking into account the element with the highest value for each eigenvector from the PCA matrix, we select and extract:
• one sentence (searching in order of appearance in the original text) in which
5https://github.com/mkobos/pca_ transform
such concept6 appears.",3 The UA-DLSI Approach,0,[0]
"During this process, if a sentence had been already selected by a previous concept to take part in the summary, we would select and extract the following sentence in which the concept appears (generic summary).
",3 The UA-DLSI Approach,0,[0]
"• all the sentences (searched in order of appearance in the original text) in which such concept appears (topic-focused summary).
",3 The UA-DLSI Approach,0,[0]
"Regarding these strategies, it is worth mentioning that if we found different concepts with the same highest value for the same eigenvector, we would extract the corresponding sentences for all these concepts.",3 The UA-DLSI Approach,0,[0]
"In the same manner, if a synset is represented by several synonyms, we would extract the corresponding sentences for each of these synonyms.",3 The UA-DLSI Approach,0,[0]
"This section describes the MultiLing 2015 task in which we participated, together with the dataset employed, and the explanation of the different variants of our approach submitted to the competition.",4 Experimental Setup,0,[0]
"The Multilingual Single Document Summarization task was initially proposed in MultiLing 2013, targeting the same goal in the current edition: to evaluate the performance of participant systems whose work is focused on generating a single document summary for all the given Wikipedia articles in some of the languages provided (at least the participants should select three languages).",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"In the context of MultiLing 2015, two datasets were provided for the MSS task: a training dataset, containing 30 articles for each of the 38 available languages with their corresponding human-generated summaries; and a test dataset, which contains the same number of documents per language, but different from the training dataset, the human summaries were not provided.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"For both datasets, the character length that the target automatic summaries should aim was also provided (i.e., the target length), which coincided with the length of the human summaries that will be later used in the
6Concepts here refer to the possible elements that the matrix can have, e.g. named entities, synsets, or terms
evaluation.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"Each automatic summary had to be as close to the target length provided as possible, and summaries exceeding the given target length were truncated to it.
",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"In order to prove the adequacy of our approach to select the relevant sentences from a document, we decided to start testing it within small goals to be able to analyze and further improve the proposed approach.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"This was the main reason for participating in the MSS task rather than in the MMS, which had implied more complexity.
",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"Concerning the language choice, since one of our main objectives was to evaluate the impact of lexical-semantic knowledge in the summary generation, some language-dependent resources were necessary (e.g. WordNet and EuroWordNet).",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"The availability of these resources also conditioned the languages that were chosen for testing our apporach, in our case: English, German, and Spanish.
",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"For each language considered, we computed the average length of the Wikipedia articles in the test corpus, both in characters and words.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
These figures are shown in Table 1.,4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"In addition, we also provide the target summary length (in characters) and the compression ratio for the summaries.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"As it can be seen, the length of the summaries compared to the original length of the Wikipedia articles (i.e., compression ratio) is very short, always below 10%.",4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
This means that generated summaries have to be very concise and precise in selecting the most relevant information.,4.1 MSS - Multilingual Single-Document Summarization Task,0,[0]
"Having provided the information about the general multilingual summarization process in Section 3, and since each participant in the MSS task was allowed to submit up to six approaches, different versions of our approach were set to participate in MultiLing 2015.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"Apart of the two types of summaries that could
be generated with our approach (T1: generic summary; T3: topic-focused summary), the incorporation of lexical-semantic knowledge was an optional substage, so we decided to test our approach also without any type of semantic knowledge, other than a list of stopwords for each language (LI: language-independent; LEX: using lexical knowlege (named entity recognition); SEM: using semantic knowledge (i.e., WordNet and EuroWordNet)).",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
This way the performance of a fully language-independent summarization approach based on PCA could be also analyzed.,4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"Moreover, due to the nature of the test dataset (Wikipedia articles), all documents included headings for structuring different sections within them, so we opt for taking advantage of this information, considering only the words in these headings for the matrix construction (OWFH), instead of working with all words in the document, except stopwords (AW).",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
Headings usually contain important concepts that reflect the main topic of the section that follows.,4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"Considering only this words, we also reduce the amount of information we have to process by 99% of the PCA matrix.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"Therefore, given the impossibility to test all the variations taking into account these issues, our submitted approaches for MultiLing 2015, specifying also their priority, were the following:
•",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"T1 LI AW (UA-DLSI-lang-1): generic language-independent summarizer considering all words in the documents.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"• T1 LI OWFH (UA-DLSI-lang-3): generic language-independent summarizer considering only the words included in the headings of the documents.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"• T1 LEXSEM AW (UA-DLSI-lang-4): generic summarizer, including lexicalsemantic knowledge into the interpretation stage, and",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"considering all words in the documents.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"• T3 LI OWFH (UA-DLSI-lang-5): topicfocused language-independent summarizer considering only the words included in the headings of the documents.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"• T3 LEXSEM AW (UA-DLSI-lang-6): topic-focused summarizer, including lexicalsemantic knowledge into the interpretation stage, and",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"considering all words in the documents.
",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"• T3 LEXSEM OWFH (UA-DLSI-lang-2): topic-focused summarizer, including lexicalsemantic knowledge into the interpretation stage, but considering only the words included in the headings of the documents.",4.2 Configuring the UA-DLSI approach to the MSS task,0,[0]
"After all participants submitted their runs to the MultiLing 2015 MSS task over the test dataset, the summaries were evaluated via automatic methods.",5 Results and Analysis,0,[0]
"ROUGE tool (Lin, 2004) was employed for automatic content evaluation, which allows the comparison between automatic and model summaries based on different types of n-grams.",5 Results and Analysis,0,[0]
"Specifically the ROUGE 1 (unigrams), 2 (bigrams), 3 (trigrams), and 4 (quadrigrams), ROUGE-SU4 (bigram similarity skipping unigrams) scores were computed.",5 Results and Analysis,0,[0]
"The files contain the overall and individual summary scores.
",5 Results and Analysis,0,[0]
"Moreover, two additional systems were proposed by the organizers.",5 Results and Analysis,0,[0]
"On the one hand, a system called “Lead”, which was the baseline summary used for the evaluation process.",5 Results and Analysis,0,[0]
This approach selects the leading substring of the article’s body text having the same length as the human summary of the article.,5 Results and Analysis,0,[0]
"On the other hand, a system called “Oracles” was also developed, where sentences were selected from the body text to maximally cover the tokens in the human summary using as few sentences as possible until its size exceeded the human summary, upon which it was truncated.
",5 Results and Analysis,0,[0]
"In this edition, five systems participated in the MSS task (details about their implementation have not made available yet).",5 Results and Analysis,0,[0]
"Three of them were applied to 38 languages, including English, Spanish and German.",5 Results and Analysis,0,[0]
They are named as CCS - that implements five variations for each language- LCSIESI and EXB.,5 Results and Analysis,0,[0]
"The fourth one, BGU-SCE has been proven for Arabic and Hebrew, besides English.
",5 Results and Analysis,0,[0]
"Table 2, Table 3, and Table 4 show the results obtained by all participants, and the two methods proposed by the organizers in the MultiLing 2015 competition for English, German, and Spanish.",5 Results and Analysis,0,[0]
"Due to size constraints, only the average results for the recall, precision and F-measure metrics of ROUGE 1 are shown, since this ROUGE metric takes into account the common vocabulary between the automatic and the human summaries, without taking into account stopwords.
",5 Results and Analysis,0,[0]
"Focusing only on the analysis of our six versions of our approach (UA-DLIS-lang-priority),
we observe that our approach with priority 3 is one of our best performing approaches considering the precision for the three tested languages.",5 Results and Analysis,0,[0]
"This version corresponds to T1 LI OWFH approach - generic language-independent summarizer considering only the words included in the headings of the documents, and this means that the title headings of the Wikipedia articles do contain enough meaningful information of the documents.",5 Results and Analysis,0,[0]
"This is an interesting finding, because we are reducing the amount of information to be processed by almost 99%.",5 Results and Analysis,0,[0]
"Moreover, this also outlines the potential of the studied PCA technique for developing completely language-independent summarizers.
",5 Results and Analysis,0,[0]
"Other versions of our proposed approach, such as the ones submitted as priority 4, and priority 1 may obtained also competitive results for some languages.",5 Results and Analysis,0,[0]
"Again, the submission with priority 1 correspond to a generic language-independent summarizer considering all words in the documents (T1 LI AW).",5 Results and Analysis,0,[0]
"It can be shown that when considering all words in the documents, instead of only the words in the headings, recall values im-
prove, but for some languages, e.g. German, to take into account all the words does not have a positive influence in general.",5 Results and Analysis,0,[0]
"Regarding the submission with priority 4 (T1 LEXSEM AW), the inclusion of lexical-semantic knowledge has been beneficial for the English results, but not for the other languages.",5 Results and Analysis,0,[0]
This may be due to the type of semantic knowledge that is being used.,5 Results and Analysis,0,[0]
"WordNet for English is much bigger in size than for German and Spanish, and therefore, this could influence the results, not obtaining the expected improvements that were expected by using languagedependent resources.",5 Results and Analysis,0,[0]
"Generally speaking, from our approaches, apart from the previously mentioned findings, we can also observe that when summarizing Wikipedia articles, generic summarization has been shown to be more appropriate.
",5 Results and Analysis,0,[0]
"Analyzing all the results achieved by the other participants, we can observe that German is the language, among the three analyzed languages within our scope, that obtains poorer ROUGE results.",5 Results and Analysis,0,[0]
"This could occur since the summaries had a compression ratio lower than 3%, which is a
very low compression ratio for the summarization task.",5 Results and Analysis,0,[0]
"Moreover, it can be seen from the tables, that all systems overperformed the “Lead” baseline, but none of them surpassed the “Oracles” system.",5 Results and Analysis,0,[0]
This was expected since the “Oracles” system was kind of upper boundary for the MSS task.,5 Results and Analysis,0,[0]
"Among the systems, the best performing ones taking into account the ROUGE 1 F-measure value were: the BGU-SCE team with their submission BGU-SCE-M-en-5 for English; CCS team, with CCS-de-4 for German; and again CCS team with CCS-es-3 for Spanish.",5 Results and Analysis,0,[0]
"Taking into account the different submissions, our versions were not among the best performing approaches, despite obtaining results in line of the other participants.",5 Results and Analysis,0,[0]
"In general, there were not very big differences in results between the teams.",5 Results and Analysis,0,[0]
"In this sense, according to ROUGE 1 F-measure, we ranked7 15th out of 22nd for English with our UA-DLSI-en-4 submission; 7th out of 13th for German with our UADLSI-de-3 submission; and 8th out of 13th with our UA-DLSI-es-1 submission.",5 Results and Analysis,0,[0]
"As it was previously discussed, for German and Spanish, the best submissions were the ones without using any type of lexical-semantic knowledge, whereas for English the use of a named entity recognizer, and a semantic knowledge base led to an improvement over the language-independent approach.
",5 Results and Analysis,0,[0]
7The two systems provided by the organization has not been taken into account for the ranking.,5 Results and Analysis,0,[0]
"From our participation in MultiLing 2015, we have tested our approach in a real competition and compared its performance with respect to stateof-the-art multilingual summarizers.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"Although in general terms, the best versions of our approach ranked at intermediate positions, the participation and evaluation process has been a positive issue for learning from errors, as well as gaining some insights into potentials and limitations that our approach and in general the multilingual summarization task may have.
",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"After analyzing the performance of the different system configurations, it becomes clear that some of our assumptions need to be reviewed.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"Nevertheless, good positions were achieved when reducing the words to compute the PCA algorithm, which let us infer that article section headings contain enough information to produce accurate and precise summaries, while decreasing the amount of information to be processed by the system.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"Moreover, our results indicate that using PCA present advantages when language independent processing is required.
",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"On the other hand, the limitations encountered are mostly related to inclusion of lexical-semantic knowledge.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"As it requires the use of external resources, the system performance becomes dependent of some aspects such as their quality, availability and size.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"The version of the system tak-
ing into account this kind of background obtains better results in English language, for which resources as WordNet have reached a state of maturity higher than for other languages.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"In addition, and regarding the format of the source documents (Wikipedia articles), topic-focused summaries have been shown to be less adequate than generic summarization.
",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"Concerning the multilingual summarization task from a broader perspective, it is worth stressing that this is a challenging task.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"On the one hand, language-independent methods exist, and they offer more capabilities to be employed for a wide range of languages; however, this type of techniques do not take into account any semantic analysis, so it is difficult that only with these techniques, abstractive summaries can be produced, thus limiting mostly to extractive summarization.
",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"In the context of the MSS task, the summary compression ratio was extremely low, compared to the length of the original documents.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"This posed the task even more challenging, since the generated summaries had to be very concise as well as precise.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"Nevertheless, it is of great value to organize this type of events and have the possibility to participate in order to advance the state of the art, addressing difficult summarization challenges necessary in the current society.",6 Potentials and Limitations of the UA-DLSI Approach,0,[0]
"In this paper we described our participation in MultiLing 2015 - Multilingual Single-document
Summarization task, presenting our approach and comparing and discussing the results obtained with respect to the other participants in the task.
",7 Conclusions,0,[0]
"Our initial development was focused on the application of the PCA technique, given its suitability for developing language-independent approaches.",7 Conclusions,0,[0]
"Although some related work has been done on summarization, we contributed to the state of the art extending the PCA scope by the inclusion of lexical and semantic knowledge in its implementation and testing it in a multilingual scenario.
",7 Conclusions,0,[0]
"Our approach was tested in three languages, English, German, and Spanish, and six different configurations were submitted to the competition, obtaining average results when compared to other participants.
",7 Conclusions,0,[0]
"From our participation in MultiLing 2015, and the further analysis of our PCA based approach given the results obtained, three main conclusions can be drawn: i) PCA is a good technique for generating language-independent summaries; ii) generic summaries were more appropriate for the type of documents dealt with (i.e., Wikipedia documents); and iii) the title headings of Wikipedia articles were meaningful enough to build the PCA matrix in the summarization process, discarding the remaining words of the document.",7 Conclusions,0,[0]
"Although this version of our approach worked with very few content, it was shown to be one of our best performing approaches.",7 Conclusions,0,[0]
"This research work has been partially funded by the University of Alicante, Generalitat Valenciana, Spanish Government and the European Commission through the projects, “Tratamiento inteligente de la información para la ayuda a la toma de decisiones” (GRE12-44), “Explotación y tratamiento de la información disponible en Internet para",Acknowledgments,0,[0]
"la anotación y generación de textos adaptados al usuario” (GRE13-15), DIIM2.0 (PROMETEOII/2014/001), ATTOS (TIN2012-38536-C0303), LEGOLANG-UAGE (TIN2012-31224), and SAM(FP7-611312).",Acknowledgments,0,[0]
In this paper we present the approach and results of our participation in the 2015 MultiLing Single-document Summarization task.,abstractText,0,[0]
Our approach is based on the Principal Component Analysis (PCA) technique enhanced with lexical-semantic knowledge.,abstractText,0,[0]
"For testing our approach, different configurations were set up, thus generating different types of summaries (i.e., generic and topic-focused), as well as testing some language-specific resources on top of the language-independent basic PCA approach, submitting a total of 6 runs for each selected language (English, German, and Spanish).",abstractText,0,[0]
"Our participation in MultiLing has been very positive, ranking at intermediate positions when compared to the other participant systems, showing that PCA is a good technique for generating language-independent summaries, but the addition of lexical-semantic knowledge may heavily depend on the size and quality of the resources available for each language.",abstractText,0,[0]
"The University of Alicante at MultiLing 2015: approach, results and further insights",title,0,[0]
"Proceedings of NAACL-HLT 2018, pages 641–651 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,0,[0]
"Humans often want to answer complex questions that require reasoning over multiple pieces of evidence, e.g., “From what country is the winner of the Australian Open women’s singles 2008?”.",1 Introduction,0,[0]
"Answering such questions in broad domains can be quite onerous for humans, because it requires searching and integrating information from multiple sources.
",1 Introduction,0,[0]
"Recently, interest in question answering (QA) has surged in the context of reading comprehension (RC), where an answer is sought for a question given one or more documents (Hermann et al., 2015; Joshi et al., 2017; Rajpurkar et al., 2016).
",1 Introduction,0,[0]
"Neural models trained over large datasets led to great progress in RC, nearing human-level performance (Wang et al., 2017).",1 Introduction,0,[0]
"However, analysis of models revealed (Jia and Liang, 2017; Chen et al., 2016) that they mostly excel at matching questions to local contexts, but struggle with questions that require reasoning.",1 Introduction,0,[0]
"Moreover, RC assumes documents with the information relevant for the answer are available – but when questions are complex, even retrieving the documents can be difficult.
",1 Introduction,0,[0]
"Conversely, work on QA through semantic parsing has focused primarily on compositionality: questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Liang et al., 2011).",1 Introduction,0,[0]
"However, this reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers.
",1 Introduction,0,[0]
"In this paper we present a framework for QA that is broad, i.e., it does not assume information is in a KB or in retrieved documents, and compositional, i.e., to compute an answer we must perform some computation or reasoning.",1 Introduction,0,[0]
"Our thesis is that answering simple questions can be achieved
641
by combining a search engine with a RC model.",1 Introduction,0,[0]
"Thus, answering complex questions can be addressed by decomposing the question into a sequence of simple questions, and computing the answer from the corresponding answers.",1 Introduction,0,[0]
Figure 1 illustrates this idea.,1 Introduction,0,[0]
"Our model decomposes the question in the figure into a sequence of simple questions, each is submitted to a search engine, and then an answer is extracted from the search result.",1 Introduction,0,[0]
"Once all answers are gathered, a final answer can be computed using symbolic operations such as union and intersection.
",1 Introduction,0,[0]
To evaluate our framework we need a dataset of complex questions that calls for reasoning over multiple pieces of information.,1 Introduction,0,[0]
"Because an adequate dataset is missing, we created COMPLEXWEBQUESTIONS, a new dataset for complex questions that builds on WEBQUESTIONSSP, a dataset that includes pairs of simple questions and their corresponding SPARQL query.",1 Introduction,0,[0]
"We take SPARQL queries from WEBQUESTIONSSP and automatically create more complex queries that include phenomena such as function composition, conjunctions, superlatives and comparatives.",1 Introduction,0,[0]
"Then, we use Amazon Mechanical Turk (AMT) to generate natural language questions, and obtain a dataset of 34,689 question-answer pairs (and also SPARQL queries that our model ignores).",1 Introduction,0,[0]
"Data analysis shows that examples are diverse and that AMT workers perform substantial paraphrasing of the original machine-generated question.
",1 Introduction,0,[0]
We propose a model for answering complex questions through question decomposition.,1 Introduction,0,[0]
"Our model uses a sequence-to-sequence architecture (Sutskever et al., 2014) to map utterances to short programs that indicate how to decompose the question and compose the retrieved answers.",1 Introduction,0,[0]
"To obtain supervision for our model, we perform a noisy alignment from machine-generated questions to natural language questions and automatically generate noisy supervision for training.1
We evaluate our model on COMPLEXWEBQUESTIONSand find that question decomposition substantially improves precision@1 from 20.8 to 27.5.",1 Introduction,0,[0]
"We find that humans are able to reach 63.0 precision@1 under a limited time budget, leaving ample room for improvement in future work.
",1 Introduction,0,[0]
"To summarize, our main contributions are:
1We differ training from question-answer pairs for future work.
1.",1 Introduction,0,[0]
A framework for answering complex questions through question decomposition.,1 Introduction,0,[0]
2,1 Introduction,0,[0]
A sequence-to-sequence model for question decomposition that substantially improves performance.,1 Introduction,0,[0]
3,1 Introduction,0,[0]
"A dataset of 34,689 examples of complex and broad questions, along with answers, web snippets, and SPARQL queries.
",1 Introduction,0,[0]
"Our dataset, COMPLEXWEBQUESTIONS, can be downloaded from http://nlp.cs.tau.",1 Introduction,0,[0]
ac.il/compwebq and our codebase can be downloaded from https://github.com/ alontalmor/WebAsKB.,1 Introduction,0,[0]
"Our goal is to learn a model that given a question q and a black box QA model for answering simple questions, SIMPQA(·), produces a computation tree t (defined below) that decomposes the question and computes the answer.",2 Problem Formulation,0,[0]
"The model is trained from a set of N question-computation tree pairs {qi, ti}Ni=1 or question-answer pairs {qi, ai}Ni=1.
",2 Problem Formulation,0,[0]
"A computation tree is a tree where leaves are labeled with strings, and inner nodes are labeled with functions.",2 Problem Formulation,0,[0]
The arguments of a function are its children sub-trees.,2 Problem Formulation,0,[0]
"To compute an answer, or denotation, from a tree, we recursively apply the function at the root to its children.",2 Problem Formulation,0,[0]
"More formally, given a tree rooted at node t, labeled by the function f , that has children c1(t), . . .",2 Problem Formulation,0,[0]
", ck(t), the denotation JtK = f(Jc1(t)K, . . .",2 Problem Formulation,0,[0]
", Jck(t)K) is an arbitrary function applied to the denotations of the root’s children.",2 Problem Formulation,0,[0]
"Denotations are computed recursively and the denotation of a string at the leaf is the string itself, i.e.,",2 Problem Formulation,0,[0]
JlK = l.,2 Problem Formulation,0,[0]
"This is closely related to “semantic functions” in semantic parsing (Berant and Liang, 2015), except that we do not in-
teract with a KB, but rather compute directly over the breadth of the web through a search engine.
",2 Problem Formulation,0,[0]
Figure 2 provides an example computation tree for our running example.,2 Problem Formulation,0,[0]
"Notice that words at the leaves are not necessarily in the original question, e.g., “city” is paraphrased to “cities”.",2 Problem Formulation,0,[0]
"More broadly, our framework allows paraphrasing questions in any way that is helpful for the function SIMPQA(·).",2 Problem Formulation,0,[0]
"Paraphrasing for better interaction with a QA model has been recently suggested by Buck et al. (2017) and Nogueira and Cho (2016).
",2 Problem Formulation,0,[0]
"We defined the function SIMPQA(·) for answering simple questions, but in fact it comprises two components in this work.",2 Problem Formulation,0,[0]
"First, the question is submitted to a search engine that retrieves a list of web snippets.",2 Problem Formulation,0,[0]
"Next, a RC model extracts the answer from the snippets.",2 Problem Formulation,0,[0]
"While it is possible to train the RC model jointly with question decomposition, in this work we pre-train it separately, and later treat it as a black box.
",2 Problem Formulation,0,[0]
"The expressivity of our QA model is determined by the functions used, which we turn to next.",2 Problem Formulation,0,[0]
"Functions in our formal language take arguments and return values that can be strings (when decomposing or re-phrasing the question), sets of strings, or sets of numbers.",3 Formal Language,0,[0]
"Our set of functions includes:
1.",3 Formal Language,0,[0]
"SIMPQA(·): Model for answering simple questions, which takes a string argument and returns a set of strings or numbers as answer.",3 Formal Language,0,[0]
2,3 Formal Language,0,[0]
"COMP(·, ·): This function takes a string containing one unique variable VAR, and a set of answers.",3 Formal Language,0,[0]
"E.g., in Figure 2 the first argument is “birthplace of VAR”, and the second argument is “{KEN FOLLETT, ADAM ZAGAJEWSKI}”.",3 Formal Language,0,[0]
The function replaces the variable with each answer string representation and returns their union.,3 Formal Language,0,[0]
"Formally, COMP(q,A) = ∪a∈ASIMPQA(q/a), where q/a denotes the string produced when replacing VAR in q with a.",3 Formal Language,0,[0]
"This is similar to function composition in CCG (Steedman, 2000), or a join operation in λ-DCS (Liang, 2013), where the string is a function applied to previously-computed values.",3 Formal Language,0,[0]
"3. CONJ(·, ·): takes two sets and returns their intersection.",3 Formal Language,0,[0]
Other set operations can be defined analogously.,3 Formal Language,0,[0]
"As syntactic sugar, we allow CONJ(·) to take strings as input, which means that we run SIMPQA(·) to obtain a set
and then perform intersection.",3 Formal Language,0,[0]
The root node in Figure 2 illustrates an application of CONJ. 4.,3 Formal Language,0,[0]
"ADD(·, ·): takes two singleton sets of numbers and returns a set with their addition.",3 Formal Language,0,[0]
Similar functions can be defined analogously.,3 Formal Language,0,[0]
"While we support mathematical operations, they were not required in our dataset.
",3 Formal Language,0,[0]
Other logical operations In semantic parsing superlative and comparative questions like “What is the highest European mountain?”,3 Formal Language,0,[0]
or “What European mountains are higher than Mont Blanc?” are answered by joining the set of European mountains with their elevation.,3 Formal Language,0,[0]
"While we could add such functions to the formal language, answering such questions from the web is cumbersome: we would have to extract a list of entities and a numerical value for each.",3 Formal Language,0,[0]
"Instead, we handle such constructions using SIMPQA directly, assuming they are mentioned verbatim on some web document.
",3 Formal Language,0,[0]
"Similarly, negation questions (“What countries are not in the OECD?”) are difficult to handle when working against a search engine only, as this is an open world setup and we do not hold a closed set of countries over which we can perform set subtraction.
",3 Formal Language,0,[0]
"In future work, we plan to interface with tables (Pasupat and Liang, 2015) and KBs (Zhong et al., 2017).",3 Formal Language,0,[0]
"This will allow us to perform set operations over well-defined sets, and handle in a compositional manner superlatives and comparatives.",3 Formal Language,0,[0]
Evaluating our framework requires a dataset of broad and complex questions that examine the importance of question decomposition.,4 Dataset,0,[0]
"While many QA datasets have been developed recently (Yang et al., 2015; Rajpurkar et al., 2016; Hewlett et al., 2016; Nguyen et al., 2016; Onishi et al., 2016; Hill et al., 2015; Welbl et al., 2017), they lack a focus on the importance of question decomposition.
",4 Dataset,0,[0]
Most RC datasets contain simple questions that can be answered from a short input document.,4 Dataset,0,[0]
"Recently, TRIVIAQA (Joshi et al., 2017) presented a larger portion of complex questions, but still most do not require reasoning.",4 Dataset,0,[0]
"Moreover, the focus of TRIVIAQA is on answer extraction from documents that are given.",4 Dataset,0,[0]
"We, conversely, highlight question decomposition for finding the relevant documents.",4 Dataset,0,[0]
"Put differently, RC is complementary to question decomposition and can be used as part of the implementation of SIMPQA.",4 Dataset,0,[0]
"In Sec-
1.",4 Dataset,0,[0]
"Seed Question
2.",4 Dataset,0,[0]
"SPARQL
3.",4 Dataset,0,[0]
"Machine-generated
4.",4 Dataset,0,[0]
"Natural language
What movies have robert pattinson starred in?",4 Dataset,0,[0]
ns:rebert_pattinson ns:film.actor.film ?c . ?,4 Dataset,0,[0]
c ns:film.performance.film ?x . ?,4 Dataset,0,[0]
x ns:film.film.produced_by,4 Dataset,0,[0]
"ns:erwin_stoff What movies have robert pattinson starred in and that was produced by Erwin Stoff?
Which Robert Pattinson film was produced by Erwin Stoff?
",4 Dataset,0,[0]
"Figure 3: Overview of data collection process.
",4 Dataset,0,[0]
tion 6,4 Dataset,0,[0]
we demonstrate that question decomposition is useful for two different RC approaches.,4 Dataset,0,[0]
"To generate complex questions we use the dataset WEBQUESTIONSSP (Yih et al., 2016), which contains 4,737 questions paired with SPARQL queries for Freebase (Bollacker et al., 2008).",4.1 Dataset collection,0,[0]
Questions are broad but simple.,4.1 Dataset collection,0,[0]
"Thus, we sample question-query pairs, automatically create more complex SPARQL queries, generate automatically questions that are understandable to AMT workers, and then have them paraphrase those into natural language (similar to Wang et al. (2015)).",4.1 Dataset collection,0,[0]
"We compute answers by executing complex SPARQL queries against Freebase, and obtain broad and complex questions.",4.1 Dataset collection,0,[0]
"Figure 3 provides an example for this procedure, and we elaborate next.
",4.1 Dataset collection,0,[0]
"Generating SPARQL queries Given a SPARQL query r, we create four types of more complex queries: conjunctions, superlatives, comparatives, and compositions.",4.1 Dataset collection,0,[0]
Table 1 gives the exact rules for generation.,4.1 Dataset collection,0,[0]
"For conjunctions, superlatives, and comparatives, we identify queries in WEBQUESTIONSSP whose denotation is a set A, |A| ≥ 2, and generate a new query r′ whose denotation is a strict subset A′,A′ ⊂",4.1 Dataset collection,0,[0]
"A,A′ 6= φ.",4.1 Dataset collection,0,[0]
For conjunctions this is done by traversing the KB and looking for SPARQL triplets that can be added and will yield a valid set A′.,4.1 Dataset collection,0,[0]
"For comparatives and superlatives we find a numerical property common to all a ∈ A, and add a triplet and restrictor to r accordingly.",4.1 Dataset collection,0,[0]
"For compositions, we find an entity e in r, and replace e with a variable y and add to r a triplet such that the denotation of that triplet is {e}.
",4.1 Dataset collection,0,[0]
"Machine-generated (MG) questions To have AMT workers paraphrase SPARQL queries into natural language, we need to present them in an understandable form.",4.1 Dataset collection,0,[0]
"Therefore, we automatically generate a question they can paraphrase.",4.1 Dataset collection,0,[0]
"When we generate new SPARQL queries, new predi-
cates are added to the query (Table 1).",4.1 Dataset collection,0,[0]
"We manually annotated 687 templates mapping KB predicates to text for different compositionality types (with 462 unique KB predicates), and use those templates to modify the original WebQuestionsSP question according to the meaning of the generated SPARQL query.",4.1 Dataset collection,0,[0]
"E.g., the template for ?x ns:book.author.works written obj is “the author who wrote OBJ”.",4.1 Dataset collection,0,[0]
"For brevity, we provide the details in the supplementary material.
",4.1 Dataset collection,0,[0]
Question Rephrasing We used AMT workers to paraphrase MG questions into natural language (NL).,4.1 Dataset collection,0,[0]
Each question was paraphrased by one AMT worker and validated by 1-2 other workers.,4.1 Dataset collection,0,[0]
"To generate diversity, workers got a bonus if the edit distance of a paraphrase was high compared to the MG question.",4.1 Dataset collection,0,[0]
"A total of 200 workers were involved, and 34,689 examples were produced with an average cost of 0.11$ per question.",4.1 Dataset collection,0,[0]
"Table 1 gives an example for each compositionality type.
",4.1 Dataset collection,0,[0]
A drawback of our method for generating data is that because queries are generated automatically the question distribution is artificial from a semantic perspective.,4.1 Dataset collection,0,[0]
"Still, developing models that are capable of reasoning is an important direction for natural language understanding and COMPLEXWEBQUESTIONS provides an opportunity to develop and evaluate such models.
",4.1 Dataset collection,0,[0]
"To summarize, each of our examples contains a question, an answer, a SPARQL query (that our models ignore), and all web snippets harvested by our model when attempting to answer the question.",4.1 Dataset collection,0,[0]
This renders COMPLEXWEBQUESTIONS useful for both the RC and semantic parsing communities.,4.1 Dataset collection,0,[0]
"COMPLEXWEBQUESTIONS builds on the WEBQUESTIONS (Berant et al., 2013).",4.2 Dataset analysis,0,[0]
"Questions in WEBQUESTIONS are usually about properties of entities (“What is the capital of France?”), often with some filter for the semantic type of the answer (“Which director”, “What city”).",4.2 Dataset analysis,0,[0]
WEBQUESTIONS also contains questions that refer to events with multiple entities (“Who did Brad Pitt play in Troy?”).,4.2 Dataset analysis,0,[0]
"COMPLEXWEBQUESTIONS contains all these semantic phenomena, but we add four compositionality types by generating composition questions (45% of the times), conjunctions (45%), superlatives (5%) and comparatives (5%).
",4.2 Dataset analysis,0,[0]
"Paraphrasing To generate rich paraphrases, we gave a bonus to workers that substantially modified MG questions.",4.2 Dataset analysis,0,[0]
"To check whether this worked, we measured surface similarity between MG and NL questions, and examined the similarity.",4.2 Dataset analysis,0,[0]
"Using normalized edit-distance and the DICE coefficient, we found that NL questions are different from MG questions and that the similarity distribution has wide support (Figure 4).
",4.2 Dataset analysis,0,[0]
We created a heuristic for approximating the amount of word re-ordering performed by AMT workers.,4.2 Dataset analysis,0,[0]
"For every question, we constructed a matrix A, where Aij is the similarity between token i in the MG question and token j in the NL question.",4.2 Dataset analysis,0,[0]
"Similarity is 1 if lemmas match, or cosine similarity according to GloVe embeddings (Pennington et al., 2014), when above a threshold, and 0 otherwise.",4.2 Dataset analysis,0,[0]
The matrix A allows us to estimate whether parts of the MG question were re-ordered when paraphrased to NL (details in supplementary material).,4.2 Dataset analysis,0,[0]
"We find that in 44.7% of the conjunction questions and 13.2% of the composition questions, word re-ordering happened, illustrating that substantial changes to the MG question have been made.",4.2 Dataset analysis,0,[0]
"Figure 5 illustrates the matrix A for a pair of questions with re-ordering.
",4.2 Dataset analysis,0,[0]
Qualitative analysis We randomly sampled 100 examples from the development set and manually identified prevalent phenomena in the data.,4.2 Dataset analysis,0,[0]
We present these types in Table 2 along with their frequency.,4.2 Dataset analysis,0,[0]
In 18% of the examples a conjunct in the MG question becomes a modifier of a wh-word in the NL question (WH-MODIFIER).,4.2 Dataset analysis,0,[0]
"In 22% substantial word re-ordering of the MG questions occurred, and in 42% a minor word re-ordering occurred (“number of building floors is 50” paraphrased as “has 50 floors”).",4.2 Dataset analysis,0,[0]
"AMT workers used a synonym in 54% of the examples, they omitted words in 27% of the examples and they added new lexical material in 29%.
",4.2 Dataset analysis,0,[0]
"To obtain intuition for operations that will be useful in our model, we analyzed the 100 examples for the types of operations that should be applied to the NL question during question decomposition.",4.2 Dataset analysis,0,[0]
"We found that splitting the NL question is insufficient, and that in 53% of the cases a word in the NL question needs to be copied to multiple questions after decomposition (row 3 in Table 3).
",4.2 Dataset analysis,0,[0]
"Moreover, words that did not appear in the MG question need to be added in 39% of the cases, and words need to be deleted in 28% of the examples.",4.2 Dataset analysis,0,[0]
We would like to develop a model that translates questions into arbitrary computation trees with arbitrary text at the tree leaves.,5 Model and Learning,0,[0]
"However, this requires training from denotations using methods such as maximum marginal likelihood or reinforcement learning (Guu et al., 2017) that are difficult to optimize.",5 Model and Learning,0,[0]
"Moreover, such approaches involve issuing large amounts of queries to a search engine at training time, incurring high costs and slowing down training.
",5 Model and Learning,0,[0]
"Instead, we develop a simple approach in this paper.",5 Model and Learning,0,[0]
We consider a subset of all possible computation trees that allows us to automatically generate noisy full supervision.,5 Model and Learning,0,[0]
"In what follows, we describe the subset of computation trees considered and their representation, a method for automatically generating noisy supervision, and a pointer network model for decoding.
",5 Model and Learning,0,[0]
"Representation We represent computation trees as a sequence of tokens, and consider trees with at most one compositional operation.",5 Model and Learning,0,[0]
"We denote a sequence of question tokens qi:j = (qi, . . .",5 Model and Learning,0,[0]
", qj), and the decoded sequence by z.",5 Model and Learning,0,[0]
"We consider the following token sequences (see Table 3):
1.",5 Model and Learning,0,[0]
SimpQA:,5 Model and Learning,0,[0]
The function SIMPQA is applied to the question q without paraphrasing.,5 Model and Learning,0,[0]
In prefix notation this is the tree SIMPQA(q).,5 Model and Learning,0,[0]
2. Comp i j:,5 Model and Learning,0,[0]
This sequence of tokens corresponds to the following computation tree:,5 Model and Learning,0,[0]
"COMP(q1:i−1◦VAR◦qj+1:|q|, SIMPQA(qi:j)), where ◦ is the concatenation operator.",5 Model and Learning,0,[0]
"This is used for questions where a substring is answered by SIMPQA and the answers replace
a variable before computing a final answer.",5 Model and Learning,0,[0]
3,5 Model and Learning,0,[0]
Conj i j:,5 Model and Learning,0,[0]
"This sequence of tokens
corresponds to the computation tree CONJ(SIMPQA(q0:i−1), SIMPQA(qj ◦ qi:|q|)).",5 Model and Learning,0,[0]
"The idea is that conjunction can be answered by splitting the question in a single point, where one token is copied to the second part as well (“film” in Table 3).",5 Model and Learning,0,[0]
"If nothing needs to be copied, then j = −1.
",5 Model and Learning,0,[0]
"This representation supports one compositional operation, and a single copying operation is allowed without any re-phrasing.",5 Model and Learning,0,[0]
"In future work, we plan to develop a more general representation, which will require training from denotations.
",5 Model and Learning,0,[0]
"Supervision Training from denotations is difficult as it involves querying a search engine frequently, which is expensive.",5 Model and Learning,0,[0]
"Therefore, we take advantage of the the original SPARQL queries and MG questions to generate noisy programs for composition and conjunction questions.",5 Model and Learning,0,[0]
"Note that these noisy programs are only used as supervision to avoid the costly process of manual annotation, but the model itself does not assume SPARQL queries in any way.
",5 Model and Learning,0,[0]
"We generate noisy programs from SPARQL queries in the following manner: First, we automatically identify composition and conjunction questions.",5 Model and Learning,0,[0]
"Because we generated the MG question, we can exactly identify the split points (i, j in composition questions and i in conjunction questions) in the MG question.",5 Model and Learning,0,[0]
"Then, we use a rulebased algorithm that takes the alignment matrix A (Section 4), and approximates the split points in the NL question and the index j to copy in conjunction questions.",5 Model and Learning,0,[0]
"The red line in Figure 5 corresponds to the known split point in the MG question, and the blue one is the approximated split point in the NL question.",5 Model and Learning,0,[0]
"The details of this rule-
based algorithm are in the supplementary material.",5 Model and Learning,0,[0]
"Thus, we obtain noisy supervision for all composition and conjunction questions and can train a model that translates questions q to representations z = z1 z2 z3, where z1 ∈ {Comp,Conj} and z2, z3 are integer indices.
",5 Model and Learning,0,[0]
"Pointer network The representation z points to indices in the input, and thus pointer networks (Vinyals et al., 2015) are a sensible choice.",5 Model and Learning,0,[0]
"Because we also need to decode the tokens COMP and CONJ, we use “augmented pointer networks”, (Zhong et al., 2017):",5 Model and Learning,0,[0]
"For every question q, an augmented question q̂ is created by appending the tokens “COMP CONJ” to q. This allows us to decode the representation z with one pointer network that at each decoding step points to one token in the augmented question.",5 Model and Learning,0,[0]
"We encode q̂ with a onelayer GRU (Cho et al., 2014), and decode z with a one-layer GRU with attention as in Jia and Liang (2016).",5 Model and Learning,0,[0]
"The only difference is that we decode tokens from the augmented question q̂ rather than from a fixed vocabulary.
",5 Model and Learning,0,[0]
"We train the model with token-level crossentropy loss, minimizing ∑ j log pθ(zj |x, z1:j−1).",5 Model and Learning,0,[0]
"Parameters θ include the GRU encoder and decoder, and embeddings for unknown tokens (that are not in pre-trained GloVe embeddings (Pennington et al., 2014)).
",5 Model and Learning,0,[0]
"The trained model decodes COMP and CONJ representations, but sometimes using SIMPQA(q) without decomposition is better.",5 Model and Learning,0,[0]
"To handle such cases we do the following: We assume that we always have access to a score for every answer, provided by the final invocation of SIMPQA (in CONJ questions this score is the maximum of the scores given by SIMPQA for the two conjuncts), and use the following rule to decide if to use the decoded representation z or SIMPQA(q).",5 Model and Learning,0,[0]
"Given the scores for answers given by z and the scores given by SIMPQA(q), we return the single answer that has the highest score.",5 Model and Learning,0,[0]
The intuition is that the confidence provided by the scores of SIMPQA is correlated with answer correctness.,5 Model and Learning,0,[0]
"In future work we will train directly from denotations and will han-
dle all logical functions in a uniform manner.",5 Model and Learning,0,[0]
"In this section, we aim to examine whether question decomposition can empirically improve performance of QA models over complex questions.
",6 Experiments,0,[0]
"Experimental setup We used 80% of the examples in COMPLEXWEBQUESTIONS for training, 10% for development, and 10% for test, training the pointer network on 24,708 composition and conjunction examples.",6 Experiments,0,[0]
"The hidden state dimension of the pointer network is 512, and we used Adagrad (Duchi et al., 2010) combined with L2 regularization and a dropout rate of 0.25.",6 Experiments,0,[0]
"We initialize 50-dimensional word embeddings using GloVe and learn embeddings for missing words.
",6 Experiments,0,[0]
Simple QA model,6 Experiments,0,[0]
"As our SIMPQA function, we download the web-based QA model of Talmor et al. (2017).",6 Experiments,0,[0]
This model sends the question to Google’s search engine and extracts a distribution over answers from the top-100 web snippets using manually-engineered features.,6 Experiments,0,[0]
"We re-train the model on our data with one new feature: for every question q and candidate answer mention in a snippet, we run RASOR, a RC model by lee et al. (2016), and add the output logit score as a feature.",6 Experiments,0,[0]
"We found that combining the web-facing model of Talmor et al. (2017) and RASOR, resulted in improved performance.
",6 Experiments,0,[0]
"Evaluation For evaluation, we measure precision@1 (p@1), i.e., whether the highest scoring answer returned string-matches one of the correct answers (while answers are sets, 70% of the questions have a single answer, and the average size of the answer set is 2.3).
",6 Experiments,0,[0]
We evaluate the following models and oracles: 1. SIMPQA:,6 Experiments,0,[0]
"running SIMPQA on the entire
question, i.e., without decomposition.",6 Experiments,0,[0]
2,6 Experiments,0,[0]
SPLITQA:,6 Experiments,0,[0]
"Our main model that answers
complex questions by decomposition.",6 Experiments,0,[0]
3,6 Experiments,0,[0]
SPLITQAORACLE:,6 Experiments,0,[0]
"An oracle model that
chooses whether to perform question decom-
position or use SIMPQA in hindsight based on what performs better.",6 Experiments,0,[0]
4. RCQA:,6 Experiments,0,[0]
"This is identical to SIMPQA, except that we replace the RC model from Talmor et al. (2017) with the the RC model DOCQA (Clark and Gardner, 2017), whose performance is comparable to state-of-the-art on TRIVIAQA.",6 Experiments,0,[0]
5,6 Experiments,0,[0]
SPLITRCQA:,6 Experiments,0,[0]
"This is identical to SPLITQA, except that we replace the RC model from Talmor et al. (2017) with DOCQA.",6 Experiments,0,[0]
6. GOOGLEBOX:,6 Experiments,0,[0]
We sample 100 random development set questions and check whether Google returns a box that contains one of the correct answers.,6 Experiments,0,[0]
7,6 Experiments,0,[0]
HUMAN:,6 Experiments,0,[0]
"We sample 100 random development set questions and manually answer the questions with Google’s search engine, including all available information.",6 Experiments,0,[0]
"We limit the amount of time allowed for answering to 4 minutes.
",6 Experiments,0,[0]
Table 4 presents the results on the development and test sets.,6 Experiments,0,[0]
"SIMPQA, which does not decompose questions obtained 20.8 p@1, while by performing question decomposition we substantially improve performance to 27.5 p@1.",6 Experiments,0,[0]
"An upper bound with perfect knowledge on when to decompose is given by SPLITQAORACLE at 33.7 p@1.
",6 Experiments,0,[0]
"RCQA obtained lower performance SIMPQA, as it was trained on data from a different distribution.",6 Experiments,0,[0]
"More importantly SPLITRCQA outperforms RCQA by 3.4 points, illustrating that this RC model also benefits from question decomposition, despite the fact that it was not created with question decomposition in mind.",6 Experiments,0,[0]
This shows the importance of question decomposition for retrieving documents from which an RC model can extract answers.,6 Experiments,0,[0]
"GOOGLEBOX finds a correct answer in 2.5% of the cases, showing that complex questions are challenging for search engines.
",6 Experiments,0,[0]
"To conclude, we demonstrated that question de-
composition substantially improves performance on answering complex questions using two independent RC models.
",6 Experiments,0,[0]
Analysis We estimate human performance (HUMAN) at 63.0 p@1.,6 Experiments,0,[0]
We find that answering complex questions takes roughly 1.3 minutes on average.,6 Experiments,0,[0]
"For questions we were unable to answer, we found that in 27% the answer was correct but exact string match with the gold answers failed; in 23.1% the time required to compute the answer was beyond our capabilities; for 15.4% we could not find an answer on the web; 11.5% were of ambiguous nature; 11.5% involved paraphrasing errors of AMT workers; and an additional 11.5% did not contain a correct gold answer.
",6 Experiments,0,[0]
SPLITQA decides if to decompose questions or not based on the confidence of SIMPQA.,6 Experiments,0,[0]
"In 61% of the questions the model chooses to decompose the question, and in the rest it sends the question as-is to the search engine.",6 Experiments,0,[0]
"If one of the strategies (decomposition vs. no decomposition) works, our model chooses that right one in 86% of the cases.",6 Experiments,0,[0]
"Moreover, in 71% of these answerable questions, only one strategy yields a correct answer.
",6 Experiments,0,[0]
We evaluate the ability of the pointer network to mimic our labeling heuristic on the development set.,6 Experiments,0,[0]
"We find that the model outputs the exact correct output sequence 60.9% of the time, and allowing errors of one word to the left and right (this often does not change the final output) accuracy is at 77.1%.",6 Experiments,0,[0]
Token-level accuracy is 83.0% and allowing one-word errors 89.7%.,6 Experiments,0,[0]
This shows that SPLITQA learned to identify decomposition points in the questions.,6 Experiments,0,[0]
"We also observed that often SPLITQA produced decomposition points that are better than the heuristic, e.g., for “What is the place of birth for the lyricist of Roman Holiday”, SPLITQA produced “the lyricist of Roman Holiday”, but the heuristic produced “the place of birth for the lyricist of Roman Holiday”.",6 Experiments,0,[0]
"Additional examples of SPLITQA question decompositions are provided in Table 5.
",6 Experiments,0,[0]
"ComplexQuestions To further examine the ability of web-based QA models, we run an experiment against COMPLEXQUESTIONS (Bao et al., 2016), a small dataset of question-answer pairs designed for semantic parsing against Freebase.
",6 Experiments,0,[0]
"We ran SIMPQA on this dataset (Table 6) and obtained 38.6 F1 (the official metric), slightly lower than COMPQ, the best system, which op-
erates directly against Freebase.",6 Experiments,0,[0]
2,6 Experiments,0,[0]
"By analyzing the training data, we found that we can decompose COMP questions with a rule that splits the question when the words “when” or “during” appear, e.g., “Who was vice president when JFK was president?”.3 We decomposed questions with this rule and obtained 39.7 F1 (SPLITQARULE).",6 Experiments,0,[0]
"Analyzing the development set errors, we found that occasionally SPLITQARULE returns a correct answer that fails to string-match with the gold answer.",6 Experiments,0,[0]
"By manually fixing these cases, our development set F1 reaches 46.9 (SPLITQARULE++).",6 Experiments,0,[0]
"Note that COMPQ does not suffer from any string matching issue, as it operates directly against the Freebase KB and thus is guaranteed to output the answer in the correct form.",6 Experiments,0,[0]
"This short experiment shows that a web-based QA model can rival a semantic parser that works against a KB, and that simple question decomposition is beneficial and leads to results comparable to state-of-the-art.",6 Experiments,0,[0]
"This work is related to a body of work in semantic parsing and RC, in particular to datasets that focus on complex questions such as TRIVIAQA (Joshi et al., 2017), WIKIHOP (Welbl et al., 2017) and RACE (Lai et al., 2017).",7 Related work,0,[0]
"Our distinction is in proposing a framework for complex QA that focuses on question decomposition.
",7 Related work,0,[0]
"Our work is related to Chen et al. (2017) and Watanabe et al. (2017), who combined retrieval and answer extraction on a large set of documents.",7 Related work,0,[0]
"We work against the entire web, and propose ques-
2By adding the output logit from RASOR, we improved test F1 from 32.6, as reported by Talmor et al. (2017), to 38.6.
",7 Related work,0,[0]
"3The data is too small to train our decomposition model.
tion decomposition for finding information.",7 Related work,0,[0]
This work is also closely related to Dunn et al. (2017) and Buck et al. (2017): we start with questions directly and do not assume documents are given.,7 Related work,0,[0]
"Buck et al. (2017) also learn to phrase questions given a black box QA model, but while they focus on paraphrasing, we address decomposition.
",7 Related work,0,[0]
"Another important related research direction is Iyyer et al. (2016), who answered complex questions by decomposing them.",7 Related work,0,[0]
"However, they used crowdsourcing to obtain direct supervision for the gold decomposition, while we do not assume such supervision.",7 Related work,0,[0]
"Moreover, they work against web tables, while we interact with a search engine against the entire web.",7 Related work,0,[0]
In this paper we propose a new framework for answering complex questions that is based on question decomposition and interaction with the web.,8 Conclusion,0,[0]
We develop a model under this framework and demonstrate it improves complex QA performance on two datasets and using two RC models.,8 Conclusion,0,[0]
"We also release a new dataset, COMPLEXWEBQUESTIONS, including questions, SPARQL programs, answers, and web snippets harvested by our model.",8 Conclusion,0,[0]
"We believe this dataset will serve the QA and semantic parsing communities, drive research on compositionality, and push the community to work on holistic solutions for QA.
",8 Conclusion,0,[0]
"In future work, we plan to train our model directly from weak supervision, i.e., denotations, and to extract information not only from the web, but also from structured information sources such as web tables and KBs.",8 Conclusion,0,[0]
"We thank Jonatahn Herzig, Ni Lao, and the anonymous reviewers for their constructive feedback.",Acknowledgements,0,[0]
"This work was supported by the Samsung runway project and the Israel Science Foundation, grant 942/16.",Acknowledgements,0,[0]
Answering complex questions is a timeconsuming activity for humans that requires reasoning and integration of information.,abstractText,0,[0]
"Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge.",abstractText,0,[0]
"Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base.",abstractText,0,[0]
"In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model.",abstractText,0,[0]
"We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers.",abstractText,0,[0]
"To illustrate the viability of our approach, we create a new dataset of complex questions, COMPLEXWEBQUESTIONS, and present a model that decomposes questions and interacts with the web to compute an answer.",abstractText,0,[0]
We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.,abstractText,0,[0]
The Web as a Knowledge-base for Answering Complex Questions,title,0,[0]
"A permutation is a 1-to-1 mapping from a finite set into itself, and allows to represent mathematically a complete ordering or n items.",1. Introduction,0,[0]
"We consider the problem of machine learning when data are permutations, which has many applications such as analyzing preferences or votes (Diaconis, 1988; Marden, 1996), tracking objects (Huang et al., 2009), or learning robustly from high-dimensional biological data (Geman et al., 2004; Lin et al., 2009; Jiao & Vert, 2018).
",1. Introduction,0,[0]
"A promising direction to learn over permutations is to first embed them to a vector space, i.e., to first represent each permutation π by a vector Φ(π) ∈ Rd, and then to learn a parametric linear or nonlinear model over Φ(π) using standard machine learning approaches.",1. Introduction,0,[0]
"For example, Kondor & Barbosa (2010) proposed an embedding to Rd with d = n!,
1University of Oxford, Oxford, UK 2MINES ParisTech & Institut Curie & Ecole Normale Supérieure, PSL Research University, Paris, France.",1. Introduction,0,[0]
"Correspondence to: Jean-Philippe Vert <jeanphilippe.vert@mines-paristech.fr>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
",1. Introduction,0,[0]
"where n is the number of items; however, in spite of computational tricks, this leads to algorithms with O(nn) complexity which become impractical as soon as we consider more than a few items.",1. Introduction,0,[0]
"Recently, Jiao & Vert (2018) showed that the well-known Kendall τ correlation defines implicitly an embedding in d = O(n2) dimensions, in which linear models can be learned with O(n ln(n))",1. Introduction,0,[0]
"complexity thanks to the so-called kernel trick (Schölkopf & Smola, 2002; Shawe-Taylor & Cristianini, 2004); they showed promising results on gene expression classification where n is a few thousands.",1. Introduction,0,[0]
"In this paper, we propose to extend this work by tackling several limitations of the Kendall kernel.
",1. Introduction,0,[0]
"First, the Kendall kernel compares two permutations by computing the number of pairs of items which are in the same order in both permutations.",1. Introduction,0,[0]
"When n is large, it may be more interesting to weight differently the contributions of different pairs, e.g., to focus more on the top-ranked items.",1. Introduction,0,[0]
"Many weighted versions of Kendall’s τ correlation have been proposed in the literature, as reviewed below in Section 3; however, to our knowledge, none of them is associated to a valid embedding of permutations in a vector space, like the original Kendall correlation.",1. Introduction,0,[0]
"Here we propose such an extension, and show that it inherits the computational benefits from the Kendall kernel while allowing to weight differently the importance of items based on their rank.
",1. Introduction,0,[0]
"Second, we discuss how to choose the position-dependent weights that define this embedding.",1. Introduction,0,[0]
"We propose to see them as parameters of the model, and to optimize them jointly with other parameters during training.",1. Introduction,0,[0]
"This is similar in spirit to the way different kernels are weighted and combined in multiple kernel learning (Lanckriet et al., 2004; Bach et al., 2004), or more generally to the notion of representation learning which became central in recent years in conjunction with neural networks or linear models (Mikolov et al., 2013; Le Morvan & Vert, 2017).",1. Introduction,0,[0]
"We show in particular that efficient alternative optimization schemes are possible in the resulting optimization problem.
",1. Introduction,0,[0]
"Third, observing that the features defining the embedding associated to the Kendall kernel are based on pairwise comparisons only, we consider extensions to higher-order comparisons, e.g., the relative ranking among triplets or quadruplets of items.",1. Introduction,0,[0]
"Such features are naturally captured by higher-
order representations of the symmetric group (Diaconis, 1988; Kondor & Barbosa, 2010), and we show that both the definition of the embedding and the algorithms to learn the weights can be effortlessly extended to such higher-order scenarios.
",1. Introduction,0,[0]
We finally present preliminary experimental results highlighting the potential of these new embeddings.,1. Introduction,0,[0]
Code to reproduce all the experiments in the present paper is available at https://github.com/YunlongJiao/ weightedkendall.,1. Introduction,0,[0]
Let us first fix some notations.,2. The Kendall kernel,0,[0]
"Given an integer n ∈ N, a permutation is a 1-to-1 mapping σ",2. The Kendall kernel,0,[0]
:,2. The Kendall kernel,0,[0]
"[1, n]",2. The Kendall kernel,0,[0]
"→ [1, n] such that σ(i) 6= σ(j) for i 6= j. We denote by Sn the set of all such permutations.",2. The Kendall kernel,0,[0]
A permutation σ ∈,2. The Kendall kernel,0,[0]
"Sn can for example represent the preference ordering of a user over n items, in which case σ(i) is the rank of item i among all items (e.g., the preferred item is the item i such that σ(i) = 1, namely, σ−1(1)).",2. The Kendall kernel,0,[0]
"Endowed with the composition operation (σ1σ2)(i) = σ1(σ2(i)), Sn is a group called the symmetric group, of cardinality n!.",2. The Kendall kernel,0,[0]
"The identity permutation will be denoted e.
A positive definite (p.d.) kernel on Sn is a symmetric function",2. The Kendall kernel,0,[0]
K :,2. The Kendall kernel,0,[0]
"Sn × Sn → R that satisfies, for any l ∈ N, α ∈",2. The Kendall kernel,0,[0]
"Rl and (σ1, . . .",2. The Kendall kernel,0,[0]
", σl) in Sln,
l∑ i=1",2. The Kendall kernel,0,[0]
l∑,2. The Kendall kernel,0,[0]
"j=1 αiαjK(σi, σj) ≥ 0 .
",2. The Kendall kernel,0,[0]
"Equivalently, a functionK : Sn×Sn → R is a p.d. kernel if and only if there exists a mapping Φ : Sn → Rd (for some d ∈ N) such that K(σ, σ′) = Φ(σ)>Φ(σ′).",2. The Kendall kernel,0,[0]
"For example, taking d = n(n",2. The Kendall kernel,0,[0]
− 1) and Φτ (σ) = ( 1σ(i)<σ(j) ),2. The Kendall kernel,0,[0]
"1≤i 6=j≤n, we see that the number of concordant pairs between two permutations is a p.d. kernel, i.e., ∀σ, σ′ ∈ Sn,
Kτ (σ, σ ′) =",2. The Kendall kernel,0,[0]
Φτ (σ) >,2. The Kendall kernel,0,[0]
"Φτ (σ ′) = ∑
1≤i6=j≤n
1σ(i)<σ(j)1σ′(i)<σ′(j) .",2. The Kendall kernel,0,[0]
"(1)
Up to constant shift and scaling (by taking 2Kτ/ ( n 2 )",2. The Kendall kernel,0,[0]
"− 1), this is equivalent to the Kendall kernel of Jiao & Vert (2018).",2. The Kendall kernel,0,[0]
"To lighten notations, we will simply call Kτ the Kendall kernel in the rest of this paper.
",2. The Kendall kernel,0,[0]
"Besides being p.d., the Kendall kernel has another interesting property: it is right-invariant, in the sense that for any σ, σ′ and π ∈ Sn, it holds that
Kτ (σ, σ ′)",2. The Kendall kernel,0,[0]
"= Kτ (σπ, σ ′π) .
",2. The Kendall kernel,0,[0]
"Right-invariance implies that the kernel does not change if we relabel the items to be ranked, which is a natural
requirement in most ranking problems (e.g., when items are only presented in alphabetical order for no other particular reason).",2. The Kendall kernel,0,[0]
"Note that any right-invariant kernel can be rewritten as, ∀σ,",2. The Kendall kernel,0,[0]
"σ′ ∈ Sn,
K(σ, σ′) = K(e, σ′σ−1) =: κ(σ′σ−1) , (2)
where κ : Sn → R. Hence a right-invariant kernel is a semigroup kernel (Berg et al., 1984), and we say that a function κ : Sn → R is p.d. when the kernel K defined by (2) is p.d.",2. The Kendall kernel,0,[0]
"In particular, note that K is symmetric if and only if κ satisfies κ(σ) = κ(σ−1) for any σ ∈ Sn.",2. The Kendall kernel,0,[0]
"For example, the p.d. function associated to the Kendall kernel (1) is:
κτ (σ) = ∑
1≤i 6=j≤n
1i<j1σ(i)<σ(j) = ∑
1≤i<j≤n
1σ(i)<σ(j) .
",2. The Kendall kernel,0,[0]
"Denoting I(2) = { (i, j) ∈",2. The Kendall kernel,0,[0]
"[1, n]2 : 1 ≤",2. The Kendall kernel,0,[0]
"i < j ≤ n }
, this can be rewritten as
κτ (σ) = ∑
(i,j)∈I(2) 1(σ(i),σ(j))∈I(2) .",2. The Kendall kernel,0,[0]
"(3)
This notation will be convenient in Section 7 when we generalize Kendall to high-order kernels for permutations.",2. The Kendall kernel,0,[0]
The Kendall kernel (1) compares two permutations by assessing how many pairs of items are ranked in the same order.,3. Related work,0,[0]
"In many cases, however, one may want to weight differently the contributions of different pairs, based for example on their rankings in both permutations.",3. Related work,0,[0]
"Typically in applications involving preference ranking and information retrieval, one may expect that the relative ordering of items ranked at the bottom of the list is usually less relevant than that at the top.",3. Related work,0,[0]
"For example, when participants are asked to express their preference over a list of predefined items, it is usually impractical for participants to accurately express their preference towards less preferable items.",3. Related work,0,[0]
"As another example from information retrieval, when we wish to compare two ranked lists of documents outputted by two search engines, differences towards the top usually matter more as those query results will be eventually presented to the user.
",3. Related work,0,[0]
Many authors have proposed weighted versions of the Kendall’s τ correlation coefficient.,3. Related work,0,[0]
"A classical one, for example, is the weighted Kendall τ",3. Related work,0,[0]
"statistics studied by Shieh (1998):
τw(σ, σ ′) = ∑ 1≤i6=j≤n w(σ(i), σ(j))1σ(i)<σ(j)1σ′(i)<σ′(j) , (4) for some weight function",3. Related work,0,[0]
w :,3. Related work,0,[0]
"[1, n]2 → R. Typical examples of the weight function include
• w(i, j) = 1/(j − 1) if j ≥ 2 and 0 otherwise, which gives the average precision correlation coefficient (Yilmaz et al., 2008).
",3. Related work,0,[0]
"• Weights of the form w(i, j) = wiwj for some vector w ∈",3. Related work,0,[0]
"Rn (Kumar & Vassilvitskii, 2010).
",3. Related work,0,[0]
"While (4) looks like a weighted version of (1), it is not symmetric in σ and σ′ (except if w is constant) since the weight of a pair of items only depends on their rankings in σ, and in particular τw is not a p.d. kernel.",3. Related work,0,[0]
"To enforce symmetry, Vigna (2015) proposes a weighted correlation of the form
τw(σ, σ ′) = ∑ 1≤i6=j≤n (w(σ(i), σ(j))",3. Related work,0,[0]
"+ w(σ′(i), σ′(j)))
× 1σ(i)<σ(j)1σ′(i)<σ′(j) , (5)
which is symmetric and right-invariant, i.e., is invariant by relabeling of the items.",3. Related work,0,[0]
"However, (5) is not a valid p.d. inner product.",3. Related work,0,[0]
"Kumar & Vassilvitskii (2010) proposes a weighted correlation of the form:
τw,p(σ, σ ′) = ∑ 1≤i 6=j≤n w(σ(i), σ(j))
",3. Related work,0,[0]
× pσ(i),3. Related work,0,[0]
"− pσ′(i) σ(i)− σ′(i) pσ(j) − pσ′(j) σ(j)− σ′(j) 1σ(i)<σ(j)1σ′(i)<σ′(j) ,
(6)
where w :",3. Related work,0,[0]
"[1, n]2 → R and p ∈ Rn.",3. Related work,0,[0]
"In particular, the additional set of weights p in (6) is motivated by cumulatively weighting the cost of swaps between adjacent positions needed to transform σ into σ′. Like (4), τw,p is not symmetric hence not p.d. Farnoud & Milenkovic (2014) notices that Kendall’s τ induces a Euclidean metric which is the shortest path distance over a Cayley’s graph (i.e., the smallest number of swaps between adjacent positions to transform a permutation into another), and proposes to set weights on the edges (i.e., on swaps between adjacent positions) in order to define the shortest path over the weighted graph as a new metric; although a valid metric, it does not induce a valid p.d. kernel in general.",3. Related work,0,[0]
"In summary, to our knowledge, no existing weighted variant of the Kendall kernel is p.d. and right-invariant.",3. Related work,0,[0]
"The following result provides a generic way to construct a weighted Kendall kernel that is p.d. and right-invariant.
Theorem 1.",4. The weighted Kendall kernel,0,[0]
"Let W : N2×N2 → R be a p.d. kernel on N2.
",4. The weighted Kendall kernel,0,[0]
Then the function KW :,4. The weighted Kendall kernel,0,[0]
"Sn × Sn → R defined by
KW (σ, σ ′) =",4. The weighted Kendall kernel,0,[0]
"∑ 1≤i 6=j≤n W ((σ(i), σ(j)), (σ′(i), σ′(j)))
× 1σ(i)<σ(j)1σ′(i)<σ′(j) (7)
is a right-invariant p.d. kernel on Sn.
",4. The weighted Kendall kernel,0,[0]
"Theorem 1 is easy to prove by writing explicitly W as an inner product, and deducing from (7) that KW can also be written as an inner product.",4. The weighted Kendall kernel,0,[0]
"Note that KW is always rightinvariant whether or not W is p.d. Note also that there may exist non p.d. weight W leading to p.d. kernels KW ; we leave it as an open question to characterize the necessary conditions on the weight function W such that KW is p.d.
Let us now consider particular cases of the weighted Kendall kernel of the following form: Corollary 1.",4. The weighted Kendall kernel,0,[0]
"Let the weights in Theorem 1 take the form W ((a, b), (c, d))",4. The weighted Kendall kernel,0,[0]
"= UabUcd for some matrix U ∈ Rn×n, then
KU (σ, σ ′) = ∑ 1≤i6=j≤n Uσ(i),σ(j)Uσ′(i),σ′(j)
× 1σ(i)<σ(j)1σ′(i)<σ′(j) (8)
is a right-invariant p.d. kernel on Sn.",4. The weighted Kendall kernel,0,[0]
Remark.,4. The weighted Kendall kernel,0,[0]
It is interesting to make explicit how Corollary 1 is derived from Theorem 1.,4. The weighted Kendall kernel,0,[0]
Let us fix the number of items to rank n ∈ N.,4. The weighted Kendall kernel,0,[0]
"The conditions in Theorem 1 on W being a kernel over N2 can now be relaxed to being a kernel over [1, n]2.",4. The weighted Kendall kernel,0,[0]
"With a slight abuse of notation, any p.d. kernel W over finite",4. The weighted Kendall kernel,0,[0]
"[1, n]2 is uniquely determined by its full Gram matrixW of size n2×n2, which can always be decomposed into W = UU> for some matrix U of size n2 × n2 due to matrix W being s.p.d.",4. The weighted Kendall kernel,0,[0]
"In other words, this factorizationbased definition W = UU> is necessary and sufficient under the conditions set in Theorem 1.",4. The weighted Kendall kernel,0,[0]
"It is now easy to see that, if we further assume that Gram matrix W has rank 1 such that U is a vector of size n2, i.e., a matrix of size n×n, Theorem 1 reduces to Corollary 1.
",4. The weighted Kendall kernel,0,[0]
"Notably in (8), Uab should not be interpreted as a weight for items a and b, but rather as a weight for (those items sent by a permutation to) positions a and b. More precisely, if two items are ranked in the same relative order in both permutations, say positions a < b in σ and c < d in σ′, then this pair contributes UabUcd to the kernel value.",4. The weighted Kendall kernel,0,[0]
"Note that if Uij is constant for any (i, j), the weighted Kendall kernel (8) reduces to the standard Kendall kernel (1).
",4. The weighted Kendall kernel,0,[0]
"While U ∈ Rn×n encodes the weights of pairs of positions, it is usually intuitive to start from individual positions.",4. The weighted Kendall kernel,0,[0]
Suppose now we are given a vector u ∈,4. The weighted Kendall kernel,0,[0]
"Rn that encodes our belief of relevance of each position in [1, n], some particularly interesting choices for U ∈ Rn×n are
•",4. The weighted Kendall kernel,0,[0]
Top-k weight:,4. The weighted Kendall kernel,0,[0]
For some predetermined k ∈,4. The weighted Kendall kernel,0,[0]
"[2, n], define Uij = 1 if i ≤ k",4. The weighted Kendall kernel,0,[0]
and j ≤ k and 0,4. The weighted Kendall kernel,0,[0]
"otherwise.
",4. The weighted Kendall kernel,0,[0]
"• Additive weight: Given some u ∈ Rn, define Uij = ui + uj .
",4. The weighted Kendall kernel,0,[0]
"• Multiplicative weight: Given some u ∈ Rn, define Uij = uiuj .
",4. The weighted Kendall kernel,0,[0]
Each of these choices can be relevant in certain applications.,4. The weighted Kendall kernel,0,[0]
"For example, the top-k weight compares two permutations using exclusively the top k ranked items, and checking if they appear at the top of both permutations and in the same relative order.",4. The weighted Kendall kernel,0,[0]
"As another example, if one wishes to attenuate the importance of items when their rank increases, the additive or multiplicative weights can be a natural choice when combined with, for example, hyperbolic reduction factor ui = 1/(i+ 1) of the average precision correlation (Yilmaz et al., 2008), or logarithmic reduction factor ui = 1/ log2(i+1) of the discounted cumulative gain widely used in information retrieval (Manning et al., 2008).",4. The weighted Kendall kernel,0,[0]
"In particular, the top-k weight can be seen as a multiplicative weight with a hard cutoff factor ui = 1 if i ≤ k",4. The weighted Kendall kernel,0,[0]
and 0 otherwise.,4. The weighted Kendall kernel,0,[0]
"Figure 1 illustrates the aforementioned three different types of u.
Note that the hyperparameter k in the top-k Kendall kernel needs to be determined in practice, which can be achieved by cross-validation for instance.",4. The weighted Kendall kernel,0,[0]
"A straightforward way to bypass this and construct a kernel agnostic to the particular choice of k is to take the average of all top-k Kendall kernels with k ranging over [1, n].",4. The weighted Kendall kernel,0,[0]
"Specifically, the top-k Kendall kernel is written
K@kU (σ, σ ′) = ∑ 1≤i 6=j≤n 1σ(i)≤k1σ(j)≤k1σ′(i)≤k1σ′(j)≤k
× 1σ(i)<σ(j)1σ′(i)<σ′(j) .",4. The weighted Kendall kernel,0,[0]
"(9)
The average Kendall kernel is then derived as
KavgW (σ, σ ′) :=
1
n n∑ k=1 K@kU (σ, σ ′)
= ∑
1≤i 6=j≤n
1 n min {σ(i), σ′(i)}1σ(i)<σ(j)1σ′(i)<σ′(j) .
(10)
",4. The weighted Kendall kernel,0,[0]
"The positive definiteness of the average Kendall kernel is evident due to the fact that it is a sum of p.d. kernels, or we can verify that the average Kendall kernel is a weighted Kendall kernel of form (7) whose weight function is indeed the min kernel (Shawe-Taylor & Cristianini, 2004) thus satisfying the condition of Theorem 1.",4. The weighted Kendall kernel,0,[0]
"However, we see that the average Kendall kernel is no longer of the special form (8) considered in Corollary 1.",4. The weighted Kendall kernel,0,[0]
"Despite being a sum of O(n2) terms, the Kendall kernel (1) can be computed efficiently in O(n ln(n)).",5. Fast computation,0,[0]
"For example, Knight (1966) proposed such an algorithm by adapting a merge sort algorithm in order to count the inversion number of any permutation.",5. Fast computation,0,[0]
"While the general weighted Kendall kernel (7) does not enjoy comparable computational efficiency in general, it does for certain choices discussed in Section 4.
",5. Fast computation,0,[0]
Theorem 2.,5. Fast computation,0,[0]
"The weighted Kendall kernel (8) can be computed in O(n ln(n)) for the top-k, additive or multiplicative weights.",5. Fast computation,0,[0]
"Besides, the average Kendall kernel (10) can also also be computed in O(n ln(n)).
",5. Fast computation,0,[0]
"The proof is constructive and deferred to the supplements, where we also detail the pseudo-code for a fast computational algorithm.1 Notably, Vigna (2015) proposed a different fast algorithm based on the form of a weighted correlation (5), which applies to our additive and multiplicative cases as well.",5. Fast computation,0,[0]
"Theorem 2 shows that, just like the standard Kendall kernel, any of the weighted Kendall kernels concerned by the theorem benefits from the kernel trick and can be used efficiently by a kernel machine such as a SVM or kernel k-means.",5. Fast computation,0,[0]
"So far, we have been focusing on studying the properties of the weighted Kendall kernel for which weights are given and fixed.",6. Learning the weights,0,[0]
"In practice, it is usually not clear how to choose the weights so that the resulting kernel best suits the learning task at hand.",6. Learning the weights,0,[0]
"We thus propose a systematic approach to learn the weights in the context of supervised learning
1C++/R implementation available in the package kernrank at https://github.com/YunlongJiao/kernrank.
with discriminative models.",6. Learning the weights,0,[0]
"More specifically, instead of first choosing a priori the weights in the weighted Kendall kernel and then learning a function by a kernel machine, the weights can be learned jointly with the function estimated by the kernel machine.
",6. Learning the weights,0,[0]
We start by making explicit the feature embedding for permutations underlying the weighted Kendall kernel (8).,6. Learning the weights,0,[0]
"For any permutation σ ∈ Sn, let Πσ ∈ {0, 1}n×n be the permutation matrix of σ ∈ Sn defined by
(Πσ)ij = 1i=σ(j) .
",6. Learning the weights,0,[0]
"In fact, Π : Sn → Rn×n is the first-order permutation representation of Sn satisfying Π>σ =",6. Learning the weights,0,[0]
Πσ−1 and ΠσΠσ′,6. Learning the weights,0,[0]
"= Πσσ′ (Diaconis, 1988).",6. Learning the weights,0,[0]
Lemma 1.,6. Learning the weights,0,[0]
"For any matrix of weights U ∈ Rn×n, let ΦU : Sn → Rn×n be defined by
∀σ ∈ Sn , ΦU (σ) = Π>σ UΠσ or elementwise ( ΦU (σ) )",6. Learning the weights,0,[0]
"ij = Uσ(i),σ(j) , (11)
and let kernel GU over Sn defined by
∀σ, σ′ ∈ Sn , GU (σ, σ′)",6. Learning the weights,0,[0]
"= 〈 ΦU (σ),ΦU (σ′) 〉 F
= n∑ i,j=1 Uσ(i),σ(j)Uσ′(i),σ′(j) .",6. Learning the weights,0,[0]
"(12)
where 〈·, ·〉F denotes the Frobenius inner product for matrices.",6. Learning the weights,0,[0]
"In particular, if U is upper (or lower) triangular with Uij = 0 for all i ≥ j (or i ≤ j), then GU = KU .
",6. Learning the weights,0,[0]
"In the remainder of the paper, we will focus on GU (12), which we call the weighted kernel for permutations.",6. Learning the weights,0,[0]
Lemma 1 shows that using the weighted kernel GU in a kernel machine amounts to learning in the feature space induced by the weighted embedding ΦU .,6. Learning the weights,0,[0]
"In the context of supervised learning with discriminative models, this reduces to fitting a linear model to data through embedding ΦU .
",6. Learning the weights,0,[0]
Theorem 3.,6. Learning the weights,0,[0]
"A general linear function on the weighted embedding ΦU with coefficients B ∈ Rn×n can be written equivalently as
hU,B(σ) := 〈 B,ΦU (σ) 〉 F
= 〈 U,ΦB(σ−1) 〉 F
= 〈 vec(U)⊗ (vec(B))",6. Learning the weights,0,[0]
"> ,Πσ ⊗Πσ 〉
F ,
(13)
where vec(·) denotes the vectorization of a matrix, ⊗ denotes the Kronecker product for matrices or the outer product for vectors, wherever appropriate.
",6. Learning the weights,0,[0]
Proof.,6. Learning the weights,0,[0]
"The first equality is the definition of hU,B .",6. Learning the weights,0,[0]
"Plugging
in (11) from Lemma 1, we have the second equality hU,B(σ) = 〈 B,ΦU (σ) 〉 F
= tr ( B> ( Π>σ UΠσ ))",6. Learning the weights,0,[0]
= tr (( ΠσBΠ > σ )>,6. Learning the weights,0,[0]
"U )
= tr (( Π>σ−1BΠσ−1 )>",6. Learning the weights,0,[0]
"U )
= 〈 U,ΦB(σ−1) 〉 F .
",6. Learning the weights,0,[0]
The last equality follows from the relationship that vec(PXQ) =,6. Learning the weights,0,[0]
"( Q> ⊗ P ) vec(X) holds for any matrices P,Q,X .",6. Learning the weights,0,[0]
"Note that the first two inner products are over n × n matrices, while the last one is over n2 × n2 matrices.
",6. Learning the weights,0,[0]
Remark.,6. Learning the weights,0,[0]
Theorem 3 has several interesting implications.,6. Learning the weights,0,[0]
The first and second equalities show that the weights U and linear coefficients B are conceptually interchangeable.,6. Learning the weights,0,[0]
"We can arbitrarily swap the roles of the coefficients B of the linear model and the weights U of the embedding, as long as we also change the representation of data from σ to σ−1.",6. Learning the weights,0,[0]
The last equality shows that a linear function with coefficients B on an n × n dimensional embedding ΦU (σ) is equivalent to a linear function with coefficients vec(U)⊗(vec(B)),6. Learning the weights,0,[0]
>,6. Learning the weights,0,[0]
"on an n2×n2 dimensional embedding Πσ ⊗Πσ .
",6. Learning the weights,0,[0]
"Theorem 3 suggests that, instead of first fixing some weights U for the weighted kernel GU and then learning a function B using this embedding, U itself can be learned jointly with B. Consequently, ΦU with the learned weights U is thus a data-driven discriminative feature embedding.",6. Learning the weights,0,[0]
"Typically, fitting a kernel machine such as SVM is formulated as solving some optimization problem over B (in case of fixed GU ).",6. Learning the weights,0,[0]
"Now, we propose to optimize over both U and B by joint optimization, which amounts to a non-convex optimization problem over vec(U)⊗ (vec(B))>",6. Learning the weights,0,[0]
"according to Theorem 3.
",6. Learning the weights,0,[0]
"A commonly used approach to finding a stationary point of such optimization problems is by alternatively optimizing over B for U fixed, and over U for B fixed, until convergence or until a given number of iterations is reached.",6. Learning the weights,0,[0]
"Interestingly, this can be easily implemented due to the conceptual interchangeability of B and U thanks to Theorem 3.",6. Learning the weights,0,[0]
"Specifically, when U is fixed, optimizing over B amounts to training a kernel machine, with the kernel GU (σ, σ′); in turn, forB fixed, optimizing overU also amounts to training a kernel machine, with the kernel GB(σ−1, (σ′)−1) instead.",6. Learning the weights,0,[0]
"Note that this naive alternating procedure implemented in kernel machines such as SVM, it will result in regularizing B and U in the same way, in the sense that the feasible region remain the same for both.
",6. Learning the weights,0,[0]
"Alternatively, it is possible to bypass the need for alternative optimization of B and U .",6. Learning the weights,0,[0]
"Due to Theorem 3, when data
are represented through Πσ ⊗ Πσ, the linear coefficients vec(U) ⊗ (vec(B))> can be directly learned by exerting some low-rank assumption on the linear model.",6. Learning the weights,0,[0]
"This is similar to the supervised quantile normalisation (SUQUAN) model proposed by Le Morvan & Vert (2017), who studied a similar optimization over a quantile distribution by learning a rank-1 linear model.",6. Learning the weights,0,[0]
"While the Kendall kernel (1) relies only on pairwise comparison between items, it may be interesting in some applications to include high-order comparison among tuples beyond pairs (Diaconis, 1988).",7. High-order kernels for permutations,0,[0]
"In this section, we extend the Kendall kernel to higher-order kernels for permutations, establish their weighted variants and show that our systematic approach of learning the weights also applies to high-order cases.
",7. High-order kernels for permutations,0,[0]
"Following the notation of (3), let us denote for any d ≤ n, I(d) = { (i1, . . .",7. High-order kernels for permutations,0,[0]
", id) ∈",7. High-order kernels for permutations,0,[0]
"[1, n]d : 1 ≤ i1 < · · · <",7. High-order kernels for permutations,0,[0]
"id ≤ n } ,
and define the p.d. function associated with the order-d kernel for permutations by
κ(d)(σ) = ∑
(i1,...,id)∈I(d) 1(σ(i1),...,σ(id))∈I(d) .",7. High-order kernels for permutations,0,[0]
"(14)
The order-d kernel is then given by the right-invariance, i.e.,
K(d)(σ, σ′) = κ(d)(σ′σ−1)
= n∑ i1,...,id=1 1(σ(i1),...,σ(id))∈I(d)1(σ′(i1),...,σ′(id))∈I(d) .
",7. High-order kernels for permutations,0,[0]
"(15)
The order-d kernel compares two permutations by counting the number of d-tuples whose relative ordering is concordant.",7. High-order kernels for permutations,0,[0]
"In particular, order-2 kernel reduces to the Kendall kernel (1), i.e., K(2) =",7. High-order kernels for permutations,0,[0]
"Kτ .
Similarly to (11) and (12), we define a weighted order-d embedding for permutations ΦU : Sn → R d︷",7. High-order kernels for permutations,0,[0]
"︸︸ ︷
n×···×n elementwise by
∀σ ∈ Sn , ( ΦU (σ) )",7. High-order kernels for permutations,0,[0]
i1...,7. High-order kernels for permutations,0,[0]
"id = Uσ(i1),...,σ(id) , (16)
where the weights",7. High-order kernels for permutations,0,[0]
U ∈ R d︷,7. High-order kernels for permutations,0,[0]
"︸︸ ︷
n×···×n is a order-d (cubical) tensor of size n at each dimension.",7. High-order kernels for permutations,0,[0]
"Notably, given U , computation of ΦU (σ) for any σ ∈",7. High-order kernels for permutations,0,[0]
"Sn can be done simply by permuting the entries in each dimension of the tensor U by
σ.",7. High-order kernels for permutations,0,[0]
"The weighted order-d kernel is then defined as ∀σ, σ′ ∈ Sn , GU (σ, σ′) = 〈 ΦU (σ),ΦU (σ′) 〉 F
= n∑ i1,...,id=1 Uσ(i1),...,σ(id)Uσ′(i1),...,σ′(id) ,
where 〈·, ·〉F denotes the Frobenius inner product for tensors.",7. High-order kernels for permutations,0,[0]
"In particular, when U is a matrix (order-2 tensor), the weighted order-2 kernel reduces to the weighted kernel (12).",7. High-order kernels for permutations,0,[0]
"Taking the special case Ui1...id = 1i1<···<id , the weighted order-d kernel GU reduced to the standard order-d kernel K(d) in (15).
",7. High-order kernels for permutations,0,[0]
"Again, we would like to learn the weights U adapted to data by jointly optimizing them with the function estimated by a kernel machine.",7. High-order kernels for permutations,0,[0]
"Recall that, in the context of supervised learning with discriminative models, working with the weighted order-d kernel GU for permutations with kernel machines amounts to fitting a linear model to data through the weighted order-d embedding ΦU .",7. High-order kernels for permutations,0,[0]
"In case of weighted high-order embedding and kernel, we can establish highorder counterpart of the results in Theorem 3.
Theorem 4.",7. High-order kernels for permutations,0,[0]
A general linear function on the weighted order-d embedding ΦU with some order-d tensor of linear coefficients B ∈,7. High-order kernels for permutations,0,[0]
R d︷,7. High-order kernels for permutations,0,[0]
"︸︸ ︷
n×···×n can be written equivalently as
hU,B(σ) := 〈 B,ΦU (σ) 〉 F
= 〈 U ,ΦB(σ−1) 〉 F
= 〈 U ⊗ B,Π⊗dσ 〉 F ,
(17)
where ⊗ denotes the outer product2 for tensors, Π⊗dσ ∈ ({0, 1}) 2d︷",7. High-order kernels for permutations,0,[0]
︸︸,7. High-order kernels for permutations,0,[0]
"︷ n×···×n is defined element-wise by
( Π⊗dσ )",7. High-order kernels for permutations,0,[0]
i1...,7. High-order kernels for permutations,0,[0]
idj1...jd = d∏,7. High-order kernels for permutations,0,[0]
"k=1 (Πx)ikjk
= 1(i1,...,id)=πx((j1,...,jd)) .
",7. High-order kernels for permutations,0,[0]
Proof.,7. High-order kernels for permutations,0,[0]
"By definition, the weighted order-d embedding ΦU (σ) (16) is the contracted product (Bader & Kolda, 2006, Section 3.3) of two tensors Π⊗dσ ∈",7. High-order kernels for permutations,0,[0]
"({0, 1}) 2d︷ ︸︸",7. High-order kernels for permutations,0,[0]
︷ n×···×n and U ∈,7. High-order kernels for permutations,0,[0]
R d︷,7. High-order kernels for permutations,0,[0]
"︸︸ ︷
n×···×n respectively at dimensions [1, . .",7. High-order kernels for permutations,0,[0]
.,7. High-order kernels for permutations,0,[0]
", d; 1, . . .",7. High-order kernels for permutations,0,[0]
", d] of both tensors, resulting in an order-d tensor (inheriting the remaining indices of the first tensor).",7. High-order kernels for permutations,0,[0]
"Specifically, we can
2With respect to standard index order (consecutively concatenating the dimensions of each tensor).
write ΦU in a compact representation
ΦU (σ) = 〈 Π⊗dσ ,U 〉 [1,...,d;1,...,d]
= ( Uσ(i1),...,σ(id) )",7. High-order kernels for permutations,0,[0]
i1...,7. High-order kernels for permutations,0,[0]
id ∈ R d︷,7. High-order kernels for permutations,0,[0]
︸︸,7. High-order kernels for permutations,0,[0]
"︷ n×···×n ,
where 〈·, ·〉[1,...,d;1,...,d] denotes the contracted product respectively at dimensions [1, . . .",7. High-order kernels for permutations,0,[0]
", d; 1, . . .",7. High-order kernels for permutations,0,[0]
", d].
",7. High-order kernels for permutations,0,[0]
"Linear function are now of the form: hU,B(σ) = 〈 B,ΦU (σ) 〉 F
= 〈 B, 〈 Π⊗dσ ,U 〉 [1,...,d;1,...,d] 〉 F
= n∑ i1,...,id=1 Bi1...idUσ(i1),...,σ(id)
= n∑ i1,...,id=1 Ui1...idBσ−1(i1),...,σ−1(id)
= 〈 U , 〈 Π⊗dσ ,B 〉 [d,...,2d;1,...,d] 〉 F
= 〈 U ,ΦB(σ−1) 〉 F
= 〈 U ⊗ B,Π⊗dσ 〉 F ,
where 〈·, ·〉F denotes the Frobenius inner product for tensors, whereas 〈·, ·〉[d,...,2d;1,...,d] denotes the contracted product respectively at dimensions [d, . . .",7. High-order kernels for permutations,0,[0]
", 2d; 1, . . .",7. High-order kernels for permutations,0,[0]
", d].
",7. High-order kernels for permutations,0,[0]
"Theorem 4 together with Theorem 3 ensures that there exists no substantial difference when we move from order-2 to order-d embedding, or their underlying weighted kernel.",7. High-order kernels for permutations,0,[0]
"Therefore, the discussion elaborated in Section 6 regarding Theorem 3 can be seamlessly migrated to higher-order cases.",7. High-order kernels for permutations,0,[0]
"For instance, as joint optimization over B and U is non-convex, alternative optimization can be employed to find a stationary point in practice.",7. High-order kernels for permutations,0,[0]
"Thanks to the conceptual interchangeability between B and U , alternating the optimization in B and U with a kernel machines amounts to simply changing the underlying kernel function accordingly.",7. High-order kernels for permutations,0,[0]
"Note that the square unfolding studied by Jiang et al. (2015, Definition 2.2) of the order-2d tensor Π⊗dσ is a matrix of dimension nd × nd, which is exactly the d-fold Kronecker product of the matrix Πσ with itself.",7. High-order kernels for permutations,0,[0]
"Therefore, we can still follow approaches developed by Le Morvan & Vert (2017) in order to directly learn U ⊗B by asserting low-rank assumptions on a linear model.",7. High-order kernels for permutations,0,[0]
"In this section, we demonstrate the use of the proposed weighted kernels compared with the standard Kendall kernel for classification on a real dataset from the European Union survey Eurobarometer 55.2",8. Numerical experiments,0,[0]
"(Christensen, 2010).",8. Numerical experiments,0,[0]
"As part
of this survey, participants were asked to rank, according to their opinion, the importance of six sources of information regarding scientific developments: TV, radio, newspapers and magazines, scientific magazines, the internet, school/university.",8. Numerical experiments,0,[0]
"The dataset also includes demographic information of the participants such as gender, nationality or age.",8. Numerical experiments,0,[0]
"We removed all respondents who did not provide a complete ranking over all six sources, leaving a total of 12, 216 participants.",8. Numerical experiments,0,[0]
"Then, we split the dataset across age groups, where 5, 985 participants were 40 years old or younger, 6, 231 were over 40.",8. Numerical experiments,0,[0]
The objective is to predict the age group of participants from their ranking of n = 6 sources of news.,8. Numerical experiments,0,[0]
"In order to compare different kernels, we chose to fit kernel SVMs with different kernels.
",8. Numerical experiments,0,[0]
"In order to assess the classification performance, we randomly sub-sampled a training set of 400 participants and a test set of 100 participants.",8. Numerical experiments,0,[0]
This random sub-sampling process was repeated 50 times and we report the classification accuracy on 50 test sets.,8. Numerical experiments,0,[0]
"Different types of weighted kernels considered are: the weighted Kendall kernels (8) with top-k weight for k = 2, . . .",8. Numerical experiments,0,[0]
", 6, additive weight for hyperbolic or logarithmic reduction factor, multiplicative weight for hyperbolic or logarithmic reduction factor, the average Kendall kernel (10), the weighted kernel (12) with learned weights via alternative optimization or via SUQUAN-SVD, an SVD-based low-rank approximation algorithm proposed by Le Morvan & Vert (2017, Algorithm 1).",8. Numerical experiments,0,[0]
"In particular, the top-6 Kendall kernel is equivalent to the standard Kendall kernel, which is seen as the baseline to benchmark with.
",8. Numerical experiments,0,[0]
Table 1 and Figure 2 summarize the classification accuracy across 50 random experiments using kernel SVM with different types of weighted kernels.,8. Numerical experiments,0,[0]
"Results show that the average Kendall kernel is the best-performing kernel of all the kernels considered, followed by the weighted kernel with weights learned via SUQUAN-SVD.",8. Numerical experiments,0,[0]
"Since we observed that
the standard deviation (SD) of accuracy scores is relatively large compared to the mean, we carried out paired Wilcoxon rank test to assess whether or not the better performance is significant where certain weighted kernels improved the standard Kendall kernel.",8. Numerical experiments,0,[0]
The p-values are reported in the last column of Table 1.,8. Numerical experiments,0,[0]
"In fact, the average Kendall outperformed the standard Kendall kernel at a significance of 0.01 whereas the weighted kernel with weights learned via SUQUAN-SVD has a significance of 0.07.",8. Numerical experiments,0,[0]
"However, compared to to SUQUAN-SVD, the weighted kernel with weights learned via alternative optimization is particularly not a good choice, which may be due to the fact that fully optimizing over the weights under a rather simple ridge-type regularization can overfit the data, leading to poor generalization.",8. Numerical experiments,0,[0]
"In conclusion, these results show that the weighted Kendall kernel can be promising for certain prediction tasks as they can outperform the standard one.
",8. Numerical experiments,0,[0]
It is interesting to make a note on the performance of topk Kendall kernels varying the cutoff parameter k and the average Kendall kernel (i.e. boxplots to the left of the first vertical dotted line in Figure 2).,8. Numerical experiments,0,[0]
"When we move away from top-6 with all pairs of items included (or equivalently the baseline standard Kendall kernel), as k decreases, the performance increases until peaking at k = 3, then as k continues to decrease, the performance decreases until k = 2 with only the top ranked pair of items considered.",8. Numerical experiments,0,[0]
This implies that choosing the number of top ranked items should be crucial in learning with rankings.,8. Numerical experiments,0,[0]
"Further, the performance can be drastically improved when we use the average Kendall kernel which crowdsourced all top-k Kendall kernels.
",8. Numerical experiments,0,[0]
"Finally, we show the weights learned via SUQUAN-SVD for a data-driven weighted kernel in Figure 3.",8. Numerical experiments,0,[0]
"Recall that the entries Uab of a weight matrix encodes the importance of
pairs of ranked positions a and b. First, the diagonal can be seen as importance of individual positions, and we observe a generally decreasing trend of importance as the position rank increases, i.e., as items become less preferred.",8. Numerical experiments,0,[0]
This is in line with the pattern widely recognized in such data involved in preference ranking.,8. Numerical experiments,0,[0]
"Second, the matrix presents a near skew-symmetric pattern, where the upper triangle contains mostly negative importance and the lower triangle mostly positive.",8. Numerical experiments,0,[0]
This suggests that shifting the relative order of σ(i) > σ(j) or σ(i) < σ(j) results in opposite direction of contribution in evaluating the kernel GU (12).,8. Numerical experiments,0,[0]
"Finally, we observe that, the more we move away from diagonal, the larger the magnitude of the values, which indicates that it is crucial to focus more on pairs of items whose ranks are more distinctively placed by a permutation.",8. Numerical experiments,0,[0]
This pattern of gradient on pairwise position importance in U identifies our foremost motivation of proposing the weighted kernels.,8. Numerical experiments,0,[0]
"We have proposed a general framework to build computationally efficient kernels for permutations, extending the Kendall kernel to incorporate weights and higher-order information, and showing how weights can be systematically optimized in a data-driven way.",9. Discussion,0,[0]
"The price to pay for these extensions is that the dimensionality of the embedding and the number of free parameters of the model can quickly increase, raising computational and statistical challenges that could be addressed in future work.",9. Discussion,0,[0]
"On the theoretical side, the kernels we proposed could naturally be analyzed in the Fourier domain (Kondor & Barbosa, 2010; Mania et al., 2016) which may lead to new interpretations of their regularization properties and reproducing kernel Hilbert spaces (Berg et al., 1984).",9. Discussion,0,[0]
We propose new positive definite kernels for permutations.,abstractText,0,[0]
"First we introduce a weighted version of the Kendall kernel, which allows to weight unequally the contributions of different item pairs in the permutations depending on their ranks.",abstractText,0,[0]
"Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be computed efficiently in O(n ln(n))",abstractText,0,[0]
"operations, where n is the number of items in the permutation.",abstractText,0,[0]
"Second, we propose a supervised approach to learn the weights by jointly optimizing them with the function estimated by a kernel machine.",abstractText,0,[0]
"Third, while the Kendall kernel considers pairwise comparison between items, we extend it by considering higher-order comparisons among tuples of items and show that the supervised approach of learning the weights can be systematically generalized to higher-order permutation kernels.",abstractText,0,[0]
The Weighted Kendall and High-order Kernels for Permutations,title,0,[0]
"In high dimensional learning problems, a sparse solution is often desired as it has better generalization and interpretation.",1. Introduction,0,[0]
"In order to promote sparse solutions, a regularization term that penalizes for the 1-norm of the vector of parameters is often augmented to an empirical loss term.",1. Introduction,0,[0]
"In regression problems, the empirical loss amounts to sum of the squared differences between the linear predictions and true targets.",1. Introduction,0,[0]
"The task of linear regression with 1-norm penalty is known as Lasso (Tibshirani, 1996).",1. Introduction,0,[0]
"To obtain meaningful solutions for the Lasso, it is required to pick a good value of the `1 regularizer.",1. Introduction,0,[0]
"To automatically choose the best regularization value, algorithms that calculate all possible solutions were developed (Efron et al., 2004; Osborne et al., 2000; Tibshirani & Taylor, 2012).
",1. Introduction,0,[0]
"These algorithms find the solution set for all possible regularization values, commonly referred to as the entire reg-
1Department of Computer Science, Princeton University 2Googol Brain.",1. Introduction,0,[0]
"Correspondence to: Yuanzhi Li <yuanzhil@cs.princeton.edu>.
",1. Introduction,0,[0]
"Proceedings of the 35th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
",1. Introduction,0,[0]
ularization path.,1. Introduction,0,[0]
The algorithms typically built upon the property that the Lasso regularization path is piecewise linear in the constituents of solution vector.,1. Introduction,0,[0]
"As a result, their running times are also governed by the total number of linear segments.",1. Introduction,0,[0]
"While experiments with real datasets suggest that the number of linear segments is in practice linear in the dimension of the problem (Rosset & Zhu, 2007), worst case settings (Mairal & Yu, 2012) can yield to exponentially many linear segments.",1. Introduction,0,[0]
"The construction of exponentially complex regression problems of (Mairal & Yu, 2012) stands in stark contrast to the aforementioned methods.",1. Introduction,0,[0]
"Provably polynomial path complexity has so far derived in vastly more restricted settings, such as the one described in (Dubiner & Singer, 2011).
",1. Introduction,0,[0]
We bridge the gap between the de facto complexity of the regularization path in real problems and the worst case analysis of the number of linear segments.,1. Introduction,0,[0]
"We show that under fairly general models, the complexity of the entire regularization path is guaranteed to be polynomial in the dimension of the problem.",1. Introduction,0,[0]
"As an important observation, settings which attain the worst case complexity of the regularization path often exhibit fragile algebraic structure.",1. Introduction,0,[0]
"In contrast, natural datasets often comes with noise, which renders those highly frail structures improbable.",1. Introduction,0,[0]
"This approach is called the smoothed analysis, introduced by the seminal paper of (Spielman & Teng, 2009).
",1. Introduction,0,[0]
The core of smoothed analysis is the assumption that the input data is subjected to a small intrinsic noise.,1. Introduction,0,[0]
"Such noise may come from uncertainty in the physical measurements when collecting the data, irrational decisions in human feedback, or simply the rounding errors in the computation process used for obtaining the data.",1. Introduction,0,[0]
"In this model, we let X ∈ Rn×d denote the data matrix where n is the number of observations is d is the dimension (number of free parameters).",1. Introduction,0,[0]
"Smoothed analysis assumptions implies that X is the sum of Xh, an unknown fixed matrix, and G, which consists of i.i.d. random samples from a normal distribution with a zero mean and low variance, X = Xh + G.",1. Introduction,0,[0]
"In this view, the data is neither completely random nor completely arbitrary.",1. Introduction,0,[0]
"The smoothed complexity of the problem is measured as the expected complexity taken over the random choices of G. Using this framework, it was proved that the smoothed running time of the simplex, k-means, and the Perceptron algorithm (Spielman & Teng, 2001; Arthur et al.,
2009; Blum & Dunagan, 2002) is in fact polynomial while the worst case complexity of these problems is exponential.
",1. Introduction,0,[0]
"We use the above smoothed analysis model to show that on “typical” instances, the total number of linear segments of the Lasso’s exact regularization path is polynomial in the problem size with high probability.",1. Introduction,0,[0]
"Informally speaking and omitting technical details, our main result can be stated as follows.
",1. Introduction,0,[0]
Let X ∈ Rn×d be a data matrix of the form X = Xh +G for any fixed matrix,1. Introduction,0,[0]
"Xh and a random Gaussian matrix G, Gi j ∼ N(0, σ2).",1. Introduction,0,[0]
"Then, for an arbitrary vector of targets y ∈ Rn, with high probability, the total number of linear segments of the Lasso’s exact regularization path for (X, y) is polynomial in n, d, and 1σ .
",1. Introduction,0,[0]
"Our result is conceptually different than the one presented in (Mairal & Yu, 2012).",1. Introduction,0,[0]
Mairal and Yu showed that there exists an approximate regularization path with a small number of linear segments.,1. Introduction,0,[0]
"However, the analysis, while being novel and inspiring, does not shed light on why, in practice, the exact number of linear segments is small as the approximated path is unlikely to coincide with the exact path.",1. Introduction,0,[0]
"Our analysis covers uncharted terrain and different aspects than the approximated path algorithms in (Mairal & Yu, 2012; Giesen et al., 2010).",1. Introduction,0,[0]
"On one hand, we show that when the input data is “typical”, namely comes from a “naturally smooth” distribution, then with high probability, the total number of linear segments, of the exact Lasso path, is already polynomially small.",1. Introduction,0,[0]
"This part of our analysis provides theoretical backing to the empirical findings reported in (Rosset & Zhu, 2007).",1. Introduction,0,[0]
"On the other hand, when the input matrix is atypical and induces a high-complexity path, then we can also obtain a low-complexity approximate regularization path by adding a small amount of random noise to the data and then solve the Lasso’s regularization path on the perturbed instance exactly.",1. Introduction,0,[0]
We also verify our analysis experimentally in section 9.,1. Introduction,0,[0]
"We show that even a tiny amount of perturbation to high-complexity data matrices, results in a dramatic drop in the number of linear segments.
",1. Introduction,0,[0]
"The technique used in this paper is morally different from the smoothed analysis obtained for simplex (Spielman & Teng, 2001), k-means (Arthur et al., 2009), and the Perceptron (Blum & Dunagan, 2002), as there is no concrete algorithm involved.",1. Introduction,0,[0]
"We develop a new framework which shows that when the total number of linear segments is excessively high, then we can tightly couple the solutions of the original, smoothed problems to another set of solutions in a manner that does not depend on G. We then use the randomness of G to show that such couplings are unlikely to exist, thus high complexity solutions are rare.",1. Introduction,0,[0]
We believe that our framework can be extended to other problems such as the regularization path of support vector machines.,1. Introduction,0,[0]
"We use uppercase boldface letters, e.g, X, to denote matrices and lowercase letters x,w to denote vectors, variables, and scalars.",2. Preliminaries,0,[0]
"We use Xi to denote the i’th column of X. Given a set S, we denote by XS ∈ Rd×|S | the sub-matrix of X whose columns are Xi for i ∈ S.",2. Preliminaries,0,[0]
"Analogously, XS̄ denotes the sub-matrix of X with columns Xi for i < S.",2. Preliminaries,0,[0]
"For a matrix X ∈ Rn×d with n ≥ d, we use the term smallest (largest) singular value of X to denote the smallest (largest) right singular value of X. We define the generalized sign of a scalar b as follows,
sign(b) =  +1 b",2. Preliminaries,0,[0]
> 0 −1,2. Preliminaries,0,[0]
"b < 0 0 b = 0 .
",2. Preliminaries,0,[0]
Let y be a vector in Rn and let X =,2. Preliminaries,0,[0]
"[X1, · · · ,Xd] be a matrix in Rn×d .",2. Preliminaries,0,[0]
"The Lasso is the following regression problem,
w[λ] = argmin w∈Rd 1 2 ‖Xw",2. Preliminaries,0,[0]
− y‖22 + λ‖w‖1 .,2. Preliminaries,0,[0]
"(1)
Here, λ > 0 is the regularization value.",2. Preliminaries,0,[0]
The value of λ influences the sparsity level of the solution w[λ].,2. Preliminaries,0,[0]
The larger λ is the sparser the solution is.,2. Preliminaries,0,[0]
"When X is of full column rank, the solution to (1) is unique.",2. Preliminaries,0,[0]
We therefore denote it by w[λ].,2. Preliminaries,0,[0]
We use P = {w[λ] | λ > 0} to denote the set of all possible solution vectors.,2. Preliminaries,0,[0]
"This set is also referred to as the entire regularization path.
",2. Preliminaries,0,[0]
To establish out main result we need a few technical lemmas.,2. Preliminaries,0,[0]
"The first Lemma from (Mairal & Yu, 2012) provides optimality conditions for w[λ].",2. Preliminaries,0,[0]
Lemma 1.,2. Preliminaries,0,[0]
"Let λ > 0, the w[λ] is the optimal solution iff it satisfies the following conditions,
1.",2. Preliminaries,0,[0]
"There exists a vector u[λ] s.t.
X>(Xw[λ]",2. Preliminaries,0,[0]
"− y) = u[λ] .
2.",2. Preliminaries,0,[0]
"Each coordinate of u[λ] satisfies, ui[λ] = { −λ sign (wi[λ]) |wi[λ]| > 0",2. Preliminaries,0,[0]
∈,2. Preliminaries,0,[0]
"[−λ,λ] o.w. .
Let us denote the sign vector as sign(w[λ]), which is obtained by applying the generalized sign function sign(·) element-wise to w[λ].",2. Preliminaries,0,[0]
"The result of (Mairal & Yu, 2012) shows that P is piecewise linear and unique in the following sense.",2. Preliminaries,0,[0]
Lemma 2.,2. Preliminaries,0,[0]
"Suppose X is of full column rank, then P = {w[λ] | λ > 0} is unique, well-defined, and w[λ] is piecewise linear.",2. Preliminaries,0,[0]
"Moreover, for any λ1, λ2 > 0, if the sign vectors at λ1 and λ2 are equal, sign(w[λ1]) = sign(w[λ2]), then w[λ1] and w[λ2] are in the same linear segment.
",2. Preliminaries,0,[0]
We use |P | to denote the total number of linear segments in P. We denote by α > 0,2. Preliminaries,0,[0]
"the smallest singular value of X. Without loss of generality, as we can rescale X and y accordingly, we assume that ‖y‖2 = 1.",2. Preliminaries,0,[0]
"To obtain our main result, we introduce the following smoothness assumption on the data matrix X. Assumption 3 (Smoothness).",2. Preliminaries,0,[0]
"X is generated according to,
X = Xh +G ,
where Xh ∈ Rn×d (n ≥ d) is a fixed unknown matrix with ‖Xh‖2 ≤ 1.",2. Preliminaries,0,[0]
"Each entry of G is an i.i.d. sample from the normal distribution with 0 mean and variance of σ2n .
",2. Preliminaries,0,[0]
"We use N(0, σ2/n) instead of N(0, σ2) for the noise distribution G.",2. Preliminaries,0,[0]
"This choice implies that when σ is a constant the spectral norm of G is also a constant in expectation, E[‖G‖2] = O(1), see for instance (Rudelson & Vershynin, 2010).",2. Preliminaries,0,[0]
"Therefore, the signal-to-noise ratio satisfies,
E [ ‖X‖2‖G‖−12 ] = O(1) .
",2. Preliminaries,0,[0]
It is convenient to think of σ as an arbitrary small constant.,2. Preliminaries,0,[0]
The analysis presented in the sequel employs a fixed constant c that does not depend on the problem size.,2. Preliminaries,0,[0]
"We use f (·) c> poly(·) (analogously, f (·) c< poly(·)) to denote the fact that the function f is everywhere greater (smaller) than a polynomial function up to a multiplicative constant.",2. Preliminaries,0,[0]
"Equipped with the above conventions and the smoothness assumption, the following lemma, due to (Sankar et al., 2006), characterizes the extremal singular values of X .
",2. Preliminaries,0,[0]
Lemma 4.,2. Preliminaries,0,[0]
Let δ > 0.,2. Preliminaries,0,[0]
"With probability of at least 1 − δ, the smallest, denoted α, and largest, denoted β, right singular values of X satisfy,
",2. Preliminaries,0,[0]
α,2. Preliminaries,0,[0]
c,2. Preliminaries,0,[0]
>,2. Preliminaries,0,[0]
"δσ
d and β
c < 1",2. Preliminaries,0,[0]
"+ σ log(1/δ) .
",2. Preliminaries,0,[0]
The bound on β lets assume henceforth that β is O(1) for any reasonable choices of σ and δ.,2. Preliminaries,0,[0]
"In our analysis We describe explicitly dependencies on ‖X ‖ for clarification and states the main results with β = O(1).
",2. Preliminaries,0,[0]
"The main result of the paper is states in the following theorem.
",2. Preliminaries,0,[0]
Theorem 5 (Lasso’s Smoothed Complexity).,2. Preliminaries,0,[0]
"Suppose assumption 3 holds for arbitrary n, d ∈ Z with n ≥ d",2. Preliminaries,0,[0]
"and σ ∈ (0,1].",2. Preliminaries,0,[0]
"Then, with a probability of at least 1 − δ (over the random selection of G), the complexity of the Lasso satisfies,
|P | c< n1.1 (
d δσ
)6 .",2. Preliminaries,0,[0]
"To prove the main theorem, we introduce several properties of X, y,w[λ] and u[λ] that are critical in the analysis of |P |.",3. Main Lemmas,0,[0]
"We then use the smoothness assumption to bound these properties.
",3. Main Lemmas,0,[0]
Definition 1 (Lipschitzness).,3. Main Lemmas,0,[0]
Let wi[λ] and ui[λ] be the value of the i’th coordinate of w[λ] and u[λ] respectively for i ∈,3. Main Lemmas,0,[0]
[d].,3. Main Lemmas,0,[0]
"The coordinate-wise Lipschitz parameters of w and u are defined as,
Lw = max i∈[d]",3. Main Lemmas,0,[0]
"sup λ>0 ∂λwi[λ] , Lu = max i∈[d] sup λ>0 ∂λui[λ] .",3. Main Lemmas,0,[0]
"By definition, Lw and Lu characterize how much each coordinate of w[λ] and u[λ] can change as we vary the value of λ.",3. Main Lemmas,0,[0]
We later use the smoothness assumption to show that Lw and Lu are polynomially small.,3. Main Lemmas,0,[0]
This implies that w[λ] and u[λ] would not change too fast with λ.,3. Main Lemmas,0,[0]
"However, Lipschitzness by itself does not give us a bound on |P | since w[λ] can still oscillate around zero and induce an excessively large number of linear segments.",3. Main Lemmas,0,[0]
"Therefore, we also need the following property which defines the restricted distance between the column space of X and y. Definition 2 (Subspace distance).",3. Main Lemmas,0,[0]
"For any s, δ > 0, let γs denote the largest value such that,
Pr [ ∃v ∈ Rd−s s.t. XS̄v",3. Main Lemmas,0,[0]
"− y 2 ≤ γs] ≤ δ , for all S ⊂",3. Main Lemmas,0,[0]
"[d] of size s.
This definition quantifies the distance of y to a subspace spanned by s ≤ d columns of X. Since y ∈ Rn, n ≥ d, and X is smooth, it can be shown that y cannot be too close to the subspace spanned by XS̄ .",3. Main Lemmas,0,[0]
"That is, γs is inversely proportional to a polynomial in n, d,1/σ.",3. Main Lemmas,0,[0]
We interchangeably use in the following the original matrix X with v ∈ Rd s.t. vi = 0 for i ∈ S and XS̄ with v ∈ Rd−s.,3. Main Lemmas,0,[0]
"Using the above properties, we prove the following theorem.
",3. Main Lemmas,0,[0]
Theorem 6 (Exact Smooth Complexity).,3. Main Lemmas,0,[0]
Let X satisfy Assumption 3.,3. Main Lemmas,0,[0]
"Then, for all s ∈",3. Main Lemmas,0,[0]
"[d] and δ > 0, with probability of at least 1 − δ",3. Main Lemmas,0,[0]
"the complexity of the Lasso satisfies,
|P |",3. Main Lemmas,0,[0]
"c< 3s ©« √
snd ( Lw α2 + Lu )",3. Main Lemmas,0,[0]
"δ2σγs ª®®¬ s s−1 .
",3. Main Lemmas,0,[0]
"The following lemma characterizes the (smoothed) values of Lw , Lu , and γs .",3. Main Lemmas,0,[0]
Lemma 7.,3. Main Lemmas,0,[0]
Let X satisfy Assumption 3.,3. Main Lemmas,0,[0]
"Then with probability of at least 1 − δ the following properties hold,
Lw,Lu c <",3. Main Lemmas,0,[0]
"√ d α2 , γs c > σ√ dn(d/δ)2/s .
",3. Main Lemmas,0,[0]
"Applying the bounds on Lw , Lu , γs , and α to Theorem 6 while letting s be a sufficiently large constant, we can directly prove Theorem 5.",3. Main Lemmas,0,[0]
"In Section 5, we use the value of α to bound Lw and Lu .",3. Main Lemmas,0,[0]
"In Section 6, we employ the smoothness of X to bound γs .",3. Main Lemmas,0,[0]
"Finally, in Section 7 we prove Theorem 6.",3. Main Lemmas,0,[0]
"Since ‖X‖2 = O(1), there exists a constant λmax = Ω(1) such that for λ ≥ λmax, w[λ] is the zero vector.",4. Proof sketch,0,[0]
"Thus, we can divide λ ∈",4. Proof sketch,0,[0]
"[0, λmax] into λmax/ν intervals, each of size ν for some (inversely polynomial) small ν.",4. Proof sketch,0,[0]
We then show that within every interval the total number of linear segments exceeds a fixed polynomial number with exponentially small probability.,4. Proof sketch,0,[0]
The total number of linear segments follows by taking a union bound over all intervals.,4. Proof sketch,0,[0]
"We need to specifically address the following two questions in the analysis.
",4. Proof sketch,0,[0]
"What if |P | within an intervals is excessively large?
",4. Proof sketch,0,[0]
"We will show that when there are N linear segments in an interval, then there must be at least log3(N) many coordinates of w[λ] , which we denote as the set S, that change their sign.",4. Proof sketch,0,[0]
"Since ν is small and w[λ] is a Lipschitz function in λ, we know that those coordinates of w[λ] must be close to zero.",4. Proof sketch,0,[0]
"Therefore, we can show that w[λ] is close to the optimal solution, v[λ], of the Lasso problem when the entries of coordinates in S are constrained to be exactly zero,
",4. Proof sketch,0,[0]
v[λ] = argmin w∈Rd 1 2 ‖Xw,4. Proof sketch,0,[0]
− y‖22,4. Proof sketch,0,[0]
"+ λ‖w‖1s.t.∀i ∈ S : wi = 0
(2)
",4. Proof sketch,0,[0]
"What if w[λ] oscillates excessively around v[λ]?
From the optimality condition of u[λ] and the smoothness of u[λ], we also know that the coordinates in S of u[λ]must be close to either −λ or λ.",4. Proof sketch,0,[0]
"Thus, uS[λ] def = X>S(Xw[λ] − y) is close to a vector on the scaled hypercube {−λ,λ} |S | .",4. Proof sketch,0,[0]
"On the other hand, if w[λ] is close to v[λ], we know that X>S(X v[λ] − y) is also close to a vector in {−λ,λ}
|S | .",4. Proof sketch,0,[0]
"However, v[λ] does not depend on XS by construction.",4. Proof sketch,0,[0]
"Therefore, the residual Xv[λ]",4. Proof sketch,0,[0]
− y also does not depend on XS .,4. Proof sketch,0,[0]
"Thus, using the randomness etched in XS we can now show that X>S(Xv[λ] − y) is close to a vector in {−λ,λ}
|S | with probability which is exponentially small in the size of S. Therefore, we know that w.h.p.",4. Proof sketch,0,[0]
the total number of linear segments in this interval is unlikely to be large.,4. Proof sketch,0,[0]
Recall that we denote the smallest singular value of X by α.,5. Bounding Lw and Lu,0,[0]
"We first show the following lemma regarding pertur-
bations of strongly convex functions.",5. Bounding Lw and Lu,0,[0]
Also recall that a second-order smooth function f :,5. Bounding Lw and Lu,0,[0]
Rd → R is α2-strongly convex if ∇2 f (x) ≥ α2 for all x ∈ Rd and is L-Lipschitz,5. Bounding Lw and Lu,0,[0]
if ‖∇ f (x)‖2 ≤,5. Bounding Lw and Lu,0,[0]
L for all x ∈ Rd .,5. Bounding Lw and Lu,0,[0]
Lemma 8 (Perturbation of strongly convex functions I).,5. Bounding Lw and Lu,0,[0]
"Let f : Rd → R be an non-negative, α2-strongly convex function.",5. Bounding Lw and Lu,0,[0]
Let g : Rd → R be a L-Lipschitz non-negative convex function .,5. Bounding Lw and Lu,0,[0]
"For any λ ≥ 0, let z[λ] be the minimizer of f (z) + λg(z), then we have, dz[λ]dλ 2 ≤ Lα2 .",5. Bounding Lw and Lu,0,[0]
Proof of Lemma 8.,5. Bounding Lw and Lu,0,[0]
For any τ ≥ 0,5. Bounding Lw and Lu,0,[0]
"and λ ≥ 0, let us abbreviate z = z[λ] and denote ε = z[λ + τ]",5. Bounding Lw and Lu,0,[0]
"− z. From α2-strong convexity of f at z and the optimality of z at λ, we know that
1 2 α2‖ε‖22 + f (z) + λg(z)",5. Bounding Lw and Lu,0,[0]
≤,5. Bounding Lw and Lu,0,[0]
f,5. Bounding Lw and Lu,0,[0]
(z + ε) + λg(z + ε) .,5. Bounding Lw and Lu,0,[0]
"(3)
Moreover, using the optimality of z + ε at λ + τ, we know that,
1 2 α2‖ε‖22 + f (z + ε)",5. Bounding Lw and Lu,0,[0]
+ (λ + τ)g(z + ε) ≤,5. Bounding Lw and Lu,0,[0]
f (z) +,5. Bounding Lw and Lu,0,[0]
(λ + τ)g(z) .,5. Bounding Lw and Lu,0,[0]
"(4)
Summing Eqs.",5. Bounding Lw and Lu,0,[0]
"(3) and (4) and rearranging terms yields,
α2‖ε‖22 ≤ τ (g(z)",5. Bounding Lw and Lu,0,[0]
"− g(z + ε)) ≤ τ‖ε‖2L ,
where the last inequality is due to the Lipschitzness assumption on g. Therefore, we get that ‖ε‖2/τ ≤ L/α2.",5. Bounding Lw and Lu,0,[0]
"Letting τ → 0+ completes the proof.
",5. Bounding Lw and Lu,0,[0]
"Using Lemma 8 with f (w) = 12 ‖Xw−y‖22 and g(w) = ‖w‖1, we obtain Lipschitz properties for w[λ] and u[λ].",5. Bounding Lw and Lu,0,[0]
Since we assume that the minimum singular value of X is α then f (w) is α2-strongly convex.,5. Bounding Lw and Lu,0,[0]
"In addition, the norm of ∇g(w) is clearly at most √ d. To simplify notation, when w[λ] is not differentiable at a point λ, we define dw[λ]/dλ = 0.",5. Bounding Lw and Lu,0,[0]
Due to Lipschitzness and strong convexity all vectors in the subgradient set ∂λ w[λ] include this particular choice for a subgradient.,5. Bounding Lw and Lu,0,[0]
"In summary we get the following corollary.
",5. Bounding Lw and Lu,0,[0]
Corollary 9 (Lipschitzness of w).,5. Bounding Lw and Lu,0,[0]
For any λ ≥ 0,5. Bounding Lw and Lu,0,[0]
"it holds that, dw[λ]dλ 2 ≤ √ d α2 .
",5. Bounding Lw and Lu,0,[0]
"Since by definition, u[λ] = X>i (Xw[λ] − y), we obtain a similar corollary for u.
Corollary 10 (Lipschitzness of u).",5. Bounding Lw and Lu,0,[0]
For any λ ≥ 0,5. Bounding Lw and Lu,0,[0]
"it holds that, du[λ]dλ 2 ≤ ‖X‖22 √ d α2 .
",5. Bounding Lw and Lu,0,[0]
"Recall that ‖X‖2 = O(1), thus, from the above corollaries we get
Lw,Lu c < √ d/α2 .",5. Bounding Lw and Lu,0,[0]
"(5)
We also use in the next section the following Lemma.
",5. Bounding Lw and Lu,0,[0]
Lemma 11 (Perturbation of strongly convex functions II).,5. Bounding Lw and Lu,0,[0]
Let f : Rd → R be an α2-strongly convex function and g : Rd → R an L-Lipschitz convex function.,5. Bounding Lw and Lu,0,[0]
"Let z1 and z2 be the minimizers of f (z) and f (z)+ g(z), respectively, then
‖z1",5. Bounding Lw and Lu,0,[0]
"− z2‖2 ≤ L α2 .
",5. Bounding Lw and Lu,0,[0]
Proof of Lemma 11.,5. Bounding Lw and Lu,0,[0]
Let ε = z2,5. Bounding Lw and Lu,0,[0]
− z1.,5. Bounding Lw and Lu,0,[0]
"From strong convexity of f and optimality of z1, we know that
1 2 α2‖ε‖22 + f (z1) ≤",5. Bounding Lw and Lu,0,[0]
f,5. Bounding Lw and Lu,0,[0]
(z2) .,5. Bounding Lw and Lu,0,[0]
"(6)
Due to the optimality of z2 for f (z) + g(z)",5. Bounding Lw and Lu,0,[0]
"we get,
1 2 α2‖ε‖22 + f (z2) + g(z2) ≤",5. Bounding Lw and Lu,0,[0]
f (z1) + g(z1) .,5. Bounding Lw and Lu,0,[0]
"(7)
Summing Eq. (6) and (7) and rearranging terms gives,
α2‖ε‖22 ≤ g(z1)",5. Bounding Lw and Lu,0,[0]
"− g(z2) ≤ L‖ε‖2 .
",5. Bounding Lw and Lu,0,[0]
We therefore get that ‖ε‖2 ≤ Lα2 .,5. Bounding Lw and Lu,0,[0]
In this section we prove that Assumption 3 also yields a bound on γs .,6. Bounding γs,0,[0]
Lemma 12.,6. Bounding γs,0,[0]
Let y ∈ Sn−1 be an arbitrary unit vector.,6. Bounding γs,0,[0]
"Then for any s ∈ {10, . . .",6. Bounding γs,0,[0]
", d} and a set S ⊂",6. Bounding γs,0,[0]
"[d] of size s, the following holds,
Pr [ ∃v ∈ Rd−s s.t. XS̄v",6. Bounding γs,0,[0]
"− y 2 c< σδ 1s√dn ] ≤ δ .
",6. Bounding γs,0,[0]
"For brevity, we denote the event above by Dγ where γ = Ω ( σδ 1 s√
dn
) .
",6. Bounding γs,0,[0]
Proof.,6. Bounding γs,0,[0]
The proof relies on the following simple fact about Gaussian random variables.,6. Bounding γs,0,[0]
"For any unit vector u, scalar τ > 0, g ∈ R, and i ∈",6. Bounding γs,0,[0]
"[d] the following holds,
Pr [ |〈u,Xi〉 − g | ≤ τ",6. Bounding γs,0,[0]
],6. Bounding γs,0,[0]
"≤ eτ √ n
σ , (8)
where Xi is a (column) vector with elements distributed i.i.d according to N(0, σ2/n).
",6. Bounding γs,0,[0]
Let us assume that there exists v ∈ Rd−s such that XS̄v,6. Bounding γs,0,[0]
− y 2 ≤ γ.,6. Bounding γs,0,[0]
"From Lemma 4, we know that with probability of at least 1 − δ, ‖X‖2 c < 1.",6. Bounding γs,0,[0]
"Since we assume that ‖y‖2 = 1, it holds that
γ ≥",6. Bounding γs,0,[0]
XS̄v,6. Bounding γs,0,[0]
− y 2 ≥ ‖y‖2,6. Bounding γs,0,[0]
"− ‖Xv‖2 ≥ 1 − ‖v‖2 ⇒ ‖v‖2 ≥ 1 − γ .
",6. Bounding γs,0,[0]
"Since γ is smaller than 1/2, there must exist a coordinate i for which |vi | ≥ 12√d .
",6. Bounding γs,0,[0]
"Now, let us fix Gj to its observed values for all j , i.",6. Bounding γs,0,[0]
"In addition, let us denote by U the subspace spanned by
{(Xh)i} ∪ {Xj | ∀ j , i, j ∈ S} ∪ {y} .
",6. Bounding γs,0,[0]
We know that U is of dimension d − s + 1.,6. Bounding γs,0,[0]
"Consider an orthonormal basis for {u1, · · · ,us−1} ∈ Rn for the subspace span({Xi})",6. Bounding γs,0,[0]
− U. Suppose there exists v such that XS̄v,6. Bounding γs,0,[0]
− y 2 ≤ γ.,6. Bounding γs,0,[0]
"Then, for every j ∈",6. Bounding γs,0,[0]
"[s − 1], multiplying by u j yields 〈u j,Xi〉vi ≤",6. Bounding γs,0,[0]
γ ⇒ |〈u,6. Bounding γs,0,[0]
"j,Xi〉| ≤ γ|vi | ≤ 2√dγ .",6. Bounding γs,0,[0]
"Note that any pair j , j ′, the inner products 〈u j,Xi〉 and 〈u j′,Xi〉 are independent.",6. Bounding γs,0,[0]
"We now use (8) over all u j and get that Dγ holds with probability of at most,(
6 √
dnγ σ
)s .
",6. Bounding γs,0,[0]
"Finally, choosing γ = Ω ( σδ 1 s√
dn
) completes the proof.",6. Bounding γs,0,[0]
"In this section, we show that when the total number of linear segments in a small interval is excessively large, the optimal solution w[λ] can be coupled with the optimal solution v[λ] of the constrained Lasso problem of (2).
",7. Coupling the solutions,0,[0]
Sign changes.,7. Coupling the solutions,0,[0]
"For a given fixed λ0 > 0 and ν > 0, let us denote by ζ(i) the number of times that the generalized sign of wi changes as λ increases from λ0 to λ0 + ν.",7. Coupling the solutions,0,[0]
"Thus, the total number of linear segments in the interval [λ0, λ0 + ν] is at least ∑d i=1 ζ(i).",7. Coupling the solutions,0,[0]
We prove the following lemmas related to the sign changes.,7. Coupling the solutions,0,[0]
Lemma 13 (Number of sign changes).,7. Coupling the solutions,0,[0]
"For any integer N > 0, any λ0, ν > 0, if ∑d i=1 ζ(i) ≥ N , then there exists at least log3(N)",7. Coupling the solutions,0,[0]
many indices j ∈,7. Coupling the solutions,0,[0]
"[d] such that ζ( j) ≥ 1.
",7. Coupling the solutions,0,[0]
Proof.,7. Coupling the solutions,0,[0]
"According to Lemma 2, each linear segment is associated with a unique sign pattern in {−1,0,1}d .",7. Coupling the solutions,0,[0]
"Since there are N segments, the pigeon hole principle implies that there must exist at least log3(N) many coordinates of w[λ] that change their sign in this interval.
",7. Coupling the solutions,0,[0]
"From the Lipschitzness of w[λ] and u[λ], we also obtain the following lemma.",7. Coupling the solutions,0,[0]
Lemma 14 (Sign change⇒ small weight).,7. Coupling the solutions,0,[0]
For i ∈,7. Coupling the solutions,0,[0]
"[d], if ζ(i) ≥ 1, then following properties hold: |wi[λ0]| ≤",7. Coupling the solutions,0,[0]
Lwν and | |ui[λ0]|,7. Coupling the solutions,0,[0]
"− λ0 | ≤ (Lu + 1)ν.
",7. Coupling the solutions,0,[0]
Proof.,7. Coupling the solutions,0,[0]
"Since ζ(i) ≥ 1, we know that there exists λ ∈",7. Coupling the solutions,0,[0]
"[λ0, λ0 + ν]",7. Coupling the solutions,0,[0]
such that wi[λ] = 0 and |ui[λ]| = λ.,7. Coupling the solutions,0,[0]
Using Lipschitzness of w[λ]we get |wi[λ0]−wi[λ]| = |wi[λ0]| ≤ Lwν.,7. Coupling the solutions,0,[0]
"For ui[λ0] we have,
| |ui[λ0]|",7. Coupling the solutions,0,[0]
− λ0 | ≤ | |ui[λ0]| − |ui[λ]| | + | |ui[λ]| − λ | + |λ,7. Coupling the solutions,0,[0]
− λ0 | ≤,7. Coupling the solutions,0,[0]
"(Lu + 1)ν
which concludes the proof.
",7. Coupling the solutions,0,[0]
We use L̃u in the sequel as a shorthand for Lu + 1.,7. Coupling the solutions,0,[0]
Based on the two lemmas above we readily get the following corollary.,7. Coupling the solutions,0,[0]
Corollary 15.,7. Coupling the solutions,0,[0]
"For any integer N > 0 and ν, λ0 ≥ 0, if∑
i ζ(i) ≥ N , then there exists a subset S ⊆",7. Coupling the solutions,0,[0]
"[d] of cardinality at least log3(N) such that ∀i ∈ S, wi[λ0] ≤ Lwν and |ui[λ0]|",7. Coupling the solutions,0,[0]
− λ0 ≤ L̃uν .,7. Coupling the solutions,0,[0]
Rare events.,7. Coupling the solutions,0,[0]
"Let S be defined as in Corollary 15, we next show that if the size of S is too large, then certain rare couplings would take place.",7. Coupling the solutions,0,[0]
"Thus, the size of S is likely to be small with high probability.",7. Coupling the solutions,0,[0]
"Throughout the rest of the paper we overload notation and denote by XS ∈ Rn×d the matrix where each column Xi , for i < S, is replaced with the zero vector.",7. Coupling the solutions,0,[0]
The matrix XS̄ is defined analogously.,7. Coupling the solutions,0,[0]
"Note that by definition, XS + XS̄ = X. Lemma 16 (Size of S).",7. Coupling the solutions,0,[0]
"For any fixed λ0, ν > 0, and a set S ⊆",7. Coupling the solutions,0,[0]
"[d], let XS̄ ∈ Rn×d be defined as above.",7. Coupling the solutions,0,[0]
"Let vS̄[λ0] be the minimizer,
vS̄[λ0] = arg min v∈Rd 1 2 ‖XS̄ v − y‖22 + λ0‖v‖1
Assume that the properties of Corollary 15 hold for a set S. Then, for every j ∈ S the following inequality holds, X>j (XS̄ vS̄[λ0]",7. Coupling the solutions,0,[0]
"− y) − λ0
≤ 2 √ |S| ‖X‖22
( Lw ‖X‖22ν
α2 + L̃uν
) .
",7. Coupling the solutions,0,[0]
"We refer to this event as E(τ)S with parameter τ = 2 √ |S|‖X‖22 ( Lw ‖X‖22ν α2 + L̃uν ) .
",7. Coupling the solutions,0,[0]
Proof.,7. Coupling the solutions,0,[0]
We know that the i’th coordinate of vS̄[λ0] is zero for all,7. Coupling the solutions,0,[0]
"i ∈ S. Therefore, we need to focus solely on the set of vectors v which are in
K def= {v ∈ Rd | ∀i ∈ S, vi = 0} .
",7. Coupling the solutions,0,[0]
"Since by definition XS = X−XS̄ we can rewrite the original objective as,
1 2 XS̄ w + XS w − y 22 + λ0‖w‖1 .
",7. Coupling the solutions,0,[0]
Let wS̄[λ0] ∈ Rd be a vector whose ith coordinate is the ith coordinate of w[λ0] for i < S and is zero otherwise and let wS[λ0] = w − wS̄[λ0].,7. Coupling the solutions,0,[0]
"From the optimality of w[λ0], we know that wS̄[λ0] is the minimizer of
h(w)",7. Coupling the solutions,0,[0]
= 1 2 XS̄,7. Coupling the solutions,0,[0]
"w+XS wS[λ0] − y 22+λ0‖w‖1 s.t. w ∈ K .
",7. Coupling the solutions,0,[0]
Let g(w) def= 12,7. Coupling the solutions,0,[0]
XS̄,7. Coupling the solutions,0,[0]
w − y 22 +λ0‖w‖1.,7. Coupling the solutions,0,[0]
"Expanding terms we get that for every w ∈ K,
h(w)",7. Coupling the solutions,0,[0]
"− g(w) = 1 2 ‖XS wS[λ0]‖22 − 〈XS wS[λ0], y〉 + 〈 XSwS[λ0],XS̄ w 〉 ,
and the gradient of h(w)",7. Coupling the solutions,0,[0]
− g(w) satisfies ∇,7. Coupling the solutions,0,[0]
((h(w) − g(w)),7. Coupling the solutions,0,[0]
"2 ≤ X>S̄ XS wS[λ0] 2 ≤ ‖X‖22
wS[λ0] 2 .",7. Coupling the solutions,0,[0]
"From our assumption that Corollary 15 holds for S, we know that ‖wS[λ0]‖∞ ≤",7. Coupling the solutions,0,[0]
Lwν,7. Coupling the solutions,0,[0]
"which in turn implies that ‖wS[λ0]‖2 ≤ √ |S|Lwν.
",7. Coupling the solutions,0,[0]
We can now apply Lemma 11 w.r.t g(w) and h(w),7. Coupling the solutions,0,[0]
− g(w) to conclude that vS̄[λ0] − wS̄[λ0] 2 ≤ ‖X‖22 √|S|Lwνα2 .,7. Coupling the solutions,0,[0]
"Therefore, for every j ∈ S the following holds X>j (XS̄ vS̄[λ0]",7. Coupling the solutions,0,[0]
"− y) − λ0
≤ X>j (XS̄ wS̄[λ0] − y)",7. Coupling the solutions,0,[0]
"− λ0 + ‖X‖42 √|S|Lwνα2 ≤
X>j (XS̄ w[λ0] − y) − λ0 + ‖X‖22 (√ |S| ‖X‖22 Lw ν
α2 +
√ |S| ‖wS[λ0]‖∞ )",7. Coupling the solutions,0,[0]
≤ 2,7. Coupling the solutions,0,[0]
"√ |S| ‖X‖22 ( Lw ‖X‖22 ν
α2 + L̃uν
) ,
which concludes the proof.
",7. Coupling the solutions,0,[0]
Using again that ‖X‖2 c < 1,7. Coupling the solutions,0,[0]
"we obtain
τ",7. Coupling the solutions,0,[0]
c < ν,7. Coupling the solutions,0,[0]
√ |S| ( Lw α2 + L̃u ) .,7. Coupling the solutions,0,[0]
"(9)
",7. Coupling the solutions,0,[0]
"This means that of gradient of objective scales as the product of the root of the size of S and the length of the interval ν.
",7. Coupling the solutions,0,[0]
"Bounding the probability of bad events Next we show that for every fixed set S of sufficiently large cardinality and sufficiently small ν, E(τ)S holds with very small probability if X satisfies the smoothness assumption.
",7. Coupling the solutions,0,[0]
Lemma 17 (Smoothing).,7. Coupling the solutions,0,[0]
"For any fixed λ0, ν > 0, τ ≥ 0, and S ⊆",7. Coupling the solutions,0,[0]
"[d], let us decompose G into G = GS̄ +GS as in Lemma 16.",7. Coupling the solutions,0,[0]
"Then, the following inequality holds,
Pr GS
[ E(τ)S GS̄ ] ≤",7. Coupling the solutions,0,[0]
( eτ√n σ,7. Coupling the solutions,0,[0]
XS̄,7. Coupling the solutions,0,[0]
vS̄[λ0],7. Coupling the solutions,0,[0]
− y 2 ) |S,7. Coupling the solutions,0,[0]
"| .
",7. Coupling the solutions,0,[0]
Proof.,7. Coupling the solutions,0,[0]
Consider the vector vS̄[λ0] defined as in Lemma 16.,7. Coupling the solutions,0,[0]
We know that vS̄[λ0] depends only on GS̄ but not on GS .,7. Coupling the solutions,0,[0]
"Therefore, for any fixed G0, by conditioning on GS̄ = G0, we get that for all j ∈ S
Pr GS [ X>j (XS̄vS̄[λ0] − y)",7. Coupling the solutions,0,[0]
"− λ0 ≤ τ] = Pr
GS [ ((Xh)j +Gj )>",7. Coupling the solutions,0,[0]
(XS̄vS̄[λ0] − y),7. Coupling the solutions,0,[0]
− λ0 ≤ τ],7. Coupling the solutions,0,[0]
"= Pr
G j [ ((Xh)j +Gj )> (XS̄vS̄[λ0] − y)",7. Coupling the solutions,0,[0]
"− λ0 ≤ τ] ≤ eτ √ n
σ (XS̄vS̄[λ0] − y) 2 .
",7. Coupling the solutions,0,[0]
Since the inequality holds for all j ∈ S and any G0 the proof is completed.,7. Coupling the solutions,0,[0]
Recall that we assume that the target vector is of unit norm ‖y‖2 = 1.,8. Proof of Theorem 6,0,[0]
"We slightly overload notation and denote by α(X) the smallest right singular value of X. From the optimality of w[λ], we know that
1 2 Xw[λ]",8. Proof of Theorem 6,0,[0]
"− y 22 + λ w[λ] 1 ≤ 12 ‖y‖22 ≤ 12 ,
which implies that ‖Xw[λ]",8. Proof of Theorem 6,0,[0]
− y‖2 ≤ 1.,8. Proof of Theorem 6,0,[0]
"From necessary conditions for optimality we also get,
X>(Xw[λ]",8. Proof of Theorem 6,0,[0]
"− y) = u[λ] .
",8. Proof of Theorem 6,0,[0]
"Thus, we have
‖u[λ]‖2 = X>(Xw[λ]",8. Proof of Theorem 6,0,[0]
"− y) 2 ≤ ‖X‖2 = O(1) .
",8. Proof of Theorem 6,0,[0]
"Therefore, there exists a constant λmax such that implies that w[λ] = 0 for λ ≥ λmax.",8. Proof of Theorem 6,0,[0]
"We next employ the randomness of G. Consider a fixed α0 and examine the event that there exists one set S of size at
least s such that E(τ)S is true, then it holds that, Pr [( ∃S : |S| = s,E(τ)S ) ∩",8. Proof of Theorem 6,0,[0]
"Dγ ∩ α(X) ≥ α0 ] ≤
∑ S0⊆[d], |S0 |=s",8. Proof of Theorem 6,0,[0]
"Pr [ ( E(τ)S ,S = S0 ) ∩",8. Proof of Theorem 6,0,[0]
"Dγ ∩ α(X) ≥ α0 ] ≤
∑ S0⊆[d], |S0 |=s",8. Proof of Theorem 6,0,[0]
"Pr [ E(τ)S0 ∩ Dγ ∩ α(X) ≥ α0 ] ≤ ( d s ) ( eτ √ n σγ )s ≤ ( eτd √ n σγ )s ,
where we used the definition of Dγ and the fact that α def = α(X) ≥ α0 to obtain the last inequality.",8. Proof of Theorem 6,0,[0]
"We now set
τ =",8. Proof of Theorem 6,0,[0]
"O ( (δν)1/s σγ
d √ n
) ,
which in turn implies that (see (9)), ν1−1/s c >
δ1/sσγ d √ ns(Lw/α20+L̃u)
⇒ ν",8. Proof of Theorem 6,0,[0]
"c> (
δ1/sσγ d √ ns(Lw/α20+L̃u)
) s s−1
.",8. Proof of Theorem 6,0,[0]
"and
obtain",8. Proof of Theorem 6,0,[0]
"Pr [( ∃S : |S| = s,E(τ)S holds ) ∩",8. Proof of Theorem 6,0,[0]
"Dγ ∩ α(X) ≥ α0 ] ≤ δν .
",8. Proof of Theorem 6,0,[0]
"Since we have at most 1/ν many intervals, taking union bound over all linear segments we get that
Pr [( N(P) ≥ 3 s
ν
) ∩",8. Proof of Theorem 6,0,[0]
Dγ ∩ (α(X) ≥ α0) ],8. Proof of Theorem 6,0,[0]
"≤ δ
Finally, for properly chosen γ and α0 we also obtain Pr[¬Dγ ∪ (α(X) < α0)] ≤ 2δ which completes the proof.
",8. Proof of Theorem 6,0,[0]
"To recap, there exists a universal constant c such that for all s ∈",8. Proof of Theorem 6,0,[0]
"[d] the complexity of the Lasso path is bounded above by,
c 3s ©« √
snd ( Lw α2 + Lu )",8. Proof of Theorem 6,0,[0]
"δ2σγs ª®®¬ s s−1 .
",8. Proof of Theorem 6,0,[0]
"We now use the bounds on Lw , Lu , and α, yielding,
",8. Proof of Theorem 6,0,[0]
Lw α2 +,8. Proof of Theorem 6,0,[0]
"Lu c < d4.5 δ4σ4 , γs c < σδ 1 s √ dn
⇒",8. Proof of Theorem 6,0,[0]
|P|,8. Proof of Theorem 6,0,[0]
"c< 3s ( √ s n d6
δ6+ 1 s σ6
) s s−1
.
",8. Proof of Theorem 6,0,[0]
By choosing s = O ( log ( nd δσ )),8. Proof of Theorem 6,0,[0]
"we get that
|P",8. Proof of Theorem 6,0,[0]
"| c< n1.1 (
d δσ
)6 .
",8. Proof of Theorem 6,0,[0]
Figure 1.,8. Proof of Theorem 6,0,[0]
Path complexity as a function of dimension for different levels of smoothing.,8. Proof of Theorem 6,0,[0]
"The theoretical non-smoothed (σ = 0) complexity is exponential in the size of the problem.
",8. Proof of Theorem 6,0,[0]
Figure 2.,8. Proof of Theorem 6,0,[0]
Path complexity in a regression task that predicts the value of a pixel from its neighboring pixels using the MNIST dataset.,8. Proof of Theorem 6,0,[0]
We performed two sets of experiments.,9. Experiments,0,[0]
Our path following implementation uses Python with Float128 for high accuracy computations.,9. Experiments,0,[0]
"In the first set of experiments, we start with the exponential complexity construction for Xh ∈ Rd×d from (Mairal & Yu, 2012), which has (3d + 1)/2 many line segments.",9. Experiments,0,[0]
We artificially added to each entry of Xh i.i.d.,9. Experiments,0,[0]
Gaussian noise of mean zero and variance σ2.,9. Experiments,0,[0]
"In this setting, the largest value of the entries of Xh is 1 and y is an all-one vector.",9. Experiments,0,[0]
We show the effect of dimension d and smoothing σ on N(P).,9. Experiments,0,[0]
We report the average over 100 random choices for smoothing per Xh.,9. Experiments,0,[0]
"As can be seen from the figure below, even for a tiny amount of entry-wise noisy of 10−10, the number of linear segments dramatically shrinks.",9. Experiments,0,[0]
"We also include a full table of results, where 1/SNR denotes − log10(σ).
",9. Experiments,0,[0]
For the next experiment we use the MNIST data set.,9. Experiments,0,[0]
We randomly selected n = 1000 images from the data set.,9. Experiments,0,[0]
We constructed the data matrix X ∈ Rn×d2 such that the i’th row of X is a randomly chosen patch from the i’th image of size,9. Experiments,0,[0]
d × d.,9. Experiments,0,[0]
"We cast the center pixel of patch i as the target yi and discard the pixel from X. Thus, the regression task amounts to predicting the center pixel yi using its surrounding pixels X(i).",9. Experiments,0,[0]
We plot the relation between the size of the patch and the path complexity N(P).,9. Experiments,0,[0]
Each point in the graph is the average over 100 random samples of patches and images.,9. Experiments,0,[0]
"As can be seen, when the amount of noise in the data is fixed and governed by the data acquisition process (the minimum pixel value is 0 and maximum is 255 for MNIST), the number of linear segments increases barely faster than linearly in the dimension.",9. Experiments,0,[0]
"We proved that the smoothed complexity of the Lasso’s regularization path is pragmatically polynomial in the input
size.",10. Conclusions,0,[0]
Our analysis contrasts worst case settings for which the Lasso’s complexity is known to be exponential.,10. Conclusions,0,[0]
"To illustrate the key idea, we provided analysis when smoothing each entry in the data matrix by adding small amount of Gaussian noise.",10. Conclusions,0,[0]
"Although not presented here, our analysis carries over to settings in which the smoothing is performed using other distributions which can be sub-Gaussian, sub-exponential, or even non i.i.d.",10. Conclusions,0,[0]
"so long as the rows of G are statistically independent.
",10. Conclusions,0,[0]
"The nature of smoothed analysis usually imposes a large polynomial factor (Spielman & Teng, 2001), as we do not make any additional assumptions on the hidden matrix Xh.",10. Conclusions,0,[0]
"However, we believe that the polynomial degree in our results can be further reduced.",10. Conclusions,0,[0]
"For example, in our proof, we used the fact that the condition number of X is ∼ 1d , which is close to being ill-conditioned.",10. Conclusions,0,[0]
For well-conditioned matrices the polynomial bound can be improved to O(d2.1n1.1).,10. Conclusions,0,[0]
"This reduction is also valid in settings when n ≥ 2d (see e.g. (Rudelson & Vershynin, 2010) for different behaviors of the condition number of Gaussian random matrices for n = d and n ≥ 2d).",10. Conclusions,0,[0]
"Furthermore, when n ≥ 2d, we can also improve γs to Ω(1), hence our polynomial dependency can be further reduced to O(d1.6n0.6).",10. Conclusions,0,[0]
A final improvement may stem from Lw and Lu .,10. Conclusions,0,[0]
"We actually proved that dw[λ]dλ 2 = O (√dα2 ) ,while we only need to use the infinity norm
dw[λ]dλ ∞. We leave these improvements and further generalizations to future research.",10. Conclusions,0,[0]
"We study the complexity of the entire regularization path for least squares regression with 1-norm penalty, known as the Lasso.",abstractText,0,[0]
Every regression parameter in the Lasso changes linearly as a function of the regularization value.,abstractText,0,[0]
The number of changes is regarded as the Lasso’s complexity.,abstractText,0,[0]
Experimental results using exact path following exhibit polynomial complexity of the Lasso in the problem size.,abstractText,0,[0]
"Alas, the path complexity of the Lasso on artificially designed regression problems is exponential.",abstractText,0,[0]
We use smoothed analysis as a mechanism for bridging the gap between worst case settings and the de facto low complexity.,abstractText,0,[0]
Our analysis assumes that the observed data has a tiny amount of intrinsic noise.,abstractText,0,[0]
We then prove that the Lasso’s complexity is polynomial in the problem size.,abstractText,0,[0]
The Well-Tempered Lasso,title,0,[0]
"Clustering data lying close to an unknown union of lowdimensional linear subspaces is a fundamental problem in unsupervised machine learning, known as Subspace Clustering or Generalized Principal Component Analysis (Vidal et al., 2016).",1. INTRODUCTION,0,[0]
"Indeed, this problem is intimately related to the extension of the classical Principal Component Analysis (PCA) to multiple subspaces, and in recent years has
1School of Information Science and Technology, ShanghaiTech University, Shanghai, China.",1. INTRODUCTION,0,[0]
"2Mathematical Institute for Data Science and Department of Biomedical Engineering, Johns Hopkins University, Baltimore, USA.",1. INTRODUCTION,0,[0]
"Correspondence to: Manolis C. Tsakiris <mtsakiris@shanghaitech.edu.cn>.
",1. INTRODUCTION,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. INTRODUCTION,0,[0]
"Copyright 2018 by the author(s).
found numerous applications in machine learning, computer vision, pattern recognition, bioinformatics and systems theory.",1. INTRODUCTION,0,[0]
"Moreover, recent work is beginning to explore connections between subspace clustering and deep learning, with the goal of learning unions of low-dimensional non-linear manifolds (Peng et al., 2016).
",1. INTRODUCTION,0,[0]
"Among a variety of subspace clustering methods (Vidal et al., 2016) including algebraic (Vidal et al., 2005; Tsakiris & Vidal, 2017b; 2018a), iterative (Bradley & Mangasarian, 2000), recursive (Fischler & Bolles, 1981; Tsakiris & Vidal, 2017a), and spectral (Aldroubi et al., 2017; Heckel & Bölcskei, 2015; Lu et al., 2012; Chen & Lerman, 2009) techniques, Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013) is one of the most popular methods.",1. INTRODUCTION,0,[0]
"The reason is that it exhibits a very competitive performance in real-world datasets, it admits efficient algorithmic implementations, and is supported by a rich body of theory (Elhamifar & Vidal, 2013; Soltanolkotabi & Candès, 2012; Wang & Xu, 2016; Soltanolkotabi et al., 2014).",1. INTRODUCTION,0,[0]
"In addition, SSC is able to cluster data from incomplete observations reasonably well (Yang et al., 2015), which is an important problem (Ongie et al., 2017; Pimentel-Alarcon & Nowak, 2016; Elhamifar, 2016; Yang et al., 2015; Heckel & Bölcskei, 2015; Pimentel-Alarcon et al., 2015; Eriksson et al., 2012; Recht, 2011; Balzano et al., 2010), since in many applications not all features are available for every data point: Users of recommendation systems only rate a few items, medical patients undergo a few tests and treatments, images are corrupted by occlusions, dynamic processes are observed across short time intervals and so on.
",1. INTRODUCTION,0,[0]
"Even though the theoretical foundations of SSC are by now mature, there are many lingering open questions.",1. INTRODUCTION,0,[0]
"For example, it is still unclear whether better conditions exist for the performance of SSC even for uncorrupted data; contrast this to the recent study of You & Vidal (2015), who establish a hierarchy of such conditions for sparse subspace recovery.",1. INTRODUCTION,0,[0]
"More importantly, even though a satisfactory theory for SSC with general noise does exist (Wang & Xu, 2016), the theoretical properties of SSC for data with missing entries remain elusive.",1. INTRODUCTION,0,[0]
The works of Wang et al. (2016) and Charles et al. (2018) are important recent efforts towards understanding SSC with missing entries.,1. INTRODUCTION,0,[0]
"However, the conditions of Wang et al. (2016) are hard to interpret and they refer to the formulation of SSC with exact self-
expressiveness equality constraint, which is not an optimal choice for corrupted data.",1. INTRODUCTION,0,[0]
"On the other hand, following Wang & Xu (2016), Charles et al. (2018) provide bounds similar to a subset of the results in the present paper.1
In this paper we provide a novel theoretical analysis of SSC for incomplete data.",1. INTRODUCTION,0,[0]
"More precisely, we provide theoretical performance guarantees for SSC applied to i)",1. INTRODUCTION,0,[0]
"ZeroFilled data (ZF-SSC), in which case all unobserved entries are filled with zeros, and ii) Projected-Zero-Filled data (PZF-SSC), in which case all unobserved entries are filled with zeros and in addition all data points are projected onto the observation pattern of the point being expressed each time.2.",1. INTRODUCTION,0,[0]
A direct comparison of the tolerable bounds of missing entries for ZF-SSC (Theorem 7) and PZF-SSC (Theorem 5) serves as a theoretical indication for the latter being a better method than the former.,1. INTRODUCTION,0,[0]
This is in agreement with experimental evaluation given here and also previously reported by Yang et al. (2015).,1. INTRODUCTION,0,[0]
"Since PZF data have in principle many more missing entries than ZF data, this is a remarkable phenomenon, of potentially wider significance to the entire class of self-expressive-based methods, e.g., (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016).
",1. INTRODUCTION,0,[0]
The rest of the paper is organized as follows.,1. INTRODUCTION,0,[0]
In §1.1 we introduce the notation and the main mathematical objects of this paper.,1. INTRODUCTION,0,[0]
"In §2 we review SSC for uncorrupted data, and discuss the two known elementary formulations of SSC for incomplete data, i.e., ZF-SSC and PZF-SSC.",1. INTRODUCTION,0,[0]
"In §3 we present the main contributions of this paper, which consist of deterministic and probabilistic characterizations of the tolerable percentage of missing entries for ZF-SSC and PZF-SSC, as well as a theoretical and experimental comparison between the two methods (all proofs can be found in our pre-print (Tsakiris & Vidal, 2018b)).",1. INTRODUCTION,0,[0]
"We conclude in §4, where we discuss the main insights of this paper as well as existing challenges.",1. INTRODUCTION,0,[0]
"The nature of the problem studied in this paper calls for a rather heavy notation, which we have strived to simplify and unify as much as possible.",1.1. Notation and Main Objects,0,[0]
"To avoid introducing complicated notation amidst other technical developments, we have found it convenient to gather all relevant objects in Definition 1,3 which the reader is encouraged to refer to
1In the terminology of the present paper Charles et al. (2018) independently study ZF-SSC.
2This is called EWZF-SSC by Yang et al. (2015); here we have taken the liberty to rename the method according to the more suggestive name PZF-SSC.
3For simplicity and clarity, and without loss of generality, we have chosen to present our theoretical results in the context of expressing a single point in terms of the remaining points in the dataset (the precise problem formulation is deferred to §2).
when necessary.",1.1. Notation and Main Objects,0,[0]
"Other than that, for ` a positive integer, we define [`] := {1, . . .",1.1. Notation and Main Objects,0,[0]
", `}",1.1. Notation and Main Objects,0,[0]
.,1.1. Notation and Main Objects,0,[0]
"For a vector w ∈ RD we define ŵ := w/‖w‖2, if w 6= 0 and ŵ",1.1. Notation and Main Objects,0,[0]
":= 0, otherwise.",1.1. Notation and Main Objects,0,[0]
"For any linear subspace V of RD, we denote by P V the square matrix that represents the orthogonal projection of RD onto V .",1.1. Notation and Main Objects,0,[0]
"Given a binary relation, RHS stands for Right-Hand-Side, and similarly for LHS.",1.1. Notation and Main Objects,0,[0]
"Finally, 〈·, ·〉 is the standard inner product of RD.",1.1. Notation and Main Objects,0,[0]
Definition 1.,1.1. Notation and Main Objects,0,[0]
"We define the following objects:
1.",1.1. Notation and Main Objects,0,[0]
The linear subspaces:,1.1. Notation and Main Objects,0,[0]
For i ∈,1.1. Notation and Main Objects,0,[0]
"[n], we let Si be a linear subspace of RD, where dimSi = di < D.
2.",1.1. Notation and Main Objects,0,[0]
"The complete data: With an abuse of notation we let
X =",1.1. Notation and Main Objects,0,[0]
"[X(1), . .",1.1. Notation and Main Objects,0,[0]
.,1.1. Notation and Main Objects,0,[0]
",X(n)]Γ ∈ RD×N (1)
denote a data matrix as well as a set (formed by the columns of this matrix) of unit `2-norm points in the union of the linear subspaces Si, i ∈",1.1. Notation and Main Objects,0,[0]
"[n], where X(i) =",1.1. Notation and Main Objects,0,[0]
"[x
(i) 1 , . . .",1.1. Notation and Main Objects,0,[0]
",x (i) Ni ] ⊂ Si, Span(X(i))",1.1. Notation and Main Objects,0,[0]
#NAME?,1.1. Notation and Main Objects,0,[0]
We define X(1)−1,1.1. Notation and Main Objects,0,[0]
":= X
(1) \{x(1)1 }, X−1 := X \{x(1)1 }, and X
(−1) :",1.1. Notation and Main Objects,0,[0]
#NAME?,1.1. Notation and Main Objects,0,[0]
3,1.1. Notation and Main Objects,0,[0]
The pattern of missing entries: For every point x (i) j ∈,1.1. Notation and Main Objects,0,[0]
RD,1.1. Notation and Main Objects,0,[0]
we consider an observation pattern ω,1.1. Notation and Main Objects,0,[0]
"(i) j ∈
{0, 1}D, where a value of 1 indicates an observed entry, while a value of 0 indicates an unobserved entry.",1.1. Notation and Main Objects,0,[0]
We assume each ω(i)j has precisely m zeros.,1.1. Notation and Main Objects,0,[0]
We let ω̃ (i) j,1.1. Notation and Main Objects,0,[0]
":= 1− ω (i) j , where 1 is the vector of all ones.
4.",1.1. Notation and Main Objects,0,[0]
The observed/unobserved coordinate subspaces: We let Ē(i)j := Span{ek : e>k ω,1.1. Notation and Main Objects,0,[0]
"(i) j 6= 0}, with ek the
canonical vector of RD with zeros everywhere and a 1 at position k.",1.1. Notation and Main Objects,0,[0]
"The orthogonal projection onto Ē(i)j is given by P̄ (i)j := diag(ω (i) j ), the matrix with ω (i) j on its diagonal and zeros everywhere else.",1.1. Notation and Main Objects,0,[0]
Ẽ(i)j is the orthogonal complement of Ē(i)j and P̃ (i) j,1.1. Notation and Main Objects,0,[0]
"= diag(ω̃ (i) j ) is the orthogonal projection onto Ẽ(i)j .
",1.1. Notation and Main Objects,0,[0]
5,1.1. Notation and Main Objects,0,[0]
"The zero-filled data (ZF-data): We let X̄ ∈ RD×N be the data X with zeros appearing in the unobserved entries, i.e., the column of X̄ associated to point x(i)j is x̄(i)j := P̄",1.1. Notation and Main Objects,0,[0]
"(i) j x (i) j , ∀i, j.
6.",1.1. Notation and Main Objects,0,[0]
The projected data: We let Ẋ := P̄,1.1. Notation and Main Objects,0,[0]
(1)1 X be the projection of the data X onto the observed coordinate subspace Ē(1)1 associated to point x (1) 1 .,1.1. Notation and Main Objects,0,[0]
"The column of
Ẋ associated to x(i)j is ẋ (i)",1.1. Notation and Main Objects,0,[0]
j,1.1. Notation and Main Objects,0,[0]
":= P̄ (1) 1 x (i) j , ∀i, j.
7.",1.1. Notation and Main Objects,0,[0]
"The projected and zero-filled data (PZF-data): We let ˙̄X be the projection of the zero-filled data onto Ē(1)1 , i.e., ˙̄X := P̄ (1) 1 X̄ .",1.1. Notation and Main Objects,0,[0]
"The column of
˙̄X associated to point x(i)j is ˙̄x (i) j",1.1. Notation and Main Objects,0,[0]
":= P̄ (1) 1 x̄ (i) j , ∀i, j.
8.",1.1. Notation and Main Objects,0,[0]
"The unobserved data: We define X̃ to be the unobserved components of the data, i.e., X̃ := X − X̄ , and x̃(i)j",1.1. Notation and Main Objects,0,[0]
:= P̃ (i) j,1.1. Notation and Main Objects,0,[0]
x,1.1. Notation and Main Objects,0,[0]
"(i) j , ∀i, j. Similarly, for PZF data
we define ˙̃X := Ẋ − ˙̄X , and ˙̃x(i)j := P̄",1.1. Notation and Main Objects,0,[0]
"(1) 1 x̃ (i) j , ∀i, j.
9.",1.1. Notation and Main Objects,0,[0]
The projected subspaces:,1.1. Notation and Main Objects,0,[0]
For i ∈,1.1. Notation and Main Objects,0,[0]
"[n], we let",1.1. Notation and Main Objects,0,[0]
Ṡi ⊂,1.1. Notation and Main Objects,0,[0]
RD be the orthogonal projection of Si onto the subspace Ē(1)1 .,1.1. Notation and Main Objects,0,[0]
"In other words, if b (i) 1 , . . .",1.1. Notation and Main Objects,0,[0]
", b (i) di
is a basis for Si, then Ṡi is the subspace of RD spanned by the vectors P̄",1.1. Notation and Main Objects,0,[0]
"(1)1 b (i) k ,∀k ∈",1.1. Notation and Main Objects,0,[0]
"[di].
10.",1.1. Notation and Main Objects,0,[0]
"The inradius: We let r be the relative inradius of the symmetrized convex hull Q of all points X(1)−1 lying in subspace S1, except point x(1)1 , i.e., r is the radius of the largest Euclidean ball of S1 contained in Q.
11.",1.1. Notation and Main Objects,0,[0]
"The dual directions: For W = X, X̄, ˙̄X corresponding to complete data X , ZF-data X̄ and PZFdata ˙̄X , consider the reduced Lasso-SSC problem
min c,e ‖c‖1",1.1. Notation and Main Objects,0,[0]
"+
λ 2 ‖e‖22 s.t. w",1.1. Notation and Main Objects,0,[0]
"(1) 1 = W (1) −1c + e, (2)
corresponding to either complete data X , ZF-data X̄ or PZF-data ˙̄X .",1.1. Notation and Main Objects,0,[0]
"Consider the dual problem
max v 〈v,w(1)1 〉 −
1
2λ ‖v‖22 s.t.",1.1. Notation and Main Objects,0,[0]
‖v >,1.1. Notation and Main Objects,0,[0]
W (1) −1‖∞ ≤ 1.,1.1. Notation and Main Objects,0,[0]
"(3)
Let v∗λ, v̄ ∗ λ, ˙̄v ∗ λ be the optimal solution to problem (3) corresponding to W = X, X̄, ˙̄X respectively; these solutions are unique because (3) is strongly convex.",1.1. Notation and Main Objects,0,[0]
"Then we define the corresponding dual directions v̂1,λ, ˆ̄v1,λ, ˆ̄̇v1,λ to be the normalized projections of v∗λ, v̄ ∗ λ, ˙̄v ∗ λ onto S1,S1, Ṡ1 respectively (if any of these projections is equal to zero, then we define the corresponding dual direction to be the zero vector).
12.",1.1. Notation and Main Objects,0,[0]
"The inter-subspace coherences: We define the intersubspace coherences for complete data, ZF-data, and PZF-data respectively as
µλ := max i>1, k∈[Ni]
|〈x(i)k , v̂1,λ〉| (4)
µ̄λ := max i>1, k∈[Ni]
|〈x̄(i)k , ˆ̄v1,λ〉| (5)
˙̄µλ",1.1. Notation and Main Objects,0,[0]
":= max i>1, k∈[Ni]
|〈 ˙̄x(i)k , ˆ̄̇v1,λ〉|.",1.1. Notation and Main Objects,0,[0]
"(6)
13.",1.1. Notation and Main Objects,0,[0]
"The intra-subspace coherences:
ζ := ‖(X(1)−1)>x (1) 1 ‖∞, (7) ζ̄",1.1. Notation and Main Objects,0,[0]
":= ‖(X̄(1)−1)>x̄ (1) 1 ‖∞, (8) ˙̄ζ := ‖( ˙̄X(1)−1)>",1.1. Notation and Main Objects,0,[0]
"˙̄x (1) 1 ‖∞, (ζ̄ = ˙̄ζ) (9)
14.",1.1. Notation and Main Objects,0,[0]
"Other quantities:
η̄",1.1. Notation and Main Objects,0,[0]
":= ‖x̄(1)1 ‖2, (10) ˙̄η := ‖ ˙̄x(1)1 ‖2, (η̄ = ˙̄η) (11)
γ̄",1.1. Notation and Main Objects,0,[0]
":= max i>1,k∈[Ni], j∈[N1]
|〈x̄(i)k ,P S⊥1 x̃ (1) j 〉",1.1. Notation and Main Objects,0,[0]
"| (12)
˙̄γ",1.1. Notation and Main Objects,0,[0]
":= max i>1,k∈[Ni], j∈[N1] |〈 ˙̄x(i)k ,P Ṡ⊥1 ˙̃x (1) j 〉|.",1.1. Notation and Main Objects,0,[0]
-13,1.1. Notation and Main Objects,0,[0]
"We begin by reviewing Sparse Subspace Clustering (SSC) for data with no corruptions (§2.1), as well as the two elementary approaches to SSC for incomplete data (§2.2), which this paper is devoted to analyzing.",2. Review of Sparse Subspace Clustering,0,[0]
"In the absence of data corruptions (noise, missing entries, outliers, etc.)",2.1. SSC With Uncorrupted Data,0,[0]
"we consider a data matrix X ∈ RD×N as in Definition 1, whose columns are unit-`2 points4 that lie in an unknown union of low-dimensional linear subspaces ⋃n i=1",2.1. SSC With Uncorrupted Data,0,[0]
"Si ⊂ RD, with di := dim(Si).",2.1. SSC With Uncorrupted Data,0,[0]
Thus X =,2.1. SSC With Uncorrupted Data,0,[0]
"[X(1) · · ·X(n)]Γ, where each X(i) := [x(i)1 · · ·x (i) Ni
] ∈ RD×Ni consists of Ni points spanning subspace Si, and Γ is an unknown permutation, indicating that the clustering of the points is unknown.
",2.1. SSC With Uncorrupted Data,0,[0]
"Among a variety of methods (Vidal et al., 2016) for retrieving the clusters {X(i)}, one may apply Sparse Subspace Clustering (SSC) (Elhamifar & Vidal, 2009; 2013), whose main principle is to express each point in X as a sparse linear combination of other points in X .",2.1. SSC With Uncorrupted Data,0,[0]
"Specifically, we seek an expression, say, of point x(1)1 as a sparse linear combination of all other points X−1",2.1. SSC With Uncorrupted Data,0,[0]
:,2.1. SSC With Uncorrupted Data,0,[0]
"= X \ {x(1)1 } by means of the basis pursuit problem (Chen et al., 1998)
min c∈RN−1
‖c‖1 s.t. x",2.1. SSC With Uncorrupted Data,0,[0]
"(1) 1 = X−1c, (14)
and then form an affinity graph in which we connect x(1)1 to those points of X−1 that correspond to the support (nonzero coefficients) of the computed optimal solution of (14).",2.1. SSC With Uncorrupted Data,0,[0]
"Clearly, we want these points to lie in the same subspace as x (1) 1 , i.e., to be points of X (1) −1",2.1. SSC With Uncorrupted Data,0,[0]
":= X
(1) \ {x(1)1 }, in which 4This assumption simplifies the theoretical analysis.
case we say that the solution is subspace preserving.",2.1. SSC With Uncorrupted Data,0,[0]
"When this is true for the expression of each and every point in X , then the corresponding affinity graph contains no connections between points in different subspaces, i.e., it is a subspace preserving graph.",2.1. SSC With Uncorrupted Data,0,[0]
"Assuming that points within each subspace are sufficiently well connected, the affinity graph will have precisely n connected components, and spectral clustering will be guaranteed to furnish the correct clusters.
",2.1. SSC With Uncorrupted Data,0,[0]
"Often, it is more practical to search for approximate sparse linear combinations rather exact ones as in (14).",2.1. SSC With Uncorrupted Data,0,[0]
"Thus one may approximately express point x(1)1 by solving the Lasso problem (Tibshirani, 2013)
",2.1. SSC With Uncorrupted Data,0,[0]
"min c,e
‖c‖1 + λ
2 ‖e‖22 s.t. x",2.1. SSC With Uncorrupted Data,0,[0]
"(1) 1 = X−1c + e, (15)
where e represents the self-representation error.",2.1. SSC With Uncorrupted Data,0,[0]
"We have the following known guarantee: Theorem 1 (SSC with uncorrupted data, deterministic (Wang & Xu, 2016)).",2.1. SSC With Uncorrupted Data,0,[0]
"Recall the notation of Definition 1, and suppose that
µλ < r and 1/ζ < λ.",2.1. SSC With Uncorrupted Data,0,[0]
"(16)
Then every optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving.
",2.1. SSC With Uncorrupted Data,0,[0]
Theorem 1 can be interpreted as follows:,2.1. SSC With Uncorrupted Data,0,[0]
"If all data points from S1 other than x(1)1 are well distributed (large r), the data points from other subspaces are sufficiently far from S1 as measured by their inner product with the dual direction v̂1,λ (small µλ), and the reconstruction error is penalized sufficiently enough (large λ), then the Lasso problem (15) is guaranteed to furnish non-zero and subspace preserving solutions.
",2.1. SSC With Uncorrupted Data,0,[0]
Theorem 2 is an even more interpretable statement and is derived by bounding in probability the terms in Theorem 1 under the following simplified fully random model.,2.1. SSC With Uncorrupted Data,0,[0]
Definition 2 (Random model).,2.1. SSC With Uncorrupted Data,0,[0]
For each i ∈,2.1. SSC With Uncorrupted Data,0,[0]
"[n], let the ith subspace be chosen uniformly at random from the Grassmannian manifold of d-dimensional subspaces of RD.",2.1. SSC With Uncorrupted Data,0,[0]
"Moreover, let N/n =:",2.1. SSC With Uncorrupted Data,0,[0]
ρd + 1 points5 be chosen uniformly at random from the intersection of each subspace and the unit sphere SD−1.,2.1. SSC With Uncorrupted Data,0,[0]
"Finally, define the quantities
α :=
√ log(ρ)
16d , β :=
√ 6 log(N)
D .",2.1. SSC With Uncorrupted Data,0,[0]
"(17)
Theorem 2 (SSC with uncorrupted data, probabilistic (Soltanolkotabi & Candès, 2012; Wang & Xu, 2016)).",2.1. SSC With Uncorrupted Data,0,[0]
Consider the random model of Definition 2.,2.1. SSC With Uncorrupted Data,0,[0]
"If ρ is larger than a universal constant, λ > 1/α, and
α > β, (18)
5For simplicity, we assume that n divides N .
then any optimal solution to the Lasso SSC problem (15) is non-zero and subspace preserving, with probability at least 1− 2/N2 − exp(−√ρd).
",2.1. SSC With Uncorrupted Data,0,[0]
"Condition (18) agrees with intuition, since it effectively says that the subspace preserving property is easier to achieve for small relative subspace dimensions d/D, fewer subspaces, and more points per subspace.",2.1. SSC With Uncorrupted Data,0,[0]
"In §3 we will give analogues of Theorems 1 and 2 for two elementary variants of SSC for incomplete data, described next.",2.1. SSC With Uncorrupted Data,0,[0]
"When the data are incomplete but otherwise uncorrupted, one may consider using a low-rank matrix completion algorithm to first complete the data and then apply SSC to the completed data.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"However, this procedure is guaranteed to succeed only when the underlying complete matrix X is of low rank and sufficiently incoherent (Candès & Recht, 2009; Recht, 2011), an assumption which might become invalid in the presence of data from many distinct subspaces.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"As a simple alternative, one may fill with zeros the unobserved entries to obtain a zero-filled data matrix X̄ exactly as in Definition 1, and subsequently solve the problem
min c,e
‖c‖1 + λ
2 ‖e‖22 s.t. x̄","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"(1) 1 = X̄−1c + e, (19)
a procedure called Zero-Filled SSC (ZF-SSC) (Yang et al., 2015).","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"In spite of its simplicity (after all we are just filling in the missing entries with zeros), as per Figs. 2(a) and 2(c) in Yang et al. (2015), ZF-SSC performs only slightly worse than low-rank matrix completion followed by SSC.
","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"Even so, ZF-SSC has an evident shortcoming: it penalizes the reconstruction error of the zero vector along the unobserved part of the point being expressed, which is clearly an undesirable feature of the method.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"More precisely, letting Ē(1)1 and Ẽ (1) 1 be, respectively, the observed and unobserved subspaces associated to point x(1)1 , and P̄ (1) 1 , P̃ (1)
1 the orthogonal projections onto them (see Definition 1), and recalling that (Ē(1)1 )⊥ = Ẽ (1) 1 , we have that
x̄ (1) 1 = P̄","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"(1) 1 x̄ (1) 1 , and (20)
X̄−1 = P̄ (1) 1 X̄−1 +","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"P̃
(1) 1 X̄−1, (21)
and so we can rewrite the objective function of ZF-SSC as
‖c‖1 + λ
2 ‖x̄(1)1 − X̄−1c‖ 2 2 = ‖c‖1+ (22)
λ 2 ‖x̄(1)1","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"− P̄ (1) 1 X̄−1c‖ 2 2 + λ 2 ‖P̃ (1)1 X̄−1c‖ 2 2. (23)
We then see that ZF-SSC penalizes the reconstruction error ‖x̄(1)1 − P̄ (1) 1 X̄−1c‖2 of the observed part of x (1) 1 , which
is desirable, as well as the norm of the vector P̃ (1)
1 X̄−1c.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"The latter is an artifact of the zero-filling process, and could bias the coefficients c away from a subspace preserving pattern.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"Thus, it is reasonable to remove this term and obtain self-expressive coefficients for x̄(1)1 by solving instead
min c,e ‖c‖1","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"+
λ 2 ‖e‖22, s.t. e = x̄ (1) 1 − ˙̄X−1c, (24)
","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
where ˙̄X := P̄,"2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"(1)1 X̄ is the projected and zero-filled data, as in Definition 1.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"Yang et al. (2015) called this approach EWZF-SSC; here we take the liberty to rename it ProjectedZero-Filled Sparse-Subspace-Clustering (PZF-SSC).
","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"PZF-SSC is known to provide accurate clustering while tolerating a higher percentage of missing entries than ZF-SSC or even low-rank matrix completion followed by SSC (e.g., see Fig. 2 in Yang et al. (2015)).","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"This is rather fascinating, since, after all, PZF-SSC works with the projected and zero-filled data ˙̄X , which have more missing entries than the zero-filled data X̄ .","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"Because of this reason, direct application of any generic noise bound, such as that of Theorem 6 in Wang & Xu (2016), would naively suggest that ZF-SSC tolerates more missing entries than PZF-SSC, contradicting experimental evidence.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
"This apparent mystery is resolved in §3, where we adopt a more sophisticated view of PZF-SSC, which unveils its advantage over ZF-SSC.","2.2. SSC With Missing Entries (ZF-SSC, PZF-SSC)",0,[0]
This section contains the main contributions of this paper.,3. SSC Theory for Incomplete Data,0,[0]
"In §3.1-3.2 we give deterministic and probabilistic theorems of correctness for PZF-SSC and ZF-SSC, respectively, in analogy with Theorems 1-2 for SSC with uncorrupted data, while in §3.3 we discuss how the conditions for the two methods compare.",3. SSC Theory for Incomplete Data,0,[0]
"As already remarked so far, PZF-SSC is experimentally known to be a superior method to ZF-SSC, i.e., it can provide an accurate clustering for a higher percentage of missing entries.",3.1. PZF-SSC Theory,0,[0]
"This is remarkable, because the projected and zero-filled data ˙̄X (see Definition 1 for notation) that PZF-SSC operates on contain more missing entries than the zero-filled data X̄ that ZF-SSC operates on.",3.1. PZF-SSC Theory,0,[0]
"On the other hand, we already saw in §2.2 that the additional zeros in ˙̄X are inflicted in such a way, that the objective function minimized by PZF-SSC is, at least on an intuitive level, more accurate than the one minimized by ZF-SSC.
",3.1. PZF-SSC Theory,0,[0]
In this paper we give a theoretical justification for the superiority of PZF-SSC over ZF-SSC.,3.1. PZF-SSC Theory,0,[0]
"Our main insight is the following observation: expressing point x̄(1)1 = ˙̄x (1) 1 as a sparse linear combination of ˙̄X−1, can be seen as ex-
pressing the complete point x̄(1)1 from partial observations ˙̄X−1 of the complete points Ẋ−1, where now the underly-
ing complete data Ẋ lie in the union of subspaces ⋃n i=1",3.1. PZF-SSC Theory,0,[0]
"Ṡi,",3.1. PZF-SSC Theory,0,[0]
"i.e., the original subspaces projected onto the coordinate subspace defined by the observation pattern of the point being expressed (see Definition 1).",3.1. PZF-SSC Theory,0,[0]
"With this in mind, inspired by the seminal work of Wang & Xu (2016), and by
1. making more frequent use of strong duality than in the proof of Theorem 6 in Wang & Xu (2016),
2. using a novel bound for the norm of the dual vector,
3.",3.1. PZF-SSC Theory,0,[0]
"and not decoupling the noise from the data,6
we arrive at the following key result:
Theorem 3 (PZF-SSC, deterministic).",3.1. PZF-SSC Theory,0,[0]
"With the notation of Definition 1, further define the positive quantity
˙̄λ∗ := 1
2
{ 1
2 ˙̄ζ",3.1. PZF-SSC Theory,0,[0]
− ˙̄µλ ˙̄γ,3.1. PZF-SSC Theory,0,[0]
"˙̄η +
√ 9
4 ˙̄ζ2 +
˙̄µλ
˙̄γ",3.1. PZF-SSC Theory,0,[0]
"˙̄η ˙̄ζ + 2 ˙̄γ ˙̄η2 + ˙̄µ2λ ˙̄γ2 ˙̄η2
} .",3.1. PZF-SSC Theory,0,[0]
"(25)
Then the interval ˙̄Λ := (1/ ˙̄ζ, ˙̄λ∗) is non-empty, if
˙̄µλ ˙̄η < ˙̄ζ.",3.1. PZF-SSC Theory,0,[0]
"(26)
",3.1. PZF-SSC Theory,0,[0]
"If in addition7 λ ∈ ˙̄Λ, then every optimal solution to the Lasso SSC problem (24) with projected and zero-filled data is non-zero and subspace preserving.
",3.1. PZF-SSC Theory,0,[0]
What is notable about Theorem 3 is the simplicity of the condition ˙̄µλ,3.1. PZF-SSC Theory,0,[0]
"˙̄η < ˙̄ζ, as well as its resemblance to the condition µλ < r of Theorem 1.",3.1. PZF-SSC Theory,0,[0]
"In fact, the quantity ˙̄µλ is a direct analogue of the inter-subspace coherence µλ, adjusted for the case of PZF data.",3.1. PZF-SSC Theory,0,[0]
"Indeed, as seen from its definition in (6), ˙̄µλ is the maximum inner product between the dual direction associated to the PZF data of subspace S1 and the PZF data from the remaining subspaces.",3.1. PZF-SSC Theory,0,[0]
"The quantity ˙̄η ≤ 1 is the Euclidean norm of the point being expressed, which in the absence of missing entries is equal to 1.
",3.1. PZF-SSC Theory,0,[0]
"Finally, to understand the quantity ˙̄ζ, we first look at its noiseless counterpart ζ defined in (7).",3.1. PZF-SSC Theory,0,[0]
"This measures how well distributed are the points X(1)−1 with respect to point x (1) 1 , or in other words, how coherent they are with that
6By that we mean that we allow our conditions to be stated in terms of the corrupted data as opposed to quantities that depend only on clean data and only on noise.",3.1. PZF-SSC Theory,0,[0]
"This latter approach, e.g. followed by Wang & Xu (2016), usually leads to less tight conditions due to the heavy use of the triangle inequality.",3.1. PZF-SSC Theory,0,[0]
"Instead, we do this decoupling in the probability analysis.
",3.1. PZF-SSC Theory,0,[0]
"7Since the interval ˙̄Λ is a function of λ, it is misleading to write “for any λ ∈ ˙̄Λ”, as Wang & Xu (2016) do in their Theorem 6: ˙̄Λ being non-empty does not alone guarantee that also λ ∈ ˙̄Λ.
point.",3.1. PZF-SSC Theory,0,[0]
"Notice here that ζ is a more relevant quantity than the inradius r, since the latter does not involve any information about the point being expressed.",3.1. PZF-SSC Theory,0,[0]
"In addition, ζ is directly computable from the data, while the inradius is in principle hard to compute.",3.1. PZF-SSC Theory,0,[0]
"Furthermore, it is almost always true that r < ζ, so that if we were to replace condition µλ < r with condition µλ < ζ, we would obtain a better result.",3.1. PZF-SSC Theory,0,[0]
"This is precisely the condition that Theorem 3 reduces to for complete data, which is a novel result itself:
Theorem 4 (SSC with uncorrupted data, deterministic).",3.1. PZF-SSC Theory,0,[0]
Consider expressing point x(1)1 in terms of the rest of the points in X via the Lasso SSC formulation (15).,3.1. PZF-SSC Theory,0,[0]
If µλ < ζ then the open interval,3.1. PZF-SSC Theory,0,[0]
"Λλ := ( ζ−1, 0.5ζ−1 + 0.5µ−1λ ) is non-empty, and if λ ∈ Λλ, then any optimal solution is non-zero and subspace preserving.
",3.1. PZF-SSC Theory,0,[0]
"Returning to the discussion of Theorem 3, we see that the quantity ˙̄ζ captures how well distributed the PZF data ˙̄X(1)−1 are with respect to the point x̄(1)1 that is being expressed, which certainly depends on both how well-distributed the original data X(1)−1 are, as well as on how uniform the observation pattern is.",3.1. PZF-SSC Theory,0,[0]
We can now interpret condition (26): the PZF data ˙̄X(1)−1 associated to the same subspace S1 as the point ˙̄x(1)1 being expressed must be well distributed with respect to that point normalized (large ˙̄ζ/,3.1. PZF-SSC Theory,0,[0]
"˙̄η), while the PZF points ˙̄X(−1) in the remaining subspaces must be sufficiently far away from the projected subspace Ṡ1, as measured by their inner product with the corresponding dual direction ˆ̄̇v1,λ ∈ Ṡ1 (small ˙̄µλ).",3.1. PZF-SSC Theory,0,[0]
"Note here that as the numberm of missing entries increases, the quantity ˙̄η decreases but so does ˙̄ζ; moreover the projection is onto a subspace of even lower dimensionD−m, which makes ˙̄µλ increase, thus overall making it harder for (26) to be satisfied.
",3.1. PZF-SSC Theory,0,[0]
"Next, we derive a probabilistic statement from Theorem 3.",3.1. PZF-SSC Theory,0,[0]
"This is done by constructing high-probability upper and lower bounds for the LHS and RHS of (26), where we exploit the fact that data corruptions due to missing entries are induced by orthogonal projections, i.e., for every x(i)j ,
x̄ (i) j",3.1. PZF-SSC Theory,0,[0]
=,3.1. PZF-SSC Theory,0,[0]
P̄,3.1. PZF-SSC Theory,0,[0]
"(i) j x (i) j = x (i) j + (−P̃
(i) j x (i) j ).",3.1. PZF-SSC Theory,0,[0]
"(27)
Theorem 5 (PZF-SSC, probabilistic).",3.1. PZF-SSC Theory,0,[0]
Consider the random model of Definition 2.,3.1. PZF-SSC Theory,0,[0]
"Suppose that for each point we do not observe exactly m < D−d entries, with the pattern of missing entries being arbitrary, but otherwise fixed apriori.",3.1. PZF-SSC Theory,0,[0]
"Suppose that the point density ρ is larger than a universal constant, and let > 0 be a parameter that controls the probability of success.",3.1. PZF-SSC Theory,0,[0]
"Then there exists a universal constant c, such that if ω := m/D satisfies
α > √ 2ω + β",3.1. PZF-SSC Theory,0,[0]
√ 1− ω,3.1. PZF-SSC Theory,0,[0]
#NAME?,3.1. PZF-SSC Theory,0,[0]
"√ + β2/3, (28)
then there exists a non-empty interval Λ ⊂ R such that for any λ ∈ Λ, any optimal solution to the PZF-SSC problem (24) is non-zero and subspace preserving, with probability",3.1. PZF-SSC Theory,0,[0]
"at least 1− 2/N2 − exp(−√ρd)− (2/n) exp(−cD ).
",3.1. PZF-SSC Theory,0,[0]
"To get an insight into how the maximal tolerable level of missing entries scales with the subspace dimension d, we note that for high-ambient dimensions D the quantity β is negligible with respect to the quantity α.",3.1. PZF-SSC Theory,0,[0]
"Similarly, ignoring the small parameter , (28) becomes approximately α ≥ √ 2ω, which by the definition of α and ω gives
PZF-SSC : m
D <
1
2
log(ρ)
16d",3.1. PZF-SSC Theory,0,[0]
"= O
( 1
d
) .",3.1. PZF-SSC Theory,0,[0]
"(29)
Informally, (29) says that the maximal tolerable percentage of missing entries of PZF-SSC as predicted by Theorem 5, scales inversely proportionally to the subspace dimension.",3.1. PZF-SSC Theory,0,[0]
Similar techniques that led to Theorems 3 and 5 can be employed to yield deterministic and probabilistic statements about ZF-SSC.,3.2. ZF-SSC Theory,0,[0]
"In particular, we have:
Theorem 6 (ZF-SSC, deterministic).",3.2. ZF-SSC Theory,0,[0]
"With the notation of Definition 1, further define the positive quantity
λ̄∗ := 1
2
{ 1
2ζ̄ − µ̄λ γ̄η̄ − 1 2η̄2 +
( 9
4ζ̄2 + µ̄λ",3.2. ZF-SSC Theory,0,[0]
"γ̄η̄ζ̄ +
2
γ̄η̄2 + µ̄2λ γ̄2η̄2 + 1 4η̄4 + 1 η̄2 ( µ̄λ γ̄η̄ − 1 2ζ̄ ))1/2} .",3.2. ZF-SSC Theory,0,[0]
"(30)
Then the interval Λ̄ := (1/ζ̄, λ̄∗) is non-empty, if
µ̄λ η̄",3.2. ZF-SSC Theory,0,[0]
+ γ̄,3.2. ZF-SSC Theory,0,[0]
"< ζ̄. (31)
",3.2. ZF-SSC Theory,0,[0]
"If in addition λ ∈ Λ̄, then every optimal solution to the Lasso SSC problem (19) with zero-filled data is non-zero and subspace preserving.
",3.2. ZF-SSC Theory,0,[0]
"The quantities µ̄λ, η̄, ζ̄ are in direct analogy with the quantities ˙̄µλ, ˙̄η, ˙̄ζ that appeared in Theorem 3, except that now they are defined in terms of ZF data instead of PZF data.",3.2. ZF-SSC Theory,0,[0]
"In fact, as seen from their definitions in (10) and (7), η̄ = ˙̄η and ζ̄ = ˙̄ζ, while in principle the inter-subspace coherences µ̄λ, ˙̄µλ need not coincide.",3.2. ZF-SSC Theory,0,[0]
"Instead, the main difference between (31) and (26) is the appearance of the quantity γ̄, whose PZF counterpart ˙̄γ appears in Theorem 3 only in the definition of the allowable interval for λ.
",3.2. ZF-SSC Theory,0,[0]
"The quantity γ̄ admits an interesting interpretation: As seen from its definition in (12), γ̄ captures the coherence between the ZF data X̄(−1) associated to subspaces Si, i > 1, and a projected version of the unobserved components
X̃ (1)
−1 of the data from S1.",3.2. ZF-SSC Theory,0,[0]
"A large such coherence intuitively means that significant information about S1, potentially crucial for the reconstruction of x̄(1)1 as a linear combination of points in X̄−1, is leaked away into X̃ (1) −1, with which X̄(−1) highly correlates (assuming large γ̄).",3.2. ZF-SSC Theory,0,[0]
"In turn, this may lead the optimization problem to favor points of X̄
(−1) in expressing x̄(1)1 , thus leading to the loss of the subspace-preserving property by the solutions to (19).
",3.2. ZF-SSC Theory,0,[0]
"Interestingly, comparison of the proofs of Theorems 3 and 6 reveals that ˙̄γ did not appear in (26) because x̄(1)1 is complete when the underlying subspace arrangement is taken to be ⋃n i=1 Ṡi, which is the natural view that we adopted for our analysis of PZF-SSC.",3.2. ZF-SSC Theory,0,[0]
"On the contrary, such a feature is not available in the analysis of ZF-SSC, as x̄(1)1 is in principle incomplete with respect to ⋃n i=1",3.2. ZF-SSC Theory,0,[0]
"Si.
",3.2. ZF-SSC Theory,0,[0]
"As we did for PZF-SSC, we use the deterministic Theorem 6 to derive a probabilistic statement: Theorem 7 (ZF-SSC, probabilistic).",3.2. ZF-SSC Theory,0,[0]
Consider the exact setting of Theorem 5.,3.2. ZF-SSC Theory,0,[0]
If ω := m/D satisfies α,3.2. ZF-SSC Theory,0,[0]
>,3.2. ZF-SSC Theory,0,[0]
"( √ 2 + √ + β2/3) √ ω + (β + √ + β2/3)
",3.2. ZF-SSC Theory,0,[0]
"√ 1− ω+√
ω(1− ω) + (1 + β + √ + β2/3)",3.2. ZF-SSC Theory,0,[0]
"√ + β2/3,(32)
",3.2. ZF-SSC Theory,0,[0]
"then there exists a non-empty interval Λ ⊂ R such that for any λ ∈ Λ, any optimal solution to the ZF-SSC problem (19) is non-zero and subspace preserving, with probability at least 1−2/N2−exp(−√ρd)−2(1",3.2. ZF-SSC Theory,0,[0]
"+1/n) exp ( −cD ) .
",3.2. ZF-SSC Theory,0,[0]
"Repeating the informal arguments that led to (29), i.e., for high ambient dimension D ignoring β and , (32) becomes α >",3.2. ZF-SSC Theory,0,[0]
√ 2ω + √ ω(1− ω).,3.2. ZF-SSC Theory,0,[0]
"Since √ ω ≥ √ ω(1− ω), we then have that this latter simplified condition is satisfied if the stronger condition α >",3.2. ZF-SSC Theory,0,[0]
(1 + √ 2),3.2. ZF-SSC Theory,0,[0]
√ ω is true.,3.2. ZF-SSC Theory,0,[0]
"This gives
ZF-SSC : m
D <
1
(1 + √ 2)2 log(ρ) 16d",3.2. ZF-SSC Theory,0,[0]
"= O
( 1
d
) , (33)
i.e., ZF-SSC can tolerate 1/d fraction of missing entries.8",3.2. ZF-SSC Theory,0,[0]
"As per (29) and (33), both PZF-SSC and ZF-SSC give subspace preserving solutions as long as the ratio of missing entries scales as 1/d. On the other hand, the multiplying constant associated to PZF-SSC is about 3 times larger, suggesting a superiority of PZF-SSC.",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"Alternatively, with
fPZF(ω) :=α− √ 2ω−β √ 1−ω−(1+β)",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"√ +β2/3, (34)
the PZF Theorem 5 asks that
fPZF(ω) > 0, (35)
8This result is in agreement with the result of Charles et al. (2018), who studied only ZF-SSC.
while the ZF Theorem 7 asks that fZF(ω) :",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"=− √ + β2/3( √ ω + √ 1− ω + √ + β2/3)
− √ ω(1− ω) + fPZF(ω) > 0, (36)
a significantly harder condition to satisfy than (35), due to the dominating negative term − √ ω(1− ω).",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"Once again, this suggests that PZF-SSC has an advantage over ZF-SSC.
",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
The actual algorithmic behavior is depicted in Fig. 1.,3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"In Figs. 1(a)-1(b) we plot the subspace preserving accuracies for PZF-SSC and ZF-SSC, defined as the ratios of the `1- norm of the N ×N self-representation matrices CPZF and CZF restricted to intra-subspace connections over the total `1 norm of CPZF and CZF respectively.",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
This quantity measures the degree to which points within a subspace use only points from the same subspace for their representation.,3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
In Figs. 1(c)-1(d),3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"we show the clustering accuracy that corresponds to spectral clustering applied on the affinity graphs defined by CPZF and CZF.
",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
There are at least four notable observations.,3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"First, as seen in Figs. 1(a)-1(b), the phase transition between subspace preserving solutions and non-subspace preserving ones is
indeed of hyperbolic nature for both methods, as theoretically predicted by (29) and (33).",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"Second, PZF-SSC has indeed higher subspace preserving accuracy than ZF-SCC, as suggested by our two theoretical arguments in the beginning of this section: For example, for 5-dimensional subspaces (d = 5) in R100 PZF-SSC can tolerate up to 34 missing entries per point, while ZF-SSC can tolerate up to 19.",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"Third, both methods start breaking down rather quickly as the number of missing entries increases: for d = 10 PZFSSC and ZF-SSC can tolerate, respectively, at most 23 and 10 missing entries per point before their solutions become non-subspace preserving, while for d = 20 the maximal tolerable number of missing entries becomes 10 and 4, respectively.",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
Notice how close the values for ZF-SSC are to D/d in each of the above cases.,3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
"Finally, even though the quality of the connections degrades quickly as d and ω increase, the clustering accuracy remains very high (close to 1) for both methods, a phenomenon that we attribute to the robustness of spectral clustering (Figs. 1(c)-1(d)).",3.3. A Comparison between PZF-SSC and ZF-SSC,0,[0]
Bounding dual vectors and inradius.,4. Discussion,0,[0]
"A feature of our theory is that the subspace separation conditions for complete, ZF and PZF data have the same geometric form, i.e.,
µλ < ζ, µ̄λη̄ + γ̄ < ζ̄, and ˙̄µλ",4. Discussion,0,[0]
˙̄η < ˙̄ζ,4. Discussion,0,[0]
"(37)
respectively.",4. Discussion,0,[0]
This nice structure comes from a novel bound on the norm of the so-called dual vector v that takes into consideration both the objective function as well as the constraint of the reduced dual problem (2).,4. Discussion,0,[0]
"Instead, Soltanolkotabi & Candès (2012) bound v exclusively from the constraint of (2).",4. Discussion,0,[0]
"The two techniques lead to a trade-off between tightness of subspace separation conditions and upper bounds for the Lasso parameter λ9 and it is an open problem to optimally bound v, which is then expected to
9This is more easily seen by comparing the conditions of Theorems 1 and 4 for complete data.
lead to jointly better conditions.",4. Discussion,0,[0]
"At any case, the probabilistic lower bound on r < ζ that we also have used in our analysis is the quantity α = √ log(ρ)/16d (AlonsoGutierrez, 2008), which even though of fundamental theoretical importance, is too pessimistic: for ρ = 5 and d = 5 eq.",4. Discussion,0,[0]
"(26) predicts at most 1 tolerable missing entry for PZFSSC in R100, while as per Fig. 1(a)",4. Discussion,0,[0]
the method handles 34 missing entries per point.,4. Discussion,0,[0]
"Can we do better than that?
PZF vs. ZF.",4. Discussion,0,[0]
"As argued theoretically by comparing Theorems 5 and 7, and corroborated experimentally by Fig. 1 (§3.3), projecting the incomplete dataset onto the observation pattern of the point being expressed increases the robustness of the self-representation of the dataset to missing entries with respect to the subspace preserving property, at least for low-dimensional subspaces.",4. Discussion,0,[0]
"Our study was solely in the context of SSC, yet we believe that working with PZF data instead of ZF data is advantageous regardless of the choice of self-expressive method (Liu et al., 2013; Lu et al., 2012; Elhamifar & Vidal, 2013; Wang et al., 2013; You et al., 2016); a conjecture to be established.
",4. Discussion,0,[0]
Beyond PZF-SSC.,4. Discussion,0,[0]
"Even though the clustering accuracy for PZF-SSC seems rather satisfactory as depicted for higher subspace dimensions and higher missing rates in Fig. 2(a), its rather poor subspace preserving accuracy shown in 2(b), suggests that PZF-SSC is still too simple a method to handle the subspace clustering problem for incomplete data, and that its performance relies to a significant extent on the robustness of spectral clustering.",4. Discussion,0,[0]
"E.g., as per Figs. 2(a)-2(b), for three 60-dimensional subspaces inside R100 and 15 missing entries per point, about 40% of the points a point connects to live in different subspaces; yet the clustering accuracy is 99%.",4. Discussion,0,[0]
"On the other hand, the more sophisticated approach of Elhamifar (2016) builds on the SSC formulation and allows for both clustering and completion in a unified framework.",4. Discussion,0,[0]
"Nevertheless, that approach comes with no theoretical guarantees and appears to be computationally burdensome, leaving as an open challenge the proposal of a theoretically sound, efficient and accurate algorithm for clustering incomplete data associated to a union of low-dimensional subspaces.",4. Discussion,0,[0]
Work funded by ShanghaiTech University and NSF grants 1447822 and 1618637.,Acknowledgements,0,[0]
"The first author thanks Yunzhen Yao for proof-reading the longer version of this manuscript and catching some mistakes, as well as for producing the experiments.",Acknowledgements,0,[0]
"He thanks Dr. Chun-Guang Li for insightful comments on an earlier version of this manuscript, Dr. Gregory Ongie for comments on the current manuscript, and Ron Boger for interesting conversations on missing entries and for sharing his code.",Acknowledgements,0,[0]
The authors thank all four anonymous reviewers for their constructive comments.,Acknowledgements,0,[0]
Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subspaces; a problem with numerous applications in pattern recognition and computer vision.,abstractText,0,[0]
"Even though the behavior of SSC for complete data is by now wellunderstood, little is known about its theoretical properties when applied to data with missing entries.",abstractText,0,[0]
"In this paper we give theoretical guarantees for SSC with incomplete data, and provide theoretical evidence that projecting the zero-filled data onto the observation pattern of the point being expressed can lead to substantial improvement in performance; a phenomenon already known experimentally.",abstractText,0,[0]
"The main insight of our analysis is that even though this projection induces additional missing entries, this is counterbalanced by the fact that the projected and zerofilled data are in effect incomplete points associated with the union of the corresponding projected subspaces, with respect to which the point being expressed is complete.",abstractText,0,[0]
The significance of this phenomenon potentially extends to the entire class of self-expressive methods.,abstractText,0,[0]
Theoretical Analysis of Sparse Subspace Clustering with Missing Entries,title,0,[0]
"Neural networks, especially large-scale deep neural networks, have made remarkable success in various applications such as computer vision, natural language processing, etc.",1. Introduction,0,[0]
"(Krizhevsky et al., 2012)(Sutskever et al., 2014).",1. Introduction,0,[0]
"However, large-scale neural networks are both memoryintensive and computation-intensive, thereby posing severe challenges when deploying those large-scale neural network models on memory-constrained and energyconstrained embedded devices.",1. Introduction,0,[0]
"To overcome these limitations, many studies and approaches, such as connection pruning (Han et al., 2015)(Gong et al., 2014), low rank approximation (Denton et al., 2014)(Jaderberg et al., 2014), sparsity regularization (Wen et al., 2016)(Liu et al., 2015)
",1. Introduction,0,[0]
Co-first authors: Liang Zhao and Siyu Liao.,1. Introduction,0,[0]
"1The City University of New York, New York, New York, USA 2Syracuse University, Syracuse, New York, USA.",1. Introduction,0,[0]
Correspondence to:,1. Introduction,0,[0]
"Bo Yuan <byuan@ccny.cuny.edu>.
",1. Introduction,0,[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,0,[0]
"Copyright 2017 by the author(s).
etc., have been proposed to reduce the model size of largescale (deep) neural networks.
",1. Introduction,0,[0]
"LDR Construction and LDR Neural Networks: Among those efforts, low displacement rank (LDR) construction is a type of structure-imposing technique for network model reduction and computational complexity reduction.",1. Introduction,0,[0]
"By regularizing the weight matrices of neural networks using the format of LDR matrices (when weight matrices are square) or the composition of multiple LDR matrices (when weight matrices are non-square), a strong structure is naturally imposed to the construction of neural networks.",1. Introduction,0,[0]
"Since an LDR matrix typically requires O(n) independent parameters and exhibits fast matrix operation algorithms (Pan, 2001), an immense space for network model and computational complexity reduction can be enabled.",1. Introduction,0,[0]
"Pioneering work in this direction (Cheng et al., 2015)(Sindhwani et al., 2015) applied special types of LDR matrices (structured matrices), such as circulant matrices and Toeplitz matrices, for weight representation.",1. Introduction,0,[0]
"Other types of LDR matrices exist such as Cauchy matrices, Vandermonde matrices, etc., as shown in Figure 1.
",1. Introduction,0,[0]
Benefits of LDR Neural Networks:,1. Introduction,0,[0]
"Compared with other types of network compression approaches, the LDR construction shows several unique advantages.",1. Introduction,0,[0]
"First, unlike heuristic weight-pruning methods (Han et al., 2015)(Gong et al., 2014) that produce irregular pruned networks, the LDR construction approach always guarantees the strong structure of the trained network, thereby avoiding the stor-
age space and computation time overhead incurred by the complicated indexing process.",1. Introduction,0,[0]
"Second, as a “train from scratch” technique, LDR construction does not need extra re-training, and hence eliminating the additional complexity to the training process.",1. Introduction,0,[0]
"Third, the reduction in space complexity and computational complexity by using the structured weight matrices are significant.",1. Introduction,0,[0]
"Different from other network compression approaches that can only provide a heuristic compression factor, the LDR construction can enable the model reduction and computational complexity reduction in Big-O complexity:",1. Introduction,0,[0]
"The storage requirement is reduced from O(n2) to O(n), and the computational complexity can be reduced from O(n2) to O(n log n) or O(n log2 n) because of the existence of fast matrix-vector multiplication algorithm (Pan, 2001)(Bini et al., 1996) for LDR matrices.",1. Introduction,0,[0]
"For example, when applying structured matrices to the fully-connected layers of AlexNet using ImageNet dataset (Deng et al., 2009), the storage requirement can be reduced by more than 4,000X while incurring negligible degradation in overall accuracy (Cheng et al., 2015).
",1. Introduction,0,[0]
"Motivation of This Work: Because of its inherent structure-imposing characteristic, convenient re-trainingfree training process and unique capability of simultaneous Big-O complexity reduction in storage and computation, LDR construction is a promising approach to achieve high compression ratio and high speedup for a broad category of network models.",1. Introduction,0,[0]
"However, since imposing the structure to weight matrices results in substantial reduction of weight storage from O(n2) to O(n), cautious researchers need to know whether the neural networks with LDR construction, referred to as LDR neural networks, will consistently yield the similar accuracy as compared with the uncompressed networks.",1. Introduction,0,[0]
"Although (Cheng et al., 2015)(Sindhwani et al., 2015) have already shown that using LDR construction still results the same accuracy or minor degradation on various datasets, such as ImageNet (Deng et al., 2009), CIFAR (Krizhevsky & Hinton, 2009) etc., the theoretical analysis, which can provide the mathematically solid proofs that the LDR neural networks can converge to the same “effectiveness” as the uncompressed neural networks, is still very necessary in order to promote the wide application of LDR neural networks for emerging and larger-scale applications.
",1. Introduction,0,[0]
"Technical Preview and Contributions: To address the above necessity, in this paper we study and provide a solid theoretical foundation of LDR neural networks on the ability to approximate an arbitrary continuous function, the error bound for function approximation, applications on shallow and deep neural networks, etc.",1. Introduction,0,[0]
"More specifically, the main contributions of this paper include:
• We prove the universal approximation property for LDR neural networks, which states that the LDR neu-
ral networks could approximate an arbitrary continuous function with arbitrary accuracy given enough parameters/neurons.",1. Introduction,0,[0]
"In other words, the LDR neural network will have the same “effectiveness” of classical neural networks without compression.",1. Introduction,0,[0]
"This property serves as the theoretical foundation of the potential broad applications of LDR neural networks.
",1. Introduction,0,[0]
"• We show that, for LDR matrices defined by O(n) parameters, the corresponding LDR neural networks are still capable of achieving integrated squared error of order O(1/n), which is identical to the error bound of unstructured weight matrices-based neural networks, thereby indicating that there is essentially no loss for restricting to the weight matrices to LDR matrices.
",1. Introduction,0,[0]
• We develop a universal training process for LDR neural networks with computational complexity reduction compared with backward propagation process for classical neural networks.,1. Introduction,0,[0]
"The proposed algorithm is the generalization of the training process in (Cheng et al., 2015)(Sindhwani et al., 2015) that restricts the structure of weight matrices to circulant matrices or Toeplitz matrices.
",1. Introduction,0,[0]
Outline: The paper is outlined as follows.,1. Introduction,0,[0]
In Section 2 we review the related work on this topic.,1. Introduction,0,[0]
Section 3 presents necessary definitions and properties of matrix displacement and LDR neural networks.,1. Introduction,0,[0]
The problem statement is also presented in this section.,1. Introduction,0,[0]
In Section 4 we prove the universal approximation property for a broad family of LDR neural networks.,1. Introduction,0,[0]
"Section 5 addresses the approximation potential (error bounds) with a limited amount of neurons on shallow LDR neural networks and deep LDR neural networks, respectively.",1. Introduction,0,[0]
The proposed detailed procedure for training general LDR neural networks are derived in Section 6.,1. Introduction,0,[0]
Section 7 concludes the article.,1. Introduction,0,[0]
"Universal Approximation & Error Bound Analysis: For feedforward neural networks with one hidden layer, (Cybenko, 1989) and (Hornik et al., 1989) proved separately the universal approximation property, which guarantees that for any given continuous function or decision function and any error bound > 0, there always exists a single-hidden layer neural network that approximates the function within integrated error.",2. Related Work,0,[0]
"However, this property does not specify the number of neurons needed to construct such a neural network.",2. Related Work,0,[0]
"In practice, there must be a limit on the maximum amount of neurons due to the computational limit.",2. Related Work,0,[0]
"Moreover, the magnitude of the coefficients can be neither too large nor too small.",2. Related Work,0,[0]
"To address these issues for general neural networks, (Hornik et al., 1989) proved that it is sufficient to approximate functions with weights and bi-
ases whose absolute values are bounded by a constant (depending on the activation function).",2. Related Work,0,[0]
"(Hornik, 1991) further extended this result to an arbitrarily small bound.",2. Related Work,0,[0]
"(Barron, 1993) showed that feedforward networks with one layer of sigmoidal nonlinearities achieve an integrated squared error with order of O(1/n), where n is the number of neurons.
",2. Related Work,0,[0]
"More recently, several interesting results were published on the approximation capabilities of deep neural networks.",2. Related Work,0,[0]
"(Delalleau & Bengio, 2011) have shown that there exist certain functions that can be approximated by threelayer neural networks with a polynomial amount of neurons, while two-layer neural networks require exponentially larger amount to achieve the same error.",2. Related Work,0,[0]
"(Montufar et al., 2014) and (Telgarsky, 2016) have shown the exponential increase of linear regions as neural networks grow deeper.",2. Related Work,0,[0]
"(Liang & Srikant, 2016) proved that with log(1/ ) layers, the neural network can achieve the error bound for any continuous function with O(polylog( )) parameters in each layer.
",2. Related Work,0,[0]
"LDR Matrices in Neural Networks: (Cheng et al., 2015) have analyzed the effectiveness of replacing conventional weight matrices in fully-connected layers with circulant matrices, which can reduce the time complexity from O(n2) to O(n log n), and the space complexity from O(n2) to O(n), respectively.",2. Related Work,0,[0]
"(Sindhwani et al., 2015) have demonstrated significant benefits of using Toeplitz-like matrices to tackle the issue of large space and computation requirement for neural networks training and inference.",2. Related Work,0,[0]
Experiments show that the use of matrices with low displacement rank offers superior tradeoffs between accuracy and time/space complexity.,2. Related Work,0,[0]
"An n × n matrix M is called a structured matrix when it has a low displacement rank γ (Pan, 2001).",3.1. Matrix Displacement,0,[0]
"More precisely, with the proper choice of operator matrices A and B, if the Sylvester displacement
∇A,B(M) := AM−MB (1)
and the Stein displacement
∆A,B(M) := M− AMB (2)
of matrix M have a rank γ bounded by a value that is independent of the size of M, then matrix M is referred to as a matrix with a low displacement rank (Pan, 2001).",3.1. Matrix Displacement,0,[0]
In this paper we will call these matrices as LDR matrices.,3.1. Matrix Displacement,0,[0]
"Even a full-rank matrix may have small displacement rank with appropriate choice of displacement operators (A,B).
",3.1. Matrix Displacement,0,[0]
"Figure 1 illustrates a series of commonly used structured matrices, including a circulant matrix, a Cauchy matrix, a Toeplitz matrix, a Hankel matrix, and a Vandermonde matrix, and Table 1 summarizes their displacement ranks and corresponding displacement operators.
",3.1. Matrix Displacement,0,[0]
"The general procedure of handling LDR matrices generally takes three steps: Compression, Computation with Displacements, Decompression.",3.1. Matrix Displacement,0,[0]
"Here compression means to obtain a low-rank displacement of the matrices, and decompression means to converting the results from displacement computations to the answer to the original computational problem.",3.1. Matrix Displacement,0,[0]
"In particular, if one of the displacement operator has the property that its power equals the identity matrix, then one can use the following method to decompress directly: Lemma 3.1.",3.1. Matrix Displacement,0,[0]
"If A is an a-potent matrix (i.e., Aq = aI for some positive integer q ≤ n), then
M = [ q−1∑ k=0 Ak∆A,B(M)Bk ] (I− aBq)−1.",3.1. Matrix Displacement,0,[0]
"(3)
Proof.",3.1. Matrix Displacement,0,[0]
"See Corollary 4.3.7 in (Pan, 2001).
",3.1. Matrix Displacement,0,[0]
One of the most important characteristics of structured matrices is their low number of independent variables.,3.1. Matrix Displacement,0,[0]
"The number of independent parameters is O(n) for an n-byn structured matrix instead of the order of n2, which indicates that the storage complexity can be potentially reduced to O(n).",3.1. Matrix Displacement,0,[0]
"Besides, the computational complexity for many matrix operations, such as matrix-vector multiplication, matrix inversion, etc., can be significantly reduced when operating on the structured ones.",3.1. Matrix Displacement,0,[0]
"The definition and analysis of structured matrices have been generalized to the case of n-by-m matrices where m 6= n, e.g., the blockcirculant matrices (Pan et al., 2015).",3.1. Matrix Displacement,0,[0]
Our application of LDR matrices to neural networks would be the general nby-m weight matrices.,3.1. Matrix Displacement,0,[0]
"For certain lemmas and theorems such as Lemma 3.1, only the form on n × n square matrices is needed for the derivation procedure in this paper.
",3.1. Matrix Displacement,0,[0]
So we omit the generalized form of such statements unless necessary.,3.1. Matrix Displacement,0,[0]
In this paper we study the viability of applying LDR matrices in neural networks.,3.2. LDR Neural Networks,0,[0]
"Without loss of generality, we focus on a feed-forward neural network with one fully-connected (hidden) layer, which is similar network setup as (Cybenko, 1989).",3.2. LDR Neural Networks,0,[0]
Here the input layer (with n neurons) and the hidden layer (with kn neurons)1 are assumed to be fully connected with a weight matrix W ∈,3.2. LDR Neural Networks,0,[0]
"Rn×kn of displacement rank at most r corresponding to displacement operators (A,B), where r n.",3.2. LDR Neural Networks,0,[0]
The domain for the input vector x is the ndimensional hypercube In :=,3.2. LDR Neural Networks,0,[0]
"[0, 1]n, and the output layer only contains one neuron.",3.2. LDR Neural Networks,0,[0]
"The neural network can be expressed as:
y = GW,θ(x) =",3.2. LDR Neural Networks,0,[0]
kn∑ j=1 αjσ(wjT,3.2. LDR Neural Networks,0,[0]
x + θj).,3.2. LDR Neural Networks,0,[0]
"(4)
Here σ(·) is the activation function, wj ∈ Rn denotes the jth column of the weight matrix W, and αj , θj ∈ R for j = 1, ..., kn.",3.2. LDR Neural Networks,0,[0]
"When the weight matrix W = [w1|w2| · · · |wkn] has a low-rank displacement, we call it an LDR neural network.",3.2. LDR Neural Networks,0,[0]
Matrix displacement techniques ensure that LDR neural network has much lower space requirement and higher computational speed comparing to classical neural networks of the similar size.,3.2. LDR Neural Networks,0,[0]
"In this paper, we aim at providing theoretical support on the accuracy of function approximation using LDR neural networks, which represents the “effectiveness” of LDR neural networks compared with the original neural networks.",3.3. Problem Statement,0,[0]
"Given a continuous function f(x) defined on [0, 1]n, we study the following tasks:
• For any > 0, find an LDR weight matrix W so that the function defined by equation (4) satisfies
max x∈[0,1]n
|f(x)−GW,θ(x)| < .",3.3. Problem Statement,0,[0]
"(5)
• Fix a positive integer n, find an upper bound so that for any continuous function f(x) there exists a bias vector θ and an LDR matrix with at most n rows satisfying equation (5).
",3.3. Problem Statement,0,[0]
"• Find a multi-layer LDR neural network that achieves error bound (5) but with fewer parameters.
",3.3. Problem Statement,0,[0]
"1Please note that this assumption does not sacrifice any generality because the n-by-m case can be transformed to n-by-kn format with the nearest k using zero padding (Cheng et al., 2015).
",3.3. Problem Statement,0,[0]
"The first task is handled in Section 4, which is the universal approximation property of LDR neural networks.",3.3. Problem Statement,0,[0]
It states that the LDR neural networks could approximate an arbitrary continuous function arbitrarily well and is the underpinning of the widespread applications.,3.3. Problem Statement,0,[0]
The error bounds for shallow and deep neural networks are derived in Section 5.,3.3. Problem Statement,0,[0]
"In addition, we derived explicit back-propagation expressions for LDR neural networks in Section 6.",3.3. Problem Statement,0,[0]
We call a family of matrices S to have representation property if for any vector v ∈,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn, there exists a matrix M ∈ SA,B such that v is a column of M .",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Note that all five types of LDR matrices shown in Fig. 1 have this representation property because of their explicit pattern.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
In this section we will prove that this property also holds for many other LDR families.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Based on this result, we are able to prove the universal approximation property of neural networks utilizing only LDR matrices.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Theorem 4.1.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Let A, B be two n× n non-singular diagonalizable matrices.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Define SrA,B as the set of matrices M such that ∆A,B(M) has rank at most r.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Then the representation property holds for SrA,B ifA andB satisfy
i)",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Aq = aI for some positive integer q ≤ n,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"and a scalar a 6= 0; ii) (I− aBq) is nonsingular; iii) the eigenvalues of B have distinguishable absolute values.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Proof.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"It suffices to prove for the case r = 1, as increasing r only provides more candidate matrices to choose from.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"By the property of Stein displacement, any matrix M ∈ S can be expressed in terms of A, B, and its displacement as follows:
M = q−1∑ k=0 Ak∆A,B(M)Bk(I− aBq)−1.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(6)
Next we express ∆A,B(M) as a product of two vectors g · hT",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
since it has rank 1.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Also write A = Q−1ΛQ, where Λ = diag(λ1, ..., λn) is a diagonal matrix generated by the eigenvalues of A. Now define ej to be the j-th unit column vector for j = 1, ..., n. Write
QMej =Q q−1∑ k=0 Ak∆A,B(M)B",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"k(I− aBq)−1ej
=Q q−1∑ k=0",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(Q−1ΛQ)kghTBk(I− aBq)−1ej
= ( q−1∑ k=0 sh,jΛ k ) Qg.
(7)
Here we use sh,j to denote the resulting scalar from matrix product hTBk(I− aBq)−1ej for k = 1, ..., n. Define T := (I − aBq)−1.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In order to prove the theorem, we need to show that there exists a vector h and an index k such that the matrix ∑q−1",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
k=0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"sh,jΛ
k is nonsingular.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In order to distinguish scalar multiplication from matrix multiplication, we use notation a ◦M to denote the multiplication of a scalar value and a matrices whenever necessary.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rewrite the expression as
q−1∑ k=0 sh,jΛ k
= q−1∑ k=0 hT · ( BkTej ◦",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"diag(λk1 , ..., λkn) )",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
#NAME?,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"diag(hT ·Bk ·T · [λk1ej | · · · |λknej ])
=diag ( hT · ( q−1∑ k=0 BkTλk1ej ) , ...,hT · ( q−1∑ k=0 BkTλknej )) .
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
The diagonal matrix ∑q−1,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
k=0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"sh,jΛ
k is nonsingular if and only if all of its diagonal entries are nonzero.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Let bij denote the column vector ∑q−1 k=0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"BT
kλki ej .",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Unless for every j there is an index ij such that bijj = 0, we can always choose an appropriate vector h so that the resulting diagonal matrix is nonsingular.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Next we will show that the former case is not possible using proof by contradiction.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Assume that there is a column bijj = 0 for every j = 1, 2, · · · , n, we must have:
0",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"=[bi11|bi22| · · · |binn]
=",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
[ q−1∑ k=0 BkTλki1e1| · · · | q−1∑ k=0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"BkTλkinen ] =
q−1∑ k=0 BkT · diag(λki1 , ..., λ k in).
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Since B is diagonalizable, we write B = P−1ΠP, where Π = diag(η1, ..., ηn).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Also we have T = (I− aBq)−1 = P−1(I− aΠq)−1P. Then
0 = q−1∑ k=0 BTkdiag(λki1 , ..., λ k in)
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"= P−1 [ q−1∑ k=0 Πk(I− aΠq)−1diag(λki1 , ..., λ k in) ]",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"P
= P−1 q−1∑ k=0 diag ( (λi1η1) k, ..., (λinηn) k ) (I− aΠq)−1P
= P−1diag ( q−1∑ k=0 (λi1η1) k, ..., q−1∑ k=0 (λinηn) k ) (I− aΠq)−1P.
This implies that λi1η1, ..., λinηn are solutions to the equation
1 + x+",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
x2 + · ·,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
·+ xq−1 = 0.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(8)
By assumption of matrix B, η1, ..., ηk have different absolute values, and so are λi1η1, ..., λi1η1, since all λk have the same absolute value because Aq = aI.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"This fact suggests that there are q distinguished solutions of equation (8), which contradicts the fundamental theorem of algebra.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Thus it is incorrect to assume that matrix ∑q−1,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
k=0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"sh,jΛ
k is singular for all h ∈ Rn.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"With this property proven, given any vector v ∈",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn, one can take the following procedure to find a matrix M ∈ S and a index j such that the j-th column of M equals v: i) Find a vector h and a index j such that matrix∑q−1 k=0",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"sh,jΛ k is non-singular;
ii)",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"By equation (7), find
g :=Q−1 ( q−1∑ k=0 sh,jΛ k )−1",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"QTv;
iii) Construct M ∈ S with g and h by equation (6).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Then its j-th column will equal to v.
With the above construction, we have shown that for any vector v ∈",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn one can find a matrix M ∈ S and a index j such that the j-th column of M equals v, thus the theorem is proved.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Our main goal of this section is to show that neural networks with many types of LDR matrices (LDR neural networks) can approximate continuous functions arbitrarily well.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In particular, we are going to show that Toeplitz matrices and circulant matrices, as specific cases of LDR matrices, have the same property.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In order to do so, we need to introduce the following definition of a discriminatory function and state one of its key property as Lemma 4.1.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Definition 4.1.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"A function σ(u) : R → R is called as discriminatory if the zero measure is the only measure µ that satisfies the following property:∫
In σ(wTx + θ)dµ(x) =",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"0,∀w ∈",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn, θ ∈ R. (9)
Lemma 4.1.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"[cf. (Cybenko, 1989)]",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Any bounded, measurable sigmoidal function is discriminatory.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Now we are ready to present the universal approximation theorem of LDR neural networks with n-by-kn weight matrix,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"W:
Theorem 4.2 (Universal Approximation Theorem for LDR Neural Networks).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Let σ be any continuous discriminatory function and SrA,B be a family of LDR matrices having representation property.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Then for any continuous function
f(x) defined on In and any > 0, there exists a function G(x) in the form of equation (4) so that its weight matrix consists of k submatrices from SrA,B and
max x∈In
|G(x)− f(x)| < .",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(10)
Proof.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Denote the i-th n×n submatrix of W as Wi.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Then W can be written as
W = [ W1|W2|...|Wk ] .",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(11)
Let SIn denote the set of all continuous functions defined on In.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Let UIn be the linear subspace of SIn that can be expressed in form of equation (4) where W consists of k sub-matrices with displacement rank at most r. We want to show that UIn is dense in the set of all continuous functions SIn .
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Suppose not, by Hahn-Banach Theorem, there exists a bounded linear functional L 6= 0",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
such that L(Ū(In)),4. The Universal Approximation Property of LDR Neural Networks,0,[0]
0,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Moreover, By Riesz Representation Theorem, L can be written as
L(h) = ∫",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In h(x)dµ(x),∀h ∈ S(In),
for some measure µ.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Next we show that for any y ∈,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn and θ ∈ R, the function σ(yTx",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"+ θ) belongs to the set UIn , and thus we must have∫
In σ(yTx + θ)dµ(x) = 0.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(12)
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
For any vector y ∈,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Rn, Theorem 4.1 guarantees that there exists an n×n LDR matrix M =",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"[b1| · · · |bn] and an index j such that bj = y. Now define a vector (α1, ..., αn) such that αj = 1 and α1 = · · · = αn = 0.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Also let the value of all bias be θ.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Then the LDR neural network function becomes
G(x) = n∑ i=1",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
αiσ(b T,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"i x + θ)
=αjσ(b T j x",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"+ θ) = σ(y Tx + θ).
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"(13)
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
From the fact that L(G(x)),4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"= 0, we derive that
0 =L(G(x))
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
= ∫,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
In n∑ i=1,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
αiσ(b T,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
i x + θ) = ∫,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"In σ(yTx + θ)dµ(x).
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Since σ(t) is a discriminatory function by Lemma 4.1.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
We can conclude that µ is the zero measure.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"As a result, the function defined as an integral with measure µmust be zero for any input function h ∈ S(In).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"The last statement contradicts the property that L 6= 0 from the Hahn-Banach
Theorem, which is obtained based on the assumption that the set UIn of LDR neural network functions are not dense in SIn .",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"As this assumption is not true, we have the universal approximation property of LDR neural networks.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Reference work (Cheng et al., 2015), (Sindhwani et al., 2015) have utilized a circulant matrix or a Toeplitz matrix for weight representation in deep neural networks.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Please note that for the general case of n-by-m weight matrices, either the more general Block-circulant matrices should be utilized or padding extra columns or rows of zeroes are needed (Cheng et al., 2015).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Circulant matrices and Topelitz matrices are both special form of LDR matrices, and thus we could apply the above universal approximation property of LDR neural networks and provide theoretical support for the use of circulant and Toeplitz matrices in (Cheng et al., 2015), (Sindhwani et al., 2015).",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Moreover, it is possible to consolidate the choice of parameters so that a block-Toeplitz matrix also shows Toeplitz structure globally.",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"Therefore we arrive at the following corollary.
",4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Corollary 4.1.,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
Any continuous function can be arbitrarily approximated by neural networks constructed with Toeplitz matrices or circulant matrices (with padding or using Block-circulant matrices).,4. The Universal Approximation Property of LDR Neural Networks,0,[0]
"With the universal approximation property proved, naturally we seek ways to provide error bound estimates for LDR neural networks.",5. Error Bounds on LDR Neural Networks,0,[0]
"We are able to prove that for LDR matrices defined by O(n) parameters (n represents the number of rows and has the same order as the number of columns), the corresponding structured neural network is capable of achieving integrated squared error of order O(1/n), where n is the number of parameters.",5. Error Bounds on LDR Neural Networks,0,[0]
"This result is asymptotically equivalent to Barron’s aforementioned result on general neural networks, indicating that there is essentially no loss for restricting to LDR matrices.
",5. Error Bounds on LDR Neural Networks,0,[0]
The functions we would like to approximate are those who are defined on a n-dimensional ball Br = {x ∈,5. Error Bounds on LDR Neural Networks,0,[0]
Rn : |x| ≤,5. Error Bounds on LDR Neural Networks,0,[0]
r} such that ∫ Br |x||f(x)|µ(dx) ≤,5. Error Bounds on LDR Neural Networks,0,[0]
"C, where µ is an arbitrary measure normalized so that µ(Br) = 1.",5. Error Bounds on LDR Neural Networks,0,[0]
"Let’s call this set ΓC,Br .",5. Error Bounds on LDR Neural Networks,0,[0]
"(Barron, 1993) considered the following set of bounded multiples of a sigmoidal function composed with linear functions:
Gσ = {ασ(yTx + θ) : |α| ≤ 2C,",5. Error Bounds on LDR Neural Networks,0,[0]
"y ∈ Rn, θ ∈ R}.",5. Error Bounds on LDR Neural Networks,0,[0]
"(14)
He proved the following theorem:
Theorem 5.1 ((Barron, 1993)).",5. Error Bounds on LDR Neural Networks,0,[0]
"For every function in ΓC,Br , every sigmoidal function σ, every probability measure, and every k ≥ 1, there exists a linear combination of
sigmoidal functions fk(x) of the form
fk(x) = k∑ j=1 αjσ(y T j x + θj), (15)
such that ∫ Br (f(x)− fk(x))2µ(dx) ≤",5. Error Bounds on LDR Neural Networks,1,"['The kernel k satisfies k(x, x′) ≤ 1 for all (x, x′), and k(x, x) = 1 for all x ∈ D; Given the stationarity assumption, the assumptions k(x, x′) ≤ 1 and k(x, x) = 1 are without loss of generality, as one can always re-scale the function and adjust the noise variance σ2 accordingly.']"
4r2C k .,5. Error Bounds on LDR Neural Networks,0,[0]
"(16)
",5. Error Bounds on LDR Neural Networks,0,[0]
Here yj ∈,5. Error Bounds on LDR Neural Networks,0,[0]
"Rn and θj ∈ R for every j = 1, 2, ..., N , Moreover, the coefficients of the linear combination may be restricted to satisfy ∑k j=1 |cj | ≤",5. Error Bounds on LDR Neural Networks,0,[0]
"2rC.
",5. Error Bounds on LDR Neural Networks,0,[0]
Now we will show how to obtain a similar result for LDR matrices.,5. Error Bounds on LDR Neural Networks,0,[0]
"Fix operator (A,B) and define
Sknσ = { kn∑ j=1 αjσ(y T j x + θj) : |αj | ≤ 2C,yj ∈ Rn,
θj ∈ R, j = 1, 2, ..., N, and [y(i−1)n+1|y(i−1)n+2| · · · |yin]
is an LDR matrix, ∀i = 1, ..., k } .
",5. Error Bounds on LDR Neural Networks,0,[0]
"(17) Moreover, let Gkσ be the set of function that can be expressed as a sum of no more than k terms from Gσ .",5. Error Bounds on LDR Neural Networks,0,[0]
"Define the metric ||f−g||µ = √∫ Br
(f(x)− g(x))2µ(dx).",5. Error Bounds on LDR Neural Networks,0,[0]
"Theorem 5.1 essentially states that the minimal distance between a function f ∈ ΓC,B and Gmσ is asymptotically O(1/n).",5. Error Bounds on LDR Neural Networks,0,[0]
"The following lemma proves that Gkσ is in fact contained in Sknσ .
",5. Error Bounds on LDR Neural Networks,0,[0]
Lemma 5.1.,5. Error Bounds on LDR Neural Networks,0,[0]
"For any k ≥ 1, Gkσ ⊂ Sknσ .
",5. Error Bounds on LDR Neural Networks,0,[0]
Proof.,5. Error Bounds on LDR Neural Networks,0,[0]
"Any function fk(x) ∈ Gkσ can be written in the form
fk(x) = k∑ j=1 αjσ(y T j x + θj).",5. Error Bounds on LDR Neural Networks,0,[0]
"(18)
",5. Error Bounds on LDR Neural Networks,0,[0]
"For each j = 1, ..., k, define a n× n LDR matrix Wj such that one of its column is yj .",5. Error Bounds on LDR Neural Networks,0,[0]
Let tij be the i-th column of Wj .,5. Error Bounds on LDR Neural Networks,0,[0]
"Let ij correspond to the column index such that tij = yj for all j. Now consider the following function
G(x) := k∑ j=1 n∑ i=1 βijσ(t",5. Error Bounds on LDR Neural Networks,0,[0]
"T ijx + θj), (19)
where βijj equals αj , and βij",5. Error Bounds on LDR Neural Networks,0,[0]
0,5. Error Bounds on LDR Neural Networks,0,[0]
if i 6= ij .,5. Error Bounds on LDR Neural Networks,0,[0]
"Notice that we
have the following equality
G(x) := k∑ j=1 n∑ i=1 βijσ(t",5. Error Bounds on LDR Neural Networks,0,[0]
"T ijx + θj)
= k∑ j=1 βijjσ(t T ijx + θj)
",5. Error Bounds on LDR Neural Networks,0,[0]
"= k∑ j=1 αjσ(y T j x + θj) = fk(x).
",5. Error Bounds on LDR Neural Networks,0,[0]
Notice that the matrix W =,5. Error Bounds on LDR Neural Networks,0,[0]
[W1|W2| · · · |Wk] consists k LDR submatrices.,5. Error Bounds on LDR Neural Networks,0,[0]
"Thus fk(x) belongs to the set Sknσ .
",5. Error Bounds on LDR Neural Networks,0,[0]
"By Lemma 5.1, we can replace Gkσ with S kn σ in Theorem 5.1 and obtain the following error bound estimates on LDR neural networks: Theorem 5.2.",5. Error Bounds on LDR Neural Networks,0,[0]
"For every disk Br ⊂ Rn, every function in ΓC,Br , every sigmoidal function σ, every normalized measure µ, and every k ≥ 1, there exists neural network defined by a weight matrix consists of k LDR submatrices such that∫
Br
(f(x)− fkn(x))2µ(dx) ≤",5. Error Bounds on LDR Neural Networks,0,[0]
"4r2C
k .",5. Error Bounds on LDR Neural Networks,0,[0]
"(20)
",5. Error Bounds on LDR Neural Networks,0,[0]
"Moreover, the coefficients of the linear combination may be restricted to satisfy ∑N k=1 |ck| ≤ 2rC.
Theorem 5.2 is the first theoretical result that gives a general error bound on LDR neural networks.",5. Error Bounds on LDR Neural Networks,0,[0]
"Empirically, (Cheng et al., 2015) reported that circulant neural networks are capable of achieving the same level of accuracy as AlexNet with more than 4,000X space saving on fully-connected layers.",5. Error Bounds on LDR Neural Networks,0,[0]
"(Sindhwani et al., 2015) applied Toeplitz-type LDR matrices to several benchmark image classification datasets, retaining the performance of stateof-the-art models with very high compression ratio.
",5. Error Bounds on LDR Neural Networks,0,[0]
"The next theorem naturally extended the result from (Liang & Srikant, 2016) to LDR neural networks, indicating that LDR neural networks can also benefit a parameter reduction if one uses more than one layers.",5. Error Bounds on LDR Neural Networks,0,[0]
"More precisely, we have the following statement: Theorem 5.3.",5. Error Bounds on LDR Neural Networks,0,[0]
"Let f be a continuous function on [0, 1] and is 2n + 1 times differentiable in (0, 1) for n = dlog 1 + 1]e.",5. Error Bounds on LDR Neural Networks,0,[0]
"If |f (k)(x)| ≤ k! holds for all x ∈ (0, 1) and k ∈",5. Error Bounds on LDR Neural Networks,0,[0]
"[ 2n+ 1 ] , then for any n× n matrices A and B satisfying the conditions of Theorem 4.1, there exists a LDR neural network GA,B(x) with O(log 1 ) layers, O(log 2 1 ) binary step units, O(log3 1 ) rectifier linear units such that
max x∈[0,1]
|f(x)−GA,B(x)| < .
",5. Error Bounds on LDR Neural Networks,0,[0]
Proof.,5. Error Bounds on LDR Neural Networks,0,[0]
"The theorem with better bounds and without assumption of being LDR neural network is proved in (Liang
& Srikant, 2016) as Theorem 4.",5. Error Bounds on LDR Neural Networks,0,[0]
"For each binary step unit or rectifier linear unit in the construction of the general neural network, attach (n − 1) dummy units, and expand the weights associated to this unit from a vector to an LDR matrix based on Theorem 4.1.",5. Error Bounds on LDR Neural Networks,0,[0]
"By doing so we need to expand the number units by a factor of order log 1 , and the asymptotic bounds are relaxed accordingly.",5. Error Bounds on LDR Neural Networks,0,[0]
"In this section, we reformulate the gradient computation of LDR neural networks.",6. Training LDR Neural Networks,0,[0]
"The computation for propagating through a fully-connected layer can be written as
y = σ(WTx + θ), (21)
where σ(·) is the activation function, W ∈",6. Training LDR Neural Networks,0,[0]
"Rn×kn is the weight matrix, x ∈",6. Training LDR Neural Networks,0,[0]
Rn is input vector and θ ∈ Rkn is bias vector.,6. Training LDR Neural Networks,0,[0]
"According to Equation (7), if Wi is an LDR matrix with operators (Ai,Bi) satisfying conditions of Theorem 4.1, then it is essentially determined by two matrices Gi ∈",6. Training LDR Neural Networks,0,[0]
"Rn×r,Hi ∈ Rn×r as
Wi =",6. Training LDR Neural Networks,0,[0]
[ q−1∑ k=0 AkiGiH T,6. Training LDR Neural Networks,0,[0]
i B k,6. Training LDR Neural Networks,0,[0]
i ] (I− aBqi ) −1.,6. Training LDR Neural Networks,0,[0]
"(22)
To fit the back-propagation algorithm, our goal is to compute derivatives ∂O∂Gi , ∂O ∂Hi
and ∂O∂x for any objective function O = O(W1, . . .",6. Training LDR Neural Networks,0,[0]
",Wk).
",6. Training LDR Neural Networks,0,[0]
"In general, given that a := WTx + θ, we can have:
∂O
∂W = x(
∂O ∂a )",6. Training LDR Neural Networks,0,[0]
"T , ∂O ∂x = W ∂O ∂a",6. Training LDR Neural Networks,0,[0]
", ∂O ∂θ = ∂O ∂a 1. (23)
where 1 is a column vector full of ones.",6. Training LDR Neural Networks,0,[0]
"Let Ĝik := AkiGi, Ĥik := H T",6. Training LDR Neural Networks,0,[0]
i B k,6. Training LDR Neural Networks,0,[0]
"i (I − aB q i ) −1, and Wik := ĜikĤik.",6. Training LDR Neural Networks,0,[0]
"The derivatives of ∂O∂Wik can be computed as following:
∂O
∂Wik =
∂O
∂Wi .",6. Training LDR Neural Networks,0,[0]
"(24)
",6. Training LDR Neural Networks,0,[0]
"According to Equation (23), if we let a = Wik, W = ĜTik and x = Ĥik, then ∂O∂Ĝik and ∂O ∂Ĥik can be derived as:
∂O ∂Ĝik =",6. Training LDR Neural Networks,0,[0]
[ ∂O ∂ĜTik ]T = [ Ĥik ∂O ∂Wik,6. Training LDR Neural Networks,0,[0]
"]T = ( ∂O ∂Wik )T ĤTik,
(25)
∂O
∂Ĥik = ĜTik
∂O
∂Wik .",6. Training LDR Neural Networks,0,[0]
"(26)
Similarly, let a = Ĝik, W = (Aki ) T and x = Gi, then
∂O ∂Gi can be derived as:
∂O
∂Gi = q−1∑ k=0 (Aki ) T ( ∂O ∂Ĝik )
= q−1∑ k=0",6. Training LDR Neural Networks,0,[0]
"(Aki ) T ( ∂O ∂Wik )T ĤTik.
(27)
Substituting with a = Ĥik, W = HTi and x = B k",6. Training LDR Neural Networks,0,[0]
"i (I − aBqi ) −1, we have ∂O∂Hi derived as:
∂O
∂Hi = q−1∑ k=0",6. Training LDR Neural Networks,0,[0]
Bki (I− aB q i ) −1,6. Training LDR Neural Networks,0,[0]
"( ∂O ∂Ĥik )T
= q−1∑ k=0",6. Training LDR Neural Networks,0,[0]
Bki (I− aB q i ) −1,6. Training LDR Neural Networks,0,[0]
"( ∂O ∂Wik )T Ĝik.
(28)
",6. Training LDR Neural Networks,0,[0]
"In this way, derivatives ∂O∂Gi and ∂O ∂Hi can be computed given ∂O∂Wik which is equal to ∂O ∂Wi
.",6. Training LDR Neural Networks,0,[0]
The essence of backpropagation algorithm is to propagate gradients backward from the layer with objective function to the input layer.,6. Training LDR Neural Networks,0,[0]
∂O,6. Training LDR Neural Networks,0,[0]
"∂Wi
can be calculated from previous layer and ∂O∂x will be propagated to the next layer if necessary.
",6. Training LDR Neural Networks,0,[0]
"For practical use one may want to choose matrices Ai and Bi with fast multiplication method such as diagonal matrices, permutation matrices, banded matrices, etc.",6. Training LDR Neural Networks,0,[0]
Then the space complexity (the number of parameters for storage) of Wi can be O(2n + 2nr) rather than O(n2) of traditional dense matrix.,6. Training LDR Neural Networks,0,[0]
The 2n is for Ai and Bi and 2nr is for Gi and Hi.,6. Training LDR Neural Networks,0,[0]
The time complexity of WTi x will be O(q(3n + 2nr)) compared with O(n2) of dense matrix.,6. Training LDR Neural Networks,0,[0]
"Particularly, when Wi is a structured matrix like the Toeplitz matrix, the space complexity will be O(2n).",6. Training LDR Neural Networks,0,[0]
This is because the Toeplitz matrix is defined by 2n parameters.,6. Training LDR Neural Networks,0,[0]
"Moreover, its matrix-vector multiplication can be accelerated by using Fast Fourier Transform (for Toeplitz and circulant matrices), resulting in time complexity O(n log n).",6. Training LDR Neural Networks,0,[0]
In this way the back-propagation computation for the layer can be done with near-linear time.,6. Training LDR Neural Networks,0,[0]
"In this paper, we have proven that the universal approximation property of LDR neural networks.",7. Conclusion,0,[0]
"In addition, we also theoretically show that the error bounds of LDR neural networks are at least as efficient as general unstructured neural network.",7. Conclusion,0,[0]
"Besides, we also develop the backpropagation based training algorithm for universal LDR neural networks.",7. Conclusion,0,[0]
Our study provides the theoretical foundation of the empirical success of LDR neural networks.,7. Conclusion,0,[0]
"Recently low displacement rank (LDR) matrices, or so-called structured matrices, have been proposed to compress large-scale neural networks.",abstractText,0,[0]
"Empirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in space and computational complexity while retaining high accuracy.",abstractText,0,[0]
We formally study LDR matrices in deep learning.,abstractText,0,[0]
"First, we prove the universal approximation property of LDR neural networks with a mild condition on the displacement operators.",abstractText,0,[0]
We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multiple-layer structure.,abstractText,0,[0]
"Finally, we propose back-propagation based training algorithm for general LDR neural networks.",abstractText,0,[0]
Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank,title,0,[0]
"Multi-armed bandit (MAB) (Berry & Fristedt, 1985; Sutton & Barto, 1998) is a classical online learning model typically described as a game between a learning agent (player) and the environment withm arms.",1. Introduction,0,[0]
"In each step, the environment generates an outcome, and the player uses a policy (or an algorithm), which takes the feedback from the previous steps as input, to select an arm to pull.",1. Introduction,0,[0]
"After pulling an arm, the player receives a reward based on the pulled arm and the environment outcome.",1. Introduction,0,[0]
"In this paper, we consider stochastic MAB problem, which means the environment outcome is drawn from an unknown distribution (Lai & Robbins, 1985), not generated by an adversary (Auer et al., 2002b).",1. Introduction,0,[0]
The goal of the player is to cumulate as much reward as possible over a total of T steps (T may be unknown).,1. Introduction,0,[0]
"The performance metric is the (expected) regret, which is the cumulative
1Tsinghua University, Beijing, China 2Microsoft Research, Beijing, China.",1. Introduction,0,[0]
"Correspondence to: Siwei Wang <wangsw15@mails.tsinghua.edu.cn>, Wei Chen <weic@microsoft.com>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
difference over T steps between always playing the arm with the optimal expected reward and playing the arms according to the policy.
",1. Introduction,0,[0]
"MAB models the key tradeoff between exploration — continuing exploring new arms not observed often, and exploitation — sticking to the best performed arm based on the observation so far.",1. Introduction,0,[0]
"A famous MAB algorithm is the upper confidence bound (UCB) policy (Gittins, 1989; Auer et al., 2002a), which achieves O(m log T/∆) distributiondependent regret, where ∆ is the minimum gap in the expected reward between an optimal arm and any non-optimal arm, and it matches the lower bound (Lai & Robbins, 1985).
",1. Introduction,0,[0]
"Combinatorial multi-armed bandit (CMAB) problem has recently become an active research area (Gai et al., 2012; Chen et al., 2016b; Gopalan et al., 2014a; Kveton et al., 2014; 2015a;b; Wen et al., 2015; Combes et al., 2015; Chen et al., 2016a; Wang & Chen, 2017).",1. Introduction,0,[0]
"In CMAB, the environment contains m base arms, but the player needs to pull a set of base arms S in each time slot, where S is called a super arm (or an action).",1. Introduction,0,[0]
The kind of reward and feedback varies in different settings.,1. Introduction,0,[0]
"In this paper, we consider the semi-bandit setting, where the feedback includes the outcomes of all base arms in the played super arm, and the reward is a function of S and the observed outcomes of arms in S. CMAB has found applications in many areas such as wireless networking, social networks, online advertising, etc.",1. Introduction,0,[0]
"Thus it is important to investigate different approaches to solve CMAB problems.
",1. Introduction,0,[0]
"An alternative approach different from UCB is the Thompson sampling (TS) approach, which is introduced much earlier by Thompson (1933), but the theoretical analysis of the TS policy comes much later — Kaufmann et al. (2012) and Agrawal & Goyal (2012) give the first regret bound for the TS policy, which essentially matches the UCB policy theoretically.",1. Introduction,0,[0]
"Moreover, TS policy often performs better than UCB in empirical simulation results, making TS an attractive policy for further studies.
TS policy follows the Bayesian inference framework to solve the MAB problems.",1. Introduction,0,[0]
"The unknown distribution of environment outcomes is parameterized, with an assumed prior distribution.",1. Introduction,0,[0]
"TS updates the prior distribution in each step with two phases: first it uses the prior distribution to sample a parameter, which is used to determine the action to
play in the current step; second it uses the feedback obtained in the current step to update the prior distribution to posterior distribution according to the Bayes’ rule.",1. Introduction,0,[0]
"To avoid confusion on these two kinds of random variables, in the rest of this paper, we use the word “sample” to denote the variable in the first phase, i.e. the random variable coming from the prior distribution.",1. Introduction,0,[0]
"The word “observation” represents the feedback random variable, which follows the unknown environment distribution.
",1. Introduction,0,[0]
"In this paper, we study the application of the Thompson sampling approach to CMAB.",1. Introduction,0,[0]
The reason that we are interested in this approach is that it has good performance in experiments.,1. Introduction,0,[0]
We found out that TS-based policy performs better than many kinds of UCB-based policy in experiments.,1. Introduction,0,[0]
"We also adjust the parameters of UCB-based policy to make it behave better, but those parameters do not have theoretical guarantees.",1. Introduction,0,[0]
"Thompson sampling policy behaves almost the same as UCB-based policies with no-guarantee parameters, and much better than those with parameters that have theoretical regret bounds.",1. Introduction,0,[0]
We can see those results in our experiments from Section 5.,1. Introduction,0,[0]
"Another interesting thing is that TS-based policy only require the reward function to be continuous, while UCB-based policy need it to be monotone as well.",1. Introduction,0,[0]
"These make TS-based policy more competitive in real applications.
",1. Introduction,0,[0]
"We consider a general CMAB case similar with (Chen et al., 2016b), i.e. we assume that (a) the problem instance satisfies a Lipschitz continuity assumption to handle non-linear reward functions, and (b) the player has access to an exact oracle for the offline optimization problem.",1. Introduction,0,[0]
"We use the standard TS policy together with the offline oracle, and refer it as the combinatorial Thompson sampling (CTS) algorithm.
CTS policy would first derive a set of parameters θ = (θ1, · · · , θm) as sample set for the base arms, and then select the optimal super arm under θ.",1. Introduction,0,[0]
"The original analysis for TS on MAB model then faces a challenge in addressing the dependency issue: it essentially requires that different super arms be related with independent samples so that when comparing them and selecting the optimal super arm, the actual optimal one is selected with high probability.",1. Introduction,0,[0]
"But when super arms are based on the same sample set θ, dependency and correlation among super arms may likely fail the above high probability analysis.
",1. Introduction,0,[0]
"One way to get around this is to independently derive a sample set θ(S) for every super arm S and compute its expected reward under θ(S), and then select the optimal super arm.",1. Introduction,0,[0]
"Obviously this solution incurs exponential sampling cost and is what we want to avoid when solving CMAB.
",1. Introduction,0,[0]
"To address the dependency challenge, we adapt an analysis of (Komiyama et al., 2015) for selecting the top-k arms to the general CMAB setting.",1. Introduction,0,[0]
"The adaptation is nontrivial
since we are dealing with arbitrary combinatorial constraints while they only deal with super arms containing k arms.
",1. Introduction,0,[0]
"We show that CTS achieves O( ∑ i∈[m] log T/∆i,min) +",1. Introduction,0,[0]
"O((2/ε)2k ∗ ) distribution-dependent regret bound for some small ε, where ∆i,min is the minimum gap between the optimal expected reward and any non-optimal expected reward containing arm i, and k∗ is the size of the optimal solution.",1. Introduction,0,[0]
"This is the first distribution-dependent regret bound for general CMAB using TS-based policy, and the result matches the theoretical performance of the UCB-based solution CUCB in (Chen et al., 2016b).",1. Introduction,0,[0]
"When considering CMAB with linear reward functions, the other complexity factors in the leading log T term also matches the regret lower bound for linear CMAB in (Kveton et al., 2015a).",1. Introduction,0,[0]
"For the exponential constant term, we show an example that it is unavoidable for Thompson sampling.
",1. Introduction,0,[0]
"Comparing to the UCB-based solution in (Chen et al., 2016b), the advantages of CTS is that: a) we do not need to assume that the expected reward is monotone to the mean outcomes of the base arms; b) it has better behaviour in experiments.",1. Introduction,0,[0]
CTS also suffers from some disadvantages.,1. Introduction,0,[0]
"For example, CTS policy can not adapt an approximation oracle as in (Chen et al., 2016b) (the regret becomes to approximate regret as well).",1. Introduction,0,[0]
"However, we claim that it is because of the difference between TS-based algorithm and UCB-based algorithm.",1. Introduction,0,[0]
"To show this, we provide a counter example for origin MAB problem, which cause an approximate regret of Θ(T ) when using TS policy.
",1. Introduction,0,[0]
Another disadvantage is that we need to assume that all the outcomes of all base arms are mutually independent.,1. Introduction,0,[0]
This is because TS policy maintains a prior distribution for every base arm’s mean value µi.,1. Introduction,0,[0]
"Only when the distributions are independent, we can use a simple method to update those prior distributions; otherwise the update method will be much more complicated for both the implmentation and the analysis.",1. Introduction,0,[0]
"This assumption is still reasonable, since many real applications satisfy this assumption.
",1. Introduction,0,[0]
"However, when applying on some further combinatorial structures, we do not need such an assumption, such as in matroid bandit.",1. Introduction,0,[0]
"Matroid bandit is a special class of CMAB (Kveton et al., 2014), in which the base arms are the elements in the ground set and the super arms are the independent sets of a matroid.",1. Introduction,0,[0]
The reward function is the sum of all outcomes of the base arms in the super arm.,1. Introduction,0,[0]
"We show that the regret of CTS is upper bounded by O( ∑ i 6∈S∗ log T/∆i) +O(m/ε
4) for some small ε, where S∗ is an optimal solution and ∆i is the minimum positive gap between the mean outcome of any arms in S∗ and the mean outcome of arm i.",1. Introduction,0,[0]
"This result does not need to assume that all arm distributions are independent, and do not have a constant term exponential with k∗.",1. Introduction,0,[0]
"It matches both the theoretical performance of the UCB-based algorithm
and the lower bound given in Kveton et al. (2014), and the constant term is similar with results in Agrawal & Goyal (2012), which appears in almost every TS analysis paper.
",1. Introduction,0,[0]
"We further conduct empirical simulations, and show that CTS performs much better than the CUCB algorithm of (Chen et al., 2016b) and C-KL-UCB algorithm based on KL-UCB of (Garivier & Cappé, 2011) on both matroid and non-matroid CMAB problem instances.
",1. Introduction,0,[0]
"In summary, our contributions include that: (a) we provide a novel analysis for the general CMAB problem, and provides the first distribution-dependent regret bound for general CMAB problems and matroid bandit problems based on Thompson sampling; (b) we show that approximation oracle can not be used in TS-based algorithms; and (c) we show that the exponential constant in our regret bound for general CMAB problems is unavoidable.
",1. Introduction,0,[0]
"Due to space constraint, complete proofs are moved to the supplementary material.",1. Introduction,0,[0]
"A number of related works on the general context of multiarmed bandit and Thompson sampling have been given, and we focus here on the most relevant studies related to CMAB.
",1.1. Related Work,0,[0]
"Our study follows the general CMAB framework of (Chen et al., 2016b), which provides a UCB-style algorithm CUCB and show a O( ∑ i∈[m] log T/∆i,min) regret bound.",1.1. Related Work,0,[0]
"Comparing with our CTS, both use an offline oracle, assume a Lipschitz continuity condition, and the bound essentially match asymptotically on time horizon T .",1.1. Related Work,0,[0]
"Their differences include: (a) CTS does not need the monotonicity property but CUCB requires that to use the offline oracle; (b) CUCB allows an approximation oracle (and uses approximate regret), but CTS requires an exact oracle; (c) the regret bound of CTS has some additional terms not related to T , which is common in TS-based regret bounds (See Section 3 for more details).",1.1. Related Work,0,[0]
"Combes et al. (2015) propose ESCB algorithm to solve CMAB with linear reward functions and independent arms, and their regret is a factor O( √ m) better than our corresponding regret bound for CTS.",1.1. Related Work,0,[0]
"However, their ESCB algorithm requires an exponential-time for computation, which is what we want to avoid when designing CMAB algorithms.
",1.1. Related Work,0,[0]
"Matroid bandit is defined and studied by Kveton et al. (2014), who provide a UCB-based algorithm with regret bound almost exactly matches CTS algorithm.",1.1. Related Work,0,[0]
"They also prove a matching lower bound using a partition matroid bandit.
",1.1. Related Work,0,[0]
Thompson sampling has also been applied to settings with combinatorial actions.,1.1. Related Work,0,[0]
"Gopalan et al. (2014b) study a general action space with a general feedback model, and provide
analytical regret bounds for the exact TS policy.",1.1. Related Work,0,[0]
"However, their general model cannot be applied to our case.",1.1. Related Work,0,[0]
"In particular, they assume that the arm outcome distribution is from a known parametric family and the prior distribution on the parameter space is finitely supported.",1.1. Related Work,0,[0]
"We instead work on arbitrary nonparametric and unknown distributions with bounded support, and even if we work on a parametric family, we allow the support of prior distributions to be infinite or continuous.",1.1. Related Work,0,[0]
"The reason is that in our CMAB setting, we only need to learn the means of base arms (same as in (Chen et al., 2016b)).",1.1. Related Work,0,[0]
"Moreover, their regret bounds are high probability bounds, not expected regret bounds, and their bounds contain a potentially very large constant, which will turn into a non-constant term when we convert them to expected regret bounds.
",1.1. Related Work,0,[0]
"In (Komiyama et al., 2015), the authors consider the TSbased policy for the top-k CMAB problem, a special case of matroid bandits where the super arms are subsets of size at most k.",1.1. Related Work,0,[0]
"Thus we generalize top-k bandits to matroid bandits, and our regret bound for matroid bandit still matches the one in (Komiyama et al., 2015).
",1.1. Related Work,0,[0]
Wen et al. (2015) analyze the regret of using TS policy for contextual CMAB problems.,1.1. Related Work,0,[0]
The key difference between their work and ours is that they use the Bayesian regret metric.,1.1. Related Work,0,[0]
"Bayesian regret takes another expectation on the prior distribution of parameters, while our regret bound works for any given parameter.",1.1. Related Work,0,[0]
"This leads to very different analytical method, and they cannot provide distributiondependent regret bounds.",1.1. Related Work,0,[0]
Russo & Van Roy (2016) also use Bayesian regret to analyze the regret bounds of TS policy for any kind of MAB problems.,1.1. Related Work,0,[0]
"Again, due to the use of Bayesian regret, their analytical method is very different and cannot be used for our purpose.",1.1. Related Work,0,[0]
"A CMAB problem instance is modeled as a tuple ([m], I, D,R,Q).",2.1. CMAB Problem Formulation,0,[0]
"[m] = {1, 2, · · · ,m} is the set of base arms; I ⊆ 2[m] is the set of super arms; D is a probability distribution in [0, 1]m, and is unknown to the player, R and Q are reward and feedback functions to be specified shortly.",2.1. CMAB Problem Formulation,0,[0]
"Let µ = (µ1, · · · , µm), where µi = EX∼D[Xi].",2.1. CMAB Problem Formulation,0,[0]
"At discrete time slot t ≥ 1, the player pulls a super arm S(t) ∈",2.1. CMAB Problem Formulation,0,[0]
"I, and the environment draws a random outcome vectorX(t) = {X1(t), · · · , Xm(t)} ∈",2.1. CMAB Problem Formulation,0,[0]
"[0, 1]m from D, independent of any other random variables.",2.1. CMAB Problem Formulation,0,[0]
"Then the player receives an unknown reward R(t) = R(S(t),X(t)), and observes the feedback Q(t) = Q(S(t),X(t)).",2.1. CMAB Problem Formulation,0,[0]
"As in (Chen et al., 2016b) and other papers studying CMAB, we consider semi-bandit feedback, that is, Q(t) =",2.1. CMAB Problem Formulation,0,[0]
"{(i,Xi(t))",2.1. CMAB Problem Formulation,0,[0]
| i ∈ S(t)}.,2.1. CMAB Problem Formulation,0,[0]
"At time t, the previous step information is
Ft−1 = {(S(τ), Q(τ))",2.1. CMAB Problem Formulation,0,[0]
: 1 ≤ τ ≤ t,2.1. CMAB Problem Formulation,0,[0]
"− 1}, which is the input to the learning algorithm to select the action S(t).",2.1. CMAB Problem Formulation,0,[0]
"Similar to (Chen et al., 2016b), we make the following two assumptions.",2.1. CMAB Problem Formulation,0,[0]
"For a parameter vectorµ, we useµS to denote the projection of µ on S, where S is a subset of all the base arms.
",2.1. CMAB Problem Formulation,0,[0]
Assumption 1.,2.1. CMAB Problem Formulation,0,[0]
"The expected reward of a super arm S ∈ I only depends on the mean outcomes of base arms in S. That is, there exists a function r such that E[R(t)]",2.1. CMAB Problem Formulation,0,[0]
"= EX(t)∼D[R(S(t),X(t))]",2.1. CMAB Problem Formulation,0,[0]
"= r(S(t),µS(t)).
",2.1. CMAB Problem Formulation,0,[0]
"The second assumption is a Lipschitz-continuity assumption of function r to deal with non-linear reward functions (it is based on one-norm).
",2.1. CMAB Problem Formulation,0,[0]
Assumption 2.,2.1. CMAB Problem Formulation,0,[0]
"There exists a constant B, such that for every super arm S and every pair of mean vectors µ and µ′, |r(S,µ)− r(S,µ′)| ≤ B||µS",2.1. CMAB Problem Formulation,0,[0]
"− µ′S ||1.
",2.1. CMAB Problem Formulation,0,[0]
"The goal of the player is to minimize the total (expected) regret under time horizon T , as defined below:
Reg(T ) , E",2.1. CMAB Problem Formulation,0,[0]
"[ T∑ t=1 (r(S∗,µ)− r(S(t),µ))",2.1. CMAB Problem Formulation,0,[0]
"] ,
where S∗ ∈ argmaxS∈I r(S,µ) is a best super arm.",2.1. CMAB Problem Formulation,0,[0]
"In matroid bandit settings, ([m], I) is a matroid, which means that I has two properties:
•",2.2. Matroid Bandit,0,[0]
"If A ∈ I, then ∀A′ ⊆",2.2. Matroid Bandit,0,[0]
"A, A′ ∈",2.2. Matroid Bandit,0,[0]
"I;
•",2.2. Matroid Bandit,0,[0]
"If A1, A2 ∈",2.2. Matroid Bandit,0,[0]
"I, |A1| > |A2|, then there exists i ∈ A1",2.2. Matroid Bandit,0,[0]
\A2 such that A2 ∪ {i} ∈ I.,2.2. Matroid Bandit,0,[0]
"The reward function is R(S,x) = ∑ i∈S xi, and thus the
expected reward function is r(S,µ) = ∑ i∈S µi.",2.2. Matroid Bandit,0,[0]
We first consider the general CMAB setting.,3. Combinatorial Thompson Sampling,0,[0]
"For this setting, we assume that the player has an exact oracle Oracle(θ) that takes a vector of parameters θ = (θ1, . . .",3. Combinatorial Thompson Sampling,0,[0]
", θm) as input, and output a super arm S = argmaxS∈I r(S,θ).
",3. Combinatorial Thompson Sampling,0,[0]
The combinatorial Thompson sampling (CTS) algorithm is described in Algorithm 1.,3. Combinatorial Thompson Sampling,0,[0]
"Initially we set the prior distribution of the means of all base arms as the Beta distribution β(1, 1), which is the uniform distribution on [0, 1].",3. Combinatorial Thompson Sampling,0,[0]
"After we get observation Q(t), we update the prior of all base arms in S(t) using procedure Update (Algorithm 2): for each observation Xi(t), we generate a Bernoulli random variable Yi(t) (the value of Yi at time t) independently with mean Xi(t), and then we update the prior Beta distribution
Algorithm 1 CTS Algorithm for CMAB 1: For each arm i, let ai = bi = 1 2: for t = 1, 2, · · ·",3. Combinatorial Thompson Sampling,0,[0]
"do 3: For all arm i, draw a sample θi(t) from Beta distribu-
tion β(ai, bi); let θ(t) = (θ1(t), . .",3. Combinatorial Thompson Sampling,0,[0]
.,3. Combinatorial Thompson Sampling,0,[0]
", θm(t)) 4: Play action S(t) = Oracle(θ(t)), get the observation Q(t) = {(i,Xi(t))",3. Combinatorial Thompson Sampling,0,[0]
": i ∈ S(t)} 5: Update({(ai, bi) | i ∈ S(t)}, Q(t))",3. Combinatorial Thompson Sampling,0,[0]
"6: end for
Algorithm 2 Procedure Update 1: Input: {(ai, bi) | i ∈ S}, Q = {(i,Xi)",3. Combinatorial Thompson Sampling,0,[0]
"| i ∈ S} 2: Output: updated {(ai, bi) | i ∈ S} 3: for all (i,Xi) ∈ Q do 4: Yi ← 1 with probabilityXi, 0 with probability 1−Xi 5: ai ← ai + Yi; bi ← bi + 1− Yi 6: end for
of base arm i using Yi(t) as the new observation.",3. Combinatorial Thompson Sampling,0,[0]
It is easy to see that {Yi(t)}t’s are i.i.d.,3. Combinatorial Thompson Sampling,0,[0]
with the same mean µi as the samples {Xi(t)}t’s.,3. Combinatorial Thompson Sampling,0,[0]
"Let ai(t) and bi(t) denote the values of ai and bi at the beginning of time step t. Then, following the Bayes’ rule, the posterior distribution of parameter µi after observation Q(t) is β(ai(t) +Yi(t), bi(t) + 1−Yi(t)), which is what the Update procedure does for updating ai and bi.",3. Combinatorial Thompson Sampling,0,[0]
"When choosing a super arm, we simply draw independent samples from all base arms’ prior distributions, i.e. θi(t) ∼ β(ai(t), bi(t)), and then send the sample vector θ(t) = (θ1(t), . . .",3. Combinatorial Thompson Sampling,0,[0]
", θm(t))",3. Combinatorial Thompson Sampling,0,[0]
to the oracle.,3. Combinatorial Thompson Sampling,0,[0]
"We use the output from the oracle S(t) as the super arm to play.
",3. Combinatorial Thompson Sampling,0,[0]
"We also need a further assumption to tackle the problem:
Assumption 3.",3. Combinatorial Thompson Sampling,0,[0]
"D = D1×D2×· · ·×Dm, i.e., the outcomes of all base arms are mutually independent.
",3. Combinatorial Thompson Sampling,0,[0]
This assumption is not necessary in CUCB algorithms.,3. Combinatorial Thompson Sampling,0,[0]
"However, when using TS method, this assumption is needed.",3. Combinatorial Thompson Sampling,0,[0]
"This is because that we are using the Bayes’ Rule, thus we need the exact likelihood function (as we can see in (Gopalan et al., 2014b)).",3. Combinatorial Thompson Sampling,0,[0]
"Only when the distributions for all the base arms are independent, we can use the Update procedure (Algorithm 2) to update their mean vector’s prior distribution.",3. Combinatorial Thompson Sampling,0,[0]
"When the distributions are correlated, the update procedure will also be much more complicated.",3. Combinatorial Thompson Sampling,0,[0]
"Let OPT = argmaxS∈I r(S,µ) be the set of optimal super arms.",3.1. Regret Upper Bound,0,[0]
Let S∗ ∈ argminS∈OPT |S| is one of the optimal super arm with minimum size k∗.,3.1. Regret Upper Bound,0,[0]
"Then we can define ∆S = r(S∗,µ)",3.1. Regret Upper Bound,0,[0]
"− r(S,µ), and ∆max = maxS∈I ∆S .",3.1. Regret Upper Bound,0,[0]
"Kmax is the maximum size of super arms, i.e. Kmax = maxS∈I |S|.",3.1. Regret Upper Bound,0,[0]
Theorem 1.,3.1. Regret Upper Bound,0,[0]
"Under Assumptions 1, 2, and 3, for all D,
Algorithm 1 has regret upper bound m∑ i=1",3.1. Regret Upper Bound,0,[0]
max S:i∈S 8B2|S| log T ∆S − 2B(k∗2 + 2)ε + (,3.1. Regret Upper Bound,0,[0]
"mK2max ε2 + 3m)∆max
+α1 · (
8∆max ε2 ( 4 ε2 + 1)k ∗ log
k∗
ε2
) , (1)
for any ε such that ∀S,∆S > 2B(k∗2 + 2)ε, where B is the Lipschitz constant in Assumption 2, and α1 is a constant not dependent on the problem instance.
",3.1. Regret Upper Bound,0,[0]
"When ε is sufficiently small, the leading log T term in the regret bound is comparable with the regret bound for CUCB in (Chen et al., 2016b).",3.1. Regret Upper Bound,0,[0]
"The term related to ε is to handle continuous Beta prior — since we will never be able to sample a θ(k)i (t) to be exactly the true value µi, we need to consider the ε neighborhood of µi.",3.1. Regret Upper Bound,0,[0]
"This ε term is common in most Thompson sampling analysis.
",3.1. Regret Upper Bound,0,[0]
The constant term has an exponential dependency on k∗.,3.1. Regret Upper Bound,0,[0]
This is because we need all the k∗ base arms in the best super arm to have samples close to their means to make sure that it is the best super arm in sampling.,3.1. Regret Upper Bound,0,[0]
"In contrast, for the top-k MAB of (Komiyama et al., 2015), there is no such exponential dependency, because they only compare one base arm at a time (this can also be seen in our matroid Bandit analysis).",3.1. Regret Upper Bound,0,[0]
"When dealing with the general actions, the regret result of (Gopalan et al., 2014b) also contains an exponentially large constant term without a close form, which is likely to be much larger than ours.",3.1. Regret Upper Bound,0,[0]
"In Section 3.3, we show that this exponential constant is unavoidable for the general CTS.
We now provide the proof outline for Theorem 1.",3.1. Regret Upper Bound,0,[0]
"First we define the following four events:
• A(t) = {S(t) /∈",3.1. Regret Upper Bound,0,[0]
"OPT}
• B(t) = {∃i ∈ S(t), |µ̂i(t)− µi| > ε|S(t)|}
• C(t) = {||θS(t)(t)− µS(t)||1 > ∆S(t) B",3.1. Regret Upper Bound,0,[0]
"− (k ∗2 + 1)ε}
• D(t) = {∃i ∈ S(t), |θi(t)− µ̂i(t)| >",3.1. Regret Upper Bound,0,[0]
"√
2 log T Ni(t) }
Then the total regret can be written as:
T∑ t=1",3.1. Regret Upper Bound,0,[0]
E [ I[A(t)]×∆S(t) ],3.1. Regret Upper Bound,0,[0]
"≤
T∑ t=1",3.1. Regret Upper Bound,0,[0]
E [ I[B(t) ∧ A(t)]×∆S(t) ],3.1. Regret Upper Bound,0,[0]
"+
T∑ t=1 E [ I[¬B(t) ∧ C(t) ∧ D(t) ∧ A(t)]×∆S(t) ]
+ T∑ t=1",3.1. Regret Upper Bound,0,[0]
E [ I[¬B(t) ∧ C(t) ∧ ¬D(t) ∧ A(t)]×∆S(t) ],3.1. Regret Upper Bound,0,[0]
#NAME?,3.1. Regret Upper Bound,0,[0]
E [ I[¬C(t) ∧ A(t)]×∆S(t) ],3.1. Regret Upper Bound,0,[0]
"The first term can be bounded by Chernoff Bound, which is( mK2max ε2 +m ) ∆max.",3.1. Regret Upper Bound,0,[0]
"The second term can be bounded by some basic results of Beta distribution, the upper bound is 2m∆max.
",3.1. Regret Upper Bound,0,[0]
The third term is a little bit tricky.,3.1. Regret Upper Bound,0,[0]
"Notice that under C(t), we can use B||θS(t)(t) − µS(t)||1 as an approximation of ∆S(t).",3.1. Regret Upper Bound,0,[0]
"However, it is hard to bound ∑ i∈S(t) |θi(t)",3.1. Regret Upper Bound,0,[0]
− µi|.,3.1. Regret Upper Bound,0,[0]
"To deal with this, we say one base arm i ∈ S(t) is sufficiently learned if Ni(t) > Li(S(t))",3.1. Regret Upper Bound,0,[0]
= 2 log T/( ∆S(t) 2B|S(t)|,3.1. Regret Upper Bound,0,[0]
"−
k∗2+2 |S(t)| ε) 2.",3.1. Regret Upper Bound,0,[0]
When computing ∑ i∈S(t) |θi(t),3.1. Regret Upper Bound,0,[0]
"− µi|, we do not count all the sufficiently learned arms in.",3.1. Regret Upper Bound,0,[0]
"To compensate their contributions, we double all the insufficiently
learned arms’ contributions from √
2 log T Ni(t)
to 2 √
2 log T Ni(t) .",3.1. Regret Upper Bound,0,[0]
One can check that the sum of contributions from insufficiently learned arms is an upper bound for the regret ∆S(t) under ¬B(t)∧C(t)∧¬D(t).,3.1. Regret Upper Bound,0,[0]
"Thus, we can upper bound the total contribution of base arm i as: ∑ i 4B √ 2 log TLmaxi where Lmaxi = maxS:i∈S Li(S).
",3.1. Regret Upper Bound,0,[0]
The difficulty comes mainly from the last term.,3.1. Regret Upper Bound,0,[0]
"Although the θi(t)’s of all base arms are mutually independent, when it comes to super arms, the value r(S,θ(t)S)’s for different super arms S are not mutually independent, because super arms may overlap one another.",3.1. Regret Upper Bound,0,[0]
"For example, Lemma 1 in (Agrawal & Goyal, 2013) is not true for considering super arms because of the lack of independence.",3.1. Regret Upper Bound,0,[0]
"This means that we cannot simply use the technique of (Agrawal & Goyal, 2013).",3.1. Regret Upper Bound,0,[0]
"Dealing with the dependency issue for this case is the main novelty in our analysis, as we now explain.
",3.1. Regret Upper Bound,0,[0]
"Let θ = (θ1, · · · , θm) be a vector of parameters, Z ⊆",3.1. Regret Upper Bound,0,[0]
"[m] and Z 6= ∅ be some base arm set and Zc be the complement of Z. Recall that θZ is the sub-vector of θ projected onto Z, and we use notation (θ′Z ,θZc) to denote replacing θi’s with θ′i’s for i ∈ Z and keeping the values θi for i ∈",3.1. Regret Upper Bound,0,[0]
"Zc unchanged.
",3.1. Regret Upper Bound,0,[0]
"Given a subset Z ⊆ S∗, we consider the following property for θZc .",3.1. Regret Upper Bound,0,[0]
For any ||θ′Z,3.1. Regret Upper Bound,0,[0]
"− µZ ||∞ ≤ ε, let θ′ = (θ′Z ,θZc):
• Z ⊆ Oracle(θ′);
• Either Oracle(θ′) ∈ OPT or ||θ′Oracle(θ′)",3.1. Regret Upper Bound,0,[0]
"− µOracle(θ′)||1 > ∆Oracle(θ′) B − (k ∗2 + 1)ε.
",3.1. Regret Upper Bound,0,[0]
"The first one is to make sure that if we have normal samples in Z at time t, then arms in Z will be played and observed.
",3.1. Regret Upper Bound,0,[0]
"These observations would update the Beta distributions of these arms to be more accurate, such that it is easier next time that the samples from these arms are also within ε of their true value.",3.1. Regret Upper Bound,0,[0]
This fact would be used later in the quantitative regret analysis.,3.1. Regret Upper Bound,0,[0]
"The second one says that if the samples in Z are normal, then ¬C(t)∧A(t) can not happen (similar to the analysis in (Agrawal & Goyal, 2013) and (Komiyama et al., 2015)).",3.1. Regret Upper Bound,0,[0]
"As time going on, the probability that ¬C(t)∧A(t) happens will become smaller and smaller, thus the expectation on its sum has an constant upper bound.
",3.1. Regret Upper Bound,0,[0]
"We use EZ,1(θ) to denote the event that the vector θZc has such a property, and emphasize that this event only depends on the values in vector θZc .",3.1. Regret Upper Bound,0,[0]
"What we want to do is to find some exact Z such that EZ,1(θ) happens when ¬C(t)∧A(t) happens.",3.1. Regret Upper Bound,0,[0]
"The following lemma shows that such Z must exist, it is the key lemma in this section.
",3.1. Regret Upper Bound,0,[0]
Lemma 1.,3.1. Regret Upper Bound,0,[0]
"Suppose that ¬C(t) ∧ A(t) happens, then there exists Z ⊆ S∗ and Z 6= ∅",3.1. Regret Upper Bound,0,[0]
"such that EZ,1(θ(t)) holds.
",3.1. Regret Upper Bound,0,[0]
"By Lemma 1, for some nonempty Z, EZ,1(θ(t)) occurs when ¬C(t) ∧ A(t) happens.",3.1. Regret Upper Bound,0,[0]
Another fact is that ||θZ(t)− µZ,3.1. Regret Upper Bound,0,[0]
||∞ > ε.,3.1. Regret Upper Bound,0,[0]
"The reason is that if ||θZ(t)− µZ ||∞ ≤ ε, by definition of the property, either S(t) ∈ OPT or ||θS(t)(t)−",3.1. Regret Upper Bound,0,[0]
"µS(t)||1 > ∆S(t) B − (k
∗2 + 1)ε, which means ¬C(t)∧A(t) can not happen.",3.1. Regret Upper Bound,0,[0]
"Let EZ,2(θ) be the event {||θZ −µZ ||∞ > ε}.",3.1. Regret Upper Bound,0,[0]
Then {¬C(t) ∧ A(t)},3.1. Regret Upper Bound,0,[0]
"→ ∨Z⊆S∗,Z 6=∅(EZ,1(θ(t))",3.1. Regret Upper Bound,0,[0]
"∧ EZ,2(θ(t))).
",3.1. Regret Upper Bound,0,[0]
"Using similar techniques in (Komiyama et al., 2015) we can get the upper bound O ( 8 ε2 ( 4 ε2 ) |Z| log |Z|ε2 ) for∑T
t=1",3.1. Regret Upper Bound,0,[0]
E,3.1. Regret Upper Bound,0,[0]
"[I{EZ,1(θ(t)), EZ,2(θ(t))}].",3.1. Regret Upper Bound,0,[0]
"We consider using an approximation oracle in our CTS algorithm as well, like what the author did in (Chen et al., 2016b) or (Wen et al., 2015).",3.2. Approximation Oracle,0,[0]
"However, we found out that Thompson sampling does not work with an approximation oracle even in the original MAB model, as shown in Theorem 2.",3.2. Approximation Oracle,0,[0]
"Notice that here we do not consider the Bayesian regret, so it does not contradict with the results in (Wen et al., 2015).
",3.2. Approximation Oracle,0,[0]
"To make it clear, we need to show the definitions of approximation oracle and approximation regret here.
",3.2. Approximation Oracle,0,[0]
Definition 1.,3.2. Approximation Oracle,0,[0]
An approximation oracle with rate λ for MAB problem is a function Oracle :,3.2. Approximation Oracle,0,[0]
"[0, 1]m → {1, · · · ,m} such that µOracle(µ) ≥ λmaxi µi.",3.2. Approximation Oracle,0,[0]
Definition 2.,3.2. Approximation Oracle,0,[0]
"The approximation regret with rate λ of MAB problem on mean vector µ is defined as:
T∑ t=1",3.2. Approximation Oracle,0,[0]
"(λmax i µi − µi(t)),
where i(t) is the arm pulled on time step t.
The TS algorithm using approximation oracle Oralce works just as Algorithm 1.
Theorem 2.",3.2. Approximation Oracle,0,[0]
"There exists an MAB instance with an approximation oracle such that when using Algorithm 1, the regret is Ω(T ).
",3.2. Approximation Oracle,0,[0]
Proof Sketch.,3.2. Approximation Oracle,0,[0]
"Consider the following MAB instance:
Problem Instance 1. m = 3, µ = [0.9, 0.82, 0.7], approximate rate λ = 0.8.",3.2. Approximation Oracle,0,[0]
"The Oracle works as following: if µ3 ≥ λmaxi µi, Oracle(µ) = 3; else if µ2 ≥ λmaxi µi, Oracle(µ) = 2; otherwise Oracle(µ) = 1.
",3.2. Approximation Oracle,0,[0]
The key idea is that we may never play the best arm (arm 1 above) when using the approximation oracle.,3.2. Approximation Oracle,0,[0]
"When the sample from the prior distribution of the best arm is good, we choose an approximate arm (arm 2 above) but not the best arm; otherwise we choose an bad arm (arm 3 above) with positive approximation regret.",3.2. Approximation Oracle,0,[0]
Thus the expected regret of each time slot depends on whether the prior distribution of the best arm at the beginning is good or not.,3.2. Approximation Oracle,0,[0]
"Since the best arm is never observed, we never update its prior distribution.",3.2. Approximation Oracle,0,[0]
Thus the expected regret in each time slot can remain a positive constant forever.,3.2. Approximation Oracle,0,[0]
"Since every arm’s sample θi(t) is chosen independently, the worst case is that we need all the samples for base arms in the best super arm to be close to their true means to choose that super arm.",3.3. The Exponential Constant Term,0,[0]
"Under this case, the probability that we have no regret in each time slot is exponentially with k∗, thus we will have such a constant term.
",3.3. The Exponential Constant Term,0,[0]
Theorem 3.,3.3. The Exponential Constant Term,0,[0]
"There exists a CMAB instance such that the regret of Algorithm 1 on this instance is at least Ω(2k ∗ ).
",3.3. The Exponential Constant Term,0,[0]
Proof Sketch.,3.3. The Exponential Constant Term,0,[0]
"Consider the following CMAB instance:
Problem Instance 2. m = k∗+1, there are only two super arms in I, where S1 = {1, 2, · · · , k∗} and S2 = {k∗ + 1}.",3.3. The Exponential Constant Term,0,[0]
The mean vector µS1 =,3.3. The Exponential Constant Term,0,[0]
"[1, · · · , 1].",3.3. The Exponential Constant Term,0,[0]
"The reward function R follows R(S1,X) = ∏ i∈S1 Xi, and R(S2,X) = 1 − ∆, while ∆ = 0.5.",3.3. The Exponential Constant Term,0,[0]
"The distributions Di are all independent Bernoulli distributions with mean µi (since µi = 1, the observations are always 1).
",3.3. The Exponential Constant Term,0,[0]
"One can show that the expected time until Algorithm 1 plays the optimal super arm S1 for the first time is Ω(2k ∗ ).
",3.3. The Exponential Constant Term,0,[0]
The exponential term comes from the bad prior distribution at the beginning of the algorithm.,3.3. The Exponential Constant Term,0,[0]
"In fact, from the proof of Theorem 1 we know that if we can pull each base arm for Õ( 1ε2 ) times at the beginning and then use the CTS policy
whose prior distribution at the beginning is the posterior distribution after those observations, then we can reduce the exponential constant term to O(m 4 ).",3.3. The Exponential Constant Term,0,[0]
"However, since ε depends on ∆min, which is unknown to the player, we can not simply run each base arm for a few time steps to avoid the exponential constant regret term.",3.3. The Exponential Constant Term,0,[0]
"Perhaps an adaptive choice can be used here, and this is a further research item.",3.3. The Exponential Constant Term,0,[0]
"In matroid bandit, we suppose the oracle we use is the greedy one since the greedy algorithm gives back the exact best super arm.",4. Matroid Bandit Case,0,[0]
"Let S∗ ∈ argmaxS∈I r(S,µ) be one of the optimal super arm.",4.1. Regret Upper Bound,0,[0]
"Define ∆i = minj|j∈S∗,µj>µi µj",4.1. Regret Upper Bound,0,[0]
− µi.,4.1. Regret Upper Bound,0,[0]
If i /∈ S∗,4.1. Regret Upper Bound,0,[0]
"but {j | j ∈ S∗, µj >",4.1. Regret Upper Bound,0,[0]
"µi} = ∅, we define ∆i = ∞, so that 1∆i = 0.",4.1. Regret Upper Bound,0,[0]
"Let K = maxS∈I |S| = |S
∗|.",4.1. Regret Upper Bound,0,[0]
We have the following theorem for CTS algorithm under the matroid bandit case: Theorem 4.,4.1. Regret Upper Bound,0,[0]
"Under Assumptions 1 and 2, the regret upper bound of Algorithm 1 for a matroid bandit is:
Reg(T ) ≤ ∑ i/∈S∗ log T ∆i",4.1. Regret Upper Bound,0,[0]
− 2ε ∆i,4.1. Regret Upper Bound,0,[0]
− ε ∆i,4.1. Regret Upper Bound,0,[0]
− 2ε + α2 · (m ε4 ),4.1. Regret Upper Bound,0,[0]
#NAME?,4.1. Regret Upper Bound,0,[0]
"such that ∀i /∈ S∗,∆i",4.1. Regret Upper Bound,0,[0]
"− 2ε > 0, where α2 is a constant not dependent on the problem instance.
",4.1. Regret Upper Bound,0,[0]
Notice that we do not need the distributions of all the base arms to be independent due to the special structure of matroid.,4.1. Regret Upper Bound,0,[0]
"When ε is small, the leading log T term of the above regret bound matches the regret lower bound∑ i/∈S∗ 1",4.1. Regret Upper Bound,0,[0]
"∆i
log T given in (Kveton et al., 2014).",4.1. Regret Upper Bound,0,[0]
"For the constant term, we have anO( 1ε4 ) factor while Agrawal & Goyal (2013) have an O( 1ε2 ) factor in their theorem.",4.1. Regret Upper Bound,0,[0]
"However, even following their analysis, we can only obtainO( 1ε4 ) and cannot recover the O( 1ε2 ) in their analysis.
",4.1. Regret Upper Bound,0,[0]
We now provide a proof outline.,4.1. Regret Upper Bound,0,[0]
"The difference from Theorem 1 is that we can use the special combinatorial structure of matroid to improve the analysis.
",4.1. Regret Upper Bound,0,[0]
"Firstly, we introduce a fact from (Kveton et al., 2014).",4.1. Regret Upper Bound,0,[0]
Fact 1.,4.1. Regret Upper Bound,0,[0]
"(Lemma 1 in (Kveton et al., 2014))",4.1. Regret Upper Bound,0,[0]
"For each S(t) = {i(1)(t), · · · , i(K)(t)} chosen by Algorithm 1 (the superscript is the order when they are chosen), we could find a bijection Lt from {1, 2, . . .",4.1. Regret Upper Bound,0,[0]
",K} to S∗ such that:
1)",4.1. Regret Upper Bound,0,[0]
"If i(k)(t) ∈ S∗, then Lt(k) = i(k)(t);
2) ∀1",4.1. Regret Upper Bound,0,[0]
"≤ k ≤ K, {i(1)(t), · · · , i(k−1)(t), Lt(k)} ∈ I.
With a bijection Lt, we could decouple the regret of playing one action S(t) to each pair of mapped arms between S(t)
and S∗. For example, the regret of time t is ∑K k=1 µLt(k) − µi(k)(t).
",4.1. Regret Upper Bound,0,[0]
"We use Ni,j(t) to denote the number of rounds that i(k)(t) = i and Lt(k) = j for i /∈",4.1. Regret Upper Bound,0,[0]
"S∗, j ∈ S∗",4.1. Regret Upper Bound,0,[0]
"within in time slots 1, 2, · · · , T , then
Reg(T ) ≤ ∑ i/∈S∗ ∑ j:j∈S∗,µj>µi E[Ni,j(t)](µj − µi).
",4.1. Regret Upper Bound,0,[0]
"We can see that if {j : j ∈ S∗, µj > µi} = ∅, then∑ i/∈S∗ ∑ j:j∈S∗,µj>µi E[Ni,j(t)](µj − µi) = 0, thus we do not need to consider the regret from base arm",4.1. Regret Upper Bound,0,[0]
"i, so we set ∆i =∞ to make 1∆i = 0.
",4.1. Regret Upper Bound,0,[0]
"Now we just need to bound the value Ni,j(t), similarly, we can defined the following three events:
• Ai,j(t) = {∃k, i(k)(t) =",4.1. Regret Upper Bound,0,[0]
"i ∧ Lt(k) = j}
• Bi(t) = {µ̂i(t) >",4.1. Regret Upper Bound,0,[0]
"µi + ε}
• Ci,j(t) = {θi(t) > µj",4.1. Regret Upper Bound,0,[0]
"− ε}
Thus
E[Ni,j(t)]",4.1. Regret Upper Bound,0,[0]
#NAME?,4.1. Regret Upper Bound,0,[0]
"[ T∑ t=1 I[Ai,j(t)]",4.1. Regret Upper Bound,0,[0]
"]
≤ E",4.1. Regret Upper Bound,0,[0]
"[ T∑ t=1 I[Ai,j(t)",4.1. Regret Upper Bound,0,[0]
∧ Bi(t)],4.1. Regret Upper Bound,0,[0]
"]
+E [ T∑ t=1 I[Ai,j(t) ∧ ¬Bi(t)",4.1. Regret Upper Bound,0,[0]
"∧ Ci,j(t)]",4.1. Regret Upper Bound,0,[0]
"]
+E [ T∑ t=1 I[Ai,j(t) ∧ ¬Ci,j(t)]",4.1. Regret Upper Bound,0,[0]
"]
We can use Chernoff Bound to get an upper bound of the first term as (m − K)(1 + 1ε2 ).",4.1. Regret Upper Bound,0,[0]
"As for the second term, basic properties of Beta distribution give an upper bound: (m−K)K + ∑ i/∈S∗
log T 2∆2i .
",4.1. Regret Upper Bound,0,[0]
"The largest difference appears in the third term, instead of Lemma 1, here we can have some further steps in matroid bandit.",4.1. Regret Upper Bound,0,[0]
Lemma 2.,4.1. Regret Upper Bound,0,[0]
"Suppose the vector θ(t) satisfy that Ai,j(t) ∧ ¬Ci,j(t) happens.",4.1. Regret Upper Bound,0,[0]
"Then if we change θj(t) to θ′j(t) > µj−ε and set other values in θ(t) unchanged to get θ′(t), then arm j must be chosen in θ′(t).
",4.1. Regret Upper Bound,0,[0]
"We use the notation θ−i to be the vector θ without θi.
",4.1. Regret Upper Bound,0,[0]
"For any j ∈ S∗, let Wj be the set of all possible values of θ satisfies that Ai,j(t)",4.1. Regret Upper Bound,0,[0]
"∧ ¬Ci,j(t) happens for some i, and W−j = {θ−j : θ ∈Wj}.
",4.1. Regret Upper Bound,0,[0]
"From Lemma 2, we know θ(t) ∈Wj only if θ−j(t) ∈W−j and θj(t) ≤ µj",4.1. Regret Upper Bound,0,[0]
− ε.,4.1. Regret Upper Bound,0,[0]
"Then similar with the analysis of Theorem 1, we can bound the value ∑T t=1",4.1. Regret Upper Bound,0,[0]
E[θ(t) ∈Wj ] ≤,4.1. Regret Upper Bound,0,[0]
O( 1ε4 ).,4.1. Regret Upper Bound,0,[0]
We conduct some preliminary experiments to empirically evaluate the performance of CTS versus CUCB and C-KLUCB.,5. Experiments,0,[0]
"The reason that we choose C-KL-UCB is that: a) in classical MAB model, KL-UCB behaves better than UCB; b) similar with TS, it is also a policy based on Bayesian Rule.",5. Experiments,0,[0]
"We also make simulations on CUCB and C-KL-UCB with chosen parameters, represented by CUCB-m and CKL-UCB-m. In CUCB, we choose the confidence radius to be radi(t) =",5. Experiments,0,[0]
"√ 3 log t 2Ni(t) , while in CUCB-m, it is √ log t 2Ni(t)
.",5. Experiments,0,[0]
"In C-KL-UCB we choose f(t) = log t+ 2 log log t, while in C-KL-UCB-m it is log t. Those chosen parameters in CUCB-m and C-KL-UCB-m make them behave better, but lack performance analysis.",5. Experiments,0,[0]
It is well known that spanning trees form a matroid.,5.1. Matroid Bandit,0,[0]
"Thus, we test the maximum spanning tree problem as an example of matroid bandits, where edges are arms, and super arms are forests.
",5.1. Matroid Bandit,0,[0]
"We first generate a random graph with M nodes, and each pair of nodes has an edge with probability p.",5.1. Matroid Bandit,0,[0]
"If the resulting graph has no spanning tree, we regenerate the graph again.",5.1. Matroid Bandit,0,[0]
"The mean of the distribution is randomly and uniformly chosen from [0, 1].",5.1. Matroid Bandit,0,[0]
The expected reward for any spanning tree is the sum of the means of all edges in it.,5.1. Matroid Bandit,0,[0]
"It is easy to see that this setting is an instance of the matroid bandit.
",5.1. Matroid Bandit,0,[0]
The results are shown in Figure 1 with the probability p = 0.6 and M = 30.,5.1. Matroid Bandit,0,[0]
"In Figure 1(a), we set all the arms to have independent distributions.",5.1. Matroid Bandit,0,[0]
"In Figure 1(b), each time slot we generate a global random variable rand uniformly in [0, 1], all edges with mean larger than rand will have outcome 1, while others have outcome 0.",5.1. Matroid Bandit,0,[0]
"In other words, the distributions of base arms are correlated.",5.1. Matroid Bandit,0,[0]
"We can see that CTS has smaller regret than CUCB, CUCB-m and CKL-UCB in both two experiments.",5.1. Matroid Bandit,0,[0]
"As for C-KL-UCB-m algorithm, it behaves better with small T , but loses when T is very large.",5.1. Matroid Bandit,0,[0]
"We emphasize that C-KL-UCB-m policy uses parameters without theoretical guarantee, thus CTS algorithm is a better choice.",5.1. Matroid Bandit,0,[0]
"In the general CMAB case, we consider the shortest path problem.",5.2. General CMAB,0,[0]
"We build two graphs for this experiment, the results of them are shown in Figure 2(a) and Figure 2(b).
",5.2. General CMAB,0,[0]
"The cost of a path is the sum of all edges’ mean in that path, while the outcome of each edge e follows a independent Bernoulli distribution with mean µe.",5.2. General CMAB,0,[0]
The objective is to find the path with minimum cost.,5.2. General CMAB,0,[0]
"To make the problem more challenging, in both graphs we construct a lot of paths from the source node s to the sink node t that only have a little larger cost than the optimal one, and some of them are totally disjoint with the optimal path.
",5.2. General CMAB,0,[0]
"Similar to the case of matroid bandit, the regret of CTS is also much smaller than that of CUCB, CUCB-m and C-KLUCB, especially when T is large.",5.2. General CMAB,0,[0]
"As for the C-KL-UCB-m algorithm, although it behaves best in the four UCB-based policies, it still has a large difference between CTS.",5.2. General CMAB,0,[0]
"In this paper, we apply combinatorial Thompson sampling to combinatorial multi-armed bandit and matroid bandit problems, and obtain theoretical regret upper bounds for those two settings.
",6. Future Work,0,[0]
There are still a number of interesting questions that may worth further investigation.,6. Future Work,0,[0]
"For example, pulling each base arm for a number of time slots at the beginning of the game can decease the constant term to non-exponential, but the point is that the player does not know how many time slots are enough.",6. Future Work,0,[0]
Thus how can we use an adaptive policy or some further assumptions to do so is a good question.,6. Future Work,0,[0]
"In this paper, we suppose that all the distributions for base arms are independent.",6. Future Work,0,[0]
Another question is how to find analysis for using CTS under correlated arm distributions.,6. Future Work,0,[0]
We study the application of the Thompson sampling (TS) methodology to the stochastic combinatorial multi-armed bandit (CMAB) framework.,abstractText,0,[0]
"We analyze the standard TS algorithm for the general CMAB, and obtain the first distributiondependent regret bound of O(m log T/∆min) for TS under general CMAB, where m is the number of arms, T is the time horizon, and ∆min is the minimum gap between the expected reward of the optimal solution and any non-optimal solution.",abstractText,0,[0]
We also show that one cannot use an approximate oracle in TS algorithm for even MAB problems.,abstractText,0,[0]
"Then we expand the analysis to matroid bandit, a special case of CMAB and for which we could remove the independence assumption across arms and achieve a better regret bound.",abstractText,0,[0]
"Finally, we use some experiments to show the comparison of regrets of CUCB and CTS algorithms.",abstractText,0,[0]
Thompson Sampling for Combinatorial Semi-Bandits,title,0,[0]
"Proceedings of NAACL-HLT 2018, pages 82–91 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,0,[0]
"Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a).",1 Introduction,0,[0]
"For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French.",1 Introduction,0,[0]
"Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal.
",1 Introduction,0,[0]
Speech can be interpreted either by transcription in the original language or translation to another language.,1 Introduction,0,[0]
"Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both signals.",1 Introduction,0,[0]
"Our contribution lies in improving the sequence-to-sequence multitask learning paradigm, by drawing on two intuitive notions: that higher-level representations are more useful than lower-level representations, and that translation should be both transitive and invertible.
",1 Introduction,0,[0]
"Higher-level intermediate representations, such as transcriptions, should in principle carry information useful for an end task like speech translation.",1 Introduction,0,[0]
"A typical multitask setup (Weiss et al., 2017) shares information at the level of encoded frames, but intuitively, a human translating speech must work from a higher level of representation, at least at the level of phonemes if not syntax or semantics.",1 Introduction,0,[0]
"Thus, we present a novel architecture for tied multitask learning with sequence-to-sequence models, in which the decoder of the second task receives information not only from the encoder, but also from the decoder of the first task.
",1 Introduction,0,[0]
"In addition, transitivity and invertibility are two properties that should hold when mapping between levels of representation or across languages.",1 Introduction,0,[0]
"We demonstrate how these two notions can be implemented through regularization of the attention matrices, and how they lead to further improved performance.
",1 Introduction,0,[0]
"We evaluate our models in three experiment settings: low-resource speech transcription and translation, word discovery on unsegmented input, and high-resource text translation.",1 Introduction,0,[0]
"Our highresource experiments are performed on English, French, and German.",1 Introduction,0,[0]
"Our low-resource speech experiments cover a wider range of linguistic diversity: Spanish-English, Mboshi-French, and AinuEnglish.
",1 Introduction,0,[0]
"In the speech transcription and translation tasks, our proposed model leads to improved performance against all baselines as well as previous multitask architectures.",1 Introduction,0,[0]
"We observe improvements of up to 5% character error rate in the transcription task, and up to 2.8% character-level BLEU in the translation task.",1 Introduction,0,[0]
"However, we didn’t observe similar improvements in the text translation experiments.",1 Introduction,0,[0]
"Finally, on the word discovery task, we improve upon previous work by about 3% F-score on both tokens and types.
82",1 Introduction,0,[0]
"Our models are based on a sequence-to-sequence model with attention (Bahdanau et al., 2015).",2 Model,0,[0]
"In general, this type of model is composed of three parts: a recurrent encoder, the attention, and a recurrent decoder (see Figure 1a).1
The encoder transforms an input sequence of words or feature frames x1, . . .",2 Model,0,[0]
", xN into a sequence of input states h1, . . .",2 Model,0,[0]
",hN :
hn = enc(hn−1, xn).
",2 Model,0,[0]
"The attention transforms the input states into a sequence of context vectors via a matrix of attention weights:
cm = ∑
n
αmnhn.
",2 Model,0,[0]
"Finally, the decoder computes a sequence of output states from which a probability distribution over output words can be computed.
",2 Model,0,[0]
"sm = dec(sm−1, cm, ym−1) P(ym) = softmax(sm).
",2 Model,0,[0]
"In a standard encoder-decoder multitask model (Figure 1b) (Dong et al., 2015; Weiss et al., 2017), we jointly model two output sequences using a shared encoder, but separate attentions and decoders:
c1m = ∑
n
α1mnhn
s1m = dec 1(s1m−1, c 1 m, y 1 m−1)
",2 Model,0,[0]
P(y1m) =,2 Model,0,[0]
"softmax(s 1 m)
and
c2m = ∑
n
α2mnhn
s2m = dec 2(s2m−1, c 2 m, y 2 m−1)
",2 Model,0,[0]
P(y2m) =,2 Model,0,[0]
"softmax(s 2 m).
",2 Model,0,[0]
"We can also arrange the decoders in a cascade (Figure 1c), in which the second decoder attends only to the output states of the first decoder:
c2m = ∑
m′",2 Model,0,[0]
"α12mm′s 1 m′
s2m = dec 2(s2m−1, c 2 m, y 2 m−1)
",2 Model,0,[0]
P(y2m) =,2 Model,0,[0]
"softmax(s 2 m).
",2 Model,0,[0]
"1For simplicity, we have assumed only a single layer for both the encoder and decoder.",2 Model,0,[0]
"It is possible to use multiple stacked RNNs; typically, the output of the encoder and decoder (cm and P(ym), respectively) would be computed from the top layer only.
",2 Model,0,[0]
"Tu et al. (2017) use exactly this architecture to train on bitext by setting the second output sequence to be equal to the input sequence (y2i = xi).
",2 Model,0,[0]
"In our proposed triangle model (Figure 1d), the first decoder is as above, but the second decoder has two attentions, one for the input states of the encoder and one for the output states of the first decoder:
c2m =",2 Model,0,[0]
[∑ m′ α 12 mm′s 1 m′ ∑ n,2 Model,0,[0]
"α 2 mnhn ] s2m = dec 2(s2m−1, c 2 m, y 2 m−1)
",2 Model,0,[0]
P(y2m) =,2 Model,0,[0]
"softmax(s 2 m).
",2 Model,0,[0]
"Note that the context vectors resulting from the two attentions are concatenated, not added.",2 Model,0,[0]
"For compactness, we will write X for the matrix whose rows are the xn, and similarly H, C, and so on.",3 Learning and Inference,0,[0]
We also write A for the matrix of attention weights:,3 Learning and Inference,0,[0]
"[A]i j = αi j.
Let θ be the parameters of our model, which we train on sentence triples (X,Y1,Y2).",3 Learning and Inference,0,[0]
"Define the score of a sentence triple to be a loglinear interpolation of the two decoders’ probabilities:
score(Y1,Y2 | X; θ) = λ log P(Y1 | X; θ) + (1 − λ) log P(Y2 | X,S1; θ)
where λ is a parameter that controls the importance of each sub-task.",3.1 Maximum likelihood estimation,0,[0]
"In all our experiments, we set λ to 0.5.",3.1 Maximum likelihood estimation,0,[0]
"We then train the model to maximize
L(θ) = ∑ score(Y1,Y2 | X; θ),
where the summation is over all sentence triples in the training data.",3.1 Maximum likelihood estimation,0,[0]
"We can optionally add a regularization term to the objective function, in order to encourage our attention mechanisms to conform to two intuitive principles of machine translation: transitivity and invertibility.
",3.2 Regularization,0,[0]
"Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015):",3.2 Regularization,0,[0]
"If source word xi aligns to target word
y1j and y 1 j aligns to target word y 2 k , then xi should also probably align to y2k .",3.2 Regularization,0,[0]
"To encourage the model to preserve this relationship, we add the following transitivity regularizer to the loss function of the triangle models with a small weight λtrans = 0.2:
Ltrans = score(Y1,Y2)",3.2 Regularization,0,[0]
"− λtrans ∥∥∥A12A1 − A2 ∥∥∥2 2.
",3.2 Regularization,0,[0]
"Invertibility attention regularizer The translation relation also ought to be roughly invertible (Levinboim et al., 2015): if, in the reconstruction version of the cascade model, source word xi aligns to target word y1j , then it stands to reason that y j is likely to align to xi.",3.2 Regularization,0,[0]
"So, whereas Tu et al. (2017) let the attentions of the translator and the reconstructor be unrelated, we try adding the following invertibility regularizer to encourage the attentions to each be the inverse of the other, again with a weight λinv = 0.2:
",3.2 Regularization,0,[0]
"Linv = score(Y1,Y2)",3.2 Regularization,0,[0]
− λinv ∥∥∥A1A12 − I ∥∥∥2 2.,3.2 Regularization,0,[0]
"Since we have two decoders, we now need to employ a two-phase beam search, following Tu et al. (2017):
1.",3.3 Decoding,0,[0]
"The first decoder produces, through standard beam search, a set of triples each consisting of a candidate transcription Ŷ1, a score P(Ŷ1), and a hidden state sequence Ŝ.
2.",3.3 Decoding,0,[0]
"For each transcription candidate from the first decoder, the second decoder now produces
through beam search a set of candidate translations Ŷ2, each with a score P(Ŷ2).
3.",3.3 Decoding,0,[0]
"We then output the combination that yields the highest total score(Y1,Y2).",3.3 Decoding,0,[0]
"All our models are implemented in DyNet (Neubig et al., 2017).2 We use a dropout of 0.2, and train using Adam with initial learning rate of 0.0002 for a maximum of 500 epochs.",3.4 Implementation,0,[0]
"For testing, we select the model with the best performance on dev.",3.4 Implementation,0,[0]
"At inference time, we use a beam size of 4 for each decoder (due to GPU memory constraints), and the beam scores include length normalization (Wu et al., 2016) with a weight of 0.8, which Nguyen and Chiang (2017) found to work well for lowresource NMT.",3.4 Implementation,0,[0]
"We focus on speech transcription and translation of endangered languages, using three different cor-
2Our code is available at: https://bitbucket.org/ antonis/dynet-multitask-models.
",4 Speech Transcription and Translation,0,[0]
"pora on three different language directions: Spanish (es) to English (en), Ainu (ai) to English, and Mboshi (mb) to French (fr).",4 Speech Transcription and Translation,0,[0]
"Spanish is, of course, not an endangered language, but the availability of the CALLHOME Spanish Speech dataset (LDC2014T23) with English translations (Post et al., 2013) makes it a convenient language to work with, as has been done in almost all previous work in this area.",4.1 Data,0,[0]
It consists of telephone conversations between relatives (about 20 total hours of audio) with more than 240 speakers.,4.1 Data,0,[0]
"We use the original train-dev-test split, with the training set comprised of 80 conversations and dev and test of 20 conversations each.
",4.1 Data,0,[0]
Hokkaido Ainu is the sole surviving member of the Ainu language family and is generally considered a language isolate.,4.1 Data,0,[0]
"As of 2007, only ten native speakers were alive.",4.1 Data,0,[0]
"The Glossed Audio Corpus of Ainu Folklore provides 10 narratives with audio (about 2.5 hours of audio) and translations in Japanese and English.3 Since there does not exist a standard train-dev-test split, we employ a cross validation scheme for evaluation purposes.",4.1 Data,0,[0]
"In each fold, one of the 10 narratives becomes the test set, with the previous one (mod 10) becoming the dev set, and the remaining 8 narratives becoming the training set.",4.1 Data,0,[0]
The models for each of the 10 folds are trained and tested separately.,4.1 Data,0,[0]
"On average, for each fold, we train on about 2000 utterances; the dev and test sets consist of about 270 utterances.
",4.1 Data,0,[0]
"3http://ainucorpus.ninjal.ac.jp/corpus/en/
We report results on the concatenation of all folds.",4.1 Data,0,[0]
"The Ainu text is split into characters, except for the equals (=) and underscore ( ) characters, which are used as phonological or structural markers and are thus merged with the following character.4
Mboshi (Bantu C25 in the Guthrie classification) is a language spoken in Congo-Brazzaville, without standard orthography.",4.1 Data,0,[0]
"We use a corpus (Godard et al., 2017) of 5517 parallel utterances (about 4.4 hours of audio) collected from three native speakers.",4.1 Data,0,[0]
"The corpus provides non-standard grapheme transcriptions (close to the language phonology) produced by linguists, as well as French translations.",4.1 Data,0,[0]
"We sampled 100 segments from the training set to be our dev set, and used the original dev set (514 sentences) as our test set.",4.1 Data,0,[0]
We employ a 3-layer speech encoding scheme similar to that of Duong et al. (2016).,4.2 Implementation,0,[0]
"The first bidirectional layer receives the audio sequence in the form of 39-dimensional Perceptual Linear Predictive (PLP) features (Hermansky, 1990) computed over overlapping 25ms-wide windows every 10ms.",4.2 Implementation,0,[0]
The second and third layers consist of LSTMs with hidden state sizes of 128 and 512 respectively.,4.2 Implementation,0,[0]
Each layer encodes every second output of the previous layer.,4.2 Implementation,0,[0]
"Thus, the sequence is downsampled by a factor of 4, decreasing the computation load for the attention mechanism and the decoders.",4.2 Implementation,0,[0]
"In the speech experiments, the decoders
4The data preprocessing scripts are released with the rest of our code.
",4.2 Implementation,0,[0]
"output the sequences at the grapheme level, so the output embedding size is set to 64.
",4.2 Implementation,0,[0]
We found that this simpler speech encoder works well for our extremely small datasets.,4.2 Implementation,0,[0]
"Applying our models to larger datasets with many more speakers would most likely require a more sophisticated speech encoder, such as the one used by Weiss et al. (2017).",4.2 Implementation,0,[0]
"In Table 2, we present results on three small datasets that demonstrate the efficacy of our models.",4.3 Results,0,[0]
We compare our proposed models against three baselines and one “skyline.”,4.3 Results,0,[0]
"The first baseline is a traditional pivot approach (line 1), where the ASR output, a sequence of characters, is the input to a character-based NMT system (trained on gold transcriptions).",4.3 Results,0,[0]
"The “skyline” model (line 2) is the same NMT system, but tested on gold transcriptions instead of ASR output.",4.3 Results,0,[0]
The second baseline is translation directly from source speech to target text (line 3).,4.3 Results,0,[0]
"The last baseline is the standard multitask model (line 4), which is similar to the model of Weiss et al. (2017).
",4.3 Results,0,[0]
"The cascade model (line 5) outperforms the baselines on the translation task, while only falling behind the multitask model in the transcription task.",4.3 Results,0,[0]
"On all three datasets, the triangle model (lines 6, 7) outperforms all baselines, including the standard multitask model.",4.3 Results,0,[0]
"On Ainu-English, we even obtain translations that are comparable to the “skyline” model, which is tested on gold Ainu transcriptions.
",4.3 Results,0,[0]
"Comparing the performance of all models across the three datasets, there are two notable trends that verify common intuitions regarding the speech transcription and translation tasks.",4.3 Results,0,[0]
"First, an increase in the number of speakers hurts the performance of the speech transcription tasks.",4.3 Results,0,[0]
"The character error rates for Ainu are smaller than the CER in Mboshi, which in turn are smaller than the CER in CALLHOME.",4.3 Results,0,[0]
"Second, the character-level BLEU scores increase as the amount of training data increases, with our smallest dataset (Ainu) having the lowest BLEU scores, and the largest dataset (CALLHOME) having the highest BLEU scores.",4.3 Results,0,[0]
"This is expected, as more training data means that the translation decoder learns a more informed character-level language model for the target language.
",4.3 Results,0,[0]
"Note that Weiss et al. (2017) report much higher
BLEU scores on CALLHOME:",4.3 Results,0,[0]
our model underperforms theirs by almost 9 word-level BLEU points.,4.3 Results,0,[0]
"However, their model has significantly more parameters and is trained on 10 times more data than ours.",4.3 Results,0,[0]
Such an amount of data would never be available in our endangered languages scenario.,4.3 Results,0,[0]
"When calculated on the wordlevel, all our models’ BLEU scores are between 3 and 7 points for the extremely low resource datasets (Mboshi-French and Ainu-English), and between 7 and 10 for CALLHOME.",4.3 Results,0,[0]
"Clearly, the size of the training data in our experiments is not enough for producing high quality speech translations, but we plan to investigate the performance of our proposed models on larger datasets as part of our future work.
",4.3 Results,0,[0]
"To evaluate the effect of using the combined score from both decoders at decoding time, we evaluated the triangle models using only the 1-best output from the speech model (lines 8, 9).",4.3 Results,0,[0]
One would expect that this would favor speech at the expense of translation.,4.3 Results,0,[0]
"In transcription accuracy, we indeed observed improvements across the board.",4.3 Results,0,[0]
"In translation accuracy, we observed a surprisingly large drop on Mboshi-French, but surprisingly little effect on the other language pairs – in fact, BLEU scores tended to go up slightly, but not significantly.
",4.3 Results,0,[0]
"Finally, Figure 2 visualizes the attention matrices for one utterance from the baseline multitask model and our proposed triangle model.",4.3 Results,0,[0]
"It is clear that our intuition was correct: the translation decoder receives most of its context from the transcription decoder, as indicated by the higher attention weights of A12.",4.3 Results,0,[0]
"Ideally, the area under the red squares (gold alignments) would account for 100% of the attention mass of A12.",4.3 Results,0,[0]
"In our triangle model, the total mass under the red squares is 34%, whereas the multitask model’s correct attentions amount to only 21% of the attention mass.",4.3 Results,0,[0]
"Although the above results show that our model gives large performance improvements, in absolute terms, its performance on such low-resource tasks leaves a lot of room for future improvement.",5 Word Discovery,0,[0]
"A possible more realistic application of our methods is word discovery, that is, finding word boundaries in unsegmented phonetic transcriptions.
",5 Word Discovery,0,[0]
"After training an attentional encoder-decoder model between Mboshi unsegmented phonetic se-
quences and French word sequences, the attention weights can be thought of as soft alignments, which allow us to project the French word boundaries onto Mboshi.",5 Word Discovery,0,[0]
"Although we could in principle perform word discovery directly on speech, we leave this for future work, and only explore singletask and reconstruction models.",5 Word Discovery,0,[0]
"We use the same Mboshi-French corpus as in Section 4, but with the original training set of 4617 utterances and the dev set of 514 utterances.",5.1 Data,0,[0]
"Our parallel data consist of the unsegmented phonetic Mboshi transcriptions, along with the word-level French translations.",5.1 Data,0,[0]
"We first replicate the model of Boito et al. (2017), with a single-layer bidirectional encoder and single layer decoder, using an embedding and hidden size of 12 for the base model, and an embedding and hidden state size of 64 for the reverse model.",5.2 Implementation,0,[0]
"In our own models, we set the embedding size to 32 for Mboshi characters, 64 for French words, and the hidden state size to 64.",5.2 Implementation,0,[0]
"We smooth the at-
tention weights A using the method of Duong et al. (2016) with a temperature T = 10 for the softmax computation of the attention mechanism.
",5.2 Implementation,0,[0]
"Following Boito et al. (2017), we train models both on the base Mboshi-to-French direction, as well as the reverse (French-to-Mboshi) direction, with and without this smoothing operation.",5.2 Implementation,0,[0]
We further smooth the computed soft alignments of all models so that amn =,5.2 Implementation,0,[0]
(amn−1 +amn +amn+1)/3 as a post-processing step.,5.2 Implementation,0,[0]
From the single-task models we extract the A1 attention matrices.,5.2 Implementation,0,[0]
"We also train reconstruction models on both directions, with and without the invertibility regularizer, extracting both A1 and A12 matrices.",5.2 Implementation,0,[0]
The two matrices are then combined so that A = A1 + (A12)T .,5.2 Implementation,0,[0]
"Evaluation is done both at the token and the type level, by computing precision, recall, and Fscore over the discovered segmentation, with the best results shown in Table 3.",5.3 Results,0,[0]
"We reimplemented the base (Mboshi-French) and reverse (FrenchMboshi) models from Boito et al. (2017), and the performance of the base model was comparable to the one reported.",5.3 Results,0,[0]
"However, we were unable to
reproduce the significant gains that were reported when using the reverse model (italicized in Table 3).",5.3 Results,0,[0]
"Also, our version of both the base and reverse singletask models performed better than our reimplementation of the baseline.
",5.3 Results,0,[0]
"Furthermore, we found that we were able to obtain even better performance at the type level by combining the attention matrices of a reconstruction model trained with the invertibility regularizer.",5.3 Results,0,[0]
"Boito et al. (2017) reported that combining the attention matrices of a base and a reverse model significantly reduced performance, but they trained the two models separately.",5.3 Results,0,[0]
"In contrast, we obtain the base (A1) and the reverse attention matrices (A12) from a model that trains them jointly, while also tying them together through the invertibility regularizer.",5.3 Results,0,[0]
"Using the regularizer is key to the improvements; in fact, we did not observe any improvements when we trained the reconstruction models without the regularizer.",5.3 Results,0,[0]
"For evaluating our models on text translation, we use the Europarl corpus which provides parallel sentences across several European languages.",6.1 Data,0,[0]
"We extracted 1,450,890 three-way parallel sentences on English, French, and German.",6.1 Data,0,[0]
"The concatenation of the newstest 2011–2013 sets (8,017 sentences) is our dev set, and our test set is the concatenation of the newstest 2014 and 2015 sets (6,003 sentences).",6.1 Data,0,[0]
"We test all architectures on the six possible translation directions between English
(en), French (fr) and German (de).",6.1 Data,0,[0]
"All the sequences are represented by subword units with byte-pair encoding (BPE) (Sennrich et al., 2016) trained on each language with 32000 operations.",6.1 Data,0,[0]
"On all experiments, the encoder and the decoder(s) have 2 layers of LSTM units with hidden state size and attention size of 1024, and embedding size of 1024.",6.2 Experimental Setup,0,[0]
"For this high resource scenario, we only train for a maximum of 40 epochs.",6.2 Experimental Setup,0,[0]
The accuracy of all the models on all six language pair directions is shown in Table 4.,6.3 Results,0,[0]
"In all cases, the best models are the baseline single-task or simple multitask models.",6.3 Results,0,[0]
"There are some instances, such as English-German, where the reconstruction or the triangle models are not statistically significantly different from the best model.",6.3 Results,0,[0]
"The reason for this, we believe, is that in the case of text translation between so linguistically close languages, the lower level representations (the output of the encoder) provide as much information as the higher level ones, without the search errors that are introduced during inference.
",6.3 Results,0,[0]
A notable outcome of this experiment is that we do not observe the significant improvements with the reconstruction models that Tu et al. (2017) observed.,6.3 Results,0,[0]
A few possible differences between our experiment and theirs are: our models are BPEbased,6.3 Results,0,[0]
", theirs are word-based; we use Adam for optimization, they use Adadelta; our model has slightly fewer parameters than theirs; we test on less typologically different language pairs than
English-Chinese.",6.3 Results,0,[0]
"However, we also observe that in most cases our proposed regularizers lead to increased performance.",6.3 Results,0,[0]
The invertibility regularizer aids the reconstruction models in achiev slightly higher BLEU scores in 3 out of the 6 cases.,6.3 Results,0,[0]
"The transitivity regularizer is even more effective: in 9 out the 12 source-target language combinations, the triangle models achieve higher performance when trained using the regularizer.",6.3 Results,0,[0]
"Some of them are statistical significant improvements, as in the case of French to English where English is the intermediate target language and German is the final target.",6.3 Results,0,[0]
The speech translation problem has been traditionally approached by using the output of an ASR system as input to a MT system.,7 Related Work,0,[0]
"For example, Ney (1999) and Matusov et al. (2005) use ASR output lattices as input to translation models, integrating speech recognition uncertainty into the translation model.",7 Related Work,0,[0]
Recent work has focused more on modelling speech translation without explicit access to transcriptions.,7 Related Work,0,[0]
"Duong et al. (2016) introduced a sequence-to-sequence model for speech translation without transcriptions but only evaluated on alignment, while Anastasopoulos et al. (2016) presented an unsupervised alignment method for speech-to-translation alignment.",7 Related Work,0,[0]
"Bansal et al. (2017) used an unsupervised term discovery system (Jansen et al., 2010) to cluster recurring audio segments into pseudowords
and translate speech using a bag-of-words model.",7 Related Work,0,[0]
"Bérard et al. (2016) translated synthesized speech data using a model similar to the Listen Attend and Spell model (Chan et al., 2016).",7 Related Work,0,[0]
"A larger-scale study (Bérard et al., 2018) used an end-to-end neural system system for translating audio books between French and English.",7 Related Work,0,[0]
"On a different line of work, Boito et al. (2017) used the attentions of a sequence-to-sequence model for word discovery.
",7 Related Work,0,[0]
"Multitask learning (Caruana, 1998) has found extensive use across several machine learning and NLP fields.",7 Related Work,0,[0]
"For example, Luong et al. (2016) and Eriguchi et al. (2017) jointly learn to parse and translate; Kim et al. (2017) combine CTC- and attention-based models using multitask models for speech transcription; Dong et al. (2015) use multitask learning for multiple language translation.",7 Related Work,0,[0]
"Toshniwal et al. (2017) apply multitask learning to neural speech recognition in a less traditional fashion: the lower-level outputs of the speech encoder are used for fine-grained auxiliary tasks such as predicting HMM states or phonemes, while the final output of the encoder is passed to a characterlevel decoder.
",7 Related Work,0,[0]
Our work is most similar to the work of Weiss et al. (2017).,7 Related Work,0,[0]
"They used sequence-to-sequence models to transcribe Spanish speech and translate it in English, by jointly training the two tasks in a multitask scenario where the decoders share the encoder.",7 Related Work,0,[0]
"In contrast to our work, they use a large corpus for training the model on roughly 163 hours of data, using the Spanish Fisher and CALL-
HOME conversational speech corpora.",7 Related Work,0,[0]
"The parameter number of their model is significantly larger than ours, as they use 8 encoder layers, and 4 layers for each decoder.",7 Related Work,0,[0]
This allows their model to adequately learn from such a large amount of data and deal well with speaker variation.,7 Related Work,0,[0]
"However, training such a large model on endangered language datasets would be infeasible.
",7 Related Work,0,[0]
Our model also bears similarities to the architecture of the model proposed by Tu et al. (2017).,7 Related Work,0,[0]
"They report significant gains in Chinese-English translation by adding an additional reconstruction decoder that attends on the last states of the translation decoder, mainly inspired by auto-encoders.",7 Related Work,0,[0]
We presented a novel architecture for multitask learning that provides the second task with higherlevel representations produced from the first task decoder.,8 Conclusion,0,[0]
Our model outperforms both the singletask models as well as traditional multitask architectures.,8 Conclusion,0,[0]
"Evaluating on extremely low-resource settings, our model improves on both speech transcription and translation.",8 Conclusion,0,[0]
"By augmenting our models with regularizers that implement transitivity and invertibility, we obtain further improvements on all low-resource tasks.
",8 Conclusion,0,[0]
These results will hopefully lead to new tools for endangered language documentation.,8 Conclusion,0,[0]
"Projects like BULB aim to collect about 100 hours of audio with translations, but it may be impractical to transcribe this much audio for many languages.",8 Conclusion,0,[0]
"For future work, we aim to extend these methods to settings where we don’t necessarily have sentence triples, but where some audio is only transcribed and some audio is only translated.
",8 Conclusion,0,[0]
Acknowledgements This work was generously supported by NSF Award 1464553.,8 Conclusion,0,[0]
We are grateful to the anonymous reviewers for their useful comments.,8 Conclusion,0,[0]
"We explore multitask models for neural translation of speech, augmenting them in order to reflect two intuitive notions.",abstractText,0,[0]
"First, we introduce a model where the second task decoder receives information from the decoder of the first task, since higher-level intermediate representations should provide useful information.",abstractText,0,[0]
"Second, we apply regularization that encourages transitivity and invertibility.",abstractText,0,[0]
We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation.,abstractText,0,[0]
It also leads to better performance when using attention information for word discovery over unsegmented input.,abstractText,0,[0]
Tied Multitask Learning for Neural Speech Translation,title,0,[0]
"We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time T behaves as Ω( √ T ) and O( √ T log T ). This gives a tight characterization up to a √
log T factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Matérn-ν kernels, with the latter requiring ν > 2. Our results certify the near-optimality of existing bounds (Srinivas et al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Matérn kernel with ν > 2.",text,0,[0]
"Bayesian optimization (BO) (Shahriari et al., 2016) is a powerful and versatile tool for black-box function optimization, with applications including parameter tuning, robotics, molecular design, sensor networks, and more.",1. Introduction,0,[0]
The idea is to model the unknown function as a Gaussian process with a given kernel function dictating the smoothness properties.,1. Introduction,0,[0]
"This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.
",1. Introduction,1,"['This model is updated using (typically noisy) samples, which are selected to steer towards the function maximum.']"
One of the most attractive properties of BO is its efficiency in terms of the number of function samples used.,1. Introduction,0,[0]
"Consequently, algorithms with rigorous guarantees on the tradeoff between samples and optimization performance are particularly valuable.",1. Introduction,0,[0]
"Perhaps the most prominent work in the literature giving such guarantees is that of (Srinivas et al.,
1Department of Computer Science & Department of Mathematics, National University of Singapore.",1. Introduction,0,[0]
"Correspondence to: Jonathan Scarlett <scarlett@comp.nus.edu.sg>.
",1. Introduction,0,[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,0,[0]
"Copyright 2018 by the author(s).
2010), who consider the cumulative regret:
RT = T∑ t=1",1. Introduction,0,[0]
"( max x f(x)− f(xt) ) , (1)
where f is the function being optimized, and xt is the point chosen at time t. Under a Gaussian process (GP) prior and Gaussian noise, it is shown in (Srinivas et al., 2010) that an algorithm called Gaussian Process Upper Confidence Bound (GP-UCB) achieves a cumulative regret of the form
RT = O ∗( √ TγT ), (2)
where γT = maxx1,...,xT I(f ;y) (with function values f = (f(x1), . . .",1. Introduction,0,[0]
", f(xT )) and noisy samples y = (y1, . . .",1. Introduction,1,"[', f(xT )) and noisy samples y = (y1, .']"
", yT )) is known as the maximum information gain.",1. Introduction,1,"[', yT )) is known as the maximum information gain.']"
"Here I(f ;y) denotes the mutual information (Cover & Thomas, 2001) between the function values and noisy samples, and O∗(·) denotes asymptotic notation up to logarithmic factors.
",1. Introduction,1,"['Here I(f ;y) denotes the mutual information (Cover & Thomas, 2001) between the function values and noisy samples, and O∗(·) denotes asymptotic notation up to logarithmic factors.']"
The guarantee (2) ensures sub-linear cumulative regret for many kernels of interest.,1. Introduction,1,['The guarantee (2) ensures sub-linear cumulative regret for many kernels of interest.']
"However, the literature is severely lacking in algorithm-independent lower bounds, and without these, it is impossible to know to what extent the upper bounds, including (2), can be improved.",1. Introduction,0,[0]
"In this work, we address this gap in detail in the special case of a onedimensional function.",1. Introduction,0,[0]
"We show that the best possible cumulative regret behaves as Θ∗( √ T ) under mild assumptions on the kernel, thus identifying both cases where (2) is nearoptimal, and cases where it is strictly suboptimal.",1. Introduction,1,"['We show that the best possible cumulative regret behaves as Θ∗( √ T ) under mild assumptions on the kernel, thus identifying both cases where (2) is nearoptimal, and cases where it is strictly suboptimal.']"
"An extensive range of BO algorithms have been proposed in the literature, typically involving the maximization of an acquisition function (Hennig & Schuler, 2012; HernándezLobato et al., 2014; Russo & Van Roy, 2014; Wang et al., 2016); see (Shahriari et al., 2016) for a recent overview.",1.1. Related Work,0,[0]
"As mentioned above, the most relevant algorithm to this work for the noisy setting is GP-UCB (Srinivas et al., 2010), which constructs confidence bounds in which the function lies with high probability, and samples the point with the highest upper confidence bound.",1.1. Related Work,0,[0]
"Several extensions to GPUCB have also been proposed, including contextual (Krause & Ong, 2011; Bogunovic et al., 2016a), batch (Contal et al., 2013; Desautels et al., 2014), and high-dimensional (Kandasamy et al., 2015; Rolland et al., 2018) variants.
",1.1. Related Work,0,[0]
"In the noiseless setting, it has been shown that it is possible to achieve bounded cumulative regret (de Freitas et al., 2012; Kawaguchi et al., 2015) under some technical assumptions.",1.1. Related Work,0,[0]
"In (de Freitas et al., 2012), this is done by keeping track of a set of potential maximizers, and sampling increasingly finely in order to shrink that set and “zoom in” towards the optimal point.",1.1. Related Work,0,[0]
"Similar ideas have also been used in the noisy setting for studying batch variants of GP-UCB (Contal et al., 2013), simultaneous online optimization (SOO) methods (Wang et al., 2014), and lookahead algorithms that use confidence bounds (Bogunovic et al., 2016b).",1.1. Related Work,0,[0]
"Returning to the noiseless setting, upper and lower bounds were given in (Grünewälder et al., 2010) for kernels satisfying certain smoothness assumptions, with the lower bounds showing that bounded cumulative regret is not always to be expected.
",1.1. Related Work,0,[0]
"Alongside the Bayesian view of the Gaussian process model, several works have also considered a non-Bayesian counterpart assuming that the function has a bounded norm in the associated reproducing kernel Hilbert space (RKHS).",1.1. Related Work,0,[0]
"Interestingly, GP-UCB still provides similar guarantees to (2) in this setting (Srinivas et al., 2010).",1.1. Related Work,0,[0]
"Moreover, lower bounds have been proved; see (Bull, 2011) for the noiseless setting, and (Scarlett et al., 2017) for the noisy setting.",1.1. Related Work,0,[0]
"In the latter, the lower bounds nearly match the GP-UCB upper bound for the squared exponential (SE) kernel, but gaps remain for the Matérn kernel.",1.1. Related Work,0,[0]
"For reference, we note that these kernels are defined as follows:
kSE(x, x ′) = exp ( − ‖x− x
′‖2 2l2
) (3)
kMatérn(x, x ′) =
21−ν
Γ(ν)
(√ 2ν‖x− x′‖
l )ν",1.1. Related Work,0,[0]
"×Bν (√ 2ν‖x− x′‖
l
) , (4)
where l > 0 is a lengthscale parameter, ν > 0 is a smoothness parameter, Bν is the modified Bessel function, and Γ is the gamma function.
",1.1. Related Work,0,[0]
"The multi-armed bandit (MAB) (Bubeck & Cesa-Bianchi, 2012) literature has developed alongside the BO literature, with the two often bearing similar concepts.",1.1. Related Work,0,[0]
"The MAB literature is far too extensive to cover here, but it is worth mentioning that sharp lower bounds are known in numerous settings (Bubeck & Cesa-Bianchi, 2012), and the abovementioned concept of “zooming in” to the optimal point has also been explored (Kleinberg et al., 2008).",1.1. Related Work,0,[0]
"To our knowledge, however, none of the existing MAB results are closely related to our own.",1.1. Related Work,0,[0]
"The main results of this paper are informally summarized as follows.
",1.2. Our Results and Their Implications,0,[0]
Main Results (Informal).,1.2. Our Results and Their Implications,0,[0]
"Under mild technical assumptions on the kernel, satisfied (for example) by the SE kernel and Matérn-ν kernel with ν > 2, the best possible cumulative regret of noisy BO in one dimension behaves as Ω( √ T ) and O( √ T log T ).
",1.2. Our Results and Their Implications,1,"['Under mild technical assumptions on the kernel, satisfied (for example) by the SE kernel and Matérn-ν kernel with ν > 2, the best possible cumulative regret of noisy BO in one dimension behaves as Ω( √ T ) and O( √ T log T ).']"
"Our results have several important implications:
• To our knowledge, our lower bound is the first of any kind in the noisy Bayesian setting, and is tight up to a√
log T factor under our technical assumptions.
",1.2. Our Results and Their Implications,1,"['Our results have several important implications: • To our knowledge, our lower bound is the first of any kind in the noisy Bayesian setting, and is tight up to a√ log T factor under our technical assumptions.']"
"• Our lower bound also establishes the order-optimality of the O∗( √ T ) upper bound of (Srinivas et al., 2010)
applied to the SE kernel, up to logarithmic factors.
",1.2. Our Results and Their Implications,0,[0]
"• On the other hand, our upper bound establishes that the upper bound of (Srinivas et al., 2010) for the Matérn-ν kernel, namely O∗(T ν+2 2ν+2 ), is strictly suboptimal for
ν > 2.",1.2. Our Results and Their Implications,1,"['• On the other hand, our upper bound establishes that the upper bound of (Srinivas et al., 2010) for the Matérn-ν kernel, namely O∗(T ν+2 2ν+2 ), is strictly suboptimal for ν > 2.']"
"For example, if ν = 3, then this is O∗(T 0.625), as opposed to our upper bound of O∗(T 0.5).",1.2. Our Results and Their Implications,1,"['For example, if ν = 3, then this is O∗(T 0.625), as opposed to our upper bound of O∗(T 0.5).']"
"(See also (Shekhar & Javidi, 2017) for recent improvements over (Srinivas et al., 2010) under the Matérn kernel in higher dimensions and/or with smaller ν).
",1.2. Our Results and Their Implications,0,[0]
• Another important implication for the Matérn kernel with ν > 2 is that the Bayesian setting is provably harder than the non-Bayesian RKHS counterpart; the latter has cumulative regret Ω(T ν+1 2ν+1 ),1.2. Our Results and Their Implications,0,[0]
"(Scarlett et al.,
2017), which is strictly worse than O( √ T log T ).
",1.2. Our Results and Their Implications,0,[0]
"Our upper bound is stated formally in Section 3, and its technical assumptions are given in Section 2.1.",1.2. Our Results and Their Implications,1,"['Our lower bound is stated formally in Section 4, and its technical assumptions are given in Section 2.1.']"
"We build on the ideas of (de Freitas et al., 2012) for the noiseless setting, while addressing highly non-trivial challenges arising in the presence of noise.
",1.2. Our Results and Their Implications,0,[0]
"Our lower bound is stated formally in Section 4, and its technical assumptions are given in Section 2.1.",1.2. Our Results and Their Implications,0,[0]
"The analysis is based on a reduction to binary hypothesis testing and an application of Fano’s inequality (Cover & Thomas, 2001).",1.2. Our Results and Their Implications,0,[0]
"This approach is inspired by previous work on lower bounds for stochastic convex optimization (Raginsky & Rakhlin, 2011), but the details are very different.",1.2. Our Results and Their Implications,0,[0]
We seek to sequentially optimize an unknown reward function f(x) over the one-dimensional domain D =,2.1. Bayesian Optimization,0,[0]
"[0, 1]; note that any interval can be transformed to this choice via re-scaling.",2.1. Bayesian Optimization,0,[0]
"At time t, we query a single point xt ∈ D and observe a noisy sample yt = f(xt) +",2.1. Bayesian Optimization,0,[0]
"zt, where zt ∼ N(0, σ2) for some noise variance σ2 > 0, with independence across different times.",2.1. Bayesian Optimization,0,[0]
"We measure the performance using the cumulative regret RT , defined in (1).
",2.1. Bayesian Optimization,0,[0]
"We henceforth assume f to be distributed according to Gaussian process (GP) (Rasmussen, 2006)",2.1. Bayesian Optimization,0,[0]
"having mean zero and kernel function k(x, x′).",2.1. Bayesian Optimization,0,[0]
The posterior distribution of f given the points xt,2.1. Bayesian Optimization,0,[0]
=,2.1. Bayesian Optimization,0,[0]
"[x1, . . .",2.1. Bayesian Optimization,0,[0]
", xt]T and observations yt =",2.1. Bayesian Optimization,0,[0]
"[y1, . . .",2.1. Bayesian Optimization,0,[0]
", yt]
T up to time t is again a GP, with the posterior mean and variance given by (Rasmussen, 2006)
µt(x) = kt(x",2.1. Bayesian Optimization,0,[0]
) T ( Kt + σ 2It )−1,2.1. Bayesian Optimization,0,[0]
"yt (5)
σt(x) 2 = k(x, x)− kt(x)T ( Kt + σ 2It )−1 kt(x), (6)
where kt(x) =",2.1. Bayesian Optimization,0,[0]
"[ k(xi, x) ]t i=1",2.1. Bayesian Optimization,0,[0]
",",2.1. Bayesian Optimization,0,[0]
"Kt = [ k(xi, xj) ]",2.1. Bayesian Optimization,0,[0]
"i,j
, and It is the t× t identity matrix.",2.1. Bayesian Optimization,0,[0]
"Here we introduce several assumptions that will be adopted in our main results, some of which were also used in the noiseless setting (de Freitas et al., 2012).
",2.2. Technical Assumptions,0,[0]
Assumption 1.,2.2. Technical Assumptions,0,[0]
"We have the following:
1.",2.2. Technical Assumptions,0,[0]
"The kernel k is stationary, depending on its inputs (x, x′) only through τ = x− x′;
2.",2.2. Technical Assumptions,1,"['The kernel k is stationary, depending on its inputs (x, x′) only through τ = x− x′; 2.']"
"The kernel k satisfies k(x, x′) ≤ 1 for all (x, x′), and k(x, x) = 1 for all x ∈ D;
Given the stationarity assumption, the assumptions k(x, x′) ≤ 1 and k(x, x) = 1 are without loss of generality, as one can always re-scale the function and adjust the noise variance σ2 accordingly.
",2.2. Technical Assumptions,0,[0]
"Next, we give some high-probability assumptions on the random function f itself.
Assumption 2.",2.2. Technical Assumptions,1,"['Next, we give some high-probability assumptions on the random function f itself.']"
"There exists a constant δ1 ∈ (0, 1) such that, with probability at least 1− δ1, we have the following:
1.",2.2. Technical Assumptions,1,"['There exists a constant δ1 ∈ (0, 1) such that, with probability at least 1− δ1, we have the following: 1.']"
"The function f has a unique maximizer x∗ such that
f(x∗) ≥ f(x′) + (7)
for any local maximum x′ that differs from x∗, for some constant > 0.
2.",2.2. Technical Assumptions,0,[0]
"The function f is twice differentiable;
3.",2.2. Technical Assumptions,0,[0]
"The function f and its first two derivatives are bounded:
|f(x)| ≤ c0, |f ′(x)| ≤",2.2. Technical Assumptions,0,[0]
"c1, |f ′′(x)| ≤ c2 (8)
for all x ∈ D and some constants (c0, c1, c2).",2.2. Technical Assumptions,0,[0]
"This implies that f is c1-Lipschitz continuous, and f ′ is c2-Lipschitz continuous.
",2.2. Technical Assumptions,1,"['This implies that f is c1-Lipschitz continuous, and f ′ is c2-Lipschitz continuous.']"
"The assumption of a unique maximizer holds with probability one in most non-trivial cases (de Freitas et al., 2012), and
(7) simply formally defines the gap to the second-highest peak.",2.2. Technical Assumptions,0,[0]
"Moreover, given twice differentiability, the remaining conditions in (8) are very mild, only requiring that the function value and its derivatives are bounded, and formally defining the corresponding constants.
",2.2. Technical Assumptions,0,[0]
"Next, we provide assumptions regarding the derivatives of f and the resulting Taylor expansions (typically around the optimizer x∗).",2.2. Technical Assumptions,0,[0]
"We adopt slightly different assumptions for the upper and lower bounds, starting with the former.",2.2. Technical Assumptions,0,[0]
Assumption 3.,2.2. Technical Assumptions,0,[0]
"There exist constants δ2 ∈ (0, 1) and ρ0 ∈( 0, 12 )
such that conditioned on the events in Assumption 2, we have with probability at least 1 − δ2 that one of the following is true:
1.",2.2. Technical Assumptions,0,[0]
"The maximizer is at an endpoint (i.e., x∗ = 0 or x∗ = 1), and satisfies the following locally linear behavior: For all ξ ∈",2.2. Technical Assumptions,0,[0]
"[0, ρ0] (if x∗ = 0) or ξ ∈",2.2. Technical Assumptions,0,[0]
"[−ρ0, 0] (if x∗ = 1), it holds that
f(x∗)− c1|ξ| ≥ f(x∗ + ξ) ≥ f(x∗)− c1|ξ| (9) for some constants c1 ≥ c1 > 0.
2.",2.2. Technical Assumptions,0,[0]
"The maximizer satisfies x∗ ∈ (ρ0, 1− ρ0), and f satisfies the following locally quadratic behavior: For all ξ ∈",2.2. Technical Assumptions,0,[0]
"[−ρ0, ρ0], we have f(x∗)− c2ξ2 ≥ f(x∗ + ξ) ≥ f(x∗)− c2ξ2 (10)
for some constants c2 ≥ c2 > 0.
",2.2. Technical Assumptions,0,[0]
"This assumption is near-identical to the main assumption adopted in the noiseless setting (de Freitas et al., 2012), and is also mild given the assumption of twice differentiability.",2.2. Technical Assumptions,0,[0]
"Indeed, (9) and (10) amount to standard Taylor expansions, with the assumptions c1 > 0 and c2 > 0 only requiring non-vanishing gradient at the endpoint (first case) or non-vanishing second derivative at the function maximizer (second case).",2.2. Technical Assumptions,0,[0]
"These conditions typically hold with probability one (de Freitas et al., 2012).
",2.2. Technical Assumptions,0,[0]
The following assumption will be used for the lower bound.,2.2. Technical Assumptions,0,[0]
Assumption 4.,2.2. Technical Assumptions,0,[0]
"There exists constants δ′2 ∈ (0, 1) and ρ0 ∈( 0, 12 )
such that conditioned on the events in Assumption 2, both of the following hold with probability at least 1− δ′2:
1.",2.2. Technical Assumptions,0,[0]
For any x ∈ D and ξ ∈,2.2. Technical Assumptions,0,[0]
"[−ρ0, ρ0] for which x+ξ ∈ D, we have
ξ ·f ′(x)+c′2ξ2 ≤ f(x+ξ)−f(x) ≤ ξ ·f ′(x)+c′2ξ2.",2.2. Technical Assumptions,0,[0]
"(11) for some (possibly negative) constants c′2, c ′ 2.
2.",2.2. Technical Assumptions,0,[0]
"The maximizer satisfies x∗ ∈ (ρ0, 1− ρ0), and f satisfies the following for all ξ ∈",2.2. Technical Assumptions,0,[0]
"[−ρ0, ρ0]: f(x∗)− c2ξ2 ≥ f(x∗ + ξ) ≥ f(x∗)− c2ξ2 (12)
for some constants c2 ≥ c2 > 0.
",2.2. Technical Assumptions,0,[0]
"x
0 0.2 0.4 0.6 0.8 1
-1.5
-1
-0.5
0
0.5
1
1.5
2 2⇢0
Locally Quadratic
Slope  c1
Gap ✏
Range  2c0
Figure 1.",2.2. Technical Assumptions,0,[0]
"Illustration of some of the main assumptions: The function is bounded within [−c0, c0] and its derivative within [−c1, c1], the gap to the second highest peak is at least , and the function is locally quadratic for points within a distance ρ0 of the maximizer.
",2.2. Technical Assumptions,0,[0]
"The first part is similar to (10), but performs a Taylor expansion around an arbitrary point rather than the specific point x∗, and the second part is precisely (10).",2.2. Technical Assumptions,0,[0]
"Note, however, that here we are assuming both of two conditions to hold, rather than one of two.",2.2. Technical Assumptions,0,[0]
"Hence, we are implicitly assuming that the first item of Assumption 3 does not have a significant probability of occurring.",2.2. Technical Assumptions,0,[0]
"For stationary kernels, the only situations where an endpoint has a high probability of being optimal are those where f varies very slowly (e.g., the SE kernel with a larger lengthscale than the domain width).",2.2. Technical Assumptions,0,[0]
"Such functions are of limited practical interest.
",2.2. Technical Assumptions,0,[0]
"Similarly to the noiseless setting (de Freitas et al., 2012), all of the above assumptions hold for the SE kernel, as well as the Matérn-ν kernel with ν > 2, with the added caveat that δ′2 in Assumption 4 is a function of the lengthscale and cannot be chosen arbitrarily.",2.2. Technical Assumptions,0,[0]
"Specifically, a smaller lengthscale implies a smaller value of δ′2.",2.2. Technical Assumptions,0,[0]
"In contrast, δ1 and δ2 in Assumptions 2 and 3 can be made arbitrary small by suitably changing the constants , c0, c1, c2, ρ0, and so on.
",2.2. Technical Assumptions,0,[0]
An illustration of some of the main assumptions and their associated constants is given in Figure 1.,2.2. Technical Assumptions,0,[0]
"Our upper bound is formally stated as follows.
",3. Upper Bound,0,[0]
Theorem 1.,3. Upper Bound,0,[0]
"(Upper Bound) Consider the problem of BO in one dimension described in Section 2.1, with time horizon T and noise variance σ2 satisfying σ2 ≥ cσ
T 1−ζ for some cσ >
0 and ζ > 0.",3. Upper Bound,0,[0]
"Under Assumptions 1, 2, and 3, there exists an algorithm satisfying the following: With probability at least 1 − δ1 − δ2 (with respect to the Gaussian process f ), the average cumulative regret (averaged over the noisy
samples) satisfies E[RT ] ≤ C ( 1 + σ",3. Upper Bound,0,[0]
√ T log T ) .,3. Upper Bound,0,[0]
"(13)
Here δ1 and δ2 are defined in Assumptions 2 and 3, and C depends only on the constants therein and (cσ, ζ).
",3. Upper Bound,0,[0]
"The assumption that σ2 ≥ cσ T 1−ζ for some (cσ, ζ) is very mild, since typically σ2 is constant with respect to T .",3. Upper Bound,0,[0]
"The proof of Theorem 1 extends immediately to a high probability guarantee with respect to both f and the noisy samples (i.e., holding with probability 1−δ1−δ2−δ for δ in Lemma 1 below).",3. Upper Bound,0,[0]
"We have stated the above form for consistency with the lower bound, which will be given in Section 4.",3. Upper Bound,0,[0]
"The algorithm considered in the proof of Theorem 1 is described informally in Algorithm 1; the details will be established throughout the proof of Theorem 1, and a complete description is given in Appendix B.
Algorithm 1 Informal description of our algorithm, based on reducing uncertainty in epochs via repeated sampling.
",3.1. High-Level Description of the Algorithm,0,[0]
"Require: Domain D, GP prior (µ0, k0), discrete subdomain L ⊆ D, time horizon T .
",3.1. High-Level Description of the Algorithm,0,[0]
"1: Initialize t = 1, epoch number i = 1, potential maximizers M(0)",3.1. High-Level Description of the Algorithm,0,[0]
"= L, and target confidence η(0).",3.1. High-Level Description of the Algorithm,0,[0]
2: while less than T samples have been taken do 3: Set η(i) = 12η(i−1).,3.1. High-Level Description of the Algorithm,0,[0]
"4: Sample each point within a subset L(i) ⊆ L repeat-
edlyK(i) times, where L(i) andK(i) are chosen such that after this sampling, all points x ∈M(i−1) satisfy upper and lower confidence bounds of the form
LCBt(x) ≤ f(x) ≤ UCBt(x),
with the gap between the two bounded by |UCBt(x)− LCBt(x)| ≤ 2η(i).
5: Update the set of potential maximizers:
M(i) = { x ∈M(i−1) :
UCBt(x) ≥ max x′∈∈M(i−1)
LCBt(x ′) } .
6: Increment i. 7: end while
As in the noiseless setting (de Freitas et al., 2012), the idea is to operate in epochs and sample a set of increasingly closelypacked points L(i) to reduce the posterior variance, but only within a set of potential maximizers that are updated according to the confidence bounds.",3.1. High-Level Description of the Algorithm,0,[0]
"As a simple means of bringing the effective noise level down, we perform resampling, i.e.,
sampling the same point K(i) times consecutively.",3.1. High-Level Description of the Algorithm,0,[0]
"In each epoch, we sample enough to be able to produce upper and lower confidence bounds UCBt(x) and LCBt(x) that differ by at most a target value 2η(i) within M(i−1), and then the target is halved for the next epoch.
",3.1. High-Level Description of the Algorithm,0,[0]
"We do not expect our algorithm to perform well in practice by any means, but it still suffices for our purposes in establishing O( √ T log T ) regret.",3.1. High-Level Description of the Algorithm,0,[0]
"Indeed, we have made no attempt to optimize the corresponding constant factors, and doing so would require more sophisticated techniques.",3.1. High-Level Description of the Algorithm,0,[0]
"Moreover, the quantities L(i), K(i), UCBt, and LCBt in Algorithm 1 are chosen as functions of both the kernel and the constants appearing in our assumptions, which limits the algorithm’s practical utility even further.",3.1. High-Level Description of the Algorithm,0,[0]
"Note, however, that these constants are merely a function of the kernel, and that suitable bounds suffice in place of exact values (e.g., lower bound on ρ0, upper bound on c0, etc.).
",3.1. High-Level Description of the Algorithm,0,[0]
"While our algorithm assumes a known time horizon T (which is used when selecting K(i); see Appendix B), this assumption can easily be dropped via a standard doubling trick.",3.1. High-Level Description of the Algorithm,0,[0]
The details are given in Appendix A.,3.1. High-Level Description of the Algorithm,0,[0]
Here we present two very standard auxiliary lemmas.,3.2. Auxiliary Lemmas,0,[0]
"We begin with a simpler version of the conditions of Srinivas et al. (Srinivas et al., 2010) guaranteeing that the posterior mean and variance provide valid confidence bounds with high probability.",3.2. Auxiliary Lemmas,0,[0]
"The reason for being slightly simpler is that we are considering a fixed time horizon.
",3.2. Auxiliary Lemmas,0,[0]
Lemma 1.,3.2. Auxiliary Lemmas,0,[0]
"Fix δ ∈ (0, 1).",3.2. Auxiliary Lemmas,0,[0]
"For any finite set of points L ⊆ D and time horizon T , under the choice βT = 2 log |L|·T δ , it holds that
|f(x)− µt(x)| ≤ β1/2T σt(x), ∀x ∈ L, t = 1, . . .",3.2. Auxiliary Lemmas,0,[0]
", T, (14)
with probability at least 1− δ.
",3.2. Auxiliary Lemmas,0,[0]
Proof.,3.2. Auxiliary Lemmas,0,[0]
"It was shown in (Srinivas et al., 2010) that for fixed x and t, the event |f(x)− µt(x)| ≤ β1/2T σt(x) holds with probability at least 1− e−βT /2.",3.2. Auxiliary Lemmas,0,[0]
"the lemma follows by substituting the choice of βT and taking the union bound over the |L| · T values of x and t.
The following lemma is also standard, and has been used (implicitly or explicitly) in the study of multiple algorithms that eliminate suboptimal points based on confidence bounds (de Freitas et al., 2012; Contal et al., 2013; Bogunovic et al., 2016b).",3.2. Auxiliary Lemmas,0,[0]
"For completeness, we give a short proof.
",3.2. Auxiliary Lemmas,0,[0]
Lemma 2.,3.2. Auxiliary Lemmas,0,[0]
"Suppose that at time t, for all x within a set of points L̃ ⊆ D, it holds that
LCBt(x) ≤ f(x) ≤ UCBt(x) (15)
for some bounds UCBt and LCBt such that
max x∈L̃ ∣∣UCBt(x)− LCBt(x)∣∣ ≤ 2η.",3.2. Auxiliary Lemmas,0,[0]
"(16) Then any point x ∈ L̃ satisfying f(x) < maxx′∈L̃ f(x′)− 4η must also satisfy
UCBt(x) <",3.2. Auxiliary Lemmas,0,[0]
max x∈L̃ LCBt(x).,3.2. Auxiliary Lemmas,0,[0]
"(17)
That is, any 4η-suboptimal point can be ruled out according to the confidence bounds (15).
",3.2. Auxiliary Lemmas,0,[0]
Proof.,3.2. Auxiliary Lemmas,0,[0]
"We have
UCBt(x) ≤ LCBt(x)",3.2. Auxiliary Lemmas,0,[0]
"+ 2η (18) ≤ f(x) + 2η (19) < max
x′∈L̃ f(x′)− 2η (20)
≤",3.2. Auxiliary Lemmas,0,[0]
max x′∈L̃ UCBt(x ′)− 2η (21) ≤,3.2. Auxiliary Lemmas,0,[0]
"max x′∈L̃ LCBt(x ′), (22)
where (18) and (22) follow from (16), (19) and (21) follow from the confidence bounds in (15), and (20) follows from the assumption f(x) < maxx′∈L̃ f(x ′)− 4η.",3.2. Auxiliary Lemmas,0,[0]
"Here we provide a high-level outline of the Proof of Theorem 1; the details are given in Appendix B.
Algorithm 1 only samples on a discrete sub-domain L. This set is chosen to be a set of regularly-spaced points that are fine enough to ensure that the cumulative regret with respect to maxx∈L f(x) is within a constant value of the cumulative regret with respect to maxx∈D f(x).",3.3. Outline of Proof of Theorem 1,0,[0]
Working with the finite set L helps to simplify the subsequent analysis.,3.3. Outline of Proof of Theorem 1,0,[0]
"We split the epochs into two classes, which we call early epochs and late epochs.",3.3. Outline of Proof of Theorem 1,0,[0]
"The late epochs are those in which we have shrunk the potential maximizers down enough to be entirely within the locally quadratic region, cf., Figure 1; here we only discuss the second case of Assumption 3, which is the more interesting of the two.",3.3. Outline of Proof of Theorem 1,0,[0]
"Since the width of the locally quadratic region is constant, we can show that this occurs after a finite number of epochs, each lasting for at most O(log T ) time.",3.3. Outline of Proof of Theorem 1,0,[0]
"Hence, even if we naively upper bound the instant regret by 2c0 according to (8), the overall regret incurred within the early epochs is insignificant.
",3.3. Outline of Proof of Theorem 1,0,[0]
"In the later epochs, we exploit the locally quadratic behavior to show that the set of potential maximizers shrinks rapidly, i.e., by a constant factor after each epoch.",3.3. Outline of Proof of Theorem 1,0,[0]
"As a result, we can let the repeatedly-sampled set L(i) in Algorithm 1 lie within a given interval that similarly shrinks, thereby controlling the number of samples we need to take in the epoch.
",3.3. Outline of Proof of Theorem 1,0,[0]
"By Lemma 2, after we attain uniform η(i)-confidence, the instant regret incurred at each time thereafter is at most 4η(i).",3.3. Outline of Proof of Theorem 1,0,[0]
"Using the fact that η(i) = η(0)2−i and summing over the epochs, we find that the overall regret behaves as in (13).
",3.3. Outline of Proof of Theorem 1,0,[0]
A notable difficulty that we omitted above is how we attain the confidence bounds in order to update the potential maximizers M(i).,3.3. Outline of Proof of Theorem 1,0,[0]
"While we directly apply Lemma 1 for the points that were repeatedly sampled, we found it difficult to do this for the non-sampled points.",3.3. Outline of Proof of Theorem 1,0,[0]
"For those, we instead use Lipschitz properties of the function.",3.3. Outline of Proof of Theorem 1,0,[0]
"In the early epochs, we use the global Lipschitz constant c1 from Assumption 2, whereas in the later epochs, we find a considerably smaller Lipschitz constant due to the locally quadratic behavior.",3.3. Outline of Proof of Theorem 1,0,[0]
Our lower bound is formally stated as follows.,4. Lower Bound,0,[0]
Theorem 2.,4. Lower Bound,0,[0]
"(Lower Bound) Consider the one-dimensional BO problem from Section 2.1, with time horizon T and noise variance σ2 satisfying σ2 ≤ c′σT 1−ζ ′",4. Lower Bound,0,[0]
for some c′σ > 0 and ζ ′,4. Lower Bound,0,[0]
> 0.,4. Lower Bound,0,[0]
"Under Assumptions 1, 2, and 4, any algorithm must yield the following: With probability at least 1−δ1−δ′2 (with respect to the Gaussian process f ), the average cumulative regret (averaged over the noisy samples) satisfies
E[RT ]",4. Lower Bound,0,[0]
≥ C ′,4. Lower Bound,0,[0]
( 1 + σ √ T ) .,4. Lower Bound,0,[0]
"(23)
Here δ1 and δ′2 are defined in Assumptions 2 and 4, and C ′ depends only on the constants therein and (c′σ, ζ ′).
",4. Lower Bound,0,[0]
The assumption that σ2 ≤ c′σT 1−ζ ′,4. Lower Bound,0,[0]
"for some (c′σ, ζ ′) is very mild, since typically σ2 is constant with respect to T .",4. Lower Bound,0,[0]
The assumption is required to avoid (23) contradicting the trivial O(T ) upper bound.,4. Lower Bound,0,[0]
"We also note that Theorem 2 immediately implies an Ω ( 1 + σ √ T )
lower bound on the expected regret E[RT ] with respect to both f and the noisy samples, as long as 1−δ1−δ′2 > 0.",4. Lower Bound,0,[0]
"As discussed following Assumption 4, the latter condition is mild.
",4. Lower Bound,0,[0]
"In the remainder of the section, we introduce some of the main tools and ideas, and then outline the proof.",4. Lower Bound,0,[0]
"We note that E[RT ] = Ω(1) is trivial, as the average regret of the first sample alone is lower bounded by a constant.",4. Lower Bound,0,[0]
"As a result, we only need to show that E[RT ]",4. Lower Bound,0,[0]
= Ω(σ √ T ).,4. Lower Bound,0,[0]
"Recall that f is a one-dimensional GP on [0, 1] with a stationary kernel k(x, x′).",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"We fix ∆ > 0, and think of the GP as being generated by the following procedure:
1.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"Generate a GP f0 with the same kernel on the larger domain [−∆, 1 + ∆];
2.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"Randomly shift f0 along the x-axis by +∆ or−∆ with equal probability, to obtain f̃ ;
3.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
Let f(x) = f̃(x) for x ∈,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"[0, 1].
Since the kernel is stationary, the shifting does not affect the distribution, so the induced distribution of f is indeed the desired GP on [0, 1].
",4.1. Reduction to Binary Hypothesis Testing,0,[0]
We consider a genie argument in which f0 is revealed to the algorithm.,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"Clearly this additional information can only help the algorithm, so any lower bound still remains valid for the original setting.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"Stated differently, the algorithm knows that f is either f+ or f−, where
f+(x) =",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"f0(x+ ∆), (24) f−(x)",4.1. Reduction to Binary Hypothesis Testing,0,[0]
= f0(x−∆).,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"(25)
See Figure 2 for an illustrative example.
",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"This argument allows us to reduce the BO problem to a binary hypothesis test with adaptive sampling, as depicted in Figure 3.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"The hypothesis, indexed by v ∈ {−,+}, is that the underlying function is fv .",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"We show that under a suitable choice of ∆, achieving small cumulative regret means that we can construct a decision rule V̂ (x) such that V̂ = v with high probability, i.e., the hypothesis test is successful.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"The contrapositive statement is then that if the hypothesis test cannot be successful, we cannot achieve small cumulative regret, from which it only remains to prove the former.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"This idea was used previously for stochastic convex optimization in (Raginsky & Rakhlin, 2011).
",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"In the remainder of the analysis, we implicitly condition on an arbitrary realization of f0, meaning that all expectations and probabilities are only with respect to the random index V and/or the noise.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"We also assume that f0 satisfies the conditions in Assumptions 1, 2, and 4, which holds with probability at least 1 − δ1 − δ′2.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"For sufficiently small ∆, the same assumptions are directly inherited by f+ and f−.
We henceforth assume that ∆ is indeed sufficiently small; we will verify that this is the case when we set its value.
",4.1. Reduction to Binary Hypothesis Testing,0,[0]
We introduce some further notation.,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"Letting x∗+, x ∗ −, and x∗0 denote the maximizers of f+, f− and f0 (which are unique by Assumption 2), we see that Assumption 4 ensures these are in the interior (0, 1), and hence the optimal values coincide: f+(x∗+) = f−(x ∗ −) = f0(x ∗ 0)",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"=: f
∗. To simplify some of the notation, instead of working with these functions directly, we consider the equivalent problem of minimizing the corresponding regret functions:
r+(x)",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"= f ∗ − f+(x), (26) r−(x)",4.1. Reduction to Binary Hypothesis Testing,0,[0]
= f ∗ − f−(x).,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"(27)
Indeed, since we assume the algorithm knows f0 and hence also the optimal value f∗, it can always choose to transform the samples as y → f∗ − y.",4.1. Reduction to Binary Hypothesis Testing,0,[0]
"In this form, we have the convenient normalization r+(x∗+) =",4.1. Reduction to Binary Hypothesis Testing,0,[0]
r−(x ∗ −) = 0.,4.1. Reduction to Binary Hypothesis Testing,0,[0]
"We first state the following useful properties of r+ and r−.
Lemma 3.",4.2. Auxiliary Lemmas,0,[0]
"The functions r+ and r− constructed above satisfy the following for sufficiently small ∆ under the conditions in Assumptions 2 and 4:
1.",4.2. Auxiliary Lemmas,0,[0]
"We have for all x ∈ D that
r+(x) < c2∆ 2 =⇒ r−(x)",4.2. Auxiliary Lemmas,0,[0]
"> c2∆2, (28)
where c2 is defined in Assumption 4.
2.",4.2. Auxiliary Lemmas,0,[0]
There exists a constant c′ > 0,4.2. Auxiliary Lemmas,0,[0]
"such that, for all x ∈ D,
|r+(x)− r−(x)| ≤ c′",4.2. Auxiliary Lemmas,0,[0]
( ∆|x− x∗0|+ ∆2 ) .,4.2. Auxiliary Lemmas,0,[0]
"(29)
3.",4.2. Auxiliary Lemmas,0,[0]
There exists a constant c′′ > 0,4.2. Auxiliary Lemmas,0,[0]
"such that, for all x ∈ D,
r+(x) ≥ c′′((x− x∗0) + ∆)2, r−(x) ≥ c′′((x− x∗0)−∆)2.",4.2. Auxiliary Lemmas,0,[0]
"(30)
Proof.",4.2. Auxiliary Lemmas,0,[0]
"See Appendix C.
The first part states that any point can be better than c2∆ 2- optimal for at most one of the two functions, the second part shows that the two functions are close for points near x∗0, and the third part shows that the instant regret is lower bounded by a quadratic function.
",4.2. Auxiliary Lemmas,0,[0]
"The first part of the Lemma 3 allows us to bound the cumulative regret using Fano’s inequality for binary hypothesis testing with adaptive sampling (Raginsky & Rakhlin, 2011).",4.2. Auxiliary Lemmas,0,[0]
"This inequality lower bounds the success probability of such a hypothesis test in terms of a mutual information quantity (Cover & Thomas, 2001).",4.2. Auxiliary Lemmas,0,[0]
"The resulting lower bound on regret is stated in the following; it is worth noting that the consideration of cumulative regret here provides a distinction from the analogous bound on the instant regret in (Raginsky & Rakhlin, 2011).
",4.2. Auxiliary Lemmas,0,[0]
Lemma 4.,4.2. Auxiliary Lemmas,0,[0]
"Under the preceding setup, we have E[RT ] ≥",4.2. Auxiliary Lemmas,0,[0]
"c2T∆2 ·H−12 ( log 2− I(V ;x,y) ) , (31)
where V is equiprobable on {+,−}, and (x,y) are the selected points and samples when the minimization algorithm is applied to rV .",4.2. Auxiliary Lemmas,0,[0]
Here H−12 :,4.2. Auxiliary Lemmas,0,[0]
"[0, log 2] → [ 0, 12 ] is the functional inverse of the binary entropy function H2(α)",4.2. Auxiliary Lemmas,0,[0]
"= α log 1 α + (1− α) log 11−α .
",4.2. Auxiliary Lemmas,0,[0]
"Since this result is particularly fundamental to our analysis, we provide a proof at the end of this section.",4.2. Auxiliary Lemmas,0,[0]
"Here we provide a high-level outline of the proof of Theorem 2; the details are given in Appendix D.
Once the lower bound in Lemma 4 is established, the main technical challenge is upper bounding the mutual information.",4.3. Outline of Proof of Theorem 2,0,[0]
"A useful property called tensorization (e.g., see (Raginsky & Rakhlin, 2011)) allows us to simplify the mutual information with the vectors (x,y) to a sum of mutual informations containing only a single pair (xt, yt): I(V ;x,y) ≤∑Tt=1 I(V ; yt|xt).",4.3. Outline of Proof of Theorem 2,0,[0]
"Each such mutual information term I(V ; yt|xt) can further be upper bounded by the KL divergence (Cover & Thomas,
2001) between the conditional output distributions corresponding to r+ and r−, which in turn equals (r+(x)−r−(x))2 2σ2",4.3. Outline of Proof of Theorem 2,0,[0]
when xt = x.,4.3. Outline of Proof of Theorem 2,0,[0]
"By substituting the property (29) given in Lemma 3, we find that I(V ;x,y) is upper bounded by a constant times 1σ2 ( ∆2E [∑T t=1 |xt − x∗0|2 ] + T∆4 ) .",4.3. Outline of Proof of Theorem 2,0,[0]
"If we can further upper bound I(V ;x,y) by a constant in (0, log 2), then (31) establishes an Ω(T∆2) lower bound.
",4.3. Outline of Proof of Theorem 2,0,[0]
We proceed by considering the cases E[RT ],4.3. Outline of Proof of Theorem 2,0,[0]
≥ c′′T∆2 and E[RT ],4.3. Outline of Proof of Theorem 2,0,[0]
"< c′′T∆2 separately, with c′′ given in (30).",4.3. Outline of Proof of Theorem 2,0,[0]
"The former case will immediately give the lower bound in Theorem 2 when we set ∆, whereas in the latter case, we can use (30) to show that E",4.3. Outline of Proof of Theorem 2,0,[0]
[∑T t=1 |xt,4.3. Outline of Proof of Theorem 2,0,[0]
"− x∗0|2 ] is upper bounded by a constant times T∆2, which means that the desired mutual information upper bound (see the previous paragraph) is attained under a choice of ∆ scaling as ( σ2
T
)1/4 .",4.3. Outline of Proof of Theorem 2,0,[0]
"Under
this choice, the lower bound E[RT ] = Ω(T∆2) evaluates to Ω(σ √ T ), as required.",4.3. Outline of Proof of Theorem 2,0,[0]
"As mentioned above, the proof of Lemma 4 follows along the lines of (Raginsky & Rakhlin, 2011), which in turn builds on previous works using Fano’s inequality to establish minimax lower bounds in statistical estimation problems; see for example (Yu, 1997).
",4.4. Proof of Lemma 4,0,[0]
"In the following, we useRT,+ = ∑T t=1 r+(xt) andRT,− =∑T
t=1 r−(xt) to denote the cumulative regret associated with r+ and r−, respectively, and we generically write RT,v to denote one of the two with v ∈ {+,−}.",4.4. Proof of Lemma 4,0,[0]
"We first use Markov’s inequality to write
E[RT ] ≥ (1− α)c2T∆2 · P[RT ≥ (1− α)c2T∆2] (32)
for any α ∈ (0, 1).",4.4. Proof of Lemma 4,0,[0]
"We proceed by analyzing the probability on the right-hand side.
",4.4. Proof of Lemma 4,0,[0]
"Recall that V is equiprobable on {+,−}, and (x,y) are generated by running the optimization algorithm on rV .",4.4. Proof of Lemma 4,0,[0]
"Given the sequence of inputs x, let V̂ be the index v̂ ∈ {+,−} with the lower cumulative regret RT,v̂ = ∑T t=1 rv̂(xt).",4.4. Proof of Lemma 4,0,[0]
"By Lemma 3, RT can be less than c2T∆ 2 for at most one of the two functions, and hence, ifRT,v ≤ (1−α)c2T∆2 then we must have V̂ = v. Therefore,
Pv [ RT ≥ (1− α)c2T∆2 ] ≥ Pv[V̂ 6= v], (33)
where, here and subsequently, Pv and Ev denote probabilities and expectations when the underlying instant regret function is rv (i.e., the underlying function that the algorithm seeks to maximize is fv).
",4.4. Proof of Lemma 4,0,[0]
"Continuing, we can lower bound the probability appearing
in (32) as follows:
P[RT ≥ (1− α)c2T∆2]
= 1
2 ∑ v∈{+,−} Pv [ RT ≥ (1− α)c2T∆2 ] (34)
",4.4. Proof of Lemma 4,0,[0]
"≥ 1 2 ∑ v∈{+,−} Pv[V̂ 6= v] (35)
≥ H−12",4.4. Proof of Lemma 4,0,[0]
"( log 2− I(V ;x,y) ) , (36)
where (35) follows from (33), and (36) follows from Fano’s inequality for binary hypothesis testing with adaptive sampling (see Eq. (22) and (24) of (Raginsky & Rakhlin, 2011)).",4.4. Proof of Lemma 4,0,[0]
"The proof is completed by combining (32) and (36), and recalling that α can be arbitrarily small.",4.4. Proof of Lemma 4,0,[0]
"We have established tight scaling laws on the regret for Bayesian optimization in one dimension, showing that the optimal scaling is Ω( √ T ) and O( √ T log T ) under mild technical assumptions on the kernel.",5. Conclusion and Discussion,0,[0]
"Our results highlight some limitations of the widespread upper bounds based on the information gain, as well as providing cases where the noisy Bayesian setting is provably more difficult than its non-Bayesian RKHS counterpart.
",5. Conclusion and Discussion,0,[0]
"An immediate direction for further work is to sharpen the constant factors in the upper and lower bounds, and to establish whether the upper bound is attained by any algorithm that can also provide state-of-the-art performance in practice.",5. Conclusion and Discussion,0,[0]
"We re-iterate that our algorithm is certainly not suitable for this purpose, as its cumulative regret contains large constant factors, and the algorithm makes use of a variety of specific constants present in the assumptions (though they are merely a function of the kernel).
",5. Conclusion and Discussion,0,[0]
"We expect our techniques to extend to any constant dimension d ≥ 1; the main ideas from the noiseless upper bound still apply (de Freitas et al., 2012), and in the lower bound we can choose an arbitrary single dimension and introduce a random shift in that direction as per Section 4.1.",5. Conclusion and Discussion,0,[0]
"While these extensions may still yield √ T poly(logT ) regret, the dependence on d would be exponential or worse in the upper bound, but constant in the lower bound, with the latter dependence certainly being suboptimal.",5. Conclusion and Discussion,0,[0]
"Multi-dimensional lower bounding techniques based on Fano’s inequality (Raginsky & Rakhlin, 2011) may improve the latter to poly(d), but overall, attaining a sharp joint dependence on T and d appears to require different techniques.
Acknowledgments.",5. Conclusion and Discussion,0,[0]
I would like to thank Ilija Bogunovic for his helpful comments and suggestions.,5. Conclusion and Discussion,0,[0]
This work was supported by an NUS startup grant.,5. Conclusion and Discussion,0,[0]
"We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise.",abstractText,0,[0]
"We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time T behaves as Ω( √ T ) and O( √ T log T ).",abstractText,0,[0]
"This gives a tight characterization up to a √ log T factor, and includes the first non-trivial lower bound for noisy BO.",abstractText,0,[0]
"Our assumptions are satisfied, for example, by the squared exponential and Matérn-ν kernels, with the latter requiring ν > 2.",abstractText,0,[0]
"Our results certify the near-optimality of existing bounds (Srinivas et al., 2009) for the SE kernel, while proving them to be strictly suboptimal for the Matérn kernel with ν > 2.",abstractText,0,[0]
Tight Regret Bounds for Bayesian Optimization in One Dimension,title,0,[0]
