4	41	Table 1 shows examples typical of the problems and proposed solutions found in troubleshootingoriented online forums.
6	51	Solution (1) is probably easiest to implement in terms of user time, effort, and expertise; solution (3) is most complex (i.e., the user should understand what signal timing is and then try to establish whether it is within the specification of the monitor), whereas solution (2) is somewhere in between.
8	101	In this paper, we present models to automatically predict the complexity of troubleshooting solutions, which we argue could improve user experience, and potentially help solve the problem faster (e.g., by prioritizing easier solutions).
20	41	We also show experimentally that users agree in their intuitions about the relative complexity of different solutions to the same problem.
32	17	We show that a supervised ranking approach using features based on the predictions of our generative models is on par with human performance on this task while outperforming competitive baselines based on length and readability of the solution text.
60	68	For our experiments, we collected 300 problems1 and their solutions from multiple web sites including the computing support pages of Microsoft, Apple, HP, as well as amateur computer help websites such as www.computerhope.com.
61	26	The problems were mostly frequently asked questions (FAQs) referring to malfunctioning personal computers and smart phones.
68	28	All words in the corpus were lemmatized and html links and numbers were replaced with placeholders.
91	64	The last column in Table 2 reports the agreement between our annotator rankings and the original ordering of solutions in the FAQ data.
93	23	This implies that the ordering may not be strictly increasing in complexity in our dataset and that our models should allow for some flexibility during learning.
95	30	For instance, annotators disagreed when multiple solutions were of similar complexity.
111	68	This model infers the vocabulary associated with a complexity level and a distribution over the numerical positions in a solution set where such a complexity level is likely to occur.
124	19	This distribution allows each problem to take a different preference and mix of complexity levels for its solutions.
129	29	The words in the solution are generated by first drawing a switch value for each word indicating if the word came from the problem’s technical or complexity vocabulary.
166	20	We assume the canonical order is the strictly increasing (1, 2, .., L) order.
171	23	At the solution set level, we draw a multinomial distribution λ over the vocabulary and a multinomial distribution θ for the proportion of L levels for this problem.
177	19	The z assignments are deterministically computed by ordering the elements of b according to the permutation defined by v. Given the conditional independencies of our model, the posterior is proportional to: L∏ m=1 [P (φm|α)]× L−1∏ r=1 [P (ρr|µ0, v0r)]× P (ψ|δ0, δ1) × N∏ i=1 [P (θi|β)× P (λi|ω)× P (vi|ρ)× P (bi|θi)] × N∏ i=1 NPi∏ j=1 |xij |∏ k=1 [P (sijk|ψ)P (wijk|sijk, φzij , λi)] where L is the number of complexity levels, N the total problems in the training corpus, NPi the size of solution set for problem Pi, and |xij | the number of words in solution xij .
181	17	For a chosen solution set Si, the sampler draws NPi levels (bi), one at a time conditioned on the assignments to all other hidden variables of the model.
186	18	We sample from the unnormalized GMM0 distribution using slice sampling.
197	18	Using these complexity vocabularies, our models can compute the expected complexity for any solution text, x.
200	18	We find that the model is able to distinguish intuitively complex solutions from simpler ones.
202	26	As mentioned earlier, our models only observe the relative ordering of solutions to individual problems; the relative complexity of two solutions from different problems is not known.
209	32	We evaluated our models by presenting them with a randomly permuted set of solutions to a problem and examining the accuracy with which they reorder them from least to most complex.
238	20	We obtained eight syntactic features based on the number of nouns, verbs, adjectives and adverbs, prepositions, pronouns, wh-adverbs, modals, and punctuation.
271	51	With regard to feature combinations, we observe that both models yield better performance when combined with Length or SynSem.
274	21	These results suggest that the solution ordering task is challenging with several factors influencing how a solution is perceived: the words used and their meaning, the writing style of the solution, and the amount of detail present in it.
282	29	Likewise, we also computed how often the models correctly predict the most complex solution.
283	37	With Random rankings, the easiest and most difficult solutions are predicted correctly 15% of the time.
284	50	Getting both correct for a single problem happens only 3% of the time.
288	86	One reason for this could be that there are multiple easy solutions to try out but the most difficult one is probably more unique and so easier to identify.
