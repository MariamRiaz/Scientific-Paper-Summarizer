20	4	It turned out that PCN always outperformed the plain model, and its accuracy tended to improve given more cycles of computation over time.
22	4	As we did not attempt to optimize the performance by trying many learning parameters or model architectures, there is much room for future studies (e.g. Han et al. (2018)) to further improve or extend the model on the basis of a similar notion.
54	6	The weights of feedforward connections from layer l − 1 to layer l are denoted as Wl−1,l.
59	3	To minimize el−1(t), lets define a loss as the sum of the squared errors normalized by the variance of the representation, σ2l−1, as in Eq.
66	4	Similar to feedforward process, the error is minimized by gradient descent, where the gradient of el(t) with respect to rl(t) is as Eq.
71	5	A rectified linear unit (ReLU) (Nair & Hinton, 2010) converts Eqs.
88	5	1a shows a 9-layer PCN, running recursive bottom-up and top-down processing based on predictive coding.
89	3	In PCN, feedback connections from one layer to its lower layer were constrained to be the transposed convolution (Dumoulin & Visin, 2016) which is the transpose of the feedforward counterparts.
90	3	As such, both feedforward and feedback connections encoded spatial filters.
92	4	The weights of feedback connections had the identical dimension as the transposed weights of feedforward connections.
98	3	In other words, the weights of feedback connections were the transposed weights of feedforward connections.
104	2	For t = 1, PCN first runs a feedback process and then a feedforward process to update the representations in the hierarchy.
108	2	This procedure is repeated over time as shown in Fig.
130	2	The learning rate was initialized as 0.01 and was divided by 10 when the error reached the plateau after training for 80, 140, 200 epochs.
134	2	Meanwhile, increasing the recursive cycles tends to make PCN converge faster.
147	4	Nevertheless, the top-down process in PCN was able to reconstruct the input with high accuracy.
148	5	Although this was sort of expected for PCN with weight sharing, reconstruction was also reasonable even for PCN without weight sharing (Fig.
153	2	However, if the input is a video, CNN processes every video frame with a feedforward pass.
159	2	To validate the hyper parameters, we randomly selected 400 samples per class from the training set and 200 samples per class from the extra set for validation, as in (Goodfellow et al., 2013).
165	9	Like what we found for the CIFAR dataset, PCN always outperformed the plain CNN counterpart.
166	3	The MNIST dataset consists of hand written digits 0-9.
169	5	For this dataset, the network architecture and training procedure were the same as for SVHN.
173	6	When it is trained for image classification, the model dynamically refines its representation of the input image towards more accurate and definitive recognition.
183	15	For image classification, PCN takes an image as the input for all cycles of its recursive computation, while the errors of top-down prediction sent to the first hidden layer vary across cycles or in time.
184	3	When the input is not a static image but a video, the input to the first hidden layer represents the errors of prediction of the present video frame given the models representations from the past frames.
185	7	This would enable the model to compute and learn representations of both spatial and temporal information in videos, which is an important aspect that awaits to be explored in future studies.
186	5	As an initial step to explore predictive coding in computer vision, it was our intention to start and compare with models with a basic CNN architecture (e.g. VGG) in order to focus on evaluation of the value of using predictive coding as a computational mechanism.
187	24	We expect that such predictive coding based computation can also be used to other network structures, e.g. ResNet and DenseNet.
188	143	In a recent work (Han et al., 2018), a variant of PCN with a deeper structure and residual connections, has been developed and tested with ImageNet.
189	123	It used notably fewer layers and parameters and but achieved competitive performance compared to classical and state-of-the-art models.
