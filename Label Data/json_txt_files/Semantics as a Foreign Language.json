{"sections": [{"text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2412\u20132421 Brussels, Belgium, October 31 - November 4, 2018. c\u00a92018 Association for Computational Linguistics\n2412"}, {"heading": "1 Introduction", "text": "Many sentence-level representations were developed with the goal of capturing the sentence\u2019s proposition structure and making it accessible for downstream applications (Montague, 1973; Carreras and Ma\u0300rquez, 2005; Banarescu et al., 2013; Abend and Rappoport, 2013). See Abend and Rappoport (2017), for a recent survey.\nWhile syntactic grammars (Marcus et al., 1993; Nivre, 2005) induce a rooted tree structure over the sentence by connecting verbal predicates to their arguments, these semantic representations often take the form of the more general labeled graph structure, and aim to capture a wider notion of propositions (e.g, nominalizations, adjectivals, or appositives). In particular, we will focus on the three graph-based semantic representations collected in the Broad-Coverage Semantic Dependency Parsing SemEval shared task (SDP) (Oepen et al., 2015): (1) DELPH-IN Bi-\n\u2217Work performed while at Bar-Ilan University.\nLexical Dependencies (DM) (Flickinger, 2000),1 (2) Enju Predicate-Argument Structures (PAS) (Miyao et al., 2014), and (3) Prague Semantic Dependencies (PSD) (Hajic et al., 2012). These annotations have garnered recent attention (e.g., (Buys and Blunsom, 2017; Peng et al., 2017a)), and were consistently annotated in parallel on over more than 30K sentences of the Wall Street Journal corpus (Charniak et al., 2000).\nIn this work we take a novel approach to graph parsing, casting sentence-level semantic parsing as a multilingual machine-translation task (MT). We deviate from current graph-parsing approaches to SDP (Peng et al., 2017a) by treating the different semantic formalisms as foreign target dialects, while having English a as a common source language (Section 3). Subsequently, we devise a neural MT sequence-to-sequence framework that is suited for the task.\nIn order to apply sequence-to-sequence models for structured prediction, a linearization function is required to interpret the model\u2019s sequential input and output. Initial work on structured prediction sequence-to-sequence modeling has focused on tree structures (Vinyals et al., 2015; Aharoni and Goldberg, 2017), as these are quite easy to linearize using the bracketed representation (as employed in the Penn TreeBank (Marcus et al., 1993)). Following, various efforts were made to port the attractiveness of sequence-to-sequence modeling to the more general graph structure of semantic representations, such as AMR or MRS (Peng et al., 2017b; Barzdins and Gosko, 2016; Konstas et al., 2017; Buys and Blunsom, 2017). However, to the best of our knowledge, all such current methods actually sidestep the challenge of graph linearization \u2013 they reduce the input graph to a tree using lossy heuristics, which are specifi-\n1 DM is automatically derived from Minimal Recursion Semantics (MRS) (Copestake et al., 1999).\ncally tailored for their target representation.\nIn contrast, we design a novel deterministic and lossless linearization (Section 4), which is applicable to any graph with ordered nodes (e.g., sentence word order). To that end, we devise solutions for the various obstacles for linearizing a graph structure, such as reentrancies (or multiple heads), non-connected components, and nonprojective relations. This lineariztion allows us to follow the spirit of Johnson et al. (2017) in training all source-target combinations in a multi-task approach (Section 5). These combinations include the three traditional text to semantic parsing tasks, as well as six additional inter-representation translation tasks, constituting of all binary combinations of the target representations (e.g., PSD to PAS, or DM to PSD).\nFollowing, we design an encoder-decoder model which has two shared encoders, one for raw English sentences and another for linearized graphs, and a single global graph decoder. Interestingly, we show that training on the auxiliary inter-representation translation tasks greatly improves the performance on the original SDP tasks, without requiring any additional manual annotation effort (Section 6).\nOur contributions are two-fold. First, we show that novel sequence-to-sequence models are able to effectively capture and recover general graph structures, making them a viable and easily extensible approach towards the SDP task. Second, beyond SDP, as the inclusion of syntactic linearization was shown beneficial in various tasks (Aharoni and Goldberg, 2017; Le et al., 2017) so does our approach prompt easy integration of graphbased representations as complementary semantic signal in various downstream applications."}, {"heading": "2 Background", "text": "We begin this section by presenting the corpus we use to train and test our model (the SDP corpus) and the current state-of-the-art in predicting semantic dependencies. Then, we discuss previous work on sequence-to-sequence models for tree prediction, which this work extends to general graph structures. Finally, we briefly describe the multilingual translation approach, which we borrow and adapt to the semantic parsing task."}, {"heading": "2.1 Semantic Dependencies", "text": "In general, the development of most semantic formalisms was carried out by disjoint and independent efforts. However, the 2014 and 2015 SemEval shared tasks (Oepen et al., 2014, 2015) have culminated in the Semantic Dependency Parsing (SDP) resource, a consistent and large corpus (roughly 39K sentences), annotated in parallel with three well-established formalisms: DELPH-IN MRS-Derived Bi-Lexical Dependencies (DM) (Flickinger, 2000), Enju PredicateArgument Structures (PAS) (Miyao et al., 2014), and Prague Semantic Dependencies (PSD) (Hajic et al., 2012). While varying in their labels and annotation guidelines, all three representations induce a graph structure, where each node corresponds to a single word in the sentence. See Table 1 for more details on this corpus, and Figure 1 for examples of the three SDP formalisms. SDP has enabled the application of machine learning models for the task. Peng et al. (2017a) have set the state-of-the-art results on all three tasks, using techniques inspired by graph-based dependency parsing models (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Kuncoro et al., 2016). Their best results were obtained by leveraging the fact that SDP was annotated on parallel texts. They reached 88% average labeled F1 score across the SDP representations on an in-domain test set, via joint prediction of the three representations using higher-order cross-representation features. The first row in Table 4 summarizes their performance for the three prediction tasks.\nIn this work we will take a different approach to structured prediction of the SDP corpus. We will design a novel sequence-to-sequence model, not necessitating parallel annotations, which are often unavailable for multi-task learning."}, {"heading": "2.2 Structured Prediction using Sequence-to-Sequence Models", "text": "In contrast to the graph-parsing algorithms discussed in Section 2.1, a recent line of work has explored the usage of more general sequence-tosequence models to perform structured prediction, focusing specifically on predicting tree structures. These approaches devise a task-specific linearization function which converts the structured representation to a sequential string, which is then used to train the recurrent neural network. During inference, the inverted linearization function is applied to the output to recover the desired structure.\nVinyals et al. (2015) showed that sequence-tosequence phrase-based constituency parsing can be achieved using a tree depth-first search (DFS) traversal as a linearization function.2 Following this work, several recent efforts have employed a similar DFS approach to AMR and MRS parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; Peng et al., 2017b; Buys and Blunsom, 2017), after reducing AMR to trees by removing\n2DFS for rooted trees is equivalent to the bracketed notation of the Penn Treebank.\nre-entrencies. Several recent works have found syntactic linearization useful outside of neural parsers. For example, in neural machine translation, Aharoni and Goldberg (2017) showed that predicting targetside linearized syntactic trees can improve the quality and grammatically of the predicted translations. In Section 4, we show for the first time that the DFS approach is a viable linearization function also for semantic dependencies, by extending it to account for the challenges introduced by the richer graph structures in SDP."}, {"heading": "2.3 Multi-lingual Machine Translation", "text": "Multi-Task Learning (MTL) is a modeling approach which shares and tunes the model parameters across several tasks. In some instances of MTL, a subset of the tasks may be defined as the \u201cmain tasks\u201d, while the other tasks are treated as auxiliaries which improve performance on the main tasks by contributing to their training signal. MTL had regained popularity in recent years thanks to its easy and wide-spread applicability in neural networks (Collobert et al., 2011; Sogaard\nand Goldberg, 2016). Perhaps most relevant to this work is Google\u2019s neural machine translation system by Johnson et al. (2017), which trained a single sequence-tosequence model to translate between multiple languages. They introduced the usage of a special tag in the source sentence to specify the desired target language. For example, <2es> indicates that the model should translate the input sentence to Spanish.\nIn Section 4, we adapt the MTL strategy to train a single model for all SDP formalisms. We use a similar \u201cto\u201d and \u201cfrom\u201d tags to indicate source and desired target representations, and show that introducing auxiliary inter-task translations can improve performance on the main target tasks, namely parsing semantic representations for raw input text."}, {"heading": "3 Task Definition", "text": "We define the task of semantic translation, as converting to, and between, different sentencelevel semantic representations. Formally, a sentence-level semantic representation according to formalism R is a tuple, MR = (S,G), where S = {w1, ..., wn} is a raw sentence, and G = (V,E | V = {v1, ..., vn}, E \u2286 V 2) is a labeled graph whose vertices have a one-to-one correspondence with the words in S,3 while its edges represent binary semantic relations, adhering to R\u2019s specifications.\nUsing these notations, our input is defined as a triplet (source, target,Msource). Preceding the input semantic representation are identifiers for source and target representation schemes (e.g., \u201cPAS\u201d, \u201cDM\u201d or \u201cPSD\u201d). The semantic translation task is then to produceMtarget. I.e., the sentence\u2019s representation under the target formalism.\nThis definition is broad enough to encapsulate many sentence-level representations, and in this work we will use the three SDP representations, as well as an empty \u201cRAW\u201d representation (where E(G) = \u2205 for all sentences) to allow for translations from raw input sentences. We note that future work may extend this framework with other graph-based sentence representations.\n3 The one-to-one node-to-word correspondence follows SDP\u2019s formulation, but can be relaxed to adjust for other graph structures."}, {"heading": "4 Graph Linearization", "text": "As discussed in Section 2, structured prediction in a sequence-to-sequence framework requires a linearization function, from the desired structure to a linear sequence, and vice versa.\nOftentimes, such linearization consists of node traversal along the edges of the input graph. While previous work have had certain structural constraints on their input (e.g., imposing tree or noncyclic constructions), in this work, we construct a lossless function which allows us to feed the sequence-to-sequence network with a linearized general graph representation and expect a linearized graph in its output.\nIn this section, we describe our linearization traversal order, which generalizes the DFS traversal applied previously only for trees. We do this by converting an SDP graph such that all nodes are reachable from node v1. We then outline the challenging aspects of graph properties (which do not exist in trees), show that they are prevalent in the SDP corpus, and describe our proposed solutions. To the best of our knowledge, this is the first work which tackles the task of general graph linearization.\nWhile our linearization can be predicted with good accuracy (as we show in following sections), there is ample room to experiment with representational variations, which we start exploring in Section 6. Our conversion code is made publicly available,4 allowing further experimentation with general graph linearization for SDP and other related tasks."}, {"heading": "4.1 Traversing Graphs with Non-Connected Components", "text": "The DFS approach is an applicable linearization of trees since a recursive traversal, which starts at the root and explores all outgoing edges, is guaranteed to visit all of the graph\u2019s nodes. However, DFS linearization is not directly applicable to SDP, as its graphs often consist of several non-connected components.\nFor such graphs, there exists no starting node from which all of the nodes are reachable via DFS traversal, and certain nodes are bound to be left out of the traditional DFS encoding. For example, the words \u201ccan\u201d and \u201cgreatest\u201d in Figure 1a reside in different components, and therefore no single path\n4https://github.com/gabrielStanovsky/ semantics-as-foreign-language\n(which traverses along the graph\u2019s edge direction) will discover both of them.\nTo overcome this limitation, we make sure that all nodes are reachable from node v1, corresponding to the first word in the sentence, from which we start our traversal. This is achieved by introducing an artificial SHIFT edge between any two consecutive nodes vi, vi+1 for which there is no directed path already connecting them. Following, it is easy to see, by induction, that all nodes are reachable from v1, as for every node vi there exists a directed path (v1, v2, ..., vi\u22121, vi). For example, revisit the previously mentioned \u201ccan\u201d and \u201cgreatest\u201d nodes in Figure 1a, which are connected using \u201cSHIFT\u201d edges."}, {"heading": "4.2 Linearizing a DFS Graph Traversal", "text": "Intuitively, our linearization is a pre-order DFS, generalizing Vinyals et al. (2015)\u2019s approach to syntactic linearization. We start from v1 and explore all paths from it, in a depth-first manner. Once a path is exhausted, either by reaching a node with no outgoing edges5 or by reaching an already visited node, we use special backtracking edges to form a path backwards \u201cup\u201d the graph, until we hit a node which still has unexplored outgoing edges.\nFormally, our linearization of a given DFS traversal is composed of 3 types of elements (see Figure 2 for example):\nFirst, a Node reference identifies a node in the graph, which in turn corresponds to word in the SDP formalism. We identify nodes using two tokens: (1) Their position in the sentence, relative to the previous node in the path (while the first position in the linearization is written in absolute terms, as \u201c0\u201d), and (2) Explicitly writing the word corresponding to the node.\nFor example, in Figure 2, traversing the ARG1 edge from \u201ceasy\u201d lands at \u201cind/2 understand\u201d, whose outgoing ARG2 edge arrives at \u201cind/-4 success\u201d.\nSecond, an Edge reference, identifies an edge label. These are denoted by a single token, composed of 2 parts: (1) The edge\u2019s formalism (in our case, the SDP representation to which it pertains), and (2) The edge label. Traversing an edge (u, v) with label L will be encoded by placing the edge reference between the node references of u and v. For instance, in Figure 2, moving from node\n5Note that after introducing the artificial \u201cSHIFT\u201d edges, only vn may have no outgoing edges.\n0 to node 1 through the edge labeled \u201cposs\u201d is encoded with the following string: \u201cind/0 Their PAS/poss ind/1 success\u201d.\nFinally, Backtracking edges, signify a step \u201cbackward\u201d in the traversal. These are denoted with a single token, similarly to edge references, with the addition of a \u201cBACK\u201d suffix. For example, in Figure 2, we backtrack from the already visited node \u201cunderstand\u201d by writing: \u201cPAS/ARG1/BACK\u201d.\nThis linearization can be deterministically and efficiently inverted back to the graph structure. This is done by building the graph while reading the linearization, adding to it nodes and edges when they first appear, and omitting possible node recurrences in the linearization (due to cycles or backtracking edges), such as \u201csuccess\u201d, which appears twice in the Figure 2.\nRedundancy in encoding We note that certain items in our proposed linearization are redundant. First, writing down the explicit word in the traversal is not necessary, as the positional index is sufficient to uniquely identify a node. Second, a single backtracking tag would have been enough to identify the specific edge which is currently being backtracked (e.g., BACK instead of PAS/verb ARG1/BACK). The latter is similar to the redundancy in the syntactic linearization of Vinyals et al. (2015), who specify the type of closing bracket, e.g., NP(...)NP instead of NP(...).\nIn Section 6 we show empirically that our model benefits from explicitly generating these redundancies during decoding."}, {"heading": "4.3 DFS Traversal Order", "text": "A graph DFS traversal does not dictate an order in which to explore the different outgoing paths at each branching point. Consider, as a recurring example, the branching point at the word \u201cvote\u201d in Figure 1c, in which we need to choose an order amongst its four neighbors.\nWhile syntactic linearization conveniently follows the ordering of the words in the sentence, Konstas et al. (2017) have noted that different child visiting linearization orders affect the performance of text generation from AMR. In particular, they found that following the order of annotation of a human expert worked best.\nIntuitively, since different graph traversals affect the sequence of encoded nodes during train-\nDFS order type Example (vote\u2019s PSD neighbors)\nRandom permutation (play, for, jocks, now)\u2217\nSentence order Neighbor\u2019s index in the sentence\n(jocks, now, for, play)\nClosest words Neighbor\u2019s absolute distance\n(now, for, play, jocks)\nSmaller-first # nodes reachable from the neighbor (now, play, for, jocks)\nTable 2: Different neighbor exploration orders. Under the name of each order type, we list the key by which we sort each node\u2019s neighbors. The \u201cExample\u201d column shows the corresponding ordering of \u201cvote\u201d\u2019s neighbors in Figure 1c. \u2217An example of one possible random permutation.\ning, the network will inevitably have to learn different weights and attention when presented with different orderings. Therefore, some traversal orderings may be easier to learn than others, leading to better (hopefully more semantic) abstractions.\nTo the best of our knowledge, the human annotation order is not available for the SDP annotations, and there is no clear a priori optimal ordering. We therefore experiment with several visiting orders, as described in Table 2. Notably, Sentence order is equivalent to the ordering used by Vinyals et al. (2015) for syntactic linearization, while Closest words orders child nodes from short to longer range-dependencies (commonly associated with syntactic versus semantic relations), and Smaller first is motivated by the easy-first approach (Goldberg and Elhadad, 2010), first encoding paths which are shorter (and easier to memorize), before longer, more complicated sequences.\nIn Section 6 we evaluate the effect of these variations on the SDP parsing task."}, {"heading": "5 Model", "text": "We start by describing our model architecture, inspired by recent MT architectures, while allowing for different types of inputs, namely English sentences and linearized graphs. Following, we present our methods for training and testing, and specific hyper-parameter configuration and implementation details."}, {"heading": "5.1 Architecture", "text": "Our architecture, depicted in Figure 3, consists of a sequence-to-sequence model using a bi-LSTM encoder-decoder with attention on input and output tokens, similar to that used by Johnson et al. (2017) for multi-lingual MT. As described in Section 3, it is trained on 9 translation tasks in parallel. We split these into two groups, consisting of 3 primary tasks and 6 auxiliary tasks, as follows:\nPRIMARY = {(RAW, tgt) | tgt \u2208 (DM, PAS, PSD)}\nAUXILIARY = {(src, tgt) \u2208 {DM, PAS, PSD}2 | src 6= tgt}\nThe PRIMARY tasks deal with converting raw sentences to linearized graph structures, which we can compare to previous published baselines and are therefore our main interest. Conversely, while the AUXILIARY tasks provide additional training signal to tune our model, they are also interesting from an analytic point-of-view, which we examine in depth in Section 6.\nTo allow the model to differentiate between the different tasks, we prefix each input sample with two tags (see example in Figure 3). First, similarly to Johnson et al. (2017), we add a tag indicating the desired target representation, e.g., <to:DM>. Second, In contrast to multi-lingual MT which omits the source language (to allow for\ncode switching), we explicitly denote the source representation, e.g., <from:PSD>. This addition further strengthens the correlation between inputs from the same representation.6\nFurther deviating from the current practice in MT, our architecture uses two encoders and a single decoder (while common MT regards the encoder-decoder as a single unit). The first shared encoder specializes in encoding raw text for all PRIMARY tasks, while a second encodes linearized graph structures for the AUXILIARY tasks. Both encoders are linked to a single decoder which converts their output representations to a linearized graph.\nIntuitively, the two encoders correspond to the different nature of input to the PRIMARY tasks (an English sentence) versus that of the AUXILIARY tasks (a linearized graph), while a single decoder allows for a common linearized graphs output format. Since the decoder is trained across all 9 tasks, both encoders are optimized to arrive at similar latent representations which are geared towards graph prediction."}, {"heading": "5.2 Training and Inference", "text": "The overall size of multi-task training data is 320, 913 samples. This constitutes a 9-fold in-\n6Moreover, \u201ccode-switching\u201d between semantic representations is inherently undesired.\ncrease over a single-model for SDP (35, 657 sentences in the SDP corpus) and a 3-fold increase over a standard MTL approach to SDP (without the AUXILIARY tasks). During training, we penalize the model on all predicted elements, including the redundant elements discussed in Section 4.2. During inference, however, these redundancies may cause contradictions leading to incoherent sequences. Namely, a word may not conform to the previous word index, and a backtracking edge may point to a different relation. To overcome this we artificially increase the softmax probabilities (dashed edges in Figure 3) so that they reflect the DFS path decoded up until that point. Specifically, we override the predicted word according to the previous index, and backtrack \u201cup\u201d the corresponding edge."}, {"heading": "5.3 Implementation Details", "text": "All of our hyper-parameters were tuned on a held out partition of 1000 sentences in the training set. In particular, we use 3 hidden layers for both of the encoders, and 2 hidden layers for the decoder. English word embeddings were fixed with 300-dimensional GloVe embeddings (Pennington et al., 2014), while the graph elements, which consist of a lexicon of roughly 400 tokens across three representations, were randomly initialized. We trained the model until convergence, roughly 20\nepochs, in about 12 hours on a GPU (NVIDIA GeForce GTX 1080 Ti), in batches of 50 sentences. All of these sentences belong to the same task, which is chosen at random before each batch.\nFinally, our models were developed using the OpenNMT-py library (Klein et al., 2017), and are made available.7"}, {"heading": "6 Evaluation", "text": "We perform several evaluations, testing the impact of alternative configurations, including the different DFS traversal orders and MTL versus single-task approach, as well as our model\u2019s performance against current state-of-the-art on each of the PRIMARY tasks."}, {"heading": "6.1 Results", "text": "The results of our different analyses are reported in Tables 3-6, as elaborated below. For all evaluations, we use the in-domain test partition of the SDP corpus, containing 1, 410 sentences. Following Peng et al. (2017a) we report performance using labeled F1 scores as well as average scores across representations. We compare the produced graphs, after applying the inverted linearization function, rather than comparing the DFS path directly, as there may be several DFS graph traversals encoding the same relations.\nDFS order matters - Table 3 depicts our model\u2019s performance when linearizing the graphs according to the different traversal orders discussed and exemplified in Table 2. Overall, we find that the \u201csmaller-first\u201d approach performs best across all datasets, and that imposing one of our orders is always preferable over random permutations. Intuitively, the \u201csmaller-first\u201d approach presents shorter, and likely easier, paths first, thus minimizing the amount of error-propagation for\n7https://github.com/gabrielStanovsky/ semantics-as-foreign-language\nfollowing decoding steps. Due to its better performance, we will report only the smaller-first\u2019s performance in all following evaluations.\nFrom English to SDP - Table 4 presents the performance of our complete model (\u201cMTL PRIMARY+AUX\u201d) versus Peng et al. (2017a). On average, our model performs within 1% F1 point from the state-of-the art (outperforming it on the harder PSD task), despite using the more general sequence-to-sequence approach instead of a dedicated graph-parsing algorithm. In addition, an ablation study shows that multi-tasking the PRIMARY tasks is beneficial over a single task setting, which in turn is outperformed by the inclusion of the AUXILIARY tasks.\nSimulating disjoint annotations - In contrast with SDP\u2019s complete overlap of annotated sentences, multi-task learning often deals with disjoint training data. To simulate such scenario, we retrained the models on a randomly selected set of 33% of the train sentences for each representation (11, 886 sentences), such that the three representations overlap on only 10% (3, 565 sentences). The results in Table 5 show that our approach is more resilient to the decrease in annotation overlap, outperforming the state-of-the-art model on the DM and PSD task, as well as on the average score. We hypothesize that this is in part thanks to our ability to use the inter-task translations, even when these exist only for part of the annotations."}, {"heading": "6.2 Translating Between Representations", "text": "As a byproduct of training on the AUXILIARY tasks, our model can also be tested on translating between the different representations. This is done by presenting it with a linearized graph of one representation and asking it to translate it to another. To the best of our knowledge, this is the first work which tries to accomplish this.\nWe report the performance of all source-target combinations in Table 6. These evaluations provide several interesting comparisons between the representations: (1) For all representations, translating from any of the other two is easier than parsing from raw text, (2) The PAS and DM representations can be converted between them with high accuracy (95.7% and 96.1%, respectively). This can be due to their structural resemblance, noted in previous work (Peng et al., 2017a; Oepen et al., 2015), and (3) While PSD serves as a viable input for conversion to DM and PAS (92.1% F1 on\naverage), it is relatively harder to convert either of them to PSD (88.6%). This might indicate that PSD subsumes some of the information in DM and PAS."}, {"heading": "7 Conclusions and Future Work", "text": "We presented a novel sequence-to-sequence approach to the task of semantic dependency parsing, by casting the problem as multi-lingual machine translation. To that end, we introduced a DFSbased graph linearization function which generalizes several previous works on tree linearization. Following, we showed that our model, inspired by neural MT, benefits from the inter-task training\nsignal, reaching performance almost on-par with current state of the art in several scenarios.\nFuture work can employ this linearization function within downstream applications, as was done with syntactic linearization, or extend this framework with other graph-based representations, such as universal dependencies (Nivre et al., 2016) or AMR (Banarescu et al., 2013)."}, {"heading": "Acknowledgements", "text": "This work was supported in part by grants from the MAGNET program of the Israeli Office of the Chief Scientist (OCS); the German Research Foundation through the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1) and the Israel Science Foundation (grant No. 1157/16)."}], "year": 2018, "references": [{"title": "Universal conceptual cognitive annotation (UCCA)", "authors": ["Omri Abend", "Ari Rappoport."], "venue": "Proceedings of the 2013 conference of the Association for Computational Linguistics.", "year": 2013}, {"title": "The state of the art in semantic representation", "authors": ["Omri Abend", "Ari Rappoport."], "venue": "ACL.", "year": 2017}, {"title": "Towards string-to-tree neural machine translation", "authors": ["Roee Aharoni", "Yoav Goldberg."], "venue": "ACL.", "year": 2017}, {"title": "Abstract meaning representation for sembanking", "authors": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider"], "year": 2013}, {"title": "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on amr parsing accuracy", "authors": ["Guntis Barzdins", "Didzis Gosko."], "venue": "SemEval@NAACL-HLT.", "year": 2016}, {"title": "Robust incremental neural semantic graph parsing", "authors": ["Jan Buys", "Phil Blunsom."], "venue": "ACL.", "year": 2017}, {"title": "Introduction to the conll-2005 shared task: Semantic role labeling", "authors": ["Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proceedings of CONLL,", "year": 2005}, {"title": "Bllip 1987-89 wsj corpus release 1", "authors": ["Eugene Charniak", "Don Blaheta", "Niyu Ge", "Keith Hall", "John Hale", "Mark Johnson."], "venue": "Linguistic Data Consortium, Philadelphia, 36.", "year": 2000}, {"title": "Natural language processing (almost) from scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u20132537.", "year": 2011}, {"title": "Minimal recursion semantics: An introduction", "authors": ["Ann. Copestake", "Dan Flickinger", "Ivan A. Sag"], "year": 1999}, {"title": "Deep biaffine attention for neural dependency parsing", "authors": ["Timothy Dozat", "Christopher D. Manning."], "venue": "CoRR, abs/1611.01734.", "year": 2016}, {"title": "On building a more effcient grammar by exploiting types", "authors": ["Dan Flickinger."], "venue": "Natural Language Engineering, 6(1):15\u201328.", "year": 2000}, {"title": "An efficient algorithm for easy-first non-directional dependency parsing", "authors": ["Yoav Goldberg", "Michael Elhadad."], "venue": "HLT-NAACL.", "year": 2010}, {"title": "Announcing prague czech-english dependency treebank 2.0", "authors": ["Jan Hajic", "Eva Hajicov\u00e1", "Jarmila Panevov\u00e1", "Petr Sgall", "Ondrej Bojar", "Silvie Cinkov\u00e1", "Eva Fuc\u0131\u0301kov\u00e1", "Marie Mikulov\u00e1", "Petr Pajas", "Jan Popelka"], "venue": "In LREC,", "year": 2012}, {"title": "Simple and accurate dependency parsing using bidirectional lstm feature representations", "authors": ["Eliyahu Kiperwasser", "Yoav Goldberg."], "venue": "TACL, 4:313\u2013 327.", "year": 2016}, {"title": "OpenNMT: Open-Source Toolkit for Neural Machine Translation", "authors": ["G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush."], "venue": "ArXiv e-prints.", "year": 2017}, {"title": "Neural amr: Sequence-to-sequence models for parsing and generation", "authors": ["Ioannis Konstas", "Srinivasan Iyer", "Mark Yatskar", "Yejin Choi", "Luke S. Zettlemoyer."], "venue": "ACL.", "year": 2017}, {"title": "Distilling an ensemble of greedy dependency parsers into one mst parser", "authors": ["Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Noah A. Smith."], "venue": "EMNLP.", "year": 2016}, {"title": "Improving sequence to sequence neural machine translation by utilizing syntactic dependency information", "authors": ["An Nguyen Le", "Ander Martinez", "Akifumi Yoshimoto", "Yuji Matsumoto."], "venue": "IJCNLP.", "year": 2017}, {"title": "Building a large annotated corpus of english: The penn treebank", "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "year": 1993}, {"title": "In-house: An ensemble of pre-existing offthe-shelf parsers", "authors": ["Yusuke Miyao", "Stephan Oepen", "Daniel Zeman."], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335\u2013340.", "year": 2014}, {"title": "The proper treatment of quantification in ordinary english", "authors": ["Richard Montague."], "venue": "Approaches to natural language, pages 221\u2013242. Springer.", "year": 1973}, {"title": "Dependency grammar and dependency parsing", "authors": ["Joakim Nivre"], "year": 2005}, {"title": "Universal dependencies v1: A multi", "authors": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan T. McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman"], "year": 2016}, {"title": "Semeval 2015 task 18: Broad-coverage semantic dependency parsing", "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinkov\u00e1", "Dan Flickinger", "Jan Hajic", "Zdenka Uresov\u00e1."], "venue": "SemEval@NAACL-HLT.", "year": 2015}, {"title": "Semeval 2014 task 8: Broad-coverage semantic dependency parsing", "authors": ["Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajic", "Angelina Ivanova", "Yi Zhang."], "venue": "Proceedings of the 8th International Workshop on", "year": 2014}, {"title": "Deep multitask learning for semantic dependency parsing", "authors": ["Hao Peng", "Sam Thomson", "Noah A. Smith."], "venue": "Proceedings of the Association for Computational Linguistics, Vancouver, Canada. Association for Computational Linguistics.", "year": 2017}, {"title": "Addressing the data sparsity issue in neural amr parsing", "authors": ["Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue."], "venue": "EACL.", "year": 2017}, {"title": "Glove: Global vectors for word representation", "authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "year": 2014}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "authors": ["Anders Sogaard", "Yoav Goldberg."], "venue": "ACL.", "year": 2016}, {"title": "Grammar as a foreign language", "authors": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey E. Hinton."], "venue": "NIPS.", "year": 2015}], "id": "SP:1eaf20ba9141e768da964b08f266f275af28ca6c", "authors": [{"name": "Gabriel Stanovsky", "affiliations": []}, {"name": "Ido Dagan", "affiliations": []}], "abstractText": "We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications.", "title": "Semantics as a Foreign Language"}