11	84	The computation of GBDT is also prohibitively expensive for applications with high dimensional sparse output.
16	15	Our goal is to develop a new approach for problems with high-dimensional and sparse output spaces that achieves faster prediction time and smaller model size than existing algorithms, but has similar prediction accuracy and training time.
23	14	To handle this, we use several unsupervised and supervised dimensional reduction algorithms as pre-processing steps.
26	25	For example, on the Wiki10-31K dataset with 30938 labels, our method takes only 1.3 secs.
59	21	(1) Gradient boosting considers the function estimation F in an additive form: F (x) = T∑ m=1 fm(x), (2) where T is the number of iterations.
62	63	GBDT uses decision trees to be the base learners.
64	14	At stage m, we form an approximate function of the loss: L(yi, Fm−1(xi) + fm(xi)) ≈ L(yi, Fm−1(xi)) + gifm(xi) + 1 2 fm(xi) 2, (3) where Fm−1(xi) = ∑m−1 j=1 fj(xi) and gi = ∂L(yi, F (xi)) ∂F (xi) |F (xi)=Fm−1(xi) .
68	23	The advantage of this gradient boosting approach is that only the expression of the gradient varies for different loss functions, while the rest of the procedure, and in particular the decision tree induction step, remains the same for different loss functions.
77	16	Following the same steps as the previous section, for each tree we want to find the cut value to minimize the following objective function: min fm 1 N N∑ i=1 ‖gi − fm(xi)‖22 + λ Mm∑ j=1 ‖wmj ‖22.
84	41	(12) Since the objectives follow a simple quadratic form, these problems can be solved in closed form as hr = 1 λN + |Vr| ∑ i∈Vr gi, hl = 1 λN + |Vl| ∑ i∈Vl gi (13) Now we can use hr and hl to form prediction: the prediction for example i is he,i = hr if i ∈ Vr and is hl if i ∈ Vl.
93	31	The prediction vector in each leaf of each tree is a dense vector of length L. This will result in a total model size of O(TML), where T is the number of trees and M is the average number of leaves in each tree.
105	16	To handle the first three issues (dense residual vectors, model size, and prediction time), we use the fact that the labels yi are high dimensional but very sparse.
106	25	For the loss function satisfies our assumptions (Assumption (7) and (8)), and if both yi and zi are sparse, then the gradient vector gi in (11) will also be a sparse vector, and the sparsity is at most ‖yi‖0 + ‖zi‖0.
110	26	Note that by definition of gi, it can have at most Tk+‖yi‖0 non-zeros after T iterations (the label vector yi is also sparse).
128	19	Decision trees usually have difficulty handling sparse features.
131	15	The most simple yet useful one is to use random projection, that is, projecting the data point to x̄i = Ḡxi using a fixed random Gaussian matrix Ḡ ∈ Rd×D as projection matrix.
133	51	Both random projection and PCA are un-supervised learn- ing approaches—in the sense that they do not use any label information; however, in our problem setting there is rich information in the high dimensional output space Y .
137	60	Using this projection has two benefits:(1) the projection incorporates the label information; and, (2) the new data after projection, X̄ is dense, and thus results in shallow and balanced trees.
158	24	So each level of the tree requires O(D‖X‖0 log(k)) time.
160	14	As discussed in the previous section, the prediction time is O(Tk log k) for prediction.
165	17	Therefore, the model size is O(kT2h).
166	42	As long as tree depth h is not too large (usually less than 12), the model size is very small.
187	22	We hand tuned the projection dimensionality d and set it to 100 for Delicious and Wiki10-31K, and 50 for others.
200	34	Figure 1(a)-(c) shows the P@1 as a function of time for three datasets.
208	31	In summary, we can see from Figure 1 that to achieve similar accuracy, GBDTSPARSE takes much less prediction time and the model size is much smaller than other methods.
209	17	Multicore Implementation: Unlike random-forest based methods, paralllelizing GBDT is not straightforward.
215	42	Also, the huge speedup from parallelization is a big advantage to use our algorithm in practice, comparing to algorithms that cannot be easily parallelized, like PD-SPARSE.
218	15	We made non-trivial modifications to GBDT (use embeddings to make features dense, introduce label vector sparsity at leaf nodes) to make it suitable for handling high dimensional output.
219	23	These improvements can significantly reduce the prediction time and model size.
220	19	As an application, we use our proposed method to solve extreme multi-label learning problem.
