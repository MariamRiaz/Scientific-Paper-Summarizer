0,1,label2,summary_sentences
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2357–2366, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"As science advances, scientists around the world continue to produce a large number of research articles, which provide the technological basis for worldwide dissemination of scientific discoveries.",1 Introduction,[0],[0]
"Online digital libraries such as Google Scholar, CiteSeerx, and PubMed store and index millions of such research articles and their metadata, and make it easier for researchers to search for scientific information.",1 Introduction,[0],[0]
These libraries require effective and efficient methods for topic classification of research articles in order to facilitate the retrieval of content that is tailored to the interests of specific individuals or groups.,1 Introduction,[0],[0]
"Supervised approaches for topic classification of research articles have been developed, which generally use either the content of the articles (Caragea et al., 2011), or take into account the citation relation between research articles (Lu and Getoor, 2003).
",1 Introduction,[0],[0]
"To be successful, these supervised approaches assume the availability of large amounts of labeled
data, which require intensive human labeling effort.",1 Introduction,[0],[0]
"In this paper, we explore a semi-supervised approach that can exploit large amounts of unlabeled data together with small amounts of labeled data for accurate topic classification of research articles, while minimizing the human effort required for data labeling.",1 Introduction,[0],[0]
"In the scholarly domain, research articles (or papers) are highly interconnected in giant citation networks, in which papers cite or are cited by other papers.",1 Introduction,[0],[0]
"We posit that, in addition to a document’s textual content and its local neighborhood in the citation network, other information exists that has the potential to improve topic classification.",1 Introduction,[0],[0]
"For example, in a citation network, information flows from one paper to another via the citation relation (Shi et al., 2010).",1 Introduction,[0],[0]
"This information flow and the topical influence of one paper on another are specifically captured by means of citation contexts, i.e., short text segments surrounding a citation’s mention.
",1 Introduction,[0],[0]
"These contexts are not arbitrary, but they often serve as brief summaries of a cited paper.",1 Introduction,[0],[0]
"We therefore hypothesize that these micro-summaries can be successfully used as an independent view of a research article in a co-training framework to reduce the amount of labeled data needed for the task of topic classification.
",1 Introduction,[0],[0]
"The idea of using terms from citation contexts stems from the analysis of hyperlinks and the graph structure of the Web, which are instrumental in Web search (Manning et al., 2008).",1 Introduction,[0],[0]
"Many search engines follow the intuition that the anchor text pointing to a page is a good descriptor of its content, and thus anchor text terms are used as additional index terms for a target webpage.",1 Introduction,[0],[0]
"The use of links and anchor text was thoroughly researched for information retrieval (Koolen and Kamps, 2010), broadening a user’s search (Chakrabarti et al., 1998), query refinement (Kraft and Zien, 2004), and enriching document representations (Metzler et al., 2009).",1 Introduction,[0],[0]
"Blum and
2357
Mitchell (1998) introduced the co-training algorithm using hyperlinks and anchor text as a second, independent view of the data for classifying webpages, in addition to a webpage content.
",1 Introduction,[0],[0]
Contributions and Organization.,1 Introduction,[0],[0]
"We present a co-training approach to topic classification of research papers that effectively incorporates information from a citation network, in addition to the information contained in each paper.",1 Introduction,[0],[0]
"The result of this classification task will aid indexing of documents in digital libraries, and hence, will lead to improved organization, search, retrieval, and recommendation of scientific documents.",1 Introduction,[0],[0]
"Our contributions are as follows:
• We propose the use of citation contexts as an additional view in a co-training approach, which results in high accuracy classifiers.",1 Introduction,[0],[0]
"To our knowledge, this has not been addressed in the literature.",1 Introduction,[0],[0]
"• We show experimentally that our co-training
classifiers significantly outperform: (1) supervised classifiers trained using either content or citation contexts independently, for the same fraction of labeled data; and (2) several other semi-supervised classifiers, trained on the same fractions of labeled and unlabeled data as co-training.",1 Introduction,[0],[0]
"• We also show that using the citation context
information available in citation networks, the human effort involved in data labeling for training accurate classifiers can be largely reduced.",1 Introduction,[0],[0]
"Our co-training classifiers trained on a very small sample of labeled data and a large sample of unlabeled data yield accurate topic classification of research articles.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we discuss related work.",1 Introduction,[0],[0]
"Section 3 describes our data and its characteristics, followed by the presentation of our proposed co-training approach in Section 4.",1 Introduction,[0],[0]
"We present experiments and results in Section 5, and conclude the paper and present future directions of our work in Section 6.",1 Introduction,[0],[0]
We discuss here the most relevant works to our study.,2 Related Work,[0],[0]
A large variety of methods have been proposed in the literature with regard to automatic text classification and topic prediction.,2 Related Work,[0],[0]
"Different classifiers have been applied on the Vector Space Model (VSM), in which a document is represented as a vector of words or phrases asso-
ciated with their TF-IDF score, i.e. term frequency - inverse document frequency (Zhang et al., 2011; Kansheng et al., 2011).",2 Related Work,[0],[0]
"VSM is the most used method due to its simple, efficient and easy to understand implementation.",2 Related Work,[0],[0]
"Another widely used model is the Latent Semantic Indexing (LSI) where co-occurrences are analyzed to find semantic relationships between words or phrases (Zhang et al., 2011; Ganiz et al., 2011).",2 Related Work,[0],[0]
"Moreover, a great range of classifiers were used for this task, including: Naı̈ve Bayes (Lewis and Ringuette, 1994), Knearest neighbors (Yang, 1999) and Support Vector Machines (Joachims, 1998).",2 Related Work,[0],[0]
"These techniques, however, all require a large number of labeled documents in order to build accurate classifiers.",2 Related Work,[0],[0]
"In contrast, we propose a co-training algorithm that only requires a small amount of labeled data in order to make accurate topic classification.
Semi-supervised methods essentially involve different means of transferring labels from labeled to unlabeled samples in the process of learning a classifier that can generalize well on new unseen data.",2 Related Work,[0],[0]
"Co-training was originally introduced in (Blum and Mitchell, 1998) where it was used to classify web pages into academic course home page or not.",2 Related Work,[0],[0]
"This approach has two views of the data as follows: the content of a web page, and the words found in the anchor text of the hyperlinks that point to the web page.",2 Related Work,[0],[0]
"Wan (2009) used co-training for cross-lingual sentiment classification of product reviews, where English and Chinese features were considered as two independent views of the data.",2 Related Work,[0],[0]
"Furthermore, Gollapalli et al. (2013) used co-training to identify authors’ homepages from the current-day university websites.",2 Related Work,[0],[0]
"The paper presents novel features, extracted from the URL of a page, that were used in conjunction with content features, forming two complementary views of the data.
",2 Related Work,[0],[0]
Citation networks have been used before in other problems.,2 Related Work,[0],[0]
Caragea et al. (2014) used citation contexts to extract informative features for keyphrase extraction.,2 Related Work,[0],[0]
"Lu and Getoor (2003) proposed an approach for document classification that used only citation links, without any textual data from the citation contexts.",2 Related Work,[0],[0]
Ritchie et al. (2006) used a combination of terms from citation contexts and existing index terms of a paper to improve indexing of cited papers.,2 Related Work,[0],[0]
"Citation contexts were also used to improve the performance of citation recommendation systems (Kataria et al., 2010) and to study author influence in document networks
(Kataria et al., 2011).",2 Related Work,[0],[0]
"Moreover, citation contexts were used for scientific paper summarization (Abu-Jbara and Radev, 2011; Qazvinian et al., 2010; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Lehnert et al., 1990)",2 Related Work,[0],[0]
"For example, in Qazvinian et al. (2010), a set of important keyphrases is extracted first from the citation contexts in which the paper to be summarized is cited by other papers and then the “best” subset of sentences that contain such keyphrases is returned as the summary.",2 Related Work,[0],[0]
Mei and Zhai (2008) used information from citation contexts to determine what sentences of a paper are of high impact (as measured by the influence of a target paper on further studies of similar or related topics).,2 Related Work,[0],[0]
"These sentences constitute the impact-based summary of the paper.
",2 Related Work,[0],[0]
"Despite the use of citation contexts and anchor text in many information retrieval and natural language processing tasks, to our knowledge, we are the first to propose the incorporation of citation context information available in citation networks in a co-training framework for topic classification of research papers.",2 Related Work,[0],[0]
The dataset used in our experiments is a subset sampled from the CiteSeerx digital library1 and labeled by Dr. Lise Getoor’s research group at the University of Maryland.,3 Data,[0],[0]
"This subset was previously used in several studies including (Lu and Getoor, 2003) and (Kataria et al., 2010).",3 Data,[0],[0]
"The dataset consists of 3186 labeled papers, with each paper being categorized into one of six classes: Agents, Artificial Intelligence (AI), Information Retrieval (IR), Machine Learning (ML), HumanComputer Interaction (HCI) and Databases (DB).",3 Data,[0],[0]
"For each paper, we acquire the citation contexts directly from CiteSeerx.",3 Data,[0],[0]
A citation context is defined as a window of n words surrounding a citation mention.,3 Data,[0],[0]
"We differentiate between cited and citing contexts for a paper as follows: let d be a target paper and C be a citation network such that d ∈ C. A cited context for d is a context in which d is cited by some paper di in C. A citing context for d is a context in which d is citing some paper dj in C. If a paper is cited in multiple contexts within another paper, the contexts are aggregated into a single context.",3 Data,[0],[0]
"For each paper in the dataset, we have at least one cited or one citing context.",3 Data,[0],[0]
"A summary of the dataset is provided in Table 1.
",3 Data,[0],[0]
"1http://citeseerx.ist.psu.edu/
As expected, we have a higher number of cited contexts than citing contexts.",3 Data,[0],[0]
This is due to the page restrictions often imposed to research articles that can limit the number of papers each article can cite.,3 Data,[0],[0]
"On the other hand, a good research paper can accumulate hundreds of citations, and hence, cited contexts over the years.
",3 Data,[0],[0]
Context lengths.,3 Data,[0],[0]
"In CiteSeerx, citation contexts have about 50 words on each side of a citation mention.",3 Data,[0],[0]
A previous study by Ritchie et al. (2008) shows that a fixed window length of about 100 words around a citation mention is generally effective for information retrieval tasks.,3 Data,[0],[0]
"For this reason, we use the contexts provided by CiteSeerx directly.",3 Data,[0],[0]
"In future, it would be interesting to study more sophisticated approaches to identifying the text that is relevant to a target citation (Abu-Jbara and Radev, 2012; Teufel, 1999) and study the influence of context lengths on our task.
",3 Data,[0],[0]
"For all experiments, our labeled dataset is split in train, validation and test sets.",3 Data,[0],[0]
The validation and test sets have about 200 papers each.,3 Data,[0],[0]
"We sampled another set of papers from the labeled dataset in order to simulate the existence of unlabeled data, with a fixed size of around 2000 papers.",3 Data,[0],[0]
The remaining 786 papers are used as labeled training data.,3 Data,[0],[0]
Each experiment was repeated 10 times with 10 different random seeds and the results were averaged.,3 Data,[0],[0]
Blum and Mitchell (1998) proposed the cotraining algorithm in the context of webpage classification.,4 Co-Training for Topic Classification,[0],[0]
"In co-training, the idea is that two classifiers trained on two different views of the data teach one another by re-training each classifier on the data enriched with predicted examples that the other classifier is most confident about.",4 Co-Training for Topic Classification,[0],[0]
"In Blum and Mitchell (1998), webpages are represented using two different views: (1) using terms from webpages’ content and (2) using terms from the anchor text of hyperlinks pointing to these pages.
",4 Co-Training for Topic Classification,[0],[0]
"Algorithm 1 Co-Training Input: L, U , ‘s’
L1 ← L, L2 ← L while U 6= ∅",4 Co-Training for Topic Classification,[0],[0]
"do
Train classifier C1 on L1 Train classifier C2 on L2 S ← ∅",4 Co-Training for Topic Classification,[0],[0]
"Move ‘s’ examples from U to S U ← U\S S1, S2 ← GetMostConfidentExamples(S,
C1, C2) L1 ← L1 ∪ S1, L2 ← L2 ∪ S2 U ← U ∪",4 Co-Training for Topic Classification,[0],[0]
"[S\(S1 ∪ S2)]
end while Ouput: The combined classifier C of C1 and C2
In this paper, we study the applicability and extension of the co-training algorithm to the task of topic classification of research papers, which are embedded in large citation networks.",4 Co-Training for Topic Classification,[0],[0]
"Here, in addition to the information contained in a paper itself, citing and cited papers capture different aspects (e.g., topicality, domain of study,",4 Co-Training for Topic Classification,[0],[0]
"algorithms used) about the target paper (Teufel et al., 2006), with citation contexts playing an instrumental role.",4 Co-Training for Topic Classification,[0],[0]
"We conjecture that citation contexts, which act as brief summaries about a cited paper, provide important clues in predicting the topicality of a target paper.",4 Co-Training for Topic Classification,[0],[0]
These clues give rise to the design of our co-training based model for topic classification of research papers.,4 Co-Training for Topic Classification,[0],[0]
"In our model, we use the content of a paper as one view and the citation contexts as another view of our data.",4 Co-Training for Topic Classification,[0],[0]
"In particular, for the content of a paper, we use its title and abstract as it is commonly used in the literature (Lu and Getoor, 2003); for the citation contexts, we use both the cited and citing contexts, as described in the previous section.
",4 Co-Training for Topic Classification,[0],[0]
Our co-training procedure is described in Algorithm 1.,4 Co-Training for Topic Classification,[0],[0]
L and U represent the labeled and unlabeled datasets and contain instances from both views.,4 Co-Training for Topic Classification,[0],[0]
The fractions of the training set are obtained from the 786 papers by selecting k% random examples from each class.,4 Co-Training for Topic Classification,[0],[0]
"For a round of co-training, we train classifiers C1 and C2 on the two views.",4 Co-Training for Topic Classification,[0],[0]
"Next, s examples are sampled from the unlabeled data into S, and C1, C2 are used to obtain predictions for these s examples.",4 Co-Training for Topic Classification,[0],[0]
"The GetMostConfidentExamples method is a generic placeholder that stands for a function that deter-
mines what examples from S are chosen to be added into training.",4 Co-Training for Topic Classification,[0],[0]
"Finally, at the end of an iteration, the examples left into S are moved back to U , and the algorithm iterates until there are no more unlabeled examples in U .",4 Co-Training for Topic Classification,[0],[0]
The final classifier C is obtained by combining C1 and C2 using the product of their class probability distributions.,4 Co-Training for Topic Classification,[0],[0]
"The class with the highest posterior probability (of the product of the two distributions) is chosen as the predicted class.
",4 Co-Training for Topic Classification,[0],[0]
"Unlike the original co-training algorithm described by Blum and Mitchell (1998), which tackled a binary classification task (course vs. noncourse page classification), we address a multiclass classification problem, where each example (i.e., research paper) is classified into one of six different classes.",4 Co-Training for Topic Classification,[0],[0]
"Moreover, in Blum and Mitchell (1998), the co-training algorithm moves p highest confidence positive examples and n highest confidence negative examples from S to L, where p : n represents the class distribution in the original labeled training set (i.e., if there are 10 positive examples and 90 negative examples in the labeled set L, then p = 1 positive and n = 9 negative examples are moved to the labeled set at each iteration of co-training).",4 Co-Training for Topic Classification,[0],[0]
"Unlike, this approach that preserves the class distribution of the original labeled training set, we move into L all examples that are classified with a confidence above a certain threshold.",4 Co-Training for Topic Classification,[0],[0]
"First, the proposed method is evaluated on the validation set.",5 Results and Discussion,[0],[0]
We first compare it against various supervised and semi-supervised baselines.,5 Results and Discussion,[0],[0]
"Next, we report the performance of our co-training algorithm under different scenarios, where either cited or citing contexts are used.",5 Results and Discussion,[0],[0]
We also show the most informative words for each classifier.,5 Results and Discussion,[0],[0]
"Finally, with the best parameters obtained on the validation set, we report the precision, recall and F1-score, obtained by each method, on the test set.
",5 Results and Discussion,[0],[0]
"In experiments, the sample size ‘s’ from Algorithm 1 is set to 300, i.e. the number of documents sampled from the unlabeled pool at each iteration; the confidence threshold is set to 0.95, i.e. if both classifiers agree on the class label and have a confidence ≥ 0.95, the instance is labeled and moved into the labeled training set.",5 Results and Discussion,[0],[0]
"These parameters are estimated on the validation set, but the results are not shown due to space limitation.
",5 Results and Discussion,[0],[0]
Evaluation Measures.,5 Results and Discussion,[0],[0]
We report results averaged over ten different runs with random splits.,5 Results and Discussion,[0],[0]
"For each random split, we return the weighted average precision, recall and F1-score.",5 Results and Discussion,[0],[0]
"In all the experiments, we use the Naı̈ve Bayes Multinomial classifier and its Weka implementation2, with term-frequencies as feature values.",5 Results and Discussion,[0],[0]
"We experimented with both TF and TF-IDF scores, using different classifiers (Support Vector Machine, Naı̈ve Bayes Multinomial, and simple Naı̈ve Bayes classifiers), but Naive Bayes Multinomial with TF performed best.",5 Results and Discussion,[0],[0]
How does co-training compare with supervised learning techniques?,5.1 Baseline Comparisons,[0],[0]
"In this experiment, we compare our co-training method with two supervised baselines: (1) when only document content is used and (2) when only citation contexts are used.
",5.1 Baseline Comparisons,[0],[0]
Figure 1 shows the F1-scores achieved using different initial training sizes.,5.1 Baseline Comparisons,[0],[0]
"We can see that overall, the citation contexts are better at predicting the topic of a document compared with the content, outperforming them in 9 out of 10 experimental settings.",5.1 Baseline Comparisons,[0],[0]
"The only exception to this trend is when a small number (5%) of training instances is available, in which case the supervised content view performs better, reaching an F1-score of 0.534.",5.1 Baseline Comparisons,[0],[0]
"Regardless, the co-training method shows significant improvement over both baselines, in all experiments.",5.1 Baseline Comparisons,[0],[0]
"Starting with an F1-score of 0.572, it continues to improve its performance as the training percentage is increasing.",5.1 Baseline Comparisons,[0],[0]
"The maximum F1score, i.e. 0.742, is reached when 30% of the labeled training set is used.",5.1 Baseline Comparisons,[0],[0]
"Note that the difference in performance between co-training and the two supervised baselines is statistically significant for
2http://www.cs.waikato.ac.nz/ml/weka/
a p value of 0.05.
",5.1 Baseline Comparisons,[0],[0]
A fully supervised baseline that uses 100% of the training set achieves an F1-score of 0.720 (using content) and 0.738 (using citation contexts).,5.1 Baseline Comparisons,[0],[0]
"In contrast, co-training requires only 15% of the labeled training set to outperform the fully supervised content baseline and 30% of the training set to outperform the fully supervised citation contexts baseline.",5.1 Baseline Comparisons,[0],[0]
"Consequently, using a co-training approach that includes citation contexts as well as the document content can not only increase the performance, but will also significantly reduce the need of expensive labeled instances.
",5.1 Baseline Comparisons,[0],[0]
"Figure 2 illustrates the confusion matrices of three experiments: (a) supervised content view, i.e. the title and abstract, (b) supervised citation contexts view, and (c) co-training that uses both views.",5.1 Baseline Comparisons,[0],[0]
These experiments use 10% of the training set.,5.1 Baseline Comparisons,[0],[0]
"Each of the matrices are represented by a heat map, i.e. the redder the color, the higher the value assigned to that position.",5.1 Baseline Comparisons,[0],[0]
An accuracy of 1 will be represented by a matrix with red blocks on the main diagonal and white blocks everywhere else.,5.1 Baseline Comparisons,[0],[0]
"This experiment was performed 10 times with 10 different seeds and the results have been averaged.
",5.1 Baseline Comparisons,[0],[0]
"As can be seen, the matrix that uses only titles and abstracts, i.e. left side, is showing the highest percentage of misclassified documents, classifying correctly about 58.8% instances, on average.",5.1 Baseline Comparisons,[0],[0]
"Using only citation contexts in a supervised framework, i.e. center matrix, we reach a higher accuracy of 60.7%.",5.1 Baseline Comparisons,[0],[0]
"The co-training method, which uses the content of the paper and citations as two independent views, significantly increases the average accuracy to 67.3%.",5.1 Baseline Comparisons,[0],[0]
This experiment shows that citation contexts are better than titles and abstracts at predicting the topic of a document.,5.1 Baseline Comparisons,[0],[0]
"Furthermore, our proposed approach, which uses the content of the paper as well as citation contexts, achieves higher results than each view used separately.",5.1 Baseline Comparisons,[0],[0]
"The difference in accuracy is statistically significant across all three experiments for a p value of 0.05.
",5.1 Baseline Comparisons,[0],[0]
"Overall, the Agents class seem to be the easiest to classify, reaching an accuracy value of 91.6% when using co-training.",5.1 Baseline Comparisons,[0],[0]
"On the other hand, the AI class is the hardest to classify.",5.1 Baseline Comparisons,[0],[0]
One reason for this is that the AI class contains the lowest number of instances in the dataset.,5.1 Baseline Comparisons,[0],[0]
"Another can be that the AI class is the most general among all classes and therefore, classifying documents with this la-
Left: using titles and abstracts; Center: using citation contexts;",5.1 Baseline Comparisons,[0],[0]
"Right: using co-training.
bel can be a difficult task even for a human.",5.1 Baseline Comparisons,[0],[0]
"Other common misclassifications occur between classes like HCI and Agents, ML and IR or AI and ML, due to their similarity.
",5.1 Baseline Comparisons,[0],[0]
How does our co-training method compare with other supervised approaches?,5.1 Baseline Comparisons,[0],[0]
"In this experiment, we compare the performance of co-training against two other methods: early and late fusion.",5.1 Baseline Comparisons,[0],[0]
"In early fusion, the feature vectors of the two views are concatenated, creating a single representation of the data.",5.1 Baseline Comparisons,[0],[0]
"In contrast, late fusion trains two separate classifiers and then combines them by taking the label with the highest confidence.
",5.1 Baseline Comparisons,[0],[0]
Figure 3 shows this comparison over different training sizes.,5.1 Baseline Comparisons,[0],[0]
"The results show that the cotraining method is more accurate than all others, performing best in all 10 experimental settings.",5.1 Baseline Comparisons,[0],[0]
"Late fusion has an overall lower performance compared with co-training, but is in a tight correlation with it.",5.1 Baseline Comparisons,[0],[0]
"On the other hand, early fusion achieves the lowest F1-score across the experiments.",5.1 Baseline Comparisons,[0],[0]
"The reported results are statistically significant at p value of 0.05, when the training percentage is between 5 and 35.",5.1 Baseline Comparisons,[0],[0]
"Therefore, we can say that train-
ing two separate classifiers, one of each view, yields higher performance compared with training a single classifier that incorporates both views.",5.1 Baseline Comparisons,[0],[0]
"Moreover, using a co-training approach that incorporates information from unlabeled data into the model, will help the two classifiers increase their confidences and minimize the error rate.
",5.1 Baseline Comparisons,[0],[0]
How does co-training compare with semisupervised methods?,5.1 Baseline Comparisons,[0],[0]
"Here, we present results comparing co-training with two other wellknown semi-supervised techniques: self-training and Naı̈ve Bayes with Expectation Maximization.
Self-Training.",5.1 Baseline Comparisons,[0],[0]
"First, we show results of the comparison of co-training with two variations of selftraining: (1) self-training using only document content, and (2) self-training using only citation contexts.",5.1 Baseline Comparisons,[0],[0]
Figure 4 shows the results of this experiment.,5.1 Baseline Comparisons,[0],[0]
"Self-training is similar to co-training, except that it uses only one view of the data (Zhu, 2005).",5.1 Baseline Comparisons,[0],[0]
"Self-training parameters, e.g., sample size ‘s’ or number of iterations, are estimated as in cotraining.
",5.1 Baseline Comparisons,[0],[0]
"Although the document content version of selftraining outperforms co-training when using 5%
of the training instances, we can see that overall, there is a significant difference in terms of F1score values in the favor of co-training.",5.1 Baseline Comparisons,[0],[0]
"In 9 out of 10 experiments, our co-training approach is superior to both self-training methods.",5.1 Baseline Comparisons,[0],[0]
"The results are statistically significant across all experimental setups for a p value of 0.05.
",5.1 Baseline Comparisons,[0],[0]
Expectation Maximization.,5.1 Baseline Comparisons,[0],[0]
"Figure 5 shows the F1-score values obtained after running NBM with EM with the same training, unlabeled and test sets.",5.1 Baseline Comparisons,[0],[0]
"The EM algorithm uses the same classifier, i.e. NBM, and the weight for each unlabeled instance is set to 1, as this setting achieved the highest results.",5.1 Baseline Comparisons,[0],[0]
"Two different experiments were performed using EM: (1) using only document content, and (2) using only citation contexts.",5.1 Baseline Comparisons,[0],[0]
"As can be seen in the figure, overall, the co-training approach significantly outperforms both variations of EM.",5.1 Baseline Comparisons,[0],[0]
"However, the co-training method falls short when using 5% of the training instances, where EM Content and EM Citations methods are achieving higher F1-score values.",5.1 Baseline Comparisons,[0],[0]
"Nonetheless, both EM variations tend to achieve an F1-score value below or equal to 0.710, whereas co-training reaches performance values of 0.74 or higher.",5.1 Baseline Comparisons,[0],[0]
"Again, the comparison results between co-training and both variations of EM are statistically significant for training sizes between 10% and 50%, for a p value of 0.05.",5.1 Baseline Comparisons,[0],[0]
Which of the two types of citation contexts (cited or citing) help the task of topic classification more and how does co-training perform in the absence of either one?,5.2 Using Different Citation Context Types,[0],[0]
The answer to this question is important as there are cases in which citation contexts are not readily available.,5.2 Using Different Citation Context Types,[0],[0]
"One frequently encountered example includes newly published research papers that have no cited contexts.
",5.2 Using Different Citation Context Types,[0],[0]
"In this case, it is important to know how our method performs when we only have one type of citation contexts.",5.2 Using Different Citation Context Types,[0],[0]
"Figure 6 shows the difference in performance when using: (1) only cited contexts, (2) only citing contexts, and (3) both context types.",5.2 Using Different Citation Context Types,[0],[0]
"Note that the content view remains the same across all three experiments.
",5.2 Using Different Citation Context Types,[0],[0]
The plot is showing that citing contexts are bringing in a significantly higher margin of knowledge compared with cited contexts.,5.2 Using Different Citation Context Types,[0],[0]
"This is consistent over different training set sizes, as shown in the figure, with a more prominent impact when a small training size is used, i.e. 5-30%.",5.2 Using Different Citation Context Types,[0],[0]
"The fact that the citing contexts achieve higher F1-score than cited contexts is consistent with the intuition that when citing a paper y, an author generally summarizes the main ideas from y using important words from a target paper x, making the citing contexts to have higher overlap with words from x.",5.2 Using Different Citation Context Types,[0],[0]
"In turn, a paper z that cites x may use paraphrasing to summarize ideas from x with words more similar to those from the content of z.
When the two types of contexts are used, cotraining achieves higher results compared with cases when only one context type is used.",5.2 Using Different Citation Context Types,[0],[0]
This experiment shows that our method can be applied for both old and new research articles.,5.2 Using Different Citation Context Types,[0],[0]
Citing contexts will be available in the text of the target paper and are independent of the existence of the cited contexts.,5.2 Using Different Citation Context Types,[0],[0]
What are the most informative words from each view: document content and citation contexts?,5.3 Informative Features,[0],[0]
Figure 7 shows the words from each view that are most useful for our topic classification task.,5.3 Informative Features,[0],[0]
"The larger the word, the more informative is for our
task.",5.3 Informative Features,[0],[0]
"To determine the informativeness of a word, we used its Information Gain score.",5.3 Informative Features,[0],[0]
"For these experiments, we used training sets consisting of 30% of the instances, setting in which we achieved the best results on the validation and test sets using our proposed co-training approach.
",5.3 Informative Features,[0],[0]
"As can be seen, the two word clouds have a high word overlap.",5.3 Informative Features,[0],[0]
"Words such as agent, database or query are almost equally important in the two views, dominating both clouds.",5.3 Informative Features,[0],[0]
"However, differences can be observed.",5.3 Informative Features,[0],[0]
"For example, words like learning, multi-agent or interface are more important in the content view.",5.3 Informative Features,[0],[0]
"On the other hand, words such as document or text achieve a higher information gain score for the citation contexts view.",5.3 Informative Features,[0],[0]
"Table 2 summarizes the results obtained by all the baselines used so far, in comparison with our proposed co-training method.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"For this experiment, we show the training percentage used, the precision, recall and F1-score for each method, in the setting in which it returned the best results.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"All mea-
sures were averaged after 10 runs with 10 different seeds.
",5.4 Co-Training vs. All Other Approaches,[0],[0]
"The results in Table 2 show that the proposed co-training method outperforms all compared models, reaching the highest F1-score of 0.742, while using the smallest amount of labeled documents, i.e. 30%.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"Using only the citing contexts, the performance is similar to that of co-training when both context types are used.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"However, using only the cited contexts, the performance decreases compared to that of the full model that uses both context types.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"We see that the citing contexts perform better, reaching an F1-score value of 0.740 compared against 0.714 when only cited contexts are used.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"Moreover, the method that uses only the citing contexts is using 10% less labeled data.
",5.4 Co-Training vs. All Other Approaches,[0],[0]
Self-training and EM show decreased performance compared with co-training.,5.4 Co-Training vs. All Other Approaches,[0],[0]
"Late Fusion outperforms Early Fusion, i.e., 0.738 vs. 0.714, both obtaining lower results than co-training, while using significantly more labeled data.
",5.4 Co-Training vs. All Other Approaches,[0],[0]
"The last two lines of the table show the results when all documents (except those in the validation and test), are used for training, in a supervised framework.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"As can be seen, a supervised method that uses only citations will achieve a higher performance, compared against a method that uses titles and abstracts.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"Nonetheless, co-training obtains higher results than both fully supervised approaches, while using only 30% of the labeled data.",5.4 Co-Training vs. All Other Approaches,[0],[0]
"In this paper, we studied the problem of using citation contexts in order to predict more accurately the topic of a research article.",6 Conclusion and Future Work,[0],[0]
"We showed that a co-training technique, which uses the paper content and its citation contexts as two conditionally independent and sufficient views of the data, can effectively incorporate cheap, unlabeled data to improve the classification performance and to reduce the need of labeled examples to only a fraction.",6 Conclusion and Future Work,[0],[0]
"The results of the experiments showed that the proposed approach performs better than other semi-supervised and supervised methods.
",6 Conclusion and Future Work,[0],[0]
This study also shows that citation contexts are rich sources of information that can be successfully used in various IR and NLP tasks.,6 Conclusion and Future Work,[0],[0]
We showed that document content and citation contexts unified under the same algorithm can dramatically decrease the annotation costs as well.,6 Conclusion and Future Work,[0],[0]
"In the future, we plan to extend co-training to include active learning for more robust classification.",6 Conclusion and Future Work,[0],[0]
"Moreover, it would be interesting to extend the co-training approach to multi-views that could potentially handle more than two feature spaces, e.g., it could include topics by Latent Dirichlet Allocation (Blei et al., 2003) as an additional view.",6 Conclusion and Future Work,[0],[0]
We are thankful to Dr. Lise Getoor for making the Citeseerx labeled subset publicly available.,Acknowledgments,[0],[0]
"We are also grateful to Dr. C. Lee Giles for the CiteSeerx data, which helped extract the citation contexts of the research papers in the collection.",Acknowledgments,[0],[0]
We very much thank our anonymous reviewers for their constructive feedback.,Acknowledgments,[0],[0]
This research is supported in part by the NSF award #1423337 to Cornelia Caragea.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions expressed here are those of the authors and do not necessarily reflect the views of NSF.",Acknowledgments,[0],[0]
"With the exponential growth of scholarly data during the past few years, effective methods for topic classification are greatly needed.",abstractText,[0],[0]
Current approaches usually require large amounts of expensive labeled data in order to make accurate predictions.,abstractText,[0],[0]
"In this paper, we posit that, in addition to a research article’s textual content, its citation network also contains valuable information.",abstractText,[0],[0]
We describe a co-training approach that uses the text and citation information of a research article as two different views to predict the topic of an article.,abstractText,[0],[0]
"We show that this method improves significantly over the individual classifiers, while also bringing a substantial reduction in the amount of labeled data required for training accurate classifiers.",abstractText,[0],[0]
Co-Training for Topic Classification of Scholarly Data,title,[0],[0]
"Combinatorial optimization is a important topic of computer science and discrete mathematics, with a wide spectrum of applications ranging from resource allocation and job scheduling, to automated planning and configuration softwares.",1. Introduction,[0],[0]
"A common problem is to minimize a modular loss function ` over a discrete space S ⊆ {0, 1}d of feasible solutions represented in a concise manner by a set of combinatorial constraints.",1. Introduction,[0],[0]
"In the offline version of this problem, all information necessary to define the optimization task is available beforehand, and the challenge is to develop algorithms which are provably or practically better than enumerating all feasible solutions.",1. Introduction,[0],[0]
"Contrastingly, in the online version of this problem (Audibert et al., 2014), the objective function ` is subject to change over time.",1. Introduction,[0],[0]
"The challenge here is more acute, since the optimization algorithm is required to perform repeated choices on S so as to minimize their average cost in the long run.
",1. Introduction,[0],[0]
"*Equal contribution 1CRIL, CNRS UMR 8188, Université d’Artois, France.",1. Introduction,[0],[0]
"Correspondence to: Frederic Koriche <koriche@cril.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Conceptually, an online combinatorial optimization problem can be cast as a repeated prediction game between a learning algorithm and its environment (Audibert et al., 2011; 2014).",1. Introduction,[0],[0]
"During each trial t, the learner chooses a feasible solution st from its decision set S and, simultaneously, the environment selects a loss vector `t ∈",1. Introduction,[0],[0]
"[0, 1]d.",1. Introduction,[0],[0]
"Then, the learner incurs the loss 〈`t, st〉 = ∑d i=1",1. Introduction,[0],[0]
"`t(i)st(i) and, in light of the feedback provided by its environment, updates its strategy in order to improve the chance of selecting better solutions on subsequent trials.
",1. Introduction,[0],[0]
"Several classes of combinatorial prediction games can be distinguished, depending on the type of decision set, and the type of observed feedback.",1. Introduction,[0],[0]
"In this paper, we focus on full information games in which it is assumed that the feedback supplied at trial t by the environment is the entire vector `t. On the other hand, we make very few assumptions about the decision set: S may be described by an arbitrary SAT formula, that is, any set of combinatorial constraints representable by Boolean clauses.",1. Introduction,[0],[0]
"As SAT encodings of discrete solution spaces are frequently used in academic and industrial applications (Biere et al., 2009), our setting covers an important class of combinatorial prediction games.
",1. Introduction,[0],[0]
"As usual, the performance of an online learning algorithm is measured according to two metrics.",1. Introduction,[0],[0]
"The first, called regret, measures the difference in cumulative loss between the algorithm and the best solution in hindsight.",1. Introduction,[0],[0]
"In this study, we make no assumption about the sequence of loss vectors; in particular `t may depend on the previous decisions s1, · · · , st−1 made by the learner.",1. Introduction,[0],[0]
"In such non-oblivious or adversarial environments, the learner is generally allowed to make decisions in a randomized way, and its predictive performance is measured by the expected regret:
RT = E",1. Introduction,[0],[0]
"[ T∑ t=1 〈`t, st〉 ] −min",1. Introduction,[0],[0]
"s∈S T∑ t=1 〈`t, s〉
The second metric is computational complexity, i.e. the amount of resources required to compute st at each round t, given the sequence of feedbacks observed so far.
",1. Introduction,[0],[0]
Related Work.,1. Introduction,[0],[0]
"In the literature of combinatorial prediction games, three main strategies have been proposed to attain an expected regret that is sublinear in the game horizon T and polynomial in the input dimension d. The
first, and arguably simplest strategy, is to Follow the Perturbed Leader (FPL): on each trial t, the learner draws at random a perturbation vector zt ∈ Rd, and then selects in S a minimizer of ηLt + zt, where η ∈ (0, 1] is a step-size parameter, andLt is the cumulative lossLt = `1 + · · ·+`t−1.",1. Introduction,[0],[0]
"Based on the pioneering work of Hannan (1957), refined in (Hutter & Poland, 2005; Kalai & Vempala, 2005), the FPL algorithm achieves an expected regret of O(d 32 √ T ).
",1. Introduction,[0],[0]
"The second strategy is based on the popular exponentially weighted average forecaster in the framework of prediction with expert advice (Cesa-Bianchi & Lugosi, 2006).",1. Introduction,[0],[0]
"The overall idea is to maintain a weight for each feasible solution s ∈ S, which decays exponentially according to the estimated cumulative loss of s. Specifically, on each trial t, the learner draws a solution st ∈ S at random from the exponential family pt(s) ∼ exp(−η〈Lt, s〉).",1. Introduction,[0],[0]
"This strategy, referred to as Expanded Hedge (EH) in (Koolen et al., 2010), attains an expected regret of O(d 32 √ T ).
",1. Introduction,[0],[0]
"Finally, the third strategy is to Follow the Regularized Leader, a paradigm often advocated in online convex optimization (Hazan, 2016).",1. Introduction,[0],[0]
"Here, the learner operates on the convex hull of S, denoted conv(S).",1. Introduction,[0],[0]
"On each trial t, the learner starts by choosing a point pt ∈ conv(S) that minimizes η〈L̂t,p〉 + F (p), where F is a regularization function.",1. Introduction,[0],[0]
"Next, pt is decomposed as a convex composition of feasible solutions in S, and then, a decision st is picked at random according to the resulting distribution.",1. Introduction,[0],[0]
"For modular loss functions, this strategy is equivalent to the Online Stochastic Mirror Descent (OSMD) algorithm (Audibert et al., 2014; Rajkumar & Agarwal, 2014), which iteratively performs a gradient descent in the dual space of conv(S) under F , and projects back to the primal space according to the Bregman divergence defined from F .",1. Introduction,[0],[0]
"Notably, when F is the Euclidean regularizer, OSMD coincides with the popular stochastic gradient descent (SGD) algorithm (Robbins & Monro, 1951).",1. Introduction,[0],[0]
"Alternatively, when F is the entropic regularizer, OSMD corresponds to the Component Hedge (CH) algorithm (Koolen et al., 2010), which achieves an optimal expected regret of O(d √ T ).
",1. Introduction,[0],[0]
"From the viewpoint of regret, the results outlined above indicate that few improvements remain to be made in full information games.",1. Introduction,[0],[0]
"However, we get a different picture if computational considerations are taken into account: all aforementioned algorithms rely on powerful oracles for making decisions in spaces S represented by combinatorial constraints.",1. Introduction,[0],[0]
"Namely, the EH algorithm is required, at each iteration, to sample a solution according to an exponential family over S , a problem which is generally #P-hard (Dyer et al., 2009).",1. Introduction,[0],[0]
"Similarly, the FPL strategy has to repeatedly solve a linear optimization task over S, which is generally NP-hard (Creignou et al., 2001).",1. Introduction,[0],[0]
"For the OSMD algorithm, and its specializations SGD and CH, the computational issue
is exacerbated by the fact that, even if the learner has access to a linear optimization oracle, it still has to perform, at each trial, a Bregman projection step for which the best known algorithms run in O(d6) time (Suehiro et al., 2012).
",1. Introduction,[0],[0]
"Although combinatorial prediction games are generally intractable, efficient implementations of sampling and optimization oracles may be obtained for several decision sets S. For example, when the feasible solutions in S coincide with the bases of a binary matroid, or the perfect matchings of a bipartite graph, linear optimization can be performed in polynomial time, and tractable forms of FPL and OSMD may be derived (Helmbold & Warmuth, 2009; Koolen et al., 2010; Takimoto & Hatano, 2013; Rajkumar & Agarwal, 2014).",1. Introduction,[0],[0]
"On the other hand, when the feasible solutions in S correspond to the paths or multi-paths of a rooted Directed Acyclic Graph (DAG), the sampling oracle may be implemented by the weight pushing technique (Mohri, 1998), that recursively evaluates the partition function of an exponential family over the edges of the input DAG.",1. Introduction,[0],[0]
"Based on this technique, tractable forms of EH can be derived (Takimoto & Warmuth, 2003; Rahmanian & Warmuth, 2017).
",1. Introduction,[0],[0]
Our Results.,1. Introduction,[0],[0]
Viewing feasible solutions as paths in a DAG is only one of many abstractions that have been proposed in the literature of circuit complexity for representing combinatorial spaces.,1. Introduction,[0],[0]
"In the related field of knowledge compilation (Darwiche & Marquis, 2002), various classes of Boolean circuits have been identified, each associated with a set of inference tasks which can be performed in polynomial time.",1. Introduction,[0],[0]
These theoretical results naturally motivate the following question: can we compile a set of constraints representing a combinatorial space S into a compact and Boolean circuit for which both solution sampling and linear optimization are tractable?,1. Introduction,[0],[0]
"By viewing the compilation process as a “pre-processing step”, we may get for free efficient implementations of sampling and optimization oracles, provided that the size of the resulting circuit is not too large.
",1. Introduction,[0],[0]
"The present study aims at solving combinatorial prediction games, by compiling decision sets into deterministic Decomposable Negation Normal Form (dDNNF) circuits (Darwiche, 2001).",1. Introduction,[0],[0]
"This class comes with generic compilers which take as input a SAT formula representing a decision set S, and return a dDNNF circuit C that encodes S (Darwiche, 2002; Lagniez & Marquis, 2017).",1. Introduction,[0],[0]
"Although the size of C may grow exponentially in the treewidth of the input formula, it is usually much smaller in practice; existing compilers are able to compress combinatorial spaces defined over thousands of variables and constraints.
",1. Introduction,[0],[0]
"With these compilation tools in hand, our contributions are threefold: (i) we show that for dDNNF circuits, the sampling oracle in EH and the linear optimization oracle in FPL, run in linear time using a simple variant of the weight-
pushing technique; (ii) for the SGD and CH strategies, we develop a Bregman projection-decomposition method that uses O(d2 ln(dT ))",1. Introduction,[0],[0]
"calls to the linear optimization oracle; (iii) we experimentally show on online configuration and planning tasks that EH and FPL are fast, but our variants of SGD and CH are more efficient to minimize the empirical regret.
",1. Introduction,[0],[0]
"Before proceeding to the core of the paper, we emphasize that the compilation approach to online optimization is not entirely new.",1. Introduction,[0],[0]
"Recently, Sakaue et.",1. Introduction,[0],[0]
"al. (2018) used the class of Ordered Binary Decision Diagrams (OBDDs) (Bryant, 1986) for implementing the EWA forecaster in combinatorial bandits.",1. Introduction,[0],[0]
"Here, S is described by a graph over d edges, together with a constraint specifying the type of objects we desire (e.g. paths or cliques).",1. Introduction,[0],[0]
"By contrast, our study assumes that S is described with an arbitrary set of Boolean constraints.",1. Introduction,[0],[0]
"So, both studies are targeting different classes of combinatorial prediction games.",1. Introduction,[0],[0]
"Moreover, it is known that dDNNF is strictly more succinct than OBDD (Darwiche & Marquis, 2002).",1. Introduction,[0],[0]
"Namely, any OBDD can be transformed in linear time and space into an equivalent dDNNF circuit, but the converse is not true: dDNNF includes simple circuits which require an exponential size representation in OBDD.",1. Introduction,[0],[0]
"In fact, the key point of compiling combinatorial prediction games is to use both tractable and succinct languages, for allowing prediction strategies to be efficient on a wide variety of combinatorial domains.",1. Introduction,[0],[0]
"For the combinatorial prediction games considered in this paper, we assume that the input decision space S is defined from a set of n binary-valued attributes, and we use X = {x1, · · · , xd}, where d = 2n, to denote the set of all “attribute-value” pairs, called literals.",2. Tractable Inference via Compilation,[0],[0]
"A solution is a vector s ∈ {0, 1}d such that s(i) + s(j) = 1 for every pair of distinct literals xi, xj ∈ X defined on the same attribute.",2. Tractable Inference via Compilation,[0],[0]
"Thus, ‖s‖1 = n for any feasible solution s ∈ S.
An NNF circuit over X is a rooted DAG, whose internal nodes are labeled by ∨ (or-node) or ∧ (and-node), and whose leaves are labeled by either a literal in X , or a constant in {0, 1}.",2. Tractable Inference via Compilation,[0],[0]
"The size of C, denoted |C|, is given by the number of its edges.",2. Tractable Inference via Compilation,[0],[0]
"The set of attributes occurring in the subgraph of C rooted at some node c is denoted att(c).
",2. Tractable Inference via Compilation,[0],[0]
"For the sake of clarity, we assume that any NNF circuit C satisfies two basic properties, namely (i) any internal node c in C has exactly two children, denoted cl and cr, and (ii) att(cl) = att(cr) 6=",2. Tractable Inference via Compilation,[0],[0]
∅ for any or-node c of C. An NNF circuit satisfying both conditions is called smooth.,2. Tractable Inference via Compilation,[0],[0]
"As shown in (Darwiche, 2001), any Boolean circuitC can be transformed in to an equivalent smooth NNF circuit of size linear in |C|.
",2. Tractable Inference via Compilation,[0],[0]
"By viewing literals as “input gates”, and nodes as “output gates”, we may specify various inference tasks on Boolean
circuits, depending on the type of input values and the semantics of nodes.",2. Tractable Inference via Compilation,[0],[0]
"As suggested by Friesen & Domingos for sum-product functions (2016), inference tasks can be captured through semiring operations.",2. Tractable Inference via Compilation,[0],[0]
"To this point, recall that a commutative semiring is a tuple (R,⊕,⊗,⊥,>) such that R is a set including the elements ⊥ and >, ⊕ is a associative and commutative binary operation on R with identity element ⊥, ⊗ is an associative binary operation on R with identity element > and absorbing element ⊥, and the operator ⊗ left and right distributes over the operator ⊕.
Inference tasks on an NNF circuit C are defined using a commutative semiring Q = (R,⊕,⊗,⊥,>) and an input vector w ∈ Rd.",2. Tractable Inference via Compilation,[0],[0]
"The output of a node c in C for Q given w is denoted Q(c |w), and recursively defined by
Q(c |w) =  w(i) if c is the literal xi, > if c is the constant 1, ⊥ if c is the constant 0, Q(cl |w)⊕Q(cr |w) if c is a node ∨, and Q(cl |w)⊗Q(cr |w) if c is a node ∧
By Q(C |w), we denote the output of the root of C for Q givenw.",2. Tractable Inference via Compilation,[0],[0]
"Of particular interest in this study are the semirings described in Table 1; maxmin, minsum, and sumprod, and used to capture the inference tasks of model checking, linear optimization, and model sampling, respectively.",2. Tractable Inference via Compilation,[0],[0]
"Given an NNF circuit C over X , the task of model checking is to decide whether a Boolean input s ∈ {0, 1}d is true in C according to the propositional semantics of nodes.",2.1. Model Checking,[0],[0]
"Obviously, s is a model of C iff maxmin(C | s) = 1, which can be determined in O(|C|) time.",2.1. Model Checking,[0],[0]
"An NNF circuit C is called a representation of a set of feasible solutions S ⊆ {0, 1}d if sol(C) = S, where sol(C) is the set of models of C.
Apart from model checking, virtually all inference tasks in NNF circuits are NP-hard.",2.1. Model Checking,[0],[0]
"Indeed, the NNF language covers the class of SAT formulas.",2.1. Model Checking,[0],[0]
"So, we need to refine this class in order to get tractable forms of optimization and sampling.",2.1. Model Checking,[0],[0]
"A Boolean circuit C is decomposable if for every and-node c of C, we have att(cl) ∩ att(cr) = ∅.",2.2. Decomposability and Optimization,[0],[0]
The class of decomposable NNF circuits is denoted DNNF.,2.2. Decomposability and Optimization,[0],[0]
"For such circuits, which are similar to Boolean sum-product networks (Poon & Domingos, 2011), we can get an efficient implementation of the linear optimization oracle.
",2.2. Decomposability and Optimization,[0],[0]
Proposition 1.,2.2. Decomposability and Optimization,[0],[0]
"Let S ⊆ {0, 1}d be a (nonempty) decision set represented by a DNNF circuit C, and let w ∈ Rd be a modular objective.",2.2. Decomposability and Optimization,[0],[0]
"Then, finding a minimizer of w in S can be done in O(|C|) time.
",2.2. Decomposability and Optimization,[0],[0]
Proof.,2.2. Decomposability and Optimization,[0],[0]
"Based on the minsum semiring, we have
min s∈S 〈w, s〉 = min s∈sol(C) 〈w, s〉 = minsum(C |w)
",2.2. Decomposability and Optimization,[0],[0]
This observation suggests a two-pass weight pushing method for finding a minimizer s of w in S in O(|C|) time.,2.2. Decomposability and Optimization,[0],[0]
"Given a topological ordering of C, the first pass stores the value Q(c |w) of each node c ∈ C, using Q = minsum.",2.2. Decomposability and Optimization,[0],[0]
"The second pass performs a top-down search over C, by selecting all children of a visited and-node, and by selecting exactly one child c′ ∈ {cl, cr} of a visited or-node c such that Q(c′ | w) = Q(c | w).",2.2. Decomposability and Optimization,[0],[0]
"Let T be the corresponding search tree, and let s ∈ {0, 1}d be the indicator vector of the set of literals occurring in T .",2.2. Decomposability and Optimization,[0],[0]
"By construction, we have Q(T |w) = Q(C |w), which implies that s is a minimizer ofw.",2.2. Decomposability and Optimization,[0],[0]
"Since S is not empty, we know that Q(C |w) <",2.2. Decomposability and Optimization,[0],[0]
"+∞. This, together with the fact",2.2. Decomposability and Optimization,[0],[0]
that minmax(T,2.2. Decomposability and Optimization,[0],[0]
| s) = 1,2.2. Decomposability and Optimization,[0],[0]
"whenever Q(T |w) < +∞, implies that s ∈ S.",2.2. Decomposability and Optimization,[0],[0]
"As the problem of counting the number of models in a DNNF circuit is #P-hard (Darwiche & Marquis, 2002), we need to refine this class in order to get an efficient implementation of the sampling oracle.",2.3. Determinism and Sampling,[0],[0]
"To this end, an NNF circuit C is called deterministic if minmax(cl | s) +",2.3. Determinism and Sampling,[0],[0]
minmax(cr | s) ≤ 1 for every or-node c ∈ C and every feasible solution s.,2.3. Determinism and Sampling,[0],[0]
"The class of deterministic DNNF circuits is denoted dDNNF.
",2.3. Determinism and Sampling,[0],[0]
Proposition 2.,2.3. Determinism and Sampling,[0],[0]
"Let S ⊆ {0, 1}d be a decision set represented by a dDNNF circuit C, and for a vector w ∈ Rd, let Pw be the exponential family on S given by:
Pw(s) = exp〈w, s〉∑
s′∈S exp〈w, s′〉
Then, sampling s ∼ Pw can be done in O(|C|) time.
",2.3. Determinism and Sampling,[0],[0]
Proof.,2.3. Determinism and Sampling,[0],[0]
"Based on the sumprod semiring, we have
Pw(s) = exp〈w, s〉∑
s′∈sol(C) exp〈w, s′〉 =
exp〈w, s〉 sumprod(C |w′)
wherew′",2.3. Determinism and Sampling,[0],[0]
"= (ew(1), · · · , ew(d)).",2.3. Determinism and Sampling,[0],[0]
"Again, such an equivalence suggests a two-pass weight pushing method for sampling a solution s according to Pw in O(|C|) time.",2.3. Determinism and Sampling,[0],[0]
"Using a topological ordering of C, the first pass stores the values Q(c |w′), where Q = sumprod.",2.3. Determinism and Sampling,[0],[0]
"The second pass performs a top-down randomized search over C, by selecting all children of a visited and-node, and by drawing at random one of the children of a visited or-node c according to the distribution p(cl) =",2.3. Determinism and Sampling,[0],[0]
Q(cl | w′)/Q(c | w′) and p(cr),2.3. Determinism and Sampling,[0],[0]
= 1− p(cl).,2.3. Determinism and Sampling,[0],[0]
"Let T be the tree of visited nodes, and s be the indicator vector of the literals in T .",2.3. Determinism and Sampling,[0],[0]
"Since S 6= ∅, we must have Q(C |w) > 0.",2.3. Determinism and Sampling,[0],[0]
"Thus, each Bernoulli test performed in T is valid, and hence, s ∈ S .",2.3. Determinism and Sampling,[0],[0]
"For any literal xi occurring in T , let p(xi) denote the probability of the (unique) path connecting the root to xi.",2.3. Determinism and Sampling,[0],[0]
"By a telescoping product of Bernoulli distributions, we get that p(xi) = ew(i)/Q(C |w′).",2.3. Determinism and Sampling,[0],[0]
"Therefore, p(s) = ∏",2.3. Determinism and Sampling,[0],[0]
"i:s(i)=1 p(xi) = Pw(s), as desired.
",2.3. Determinism and Sampling,[0],[0]
We close this section by highlighting some interesting subclasses of dDNNF.,2.3. Determinism and Sampling,[0],[0]
"A decision node is an or-node of the form (xi ∧ c′l) ∨ (xi ∧ c′r), where xi and xi are opposite literals, and c′l and c ′",2.3. Determinism and Sampling,[0],[0]
r are arbitrary nodes.,2.3. Determinism and Sampling,[0],[0]
"The class of Free Binary Decision Diagrams (FBDD) is the subset of dDNNF in which every or-node is a decision node, and at least one child of any and-node is a literal (Wegener, 2000).",2.3. Determinism and Sampling,[0],[0]
"For example, if in the dDNNF circuit of Figure 1, we replace the or-node (in blue) by a simple literal, say x3, then we get an FBDD circuit.",2.3. Determinism and Sampling,[0],[0]
The family of Ordered Binary Decision Diagrams (OBDD) is the subclass of FBDD obtained by imposing a fixed ordering on the decision variables.,2.3. Determinism and Sampling,[0],[0]
"Alternatively, the well-known family of (Binary) Decision Trees (DT) is the subclass of FBDD circuits for which the primal graph is cycle-free.",2.3. Determinism and Sampling,[0],[0]
"Since all these classes are (strict) subsets of dDNNF, they admit linear-time algorithms for linear optimization and model sampling.",2.3. Determinism and Sampling,[0],[0]
"After an excursion into compilation languages, we are now ready to provide efficient characterizations of combinatorial prediction strategies.",3. Tractable Prediction via Compilation,[0],[0]
"Our results are summarized in Table 2.
",3. Tractable Prediction via Compilation,[0],[0]
"Notably, using the fact that ‖s‖1 = d/2, the regret bounds for EH and FPL can easily be derived from (Audibert et al., 2011) and (Hutter & Poland, 2005), respectively.",3. Tractable Prediction via Compilation,[0],[0]
Both strategies are straightforward to implement on dDNNF circuits.,3. Tractable Prediction via Compilation,[0],[0]
"Indeed, recall that EH draws, at each trial t, a feasible solution st ∈ S at random according to the distribution P−ηLt , where Lt = `1 + · · ·+ `t−1.",3. Tractable Prediction via Compilation,[0],[0]
"So, by direct application of Proposition 2, this strategy runs in O(|C|) time per round, using a dDNNF representation C of the decision set S. For the FPL strategy, each round t is performed by choosing a minimizer st ∈ S of the objective function ηLt − zt, where zt ∈ Rd is a perturbation vector whose components are independent exponentially distributed random variables.",3. Tractable Prediction via Compilation,[0],[0]
"By Proposition 1, the FPL strategy also runs inO(|C|) time per round, using a dDNNF encoding C of S, and the fact that |C| is in Ω(d).
",3. Tractable Prediction via Compilation,[0],[0]
"However, the OSMD strategy and its specializations, SGD and CH, require more attention, due to the projectiondecomposition step involved at each iteration.",3. Tractable Prediction via Compilation,[0],[0]
"The overall idea of Online Mirror Descent (OMD) is to “follow the regularized leader” through a primal-dual approach (Nemirovski & Yudin, 1983; Beck & Teboulle, 2003).",3.1. Online Stochastic Mirror Descent,[0],[0]
"Let K be a convex set, and let int(K) denotes its interior.",3.1. Online Stochastic Mirror Descent,[0],[0]
"Given a regularization function F defined on K, OMD iteratively performs a gradient descent in the interior of the dual space K∗, and projects back the dual point into the primal space K.",3.1. Online Stochastic Mirror Descent,[0],[0]
"The connection between K and K∗ is ensured using the gradients ∇F and ∇F ∗, where F ∗ is the convex conjugate of F , defined on K∗.",3.1. Online Stochastic Mirror Descent,[0],[0]
"The projection step is captured by the Bregman divergence of F , which is a function BF : K × int(K)→ R given by:
BF (p, q) = F (p)− F (q)− 〈∇F (q),p− q〉
In the stochastic variant of OMD, introduced by Audibert et.",3.1. Online Stochastic Mirror Descent,[0],[0]
"al. (2011; 2014), and specified in Algorithm 1, each projection is performed onto the subset conv(S) of K, and the resulting point pt is decomposed into a convex combination of feasible solutions in S, from which one is picked at random for the prediction task.
",3.1. Online Stochastic Mirror Descent,[0],[0]
"Algorithm 1 OSMD
Input: decision set S ⊆ {0, 1}d, horizon T ∈ Z+ Parameters: regularizer F on K ⊇ conv(S), step-size η ∈ (0, 1]
set u1 = 0 for t",3.1. Online Stochastic Mirror Descent,[0],[0]
"= 1 to T do
set pt ∈ Argminp∈conv(S)BF (p,∇F ∗(ut))",3.1. Online Stochastic Mirror Descent,[0],[0]
"play st ∼ pt and observe `t set ut+1 = ∇F (pt)− η`t
end for
For common regularizers, the gradient∇F (pt) and its dual ∇F ∗(ut) are easily calculable, and we shall assume that the time spent for their construction is negligible compared with the running time of the linear optimization oracle.",3.1. Online Stochastic Mirror Descent,[0],[0]
"In fact, the computational bottleneck of OSMD is to find a minimizer pt of BF (p,∇F ∗(ut)) in the convex hull of S, and to decompose pt into a convex combination of solutions in S. Fortunately, under reasonable assumptions about the curvature of BF , this projection-decomposition step can be efficiently computed, using recent results in projection-free convex optimization algorithms.
",3.1. Online Stochastic Mirror Descent,[0],[0]
"To this end, we need additional definitions.",3.1. Online Stochastic Mirror Descent,[0],[0]
"For a convex set K, a differentiable function f : K → R is called α-strongly convex with respect to a norm ‖ · ‖ if
f(p′)− f(p) ≥ 〈∇f(p),p′ − p〉+ α 2 ‖p′",3.1. Online Stochastic Mirror Descent,[0],[0]
"− p‖2
",3.1. Online Stochastic Mirror Descent,[0],[0]
"Furthermore, f is called β-smooth1 with respect to ‖ · ‖ if
f(p′)− f(p) ≤",3.1. Online Stochastic Mirror Descent,[0],[0]
"〈∇f(p),p′ − p〉+ β 2 ‖p′",3.1. Online Stochastic Mirror Descent,[0],[0]
"− p‖2
Based on these notions, we say that a Bregman divergence BF has the condition number β/α if BF is both α-strongly convex and β-smooth with respect to the Euclidean norm ‖ · ‖2 in its first argument.",3.1. Online Stochastic Mirror Descent,[0],[0]
"For such regularizers, the next result states that the projection-decomposition step can be approximated in low polynomial time, by exploiting the Pairwise Conditional Gradient (PCG) method, a variant of the Frank-Wolfe convex optimization algorithm, whose convergence rate has been analyzed in (Lacoste-Julien & Jaggi, 2015; Garber & Meshi, 2016; Bashiri & Zhang, 2017).
",3.1. Online Stochastic Mirror Descent,[0],[0]
Lemma 1.,3.1. Online Stochastic Mirror Descent,[0],[0]
"Let S ⊆ {0, 1}d be a decision set represented by a dDNNF circuit C, and F be a regularizer onK ⊇ conv(S) such that BF has condition number β/α.",3.1. Online Stochastic Mirror Descent,[0],[0]
"Then, for any q ∈ int(K) and ∈ (0, 1), one can find inO(βαd
2|C|ln βd ) time a convex decomposition of p ∈ conv(S) such that
BF (p, q)− min p′∈conv(C) BF (p ′, q) ≤
1This notion of geometric smoothness should not be confused with the structural smoothness of NNF circuits in Section 2.
",3.1. Online Stochastic Mirror Descent,[0],[0]
"Algorithm 2 PCG
Input: S ⊆ {0, 1}d, f : K → R, m ∈ Z+",3.1. Online Stochastic Mirror Descent,[0],[0]
"Parameters: step-sizes {ηj}mj=1
let p1 be some point in S for j",3.1. Online Stochastic Mirror Descent,[0],[0]
"= 1 to m do
let ∑j i=1",3.1. Online Stochastic Mirror Descent,[0],[0]
"αisi be the convex decomposition of pj set s+j ∈ Argminp∈conv(S)〈∇f(pj),p〉 set s−j ∈ Argmins∈{s1,···,sj}〈−∇f(pj), s〉 set pj+1 = pj +",3.1. Online Stochastic Mirror Descent,[0],[0]
"ηj(s+j − s − j )
end for
Proof.",3.1. Online Stochastic Mirror Descent,[0],[0]
"Observe that conv(S) is a simplex-like polytope (Bashiri & Zhang, 2017), defined by the linear constraints p ≥ 0, ∑N i=1",3.1. Online Stochastic Mirror Descent,[0],[0]
"αisi = p, α ≥ 0, and ∑N i=1",3.1. Online Stochastic Mirror Descent,[0],[0]
"αi = 1, where N = |S|.",3.1. Online Stochastic Mirror Descent,[0],[0]
"So, conv(S) and BF satisfy the conditions of Theorem 1 in (Garber & Meshi, 2016), and using the step-sizes advocated by the authors, we get that
BF (pm, q)−BF (p∗, q) ≤ βd
2 exp
( − α
8βd2 m ) where pm is the point obtained at the last iteration of PCG, and p∗ is the (unique) minimizer of BF (p, q) on conv(S).",3.1. Online Stochastic Mirror Descent,[0],[0]
"Therefore, after m ≥ (8d2β/α) ln(βd/(2 )) iterations, we haveBF (pm, q)−BF (p∗, q) ≤ .",3.1. Online Stochastic Mirror Descent,[0],[0]
"Finally, since each iteration of PCG makes one call to the linear optimization oracle, the runtime complexity follows from Proposition 1.
",3.1. Online Stochastic Mirror Descent,[0],[0]
"By OSMD+PCG, we denote the refined version of the OSMD algorithm that uses the PCG method at each trial t in order to approximate the Bregman projection-decomposition step.",3.1. Online Stochastic Mirror Descent,[0],[0]
"In addition to a regularizer F and a step-size η, OSMD+PCG takes as parameters a sequence { t}Tt=1 such that
BF (pt, qt)−BF (p∗t , qt) ≤ t
where pt is the point returned by PCG, qt = ∇F ∗(ut), and p∗t is the minimizer of BF (p, qt) over conv(S).
",3.1. Online Stochastic Mirror Descent,[0],[0]
Theorem 1.,3.1. Online Stochastic Mirror Descent,[0],[0]
"Suppose that OSMD+PCG takes as input a dDNNF representation C of a decision set S ⊆ {0, 1}d, and a horizon T , and uses a regularizer F on K ⊇ conv(S) such thatBF has condition number β/α, together with a stepsize η ∈ (0, 1] and a sequence of { t}Tt=1 such that t = γ/t2 for γ > 0.",3.1. Online Stochastic Mirror Descent,[0],[0]
"Then, OSMD+PCG attains the expected regret
(1) RT ≤
√ 2γd
α (lnT + 1) +
1 η max s∈S BF (s,p ∗ 1)
+ 1
η T∑ t=1 BF∗(∇F (p∗t )− η`t,∇F (pt))
with a per-round running time in O ( β
α d2|C|ln βdT γ
) .
",3.1. Online Stochastic Mirror Descent,[0],[0]
Proof.,3.1. Online Stochastic Mirror Descent,[0],[0]
Let s∗ ∈ S be the optimal solution chosen with the benefit of hindsight.,3.1. Online Stochastic Mirror Descent,[0],[0]
"By decomposing the regret, we have
RT = T∑ t=1 〈`t,p∗t − s∗〉+ T∑ t=1",3.1. Online Stochastic Mirror Descent,[0],[0]
"E〈`t, st − p∗t 〉 (2)
",3.1. Online Stochastic Mirror Descent,[0],[0]
"By Theorem 2 in (Audibert et al., 2014), the first term in (2) is bounded by the last two terms in (1).",3.1. Online Stochastic Mirror Descent,[0],[0]
"For the second term in (2), we get from the Cauchy-Schwarz inequality that
E〈`t, st − p∗t 〉 ≤ ‖`t‖2‖pt − p∗t ‖2≤",3.1. Online Stochastic Mirror Descent,[0],[0]
"√ d‖pt − p∗t ‖2
Moreover, by applying the Generalized Pythagorean Theorem (Cesa-Bianchi & Lugosi, 2006), we know that BF (p, qt) ≥ BF (p,p∗t ) + BF (p∗t , qt), for any p ∈ conv(S).",3.1. Online Stochastic Mirror Descent,[0],[0]
"Using p = pt and rearranging,
BF (pt,p ∗ t ) ≤ BF (pt, qt)−BF (p∗t , qt) ≤ t (3)
Since BF is α-strongly convex with respect to ‖ · ‖2 in its first argument, we also have α2 ‖pt",3.1. Online Stochastic Mirror Descent,[0],[0]
"− p ∗ t ‖22≤ BF (pt,p∗t ).",3.1. Online Stochastic Mirror Descent,[0],[0]
"Thus by plugging this inequality into (3), we get that E〈`t, st − p∗t 〉 ≤ √ 2d t/α.",3.1. Online Stochastic Mirror Descent,[0],[0]
"Finally, by substituting t with ρ/t2, summing other T , and applying the logarithmic bound on harmonic series, we obtain the desired result.",3.1. Online Stochastic Mirror Descent,[0],[0]
The (online) SGD algorithm is derived from OSMD using the Euclidean regularizer F (p) = 12 ‖p‖ 2 2.,3.2. Stochastic Gradient Descent,[0],[0]
"In this simple framework, the primal and dual spaces coincide with Rd, and hence, F ∗(u) = u, ∇F (p) = p, and ∇F ∗(u) =",3.2. Stochastic Gradient Descent,[0],[0]
"u. Furthermore, BF has the condition number 1/1, since BF (p, q) = 1 2 ‖p− q‖ 2 2.",3.2. Stochastic Gradient Descent,[0],[0]
"We denote by SGD+PCG the instance of OSMD+PCG defined on the Euclidean regularizer.
",3.2. Stochastic Gradient Descent,[0],[0]
Proposition 3.,3.2. Stochastic Gradient Descent,[0],[0]
"The SGD+PCG algorithm achieves an expected regret bounded by d( √ T + lnT + 1) with a per-round runtime complexity in O(d2|C|ln(dT )) using η = 1/ √ T and γ = d/2.
Proof.",3.2. Stochastic Gradient Descent,[0],[0]
"This simply follows from Theorem 1, together with the fact that maxs∈S BF (s,p∗1) ≤",3.2. Stochastic Gradient Descent,[0],[0]
d and ‖`t‖22≤ d.,3.2. Stochastic Gradient Descent,[0],[0]
The CH algorithm is derived from OSMD using the entropic regularizer F (p) = ∑d i=1,3.3. Component Hedge,[0],[0]
"p(i)(ln p(i)− 1), for which the
conjugate is F ∗(u) = ∑d i=1",3.3. Component Hedge,[0],[0]
expu(i).,3.3. Component Hedge,[0],[0]
"Here, we cannot find a finite condition number for the associated divergence BF (p, q) = ∑d i=1",3.3. Component Hedge,[0],[0]
p(i),3.3. Component Hedge,[0],[0]
ln p(i) q(i),3.3. Component Hedge,[0],[0]
− (p(i),3.3. Component Hedge,[0],[0]
"− q(i)), since its gradient is unbounded.",3.3. Component Hedge,[0],[0]
"This issue may, however, be circumvented using a simple trick advocated in (Krichene et al., 2015), which consists in replacing the entropic regularizer with the function Fδ(p) = F (p + δ), where
δ ∈ (0, 1) and δ = (δ, · · · , δ).",3.3. Component Hedge,[0],[0]
"For this function, the primal space is (−δ,+∞), and since F ∗δ (u) = F ∗(u) − 〈u, δ〉, the dual space is Rd.",3.3. Component Hedge,[0],[0]
"It is easy to show that
∂Fδ(p)
∂p(i) = ln(p(i)",3.3. Component Hedge,[0],[0]
"+ δ)
∂F ∗δ",3.3. Component Hedge,[0],[0]
"(u)
∂u(i) = eu(i)",3.3. Component Hedge,[0],[0]
"− δ
BFδ(p, q) = BF (p+δ, q+δ) B ∗",3.3. Component Hedge,[0],[0]
"Fδ (u,v) =",3.3. Component Hedge,[0],[0]
"B∗F (u,v)
where B∗F (u,v) = ∑d i=1",3.3. Component Hedge,[0],[0]
"e
v(i)(ev(i)−u(i) +v(i)−u(i)−1).",3.3. Component Hedge,[0],[0]
"Furthermore, since the first and second order partial derivatives of B∗Fδ(p, q) at the coordinate p(i) are
∂BFδ(p, q)
∂p(i) =",3.3. Component Hedge,[0],[0]
"ln
p(i)",3.3. Component Hedge,[0],[0]
"+ δ
q(i) + δ
∂2BFδ(p, q)
∂2p(i)",3.3. Component Hedge,[0],[0]
"= ln
1
p(i)",3.3. Component Hedge,[0],[0]
"+ δ
it follows that BFδ has the condition number 1+δ/δ.",3.3. Component Hedge,[0],[0]
"Indeed, given an arbitrary point q ∈ int(−δ,+∞), let Hq(p) denote the the Hessian matrix of BFδ(p, q) at p ∈ conv(S).",3.3. Component Hedge,[0],[0]
"Then, for any z ∈ Rd, the diagonal entries of Hq(p) satisfy
1 1 + δ ≤ ∂
2BF (p, q)
∂2p(i) z(i)2 ≤ 1 δ
using the fact that p(i) ∈",3.3. Component Hedge,[0],[0]
"[0, 1].",3.3. Component Hedge,[0],[0]
"Thus, αI 4 Hq(p) 4 βI for α = 1/1+δ and β = 1/δ.",3.3. Component Hedge,[0],[0]
"In what follows, the instance of OSMD+PCG that uses Fδ as regularizer is called δ-CH+PCG.
Proposition 4.",3.3. Component Hedge,[0],[0]
"The δ-CH+PCG algorithm achieves an expected regret bounded by d(1 + 2δ)( √ T + lnT + 1)
with a per-round runtime complexity in O ( d2|C|/δ ln dT/δ ) using η = 1/√T and γ = 2d(1/2 + δ)/(1 + δ).
",3.3. Component Hedge,[0],[0]
Proof.,3.3. Component Hedge,[0],[0]
The runtime complexity simply follows from Theorem 1.,3.3. Component Hedge,[0],[0]
"The regret bound is obtained by bounding the second and third terms of (1), and using the above values for η and γ.",3.3. Component Hedge,[0],[0]
"Using s∗1 as a maximizer of the second term of (1), we have BFδ(s ∗ 1,p ∗ 1) = Fδ(s ∗ 1)",3.3. Component Hedge,[0],[0]
− Fδ(p∗1).,3.3. Component Hedge,[0],[0]
"Using the notation p̃1 = p∗1 + δ and r = d(1/2 + δ), we get that
Fδ(s ∗ 1)− Fδ(p∗1) ≤ d∑ i=1",3.3. Component Hedge,[0],[0]
p̃1(i),3.3. Component Hedge,[0],[0]
ln 1 p̃1(i) ≤,3.3. Component Hedge,[0],[0]
"r ln d r
which is bounded by r.",3.3. Component Hedge,[0],[0]
"For the third term of (1), observe that Fδ is 1(1+δ)d -strongly convex with respect to the norm ‖ · ‖1, since ‖p",3.3. Component Hedge,[0],[0]
− p′‖21≤ d‖p,3.3. Component Hedge,[0],[0]
− p′‖22.,3.3. Component Hedge,[0],[0]
"By Theorem 3 in (Kakade et al., 2012), it follows that F ∗δ is (1 + δ)d-smooth with respect to the norm ‖ · ‖∞.",3.3. Component Hedge,[0],[0]
"Therefore,
1 η BF∗(∇F (p∗t )− η`t,∇F (pt)) ≤",3.3. Component Hedge,[0],[0]
η 2,3.3. Component Hedge,[0],[0]
d(1,3.3. Component Hedge,[0],[0]
"+ δ)‖`t‖2∞
",3.3. Component Hedge,[0],[0]
which is bounded by ηr.,3.3. Component Hedge,[0],[0]
"In order to evaluate the performance of the different online combinatorial optimization strategies examined in Section 3, we have considered 16 instances of the SAT Library,2 described in Table 3.",4. Experiments,[0],[0]
"Namely, the first six rows of the table are (car) configuration tasks, while the remaining rows are planning problems.",4. Experiments,[0],[0]
"In the first four columns of the table are reported the name of the SAT instance, the number of attributes (d/2), the number of constraints (|SAT|), and the number |S| of feasible solutions.",4. Experiments,[0],[0]
"We have used the recent D4 compiler 3 (Lagniez & Marquis, 2017) for transforming SAT instances into dDNNF circuits.",4. Experiments,[0],[0]
"The size |C| of the compiled circuit is reported in the fifth column.
",4. Experiments,[0],[0]
"In order to simulate combinatorial prediction games, we have used the following protocol.",4. Experiments,[0],[0]
"Suppose that the set X = {x1, · · · , xd} of literals is sorted in a lexicographic way, so that for each odd integer i, the pair (xi, xi+1) encodes both configurations of the same binary attribute.",4. Experiments,[0],[0]
"First, we construct a vector µ0 of d/2 independent Bernoulli variables.",4. Experiments,[0],[0]
"At each round t ∈ {1, · · · , T}, µt is set to µt−1 with probability 0.9, or picked uniformly at random from [0, 1]d/2 with probability 0.1.",4. Experiments,[0],[0]
"Then, the feedback supplied to the learner is a vector `t ∈ {0, 1}d such that `t(i) + `t(i + 1) = 1, and `t(i) = 1 with probability µt(i+1/2) for each odd integer i.",4. Experiments,[0],[0]
"So, `t(i + 1) = 1 with probability 1 − µt(i+1/2).",4. Experiments,[0],[0]
"Although this protocol is essentially stochastic, the environment secretly resets µt with probability 0.1 at each round to foil the learner.
",4. Experiments,[0],[0]
"The combinatorial prediction strategies were implemented in C++ and tested on a six-core Intel i7-5930K with 32 GiB RAM.4 For the FPL and EH algorithms, we used the step-size η reported in (Audibert et al., 2011) and (Hutter & Poland, 2005), respectively.",4. Experiments,[0],[0]
"Concerning the SGD+PCG and δ-CH+PCG algorithms, we used for η and γ the values determined by our theoretical analysis; the step-sizes {ηt} of PCG were computed from binary search as advocated by Garber & Meshi (2016) in their experiments, and the value of δ was fixed to 1/ln d",4. Experiments,[0],[0]
in order to keep a quadratic runtime complexity for δ-CH+PCG.,4. Experiments,[0],[0]
"Finally, the horizon T was set to 103, and a timeout of one day was fixed for learning.
",4. Experiments,[0],[0]
"In our experiments, the regret is measured by the difference in cumulative loss between the algorithm and the best feasible solution in hindsight, which is obtained using the linear optimization oracle at horizon T .",4. Experiments,[0],[0]
"This measure is averaged on 10 simulations, and divided by T to yield an average empirical regret.",4. Experiments,[0],[0]
"Similarly, the per-round runtimes (in seconds) are averaged on 10 simulations.",4. Experiments,[0],[0]
"The corresponding results are reported in the last four columns of Table 3.
2www.cs.ubc.ca/˜hoos/SATLIB/ 3www.cril.univ-artois.fr/KC/d4.html 4www.github.com/frederic-koriche/ccpg.git
Here, the symbol “−” indicates that the learner was not able to perform the T rounds in one day.",4. Experiments,[0],[0]
"From the viewpoint of regret, SGD+PCG and δ-CH+PCG outperform EH and FPL, which confirms our theoretical results.",4. Experiments,[0],[0]
We mention in passing that SGD+PCG and δ-CH+PCG are remarkably stable.,4. Experiments,[0],[0]
"Contrastingly, FPL exhibits a larger variance.
",4. Experiments,[0],[0]
"Concerning runtimes, EH and FPL are unsurprisingly faster than SGD+PCG and δ-CH+PCG.",4. Experiments,[0],[0]
"Notably, for the hard-to-compile instances c140-fc and c163-fw, both EH and FPL were able to perform each trial in few tens of seconds, while OSMD+PCG algorithms took several minutes per-round (and hence, they were unable to process 103 rounds in one day), due to the time spent in approximating the Bregman projection step.",4. Experiments,[0],[0]
"Yet, it is important to emphasize that the convergence rate of PCG is, in practice, much faster than the theoretical bound of Õ(d2|C|).",4. Experiments,[0],[0]
Both SGD+PCG and δ-CH+PCG were able to process nearly all instances in few seconds per round.,4. Experiments,[0],[0]
"For circuits of moderate size, all algorithms run in less than one second per trial.",4. Experiments,[0],[0]
"We also observed that SGD+PCG is slightly faster than δ-CH+PCG, especially for large domains where small values of δ have a significant impact on the the runtime complexity.",4. Experiments,[0],[0]
"In essence, SGD+PCG offers the best compromise between predictive performance and running time; since all feasible solutions are dense (‖s‖1= d/2), there is no significant difference in accuracy between SGD+PCG and δ-CH+PCG.",4. Experiments,[0],[0]
"We have proposed a general framework for compiling online combinatorial optimization problems, whose space of feasible solutions is described using a set of Boolean constraints.",5. Conclusions,[0],[0]
"Namely, we have focused on the class of dDNNF circuits which is endowed with fast inference algorithms for the linear optimization oracle and the sampling oracle.",5. Conclusions,[0],[0]
"Based on
this framework, we have shown than both EH and FPL admit fast implementation for tackling large scale online combinatorial problems.",5. Conclusions,[0],[0]
"A particular attention was devoted to the generic OSMD strategy, which involves a computationally expensive projection-decomposition step at each iteration.",5. Conclusions,[0],[0]
"To this point, we made use of projection-free algorithms, and in particular the PCG method, for approximating this operation.",5. Conclusions,[0],[0]
"The resulting algorithms, SGD+PCG and δ-CH-PCG, are inevitably slower than EH and FPL, but achieve a better regret performance, as corroborated by our experiments.
",5. Conclusions,[0],[0]
We conclude with a few remarks.,5. Conclusions,[0],[0]
"In light of the current results, a natural perspective of research is to extend our framework to other classes of combinatorial prediction games.",5. Conclusions,[0],[0]
"Notably, the semi-bandit setting seems within reach.",5. Conclusions,[0],[0]
"Indeed, the semi-bandit variant of EH, often referred to as EXP2 (Audibert et al., 2014), uses importance weights for estimating the loss at each iteration.",5. Conclusions,[0],[0]
"By simple adaptation of Proposition 2, such weights can be computed in linear time.",5. Conclusions,[0],[0]
"Similarly, the semi-bandit extension of FPL exploits the geometric sampling method for estimating loss vectors (Neu & Bartók, 2016).",5. Conclusions,[0],[0]
"Again, this iterative method can be implemented in linear time (per iteration) using Proposition 1.",5. Conclusions,[0],[0]
"Less obvious, however, is the extension of OSMD to semi-bandits: although the extension of CH achieves an optimal expected regret in this setting, its practical use remain limited due to projection-decomposition step.",5. Conclusions,[0],[0]
"An interesting open question is to determine whether a combination of CH with PCG is able, in the semi-bandit case, to achieve a quasi-optimal regret in low-polynomial time.",5. Conclusions,[0],[0]
"Of course, the bandit setting is even more challenging.",5. Conclusions,[0],[0]
"To this point, Sakaue et.",5. Conclusions,[0],[0]
"al. (2018) have paved the way using OBDDs for an efficient implementation of the COMBBAND algorithm (Cesa-Bianchi & Lugosi, 2012), Extending their approach to dDNNF, which is more succinct than OBDD, is a promising direction of future research.",5. Conclusions,[0],[0]
"In online optimization, the goal is to iteratively choose solutions from a decision space, so as to minimize the average cost over time.",abstractText,[0],[0]
"As long as this decision space is described by combinatorial constraints, the problem is generally intractable.",abstractText,[0],[0]
"In this paper, we consider the paradigm of compiling the set of combinatorial constraints into a deterministic and Decomposable Negation Normal Form (dDNNF) circuit, for which the tasks of linear optimization and solution sampling take linear time.",abstractText,[0],[0]
"Based on this framework, we provide efficient characterizations of existing combinatorial prediction strategies, with a particular attention to mirror descent techniques.",abstractText,[0],[0]
These strategies are compared on several real-world benchmarks for which the set of Boolean constraints is preliminarily compiled into a dDNNF circuit.,abstractText,[0],[0]
Compiling Combinatorial Prediction Games,title,[0],[0]
"Researchers have demonstrated impressive successes in building agents that can achieve excellent performance in difficult tasks, e.g. (Mnih et al., 2015; Silver et al., 2016).",1. Introduction,[0],[0]
"However, these successes have mostly been confined to situations where it is possible to train a large number of times on a single known task.",1. Introduction,[0],[0]
"On the other hand, in some situations, the tasks of interest are not known at training time or the space of tasks is so large that an agent will not realistically be able to train many times on any single task in the space.
",1. Introduction,[0],[0]
"We might hope that the tasks of interest are compositional: for example, cracking an egg is the same whether one is
*Equal contribution 1Facebook AI Research, New York, NY, USA 2New York University, New York, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Adam Lerer <alerer@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
making pancakes or an omelette.",1. Introduction,[0],[0]
"If the space of tasks we want an agent to be able to solve has compositional structure, then a state abstraction that exposes this structure could be used both to specify instructions to the agent, and to plan through sub-tasks that allow the agent to complete its instructions.
",1. Introduction,[0],[0]
In this work we show how to train agents that can solve complex tasks by planning over a sequence of previously experienced simpler ones.,1. Introduction,[0],[0]
"The training protocol relies on a state abstraction that is manually specified, consisting of a set of binary attributes designed to capture properties of the environment we consider important.",1. Introduction,[0],[0]
"These attributes, learned at train time from a set of (state, attribute) pairs, provide a natural way to specify tasks, and a natural state abstraction for planning.",1. Introduction,[0],[0]
"Once the agent learns how its actions affect the environment in terms of the attribute representation, novel tasks can be solved compositionally by executing a plan consisting of a sequence of transitions between abstract states defined by those attributes.",1. Introduction,[0],[0]
"Thus, as in (Dayan & Hinton, 1992; Dietterich, 2000; Vezhnevets et al., 2017), temporal abstractions are explicitly linked with state abstractions.
",1. Introduction,[0],[0]
"Our approach is thus a form of model-based planning, where the agent first learns a model of its environment (the mapping from states to attributes, and the attribute transition graph), and then later uses that model for planning.",1. Introduction,[0],[0]
"There is no supervision or reward given for completing the tasks of interest; outside of the (state, attribute) pairs, the agent receives no other reward or extrinsic supervision.",1. Introduction,[0],[0]
"In the experiments below, we will show empirically that this kind of approach can be useful on problems that can be challenging for standard reinforcement learning.
",1. Introduction,[0],[0]
We evaluate compositional planning in several environments.,1. Introduction,[0],[0]
"We first consider 3D block stacking, and show that we can compose single-action tasks seen during training to perform multi-step tasks.",1. Introduction,[0],[0]
"Second, we plan over multi-step policies in 2-D grid world tasks.",1. Introduction,[0],[0]
"Finally, we see how our approach scales to a unit-building task in StarCraft.
2.",1. Introduction,[0],[0]
The Attribute Planner Model,1. Introduction,[0],[0]
"We consider an agent in a Markov environment, i.e. at each time the agent observes the state s and takes action
a, which uniquely determines the probability P (s, a, s0) of transitioning from s to s0.",1. Introduction,[0],[0]
We augment the environment with a map f : S ! {⇢} from states to a set of userdefined attributes ⇢.,1. Introduction,[0],[0]
"We assume that either f is provided or a small set of hand-labeled (s, ⇢) pairs are provided in order to learning a mapping f̂ .",1. Introduction,[0],[0]
"Hence, the attributes are human defined and constitute a form of supervision.",1. Introduction,[0],[0]
Here we consider attributes that are sets of binary vectors.,1. Introduction,[0],[0]
"These user-specified attributes parameterize the set of goals that can be specified at test time.
",1. Introduction,[0],[0]
"The agent’s objective at test time is, given a set of goal attributes ⇢g , to take a sequence of actions in the environment that end with the agent in a state that maps to ⇢g. During training, the agent constructs a model with three parts:
1.",1. Introduction,[0],[0]
"a neural-net based attribute detector f̂ , which maps states s to a set of attributes ⇢, i.e. ⇢ = f̂(s).
2.",1. Introduction,[0],[0]
"a neural net-based policy ⇡(s, ⇢g) which takes a pair of inputs: the current state s and attributes of an (intermediate) goal state ⇢g , and outputs a distribution over actions.
",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"a transition table c⇡(⇢i, ⇢j) that records the empirical probability that ⇡(s⇢i , ⇢j) can succeed at transiting successfully from ⇢i to ⇢j in a small number of steps.
",1. Introduction,[0],[0]
The transition table c⇡ can be interpreted as a graph where the edge weights are probabilities.,1. Introduction,[0],[0]
"This high-level attribute graph is then searched at test time to find a path to the goal with maximum probability of success, with the policy network performing the low-level actions to transition between adjacent attribute sets.",1. Introduction,[0],[0]
"The first step in training the Attribute Planner is to fit the neural network detector f̂ that maps states s to attributes ⇢, using the labeled states provided.",2.1. Training the Attribute Planner,[0],[0]
"If a hardcoded function f is provided, then this step can be elided.
",2.1. Training the Attribute Planner,[0],[0]
"In the second step, the agent explores its environment using an exploratory policy.",2.1. Training the Attribute Planner,[0],[0]
"Every time an attribute transition (⇢i, ⇢j) is observed, it is recorded in an intermediate transition table c⇡e .",2.1. Training the Attribute Planner,[0],[0]
"This table will be used in later steps to keep track of which transitions are possible.
",2.1. Training the Attribute Planner,[0],[0]
"The most naive exploratory policy takes random actions, but the agent can explore more efficiently if it performs count-based exploration in attribute space.",2.1. Training the Attribute Planner,[0],[0]
"We use a neural network exploration policy that we train via reinforcement learning with a count-based reward1 proportional to c⇡e(⇢i, ⇢j) 0.5 upon every attribute transition (⇢i, ⇢j), where c⇡e(⇢i, ⇢j) is the visit count of this transition during exploration.",2.1. Training the Attribute Planner,[0],[0]
"This bonus is as in (Strehl & Littman, 2008), but with no empirical reward from the environment.",2.1. Training the Attribute Planner,[0],[0]
The precise choice of exploration bonus is discussed in App.,2.1. Training the Attribute Planner,[0],[0]
"A.
",2.1. Training the Attribute Planner,[0],[0]
"Now that we have a graph of possible transitions, we next train the low-level goal-conditional policy ⇡ and the main transition table c⇡ .",2.1. Training the Attribute Planner,[0],[0]
"From state s with attributes ⇢, the model picks an attribute set ⇢g randomly from the neighbors of ⇢ in c⇡e weighted by their visit count in the Explore phase and sets that as the goal for ⇡.",2.1. Training the Attribute Planner,[0],[0]
"Once the goal is achieved or a timeout is reached, the policy is updated and the main transition table c⇡ is updated to reflect the success or failure.",2.1. Training the Attribute Planner,[0],[0]
"⇡ is updated via reinforcement learning, with a reward r of
1Although the reward is non-stationary, we find (as in the literature) that it is empirically effective.
1 if ⇢g was reached and 0 otherwise2.",2.1. Training the Attribute Planner,[0],[0]
"See Algorithm 1 for pseudocode of AP training.
",2.1. Training the Attribute Planner,[0],[0]
"In the case of block stacking (Sec. 4.1), the attribute transitions consist of a single step, so we treat ⇡ as an “inverse model” in the style of (Agrawal et al., 2016; Andrychowicz et al., 2017), and rather than using reinforcement learning, we can train ⇡ in a supervised fashion by taking random actions and training ⇡ to predict the action taken given the initial state and final attributes.
",2.1. Training the Attribute Planner,[0],[0]
"Algorithm 1 Attribute Planner Training Input: Labeled pairs {(si, ⇢i)}, exploratory policy ⇡e, N1, N2, tmax.
//",2.1. Training the Attribute Planner,[0],[0]
"Step 1: Train attribute detector Fit f̂ on {(si, ⇢i)} with supervised learning.
",2.1. Training the Attribute Planner,[0],[0]
"// Step 2: Explore
for t = 1 ...",2.1. Training the Attribute Planner,[0],[0]
N1 do Act according to ⇡e(st 1).,2.1. Training the Attribute Planner,[0],[0]
"Compute attributes ⇢t f̂(st) if ⇢t 6= ⇢t 1 then
Record the transition: c⇡e(⇢t 1, ⇢t) +",2.1. Training the Attribute Planner,[0],[0]
"= 1 Optional: Update ⇡e with count-based reward.
//",2.1. Training the Attribute Planner,[0],[0]
"Step 3: Train policy ⇡ and c⇡ tlast 0, ⇢s ;, ⇢e RandNeighbor(c⇡e , ⇢s) for t = 1 ...",2.1. Training the Attribute Planner,[0],[0]
"N2 do
Compute attributes ⇢t f̂(st)",2.1. Training the Attribute Planner,[0],[0]
"if t = 1 or ⇢t 6= ⇢s or t tlast tmax then
r 1 if ⇢t = ⇢e, otherwise 0.",2.1. Training the Attribute Planner,[0],[0]
"UpdatePolicy(⇡, r) Record attempt: A⇡(⇢t 1, ⇢t) += 1 Record success: S⇡(⇢t 1, ⇢t) += r ⇢s ⇢t, ⇢e RandNeighbor(c⇡e , ⇢s) tlast t
Take an action according to ⇡(st 1, ⇢e)
for (⇢i, ⇢j) 2",2.1. Training the Attribute Planner,[0],[0]
"A⇡ do c⇡(⇢i, ⇢j) S⇡(⇢i, ⇢j)/A⇡(⇢i, ⇢j).",2.1. Training the Attribute Planner,[0],[0]
Once the model has been built we can use it for planning.,2.2. Evaluating the model,[0],[0]
"That is, given an input state s and target set of attributes ⇢T , we find a path [⇢0, ⇢1, ..., ⇢m] on the graph G with ⇢0 = f(s) and ⇢m = ⇢T minimizing Pm 1 i=0 log c⇡(⇢i, ⇢i+1) which maximizes the probability of success of the path (assuming independence).",2.2. Evaluating the model,[0],[0]
"The probability c⇡ is computed in Algorithm 1 as the ratio of observed successes and attempts
2Since c⇡ is collecting statistics about ⇡ which is nonstationary.",2.2. Evaluating the model,[0],[0]
"So c⇡ should really be updated only after a burn-in period of ⇡, or a moving average should be used for the statistics.
during training.
",2.2. Evaluating the model,[0],[0]
"The optimal path can be found using Dijkstra’s algorithm with a distance metric of log(c⇡(⇢i, ⇢i+1)).",2.2. Evaluating the model,[0],[0]
"The policy is then used to move along the resulting path between attribute set, i.e. we take actions according to a = ⇡(s, ⇢1), then once f(s) = ⇢1, we change to a = ⇡(s, ⇢2) and so on.",2.2. Evaluating the model,[0],[0]
"At each intermediate step, if the current attributes don’t match the attributes on the computed path, then a new path is computed using the current attributes as a starting point (or, equivalently, the whole path is recomputed at each step).
",2.2. Evaluating the model,[0],[0]
"Algorithm 2 Attribute Planner Inference Input: Low-level policy ⇡, graph c⇡, attribute detector f̂ , target attributes ⇢T .",2.2. Evaluating the model,[0],[0]
"do
⇢ f̂(s)",2.2. Evaluating the model,[0],[0]
"[⇢0, ..., ⇢m] ShortestPath(c⇡, ⇢, ⇢g) Act according to ⇡(st 1, ⇢1).
",2.2. Evaluating the model,[0],[0]
"while ⇢ 6= ⇢T
2.3.",2.2. Evaluating the model,[0],[0]
"An Aside: Which attributes should we include?
",2.2. Evaluating the model,[0],[0]
"Since we use user-specified attributes for planning, which attributes are important to include?",2.2. Evaluating the model,[0],[0]
"The set of attributes must be able to specify our goals of interest, and should be parsimonious since extra attributes will increase the size of the graph and thus degrade the statistics on each edge/.
On the other hand, the attributes should have a property that we will call “ignorability” which says that the probability of being able to transition from ⇢i to ⇢j should only depend on the attributes ⇢i, not the exact state; i.e. P⇡(f(st0) = ⇢j |f(st))",2.2. Evaluating the model,[0],[0]
=,2.2. Evaluating the model,[0],[0]
P⇡(f(st0) = ⇢j |st) 3.,2.2. Evaluating the model,[0],[0]
"To the extent that this condition is violated, then transitions are aliased, and a planned transition may not be achievable by the policy from the particular state s even though it’s achievable from other states with the same properties, causing the model to fail to achieve its goal.",2.2. Evaluating the model,[0],[0]
"For example, in the block stacking task in 4.1, there will be nontrivial aliasing; we will show that some amount of aliasing is not deadly for our model.",2.2. Evaluating the model,[0],[0]
"Many researchers have recognized the importance of methods that can divide a MDP into subprocesses (Thrun & Schwartz, 1994; Parr & Russell, 1998; Sutton et al., 1999; Dietterich, 2000).",3. Related work,[0],[0]
"Perhaps the most standard formalism today is the options framework of (Sutton et al., 1999), which deals with multistep “macro-actions” in the setting of reinforcement learning.",3. Related work,[0],[0]
"Recent works, like (Kulkarni et al., 2016; Harb et al., 2017), have shown how options can be used (and even discovered in (Harb et al., 2017)) with
3The particular sequence of actions that effects the transition from ⇢i to ⇢j may still be conditional on the state.
function approximation via deep learning.
",3. Related work,[0],[0]
Our work is also a hierarchical approach to controlling an agent in a Markovian environment.,3. Related work,[0],[0]
"However, the paradigm we consider differs from reinforcement learning: we consider a setup where no reward or supervision is provided other than the (s, f(s)) pairs, and show than an agent can learn to decompose a transition between far away ⇢, ⇢0 into a sequence of short transitions.",3. Related work,[0],[0]
"If we were to frame the problem as HRL, considering each ⇡(·, ⇢) as a macro action4, in order for the agent to learn to sequence the ⇡(·, ⇢i), the environment would need to give reward for the completion of complex tasks, not just simple ones.
",3. Related work,[0],[0]
"As in (Sutton et al., 2011; Schaul et al., 2015; Dosovitskiy & Koltun, 2016; Fern et al., 2004), we have policies parameterized by state and target attributes.",3. Related work,[0],[0]
"In (Dosovitskiy & Koltun, 2016), an agent is given supervision of future values of attributes of the state considered important for describing tasks.",3. Related work,[0],[0]
"Unlike in that work, our attributes are functions of the current state, and the model uses its own estimator to learn the dynamics at the level of attributes as a graph.",3. Related work,[0],[0]
"Thus, our model gets no extrinsic supervision of environment dynamics or goal attainment at the level of attributes.",3. Related work,[0],[0]
"Our training of c⇡ and then using cpi for task generation recalls (Fern et al., 2004).",3. Related work,[0],[0]
"In that work, as training progresses, goals are farther and farther away (as measured by the steps of a random walk on attributes); but in our work the goals are always one attribute away.",3. Related work,[0],[0]
"This is because our ⇡ only needs to know how to transition between nearby attribute sets, thanks to the planner.",3. Related work,[0],[0]
"In contrast (Fern et al., 2004) aims to train a reactive policy that can handle long transitions too, obviating the need for a planner.",3. Related work,[0],[0]
"Moreover, (Fern et al., 2004) works entirely in the symbolic space, whereas we interfaces the perceptual space with the symbolic space.",3. Related work,[0],[0]
"In (van Seijen et al., 2017), human provided attributes are used as a general value function (GVF) in Ms. Pacman, showing that using a weighted combination of these can lead to higher scores than standard rewards; but again, in contrast to our work, their goal is a better reactive policy.
",3. Related work,[0],[0]
"Our approach is closely related to factored MDP (Boutilier et al., 1995; 2000; Guestrin et al., 2003b).",3. Related work,[0],[0]
"In these works, it is assumed that the environment can be represented by discrete attributes, and that transitions between the attributes by an action can be modeled as a Bayesian network.",3. Related work,[0],[0]
The value of each attribute after an action is postulated to depend in a known way on attributes from before the action.,3. Related work,[0],[0]
The present work differs from these in that the attributes do not determine the state and the dependency graph is not assumed to be known.,3. Related work,[0],[0]
"More importantly, the focus in this work is on
4All the “macro actions’ in our examples in 4.1 are degenerate in the sense that they return after one step, but we still are able to show generalization to long trajectories from (unsupervised) training only on short ones
organizing the space of tasks through the attributes rather than being able to better plan a specific task; and in particular being able to generalize to new, more complex tasks at test time.
",3. Related work,[0],[0]
"Our approach is also related to Relational MDP and Object Oriented MDP (Hernandez-Gardiol & Kaelbling, 2003; van Otterlo, 2005; Diuk et al., 2008; Abel et al., 2015), where states are described as a set of objects, each of which is an instantiation of canonical classes, and each instantiated object has a set of attributes.",3. Related work,[0],[0]
"Our work is especially related to (Guestrin et al., 2003a), where the aim is to show that by using a relational representation of an MDP, a policy from one domain can generalize to a new domain.",3. Related work,[0],[0]
"However, in the current work, the attributes are taken directly as functions of the state, as opposed to defined for object classes, and we do not have any explicit encoding of how objects interact.",3. Related work,[0],[0]
"The model is given some examples of various attributes, and builds a parameterized model that maps into the attributes.
",3. Related work,[0],[0]
"The Programmable Agents of (Denil et al., 2017) put the notions of objects and attributes (as in relational MDP) into an end-to-end differentiable neural architecture.",3. Related work,[0],[0]
"Their model also learns mappings from states to attributes, and is evaluated on a block manipulation task.",3. Related work,[0],[0]
"In their work, the attributes are used to generalize to different combinations of object properties at test time, while we use it to generalize compositionally to more complex tasks.",3. Related work,[0],[0]
"Also, while our model uses explicit search to reason over attributes, they use an end-to-end neural architecture.
",3. Related work,[0],[0]
There is a large literature on quickly adapting to a new learning problem given a set or a history of related learning problems.,3. Related work,[0],[0]
"Our approach in this work has a similar motivation to (Isele et al., 2016), where tasks are augmented with descriptors and featurized, and the coefficients of the task features in a sparse dictionary are used to weight a set of vectors defining the model for the associated task.",3. Related work,[0],[0]
"Similarly, the task is specified by a feature as an input into a model in (Lopez-Paz & Ranzato, 2017).",3. Related work,[0],[0]
"However, in our work, although the attributes are used to parameterize tasks, rather than directly featurize the tasks, they are features of a state; and we learn the mapping from state to attributes.",3. Related work,[0],[0]
"This allows our agent to learn how to transit between sets of attributes unsupervised, and plan in that space.
",3. Related work,[0],[0]
Several recent deep reinforcement learning works have used modular architectures and hierarchy to achieve generalization to new tasks.,3. Related work,[0],[0]
"For example, (Tessler et al., 2017) uses pre-trained skills for transfer.",3. Related work,[0],[0]
"(Oh et al., 2017) uses a metacontroller that selects parameterized skills and analogical supervision on outer-product structured tasks.",3. Related work,[0],[0]
"However, our “meta-controller” is the search over attributes, rather than a reactive model, which allows explicit planning.",3. Related work,[0],[0]
"Furthermore, although our assignments of attributes serves a similar purpose to their analogical supervision (and outer-
product task structure),the methods are complementary; we can imagine augmenting our attributes with analogical supervision.
",3. Related work,[0],[0]
"In (Andreas et al., 2017), generalization is achieved through supervision in the form of “policy sketches”, which are symbolic representations of the high level steps necessary to complete a given task.",3. Related work,[0],[0]
The low level steps in executing modules in the sketches are composable.,3. Related work,[0],[0]
"Our work is similar in that high level annotation is used to enable generalization, but the mechanism in this work is different.",3. Related work,[0],[0]
"Note that the approaches in (Andreas et al., 2017) is also complementary to the one described here; in future work we wish to explore combining them.
",3. Related work,[0],[0]
In this work we use an explicit memory of sets of attributes the model has seen.,3. Related work,[0],[0]
"Several previous works have used nonparametric memories for lowering the sample complexity of learning, e.g. (Blundell et al., 2016; Pritzel et al., 2017).",3. Related work,[0],[0]
"Like these, we lean on the fact that with a good representation of a state, it can be useful to memorize what to do in given situation (having only done it a small number of times) and explicitly look it up.",3. Related work,[0],[0]
"In our case, the “good representation” is informed by the user-specified attributes.
",3. Related work,[0],[0]
"Our approach is also related to (Machado et al., 2017), which builds up a multiscale representation of an MDP using Eigenvectors of the transition matrix of the MDP, in the sense that we collect data on possible transitions between attributes in a first phase of training, and then use this knowledge at test time.
",3. Related work,[0],[0]
"There is a large literature on using symbolic representations for planning, for example the STRIPS formalism (Fikes & Nilsson, 1971).",3. Related work,[0],[0]
"In (Konidaris et al., 2018), the authors propose a model that learns the symbols for a STRIPS-style representation.",3. Related work,[0],[0]
"Like in our work, their model learns the interface between the raw state observations and the planner.",3. Related work,[0],[0]
"However, in that work, the abstract structure is given by a set of pre-defined options with fixed policies.",3. Related work,[0],[0]
We perform experiments with the Attribute Planner (AP) in three environments.,4. Experiments,[0],[0]
"First, we consider a 3D block stacking environment.",4. Experiments,[0],[0]
"Here, we demonstrate that AP allows compositional generalization by training a low level policy on single-action tasks in a supervised fashion and showing that with the AP algorithm it can perform multi-step tasks at test time.
",4. Experiments,[0],[0]
"Second, we consider 2D grid worlds in Mazebase (Sukhbaatar et al., 2015), where we evaluate AP’s performance when the low-level policy is temporally extended and must be learned via RL.
",4. Experiments,[0],[0]
"Finally, we evaluate AP on a build order planning task in
Starcraft to see how AP scales to a more complex task that is of broader interest We further show that an exploratory policy over attributes allows the agent to explore attribute transitions where random search fails.
",4. Experiments,[0],[0]
Baselines:,4. Experiments,[0],[0]
"In all experiments, we compare against baseline policies trained with reinforcement learning.",4. Experiments,[0],[0]
"These baseline policies take the state and goal as inputs, and use the same neural network architecture as the policy used for the Attribute Planner.",4. Experiments,[0],[0]
We consider several training regimes for the baseline policies: (i) training only with nearby goals like AP; (ii) training on the multi-step evaluation tasks; and (iii) training on a curriculum that transitions from nearby goals to evaluation tasks.,4. Experiments,[0],[0]
"Policies (ii) and (iii) are trained on full sequences, thus have an inherent advantage over our model.
",4. Experiments,[0],[0]
"In the block stacking task, we further compare against a state-of-the-art algorithm for hierarchical RL: Option-Critic with deliberation cost (Harb et al., 2017), as well as an inverse model trained by supervised learning.",4. Experiments,[0],[0]
"We consider a 3D block stacking environment in Mujoco (Todorov et al., 2012).",4.1. Block Stacking,[0],[0]
"In this experiment, we train AP only on single-action trajectories and evaluate on multi-step tasks, in order to evaluate AP’s ability to generalize using planning.",4.1. Block Stacking,[0],[0]
"We compare AP with baselines trained on both single-action, multi-action, and curriculum tasks.
",4.1. Block Stacking,[0],[0]
"In this environment, there are 4 blocks of different colors, and actions consist of dropping a block in a 3 ⇥ 3 grid of positions, resulting in 36 total actions.",4.1. Block Stacking,[0],[0]
"A block cannot be moved when it is underneath another block, so some actions have no effect.
",4.1. Block Stacking,[0],[0]
"The input to the model is the observed image, and there are a total of 36 binary properties corresponding to the relative x and y positions of the blocks and whether blocks are stacked on one another.",4.1. Block Stacking,[0],[0]
"For example, one property corresponds to “blue is on top of yellow”.",4.1. Block Stacking,[0],[0]
"Each training episode is initiated from a random initial state and lasts only one step, i.e. dropping a single block in a new location.",4.1. Block Stacking,[0],[0]
"Further model and training details and results on a continuous variant of this environment are provided in Appendix B.
Table 1 compares the performance of different models on several block stacking tasks.",4.1. Block Stacking,[0],[0]
"In the multi-step task, the goal is chosen as the properties of a new random initialization.",4.1. Block Stacking,[0],[0]
These tasks typically require 3 8 steps to complete.,4.1. Block Stacking,[0],[0]
"In the 4-stack task, the goal is a vertical stack of blocks in the order red, green, blue, yellow.",4.1. Block Stacking,[0],[0]
"In the underspecified task, we consider a multi-step goal where only 70% of the attributes are provided at random.",4.1. Block Stacking,[0],[0]
"The AP model handles these naturally by finding the shorted path to any satisfactory attribute set.
",4.1. Block Stacking,[0],[0]
"The single-step reactive policies perform similarly to AP when evaluated on the single-step tasks it sees during training (see Table 5 in the Appendix), but perform poorly when transferred to multi-step tasks, while AP generalizes well to complex task.",4.1. Block Stacking,[0],[0]
"The AP model also solves underspecified tasks even though they are not seen explicitly during training.
",4.1. Block Stacking,[0],[0]
The attribute detector f̂ predicts the full attribute set with < 0.1% error when trained on the full dataset of 1 million examples.,4.1. Block Stacking,[0],[0]
"If trained on only 10,000 examples, the attribute detector has an error rate of 1.4%.",4.1. Block Stacking,[0],[0]
"Training the AP model with this less-accurate attribute detector degrades multi-step performance by only 0.9%.
",4.1. Block Stacking,[0],[0]
"We also consider a variant of the block stacking task with a continuous action space, in which an action consists of dropping a block at any x-y position.",4.1. Block Stacking,[0],[0]
"While performance degrades substantially for all models in the continuous action space, AP continues to outperform reactive policies on multi-step tasks.",4.1. Block Stacking,[0],[0]
"See Appendix B for the full results.
",4.1. Block Stacking,[0],[0]
Property Aliasing: The “ignorability” assumption we made in Section 2 is violated in the block stacking task.,4.1. Block Stacking,[0],[0]
"To see why, consider a transition from “red left of blue and yellow” to “red right of blue and yellow”.",4.1. Block Stacking,[0],[0]
"This can typically be accomplished in one step, but if blue and yellow are already on the far right, it cannot.",4.1. Block Stacking,[0],[0]
"Thus, states where this transition are possible and impossible are aliased with the same properties.",4.1. Block Stacking,[0],[0]
"Table 2 shows that the performance is nearly perfect for individual transitions (1-step tasks), and the graph is well-connected after 1 million training examples, so the main source of error on these tasks is in fact
aliasing.",4.1. Block Stacking,[0],[0]
"Figure 3 shows an example plan that becomes stuck due to aliasing.
",4.1. Block Stacking,[0],[0]
The transition table c⇡ is important for mitigating the effects of aliasing in the block stacking task.,4.1. Block Stacking,[0],[0]
"The graph search finds the path with the highest probability of success (i.e. the product of probabilities on each edge), so it avoids edges that have high aliasing.",4.1. Block Stacking,[0],[0]
"In Table 1, we consider an ablation of c⇡ from the AP model, in which the probability of transitioning from an edge (⇢i, ⇢j) is estimated as the fraction of transitions from ⇢i that ended in ⇢j during the Explore phase.",4.1. Block Stacking,[0],[0]
This ablation performs substantially worse than the full AP model.,4.1. Block Stacking,[0],[0]
"We next consider tasks in which a multi-step low-level policy is required to transition between neighboring attributes.
",4.2. Grid Worlds,[0],[0]
"We consider two classes of small 2-D environments in Mazebase (Sukhbaatar et al., 2015), where the worlds are randomly generated for each episode.",4.2. Grid Worlds,[0],[0]
"The action space for each consists of movements in the four cardinal directions plus additional environment-specific actions.
",4.2. Grid Worlds,[0],[0]
"Colored Switches The first environment consists of four switches, each with four possible colors.",4.2. Grid Worlds,[0],[0]
An extra toggle action cycles the color of a switch if the agent is standing on it.,4.2. Grid Worlds,[0],[0]
"The attributes for this environment are the states of the switches and the tasks are to change the switches into
a specified configuration, as shown in Fig. 4(right).",4.2. Grid Worlds,[0],[0]
"The locations and colors of the switches are randomly initialized for each episode.
",4.2. Grid Worlds,[0],[0]
"Crafting In the second environment, similar to the one used in (Andreas et al., 2017), an agent needs to collect resources and combine them to form items.",4.2. Grid Worlds,[0],[0]
"In addition to moving in the cardinal directions, the agent has a “grab” action that allows it to pick up a resource from the current location and add it to its inventory.",4.2. Grid Worlds,[0],[0]
The agent also has a “craft” action that combines a set of items to create a new item if the agent has the prerequisite items in its inventory and the agent is standing on a special square (a “crafting table”) corresponding to the item to be crafted.,4.2. Grid Worlds,[0],[0]
"The attributes for this environment are the items in the inventory, and the task is to add a specified (crafted) item to the inventory.",4.2. Grid Worlds,[0],[0]
"In the environment, there are three types of resources and three types of products (see Fig. 4(left)).",4.2. Grid Worlds,[0],[0]
"The game always starts with three resources and an empty inventory.
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ !",4.2. Grid Worlds,[0],[0]
"=
+ + !",4.2. Grid Worlds,[0],[0]
"=
1
Goal:
Crafting key:
Switch color gameCrafting game
Goal:
Figure 4.",4.2. Grid Worlds,[0],[0]
Left: Crafting mazebase game.,4.2. Grid Worlds,[0],[0]
Right: Colored switches game.,4.2. Grid Worlds,[0],[0]
"See text for details.
",4.2. Grid Worlds,[0],[0]
"In both environments, the agent’s observation consists of a bag of words, where the words correspond to (feature, location).",4.2. Grid Worlds,[0],[0]
"Features consist of item types, names, and their other properties.",4.2. Grid Worlds,[0],[0]
"The locations include position relative to the agent in the maze, and also a few special slots for inventory, current, and target attributes.
",4.2. Grid Worlds,[0],[0]
Training proceeds according to Algorithm 1.,4.2. Grid Worlds,[0],[0]
"During the explore phase, an exploratory policy is trained with reinforcement learning using a count-based reward proportional
to c⇡e(⇢i, ⇢j)P",4.2. Grid Worlds,[0],[0]
"i,j c⇡e(⇢i, ⇢j) + 0.001 !",4.2. Grid Worlds,[0],[0]
"0.5
for making a transition (⇢i, ⇢j), where c⇡e(⇢i, ⇢j) is the number of times transition (⇢i, ⇢j) has been seen so far.",4.2. Grid Worlds,[0],[0]
"We discuss this exploration bonus in Appendix A.
During the final phase of training we simultaneously compute ⇡ and c⇡ , so we use an exponentially decaying average of the success rate of ⇡ to deal with it’s nonstationarity:
c⇡(⇢i, ⇢j) =
PT t=1
T tSt⇡(⇢i, ⇢j)PT t=1 T tAt⇡(⇢i, ⇢j) ,
where T is the number of training epochs, At⇡ is the number of attempted transitions (⇢i, ⇢j) during epoch t, and St⇡ is the number of successful transitions.",4.2. Grid Worlds,[0],[0]
A decay rate of = 0.9 is used.,4.2. Grid Worlds,[0],[0]
"More details of the model and training are provided in Appendix C.
In the switches environment, multi-step test tasks are generated by setting a random attribute as target, which can require up to 12 attribute transitions.",4.2. Grid Worlds,[0],[0]
"In the crafting environment, test tasks are generated by randomly selecting a (crafted) item as a target.",4.2. Grid Worlds,[0],[0]
"Since we do not care about other items in the inventory, the target state is underspecified.
",4.2. Grid Worlds,[0],[0]
We produce a curriculum baseline by gradually increase the upper bound on the difficulty of tasks during training.,4.2. Grid Worlds,[0],[0]
"In the switches environment, the difficulty corresponds to the number of toggles necessary for solving the task.",4.2. Grid Worlds,[0],[0]
"The craft environment has two levels of difficulty: tasks can be completed by a single grab or craft action, and tasks that require multiple such actions.
",4.2. Grid Worlds,[0],[0]
Table 3 compares our Attribute Planner (AP) to a reinforcement learning baseline on the mazebase tasks.,4.2. Grid Worlds,[0],[0]
"The AP planning outperforms purely reactive training regardless of whether one-step, multi-step, or a curriculum of training examples is provided.",4.2. Grid Worlds,[0],[0]
"Finally, we test our approach for planning a build order in StarCraft: Brood War (Synnaeve et al., 2016).",4.3. StarCraft,[0],[0]
"We consider the space of tasks of building particular units in a fixed time of 500 steps, e.g. “build 1 barracks and 2 marines“.
",4.3. StarCraft,[0],[0]
"This task is challenging for RL because the agent must complete a number of distinct steps, e.g. mine enough ore, then build a barracks, and finally train marines using the barracks, before receiving a reward.",4.3. StarCraft,[0],[0]
Each of these steps requires the agent have to control multiple units of different types using low-level actions similar to how a human plays the game.,4.3. StarCraft,[0],[0]
"See Appendix D for more details.
",4.3. StarCraft,[0],[0]
"As in (Sukhbaatar et al., 2017), we restrict the game to the Terran race and only allow construction of certain units.",4.3. StarCraft,[0],[0]
"In the small version, the agent can mine ore and build SCVs, supply depots, barracks, and marines.",4.3. StarCraft,[0],[0]
"In the large version, an engineering bay and missile turrets are included as well.",4.3. StarCraft,[0],[0]
"The attributes are chosen to be the number of units and resources of each type, specifically
{min(bNore/25c, 40), NSCV , Ndepot, Nbarracks, Nmarine}
where Nx is the number of x present in the game, including units under construction.",4.3. StarCraft,[0],[0]
"The large version also include {Neng.bay, Nturrets}.
",4.3. StarCraft,[0],[0]
Models are trained for a total of 30 million steps.,4.3. StarCraft,[0],[0]
AP uses 16 million steps for exploration and 14 million steps for training ⇡.,4.3. StarCraft,[0],[0]
"Table 4 shows the final performance of the AP model and reactive RL baselines on this task after 30 million steps of training.
",4.3. StarCraft,[0],[0]
"AP exploration finds 120,000 and 420,000 edges for the small and large versions, respectively.",4.3. StarCraft,[0],[0]
The size and scaling of this graph show the limitations of a fully explicit graph.,4.3. StarCraft,[0],[0]
"In fact, we represent the ore attribute as bNore/25c because it decreases the size of the graph by a factor of 25: otherwise for each transition w.r.t.",4.3. StarCraft,[0],[0]
"the other attributes, the graph would have a separate edge for each valid value of total ore.
",4.3. StarCraft,[0],[0]
Count-based exploration over attributes is vital during the Explore phase in StarCraft.,4.3. StarCraft,[0],[0]
"If a random policy is used in the small version, only 2047 edges are discovered as opposed to 120,000 using count-based exploration, and the final performance is reduced from 31.7% to 6.4%.",4.3. StarCraft,[0],[0]
Our results show that structuring the space of tasks with high level attributes allows an agent to compose policies for simple tasks into solutions of more complex tasks.,5. Discussion,[0],[0]
"The agent plans a path to the final goal at the level of the attributes, and executes the steps in this path with a reactive policy.",5. Discussion,[0],[0]
"Thus, supervision of an agent by labeling attributes can lead to generalization from simple tasks at train time to more complex tasks at test time.",5. Discussion,[0],[0]
"There are several fronts for further work:
Sample complexity of the planning module: In Table 2 we can see both the benefits and the liabilities of the explicit non-parametric form for c⇡ .",5. Discussion,[0],[0]
"By 10K samples, the parametric lower level policy is already able to have a reasonable success rate.",5. Discussion,[0],[0]
"However, because in this environment, there are over 200K edges in the graph, most of the edges have not been seen, and without any weight-sharing, our model cannot estimate these transition probabilities.",5. Discussion,[0],[0]
"On the other hand, by 100K samples the model has seen enough of the graph to make nontrivial plans; and the non-parametric form of the graph makes planning straightforward.
",5. Discussion,[0],[0]
"In future work, we hope to combine parametric models for c⇡ with search to increase the sample efficiency of the planning module.",5. Discussion,[0],[0]
"Alternatively, we might hope to make progress on dynamic abstraction (projecting out some of the attributes) depending on the current state and goal, which would make the effective number of edges of the graph smaller.
",5. Discussion,[0],[0]
"Exploration We have shown that the attributes ⇢ and counts c⇡, in addition to their usefulness for planning, provide a framework for incentivizing exploration.",5. Discussion,[0],[0]
"In this work we considered a simple count-based exploration strategy, which achieved better exploration in attribute space than random exploration.",5. Discussion,[0],[0]
"However, this setting of pure exploration where there are no empirical rewards is different from the classic problem of exploration in an MDP, and warrants further exploration (see Appendix A).
",5. Discussion,[0],[0]
Learning the attributes: Discovering the attributes automatically would remove much of the need for human supervision.,5. Discussion,[0],[0]
"Recent work, such as (Thomas et al., 2017), demonstrates how this could be done.",5. Discussion,[0],[0]
"Another avenue for discovering attributes is to use a few “seed” attributes, which is necessary for task specification anyway, and use aliasing as a signal that some attributes need to be refined.",5. Discussion,[0],[0]
The tasks that an agent will need to solve often are not known during training.,abstractText,[0],[0]
"However, if the agent knows which properties of the environment are important then, after learning how its actions affect those properties, it may be able to use this knowledge to solve complex tasks without training specifically for them.",abstractText,[0],[0]
"Towards this end, we consider a setup in which an environment is augmented with a set of user defined attributes that parameterize the features of interest.",abstractText,[0],[0]
"We propose a method that learns a policy for transitioning between “nearby” sets of attributes, and maintains a graph of possible transitions.",abstractText,[0],[0]
"Given a task at test time that can be expressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over paths through attribute space to get a high level plan, and then uses its low level policy to execute the plan.",abstractText,[0],[0]
"We show in 3D block stacking, gridworld games, and StarCraft that our model is able to generalize to longer, more complex tasks at test time by composing simpler learned policies.",abstractText,[0],[0]
Composable Planning with Attributes,title,[0],[0]
"Clustering is one of the most widely used techniques in data analysis (Xu & Wunsch, 2005; Jain, 2010).",1. Introduction,[0],[0]
"Despite a rich literature on pure continuous data or pure categorical data, the clustering problem remains challenging for mixed-type data, i.e., data with both types of attributes (Everitt et al., 2001).",1. Introduction,[0],[0]
"Mixed-type data are ubiquitous in real world domains, e.g., social science, biomedicine and finance, where categorical attributes often describe demographic information or questionnaire responses, and continuous attributes often correspond to quantitative measurements.",1. Introduction,[0],[0]
"However, only a very limited number of clustering methods have been proposed for such data (Everitt et al., 2001; Huang, 1998).",1. Introduction,[0],[0]
"The major challenge is the lack of a good geometric intuition of data on the mixed-type domain;
1City University of New York (CUNY), New York, USA 2University of Sussex, Falmer, United Kingdom 3National Research University Higher School of Economics, Moscow, Russia 4Ohio State University, Columbus, USA.",1. Introduction,[0],[0]
"Correspondence to: Chao Chen <chao.chen.cchen@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"such intuition is the important basis of many successful geometric clustering methods, e.g., k-means (MacQueen et al., 1967), Ward’s method (Ward Jr, 1963), DBSCAN (Ester et al., 1996), to name a few.",1. Introduction,[0],[0]
"In practice, what usually being done is to convert mixed-type data to either pure continuous or pure categorical domain, and subsequently use existing geometric clustering methods.",1. Introduction,[0],[0]
"A metric for directly dealing with mixed-type data is also available, based on Gower’s coefficient (1971).",1. Introduction,[0],[0]
The uptake of geometric clustering methods is mostly driven by their lightweight computational requirements.,1. Introduction,[0],[0]
"However, these methods lack a well justified underlying probabilistic model, are sensitive to the choice of underlying metric, and do not give a principled answer to the fundamental question of required number of clusters for the data at hand.
",1. Introduction,[0],[0]
"In this paper, we propose a probabilistic clustering method for mixed-type data, which admits at least four attractive properties.",1. Introduction,[0],[0]
"First, our probabilistic method goes beyond the widely-adopted class conditional independence assumption of feature variables, e.g., as in the latent class model (McCutcheon, 1987).",1. Introduction,[0],[0]
"Second, our method is based on the global topographical features, i.e., peaks and mountains, of the density function, rather than the distances between data points.",1. Introduction,[0],[0]
The argument for topographical features is to sidestep a premature specification of the metric space in which our mixed-type data will achieve the best grouping.,1. Introduction,[0],[0]
"Third, our method is able to utilize a persistent homology theory to automatically determine the number of clusters in the data.",1. Introduction,[0],[0]
"Fourth, the proposed method can be easily parallelized to achieve a competitive running time with respect to many lightweight geometric clustering methods.
",1. Introduction,[0],[0]
"From the modeling perspective, we compose tree graphical models with topographical features to achieve a probabilistic mixed-type clustering model.",1. Introduction,[0],[0]
Graphical models provide a way of factorizing a joint probability distribution into a product of local interactions.,1. Introduction,[0],[0]
These local interactions capture dependency among feature variables.,1. Introduction,[0],[0]
While a Bayesian network or a Markov random field can be built with a set of nodes representing each feature variable.,1. Introduction,[0],[0]
The graph structure and parameter estimation can be computationally expensive.,1. Introduction,[0],[0]
"By constraining the graph to be a tree, the structure and parameter can be learned efficiently.",1. Introduction,[0],[0]
"Other than computational benefits, tree-structured
graphical models also provide a modeling elegance; with a tree structure, we have a factorization that explicitly corresponds to empirical univariate and bivariate marginal distributions.",1. Introduction,[0],[0]
"For the bivariate distributions, we can then adapt the product kernel density estimation (Scott, 2015) to capture interaction between continuous-continuous variables, between categorical-categorical variables, and between categorical-continuous variables.
",1. Introduction,[0],[0]
"Having modeled the data generation process via a tree graphical model, we are left with finding a robust approach for assigning each data point to its cluster.",1. Introduction,[0],[0]
"To achieve this, we adopt a topological perspective, namely, we view a probability distribution as a terrain function, called the density landscape, and capture its topographical features as the basis for defining clusters.",1. Introduction,[0],[0]
The topographical features include modes (peaks) and their attractive basins.,1. Introduction,[0],[0]
"For high-dimension and sparse data, it is natural to have many modes.",1. Introduction,[0],[0]
"To avoid over-segmentation of the data and generation of many clusters with only few members, we employ a persistent homology theory (Edelsbrunner & Harer, 2010) to measure the saliency of all modes and merge the trivial ones.",1. Introduction,[0],[0]
Our principled method for clustering mixed-type data respects the underlying topographical features of the density landscape and achieves competitive performance on real data.,1. Introduction,[0],[0]
Clustering has been extensively studied in machine learning and data mining.,1.1. Related Work,[0],[0]
Many comprehensive surveys have been produced detailing the landscape of clustering problems and models.,1.1. Related Work,[0],[0]
"Here we will review related work in the context of geometric versus probabilistic clustering methods for mixed-type data and clustering methods that rely on topographical features such as modes and their attractive basins.
",1.1. Related Work,[0],[0]
Geometric clustering methods A straightforward approach for mixed-type data clustering is to map them into either pure continuous or pure categorical domains before applying a standard clustering method.,1.1. Related Work,[0],[0]
"A metric based on Gower’s coefficient (Gower, 1971) has been proposed for mixed-type data, which rescales the difference in all dimensions, continuous or categorical, and take the average.",1.1. Related Work,[0],[0]
One can apply any distance based method using these metrics.,1.1. Related Work,[0],[0]
"However, all these methods are heuristic; there is no good justification for the underlying geometric intuition of these methods on such a counter-intuitive metric space, despite some successful stories in practice.",1.1. Related Work,[0],[0]
"For example, K-Prototypes algorithm (Huang, 1998) uses a weighted sum of the Euclidean distance and Hamming distance and adopts the K-Means method (Faber, 1994), which iteratively finds the mean of each cluster and re-associates data to different clusters.",1.1. Related Work,[0],[0]
"When the data is pure categorical, the method is called K-Modes (Huang, 1997).",1.1. Related Work,[0],[0]
"Chiu et al. (2001) proposed a hierarchical clustering method, in
which distance between clusters are measured using their log-likelihood, which treat continuous and categorical domain separately.
",1.1. Related Work,[0],[0]
Probabilistic clustering methods Graphical models have been applied to clustering before.,1.1. Related Work,[0],[0]
"Zhang (2004) proposed a latent tree model, i.e., a Bayesian tree whose leaf nodes correspond to all observed dimensions and internal nodes are latent variables determining different clusters.",1.1. Related Work,[0],[0]
"Such tree structure can be learned using efficient algorithms (Chen et al., 2012; Liu et al., 2015).",1.1. Related Work,[0],[0]
"However, this method is only restricted to categorical data.",1.1. Related Work,[0],[0]
Lee & Hastie (2015) proposed a loopy graphical model to model mixed-type data.,1.1. Related Work,[0],[0]
"Their model reduces to a discrete Markov random field when all attributes are categorical, and a Gaussian graphical model when all attributes are continuous.",1.1. Related Work,[0],[0]
"Parameters are learned using pseudo-likelihood estimation (Besag, 1975) and edges are selected using group sparsity penalties (Yuan & Lin, 2006; Huang & Zhang, 2010).",1.1. Related Work,[0],[0]
"However, an efficient inference model is missing in order to apply such model to clustering.
",1.1. Related Work,[0],[0]
Clustering by mode-seeking The density landscape has been exploited before to extract global properties of the data and to achieve better clustering quality.,1.1. Related Work,[0],[0]
"Mode-seeking methods, i.e., associating data to modes representing clusters, have been proposed before in continuous domain (Cheng, 1995; Comaniciu & Meer, 2002b).",1.1. Related Work,[0],[0]
"But such methods rely on a kernel density estimation, which suffers from the curse of dimensionality and thus do not scale to high dimensions (Wasserman, 2013, chap. 20).",1.1. Related Work,[0],[0]
Chen & Quadrianto (2016) proposed a mode-seeking method for categorical data clustering.,1.1. Related Work,[0],[0]
"However, their method tends to produce trivial modes/clusters and thus over-segments the data, mainly due to the lack a principled way to merge modes into clusters of proper size.
",1.1. Related Work,[0],[0]
"Persistent homology for merging clusters In recent years, novel approaches have been proposed to merge modes/clusters based on the topographical landscape of the density function.",1.1. Related Work,[0],[0]
Chazal et al. (2013) used topological persistence to guide the merging of data into clusters.,1.1. Related Work,[0],[0]
"Their method, although theoretically sound, relies on a k-nearest neighbor graph of the data and a given density function, e.g., a kernel density estimation (Silverman, 1986) or a distance from measure (Chazal et al., 2011).",1.1. Related Work,[0],[0]
This method assumes that the data is a high quality sample of the domain and the k-nearest neighbor graph faithfully captures the topographical characteristics of the distribution.,1.1. Related Work,[0],[0]
"However, this condition is often too strong to assume in practice, where most datasets are relatively sparse.",1.1. Related Work,[0],[0]
"In this paper, we propose to start with mode-seeking, and leverage these modes and the gradient paths as a more accurate account of the density landscape.",1.1. Related Work,[0],[0]
Our idea proves to be a better solution and a good complement to the theoretical tool.,1.1. Related Work,[0],[0]
"We also refer to other topological and geometrical studies into the
global structures of hierarchical clustering (Eldridge et al., 2015; Carlsson & Mémoli, 2010).",1.1. Related Work,[0],[0]
"A probabilistic graphical model (Koller & Friedman, 2009) consists of a set of inter-dependent random variables X = (X1, . . .",2. Background,[0],[0]
", XD), a potential function f , and a graph G = (V, E).",2. Background,[0],[0]
Each element in the node set V represents one random variable from X .,2. Background,[0],[0]
The edges represents the dependence relations between pairs of variables.,2. Background,[0],[0]
There are two different kinds of variables in our setting: continuous ones and discrete ones variables.,2. Background,[0],[0]
"For simplification, we assume each discrete variable takes discrete values Xi ∈ L = {1, . . .",2. Background,[0],[0]
", L}.",2. Background,[0],[0]
"In this paper, we use discrete and categorical interchangeably and focus on non-ordinal discrete variables, although ordinal discrete variables are of interest in practice as well.",2. Background,[0],[0]
"In our setting, only Hamming distance can be used for discrete variables.
",2. Background,[0],[0]
"A value assignment to all random variables x = (x1, . . .",2. Background,[0],[0]
", xD) is called a configuration.",2. Background,[0],[0]
"A potential function f : x → R assigns to each configuration a real value, which is inversely proportional to the logarithm of the probability distribution, p(x) = exp(−f(x)",2. Background,[0],[0]
"− A), where A is the log-partition function.",2. Background,[0],[0]
"In this paper, we focus on tree structured graphical models, represented by T = (V, E).",2. Background,[0],[0]
"For a tree model, the probability and potential of a configuration can be factorized into a product (Bach & Jordan, 2003):
p(x) = ∏
(i,j)∈E
p(xi, xj)
p(xi)p(xj)",2. Background,[0],[0]
"∏ k∈V p(xk), (2.1)
where p(xi, xj) is the bivariate marginal density of the variable Xi and Xj , and p(xk) is the univariate marginal density of the variable Xk.
",2. Background,[0],[0]
"When the true distribution can be represented by a tree, we can use the algorithm by Chow & Liu (1968) to reconstruct the tree model.",2. Background,[0],[0]
"First, we compute the mutual information between all pairs of variables:
MIij = ∫ xi,xj p(xi, xj) log p(xi, xj) p(xi)p(xj)",2. Background,[0],[0]
"dxidxj ,
using empirical univariate and bivariate marginals.",2. Background,[0],[0]
The integral is replaced by sum when Xi and Xj have discrete values.,2. Background,[0],[0]
"Next, we compute the maximum spanning tree of a complete graph with D nodes, using the mutual information as edge weights.",2. Background,[0],[0]
"The computed tree is the desired tree model with the optimal KL-divergence from the true tree distribution (Liu et al., 2011).",2. Background,[0],[0]
More details of the selection of the models for univariate and bivariate densities will be given in Section 3.,2. Background,[0],[0]
Our method first estimates the underlying probabilistic density function from given data.,3. Method,[0],[0]
We choose tree-models as they strike a elegant balance between computational efficiency and flexibility of the model.,3. Method,[0],[0]
"Next, we propose to cluster data based on the density landscape: associating data with modes/peaks of the density, and merge them based on advanced persistent homology theory.",3. Method,[0],[0]
"First, we formalize the definition of modes in the mixed-type domain.",3. Method,[0],[0]
"Then we present algorithms for modes-seeking (Section 3.2) and for modes-merging (Section 3.3).
",3. Method,[0],[0]
We first formalize what a mode is in a D-dimensional mixed-type data domain.,3. Method,[0],[0]
Our definition is not restricted to the underlying model.,3. Method,[0],[0]
Denote by Id and Ic the index sets of discrete- and continuous-valued random variables.,3. Method,[0],[0]
"Denote by distH(x, x′) the Hamming distance between x and x′ within the discrete dimensions, and distL2(x, x′) the L2 distance within the continuous dimensions.",3. Method,[0],[0]
We call a discrete neighborhood of x with radius δ > 0,3. Method,[0],[0]
"as all elements with no more than δ Hamming distance and zero Euclidean distance from x, formally,
N dδ (x) = {x′ | distd(x, x′) ≤ δ ∧ distc(x, x′) = 0}.
",3. Method,[0],[0]
"Similarly, we define a continuous neighborhood of x with radius > 0",3. Method,[0],[0]
"as
N c (x) = {x′ | distd(x, x′) = 0 ∧ distc(x, x′) ≤ }.
",3. Method,[0],[0]
"Given a probability density function, p(X), a mode is a local maximum in both the continuous neighborhood and discrete neighborhood, formally: Definition 1 (Modes).",3. Method,[0],[0]
A point x ∈ X is a mode if and only if there exists positive numbers > 0,3. Method,[0],[0]
and δ > 0 such that (1) p(x) ≥ p(x′) for any x′ ∈ N c (x); and (2) p(x) ≥ p(x′) for any x′ ∈ N dδ,3. Method,[0],[0]
(x).,3. Method,[0],[0]
"It suffices to use the smallest positive integer for the discrete neighborhood, δ = 1.",3. Method,[0],[0]
"In this paper, we focus on a tree-structured graphical model.",3. Method,[0],[0]
"Next, we describe our tree model in details within the mixed-type setting.",3. Method,[0],[0]
"We formalize the univariate and bivariate marginal densities p(xi) and p(xi, xj) in the tree model (Eq. (2.1)).",3.1. Instantiating the Tree Model,[0],[0]
"We assume a set of N data {y1, y2, · · · , yN} is given.",3.1. Instantiating the Tree Model,[0],[0]
"For discrete dimensions, we use Multinoulli distribution with Dirichlet prior α = 1, ∀i, j ∈",3.1. Instantiating the Tree Model,[0],[0]
"Id:
p(xi) = Nxi + 1
N + L , with Nxi = N∑ n=1 Jyni = xiK,
p(xi, xj) = Nxi,xj + 1
N + L2 ,
with Nxi,xj = N∑ n=1 Jyni = xi ∧ ynj = xjK.
For continuous variables, we use one-dimensional kernel density estimation for univariate density, and product kernel (Scott, 2015) for univariate and bivariate marginal density.",3.1. Instantiating the Tree Model,[0],[0]
"Formally, ∀i, j ∈ Ic,
p(xi) = 1
N N∑ n=1 Kh1i(y n",3.1. Instantiating the Tree Model,[0],[0]
i,3.1. Instantiating the Tree Model,[0],[0]
"− xi), and
",3.1. Instantiating the Tree Model,[0],[0]
"p(xi, xj) = 1
N N∑ n=1",3.1. Instantiating the Tree Model,[0],[0]
{,3.1. Instantiating the Tree Model,[0],[0]
Kh2i(y n,3.1. Instantiating the Tree Model,[0],[0]
i,3.1. Instantiating the Tree Model,[0],[0]
"− xi)Kh2j (ynj − xj) } ,
(3.1)
We use a one-dimensional Gaussian kernel, denoted as Kh(z) =
1√ 2πh
exp ( − z 2
2h2
) .",3.1. Instantiating the Tree Model,[0],[0]
"Following standard non-
parametric statistics literature (Fan & Gijbels, 1996; Tsybakov, 2009), the kernel bandwidths for univariate and bivariate density are chosen as
hti = 1.06·min { σ∗i , q∗i,0.75 − q∗i,0.25 1.34 } ·N− 1 2β+t , t = 1, 2,
where σ∗i , q ∗ i,0.75 and q ∗ i,0.25 are the standard deviation, the 75% and 25% sample quantiles of Xi, respectively.",3.1. Instantiating the Tree Model,[0],[0]
"The variable β is the order of the kernel (Fan & Gijbels, 1996) and is set to 2 by default.
",3.1. Instantiating the Tree Model,[0],[0]
The choice of a product kernel is justified by two reasons.,3.1. Instantiating the Tree Model,[0],[0]
"First, a product kernel reduces to the product of onedimensional kernels, which are more reliable that a direct 2D kernel density estimation.",3.1. Instantiating the Tree Model,[0],[0]
"Second, the product kernel proves to be convenient to be adopt to bivariate densities for variables with mixed-type as follows.",3.1. Instantiating the Tree Model,[0],[0]
"For a mixed-type pair of variables, (Xi, Xj), i ∈ Ic, j ∈",3.1. Instantiating the Tree Model,[0],[0]
"Id, we take the limit of h2i to zero in the product kernel formula (Equation (3.1)).",3.1. Instantiating the Tree Model,[0],[0]
"The first kernel becomes the Dirac-delta function, leading to the following bivariate marginal
p(xi, xj) = 1
N N∑ n=1",3.1. Instantiating the Tree Model,[0],[0]
{,3.1. Instantiating the Tree Model,[0],[0]
Jynj = xjKKh2i(yni,3.1. Instantiating the Tree Model,[0],[0]
"− xi) } .
",3.1. Instantiating the Tree Model,[0],[0]
Building the tree model.,3.1. Instantiating the Tree Model,[0],[0]
"Using these empirical univariate and bivariate marginal densities, we estimate all pairwise mutual information, and then compute the tree (V, E) using the Chow-Liu algorithm.",3.1. Instantiating the Tree Model,[0],[0]
Plugging the univariate and bivariate marginal densities into Eq.,3.1. Instantiating the Tree Model,[0],[0]
"(2.1), we have the complete density distribution (the tree model).",3.1. Instantiating the Tree Model,[0],[0]
"Next, we present our algorithm for finding the modes over the density landscape of the computed model.",3.1. Instantiating the Tree Model,[0],[0]
Our algorithm assigns each data to a mode via a gradient ascent procedure.,3.2. Mode-Seeking Algorithm,[0],[0]
"For a mixed-domain, a gradient is not well defined.",3.2. Mode-Seeking Algorithm,[0],[0]
"Following the definition of modes (Def. 1), we formulate a gradient step as an optimization within either the continuous neighborhood N c (x) or the discrete
neighborhood N dδ (x), with δ = 1.",3.2. Mode-Seeking Algorithm,[0],[0]
"The two procedures have to be taken alternatively in order to continue increasing the probability until a mode is reached.
",3.2. Mode-Seeking Algorithm,[0],[0]
"Our algorithm starts at each data, s, iteratively walks to a nearby point with bigger probability until convergence.",3.2. Mode-Seeking Algorithm,[0],[0]
"The final position is the mode of interest and will be associated with the data, s. For ease of computation, we use the potential function f(x) instead of the probability density function:
f(x) =",3.2. Mode-Seeking Algorithm,[0],[0]
"− ∑
(i,j)∈E log p(xi, xj)− ∑ i∈V (1− di) log p(xi),
(3.2) in which di is the degree of node i in the tree.",3.2. Mode-Seeking Algorithm,[0],[0]
It is easy to verify that p(x) ∝ −f(x).,3.2. Mode-Seeking Algorithm,[0],[0]
"Therefore, modes of p(x) are the local minima of f(x), following the same definition in Def. 1.",3.2. Mode-Seeking Algorithm,[0],[0]
"We follow the aforementioned iterative procedure, except at each step, we find a nearby point with smaller potential.
",3.2. Mode-Seeking Algorithm,[0],[0]
"At each step of the algorithm, we first update all discrete variables until no better elements exist within the discrete neighborhood N dδ",3.2. Mode-Seeking Algorithm,[0],[0]
(x),3.2. Mode-Seeking Algorithm,[0],[0]
with δ = 1.,3.2. Mode-Seeking Algorithm,[0],[0]
"Next, we update all continuous variables using gradient descent, until the gradient of f at continuous dimensions∇cf becomes zero.",3.2. Mode-Seeking Algorithm,[0],[0]
"Our main algorithm is summarized in Alg. 1.
",3.2. Mode-Seeking Algorithm,[0],[0]
Algorithm 1 Mode-Seeking Algorithm 1,3.2. Mode-Seeking Algorithm,[0],[0]
": Input: Data D = {si | i = 1, · · · , N}; a potential
function f .",3.2. Mode-Seeking Algorithm,[0],[0]
"2: Output: A set of modes,M; mode indices associated
to each data {ci | i = 1, · · · , N} 3: M← ∅ 4: for i = 1 to N do 5: x← si 6: repeat 7: repeat 8: x← argminz∈Nd1 (x) f(z) 9: until x converges
10: repeat 11: x← x− η∇cf 12: until x converges 13: until x converges 14: if x /∈M then 15: M←M∪ {x} 16: end if 17: ci ← the index of x inM 18: end for
Here η is the stepsize.",3.2. Mode-Seeking Algorithm,[0],[0]
"The best neighbor within Hamming distance one, argminz∈Nd1 (x) f(z), can be computed using dynamic programming.",3.2. Mode-Seeking Algorithm,[0],[0]
"This can be achieved by directly adapting the algorithm by (Chen & Quadrianto, 2016).
",3.2. Mode-Seeking Algorithm,[0],[0]
"It remains to compute the gradient of f in the contin-
uous domain, ∇cf .",3.2. Mode-Seeking Algorithm,[0],[0]
For each continuous variable,3.2. Mode-Seeking Algorithm,[0],[0]
"i ∈ Ic, relevant terms in the energy function (Eq. (3.2)) can be divided into three groups, the univariate term, the bivariate terms with a continuous neighbor, j ∈ Ic, and the bivariate terms with a discrete neighbor, j ∈ Id. Treating them differently, the partial derivative:
∂f(x)
∂xi = −(1− di)
∑N n=1 Kh1i(y n",3.2. Mode-Seeking Algorithm,[0],[0]
"i − xi)
yni",3.2. Mode-Seeking Algorithm,[0],[0]
"−xi h21i∑N
n=1 Kh1i(y n",3.2. Mode-Seeking Algorithm,[0],[0]
i,3.2. Mode-Seeking Algorithm,[0],[0]
"− xi)
",3.2. Mode-Seeking Algorithm,[0],[0]
"− ∑
j∈Ic:(i,j)∈E
∑N n=1 Kh2i(y n",3.2. Mode-Seeking Algorithm,[0],[0]
i,3.2. Mode-Seeking Algorithm,[0],[0]
"− xi)Kh2j (ynj − xj)
",3.2. Mode-Seeking Algorithm,[0],[0]
yni,3.2. Mode-Seeking Algorithm,[0],[0]
"−xi h22i∑N
n=1 Kh2i(y n i",3.2. Mode-Seeking Algorithm,[0],[0]
"− xi)Kh2j (ynj − xj)
",3.2. Mode-Seeking Algorithm,[0],[0]
"− ∑
k∈Id:(i,j)∈E
∑N n=1 Kh2i(y n",3.2. Mode-Seeking Algorithm,[0],[0]
i,3.2. Mode-Seeking Algorithm,[0],[0]
"− xi)Jynk = xkK
yni",3.2. Mode-Seeking Algorithm,[0],[0]
"−xi h22i∑N
n=1 Kh2i(y n i − xi)
(3.3)
",3.2. Mode-Seeking Algorithm,[0],[0]
"Algorithm 2 Merging Data Using Topological Persistence
1: Input: Ĝ = (V̂, Ê), density function p : V̂ → R+, persistence threshold τ 2: Output: Clusters C 3: C ← ∅ 4: Sort elements in V̂ according to the density function
values, so that p(vi) ≥ p(vi+1), ∀vi, vi+1 ∈ V̂ .",3.2. Mode-Seeking Algorithm,[0],[0]
"5: for i = 1 to |V̂| do 6: nbd← {vj | (vi, vj) ∈ Ê ∧ j < i} 7: // neighbors of vi with smaller indices (bigger p) 8: if nbd = ∅",3.2. Mode-Seeking Algorithm,[0],[0]
"then 9: create a new cluster c = {vi}
10: birth(c)← p(vi) 11: C ← C ∪ {c} 12: else 13:",3.2. Mode-Seeking Algorithm,[0],[0]
Cnbd ← all clusters containing nodes in nbd 14: cmax ← argmaxc∈Cnbd birth(c) 15: for all c ∈ Cnbd and c 6= cmax do 16: persistence(c)← birth(c)− p(vi) 17: if persistence(c) < τ,3.2. Mode-Seeking Algorithm,[0],[0]
then 18: // merge c into cmax 19: cmax ← cmax ∪ c 20: C ← C\{c} 21: end if 22: end for 23: // assign vi to cmax 24: cmax ← cmax ∪ {vi} 25: end if 26: end for,3.2. Mode-Seeking Algorithm,[0],[0]
The modes computed in Alg. 1 provide a clustering of the data.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"However, in practice, the data is often relatively sparse.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In such cases, the method tends to produce a large
number of modes, and thus over-segments the data into small clusters.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"There are ways to merge these small clusters (Ward Jr, 1963; Day & Edelsbrunner, 1984).",3.3. Merging Clusters Using Topological Persistence,[0],[0]
But they rely on a distance metric to measure similarities between clusters.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Instead, we propose a principled approach that is only based on the density landscape, i.e., the topographical features such as peaks, ridges, valleys.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
Our method is built on the theory of persistent homology.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"We focus on zerodimensional topological structures in this paper, although the theory is much more general.
",3.3. Merging Clusters Using Topological Persistence,[0],[0]
Persistence of modes.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"We estimate the saliency of a peak (mode) using its “relative height”, namely, the difference between its height and the level at which its basin of attraction meets the one of another higher mode.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Formally, we filter the domain using a function value threshold t from +∞ to −∞. As t decreases, we monitor the topological changes of the progressively growing superlevel set, X t = {x ∈ X | p(x) ≥ t}, that is, the domain whose probability density value is no smaller than t. Each mode attributes to the birth of a new connected component in the superlevel set and the component is killed when it meets another component created by a higher mode.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"The density value of the creating mode and the density value of the point at which the two components meet (called a saddle) are called the birth and death times, and their difference, called the persistence, measures the saliency of this mode.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"See Figure 1 for an illustration.
",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"The merging of connected components as we decrease the threshold t provides a natural way to merge modes; when two connected components meet, we merge them if one of them has≤ τ",3.3. Merging Clusters Using Topological Persistence,[0],[0]
persistence (Figure 1).,3.3. Merging Clusters Using Topological Persistence,[0],[0]
This gives us a principled way to merge modes.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Based on the convergence of tree-model estimation (Liu et al., 2011) and the stability
of persistent homology (Cohen-Steiner et al., 2007), this method is guaranteed to be robust to noise and L∞ perturbation of the density function.
",3.3. Merging Clusters Using Topological Persistence,[0],[0]
Sample-based persistence computation.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Given a dense uniform sampling of the whole domain X , we can trust these samples will describe the density landscape faithfully.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In practice, however, a uniform sampling will have exponential size to the dimension.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Chazal et al. (2013) used the k-nearest neighbor graph of the input data, D, assuming they are good samples from the density function.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"However, in practice, the data is often relatively sparse and cannot represent the landscape well enough to produce a high quality mode-merging hierarchy.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In fact, it is very likely that the modes are not included in the data and thus the birth time (as well as the persistence) will be under-estimated.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"See Figure 2(left) for an illustration.
",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In this paper, we propose to compute persistence based on all points we encountered during the mode-seeking procedure.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In Algorithm 1, we collect the point x computed after each iteration (after line 12).",3.3. Merging Clusters Using Topological Persistence,[0],[0]
The gradient step also provides a natural edge connecting these points.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
This tree structured graph give us a high-quality description of the attractive basin of each mode.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
This provides us a well-suited underlying graph describing the density landscape.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
See Figure 2(right).,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Finally, to ensure the graph is fully connected, and the space between modes are well described, we add edges (green edges) connecting points from neighboring attractive basins, as well as the lowest point along these edges (green markers).",3.3. Merging Clusters Using Topological Persistence,[0],[0]
Note that this is the only time when the distance metric plays a role in our model.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"We use a sum of the Hamming distance and Euclidean distance.
Algorithm.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"Given a graph Ĝ = (V̂, Ê), in which each node is assigned a probability density, we compute the persistence-based merge tree as follows.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
Sort all nodes in decreasing order of their density function values.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
Add them into the superlevel set one-by-one.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"To add a node vi, we check whether it is adjacent to any nodes that have been included.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"If not, vi, which must be a mode itself, creates a new connected component with the birth time p(vi).",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"If vi is connected to multiple existing connected components, we keep the one with the earliest birth time, cmax, and merge some others into cmax.",3.3. Merging Clusters Using Topological Persistence,[0],[0]
"In particular, for each other adjacent connected component, we check whether its life length so far is less than τ .",3.3. Merging Clusters Using Topological Persistence,[0],[0]
The ones with ≤ τ,3.3. Merging Clusters Using Topological Persistence,[0],[0]
life length will be merged into cmax.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
We add vi into,3.3. Merging Clusters Using Topological Persistence,[0],[0]
the connected component cmax See Figure 3 for an illustration.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
See Alg. 2 for the pseudocode.,3.3. Merging Clusters Using Topological Persistence,[0],[0]
"We compare our methods with existing clustering methods on several real world mixed-type datasets from UCI repository (Lichman, 2013): Contraceptive Method Choice dataset (CMC), Credit Approval dataset (CRX),
German Credit Approval (German), and Statlog Heart Disease dataset (Heart).",4. Experiments,[0],[0]
See the table below for more details.,4. Experiments,[0],[0]
"All datasets have 60% to 70% of the features being discrete.
",4. Experiments,[0],[0]
Table 1.,4. Experiments,[0],[0]
"Datasets
Data # of samples Dimension # of clusters CMC 1473 9 3 Heart 297 13 5 CRX 653 15 2
German 1000 20 2",4. Experiments,[0],[0]
Our method can be straightforwardly parallelized.,4. Experiments,[0],[0]
We run the mode-seeking for all data points (the for-loop in Alg. 1) in parallel.,4. Experiments,[0],[0]
"On average, the mode-seeking of a single data takes 6 gradient ascent steps and 5.87 seconds.",4. Experiments,[0],[0]
"On a cluster with 48 cores, our program finishes within 3 minutes for any of the datasets.",4. Experiments,[0],[0]
"If running in a sequential manner, the time will be linear to the dataset size.",4. Experiments,[0],[0]
"After all data are processed, we collect all relevant points and run a persistence-based merging sequentially.",4. Experiments,[0],[0]
This step takes less than 20 seconds for any of the datasets.,4. Experiments,[0],[0]
The persistence-based merging depends on a threshold τ .,4. Experiments,[0],[0]
It is hard to select a universal one due to the large variation among datasets.,4. Experiments,[0],[0]
"Instead, we choose the τ for each dataset so that the desired the nubmer of clusters remain after merging.",4. Experiments,[0],[0]
This is a fair comparison; all clustering methods we compare with use an oracle number of clusters.,4. Experiments,[0],[0]
We empirically set the parameter δ to one.,4. Experiments,[0],[0]
"Using a bigger δ hurts the performance as it would try to ‘smooth’ the landscape in the categorical domain.
",4. Experiments,[0],[0]
"All methods can be grouped into five different groups, based on the underlying domain and the approach.",4. Experiments,[0],[0]
The first group assumes a continuous domain and an Euclidean metric.,4. Experiments,[0],[0]
"We project the mixed-type data into the continuous domain and directly apply such methods, including k-means (Faber, 1994), Affinity Propagation (Frey & Dueck, 2007), Mean Shift (Cheng, 1995; Comaniciu & Meer, 2002a), Spectral Clustering (Kamvar et al., 2003), Ward’s algorithm (Ward Jr, 1963), Agglomerative clustering (Day & Edelsbrunner, 1984) and DBSCAN (Ester et al., 1996).
",4. Experiments,[0],[0]
"The second group are methods designed for pure categorical domain, e.g., K-Modes (Huang, 1997), ROCK (Guha et al., 1999), mixture of multinoulli (latent class analysis) (McCutcheon, 1987).",4. Experiments,[0],[0]
We convert mixed-type data into categorical data by thresholding continuous values at the median.,4. Experiments,[0],[0]
"We also include Affinity Propagation, Spectral Clustering and DBSCAN in this group; these methods can be applied to any distance metrics.",4. Experiments,[0],[0]
"We compute pairwise Hamming distance between data as the input of these three methods.
",4. Experiments,[0],[0]
"For the third group, we use these three methods, but using a distance matrix based on Gower’s coefficient (Gower, 1971), which was designed specifically for mixed-domain.",4. Experiments,[0],[0]
The fourth group uses a simply sum of the Euclidean distance (restricted to continuous dimensions) and Hamming distance (restricted to categorical dimensions).,4. Experiments,[0],[0]
"A good rep-
resentative in such group is K-Prototypes (Huang, 1998).",4. Experiments,[0],[0]
"We again applied the three methods (Affinity, Spectral and DBSCAN) on this new metric.
",4. Experiments,[0],[0]
"In the last group, we compare our method and a few other topological methods.",4. Experiments,[0],[0]
We compare to the method using only modes for clustering.,4. Experiments,[0],[0]
"This is essentially an adaptation of (Chen & Quadrianto, 2016) to the mixed-type domain.",4. Experiments,[0],[0]
"We also compare to (Chazal et al., 2013) by computing the persistence on the k-nearest neighbor graph, using our tree-model as the underlying density estimation.",4. Experiments,[0],[0]
"Finally, we also show the result of our method.
",4. Experiments,[0],[0]
The results are listed in Table 2.,4. Experiments,[0],[0]
"We use the Adjusted Mutual Information (AMI) (Vinh et al., 2010) and Adjusted Rand Score (ARS) (Hubert & Arabie, 1985) to evaluate all
methods.",4. Experiments,[0],[0]
"For all methods requiring random initializations, we run each one for 10 times and take the average performance.",4. Experiments,[0],[0]
"When necessary, we provide a true number of clusters as an oracle.",4. Experiments,[0],[0]
The cells with N/A correspond to the cases when the program crashes.,4. Experiments,[0],[0]
"It is most likely because the Gower’s coefficient and Hamming distance does not give us a well-conditioned distance matrix for the spectral clustering method.
",4. Experiments,[0],[0]
Discussion.,4. Experiments,[0],[0]
"Our method outperforms most methods from all other four groups, using different types of metrics.",4. Experiments,[0],[0]
We also observe that a few methods based on pure categorical domain are quite competitive.,4. Experiments,[0],[0]
"Similarly, K-prototype, a popular tool for mixed-type data, has good performance on some data.",4. Experiments,[0],[0]
"Outperforming other topological methods (modes only and persistence only) demonstrate the significance of our contribution.
",4. Experiments,[0],[0]
Our current experiments assume the correct number of clusters is given.,4. Experiments,[0],[0]
"It is possible to prove that with sufficient samples and the correct threshold τ , the persistence-based clustering can find the correct number of cluster and the right clustering for most data points in a sense similar to the elegant result in (Chazal et al., 2013).",4. Experiments,[0],[0]
"A closely related theoretical result is in (Eldridge et al., 2015), which shows that the hierarchical clustering tree constructed by a similar merging procedure is consistent for points sampled from a nice density distribution over RD.",4. Experiments,[0],[0]
"In this paper, we propose a probabilistic clustering method for mixed-type data.",5. Conclusions,[0],[0]
We design a tree-structured graphical model for the mixed-type domain.,5. Conclusions,[0],[0]
We also develop methods based on a topographical view of the density landscape.,5. Conclusions,[0],[0]
"We design algorithms to capture modes of the density landscape and merge trivial modes based on the theory of persistent homology.
Acknowledgments.",5. Conclusions,[0],[0]
XN and CC have been partly funded by the grant PSC-CUNY 69844-00 47.,5. Conclusions,[0],[0]
NQ has been partly funded by the Russian Academic Excellence Project ‘5- 100’.,5. Conclusions,[0],[0]
YW has been partly supported by the grant NSF DMS-1547357.,5. Conclusions,[0],[0]
The authors gratefully acknowledge use of the services and facilities of CUNY Queens Colleges Center for Computational Infrastructure for the Sciences (CCIS).,5. Conclusions,[0],[0]
Clustering data with both continuous and discrete attributes is a challenging task.,abstractText,[0],[0]
Existing methods often lack a principled probabilistic formulation.,abstractText,[0],[0]
"In this paper, we propose a clustering method based on a tree-structured graphical model to describe the generation process of mixed-type data.",abstractText,[0],[0]
"Our tree-structured model factorizes into a product of pairwise interactions, and thus localizes the interaction between feature variables of different types.",abstractText,[0],[0]
"To provide a robust clustering method based on the tree-model, we adopt a topographical view and compute peaks of the density function and their attractive basins for clustering.",abstractText,[0],[0]
"Furthermore, we leverage the theory from topology data analysis to adaptively merge trivial peaks into large ones in order to achieve meaningful clusterings.",abstractText,[0],[0]
Our method outperforms state-of-the-art methods on mixed-type data.,abstractText,[0],[0]
Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data,title,[0],[0]
"How to model rank data and how to make optimal statistical inferences from rank data are important topics at the interface of statistics, computer science, and economics.",1. Introduction,[0],[0]
"Random utility models (RUMs) (Thurstone, 1927) are one of the most widely-applied statistical models for rank data.",1. Introduction,[0],[0]
"In an RUM, each alternative ai is parameterized by a utility distribution µi.",1. Introduction,[0],[0]
Agents’ rankings are generated in two steps.,1. Introduction,[0],[0]
"In the first step, a latent utility ui for each alternative ai is generated from µi.",1. Introduction,[0],[0]
"In the second step, the alternatives are ranked w.r.t.",1. Introduction,[0],[0]
their utilities ui in descending order.,1. Introduction,[0],[0]
"The logit model and the probit model, which are very popular in statistics and economics, both have random utility interpretations.
",1. Introduction,[0],[0]
"While providing better fitness to the rank data (Azari Soufiani et al., 2012; Zhao et al., 2018b), general RUMs are computationally hard to tackle due to the lack of closed-form formulas for the likelihood function.",1. Introduction,[0],[0]
"The only known exception is the Plackett-Luce model (Plackett, 1975; Luce, 1959),
1Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Zhibing Zhao <zhaoz6@rpi.edu>, Lirong Xia <xial@cs.rpi.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
which is the RUM with Gumbel distributions.",1. Introduction,[0],[0]
"RUMs, especially the Plackett-Luce model, have been widely applied to model and predict human behavior (McFadden, 2000), where the standard case of discrete choice models can be viewed as the Plackett-Luce model restricted to top choices.",1. Introduction,[0],[0]
"Other notable recent applications include elections (Gormley & Murphy, 2008), crowdsourcing (Pfeiffer et al., 2012), recommender systems (Wang et al., 2016), preference elicitation (Azari Soufiani et al., 2013b; Zhao et al., 2018a), marketing (Berry et al., 1995), health care (Bockstael, 1999), transportation (Bhat et al., 2007), and security (Yang et al., 2011).
",1. Introduction,[0],[0]
Recently there has been a growing interest in designing faster and more accurate algorithms for RUMs.,1. Introduction,[0],[0]
Many algorithms in previous work share the following rank-breakingthen-optimization architecture.,1. Introduction,[0],[0]
"First, rank data are converted to pairwise comparison data.",1. Introduction,[0],[0]
"Second, based on the pairwise comparisons, various optimization algorithms are designed to estimate the ground truth (Negahban et al., 2012; Azari Soufiani et al., 2013a; 2014; Chen & Suh, 2015; Khetan & Oh, 2016b;a).
",1. Introduction,[0],[0]
"Pairwise data are often obtained from rank data by applying rank-breaking, which allows for a smooth tradeoff between computational efficiency and statistical efficiency (Azari Soufiani et al., 2013a; 2014; Khetan & Oh, 2016b;a).",1. Introduction,[0],[0]
"Given m alternatives, a rank-breaking scheme is modeled by a weighted undirected graph G (see Figure 1 for an example) over {1, . . .",1. Introduction,[0],[0]
",m} (the vertices are positions in a ranking), such that for any ranking R over the m alternatives and any distinct i1, i2 ≤ m, we obtain gi1i2 (the weight on the edge {i1, i2} in G) pairwise comparisons between alternatives at positions i1 and i2 of R.
Our Contributions.",1. Introduction,[0],[0]
"By leveraging the celebrated composite marginal likelihood (CML) methods (Lindsay, 1988; Varin, 2008), we propose a novel and flexible rank-breakingthen-CML framework.",1. Introduction,[0],[0]
"Given an RUM, our framework, denoted by RBCML(G,W), is defined by a weighted rankbreaking graph G and a CML-weight vectorW = {",1. Introduction,[0],[0]
"wi1i2 : i1, i2 ≤ m, i1 6= i2}, which contains one non-negative weight for each pair of alternatives (ai1 , ai2).",1. Introduction,[0],[0]
We note that both G andW are the algorithm designer’s choices.,1. Introduction,[0],[0]
"Given rank data P , we compute ~θ to maximize the following com-
posite log-likelihood function.",1. Introduction,[0],[0]
"CLLM(~θ, P ) = ∑ i1 6=i2",1. Introduction,[0],[0]
(,1. Introduction,[0],[0]
"κi1i2wi1i2 ln pi1i2( ~θ))
",1. Introduction,[0],[0]
Here ~θ represents the parameters of RUM.,1. Introduction,[0],[0]
"Given G, κi1i2 is the percentage of pairwise comparisons ai1 ai2 in the data.",1. Introduction,[0],[0]
"pi1i2(~θ) is the probability of ai1 ai2 under RUM with ~θ, which is the total probability of generating a ranking with ai1 ai2 given ~θ.",1. Introduction,[0],[0]
We note that the RBCML framework is very general because any combination of G andW can be used.,1. Introduction,[0],[0]
"A breaking graph G is uniform, if all edges have the same weight.",1. Introduction,[0],[0]
Let Gu denote the breaking graph whose weights are all 1.,1. Introduction,[0],[0]
"A CML-weight vectorW is symmetric, if for all i1 6= i2, we have wi1i2 = wi2i1 .",1. Introduction,[0],[0]
"W is uniform, if all weights are 1, denoted byWu.
",1. Introduction,[0],[0]
Theoretical contributions.,1. Introduction,[0],[0]
"For convenience we let position-k breaking denote the breaking that consists of all unit-weight edges between position k and all positions after k. E.g. the position-1 breaking consists of all unit-weight pairwise comparisons in positions {(1, 2), (1, 3), . . .",1. Introduction,[0],[0]
", (1,m)}.",1. Introduction,[0],[0]
A weighted union of position-k breakings is a breaking that has the same weight (possibly zero) for each k.,1. Introduction,[0],[0]
"An example is shown in Figure 1, which is the union of 1/3 position-1 breaking and 1/2 position-2 breaking.",1. Introduction,[0],[0]
"Our theoretical results carry the following message about “good"" RBCMLs.
",1. Introduction,[0],[0]
"We should use RBCML(G,W) with connected and symmetricW .",1. Introduction,[0],[0]
"For Plackett-Luce model, we should use a breaking G that is the weighted union of multiple position-k breakings.",1. Introduction,[0],[0]
"For RUMs with symmetric utility distributions, we should use Gu.
",1. Introduction,[0],[0]
"The message is established via a series of theorems (Theorems 1, 2, 5, 8, and 9).",1. Introduction,[0],[0]
"Theorems 1 and 2, which prove that strict log-concavity is preserved under convolution and under marginalization, are of independent interest.
",1. Introduction,[0],[0]
Algorithmic contributions.,1. Introduction,[0],[0]
"Experiments on synthetic data for Gaussian RUMs, where each utility distribution is Gaussian, show that RBCML(Gu,Wu) achieves better statistical efficiency and computational efficiency than the GMM algorithm by Azari Soufiani et al. (2014).",1. Introduction,[0],[0]
"For the Plackett-Luce model, we propose an RBCML with a heuristicWH .",1. Introduction,[0],[0]
"We compare our RBCML for the Plackett-Luce model with the consistent rank-breaking algorithm by Khetan & Oh (2016b) and the I-LSR algorithm by Maystre & Grossglauser (2015) via experiments on synthetic data and show that our RBCML provides a tradeoff between statistical efficiency and computational efficiency.
",1. Introduction,[0],[0]
Related Work and Discussions.,1. Introduction,[0],[0]
Our RBCML framework leverages the strengths of rank breaking and CML.,1. Introduction,[0],[0]
"The major advantage of CML is that often marginal likelihood functions are much easier to optimize than the full likeli-
hood function.",1. Introduction,[0],[0]
"However, for RUMs, even computing the marginal likelihood may take too much time, as CML needs to count the number of pairwise comparisons between alternatives in the rankings, which takes O(m2n) time, where m is the number of alternatives and n is the number of rankings.",1. Introduction,[0],[0]
"Therefore, standard CML becomes inefficient when m or n are large.",1. Introduction,[0],[0]
RBCML overcomes such inefficiency by applying rank-breaking.,1. Introduction,[0],[0]
"The computational complexity of rank-breaking can be O(kmn) for any k ≤ m. Often a tradeoff between computational efficiency and statistical efficiency must be made.
",1. Introduction,[0],[0]
"RBCML generalizes the algorithm proposed by Khetan & Oh (2016b), which focused on the Plackett-Luce model and whose optimization technique turns out to be CML with Wu.1",1. Introduction,[0],[0]
"The comparison between RBCML and other related work is summarized in Table 1.
",1. Introduction,[0],[0]
"Our theorems on strict log-concavity of composite likelihood function generalize Hunter (2004)’s result, which was proved for Plackett-Luce with Gu andWu.",1. Introduction,[0],[0]
"Our results can be applied to not only otherW’s under Plackett-Luce, but also other RUMs where the PDFs of utility distributions are strictly log-concave, e.g. Gaussians.",1. Introduction,[0],[0]
"Technically, proving our results for general RUMs is much more challenging due to the lack of closed-form formulas for the likelihood function.",1. Introduction,[0],[0]
"Another line of previous work proved (non-strict) log-concavity for special cases of RBCML (Azari Soufiani et al., 2012; Khetan & Oh, 2016a;b).",1. Introduction,[0],[0]
"Again, our theorems are stronger because (1) our theorems work for a more general class of RBCML, and (2) strict log-concavity is more desirable than log-concavity because the formal implies the uniqueness of the solution.
",1. Introduction,[0],[0]
The key step in our proofs is the preservation of strict logconcavity under convolution (Theorem 1) and marginalization (Theorem 2).,1. Introduction,[0],[0]
"Surprisingly, we were not able to find these theorems in the literature, despite that it is well-known that (non-strict) log-concavity and strong log-concavity are preserved under convolution and marginalization (Saumard & Wellner, 2014).",1. Introduction,[0],[0]
Our proofs of Theorems 1 and 2 are based on a careful examination of the condition for equality in the Prékopa-Leindler inequality proved by Dubuc (1977).,1. Introduction,[0],[0]
"We believe that Theorems 1 and 2 are of independent interest.
",1. Introduction,[0],[0]
Xu & Reid (2011) provided sufficient conditions for general CML methods to satisfy consistency and asymptotic normality.,1. Introduction,[0],[0]
"Unfortunately, some of the conditions by Xu & Reid (2011) do not hold for RBCML.",1. Introduction,[0],[0]
"Therefore, we derive new proof of consistency and asymptotic normality for RBCML.
Khetan & Oh (2016b;a) provide sufficient conditions on rank-breakings for CML with Wu to be consistent under
1Khetan & Oh (2016b)’s algorithm works for special partial orders.",1. Introduction,[0],[0]
"In this paper, we only focus on comparisons between RBCML and their algorithms restricted to linear orders.
",1. Introduction,[0],[0]
the Plackett-Luce model.,1. Introduction,[0],[0]
"It is an open question what are all consistent rank-breakings for CML, even withWu.",1. Introduction,[0],[0]
"We answer this question for Plackett-Luce (Theorem 8), as well as a large class of other RUMs (Theorem 9), and for all W’s.",1. Introduction,[0],[0]
"Let A = {a1, a2, · · · , am} denote the set of m alternatives.",2. Preliminaries,[0],[0]
Let L(A) denote the set of all linear orders (rankings) over A. A ranking R ∈ L(A) is denoted by ai1 ai2 . . .,2. Preliminaries,[0],[0]
"aim , where ai1 is ranked at the top, ai2 is ranked at the second position, etc.",2. Preliminaries,[0],[0]
"We write a R b if a is ranked higher than b in R. Let P = {R1, R2, . . .",2. Preliminaries,[0],[0]
", Rn} denote the collection of n rankings, called a preference profile.
",2. Preliminaries,[0],[0]
Definition 1 (Random utility models (RUMs)),2. Preliminaries,[0],[0]
A random utility modelM over A associates each alternative ai with a utility distribution µi(·|~θi).,2. Preliminaries,[0],[0]
"The parameter space is Θ = {~θ = {~θi|i = 1, 2, . . .",2. Preliminaries,[0],[0]
",m}}.",2. Preliminaries,[0],[0]
The sample space is L(A)n.,2. Preliminaries,[0],[0]
Each ranking is generated i.i.d.,2. Preliminaries,[0],[0]
in two steps.,2. Preliminaries,[0],[0]
"First, for each i ≤ m, a latent utility ui is generated from µi(·|~θi) independently, and second, the alternatives are ranked according to their utilities in the descending order.",2. Preliminaries,[0],[0]
"Given a parameter ~θ, the probability of generating R = ai1 ai2 . . .",2. Preliminaries,[0],[0]
"aim is
PrM(R|~θ) = ∫ ∞ −∞ ∫ ∞ uim · · · ∫ ∞ ui2 µim(uim |~θim) · · ·
µi1(ui1 |~θi1)dui1dui2 · · · duim
In this paper, we focus on the location family, where the shapes of the utility distributions are fixed and each utility distribution µi is only parameterized by its mean, denoted by θi.",2. Preliminaries,[0],[0]
Let πi denote the distribution obtained from µi(·|θi) by shifting the mean to 0.,2. Preliminaries,[0],[0]
"For the location family, we have πi(ui|θi) = π(ui − θi).",2. Preliminaries,[0],[0]
"Because shifting the means of all alternatives by the same distance will not affect the distribution of the rankings, w.l.o.g.",2. Preliminaries,[0],[0]
we let θm = 0 throughout the paper.,2. Preliminaries,[0],[0]
"Moreover, we assume that the PDF of each utility distribution is continuous and positive everywhere.",2. Preliminaries,[0],[0]
We further say that an RUM is symmetric if the PDF of each utility distribution is symmetric around its mean.,2. Preliminaries,[0],[0]
"We use Gaussian RUMs to denote the RUMs where all utility distributions are Gaussian.
",2. Preliminaries,[0],[0]
"For any combination of m probability distributions π1, . . .",2. Preliminaries,[0],[0]
", πm whose means are 0, we let RUM(π1, . .",2. Preliminaries,[0],[0]
.,2. Preliminaries,[0],[0]
", πm) denote the RUM location family where the shapes of utility distributions are π1, . . .",2. Preliminaries,[0],[0]
", πm.",2. Preliminaries,[0],[0]
"For any probability distribution π whose mean is 0, let RUM(π) denote the RUM where the shapes of all utility distributions are π.
",2. Preliminaries,[0],[0]
"Given a profile P and a parameter ~θ, we have PrM(P |~θ) = ∏n j=1 PrM(Rj |~θ).",2. Preliminaries,[0],[0]
"Because all utilities are drawn independently, the probability of pairwise comparison is PrM(ai1 ai2 |~θ) =∫∞ −∞ ∫∞",2. Preliminaries,[0],[0]
"ui2 µi1(ui1 |~θ)µi2(ui2 |~θ)dui1dui2 .
",2. Preliminaries,[0],[0]
Example 1 (Plackett-Luce model as an RUM) Let µi(·|θi) be the Gumbel distribution where µi(xi|θi) =,2. Preliminaries,[0],[0]
"e−(xi−θi)−e
−(xi−θi) .",2. Preliminaries,[0],[0]
For any ranking R = ai1 ai2 . . .,2. Preliminaries,[0],[0]
"aim , we have PrPL(R|~θ) = ∏m−1 t=1",2. Preliminaries,[0],[0]
"e θit∑m
l=t e θil
.",2. Preliminaries,[0],[0]
"The probability
of ai1 ai2 under the Plackett-Luce model is PrPL(ai1 ai2 |~θ) =",2. Preliminaries,[0],[0]
"e θi1
e θi1 +e θi2 .
",2. Preliminaries,[0],[0]
"A weighted (rank-)breaking G = {gii′ : i < i′ ≤ m} can be represented by a weighted undirected graph over positions {1, . . .",2. Preliminaries,[0],[0]
",m}, such that for any gii′ > 0, there is an edge between i and i′ whose weight is gii′ .",2. Preliminaries,[0],[0]
"We say that G is uniform, if all weights are the same.",2. Preliminaries,[0],[0]
Let Gu denote the the uniform breaking where all weights are 1.,2. Preliminaries,[0],[0]
"For any 1 ≤ k ≤ m− 1, the position-k breaking is the graph where for any l > k, there is an edge with weight 1 between k and l. For any ~θ ∈ Rm−1, any weighted rank-breaking G, any pair of alternatives ai1 , ai2 , let Gai1 ai2 (R) = gii′ such that ai1 and ai2 are ranked at the ith position and the i
′th position in R, respectively.",2. Preliminaries,[0],[0]
"Given a profile P , we define
κi1i2 = ∑n j=1 Gai1 ai2 (Rj)
n , and let κ̄i1i2 = E[κi1i2 |~θ].",2. Preliminaries,[0],[0]
We note that κi1i2 is a function of the preference profile.,2. Preliminaries,[0],[0]
"κ̄i1i2 is the expected κi1i2 value for perfect data given ~θ, which means that it is a function of the ground truth parameter ~θ.
",2. Preliminaries,[0],[0]
Example 2,2. Preliminaries,[0],[0]
"Let m = 3, n = 2.",2. Preliminaries,[0],[0]
"The profile P = {a1 a2 a3, a3 a2 a1}.",2. Preliminaries,[0],[0]
"Let G = {g12 = g13 = 13 , g23 = 1 2} as shown in Figure 1 (a).",2. Preliminaries,[0],[0]
"Then we have κ12 = κ13 = 1 3/n = 1 6 , κ23 = 1 2/n = 1 4 , κ32 = κ31 = 1 3/n = 1 6 , κ21 = 1 2/n = 1 4 .
",2. Preliminaries,[0],[0]
"Position 1
Position 2
Position 3
 
 
 
1 1
2 2
a1 a2
a3
Position 1
Position 2
Position 3
 
 
 
1 1
2 2
a1 a2
a3
(a) G. (b)W .
",2. Preliminaries,[0],[0]
Figure 1.,2. Preliminaries,[0],[0]
A rank-breaking G and a CML-weight vectorW .,2. Preliminaries,[0],[0]
"LetW = {wii′ : ai,",3. Composite Marginal Likelihood Methods,[0],[0]
ai′ ∈ A} denote a CML-weight vector.,3. Composite Marginal Likelihood Methods,[0],[0]
"We say thatW is symmetric, if for any pair of alternatives ai, ai′ , we have wii′ = wi′i > 0.",3. Composite Marginal Likelihood Methods,[0],[0]
"We say thatW is uniform, if all wii′ ’s are equal.",3. Composite Marginal Likelihood Methods,[0],[0]
"LetWu denote a uniformW .
",3. Composite Marginal Likelihood Methods,[0],[0]
We note that vertices inW corresponds to the alternatives while vertices in G corresponds to positions in a ranking.,3. Composite Marginal Likelihood Methods,[0],[0]
"For example, vertex i inW corresponds to ai, while vertex i in G corresponds to the ith position in a ranking.
",3. Composite Marginal Likelihood Methods,[0],[0]
Example 3,3. Composite Marginal Likelihood Methods,[0],[0]
"A symmetricW is shown in Figure 1 (b), where w12 = w21 = 1 and w23 = w32 = 2.
",3. Composite Marginal Likelihood Methods,[0],[0]
"Given G andW , we propose the rank-breaking-then-CML framework for RUMs, denoted by RBCML(G,W), to be the maximizer of composite log-marginal likelihood, which is defined below.
",3. Composite Marginal Likelihood Methods,[0],[0]
Definition 2 (Composite marginal likelihood for RUMs),3. Composite Marginal Likelihood Methods,[0],[0]
"Given an RUMM, for any preference profile P and any θ, let pi1i2(~θ)",3. Composite Marginal Likelihood Methods,[0],[0]
= PrM(ai1 ai2 |~θ).,3. Composite Marginal Likelihood Methods,[0],[0]
"The composite marginal likelihood is CLM(~θ, P ) = ∏ i1 6=i2(pi1i2(
~θ))κi1i2wi1i2 .",3. Composite Marginal Likelihood Methods,[0],[0]
"The composite log-marginal likelihood becomes:
CLLM(~θ, P ) = ∑ i1 6=i2 κi1i2wi1i2 ln pi1i2( ~θ) (1)
We let RBCML(G,W)(P )",3. Composite Marginal Likelihood Methods,[0],[0]
"= arg max~θ CLLM(~θ, P ).",3. Composite Marginal Likelihood Methods,[0],[0]
"For the Plackett-Luce model the composite (log-)marginal likelihood has a closed-form formula.
",3. Composite Marginal Likelihood Methods,[0],[0]
Definition 3 (CML for Plackett-Luce),3. Composite Marginal Likelihood Methods,[0],[0]
"For any ~θ and preference profile P , the composite marginal likelihood for the Plackett-Luce model is CLPL(~θ, P ) =∏ i1<i2 ( e θi1
e θi1 +e θi2 )",3. Composite Marginal Likelihood Methods,[0],[0]
"κi1i2wi1i2 ( e
θi2 e θi1 +e θi2 )",3. Composite Marginal Likelihood Methods,[0],[0]
κi2i1wi2i1 .,3. Composite Marginal Likelihood Methods,[0],[0]
"The
composite log-marginal likelihood is
CLLPL(~θ, P ) =",3. Composite Marginal Likelihood Methods,[0],[0]
"∑ i1<i2 (κi1i2wi1i2θi1 + κi2i1wi2i1θi2
− (κi1i2wi1i2 + κi2i1wi2i1) ln(eθi1 + eθi2 ))",3. Composite Marginal Likelihood Methods,[0],[0]
"(2)
The first order conditions are, for all i, ∂CLLPL( ~θ,P )",3. Composite Marginal Likelihood Methods,[0],[0]
"∂θi =∑
i′ 6=i(κii′wii′ − (κii′wii′ + κi′iwi′i)",3. Composite Marginal Likelihood Methods,[0],[0]
"eθi eθi+eθi′ ).
",3. Composite Marginal Likelihood Methods,[0],[0]
"Example 4 Continuing Example 2 and Example 3,
CLLPL(~θ, P )",3. Composite Marginal Likelihood Methods,[0],[0]
"= 1
6 θ1 +
1 4 θ2 − ( 1 6 + 1 4 ) ln(eθ1",3. Composite Marginal Likelihood Methods,[0],[0]
"+ eθ2)
+ 1
2 θ2 − (
1 2 + 1 3 ) ln(eθ2",3. Composite Marginal Likelihood Methods,[0],[0]
"+ 1)
",3. Composite Marginal Likelihood Methods,[0],[0]
"By solving the first order conditions, we have eθ1 = 1 and eθ2 = 1.5.",3. Composite Marginal Likelihood Methods,[0],[0]
"So the outcome of RBCML is θ1 = 0, θ2 = ln 1.5.",3. Composite Marginal Likelihood Methods,[0],[0]
We recall that θ3 = 0 in this paper.,3. Composite Marginal Likelihood Methods,[0],[0]
Definition 4 (Log-concavity and strict log-concavity),4. Preservation of Strict Log-Concavity,[0],[0]
A function f(~x) > 0 is log-concave if ∀0 < λ,4. Preservation of Strict Log-Concavity,[0],[0]
"< 1, we have f(λ~x +",4. Preservation of Strict Log-Concavity,[0],[0]
(,4. Preservation of Strict Log-Concavity,[0],[0]
1 − λ)~y) ≥ f(~x)λf(~y)1−λ.,4. Preservation of Strict Log-Concavity,[0],[0]
"If the inequality is always strict, then f is strictly log-concave.
",4. Preservation of Strict Log-Concavity,[0],[0]
"Theorem 1 (Preservation under convolution) Let f(x) and g(x) be two continuous and strictly log-concave functions on R. Then f ∗ g is also strictly log-concave.
",4. Preservation of Strict Log-Concavity,[0],[0]
Proof: The proof is done by examining the equality condition for the Prékopa-Leindler inequality.,4. Preservation of Strict Log-Concavity,[0],[0]
"Let h = f ∗ g, namely, for any y ∈ R, h(y) = ∫ R f(y",4. Preservation of Strict Log-Concavity,[0],[0]
− x)g(x)dx.,4. Preservation of Strict Log-Concavity,[0],[0]
"Because f and g are continuous, so does h. To prove the strict log-concavity of h, it suffices to prove that for any different y1, y2 ∈ R, h(y1+y22 )",4. Preservation of Strict Log-Concavity,[0],[0]
>,4. Preservation of Strict Log-Concavity,[0],[0]
"√ h(y1)h(y2).
",4. Preservation of Strict Log-Concavity,[0],[0]
Suppose for the sake of contradiction that this is not true.,4. Preservation of Strict Log-Concavity,[0],[0]
"Since log-concavity preserves under convolution (Saumard & Wellner, 2014), h is log-concave.",4. Preservation of Strict Log-Concavity,[0],[0]
"So, there exist y1 < y2 such that h(y1+y22 )",4. Preservation of Strict Log-Concavity,[0],[0]
=,4. Preservation of Strict Log-Concavity,[0],[0]
√ h(y1)h(y2).,4. Preservation of Strict Log-Concavity,[0],[0]
"Let Λ(x, y) = f(y",4. Preservation of Strict Log-Concavity,[0],[0]
− x)g(x).,4. Preservation of Strict Log-Concavity,[0],[0]
"We further define
H(x) = Λ(x, y1 + y2
2 ) =",4. Preservation of Strict Log-Concavity,[0],[0]
f( y1 + y2 2,4. Preservation of Strict Log-Concavity,[0],[0]
"− x)g(x)
F (x) = Λ(x, y1) = f(y1",4. Preservation of Strict Log-Concavity,[0],[0]
− x)g(x) G(x),4. Preservation of Strict Log-Concavity,[0],[0]
"= Λ(x, y2) =",4. Preservation of Strict Log-Concavity,[0],[0]
"f(y2 − x)g(x)
",4. Preservation of Strict Log-Concavity,[0],[0]
"Because (non-strict) log-concavity is preserved under convolution, Λ(x, y) is log-concave.",4. Preservation of Strict Log-Concavity,[0],[0]
"We have that for any x ∈ R, H(x) ≥ √ F (x)G(x).",4. Preservation of Strict Log-Concavity,[0],[0]
"The Prékopa-Leindler inequality asserts that∫ R H(x)dx ≥ √∫ R F (x)dx ∫ R G(x)dx (3) Because h(y1+y22 ) = ∫ RH(x)dx, h(y1) = ∫ R F (x)dx,
h(y2) = ∫",4. Preservation of Strict Log-Concavity,[0],[0]
"RG(x)dx, and h( y1+y2 2 ) =",4. Preservation of Strict Log-Concavity,[0],[0]
"√ h(y1)h(y2), (3) becomes an equation.",4. Preservation of Strict Log-Concavity,[0],[0]
It was proved by Dubuc (1977) that: there exist a > 0 and b ∈ R such that the following conditions hold almost everywhere for x ∈ R (see the translation of Dubuc’s result in English by Ball & Böröczky (2010)).,4. Preservation of Strict Log-Concavity,[0],[0]
1.,4. Preservation of Strict Log-Concavity,[0],[0]
"F (x) = aH(x+ b), 2. G(x)",4. Preservation of Strict Log-Concavity,[0],[0]
"= a−1H(x− b).
",4. Preservation of Strict Log-Concavity,[0],[0]
"The first condition means that for almost every x ∈ R,
f(y1 − x)g(x) = af( y1 + y2
2 − x−",4. Preservation of Strict Log-Concavity,[0],[0]
"b)g(x+ b)
⇐⇒ g(x) g(x+",4. Preservation of Strict Log-Concavity,[0],[0]
"b)
",4. Preservation of Strict Log-Concavity,[0],[0]
"= a f(y1+y22 − x− b)
f(y1 − x) (4)
",4. Preservation of Strict Log-Concavity,[0],[0]
"The second condition means that for almost all x ∈ R, f(y2 − x)g(x) = a−1f(y1+y22",4. Preservation of Strict Log-Concavity,[0],[0]
− x + b)g(x − b) ⇐⇒ g(x−b) g(x) = a f(y2−x) f,4. Preservation of Strict Log-Concavity,[0],[0]
( y1+y2 2 −x+b) .,4. Preservation of Strict Log-Concavity,[0],[0]
"Therefore, for almost all x ∈ R,
g(x)
g(x+ b) = a f(y2 − x− b) f(y1+y22",4. Preservation of Strict Log-Concavity,[0],[0]
"− x)
(5)
Combining (4) and (5), for almost every x ∈ R we have
g(x)
g(x+ b) = a f(y2 − x− b) f(y1+y22",4. Preservation of Strict Log-Concavity,[0],[0]
− x) = a f(y1+y22 − x− b) f(y1,4. Preservation of Strict Log-Concavity,[0],[0]
"− x) (6)
",4. Preservation of Strict Log-Concavity,[0],[0]
"Because f(x) is strictly log-concave, for any fixed c 6= 0, f(x+c) f(x) is strictly monotonic.",4. Preservation of Strict Log-Concavity,[0],[0]
Because y1 6= y2 and y2−x− b− (y1+y22 − x) = y1+y2 2,4. Preservation of Strict Log-Concavity,[0],[0]
− x− b− (y1− x) = y2−y1 2,4. Preservation of Strict Log-Concavity,[0],[0]
"− b, we must have that y2−y12",4. Preservation of Strict Log-Concavity,[0],[0]
"− b = 0, namely b = y2−y1
2 .",4. Preservation of Strict Log-Concavity,[0],[0]
"Therefore, (6) becomes g(x)
g(x+ y2−y1",4. Preservation of Strict Log-Concavity,[0],[0]
2 ) =,4. Preservation of Strict Log-Concavity,[0],[0]
"a for almost every
x ∈ R, which contradicts the strict log-concavity of g.",4. Preservation of Strict Log-Concavity,[0],[0]
This means that h = f ∗,4. Preservation of Strict Log-Concavity,[0],[0]
"g is strictly log-concave.
",4. Preservation of Strict Log-Concavity,[0],[0]
"Theorem 2 (Preservation under marginalization) Let h(x, y) be a strictly log-concave function on R2.",4. Preservation of Strict Log-Concavity,[0],[0]
"Then∫ R h(x, y)dx is strictly log-concave on R.
Again, the proof is done by examining the equality condition for the Prékopa-Leindler inequality.",4. Preservation of Strict Log-Concavity,[0],[0]
All missing proofs can be found in the supplementary material.,4. Preservation of Strict Log-Concavity,[0],[0]
"For any profile P , let G(P ) denote the weighted directed graph where each represents an alternative.",5. Strict Log-Concavity of CML,[0],[0]
"For any 1 ≤ i 6= i′ ≤ m, the weight on the edge from i to i′ is κii′ .",5. Strict Log-Concavity of CML,[0],[0]
"A weighted directed graph is (weakly) connected, if after removing the directions on all edges, the resulting undirected graph is connected.",5. Strict Log-Concavity of CML,[0],[0]
"A weighted directed graph is strongly connected, if there is a directed path with positive weights between any pair of vertices.",5. Strict Log-Concavity of CML,[0],[0]
"Given any pair of weighted graphs G1 and G2, we let G1 ⊗ G2 denote the weighted graph where the weights on each edge is the multiplication of the weights of same edge in G1 and G2.
Theorem 3",5. Strict Log-Concavity of CML,[0],[0]
"Given any profile P , the composite likelihood function for Plackett-Luce, i.e. CLPL(~θ, P ), is strictly logconcave if and only if W ⊗ G(P ) is weakly connected.",5. Strict Log-Concavity of CML,[0],[0]
"arg max~θ CLPL(
~θ, P ) is bounded if and only ifW ⊗G(P ) is strongly connected.
",5. Strict Log-Concavity of CML,[0],[0]
"The proof is similar to the log-concavity of likelihood for BTL by (Hunter, 2004).",5. Strict Log-Concavity of CML,[0],[0]
"For general RUMs we prove a similar theorem.
",5. Strict Log-Concavity of CML,[0],[0]
Theorem 4 Let M be an RUM where the CDF of each utility distribution is strictly log-concave.,5. Strict Log-Concavity of CML,[0],[0]
"Given any profile P , the composite likelihood function forM, i.e. CLM(~θ, P ), is strictly log-concave if and only ifW ⊗G(P ) is weakly connected.",5. Strict Log-Concavity of CML,[0],[0]
"arg max~θ CLM(
~θ, P ) is bounded if and only if W ⊗G(P ) is strongly connected.
",5. Strict Log-Concavity of CML,[0],[0]
"Proof sketch: It is not hard to check that whenW ⊗G(P ) is not connected, there exist ~θ(1) and ~θ(2) such that for any 0 <",5. Strict Log-Concavity of CML,[0],[0]
"λ < 1 we have CLLPL(~θ(1), P ) = CLLPL(~θ(2), P ) = λCLLPL(~θ(1), P ) +",5. Strict Log-Concavity of CML,[0],[0]
"(1−λ)CLLPL(~θ(2), P ), which violates strict log-concavity.",5. Strict Log-Concavity of CML,[0],[0]
"Suppose W ⊗ G(P ) is weakly connected, it suffices to prove for any i1 6= i2, Pr(ai1 ai2 |~θ) is strictly log-concave.",5. Strict Log-Concavity of CML,[0],[0]
We can write this as an integral over ui2 − ui1 : Pr(ui1,5. Strict Log-Concavity of CML,[0],[0]
> ui2 |~θ) =,5. Strict Log-Concavity of CML,[0],[0]
∫∞ 0,5. Strict Log-Concavity of CML,[0],[0]
Pr(ui2,5. Strict Log-Concavity of CML,[0],[0]
"− ui1 = s|~θ)ds.
",5. Strict Log-Concavity of CML,[0],[0]
"Let π∗i2(·|~θ) denote the flipped distribution of πi2(·|~θ) around x = s, then we have π∗i2(s − x|~θ)",5. Strict Log-Concavity of CML,[0],[0]
= πi2(s + x|~θ).,5. Strict Log-Concavity of CML,[0],[0]
"Further we have Pr(ui1 > ui2 |~θ) =∫∞
0 ∫∞ −∞ πi1(x|θi1)πi2(x+ s|θi2)dxds = ∫∞ 0 πi1 ∗ π∗i2ds.",5. Strict Log-Concavity of CML,[0],[0]
"By Theorem 1, πi1 ∗ π∗i2 is strictly log-concave.",5. Strict Log-Concavity of CML,[0],[0]
"Then we prove that tail probability of a strictly log-concave distribution is also strictly log-concave.
",5. Strict Log-Concavity of CML,[0],[0]
The proof for boundedness is similar to the proof of a similar condition for BTL by Hunter (2004).,5. Strict Log-Concavity of CML,[0],[0]
"Given any RUM M and any parameter ~θ, we define ELLM(~θ)",6. Asymptotic Properties of RBCML,[0],[0]
"= E[CLLM(~θ,R)] and let ∇ELLM(~θ) be the gradient of ELLM(~θ), whose ith element is∇iELLM(~θ) =∑ i′ 6=i( κ̄ii′wii′
pii′ ( ~θ)
∂pii′ ( ~θ)
∂θi + κ̄i′iwi′i
pi′i( ~θ)
∂pi′i( ~θ)
∂θi ).",6. Asymptotic Properties of RBCML,[0],[0]
"Let H(~θ, P ) be
the Hessian matrix evaluated at ~θ.",6. Asymptotic Properties of RBCML,[0],[0]
"And let H0(~θ0) denote the expected Hessian of CLLM(~θ, P ) at ~θ0, where ~θ0 is the ground truth parameter.
",6. Asymptotic Properties of RBCML,[0],[0]
Theorem 5 (Consistency and asymptotic normality),6. Asymptotic Properties of RBCML,[0],[0]
"Given any RUM M, any ~θ0 and any profile P with n rankings.",6. Asymptotic Properties of RBCML,[0],[0]
"Let ~θ∗ be the output of RBCML(G,W).",6. Asymptotic Properties of RBCML,[0],[0]
"When n→∞, we have ~θ∗ p−→ ~θ0 and √ n(~θ∗",6. Asymptotic Properties of RBCML,[0],[0]
"− ~θ0) d−→ N(0, H−10 (~θ0)Var[∇CLLM(~θ0, R)]H −1 0 ( ~θ0))",6. Asymptotic Properties of RBCML,[0],[0]
"if and only if ~θ0 is the only solution to
∇ELLM(~θ) = ~0, (7)
Proof: The “only if"" direction is straightforward.",6. Asymptotic Properties of RBCML,[0],[0]
"The solution to (7) is unique because CLLM(~θ, P ) is strictly concave.",6. Asymptotic Properties of RBCML,[0],[0]
"Suppose ~θ1, other than ~θ0, is the solution to (7), then
when n → ∞, ~θ1 will be the estimate of RBCML(G,W), which means RBCML(G,W) is not consistent.
",6. Asymptotic Properties of RBCML,[0],[0]
"Now we prove the “if"" direction.",6. Asymptotic Properties of RBCML,[0],[0]
First we prove consistency.,6. Asymptotic Properties of RBCML,[0],[0]
"It is required by Xu & Reid (2011) that for different parameters, the probabilities for any composite likelihood event are different, which is not true in our case.",6. Asymptotic Properties of RBCML,[0],[0]
"A simple counterexample is θ(1)1 = 1, θ (2) 1 = 2, θ (1) 2 = θ (1) 3 = θ (2) 2 = θ (2) 3 = 0.",6. Asymptotic Properties of RBCML,[0],[0]
Then Pr(a2 a3|~θ(1)),6. Asymptotic Properties of RBCML,[0],[0]
"= Pr(a2 a3|~θ(2)).
",6. Asymptotic Properties of RBCML,[0],[0]
"By the law of large numbers, we have for any , Pr(|CLLM(~θ, P )",6. Asymptotic Properties of RBCML,[0],[0]
− ELLM(~θ)| ≤ /2),6. Asymptotic Properties of RBCML,[0],[0]
→ 1 as n → ∞.,6. Asymptotic Properties of RBCML,[0],[0]
"This implies limn→∞ Pr(CLLM(~θ∗, P ) ≤ ELLM(~θ∗) + /2)",6. Asymptotic Properties of RBCML,[0],[0]
= 1.,6. Asymptotic Properties of RBCML,[0],[0]
Similarly we have limn→∞ Pr(ELLM(~θ0) ≤,6. Asymptotic Properties of RBCML,[0],[0]
"CLLM(~θ0, P ) + /2)",6. Asymptotic Properties of RBCML,[0],[0]
= 1.,6. Asymptotic Properties of RBCML,[0],[0]
"Since ~θ∗ maximize CLLM(~θ, P ), we have Pr(CLLM(~θ0, P ) ≤",6. Asymptotic Properties of RBCML,[0],[0]
"CLLM(~θ∗, P ))",6. Asymptotic Properties of RBCML,[0],[0]
= 1.,6. Asymptotic Properties of RBCML,[0],[0]
The above three equations imply that limn→∞ Pr(ELLM(~θ0)− ELLM(~θ∗) ≤ ),6. Asymptotic Properties of RBCML,[0],[0]
"= 1.
",6. Asymptotic Properties of RBCML,[0],[0]
Let Θ be the subset of parameter space s.t.,6. Asymptotic Properties of RBCML,[0],[0]
"∀~θ ∈ Θ , ELLM(~θ0)− ELLM(~θ) ≤ .",6. Asymptotic Properties of RBCML,[0],[0]
"Because ELLM(~θ) is strictly concave, Θ is compact and has a unique maximum at ~θ0.",6. Asymptotic Properties of RBCML,[0],[0]
"Thus for any > 0, limn→∞ Pr(~θ∗ ∈ Θ ) = 1.",6. Asymptotic Properties of RBCML,[0],[0]
"This implies consistency, i.e., ~θ∗ p−→ ~θ0.
",6. Asymptotic Properties of RBCML,[0],[0]
Now we prove asymptotic normality.,6. Asymptotic Properties of RBCML,[0],[0]
"By mean value theorem, we have 0 = ∇CLLM(~θ∗, P ) = ∇CLLM(~θ0, P ) + H(α~θ∗ + (1 − α)~θ0, P )(~θ∗ − ~θ0), where 0 ≤ α ≤ 1.",6. Asymptotic Properties of RBCML,[0],[0]
"Therefore, we have √ n(~θ∗",6. Asymptotic Properties of RBCML,[0],[0]
"− ~θ) = −H−1(α~θ∗ + (1 − α)~θ0, P )( √ n∇CLLM(~θ0, P )).",6. Asymptotic Properties of RBCML,[0],[0]
"Since ∇CLLM(~θ0, P ) = 1 n ∑n j=1∇CLLM(~θ0, Rj), by the central limit theorem, we have √ n∇CLLM(~θ0, P )",6. Asymptotic Properties of RBCML,[0],[0]
"d−→ N(0,Var[∇CLLM(~θ0, R)])
",6. Asymptotic Properties of RBCML,[0],[0]
"Because ~θ∗ p−→ ~θ0 and H is continuous, we have H(α~θ∗ + (1 − α)~θ0, P ) p−→ H(~θ0, P ).",6. Asymptotic Properties of RBCML,[0],[0]
"Since H(~θ, P ) = 1 n ∑n j=1H",6. Asymptotic Properties of RBCML,[0],[0]
"( ~θ,Rj), by law of large numbers, we have H(~θ, P ) p−→ H0(~θ0).",6. Asymptotic Properties of RBCML,[0],[0]
"Therefore, we have
√ n(~θ∗",6. Asymptotic Properties of RBCML,[0],[0]
"− ~θ) = −H−10 (~θ0)( √ n∇CLLM(~θ0, P )),
which implies that Var[ √ n(~θ∗",6. Asymptotic Properties of RBCML,[0],[0]
"− ~θ)] = H−10 ( ~θ0)Var[∇CLLM(~θ0, R)]H−10 (~θ0).",6. Asymptotic Properties of RBCML,[0],[0]
"Formal proofs of theorems in this section depends on a series of lemmas, which can be found in the appendix.",7. Consistency of RBCML,[0],[0]
"The full proofs can also be found in the appendix.
",7. Consistency of RBCML,[0],[0]
"Theorem 6 RBCML(G,Wu) is consistent for PlackettLuce if and only if the breaking is weighted union of positionk breakings.
",7. Consistency of RBCML,[0],[0]
Proof sketch:,7. Consistency of RBCML,[0],[0]
"The “if"" direction is proved in (Khetan & Oh,
2016b).",7. Consistency of RBCML,[0],[0]
"We only prove the “only if"" direction by induction on m. When m = 2, the only breaking is the comparison between the two alternatives.",7. Consistency of RBCML,[0],[0]
"The conclusion holds.
",7. Consistency of RBCML,[0],[0]
"Suppose it holds for m = l, then when m = l + 1, we first prove a lemma which says that by restricting G to any set of continuous positions, the theorem must hold for the subgraph.",7. Consistency of RBCML,[0],[0]
"Then, we focus on G[2,m], which is the subgraph of G on {2,. . .",7. Consistency of RBCML,[0],[0]
",m}.",7. Consistency of RBCML,[0],[0]
"G[2,m] must be a weighted union of position-k breakings.",7. Consistency of RBCML,[0],[0]
"Then we focus on G[1,m−1].",7. Consistency of RBCML,[0],[0]
"The only remaining case is to prove that the weight on edge {1,m} is the same as the weight on edges {1, i} for all i ≤ m− 1.
",7. Consistency of RBCML,[0],[0]
"Suppose for the sake of contradiction this is not true, then we can subtract a weighted union of position-k breakings from the graph, so that the remaining graph has a single edge {1,m}.",7. Consistency of RBCML,[0],[0]
"We then prove that such an single-edge breaking is inconsistent by proving that (7) is not satisfied, which leads to a contradiction.
",7. Consistency of RBCML,[0],[0]
"Theorem 7 Let π1, π2, . . .",7. Consistency of RBCML,[0],[0]
", πm denote the utility distributions for a symmetric RUM.",7. Consistency of RBCML,[0],[0]
"Suppose there exists πi s.t. (1) (lnπi(x))′ is monotonically decreasing, and (2) limx→−∞(lnπi(x))
′ →∞.",7. Consistency of RBCML,[0],[0]
"Then, RBCML(G,Wu) is consistent if and only if G is uniform.
",7. Consistency of RBCML,[0],[0]
Proof sketch: Define the single-edge breaking G1 = {g1m = 1}.,7. Consistency of RBCML,[0],[0]
"We first prove RBCML(G1,Wu) is not consistent.",7. Consistency of RBCML,[0],[0]
Then we prove the theorem by induction on m. m = 2 is trivial because the only breaking is uniform.,7. Consistency of RBCML,[0],[0]
"For m = 3, we first prove that the single-edge breaking G1 = {g13 = 1} is not consistent.",7. Consistency of RBCML,[0],[0]
"Suppose the breaking is G = {g12 = x, g23 = y, g13 = z}.",7. Consistency of RBCML,[0],[0]
"Let G∗ = {g12 = y, g23 = x, g13 = z}.",7. Consistency of RBCML,[0],[0]
"We prove that RBCML(G∗,Wu) is consistent forM∗, which is the RUM obtained fromM by flipping the shapes of the utility distributions.",7. Consistency of RBCML,[0],[0]
"BecauseM is symmetric, we haveM∗ = M.",7. Consistency of RBCML,[0],[0]
"Then we prove that RBCML(G + G∗,Wu) is consistent.",7. Consistency of RBCML,[0],[0]
"If x+ y < 2z, We subtract (x+ y)Gu from G + G∗",7. Consistency of RBCML,[0],[0]
"and get a consistent breaking (2z − (x+ y))G1, which is a contradiction.",7. Consistency of RBCML,[0],[0]
"For the case where x+ y = 2z we use the premise in the theorem statement to directly prove that the breaking is inconsistent.
",7. Consistency of RBCML,[0],[0]
Suppose the theorem holds for m = k.,7. Consistency of RBCML,[0],[0]
"When m = k + 1, W.l.o.g.",7. Consistency of RBCML,[0],[0]
"we let π2 satisfy the conditions that (lnπi(x))′ is monotonically decreasing and limx→−∞(lnπi(x))′ →∞. Let θ1 = L, θm = −L, and θ2 = . . .",7. Consistency of RBCML,[0],[0]
= θm−1 = 0.,7. Consistency of RBCML,[0],[0]
"So when L→∞, with probability goes to 1, a1 is ranked at the top and am is ranked at the bottom.",7. Consistency of RBCML,[0],[0]
"We then focus on G[2,m] and G[1,m−1].",7. Consistency of RBCML,[0],[0]
"By induction hypothesis, G[2,m] (respectively, G[1,m−1]) is either uniform or empty.",7. Consistency of RBCML,[0],[0]
"If G[2,m] is empty, then G[1,m−1] is also empty.",7. Consistency of RBCML,[0],[0]
"Because G is nonempty, we must have G = CG1, where C > 0.",7. Consistency of RBCML,[0],[0]
This is a contradiction.,7. Consistency of RBCML,[0],[0]
"If G[2,m] is uniform but G is not uniform, then the single edge breaking G1 must be consistent, which is a contradiction.
",7. Consistency of RBCML,[0],[0]
"Corollary 1 Theorem 7 holds for any RUM with symmetric distributions where any single distribution is Gaussian.
",7. Consistency of RBCML,[0],[0]
"The following two theorems give stronger characterizations by leveraging Theorems 6 and 7.
",7. Consistency of RBCML,[0],[0]
"Theorem 8 RBCML(G,W) for Plackett-Luce is consistent if and only if G is the weighted union of position-k breakings andW is connected and symmetric.
",7. Consistency of RBCML,[0],[0]
Theorem 9 Let π be any symmetric distribution that satisfies the condition in Theorem 7.,7. Consistency of RBCML,[0],[0]
"Then RBCML(G,W) is consistent for RUM(π) if and only if G is uniform andW is connected and symmetric.
",7. Consistency of RBCML,[0],[0]
The proofs for Theorems 8 and 9 are similar.,7. Consistency of RBCML,[0],[0]
"The “if"" direction can be proved by verifying that the ground truth parameter is the solution to (7).",7. Consistency of RBCML,[0],[0]
"For the “only if"" direction, we first prove that consistency of RBCML(G,W) implies consistency of RBCML(G,Wu), which further implies G is the weighted union of position-k breakings for PLs (Theorem 6) or uniform breaking for RUMs (Theorem 7).",7. Consistency of RBCML,[0],[0]
"Given this condition on G, we prove that W must be connected and symmetric.",7. Consistency of RBCML,[0],[0]
The asymptotic covariance of RBCML depends on G and W .,8. The RBCML Framework,[0],[0]
"The optimal G and W depend on the ground truth parameter ~θ02, which is exactly what we want.",8. The RBCML Framework,[0],[0]
"To tackle this problem, we propose the adaptive RBCML framework, guided by our Theorems 8 and 9 and shown as Algorithm
2Khetan & Oh (2016b) proposed a breaking G, which is not a function of ~θ0.
1.",8. The RBCML Framework,[0],[0]
"In this algorithm, G andW are iteratively updated given the estimate of ~θ from the previous iteration.",8. The RBCML Framework,[0],[0]
"Algorithm 1 Adaptive RBCML Input: Profile P of n rankings, the number of iterations T , the heuristics of breaking G(~θ) and the weightsW(~θ).",8. The RBCML Framework,[0],[0]
"Output: Estimated parameter ~θ∗. Initialize ~θ(0) = ~0
1: for t = 1 to T do 2: Compute G(~θ(t−1))",8. The RBCML Framework,[0],[0]
andW(~θ(t−1)).,8. The RBCML Framework,[0],[0]
"3: Estimate ~θ(t) using G(~θ(t−1)) and W(~θ(t−1)) by maximizing (1) (or (2) for Plackett-Luce) 4: end for
No efficient way of computing the optimal G(~θ) andW(~θ) is known since the asymptotic covariance is generally hard to compute, where an expectation is taken over m! rankings.",8. The RBCML Framework,[0],[0]
How to efficiently compute the optimal G andW is a promising future direction.,8. The RBCML Framework,[0],[0]
"In the experiments of this paper, we use Gu andWu for Gaussian RUMs since Gu is the only consistent breaking.",8. The RBCML Framework,[0],[0]
"For the Plackett-Luce model, we use the G proposed by Khetan & Oh (2016b) and a heuristic W(~θ) (See Section 9).",8. The RBCML Framework,[0],[0]
We compare RBCML with state-of-the-art algorithms for both Gaussian RUMs (GMM algorithm by Azari Soufiani et al. (2014)) and the Plackett-Luce model (the I-LSR algorithm by Maystre & Grossglauser (2015) and the consistent rank-breaking algorithm by Khetan & Oh (2016b)).,9. Experiments,[0],[0]
"In both experiments, we generate synthetic datasets of full rankings over m = 10 alternatives.",9. Experiments,[0],[0]
"The ground truth parameter is generated uniformly at random between 0 and 5 and shifted
s.t. θ10 = 0.",9. Experiments,[0],[0]
"For Gaussian RUMs, the utility distribution of ai is N(θi, 1).",9. Experiments,[0],[0]
"The results are averaged over 50000 trials.
",9. Experiments,[0],[0]
Metrics.,9. Experiments,[0],[0]
"We measure statistical efficiency by n × MSE, where n is the number of rankings in the dataset.",9. Experiments,[0],[0]
"We use n×MSE rather than the standard MSE, because it is easier to see the difference between algorithms w.r.t.",9. Experiments,[0],[0]
the former.,9. Experiments,[0],[0]
"The reason is that n×MSE approaches a positive constant as n → ∞, due to asymptotic normality of RBCML.",9. Experiments,[0],[0]
"We use running time to measure computational efficiency of each algorithm.
",9. Experiments,[0],[0]
Gaussian RUMs.,9. Experiments,[0],[0]
"We use a one-step (T = 1 in Algorithm 1) RBCML(Gu,Wu) for Gaussian RUMs and the results are shown in Figure 3.",9. Experiments,[0],[0]
"We use uniform breaking rather than other breakings because it is the only consistent breaking according to our theoretical results.
",9. Experiments,[0],[0]
We observe that our RBCML outperforms the GMM algorithm by Azari Soufiani et al. (2014) w.r.t.,9. Experiments,[0],[0]
"both statistical efficiency and computational efficiency.
",9. Experiments,[0],[0]
The Plackett-Luce Model.,9. Experiments,[0],[0]
"We use a two-step (T = 2 in Algorithm 1) RBCML, where the first step is exactly the algorithm by Khetan & Oh (2016b) (denoted by K-O Breaking).",9. Experiments,[0],[0]
"In the second step, we still use the breaking by Khetan & Oh (2016b) but propose a heuristicW(~θ).",9. Experiments,[0],[0]
"For any pair of alternatives ai1 and ai2 , we let wi1i2 =",9. Experiments,[0],[0]
"wi2i1 = 1 |θi1−θi2 |+4
.",9. Experiments,[0],[0]
The intuition is that we should put a higher weight on the pair of alternatives that are closer to each other.,9. Experiments,[0],[0]
"Moreover, we use the output of the first step as the starting point of the second step optimization to improve computational efficiency.
",9. Experiments,[0],[0]
The results are shown in Figure 2.,9. Experiments,[0],[0]
We use 2-LSR to denote the two-iteration I-LSR algorithms by Maystre & Grossglauser (2015).,9. Experiments,[0],[0]
"LSR (one-iteration I-LSR) results are not
shown because of the high n×MSE and runtime for large n.",9. Experiments,[0],[0]
"The “CR bound"" line is n times the trace of Cramér-Rao bound (Cramér, 1946; Rao, 1945), which is the lower bound of the covariance matrix of any unbiased estimator.",9. Experiments,[0],[0]
"Because Cramér-Rao bound decreases at the rate of 1/n, the CR bound line is horizontal.",9. Experiments,[0],[0]
"Since RBCML is not necessarily unbiased, the Cramér-Rao bound is not a lower bound for RBCML.
",9. Experiments,[0],[0]
"We observe that on datasets with large numbers of rankings (“ "" means “is better than""): • Statistical efficiency: 2-LSR RBCML K-O Breaking.",9. Experiments,[0],[0]
"• Runtime: K-O Breaking RBCML 2-LSR.
",9. Experiments,[0],[0]
Beyond the experiments.,9. Experiments,[0],[0]
We have only shown the RBCML with simple G andW .,9. Experiments,[0],[0]
Other configurations of G andW can potentially have better performances or achieve other tradeoffs.,9. Experiments,[0],[0]
"Exploring RBCMLs for Gaussian RUMs, the Plackett-Luce model, as well as other RUMs is an interesting direction for future work.",9. Experiments,[0],[0]
We propose a flexible rank-breaking-then-compositemarginal-likelihood (RBCML) framework for learning RUMs.,10. Summary and Future Work,[0],[0]
"We characterize conditions for the objective function to be strictly log-concave, and for RBCML to be consistent and asymptotically normal.",10. Summary and Future Work,[0],[0]
"Experiments show that RBCML for Gaussian RUMs improve both statistical efficiency and computational efficiency, and the proposed RBCML for the Plackett-Luce model is competitive against state-of-the-art algorithms in that it provides a tradeoff between statistical efficiency and computational efficiency.",10. Summary and Future Work,[0],[0]
"For future work we plan to find efficient ways to compute optimal choices of G andW , and to extend the algorithm to partial orders.",10. Summary and Future Work,[0],[0]
We thank all anonymous reviewers for helpful comments and suggestions.,Acknowledgments,[0],[0]
This work is supported by NSF #1453542 and ONR #N00014-17-1-2621.,Acknowledgments,[0],[0]
"We propose a novel and flexible rank-breakingthen-composite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model.",abstractText,[0],[0]
We characterize conditions for the objective function of RBCML to be strictly log-concave by proving that strict log-concavity is preserved under convolution and marginalization.,abstractText,[0],[0]
We characterize necessary and sufficient conditions for RBCML to satisfy consistency and asymptotic normality.,abstractText,[0],[0]
Experiments on synthetic data show that RBCML for Gaussian RUMs achieves better statistical efficiency and computational efficiency than the state-of-the-art algorithm and our RBCML for the Plackett-Luce model provides flexible tradeoffs between running time and statistical efficiency.,abstractText,[0],[0]
Composite Marginal Likelihood Methods for Random Utility Models,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 185–196 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
185",text,[0],[0]
"Grammar, as per a common metaphor, gives speakers of a language a shared toolbox to construct and deconstruct meaningful and fluent utterances.",1 Introduction,[0],[0]
"Being highly analytic, English relies heavily on word order and closed-class function words like prepositions, determiners, and conjunctions.",1 Introduction,[0],[0]
"Though function words bear little semantic content, they are nevertheless crucial to the meaning.",1 Introduction,[0],[0]
"Consider prepositions: they serve, for example, to convey place and time (We met at/in/outside the restaurant for/after an hour), to express configurational relationships like quantity, possession, part/whole, and membership (the coats of dozens of children in the class), and to indicate semantic roles in argument structure (Grandma cooked dinner for the children
∗nathan.schneider@georgetown.edu
vs. Grandma cooked the children for dinner).",1 Introduction,[0],[0]
"Frequent prepositions like for are maddeningly polysemous, their interpretation depending especially on the object of the preposition—I rode the bus for 5 dollars/minutes—and the governor of the prepositional phrase (PP): I Ubered/asked for $5.",1 Introduction,[0],[0]
Possessives are similarly ambiguous: Whistler’s mother/painting/hat/death.,1 Introduction,[0],[0]
"Semantic interpretation requires some form of sense disambiguation, but arriving at a linguistic representation that is flexible enough to generalize across usages and types, yet simple enough to support reliable annotation, has been a daunting challenge (§2).
",1 Introduction,[0],[0]
This work represents a new attempt to strike that balance.,1 Introduction,[0],[0]
"Building on prior work, we argue for an approach to describing English preposition and possessive semantics with broad coverage.",1 Introduction,[0],[0]
"Given the semantic overlap between prepositions and possessives (the hood of the car vs. the car’s hood or its hood), we analyze them using the same inventory of semantic labels.1",1 Introduction,[0],[0]
"Our contributions include:
• a new hierarchical inventory (“SNACS”) of 50 supersense classes, extensively documented in guidelines for English (§3); • a gold-standard corpus with comprehensive annotations: all types and tokens of prepositions and possessives are disambiguated (§4; example sentences appear in figure 1); • an interannotator agreement study that
1Some uses of certain other closed-class markers— intransitive particles, subordinators, infinitive to—are also included (§3.1).
shows the scheme is reliable and generalizes across genres—and for the first time demonstrating empirically that the lexical semantics of a preposition can sometimes be detached from the PP’s semantic role (§5); • disambiguation experiments with two supervised classification architectures to establish the difficulty of the task (§6).",1 Introduction,[0],[0]
"Studies of preposition semantics in linguistics and cognitive science have generally focused on the domains of space and time (e.g., Herskovits, 1986; Bowerman and Choi, 2001; Regier, 1996; Khetarpal et al., 2009; Xu and Kemp, 2010; Zwarts and Winter, 2000) or on motivated polysemy structures that cover additional meanings beyond core spatial senses (Brugman, 1981; Lakoff, 1987; Tyler and Evans, 2003; Lindstromberg, 2010).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Possessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015).
",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general-
2Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012).
ize more easily to new types and usages.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"However, several limitations of our approach became clear to us over time.
",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"First, as pointed out by Hwang et al. (2017), the one-label-per-token assumption in STREUSLE is flawed because it in some cases puts into conflict the semantic role of the PP with respect to a predicate, and the lexical semantics of the preposition itself.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotation study or release a corpus to establish its feasibility empirically.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"We address that gap here.
",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Second, 75 categories is an unwieldy number for both annotators and disambiguation systems.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Some are quite specialized and extremely rare in STREUSLE 3.0, which causes data sparseness issues for supervised learning.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"In fact, the only published disambiguation system for preposition supersenses collapsed the distinctions to just 12 labels (Gonen and Goldberg, 2016).",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
Hwang et al. (2017) remarked that solving the aforementioned problem could remove the need for many of the specialized categories and make the taxonomy more tractable for annotators and systems.,2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"We substantiate this here, defining a new hierarchy with just 50 categories (SNACS, §3) and providing disambiguation results for the full set of distinctions.
",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Finally, given the semantic overlap of possessive case and the preposition of, we saw an opportunity to broaden the application of the scheme to include possessives.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Our reannotated corpus, STREUSLE 4.0, thus has supersense annotations for over 1000 possessive tokens that were not semantically annotated in version 3.0.",2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
We include these in our annotation and disambiguation experiments alongside reannotated preposition tokens.,2 Background: Disambiguation of Prepositions and Possessives,[0],[0]
"Apart from canonical prepositions and possessives, there are many lexically and semantically overlap-
3https://github.com/nert-gu/streusle/
ping closed-class items which are sometimes classified as other parts of speech, such as adverbs, particles, and subordinating conjunctions.",3.1 Lexical Categories of Interest,[0],[0]
"The Cambridge Grammar of the English Language (Huddleston and Pullum, 2002) argues for an expansive definition of ‘preposition’ that would encompass these other categories.",3.1 Lexical Categories of Interest,[0],[0]
"As a practical measure, we decided to encourage annotators to focus on the semantics of these functional items rather than their syntax, so we take an inclusive stance.
",3.1 Lexical Categories of Interest,[0],[0]
Another consideration is developing annotation guidelines that can be adapted for other languages.,3.1 Lexical Categories of Interest,[0],[0]
"This includes languages which have postpositions, circumpositions, or inpositions rather than prepositions; the general term for such items is adpositions.4 English possessive marking (via ’s or possessive pronouns like my) is more generally an example of case marking.",3.1 Lexical Categories of Interest,[0],[0]
"Note that prepositions (4a–4c) differ in word order from possessives (4d), though semantically the object of the preposition and the possessive nominal pattern together:
(4) a. eat in a restaurant b. the man in a blue shirt c. the wife of the ambassador d. the ambassador’s wife
Cross-linguistically, adpositions and case marking are closely related, and in general both grammatical strategies can express similar kinds of semantic relations.",3.1 Lexical Categories of Interest,[0],[0]
"This motivates a common semantic inventory for adpositions and case.
",3.1 Lexical Categories of Interest,[0],[0]
"We also cover multiword prepositions (e.g., out_of, in_front_of), intransitive particles (He flew away), purpose infinitive clauses (Open the door to let in some air5), prepositions with clausal complements (It rained before the party started), and idiomatic prepositional phrases (at_large).",3.1 Lexical Categories of Interest,[0],[0]
Our annotation guidelines give further details.,3.1 Lexical Categories of Interest,[0],[0]
"The hierarchy of preposition and possessive supersenses, which we call Semantic Network of Adposition and Case Supersenses (SNACS), is shown in figure 2.",3.2 The SNACS Hierarchy,[0],[0]
"It is simpler than its predecessor— Schneider et al.’s (2016) preposition supersense hierarchy—in both size and structural complexity.
",3.2 The SNACS Hierarchy,[0],[0]
"4In English, ago is arguably a postposition because it follows rather than precedes its complement: five minutes ago, not *ago five minutes.
",3.2 The SNACS Hierarchy,[0],[0]
"5 To can be rephrased as in_order_to and have prepositional
counterparts like in Open the door for some air.
",3.2 The SNACS Hierarchy,[0],[0]
"Circumstance 77
Temporal 0
Time 371
StartTime 28 EndTime 31
Frequency 9
Duration 91 Interval 35
Locus 846
Source 189 Goal 419
Path 49
Direction 161 Extent 42
Means",3.2 The SNACS Hierarchy,[0],[0]
"17 Manner 140 Explanation 123
Purpose 401
Participant 0
Causer 15
Agent 170
Co-Agent 65
Theme 238
Co-Theme 14",3.2 The SNACS Hierarchy,[0],[0]
"Topic 296
Stimulus 123 Experiencer 107
Originator 134
Recipient 122
Cost 48 Beneficiary 110
Instrument 30
Configuration 0
Identity 85
Species 39
Gestalt 709
Possessor 492 Whole 250 Characteristic 140
Possession 21",3.2 The SNACS Hierarchy,[0],[0]
"PartPortion 57
Stuff 25
Accompanier 49
InsteadOf 10 ComparisonRef 215
RateUnit 5 Quantity 191
Approximator 76
SocialRel 240
OrgRole 103
Figure 2: SNACS hierarchy of 50 supersenses and their token counts in the annotated corpus described in §4.",3.2 The SNACS Hierarchy,[0],[0]
"Counts are of direct uses of labels, excluding uses of subcategories.",3.2 The SNACS Hierarchy,[0],[0]
"Role and function positions are not distinguished (so if a token has different role and function labels, it will count toward two supersense frequencies).
",3.2 The SNACS Hierarchy,[0],[0]
SNACS has 50 supersenses at 4 levels of depth; the previous hierarchy had 75 supersenses at 7 levels.,3.2 The SNACS Hierarchy,[0],[0]
"The top-level categories are the same:
• CIRCUMSTANCE:",3.2 The SNACS Hierarchy,[0],[0]
"Circumstantial information, usually non-core properties of events (e.g., location, time, means, purpose) •",3.2 The SNACS Hierarchy,[0],[0]
PARTICIPANT: Entity playing a role in an event • CONFIGURATION:,3.2 The SNACS Hierarchy,[0],[0]
"Thing, usually an entity or property, involved in a static relationship to some other entity The 3 subtrees loosely parallel adverbial adjuncts, event arguments, and adnominal complements, respectively.",3.2 The SNACS Hierarchy,[0],[0]
"The PARTICIPANT and CIRCUMSTANCE subtrees primarily reflect semantic relationships prototypical to verbal arguments/adjuncts and were inspired by VerbNet’s thematic role hierarchy (Palmer et al., 2017; Bonial et al., 2011).",3.2 The SNACS Hierarchy,[0],[0]
"Many CIRCUMSTANCE subtypes, like LOCUS (the concrete or abstract location of something), can be governed by eventive and non-eventive nominals as well as verbs: eat in the restaurant, a party in the restaurant, a table in the restaurant.",3.2 The SNACS Hierarchy,[0],[0]
"CONFIGURATION mainly encompasses non-spatiotemporal relations holding between entities, such as quantity, possession, and part/whole.",3.2 The SNACS Hierarchy,[0],[0]
"Unlike the previous hierarchy, SNACS does not use multiple inheritance, so there is no overlap between the 3 regions.
",3.2 The SNACS Hierarchy,[0],[0]
"The supersenses can be understood as roles in fundamental types of scenes (or schemas) such as: LOCATION—THEME is located at LO-
CUS; MOTION—THEME moves from SOURCE along PATH to GOAL; TRANSITIVE ACTION— AGENT acts on THEME, perhaps using an INSTRUMENT;",3.2 The SNACS Hierarchy,[0],[0]
"POSSESSION—POSSESSION belongs to POSSESSOR; TRANSFER—THEME changes possession from ORIGINATOR to RECIPIENT, perhaps with COST;",3.2 The SNACS Hierarchy,[0],[0]
"PERCEPTION—EXPERIENCER is mentally affected by STIMULUS; COGNITION— EXPERIENCER contemplates TOPIC; COMMUNICATION—information (TOPIC) flows from ORIGINATOR to RECIPIENT, perhaps via an INSTRUMENT.",3.2 The SNACS Hierarchy,[0],[0]
"For AGENT, CO-AGENT, EXPERIENCER, ORIGINATOR, RECIPIENT, BENEFICIARY, POSSESSOR, and SOCIALREL, the object of the preposition is prototypically animate.
",3.2 The SNACS Hierarchy,[0],[0]
"Because prepositions and possessives cover a vast swath of semantic space, limiting ourselves to 50 categories means we need to address a great many nonprototypical, borderline, and special cases.",3.2 The SNACS Hierarchy,[0],[0]
"We have done so in a 75-page annotation manual with over 400 example sentences (Schneider et al., 2018).
",3.2 The SNACS Hierarchy,[0],[0]
"Finally, we note that the Universal Semantic Tagset (Abzianidze and Bos, 2017) defines a crosslinguistic inventory of semantic classes for content and function words.",3.2 The SNACS Hierarchy,[0],[0]
"SNACS takes a similar approach to prepositions and possessives, which in Abzianidze and Bos’s (2017) specification are simply tagged REL, which does not disambiguate the nature of the relational meaning.",3.2 The SNACS Hierarchy,[0],[0]
Our categories can thus be understood as refinements to REL.,3.2 The SNACS Hierarchy,[0],[0]
Hwang et al. (2017) have pointed out the perils of teasing apart and generalizing preposition semantics so that each use has a clear supersense label.,3.3 Adopting the Construal Analysis,[0],[0]
One key challenge they identified is that the preposition itself and the situation as established by the verb may suggest different labels.,3.3 Adopting the Construal Analysis,[0],[0]
"For instance:
(5) a. Vernon works at Grunnings.",3.3 Adopting the Construal Analysis,[0],[0]
"b. Vernon works for Grunnings.
",3.3 Adopting the Construal Analysis,[0],[0]
"The semantics of the scene in (5a, 5b) is the same: it is an employment relationship, and the PP contains the employer.",3.3 Adopting the Construal Analysis,[0],[0]
"SNACS has the label ORGROLE for this purpose.6 At the same time, at in (5a) strongly suggests a locational relationship, which would correspond to the label LOCUS; consistent with this
6ORGROLE is defined as “Either a party in a relation between an organization/institution and an individual who has a stable affiliation with that organization, such as membership or a business relationship.”
",3.3 Adopting the Construal Analysis,[0],[0]
"hypothesis, Where does Vernon work? is a perfectly good way to ask a question that could be answered by the PP.",3.3 Adopting the Construal Analysis,[0],[0]
"In this example, then, there is overlap between locational meaning and organizationalbelonging meaning.",3.3 Adopting the Construal Analysis,[0],[0]
(5b) is similar except the for suggests a notion of BENEFICIARY:,3.3 Adopting the Construal Analysis,[0],[0]
the employee is working on behalf of the employer.,3.3 Adopting the Construal Analysis,[0],[0]
Annotators would face a conundrum if forced to pick a single label when multiple ones appear to be relevant.,3.3 Adopting the Construal Analysis,[0],[0]
"Schneider et al. (2016) handled overlap via multiple inheritance, but entertaining a new label for every possible case of overlap is impractical, as this would result in a proliferation of supersenses.
",3.3 Adopting the Construal Analysis,[0],[0]
"Instead, Hwang et al. (2017) suggest a construal analysis in which the lexical semantic contribution, or henceforth the function, of the preposition itself may be distinct from the semantic role or relation mediated by the preposition in a given sentence, called the scene role.",3.3 Adopting the Construal Analysis,[0],[0]
The notion of scene role is a widely accepted idea that underpins the use of semantic or thematic roles: semantics licensed by the governor7 of the prepositional phrase dictates its relationship to the prepositional phrase.,3.3 Adopting the Construal Analysis,[0],[0]
"The innovative claim is that, in addition to a preposition’s relationship with its head, the prepositional choice introduces another layer of meaning or construal that brings additional nuance, creating the difficulty we see in the annotation of (5a, 5b).",3.3 Adopting the Construal Analysis,[0],[0]
Construal is notated by ROLE;FUNCTION.,3.3 Adopting the Construal Analysis,[0],[0]
"Thus, (5a) would be annotated ORGROLE;LOCUS and (5b) as ORGROLE;BENEFICIARY to expose their common truth-semantic meaning but slightly different portrayals owing to the different prepositions.
",3.3 Adopting the Construal Analysis,[0],[0]
"Another useful application of the construal analysis is with the verb put, which can combine with any locative PP to express a destination:
(6) Put it on/by/behind/on_top_of/. . .",3.3 Adopting the Construal Analysis,[0],[0]
the door.,3.3 Adopting the Construal Analysis,[0],[0]
"GOAL;LOCUS
I.e., the preposition signals a LOCUS, but the door serves as the GOAL with respect to the scene.",3.3 Adopting the Construal Analysis,[0],[0]
"This approach also allows for resolution of various se-
7By “governor” of the preposition or prepositional phrase, we mean the head of the phrase to which the PP attaches in a constituency representation.",3.3 Adopting the Construal Analysis,[0],[0]
"In a dependency representation, this would be the head of the preposition itself or of the object of the preposition depending on which convention is used for PP headedness: e.g., the preposition heads the PP in CoNLL and Stanford Dependencies whereas the object is the head in Universal Dependencies.",3.3 Adopting the Construal Analysis,[0],[0]
The governor is most often a verb or noun.,3.3 Adopting the Construal Analysis,[0],[0]
"Where the PP is a predicate complement (e.g. Vernon is with Grunnings), there is no governor to specify the nature of the scene, so annotators must rely on world knowledge and context to determine the scene.
",3.3 Adopting the Construal Analysis,[0],[0]
"mantic phenomena including perceptual scenes (e.g., I care about education, where about is both the topic of cogitation and perceptual stimulus of caring: STIMULUS;TOPIC), and fictive motion (Talmy, 1996), where static location is described using motion verbiage (as in The road runs through the forest: LOCUS;PATH).
",3.3 Adopting the Construal Analysis,[0],[0]
Both role and function slots are filled by supersenses from the SNACS hierarchy.,3.3 Adopting the Construal Analysis,[0],[0]
Annotators have the option of using distinct supersenses for the role and function; in general it is not a requirement (though we stipulate that certain SNACS supersenses can only be used as the role).,3.3 Adopting the Construal Analysis,[0],[0]
"When the same label captures both role and function, we do not repeat it: Vernon lives in/LOCUS England.",3.3 Adopting the Construal Analysis,[0],[0]
"Figure 1 shows some real examples from our corpus.
",3.3 Adopting the Construal Analysis,[0],[0]
We apply the construal analysis in SNACS annotation of our corpus to test its feasibility.,3.3 Adopting the Construal Analysis,[0],[0]
"It has proved useful not only for prepositions, but also possessives, where the general sense of possession may overlap with other scene relations, like creator/initial-possessor (ORIGINATOR): Da Vinci’s/ORIGINATOR;POSSESSOR sculptures.",3.3 Adopting the Construal Analysis,[0],[0]
"We applied the SNACS annotation scheme (§3) to prepositions and possessives in the STREUSLE corpus (§2), a collection of online consumer reviews taken from the English Web Treebank (Bies et al., 2012).",4 Annotated Reviews Corpus,[0],[0]
"The sentences from the English Web Treebank also comprise the primary reference treebank for English Universal Dependencies (UD; Nivre et al., 2016), and we bundle the UD version 2 syntax alongside our annotations.",4 Annotated Reviews Corpus,[0],[0]
Table 1 shows the total number of tokens present and those that we annotated.,4 Annotated Reviews Corpus,[0],[0]
"Altogether, 5,455 tokens were annotated for scene role and function.
",4 Annotated Reviews Corpus,[0],[0]
Table 2 shows the most and least common labels occurring as scene role and function.,4 Annotated Reviews Corpus,[0],[0]
"Three labels never appear in the annotated corpus: TEMPORAL from the CIRCUMSTANCE hierarchy, and PARTICIPANT and CONFIGURATION which are both the highest supersense in their respective hierarchies.",4 Annotated Reviews Corpus,[0],[0]
"While all remaining supersenses are attested as scene roles, there are some that never occur as functions, such as ORIGINATOR, which is most often realized as POSSESSOR or SOURCE, and EXPERIENCER.",4 Annotated Reviews Corpus,[0],[0]
"It is interesting to note that every subtype of CIRCUMSTANCE (except TEMPORAL) appears as both scene role and function, whereas many of the subtypes of the other two hierarchies are lim-
8Blodgett and Schneider (2018) detail the extension of the scheme to possessives.
",4 Annotated Reviews Corpus,[0],[0]
"9In the corpus, lexical expression tokens appear alongside a lexical category indicating which inventory of supersenses, if any, applies.",4 Annotated Reviews Corpus,[0],[0]
"SNACS-annotated units are those with ADP (adposition), PP, PRON.POSS (possessive pronoun), etc., whereas DISC (discourse) and CCONJ expressions do not receive any supersense.",4 Annotated Reviews Corpus,[0],[0]
"Refer to the STREUSLE README for details.
ited to either role or function.",4 Annotated Reviews Corpus,[0],[0]
"This reflects our view that prepositions primarily capture circumstantial notions such as space and time, but have been extended to cover other semantic relations.10",4 Annotated Reviews Corpus,[0],[0]
"Because the online reviews corpus was so central to the development of our guidelines, we sought to estimate the reliability of the annotation scheme on a new corpus in a new genre.",5 Interannotator Agreement Study,[0],[0]
"We chose SaintExupéry’s novella The Little Prince, which is readily available in many languages and has been annotated with semantic representations such as AMR (Banarescu et al., 2013).",5 Interannotator Agreement Study,[0],[0]
"The genre is markedly different from online reviews—it is quite literary, and employs archaic or poetic figures of speech.",5 Interannotator Agreement Study,[0],[0]
"It is also a translation from French, contributing to the markedness of the language.",5 Interannotator Agreement Study,[0],[0]
This text is therefore a challenge for an annotation scheme based on colloquial contemporary English.,5 Interannotator Agreement Study,[0],[0]
"We addressed this issue by running 3 practice rounds of annotation on small passages from The Little Prince, both to assess whether the scheme was applicable without major guidelines changes and to prepare the annotators for this genre.",5 Interannotator Agreement Study,[0],[0]
"For the final annotation study, we chose chapters 4 and 5, in which 242 markables of 52 types were identified heuristically (§6.2).",5 Interannotator Agreement Study,[0],[0]
"The types of, to, in, as, from, and for, as well as possessives, occurred at least 10 times.",5 Interannotator Agreement Study,[0],[0]
"Annotators had the option to mark units as false positives using special labels (see §4) in addition to expressing uncertainty about the unit.
",5 Interannotator Agreement Study,[0],[0]
"For the annotation process, we adapted the open source web-based annotation tool UCCAApp (Abend et al., 2017) to our workflow, by extending it with a type-sensitive ranking module for the list of categories presented to the annotators.",5 Interannotator Agreement Study,[0],[0]
Annotators.,5 Interannotator Agreement Study,[0],[0]
"Five annotators (A, B, C, D, E), all authors of this paper, took part in this study.",5 Interannotator Agreement Study,[0],[0]
All are computational linguistics researchers with advanced training in linguistics.,5 Interannotator Agreement Study,[0],[0]
"Their involvement in the development of the scheme falls on a spectrum, with annotator A being the most active figure in guidelines development, and annotator E not being
10All told, 41 supersenses are attested as both role and function for the same token, and there are 136 unique construal combinations where the role differs from the function.",5 Interannotator Agreement Study,[0],[0]
"Only four supersenses are never found in such a divergent construal: EXPLANATION, SPECIES, STARTTIME, RATEUNIT.",5 Interannotator Agreement Study,[0],[0]
"Except for RATEUNIT which occurs only 5 times, their narrow use does not arise because they are rare.",5 Interannotator Agreement Study,[0],[0]
"EXPLANATION, for example, occurs over 100 times, more than many labels which often appear in construal.
involved in developing the guidelines and learning the scheme solely from reading the manual.",5 Interannotator Agreement Study,[0],[0]
"Annotators A, B, and C are native speakers of English, while Annotators D and E are nonnative but highly fluent speakers.
",5 Interannotator Agreement Study,[0],[0]
Results.,5 Interannotator Agreement Study,[0],[0]
"In the Little Prince sample, 40 out of 47 possible supersenses were applied at least once by some annotator; 36 were applied at least once by a majority of annotators; and 33 were applied at least once by all annotators.",5 Interannotator Agreement Study,[0],[0]
"APPROXIMATOR, COTHEME, COST, INSTEADOF, INTERVAL, RATEUNIT, and SPECIES were not used by any annotator.
",5 Interannotator Agreement Study,[0],[0]
"To evaluate interannotator agreement, we excluded 26 tokens for which at least one annotator has assigned a non-semantic label, considering only the 216 tokens that were identified correctly as SNACS targets and were clear to all annotators.",5 Interannotator Agreement Study,[0],[0]
"Despite varying exposure to the scheme, there is no obvious relationship between annotators’ backgrounds and their agreement rates.11
Table 3 shows the interannotator agreement rates, averaged across all pairs of annotators.",5 Interannotator Agreement Study,[0],[0]
"Average agreement is 74.4% on the scene role and 81.3% on the function (row 1).12 All annotators agree on the role for 119, and on the function for 139 tokens.",5 Interannotator Agreement Study,[0],[0]
"Agreement is higher on the function slot than on the scene role slot, which implies that the former is an easier task than the latter.",5 Interannotator Agreement Study,[0],[0]
"This is expected considering the definition of construal: the function of an adposition is more lexical and less contextdependent, whereas the role depends on the context (the scene) and can be highly idiomatic (§3.3).
",5 Interannotator Agreement Study,[0],[0]
"The supersense hierarchy allows us to analyze agreement at different levels of granularity (rows
11See table 7 in appendix A for a more detailed description of the annotators’ backgrounds and pairwise IAA results.
",5 Interannotator Agreement Study,[0],[0]
"12Average of pairwise Cohen’s k is 0.733 and 0.799 on, respectively, role and function, suggesting strong agreement.",5 Interannotator Agreement Study,[0],[0]
"However, it is worth noting that annotators selected labels from a ranked list, with the ranking determined by preposition type.",5 Interannotator Agreement Study,[0],[0]
"The model of chance agreement underlying k does not take the identity of the preposition into account, and thus likely underestimates the probability of chance agreement.
",5 Interannotator Agreement Study,[0],[0]
2–4 in table 3; see also confusion matrix in supplement).,5 Interannotator Agreement Study,[0],[0]
"Coarser-grained analyses naturally give better agreement, with depth-1 coarsening into only 3 categories.",5 Interannotator Agreement Study,[0],[0]
Results show that most confusions are local with respect to the hierarchy.,5 Interannotator Agreement Study,[0],[0]
We now describe systems that identify and disambiguate SNACS-annotated prepositions and possessives in two steps.,6 Disambiguation Systems,[0],[0]
Target identification heuristics (§6.2) first determine which tokens (single-word or multiword) should receive a SNACS supersense.,6 Disambiguation Systems,[0],[0]
A supervised classifier then predicts a supersense analysis for each identified target.,6 Disambiguation Systems,[0],[0]
"The research objectives are (a) to study the ability of statistical models to learn roles and functions of prepositions and possessives, and (b) to compare two different modeling strategies (feature-rich and neural), and the impact of syntactic parsing.",6 Disambiguation Systems,[0],[0]
Our experiments use the reviews corpus described in §4.,6.1 Experimental Setup,[0],[0]
We adopt the official training/development/ test splits of the Universal Dependencies (UD) project; their sizes are presented in table 1.,6.1 Experimental Setup,[0],[0]
All systems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters.,6.1 Experimental Setup,[0],[0]
Gold tokenization was used throughout.,6.1 Experimental Setup,[0],[0]
"Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation—i.e., tokens with special labels (see §4) were excluded.
",6.1 Experimental Setup,[0],[0]
"To test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0",6.1 Experimental Setup,[0],[0]
"(Manning et al., 2014).13 Named entity tags from the default 12-class CoreNLP model were used in all conditions.",6.1 Experimental Setup,[0],[0]
"§3.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as
13The CoreNLP parser was trained on all 5 genres of the English Web Treebank—i.e., a superset of our training set.",6.2 Target Identification,[0],[0]
"Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested with UDv1 parses produced by CoreNLP.
",6.2 Target Identification,[0],[0]
well as possessive case markers and multiword expressions.,6.2 Target Identification,[0],[0]
"61.2% of the units annotated in our corpus are adpositions according to gold POS annotation, 20.2% are possessives, and 18.6% belong to other POS classes.",6.2 Target Identification,[0],[0]
"Furthermore, 14.1% of tokens labeled as adpositions or possessives are not annotated because they are part of a multiword expression (MWE).",6.2 Target Identification,[0],[0]
"It is therefore neither obvious nor trivial to decide which tokens and groups of tokens should be selected as targets for SNACS annotation.
",6.2 Target Identification,[0],[0]
"To facilitate both manual annotation and automatic classification, we developed heuristics for identifying annotation targets.",6.2 Target Identification,[0],[0]
"The algorithm first scans the sentence for known multiword expressions, using a blacklist of non-prepositional MWEs that contain preposition tokens (e.g., take_care_of ) and a whitelist of prepositional MWEs (multiword prepositions like out_of and PP idioms like in_town).",6.2 Target Identification,[0],[0]
Both lists were constructed from the training data.,6.2 Target Identification,[0],[0]
"From segments unaffected by the MWE heuristics, single-word candidates are identified by matching a high-recall set of parts of speech, then filtered through 5 different heuristics for adpositions, possessives, subordinating conjunctions, adverbs, and infinitivals.",6.2 Target Identification,[0],[0]
"Most of these filters are based on lexical lists learned from the training portion of the STREUSLE corpus, but there are some specific rules for infinitivals that handle forsubjects (I opened the door for Steve to take out the trash—to, but not for, should receive a supersense) and comparative constructions with too and enough (too short to ride).",6.2 Target Identification,[0],[0]
The next step of disambiguation is predicting the role and function labels.,6.3 Classification,[0],[0]
We explore two different modeling strategies.,6.3 Classification,[0],[0]
Feature-rich Model.,6.3 Classification,[0],[0]
"Our first model is based on the features for preposition relation classification developed by Srikumar and Roth (2013), which were themselves extended from the preposition sense disambiguation features of Hovy et al. (2010).",6.3 Classification,[0],[0]
"We briefly describe the feature set here, and refer the reader to the original work for further details.",6.3 Classification,[0],[0]
"At a high level, it consists of features extracted from selected neighboring words in the dependency tree (i.e., heuristically identified governor and object) and in the sentence (previous verb, noun and adjective, and next noun).",6.3 Classification,[0],[0]
"In addition, all these features are also conjoined with the lemma of the rightmost word in the preposition token to capture
target-specific interactions with the labels.",6.3 Classification,[0],[0]
"The features extracted from each neighboring word are listed in the supplementary material.
",6.3 Classification,[0],[0]
"Using these features extracted from targets, we trained two multi-class SVM classifiers to predict the role and function labels using the LIBLINEAR library (Fan et al., 2008).
",6.3 Classification,[0],[0]
Neural Model.,6.3 Classification,[0],[0]
Our second classifier is a multilayer perceptron (MLP) stacked on top of a BiLSTM.,6.3 Classification,[0],[0]
"For every sentence, tokens are first embedded using a concatenation of fixed pre-trained word2vec (Mikolov et al., 2013) embeddings of the word and the lemma, and an internal embedding vector, which is updated during training.14 Token embeddings are then fed into a 2-layer BiLSTM encoder, yielding a list of token representations.
",6.3 Classification,[0],[0]
"For each identified target unit u, we extract its first token, and its governor and object headword.",6.3 Classification,[0],[0]
"For each of these tokens, we construct a feature vector by concatenating its token representation with embeddings of its (1) language-specific POS tag, (2) UD dependency label, and (3) NER label.",6.3 Classification,[0],[0]
"We additionally concatenate embeddings of u’s lexical category, a syntactic label indicating whether u is predicative/stranded/subordinating/none of these, and an indicator of whether either of the two tokens following the unit is capitalized.",6.3 Classification,[0],[0]
"All these embeddings, as well as internal token embedding vectors, are considered part of the model parameters and are initialized randomly using the Xavier initialization (Glorot and Bengio, 2010).",6.3 Classification,[0],[0]
"A NONE label is used when the corresponding feature is not given, both in training and at test time.",6.3 Classification,[0],[0]
"The concatenated feature vector for u is fed into two separate 2-layered MLPs, followed by a separate softmax layer that yields the predicted probabilities for the role and function labels.
",6.3 Classification,[0],[0]
We tuned hyperparameters on the development set to maximize F-score (see supplementary material).,6.3 Classification,[0],[0]
"We used the cross-entropy loss function, optimizing with simple gradient ascent for 80 epochs with minibatches of size 20.",6.3 Classification,[0],[0]
Inverted dropout was used during training.,6.3 Classification,[0],[0]
"The model is implemented with the DyNet library (Neubig et al., 2017).
",6.3 Classification,[0],[0]
"The model architecture is largely comparable to that of Gonen and Goldberg (2016), who experimented with a coarsened version of STREUSLE 3.0.",6.3 Classification,[0],[0]
"The main difference is their use of unlabeled multilingual datasets to improve pre-
14Word2vec is pre-trained on the Google News corpus.",6.3 Classification,[0],[0]
"Zero vectors are used where vectors are not available.
diction by exploiting the differences in preposition ambiguities across languages.",6.3 Classification,[0],[0]
"Following the two-stage disambiguation pipeline (i.e. target identification and classification), we separate the evaluation across the phases.",6.4 Results & Analysis,[0],[0]
"Table 4 reports the precision, recall, and F-score (P/R/F) of the target identification heuristics.",6.4 Results & Analysis,[0],[0]
Table 5 reports the disambiguation performance of both classifiers with gold (left) and automatic target identification (right).,6.4 Results & Analysis,[0],[0]
"We evaluate each classifier along three dimensions—role and function independently, and full (i.e. both role and function together).",6.4 Results & Analysis,[0],[0]
"When we have the gold targets, we only report accuracy because precision and recall are equal.",6.4 Results & Analysis,[0],[0]
"With automatically identified targets, we report P/R/F for each dimension.",6.4 Results & Analysis,[0],[0]
Both tables show the impact of syntactic parsing on quality.,6.4 Results & Analysis,[0],[0]
The rest of this section presents analyses of the results along various axes.,6.4 Results & Analysis,[0],[0]
Target identification.,6.4 Results & Analysis,[0],[0]
The identification heuristics described in §6.2 achieve an F1 score of 89.2% on the test set using gold syntax.15,6.4 Results & Analysis,[0],[0]
Most false positives (47/54=87%) can be ascribed to tokens that are part of a (non-adpositional or larger adpositional) multiword expression.,6.4 Results & Analysis,[0],[0]
"9 of the 50 false negatives (18%) are rare multiword expressions not occurring in the training data and there are 7 partially identified ones, which are counted as both false positives and false negatives.
",6.4 Results & Analysis,[0],[0]
Automatically generated parse trees slightly decrease quality (table 4).,6.4 Results & Analysis,[0],[0]
"Target identification, being the first step in the pipeline, imposes an upper bound on disambiguation scores.",6.4 Results & Analysis,[0],[0]
"We observe this degradation when we compare the Gold ID and the Auto ID blocks of table 5, where automatically identified targets decrease F-score by about 10 points in all settings.16
Classification.",6.4 Results & Analysis,[0],[0]
"Along with the statistical classifier results in table 5, we also report performance
15Our evaluation script counts tokens that received special labels in the gold standard (see §4) as negative examples of SNACS targets, with the exception of the tokens labeled as unintelligible/nonnative/etc., which are not counted toward or against target ID performance.
16A variant of the target ID module, optimized for recall, is used as preprocessing for the agreement study discussed in §5.",6.4 Results & Analysis,[0],[0]
"With this setting, the heuristic achieves an F1 score of 90.2% (P=85.3%, R=95.6%) on the test set.
for the most frequent baseline, which selects the most frequent role–function label pair given the (gold) lemma according to the training data.",6.4 Results & Analysis,[0],[0]
"Note that all learned classifiers, across all settings, outperform the most frequent baseline for both role and function prediction.",6.4 Results & Analysis,[0],[0]
"The feature-rich and the neural models perform roughly equivalently despite the significantly different modeling strategies.
",6.4 Results & Analysis,[0],[0]
Function and scene role performance.,6.4 Results & Analysis,[0],[0]
"Function prediction is consistently more accurate than role prediction, with roughly a 10-point gap across all systems.",6.4 Results & Analysis,[0],[0]
"This mirrors a similar effect in the interannotator agreement scores (see §5), and may be due to the reduced ambiguity of functions compared to roles (as attested by the baseline’s higher accuracy for functions than roles), and by the more literal nature of function labels, as opposed to role labels that often require more context to determine.
Impact of automatic syntax.",6.4 Results & Analysis,[0],[0]
"Automatic syntactic analysis decreases scores by 4 to 7 points, most likely due to parsing errors which affect the identification of the preposition’s object and governor.",6.4 Results & Analysis,[0],[0]
"In the auto ID/auto syntax condition, the worse target ID performance with automatic parses (noted above) contributes to lower classification scores.",6.4 Results & Analysis,[0],[0]
We can use the structure of the SNACS hierarchy to probe classifier performance.,6.5 Errors & Confusions,[0],[0]
"As with the interannotator study, we evaluate the accuracy of predicted labels when they are coarsened post hoc by moving up the hierarchy to a specific depth.",6.5 Errors & Confusions,[0],[0]
"Table 6 shows this for the feature-rich classifier for different depths, with depth-1 representing the coarsening of the labels into the 3 root labels.",6.5 Errors & Confusions,[0],[0]
Depth-4 (Exact) represents the full results in table 5.,6.5 Errors & Confusions,[0],[0]
These results show that the classifiers often mistake a label for another that is nearby in the hierarchy.,6.5 Errors & Confusions,[0],[0]
"Examining the most frequent confusions of both models, we observe that LOCUS is overpredicted
(which makes sense as it is most frequent overall), and SOCIALROLE–ORGROLE and GESTALT– POSSESSOR are often confused (they are close in the hierarchy: one inherits from the other).",6.5 Errors & Confusions,[0],[0]
"This paper introduced a new approach to comprehensive analysis of the semantics of prepositions and possessives in English, backed by a thoroughly documented hierarchy and annotated corpus.",7 Conclusion,[0],[0]
We found good interannotator agreement and provided initial supervised disambiguation results.,7 Conclusion,[0],[0]
"We expect that future work will develop methods to scale the annotation process beyond requiring highly trained experts; bring this scheme to bear on other languages; and investigate the relationship of our scheme to more structured semantic representations, which could lead to more robust models.",7 Conclusion,[0],[0]
"Our guidelines, corpus, and software are available at https://github.com/nert-gu/streusle/ blob/master/ACL2018.md.",7 Conclusion,[0],[0]
"We thank Oliver Richardson, whose codebase we adapted for this project; Na-Rae Han, Archna Bhatia, Tim O’Gorman, Ken Litkowski, Bill Croft, and Martha Palmer for helpful discussions and support; and anonymous reviewers for useful feedback.",Acknowledgments,[0],[0]
This research was supported in part by DTRA HDTRA116-1-0002/,Acknowledgments,[0],[0]
"Project #1553695, by DARPA 15-18- CwC-FP-032, and by grant 2016375 from the United States–Israel Binational Science Foundation (BSF), Jerusalem, Israel.",Acknowledgments,[0],[0]
Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation.,abstractText,[0],[0]
"We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English.",abstractText,[0],[0]
"Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene.",abstractText,[0],[0]
"Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.",abstractText,[0],[0]
Comprehensive Supersense Disambiguation of English Prepositions and Possessives,title,[0],[0]
"In many real-world domains, data acquisition is costly.",1. Introduction,[0],[0]
"For instance, magnetic resonance imaging (MRI) requires scan times proportional to the number of measurements, which can be significant for patients (Lustig et al., 2008).",1. Introduction,[0],[0]
"Geophysical applications like oil drilling require expensive simulation of seismic waves (Qaisar et al., 2013).",1. Introduction,[0],[0]
"Such appli-
1Computer Science Department, Stanford University, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Manik Dhar <dmanik@cs.stanford.edu>, Aditya Grover",1. Introduction,[0],[0]
"<adityag@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
cations, among many others, can benefit significantly from compressed sensing techniques to acquire signals efficiently (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",1. Introduction,[0],[0]
"In compressed sensing, we wish to acquire an n-dimensional signal x ∈ Rn using only m n measurements linear in x.",1. Introduction,[0],[0]
"The measurements could potentially be noisy, but even in the absence of any noise we need to impose additional structure on the signal to guarantee unique recovery.",1. Introduction,[0],[0]
"Classical results on compressed sensing impose structure by assuming the underlying signal to be approximately l-sparse in some known basis, i.e., the l-largest entries dominate the rest.",1. Introduction,[0],[0]
"For instance, images and audio signals are typically sparse in the wavelet and Fourier basis respectively (Mallat, 2008).",1. Introduction,[0],[0]
"If the matrix of linear vectors relating the signal and measurements satisfies certain mild conditions, then one can provably recover x with only m = O(l log nl ) measurements using LASSO (Tibshirani, 1996; Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006; Bickel et al., 2009).
",1. Introduction,[0],[0]
"Alternatively, structural assumptions on the signals being sensed can be learned from data, e.g., using a dataset of typical signals (Baraniuk et al., 2010; Peyre, 2010; Chen et al., 2010; Yu & Sapiro, 2011).",1. Introduction,[0],[0]
"Particularly relevant to this work, Bora et al. (2017) proposed an approach where structure is provided by a deep generative model learned from data.",1. Introduction,[0],[0]
"Specifically, the underlying signal x being sensed is assumed to be close to the range of a deterministic function expressed by a pretrained, latent variable modelG : Rk → Rn such that x ≈ G(z) where z ∈ Rk denote the latent variables.",1. Introduction,[0],[0]
"Consequently, the signal x is recovered by optimizing for a latent vector z that minimizes the `2 distance between the measurements corresponding to G(z) and the actual ones.",1. Introduction,[0],[0]
"Even though the objective being optimized in this case is non-convex, empirical results suggest that the reconstruction error decreases much faster than LASSO-based recovery as we increase the number of measurements.
",1. Introduction,[0],[0]
"A limitation of the above approach is that the recovered signal is constrained to be in the range of the generator function G. Hence, if the true signal being sensed is not in the range of G, the algorithm cannot drive the reconstruction error to zero even when m ≥ n",1. Introduction,[0],[0]
(even if we ignore error due to measurement noise and non-convex optimization).,1. Introduction,[0],[0]
"This is also observed empirically, as the reconstruction error of generative model-based recovery saturates as we keep
increasing the number of measurements m. On the other hand, LASSO-based recovery continues to shrink the error with increasing number of measurements, eventually outperforming the generative model-based recovery.
",1. Introduction,[0],[0]
"To overcome this limitation, we propose a framework that allows recovery of signals with sparse deviations from the set defined by the range of the generator function.",1. Introduction,[0],[0]
"The recovered signals have the general form of G(ẑ) + ν̂, where ν̂ ∈ Rn is a sparse vector.",1. Introduction,[0],[0]
This allows the recovery algorithm to consider signals away from the range of the generator function.,1. Introduction,[0],[0]
"Similar to LASSO, we relax the hardness in optimizing for sparse vectors by minimizing the `1 norm of the deviations.",1. Introduction,[0],[0]
"Unlike LASSO-based recovery, we can exploit the rich structure imposed by a (deep) generative model (at the expense of solving a hard optimization problem if G is non-convex).",1. Introduction,[0],[0]
"In fact, we show that LASSO-based recovery is a special case of our framework if the generator function G maps all z to the origin.",1. Introduction,[0],[0]
"Unlike generative model-based recovery, the signals recovered by our algorithm are not constrained to be in the range of the generator function.
",1. Introduction,[0],[0]
"Our proposed algorithm, referred to as Sparse-Gen, has desirable theoretical properties and empirical performance.",1. Introduction,[0],[0]
"Theoretically, we derive upper bounds on the reconstruction error for an optimal decoder with respect to the proposed model and show that this error vanishes with m = n measurements.",1. Introduction,[0],[0]
"We confirm our theory empirically, wherein we find that recovery using Sparse-Gen with variational autoencoders (Kingma & Welling, 2014) as the underlying generative model outperforms both LASSO-based and generative model-based recovery in terms of the reconstruction errors for the same number of measurements for MNIST and Omniglot datasets.",1. Introduction,[0],[0]
"Additionally, we observe significant improvements in the more practical and novel task of transfer compressed sensing where a generative model on a data-rich, source domain provides a prior for sensing a data-scarce, target domain.",1. Introduction,[0],[0]
"In this section, we review the necessary background and prior work in modeling domain specific structure in compressed sensing.",2. Preliminaries,[0],[0]
"We are interested in solving the following system of equations,
y = Ax (1)
where x ∈ Rn is the signal of interest being sensed through measurements y ∈ Rm, and A ∈ Rm×n is a measurement matrix.",2. Preliminaries,[0],[0]
"For efficient acquisition of signals, we will design measurement matrices such that m n. However, the system is under-determined whenever rank(A) <",2. Preliminaries,[0],[0]
"n. Hence, unique recovery requires additional assumptions on x. We now discuss two ways to model the structure of x.
Sparsity.",2. Preliminaries,[0],[0]
Sparsity in a well-chosen basis is natural in many domains.,2. Preliminaries,[0],[0]
"For instance, natural images are sparse in the wavelet basis whereas audio signals exhibit sparsity in the Fourier basis (Mallat, 2008).",2. Preliminaries,[0],[0]
"Hence, it is natural to assume the domain of signals x we are interested in recovering is
Sl(0) = {x : ‖x− 0‖0 ≤",2. Preliminaries,[0],[0]
l}.,2. Preliminaries,[0],[0]
"(2)
This is the set of l-sparse vectors with the `0 distance measured from the origin.",2. Preliminaries,[0],[0]
"Such assumptions dominate the prior literature in compressed sensing and can be further relaxed to recover approximately sparse signals (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2. Preliminaries,[0],[0]
Latent variable generative models.,2. Preliminaries,[0],[0]
"A latent variable model specifies a joint distribution Pθ(x, z) over the observed data x (e.g., images) and a set of latent variables z ∈",2. Preliminaries,[0],[0]
"Rk (e.g., features).",2. Preliminaries,[0],[0]
"Given a training set of signals {x1, · · · , xM}, we can learn the parameters θ of such a model, e.g., via maximum likelihood.",2. Preliminaries,[0],[0]
"When Pθ(x, z) is parameterized using deep neural networks, such generative models can effectively model complex, high-dimensional signal distributions for modalities such as images and audio (Kingma & Welling, 2014; Goodfellow et al., 2014).
",2. Preliminaries,[0],[0]
"Given a pretrained latent variable generative model with parameters θ, we can associate a generative model function G : Rk → Rn mapping a latent vector z to the mean of the conditional distribution Pθ(x|z).",2. Preliminaries,[0],[0]
"Thereafter, the space of signals that can be recovered with such a model is given by the range of the generator function,
SG = {G(z) : z ∈ Rk}.",2. Preliminaries,[0],[0]
"(3)
Note that the set is defined with respect to the latent vectors z, and we omit the dependence of G on the parameters θ (which are fixed for a pretrained model) for brevity.",2. Preliminaries,[0],[0]
"Signal recovery in compressed sensing algorithm typically involves solving an optimization problem consistent with the modeling assumptions on the domain of the signals being sensed.
",2.1. Recovery algorithms,[0],[0]
Sparse vector recovery using LASSO.,2.1. Recovery algorithms,[0],[0]
"Under the assumptions of sparsity, the signal x can be recovered by solving an `0 minimization problem (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2.1. Recovery algorithms,[0],[0]
"min x ‖x‖0
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (4)
",2.1. Recovery algorithms,[0],[0]
"The objective above is however NP-hard to optimize, and hence, it is standard to consider a convex relaxation,
min x ‖x‖1
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (5)
",2.1. Recovery algorithms,[0],[0]
"In practice, it is common to solve the Lagrangian of the above problem.",2.1. Recovery algorithms,[0],[0]
We refer to this method as LASSO-based recovery due to similarities of the objective in Eq.,2.1. Recovery algorithms,[0],[0]
"(5) to the LASSO regularization used broadly in machine learning (Tibshirani, 1996).",2.1. Recovery algorithms,[0],[0]
"LASSO-based recovery is the predominant technique for recovering sparse signals since it involves solving a tractable convex optimization problem.
",2.1. Recovery algorithms,[0],[0]
In order to guarantee unique recovery to the underdetermined system in Eq.,2.1. Recovery algorithms,[0],[0]
"(1), the measurement matrix A is designed to satisfy the Restricted Isometry Property (RIP) or the Restricted Eigenvalue Condition (REC) for l-sparse matrices with high probability (Candès & Tao, 2005; Bickel et al., 2009).",2.1. Recovery algorithms,[0],[0]
We define these conditions below.,2.1. Recovery algorithms,[0],[0]
Definition 1.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter α ∈ (0, 1), a matrix A ∈ Rm×n is said to satisfy RIP(l, α) if ∀ x ∈ Sl(0),
(1− α)‖x‖2 ≤ ‖Ax‖2 ≤ (1 + α)‖x‖2.
",2.1. Recovery algorithms,[0],[0]
Definition 2.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter γ > 0, a matrix A ∈ Rm×n is said to satisfy REC(l, γ) if ∀ x ∈ Sl(0),
‖Ax‖2 ≥ γ‖x‖2.
",2.1. Recovery algorithms,[0],[0]
"Intuitively, RIP implies that A approximately preserves Euclidean norms for sparse vectors and REC implies that sparse vectors are far from the nullspace of A. Many classes of matrices satisfy these conditions with high probability, including random Gaussian and Bernoulli matrices where every entry of the matrix is sampled from a standard normal and uniform Bernoulli distribution respectively (Baraniuk et al., 2008).
",2.1. Recovery algorithms,[0],[0]
Generative model vector recovery using gradient descent.,2.1. Recovery algorithms,[0],[0]
If the signals being sensed are assumed to lie close to the range SG of a generative model function G as defined in Eq.,2.1. Recovery algorithms,[0],[0]
"(3) , then we can recover the best approximation to the true signal by `2-minimization over z,
min z ‖AG(z)− y‖22.",2.1. Recovery algorithms,[0],[0]
"(6)
The function G is typically expressed as a deep neural network which makes the overall objective non-convex, but differentiable almost everywhere w.r.t z.",2.1. Recovery algorithms,[0],[0]
"In practice, good reconstructions can be recovered by gradient-based optimization methods.",2.1. Recovery algorithms,[0],[0]
"We refer to this method proposed by Bora et al. (2017) as generative model-based recovery.
",2.1. Recovery algorithms,[0],[0]
"To guarantee unique recovery, generative model-based recovery makes two key assumptions.",2.1. Recovery algorithms,[0],[0]
"First, the generator functionG is assumed to be L-Lipschitz, i.e., ∀ z1, z2 ∈ Rk,
‖G(z1)−G(z2)‖2 ≤ L‖z1",2.1. Recovery algorithms,[0],[0]
"− z2‖2.
",2.1. Recovery algorithms,[0],[0]
"Secondly, the measurement matrix A is designed to satisfy the Set-Restricted Eigenvalue Condition (S-REC) with high probability (Bora et al., 2017).
",2.1. Recovery algorithms,[0],[0]
Definition 3.,2.1. Recovery algorithms,[0],[0]
Let S ⊆ Rn.,2.1. Recovery algorithms,[0],[0]
"For some parameters γ > 0, δ ≥ 0, a matrix A ∈ Rm×n is said to satisfy the SREC(S, γ, δ) if ∀ x1, x2 ∈ S,
‖A(x1 − x2)‖2 ≥",2.1. Recovery algorithms,[0],[0]
γ‖x1,2.1. Recovery algorithms,[0],[0]
− x2‖2,2.1. Recovery algorithms,[0],[0]
"− δ.
",2.1. Recovery algorithms,[0],[0]
S-REC generalizes REC to an arbitrary set of vectors S as opposed to just considering the set of approximately sparse vectors Sl(0) and allowing an additional slack term δ.,2.1. Recovery algorithms,[0],[0]
"In particular, S is chosen to be the range of the generator function G for generative model-based recovery.",2.1. Recovery algorithms,[0],[0]
The modeling assumptions based on sparsity and generative modeling discussed in the previous section can be limiting in many cases.,3. The Sparse-Gen framework,[0],[0]
"On one hand, sparsity assumes a relatively weak prior over the signals being sensed.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we observe that the recovered signals xL have large reconstruction error ‖xL− x‖22 especially when the number of measurements m is small.",3. The Sparse-Gen framework,[0],[0]
"On the other hand, generative models imposes a very strong, but rigid prior which works well when the number of measurements is small.",3. The Sparse-Gen framework,[0],[0]
"However, the performance of the corresponding recovery methods saturates with increasing measurements since the recovered signal xG = G(zG) is constrained to lie in the range of the generator function G. If zG ∈ Rk is the optimum value returned by an optimization procedure for Eq.",3. The Sparse-Gen framework,[0],[0]
"(6), then the reconstruction error ‖xG − x‖22 is limited by the dimensionality of the latent space and the quality of the generator function.
",3. The Sparse-Gen framework,[0],[0]
"To sidestep the above limitations, we consider a strictly more expressive class of signals by allowing sparse deviations from the range of a generator function.",3. The Sparse-Gen framework,[0],[0]
"Formally, the domain of the recovered signals is given by,
Sl,G = ∪z∈Dom(G)Sl(G(z)) (7)
where Sl(G(z)) denotes the set of sparse vectors centered on G(z) and z varies over the domain of G (typically Rk).",3. The Sparse-Gen framework,[0],[0]
"We refer to this modeling assumption and the consequent algorithmic framework for recovery as Sparse-Gen.
Based on this modeling assumption, we will recover signals of the form G(z) + ν for some ν ∈",3. The Sparse-Gen framework,[0],[0]
Rn that is preferably sparse.,3. The Sparse-Gen framework,[0],[0]
"Specifically, we consider the optimization of a hybrid objective,
min z,ν ‖ν‖0
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (8)
In the above optimization problem the objective is nonconvex and non-differentiable, while the constraint is nonconvex (for general G), making the above optimization
problem hard to solve.",3. The Sparse-Gen framework,[0],[0]
"To ease the optimization problem, we propose two modifications.",3. The Sparse-Gen framework,[0],[0]
"First, we relax the `0 minimization to an `1 minimization similar to LASSO.
",3. The Sparse-Gen framework,[0],[0]
"min z,ν",3. The Sparse-Gen framework,[0],[0]
"‖ν‖1
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (9)
",3. The Sparse-Gen framework,[0],[0]
"Next, we square the non-convex constraint on both sides and consider the Lagrangian of the above problem to get the final unconstrained optimization problem for Sparse-Gen,
min z,ν ‖ν‖1 + λ‖A (G(z) + ν)− y‖22 (10)
where λ is the Lagrange multiplier.
",3. The Sparse-Gen framework,[0],[0]
The above optimization problem is non-differentiable w.r.t.,3. The Sparse-Gen framework,[0],[0]
ν and non-convex w.r.t.,3. The Sparse-Gen framework,[0],[0]
z,3. The Sparse-Gen framework,[0],[0]
(if G is non-convex).,3. The Sparse-Gen framework,[0],[0]
"In practice, it can be solved in practice using gradient descent (since the non-differentiability is only at a finite number of points) or using sequential convex programming (SCP).",3. The Sparse-Gen framework,[0],[0]
"SCP is an effective heuristic for non-convex problems where the convex portions of the problem are solved using a standard convex optimization technique (Boyd & Vandenberghe, 2004).",3. The Sparse-Gen framework,[0],[0]
"In the case of Eq. (10), the optimization w.r.t. ν (for fixed z) is a convex optimization problem whereas the non-convexity typically involves differentiable terms (w.r.t. z) if G is a deep neural network.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we find excellent recovery by standard first order gradient-based methods (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2015).
",3. The Sparse-Gen framework,[0],[0]
"Unlike LASSO-based recovery which recovers only sparse signals, Sparse-Gen can impose a stronger domain-specific prior using a generative model.",3. The Sparse-Gen framework,[0],[0]
"If we fix the generator function to map all z to the origin, we recover LASSO-based recovery as a special case of Sparse-Gen. Additionally, Sparse-Gen is not constrained to recover signals over the range of G, as in the case of generative model-based recovery.",3. The Sparse-Gen framework,[0],[0]
"In fact, it can recover signals with sparse deviations from the range of G. Note that the sparse deviations can be
defined in a basis different from the canonical basis.",3. The Sparse-Gen framework,[0],[0]
"In such cases, we consider the following optimization problem,
min z,ν ‖Bν‖1 + λ‖A",3. The Sparse-Gen framework,[0],[0]
"(G(z) + ν)− y‖22 (11)
where B is a change of basis matrix that promotes sparsity of the vector Bν",3. The Sparse-Gen framework,[0],[0]
.,3. The Sparse-Gen framework,[0],[0]
Figure 1 illustrates the differences in modeling assumptions between Sparse-Gen and other frameworks.,3. The Sparse-Gen framework,[0],[0]
The proofs for all results in this section are given in the Appendix.,4. Theoretical Analysis,[0],[0]
"Our analysis and experiments account for measurement noise in compressed sensing, i.e.,
y = Ax+ .",4. Theoretical Analysis,[0],[0]
"(12)
Let ∆ :",4. Theoretical Analysis,[0],[0]
Rm → Rn denote an arbitrary decoding function used to recover the true signal x from the measurements y ∈ Rm.,4. Theoretical Analysis,[0],[0]
"Our analysis will upper bound the `2-error in recovery incurred by our proposed framework using mixed norm guarantees (in particular, `2/`1).",4. Theoretical Analysis,[0],[0]
"To this end, we first state some key definitions.",4. Theoretical Analysis,[0],[0]
"Define the least possible `1 error for recovering x under the Sparse-Gen modeling as,
σSl,G(x) = inf x̂∈Sl,G
‖x− x̂‖1
where the optimal x̂ is the closest point to x in the allowed domain Sl,G. We now state the main lemma guiding the theoretical analysis.",4. Theoretical Analysis,[0],[0]
Lemma 1.,4. Theoretical Analysis,[0],[0]
"Given a function G : Rk → Rn and measurement noise with ‖ ‖2 ≤ max, let A be any matrix that satisfies S-REC(S1.5l,G, (1− α), δ) and RIP(2l, α) for some α ∈ (0, 1), l > 0.",4. Theoretical Analysis,[0],[0]
"Then, there exists a decoder ∆ :",4. Theoretical Analysis,[0],[0]
"Rm → Rn such that,
‖x−∆(Ax+ )‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, where C0 = 2((1+α)(1−α)−1 +1), C1 = 2(1− α)−1, and δ′ = δ(1− α)−1.
",4. Theoretical Analysis,[0],[0]
The above lemma shows that there exists a decoder such that the error in recovery can be upper bounded for measurement matrices satisfying S-REC and RIP.,4. Theoretical Analysis,[0],[0]
Note that Lemma 1 only guarantees the existence of such a decoder and does not prescribe an optimization algorithm for recovery.,4. Theoretical Analysis,[0],[0]
"Apart from the errors due to the bounded measurement noise max and a scaled slack term appearing in the S-REC condition δ′, the major term in the upper bound corresponds to (up to constants) the minimum possible error incurred by the best possible recovery vector in Sl,G given by σl,G(x).",4. Theoretical Analysis,[0],[0]
"Similar terms appear invariably in the compressed sensing literature and are directly related to the modeling assumptions regarding x (for example, Theorem 8.3 in Cohen et al. (2009)).
",4. Theoretical Analysis,[0],[0]
"Our next lemma shows that random Gaussian matrices satisfy the S-REC (over the range of Lipschitz generative model functions) and RIP conditions with high probability for G with bounded domain, both of which together are sufficient conditions for Lemma 1 to hold.
",4. Theoretical Analysis,[0],[0]
Lemma 2.,4. Theoretical Analysis,[0],[0]
LetG : Bk(r)→ Rn be anL-Lipschitz function where Bk(r) = {z | z ∈,4. Theoretical Analysis,[0],[0]
"Rk, ‖z‖2 ≤ r} is the `2-norm ball in Rk.",4. Theoretical Analysis,[0],[0]
"For α ∈ (0, 1), if
m = O
( 1
α2
( k log ( Lr
δ
)",4. Theoretical Analysis,[0],[0]
+,4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
"then a random matrix A ∈ Rm×n with i.i.d. entries such that Aij ∼ N ( 0, 1m ) satisfies the S-REC(S1.5l,G, 1− α, δ) and RIP(2l, α) with 1− e−Ω(α2m) probability.
",4. Theoretical Analysis,[0],[0]
"Using Lemma 1 and Lemma 2, we can bound the error due to decoding with generative models and random Gaussian measurement matrices in the following result.
",4. Theoretical Analysis,[0],[0]
Theorem 1.,4. Theoretical Analysis,[0],[0]
Let G : Bk(r)→,4. Theoretical Analysis,[0],[0]
Rn be an L-Lipschitz function.,4. Theoretical Analysis,[0],[0]
"For any α ∈ (0, 1), l > 0, let A ∈ Rm×n be a random Gaussian matrix with
m = O
( 1
α2
( k log ( Lr
δ
) +",4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
rows of i.i.d.,4. Theoretical Analysis,[0],[0]
"entries scaled such that Ai,j ∼ N(0, 1/m).",4. Theoretical Analysis,[0],[0]
Let ∆ be the decoder satisfying Lemma 1.,4. Theoretical Analysis,[0],[0]
"Then, we have with 1− e−Ω(α2m) probability,
‖x−∆(Ax+ )",4. Theoretical Analysis,[0],[0]
"‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, ‖ ‖2 ≤ max, where C0, C1, γ, δ′ are constants defined in Lemma 1.
",4. Theoretical Analysis,[0],[0]
"From the above lemma, we see that the number of measurements needed to guarantee upper bounds on the reconstruction error of any signal with high probability depends on two terms.",4. Theoretical Analysis,[0],[0]
"The first term includes dependence on the Lipschitz constant L of the generative model function G. A high Lipschitz constant makes recovery harder (by requiring a larger
number of measurements), but only contributes logarithmically.",4. Theoretical Analysis,[0],[0]
"The second term, typical of results in sparse vector recovery, shows a logarithmic growth on the dimensionality n of the signals.",4. Theoretical Analysis,[0],[0]
"Ignoring logarithmic dependences and constants, recovery using Sparse-Gen requires about O(k + l) measurements for recovery.",4. Theoretical Analysis,[0],[0]
Note that Theorem 1 assumes access to an optimization oracle for decoding.,4. Theoretical Analysis,[0],[0]
"In practice, we consider the solutions returned by gradient-based optimization methods to a non-convex objective defined in Eq.",4. Theoretical Analysis,[0],[0]
"(11) that are not guaranteed to correspond to the optimal decoding in general.
",4. Theoretical Analysis,[0],[0]
"Finally, we obtain tighter bounds for the special case when G is expressed using a neural network with only ReLU activations.",4. Theoretical Analysis,[0],[0]
"These bounds do not rely explicitly on the Lipschitz constant L or require the domain of G to be bounded.
",4. Theoretical Analysis,[0],[0]
Theorem 2.,4. Theoretical Analysis,[0],[0]
"If G : Rk → Rn is a neural network of depth d with only ReLU activations and at most c nodes in each layer, then the guarantees of Theorem 1 hold for
m = O
( 1
α2
( (k + l)d log",4. Theoretical Analysis,[0],[0]
c+,4. Theoretical Analysis,[0],[0]
(k + l),4. Theoretical Analysis,[0],[0]
"log(n/l) )) .
",4. Theoretical Analysis,[0],[0]
"Our theoretical analysis formalizes the key properties of recovering signals using Sparse-Gen. As shown in Lemma 1, there exists a decoder for recovery based on such modeling assumptions that extends recovery guarantees based on vanilla sparse vector recovery and generative model-based recovery.",4. Theoretical Analysis,[0],[0]
Such recovery requires measurement matrices that satisfy both the RIP and S-REC conditions over the set of vectors that deviate in sparse directions from the range of a generative model function.,4. Theoretical Analysis,[0],[0]
"In Theorems 1-2, we observed that the number of measurements required to guarantee recovery with high probability grow almost linearly (with some logarithmic terms) with the latent space dimensionality k of the generative model and the permissible sparsity l for deviating from the range of the generative model.",4. Theoretical Analysis,[0],[0]
We evaluated Sparse-Gen for compressed sensing of highdimensional signals from the domain of benchmark image datasets.,5. Experimental Evaluation,[0],[0]
"Specifically, we considered the MNIST dataset of handwritten digits (LeCun et al., 2010) and the OMNIGLOT dataset of handwritten characters (Lake et al., 2015).",5. Experimental Evaluation,[0],[0]
"Both these datasets have the same data dimensionality (28× 28), but significantly different characteristics.",5. Experimental Evaluation,[0],[0]
The MNIST dataset has fewer classes (10 digits from 0-9) as opposed to Omniglot which shows greater diversity (1623 characters across 50 alphabets).,5. Experimental Evaluation,[0],[0]
"Additional experiments with generative adversarial networks on the CelebA dataset are reported in the Appendix.
Baselines.",5. Experimental Evaluation,[0],[0]
"We considered methods based on sparse vector recovery using LASSO (Tibshirani, 1996; Candès & Tao,
2005) and generative model based recovery using variational autoencoders (VAE) (Kingma & Welling, 2014; Bora et al., 2017).",5. Experimental Evaluation,[0],[0]
"For VAE training, we used the standard train/held-out splits of both datasets.",5. Experimental Evaluation,[0],[0]
Compressed sensing experiments that we report were performed on the entire test set of images.,5. Experimental Evaluation,[0],[0]
"The architecture and other hyperparameter details are given in the Appendix.
",5. Experimental Evaluation,[0],[0]
Experimental setup.,5. Experimental Evaluation,[0],[0]
"For the held-out set of instances, we artificially generated measurements y through a random matrix A ∈ Rm×n with entries sampled i.i.d.",5. Experimental Evaluation,[0],[0]
from a Gaussian with zero mean and standard deviation of 1/m. Measurement noise is sampled from zero mean and diagonal scalar covariance matrix with entries as 0.01.,5. Experimental Evaluation,[0],[0]
"For evaluation, we report the reconstruction error measured as ‖x̂− x‖p where x̂ is the recovered signal and p is a norm of interest, varying the number of measurementsm from 50 to the highest value of 750.",5. Experimental Evaluation,[0],[0]
"We report results for the p = {1, 2,∞} norms.
",5. Experimental Evaluation,[0],[0]
"We evaluated sensing of both continuous signals (MNIST) with pixel values in range [0, 1] and discrete signals (Omniglot) with binary pixel values {0, 1}.",5. Experimental Evaluation,[0],[0]
"For all algorithms considered, recovery was performed by optimizing over a continuous space.",5. Experimental Evaluation,[0],[0]
"In the case of sparse recovery methods (including Sparse-Gen) it is possible that unconstrained optimization returns signals outside the domain of interest, in which case they are projected to the required domain by simple clipping, i.e., any signal less than zero is clipped to 0 and similarly any signal greater than one is clipped to 1.
Results and Discussion.",5. Experimental Evaluation,[0],[0]
The reconstruction errors for varying number of measurements are given in Figure 2.,5. Experimental Evaluation,[0],[0]
"Consistent with the theory, the strong prior in generative modelbased recovery methods outperforms the LASSO-based methods for sparse vector recovery.",5. Experimental Evaluation,[0],[0]
"In the regime of low measurements, the performance of algorithms that can incorporate the generative model prior dominates over methods modeling sparsity using LASSO.",5. Experimental Evaluation,[0],[0]
"The performance of plain generative model-based methods however saturates with increasing measurements, unlike Sparse-Gen and LASSO which continue to shrink the error.",5. Experimental Evaluation,[0],[0]
"The trends are consistent for both MNIST and Omniglot, although we observe the relative magnitudes of errors in the case of Omniglot are much higher than that of MNIST.",5. Experimental Evaluation,[0],[0]
This is expected due to the increased diversity and variations of the structure of the signals being sensed in the case of Omniglot.,5. Experimental Evaluation,[0],[0]
We also observe the trends to be consistent across the various norms considered.,5. Experimental Evaluation,[0],[0]
One of the primary motivations for compressive sensing is to directly acquire the signals using few measurements.,5.1. Transfer compressed sensing,[0],[0]
"On the contrary, learning a deep generative model requires access to large amounts of training data.",5.1. Transfer compressed sensing,[0],[0]
"In several applications, getting the data for training a generative model might not be feasible.",5.1. Transfer compressed sensing,[0],[0]
"Hence, we test the generative model-based recovery on the novel task of transfer compressed sensing.
",5.1. Transfer compressed sensing,[0],[0]
Experimental setup.,5.1. Transfer compressed sensing,[0],[0]
We train the generative model on a source domain (assumed to be data-rich) and related to a data-hungry target domain we wish to sense.,5.1. Transfer compressed sensing,[0],[0]
"Given the matching dimensions of MNIST and Omniglot, we conduct experiments transferring from MNIST (source) to Omniglot (target) and vice versa.
Results and Discussion.",5.1. Transfer compressed sensing,[0],[0]
The reconstruction errors for the norms considered are given in Figure 3.,5.1. Transfer compressed sensing,[0],[0]
"For both the sourcetarget pairs, we observe that the Sparse-Gen consistently performs well.",5.1. Transfer compressed sensing,[0],[0]
Vanilla generative model-based recovery shows hardly an improvements with increasing measurements.,5.1. Transfer compressed sensing,[0],[0]
We can qualitatively see this phenomena for transferring from MNIST (source) to Omniglot (target) in Figure 4.,5.1. Transfer compressed sensing,[0],[0]
"With only m = 100 measurements, all models perform poorly and generative model based methods particularly continue to sense images similar to MNIST.",5.1. Transfer compressed sensing,[0],[0]
"On the other hand, there is a noticeable transition at m = 200 measurements for SparseVAE where it adapts better to the domain being sensed than plain generative model-based recovery and achieves lower reconstruction error.",5.1. Transfer compressed sensing,[0],[0]
"Since the introduction of compressed sensing over a decade ago, there has been a vast body of research studying various extensions and applications (Candès & Tao, 2005; Donoho,
2006; Candès et al., 2006).",6. Related Work,[0],[0]
"This work explores the effect of modeling different structural assumptions on signals in theory and practice.
",6. Related Work,[0],[0]
Themes around sparsity in a well-chosen basis has driven much of the research in this direction.,6. Related Work,[0],[0]
"For instance, the paradigm of model-based compressed sensing accounts for the interdependencies between the dimensions of a sparse data signal (Baraniuk et al., 2010; Duarte & Eldar, 2011; Gilbert et al., 2017).",6. Related Work,[0],[0]
"Alternatively, adaptive selection of basis vectors from a dictionary that best capture the structure of the particular signal being sensed has also been explored (Peyre, 2010; Tang et al., 2013).",6. Related Work,[0],[0]
"Many of these methods have been extended to recovery of structured tensors (Zhang et al., 2013; 2014).",6. Related Work,[0],[0]
"In another prominent line of research involving Bayesian compressed sensing, the sparseness assumption is formalized by placing sparsenesspromoting priors on the signals (Ji et al., 2008; He & Carin, 2009; Babacan et al., 2010; Baron et al., 2010).
",6. Related Work,[0],[0]
Research exploring structure beyond sparsity is relatively scarce.,6. Related Work,[0],[0]
Early works in this direction can be traced to Baraniuk & Wakin (2009) who proposed algorithms for recovering signals lying on a smooth manifold.,6. Related Work,[0],[0]
The generative model-based recovery methods consider functions that do not necessarily define manifolds since the range of a generator function could intersect with itself.,6. Related Work,[0],[0]
"Yu & Sapiro (2011) coined the term statistical compressed sensing and proposed
algorithms for efficient sensing of signals from a mixture of Gaussians.",6. Related Work,[0],[0]
The recent work in deep generative model-based recovery differs in key theoretical aspects as well in the use of a more expressive family of models based on neural networks.,6. Related Work,[0],[0]
A related recent work by Hand & Voroninski (2017) provides theoretical guarantees on the solution recovered for solving non-convex linear inverse problems with deep generative priors.,6. Related Work,[0],[0]
"Empirical advances based on well-designed deep neural network architectures that sacrifice many of the theoretical guarantees have been proposed for applications such as MRI (Mardani et al., 2017; 2018).",6. Related Work,[0],[0]
"Many recent methods propose to learn mappings of signals to measurements using neural networks, instead of restricting them to be linear, random matrices (Mousavi et al., 2015; Kulkarni et al., 2016; Chang et al., 2017; Lu et al., 2018).
",6. Related Work,[0],[0]
"Our proposed framework bridges the gap between algorithms that model structure using sparsity and enjoy good theoretical properties with advances in deep generative models, in particular their use for compressed sensing.",6. Related Work,[0],[0]
"The use of deep generative models as priors for compressed sensing presents a new outlook on algorithms for inexpen-
sive data acquisition.",7. Conclusion and Future Work,[0],[0]
"In this work, we showed that these priors can be used in conjunction with classical modeling assumptions based on sparsity.",7. Conclusion and Future Work,[0],[0]
"Our proposed framework, Sparse-Gen, generalizes both sparse vector recovery and recovery using generative models by allowing for sparse deviations from the range of a generative model function.",7. Conclusion and Future Work,[0],[0]
"The benefits of using such modeling assumptions are observed both theoretically and empirically.
",7. Conclusion and Future Work,[0],[0]
"In the future, we would like to design algorithms that can better model the structure within sparse deviations.",7. Conclusion and Future Work,[0],[0]
"Followup work in this direction can benefit from the vast body of prior work in structured sparse vector recovery (Duarte & Eldar, 2011).",7. Conclusion and Future Work,[0],[0]
"From a theoretical perspective, a better understanding of the non-convexity resulting from generative model-based recovery can lead to stronger guarantees and consequently better optimization algorithms for recovery.",7. Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to extend Sparse-Gen for compressed sensing of other data modalities such as graphs for applications in network tomography and reconstruction (Xu et al., 2011).",7. Conclusion and Future Work,[0],[0]
"Real-world graph networks are typically sparse in the canonical basis and can be modeled effectively using deep generative models (Grover et al., 2018), which is consistent with the modeling assumptions of the Sparse-Gen framework.",7. Conclusion and Future Work,[0],[0]
"We are thankful to Tri Dao, Jonathan Kuck, Daniel Levy, Aditi Raghunathan, and Yang Song for helpful comments on early drafts.",Acknowledgements,[0],[0]
"This research was supported by Intel Corporation, TRI, a Hellman Faculty Fellowship, ONR, NSF (#1651565, #1522054, #1733686 ) and FLI (#2017-158687).",Acknowledgements,[0],[0]
AG is supported by a Microsoft Research PhD Fellowship.,Acknowledgements,[0],[0]
"In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal.",abstractText,[0],[0]
"Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model.",abstractText,[0],[0]
A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements.,abstractText,[0],[0]
"However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined.",abstractText,[0],[0]
"We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals.",abstractText,[0],[0]
"Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior.",abstractText,[0],[0]
"Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",abstractText,[0],[0]
Modeling Sparse Deviations for Compressed Sensing using Generative Models,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 391–399, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
391",text,[0],[0]
"The language understanding (LU) module is a key component of dialogue system (DS), parsing user’s utterances into corresponding semantic concepts (or semantic slots 1).",1 Introduction,[0],[0]
"For example, the utterance “Show me flights from Boston to New York” can be parsed into (from city=Boston, to city=New York) (Pieraccini et al., 1992).",1 Introduction,[0],[0]
"Typically, the LU is seen as a plain slot filling task.
",1 Introduction,[0],[0]
∗The corresponding author is Kai Yu.,1 Introduction,[0],[0]
1Slot and concept are equal in LU.,1 Introduction,[0],[0]
"They will be mixed in
the rest of this paper to some extent.
",1 Introduction,[0],[0]
"With sufficient in-domain data and deep learning models (e.g. recurrent neural networks, bidirectional long-short term memory network), statistical methods have achieved satisfactory performance in the slot filling task recently (Kurata et al., 2016; Vu, 2016; Liu and Lane, 2016).
",1 Introduction,[0],[0]
"However, retrieving sufficient in-domain data for training LU model (Tur et al., 2010) is unrealistic, especially when the semantic slot extends or dialogue domain changes.",1 Introduction,[0],[0]
"The ability of LU approaches to cope with changed domains and limited data is a key to the deployment of commercial dialogue systems (e.g. Apple Siri, Amazon Alexa, Google Home, Microsoft Cortana etc).
",1 Introduction,[0],[0]
"In this paper, we investigate substructure of semantic slots to find out slot relations and promote data reuse.",1 Introduction,[0],[0]
"We represent semantic slots with a hierarchical structure based on atomic concept tuple, as shown in Figure 1.",1 Introduction,[0],[0]
"Each semantic slot is composed of different atomic concepts, e.g. slot “from city” can be defined as a tuple of atoms [“from location”,“city name”],
and “date of birth” can be defined as [“date”,“birth”].
",1 Introduction,[0],[0]
"Unlike the traditional slot definition on a plain level, modeling on the atomic concepts helps identify linguistic patterns of related slots by atom sharing, and even decrease the required amount of training data.",1 Introduction,[0],[0]
"For example, the training and test sets are unmatched in Figure 2, whereas the patterns of atomic concepts (e.g. “from”, “to”, “city”) can be shared.
",1 Introduction,[0],[0]
"In this paper, we investigate the slot filling task switching from plain slots to hierarchical structures by proposing the novel atomic concept tuples which are constructed manually.",1 Introduction,[0],[0]
"For comparison, we also introduce a competitive method which automatically learns slot representation from the word sequence of each slot name.",1 Introduction,[0],[0]
"Our methods are applied to value set mismatch and domain adaptation problems on ATIS (Hemphill et al., 1995) and DSTC 2&3 (Henderson et al., 2013) respectively.",1 Introduction,[0],[0]
"As shown in the experimental results, the slot-filling based on concept transfer learning is effective in solving the value set mismatch and domain adaptation problems.",1 Introduction,[0],[0]
"The concept transfer learning method especially achieves state-of-theart performance (F1-score 96.08%) on the ATIS task.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
The next section is about the relation to prior work.,1 Introduction,[0],[0]
The atomic concept tuple is introduced in section 3.,1 Introduction,[0],[0]
The proposed concept transfer learning is then described in section 4.,1 Introduction,[0],[0]
Section 5 describes a competitive method with slot embedding derived from the literal descriptions of slot names.,1 Introduction,[0],[0]
"In section 6, the proposed approach is evaluated on the value set mismatch and domain adaptation problems.",1 Introduction,[0],[0]
"Finally, our conclusions are presented in section 7.",1 Introduction,[0],[0]
Slot Filling in LU Zettlemoyer and Collins (2007) proposed a grammar induction method by learning a Probabilistic Combinatory Categorial Grammar (PCCG) from logical-form annotations.,2 Related Work,[0],[0]
"As a
grammar-based method, PCCG is close to a hierarchical concepts structure in grammar generation and combination.",2 Related Work,[0],[0]
"But this grammar-based method does not possess high generalization capability for atomic concept sharing, and heavily depends on a well-defined lexicon set.
",2 Related Work,[0],[0]
Recent research on statistical slot filling in LU has been focused on the Recurrent Neural Network (RNN) and its extensions.,2 Related Work,[0],[0]
"At first, RNN outperformed CRF (Conditional Random Field) on the ATIS dataset (Yao et al., 2013; Mesnil et al., 2013).",2 Related Work,[0],[0]
"Long-short term memory network (LSTM) was introduced to obtain a marginal improvement over RNN (Yao et al., 2014).",2 Related Work,[0],[0]
"After that, many RNN variations were proposed: encoder-labeler model (Kurata et al., 2016), attention model (Liu and Lane, 2016; Zhu and Yu, 2017) etc.",2 Related Work,[0],[0]
"However, these work only predicted the plain semantic slot, not the structure of atomic concepts.
",2 Related Work,[0],[0]
"Domain Adaptation in LU For the domain adaptation in LU, Zhu et al. (2014) proposed generating spoken language surface forms by using patterns of the source domain and the ontology of the target domain.",2 Related Work,[0],[0]
"With regard to the unsupervised LU, Heck and Hakkani-Tur (2012) exploited the structure of semantic knowledge graphs from the web to create natural language surface forms of entity-relation-entity portions of knowledge graphs.",2 Related Work,[0],[0]
"For the zero-shot learning of LU, Ferreira et al. (2015); Yazdani and Henderson (2015) proposed a model to calculate similarity scores between an input sentence and semantic items.",2 Related Work,[0],[0]
"In this paper, we focus on the extension of slots with limited seed data.",2 Related Work,[0],[0]
"Although concept definition is one of the most crucial problems of LU, there is no unified surface form for the domain ontology.",3 Atomic Concept Tuples,[0],[0]
"Even for the same semantic slot, names of this slot may be quite different.",3 Atomic Concept Tuples,[0],[0]
"For example, the city where the flight departs may be called “from city”, “depart city” or “from loc.city name”.",3 Atomic Concept Tuples,[0],[0]
"Ontology definitions from different groups may be similar but not consistent, which is not convenient for data reuse.",3 Atomic Concept Tuples,[0],[0]
"Meanwhile, semantic slots defined in traditional LU systems are on a plain level, while there is no structure to indicate their relation.
",3 Atomic Concept Tuples,[0],[0]
"To solve this problem, we propose to use atomic concepts to represent the semantic slots.",3 Atomic Concept Tuples,[0],[0]
Atomic concepts are exploited to break down the slots.,3 Atomic Concept Tuples,[0],[0]
"We
represent the semantic slots as atomic concept tuples (Figure 1 is an example).",3 Atomic Concept Tuples,[0],[0]
"The semantic slot composed of these atomic concepts can keep a unified resource for concept definition and extend the semantic knowledge flexibly.
",3 Atomic Concept Tuples,[0],[0]
We propose a criteria to construct atomic concept manually.,3 Atomic Concept Tuples,[0],[0]
"For a given vocabulary C of the atomic concepts, a semantic slot s can be represented by a tuple",3 Atomic Concept Tuples,[0],[0]
"[c1, c2, ..., ck], where ci ∈ C is in the i-th dimension and k is tuple length.",3 Atomic Concept Tuples,[0],[0]
"In particular, a “null” atom is introduced for each dimension.",3 Atomic Concept Tuples,[0],[0]
Table 1 illustrates an example of slot representation on the ATIS task.,3 Atomic Concept Tuples,[0],[0]
"To avoid a scratch concept branch, we make a constraint:
Ci ∩ Cj = {null}, 1 ≤ i 6= j ≤ k
where Ci (1 ≤ i ≤ k) denotes all possible atomic concepts which exist in dimension i (i.e. ci ∈ Ci).",3 Atomic Concept Tuples,[0],[0]
"The concept tuple is ordered.
",3 Atomic Concept Tuples,[0],[0]
"In general, atomic concepts can be classified into two categories, one is value-aware and the other is context-aware.",3 Atomic Concept Tuples,[0],[0]
The principle for defining slot as a concept branch is: lower dimension less context-aware.,3 Atomic Concept Tuples,[0],[0]
"For example, “city name” and “airport name” depend on rare context (valueaware).",3 Atomic Concept Tuples,[0],[0]
They should be located in the first dimension.,3 Atomic Concept Tuples,[0],[0]
"“from location” depends on the context like a pattern of “a flight leaves [city name]”, which should be in the second dimension.",3 Atomic Concept Tuples,[0],[0]
"The atomic concept tuple shows the inner relation between different semantic slots explicitly.
",3 Atomic Concept Tuples,[0],[0]
"Therefore, the procedure of constructing atomic concept tuples for slots can be divided into the following steps.
",3 Atomic Concept Tuples,[0],[0]
"• Firstly, we build a vocabularyC of the atomic concepts for all the slots.",3 Atomic Concept Tuples,[0],[0]
"By analyzing the conceptual intersection of different slots, we can split the slots into smaller ones which are called atomic concepts.",3 Atomic Concept Tuples,[0],[0]
"After that, each slot is represented as a set of atomic concepts which are not ordered.
",3 Atomic Concept Tuples,[0],[0]
"• Secondly, we gather the atoms into different groups.",3 Atomic Concept Tuples,[0],[0]
Atomic concepts from the same group should be mutually exclusive.,3 Atomic Concept Tuples,[0],[0]
"Therefore we can investigate the inner relation and outer relation of these groups.
",3 Atomic Concept Tuples,[0],[0]
"• Finally, each group is associated with one dimension (Ci) of the atomic concept tuple.",3 Atomic Concept Tuples,[0],[0]
The groups are ordered depending on whether they are value-aware or contextaware.,3 Atomic Concept Tuples,[0],[0]
The slot filling is typically considered as a sequence labelling problem.,4 Concept Transfer Learning,[0],[0]
"In this paper, we only consider the sequence-labelling based slot filling task.",4 Concept Transfer Learning,[0],[0]
"The input (word) sequence is denoted by w = (w1, w2, ..., wN ), and the output (slot tag) sequence is denoted by s = (s1, s2, ..., sN ).",4 Concept Transfer Learning,[0],[0]
"Since a slot may be mapped to several continuous words, we follow the popular in/out/begin (IOB) representation (e.g. an example in Figure 3).
",4 Concept Transfer Learning,[0],[0]
"The typical slot filling task predicts a plain slot sequence given a word sequence, dubbed as plain slot-filling (PS).
",4 Concept Transfer Learning,[0],[0]
"In this paper, the popular bidirectional LSTMRNN (BLSTM) is used to model the sequence labeling problem (Graves, 2012).",4 Concept Transfer Learning,[0],[0]
It can be exploited to capture both past and future features for a specific time frame.,4 Concept Transfer Learning,[0],[0]
"The BLSTM reads the input sentence w and generates N hidden states hi = ←− hi ⊕ −→ hi , i ∈ {1, .., N}:
←−",4 Concept Transfer Learning,[0],[0]
hi = b,4 Concept Transfer Learning,[0],[0]
"( ←−− hi+1, ewi);",4 Concept Transfer Learning,[0],[0]
−→ hi = f,4 Concept Transfer Learning,[0],[0]
"( −−→ hi−1, ewi)
where ←− hi is the hidden vector of the backward pass in BLSTM and −→ hi is the hidden vector of the forward pass in BLSTM at time i, b and f are LSTM units of the backward and forward passes respectively, ew denotes the word embedding for each word w, and ⊕ denotes the vector concatenation operation.",4 Concept Transfer Learning,[0],[0]
We write the entire operation as a mapping BLSTMΘw,4 Concept Transfer Learning,[0],[0]
"(Θw refers to the parameters):
(h1...hN ) = BLSTMΘw(w1...wN ) (1)
Therefore, the plain slot filling defines a distribution over slot tag sequences given an input word
sequence:
p(s|w) = N∏ i=1 p(si|hi)
= N∏ i=1",4 Concept Transfer Learning,[0],[0]
"softmax(Wo · hi)T δsi
(2)
where the matrix Wo (output layer) consists of the vector representations of each slot tag, the symbol",4 Concept Transfer Learning,[0],[0]
"δd is a Kronecker delta with a dimension for each slot tag, and the softmax function is used to estimate the probability distribution over all possible plain slots.",4 Concept Transfer Learning,[0],[0]
The slot is indicated as an atomic concept tuple based on hierarchical concept structure.,4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"Slot filling is considered as a concept-tuple labelling task.
",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"(a) Atomic concept independent Slot filling can be transferred to a multi-task sequence labelling problem, regarding these atomic concepts independently (i.e. AC).",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
Each task predicts one atomic concept by a respective output layer.,4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"Thus, the slot filling problem can be formulated as
p(s|w) = N∏ i=1",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"[p(IOBi|hi) k∏ j=1 p(cij |hi)]
where the semantic slot si is represented by an atomic concept branch [ci1, ci2, ..., cik], and IOBi is the IOB schema tag at time i. As illustrated in Figure 4(a), the semantic slot “from city” can be represented as [“city name”,“from loc”].",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"The
prediction of IOB is regarded as another task specifically.",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"All tasks share the same parameters except for the output layers.
",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
(b) Atomic concept dependent Atomic concepts can also be regarded dependently (i.e. ACD) so that atomic concept prediction depends on the former predicted results.,4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"The slot filling problem can be formulated as
p(s|w)
= N∏ i=1",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"[p(IOBi|hi)p(ci1|hi) k∏ j=2 p(cij |hi, ci,1:j−1)]
where ci,1:j−1 = (ci,1, ..., ci,j−1) is the predicted result of former atomic concepts of slot tag si, indicating a structured multi-task learning framework.
",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"In this paper, we make some simplifications on concept dependence.",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"We predict atomic concept only based on the last atomic concept, as shown in Figure 4(b).",4.1 Atomic-Concepts Based Slot Filling,[0],[0]
"Since our approach is a structured multi-task learning problem, the model loss is summed over each task during training.",4.2 Training and Decoding,[0],[0]
"For the domain adaptation, we firstly gather training data from the source domain and seed data from the target domain to be a union set.",4.2 Training and Decoding,[0],[0]
"Subsequently, the union data is fed into the slot filling model.
",4.2 Training and Decoding,[0],[0]
"During the decoding stage, we combine predicted atomic concepts with probability multiplication.",4.2 Training and Decoding,[0],[0]
The evaluation is made on the top-best hypothesis.,4.2 Training and Decoding,[0],[0]
"Although the atomic-concepts based slot
filling may predict an unseen slot.",4.2 Training and Decoding,[0],[0]
We didn’t perform any post-processing but considered the unseen slot as a wrong prediction.,4.2 Training and Decoding,[0],[0]
"In the section, we introduce a competitive system which uses the literal description of the slot as an input of the slot filling model.",5 Literal Description of Slot Name,[0],[0]
"The literal description of slot used in this paper is the word sequence of each slot name, which can be obtained automatically.",5 Literal Description of Slot Name,[0],[0]
"As the names of relative slots may include the same or similar word, the word sequence of slot name can also help reveal the relation between different slots.",5 Literal Description of Slot Name,[0],[0]
"Therefore, it is very meaningful to compare this method with the atomic concept tuples involving human knowledge.
",5 Literal Description of Slot Name,[0],[0]
The architecture of this competitive system is illustrated in Figure 5.,5 Literal Description of Slot Name,[0],[0]
"First, it assumes that each slot name is a meaningful natural language description so that the slot filling task is tractable from the input word sequence and slot name.",5 Literal Description of Slot Name,[0],[0]
"Second, another BLSTM model is applied to derive softmax embedding from the slot names.",5 Literal Description of Slot Name,[0],[0]
"In this method, we also split the slot filling task into IOB tag prediction and slot name prediction.",5 Literal Description of Slot Name,[0],[0]
"In other words, the slot tag si is broken down into IOBi and slot name SNi, e.g. the slot tag “B-from city” is split into “B” and “from city”.",5 Literal Description of Slot Name,[0],[0]
"The details are indicated below.
",5 Literal Description of Slot Name,[0],[0]
"With the BLSTM applied on the input sequence, we have hidden vectors hi, i ∈ {1, .., N} as shown in Eqn.",5 Literal Description of Slot Name,[0],[0]
(1).,5 Literal Description of Slot Name,[0],[0]
"This model redefines the distribution
over slot tag sequences given an input word sequence, compared with Eqn.",5 Literal Description of Slot Name,[0],[0]
"(2):
p(s|w) = N∏ i=1 p(IOBi|hi)p(SNi|hi)
where p(IOBi|hi) predicts the IOB tag and p(SNi|hi) makes a prediction for the slot name.",5 Literal Description of Slot Name,[0],[0]
"We define
p(SNi|hi) =",5 Literal Description of Slot Name,[0],[0]
"softmax(W · hi)T δSNi
where W ∈ RA×B is a matrix, hi ∈ RB is a vector, A is the number of all different slot names.",5 Literal Description of Slot Name,[0],[0]
"The matrix W consists of the embedding of each slot name (i.e. each row vector of W with length B).
",5 Literal Description of Slot Name,[0],[0]
"To capture the slot relation within different slot names, we apply another BLSTM model (as shown in the orange dotted circle of Figure 5) onto the word sequence (literal description) of each slot name.",5 Literal Description of Slot Name,[0],[0]
"For the j-th slot name (j ∈ {1, .., A}) with a word sequence xj = (xj1, ..., x j Nj ), we have
←− vjn = lstm b( ←−− vjn+1, exjn);",5 Literal Description of Slot Name,[0],[0]
"−→ vjn = lstm f ( −−→ vjn−1, exjn)
where ←− vjn is the hidden vector of the backward pass and −→ vjn is the hidden vector of the forward pass at time n",5 Literal Description of Slot Name,[0],[0]
"(n ∈ {1, .., Nj}), ex denotes the word embedding for each word x.",5 Literal Description of Slot Name,[0],[0]
"We take the tails of both backward and forward pass as the slot embedding, i.e.
Wj = ←− vj1 ⊕ −→ vjNj
where Wj is the j-th row vector of matrix W .",5 Literal Description of Slot Name,[0],[0]
The relative slots using the same or similar word in slot naming will be close in the space of slot embedding inherently.,5 Literal Description of Slot Name,[0],[0]
"Therefore, this method is a competitive system to the atomic concept tuples.",5 Literal Description of Slot Name,[0],[0]
We will show the comparison in the following section.,5 Literal Description of Slot Name,[0],[0]
"We evaluate our atomic-concept methods on two tasks: value set mismatch and domain adaptation.
",6 Experiments,[0],[0]
Value set mismatch task evaluates the generalization capability of different slot filling models.,6 Experiments,[0],[0]
"In a language understanding (LU) system, each slot has a value set with all possible values which can be assigned to it.",6 Experiments,[0],[0]
"Since the semantically annotated data is always limited, only a part of values
is seen in the training data.",6 Experiments,[0],[0]
Will the slot filling model perform well on the unseen values?,6 Experiments,[0],[0]
"To answer this question, we synthesize a test set by the values mismatched with the training set of ATIS corpus.",6 Experiments,[0],[0]
"Our methods may take advantages of the prior knowledge about slot relations based on the atomic concepts and the literal descriptions of slot names.
",6 Experiments,[0],[0]
Domain adaptation task evaluates the adaptation capability of our methods when they meet new slots in the target domain.,6 Experiments,[0],[0]
"In this task, a seed training set of the target domain is provided.",6 Experiments,[0],[0]
"However, it is very limited: 1) some new slots may not be covered; 2) not all contexts are covered for each new slot.",6 Experiments,[0],[0]
The atomic-concepts based method would alleviate this problem.,6 Experiments,[0],[0]
Each slot is defined as a tuple of atomic concepts in our method.,6 Experiments,[0],[0]
"Therefore, it is possible to learn an unseen slot of the target domain if its atomic concepts exist in the data of the source domain and the seed data of the target domain.",6 Experiments,[0],[0]
It is also possible to see more contexts for a new slot if its atomic concepts exist in the source domain which has much more data.,6 Experiments,[0],[0]
ATIS corpus has been widely used as a benchmark by the LU community.,6.1 Value Set Mismatch,[0],[0]
"The training data consists of 4978 sentences and the test data consists of 893 sentences.
",6.1 Value Set Mismatch,[0],[0]
"In this task, we perform an adaptation for unmatched training and test sets, in which there are many unseen slot-value pairs in the test set (Figure 2 is an example).",6.1 Value Set Mismatch,[0],[0]
It is a common problem in the development of commercial dialogue system since it is impossible to collect data covering all possible slot-value pairs.,6.1 Value Set Mismatch,[0],[0]
"We simulate this problem on the ATIS dataset (Hemphill et al., 1995) by creating an unmatched test set (ATIS X test).
",6.1 Value Set Mismatch,[0],[0]
ATIS X test is synthesized from the standard ATIS test set by randomly replacing the value of each slot with an unseen one.,6.1 Value Set Mismatch,[0],[0]
"The unseen value sets are collected from the training set according to bottom-level concepts (e.g. “city name”, “airport name”).",6.1 Value Set Mismatch,[0],[0]
"For example, if the value set of “from city” is {“New York”, “Boston”} and the value set of “to city” is {“Boston”}, then the unseen value for “to city” is “New York”.",6.1 Value Set Mismatch,[0],[0]
The test sentence “Flights to [xx:to city]” can be replaced to “Flights to [New York:to city]”.,6.1 Value Set Mismatch,[0],[0]
"Finally, the ATIS X test gets the same sentence number to the standard ATIS test set.",6.1 Value Set Mismatch,[0],[0]
We randomly selected 80% of the training data for model training and the remaining 20% for validation.,6.1.1 Experimental Settings,[0],[0]
We deal with unseen words in the test set by marking any words with only one single occurrence in the training set as 〈unk〉.,6.1.1 Experimental Settings,[0],[0]
"We also converted sequences of numbers to the string DIGIT, e.g. 1990 is converted to DIGIT*4 (Zhang and Wang, 2016).",6.1.1 Experimental Settings,[0],[0]
"Regarding BLSTM model, we set the dimension of word embeddings to 100 and the number of hidden units to 100.",6.1.1 Experimental Settings,[0],[0]
"For training, the network parameters are randomly initialized in accordance with the uniform distribution (-0.2, 0.2).",6.1.1 Experimental Settings,[0],[0]
Stochastic gradient descent (SGD) is used for updating parameters.,6.1.1 Experimental Settings,[0],[0]
"The dropout with a probability of 0.5 is applied to the non-recurrent connections during the training stage.
",6.1.1 Experimental Settings,[0],[0]
"We try different learning rates by grid-search in range of [0.008, 0.04].",6.1.1 Experimental Settings,[0],[0]
We keep the learning rate for 100 epochs and save the parameters that give the best performance on the validation set.,6.1.1 Experimental Settings,[0],[0]
"Finally, we report the F1-score of the semantic slots on the test set with parameters that have achieved the best F1-score on the validation set.",6.1.1 Experimental Settings,[0],[0]
The F1-score is calculated using CoNLL evaluation script.,6.1.1 Experimental Settings,[0],[0]
2,6.1.1 Experimental Settings,[0],[0]
Table 2 summarizes the recently published results on the ATIS slot filling task and compares them with the results of our proposed methods on the standard ATIS test set.,6.1.2 Experimental Results and Analysis,[0],[0]
We can see that RNN outperforms CRF because of the ability to capture long-term dependencies.,6.1.2 Experimental Results and Analysis,[0],[0]
LSTM beats RNN by solving the problem of vanishing or exploding gradients.,6.1.2 Experimental Results and Analysis,[0],[0]
BLSTM further improves the result by considering both the past and future features.,6.1.2 Experimental Results and Analysis,[0],[0]
Encoder-decoder achieves the state-of-theart performance by modeling the label dependencies.,6.1.2 Experimental Results and Analysis,[0],[0]
Encoder-labeler is a similar method to the Encoder-decoder.,6.1.2 Experimental Results and Analysis,[0],[0]
"These systems are designed to predict the plain semantic slots traditionally.
",6.1.2 Experimental Results and Analysis,[0],[0]
"Compared with the published results, our method outperforms the previously published F1score, illustrated in Table 2.",6.1.2 Experimental Results and Analysis,[0],[0]
AC gets a marginal improvement (+0.15%) over PS by predicting the atomic concepts independently instead of the plain slots.,6.1.2 Experimental Results and Analysis,[0],[0]
"Moreover, ACD predicts the atomic concepts dependently, gains 0.50% (significant level 95%) over the AC.",6.1.2 Experimental Results and Analysis,[0],[0]
"Worth to mention that ACD achieves a new state-of-the-art performance of the
2http://www.cnts.ua.ac.be/conll2000/chunking/output.html
standard slot-tagging task on the ATIS dataset, with only the lexicon features 3.
",6.1.2 Experimental Results and Analysis,[0],[0]
Our methods are also tested on the ATIS X test to measure the ability of generalization.,6.1.2 Experimental Results and Analysis,[0],[0]
"For comparison, we also apply dictionary features (ngram indication) of value sets (e.g. some kind of gazetteers) collected from training data into the PS model (i.e. PS+dict-feats in Table 2).",6.1.2 Experimental Results and Analysis,[0],[0]
"From Table 2, we can see that: 1) The plain slot filling models (PS, Encoder-decoder) are not on par with other models.",6.1.2 Experimental Results and Analysis,[0],[0]
2),6.1.2 Experimental Results and Analysis,[0],[0]
"The atomic-concepts based slot filling gets a slight improvement over the PS with dict-feats, considering the concepts independently (AC).",6.1.2 Experimental Results and Analysis,[0],[0]
3),6.1.2 Experimental Results and Analysis,[0],[0]
"The atomic-concepts based slot fillings (ACD gains a large margin over AC, considering the concepts dependently.",6.1.2 Experimental Results and Analysis,[0],[0]
"4) The method based on slot name embedding (described in Section 5) achieves a slight improvement than AC, which implies that it is possible to reveal the relationship between slots automatically.
",6.1.2 Experimental Results and Analysis,[0],[0]
"3There are other published results that achieved better performance by using Name Entity features, e.g. Mesnil et al. (2013) got 96.24% F1-score.",6.1.2 Experimental Results and Analysis,[0],[0]
The NE features are manually annotated and strong information.,6.1.2 Experimental Results and Analysis,[0],[0]
So it would be more meaningful to use only lexicon features.,6.1.2 Experimental Results and Analysis,[0],[0]
"Meanwhile, several other works can obtain competitive results by using the intent classification as another task for joint training, e.g. Liu and Lane (2016) achieved 95.98% F1-score.",6.1.2 Experimental Results and Analysis,[0],[0]
"In this paper, we consider the slot filling task only.
",6.1.2 Experimental Results and Analysis,[0],[0]
"Case study: As illustrated in Table 3, the plain slot filling (PS) predicts the label of “late” wrongly, whereas the atomic-concepts based slot fillings (i.e. AC and ACD) get the accurate annotation.",6.1.2 Experimental Results and Analysis,[0],[0]
The word of “late” is never covered by the slot “period of day” in the training set.,6.1.2 Experimental Results and Analysis,[0],[0]
It is hard for the plain slot filling (PS) to predict an unseen mapping correctly.,6.1.2 Experimental Results and Analysis,[0],[0]
"Luckily, the “late” is covered by the family of the slot “period of day” in the training set, e.g. “arrive time.period of day”.",6.1.2 Experimental Results and Analysis,[0],[0]
"Therefore, AC and ACD can learn this by modeling the atomic concepts separately.",6.1.2 Experimental Results and Analysis,[0],[0]
"Our methods are also evaluated on the DSTC 2&3 task (Henderson et al., 2013) which is considered to be a realistic domain adaptation problem.
",6.2 Domain Adaptation,[0],[0]
DSTC 2 (source domain) comprises of dialogues from the restaurant information domain in Cambridge.,6.2 Domain Adaptation,[0],[0]
"We use the dstc2 train set (1612 dialogues) for training and the dstc2 dev (506 dialogues) for validation.
",6.2 Domain Adaptation,[0],[0]
"DSTC 3 (target domain) introduces the tourist information domain about restaurant, pubs and coffee shops in Cambridge, which is an extension of DSTC 2.",6.2 Domain Adaptation,[0],[0]
"We use seed data dstc3 seed (only 11 dialogues) as the training set of the target domain.
",6.2 Domain Adaptation,[0],[0]
"DSTC3 S test: In this paper, we focus on three new semantic slots: “has tv, has internet, children allowed”.",6.2 Domain Adaptation,[0],[0]
4 They only exist in the DSTC 3 dataset and have few appearances in the seed data.,6.2 Domain Adaptation,[0],[0]
"A test set is chosen for specific evaluation on these new semantic slots, by gathering all the sentences (688 sentences) whose annotation contains these three slots and randomly selecting 1000 sentences irrelevant to these three slots from the dstc3 test set.",6.2 Domain Adaptation,[0],[0]
"This test set is named as DSTC3 S test (1688 sentences).
",6.2 Domain Adaptation,[0],[0]
"The union of a slot and action is taken as a plain semantic slot (e.g. “confirm.food=Chinese”), since each slot is tied with an action (e.g. “inform”, “deny” and “confirm”) in DSTC 2&3.",6.2 Domain Adaptation,[0],[0]
The slot and action are taken as atomic concepts.,6.2 Domain Adaptation,[0],[0]
"For the slot filling task, only the semantic annotation with aligned information is kept, e.g. the semantic tuple “request(phone)” is ignored.",6.2 Domain Adaptation,[0],[0]
"We use transcripts as input, and make slot-value alignment by
4For each slot of “has tv, has internet, children allowed”, the semantic annotation “request(slot)” is replaced with “confirm(slot=True)”.",6.2 Domain Adaptation,[0],[0]
"Then we have the slot-tagging format, e.g. ”does it have [television:confirm.has tv]”.
string matching simply.",6.2 Domain Adaptation,[0],[0]
"The experimental settings are similar to the ATIS’s, whereas the seed data in DSTC 3 is also used for validation.
",6.2.1 Experimental Results and Analysis,[0],[0]
The performance of our methods in the DSTC 2&3 task is illustrated in Table 4.,6.2.1 Experimental Results and Analysis,[0],[0]
"We can see that: 1) By incorporating the data of the source domain (dstc2 train), PS and AC achieve improvements respectively.",6.2.1 Experimental Results and Analysis,[0],[0]
2) AC gains more than PS by modeling the plain semantic slot as atomic concepts.,6.2.1 Experimental Results and Analysis,[0],[0]
The atomic concepts promote the associated slots to share input features for the same atoms.,6.2.1 Experimental Results and Analysis,[0],[0]
3),6.2.1 Experimental Results and Analysis,[0],[0]
The atomic-concepts based slot filling considering the concepts dependently (ACD) gains little (0.17%) over AC considering the concepts independently.,6.2.1 Experimental Results and Analysis,[0],[0]
"It may be due to the small size of dstc3 seed.
",6.2.1 Experimental Results and Analysis,[0],[0]
"Case study: Several cases from these models (trained on the union set of dstc2 train and dstc3 seed) are also chosen to explain why the atomic-concepts based slot filling outperforms the typical plain slot filling, as shown in Table 5.",6.2.1 Experimental Results and Analysis,[0],[0]
"From the above part of Table 5, we can see PS predicts a wrong slot.",6.2.1 Experimental Results and Analysis,[0],[0]
Because the grammar “does it have [something]” is only for the plain slot “confirm.hastv” in the seed data.,6.2.1 Experimental Results and Analysis,[0],[0]
"From the below part of Table 5, we can see that only ACD which considers the concepts dependently predicts the right slot.",6.2.1 Experimental Results and Analysis,[0],[0]
"Since “confirm.childrenallowed” never exists in the seed data, PS can’t learn patterns about it.",6.2.1 Experimental Results and Analysis,[0],[0]
"Limited by the quantity of the seed data, AC also doesn’t extract the semantics correctly.",6.2.1 Experimental Results and Analysis,[0],[0]
"To address data sparsity problem of language understanding (LU) task, we present a novel method of concept definition based on well-defined atomic concepts.",7 Conclusion,[0],[0]
We present the concept transfer learning for slot filling on the atomic concept level to solve the problem of adaptive LU.,7 Conclusion,[0],[0]
"The experiments on the ATIS and DSTC 2&3 datasets show our method obtains promising results and outperforms the traditional slot filling, due to the knowledge sharing of atomic concepts.
",7 Conclusion,[0],[0]
The atomic concepts are constructed manually in this paper.,7 Conclusion,[0],[0]
"In future work, we want to explore more flexible concept definition for concept transfer learning of LU.",7 Conclusion,[0],[0]
"Moreover, we also propose a competitive method based on slot name embedding which can be extracted from the literal description of the slot name automatically.",7 Conclusion,[0],[0]
The experimental result shows that it lays foundation for finding a more flexible concept definition method for adaptive LU.,7 Conclusion,[0],[0]
"This work has been supported by the China NSFC project (No. 61573241), Shanghai International Science and Technology Cooperation Fund (No. 16550720300) and the JiangSu NSFC project (BE2016078).",Acknowledgments,[0],[0]
Experiments have been carried out on the PI supercomputer at Shanghai Jiao Tong University.,Acknowledgments,[0],[0]
We also thank Tianfan Fu for comments that greatly improved the manuscript.,Acknowledgments,[0],[0]
Concept definition is important in language understanding (LU) adaptation since literal definition difference can easily lead to data sparsity even if different data sets are actually semantically correlated.,abstractText,[0],[0]
"To address this issue, in this paper, a novel concept transfer learning approach is proposed.",abstractText,[0],[0]
"Here, substructures within literal concept definition are investigated to reveal the relationship between concepts.",abstractText,[0],[0]
"A hierarchical semantic representation for concepts is proposed, where a semantic slot is represented as a composition of atomic concepts.",abstractText,[0],[0]
"Based on this new hierarchical representation, transfer learning approaches are developed for adaptive LU.",abstractText,[0],[0]
"The approaches are applied to two tasks: value set mismatch and domain adaptation, and evaluated on two LU benchmarks: ATIS and DSTC 2&3.",abstractText,[0],[0]
Thorough empirical studies validate both the efficiency and effectiveness of the proposed method.,abstractText,[0],[0]
"In particular, we achieve state-ofthe-art performance (F1-score 96.08%) on ATIS by only using lexicon features.",abstractText,[0],[0]
Concept Transfer Learning for Adaptive Language Understanding,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2153–2162, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks. In this work we study various model architectures and different ways to represent and aggregate the source information in an endto-end neural dialogue system framework. A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector. The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two. Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance. Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.",text,[0],[0]
"Recurrent Neural Network (RNN)-based conditional language models (LM) have been shown to be very effective in tackling a number of real world problems, such as machine translation (MT) (Cho et al., 2014) and image caption generation (Karpathy and Fei-Fei, 2015).",1 Introduction,[0],[0]
"Recently, RNNs were applied to task of generating sentences from an explicit semantic representation (Wen et al., 2015a).",1 Introduction,[0],[0]
"Attention-based methods (Mei et al., 2016) and Long Short-term Memory (LSTM)-like (Hochreiter
and Schmidhuber, 1997) gating mechanisms (Wen et al., 2015b) have both been studied to improve generation quality.",1 Introduction,[0],[0]
"Although it is now clear that LSTMbased conditional LMs can generate plausible natural language, less effort has been put in comparing the different model architectures.",1 Introduction,[0],[0]
"Furthermore, conditional generation models are typically tested on relatively straightforward tasks conditioned on a single source (e.g. a sentence or an image) and where the goal is to optimise a single metric (e.g. BLEU).",1 Introduction,[0],[0]
"In this work, we study the use of conditional LSTMs in the generation component of neural network (NN)-based dialogue systems which depend on multiple conditioning sources and optimising multiple metrics.
",1 Introduction,[0],[0]
"Neural conversational agents (Vinyals and Le, 2015; Shang et al., 2015) are direct extensions of the sequence-to-sequence model (Sutskever et al., 2014) in which a conversation is cast as a source to target transduction problem.",1 Introduction,[0],[0]
"However, these models are still far from real world applications because they lack any capability for supporting domain specific tasks, for example, being able to interact with databases (Sukhbaatar et al., 2015; Yin et al., 2016) and aggregate useful information into their responses.",1 Introduction,[0],[0]
"Recent work by Wen et al. (2016a), however, proposed an end-to-end trainable neural dialogue system that can assist users to complete specific tasks.",1 Introduction,[0],[0]
"Their system used both distributed and symbolic representations to capture user intents, and these collectively condition a NN language generator to generate system responses.",1 Introduction,[0],[0]
"Due to the diversity of the conditioning information sources, the best way to represent and combine them is non-trivial.
2153
",1 Introduction,[0],[0]
"In Wen et al. (2016a), the objective function for learning the dialogue policy and language generator depends solely on the likelihood of the output sentences.",1 Introduction,[0],[0]
"However, this sequential supervision signal may not be informative enough to learn a good conditioning vector representation resulting in a generation process which is dominated by the LM.",1 Introduction,[0],[0]
"This can often lead to inappropriate system outputs.
",1 Introduction,[0],[0]
"In this paper, we therefore also investigate the use of snapshot learning which attempts to mitigate this problem by heuristically applying companion supervision signals to a subset of the conditioning vector.",1 Introduction,[0],[0]
"This idea is similar to deeply supervised nets (Lee et al., 2015) in which the final cost from the output layer is optimised together with the companion signals generated from each intermediary layer.",1 Introduction,[0],[0]
We have found that snapshot learning offers several benefits: (1) it consistently improves performance; (2) it learns discriminative and robust feature representations and alleviates the vanishing gradient problem; (3) it appears to learn transparent and interpretable subspaces of the conditioning vector.,1 Introduction,[0],[0]
"Machine learning approaches to task-oriented dialogue system design have cast the problem as a partially observable Markov Decision Process (POMDP) (Young et al., 2013) with the aim of using reinforcement learning (RL) to train dialogue policies online through interactions with real users (Gašić et al., 2013).",2 Related Work,[0],[0]
"In order to make RL tractable, the state and action space must be carefully designed (Young et al., 2010) and the understanding (Henderson et al., 2014; Mrkšić et al., 2015) and generation (Wen et al., 2015b; Wen et al., 2016b) modules were assumed available or trained standalone on supervised corpora.",2 Related Work,[0],[0]
"Due to the underlying hand-coded semantic representation (Traum, 1999), the conversation is far from natural and the comprehension capability is limited.",2 Related Work,[0],[0]
"This motivates the use of neural networks to model dialogues from end to end as a conditional generation problem.
",2 Related Work,[0],[0]
"Interest in generating natural language using NNs can be attributed to the success of RNN LMs for large vocabulary speech recognition (Mikolov et al., 2010; Mikolov et al., 2011).",2 Related Work,[0],[0]
"Sutskever et al. (2011) showed that plausible sentences can be
obtained by sampling characters one by one from the output layer of an RNN.",2 Related Work,[0],[0]
"By conditioning an LSTM on a sequence of characters, Graves (2013) showed that machines can synthesise handwriting indistinguishable from that of a human.",2 Related Work,[0],[0]
"Later on, this idea has been tried in several research fields, for example, generating image captions by conditioning an RNN on a convolutional neural network (CNN) output (Karpathy and Fei-Fei, 2015; Xu et al., 2015); translating a source to a target language by conditioning a decoder LSTM on top of an encoder LSTM (Cho et al., 2014; Bahdanau et al., 2015); or generating natural language by conditioning on a symbolic semantic representation (Wen et al., 2015b; Mei et al., 2016).",2 Related Work,[0],[0]
"Among all these methods, attention-based mechanisms (Bahdanau et al., 2015; Hermann et al., 2015; Ling et al., 2016) have been shown to be very effective improving performance using a dynamic source aggregation strategy.
",2 Related Work,[0],[0]
"To model dialogue as conditional generation, a sequence-to-sequence learning (Sutskever et al., 2014) framework has been adopted.",2 Related Work,[0],[0]
Vinyals and Le (2015) trained the same model on several conversation datasets and showed that the model can generate plausible conversations.,2 Related Work,[0],[0]
"However, Serban et al. (2015b) discovered that the majority of the generated responses are generic due to the maximum likelihood criterion, which was latter addressed by Li et al. (2016a) using a maximum mutual information decoding strategy.",2 Related Work,[0],[0]
"Furthermore, the lack of a consistent system persona was also studied in Li et al. (2016b).",2 Related Work,[0],[0]
"Despite its demonstrated potential, a major barrier for this line of research is data collection.",2 Related Work,[0],[0]
"Many works (Lowe et al., 2015; Serban et al., 2015a; Dodge et al., 2016) have investigated conversation datasets for developing chat bot or QA-like general purpose conversation agents.",2 Related Work,[0],[0]
"However, collecting data to develop goal oriented dialogue systems that can help users to complete a task in a specific domain remains difficult.",2 Related Work,[0],[0]
"In a recent work by Wen et al. (2016a), this problem was addressed by designing an online, parallel version of Wizard-of-Oz data collection (Kelley, 1984) which allows large scale and cheap in-domain conversation data to be collected using Amazon Mechanical Turk.",2 Related Work,[0],[0]
"An NNbased dialogue model was also proposed to learn from the collected dataset and was shown to be able to assist human subjects to complete specific tasks.
",2 Related Work,[0],[0]
"Snapshot learning can be viewed as a special form of weak supervision (also known as distant- or self supervision) (Craven and Kumlien, 1999; Snow et al., 2004), in which supervision signals are heuristically labelled by matching unlabelled corpora with entities or attributes in a structured database.",2 Related Work,[0],[0]
"It has been widely applied to relation extraction (Mintz et al., 2009) and information extraction (Hoffmann et al., 2011) in which facts from a knowledge base (e.g. Freebase) were used as objectives to train classifiers.",2 Related Work,[0],[0]
"Recently, self supervision was also used in memory networks (Hill et al., 2016) to improve the discriminative power of memory attention.",2 Related Work,[0],[0]
"Conceptually, snapshot learning is related to curriculum learning (Bengio et al., 2009).",2 Related Work,[0],[0]
"Instead of learning easier examples before difficult ones, snapshot learning creates an easier target for each example.",2 Related Work,[0],[0]
"In practice, snapshot learning is similar to deeply supervised nets (Lee et al., 2015) in which companion objectives are generated from intermediary layers and optimised altogether with the output objective.",2 Related Work,[0],[0]
The testbed for this work is a neural network-based task-oriented dialogue system proposed by Wen et al. (2016a).,3 Neural Dialogue System,[0],[0]
"The model casts dialogue as a source to target sequence transduction problem (modelled by a sequence-to-sequence architecture (Sutskever et al., 2014))",3 Neural Dialogue System,[0],[0]
"augmented with the dialogue history (modelled by a belief tracker (Henderson et al., 2014)) and the current database search outcome (modelled by a database operator).",3 Neural Dialogue System,[0],[0]
The model consists of both encoder and decoder modules.,3 Neural Dialogue System,[0],[0]
The details of each module are given below.,3 Neural Dialogue System,[0],[0]
"At each turn t, the goal of the encoder is to produce a distributed representation of the system action mt, which is then used to condition a decoder to generate the next system response in skeletal form1.",3.1 Encoder Module,[0],[0]
"It consists of four submodules: intent network, belief tracker, database operator, and policy network.",3.1 Encoder Module,[0],[0]
Intent Network,3.1 Encoder Module,[0],[0]
"The intent network takes a sequence of tokens1 and converts it into a sentence embedding representing the user intent using an LSTM
1Delexicalisation: slots and values are replaced by generic tokens (e.g. keywords like Chinese food are replaced by [v.food] [s.food] to allow weight sharing.
network.",3.1 Encoder Module,[0],[0]
The hidden layer of the LSTM at the last encoding step zt is taken as the representation.,3.1 Encoder Module,[0],[0]
"As mentioned in Wen et al. (2016a), this representation can be viewed as a distributed version of the speech act (Traum, 1999) used in traditional systems.",3.1 Encoder Module,[0],[0]
"Belief Trackers In addition to the intent network, the neural dialogue system uses a set of slot-based belief trackers (Henderson et al., 2014; Mrkšić et al., 2015) to track user requests.",3.1 Encoder Module,[0],[0]
"By taking each user input as new evidence, the task of a belief tracker is to maintain a multinomial distribution p over values v ∈",3.1 Encoder Module,[0],[0]
"Vs for each informable slot2 s, and a binary distribution for each requestable slot2.",3.1 Encoder Module,[0],[0]
These probability distributions pst are called belief states of the system.,3.1 Encoder Module,[0],[0]
"The belief states pst , together with the intent vector zt, can be viewed as the system’s comprehension of the user requests up to turn t. Database Operator Based on the belief states pst , a DB query is formed by taking the union of the maximum values of each informable slot.",3.1 Encoder Module,[0],[0]
"A vector xt representing different degrees of matching in the DB (no match, 1 match, ... or more than 5 matches) is produced by counting the number of matched entities and expressing it as a 6-bin 1-hot encoding.",3.1 Encoder Module,[0],[0]
"If xt is not zero, an associated entity pointer is maintained which identifies one of the matching DB entities selected at random.",3.1 Encoder Module,[0],[0]
The entity pointer is updated if the current entity no longer matches the search criteria; otherwise it stays the same.,3.1 Encoder Module,[0],[0]
"Policy Network Based on the vectors zt, pst , and xt from the above three modules, the policy network combines them into a single action vector mt by a three-way matrix transformation,
mt = tanh(Wzmzt + Wxmxt + ∑ s∈G W s pmp s t ) (1)
where matrices Wzm, Wspm, and Wxm are parameters and G is the domain ontology.",3.1 Encoder Module,[0],[0]
"Conditioned on the system action vector mt provided by the encoder module, the decoder module uses a conditional LSTM LM to generate the required system output token by token in skeletal form1.",3.2 Decoder Module,[0],[0]
"The final system response can then be formed
2Informable slots are slots that users can use to constrain the search, such as food type or price range; Requestable slots are slots that users can ask a value for, such as phone number.",3.2 Decoder Module,[0],[0]
"This information is specified in the domain ontology.
by substituting the actual values of the database entries into the skeletal sentence structure.",3.2 Decoder Module,[0],[0]
"In this paper we study and analyse three different variants of LSTM-based conditional generation architectures: Language Model Type The most straightforward way to condition the LSTM network on additional source information is to concatenate the conditioning vector mt together with the input word embedding wj and previous hidden layer hj−1,
 
ij fj",3.2.1 Conditional Generation Network,[0],[0]
"oj ĉj
  =  
sigmoid sigmoid sigmoid tanh
 W4n,3n  
",3.2.1 Conditional Generation Network,[0],[0]
"mt wj hj−1
 
cj = fj cj−1 +",3.2.1 Conditional Generation Network,[0],[0]
"ij ĉj hj = oj tanh(cj)
where index j is the generation step, n is the hidden layer size, ij , fj ,oj ∈",3.2.1 Conditional Generation Network,[0],[0]
"[0, 1]n are input, forget, and output gates respectively, ĉj and cj are proposed cell value and true cell value at step j, and W4n,3n are the model parameters.",3.2.1 Conditional Generation Network,[0],[0]
The model is shown in Figure 1a.,3.2.1 Conditional Generation Network,[0],[0]
"Since it does not differ significantly from the original LSTM, we call it the language model type (lm) conditional generation network.",3.2.1 Conditional Generation Network,[0],[0]
Memory Type,3.2.1 Conditional Generation Network,[0],[0]
"The memory type (mem) conditional generation network was introduced by Wen et al. (2015b), shown in Figure 1b, in which the conditioning vector mt is governed by a standalone reading gate rj .",3.2.1 Conditional Generation Network,[0],[0]
"This reading gate decides how much information should be read from the conditioning vector and directly writes it into the memory cell cj ,
 
ij",3.2.1 Conditional Generation Network,[0],[0]
fj,3.2.1 Conditional Generation Network,[0],[0]
"oj rj
  =  
sigmoid sigmoid sigmoid sigmoid
 W4n,3n  
mt wj hj−1
 
ĉj = tanh ( Wc(wj ⊕ hj−1) )
",3.2.1 Conditional Generation Network,[0],[0]
cj = fj cj−1 +,3.2.1 Conditional Generation Network,[0],[0]
ij ĉj,3.2.1 Conditional Generation Network,[0],[0]
+,3.2.1 Conditional Generation Network,[0],[0]
"rj mt
hj = oj tanh(cj)
where Wc is another weight matrix to learn.",3.2.1 Conditional Generation Network,[0],[0]
The idea behind this is that the model isolates the conditioning vector from the LM so that the model has more flexibility to learn to trade off between the two.,3.2.1 Conditional Generation Network,[0],[0]
"Hybrid Type Continuing with the same idea as the memory type network, a complete separation of conditioning vector and LM (except for the gate controlling the signals) is provided by the hybrid type network shown in Figure 1c,
 
ij fj",3.2.1 Conditional Generation Network,[0],[0]
"oj rj
  =  
sigmoid sigmoid sigmoid sigmoid
 W4n,3n  
mt wj hj−1
 
ĉj = tanh ( Wc(wj ⊕ hj−1) )
",3.2.1 Conditional Generation Network,[0],[0]
"cj = fj cj−1 + ij ĉj
hj = oj tanh(cj) +",3.2.1 Conditional Generation Network,[0],[0]
"rj mt
This model was motivated by the fact that long-term dependency is not needed for the conditioning vector because we apply this information at every step j anyway.",3.2.1 Conditional Generation Network,[0],[0]
The decoupling of the conditioning vector and the LM is attractive because it leads to better interpretability of the results and provides the potential to learn a better conditioning vector and LM.,3.2.1 Conditional Generation Network,[0],[0]
Attention,3.2.2 Attention and Belief Representation,[0],[0]
An attention-based mechanism provides an effective approach for aggregating multiple information sources for prediction tasks.,3.2.2 Attention and Belief Representation,[0],[0]
"Like Wen et al.
(2016a), we explore the use of an attention mechanism to combine the tracker belief states in which the policy network in Equation 1 is modified as
mjt =",3.2.2 Attention and Belief Representation,[0],[0]
"tanh(Wzmzt + Wxmxt + ∑ s∈G α j sWspmp s t )
where the attention weights αjs are calculated by,
αjs = softmax ( rᵀ tanh ( Wr · (vt ⊕ pst ⊕wtj ⊕ htj−1) ))
",3.2.2 Attention and Belief Representation,[0],[0]
where vt = zt + xt and matrix Wr and vector r are parameters to learn.,3.2.2 Attention and Belief Representation,[0],[0]
Belief Representation The effect of different belief state representations on the end performance are also studied.,3.2.2 Attention and Belief Representation,[0],[0]
"For user informable slots, the full belief state pst is the original state containing all categorical values; the summary belief state contains only three components: the summed value of all categorical probabilities, the probability that the user said they “don’t care” about this slot and the probability that the slot has not been mentioned.",3.2.2 Attention and Belief Representation,[0],[0]
"For user requestable slots, on the other hand, the full belief state is the same as the summary belief state because the slot values are binary rather than categorical.",3.2.2 Attention and Belief Representation,[0],[0]
"Learning conditional generation models from sequential supervision signals can be difficult, because it requires the model to learn both long-term word dependencies and potentially distant source encoding functions.",3.3 Snapshot Learning,[0],[0]
"To mitigate this difficulty, we introduce a novel method called snapshot learning to create a vector of binary labels Υjt ∈",3.3 Snapshot Learning,[0],[0]
"[0, 1]d, d < dim(mjt )",3.3 Snapshot Learning,[0],[0]
"as the snapshot of the remaining part of the output sentence Tt,j:|Tt| from generation step",3.3 Snapshot Learning,[0],[0]
"j. Each element of the snapshot vector is an indicator function of a certain event that will happen in the future, which can be obtained either from the system response or dialogue context at training time.",3.3 Snapshot Learning,[0],[0]
"A companion cross entropy error is then computed to force a subset of the conditioning vector m̂jt ⊂ mjt to be close to the snapshot vector,
Lss(·) =",3.3 Snapshot Learning,[0],[0]
"− ∑
t ∑ j E[H(Υ j t , m̂ j t )]",3.3 Snapshot Learning,[0],[0]
"(2)
whereH(·) is the cross entropy function, Υjt and m̂jt are elements of vectors",3.3 Snapshot Learning,[0],[0]
"Υjt and m̂ j t , respectively.",3.3 Snapshot Learning,[0],[0]
"In order to make the tanh activations of m̂jt compatible with the 0-1 snapshot labels, we squeeze each
value of m̂jt by adding 1 and dividing by 2 before computing the cost.
",3.3 Snapshot Learning,[0],[0]
"The indicator functions we use in this work have two forms: (1) whether a particular slot value (e.g., [v.food]1) is going to occur, and (2) whether the system has offered a venue3, as shown in Figure 2.",3.3 Snapshot Learning,[0],[0]
The offer label in the snapshot is produced by checking the delexicalised name token ([v.name]) in the entire dialogue.,3.3 Snapshot Learning,[0],[0]
"If it has occurred, every label in subsequent turns is labelled with 1.",3.3 Snapshot Learning,[0],[0]
Otherwise it is labelled with 0.,3.3 Snapshot Learning,[0],[0]
"To create snapshot targets for a particular slot value, the output sentence is matched with the corresponding delexicalised token turn by turn, per generation step.",3.3 Snapshot Learning,[0],[0]
"At each generation step, the target is labelled with 0 if that delexicalised token has been generated; otherwise it is set to 1.",3.3 Snapshot Learning,[0],[0]
"However, for the models without attention, the targets per turn are set to the same because the condition vector will not be able to learn the dynamically changing behaviour without attention.",3.3 Snapshot Learning,[0],[0]
"Dataset The dataset used in this work was collected in the Wizard-of-Oz online data collection described by Wen et al. (2016a), in which the task of the system is to assist users to find a restaurant in Cambridge, UK area.",4 Experiments,[0],[0]
"There are three informable slots (food, pricerange, area) that users can use to constrain the search and six requestable slots (address, phone, postcode plus the three informable
3Details of the specific application used in this study are given in Section 4 below.
slots) that the user can ask a value for once a restaurant has been offered.",4 Experiments,[0],[0]
There are 676 dialogues in the dataset (including both finished and unfinished dialogues) and approximately 2750 turns in total.,4 Experiments,[0],[0]
The database contains 99 unique restaurants.,4 Experiments,[0],[0]
Training The training procedure was divided into two stages.,4 Experiments,[0],[0]
"Firstly, the belief tracker parameters θb were pre-trained using cross entropy errors between tracker labels and predictions.",4 Experiments,[0],[0]
"Having fixed the tracker parameters, the remaining parts of the model θ\b are trained using the cross entropy errors from the generation network LM,
L(θ\b) =",4 Experiments,[0],[0]
"− ∑
t ∑ j H(y t j ,p t j) + λLss(·) (3)
where ytj and p t j are output token targets and predictions respectively, at turn t of output step j, Lss(·) is the snapshot cost from Equation 2, and λ is the tradeoff parameter in which we set to 1 for all models trained with snapshot learning.",4 Experiments,[0],[0]
We treated each dialogue as a batch and used stochastic gradient descent with a small l2 regularisation term to train the model.,4 Experiments,[0],[0]
"The collected corpus was partitioned into a training, validation, and testing sets in the ratio 3:1:1.",4 Experiments,[0],[0]
Early stopping was implemented based on the validation set considering only LM log-likelihoods.,4 Experiments,[0],[0]
Gradient clipping was set to 1.,4 Experiments,[0],[0]
"The hidden layer sizes were set to 50, and the weights were randomly
initialised between -0.3 and 0.3 including word embeddings.",4 Experiments,[0],[0]
"The vocabulary size is around 500 for both input and output, in which rare words and words that can be delexicalised have been removed.
",4 Experiments,[0],[0]
"Decoding In order to compare models trained with different recipes rather than decoding strategies, we decode all the trained models with the average log probability of tokens in the sentence.",4 Experiments,[0],[0]
"We applied beam search with a beamwidth equal to 10, the search stops when an end-of-sentence token is generated.",4 Experiments,[0],[0]
"In order to consider language variability, we ran decoding until 5 candidates were obtained and performed evaluation on them.
",4 Experiments,[0],[0]
Metrics We compared models trained with different recipes by performing a corpus-based evaluation in which the model is used to predict each system response in the held-out test set.,4 Experiments,[0],[0]
"Three evaluation metrics were used: BLEU score (on top-1 and top5 candidates) (Papineni et al., 2002), slot matching rate and objective task success rate (Su et al., 2015).",4 Experiments,[0],[0]
"The dialogue is marked as successful if both: (1) the offered entity matches the task that was specified to the user, and (2) the system answered all the associated information requests (e.g. what is the address?) from the user.",4 Experiments,[0],[0]
"The slot matching rate is the percentage of delexicalised tokens (e.g. [s.food] and [v.area]1) appear in the candidate also appear in the
reference.",4 Experiments,[0],[0]
We computed the BLEU scores on the skeletal sentence forms before substituting with the actual entity values.,4 Experiments,[0],[0]
All the results were averaged over 10 random initialised networks.,4 Experiments,[0],[0]
Results Table 1 shows the evaluation results.,4 Experiments,[0],[0]
The numbers to the left and right of each table cell are the same model trained w/o and w/ snapshot learning.,4 Experiments,[0],[0]
The first observation is that snapshot learning consistently improves on most metrics regardless of the model architecture.,4 Experiments,[0],[0]
This is especially true for BLEU scores.,4 Experiments,[0],[0]
"We think this may be attributed to the more discriminative conditioning vector learned through the snapshot method, which makes the learning of the conditional LM easier.
",4 Experiments,[0],[0]
"In the first block belief state representation, we compare the effect of two different belief representations.",4 Experiments,[0],[0]
"As can be seen, using a succinct representation is better (summary>full) because the identity of each categorical value in the belief state does not help when the generation decisions are done in skeletal form.",4 Experiments,[0],[0]
"In fact, the full belief state representation may encourage the model to learn incorrect coadaptation among features when the data is scarce.
",4 Experiments,[0],[0]
"In the conditional architecture block, we compare the three different conditional generation architectures as described in section 3.2.1.",4 Experiments,[0],[0]
"This result shows that the language model type (lm) and memory type (mem) networks perform better in terms of BLEU score and slot matching rate, while the hybrid type (hybrid) networks achieve higher task success.",4 Experiments,[0],[0]
"This is probably due to the degree of separation be-
tween the LM and conditioning vector: a coupling approach (lm, mem) sacrifices the conditioning vector but learns a better LM and higher BLEU; while a complete separation (hybrid) learns a better conditioning vector and offers a higher task success.
",4 Experiments,[0],[0]
"Lastly, in the attention-based model block we train the three architectures with the attention mechanism and compare them again.",4 Experiments,[0],[0]
"Firstly, the characteristics of the three models we observed above also hold for attention-based models.",4 Experiments,[0],[0]
"Secondly, we found that the attention mechanism improves all the three architectures on task success rate but not BLEU scores.",4 Experiments,[0],[0]
"This is probably due to the limitations of using n-gram based metrics like BLEU to evaluate the generation quality (Stent et al., 2005).",4 Experiments,[0],[0]
Gate Activations We first studied the average activation of each individual gate in the models by averaging them when running generation on the test set.,5 Model Analysis,[0],[0]
We analysed the hybrid models because their reading gate to output gate activation ratio (rj/oj) shows clear tradeoff between the LM and the conditioning vector components.,5 Model Analysis,[0],[0]
"As can be seen in Ta-
ble 2, we found that the average forget gate activations (fj) and the ratio of the reading gate to the output gate activation (rj/oj) have strong correlations to performance: a better performance (row 3>row 2>row 1) seems to come from models that can learn a longer word dependency (higher forget gate ft activations) and a better conditioning vector (therefore higher reading to output gate ratio rj/oj).",5 Model Analysis,[0],[0]
Learned Attention We have visualised the learned attention heat map of models trained with and without snapshot learning in Figure 3.,5 Model Analysis,[0],[0]
The attention is on both the informable slot trackers (first three columns) and the requestable slot trackers (the other columns).,5 Model Analysis,[0],[0]
We found that the model trained with snapshot learning (Figure 3b) seems to produce a more accurate and discriminative attention heat map comparing to the one trained without it (Figure 3a).,5 Model Analysis,[0],[0]
This may contribute to the better performance achieved by the snapshot learning approach.,5 Model Analysis,[0],[0]
Snapshot Neurons,5 Model Analysis,[0],[0]
"As mentioned earlier, snapshot learning forces a subspace of the conditioning vector m̂jt to become discriminative and interpretable.",5 Model Analysis,[0],[0]
Three example generated sentences together with the snapshot neuron activations are shown in Figure 4.,5 Model Analysis,[0],[0]
"As can be seen, when generating words one by one, the neuron activations were changing to detect different events they were assigned by the snapshot training signals: e.g. in Figure 4b the light blue and orange neurons switched their domination role when the token [v.address] was generated; the offered neuron is in a high activation state in Figure 4b because the system was offering a venue, while in Figure 4a it is not activated because the system was still helping the user to find a venue.",5 Model Analysis,[0],[0]
This paper has investigated different conditional generation architectures and a novel method called snapshot learning to improve response generation in a neural dialogue system framework.,6 Conclusion and Future Work,[0],[0]
The results showed three major findings.,6 Conclusion and Future Work,[0],[0]
"Firstly, although the hybrid type model did not rank highest on all metrics, it is nevertheless preferred because it achieved the highest task success and also it provided more interpretable results.",6 Conclusion and Future Work,[0],[0]
"Secondly, snapshot learning provided gains on virtually all metrics regardless of the architecture used.",6 Conclusion and Future Work,[0],[0]
"The analysis suggested that the benefit of snapshot learning mainly comes from the more discriminative and robust subspace representation learned from the heuristically labelled companion signals, which in turn facilitates optimisation of the final target objective.",6 Conclusion and Future Work,[0],[0]
"Lastly, the results suggested that by making a complex system more interpretable at different levels not only helps our understanding but also leads to the highest success rates.
",6 Conclusion and Future Work,[0],[0]
"However, there is still much work left to do.",6 Conclusion and Future Work,[0],[0]
This work focused on conditional generation architectures and snapshot learning in the scenario of generating dialogue responses.,6 Conclusion and Future Work,[0],[0]
It would be very helpful if the same comparison could be conducted in other application domains such as machine translation or image caption generation so that a wider view of the effectiveness of these approaches can be assessed.,6 Conclusion and Future Work,[0],[0]
"Furthermore, removing slot-value delexicalisation and learning confirmation behaviour in noisy speech conditions are also main research problems from the system development prospective.",6 Conclusion and Future Work,[0],[0]
"Tsung-Hsien Wen and David Vandyke are supported by Toshiba Research Europe Ltd, Cambridge Research Laboratory.",Acknowledgments,[0],[0]
Recently a variety of LSTM-based conditional language models (LM) have been applied across a range of language generation tasks.,abstractText,[0],[0]
In this work we study various model architectures and different ways to represent and aggregate the source information in an endto-end neural dialogue system framework.,abstractText,[0],[0]
A method called snapshot learning is also proposed to facilitate learning from supervised sequential signals by applying a companion cross-entropy objective function to the conditioning vector.,abstractText,[0],[0]
"The experimental and analytical results demonstrate firstly that competition occurs between the conditioning vector and the LM, and the differing architectures provide different trade-offs between the two.",abstractText,[0],[0]
"Secondly, the discriminative power and transparency of the conditioning vector is key to providing both model interpretability and better performance.",abstractText,[0],[0]
"Thirdly, snapshot learning leads to consistent performance improvements independent of which architecture is used.",abstractText,[0],[0]
Conditional Generation and Snapshot Learning in Neural Dialogue Systems,title,[0],[0]
"We consider the problem of estimating the parameters θ ∈ RM of an unnormalised statistical model φ(u;θ) : X 7→ R+ from observed data X = {x1, . . .",1. Introduction,[0],[0]
",xN}, where the
*Equal contribution 1UMIC, RWTH Aachen University, Aachen, Germany (affiliated with KTH Royal Institute of Technology and University of Edinburgh during project timespan) 2School of Informatics, University of Edinburgh, Edinburgh, United Kingdom.",1. Introduction,[0],[0]
"Correspondence to: Ciwan Ceylan <ceylan@vision.rwthaachen.de>, Michael Gutmann <michael.gutmann@ed.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
xi ∈ X are independently sampled from the unknown data distribution pd.,1. Introduction,[0],[0]
"Unnormalised models output non-negative numbers but do not integrate or sum to one, i.e. they are statistical models that are defined up to the partition function Z(θ) = ∫ φ(u;θ) du.",1. Introduction,[0],[0]
"Unnormalised models are widely used, e.g. to model images (Köster & Hyvärinen, 2010; Gutmann & Hyvärinen, 2013), natural language (Mnih & Teh, 2012; Zoph et al., 2016), or memory (Hopfield, 1982).
",1. Introduction,[0],[0]
"If the partition function Z(θ) can be evaluated analytically in closed form, the unnormalised model φ(u;θ) can be easily converted to a (normalised) statistical model p(u;θ) = φ(u;θ)/Z(θ) that can be estimated by maximising the likelihood.",1. Introduction,[0],[0]
"However, for most unnormalised models the integral defining the partition function is analytically intractable and computationally expensive to approximate.
",1. Introduction,[0],[0]
"Several methods have been proposed in the literature to estimate unnormalised models including Monte Carlo maximum likelihood (Geyer, 1994), contrastive divergence (Hinton, 2002), score matching (Hyvärinen, 2005), and noisecontrastive estimation (Gutmann & Hyvärinen, 2010; 2012) and its generalisations (Pihlaja et al., 2010; Gutmann & Hirayama, 2011).",1. Introduction,[0],[0]
The basic idea of noise-contrastive estimation (NCE) is to formulate the density estimation problem as a classification problem where the model is trained to distinguish between the observed data and some reference (noise) data.,1. Introduction,[0],[0]
"NCE is used in several application domains (Mnih & Teh, 2012; Chen et al., 2015; Tschiatschek et al., 2016) and similar “learning by comparison” ideas are employed for learning with generative latent variable models (Gutmann et al., 2014; Goodfellow et al., 2014).
",1. Introduction,[0],[0]
"In NCE, the choice of the auxiliary noise distribution is left to the user.",1. Introduction,[0],[0]
"While simple distributions, e.g. uniform or Gaussian distributions, have successfully been used (Gutmann & Hyvärinen, 2012; Mnih & Teh, 2012), the estimation performance of NCE depends on the distribution chosen and more tailored distributions were found to typically yield better results, see e.g. (Ji et al., 2016).",1. Introduction,[0],[0]
"Intuitively, the noise samples in NCE ought to resemble the observed data in order for the classification problem not to be too easy.",1. Introduction,[0],[0]
"To alleviate the burden on the user to generate such noise, we here propose conditional noise-contrastive estimation that semiautomatically generates the noise based on the observed data.
",1. Introduction,[0],[0]
The rest of the paper is structured as follows.,1. Introduction,[0],[0]
"In Section 2, we present the theory of conditional noise-contrastive estimation (CNCE), establish basic properties, and prove that a limiting case yields score matching.",1. Introduction,[0],[0]
"In Section 3, we validate the theory on synthetic data and compare the estimation performance of CNCE with NCE.",1. Introduction,[0],[0]
"In Section 4, we apply CNCE to real data and show that it can handle complex models by estimating a four-layer neural network model of natural images, and Section 5 concludes the paper.",1. Introduction,[0],[0]
Conditional noise-contrastive estimation (CNCE) turns an unsupervised estimation problem into a supervised learning problem by training the model to distinguish between data and noise samples.,2. Conditional noise-contrastive estimation,[0],[0]
"This is the same high-level approach as NCE takes, but in contrast to NCE, the novel idea of CNCE is to generate the noise samples with the aid of the observed data samples.",2. Conditional noise-contrastive estimation,[0],[0]
"Therefore, unlike NCE, CNCE does not assume the noise samples to be generated independently of the data samples, but rather to be drawn from a conditional noise distribution pc.",2. Conditional noise-contrastive estimation,[0],[0]
"The generated noise samples are paired with the data samples, with κ noise samples yij ∈ Y, j = 1, . . .",2. Conditional noise-contrastive estimation,[0],[0]
", κ per observed data point xi.",2. Conditional noise-contrastive estimation,[0],[0]
"Thus, a total of N · κ noise samples yij ∼ pc(yij |xi) are generated from pc.",2. Conditional noise-contrastive estimation,[0],[0]
We denote the collection of all noise samples by Y.,2. Conditional noise-contrastive estimation,[0],[0]
"In what follows, we assume that X = Y, but this assumption can be relaxed to X ⊆ Y (see Supplementary Materials A).",2. Conditional noise-contrastive estimation,[0],[0]
"In any case, we denote the union of X and Y by U.
We derive the loss function for CNCE in analogy to the derivation of the loss function for NCE.",2. Conditional noise-contrastive estimation,[0],[0]
"We divide all pairs of data and noise samples into two classes, Cα and Cβ, of equal size.",2. Conditional noise-contrastive estimation,[0],[0]
"Class Cα is formed by tuples (u1,u2) with u1 ∈ X and u2 ∈ Y, whileCβ is formed by tuples (u1,u2) with u1 ∈ Y and u2 ∈ X.",2. Conditional noise-contrastive estimation,[0],[0]
"Consequently, the probability distributions for the classes Cα and Cβ are given by
pα(u1,u2) = pd(u1)pc(u2|u1), (1) pβ(u1,u2) = pd(u2)pc(u1|u2), (2)
where pd denotes the distribution of the xi.",2. Conditional noise-contrastive estimation,[0],[0]
"The class conditional distributions can be obtained by Bayes’ rule,
pCα|u(u1,u2) = pα(u1,u2)
pα(u1,u2) + pβ(u1,u2) (3)
",2. Conditional noise-contrastive estimation,[0],[0]
"= 1
1 + pd(u2)pc(u1|u2)pd(u1)pc(u2|u1) , (4)
pCβ|u(u1,u2) = 1
1 + pd(u1)pc(u2|u1)pd(u2)pc(u1|u2) .",2. Conditional noise-contrastive estimation,[0],[0]
"(5)
The prior class probabilities cancel because there are equally many samples in each class.
",2. Conditional noise-contrastive estimation,[0],[0]
"By replacing pd(·) with the model φ( · ;θ)/Z(θ), the partition functions cancel and the following parametrised ver-
sions of the class conditional distributions are obtained
pCα|u(u1,u2;θ) = 1
1 + φ(u2;θ)pc(u1|u2)φ(u1;θ)pc(u2|u1) , (6)
pCβ|u(u1,u2;θ) = 1
1 + φ(u1;θ)pc(u2|u1)φ(u2;θ)pc(u1|u2) .",2. Conditional noise-contrastive estimation,[0],[0]
"(7)
The CNCE loss function is now formed as the negative log likelihood over the conditional class probabilities, in the same manner as in NCE (Gutmann & Hyvärinen, 2012),
JN (θ) = 2
κN κ∑ j=1 N∑ i=1 log",2. Conditional noise-contrastive estimation,[0],[0]
"[1 + exp(−G(xi,yij ;θ))] , (8)
G(u1,u2;θ) = log φ(u1;θ)pc(u2|u1) φ(u2;θ)pc(u1|u2) .",2. Conditional noise-contrastive estimation,[0],[0]
"(9)
The CNCE loss function JN is the sample version of J (θ) = 2Exy log (1 + exp(−G(x,y;θ))), which is obtained by taking both N and κ to the∞ limit.",2. Conditional noise-contrastive estimation,[0],[0]
"To further develop the theory, it is helpful to write J (θ) as a functional of G, which gives
J̃",2. Conditional noise-contrastive estimation,[0],[0]
"[G] = 2Exy log (1 + exp(−G(x,y))) .",2. Conditional noise-contrastive estimation,[0],[0]
"(10)
We then obtain the following theorem:
Theorem (Nonparametric estimation).",2. Conditional noise-contrastive estimation,[0],[0]
"LetG : U×U→ R be a function of the form
G(u1,u2) =",2. Conditional noise-contrastive estimation,[0],[0]
f(u1)− f(u2) +,2. Conditional noise-contrastive estimation,[0],[0]
"log pc(u2|u1) pc(u1|u2) , (11)
where f is a function from U to R. Under the assumption X = Y, J̃ attains a unique minimum at
G∗(u1,u2) = log pd(u1)pc(u2|u1) pd(u2)pc(u1|u2)
(12)
for (u1,u2) ∈ X×X with pd(u1) > 0",2. Conditional noise-contrastive estimation,[0],[0]
and pc(u1|u2),2. Conditional noise-contrastive estimation,[0],[0]
"> 0.
",2. Conditional noise-contrastive estimation,[0],[0]
The proof of a more general version is given in Supplementary Materials A.,2. Conditional noise-contrastive estimation,[0],[0]
"The theorem shows that in the limit of large N and κ, the optimal function f equals log pd up to an additive constant.",2. Conditional noise-contrastive estimation,[0],[0]
"For parametrisations that are flexible enough so thatG(u1,u2;θ∗) = G∗(u1,u2) for some value θ∗, the theorem together with the definition of G(u1,u2;θ) in (9) implies that φ(u;θ∗) ∝ pd(u).",2. Conditional noise-contrastive estimation,[0],[0]
"We have here the proportionality sign because the normalising constant is not estimated in CNCE.
",2. Conditional noise-contrastive estimation,[0],[0]
"While the theorem above concerns nonparametric estimation, and hence does not take into account how G is parametrised, it forms the basis for a consistency proof of CNCE.",2. Conditional noise-contrastive estimation,[0],[0]
"A standard approach is to identify conditions under which JN (θ) converges uniformly in probability to J (θ) and then to appeal to e.g. Theorem 5.7 of (van der Vaart,
1998).",2. Conditional noise-contrastive estimation,[0],[0]
A similar approach where the Kullback-Leibler divergence takes the role of J can be used to prove consistency of maximum likelihood estimation.,2. Conditional noise-contrastive estimation,[0],[0]
"The conditions for uniform convergence are typically fairly technical and we here forego this endeavour and instead provide empirical evidence for consistency in Section 3.
",2. Conditional noise-contrastive estimation,[0],[0]
"The generic CNCE algorithm generally takes two steps: obtain the noise samples by sampling from the conditional noise distribution pc, and then minimise the loss function JN over the parameters θ.",2. Conditional noise-contrastive estimation,[0],[0]
"The user decides the trade-off between precision and computational expenditures via κ and also needs to provide pc.
",2. Conditional noise-contrastive estimation,[0],[0]
There are two advantages to choosing pc over choosing the noise distribution in NCE.,2. Conditional noise-contrastive estimation,[0],[0]
"First, the observed data samples can be leveraged for sampling the noise, meaning that a resemblance to pd is easier to achieve than it would be for NCE.",2. Conditional noise-contrastive estimation,[0],[0]
"Indeed, all simulations in the paper were performed with the simple Gaussian specified below.",2. Conditional noise-contrastive estimation,[0],[0]
"Second, if pc is known to be symmetric, i.e. pc(u1|u2) = pc(u2|u1), it does not need to be evaluated because the densities cancel out in Equation (9).
",2. Conditional noise-contrastive estimation,[0],[0]
"A simple symmetric choice of pc when x and y ∈ RD is
pc(y|x; ε) = N (y;x, ε21), yij = xi + εξij .",2. Conditional noise-contrastive estimation,[0],[0]
"(13)
Here 1 is the identity matrix, ξij ∈",2. Conditional noise-contrastive estimation,[0],[0]
RD is a multivariate standard normal random variable and ε ∈,2. Conditional noise-contrastive estimation,[0],[0]
"[0,∞) a scalar parameter that corresponds to the standard deviation of each dimension, and which therefore controls the similarity between Y and X. It is here assumed that the data have been standardised (Murphy, 2012, Chaper 4) so that the empirical variances of the data are one for each dimension.",2. Conditional noise-contrastive estimation,[0],[0]
"Otherwise, different values of ε ought to be used for each dimension.
",2. Conditional noise-contrastive estimation,[0],[0]
"CNCE is also applicable to discrete random variables, e.g. by using a multinoulli distribution over y conditioned on x, and non-negative data (see Supplementary Materials C).
",2. Conditional noise-contrastive estimation,[0],[0]
"In our simulations, we adjust ε using simple heuristics so that the gradients of the loss function are not too small.",2. Conditional noise-contrastive estimation,[0],[0]
"This typically occurs when ε is too large so that the noise and data are easily distinguishable, but also when ε is too small.",2. Conditional noise-contrastive estimation,[0],[0]
It can be verified that the loss function attains the value 2 log(2) for ε = 0 independent of the model and θ.,2. Conditional noise-contrastive estimation,[0],[0]
"In brief, the heuristic algorithm starts with a small ε that is incremented until the value of the loss function is sufficiently far away from 2 log(2).
",2. Conditional noise-contrastive estimation,[0],[0]
"While small ε cause the gradients to be small in absolute terms, the following theorem shows that the loss function remains meaningful and that CNCE then corresponds to score matching (Hyvärinen, 2005).
",2. Conditional noise-contrastive estimation,[0],[0]
Theorem (Connection to score matching).,2. Conditional noise-contrastive estimation,[0],[0]
"Assume that φ(u;θ) is an unnormalised probability density and that
fθ(u) = log φ(u;θ) is twice differentiable.",2. Conditional noise-contrastive estimation,[0],[0]
"If y = x + εξ where ξ is a vector of uncorrelated random variables of mean zero and variance one that are independent from x and have a symmetric density, then
J (θ) =ε 2 2 Ex [∑
i
∂2fθ(x)
∂x2i +
1 2 ||∇xfθ(x)||22 ]",2. Conditional noise-contrastive estimation,[0],[0]
+ 2 log(2),2. Conditional noise-contrastive estimation,[0],[0]
+O(ε3).,2. Conditional noise-contrastive estimation,[0],[0]
"(14)
The term in the brackets is the loss function that is minimised in score matching (Hyvärinen, 2005).",2. Conditional noise-contrastive estimation,[0],[0]
"The theorem is proved in Supplementary Materials B. Note that pc in (13) fulfills the conditions in the theorem.
",2. Conditional noise-contrastive estimation,[0],[0]
The theorem can be understood as follows:,2. Conditional noise-contrastive estimation,[0],[0]
Score matching consists in finding parameter values so that the slope of the model pdf matches the slope of the data pdf.,2. Conditional noise-contrastive estimation,[0],[0]
"For symmetric conditional noise distributions pc, the nonlinearity G in Equation (9) equals G(u1,u2;θ) = log φ(u1;θ) − log φ(u2;θ) = fθ(u1) − fθ(u2).",2. Conditional noise-contrastive estimation,[0],[0]
"From (12), we know that at the optimum of J (θ), G(u1,u2;θ) matches log pd(u1) − log pd(u2).",2. Conditional noise-contrastive estimation,[0],[0]
The values which the arguments u1 and u2 take during the minimisation are determined by the conditional noise distribution.,2. Conditional noise-contrastive estimation,[0],[0]
"For small ε, the arguments are always close to each other, so that G(u1,u2;θ) is approximately proportional to a directional derivative of fθ(u) = log φ(u;θ) along a random direction.",2. Conditional noise-contrastive estimation,[0],[0]
"This means that for small ε, J (θ) is minimised when the slope of the model pdf matches the slope of the data pdf, as in score matching.",2. Conditional noise-contrastive estimation,[0],[0]
We here validate consistency and compare CNCE with NCE on synthetic data.,3. Empirical validation of the theory,[0],[0]
The models below were used in unnormalised form for CNCE and NCE.,3. Empirical validation of the theory,[0],[0]
"For the results with MLE, the models were first normalised.",3. Empirical validation of the theory,[0],[0]
Additional results for nonnegative and discrete data are provided in Supplementary Materials C.,3. Empirical validation of the theory,[0],[0]
"The Gaussian model is an unnormalised multivariate Gaussian model in five dimensions with zero mean and parametrised precision matrix Λ. As the precision matrix is symmetric, the Gaussian model has 15 parameters,
log φ(u;Λ) = −1",3.1. Models,[0],[0]
"2 uTΛu, u ∈ R5.",3.1. Models,[0],[0]
"(15)
",3.1. Models,[0],[0]
"The estimation error was measured as the Euclidean distance between the true and estimated parameters.
",3.1. Models,[0],[0]
"The ICA model is commonly used in signal possessing for blind source separation (Hyvärinen & Oja, 2000).",3.1. Models,[0],[0]
"Assuming equally many sources as data dimensions, D = 4, and a
Laplacian distribution for the sources, the unnormalised ICA model is
log φ(u;B) =",3.1. Models,[0],[0]
− √ 2 D∑ j=1 |bj ·,3.1. Models,[0],[0]
"u|, u ∈ R4.",3.1. Models,[0],[0]
"(16)
",3.1. Models,[0],[0]
The model is parametrised by the demixing matrix B and has D2 = 16 free parameters.,3.1. Models,[0],[0]
"The (normalised) ICA model can be estimated using MLE (Hyvärinen & Oja, 2000, 4.4.1).",3.1. Models,[0],[0]
"The estimation error was calculated as the Euclidean distance between true and estimated parameter vector after accounting for the sign and order ambiguity of the ICA model (Hyvärinen & Oja, 2000, 2.2) in the same manner as in (Gutmann & Hyvärinen, 2012).
",3.1. Models,[0],[0]
"Both the Gaussian and the ICA model were previously used to validate the consistency of NCE, and a Gaussian noise distribution achieved good estimation performance (Gutmann & Hyvärinen, 2012).",3.1. Models,[0],[0]
"In order to investigate the potential benefit of the adaptive noise of CNCE, we used the following more challenging “ring model” where the data lie in lower dimensional manifold.
",3.1. Models,[0],[0]
"The Ring model is given by
log φ(u;µr, γr) =",3.1. Models,[0],[0]
"− γr 2 (‖u‖2 − µr)2, u ∈ R5.",3.1. Models,[0],[0]
"(17)
",3.1. Models,[0],[0]
The model is best understood in polar coordinates: the angular components are uniformly distributed and the radial direction is Gaussian with mean µr and precision,3.1. Models,[0],[0]
γr.,3.1. Models,[0],[0]
"The mean is assumed known, and the task is to estimate the precision parameter γr.",3.1. Models,[0],[0]
"Figure 1 shows the (normalised) pdf for the ring model in two dimensions, as well as the NCE noise and the CNCE noise generated according to Equation (13).",3.1. Models,[0],[0]
"As often done in NCE, a Gaussian noise is chosen to match the mean and covariance of the data distribution.",3.1. Models,[0],[0]
"Because of the manifold structure of the data, the NCE noise is concentrated in areas where the data distribution takes small values, which is in contrast to the CNCE noise that well covers the data manifold.",3.1. Models,[0],[0]
Figures 2a and 2b show the estimation error as a function of the number of data points N .,3.2. Results,[0],[0]
"For both the Gaussian and ICA models, the CNCE error decreases linearly in the loglog domain as the sample size increases, which indicates convergence in quadratic mean, and hence consistency.",3.2. Results,[0],[0]
"Furthermore, as the number of noise-per-data points κ grows, the error appears to approach the MLE error.
",3.2. Results,[0],[0]
The MLE of the ICA model had a tendency to get stuck in local minima for a small part of the estimations (13 out of 100).,3.2. Results,[0],[0]
"Consequently, the 0.9 quantile for MLE in Figure 2b shows a high and relatively constant error corresponding to such local minima.",3.2. Results,[0],[0]
"While this also occurred for CNCE, it is not visible in Figure 2b as it occurred less often (7/100 simulations).
",3.2. Results,[0],[0]
"As shown in Figure 2c, NCE performs better than CNCE for the Gaussian model given the same number of noise and data samples.",3.2. Results,[0],[0]
"For the ICA model, they are roughly onpar for sufficiently many data samples, see Figure 2d.",3.2. Results,[0],[0]
An advantage for NCE on these models may not be surprising given that the NCE noise distribution already covers the data distribution very well.,3.2. Results,[0],[0]
"Furthermore, Figures 2e and 2f show that the difference between NCE and CNCE decreases as ratio of noise to data samples increases.
",3.2. Results,[0],[0]
Figure 3 shows the results for the ring model using κ = 10.,3.2. Results,[0],[0]
CNCE achieves about one order of magnitude lower estimation error compared to NCE.,3.2. Results,[0],[0]
"With reference to Figure 1, this vast improvement over NCE can be understood as follows: For the noise distribution used in NCE, the majority of the noise samples end up inside the ring where the data sample probability is low, so that they are not useful for learning (the classification problem is too easy, with the noise not providing enough contrast).",3.2. Results,[0],[0]
"CNCE, on the other hand, automatically generates suitably contrastive noise on (or close to) the data manifold, which facilitates learning.",3.2. Results,[0],[0]
"To show that CNCE can be used to estimate complex unnormalised models, we used it for unsupervised deep learning and estimated a four-layer feed-forward neural network model from natural images.",4. Neural image model,[0],[0]
"The model extends the two- and three-layer models of natural images previously estimated with NCE (Gutmann & Hyvärinen, 2012; 2013).",4. Neural image model,[0],[0]
We here focus on the learned features.,4. Neural image model,[0],[0]
"In Supplementary Materials D, we present a qualitative comparison with NCE.
",4. Neural image model,[0],[0]
"The data X are image patches of size 32× 32 px, sampled from 11 different monochrome images depicting wild life scenes (van Hateren & van der Schaaf, 1998) in the same manner as (Gutmann & Hyvärinen, 2013).",4. Neural image model,[0],[0]
Figure 4a shows examples of the extracted image patches.,4. Neural image model,[0],[0]
The sampled image patches were vectorised and both the ensemble mean and local mean (DC component) were subtracted.,4. Neural image model,[0],[0]
"The resulting data were then whitened and their dimensionality reduced to D = 600 by principal component analysis (Murphy, 2012, Chapter 12.2), retaining 98% of the variance.",4. Neural image model,[0],[0]
We denote the data (random vector) after preprocessing by u(1).,4. Neural image model,[0],[0]
The unnormalised image model φ defined below consist of a “structured” part φ̃ that models the non-Gaussianity of the natural image data and a Gaussian part that accounts for the covariance structure.,4.1. Model specification,[0],[0]
"In the PCA space, the model is
log φ(u(1);θ) = log φ̃(u(1);θ)− 1 2 u(1) · u(1), (18)
where · denotes the inner product between two vectors.",4.1. Model specification,[0],[0]
"This corresponds to a model for images defined in the subspace spanned by the first D principle component directions.
",4.1. Model specification,[0],[0]
The Gaussian term in (18) tends to mask the non-Gaussian structure that we are primarily interested in.,4.1. Model specification,[0],[0]
"In order to better learn about the non-Gaussian properties of natural images, we define the conditional noise distribution as
log pc(u2|u1) = log p̃c(u2|u1)− 1
2 u2 · u2 + const, (19)
where p̃c is the Gaussian noise distribution in (13).",4.1. Model specification,[0],[0]
"With this choice, the two Gaussian terms of the model and noise cancel in the nonlinearity G(u1,u2;θ), so that
G(u1,u2;θ) = log φ̃(u1;θ)p̃c(u2|u1) φ̃(u2;θ)p̃c(u1|u2) .",4.1. Model specification,[0],[0]
"(20)
Due to the cancelling, φ̃ in Equation (18) is considered the effective model and p̃c the effective conditional noise distribution.",4.1. Model specification,[0],[0]
"Examples of noise patches sampled from p̃c are shown in Figure 4b.
",4.1. Model specification,[0],[0]
"We next define the (effective) model φ̃ via a four layers deep, fully connected, feed-forward neural network.",4.1. Model specification,[0],[0]
"The general idea is that we iterate between feature extraction and pooling layers (Gutmann & Hyvärinen, 2013).",4.1. Model specification,[0],[0]
"Unlike in many image models, we here do not impose translation invariance by using convolutional networks; neither do we fix the pooling layers but learn them from data.",4.1. Model specification,[0],[0]
"The input and output dimensions of each layer are provided in Supplementary Materials D.
The preprocessed image patches u(1) are first passed through a gain-control stage where they are centred and rescaled to cancel out some effects of the lighting conditions (Gutmann & Hyvärinen, 2012),
ũ(u) =",4.1. Model specification,[0],[0]
"√ D − 1 u− 〈u〉
‖u− 〈u〉‖2 , 〈u〉 = 1 D D∑ k=1 uk.",4.1. Model specification,[0],[0]
"(21)
Then they are passed through a feature extraction and a pooling layer,
z",4.1. Model specification,[0],[0]
"(1) j = w (1) j · ũ(u (1)), (22)
z",4.1. Model specification,[0],[0]
(2) j = log ( q (2) j · ( z(1) )2 + 1 ) .,4.1. Model specification,[0],[0]
"(23)
Both the features w(1)j and pooling weights q (2) j are free parameters; we thus learn which 1st layer outputs to pool together.",4.1. Model specification,[0],[0]
"The pooling weights are restricted to be nonnegative, which we enforce by writing them as q(2)j = (w (2) j )
2, with element-wise squaring.",4.1. Model specification,[0],[0]
"The log nonlinearity counteracts the squaring, leading to an approximation of the max operation (Gutmann & Hyvärinen, 2013).
",4.1. Model specification,[0],[0]
"We then repeat this processing block of gain control, feature extraction, and pooling: The outputs z(2)j of the 2
nd layer are passed through the same gain control stage as the image patches, i.e. whitening, dimensionality reduction and rescaling, in line with previous work (Gutmann & Hyvärinen, 2013), followed by feature extraction and pooling,
z (3) j = w (3) j · ũ (3), z (4) j = q",4.1. Model specification,[0],[0]
(4) j · z (3).,4.1. Model specification,[0],[0]
"(24)
The pooling weights q(4)j are restricted to be non-negative, which is enforced as for the second layer.",4.1. Model specification,[0],[0]
We here work with a simpler pooling model than in Equation (23).,4.1. Model specification,[0],[0]
"An output z (4) j of the pooling layer is large if q (4) j pools over units that are concurrently active, which is related to detecting sign congruency (Gutmann & Hyvärinen, 2009).
",4.1. Model specification,[0],[0]
"The unnormalised model φ̃ is then given by the total activation of the units in each layer, which means that the overall population activity indicates how likely an input is.",4.1. Model specification,[0],[0]
"Following (Gutmann & Hyvärinen, 2012; 2013) we used
log φ̃(L)(u(1);θ) = K(L)∑",4.1. Model specification,[0],[0]
"j=1 fth ( z (L) j + b (L) j ) (25)
for L = 2, 3, 4 where fth is a smooth rectifying linear unit1 and b(L)j threshold parameters that are also learned from the data.",4.1. Model specification,[0],[0]
"The thresholding causes only strongly active units to contribute to log φ̃(L)(u(1);θ), which is related to sparse coding (Gutmann & Hyvärinen, 2012).",4.1. Model specification,[0],[0]
"In the case L = 1, the outputs z(1)j were passed through the additional nonlinearity log((·)2 + 1) prior to thresholding.",4.1. Model specification,[0],[0]
"This corresponds to computing the 2nd layer outputs with the 2nd layer weights fixed to correspond together to the identity matrix.
",4.1. Model specification,[0],[0]
"We learned the weights hierarchically one layer at a time, e.g. after learning of the 1st layer weights, we kept them fixed and learned the second layer weight vector w(2)j etc.",4.1. Model specification,[0],[0]
"The learned features, i.e receptive fields (RFs) of the 1st layer neurons, can be visualised as images.",4.2. Estimation results,[0],[0]
The learned 2nd layer weight vectors are sparse and the non-zero weights indicate over which 1st layer units the pooling happens.,4.2. Estimation results,[0],[0]
"In Figure 5, we visualise randomly selected 2nd layer units,
1fth(u) = 0.25 log(cosh(2u))",4.2. Estimation results,[0],[0]
"+ 0.5u+ 0.17
and the 1st layer units that they pool together.",4.2. Estimation results,[0],[0]
"The 1st layer has learned Gabor features (Hyvärinen et al., 2009, Chapter 3) and the 2nd layer tends to pool these features according to frequency, orientation and locality, in line with previous models of natural images (Hyvärinen et al., 2009).
",4.2. Estimation results,[0],[0]
"To visualise the learned weights on the 3rd layer, we followed (Gutmann & Hyvärinen, 2013) and visualised them as space-orientation receptive fields.",4.2. Estimation results,[0],[0]
"That is, we probed the learned neural network with Gabor stimuli at different locations, orientations, and frequency, and visualised the response of the 3rd layer units as a polar plot.",4.2. Estimation results,[0],[0]
"The polar plot is centred on the probing location, and the maximal radius is an indicator of the envelope and hence spatial frequency of the Gabor stimulus (larger circles correspond to lower spatial frequencies).",4.2. Estimation results,[0],[0]
"We visualised the pooling on the 4th layer as for the 2nd layer by indicating the pooling strength with bars underneath the space-orientation receptive fields.
",4.2. Estimation results,[0],[0]
Figure 6 shows examples of the learned 3rd and 4th layer units as well natural image inputs that elicit strong responses for the 4th layer units shown.,4.2. Estimation results,[0],[0]
"The learned 3rd layer units detect longer straight or bended contours, which is largely in line with previous findings (Gutmann & Hyvärinen, 2013).",4.2. Estimation results,[0],[0]
The learned 4th layer unit on the top in the figure (unit 4) has learned to pool together 3rd layer units that share the same spatial orientation preference but are tuned to different spatial frequencies.,4.2. Estimation results,[0],[0]
"This is line with previous modelling results (Hyvärinen et al., 2005) where similar pooling emerged in a model with more restrictive assumptions.",4.2. Estimation results,[0],[0]
"The learned 4th layer unit shown on the bottom (unit 19) is tuned to vertical and horizontal low-frequency structure that bend around the southwest corner, which corresponds to a low-frequency corner detector.",4.2. Estimation results,[0],[0]
"The full set of learned units is shown in the same way in Supplementary Materials D. Overall, the results show that CNCE both yields results that are in line with previous work and further finds novel and intuitively reasonable pooling patterns on the newly considered fourth layer.",4.2. Estimation results,[0],[0]
"In this paper, we addressed the problem of density estimation for unnormalised models where the normalising partition function cannot be computed.",5. Conclusions,[0],[0]
We proposed a new method that follows the principles of noise-contrastive estimation and “learning by comparison”.,5. Conclusions,[0],[0]
"In contrast to noisecontrastive estimation (NCE), in the proposed conditional noise-contrastive estimation (CNCE), the contrastive noise is allowed to depend on the data.
",5. Conclusions,[0],[0]
"The main advantage of allowing the noise distribution to depend on the data is that the information in the data can be leveraged to produce, with rather simple conditional noise distributions as for example a Gaussian, noise samples that are well adapted to a wide range of different data and model
2 4 6 8 10 12 14 16 18 20 22
types.",5. Conclusions,[0],[0]
"A second advantage is that for symmetric conditional noise distributions, a closed form expression for the conditional noise is not needed, which both enables a wider choice of distributions and has computational benefits.",5. Conclusions,[0],[0]
"If the value of the normalisation constant is not of interest, a third advantage of the proposed approach is that the intractable partition function cancels out.",5. Conclusions,[0],[0]
"Unlike in noise-contrastive estimation, there is thus never a need to introduce an additional parameter for the scaling of the model.
",5. Conclusions,[0],[0]
We provided theoretical and empirical arguments that CNCE provides a consistent estimator and proved that score matching emerges as a limiting case.,5. Conclusions,[0],[0]
"As score matching makes more stringent assumptions but does not rely on sampling, it is an open question whether we can use this result to e.g. devise a hybrid approach where parts of the model are automatically estimated with the more suitable method.
",5. Conclusions,[0],[0]
"We further found that the relative performances of NCE and CNCE are model dependent, but that CNCE has an
advantage in the important case where the data lie in a lower dimensional manifold.
",5. Conclusions,[0],[0]
"An inherent limitation of empirical comparisons, and hence also those performed here, is that the results depend on the models and noise distributions used.",5. Conclusions,[0],[0]
"However, given the adaptive nature of CNCE, simple Gaussian conditional noise distributions are likely widely useful, as exemplified by our results on unsupervised deep learning of a neural image model.
",5. Conclusions,[0],[0]
"The proposed method further allows one to iteratively adapt the conditional noise distribution to make the classification task successively more challenging, as it was done in some simulations for NCE (Gutmann & Hyvärinen, 2010), and generally for learning in generative latent variable models (Gutmann et al., 2014; Goodfellow et al., 2014).",5. Conclusions,[0],[0]
This is an interesting direction of future work on CNCE.,5. Conclusions,[0],[0]
"MUG would like to thank Jun-ichiro Hirayama at ATR and RIKEN AIP, Japan, for helpful discussions.",Acknowledgements,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgements,[0],[0]
"Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult.",abstractText,[0],[0]
"Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning.",abstractText,[0],[0]
"In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise.",abstractText,[0],[0]
An open question is how to best choose the auxiliary noise distribution.,abstractText,[0],[0]
We here propose a new method that addresses this issue.,abstractText,[0],[0]
"The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples.",abstractText,[0],[0]
The noise can thus be generated in a semiautomated manner.,abstractText,[0],[0]
"We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold.",abstractText,[0],[0]
Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.,abstractText,[0],[0]
Conditional Noise-Contrastive Estimation of Unnormalised Models,title,[0],[0]
"Ensemble methods have played a critical role in the machine learning community to obtain better predictive performance than what could be obtained from any of the constituent learning models alone, e.g., Bayesian model/parameter averaging (Domingos, 2000), boosting (Freund et al., 1999) and bagging (Breiman, 1996).",1. Introduction,[0],[0]
"Recently, they have been successfully applied to enhancing the power of many deep neural networks, e.g., 80% of
1School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Repulic of Korea.",1. Introduction,[0],[0]
"Correspondence to: Jinwoo Shin <jinwoos@kaist.ac.kr>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
top-5 best-performing teams on ILSVRC challenge 2016 (Krizhevsky et al., 2012) employ ensemble methods.",1. Introduction,[0],[0]
They are easy and trustworthy to apply for most scenarios.,1. Introduction,[0],[0]
"While there exists a long history on ensemble methods, the progress on developing more advanced ensembles specialized for deep neural networks has been slow.",1. Introduction,[0],[0]
"Despite continued efforts that apply various ensemble methods such as bagging and boosting to deep models, it has been observed that traditional independent ensembles (IE) which train models independently with random initialization achieve the best performance (Ciregan et al., 2012; Lee et al., 2015).",1. Introduction,[0],[0]
"In this paper, we focus on developing more advanced ensembles for deep models utilizing the concept of multiple choice learning (MCL).
",1. Introduction,[0],[0]
"The MCL concept was originally proposed in (GuzmanRivera et al., 2012) under the scenario when inference procedures are cascaded:
(a) First, generate a set of plausible outputs.
",1. Introduction,[0],[0]
"(b) Then, pick the correct solution form the set.
",1. Introduction,[0],[0]
"For example, (Park & Ramanan, 2011; Batra et al., 2012) proposed human-pose estimation methods which produce multiple predictions and then refine them by employing a temporal model, and (Collins & Koo, 2005) proposed a sentence parsing method which re-ranks the output of an initial system which produces a set of plausible outputs (Huang & Chiang, 2005).",1. Introduction,[0],[0]
"In such scenarios, the goal of the first stage (a) is generating a set of plausible outputs such that at least one of them is correct for the second stage (b), e.g., human operators.",1. Introduction,[0],[0]
"Under this motivation, MCL has been studied (Guzman-Rivera et al., 2014; 2012; Lee et al., 2016), where various applications have been demonstrated, e.g., image classification (Krizhevsky & Hinton, 2009), semantic segmentation (Everingham et al., 2010) and image captioning (Lin et al., 2014b).",1. Introduction,[0],[0]
"It trains an ensemble of multiple models by minimizing the so-called oracle loss, only focusing on the most accurate prediction produced by them.",1. Introduction,[0],[0]
"Consequently, it makes each model specialized for a certain subset of data, not for the entire one similarly as mixture-of-expert schemes (Jacobs et al., 1991).
",1. Introduction,[0],[0]
"Although MCL focuses on the first stage (a) in cascaded scenarios and thus can produce diverse/plausible outputs, it might be not useful if one does not have a good scheme for
the second stage (b).",1. Introduction,[0],[0]
"One can use a certain average/voting scheme of the predictions made by models for (b), but MCL using deep neural networks often fails to make a correct decision since each network tends to be overconfident in its prediction.",1. Introduction,[0],[0]
"Namely, the oracle error/loss of MCL is low, but its top-1 error rate might be very high.
",1. Introduction,[0],[0]
Contribution.,1. Introduction,[0],[0]
"To address the issue, we develop the concept of confident MCL (CMCL) that does not lose any benefit of the original MCL, while its target loss and architecture are redesigned for making the second stage (b) easier.",1. Introduction,[0],[0]
"Specifically, it targets to generate a set of diverse/plausible confident predictions from which one can pick the correct one using a simple average/voting scheme.",1. Introduction,[0],[0]
"To this end, we first propose a new loss function, called confident oracle loss, for relaxing the overconfidence issue of MCL.",1. Introduction,[0],[0]
Our key idea is to additionally minimize the Kullback-Leibler divergence from a predictive distribution to the uniform one in order to give confidence to non-specialized models.,1. Introduction,[0],[0]
"Then, CMCL that minimizes the new loss can be efficiently trained like the original MCL for certain classes of models including neural networks, via stochastic alternating minimization (Lee et al., 2016).",1. Introduction,[0],[0]
"Furthermore, when CMCL is applied to deep models, we propose two additional regularization techniques for boosting its performance: feature sharing and stochastic labeling.",1. Introduction,[0],[0]
"Despite the new components, we note that the training complexity of CMCL is almost same to that of MCL or IE.
We apply the new ensemble model trained by the new training scheme for several convolutional neural networks (CNNs) including VGGNet (Simonyan & Zisserman, 2015), GoogLeNet (Szegedy et al., 2015), and ResNet (He et al., 2016) for image classification on the CIFAR (Krizhevsky & Hinton, 2009) and SVHN (Netzer et al., 2011) datasets, and fully-convolutional neural networks (FCNs) (Long et al., 2015) for foreground-background segmentation on the iCoseg dataset (Batra et al., 2010).",1. Introduction,[0],[0]
"First, for the image classification task, CMCL outperforms all baselines, i.e., the traditional IE and the original MCL, in top-1 error rates.",1. Introduction,[0],[0]
"In particular, CMCL of 5 ResNet with 20 layers provides 14.05% and 6.60% relative reductions in the top-1 error rates from the corresponding IE on CIFAR-10 and SVHN, respectively.",1. Introduction,[0],[0]
"Second, for the foreground-background segmentation task, CMCL using multiple FCNs with 4 layers also outperforms all baselines in top-1 error rates.",1. Introduction,[0],[0]
Each model trained by CMCL generates high-quality solutions by specializing for specific images while each model trained by IE does not.,1. Introduction,[0],[0]
"We believe that our new approach should be of broader interest for many deep learning tasks requiring high accuracy.
Organization.",1. Introduction,[0],[0]
"In Section 2, we introduce necessary backgrounds for multiple choice learning and the corresponding loss function.",1. Introduction,[0],[0]
"We describe the proposed loss and the corre-
sponding training scheme in Section 3.",1. Introduction,[0],[0]
Section 4 provides additional techniques for the proposed ensemble model.,1. Introduction,[0],[0]
Experimental results are reported in Section 5.,1. Introduction,[0],[0]
"In this section, we describe the basic concept of multiple choice learning (MCL) (Guzman-Rivera et al., 2014; 2012).",2.1. Multiple Choice Learning,[0],[0]
"Throughout this paper, we denote the set {1, . . .",2.1. Multiple Choice Learning,[0],[0]
", n} by [n] for positive integer n.",2.1. Multiple Choice Learning,[0],[0]
The MCL scheme is a type of ensemble learning that produces diverse outputs of high quality.,2.1. Multiple Choice Learning,[0],[0]
"Formally, given a training dataset D = {(xi, yi) | i ∈",2.1. Multiple Choice Learning,[0],[0]
"[N ], xi ∈ X , yi ∈ Y}, we consider an ensemble ofM models f , i.e., (f1, . . .",2.1. Multiple Choice Learning,[0],[0]
", fM ).",2.1. Multiple Choice Learning,[0],[0]
"For some task-specific loss function ` (y, f (x)), the oracle loss over the dataset D is defined as follows:
LO (D) = N∑ i=1",2.1. Multiple Choice Learning,[0],[0]
"min m∈[M ] ` (yi, fm (xi)) , (1)
while the traditional independent ensemble (IE) loss is
LE (D) =",2.1. Multiple Choice Learning,[0],[0]
N∑ i=1,2.1. Multiple Choice Learning,[0],[0]
"∑ m∈[M ] ` (yi, fm (xi)) .",2.1. Multiple Choice Learning,[0],[0]
"(2)
If all models have the same capacity and one can obtain the (global) optimum of the IE loss with respect to the model parameters, then all trained models should produce the same outputs, i.e., f1 = . . .",2.1. Multiple Choice Learning,[0],[0]
= fM .,2.1. Multiple Choice Learning,[0],[0]
"On the other hand, the oracle loss makes the most accurate model optimize the loss function ` (y, f (x)) for each data x. Therefore, MCL produces diverse outputs of high quality by forcing each model to be specialized on a part of the entire dataset.
",2.1. Multiple Choice Learning,[0],[0]
Minimizing the oracle loss (1) is harder than minimizing the independent ensemble loss (2) since the min function is a non-continuous function.,2.1. Multiple Choice Learning,[0],[0]
"To address the issue, (GuzmanRivera et al., 2012) proposed an iterative block coordinate decent algorithm and (Dey et al., 2015) reformulated this problem as a submodular optimization task in which ensemble models are trained sequentially in a boosting-like manner.",2.1. Multiple Choice Learning,[0],[0]
"However, when one considers an ensemble of deep neural networks, it is challenging to apply these methods since they require either costly retraining or sequential training.",2.1. Multiple Choice Learning,[0],[0]
"Recently, (Lee et al., 2016) overcame this issue by proposing a stochastic gradient descent (SGD) based algorithm.",2.1. Multiple Choice Learning,[0],[0]
"Throughout this paper, we primarily focus on ensembles of deep neural networks and use the SGD algorithm for optimizing the oracle loss (1) or its variants.",2.1. Multiple Choice Learning,[0],[0]
"The oracle loss (1) used for MCL is useful for producing diverse/plausible outputs, but it is often inappropriate for ap-
plications requiring a single choice, i.e., top-1 error.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"This is because ensembles of deep neural networks tend to be overconfident in their predictions, and it is hard to judge a better solution from their outputs.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"To explain this in more detail, we evaluate the performance of ensembles of convolutional neural networks (CNNs) for the image classification task on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009).",2.2. Oracle Loss for Top-1 Choice,[0],[0]
We train ensembles of 5 CNNs (two convolutional layers followed by a fully-connected layer) using MCL.,2.2. Oracle Loss for Top-1 Choice,[0],[0]
We also train the models using traditional IE which trains each model independently under different random initializations.,2.2. Oracle Loss for Top-1 Choice,[0],[0]
Figure 1 summarizes the class-wise test set accuracy of each ensemble member.,2.2. Oracle Loss for Top-1 Choice,[0],[0]
"In the case of MCL, most models become specialists for certain classes (see Figure 1(a)), while they are generalized in the case of traditional IE as shown in Figure 1(c).",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"However, as expected, each model trained by MCL significantly outperforms for its specialized classes than that trained by IE.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"For choosing a single output, similar to (Wan et al., 2013; Ciregan et al., 2012), one can average the output probabilities from ensemble members trained by MCL, but the corresponding top-1 classification error rate is often very high (e.g., see Table 1 in Section 5).",2.2. Oracle Loss for Top-1 Choice,[0],[0]
This is because each model trained by MCL is overconfident for its non-specialized classes.,2.2. Oracle Loss for Top-1 Choice,[0],[0]
"To quantify this, we also compute the entropy of the predictive distribution on the test data and use this to evaluate the quality of confidence/uncertainty level.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
Figure 2(a) reports the entropy extracted from the predictive distribution of one of ensemble models trained by MCL.,2.2. Oracle Loss for Top-1 Choice,[0],[0]
"One can observe that it has low entropy as expected for its specialized classes (i.e., classes that the model has a test accuracy higher than 90%).",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"However, even for non-specialized classes, it also has low entropy.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"Due to this, with respect to top-1 error rates, simple averaging of models trained by MCL performs much worse than that of IE.",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"Such issue typically occurs in deep neural networks since it is well known that they are poor at quantifying predictive uncertainties, and tend to be easily overconfident (Nguyen et al., 2015).",2.2. Oracle Loss for Top-1 Choice,[0],[0]
"In this section, we propose a modified oracle loss for relaxing the issue of MCL described in the previous section.",3.1. Confident Oracle Loss,[0],[0]
"Suppose that the m-th model outputs the predictive distribution Pθm (y | x) given input x, where θm denotes the model parameters.",3.1. Confident Oracle Loss,[0],[0]
"Then, we define the confident oracle loss as the following integer programming variant of (1):
LC(D) = min vmi N∑ i=1 M∑ m=1",3.1. Confident Oracle Loss,[0],[0]
"( vmi ` (yi, Pθm (y | xi))
",3.1. Confident Oracle Loss,[0],[0]
+ β,3.1. Confident Oracle Loss,[0],[0]
(1− vmi )DKL (U (y) ‖,3.1. Confident Oracle Loss,[0],[0]
Pθm (y | xi)) ),3.1. Confident Oracle Loss,[0],[0]
"(3a)
subject to M∑ m=1 vmi = 1, ∀i, (3b)
vmi ∈ {0, 1}, ∀i,m (3c)
whereDKL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution, β is a penalty parameter, and vmi is a flag variable to decide the assignment of xi to the m-th model.",3.1. Confident Oracle Loss,[0],[0]
"By minimizing the KL divergence from the predictive distribution to the uniform one, the new loss forces the predictive distribution to be closer to the uniform one, i.e., zero confidence, on non-specialized data, while those for specialized data still follow the correct one.",3.1. Confident Oracle Loss,[0],[0]
"For example, for classification tasks, the most accurate model for each data is allowed to optimize the classification loss, while others are forced to give less confident predictions by minimizing the KL divergence.",3.1. Confident Oracle Loss,[0],[0]
"We remark that although we optimize the KL divergence only for non-specialized data, one can also do it even for specialized data to regularize each model (Pereyra et al., 2017).",3.1. Confident Oracle Loss,[0],[0]
"In order to minimize the confident oracle loss (3) efficiently, we use the following procedure (Guzman-Rivera et al., 2012), which optimizes model parameters {θm} and assignment variables {vmi } alternatively:
1.",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Fix {θm} and optimize {vmi }.
",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Under fixed model parameters {θm}, the objective (3a) is decomposable with respect to assignments {vmi } and it is easy to find optimal {vmi }.
2.",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Fix {vmi } and optimize {θm}.
",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Under fixed assignments {vmi }, the objective (3a) is decomposable with respect to model parameters {θm}, and it requires each model to be trained independently.
",3.2. Stochastic Alternating Minimization for Training,[0],[0]
The above scheme iteratively assigns each data to a particular model and then independently trains each model only using its assigned data.,3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Even though it monotonically decreases the objective, it is still highly inefficient since it requires training each model multiple times until assignments {vmi } converge.",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"To address the issue, we propose deciding assignments and update model parameters to the gradient directions once per each batch, similarly to (Lee et al., 2016).",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"In other words, we perform a single gradientupdate on parameters in Step 2, without waiting for their convergence to a (local) optimum.",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"In fact, (Lee et al., 2016) show that such stochastic alternating minimization works well for the oracle loss (1).",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"We formally describe a detailed training procedure as the ‘version 0’ of Algorithm 1, and we will introduce the alternative ‘version 1’ later.",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"This direction is complementary to ours, and we do not explore in this paper.
",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Algorithm 1 Confident MCL (CMCL) Input: Dataset D = {(xi, yi) | xi ∈ X , yi ∈ Y} and penalty parameter β Output: Ensemble of M trained models repeat
Let U (y) be a uniform distribution Sample random batch B ⊂ D for m = 1 to M do
Compute the loss of the m-th model:
Lmi ←β ∑ m̂ 6=m DKL (U (y) ‖",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Pθm̂ (y | xi))
+ ` (yi, Pθm (yi | xi)) , ∀(xi, yi) ∈ B
end for for m = 1 to M do
for i = 1 to |B| do if the m-th model has the lowest loss then
Compute the gradient of the training loss ` (yi, Pθm (yi | xi))",3.2. Stochastic Alternating Minimization for Training,[0],[0]
w.r.t θm else /∗,3.2. Stochastic Alternating Minimization for Training,[0],[0]
version 0: exact gradient ∗/ Compute the gradient of the KL divergence βDKL,3.2. Stochastic Alternating Minimization for Training,[0],[0]
(U (y) ‖,3.2. Stochastic Alternating Minimization for Training,[0],[0]
Pθm (y | xi)),3.2. Stochastic Alternating Minimization for Training,[0],[0]
w.r.t θm /∗ version 1: stochastic labeling ∗/ Compute the gradient of the cross entropy loss −β logPθm (ŷi | xi) using ŷi,3.2. Stochastic Alternating Minimization for Training,[0],[0]
"w.r.t θm where ŷi ∼ U (y)
end if end for Update the model parameters
end for until convergence",3.2. Stochastic Alternating Minimization for Training,[0],[0]
"Similar to Section 2.2, we evaluate the performance of the proposed training scheme using 5 CNNs for image classification on the CIFAR-10 dataset.",3.3. Effect of Confident Oracle Loss,[0],[0]
"As shown in Figure 1(b), ensemble models trained by CMCL using the exact gradient (i.e., version 0 of Algorithm 1) become specialists for certain classes.",3.3. Effect of Confident Oracle Loss,[0],[0]
"For specialized classes, they show the similar performance compared to the models trained by MCL, i.e., minimizing the oracle loss (1), which considers only specialization (see Figure 1(a)).",3.3. Effect of Confident Oracle Loss,[0],[0]
"For non-specialized classes, ensemble members of CMCL are not overconfident, which makes it easy to pick a correct output via simple voting/averaging.",3.3. Effect of Confident Oracle Loss,[0],[0]
"We indeed confirm that each model trained by CMCL has not only low entropy for its specialized classes, but also exhibits high entropy for nonspecialized classes as shown in Figure 2(b).
",3.3. Effect of Confident Oracle Loss,[0],[0]
"We also evaluate the quality of confidence/uncertainty level on unseen data using SVHN (Netzer et al., 2011).",3.3. Effect of Confident Oracle Loss,[0],[0]
"Somewhat surprisingly, each model trained by CMCL only using CIFAR-10 training data exhibits high entropy for SVHN test data, whereas models trained by MCL and IE are overconfident on it (see Figure 2(a) and 2(c)).",3.3. Effect of Confident Oracle Loss,[0],[0]
"We emphasize that our method can produce confident predictions significantly better than the proposed method by (Lakshminarayanan et al., 2016), which uses the averaged probability of ensemble models trained by IE to obtain high quality uncertainty estimates (see Figure 2(c)).",3.3. Effect of Confident Oracle Loss,[0],[0]
"In this section, we introduce advanced techniques for reducing the overconfidence and improving the performance.",4. Regularization Techniques,[0],[0]
We first propose a feature sharing scheme that stochastically shares the features among member models of CMCL to further address the overconfidence issue.,4.1. Feature Sharing,[0],[0]
The primary reason why deep learning models are overconfident is that they do not always extract general features from data.,4.1. Feature Sharing,[0],[0]
"For examples, assume that some deep model only trains frogs and roses for classifying them.",4.1. Feature Sharing,[0],[0]
"Although there might exist many kinds of features on their images, the model might make a decision based only on some specific features, e.g., colors.",4.1. Feature Sharing,[0],[0]
"In this case, ‘red’ apples can be classified as rose with high confidence.",4.1. Feature Sharing,[0],[0]
Such an issue might be more severe in CMCL (and MCL) compared to IE since members of CMCL are specialized to certain data.,4.1. Feature Sharing,[0],[0]
"To address the issue, we suggest the feature ensemble approach that encourages each model to generate meaningful abstractions from rich features extracted from other models.
",4.1. Feature Sharing,[0],[0]
"Formally, consider an ensemble ofM neural networks with L hidden layers.",4.1. Feature Sharing,[0],[0]
"We denote the weight matrix for layer
` of model m ∈",4.1. Feature Sharing,[0],[0]
"[M ] and `-th hidden feature of model m by W`m and h ` m, respectively.",4.1. Feature Sharing,[0],[0]
"Instead of sharing the whole units of a hidden feature, we introduce random binary masks determining which units to be shared with other models.",4.1. Feature Sharing,[0],[0]
"We denote the mask for layer ` from model n to m as σ`nm ∼ Bernoulli(λ), which has the same dimension with h`n (we use λ = 0.7 in all experiments).",4.1. Feature Sharing,[0],[0]
"Then, the `-th hidden feature of model m with sharing (`− 1)-th hidden features is defined as follows:
h`m (x) = φ W`m h`−1m (x) + ∑
n 6=m
σ`nm ?",4.1. Feature Sharing,[0],[0]
h,4.1. Feature Sharing,[0],[0]
`−1 n,4.1. Feature Sharing,[0],[0]
"(x)  , where ? denotes element-wise multiplication and φ is the activation function.",4.1. Feature Sharing,[0],[0]
Figure 2(d) illustrates the proposed feature sharing scheme in an ensemble of deep neural networks.,4.1. Feature Sharing,[0],[0]
It makes each model learn more generalized features by sharing the features among them.,4.1. Feature Sharing,[0],[0]
"However, one might expect that it might make each model overfitted due to the increased number of parameters that induces a single prediction, i.e., the statistical dependencies among outputs of models increase, which would hurt the ensemble effect.",4.1. Feature Sharing,[0],[0]
"In order to handle this issue, we introduce the randomness in sharing across models in a similar manner to DropOut (Srivastava et al., 2014) using the random binary masks σ .",4.1. Feature Sharing,[0],[0]
"In addition, we propose sharing features at lower layers since sharing the higher layers might overfit the overall networks more.",4.1. Feature Sharing,[0],[0]
"For example, in all experiments with CNNs in this paper, we commonly apply feature sharing for hidden features just before the first pooling layer.",4.1. Feature Sharing,[0],[0]
"We also remark that such feature sharing strategies for better generalization have also been investigated in the literature for different purposes (Misra et al., 2016; Rusu et al., 2016).",4.1. Feature Sharing,[0],[0]
"For more efficiency in minimizing the confident oracle loss, we also consider a noisy unbiased estimator of gradients of the KL divergence with Monte Carlo samples from the uniform distribution.",4.2. Stochastic Labeling,[0],[0]
"The KL divergence from the predictive distribution to the uniform distribution can be written as follows:
DKL (U (y) ‖",4.2. Stochastic Labeling,[0],[0]
Pθ (y | x)),4.2. Stochastic Labeling,[0],[0]
= ∑ y U (y) log U (y),4.2. Stochastic Labeling,[0],[0]
"Pθ (y | x)
= ∑ y U (y)",4.2. Stochastic Labeling,[0],[0]
"logU (y)− ∑ y U (y) logPθ (y | x).
",4.2. Stochastic Labeling,[0],[0]
"Hence, the gradient of the above KL divergence with respect to the model parameter θ becomes
5θDKL (U (y) ‖",4.2. Stochastic Labeling,[0],[0]
Pθ (y | x)),4.2. Stochastic Labeling,[0],[0]
"= −EU(y)[5θlogPθ (y | x)].
",4.2. Stochastic Labeling,[0],[0]
"From the above, we induce the following noisy unbiased estimator of gradients with Monte Carlo samples from the uniform distribution:
− EU(y)[5θlogPθ (y | x)]",4.2. Stochastic Labeling,[0],[0]
"w − 1
S ∑ s 5θlogPθ (ys | x),
where ys ∼ U (y) and S is the number of samples.",4.2. Stochastic Labeling,[0],[0]
This random estimator takes samples from the uniform distribution U (y) and constructs estimates of the gradient using them.,4.2. Stochastic Labeling,[0],[0]
"In other words, 5θlogPθ (ys | x) is the gradient of the cross entropy loss under assigning a random label to x.",4.2. Stochastic Labeling,[0],[0]
This stochastic labeling provides efficiency in implementation/computation and stochastic regularization effects.,4.2. Stochastic Labeling,[0],[0]
"We formally describe detailed procedures, as the version 1 of Algorithm 1.",4.2. Stochastic Labeling,[0],[0]
"We evaluate our algorithm for both classification and foreground-background segmentation tasks using CIFAR10 (Krizhevsky & Hinton, 2009), SVHN (Netzer et al., 2011) and iCoseg (Batra et al., 2010) datasets.",5. Experiments,[0],[0]
"In all experiments, we compare the performance of CMCL with those of traditional IE and MCL using deep models.",5. Experiments,[0],[0]
We provide the more detailed experimental setups including model architectures in the supplementary material.1,5. Experiments,[0],[0]
Setup.,5.1. Image Classification,[0],[0]
"The CIFAR-10 dataset consists of 50,000 training and 10,000 test images with 10 image classes where each image consists of 32× 32 RGB pixels.",5.1. Image Classification,[0],[0]
"The SVHN dataset consists of 73,257 training and 26,032 test images.2 We pre-process the images with global contrast normalization and ZCA whitening following (Ian J. Goodfellow & Bengio, 2013; Zagoruyko & Komodakis, 2016), and do not use any data augmentation.",5.1. Image Classification,[0],[0]
"Using these datasets, we train various CNNs, e.g., VGGNet (Simonyan & Zisserman, 2015), GoogLeNet",5.1. Image Classification,[0],[0]
"(Szegedy et al., 2015), and ResNet (He et al., 2016).",5.1. Image Classification,[0],[0]
"Similar to (Zagoruyko & Komodakis, 2016), we use the softmax classifier, and train each model by minimizing the cross-entropy loss using the stochastic gradient descent method with Nesterov momentum.
",5.1. Image Classification,[0],[0]
"For evaluation, we measure the top-1 and oracle error rates on the test dataset.",5.1. Image Classification,[0],[0]
The top-1 error rate is calculated by averaging output probabilities from all models and predicting the class of the highest probability.,5.1. Image Classification,[0],[0]
"The oracle error rate is the rate of classification failure over all outputs of individual ensemble members for a given input, i.e., it measures whether none of the members predict the correct class for
1Our code is available at https://github.com/ chhwang/cmcl.
2We do not use the extra SVHN dataset for training.
an input.",5.1. Image Classification,[0],[0]
"While a lower oracle error rate suggests higher diversity, a lower oracle error rate does not always bring a higher top-1 accuracy as this metric does not reveal the level of overconfidence of each model.",5.1. Image Classification,[0],[0]
"By collectively measuring the top-1 and oracle error rates, one can grasp the level of specialization and confidence of a model.
",5.1. Image Classification,[0],[0]
Contribution by each technique.,5.1. Image Classification,[0],[0]
Table 1 validates contributions of our suggested techniques under comparison with other ensemble methods IE and MCL.,5.1. Image Classification,[0],[0]
We evaluate an ensemble of five simple CNN models where each model has two convolutional layers followed by a fully-connected layer.,5.1. Image Classification,[0],[0]
We incrementally apply our optimizations to gauge the stepwise improvement by each component.,5.1. Image Classification,[0],[0]
One can note that CMCL significantly outperforms MCL in the top1 error rate even without feature sharing or stochastic labeling while it still provides a comparable oracle error rate.,5.1. Image Classification,[0],[0]
"By sharing the 1st ReLU activated features, the top-1 error rates are improved compared to those that employ only confident oracle loss.",5.1. Image Classification,[0],[0]
Stochastic labeling further improves both error rates.,5.1. Image Classification,[0],[0]
"This implies that stochastic labeling not only reduces computational burdens but also provides regularization effects.
",5.1. Image Classification,[0],[0]
Overlapping.,5.1. Image Classification,[0],[0]
"As a natural extension of CMCL, we also consider picking K specialized models instead of having only one specialized model, which was investigated for original MCL (Guzman-Rivera et al., 2012; Lee et al., 2016).",5.1. Image Classification,[0],[0]
"This is easily achieved by modifying the constraint (3b) as ∑M m=1 v m i = K, where K is an overlap parameter that controls training data overlap between the models.",5.1. Image Classification,[0],[0]
This simple but natural scheme brings extra gain in top-1 performance by generalizing each model better.,5.1. Image Classification,[0],[0]
"Table 2 compares the performance of various ensemble methods with varying values of K. Under the choice of K = 4, CMCL of 10 CNNs provides 9.13% relative reduction in the top-1 error rates from the corresponding IE.",5.1. Image Classification,[0],[0]
"Somewhat interestingly, IE has similar error rates on ensembles of both 5 and 10 CNNs, which implies that the performance of CMCL might be impossible to achieve using IE even if one increases the number of models in IE.
",5.1. Image Classification,[0],[0]
Large-scale CNNs.,5.1. Image Classification,[0],[0]
"We now evaluate the performance of our ensemble method when it is applied to larger-scale CNN models for image classification tasks on CIFAR-10
and SVHN datasets.",5.1. Image Classification,[0],[0]
"Specifically, we test VGGNet (Simonyan & Zisserman, 2015), GoogLeNet (Szegedy et al., 2015), and ResNet (He et al., 2016).",5.1. Image Classification,[0],[0]
"We share the nonlinear activated features right before the first pooling layer, i.e., the 6th, 2nd, and 1st ReLU activations for ResNet with 20 layers, VGGNet with 17 layers, and GoogLeNet with 18 layers, respectively.",5.1. Image Classification,[0],[0]
This choice is for maximizing the regularization effect of feature sharing while minimizing the statistical dependencies among the ensemble models.,5.1. Image Classification,[0],[0]
"For all models, we choose the best hyperparameters for confident oracle loss among the penalty parameter β ∈ {0.5, 0.75, 1, 1.25, 1.5} and the overlapping parameter K ∈ {2, 3, 4}.",5.1. Image Classification,[0],[0]
Table 3 shows that CMCL consistently outperforms all baselines with respect to the top-1 error rate while producing comparable oracle error rates to those of MCL.,5.1. Image Classification,[0],[0]
We also apply the feature sharing to IE as reported in Figure 4(a).,5.1. Image Classification,[0],[0]
"Even though the feature sharing also improves the performance of IE, CMCL still outperforms IE: CMCL provides 6.11% relative reduction of the top-1
error rate from the IE with feature sharing under the choice of M = 10.",5.1. Image Classification,[0],[0]
"We also remark that IE with feature sharing has similar error rates as the ensemble size increases, while CMCL does not (i.e., the gain is more significant for CMCL).",5.1. Image Classification,[0],[0]
This implies that feature sharing is more effectively working for CMCL.,5.1. Image Classification,[0],[0]
"In this section, we evaluate if ensemble models trained with CMCL produce high-quality segmentation of foreground and background of an image with the iCoseg dataset.",5.2. Foreground-Background Segmentation,[0],[0]
"The foreground-background segmentation is formulated as a pixel-level classification problem with 2 classes, i.e., 0 (background) or 1 (foreground).",5.2. Foreground-Background Segmentation,[0],[0]
"To tackle the problem, we design fully convolutional networks (FCNs) model (Long et al., 2015) based on the decoder architecture presented in (Radford et al., 2016).",5.2. Foreground-Background Segmentation,[0],[0]
The dataset consists of 38 groups of related images with pixel-level ground truth on foregroundbackground segmentation of each image.,5.2. Foreground-Background Segmentation,[0],[0]
"We only use im-
ages that are larger than 300 × 500 pixels.",5.2. Foreground-Background Segmentation,[0],[0]
"For each class, we randomly split 80% and 20% of the data into training and test sets, respectively.",5.2. Foreground-Background Segmentation,[0],[0]
"We train on 75 × 125 resized images using the bicubic interpolation (Keys, 1981).",5.2. Foreground-Background Segmentation,[0],[0]
"Similar to (Guzman-Rivera et al., 2012; Lee et al., 2016), we initialize the parameters of FCNs with those trained by IE for MCL and CMCL.",5.2. Foreground-Background Segmentation,[0],[0]
"For all experiments, CMCL is used with both feature sharing and stochastic labeling.
",5.2. Foreground-Background Segmentation,[0],[0]
"Similar to (Guzman-Rivera et al., 2012), we define the percentage of incorrectly labeled pixels as prediction error rate.",5.2. Foreground-Background Segmentation,[0],[0]
"We measure the oracle error rate (i.e., the lowest error rate over all models for a given input) and the top-1 error rate.",5.2. Foreground-Background Segmentation,[0],[0]
"The top-1 error rate is measured by following the predictions of the member model that has a lower pixel-wise entropy, i.e., picking the output of a more confident model.",5.2. Foreground-Background Segmentation,[0],[0]
"For each ensemble method, we vary the number of ensemble models and measure the oracle error rate and test error rate.",5.2. Foreground-Background Segmentation,[0],[0]
Figure 4(b) and 4(c) show both top-1 and oracle error rates for all ensemble methods.,5.2. Foreground-Background Segmentation,[0],[0]
We remark that the ensemble models trained by CMCL consistently improves the top-1 error rate over baselines.,5.2. Foreground-Background Segmentation,[0],[0]
"In an ensemble of 5 models, we find that CMCL achieve up to 6.77% relative reduction
in the top-1 error rate from the corresponding IE.",5.2. Foreground-Background Segmentation,[0],[0]
"As shown in Figure 3, an individual model trained by CMCL generates high-quality solutions by specializing itself in specific images (e.g., model 1 is specialized for ‘lobster’",5.2. Foreground-Background Segmentation,[0],[0]
while model 2 is specialized for ‘duck’) while each model trained by IE does not.,5.2. Foreground-Background Segmentation,[0],[0]
"This paper proposes CMCL, a novel ensemble method of deep neural networks that produces diverse/plausible confident prediction of high quality.",6. Conclusion,[0],[0]
"To this end, we address the over-confidence issues of MCL, and propose a new loss, architecture and training method.",6. Conclusion,[0],[0]
"In our experiments, CMCL outperforms not only the known MCL, but also the traditional IE, with respect to the top-1 error rates in classification and segmentation tasks.",6. Conclusion,[0],[0]
The recent trend in the deep learning community tends to make models bigger and wider.,6. Conclusion,[0],[0]
We believe that our new ensemble approach brings a refreshing angle for developing advanced large-scale deep neural networks in many related applications.,6. Conclusion,[0],[0]
"This work was supported in part by the ICT R&D Program of MSIP/IITP, Korea, under [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion], R0190-162012, [High Performance Big Data Analytics Platform Performance Acceleration Technologies Development], and by the National Research Council of Science & Technology (NST) grant by the Korea government (MSIP) (No. CRC-15-05-ETRI).",Acknowledgements,[0],[0]
Ensemble methods are arguably the most trustworthy techniques for boosting the performance of machine learning models.,abstractText,[0],[0]
"Popular independent ensembles (IE) relying on naı̈ve averaging/voting scheme have been of typical choice for most applications involving deep neural networks, but they do not consider advanced collaboration among ensemble models.",abstractText,[0],[0]
"In this paper, we propose new ensemble methods specialized for deep neural networks, called confident multiple choice learning (CMCL): it is a variant of multiple choice learning (MCL) via addressing its overconfidence issue.",abstractText,[0],[0]
"In particular, the proposed major components of CMCL beyond the original MCL scheme are (i) new loss, i.e., confident oracle loss, (ii) new architecture, i.e., feature sharing and (iii) new training method, i.e., stochastic labeling.",abstractText,[0],[0]
"We demonstrate the effect of CMCL via experiments on the image classification on CIFAR and SVHN, and the foregroundbackground segmentation on the iCoseg.",abstractText,[0],[0]
"In particular, CMCL using 5 residual networks provides 14.05% and 6.60% relative reductions in the top-1 error rates from the corresponding IE scheme for the classification task on CIFAR and SVHN, respectively.",abstractText,[0],[0]
Confident Multiple Choice Learning,title,[0],[0]
"Real-world applications of binary classification to complex decision problems have led to the design of a wide range of evaluation metrics (Choi & Cha, 2010).",1. Introduction,[0],[0]
"Prominent examples include area under the ROC curve (AUC) for imbalanced labels (Menon et al., 2013), F-measure for information retrieval (Lewis, 1995), and precision at the top (Kar et al., 2014; 2015; Jasinska et al., 2016).",1. Introduction,[0],[0]
"To this end, several algorithms have been proposed for optimizing many of these metrics, primarily focusing on large-scale learning, without a conscious emphasis on statistical consequences of choosing models and their asymptotic behavior (Kar et al., 2015; Joachims, 2005).",1. Introduction,[0],[0]
"Wide use of such complex metrics has also re-invigorated research into their theoretical properties, which can then serve as a guide to prac-
Authors listed in the alphabetical order 1Institute of Computing Science, Poznan University of Technology, Poland 2Department of Computer Science, University of Illinois at UrbanaChampaign, USA 3Microsoft Research, India.",1. Introduction,[0],[0]
"Correspondence to: Wojciech Kotłowski <wkotlowski@cs.put.poznan.pl>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"tice (Koyejo et al., 2014a; Narasimhan et al., 2014a; Dembczyński et al., 2012; Waegeman et al., 2014; Natarajan et al., 2016).
",1. Introduction,[0],[0]
"Complex evaluation metrics for binary classification are best described as set metrics, or non-decomposable metrics – as, in general, the evaluation for a set of predictions cannot be decomposed into the average of individual instance evaluations.",1. Introduction,[0],[0]
This is in contrast to decomposable metrics such as accuracy which are defined as the empirical average of the instance evaluations.,1. Introduction,[0],[0]
"This property is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency.",1. Introduction,[0],[0]
"On one hand, Population Utility (PU) focuses on estimation – so a consistent PU classifier is one which correctly estimates the population optimal utility as the size of the training set (equiv.",1. Introduction,[0],[0]
test set) increases.,1. Introduction,[0],[0]
The PU approach has strongest roots in classical statistical analysis which often deals with asymptotically optimal estimation.,1. Introduction,[0],[0]
"On the other hand, Expected Test Utility (ETU) focuses on generalization.",1. Introduction,[0],[0]
"Thus, the consistent ETU classifier is one which optimizes the expected prediction error over test sets of a pre-defined size.",1. Introduction,[0],[0]
The ETU approach has strongest roots in statistical machine learning which prizes generalization as the primary goal.,1. Introduction,[0],[0]
"Importantly, these distinctions are irrelevant when the metric is a linear function of the confusion matrix e.g. (weighted) accuracy and other linear metrics.",1. Introduction,[0],[0]
"To the best of our knowledge, this dichotomy was first explicitly noted by Ye et al. (2012) in the context of F-measure.1 Like in Ye et al. (2012), our goal is not to adjudicate the correctness of either approach, but instead to explore deep connections, and highlight significant differences between both approaches for a wide range of metrics.
",1. Introduction,[0],[0]
"Contributions: We present a variety of results comparing and contrasting the PU and ETU approaches for consistent classification:
• We show that for a wide range of metrics, PU and ETU are asymptotically equivalent with respect to the size of the test set, subject to a certain p-Lipschitzness
1Note that Ye et al. (2012) termed the two approaches Empirical Utility Maximization (EUM) and Decision Theoretic Approach (DTA), respectively.",1. Introduction,[0],[0]
"We have instead chosen the more descriptive names Population Utility (PU) and Expected Test Utility (ETU).
condition which is satisfied by many metrics of interest.",1. Introduction,[0],[0]
This further implies asymptotic equivalence of the Bayes optimal classifiers (Section 3.1).,1. Introduction,[0],[0]
"Similar results were previously only known for F-measure.
",1. Introduction,[0],[0]
"• We provide lower bounds for the difference between PU and ETU metrics for finite test sets, and for certain metrics – thereby highlighting the difference between PU and ETU consistent classifiers with small test sets (Section 3.2).
",1. Introduction,[0],[0]
"• We analyze approximate ETU classification using low order Taylor approximations, showing that the approximation can be computed with effectively linear complexity, yet achieves low error under standard assumptions (Section 4.1).
",1. Introduction,[0],[0]
"• We consider the effects of model mis-specification and find that ETU may be more sensitive than PU, but this may be alleviated by properly calibrating the estimated probabilities (Section 4.2).
",1. Introduction,[0],[0]
"In addition, we present experimental results using simulated and real data to evaluate our theoretical claims (Section 5).",1. Introduction,[0],[0]
"We consider the binary classification problem, where the input is a feature vector x ∈ X , and the output is a label y ∈ {0, 1}.",2. Preliminaries and Problem Setup,[0],[0]
"We assume the examples (x, y) are generated i.i.d.",2. Preliminaries and Problem Setup,[0],[0]
"according to P(x, y).",2. Preliminaries and Problem Setup,[0],[0]
"A classifier is a mapping h : X → {0, 1}.",2. Preliminaries and Problem Setup,[0],[0]
"We let 1C denote the indicator function i.e. equal to one if C is satisfied, and zero otherwise.
",2. Preliminaries and Problem Setup,[0],[0]
"Given a distribution P and a binary classifier h, define:
TP(h) =",2. Preliminaries and Problem Setup,[0],[0]
"P(h = 1, y = 1), TN(h) = P(h = 0, y = 0), FP(h) =",2. Preliminaries and Problem Setup,[0],[0]
"P(h = 1, y = 0), FN(h) =",2. Preliminaries and Problem Setup,[0],[0]
"P(h = 0, y = 1),
which are entries of the so-called confusion matrix, namely true positives, true negatives, false positives and false negatives.",2. Preliminaries and Problem Setup,[0],[0]
"In this paper, we are interested in optimizing performance metrics Φ(h,P) (we use explicit dependence on P because we will also consider the empirical version of Φ) that are functions of the above four quantities.",2. Preliminaries and Problem Setup,[0],[0]
"However, since the entries of the confusion matrix are interdependent, it suffices to only use their three independent combinations.",2. Preliminaries and Problem Setup,[0],[0]
"Following Natarajan et al. (2016), we parametrize Φ(h,P) = Φ(u(h), v(h), p) by means of:
u(h) = TP(h), v(h) =",2. Preliminaries and Problem Setup,[0],[0]
"P(h = 1), and p = P(y = 1).
",2. Preliminaries and Problem Setup,[0],[0]
"As argued by Natarajan et al. (2016), any metric being a function of the confusion matrix can be parameterized in this way.",2. Preliminaries and Problem Setup,[0],[0]
"Table 1 lists popular examples of such metrics
with explicit parameterization Φ(u, v, p).",2. Preliminaries and Problem Setup,[0],[0]
"Throughout the paper we assume Φ(u, v, p) is bounded from above and from below.2",2. Preliminaries and Problem Setup,[0],[0]
Definition 1 (Population Utility (PU)).,2.1. Formal Definitions of PU and ETU,[0],[0]
"Given a distribution P and classifier h, the PU of h for a performance metric Φ is defined as Φ(u(h), v(h), p).",2.1. Formal Definitions of PU and ETU,[0],[0]
"We let h∗PU denote any maximizer of the PU,
h∗PU ∈ argmax h Φ(u(h), v(h), p) .
",2.1. Formal Definitions of PU and ETU,[0],[0]
"In words, the PU is obtained by taking the value of metric Φ evaluated at the expected confusion matrix of h over P. Thus, one can think of the PU as evaluating the classifier h on a “single test set of infinite size” drawn i.i.d.",2.1. Formal Definitions of PU and ETU,[0],[0]
"from P.
",2.1. Formal Definitions of PU and ETU,[0],[0]
"In contrast, ETU evaluates the expected utility for a fixedsize test set.",2.1. Formal Definitions of PU and ETU,[0],[0]
"Formally, given a sample S = {(xi, yi)}ni=1 of size n, generated i.i.d.",2.1. Formal Definitions of PU and ETU,[0],[0]
"from P, we let û(h), v̂(h), p̂ denote the corresponding empirical quantities:
û(h)",2.1. Formal Definitions of PU and ETU,[0],[0]
"= 1
n n∑ i=1",2.1. Formal Definitions of PU and ETU,[0],[0]
"h(xi)yi, v̂(h) = 1 n n∑ i=1",2.1. Formal Definitions of PU and ETU,[0],[0]
"h(xi), p̂ = 1 n",2.1. Formal Definitions of PU and ETU,[0],[0]
"n∑ i=1 yi,
and the empirical value of metric Φ is then Φ(û(h), v̂(h), p̂).
",2.1. Formal Definitions of PU and ETU,[0],[0]
Definition 2 (Expected Test Utility (ETU)).,2.1. Formal Definitions of PU and ETU,[0],[0]
"Let x = (x1, . . .",2.1. Formal Definitions of PU and ETU,[0],[0]
", xn) ∈ Xn be an arbitrary sequence of inputs.",2.1. Formal Definitions of PU and ETU,[0],[0]
"Given a distribution P and a classifier h, the ETU of h for a performance metric Φ conditioned on x is defined as:3
Ey|x",2.1. Formal Definitions of PU and ETU,[0],[0]
"[ Φ ( û(h), v̂(h), p̂ )] ,
2In fact, for essentially all metrics used in practice it holds 0 ≤ Φ(u, v, p) ≤ 1.
3The conditional expectation y|x is defined up to a zeromeasure set (over x), but this does not create any problems as we always consider x being sampled from the data distribution.
where the expectation over y = (y1, . . .",2.1. Formal Definitions of PU and ETU,[0],[0]
", yn) is with respect to the conditional distribution P(y|x)",2.1. Formal Definitions of PU and ETU,[0],[0]
i.i.d.,2.1. Formal Definitions of PU and ETU,[0],[0]
over the examples.,2.1. Formal Definitions of PU and ETU,[0],[0]
"We let h∗ETU(x) denote any maximizer of the ETU,
h∗ETU(x) ∈ argmax h
Ey|x",2.1. Formal Definitions of PU and ETU,[0],[0]
"[ Φ ( û(h), v̂(h), p̂ )] .
",2.1. Formal Definitions of PU and ETU,[0],[0]
One can think of ETU as evaluating the classifier h on “infinitely many test sets of size n” drawn i.i.d.,2.1. Formal Definitions of PU and ETU,[0],[0]
from P. We will see (in Section 4) that the optimal predictions (in both PU and ETU approaches) can be accurately estimated using the conditional probabilities P(yi|xi).,2.1. Formal Definitions of PU and ETU,[0],[0]
"In practice, we first obtain an estimator of the conditional probability and then compute the optimal predictions on test data based on their conditional probability estimates.
",2.1. Formal Definitions of PU and ETU,[0],[0]
Remark 1.,2.1. Formal Definitions of PU and ETU,[0],[0]
"More generally, ETU optimizes the expected utility Ey,x [ Φ ( û(h), v̂(h), p̂ )] .",2.1. Formal Definitions of PU and ETU,[0],[0]
"However, clearly, it is suf-
ficient to analyze the predictions at any given x (Natarajan et al., 2016) as in Definition 2.",2.1. Formal Definitions of PU and ETU,[0],[0]
"The two frameworks treat the metrics as utility measures (i.e., they are to be maximized).",2.2. Well-behaved Performance Metrics,[0],[0]
"Further, it is reasonable to expect that Φ(h,P) is non-decreasing in true positive and true negative rates (and indeed, virtually all performance measures used in practice behave this way).",2.2. Well-behaved Performance Metrics,[0],[0]
"As shown by Natarajan et al. (2016), such monotonicity in true positive and true negative rates implies another property, called TP monotonicity, which is better suited to the parameterization employed here.
",2.2. Well-behaved Performance Metrics,[0],[0]
Definition 3 (TP monotonicity).,2.2. Well-behaved Performance Metrics,[0],[0]
"Φ(u,",2.2. Well-behaved Performance Metrics,[0],[0]
"v, p) is said to be TP monotonic if for any v, p and u1 > u2, it holds that Φ(u1, v, p)",2.2. Well-behaved Performance Metrics,[0],[0]
>,2.2. Well-behaved Performance Metrics,[0],[0]
"Φ(u2, v, p).
",2.2. Well-behaved Performance Metrics,[0],[0]
"It is easy to verify that all measures in Table 1 are TP monotonic.
",2.2. Well-behaved Performance Metrics,[0],[0]
"A contribution in this work is to develop a notion of regularity for metrics, that helps establish statistical connections between the two frameworks and their optimal classifiers.",2.2. Well-behaved Performance Metrics,[0],[0]
"We call it p-Lipschitzness, defined next.
",2.2. Well-behaved Performance Metrics,[0],[0]
"Definition 4 (p-Lipschitzness). Φ(u, v, p) is said to be p-Lipschitz if:
|Φ(u, v, p)−Φ(u′, v′, p′)| ≤ Up|u−u′|+Vp|v−v′|+Pp|p−p′|,
for any feasible u, v, p, u′, v′, p′. The Lipschitz constants Up, Vp, Pp are allowed to depend on p, in contrast to the standard Lipschitz functions.
",2.2. Well-behaved Performance Metrics,[0],[0]
"The rationale behind p-Lipschitzness is that we want to control the change in value of the measure under small
changes in their arguments.",2.2. Well-behaved Performance Metrics,[0],[0]
This property turns out to be essential to show equivalence between ETU and PU approaches.,2.2. Well-behaved Performance Metrics,[0],[0]
"On the other hand, if we simply used a standard definition of Lipschitz function (with global constants), it would not be satisfied by many interesting measures.",2.2. Well-behaved Performance Metrics,[0],[0]
"Hence, we weaken the Lipschitz property by allowing the constant to vary as a function of p.",2.2. Well-behaved Performance Metrics,[0],[0]
"One can also show that general linear-fractional performance metrics studied in (Koyejo et al., 2014a; Narasimhan et al., 2015; Kotłowski & Dembczyński, 2016) satisfy p-Lipschitzness under mild conditions (Appendix A).
Proposition 1.",2.2. Well-behaved Performance Metrics,[0],[0]
"All measures in Table 1 are p-Lipschitz.
Proof.",2.2. Well-behaved Performance Metrics,[0],[0]
We only give a proof for Fβ-measure here (See Appendix A for the rest).,2.2. Well-behaved Performance Metrics,[0],[0]
"For ease, let us denote Fβ(u, v, p) by Fβ and Fβ(u′, v′, p′) by F ′β .",2.2. Well-behaved Performance Metrics,[0],[0]
"Let ∆u = u−u′, ∆v = v−v′, ∆p = p− p′. We have:
|Fβ",2.2. Well-behaved Performance Metrics,[0],[0]
− F ′β | = (1 + β2),2.2. Well-behaved Performance Metrics,[0],[0]
"|u(β2p′ + v′)− u′(β2p+ v)|
(β2p+ v)(β2p′ + v′)
=",2.2. Well-behaved Performance Metrics,[0],[0]
(1 + β2),2.2. Well-behaved Performance Metrics,[0],[0]
|∆u(β2p′ + v′)−,2.2. Well-behaved Performance Metrics,[0],[0]
"u′β2∆p− u′∆v|
(β2p+ v)(β2p′ + v′)
≤ 1 + β 2
β2p+v
( |∆u|+ β 2u′
β2p′+v′ |∆p|+",2.2. Well-behaved Performance Metrics,[0],[0]
"u
′
β2p′+v′ |∆v|
) .
",2.2. Well-behaved Performance Metrics,[0],[0]
"Since u′ ≤ min{p′, v′}, we have β 2u′ β2p′+v′ ≤ 1, u′
β2p′+v′ ≤ 1, and thus we can choose Up = Vp = Pp = 1+β 2
β2p .
",2.2. Well-behaved Performance Metrics,[0],[0]
"As for an example of a metric which is not p-Lipschitz, consider the precision defined as Φ(u, v, p) = uv .",2.2. Well-behaved Performance Metrics,[0],[0]
"Indeed, if v is close to zero, choosing v′ =",2.2. Well-behaved Performance Metrics,[0],[0]
"2v, u′ = u and p′ = p gives:
Φ(u, v, p)− Φ(u′, v′, p′) =",2.2. Well-behaved Performance Metrics,[0],[0]
"u 2v ,
which can be arbitrarily large for sufficiently small v, while the difference |v",2.2. Well-behaved Performance Metrics,[0],[0]
− v′| = v is small.,2.2. Well-behaved Performance Metrics,[0],[0]
"As it turns out in Section 3.2, this pathological behavior of the precision metric is responsible for a large deviation between PU and ETU, which suggests that p-Lipschitzness is in some sense necessary to establish connections.",2.2. Well-behaved Performance Metrics,[0],[0]
Most of the existing literature on optimizing nondecomposable classification metrics focus on one of the two approaches in isolation.,3. Equivalence of PU and ETU,[0],[0]
"In this section, we show that the two approaches are in fact asymptotically equivalent, for a range of well-behaved metrics.",3. Equivalence of PU and ETU,[0],[0]
"Informally, given a distribution P and a performance metric Φ, our first result is that for sufficiently large n, the PU of the associated h∗ETU is arbitrarily close to that of h∗PU, and likewise, the ETU of h∗PU is arbitrarily close to that of h ∗ ETU.",3. Equivalence of PU and ETU,[0],[0]
"In contrast, we also
show that the PU and ETU optimal classifiers may suffer differences for small samples.",3. Equivalence of PU and ETU,[0],[0]
"The intuition behind the equivalence lies in the observation that the optimal classifiers under the two approaches exhibit a very simple, similar form, under mild assumptions on the distribution (Koyejo et al., 2014b; Narasimhan et al., 2014b; Natarajan et al., 2016).",3.1. Asymptotic Equivalence,[0],[0]
Let η(x) := P(y = 1|x) denote the conditional probability of positive class as a function of x.,3.1. Asymptotic Equivalence,[0],[0]
"The following lemma shows that for any fixed classifier h that thresholds η(x), and sufficiently large sample size n, its performance measured with respect to PU and ETU are close, in particular, differ by a factor that decays as fast as Õ(1/ √ n).",3.1. Asymptotic Equivalence,[0],[0]
"In fact, the result holds uniformly over all such binary classifiers.
",3.1. Asymptotic Equivalence,[0],[0]
Lemma 1.,3.1. Asymptotic Equivalence,[0],[0]
"Let H = {h | h = 1η(x)≥τ , τ ∈",3.1. Asymptotic Equivalence,[0],[0]
"[0, 1]}, be the class of thresholded binary decision functions.",3.1. Asymptotic Equivalence,[0],[0]
Let Φ be a performance metric which is p-Lipschitz.,3.1. Asymptotic Equivalence,[0],[0]
"Then, with probability at least 1 − δ over a random sample S = {(xi, yi)}ni=1 of size n generated i.i.d.",3.1. Asymptotic Equivalence,[0],[0]
"from P, it holds uniformly over all h ∈ H,∣∣∣Φ(u(h),v(h), p(h))− Ey|x",3.1. Asymptotic Equivalence,[0],[0]
"[Φ(û(h), v̂(h), p̂(h))]",3.1. Asymptotic Equivalence,[0],[0]
"∣∣∣
≤",3.1. Asymptotic Equivalence,[0],[0]
"4Lp
√ 2 log(n+ 1)
n + 3Lp √ log 4δ 2n + Lp√",3.1. Asymptotic Equivalence,[0],[0]
"n ,
where û(h), v̂(h), p̂(h) are empirical quantities evaluated on S, and Lp = max{Up, Vp, Pp}.",3.1. Asymptotic Equivalence,[0],[0]
Remark 2.,3.1. Asymptotic Equivalence,[0],[0]
Lemma 1 generalizes the result obtained by Ye et al. (2012) for Fβ-measure to arbitrary p-Lipschitz metrics.,3.1. Asymptotic Equivalence,[0],[0]
"Furthermore, using more careful bounding technique, we are able to get a better dependence on the sample size n, essentially Õ(1/ √ n)",3.1. Asymptotic Equivalence,[0],[0]
(neglecting logarithmic terms).,3.1. Asymptotic Equivalence,[0],[0]
"In fact, this dependence cannot be improved any further in general (See Appendix B).
",3.1. Asymptotic Equivalence,[0],[0]
The uniform convergence result in Lemma 1 enables the first main result of this work.,3.1. Asymptotic Equivalence,[0],[0]
"In particular, the convergence holds when the optimal classifiers with respect to ETU and PU are of the thresholded form, i.e. h∗PU ∈ H, and h∗ETU(x) ∈ H almost surely (with respect to random sample of inputs x), where H = {h | h = 1η(x)≥τ , τ ∈",3.1. Asymptotic Equivalence,[0],[0]
"[0, 1]} is the class of threshold functions on function η(x).",3.1. Asymptotic Equivalence,[0],[0]
"Several recent results have shown that the optimal classifier for many popular metrics (including all metrics in Table 1) indeed has the thresholded form (Narasimhan et al., 2014a; Lewis, 1995), under a mild condition related to continuity of the distribution of η(x) (See the proof of Theorem 1 in Appendix B.2 for details):
Assumption 1.",3.1. Asymptotic Equivalence,[0],[0]
"The random variable η(x) has a density (with respect to the Lebesgue measure) on [0, 1].
",3.1. Asymptotic Equivalence,[0],[0]
We are now ready to state the result.,3.1. Asymptotic Equivalence,[0],[0]
"Proofs omitted in the main text are supplied in the Appendix.
Theorem 1.",3.1. Asymptotic Equivalence,[0],[0]
"Let Φ be a performance metric that is TP monotonic and p-Lipschitz, and P be a distribution satisfying Assumption 1.",3.1. Asymptotic Equivalence,[0],[0]
Consider the ETU optimal classifier h∗ETU (Definition 2) and,3.1. Asymptotic Equivalence,[0],[0]
the PU optimal classifier h ∗ PU (Definition 1).,3.1. Asymptotic Equivalence,[0],[0]
"Then, for any given and δ, we can choose n large enough (in Definition 2 of ETU), such that, with probability at least 1 − δ over the random choice of the sample of inputs x, we have:∣∣∣Φ(u(h∗ETU(x)),v(h∗ETU(x)), p)
− Φ ( u(h∗PU), v(h ∗ PU), p )∣∣∣ ≤ .",3.1. Asymptotic Equivalence,[0],[0]
"Similarly, for large enough n, with probability 1− δ,∣∣∣Ey|x[Φ(û(h∗ETU(x)), v̂(h∗ETU(x)), p̂)]
−Ey|x [ Φ ( û(h∗PU), v̂(h ∗ PU), p̂ )] ∣∣∣ ≤ .",3.1. Asymptotic Equivalence,[0],[0]
Remark 3.,3.1. Asymptotic Equivalence,[0],[0]
"In essence, Theorem 1 suggests that, for large sample sizes, the optimal in the sense of one approach gives an accurate estimate (or a proxy) of the optimal in the sense of the other approach.",3.1. Asymptotic Equivalence,[0],[0]
Our characterization of p-Lipschitzness is key to showing the equivalence.,3.1. Asymptotic Equivalence,[0],[0]
"The aforementioned result is asymptotic; to elucidate the point, we now give an example where optimal classifiers corresponding to PU and ETU differ.",3.2. Finite Sample Regime,[0],[0]
"It is important to be aware of such extremities, especially when one applies a learned model to test data of modest sample sizes.",3.2. Finite Sample Regime,[0],[0]
"The way we argue a lower bound is by specifying a metric and a distribution, such that on a randomly obtained test set of modest size, say m, the gap in the empirical metric computed on the test data for the two optimal classifiers can be large.",3.2. Finite Sample Regime,[0],[0]
"As one is typically primarily interested in the empirical metric on a given test set, focusing on the empirical metric ensures fairness and forbids favoring either definition.
",3.2. Finite Sample Regime,[0],[0]
Example.,3.2. Finite Sample Regime,[0],[0]
"For some constant α > 0, consider the (adjusted) empirical precision metric defined as:
ΦPrec(û(h(x)), v̂(h(x)), p) = û(h(x))
v̂(h(x))",3.2. Finite Sample Regime,[0],[0]
"+ α .
Note that ΦPrec ∈",3.2. Finite Sample Regime,[0],[0]
"[0, 11+α ]; Furthermore, it is p-Lipschitz, with Lipschitz constant Vp ∝",3.2. Finite Sample Regime,[0],[0]
1α (see Definition 4).,3.2. Finite Sample Regime,[0],[0]
"Thus, choosing very small values of α implies very high Lipschitz constant, and in turn the metric becomes less “stable”.",3.2. Finite Sample Regime,[0],[0]
"To establish the desired lower bound, we choose a small 0 <
α 1.",3.2. Finite Sample Regime,[0],[0]
"Let {X1,X2,X3} denote a partition of the instance space X , i.e. ∪3i=1Xi = X and Xi ∩ Xj = 0, for any pair (i, j).",3.2. Finite Sample Regime,[0],[0]
"Consider the joint distribution P defined as:
P(y = 1|x ∈ X1) = 1 , P(y = 1|x ∈ X3) = 0, P(y = 1|x ∈ X2) = 1− = 1− √ α, (1)
P(X1) + P(X3) = 2 , P(X2) = 1− 2.
for some 1 2 > 0 and note that the distribution is defined to be dependent on our choice of α.",3.2. Finite Sample Regime,[0],[0]
"The last line in the above set of equations suggests that the distribution has a small region where labels are deterministically positive or negative, but overwhelmingly positive elsewhere.",3.2. Finite Sample Regime,[0],[0]
Theorem 2.,3.2. Finite Sample Regime,[0],[0]
"Let x = {x1, x2, . . .",3.2. Finite Sample Regime,[0],[0]
", xn} denote a set of instances drawn i.i.d.",3.2. Finite Sample Regime,[0],[0]
"from the distribution P. Let y = {y1, y2, . . .",3.2. Finite Sample Regime,[0],[0]
", yn} denote their labels drawn from the same distribution.",3.2. Finite Sample Regime,[0],[0]
"With probability at least (1− 2 − n),
ΦPrec(û(h ∗ ETU(x)), v̂(h ∗ ETU(x)), p̂)",3.2. Finite Sample Regime,[0],[0]
"−
ΦPrec(û(h ∗ PU(x)), v̂(h ∗ PU(x)), p̂) ≥
1
n(1 + α) .",3.2. Finite Sample Regime,[0],[0]
Characterization of the optimal classifier as a thresholding of the conditional probability yields simple and efficient PU consistent estimators.,4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
"The idea is to first obtain an estimator for the conditional probability using training data, and then search for an optimal threshold on a separate validation set (Narasimhan et al., 2014b; Koyejo et al., 2014b).",4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
Threshold search can be efficiently implemented in linear time (assuming probabilities are pre-sorted).,4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
"In contrast, although a similar thresholding characterization exists for ETU (Natarajan et al., 2016), evaluation and prediction require the computation of an expensive expectation (Definition 2).",4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
"For general metrics, there is an O(n3) procedure to determine the optimal test set labeling (Jansche, 2007; Chai, 2005; Natarajan et al., 2016), and the procedure can be sped up to O(n2) in some special cases (Ye et al., 2012; Natarajan et al., 2016).",4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
"Here, we consider an approximation to ETU that requires only O(n) computation, yet achieves error O(n−3/2) compared to exact optimization.",4. Algorithms: Optimization and Conditional Probability Estimation,[0],[0]
"Recall that ETU seeks to find the classifier of the form:
h∗ETU(x) =",4.1. Approximation Algorithms,[0],[0]
"argmax h
Ey|x",4.1. Approximation Algorithms,[0],[0]
"[ Φ(û(h), v̂(h), p̂) ] .
",4.1. Approximation Algorithms,[0],[0]
"Following (Lewis, 1995; Natarajan et al., 2016) we know that when Φ is TP monotonic, it suffices to sort observations in decreasing order according to η(x) and assign positive labels to top k of them, for k = 0, . . .",4.1. Approximation Algorithms,[0],[0]
", n. Unfortunately, for each k, we need to calculate the expected utility
Algorithm 1 Approximate ETU Consistent Classifier 1: Input: Φ and sorted estimates of ηi, i = 1, 2, . . .",4.1. Approximation Algorithms,[0],[0]
", n 2:",4.1. Approximation Algorithms,[0],[0]
"Init s∗i = 0,∀i ∈",4.1. Approximation Algorithms,[0],[0]
"[n], p̂ = 1n",4.1. Approximation Algorithms,[0],[0]
"∑n i=1 yi, û0 = 0
3: Set Φ0 = Φ(0, 0, p̂) 4: for k = 1, 2, . . .",4.1. Approximation Algorithms,[0],[0]
", n",4.1. Approximation Algorithms,[0],[0]
"do 5: Set ûk = (k−1)ûk−1+ηk k , v̂k = k n 6: Set Φk = Φ(ûk, v̂k, p̂) (via Lemmas 2 or 3) 7: end for 8:",4.1. Approximation Algorithms,[0],[0]
"k∗ ← arg maxk=0,...,n Φk. 9: return s∗ s.t. s∗i",4.1. Approximation Algorithms,[0],[0]
← 1 for i ∈,4.1. Approximation Algorithms,[0],[0]
"[k∗].
measure, which is time consuming – requiring O(n2) in general.",4.1. Approximation Algorithms,[0],[0]
"Our goal here is to approximate this term, so that it can be computed in O(n) time, then the whole procedure can be implemented in amortized time O(n).
",4.1. Approximation Algorithms,[0],[0]
"Fix a binary classifier h : X → {0, 1} and the input sample x = (x1, . . .",4.1. Approximation Algorithms,[0],[0]
", xn).",4.1. Approximation Algorithms,[0],[0]
"Let û(h), v̂(h), p̂ denote the empirical quantities, as defined in Section 3.1.",4.1. Approximation Algorithms,[0],[0]
"Furthermore, we define semi-empirical quantities:
ũ(h) = 1
n n∑ i=1 h(xi)η(xi), and p̃ = 1 n n∑ i=1 η(xi)
(there is no need to define ṽ(h)).",4.1. Approximation Algorithms,[0],[0]
Note that ũ(h) = Ey|x,4.1. Approximation Algorithms,[0],[0]
"[ û(h) ] , and p̃ = Ey|x [p̂].
",4.1. Approximation Algorithms,[0],[0]
Zeroth-order approximation.,4.1. Approximation Algorithms,[0],[0]
"Our first approximation is based on Taylor-expanding the measure up to the second order:
Lemma 2.",4.1. Approximation Algorithms,[0],[0]
"If Φ is twice-differentiable in (u, p) and all its second-order derivatives are bounded by constant A, then:∣∣Ey|x [Φ(û(h), v̂(h), p̂)]− Φ(ũ(h), v̂(h), p̃)∣∣ ≤",4.1. Approximation Algorithms,[0],[0]
"A
2n .
",4.1. Approximation Algorithms,[0],[0]
We note that the first order terms vanish in the Taylor approximation (proof in Appendix).,4.1. Approximation Algorithms,[0],[0]
"This constitutes a simple, yet powerful method for approximating ETU utility.",4.1. Approximation Algorithms,[0],[0]
Algorithm 1 outlines the resulting algorithm.,4.1. Approximation Algorithms,[0],[0]
"As shown, the classifier can be computed inO(n) time overall, assuming the data is already sorted according to η(xi) (otherwise, the procedure is dominated by sorting time O(n log n)).",4.1. Approximation Algorithms,[0],[0]
"We note that (Lewis, 1995) proposed a similar first order approximation, albeit without any rigorous guarantee.
",4.1. Approximation Algorithms,[0],[0]
Second order approximation.,4.1. Approximation Algorithms,[0],[0]
"Naturally, we can get a better approximation by Taylor-expanding the measure up to the third order.
",4.1. Approximation Algorithms,[0],[0]
Lemma 3.,4.1. Approximation Algorithms,[0],[0]
Assume,4.1. Approximation Algorithms,[0],[0]
"Φ is three times differentiable in (u, p) and assume all its third-order derivatives are bounded by constant B. Let ∇2uu,∇2up,∇2pp denote the second-order
derivative terms evaluated at (ũ, p̃), and likewise define ∇2up,∇2pp.",4.1. Approximation Algorithms,[0],[0]
We then have:∣∣Ey|x,4.1. Approximation Algorithms,[0],[0]
"[Φ(û(h), v̂(h), p̂)]− Φappr(h)∣∣ ≤",4.1. Approximation Algorithms,[0],[0]
"B
3n3/2 ,
where:
Φappr(h) = Φ(ũ(h), v̂(h), p̃)
+ 1
2 (∇2uu + 2∇2up)su +∇2ppsp,
and
sp := 1
n2 n∑ i=1",4.1. Approximation Algorithms,[0],[0]
"η(xi)(1− η(xi)),
",4.1. Approximation Algorithms,[0],[0]
"su := 1
n2 n∑ i=1 h(xi)η(xi)(1− η(xi)).
",4.1. Approximation Algorithms,[0],[0]
Theorem 3 (Consistency).,4.1. Approximation Algorithms,[0],[0]
Given n instances x =,4.1. Approximation Algorithms,[0],[0]
"(x1, x2, . . .",4.1. Approximation Algorithms,[0],[0]
", xn), sort them in decreasing order of η(xi).",4.1. Approximation Algorithms,[0],[0]
"For 0 ≤ k ≤ n, let s(k) denote the vector with positions corresponding to top k of the sorted instances set to 1, and 0 otherwise.",4.1. Approximation Algorithms,[0],[0]
"(a) Suppose first order derivatives are bounded by A, let:
h∗a = arg max s(k) Φ(ũ(s(k)), v̂(s(k)), p̃),
We have:
Φ(h∗ETU)− Φ(ũ(h∗a), v̂(h∗a), p̃) ≤",4.1. Approximation Algorithms,[0],[0]
"A
2n .
",4.1. Approximation Algorithms,[0],[0]
"(b) Suppose second order derivatives are bounded by B, let:
h∗b = arg max s(k)",4.1. Approximation Algorithms,[0],[0]
"Φappr(s (k)),
where Φappr(h) is defined in Lemma 3.",4.1. Approximation Algorithms,[0],[0]
"We have:
Φ(h∗ETU)− Φappr(h∗b) ≤ 2B
3n3/2 .
",4.1. Approximation Algorithms,[0],[0]
"As before, the approximation can be computed in O(n) total time.",4.1. Approximation Algorithms,[0],[0]
"We could also expand the function up to orders higher than the third order, and get better approximations (still with O(n) computation if the order of the expansion is independent of n) at the cost of an even more complicated approximation formula.",4.1. Approximation Algorithms,[0],[0]
"In experiments, we find that on real datasets with test data sets of size 100 or more, even the zeroth order approximation is highly accurate.",4.1. Approximation Algorithms,[0],[0]
"So far, we assumed that we have access to the true class conditional density η(x) = P(y = 1|x) and the resulting
classifier is a threshold function on η(x).",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In practice, one employs some probability estimation procedure and gets η̂(x), which we call a",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"model.4 Then, one uses η̂(x) as if it were a true conditional probability η(x) to obtain PU or ETU classifiers.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Note that since η(x) is unknown, and we only have access to η̂(x), the best we can hope for is to choose the optimal threshold on η̂(x) (for PU) or choose the optimal number of test set observations k to be classified as positive after sorting them according to η̂(x) (for ETU).",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Next, we investigate these finite sample effects in practical PU and ETU procedures.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"For this analysis, we treat η̂(x) as given and fixed, make no other assumptions on how it was obtained.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Let Ĥ = {h | h = 1η̂(x)≥τ , τ ∈",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"[0, 1]} denote the class of binary threshold functions on η̂(x).
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Consider PU first, and let h∗ be the PU-optimal classifier from Ĥ, i.e.:
h∗",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"= argmax h∈Ĥ Φ(u(h), v(h), p).
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In practice, however, one does not have access to P, and thus u(h), v(h), p cannot be computed.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Instead, given η̂(x), one uses a validation sample S = {(xi, yi)}ni=1 to choose a threshold on η̂(x) (and thus, a classifier from Ĥ), by directly optimizing the empirical version of the metric on S:
ĥ = argmax h∈Ĥ Φ(û(h), v̂(h), p̂).
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"We would like to assess how close is ĥ to h∗. By following the proof of Lemma 1 (which never assumes the class H is based on thresholding η(x)), it is easy to show that with high probability,∣∣Φ(u(ĥ), v(ĥ), p)− Φ(u(h∗), v(h∗),",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"p)∣∣ ≤ O( 1√
n
) .
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Thus, if we have a sufficiently large validation sample at our disposal, we can set the threshold which maximizes the empirical version of the metric, and our performance is guaranteed to be Õ(1/ √ n)",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
close to the performance of the Φ-optimal classifier from Ĥ.,4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In other words, PU does not require to know the true distribution in order to select the best classifier in Ĥ, only a sufficiently large validation sample is required.
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In contrast, ETU procedure is inherently based on using η̂(x) as a replacement for η(x) (which we do not know) to decide upon label assignments.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Let x = (x1, . . .",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
", xn) be the input sample of size n. Assume for simplicity the distribution of η(x) and η̂(x) are continuous on",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"[0, 1], so that for any i 6= j, η(xi) 6= η(xj) with probability one, and
4For instance, η̂(x) could be obtained from logistic regression or neural network with soft-max function on the final layer.
similarly for η̂. Then, given x and η̂, the ETU procedure chooses the classifier of the form:
ĥ = argmax h∈Ĥ
Ey∼η̂(x)",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"[ Φ(û(h), v̂(h), p̂) ] .
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Likewise, the optimal ETU classifier in Ĥ is given by:
h∗ = argmax h∈Ĥ
Ey∼η(x) [ Φ(û(h), v̂(h), p̂) ] ,
i.e. by definition, the optimal classifier in the restricted class H involves the expectation with respect to the true η.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
Let us denote ΦETU = Ey∼η(x),4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"[ Φ(û(h), v̂(h), p̂) ] , so that h∗ maximizes ΦETU.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In the supplementary material, we show that under some mild assumptions on Φ:
Ex [∣∣ΦETU(ĥ)− ΦETU(h∗)∣∣] ≤",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Õ( 1√
n
)",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
+,4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Pp|p− pη̂|
+ sup h∈Ĥ
Up|u(h)− uη̂(h)|,
where pη̂ = E [ η̂(x) ] and uη̂(h) =",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"E [ h(x)η̂(x) ] , are the quantities corresponding to p and u(h), which were calculated by replacing the conditional probability η with its estimate η̂.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Thus, while for the PU procedure, the difference between ĥ and h∗ diminishes as n grows, it is not the case of ETU, as there are two bias terms |p − pη̂| and |u(h)−uη̂(h)|which do not depend on n.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"These terms correspond to using incorrect conditional probability η̂ while selecting the classifier, and are present even if the sample size tends to infinity.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Thus, it seems crucial for the success of ETU procedure to have η̂ calibrated with respect to the true distribution.
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
A popular choice for class probability estimation is to use logistic regression.,4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"However, if the model is misspecified, which happens often in practice, the aforementioned discussion suggests that the desired ETU solution may not be achieved.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Therefore, we need to learn the class probability function more carefully.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"Here, we consider two variants.
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
The first is to use the Isotron algorithm.,4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"In case of the generalized linear model, i.e. P(y|x) = γ(w∗, x) for some unknown link function γ and model w∗, Kalai & Sastry (2009) proposed a simple and elegant algorithm (see Appendix E) that alternatively learns w∗ and the link function γ (approximated by a piecewise linear function).",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"It provably learns the model under certain assumptions on P. The model w∗ and link function γ are learned using training data, and at prediction time, the link function and the scores of training data (i.e., xTi w) are used to calibrate the class probabilities η(x) of test instances.
",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"We also consider using a recalibrated logistic model, i.e., we first estimate the class probabilities via standard logistic regression, and recalibrate the probabilities by running one update of the γ function in Isotron algorithm (which
essentially solves a quadratic problem known as the Pool of Adjacent Violators).",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"At test time, we use the learnt γ and the logistic model to estimate η(x) for test instances.",4.2. Conditional Probability Estimation and Model Misspecification,[0],[0]
"We empirically evaluate the effectiveness and accuracy of ETU approximations introduced in Section 4.1, on synthetic as well as real datasets.",5. Experiments,[0],[0]
"We also show on several benchmark datasets that, by carefully calibrating the conditional probabilities in ETU, we can improve the classification performance.",5. Experiments,[0],[0]
We consider F1 and Jaccard metrics from Table 1.,5.1. Convergence of Approximations,[0],[0]
We sample conditional probabilities ηi for n instances from the uniform distribution.,5.1. Convergence of Approximations,[0],[0]
"The optimal predictions (see Definition 2) are obtained using Algorithm 1 of (Natarajan et al., 2016) (which is equivalent to searching over 2n possible label vectors).",5.1. Convergence of Approximations,[0],[0]
Then we compute the approximate optimal predictions using the first and the second order approximations discussed in Section 4.1.,5.1. Convergence of Approximations,[0],[0]
"For each metric, we measure the deviation between the true and the approximate optimal values with increasing sample size in Figure 1.",5.1. Convergence of Approximations,[0],[0]
We observe linear convergence for the first order approximation and quadratic convergence for the second order approximation.,5.1. Convergence of Approximations,[0],[0]
"This suggests that the bounds in Theorem 3 indeed can be improved for some metrics, if not in general.",5.1. Convergence of Approximations,[0],[0]
"We report results on seven multiclass and multilabel benchmark datasets: (1) LETTERS: 16000 train, 4000 test instances, (2) SCENE: 1137 train, 1093 test (3) YEAST: 1500 train, 917 test (4) WEBPAGE: 6956 train, 27824 test (5) IMAGE: 1300 train, 1010 test (6) BREAST CANCER: 463 train, 220 test instances, (7) SPAMBASE: 3071 train, 1530 test",5.2. Approximations on Real Data,[0],[0]
instances.5,5.2. Approximations on Real Data,[0],[0]
"In case of multiclass datasets, we report results (using one-vs-all classifiers) averaged over classes (as
5See (Koyejo et al., 2014b; Ye et al., 2012) for details.
in Natarajan et al. (2016)).
",5.2. Approximations on Real Data,[0],[0]
"We compare the exact ETU optimal, computed using the algorithm of Natarajan et al. (2016), with the approximations.",5.2. Approximations on Real Data,[0],[0]
The results for F1 and Jaccard metrics are presented in Table 2.,5.2. Approximations on Real Data,[0],[0]
"The results convincingly show that the approximations are highly accurate, and almost always indistinguishable from optimizing true metrics, on real datasets.",5.2. Approximations on Real Data,[0],[0]
"Note that even the first-order approximation (in fact, this is zeroth-order, as the first order term is zero; see Section 4) achieves high accuracy, as the test set sizes are relatively large.",5.2. Approximations on Real Data,[0],[0]
"We now study how class probability estimation (CPE) and model misspecification affects the performances of PU and ETU approaches, on the seven benchmark datasets.",5.3. Model Misspecification,[0],[0]
"We compare four methods: (a) ETU with logistic regression based CPE, (b) ETU with Isotron based CPE (discussed in Section 4.1), (c) ETU with recalibrated logistic regression based CPE (discussed in Section 4.1), and (d) PU using logistic regression based CPE followed by threshold tuning on validation set (Koyejo et al., 2014b).",5.3. Model Misspecification,[0],[0]
"Additional comparisons to structured SVM (Joachims, 2005) and other classifiers are available in previously published work by others (Koyejo et al., 2014b; Natarajan et al., 2016), and are omitted here.
",5.3. Model Misspecification,[0],[0]
The results are presented in Table 3.,5.3. Model Misspecification,[0],[0]
We observe that the logistic model (column 1) is insufficient for many of the datasets.,5.3. Model Misspecification,[0],[0]
The results improve in several cases using the estimated generalized linear model with Isotron (column 2).,5.3. Model Misspecification,[0],[0]
"However, there is a confounding factor that the two algorithms are very different, and noticed improvement may not necessarily be due to better CPE.",5.3. Model Misspecification,[0],[0]
"To isolate this, recalibrated logistic model results are presented in column 3.",5.3. Model Misspecification,[0],[0]
"The results are in general much better than the standard logistic model, which suggests that it is indeed the case of model misspecification in these datasets.",5.3. Model Misspecification,[0],[0]
"Finally, we present the results with PU algorithm in column 4.",5.3. Model Misspecification,[0],[0]
"We find that the results closely match that of the recalibrated logistic model (except in the case of SCENE dataset); thus, correcting for model misspecification helps demonstrate the theorized asymptotic equivalence of PU and ETU approaches in practice.",5.3. Model Misspecification,[0],[0]
We have presented new results which elucidate the relationship between the two notions of consistency for complex binary classification metrics.,6. Conclusions and Future Work,[0],[0]
"Next, we plan to explore surrogates to further improve training efficiency nondecomposable metrics.",6. Conclusions and Future Work,[0],[0]
"We will also extend to more complex prediction problems such as multilabel classification, where a similar dichotomy exists.",6. Conclusions and Future Work,[0],[0]
W. Kotłowski has been supported by the Polish National Science Centre under Grant No. 2013/11/D/ST6/03050.,Acknowledgments,[0],[0]
Statistical learning theory is at an inflection point enabled by recent advances in understanding and optimizing a wide range of metrics.,abstractText,[0],[0]
Of particular interest are non-decomposable metrics such as the F-measure and the Jaccard measure which cannot be represented as a simple average over examples.,abstractText,[0],[0]
"Non-decomposability is the primary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency.",abstractText,[0],[0]
"In this manuscript we analyze both settings, from statistical and algorithmic points of view, to explore the connections and to highlight differences between them for a wide range of metrics.",abstractText,[0],[0]
"The analysis complements previous results on this topic, clarifies common confusions around both settings, and provides guidance to the theory and practice of binary classification with complex metrics.",abstractText,[0],[0]
Consistency Analysis for Binary Classification Revisited,title,[0],[0]
Competitive analysis of online algorithms has been an area of spirited research with beautiful results over the past two decades.,1. Introduction,[0],[0]
"At its heart, this area is about decision making under uncertainty about the future—the input is revealed in an online manner, and at every point in time the algorithm must make an irrevocable choice.",1. Introduction,[0],[0]
"A standard example is that of caching algorithms—at every time step the algorithm must make a choice about which elements to keep in the cache, and which elements to evict (Fiat et al., 1991).",1. Introduction,[0],[0]
"The generalization of caching to metric spaces is encapsu-
*Equal contribution 1Google, Zurich, Switzerland 2Google, New York, New York, USA.",1. Introduction,[0],[0]
Correspondence to: Silvio Lattanzi <silviol@google.com,1. Introduction,[0],[0]
">, Sergei Vassilvitskii <sergeiv@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
lated in the k-server problem, which has been the subject of intense study (Bansal et al., 2015; Manasse et al., 1990).
",1. Introduction,[0],[0]
The key metric in online algorithms is the competitive ratio.,1. Introduction,[0],[0]
"It measures the quality of the solution obtained by an online algorithm versus an offline optimum, which has the luxury of seeing the whole input before making any decisions.",1. Introduction,[0],[0]
"In situations where the competitive ratio is relatively small, for example, the list update problem (Sleator & Tarjan, 1985), this is a great measure by which we can compare different algorithms.",1. Introduction,[0],[0]
"However, in some scenarios strong lower bounds on the competitive ratio imply that any algorithm that makes irrevocable choices will necessarily perform poorly when compared to an offline optimum.
",1. Introduction,[0],[0]
Online clustering is one such example.,1. Introduction,[0],[0]
"In this setting points x1, x2, . . .",1. Introduction,[0],[0]
"arrive one at a time, and must be instantly given one of k cluster labels.",1. Introduction,[0],[0]
"As is typical, the goal is to have the highest quality clustering (under some pre-specified objective function, like k-CENTER or k-MEDIAN) at every point in time.",1. Introduction,[0],[0]
"As Liberty et al. (2016) showed, not only do online clustering algorithms have an unbounded competitive ratio, but one must use bi-criteria approximations to have any hope of a constant approximate solution.
",1. Introduction,[0],[0]
Another approach to evade strong lower bounds is to make additional assumptions about the input to the problem.,1. Introduction,[0],[0]
"For example, one may assume that the input comes in a random (or partially random) order.",1. Introduction,[0],[0]
"This assumption has been a fruitful avenue when studying online problems in different contexts, as the classic secretary problem (Ferguson, 1989; Kesselheim et al., 2015; Kleinberg, 2005) or matching (Karp et al., 1990; Mahdian & Yan, 2011).",1. Introduction,[0],[0]
"Another alternative is to assume some additional structure on the distribution that points are coming from (Feldman et al., 2009).",1. Introduction,[0],[0]
"A big downside of both of these assumptions is that they are hard to test and validate in practice, which is why we take a different approach in this work.",1. Introduction,[0],[0]
"While the irrevocability of past choices makes sense from a theoretical standpoint, for some practical problems this requirement is unrealistically draconian.",1.1. Consistency,[0],[0]
"For example, consider a load balancer, which, when faced with requests arriving online, assigns them to different machines.",1.1. Consistency,[0],[0]
"Better cache performance dictates that similar requests should be
assigned to the same machine, thus the load balancer is essentially performing online clustering.",1.1. Consistency,[0],[0]
"However, fundamentally, nothing is preventing the load balancer from reassigning some of the past jobs to other machines.",1.1. Consistency,[0],[0]
"In this situation, a re-clustering—a reassignment of jobs to machines to increase performance—is not an impossible operation.
",1.1. Consistency,[0],[0]
"Another common example of a costly, but not prohibitive recomputation comes from standard applications of unsupervised clustering: feature engineering for large scale machine learned systems.",1.1. Consistency,[0],[0]
"In this setting a feature vector x, is augmented with the id of a cluster it falls in, x′, and the full vector (x, x′) is given as input to the learner.",1.1. Consistency,[0],[0]
This is mainly done to introduce expressiveness and non-linearity to simple systems.,1.1. Consistency,[0],[0]
"In this situation, changing the clustering would entail changing the set of features passed to the learner, and retraining the whole system; thus one certainly does not want to do it at every time step, but it can be done if the gains are worthwhile.
",1.1. Consistency,[0],[0]
"From a theoretical perspective, the ability to correct for past mistakes offers the ability for much better solutions.",1.1. Consistency,[0],[0]
"In particular for clustering problems, it avoids the lower bounds introduced by Liberty et al. (2016).",1.1. Consistency,[0],[0]
"As we will show, the option to recluster dramatically improves the quality of the solution, even if it is taken rarely.",1.1. Consistency,[0],[0]
"More formally, we will introduce a parameter β which controls the number of times the solution changes.",1.1. Consistency,[0],[0]
"Setting β = 0 is equivalent to online algorithms, whereas a large value of β is equivalent to recomputing the answer from scratch at every time step.",1.1. Consistency,[0],[0]
"In this paper we focus on exploring the trade-off between the approximation ratio of clustering algorithms, and the number of times we must recompute the results.
",1.2. Our Contributions,[0],[0]
"We begin by formally defining the notion of (α, β)consistent clustering in Section 3.",1.2. Our Contributions,[0],[0]
"Then we prove a lower bound, showing that any constant competitive algorithm must change its cluster centers at least Ω(k log n) times (Section 3.1).",1.2. Our Contributions,[0],[0]
"Then we show that a known algorithm by Charikar et al. (2004) achieves this bound for the kCENTER problem, and we develop a new algorithm for other clustering objectives, and show that it requires at most O(k2 log4 n) reclusterings, an exponential improvement over the naive solution (Section 5).",1.2. Our Contributions,[0],[0]
"Finally, we show that the proposed algorithms perform well on real world datasets (Section 7).",1.2. Our Contributions,[0],[0]
There are two avenues for related work that we build on in this paper.,1.3. Related Work,[0],[0]
"The first is clustering algorithms, particularly the online clustering variants.",1.3. Related Work,[0],[0]
"In their seminal work Charikar et al. (2004) gave algorithms for the k-CENTER
problem.",1.3. Related Work,[0],[0]
The case of k-MEDIAN and k-MEANS proved more complex.,1.3. Related Work,[0],[0]
"For the former, Meyerson (2001) gave an O(log n) competitive ration for closely related online facility location problem.",1.3. Related Work,[0],[0]
This result was further improved by Fotakis (2008) and Anagnostopoulos et al. (2004).,1.3. Related Work,[0],[0]
The latter was recently studied by Liberty et al. (2016) who gave bicriteria approximations and showed that these are necessary in an online setting.,1.3. Related Work,[0],[0]
"For the soft partition version of the k-clustering problem, an Expectation Maximization algorithm was suggested by Liang & Klein (2009).
",1.3. Related Work,[0],[0]
"The second, closely related area, is that of streaming algorithms.",1.3. Related Work,[0],[0]
"The literature of clustering in the streaming model is very rich, we highlight the most relevant results.",1.3. Related Work,[0],[0]
The first paper to study clustering problem is by Charikar et al. (2004) studying the k-CENTER problem.,1.3. Related Work,[0],[0]
Guha et al. (2000) give the first single pass constant approximation algorithm to the k-MEDIAN variant.,1.3. Related Work,[0],[0]
Subsequently their result has been is improved by Charikar et al. (2003).,1.3. Related Work,[0],[0]
"Finally, the best algorithm for the closely related variant of facility location is due to Czumaj et al. (2013), who gave a (1 + )- approximation for the problem.",1.3. Related Work,[0],[0]
"Let X be a set of n points, and d : X × X → R a distance function.",2. Preliminaries,[0],[0]
"We assume that d is symmetric and that (X, d) form a metric space, that is d(x, x) = 0 for any x ∈ X; d(x, y) = d(y, x) ≥ 0 for any x, y ∈ X; and, for any x, y, z ∈ X , d(x, y) ≤ d(x, z) + d(z, x).",2. Preliminaries,[0],[0]
"Finally, by scaling d, let minx,y d(x, y) = 1 and denote by ∆ the maximum pairwise distance, maxx,y d(x, y).",2. Preliminaries,[0],[0]
"We will assume that ∆ is bounded by a polynomial in n, therefore log ∆ = O(log n).
",2. Preliminaries,[0],[0]
Consider a set of k points,2. Preliminaries,[0],[0]
c,2. Preliminaries,[0],[0]
"= {c1, c2, . . .",2. Preliminaries,[0],[0]
", ck} ⊆ X,which we will refer to as centers.",2. Preliminaries,[0],[0]
"For each ci, let Ci ⊆ X be the set of points in X closer to ci than to any other center c ∈",2. Preliminaries,[0],[0]
"C. 1 Formally, Ci = {x ∈ X | d(x, ci) ≤ minc∈c d(x, c)}.
",2. Preliminaries,[0],[0]
"Given a p > 0, in the rest of the paper we refer to the cost of a point x with to respect to a set of centers as: costp(x, c) = minci d(x, ci)
",2. Preliminaries,[0],[0]
p.,2. Preliminaries,[0],[0]
"And cost of a cluster Ci as: costp(X,Ci) = ∑ x∈Ci d(x, ci)",2. Preliminaries,[0],[0]
"p.
",2. Preliminaries,[0],[0]
Now we are ready to define our problem.,2. Preliminaries,[0],[0]
"For any p > 0 we can define the cost of clustering of pointsX with respect to the centers c ⊆ X as: costp(X, c) = ∑",2. Preliminaries,[0],[0]
"x∈X costp(x, c) =∑k
i=1 ∑",2. Preliminaries,[0],[0]
"x∈Ci d(x, ci) p.
",2. Preliminaries,[0],[0]
"The k-clustering family of problems asks to find the set of centers c that minimize costp for a specific p. When p = 1, cost1(X, c) is precisely the k-MEDIAN clustering
1 For clarity of the exposition we will assume that all of the pairwise distances are unique.",2. Preliminaries,[0],[0]
"The results still hold when ties are broken lexicographically.
objective.",2. Preliminaries,[0],[0]
Setting p = 2 is equivalent to the k-MEDOIDS problem2.,2. Preliminaries,[0],[0]
"Finally, with p =∞, we recover the k-CENTER problem, which asks to minimize the maximum distance of any point to its nearest cluster center.
",2. Preliminaries,[0],[0]
"Observe that although d(·, ·) satisfies the triangle inequality, when raised to p-th power we need to relax the condition.",2. Preliminaries,[0],[0]
"In particular we have that for any x, y, z ∈ X: d(x, y)p ≤ 2p−1(d(x, z)p + d(z, y)p).
",2. Preliminaries,[0],[0]
"When p is clear from the context, we will refer to costp(X, c) as the cost of the clustering and denote it cost(X, c).",2. Preliminaries,[0],[0]
"We will us optp(X) to denote the optimum cost for the metric space (X, d).",2. Preliminaries,[0],[0]
"We will use c∗ = {c∗1, c∗2, . . .",2. Preliminaries,[0],[0]
", c∗k} to denote the optimal solution.
",2. Preliminaries,[0],[0]
"The k clustering problem is NP-hard to solve exactly, thus we consider approximate solutions.",2. Preliminaries,[0],[0]
"We say that a clustering generated from a set of centers c is α-approximate if costp(X, c) ≤ α ·optp(X).",2. Preliminaries,[0],[0]
"The best known approximation factors are 2 for the k-CENTER problem (Gonzalez, 1985), 1 + √
3 + for the k-MEDIAN problem (Li & Svensson, 2016), and 9 + for the k-MEDOIDS problem (Kanungo et al., 2004).",2. Preliminaries,[0],[0]
"As noted in the introduction, in many online clustering applications the choices made by the online algorithm are not irrevocable, but simply expensive to change.",3. Consistency,[0],[0]
"Moreover, by allowing a small number of full recomputations, we can circumvent the stringent lower bounds on competitive ratio for online clustering.
",3. Consistency,[0],[0]
"To this end, our goal in this work is to better understand the trade-off between the approximation ratio of online clustering algorithms, and the number of times the representative centers change.
",3. Consistency,[0],[0]
We focus on a dynamic setting where the points arrive sequentially.,3. Consistency,[0],[0]
"Let xt denote the point that arrives at time t, and denote by Xt the set of points that has arrived from the beginning.",3. Consistency,[0],[0]
"Thus X0 = ∅, and Xi+1",3. Consistency,[0],[0]
"= Xi ∪ {xi+1} = {x1, x2, . . .",3. Consistency,[0],[0]
", xi+1}.
",3. Consistency,[0],[0]
"For any two sets of centers c, c′ let |c−c′| denote the number of elements present in c, but not in c′: |c − c′| = |c \",3. Consistency,[0],[0]
(c ∩ c′)|.,3. Consistency,[0],[0]
"Observe that when c and c′ have the same cardinality, |c− c′| = |c′ − c|.",3. Consistency,[0],[0]
Definition 3.1.,3. Consistency,[0],[0]
"Given a sequence of sets of centers, c0, c1, . . .",3. Consistency,[0],[0]
", ct and a positive monotone non-decreasing function β : Z → R, we say that the sequence is β-consistent if for all T , ∑T t=1 |ct − ct−1| ≤ β(T ).
",3. Consistency,[0],[0]
"In other words, a sequence is β-consistent, if at time T at
2In the Euclidean space if the centers do not need to be part of the input, setting p = 2 recovers the k-MEANS problem.
most β(T ) centers have changed between successive sets.
",3. Consistency,[0],[0]
Definition 3.2.,3. Consistency,[0],[0]
"Given a sequence of points x1, x2, . . .",3. Consistency,[0],[0]
", xT , and a parameter p, a sequence of centers c1, c2, . . .",3. Consistency,[0],[0]
", cT is (α, β)-consistent if: (i) Approximation.",3. Consistency,[0],[0]
"At every time t, the centers ct form an α approximate solution to the optimum solution at that time: costp(Xt, ct) ≤ α · optp(Xt) for all t ≤ T .",3. Consistency,[0],[0]
(ii) Consistency.,3. Consistency,[0],[0]
The sets of centers form a β-consistent sequence.,3. Consistency,[0],[0]
"Before we look for (α, β) consistent algorithms it is useful to understand what values are possible.",3.1. A lower bound,[0],[0]
We show that it is impossible to get a constant approximation and achieve consistency of o(log n) for any of the k clustering problems.,3.1. A lower bound,[0],[0]
"Later, in Section 6 we will give a non-constructive result that shows that there is always a sequence of clusterings that is simultaneously constant-approximate and O(k log2 n) consistent.3
Lemma 3.3.",3.1. A lower bound,[0],[0]
"There exists a sequence of points such that for any constant α > 0, any algorithm that returns an α-approximate solution while processing n points must be Ω(k log n)-consistent.
Proof.",3.1. A lower bound,[0],[0]
"For ease of exposition, assume that p = 1, and consider points lying in (k − 1)-dimensional Euclidean space, Rk−1.",3.1. A lower bound,[0],[0]
"We begin by adding a point x0 at the origin, and points x1, . . .",3.1. A lower bound,[0],[0]
", xk−1 in positions e1, e2, . . .",3.1. A lower bound,[0],[0]
", ek−1, where ej is the standard basis vector that is 1 in the j-th dimension, and 0 everywhere else.
",3.1. A lower bound,[0],[0]
"We then proceed in phases, where in phase 1 ≤",3.1. A lower bound,[0],[0]
i < log n,3.1. A lower bound,[0],[0]
we add points at position (γ)i · ej for each j ∈,3.1. A lower bound,[0],[0]
"[1, k − 1], for some γ > 0 that we will set later.",3.1. A lower bound,[0],[0]
"In phase log n we add the remaining n− (k− 1) log n− 1 points at arbitrary positions within the convex hull of already added points.
",3.1. A lower bound,[0],[0]
Let Pi be the set of points at the end of phase i. Consider any algorithm that returns anα-approximate solution onPi.,3.1. A lower bound,[0],[0]
"Let p1, p2, . . .",3.1. A lower bound,[0],[0]
", pk−1 be the points added to the input during phase",3.1. A lower bound,[0],[0]
"i, pj = γi · ej .",3.1. A lower bound,[0],[0]
"Then Pi = Pi−1 ∪ {p1, . . .",3.1. A lower bound,[0],[0]
", pk−1}.",3.1. A lower bound,[0],[0]
"One feasible solution choses as centers the points added in phase i as well as the origin, C = {p1, p2, . . .",3.1. A lower bound,[0],[0]
", pk−1, 0}.
",3.1. A lower bound,[0],[0]
"For every point in Pi−1 the origin is closer than any of the other centers, therefore the total cost is: opt(Pi) ≤",3.1. A lower bound,[0],[0]
"cost(Pi, C) =",3.1. A lower bound,[0],[0]
(k − 1) ∑i−1,3.1. A lower bound,[0],[0]
z=1 γ z ≤,3.1. A lower bound,[0],[0]
(k − 1)γ i−1 γ−1 .,3.1. A lower bound,[0],[0]
"On the other hand, consider a set of centers c′ that does not include some pj = γiej .",3.1. A lower bound,[0],[0]
"The closest point to pj is at γi−1ej , which is at distance γi−1(γ − 1) away.",3.1. A lower bound,[0],[0]
"There-
3Note that we assume throughout the paper that the maximum distance between any two points, ∆, is polynomial in n. Alternatively we can restate the lower bound in this section as a Ω(k log ∆) upper bound in section 6 as a O(k log2 ∆).
",3.1. A lower bound,[0],[0]
"fore, cost(Pi, c′) ≥ cost({pj}, c′) = γi−1(γ − 1).",3.1. A lower bound,[0],[0]
"If γ ≥ (2 + kα) then we can bound the approximation ratio as: cost(Pi,c
′) opt(Pi)",3.1. A lower bound,[0],[0]
≥ γ i−1(γ−1) (k−1) γi−1γ−1,3.1. A lower bound,[0],[0]
≥,3.1. A lower bound,[0],[0]
"γ i−1(γ−1)2 (k−1)γi ≥ γ−2 k−1 > α
so c′ cannot be an α-approximate solution.",3.1. A lower bound,[0],[0]
Therefore at the end of phase 1 ≤,3.1. A lower bound,[0],[0]
"i < log n, any α-approximate set of centers, must include all points added in phase i.",3.1. A lower bound,[0],[0]
"Thus any sequence of sets of centers must be Ω(k log n)-consistent.
",3.1. A lower bound,[0],[0]
"Note that considering any p > 1 only makes any omission of point pj even more costly, as compared to the optimum solution.
",3.1. A lower bound,[0],[0]
4.,3.1. A lower bound,[0],[0]
"Warm up: k-CENTER Clustering To gain some intuition about consistent clustering, we begin with the k-CENTER objective.",3.1. A lower bound,[0],[0]
"Given a dataset X , the goal is to identify k centers",3.1. A lower bound,[0],[0]
"c = {c1, . . .",3.1. A lower bound,[0],[0]
", ck} that minimize: maxx∈X minc∈c d(x, c).",3.1. A lower bound,[0],[0]
"This problem is known to be NP-hard, but a simple 2-approximation algorithm exists in the batch setting (Gonzalez, 1985).",3.1. A lower bound,[0],[0]
"In the streaming setting, when points arrive one at a time, the DOUBLING algorithm by Charikar et al. (2004) was the first algorithm discovered for this problem.",3.1. A lower bound,[0],[0]
The algorithm maintains an 8-approximation.,3.1. A lower bound,[0],[0]
"Furthermore, it works in O(log ∆) = O(log n) phases and the total consistency cost of each phase is k; thus we get the following lemma.
",3.1. A lower bound,[0],[0]
Lemma 4.1.,3.1. A lower bound,[0],[0]
"The DOUBLING algorithm for the k-CENTER problem is (8, O(k log n))-consistent.",3.1. A lower bound,[0],[0]
"In this section we present our main result, an algorithm that achieves a polylogarithmic consistency factor.",5. Main Algorithm,[0],[0]
"More precisely, we show that for every constant p ≥ 1, it is possible to design an algorithm for the Consistent k-clustering problem under costp that is constant approximate, and O(k2 log4 n)-consistent.
",5. Main Algorithm,[0],[0]
"In the remainder of the section we first present the main ideas behind our algorithm, then prove some useful technical lemmas, and finally present the full algorithm.",5. Main Algorithm,[0],[0]
"Before delving into the details, we highlight the three main building blocks of our algorithm.
",5.1. Main ideas,[0],[0]
"The first is the Meyerson sketch for online facility location (Meyerson, 2001).",5.1. Main ideas,[0],[0]
This sketch has already been used by Charikar et al. (2003) to solve the k-median problem on data streams.,5.1. Main ideas,[0],[0]
"We show that the main ingredients of the sketch continue to work under costp objectives, and use it to generally reduce the number of points under considerations from n to k · poly log n.
One caveat of this sketch is that to use it we need to have access to a good lower bound on the cost of the optimal solution at any point in time.",5.1. Main ideas,[0],[0]
We obtain it by running the Θ(p) approximation algorithm described by Gupta & Tangwongsan (2008) on all available points.,5.1. Main ideas,[0],[0]
"In this way, at any point in time we have a good approximation of the optimum solution.",5.1. Main ideas,[0],[0]
"Then we divide the progress of our algorithm into log n phases based on this lower bound and in each phase we use a different sketch.
",5.1. Main ideas,[0],[0]
"Finally, while the Meyerson sketch maintains O(k log2 n) possible centers, to computer the k-clustering, we have to reduce these points into exactly k final centers.",5.1. Main ideas,[0],[0]
We first show that this is possible and then we prove that we do not need to recluster frequently.,5.1. Main ideas,[0],[0]
"In fact we will do it only when either a new point is added to the Meyerson sketch— O(k log2 n) times—or when the number of points assigned to one of these elements of the Meyerson sketch doubles— O(k log n) events per sketch.
",5.1. Main ideas,[0],[0]
"By putting all of these ingredients together, we show that the number of times we need to fully recluster is at most O(k log3 n) per phase, or that we haveO(k2 log4 n)",5.1. Main ideas,[0],[0]
cluster changes in total.,5.1. Main ideas,[0],[0]
We present the Meyerson sketch and prove some useful properties.,5.2. The Meyerson sketch,[0],[0]
"We assume to have access to a lower bound to the cost of the optimal solution L, such that Lp ≥ βoptp, for some constant 0 ≤ β ≤ 1.",5.2. The Meyerson sketch,[0],[0]
(We will remove the assumption later.),5.2. The Meyerson sketch,[0],[0]
"Then the algorithm works in phases, such that at any time in phase j, L ∈",5.2. The Meyerson sketch,[0],[0]
"[2j−1, 2j).",5.2. The Meyerson sketch,[0],[0]
"So in each phase j we can use the same lower bound Lpj = 2 j−1 and have Lpj ≥ βoptp 2 .
",5.2. The Meyerson sketch,[0],[0]
In each phase j we create 2 log n Meyerson sketches as described in Algorithm 1.,5.2. The Meyerson sketch,[0],[0]
"Then we combine them in a single sketch as described in Algorithm 2.
",5.2. The Meyerson sketch,[0],[0]
"Algorithm 1 Single Meyerson sketch 1: Input: A sequence of points x0, x1, x2, . . .",5.2. The Meyerson sketch,[0],[0]
", xn.",5.2. The Meyerson sketch,[0],[0]
"A finite p. 2: Output: A set S that is a constant bi-criteria approximate
solution for the k-clustering problem.",5.2. The Meyerson sketch,[0],[0]
"3: S ← ∅ 4: Let X be a set of points and let L be such that L ≥ γoptp(X), for some constant γ > 0 5: for x ∈ X do 6: if S == ∅ then 7: S ← {x} 8: else 9: Let δ = d(x, S)p
10: With probability min ( δk(1+logn)",5.2. The Meyerson sketch,[0],[0]
"Lp , 1 ) add x to S
11: Return S
For simplicity we first analyze the property of a single Meyerson sketch.",5.2. The Meyerson sketch,[0],[0]
"In particular we give a bound on both the
number of points selected by a single sketch, as well as the quality of the approximation.",5.2. The Meyerson sketch,[0],[0]
"The Lemma generalizes the results in (Charikar et al., 2003; Meyerson, 2001) to all finite p and follows the general structure their proof so it is deferred to the extended version of the paper.
",5.2. The Meyerson sketch,[0],[0]
Lemma 5.1.,5.2. The Meyerson sketch,[0],[0]
"For a constant γ ∈ (0, 1), with probability at least 12 the set S computed by Algorithm 1 has: (i)
size at most 4k(1 + log n)",5.2. The Meyerson sketch,[0],[0]
"( 22p+1 γp + 1 )
; (ii) costp(S) ≤ 64optp(X).
",5.2. The Meyerson sketch,[0],[0]
From Lemma 5.1 we know that with constant probability a single Meyerson sketch is of size O(k log n) and contains a set of points that give a good solution to our problem.,5.2. The Meyerson sketch,[0],[0]
"Thus, if we construct 2 log n single Meyerson sketches in parallel, at least one of them gives a constant approximation to the optimum at every point in time with probability at least 1 − O(n−1).",5.2. The Meyerson sketch,[0],[0]
"The observation inspired the design of Algorithm 2, whose properties are formalized next.
",5.2. The Meyerson sketch,[0],[0]
Lemma 5.2.,5.2. The Meyerson sketch,[0],[0]
"For a constant γ ∈ (0, 1), with probability 1 − O(n−1) the set M = ∪2 logni=1 Mi computed by Algorithm 2 has: size at most O(k log2 n) and costp(M) ≤ 64optp(X).
",5.2. The Meyerson sketch,[0],[0]
Proof.,5.2. The Meyerson sketch,[0],[0]
"As mentioned above, Lemma 5.1 implies that if we construct 2 log n single Meyerson sketches in parallel, with probability 1 − O(n−1), at least one of them gives a constant approximation to the optimum at every point in time.",5.2. The Meyerson sketch,[0],[0]
Furthermore in total it contains only 4k(1 + log n),5.2. The Meyerson sketch,[0],[0]
"( 22p+1 γp + 1 ) points.
",5.2. The Meyerson sketch,[0],[0]
Now in Algorithm 2 we are almost building 2 log n Meyerson sketches; the only difference is that we stop adding points to a single sketch when it becomes too large.,5.2. The Meyerson sketch,[0],[0]
This modification does not change the probability that there exist at least one single sketch that gives a constant approximation to the optimum at every point in time and has at most 4k(1 + log n),5.2. The Meyerson sketch,[0],[0]
"( 22p+1 γp + 1 ) points.
",5.2. The Meyerson sketch,[0],[0]
Thus with probability 1 − O(n−1) at least one of the sketches constructed in Lemma 5.1 gives a constant approximation to the optimum at every point in time.,5.2. The Meyerson sketch,[0],[0]
Merging other sketches to this sketch does not affect this property.,5.2. The Meyerson sketch,[0],[0]
Furthermore the number of points in each sketch is explicitly bounded by 4k(1 + log n),5.2. The Meyerson sketch,[0],[0]
"( 22p+1 γp + 1 )
so the total number of points in M is bounded by 8k log n(1 + log n)",5.2. The Meyerson sketch,[0],[0]
"( 22p+1 γp + 1 )
Note that in some cases we do not need to recompute all the sketches from scratch but we need only to update them, so we can define a faster update function described in Algorithm 3.
",5.2. The Meyerson sketch,[0],[0]
"Algorithm 2 ComputeMeyerson(Xt, φ) 1: Input: A sequence of points Xt, a lower bound to the opti-
mum φ.",5.2. The Meyerson sketch,[0],[0]
"2: Output: 2 logn independent Meyerson sketches M1, . . .",5.2. The Meyerson sketch,[0],[0]
",M2 logn 3:",5.2. The Meyerson sketch,[0],[0]
"Lp = φ γ
, 4: for i ∈",5.2. The Meyerson sketch,[0],[0]
[2 logn] do: .,5.2. The Meyerson sketch,[0],[0]
Initialize all Meyerson sketches 5: Mi ← x0 6: for x ∈,5.2. The Meyerson sketch,[0],[0]
Xt do: 7: for i ∈,5.2. The Meyerson sketch,[0],[0]
[2 logn] do: .,5.2. The Meyerson sketch,[0],[0]
"If Mi is not too large, analyze x 8: if |Mi| ≤ 4k(1 + logn) ( 22p+1
γp + 1
) then:
9: Let δ = d(x,Mi)p 10: p̂ = min ( δk(1+logn)",5.2. The Meyerson sketch,[0],[0]
"Lp , 1 ) 11: Add x to Mi with probability p̂ 12: Return M1, . . .",5.2. The Meyerson sketch,[0],[0]
",M2 logn
Algorithm 3 UpdateMeyerson(M1, . . .",5.2. The Meyerson sketch,[0],[0]
",Ms, xt, φ) 1: Input: A point xt, a lower bound to the optimum φ and s
independent Meyerson sketches M1, . . .",5.2. The Meyerson sketch,[0],[0]
",Ms. 2: Output: s independent Meyerson sketches M1, . . .",5.2. The Meyerson sketch,[0],[0]
",Ms 3:",5.2. The Meyerson sketch,[0],[0]
"Lp = φ
γ ,
4: for i ∈",5.2. The Meyerson sketch,[0],[0]
[s] do: .,5.2. The Meyerson sketch,[0],[0]
"If Mi is not too large, analyze xt 5: if |Mi| ≤ 4k(1 + logn)
( 22p+1
γp + 1
) then:
6: Let δ = d(xt,Mi)p 7: p̂ = min ( δk(1+logn)",5.2. The Meyerson sketch,[0],[0]
"Lp , 1 ) 8: Add xt to Mi with probability p̂ 9: Return M1, . . .",5.2. The Meyerson sketch,[0],[0]
",Ms
In the rest of the paper we refer to a single Meyerson sketch as Mi and to their union as M .",5.2. The Meyerson sketch,[0],[0]
Our next step is to show that in the Meyerson sketch there exists a subset of k centers that gives an approximately optimal solution.,5.3. From Meyerson to k clusters,[0],[0]
"We follow the approach in Guha et al. (2000) and show that by weighing the points in the Meyerson sketch with the number of original data points assigned to them, and then running a weighted k-clustering algorithm to recluster them into k clusters, we can achieve a constant approximate solution.
",5.3. From Meyerson to k clusters,[0],[0]
Before formalizing this observation we give some additional notation.,5.3. From Meyerson to k clusters,[0],[0]
"In the remainder of the section we denote the weight of a point x in the Meyerson sketch with w(x), the cost of the centers used in Meyerson sketch with costM, and the cost of the aforementioned weighted clustering instance with costL. Finally we refer to the optimal set of centers for the weighted k-clustering instance as c′.
We begin with two technical Lemmas.
",5.3. From Meyerson to k clusters,[0],[0]
Lemma 5.3.,5.3. From Meyerson to k clusters,[0],[0]
"For any constant p ≥ 1, costp(X, c′) ≤ 2p−1 (costM + costL)
Lemma 5.4.",5.3. From Meyerson to k clusters,[0],[0]
"For any constant p ≥ 1, costL ≤
22p−1 ( costM + optp )",5.3. From Meyerson to k clusters,[0],[0]
Note that combining Lemmas 5.3 and 5.4 the following Corollary follows.,5.3. From Meyerson to k clusters,[0],[0]
Corollary 5.5.,5.3. From Meyerson to k clusters,[0],[0]
"For any constant p ≥ 1, costp(c′) ≤ 23p−1 ( costM + optp
)",5.3. From Meyerson to k clusters,[0],[0]
We defer the proofs of lemma 5.3 and lemma 5.4 to the extended version of the paper.,5.3. From Meyerson to k clusters,[0],[0]
"Those proofs are similar in spirit to those in (Bateni et al., 2014; Guha et al., 2000), but are generalized here for all p.
Thanks to Corollary 5.5 we know that by using a Meyerson sketch, M contains a good approximation for our problem.",5.3. From Meyerson to k clusters,[0],[0]
"In the next subsection we show how to use this to obtain a solution for the consistency problem.
",5.3. From Meyerson to k clusters,[0],[0]
"Before doing this we define two algorithms that allow us to construct a weighted clustering instance starting from a Meyerson sketch (Algorithm 4) and to update the weights for a weighted instance (Algorithm 5).
",5.3. From Meyerson to k clusters,[0],[0]
"Algorithm 4CreateWeightedInstance(M1, . .",5.3. From Meyerson to k clusters,[0],[0]
.,5.3. From Meyerson to k clusters,[0],[0]
",Ms, φ,Xt) 1: Input: A sequence of points Xt, a lower bound to the opti-
mum φ and s independent Meyerson sketches M1, . . .",5.3. From Meyerson to k clusters,[0],[0]
",Ms. 2: Output: A weighted k-clustering instance (M,w).",5.3. From Meyerson to k clusters,[0],[0]
3: Let M = ∪iMi 4:,5.3. From Meyerson to k clusters,[0],[0]
Assign points in Xt to the closest point in M 5: Let w(y) to be equal to the number of points assigned to y ∈ M 6:,5.3. From Meyerson to k clusters,[0],[0]
"Return (M,w)
Algorithm 5 UpdateWeights(M,w, x) 1: Input: A point x, the current weights w and the Meyerson
sketch M .",5.3. From Meyerson to k clusters,[0],[0]
"2: Output: A weighted k-clustering instance (M,w).",5.3. From Meyerson to k clusters,[0],[0]
3: Assign x to the closest point in M 4: Let mx be the closest point to x in M 5: w(mx) = w(mx),5.3. From Meyerson to k clusters,[0],[0]
"+ 1 6: Return (M,w)",5.3. From Meyerson to k clusters,[0],[0]
"We are now ready to formally state and prove the correctness of our main algorithm, which we present in Algorithm 6.",5.4. The algorithm,[0],[0]
"The input of our algorithm is a sequence of points x0, x1, x2, . . .",5.4. The algorithm,[0],[0]
", xn. Recall, that we denote the prefix up to t as Xt, and the cost of the solution using centers c as costp(Xt, c).",5.4. The algorithm,[0],[0]
"Finally we assume to have access to a γapproximation algorithm A for the weighted k-clustering problem for any constant p-norm (we can use for example the local search algorithm described by Gupta & Tangwongsan (2008)).
",5.4. The algorithm,[0],[0]
We can now state our main theorem.,5.4. The algorithm,[0],[0]
Theorem 5.6.,5.4. The algorithm,[0],[0]
"For any constant p ≥ 1, with probability 1 − O(n−1), Algorithm 6 returns a sequence of
Algorithm 6",5.4. The algorithm,[0],[0]
"Consistent k-clustering algorithm 1: Input: A sequence of points x0, x1, x2, . . .",5.4. The algorithm,[0],[0]
", xn. 2: Output: A sequence of centers c0, c1, c2, . . .",5.4. The algorithm,[0],[0]
", cn 3: Select the first k points as centers c0 = {x0, x1, x2, . . .",5.4. The algorithm,[0],[0]
", xk} 4: t← 0 5: while costp(c0, Xt) = 0",5.4. The algorithm,[0],[0]
do: 6: ct ← c0; Output ct; t← t+ 1 7: φ← 0 .,5.4. The algorithm,[0],[0]
"Initialize lower bound to the optimum M1, ...,M2 logn ← ComputeMeyerson(X0, φ)
8: c← ∅; s← 2 logn 9: while t ≤ n",5.4. The algorithm,[0],[0]
"do:
10: Run A on Xt to get approximated solution c′ 11: if costp(Xt, c′) ≥",5.4. The algorithm,[0],[0]
2φ then: .,5.4. The algorithm,[0],[0]
New l.b.,5.4. The algorithm,[0],[0]
"for φ 12: φ← costp(Xt, c′), 13: M1, ...,Ms ← ComputeMeyerson(Xt, φ) 14: (M,w)← GetWeightedProb(M1, ...,Ms, φ,Xt) 15: Solve (M,w) using algorithm A 16: Let ct be the set of centers computed by A 17: else: .",5.4. The algorithm,[0],[0]
"Update Meyerson and recluster if needed 18: M1,M2, ..← UpdateMeyerson(M1, ..,Ms, xt, φ) 19: Let M = ∪iMi, 20: if xt ∈M then: .",5.4. The algorithm,[0],[0]
"xt is in Meyerson sketch 21: (M,w)← GetWeightedProb(M1, ...,Ms, φ,Xt) 22:",5.4. The algorithm,[0],[0]
"Solve (M,w) using algorithm A 23: Let ct be the set of centers computed by A 24: else: 25: (M,w)← UpdateWeights(M,w, x) 26: Let mt be the closest point to xt in M 27: if w(mt) is a power of 2 then: 28: .",5.4. The algorithm,[0],[0]
Weight of a point “doubled” 29:,5.4. The algorithm,[0],[0]
"Solve (M,w) using algorithm A 30: Let ct be the set of computed centers 31: else: 32: ct = ct−1 33: Output ct; t← t+ 1
centers c0, c1, c2, . . .",5.4. The algorithm,[0],[0]
", cn such that at any point in time t costp(ct, Xt) ≤ αpoptp(Xt) for a constant α and the total inconsistency factor of the solution is O(k2 log4 n)
",5.4. The algorithm,[0],[0]
Proof.,5.4. The algorithm,[0],[0]
"We start by bounding the inconsistency factor,∑n−1 i=1",5.4. The algorithm,[0],[0]
|ci+1,5.4. The algorithm,[0],[0]
"− ci|.
During the execution of Algorithm 6 the set of centers changes if and only if one of the three following conditions is met: (i) the cost of the clustering on Xt computed byA increases by a factor of 2, (ii) we add a new point to a Meyerson sketch, (iii) a new point is assigned to a point of the Meyerson sketch, mt, and the weight of mt is a power of 2 after this addition.",5.4. The algorithm,[0],[0]
"Note that every time we change the centers, we fully recluster, and so increase the consistency factor by k in the worst case.",5.4. The algorithm,[0],[0]
"Therefore to prove the theorem we need to show that one of these conditions is met at most O(k log4 n) times.
",5.4. The algorithm,[0],[0]
"From our assumptions we know that the spread of the point set is polynomial in n, which implies the same bound on the cost of the optimum solution.",5.4. The algorithm,[0],[0]
"Therefore, the cost of the solution computed by A doubles at most O(log n) times.
",5.4. The algorithm,[0],[0]
"For the same reason we update the lower bounds, φ at most O(log n) times during the execution of our algorithm.",5.4. The algorithm,[0],[0]
This in turn implies that we rebuild the Meyerson sketches from scratch at mostO(log n) times.,5.4. The algorithm,[0],[0]
Given that we runO(log n),5.4. The algorithm,[0],[0]
"Meyerson sketches in parallel, during the execution of the algorithm we use at most O(log2 n)",5.4. The algorithm,[0],[0]
Meyerson sketches.,5.4. The algorithm,[0],[0]
"Furthermore each Meyerson sketch has at most O(k log n) centers, thus in total we can add at most O(k log3 n) points under condition (ii).
",5.4. The algorithm,[0],[0]
"Finally note that while a Meyerson sketch is fixed, the weight of every point in the sketch can only grow.",5.4. The algorithm,[0],[0]
"In addition, the weight is always is bounded by n, and therefore can double at most log n times per sketch point, resulting in O(k log2 n) changes under a fixed Meyerson sketch.",5.4. The algorithm,[0],[0]
Therefore condition (iii) holds at most O(k log4 n) times.,5.4. The algorithm,[0],[0]
"So overall at least one of the conditions is satisfied at most O(k log4 n) times, thus the algorithm is O(k2 log4 n)-consistent.
",5.4. The algorithm,[0],[0]
To finish our proof we need to show that at any point in time our algorithm returns with probability 1−o(n−1) a constant approximation to the optimum.,5.4. The algorithm,[0],[0]
Note that by corollary 5.5 we know that for any constant p ≥ 1 the cost of a solution computed on the Meyerson sketch can be bounded by costp(c ′) ≤ 23p−1 ( costM + optp ) .,5.4. The algorithm,[0],[0]
From Lemma 5.2 we know that the Meyerson sketch guarantees with probability 1− o(n−1) that costM ≤ 16optp(X).,5.4. The algorithm,[0],[0]
So we have the cost of the optimal set of centers in the Meyerson sketch at any point in time is at mostO(αpoptp),5.4. The algorithm,[0],[0]
w.h.p.,5.4. The algorithm,[0],[0]
"for a constant α4.
",5.4. The algorithm,[0],[0]
"While we cannot compute the optimal set of centers in the Meyerson sketch, we can find an O(p) approximation for every constant p by relying on the local search algorithm of Gupta & Tangwongsan (2008).",5.4. The algorithm,[0],[0]
"Therefore, every time we recompute the centers usingA we are sure that we obtain a constant approximation.
",5.4. The algorithm,[0],[0]
"Finally it remains to show that when none of the three conditions are met, and we simply add a point to the solution without recomputing the centers we retain an approximately optimal solution.",5.4. The algorithm,[0],[0]
"By Lemma 5.3 we know that for any constant p ≥ 1, costp(c′) ≤ 2p−1",5.4. The algorithm,[0],[0]
"(costM + costL) .
",5.4. The algorithm,[0],[0]
"Moreover, we can always bound the cost of Meyerson sketch with 16optp(X).
",5.4. The algorithm,[0],[0]
It remains to get a bound on costL. Note that the number of points assigned to any point in M did not double since the previous reclustering.,5.4. The algorithm,[0],[0]
"Therefore, in the weighted reclustering formulation the weight of all points increased by a factor less than 2.",5.4. The algorithm,[0],[0]
"Therefore, costL at this point is bounded by at most twice costL computed when we last reclustered.",5.4. The algorithm,[0],[0]
"Therefore, costp(c′) ≤ 24p−1 ( costM + optp ) and the cur-
4We do not make an attempt to optimize the constant factors.",5.4. The algorithm,[0],[0]
"As we show in the experimental section, in practice the algorithm gives a very good approximation.
rent solution remains approximately optimal.",5.4. The algorithm,[0],[0]
How many times do we need to change the centers to obtain a good k-clustering at any point in time?,6. Optimizing Consistency,[0],[0]
"In Section 5 we presented an algorithm that is O(k2 log4 n)-consistent, while in Subsection 3.1 we showed that at least Ω(k log n) changes are needed (assuming that ∆ is polynomial in n).",6. Optimizing Consistency,[0],[0]
"We give an existential result, we show that for any input sequence there exist a solution that is constant approximate and O(k log2 n)-consistent.",6. Optimizing Consistency,[0],[0]
"In interest of space we deferred the proof of the lemma to the extended version of the paper.
",6. Optimizing Consistency,[0],[0]
Theorem 6.1.,6. Optimizing Consistency,[0],[0]
"For any sequence x0, x1, . . .",6. Optimizing Consistency,[0],[0]
", xn there exists a sequence of solutions c0, c1, . . .",6. Optimizing Consistency,[0],[0]
", cn such that ∀i, costp(Xi, ci) ≤ αoptp(Xi) for some constant α, and the∑ i |ci+1 − ci‖ = O(k log 2 n).",6. Optimizing Consistency,[0],[0]
We demonstrate the efficacy of our algorithm by tracking both the quality of the solution and the number of reclusterings needed to maintain it on a number of diverse datasets.,7. Experiments,[0],[0]
"As we will show, the theoretical guarantees that we prove in the previous section provide a loose bound on the number of reclusterings; in practice the number of times we recompute the solution grows logarithmically with time.
",7. Experiments,[0],[0]
"Data We evaluate our algorithm on three datasets from the UCI Repository (Lichman, 2013) that vary in data size and dimensionality.",7. Experiments,[0],[0]
"(i) SKINTYPE has 245, 057 points lying in 4-dimensions.",7. Experiments,[0],[0]
"(ii) SHUTTLE has 58, 000 points in 9 dimensions.",7. Experiments,[0],[0]
"(iii) COVERTYPE has 581, 012 points in 54 dimensions.",7. Experiments,[0],[0]
"For each of the datasets we try values of k in {10, 50, 100}, and observe that the qualitative results are consistent across datasets and values of k.
Algorithm Modifications In the development of the algorithm we made a number of decisions to obtain high probability results.",7. Experiments,[0],[0]
"The key among them was to run O(log n) copies of the Meyerson sketch, since each sketch succeeds only with constant probability.",7. Experiments,[0],[0]
"We eschew this change in the implementation, and maintain just a single sketch, at the cost of incurring a worse solution quality.
",7. Experiments,[0],[0]
"Metrics and results The goal of this work is to give algorithms that maintain a good clustering, but only recluster judiciously, when necessary.",7. Experiments,[0],[0]
"To that end, we focus on two main metrics: number of reclusterings and solution quality.
",7. Experiments,[0],[0]
Reclustering We plot the number of reclusterings as a function of time for the three different datasets in Figure 1.,7. Experiments,[0],[0]
"Note that the x-axis is on log-scale, and thus a straight line
represents number of reclusterings that grows logarithmically with time.",7. Experiments,[0],[0]
"Qualitatively we make two observations, across all datasets, and values of k.
First, the rate of reclustering (defined as the fraction of time the algorithm recomputes the solution) is approximately logn/n, which tends to 0 as the dataset size grows.",7. Experiments,[0],[0]
"Further, the rate is higher for higher values of k, a fact also suggested by our theoretical analysis.
",7. Experiments,[0],[0]
"Unlike the SHUTTLE and COVERTYPE datasets, the SKINTYPE dataset exhibits a change in behavior, where initially the reclustering rate is relatively high, but then it sharplly drops after about O(2k) steps.",7. Experiments,[0],[0]
This is explained by the fact that the order of the points in this data set is not random.,7. Experiments,[0],[0]
"Therefore initially the algorithm reclusters at a high rate, once all of the parts of the input space are explored, the rate of reclustering slows.",7. Experiments,[0],[0]
"When we run the algorithm on a randomly permuted instance of SKINTYPE, this phase transition in behavior disappears.
",7. Experiments,[0],[0]
"Approximation Ratio We plot the approximation ratio of the solution (as compared to the best obtained by ten runs of k-means++ (Arthur & Vassilvitskii, 2007)) in Figure 2.
",7. Experiments,[0],[0]
"For the SKIN and COVERTYPE datasets, the approximation ratio stays relatively low, largely bounded by 4, after an initial period.",7. Experiments,[0],[0]
"A more careful examination of the plots shows exactly the times when the consistent algorithm allows the solution to degrade, and when it decides to recompute the solution from scratch.",7. Experiments,[0],[0]
"The latter are indicated by sharp drops in the approximation ratio, whereas the former are the relatively flat patterns.
",7. Experiments,[0],[0]
"It is interesting to note that the additional points sometimes worsen the approximation (as indicated by the lines sloping upwards), but sometimes actually improve the approximation.",7. Experiments,[0],[0]
"This is due to the fact that decisions made by the online algorithm balance optimality at that point in time, with the potential location of points arriving in the future.",7. Experiments,[0],[0]
"The latter is most apparent in the k = 100 experiment of the SHUTTLE dataset.
",7. Experiments,[0],[0]
All of the datasets sometimes exhibit large fluctuations in the approximation ratio.,7. Experiments,[0],[0]
"This is an artifact of using a single Myerson sketch, which does not capture the structure of the points with small, but constant probability.",7. Experiments,[0],[0]
We introduced the notion of consistent clustering: a variant of online clustering which balances the need for maintaining an approximately optimal solution with the cost of reclustering.,8. Conclusions and Future Work,[0],[0]
"We proved Ω(klog n) lower bounds, and gave algorithms for all k-clustering variants that come close to achieving this bound.
",8. Conclusions and Future Work,[0],[0]
The notion of quantifying the worst case number of changes necessary to maintain a constant approximate solution in an online setting is interesting to study in contexts other than k-clustering.,8. Conclusions and Future Work,[0],[0]
"For example, one can consider online graph problems, such as online matching and online densest subgraph, or other types of clustering problems, such as hierarchical or correlation clustering.",8. Conclusions and Future Work,[0],[0]
The study of online algorithms and competitive analysis provides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner.,abstractText,[0],[0]
"While in some scenarios the decisions are indeed irrevocable, there are many practical situations when changing a previous decision is not impossible, but simply expensive.",abstractText,[0],[0]
In this work we formalize this notion and introduce the consistent k-clustering problem.,abstractText,[0],[0]
"With points arriving online, the goal is to maintain a constant approximate solution, while minimizing the number of reclusterings necessary.",abstractText,[0],[0]
"We prove a lower bound, showing that Ω(k log n) changes are necessary in the worst case for a wide range of objective functions.",abstractText,[0],[0]
"On the positive side, we give an algorithm that needs onlyO(k log n) changes to maintain a constant competitive solution, an exponential improvement on the naive solution of reclustering at every time step.",abstractText,[0],[0]
"Finally, we show experimentally that our approach performs much better than the theoretical bound, with the number of changes growing approximately as O(log n).",abstractText,[0],[0]
Consistent k-Clustering,title,[0],[0]
"Reinforcement Learning (RL) techniques were successfully applied in fields such as robotics, games, marketing and more (Kober et al., 2013; Al-Rawi et al., 2015; Barrett et al., 2013).",1. Introduction,[0],[0]
We consider the problem of off-policy evaluation (OPE) – assessing the performance of a complex strategy without applying it.,1. Introduction,[0],[0]
An OPE formulation is often considered in domains with limited sampling capability.,1. Introduction,[0],[0]
"For example, marketing and recommender systems (Theocharous and Hallak, 2013; Theocharous et al., 2015) directly relate policies to revenue.",1. Introduction,[0],[0]
"A more extreme example is drug administration, as there are only few patients in
1The Technion, Haifa, Israel.",1. Introduction,[0],[0]
"Correspondence to: Assaf Hallak <ifogph@gmail.com>, Shie Mannor <shie@ee.technion.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"the testing population, and sub-optimal policies can have life threatening effects (Hochberg et al., 2016).",1. Introduction,[0],[0]
"OPE can also be useful as a module for policy optimization in a policy improvement scheme (Thomas et al., 2015a).
",1. Introduction,[0],[0]
"In this paper, we consider the OPE problem in an on-line setup where each new sample is immediately used to update our current value estimate of some previously unseen policy.",1. Introduction,[0],[0]
"We propose and analyze a new algorithm called COP-TD(λ,β) for estimating the value of the target policy; COP-TD(λ,β) has the following properties:
1.",1. Introduction,[0],[0]
"Easy to understand and implement on-line.
",1. Introduction,[0],[0]
2.,1. Introduction,[0],[0]
"Allows closing the gap to consistency such that the limit point is the same that would have been obtained by on-policy learning with the target policy.
",1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"Empirically comparable to state-of-the art algorithms.
",1. Introduction,[0],[0]
"Our algorithm resembles (Sutton et al., 2015)’s Emphatic TD that was extended by (Hallak et al., 2015) to the general parametric form ETD(λ,β).",1. Introduction,[0],[0]
We clarify the connection between the algorithms and compare them empirically.,1. Introduction,[0],[0]
"Finally, we introduce an additional related heuristic called Log-COP-TD(λ,β) and motivate it.",1. Introduction,[0],[0]
"We consider the standard discounted Markov Decision Process (MDP) formulation (Bertsekas and Tsitsiklis, 1996) with a single long trajectory.",2. Notations and Background,[0],[0]
"Let M = (S,A,P,R, ζ, γ) be an MDP where S is the finite state space and A is the finite action space.",2. Notations and Background,[0],[0]
"The parameter P sets the transition probabilities Pr(s′|s, a) given the previous state s ∈ S and action a ∈ A, where the first state is determined by the distribution ζ.",2. Notations and Background,[0],[0]
"The parameter R sets the reward distribution r(s, a) obtained by taking action a in state s and γ is the discount factor specifying the exponential reduction in reward with time.",2. Notations and Background,[0],[0]
"The process advances as follows:
A state s0 is sampled according to the distribution ζ(s).",2. Notations and Background,[0],[0]
"Then, at each time step t starting from t = 0 the agent draws an action at according to the stochastic behavior policy µ(a|st), a reward rt .",2. Notations and Background,[0],[0]
"= r(st, at) is accumulated by the agent, and the next state st+1 is sampled using the transition probability Pr(s′|st, at).
",2. Notations and Background,[0],[0]
"The expected discounted accumulated reward starting from a specific state and choosing an action by some policy π is called the value function, which is also known to satisfy the Bellman equation in a vector form:
V π(s) =",2. Notations and Background,[0],[0]
"Eπ [ ∞∑ t=0 γtrt ∣∣∣ s0 = s] , TπV .=",2. Notations and Background,[0],[0]
"Rπ + γPπV, where [Rπ]s .",2. Notations and Background,[0],[0]
=,2. Notations and Background,[0],[0]
"Eπ [r(s, π(s))]",2. Notations and Background,[0],[0]
"and [Pπ]s,s′ .",2. Notations and Background,[0],[0]
=,2. Notations and Background,[0],[0]
"Eπ [Pr(s′|s, π(s))] are the policy induced reward vector and transition probability matrix respectively; Tπ is called the Bellman operator.",2. Notations and Background,[0],[0]
The problem of estimating V π(s) from samples is called policy evaluation.,2. Notations and Background,[1.0],['The problem of estimating V π(s) from samples is called policy evaluation.']
"If the target policy π is different than the behavior policy µ which generated the samples, the problem is called off-policy evaluation (OPE).",2. Notations and Background,[1.0],"['If the target policy π is different than the behavior policy µ which generated the samples, the problem is called off-policy evaluation (OPE).']"
"The TD(λ) (Sutton, 1988) algorithm is a standard solution to on-line on-policy evaluation: Each time step the temporal difference error updates the current value function estimate, such that eventually the stochastic approximation process will converge to the true value function.",2. Notations and Background,[0],[0]
"The standard form of TD(λ) is given by:
R (n) t,st = n−1∑ i=0 γirt+i",2. Notations and Background,[0],[0]
"+ γ nV̂t(st+n),
Rλt,st =(1− λ) ∞∑ n=0 λnR(n+1)st ,
V̂t+1(st) =V̂t(st) +",2. Notations and Background,[0],[0]
"αt ( Rλt,st − V̂t(st) ) ,
(1)
where αt is the step size.",2. Notations and Background,[0],[0]
"The value R (n) t,st is an estimate of the current state’s V (st), looking forward n steps, and Rλt,st is an exponentially weighted average of all of these estimates going forward till infinity.",2. Notations and Background,[0],[0]
"Notice that Equation 1 does not specify an on-line implementation since R(n)t,st depends on future observations, however there exists a compact on-line implementation using eligibility traces (Bertsekas and Tsitsiklis (1996) for on-line TD(λ), and Sutton et al. (2014), Sutton et al. (2015) for off-policy TD(λ)).",2. Notations and Background,[0],[0]
"The underlying operator of TD(λ) is given by:
Tλπ V = (1− λ) ∞∑ n=0 λn ( n∑ i=0 γiP",2. Notations and Background,[0],[0]
iπRπ + γ n+1Pn+1π V ) =,2. Notations and Background,[0],[0]
(1− λ)(I,2. Notations and Background,[0],[0]
"− λTπ)−1TπV,
and is a γ(1−λ)1−λγ -contraction (Bertsekas, 2012).
",2. Notations and Background,[0],[0]
We denote by dµ(s) the stationary distribution over states induced by taking the policy µ and mark Dµ = diag(dµ).,2. Notations and Background,[0],[0]
"Since we are concerned with the behavior at infinite horizon, we assume ζ(s) = dµ(s).",2. Notations and Background,[0],[0]
"In addition, we assume that the MDP is ergodic for the two specified policies µ,",2. Notations and Background,[0],[0]
"π so ∀s ∈ S : dµ(s) > 0, dπ(s) > 0 and that the OPE problem is proper – π(a|s) >",2. Notations and Background,[0],[0]
"0⇒ µ(a|s) > 0.
",2. Notations and Background,[0],[0]
"When the state space is too large to hold V π(s), a linear function approximation scheme is used: V π(s)",2. Notations and Background,[0],[0]
≈,2. Notations and Background,[0],[0]
"θ>π φ(s), where θ is the optimized weight vector and φ(s) is the feature vector of state s composed of k features.",2. Notations and Background,[0],[0]
"We denote by Πdπ the projection to the subspace spanned by the features with respect to the dπ-weighted norm, and by Φ ∈ RS,k the matrix whose lines consist of the feature vectors for each state and assume its columns are linearly independent.
TD(λ) can be adjusted to find the fixed point of ΠdπT λ π (Sutton and Barto, 1998):
R (n) t,st = n−1∑ i=0 γirt+i",2. Notations and Background,[0],[0]
"+ γ nθ>t φ(st+n),
Rλt,st =(1− λ) ∞∑",2. Notations and Background,[0],[0]
"n=0 λnR(n+1)st ,
θt+1 =θt +",2. Notations and Background,[0],[0]
αt,2. Notations and Background,[0],[0]
"( Rλt,st − θ > t φ(st) ) φ(st).
",2. Notations and Background,[0],[0]
"Finally, we define OPE-related quantities:
ρt .",2. Notations and Background,[0],[0]
"= π(at|st) µ(at|st) , Γnt .",2. Notations and Background,[0],[0]
"= n−1∏ i=0 ρt−1−i, ρd(s) .",2. Notations and Background,[0],[0]
"= dπ(s) dµ(s) ,
we call ρd the covariate shift ratio (as denoted under different settings by (Hachiya et al., 2012)).
",2. Notations and Background,[0],[0]
"We summarize the assumptions used in the proofs:
1.",2. Notations and Background,[0],[0]
"For both policies the induced Markov chain is ergodic.
2.",2. Notations and Background,[0],[0]
"The first state s0 is distributed according to the stationary distribution of the behavior policy dµ(s).
3.",2. Notations and Background,[0],[0]
The problem is proper: π(a|s) >,2. Notations and Background,[0],[0]
"0⇒ µ(a|s) > 0.
4.",2. Notations and Background,[0],[0]
"The feature matrix Φ has full rank k.
Assumption 1 is commonly used for convergence theorems as it verifies the value function is well defined on all states regardless of the initial sampled state.",2. Notations and Background,[0],[0]
Assumption 2 can be relaxed since we are concerned with the long-term properties of the algorithm past its mixing time – we require it for clarity of the proofs.,2. Notations and Background,[0],[0]
Assumption 3 is required so the importance sampling ratios will be well defined.,2. Notations and Background,[1.0],['Assumption 3 is required so the importance sampling ratios will be well defined.']
Assumption 4 guarantees the optimal θ is unique which greatly simplifies the proofs.,2. Notations and Background,[0],[0]
We can roughly categorize previous OPE algorithms to two main families.,3. Previous Work,[0],[0]
Gradient based methods that perform stochastic gradient descent on error terms they want to minimize.,3. Previous Work,[0],[0]
"These include GTD (Sutton et al., 2009a), GTD-2,
TDC (Sutton et al., 2009b) and HTD (White and White, 2016).",3. Previous Work,[0],[0]
"The main disadvantages of gradient based methods are (A) they usually update an additional error correcting term, which means another time-step parameter needs to be controlled; and (B) they rely on estimating non-trivial terms, an estimate that tends to converge slowly.",3. Previous Work,[0],[0]
The other family uses importance sampling (IS) methods that correct the gains between on-policy and off-policy updates using the IS-ratios ρt’s.,3. Previous Work,[0],[0]
"Among these are full IS (Precup et al., 2001) and ETD(λ,β) (Sutton et al., 2015).",3. Previous Work,[0],[0]
"These methods are characterized by the bias-variance trade-off they resort to – navigating between biased convergent values (or even divergent), and very slow convergence stemming from the high variance of IS correcting factors (the ρt products).",3. Previous Work,[0],[0]
"There are also a few algorithms that fall between the two, for example TO-GTD (van Hasselt et al., 2014) and WISTD(λ) (Mahmood and Sutton, 2015).
",3. Previous Work,[0],[0]
"A comparison of these algorithms in terms of convergence rate, synergy with function approximation and more is available in (White and White, 2016; Geist and Scherrer, 2014).",3. Previous Work,[1.0],"['A comparison of these algorithms in terms of convergence rate, synergy with function approximation and more is available in (White and White, 2016; Geist and Scherrer, 2014).']"
We focus in this paper on the limit point of the convergence.,3. Previous Work,[1.0],['We focus in this paper on the limit point of the convergence.']
"For most of the aforementioned algorithms, the process was shown to converge almost surely to the fixed point of the projected Bellman operator ΠdTπ where d is some stationary distribution (usually dµ), however the d in question was never1 dπ as we would have obtained from running on-policy TD with the target policy (also see (Kolter, 2011) for relevant discussion).",3. Previous Work,[1.0],"['For most of the aforementioned algorithms, the process was shown to converge almost surely to the fixed point of the projected Bellman operator ΠdTπ where d is some stationary distribution (usually dµ), however the d in question was never1 dπ as we would have obtained from running on-policy TD with the target policy (also see (Kolter, 2011) for relevant discussion).']"
"The algorithm achieving the closest result is ETD(λ,β) which replaced d with f =",3. Previous Work,[0],[0]
( I − βP>π )−1,3. Previous Work,[0],[0]
"dµ, where β trades-off some of the process’ variance with the bias in the limit point.",3. Previous Work,[0],[0]
"Hence, our main contribution is a consistent algorithm which can converge to the same value that would have been obtained by running an on-policy scheme with the same policy.",3. Previous Work,[1.0],"['Hence, our main contribution is a consistent algorithm which can converge to the same value that would have been obtained by running an on-policy scheme with the same policy.']"
"Here we provide a motivating example showing that even in simple cases with “close” behavior and target policies, the two induced stationary distributions can differ greatly.",4. Motivation,[1.0],"['Here we provide a motivating example showing that even in simple cases with “close” behavior and target policies, the two induced stationary distributions can differ greatly.']"
"Choosing a specific linear parameterization further emphasizes the difference between applying on-policy TD with the target policy, and applying inconsistent off-policy TD.
",4. Motivation,[1.0000000616323241],"['Choosing a specific linear parameterization further emphasizes the difference between applying on-policy TD with the target policy, and applying inconsistent off-policy TD.']"
"Assume a chain MDP with numbered states 1, 2, ..|S|, where from each state s you can either move left to state s − 1, or right to state s + 1.",4. Motivation,[0],[0]
If you’ve reached the beginning or the end of the chain (states 1 or |S|) then taking a step further does not affect your location.,4. Motivation,[0],[0]
"Assume the behavior policy moves left with probability 0.5 + , while the target policy moves right with probability 0.5+ .",4. Motivation,[1.0],"['Assume the behavior policy moves left with probability 0.5 + , while the target policy moves right with probability 0.5+ .']"
"It is easy
1Except full IS, however its variance is too high to be applicable in practice.
",4. Motivation,[0],[0]
to see that the stationary distributions are given by: dµ(s) ∝,4. Motivation,[0],[0]
"(
0.5− 0.5 +
)s , dπ(s) ∝",4. Motivation,[0],[0]
"( 0.5 +
0.5−
)s .
",4. Motivation,[0],[0]
"For instance, if we have a length 100 chain with = 0.01, for the rightmost state we have dµ(|S|) ≈ 8 · 10−4, dπ(|S|) ≈ 0.04.",4. Motivation,[1.0],"['For instance, if we have a length 100 chain with = 0.01, for the rightmost state we have dµ(|S|) ≈ 8 · 10−4, dπ(|S|) ≈ 0.04.']"
"Let’s set the reward to be 1 for the right half of the chain, so the target policy is better since it spends more time in the right half.",4. Motivation,[0],[0]
"The value of the target policy in the edges of the chain for γ = 0.99 is V π(1) = 0.21, V π(100) = 99.97.
",4. Motivation,[0],[0]
Now what happens if we try to approximate the value function using one constant feature φ(s) ≡ 1?,4. Motivation,[0],[0]
"The fixed point of ΠdµTπ is θ = 11.92, while the fixed point of ΠdπTπ is θ = 88.08 – a substantial difference.",4. Motivation,[0],[0]
"The reason for this difference lies in the emphasis each projection puts on the states: according to Πdµ , the important states are in the left half of the chain – these with low value function, and therefore the value estimation of all states is low.",4. Motivation,[0],[0]
"However, according to Πdπ the important states are concentrated on the right part of the chain since the target policy will visit these more often.",4. Motivation,[0],[0]
"Hence, the estimation error is emphasized on the right part of the chain and the value estimation is higher.",4. Motivation,[0],[0]
"When we wish to estimate the value of the target policy, we want to know what will happen if we deploy it instead of the behavior policy, thus taking the fixed point of ΠdπTπ better represents the off-policy evaluation solution.
5.",4. Motivation,[0],[0]
"COP-TD(λ, β) Most off-policy algorithms multiply the TD summand of TD(λ) with some value that depends on the history and the current state.",4. Motivation,[0],[0]
"For example, full IS-TD by (Precup et al., 2001) examines the ratio between the probabilities of the trajectory under both policies:
Pπ(s0, a0, s1, . . .",4. Motivation,[0],[0]
", st, at) Pµ(s0, a0, s1, . . .",4. Motivation,[0],[0]
", st, at) =",4. Motivation,[0],[0]
t∏ m=0,4. Motivation,[0],[0]
ρm,4. Motivation,[0],[0]
= Γ t tρt.,4. Motivation,[0],[0]
"(2)
In problems with a long horizon, or these that start from the stationary distribution, we suggest using the time-invariant covariate shift ρd multiplied by the current ρt.",4. Motivation,[1.000000012119872],"['(2) In problems with a long horizon, or these that start from the stationary distribution, we suggest using the time-invariant covariate shift ρd multiplied by the current ρt.']"
"The intuition is the following: We would prefer using the probabilities ratio given in Equation 2, but it has very high variance, and after many time steps we might as well look at the stationary distribution ratio instead.",4. Motivation,[0],[0]
"This direction leads us to the following update equations:
θt+1 = θt + αtρd(st)ρt ( rt + θ > t (γφ(st+1)− φ(st)) ) φ(st).
",4. Motivation,[0],[0]
(3) Lemma 1.,4. Motivation,[0],[0]
If the αt satisfy ∑∞ t=0,4. Motivation,[0],[0]
"αt =∞, ∑∞ t=0",4. Motivation,[0],[0]
α 2 t <∞ then the process described by Eq.,4. Motivation,[0],[0]
"(3) converges almost surely to the fixed point of ΠπTπV = V .
",4. Motivation,[0],[0]
"The proof follows the ODE method (Kushner and Yin, 2003) similarly to Tsitsiklis and Van Roy (1997) (see the appendix for more details).
",4. Motivation,[0],[0]
"Since ρd(s) is generally unknown, it is estimated using an additional stochastic approximation process.",4. Motivation,[1.0],"['Since ρd(s) is generally unknown, it is estimated using an additional stochastic approximation process.']"
"In order to do so, we note the following Lemma:
Lemma 2.",4. Motivation,[0],[0]
"Let ρ̂d be an unbiased estimate of ρd, and for every n = 0, 1, . . .",4. Motivation,[0],[0]
", t define Γ̃nt .",4. Motivation,[0],[0]
= ρ̂d(st−n)Γ n t .,4. Motivation,[0],[0]
"Then:
Eµ [ Γ̃nt |st ] = ρd(st).
",4. Motivation,[0],[0]
"For any state st there are t→∞ such quantities {Γ̃nt }tn=0, where we propose to weight them similarly to TD(λ):
Γ̃βt = (1− β) ∞∑ n=0 βnΓ̃n+1t .
",4. Motivation,[0],[0]
"Note that ρd(s), unlike V (s), is restricted to a close set since its dµ-weighted linear combination is equal to 1 and all of its entries are non-negative; We denote this dµweighted simplex by ∆dµ , and let Π∆dµ be the (non-linear) projection to this set with respect to the Euclidean norm (Π∆dµ can be calculated efficiently, (Chen and Ye, 2011)).",4. Motivation,[0],[0]
"Now, we can devise a TD algorithm which estimates ρd and uses it to find θ, which we call COP-TD(0, β) (Consistent Off-Policy TD).
",4. Motivation,[0.9999999975909473],"['Now, we can devise a TD algorithm which estimates ρd and uses it to find θ, which we call COP-TD(0, β) (Consistent Off-Policy TD).']"
"Algorithm 1 COP-TD(0,β), Input: θ0, ρ̂d,0,
1: Init: F0 = 0, n β 0 = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: N(st) = N(st) + 1, ∀s ∈ S : d̂µ(s) =",4. Motivation,[0],[0]
"N(s)t 6: nβt = βn β t + 1 7: Update Γnt ’s weighted average: 8: Ft = ρt−1(βFt−1 + est−1) 9: Update & project by ρd’s TD error:
10: δdt = F>t ρ̂d,t
nβt︸ ︷︷ ︸ →Γ̃βt
−ρ̂d,t(st)
11: ρ̂d,t+1 = Π∆d̂µ",4. Motivation,[0],[0]
"( ρ̂d,t + α d t δ",4. Motivation,[0],[0]
d t est ) 12: Off-policy TD(0): 13: δt = rt,4. Motivation,[0],[0]
"+ θ>t (γφ(st+1)− φ(st)) 14: θt+1 = θt + αtρ̂d,t+1(st)ρtδtφ(st) 15: end for
Similarly to the Bellman operator for TD-learning, we define the underlying COP-operator Y and its β extension:
Y u = D−1µ P > π",4. Motivation,[0],[0]
"Dµu,
Y βu = (1− β)D−1µ P>π (I − βP>π )−1Dµu.
",4. Motivation,[0],[0]
The following Lemma may give some intuition on the convergence of the ρd estimation process: Lemma 3.,4. Motivation,[1.0],['The following Lemma may give some intuition on the convergence of the ρd estimation process: Lemma 3.']
"Under the ergodicity assumption, denote the eigenvalues of Pπ by 0 ≤ · · · ≤ |ξ2| < ξ1 = 1.",4. Motivation,[0],[0]
"Then Y β is a maxi 6=1
(1−β)|ξi| |1−βξi| < 1-contraction in the L2-norm on the
orthogonal subspace to ρd, and ρd is a fixed point of Y β .
",4. Motivation,[0.9999999391620586],"['Then Y β is a maxi 6=1 (1−β)|ξi| |1−βξi| < 1-contraction in the L2-norm on the orthogonal subspace to ρd, and ρd is a fixed point of Y β .']"
The technical proof is given in the appendix.,4. Motivation,[0],[0]
Theorem 1.,4. Motivation,[0],[0]
"If the step sizes satisfy ∑ t αt = ∑ t α d t =
∞, ∑ t(α 2 t +",4. Motivation,[0],[0]
"(α d t )
",4. Motivation,[0],[0]
"2) < ∞, αt αdt → 0, tαdt → 0, and E [ (βnΓnt ) 2|st ] ≤",4. Motivation,[0],[0]
"C for some constant C and every t and n, then after applying COP-TD(0, β), ρ̂d,t converges to ρd almost surely, and θt converges to the fixed point of ΠπTπV .
",4. Motivation,[0],[0]
"Notice that COP-TD(0, β) given in Alg.",4. Motivation,[0],[0]
1 is infeasible in problems with large state spaces since ρd ∈ R|S|.,4. Motivation,[0],[0]
"Like TD(λ), we can introduce linear function approximation: represent ρd(s) ≈ θ>ρ φρ(s) where θρ is a weight vector and φρ(s) is the off-policy feature vector and adjust the algorithm accordingly.",4. Motivation,[0],[0]
"For ρ̂d to still be contained in the set ∆dµ , we pose the requirement on the feature vectors: φρ(s) ∈",4. Motivation,[0],[0]
"Rk+, and ∑ s dµ(s)θ > ρ φρ(s) = 1 ( noted as
the simplex projection Π∆Eµ[φρ(s)] ) .",4. Motivation,[0],[0]
"In practice, the latter
requirement can be approximated: ∑ s dµ(s)θ > ρ φρ(s)",4. Motivation,[0],[0]
≈ 1 t θ,4. Motivation,[0],[0]
>,4. Motivation,[0],[0]
"ρ ∑ t φρ(st) = 1 resulting in an extension of the previously applied dµ estimation (step 5 in COP-TD(0, β)).",4. Motivation,[0],[0]
"We provide the full details in Algorithm 2, which also incorporates non-zero λ ( similarly to ETD(λ,β) ) .
",4. Motivation,[0],[0]
"Algorithm 2 COP-TD(λ,β) with Function Approximation, Input: θ0, θρ,0
1: Init: F0 = 0, n β 0 = 1, Nφ = 0, e0 = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: nβt = βn β t + 1, Nφ = Nφ + φρ(st), d̂φρ =",4. Motivation,[0],[0]
"Nφ t 6: Update Γnt ’s weighted average: 7: Ft = ρt−1(βFt−1 + φρ(st−1)) 8: Update & project by ρd’s TD error: 9: δdt = θ > ρ,t−1 ( Ft nβt − φρ(st)
) 10: θρ,t+1 = Π∆d̂φρ",4. Motivation,[0],[0]
"( θρ,t + α d t δ",4. Motivation,[0],[0]
d,4. Motivation,[0],[0]
"t φρ(st)
)",4. Motivation,[0],[0]
11:,4. Motivation,[0],[0]
"Off-policy TD(λ): 12: Mt = λ+ (1− λ)θ>ρ,t+1φρ(st) 13:",4. Motivation,[0],[0]
et = ρt (λγet +Mtφ(st+1)),4. Motivation,[0],[0]
"14: δt = rt + θ>t (γφ(st+1)− φ(st)) 15: θt+1 = θt + αtδtet 16: end for
Theorem 2.",4. Motivation,[0],[0]
"If the step sizes satisfy ∑ t αt = ∑ t α d t =
∞, ∑ t(α 2 t +",4. Motivation,[0],[0]
"(α d t )
",4. Motivation,[0],[0]
"2) < ∞, αt αdt → 0, tαdt → 0, and E [ (βnΓnt ) 2|st ] ≤",4. Motivation,[0],[0]
"C for some constant C and every t, n,
then after applying COP-TD(0, β) with function approximation satisfying φρ(s) ∈",4. Motivation,[0],[0]
"Rk+, ρ̂d,t converges to the fixed point of Π∆Eµ[φρ]ΠφρY
β denoted by ρCOPd almost surely, and if θt converges it is to the fixed point of Πdµ◦ρCOPd TπV , where ◦ is a coordinate-wise product of vectors.
",4. Motivation,[0],[0]
The proof is given in the appendix and also follows the ODE method.,4. Motivation,[0],[0]
"Notice that a theorem is only given for λ = 0, convergence results for general λ should follow the work by Yu (2015).
",4. Motivation,[0],[0]
"A possible criticism on COP-TD(0,β) is that it is not actually consistent, since in order to be consistent the original state space has to be small, in which case every off-policy algorithm is consistent as well.",4. Motivation,[0],[0]
"Still, the dependence on another set of features allows to trade-off accuracy with computational power in estimating ρd and subsequently V .",4. Motivation,[1.0],"['Still, the dependence on another set of features allows to trade-off accuracy with computational power in estimating ρd and subsequently V .']"
"Moreover, smart feature selection may further reduce this gap, and COP-TD(0, β) is still the first algorithm addressing this issue.",4. Motivation,[0],[0]
"We conclude with linking the error in ρd’s estimate with the difference in the resulting θ, which suggests that a well estimated ρd results in consistency: Corollary 1.",4. Motivation,[1.0],"['We conclude with linking the error in ρd’s estimate with the difference in the resulting θ, which suggests that a well estimated ρd results in consistency: Corollary 1.']"
Let 0,4. Motivation,[0],[0]
< < 1.,4. Motivation,[0],[0]
If (1 − )ρd ≤ ρCOPd ≤ (1 + ),4. Motivation,[0],[0]
"ρd, then the fixed point of COP-TD(0,β) with function approximation θCOP satisfies the following, where ‖ · ‖∞ is the L∞ induced norm:
‖θ∗",4. Motivation,[0],[0]
− θCOP‖∞ ≤ ‖A−1π Φ>‖∞,4. Motivation,[0],[0]
"( Rmax + (1 + γ)‖Φ‖∞‖θCOP‖∞ ) ,
whereAπ = Φ>Dπ(I−γPπ)Φ, and θ∗ sets the fixed point of the operator ΠdπTπV .",4. Motivation,[0],[0]
"Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD.","5.1. Relation to ETD(λ, β)",[1.0],"['Recently, Sutton et al. (2015) had suggested an algorithm for off-policy evaluation called Emphatic TD.']"
"Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(λ, β), which was shown to perform extremely well empirically by White and White (2016).","5.1. Relation to ETD(λ, β)",[1.0],"['Their algorithm was later on extended by Hallak et al. (2015) and renamed ETD(λ, β), which was shown to perform extremely well empirically by White and White (2016).']"
"ETD(0, β) can be represented as:
Ft = (1− β) ∞∑ n=0 βnΓnt ,
θt+1 = θt + αtFtρt ( rt + θ >","5.1. Relation to ETD(λ, β)",[0],[0]
t (γφ(st+1)− φ(st)) ) .,"5.1. Relation to ETD(λ, β)",[0],[0]
"(4)
As mentioned before, ETD(λ, β) converges to the fixed point of ΠfTλπ (Yu, 2015), where f = E","5.1. Relation to ETD(λ, β)",[0],[0]
"[Ft|st] = (I − βPπ)
−1dµ.","5.1. Relation to ETD(λ, β)",[0],[0]
"Error bounds can be achieved by showing that the operator ΠfTλπ is a contraction under certain requirements on β and that the variance of Ft is directly related to β as well (Hallak et al., 2015) (and thus affects the convergence rate of the process).
","5.1. Relation to ETD(λ, β)",[0],[0]
"When comparing ETD(λ,β)’s form to COP-TD(λ,β)’s, instead of spending memory and time resources on a
state/feature-dependent Ft, ETD(λ,β) uses a one-variable approximation.","5.1. Relation to ETD(λ, β)",[0],[0]
"The resulting Ft is in fact a one-step estimate of ρd, starting from ρ̂d(s) ≡ 1 (see Equations 9, 4), up to a minor difference: F ETDt = βF","5.1. Relation to ETD(λ, β)",[0],[0]
"COP-TD t + 1 (which following our logic adds bias to the estimate 2).
","5.1. Relation to ETD(λ, β)",[0],[0]
"Unlike ETD(λ, β), COP-TD(λ,β)’s effectiveness depends on the available resources.","5.1. Relation to ETD(λ, β)",[0],[0]
The number of features φρ(s) can be adjusted accordingly to provide the most affordable approximation.,"5.1. Relation to ETD(λ, β)",[0],[0]
"The added cost is fine-tuning another stepsize, though β’s effect is less prominent.","5.1. Relation to ETD(λ, β)",[0],[0]
"We now present a heuristic algorithm which works similarly to COP-TD(λ, β).",6. The Logarithm Approach for Handling Long Products,[0],[0]
"Before presenting the algorithm, we explain the motivation behind it.",6. The Logarithm Approach for Handling Long Products,[0],[0]
Konidaris et al. (2011) suggested a statistical interpretation of TD(λ).,6.1. Statistical Interpretation of TD(λ),[0],[0]
"They show that under several assumptions the TD(λ) estimate Rλst is the maximum likelihood estimator of V (st) given Rnst : (1) Each R n st is an unbiased estimator of V (st); (2) The random variables Rnst are independent and specifically uncorrelated; (3) The random variables Rnst are jointly normally distributed; and (4) The variance of each Rnst is proportional to λ n.
Under Assumptions 1-3 the maximum likelihood estimator of V (s) given its previous estimate can be represented as a linear convex combination of Rnst with weights:
wn =
[ Var ( R (n) st )]−1 ∑∞ m=0",6.1. Statistical Interpretation of TD(λ),[0],[0]
"[ Var ( R (m) st
)]−1 .",6.1. Statistical Interpretation of TD(λ),[0],[0]
"Subsequently, in Konidaris et al. (2011) Assumption 4 was relaxed and instead a closed form approximation of the variance was proposed.",6.1. Statistical Interpretation of TD(λ),[0],[0]
"In a follow-up paper by Thomas et al. (2015b), the second assumption was also removed and the weights were instead given as: wn = 1>cov(Rst )",6.1. Statistical Interpretation of TD(λ),[0],[0]
"en 1>cov(Rst )1
, where the covariance matrix can be estimated from the data, or otherwise learned through some parametric form.
",6.1. Statistical Interpretation of TD(λ),[0],[0]
"While both the approximated variance and learned covariance matrix solutions improve performance on several benchmarks, the first uses a rather crude approximation, and the second solution is both state-dependent and based on noisy estimates of the covariance matrix.",6.1. Statistical Interpretation of TD(λ),[0],[0]
"In addition, there aren’t efficient on-line implementations since all past
2We have conducted several experiments with an altered ETD and indeed obtained better results compared with the original, these experiments are outside the scope of the paper.
",6.1. Statistical Interpretation of TD(λ),[0],[0]
weights should be recalculated to match a new sample.,6.1. Statistical Interpretation of TD(λ),[0],[0]
"Still, the suggested statistical justification is a valuable tool in assessing the similar role of β in ETD(λ, β).",6.1. Statistical Interpretation of TD(λ),[0],[0]
"As was shown by Konidaris et al. (2011), we can use statedependent weights instead of β exponents to obtain better estimates.",6.2. Variance Weighted Γnt,[0],[0]
"The second moments are given explicitly as
follows3: E [ (Γnt ) 2 |st ] = d>µ P̃ n−1est dµ(st) , where [ P̃ ] s,s′
=∑ a∈A π2(a|s) µ(a|s) P (s ′|s, a).
",6.2. Variance Weighted Γnt,[0],[0]
These can be estimated for each state separately.,6.2. Variance Weighted Γnt,[0],[0]
"Notice that the variances increase exponentially depending on the largest eigenvalue of P̃ (as Assumption 4 dictates), but this is merely an asymptotic behavior and may be relevant only when the weights are already negligible.",6.2. Variance Weighted Γnt,[0],[0]
"Hence, implementing this solution on-line should not be a problem with the varying weights, as generally only the first few of these are non-zero.",6.2. Variance Weighted Γnt,[0],[0]
While this solution is impractical in problems with large state spaces parameterizing or approximating these variances (similarly to Thomas et al. (2015b)) could improve performance in specific applications.,6.2. Variance Weighted Γnt,[0],[0]
"Assumption 3 in the previous section is that the sampled estimators (R(n),Γnt ) are normally distributed.","6.3. Log-COP-TD(λ, β)",[0],[0]
"For on policy TD(λ), this assumption might seem not too harsh as the estimators R(n) represent growing sums of random variables.","6.3. Log-COP-TD(λ, β)",[0],[0]
"However, in our case the estimators Γnt are growing products of random variables.","6.3. Log-COP-TD(λ, β)",[0],[0]
"To correct this issue we can define new estimators using a logarithm on each Γ̃nt :
log [ρd(st)]","6.3. Log-COP-TD(λ, β)",[0],[0]
"= log
[ E [ ρ̂d(st−m)
t−1∏ k=t−m
ρk ∣∣ st]]
≈ log [ρ̂d(st−m)]","6.3. Log-COP-TD(λ, β)",[0],[0]
"+ t−1∑
k=t−m
E","6.3. Log-COP-TD(λ, β)",[0],[0]
"[log [ρk] |st] .
(5)
","6.3. Log-COP-TD(λ, β)",[0],[0]
"This approximation is crude – we could add terms reducing the error through Taylor expansion, but these would be complicated to deal with.","6.3. Log-COP-TD(λ, β)",[0],[0]
"Hence, we can relate to this method mainly as a well-motivated heuristic.
","6.3. Log-COP-TD(λ, β)",[0],[0]
"Notice that this formulation resembles the standard MDP formulation, only with the corresponding ”reward” terms log[ρt] going backward instead of forward, and no discount factor.","6.3. Log-COP-TD(λ, β)",[0],[0]
"Unfortunately, without a discount factor we
3The covariances can be expressed analytically as well, for clarity we drop this immediate result.
cannot expect the estimated value to converge, so we propose using an artificial one γlog.","6.3. Log-COP-TD(λ, β)",[0],[0]
We can incorporate function approximation for this formulation as well.,"6.3. Log-COP-TD(λ, β)",[0],[0]
"Unlike COP-TD(λ, β), we can choose the features and weights as we wish with no restriction, besides the linear constraint on the resulting ρd through the weight vector θρ.","6.3. Log-COP-TD(λ, β)",[0],[0]
This can be approximately enforced by normalizing θρ using X t .,"6.3. Log-COP-TD(λ, β)",[0],[0]
= 1t ∑ t exp(θ,"6.3. Log-COP-TD(λ, β)",[0],[0]
>,"6.3. Log-COP-TD(λ, β)",[0],[0]
"ρ,tφ(st))","6.3. Log-COP-TD(λ, β)",[0],[0]
(which should equal 1 if we were exactly correct).,"6.3. Log-COP-TD(λ, β)",[0],[0]
"We call the resulting algorithm LogCOP-TD(λ,β).
","6.3. Log-COP-TD(λ, β)",[0],[0]
"Algorithm 3 Log-COP-TD(λ,β) with Function Approximation, Input: θ0,θρ,0
1: Init: F0 = 0, n0(β) = 1, N(s) = 0 2: for t = 1, 2, ... do 3: Observe st, at, rt, st+1 4: Update normalization terms: 5: nβt = βn β t + 1, Nφ = γlog(βNφ +
φρ(st)), X = X + exp(θ","6.3. Log-COP-TD(λ, β)",[0],[0]
>,"6.3. Log-COP-TD(λ, β)",[0],[0]
"ρ,tφ(st))
","6.3. Log-COP-TD(λ, β)",[0],[0]
6: Update log(Γnt )’s weighted average: 7: Ft = βγlogFt−1 + n,"6.3. Log-COP-TD(λ, β)",[0],[0]
β,"6.3. Log-COP-TD(λ, β)",[0],[0]
"t log[ρ(st−1)] 8: Update & project by log(ρd)’s TD error: 9: δdt =
Ft nβt + θ>ρ,t
( Nφ
nβt − φρ(st) )","6.3. Log-COP-TD(λ, β)",[0],[0]
"10: θρ,t+1 = θρ,t + αdt δ d t φρ(st) 11:","6.3. Log-COP-TD(λ, β)",[0],[0]
"Off-policy TD(λ): 12: Mt = λ+ (1− λ) exp ( θ>ρ,t+1φρ(st) )","6.3. Log-COP-TD(λ, β)",[0],[0]
/(X/t),"6.3. Log-COP-TD(λ, β)",[0],[0]
15: θt+1 = θt + αtδtet 16: end for,14: δt = rt + θ>t (γφ(st+1)− φ(st)),[0],[0]
"An interesting phenomenon occurs when the behavior and target policies employ a feature based Boltzmann distribution for choosing the actions: µ(a|s) = exp ( θ>a,µφ(s) ) ,
and π(a|s) = exp ( θ>a,πφ(s) ) , where a constant feature is added to remove the (possibly different) normalizing constant.",6.4. Using the Original Features,[0],[0]
"Thus, log(ρt) = (θa,π − θa,µ)>φ(st), and LogCOP-TD(λ,β) obtains a parametric form that depends on the original features instead of a different set.",6.4. Using the Original Features,[0],[0]
"As we propose to use linear function approximation for ρd(s) and log (ρd(s)) one cannot help but wonder how hard it is to approximate these quantities, especially compared to the value function.",6.5. Approximation Hardness,[0],[0]
"The comparison between V (s) and ρd(s) is problematic for several reasons:
1.",6.5. Approximation Hardness,[0],[0]
"The ultimate goal is estimating V π(s), approximation errors in ρd(s) are second order terms.
",6.5. Approximation Hardness,[0],[0]
2.,6.5. Approximation Hardness,[0],[0]
"The value function V π(s) depends on the policy-
induced reward function and transition probability matrix, while ρd(s) depends on the stationary distributions induced by both policies.",6.5. Approximation Hardness,[0],[0]
Since each depends on at least one distinct factor - we can expect different setups to result in varied approximation hardness.,6.5. Approximation Hardness,[0],[0]
"For example, if the reward function has a poor approximation then so will V π(s), while extremely different behavior and target policies can cause ρd(s) to behave erratically.
3.",6.5. Approximation Hardness,[0],[0]
"Subsequently, the choice of features for approximating V π(s) and ρd(s) can differ significantly depending on the problem at hand.
",6.5. Approximation Hardness,[0],[0]
"If we would still like to compare V π(s) and ρd(s), we could think of extreme examples:
• When π = µ, ρd(s) ≡ 1, when R(s) ≡ 0",6.5. Approximation Hardness,[0],[0]
"then V π(s) ≡ 0.
",6.5. Approximation Hardness,[0],[0]
• In the chain MDP example in Section 4 we saw that ρd(s) is an exponential function of the location in the chain.,6.5. Approximation Hardness,[0],[0]
Setting reward in one end to 1 will result in an exponential form for V π(s) as well.,6.5. Approximation Hardness,[0],[0]
"Subsequently, in the chain MDP example approximating log (ρd(s)) is easier than ρd(s) as we obtain a linear function of the position; This is not the general case.",6.5. Approximation Hardness,[0],[0]
We have performed 3 types of experiments.,7. Experiments,[0],[0]
"Our first batch of experiments (Figure 1) demonstrates the accuracy of predicting ρd by both COP-TD(λ, β) and Log-COP-TD(λ, β).",7. Experiments,[0],[0]
"We show two types of setups in which visualization of ρd is relatively clear - the chain MDP example mentioned in Section 4 and the mountain car domain (Sutton and Barto, 1998) in which the state is determined by only two continuous variables - the car’s position and speed.",7. Experiments,[0],[0]
"The parameters λ and β exhibited low sensitivity in these tasks so they were simply set to 0, we show the estimated ρd after 106 iterations.",7. Experiments,[0],[0]
"For the chain MDP (top two plots, notice the logarithmic scale) we first approximate ρd without any function approximation (top-left) and we can see COP-TD manages to converge to the correct value while Log-COPTD is much less exact.",7. Experiments,[0],[0]
When we use linear feature space (constant parameter and position),7. Experiments,[0],[0]
Log-COP-TD captures the true behavior of ρd much better as expected.,7. Experiments,[0],[0]
The two lower plots show the error (in color) in ρd estimated for the mountain car with a pure exploration behavior policy vs. a target policy oriented at moving right.,7. Experiments,[0],[0]
The z-axis is the same for both plots and it describes a much more accurate estimate of ρd obtained through simulations.,7. Experiments,[0],[0]
The features used were local state aggregation.,7. Experiments,[0],[0]
"We can see that both algorithms succeed similarly on the position-speed pairs which are sampled often due to the behavior policy and the
mountain.",7. Experiments,[0],[0]
"When looking at more rarely observed states, the estimate becomes worse for both algorithms, though Log-COP-TD seems to be better performing on the spike at position > 0.
",7. Experiments,[0],[0]
"Next we test the sensitivity of COP-TD(λ, β) and LogCOP-TD(λ,β) to the parameters β and γlog (Figure 2) on two distinct toy examples - the chain MDP introduced before but with only 30 states with the position-linear features, and a random MDP with 32 states, 2 actions and a 5-bit binary feature vector along with a free parameter (this compact representation was suggested by White and White (2016) to approximate real world problems).",7. Experiments,[0],[0]
"The policies on the chain MDP were taken as described before, and on the random MDP a state independent 0.75/0.25 probability to choose an action by the behavior/target policy.",7. Experiments,[0],[0]
"As we can see, larger values of β cause noisier estimations in the random MDP for COP-TD(λ, β), but has little effect in other venues.",7. Experiments,[0],[0]
"As for γlog - we can see that if it is too large or too small the error behaves sub-optimally, as expected for the crude approximation of Equation 5.",7. Experiments,[0],[0]
"In conclusion, unlike ETD(λ, β), Log/COP-TD(λ, β) are much less effected by β, though γlog should be tuned to improve results.
",7. Experiments,[0],[0]
"Our final experiment (Figure 3) compares our algorithms to ETD(λ, β) and GTD(λ, β) over 4 setups: chain MDP with 100 states with right half rewards 1 with linear features, a 2 action random MDP with 256 states and binary features, acrobot (3 actions) and cart-pole balancing (21 actions) (Sutton and Barto, 1998) with reset at success and state aggregation to 100 states.",7. Experiments,[0],[0]
"In all problems we used the same features for ρd and V π(s) estimation, γ = 0.99, constant step size 0.05 for the TD process and results were averaged over 10 trajectories, other parameters (λ, β, other step sizes, γlog) were swiped over to find the best ones.",7. Experiments,[0],[0]
"To
reduce figure clutter we have not included standard deviations though the noisy averages still reflect the variance in the process.",7. Experiments,[0],[0]
"Our method of comparison on the first 2 setups estimates the value function using the suggested algorithm, and finds the dπ weighted average of the error between V and the on-policy fixed point ΠπTVπ:
‖V̂ −ΠπTVπ‖2dπ = ∑ s dπ(s) [ (θ∗ − θ̂)>φ(s) ]2 ,
where θ∗ is the optimal θ obtained by on-policy TD using the target policy.",7. Experiments,[0],[0]
"On the latter continuous state problems we applied on-line TD on a different trajectory following the target policy, used the resulting θ value as ground truth and taken the sum of squared errors with respect to it.",7. Experiments,[0],[0]
The behavior and target policies for the chain MDP and random MDP are as specified before.,7. Experiments,[0],[0]
"For the acrobot problem the behavior policy is uniform over the 3 actions and the target policy chooses between these with probabilities ( 16 , 1 3 , 1 2 ).",7. Experiments,[0],[0]
"For the cart-pole the action space is divided to 21 actions from -1 to 1 equally, the behavior policy chooses among these uniformly while the target policy is 1.5 times more prone to choosing a positive action than a negative one.
",7. Experiments,[0],[0]
"The experiments show that COP-TD(λ, β) and Log-COPTD(λ, β) have comparable performance to ETD(λ, β) where at least one is better in every setup.",7. Experiments,[0],[0]
The advantage in the new algorithms is especially seen in the chain MDP corresponding to a large discrepancy between the stationary distribution of the behavior and target policy.,7. Experiments,[0],[0]
"GTD(λ) is consistently worse on the tested setups, this might be due to the large difference between the chosen behavior and target policies which affects GTD(λ) the most.",7. Experiments,[0],[0]
Research on off-policy evaluation has flourished in the last decade.,8. Conclusion,[0],[0]
"While a plethora of algorithms were suggested so far, ETD(λ, β) by Hallak et al. (2015) has perhaps the simplest formulation and theoretical properties.",8. Conclusion,[0],[0]
"Unfortunately, ETD(λ, β) does not converge to the same point achieved by on-line TD when linear function approximation is applied.
",8. Conclusion,[0],[0]
"We address this issue with COP-TD(λ,β) and proved it can achieve consistency when used with a correct set of features, or at least allow trading-off some of the bias by adding or removing features.",8. Conclusion,[0],[0]
"Despite requiring a new set of features and calibrating an additional update function, COP-TD(λ,β)’s performance does not depend as much on β as ETD(λ,β), and shows promising empirical results.
",8. Conclusion,[0],[0]
We offer a connection to the statistical interpretation of TD(λ) that motivates our entire formulation.,8. Conclusion,[0],[0]
This interpretation leads to two additional approaches: (a) weight the Γnt using estimated variances instead of β exponents and (b) approximating log[ρd] instead of ρd; both approaches deserve consideration when facing a real application.,8. Conclusion,[0],[0]
This Research was supported in part by the Israel Science Foundation (grant No. 920/12) and by the European Research Council under the European Union’s Seventh Framework Programme (FP/2007-2013)/ ERC Grant Agreement n.306638.,9. Acknowledgments,[0],[0]
The problem of on-line off-policy evaluation (OPE) has been actively studied in the last decade due to its importance both as a stand-alone problem and as a module in a policy improvement scheme.,abstractText,[0],[0]
"However, most Temporal Difference (TD) based solutions ignore the discrepancy between the stationary distribution of the behavior and target policies and its effect on the convergence limit when function approximation is applied.",abstractText,[0],[0]
"In this paper we propose the Consistent Off-Policy Temporal Difference (COP-TD(λ, β)) algorithm that addresses this issue and reduces this bias at some computational expense.",abstractText,[0],[0]
"We show that COP-TD(λ, β) can be designed to converge to the same value that would have been obtained by using on-policy TD(λ) with the target policy.",abstractText,[0],[0]
"Subsequently, the proposed scheme leads to a related and promising heuristic we call logCOP-TD(λ, β).",abstractText,[0],[0]
Both algorithms have favorable empirical results to the current state of the art online OPE algorithms.,abstractText,[0],[0]
"Finally, our formulation sheds some new light on the recently proposed Emphatic TD learning.",abstractText,[0],[0]
Consistent On-Line Off-Policy Evaluation,title,[0],[0]
