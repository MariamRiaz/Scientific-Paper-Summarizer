0	28	Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers.
1	53	To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annota- tions to target languages (Yarowsky et al., 2001; Agić et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; Täckström et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013).
2	15	However, only one or two compatible sources of distant supervision are typically employed.
3	44	In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint.
6	51	Our system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (DSDS).
7	18	We use it to combine: i) multi-source annotation projection, ii) instance selection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations.
8	81	We examine how far we can get by exploiting only the wide-coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.
10	15	We demonstrate: i) substantial gains in carefully selecting high-quality instances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii) the importance of word embeddings initialization for faster convergence.
11	14	DSDS is illustrated in Figure 1.
12	28	The base model is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016; Kiperwasser and Goldberg, 2016).
18	26	We apply the approach by Agić et al. (2016), where labels are projected from multiple sources and then decoded through weighted majority voting with word alignment probabilities and source POS tagger confidences.
23	21	We rank the target sentences by percentage of words covered by word alignment from 21 sources of Agić et al. (2016), and select the top k covered instances for training.
27	12	There are several ways to exploit such information: i) as type constraints during encoding (Täckström et al., 2013), ii) to guide unsupervised learning (Li et al., 2012), or iii) as additional signal at training.
42	11	– LI: Wiktionary supervision (Li et al., 2012).
53	18	For the learning curve, we average over 5 random samples with 3 runs each.
56	19	Most prior work on annotation projection resorts to arbitrary selection; informed selection clearly helps in this noisy data setup, as shown in Figure 2 (a).
57	41	Training on 5k instances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.
59	24	From now on we consider the 5k model trained with Polyglot as our baseline (Table 1, column “5k”), obtaining a mean accuracy of 83.0 over 21 languages.
60	12	Polyglot initialization offers a large boost; on average +3.8% absolute improvement in accuracy for our 5k training scheme, as shown in Figure 2 (b).
61	36	The big gap in low-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracy when training on only 500 instances.
63	27	Embedding Wiktionary tags reaches 83.7 accuracy on average, versus 83.4 for n-hot encoding, and 83.2 for type constraints.
64	11	Only on 4 out of 21 languages are type constraints better.
66	13	The best approach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, and resulting in our final model.
69	25	It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011), compared to their 83.4.
72	23	The inclusion of lexicons results in higher coverage and is part of the explanation for the improvement of DSDS; see correlation in Figure 3 (a).
73	23	What is more interesting is that our model benefits from the lexicon beyond its content: OOV accuracy for words not present in the lexicon overall improves, besides the expected improvement on known OOV, see Figure 3 (b).
91	12	Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC.
93	12	Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011), and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.
100	24	We show that our approach of distant supervision from disparate sources (DSDS) is simple yet surprisingly effective for low-resource POS tagging.
101	28	Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging performance.
