0	36	Open Information Extraction (Open IE) involves generating a structured representation of information in text, usually in the form of triples or n-ary propositions.
1	9	An Open IE system not only extracts arguments but also relation phrases from the given text, which does not rely on pre-defined ontology schema.
2	11	For instance, given the sentence “deep learning is a subfield of machine learning”, the triple (deep learning; is a subfield of ; machine learning) can be extracted, where the relation phrase “is a subfield of ” indicates the semantic relationship between two arguments.
3	20	Open IE plays a key role in natural language understanding and fosters many downstream NLP applications such as knowledge base construction, question answering, text comprehension, and others.
4	11	The Open IE system was first introduced by TEXTRUNNER (Banko et al., 2007), followed by several popular systems such as REVERB (Fader et al., 2011), OLLIE (Mausam et al., 2012), ClausIE (Del Corro and Gemulla, 2013) Stanford OPENIE (Angeli et al., 2015), PropS (Stanovsky et al., 2016) and most recently OPENIE41 (Mausam, 2016) and OPENIE52.
5	13	Although these systems have been widely used in a variety of applications, most of them were built on hand-crafted patterns from syntactic parsing, which causes errors in propagation and compounding at each stage (Banko et al., 2007; Gashteovski et al., 2017; Schneider et al., 2017).
7	11	To this end, we propose a neural Open IE approach with an encoder-decoder framework.
15	11	The contributions of this paper are threefold.
20	14	The conditional probability of P (Y |X) can be decomposed as: P (Y |X) = P (Y |x1, x2, ..., xm) = n∏ i=1 p(yi|y1, y2, ..., yi−1;x1, x2, ...xm) (1) In this work, we only consider the binary extractions from sentences, leaving n-ary extractions and nested extractions for future research.
24	10	In this work, both the encoder and decoder are implemented using Recurrent Neural Networks (RNN) and the model architecture is shown in Figure 1.
25	14	The encoder uses a 3-layer stacked Long ShortTerm Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network to covert the input sequence X = (x1, x2, ...xm) into a set of hidden representations h = (h1, h2, ..., hm), where each hidden state is obtained iteratively as follows: ht = LSTM(xt, ht−1) (2) The decoder also uses a 3-layer LSTM network to accept the encoder’s output and generate a variable-length sequence Y as follows: st = LSTM(yt−1, st−1, c) p(yt) = softmax(yt−1, st, c) (3) where st is the hidden state of the decoder LSTM at time t, c is the context vector that is introduced later.
35	10	For the training data, we used Wikipedia dump 201801013 and extracted all the sentences that are 40 words or less.
36	10	OPENIE4 is used to analyze the sentences and extract all the tuples with binary relations.
37	11	To further obtain highquality tuples, we only kept the tuples whose confidence score is at least 0.9.
38	13	Finally, there are a total of 36,247,584 〈sentence, tuple〉 pairs extracted.
40	21	For the test data, we used a large benchmark dataset (Stanovsky and Dagan, 2016) that contains 3,200 sentences with 10,359 extractions4.
44	17	We used 4 M60 GPUs for parallel training, which takes 3 days.
49	9	We trained the model for 40 epochs and started learning rate decay from the 11th epoch with a decay rate 0.7.
50	10	The dropout rate is set to 0.3.
51	13	We split the data into 20 partitions and used data sampling in OpenNMT to train the model.
53	12	We used the script in (Stanovsky and Dagan, 2016)5 to evaluate the precision and recall of different baseline systems as well as the neural Open IE system.
65	12	For the baseline systems, we obtained the Open IE extractions using a Xeon 2.4 GHz CPU.
68	10	The results are shown in Table 1.
70	53	By using GPU, the neural approach takes 172s to extract the tuples from the test data, which is comparable with conventional approaches.
71	29	As the neural approach does not depend on other NLP tools, we can further optimize the computational cost in future research efforts.
92	22	We proposed a neural Open IE approach using an encoder-decoder framework.
93	128	The neural Open IE model is trained with highly confident binary extractions bootstrapped from a state-of-the-art Open IE system, therefore it can generate highquality tuples without any hand-crafted patterns from other NLP tools.
94	56	Experiments show that our approach achieves very promising results on a large benchmark dataset.
95	133	For future research, we will further investigate how to generate more complex tuples such as nary extractions and nested extractions with the neural approach.
96	127	Moreover, other frameworks such as convolutional sequence-to-sequence and transformer models could apply to achieve better performance.
