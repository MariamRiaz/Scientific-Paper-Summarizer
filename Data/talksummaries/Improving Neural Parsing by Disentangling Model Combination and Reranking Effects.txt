13	117	In this paper, we present experiments to isolate the degree to which each gain occurs for each of two state-of-the-art generative neural parsing models: the Recurrent Neural Network Grammar generative parser (RG) of Dyer et al. (2016), and the LSTM language modeling generative parser (LM) of Choe and Charniak (2016).
14	63	In particular, we present and use a beam-based search procedure with an augmented state space that can search directly in the generative models, allowing us to explore A→ A for these generative parsers A independent of any base parsers.
15	261	Our findings suggest the presence of model combination effects in both generative parsers: when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser (the RNNG discriminative parser, RD (Dyer et al., 2016)), performance decreases when compared to using just candidates from the base parser, i.e., B ∪ A→ A has lower evaluation performance than B→ A (Section 3.1).
16	53	This result suggests that both generative models benefit from fortuitous search errors in the rescoring setting – there are trees with higher probability 161 under the generative model than any tree proposed by the base parser, but which would decrease evaluation performance if selected.
17	31	Because of this, we hypothesize that model combination effects between the base and generative models are partially responsible for the high performance of the generative reranking systems, rather than the generative model being generally superior.
18	34	Here we consider our second question: if crossscoring gains are at least partly due to implicit model combination, can we gain even more by combining the models explicitly?
19	295	We find that this is indeed the case: simply taking a weighted average of the scores of both models when selecting a parse from the base parser’s candidate list improves over using only the score of the generative model, in many cases substantially (Section 3.2).
20	7	Using this technique, in combination with ensembling, we obtain new state-of-the-art results on the Penn Treebank: 94.25 F1 when training only on gold parse trees and 94.66 F1 when using external silver data.
21	36	All of the parsers we investigate in this work (the discriminative parser RD, and the two generative parsers RG and LM, see Section 1) produce parse trees in a depth-first, left-to-right traversal, using the same basic actions: NT(X ), which opens a new constituent with the non-terminal symbol X; SHIFT / GEN(w), which adds a word; and REDUCE, which closes the current constituent.
22	48	We refer to Dyer et al. (2016) for a complete description of these actions, and the constraints on them necessary to ensure valid parse trees.1 The primary difference between the actions in the discriminative and generative models is that, whereas the discriminative model uses a SHIFT action which is fixed to produce the next word in the sentence, the generative models use GEN(w) to define a distribution over all possible words w in the lexicon.
23	10	This stems from the generative model’s definition of a joint probability p(x, y) over all possible sentences x and parses y.
24	98	To use a generative model as a parser, we are interested in finding the maximum probability parse for a given sentence.
25	2	This is made more complicated by not having an explicit representation for p(y|x), as we do in the discriminative setting.
26	6	However, we can start by applying similar approximate search procedures as are used for the discriminative parser, constraining the set of actions such that it is only possible to produce the observed sentence: i.e. only allow a GEN(w) action when w is the next terminal in the sentence, and prohibit GEN actions if all terminals have been produced.
27	116	Past work on discriminative neural constituency parsers has shown the effectiveness of beam search with a small beam (Vinyals et al., 2015) or even greedy search, as in the case of RD (Dyer et al., 2016).
28	26	The standard beam search procedure, which we refer to as action-synchronous, maintains a beam of K partially-completed parses that all have the same number of actions taken.
31	6	Unfortunately, we find that action-synchronous beam search breaks down for both generative models we explore in this work, failing to find parses that are high scoring under the model.
32	20	This stems from the probabilities of the actions NT(X ) for all labels X almost always being greater than the probability of GEN(w) for the particular word w which must be produced next in a given sentence.
33	27	Qualitatively, the search procedure prefers to open constituents repeatedly up until the maximum number allowed by the model.
34	24	While these long chains of non-terminals will usually have lower probability than the correct sequence at the point where they finally generate the next word, they often have higher probability up until the word is generated, and so they tend to push the correct sequence off the beam before this point is reached.
35	39	This search failure produces very low evaluation performance: with a beam of size K = 100, action-synchronous beam search achieves 29.1 F1 for RG and 27.4 F1 for LM on the development set.
36	5	To deal with this issue, we force partial parse candidates to compete with each other on a wordby-word level, rather than solely on the level of individual actions.
37	37	The word-synchronous beam search we apply is very similar to approximate decoding procedures developed for other generative models (Henderson, 2003; Titov and Henderson, 2010; Buys and Blunsom, 2015) and can be viewed as a simplified version of the procedure used in the generative top-down parsers of Roark (2001) and Charniak (2010).
38	33	In word-synchronous search, we augment the beam state space, identifying beams by tuples (|W |, |Aw|), where |W | is the number of words that have been produced so far in the sentence, and |Aw| is the number of structural actions that have been taken since the last word was produced.
