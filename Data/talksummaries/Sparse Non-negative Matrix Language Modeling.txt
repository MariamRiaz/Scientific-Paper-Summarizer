11	60	recurrent neural network language models (Mikolov, 2012).
12	69	Unfortunately, either method comes at a large computational cost which makes training and evaluation on a large corpus impractical.
13	28	In this paper we present a novel probability estimation technique, called Sparse Non-negative Matrix (SNM) estimation.
44	10	Contrary to neural networks and ME, SNM language models do not estimate P (wk|Φ(wk−11 )) in a loglinear fashion, but are in fact linear models: P (wk|Φ(wk−11 )) = ŷwk∑ t′∈V ŷt′ (6) where ŷ is defined as in Eq.
45	129	Like ME however, SNM uses features F that are predefined and arbitrary, e.g. n-grams, skip-grams, bags of words, syntactic features, ...
46	29	The features are extracted from the left context of wk and stored in a feature activation vector x = Φ(wk−11 ), which is binary-valued, i.e. xf represents the presence or absence of the feature with index f .
56	36	For example, for SNM models using variable-length n-gram features, the maximum number of active features is n; in our experiments with a large variety of skip-grams, it was around 100.
65	24	Given the word sequence the quick brown fox, we extract the following elementary metafeatures from the 3-gram feature the quick brown and the target fox: • feature identity: [the quick brown] • feature type: 3-gram • feature count: Cf∗ • target identity: fox • feature-target count: Cft We also allow conjunctions of (single or multiple) elementary meta-features to form more complex meta-features.
105	54	One practical way to further prevent overfitting and adapt the model to a specific task is to use heldout data, i.e. compute the count matrix C on the training data and estimate the parameters θ on the held-out data.
116	15	Several attempts have been made to reduce training time, focusing mostly on reducing the large factors T or V : • vocabulary shortlisting (Schwenk and Gauvain, 2004) • subsampling (Schwenk and Gauvain, 2005; Xu et al., 2011) • class-based (Goodman, 2001b; Morin and Ben- gio, 2005; Mikolov et al., 2011) • noise-contrastive estimation (Gutmann and Hyvärinen, 2012; Chen et al., 2015) However, these techniques either come with a serious performance degradation (Le et al., 2013) or do not sufficiently speed up training.
119	14	The same applies to noisecontrastive estimation.
134	64	In the first set of experiments, we used all variablelength n-gram features that appeared at least once in the training data up to a given length.
135	12	This yields at most n active features: one for each m-gram of length 0 ≤ m < n where m = 0 corresponds to an empty feature which is always present and produces the unigram distribution.
136	92	The number of features is smaller than n when the context is shorter than n−1 words (near sentence boundaries) and during evaluation where an n-gram that did not occur in the training data is discarded.
137	17	When trained using these features, SNMLMs come very close to n-gram models with interpolated Kneser-Ney (KN) smoothing (Kneser and Ney, 1995), where no count cut-off was applied and the discount does not change with the order of the model.
141	18	To incorporate skip-gram features, we can either build a ‘pure’ skip-gram SNMLM that contains no regular n-gram features (except for unigrams) and interpolate this model with KN, or we can build a single SNMLM that has both the regular n-gram features and the skip-gram features.
145	29	As can be seen from Table 2, it is better to incorporate all features into one single SNM model than to interpolate with a KN 5-gram model (KN5).
148	47	The best SNMLM results so far (SNM10-skip) were achieved using 10-grams, together with skip-grams defined by the following feature extractors: • s = 1; 1 ≤ r + a ≤ 5 • r = 1; 1 ≤ s ≤ 10 (tied); 1 ≤ r + a ≤ 4 This mixture of rich (large context) short-distance and shallow long-distance features enables the model to achieve state-of-the-art results.
149	63	Table 3 compares its perplexity to KN5 as well as to the following language models: • Stupid Backoff LM (SBO) (Brants et al., 2007) • Hierarchical Softmax Maximum Entropy LM (HSME) (Goodman, 2001b; Morin and Bengio, 2005) • Recurrent Neural Network LM with Maximum Entropy (RNNME) (Mikolov, 2012) Describing these models however is beyond the scope of this paper.
150	64	Instead we refer the reader to Chelba et al. (2014) for a detailed description.
151	55	The table also lists the number of model parameters, which in the case of SNMLMs consist of the non-zero entries and precomputed row sums of M. When we compare the perplexity of SNM10-skip with the state-of-the-art RNNLM with 1024 hidden neurons (RNNME-1024), the difference is only 3%.
154	147	As far as we know, the resulting perplexity of 41.3 is already the best ever reported on this corpus, beating the optimized combination of several models, reported in Chelba et al. (2014) by 6%.
155	12	Finally, interpolation over all models shows that the contribution of other models as well as the additional perplexity reduction of 0.3 is negligible.
157	60	More specifically, we compare the training runtime (in machine hours) of the best SNM model to the best RNN and n-gram models: • KN5: 28 machine hours • SNM5: 115 machine hours • SNM10-skip: 487 machine hours • RNNME-1024: 5760 machine hours As these models were trained using different architectures (number of CPUs, type of distributed computing, etc.
160	28	Moreover, the large difference between KN5 and SNM5 suggests that our vanilla implementation can be further improved to achieve even larger speed-ups.
161	32	In addition to the experiments on the One Billion Word Benchmark, we also conducted experiments on a small subset of the LDC English Gigaword corpus.
163	71	The corpus is the one used in Tan et al. (2012), which we acquired with the help of the authors and is now available at http://www.esat.
164	13	kuleuven.be/psi/spraak/downloads/4.
167	29	OOV words are mapped to an <UNK> token.
172	39	With regards to n-gram modeling, the results are analogous to the 1B word experiment: SNM5 is close to KN5; both outperform Katz5 by a large margin.
