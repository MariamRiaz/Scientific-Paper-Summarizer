2	23	This allows to learn high-quality cross-lingual representations without expensive supervision, opening new research avenues like unsupervised neural machine translation (Artetxe et al., 2018b; Lample et al., 2018).
3	28	While most embedding mapping methods rely on a small seed dictionary, adversarial training has recently produced exciting results in fully unsu- pervised settings (Zhang et al., 2017a,b; Conneau et al., 2018).
4	33	However, their evaluation has focused on particularly favorable conditions, limited to closely-related languages or comparable Wikipedia corpora.
9	20	In this paper, we follow this second approach and propose a new unsupervised method to build an initial solution without the need of a seed dictionary, based on the observation that, given the similarity matrix of all words in the vocabulary, each word has a different distribution of similarity values.
10	118	Two equivalent words in different languages should have a similar distribution, and we can use this fact to induce the initial set of word pairings (see Figure 1).
35	21	Our proposed method consists of four sequential steps: a pre-processing that normalizes the embeddings (§3.1), a fully unsupervised initialization scheme that creates an initial solution (§3.2), a robust self-learning procedure that iteratively improves this solution (§3.3), and a final refinement step that further improves the resulting mapping through symmetric re-weighting (§3.4).
40	36	In order to overcome this challenge and build an initial solution, we propose to first construct two alternative representations X ′ and Z ′ that are aligned across their jth dimension X ′∗j and Z ′ ∗j , which can later be used to build an initial dictionary that aligns their respective vocabularies.
42	48	More concretely, assuming that the embedding spaces are perfectly isometric, the similarity matrices MX and MZ would be equivalent up to a permutation of their rows and columns, where the permutation in question defines the dictionary across both languages.
58	34	The underlying optimization objective is independent from the initial dictionary, and the algorithm is guaranteed to converge to a local optimum of it.
59	48	However, the method does not work if starting from a completely random solution, as it tends to get stuck in poor local optima in that case.
62	41	For that reason, we next propose some key improvements in the dictionary induction step to make self-learning more robust and learn better mappings: • Stochastic dictionary induction.
63	20	In order to encourage a wider exploration of the search space, we make the dictionary induction stochastic by randomly keeping some elements in the similarity matrix with probability p and setting the remaining ones to 0.
65	29	So as to find a fine-grained solution once the algorithm gets into a good region, we increase this value during training akin to simulated annealing, starting with p = 0.1 and doubling this value every time the objective function at step 1 above does not improve more than ǫ = 10−6 for 50 iterations.
71	74	This phenomenon is known to occur as an effect of the curse of dimensionality, and causes a few points (known as hubs) to be nearest neighbors of many other points (Radovanović et al., 2010a,b).
79	27	In order to mitigate this issue and encourage diversity, we propose inducing the dictionary in both directions and taking their corresponding concatenation, soD = DX→Z +DZ→X .
81	62	As the only difference, this first solution does not use the stochastic zeroing in the similarity matrix, as there is no need to encourage diversity (X ′ and Z ′ are only used once), and the threshold for vocabulary cutoff is set to k = 4, 000, so X ′ and Z ′ can fit in memory.
85	24	However, re-weighting also accentuates the problem of local optima when incorporated into self-learning as, by increasing the relevance of dimensions that best match for the current solution, it discourages to explore other regions of the search space.
95	30	In order to get a wider picture of how our method compares to previous work in different conditions, including more challenging settings, we carry out our experiments in the widely used dataset of Dinu et al. (2015) and the subsequent extensions of Artetxe et al. (2017, 2018a), which together comprise English-Italian, English-German, English-Finnish and EnglishSpanish.
96	57	More concretely, the dataset consists of 300-dimensional CBOW embeddings trained on WacKy crawling corpora (English, Italian, German), Common Crawl (Finnish) and WMT News Crawl (Spanish).
100	20	For completeness, we also test our method in the Spanish-English, Italian-English and TurkishEnglish datasets of Zhang et al. (2017a), which consist of 50-dimensional CBOW embeddings trained on Wikipedia, as well as gold standard dictionaries4 from Open Multilingual WordNet (Spanish-English and Italian-English) and Google Translate (Turkish-English).
103	24	Together with it, we also test the methods of Zhang et al. (2017a) and Conneau et al. (2018) using the publicly available implementations from the authors5.
106	42	Given the instability of these methods, we perform 10 runs for each, and report the best and average accuracies, the number of successful runs (those with >5% accuracy) and the average runtime.
125	31	Table 3 shows the results of the proposed method in comparison to previous systems, including those with different degrees of supervision.
127	20	Despite being fully unsupervised, our method achieves the best results in all language pairs but one, even surpassing previous supervised approaches.
141	47	The results show that our method succeeds in all cases, providing the best results with respect to all previous work on unsupervised and supervised mappings.
142	34	The ablation analysis shows that our initial solution is instrumental for making self-learning work without supervision.
143	21	In order to make selflearning robust, we also added stochasticity to dictionary induction, used CSLS instead of nearest neighbor, and produced bidirectional dictionaries.
144	28	Results also improved using smaller in- termediate vocabularies and re-weighting the final solution.
145	41	Our implementation is available as an open source project at https://github.
146	219	In the future, we would like to extend the method from the bilingual to the multilingual scenario, and go beyond the word level by incorporating embeddings of longer phrases.
