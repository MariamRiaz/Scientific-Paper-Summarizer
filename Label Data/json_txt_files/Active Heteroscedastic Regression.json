{"sections": [{"heading": "1. Introduction", "text": "An active learner is given a model class \u0398, a large sample of unlabeled data drawn from an underlying distribution Px and access to a labeling oracle O which can provide a label for any of the unlabeled instances. The goal of the learner is to find a model \u03b8 \u2208 \u0398 that fits the data to a given accuracy while making as few label queries to the oracle as possible.\nThere has been a lot of theoretical literature on active learning, most of which has been in the context of binary classification in the PAC model (Balcan et al., 2009; Hanneke, 2007; Dasgupta et al., 2007; Beygelzimer et al., 2009; Awasthi et al., 2014; Zhang and Chaudhuri, 2014). For classification, the problem is known to be particularly difficult when there is no perfect classifier in the class that best fits the labeled data induced by the oracle\u2019s responses. Prior work in the PAC model has shown that the difficulty of the problem is alleviated when the \u201cnoise\u201d is more benign \u2013 for\nAuthors listed in the alphabetical order 1University of California, San Diego 2Microsoft Research, India. Correspondence to: Nagarajan Natarajan <t-nanata@microsoft.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nexample, when there is a ground truth classifier that induces a labeling and the oracle\u2019s responses are perturbed versions of these labels (Hanneke, 2007; Awasthi et al., 2014; Zhang and Chaudhuri, 2014; Awasthi et al., 2016) corrupted by certain kinds of noise. In particular, significant improvements in label complexity have been obtained under what is known as the Tsybakov noise conditions, which model the realistic case of noise that decreases as we move further from the decision boundary.\nThe case of active learning under regression however is significantly less well-understood. In particular, we only have a theoretical understanding of the two extreme cases \u2013 no noise (as in, no model mismatch) and arbitrary model mismatch. Chaudhuri et al. (2015) show that allowing the learner to actively select instances for labeling under regression with no model mismatch can only improve the convergence rates by a constant factor; moreover, in many natural cases, such as when the unlabeled data is drawn from a uniform Gaussian, there is no improvement. Sabato and Munos (2014) look at the other extreme case \u2013 when arbitrary model mismatch is allowed \u2013 and provide an algorithm that attempts to \u201clearn\u201d the locations of the mismatch through increasingly refined partitions of the space, and then learn a model accordingly. However if the model mismatch is allowed to be arbitrary, then this algorithm either requires an extremely refined partition leading to a very high running time, or a large number of labels. More recently, Anava and Mannor (2016) study an online learning approach for estimating heteroscedastic variances and provide general task-dependent regret bounds, but not exact parameter recovery gaurantees.\nIn this paper we take a step towards closing this gap in understanding by considering active regression under a realistic yet more benign \u201cnoise\u201d model \u2013 when the variance of the label noise depends linearly on the example x. Specifically, the oracle\u2019s response on an unlabeled example x is distributed as N (\u27e8x,\u03b2\u2217\u27e9,\u03c32x) with \u03c3x = \u27e8f\u2217, x\u27e9; here \u03b2\u2217 is the unknown vector of regression coefficients and f\u2217 is an unknown parameter vector. In classical statistics, this framework is called heteroscedastic regression, and is known to arise in econometric and medical applications.\nWhile the usual least squares estimator for heteroscedastic regression is still statistically consistent, we find that\neven in the passive learning case, optimal convergence rates for heteroscedastic regression are not known. We thus begin with a convergence analysis of heteroscedastic regression for passive learning when the distribution Px over the unlabeled examples is a spherical Gaussian (in d dimensions). We show that even in this very simple case, the usual least squares estimator is sub-optimal, even when the noise model f\u2217 is known to the learner. Instead, we propose a weighted least squares estimator, and show that its rate of convergence is O\u0303 ( \u2225f\u2217\u22252(1/n + d2/n2) ) when the noise model is known, and O\u0303 ( \u2225f\u2217\u22252(d/n) ) when it needs to be estimated from the data; here, n denotes the number of labeled examples used to obtain the estimator. The latter matches the convergence rates of the least squares estimator for the usual homoskedastic linear regression, where \u2225f\u2217\u22252 plays the role of the variance \u03c32.\nWe next turn to active heteroscedastic regression and propose a two-stage active estimator. We show that when the noise model is known, the convergence rate of active heteroscedastic regression is O\u0303 ( \u2225f\u2217\u22252(1/n+d2/n4) ) , a small improvement over passive. However, in the more realistic case where the noise model is unknown, the rates become O ( \u2225f\u2217\u22252(1/n+d2/n2) ) , which improves over the passive estimator by a factor of d. Our results extend to the case when the distribution Px over unlabeled examples is an arbitrary Gaussian with covariance matrix \u03a3 and the norm used is the \u03a3 norm. Our work illustrates that just like binary classification, even a partial knowledge of the nature of the model mismatch significantly helps the label complexity of active learning.\nOur work is just a first step towards a study of active maximum likelihood estimation under controlled yet realistic forms of noise. There are several avenues for future work. For simplicity, the convergence bounds we present relate to the case when the distribution Px is a Gaussian. An open problem is to combine our techniques with the techniques of (Chaudhuri et al., 2015) and establish convergence rates for general unlabeled distributions. Another interesting line of future work is to come up with other, realistic noise models that apply to maximum likelihood estimation problems such as regression and logistic regression, and determine when active learning can help under these noise models. Summary of our main results in this work is given in Table 1. We conclude the paper presenting simulations supporting our theoretical bounds as well as experiments on real-world data."}, {"heading": "2. Problem Setup and Preliminaries", "text": "Let x denote instances in Rd. Let Px denote a fixed unknown distribution over instances x. The response y is gen-\nerated according to the model: y = \u27e8\u03b2\u2217,x\u27e9 + \u03b7x, where \u03b7x denotes instance-dependent corruption, and \u03b2\u2217 is the ground-truth parameter. In this work, we consider the following heteroscedastic model:\n\u03b7x \u223c N (0,\u03c32(x)) , (1)\nwith a standard parametric model for heteroscedastic noise given by a linear model:\n\u03b7x \u223c N (0, \u27e8f\u2217,x\u27e92) , (2)\nfor some unknown f\u2217 \u0338= \u03b2\u2217. Each response is independently corrupted via (2). The goal is to recover \u03b2\u2217 using instances drawn from Px and their responses sampled from N (\u27e8\u03b2\u2217,x\u27e9, \u27e8f\u2217,x\u27e92). Remark 1. The noise \u03b7x can be sampled from any subGaussian distribution with E[\u03b7x] = 0 and bounded second moment E[\u03b72x] \u2264 \u03c32 (for some constant \u03c3). For simplicity, we will consider the Gaussian model (1).\nOur approach is based on maximum likelihood estimator (MLE) for regression. In the homoscedastic setting (i.e. \u03c3(x)2 = \u03c3 for all x in (1)), MLE is known to give minimax optimal rates1. The standard least squares estimator computed on n iid training instances (xi, yi), i = 1, 2, . . . , n is given by:\n\u03b2\u0302LS =\n( n\u2211\ni=1\nxix T i\n)\u22121 n\u2211\ni=1\nxiyi , (3)\nand is the solution to the minimization problem:\n\u03b2\u0302LS = argmin \u03b2\nn\u2211\ni=1\n(\u27e8\u03b2,xi\u27e9 \u2212 yi)2.\nIn the heteroscedastic setting, it is easy to show that the standard least squares estimator is consistent. Remark 2. Standard least squares estimator is consistent for the heteroscedastic noise model (2): Assuming xi \u2208 Rd, i = 1, 2, . . . , n are drawn iid from the standard multivariate Gaussian, we have the rate:\n\u2225\u03b2\u0302LS \u2212 \u03b2\u2217\u222522 = O ( \u2225f\u2217\u22252 d\nn\n) .\n1A notable exception is the Stein\u2019s estimator that may do better for high dimensional spaces (Stein et al., 1956)\nWhile the estimator (3) is consistent, it does not exploit the knowledge of the noise model, and does not give better rates even when the noise model f\u2217 is known exactly. We look at the maximum likelihood estimator for the heteroscedastic noise (1); which is given by the weighted least squares estimator (or sometimes referred to as generalized least squares estimator):\n\u03b2\u0302GLS =\n( n\u2211\ni=1\nwixix T i\n)\u22121 n\u2211\ni=1\nwixiyi, (4)\nwhere wi = 1\u03c3(xi)2 . When the weights are known, it has been shown that the weighted estimator is the \u201ccorrect\u201d estimator to study; in particular, it is the minimum variance unbiased linear estimator (Theorem 10.7, Greene (2002)). However, we do not know of strong learning rate guarantees for the weighted least squares model in general, or in particular for the model (2), compared to the ordinary least squares estimator. This raises two important questions for which we provide answers in the subsequent sections.\n1. What are the rates of convergence of the maximum likelihood estimator for the heteroscedastic model when the noise model, aka, f\u2217 is unknown?\n2. Can we achieve a better label requirement via active learning?\nThe problem is formally stated as follows. Given a set of m instances U = {x1,x2, . . . ,xm} sampled i.i.d. from the underlying Px, a label budget n \u2264 m, and access to label oracle O that generates responses yi according to the heteroscedastic noise model (2), we want an estimator \u03b2\u0302 of the regression model parameter \u03b2\u2217 such that the estimation error is small, i.e. \u2225\u03b2\u0302 \u2212 \u03b2\u2217\u22252 \u2264 O(\u03f5). Remark 3. Existing active regression methods (Sabato and Munos, 2014; Chaudhuri et al., 2015) do not consider the heteroscedastic noise model. Note that when f\u2217 is known exactly, one can reduce heteroscedastic model to a homoscedastic model, by scaling instances x and their responses by 1/\u27e8f\u2217,x\u27e9. However, we still may not be able to apply the existing active learning results to the transformed problem, as the modified data distribution may no longer satisfy required nice properties. The resulting estimators do not yield advantages over passive learning, even in simple cases such as when Px is spherical Gaussian.\nNotation. Id denotes the identity matrix of size d. We use bold letters to denote vectors and capital letters to denote matrices."}, {"heading": "3. Basic Idea: Noise Model is Known Exactly", "text": "To motivate our approach, we begin with the basic heteroscedastic setting: when f\u2217 is known exactly in (2). Even\nin this arguably simple setting, the rates for passive and active learning are a priori not clear, and the exercise turns out to be non-trivial. The results and the analysis here help gain insights into label complexities achievable via passive and active learning strategies.\nIn the standard (passive) learning setting, we sample n instances uniformly from the set U and compute the maximum likelihood estimator given in (4) with weights set to wi = 1/\u27e8f\u2217,xi\u27e92. The procedure is given in Algorithm 1. The resulting estimator is unbiased, i.e. E[\u03b2\u0302GLS|X] = \u03b2\u2217. Let W denote the diagonal weighting matrix with Wii = wi. The variance of the estimator is given by: Var(\u03b2\u0302GLS|X) = (XTWX)\u22121. The question of interest is if and when the weighted estimator \u03b2\u0302GLS is qualitatively better than the ordinary least squares estimator \u03b2\u0302LS. The following theorem shows that the variance of the latter, and in turn the estimation error, can be potentially much larger; and in particular, the difference between their estimation errors is at least a factor of dimensionality d. Theorem 1 (Passive Regression With Noise Oracle). Let \u03b2\u0302GLS denote the estimator in (4) (or the output of Algorithm 1) where xi \u223c N (0, Id) i.i.d., with label budget n \u2265 2d ln d and \u03b2\u0302LS denote the ordinary least squares estimator (3). There exist positive constants C \u2032 and c \u2265 1 such that, with probability at least 1 \u2212 dnc , both the statements hold:\n\u2225\u03b2\u0302GLS \u2212 \u03b2\u2217\u222522 \u2264 C \u2032\u2225f\u2217\u22252 ( 1\nn +\nd2 lnn\nn2\n) ,\n\u2225\u03b2\u0302LS \u2212 \u03b2\u2217\u222522 = \u0398 ( \u2225f\u2217\u22252 d\nn\n) .\nRemark 4. We present the results for instances sampled from N (0, Id) for clarity. The estimation error bounds can be naturally extended to the case of Gaussian distribution with arbitrary covariance matrix \u03a3. In this case, the bounds (in Theorem 1, for example) continue to hold for the estimation error measured w.r.t. \u03a3, i.e. (\u03b2\u0302 \u2212 \u03b2\u2217)T\u03a3(\u03b2\u0302 \u2212 \u03b2\u2217). Furthermore, with some calculations, we can obtain analogous bounds for sub-Gaussian distributions, with distribution-specific constants featuring in the resulting bounds. Remark 5. In Theorem 1, when n > d, d2/n2 term is the lower-order term, and thus, up to constants, the error of the weighted least squares estimator is at most \u2225f\u2217\u22252(1/n) while that of the ordinary least squares estimator is at least \u2225f\u2217\u22252(d/n). Thus, if the noise model is known, the weighted least squares estimator can give a factor of d improvement in convergence rate. Remark 6 (Technical challenges). The proofs of key results in this paper involve controlling quantities such as sum of ratios of Gaussian random variables, ratios of chi-squared\nrandom variables, etc. which do not even have expectation, let alone higher moments; so, standard concentration arguments cannot be made. However, in many of these cases, we can show that our error bounds hold with sufficiently high probability.\nThe following lemma is key to showing Theorem 1; the proof sketch illustrates some of the aforementioned technical challenges. Unlike typical results in this domain, which bound tr(A\u22121) by providing concentration bounds for A, we bound tr(A\u22121) by providing lower bound on each eigenvalue of A.\nLemma 1. Let X \u2208 Rn\u00d7d where the rows xi are sampled i.i.d. from N (0, Id). Assume n \u2265 2d ln d. Let W denote a diagonal matrix, with Wii = 1/\u27e8xi, f\u27e92, for fixed f \u2208 Rd. Let \u03c31 \u2265 \u03c32 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3d denote the eigenvalues of XTWX . Then, with probability at least (1\u2212 dnc ):\n1. \u03c3d(XTWX) \u2265 n\u2225f\u22252 and\n2. \u03c3i(XTWX) \u2265 C \u2032 n 2 d\u2225f\u22252 lnn for i = 1, 2, . . . , d\u2212 1,\nwhere c \u2265 1 and C \u2032 > 0 are constants.\nProof. We give a sketch of the proof here (See Appendix B.2 for details). To show a lower bound on the smallest eigenvalue, we first show that the smallest eigenvector is very close to f , with sufficiently large probability. To do so, we exploit the fact that the smallest eigenvalue is at most n/\u2225f\u22252 which can be readily seen. For the second part, we consider the variational characterization of d\u2212 1st singular value given by:\n\u03c3d\u22121(X TWX) = max\nU :dim(U)=d\u22121 min v\u2208U,\u2225v\u2225=1 vTXTWXv.\nWe look at the particular subspace that is orthogonal to f\u2217 to get the desired upper bound. One key challenge here is controlling quantity of the form \u2211n i=1 g2i h2i\nwhere gi and hi are iid Gaussian variables. We use a blocking technique based on the magnitude of \u27e8f,xi\u27e9, and lower bound the quantity with just the first block (as all the quantities involved are positive). This requires proving a bound on the order statistics of iid Gaussian random variables (Lemma 7 in Appendix A).\nTheorem 1 shows that weighting \u201cclean\u201d instances (i.e. \u27e8f\u2217,x\u27e9 \u2248 0) much more than \u201cnoisy\u201d instances yields a highly accurate estimator of \u03b2\u2217. But can we instead prefer querying labels on instances where we know a priori the response will be relatively noise-free? This motivates a simple active learning solution \u2014 in principle, if we actually know f\u2217, we could query the labels of instances with low noise, and hope to get an accurate estimator. The active\nlearning procedure is given in Algorithm 2. Besides label budget n, it takes another parameter \u03c4 as input, which is a threshold on the noise level.\nWe state the convergence for Algorithm 2 below:\nTheorem 2 (Active Regression with Noise Oracle). Consider the output estimator \u03b2\u0302 of Algorithm 2, with input label budget n \u2265 2d ln d, unlabeled set U with |U| = m and xi \u223c N (0, Id) i.i.d., and \u03c4 = 2n/m. Then, with probability at least 1\u2212 1/nc:\n\u2225\u03b2\u0302 \u2212 \u03b2\u2217\u222522 \u2264 C \u2032\u2225f\u2217\u22252 ( 1\nn +\nd2 lnn\nm2\n) ,\nfor some constants c > 1 and C \u2032 > 0.\nRemark 7. We observe that the estimation error via active learning (Theorem 2) goes to 1/n as the size of the unlabeled examples m becomes larger. Note that O(1/n) is the error for 1-dimensional problem and is much better than d2/n2 we get from uniform sampling.\nRemark 8. If we have m \u2265 n2 unlabeled samples, then we observe that active learning (Theorem 2) achieves a better convergence rate compared to that of passive learning (Theorem 1) \u2014 the lower order term in case of active learning is O( d 2\nn4 ) versus passive learning which is O( d2\nn2 ). The convergence is superior especially when n < d2 (as we also observe in simulations).\nThe proof of Theorem 2 relies on two lemmas stated below.\nLemma 2. Let X \u2208 Rn\u00d7d denote the design matrix whose rows xi are sampled from U such that they satisfy |\u27e8xi, f\u27e9| \u2264 \u2225f\u2225\u03c4 for fixed f \u2208 Rd. Assume n \u2265 2d ln d. Let \u03c31 \u2265 \u03c32 \u2265 . . .\u03c3d denote the eigenvalues of XTX . Then, with probability at least (1\u2212 1nc ):\n1. \u03c3d(XTX) \u2265 n\u03c42,\n2. \u03c3i(XTX) \u2265 12n, for i = 1, 2, . . . , d\u2212 1,\nfor some constants C > 0 and c \u2265 1. Lemma 3. For each xi \u2208 U , where |U| = m, define gi = \u27e8xi, z\u27e9, for any fixed z \u2208 Rd. Then, with probability at least exp(\u2212m\u03c4 3\n4\u2225z\u22252 ):\n\u2223\u2223\u2223\u2223 { i : |gi| \u2264 \u2225z\u2225\u03c4 }\u2223\u2223\u2223\u2223 \u2265 m\u03c4\n2 ."}, {"heading": "4. Estimating Noise: Algorithms and Guarantees", "text": "In this section, we will first show that we can obtain a consistent estimator of f\u2217, as long as we have a reasonably good estimate of \u03b2\u2217. Let \u03b20 denote the ordinary least\nAlgorithm 1 Passive Regression With Noise Oracle Input: Labeling oracle O, instances U = {xi, i \u2208 [m]}, label budget n, noise model f\u2217 1. Choose a subset L of size n from U uniformly at random from U . Query their labels using O. 2. Estimate \u03b2\u0302 using (4) on L, with wi = 1/\u27e8f\u2217,xi\u27e92. Output: \u03b2\u0302.\nAlgorithm 2 Active Regression With Noise Oracle Input: Labeling oracle O, noise model f\u2217, instances U = {xi, i \u2208 [m]}, label budget n, noise tolerance \u03c4 . 1. Choose a subset L of size at most n from U with expected noise up to the given tolerance \u03c4 , i.e. for all xi \u2208 L, |\u27e8xi, f\u2217\u27e9| \u2264 \u03c4 . Query their labels using O. 2. Estimate \u03b2\u0302 as \u03b2\u0302 = (XTWX)\u22121XTWy where X \u2208 Rn\u00d7d and y \u2208 Rn, and W is a diagonal matrix with Wii =\n1 \u27e8f\u2217,xi\u27e92 . Output: \u03b2\u0302.\nsquares estimator of \u03b2\u2217, obtained by using (3), on m1 labeled instances, chosen i.i.d. from N (0, Id). The largest eigenvector of the residual-weighted empirical covariance matrix given by:\nS\u0302 = 1\nm1\nm1\u2211\ni=1\n(yi \u2212 \u27e8xi, \u03b2\u03020\u27e9)2xixTi . (5)\ngives a sufficiently good estimate of f\u2217. This is established formally in the following lemma. Lemma 4. Let m1 = \u2126(d log3 d). Then, with probability at least 1 \u2212 1\nm51 , the largest eigenvector f\u0302 of S\u0302 in (5)\nconverges to f\u2217:\n\u2225f\u0302 \u2212 f\u2217\u222522 \u2264 C1E[\u2225\u03b20 \u2212 \u03b2\u2217\u222522] +O ( d\nm1\n) ,\nfor some positive constant C1, and expectation is wrt the randomness in the estimator \u03b20.\nWe first discuss the implications of using the estimated f\u0302 in order to obtain the generalized least square estimator given in (4) and then present the active learning solution."}, {"heading": "4.1. Weighted Least Squares", "text": "We now consider a simple (passive) learning algorithm for the setting where the noise model is estimated, based on the weighted least squares solution discussed in Section 3. We first get a good estimator of f\u2217 (as in Lemma 4), and then obtain the weighted least squares estimator: \u03b2\u0302 = (XT W\u0302X)\u22121XT W\u0302y, where W\u0302 is the diagonal matrix of inverse noise variances obtained using the estimate f\u0302 with a small additive offset \u03b3. The procedure is presented in Algorithm 3. Remark 9. Algorithm 3 can be thought of as a special case of the well-known iterative weighted least squares (i.e. with just one iteration), that has been studied in the past (Carroll et al., 1988).\nIt is well-known heuristic to first estimate the weights and then obtain the weighted estimator in practice; the approach has been widely in use for decades now in multiple communities including Econometrics and Bioinformatics (Harvey, 1976; Greene, 2002). However, we do not know of strong convergence rates for the solution. To our knowledge, the most comprehensive analysis was done by (Carroll et al., 1988). Their analysis is not directly applicable to us for reasons two-fold: (i) they focus on using a maximum likelihood estimate of the parameters in the heteroscedastic noise model, and does not apply to our noise model (2), and (ii) their analysis relies the noise being smooth (for obtaining tighter Taylor series approximation). More importantly, their analysis conceals a lot of significant factors in both d and n, and the resulting statements about convergence rates are not useful (See Appendix C).\nTheorem 3. Consider the output estimator \u03b2\u0302 of Algorithm 3, with label budget n \u2265 2d ln d and offset \u03b32 =\u221a\nd n . Then, with probability at least 1\u2212 1/n c:\n\u2225\u03b2\u0302 \u2212 \u03b2\u2217\u222522 \u2264 C\u2225f\u2217\u22252 d ln4 n\nn ,\nfor some constants C > 0 and c \u2265 1. Remark 10. Note that the above result holds for the specific choice of \u03b3. When \u03b3 = 0, we get the weighted least squares estimator analogous to the one used in Algorithm 1. However, when estimating weights with \u03b3 = 0, the resulting estimator \u03b2\u0302 has poor convergence rate. In particular, we observe empirically that the error \u2225\u03b2\u0302 \u2212 \u03b2\u2217\u22252 scales as O(d 3\nn )."}, {"heading": "4.2. Active Regression", "text": "We now show that active learning can help overcome the inadequacy of the passive weighted least squares solution. The proposed active regression algorithm, presented in Al-\ngorithm 4, consists of two stages. In the first stage, we obtain an estimate f\u0302 similar to that in Algorithm 3. Note that if f\u0302 is indeed very close to f\u2217, f\u0302 serves as a good proxy for selecting instances whose labels are nearly noise-free. To this end, we sample instances that have sufficiently small noise: |\u27e8f\u0302 ,x\u27e9| \u2264 \u03c4 , where \u03c4 is an input parameter to the algorithm. If f\u0302 is exact, then the algorithm reduces to the strategy outlined in Algorithm 2. Our algorithm follows the strategy of using a single-round of interaction (in light of the analysis presented in the passive learning setting) to achieve a good estimate of the underlying \u03b2\u2217 akin to the active MLE estimation algorithm studied by Chaudhuri et al. (2015). Lemma 5. Let f\u0302 denote an estimator of f\u2217 satisfying \u2225f\u0302 \u2212 f\u2217\u22252 \u2264 \u2206. For a given \u03c4 , let L denote a set of |L| \u2265 2d log d instances sampled from m unlabeled instances U , such that |\u27e8f\u0302 ,xi\u27e9| \u2264 \u03c4 , for all xi \u2208 L, and let yi denote their corresponding labels. Consider the ordinary least squares estimator obtained using L, i.e.:\n\u03b2\u0302 =\n( \u2211\nxi\u2208L xix\nT i\n)\u22121 \u2211\nxi\u2208L xiyi .\nThen, with probability at least 1\u2212 1nc :\n\u2225\u03b2\u0302 \u2212 \u03b2\u2217\u222522 \u2264 C\u2225f\u2217\u22252(\u03c42 +\u22062) ( 1\nm\u03c43 + d\u2212 1 m\u03c4\n) .\nfor some constants C > 0 and c \u2265 1. Remark 11. The bound in the above theorem recovers the known variance case discussed in Theorem 2, where the estimation error \u22062 = 0 and the choice of \u03c4 = 2nm .\nCompared to the passive learning error bound in Theorem 3, we hope to get leverage \u2014 as long we can choose \u03c4 sufficiently small, and yet guarantee that the number of samples m2 in Step 4 of Algorithm 4 is sufficiently large. The following theorem shows that this is indeed the case, and that the proposed active learning solution achieves optimal learning rate. Theorem 4 (Active Regression with Noise Estimation). Consider the output estimator \u03b2\u0302 of Algorithm 4, with input label budget n, unlabeled examples m \u2265 n2, m1 = n2 and \u03c4 = 2 \u221a d n . Then, we have, with probability at least 1\u2212 1/nc:\n\u2225\u03b2\u0302 \u2212 \u03b2\u2217\u222522 \u2264 C\u2225f\u2217\u22252 ( 1\nn +\nd2\nn2\n) .\nfor some constants C > 0 and c > 1. Remark 12. We observe that active learning (Theorem 4) has the same convergence rate for sufficiently large n, as that of the case when f\u2217 is known exactly (Theorem 2). Note that d2/n2 and d2/m2 are lower-order terms in the compared bounds.\nRemark 13. Unlike in the case when noise model was known (Theorem 2), here we can not do better even with infinite unlabeled examples. The source of trouble is the estimation error in f\u0302 , so beyond a point even active learning does not provide improvement. Note that we do not compute weighted least squares estimator in the final step of Algorithm 4 unlike in Algorithm 2, for the same reason."}, {"heading": "5. Simulations", "text": "We now present simulations that support the convergence bounds developed in this work. The setup is as follows. We sample unlabeled instances x1,x2, . . . ,xm from N (0, Id). Labels are generated according to the heteroscedastic model: yi = \u27e8\u03b2\u2217,xi\u27e9 + gi\u27e8f\u2217,xi\u27e9, where gi are iid standard Gaussian random variables. We fix \u2225f\u2217\u22252 = 1 and d = 10. We look at how the model estimation error (in case of Algorithms 1 and 2) \u2225\u03b2\u0302 \u2212 \u03b2\u2217\u2225 decays as a function of the label budget n (m = 2n2 for all the simulations). We also check the estimation error of the noise model in case of Algorithms 3 and 4.\nThe results for convergence of model estimation when the noise model is known are presented in Figure 1 (a)-(d). In passive learning, the bounds in Theorem 1 suggest that when n \u2264 d2, \u2225\u03b2\u2217 \u2212 \u03b2\u0302\u2225 = O( dn ); but once n > d\n2, we get a convergence of O(1/ \u221a n). We observe that the result in Figure 1 (a) closely matches the given bounds2. In case of active learning, the bounds in Theorem 2, for the case when m \u2265 n2, suggest that we get an error rate of \u2225\u03b2\u2217 \u2212 \u03b2\u0302\u2225 = O( dn2 ). We observe a similar phenomenon in the Figure 1 (b). Turning to the noise estimation setting for passive learning, we see in Figure 1 (c) that the estimation error of \u03b2\u2217 as well as f\u2217 decay as \u221a d/n (as suggested by Theorem 3); for active learning, we see in Figure 1 (d) that the estimation error of \u03b2\u2217 is noticeably better, in particular, better than that of f\u2217, and approaches 1/ \u221a n as n becomes larger than d2.\nWe also study the performance of the algorithms on two real-world datasets from UCI: (1) WINE QUALITY with m = 6500 and d = 11, and (2) MSD (a subset of the million song dataset) with m = 515345 and d = 90. For each dataset, we create a 70-30 train-test split, and learn the best linear regressor using ordinary least squares, which forms our \u03b2\u2217. We then sample labels using \u03b2\u2217 and a simulated heteroscedastic noise f\u2217. We compare active and passive learning algorithms on the root mean square error (RMSE) obtained on the test set. In Figure 1 (e), we see that active learning with noise estimation gives a significant reduction in RMSE early on for WINE QUALITY. We also see that weighted least squares gives slight benefit over or-\n2For better resolution, we plot \u2225\u03b2\u2217\u2212\u03b2\u0302\u2225 rather than \u2225\u03b2\u2217\u2212\u03b2\u0302\u22252 given in the theorem statements\ndinary least squares. On MSD dataset 3, again we observe that our active learning algorithm consistently achieves a marginal reduction in RMSE as the number of labeled examples increases."}, {"heading": "6. Conclusions and Future Work", "text": "In conclusion, we consider active regression under a heteroscedastic noise model. Previous work has looked at active regression either with no model mismatch (Chaudhuri et al., 2015) or arbitrary model mismatch (Sabato and Munos, 2014). In the first case, active learning provided no improvement even in the simple case where the unlabeled examples were drawn from Gaussians. In the second case, under arbitrary model mismatch, the algorithm either required a very high running time or a large number of labels. We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression. Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise has the potential to lead to significant gains in the label requirement of active learning.\nThere are several avenues for future work. For simplicity, the convergence bounds we present relate to the case when the distribution Px over unlabeled examples is a Gaussian. An open problem is to combine our techniques with the techniques of (Chaudhuri et al., 2015) and establish convergence rates for general unlabeled distributions. Another interesting line of future work is to come up with other, realistic noise models that apply to maximum likelihood estimation problems such as regression and logistic regression, and determine when active learning can help under these noise models.\n3here, the response variable is the year of the song; we make the response mean zero in our experiments\nAlgorithm 3 Least Squares with Estimated Weights Input: Labeling oracle O, unlabeled samples U = {xi, i \u2208 [m]}, label budget n, parameter m1, offset \u03b3. 1. Draw m1 examples uniformly at random from U and query their labels y using O. 2. Estimate \u03b2\u03020 by solving y \u2248 X\u03b2\u03020 where X \u2208 Rm1\u00d7d has xi as rows and y \u2208 Rm1 is the vector of labels. 3. Draw a subset L of n examples uniformly at random from U . Form X \u2208 Rn\u00d7d and y \u2208 Rn. 4. Compute f\u0302 as the largest eigenvector of the residual-weighted empirical covariance given in (5). 5. Set w\u0302i = 1\u27e8xi,f\u0302\u27e92+\u03b32 ., for xi \u2208 L.\n6. Estimate \u03b2\u0302 by solving: \u03b2\u0302 = (XT W\u0302X)\u22121XT W\u0302y, where W\u0302 is diagonal matrix with W\u0302ii = w\u0302i. Output: \u03b2\u0302.\nAlgorithm 4 Active Regression Input: Labeling oracle O, unlabeled samples U = {xi, i \u2208 [m]}, label budget n, parameters m1, \u03c4 . 1. Draw m1 examples uniformly at random from U and query their labels y using O. 2. Estimate \u03b2\u03020 by solving y \u2248 X\u03b2\u03020 where X \u2208 Rm1\u00d7d and y \u2208 Rm1 . 3. Compute f\u0302 as the largest eigenvector of S\u0302 given in (5). 4. Choose a subset L of m2 = n \u2212 m1 instances from U with estimated noise variance up to tolerance \u03c42, i.e. for all xi \u2208 L, |\u27e8xi, f\u0302\u27e9|2 \u2264 \u03c42. Query their labels using O. 5. Estimate \u03b2\u0302 as \u03b2\u0302 = (XTX)\u22121XTy where X \u2208 Rm2\u00d7d and y \u2208 Rm2 . Output: \u03b2\u0302."}], "year": 2017, "references": [{"title": "Heteroscedastic sequences: Beyond gaussianity", "authors": ["Oren Anava", "Shie Mannor"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "The power of localization for efficiently learning linear separators with noise", "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Philip M. Long"], "venue": "In Symposium on Theory of Computing,", "year": 2014}, {"title": "Learning and 1-bit compressed sensing under asymmetric noise", "authors": ["Pranjal Awasthi", "Maria-Florina Balcan", "Nika Haghtalab", "Hongyang Zhang"], "venue": "In Proceedings of the 29th Conference on Learning Theory, COLT 2016,", "year": 2016}, {"title": "Agnostic active learning", "authors": ["M.-F. Balcan", "A. Beygelzimer", "J. Langford"], "venue": "J. Comput. Syst. Sci.,", "year": 2009}, {"title": "Importance weighted active learning", "authors": ["A. Beygelzimer", "S. Dasgupta", "J. Langford"], "venue": "In ICML,", "year": 2009}, {"title": "The effect of estimating weights in weighted least squares", "authors": ["Raymond J Carroll", "CF Jeff Wu", "David Ruppert"], "venue": "Journal of the American Statistical Association,", "year": 1988}, {"title": "Convergence rates of active learning for maximum likelihood estimation", "authors": ["Kamalika Chaudhuri", "Sham M Kakade", "Praneeth Netrapalli", "Sujay Sanghavi"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "A general agnostic active learning algorithm", "authors": ["S. Dasgupta", "D. Hsu", "C. Monteleoni"], "venue": "In NIPS,", "year": 2007}, {"title": "On the minimum of several random variables", "authors": ["Yehoram Gordon", "Alexander Litvak", "Carsten Sch\u00fctt", "Elisabeth Werner"], "venue": "Proceedings of the American Mathematical Society,", "year": 2006}, {"title": "Econometric analysis", "authors": ["William H Greene"], "year": 2002}, {"title": "A bound on the label complexity of agnostic active learning", "authors": ["S. Hanneke"], "venue": "In ICML,", "year": 2007}, {"title": "Estimating regression models with multiplicative heteroscedasticity", "authors": ["Andrew C Harvey"], "venue": "Econometrica: Journal of the Econometric Society,", "year": 1976}, {"title": "Alternating minimization for regression problems with vector-valued outputs", "authors": ["Prateek Jain", "Ambuj Tewari"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Active regression by stratification", "authors": ["Sivan Sabato", "Remi Munos"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution", "authors": ["Charles Stein"], "venue": "In Proceedings of the Third Berkeley symposium on mathematical statistics and probability,", "year": 1956}, {"title": "Beyond disagreement-based agnostic active learning", "authors": ["Chicheng Zhang", "Kamalika Chaudhuri"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2014}], "id": "SP:593dde13e14a9230953e6297aa33d741b9097d40", "authors": [{"name": "Kamalika Chaudhuri", "affiliations": []}, {"name": "Prateek Jain", "affiliations": []}, {"name": "Nagarajan Natarajan", "affiliations": []}], "abstractText": "An active learner is given a model class \u0398, a large sample of unlabeled data drawn from an underlying distribution and access to a labeling oracle that can provide a label for any of the unlabeled instances. The goal of the learner is to find a model \u03b8 \u2208 \u0398 that fits the data to a given accuracy while making as few label queries to the oracle as possible. In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model, where the noise depends on the instance. We provide bounds on the convergence rates of active and passive learning for heteroscedastic regression. Our results illustrate that just like in binary classification, some partial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning.", "title": "Active Heteroscedastic Regression"}