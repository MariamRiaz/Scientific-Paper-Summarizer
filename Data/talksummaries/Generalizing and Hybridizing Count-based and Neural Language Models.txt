0	41	Language models (LMs) are statistical models that, given a sentence wI1 := w1, .
3	40	The most traditional and broadly used language modeling paradigm is that of count-based LMs, usually smoothed n-grams (Witten and Bell, 1991; Chen and Goodman, 1996).
7	103	Specifically, we define MODLMs as all LMs that take the following form, calculating the probabilities of the next word in a sentence wi given preceding context c according to a mixture of several component probability distributions Pk(wi|c): P (wi|c) = K∑ k=1 λk(c)Pk(wi|c).
10	25	1 can be used to describe not only n-gram models, but also feed-forward (Nakamura et al., 1990; Bengio et al., 2006; Schwenk, 2007) and 1163 recurrent (Mikolov et al., 2010; Sundermeyer et al., 2012) neural network LMs (§3).
12	37	It is also useful practically, in that this new view of these traditional models allows us to create new models that combine the desirable features of n-gram and neural models, such as: neurally interpolated n-gram LMs (§4.1), which learn the interpolation weights of n-gram models using neural networks, and neural/n-gram hybrid LMs (§4.2), which add a count-based n-gram component to neural models, allowing for flexibility to add large-scale external data sources to neural LMs.
15	32	As mentioned above, MODLMs are LMs that take the form of Eq.
16	49	This can be re-framed as the following matrix-vector multiplication: pᵀc = Dcλ ᵀ c, where pc is a vector with length equal to vocabulary size, in which the jth element pc,j corresponds to P (wi = j|c), λc is a size K vector that contains the mixture weights for the distributions, and Dc is a Jby-K matrix, where element dc,j,k is equivalent to the probability Pk(wi = j|c).2 An example of this formulation is shown in Fig.
17	48	Note that all columns in D represent probability distributions, and thus must sum to one over the J words in the vocabulary, and that all λ must sum to 1 over the K distributions.
20	68	In the sequel we show how this formulation can be used to describe several existing LMs (§3) as well as several novel model structures that are more powerful and general than these existing models (§4).
21	44	3.1 n-gram LMs as Mixtures of Distributions First, we discuss how count-based interpolated ngram LMs fit within the MODLM framework.
22	25	Maximum likelihood estimation: n-gram models predict the next word based on the previous N -1 words.
46	24	2b, not calculating p directly, but instead calculating mixture weights λ = softmax(hWs + bs), and defining the MODLM’s distribution matrix D as a J-by-J identity matrix.
48	60	While it may not be clear why it is useful to define neural LMs in this somewhat roundabout way, we describe in §4 how this opens up possibilities for novel expansions to standard models.
55	43	In these models, we setD to be the same matrix used in n-gram LMs, but calculateλ(c) using a neural network model.
95	22	Models were evaluated every 500k-3M words, and the model with the best development likelihood was used.
113	33	Interestingly, even when using simple ML distributions, the best neurally interpolated n-gram model nearly matches the heuristic KN method, demonstrating that the proposed model can automatically learn interpolation functions that are nearly as effective as carefully designed heuristics.5 6.3 Results for Neural/n-gram Hybrids In experiments with hybrid models, we test a neural/n-gram hybrid LM using LSTM networks with both Kronecker δ and KN smoothed 5-gram distributions, trained either with or without block dropout.
118	32	5, and the amount of the probability mass in λ(c) assigned to the non-δ distributions in the hybrid models.
119	48	From this, we can see that the model with block dropout quickly converges to a better result than the LSTM LM, but the model without converges to a worse result, assigning too much probability mass to the dense count-based distributions, demonstrating the learning problems mentioned in §5.2.
121	49	One reason can be found in the behavior with regards to low-frequency words.
122	35	4, we show perplexities for words that appear n times or less in the training corpus, for n = 10, n = 100, n = 1000 and n = ∞ (all words).
126	24	To examine the ability of the hybrid models to use counts trained over larger amounts of data, we perform experiments using two larger data sets: WSJ: The PTB uses data from the 1989 Wall Street Journal, so we add the remaining years between 1987 and 1994 (1.81M sents., 38.6M words).
129	57	The former has the advantage of training the net on much larger data.
130	32	The latter has two main advantages: 1) when the smaller data is of a particular domain the mixture weights can be learned to match this in-domain data; 2) distributions can be trained on data such as Google n-grams (LDC2006T13), which contain n-gram counts but not full sentences.
131	22	6, we can first see that the neural/n-gram hybrids significantly outperform the traditional neural LMs in the scenario with larger data as well.
135	42	Finally, because the proposed neural/n-gram hybrid models combine the advantages of neural and ngram models, we compare with the more standard method of training models independently and combining them with static interpolation weights tuned on the validation set using the EM algorithm.
137	41	From the results, we can see that when only PTB data is used, the methods have similar results, but with the more diverse data sets the proposed method edges out its static counterpart.7
145	37	In this paper, we proposed a framework for language modeling that generalizes both neural network and count-based n-gram LMs.
146	31	This allowed us to learn more effective interpolation functions for count-based n-grams, and to create neural LMs that incorporate information from count-based models.
147	153	As the framework discussed here is general, it is also possible that they could be used in other tasks that perform sequential prediction of words such as neural machine translation (Sutskever et al., 2014) or dialog response generation (Sordoni et al., 2015).
148	68	In addition, given the positive results using block dropout for hybrid models, we plan to develop more effective learning methods for mixtures of sparse and dense distributions.
