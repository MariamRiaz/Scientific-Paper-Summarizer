0	79	Tree Adjoining Grammar (TAG, Joshi and Schabes (1997)) and Combinatory Categorial Grammar (CCG, Steedman and Baldridge (2011)) are both mildly context-sensitive grammar formalisms that are lexicalized: every elementary structure (elementary tree for TAG and category for CCG) is associated with exactly one lexical item, and every lexical item of the language is associated with a finite set of elementary structures in the grammar (Rambow and Joshi, 1994).
2	13	The first phase of supertagging can be considered as “almost parsing” because supertags for a sentence almost always determine a unique parse (Bangalore and Joshi, 1999).
4	24	We focus on TAG parsing in this work.
14	17	For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models.
27	28	Supertagging assigns to words elementary trees (supertags) chosen from a finite set, and parsing determines how these elementary trees can be combined to form a derivation tree that yield the observed sentence.
52	27	In order to obtain predicted POS tags and supertags of the training data for subsequent parser input, we also perform 10- fold jackknife training.
56	14	For example, Bangalore et al. (2009) proposes the MICA parser, an Earley parser that exploits a TAG grammar that has been transformed into a variant of a probabilistic CFG.
59	31	Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.
61	13	The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representation obtained from CNNs in the same fashion as in the supertagger.1 We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017).
78	19	In training, we simply take the greedy maximum probability to predict the parent of each word.
82	14	Let pi be the index of the predicted head of the ith word, and r be the number of dependency relations in the dataset.
108	14	Indeed, in experiments described in a later section, we show empirically that predicting POS tags and supertags does indeed benefit performance on parsing (as well as the tagging tasks).
126	15	These results suggest that morphological information and word information from character-level CNNs and word embeddings overwhelm the information from predicted POS tags and supertags.
130	34	We provide joint modeling results for supertagging and parsing in Tables 2 and 3.
131	101	For these joint models, we employed the best parsing configuration (4 layers of BiLSTMs, character-level CNNs, and highway connections), with and without POS tagging added as an additional task.
132	25	We can observe that our full joint model that performs POS tagging, supertagging, and parsing further improves performance in all of the three tasks, yielding the test result of 91.89 LAS and 93.26 UAS points, an improvement of more than 2.2 points each from the state-of-the-art.
133	15	Figures 2 and 3 illustrate the relative performance of the feed-forward neural network shiftreduce TAG parser (Kasai et al., 2017) and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse.
136	18	Since the shiftreduce parser builds a parse sequentially with one parsing action depending on those that come before it, we would expect to find a propogation of errors made in establishing shorter dependencies to the establishment of longer dependencies.
139	21	Our current implementation processes 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009).5
147	23	Consider, for instance, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a transitive verb (t99) is to a subject relative headed by an intransitive verb (t109).
165	47	For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for instance, that John knows Mary or John saw an animal.
167	18	Xu et al. (2017) provided a set of linguisticallymotivated transformations to use TAG derivation trees to solve the PETE task.
169	31	We present test results from two configurations in Table 5.
170	59	One configuration is a pipeline approach that runs our BiLSTM POS tagger, supertagger, and parser.
172	56	The joint method yields 78.1% in accuracy and 76.4% in F1, improvements of 2.4 and 2.7 points over the previously reported best results.
174	29	The corpus comprises 7 constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object wh-questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm).
176	28	We instead conduct an automatic evaluation using a procedure that converts TAG parses to structures directly comparable to those specified in the unbounded dependency corpus.
179	15	Our joint parser outperforms the other parsers, including the neural network shift-reduce TAG parser (Kasai et al., 2017).
182	41	Nonetheless, we see that the rich structural representations that a TAG parser provides enables substantial improvements in the extraction of unbounded dependencies.
197	27	In this work, we presented a state-of-the-art TAG supertagger, a parser, and a joint parser that performs POS tagging, supertagging, and parsing.
