1	85	The combination of visual and textual content poses new challenges for information systems which need not only to deal with the semantics of text but also that of images.
2	33	Recent work (Barbieri et al., 2017) has shown that textual information can be used to predict emojis associated to text.
3	26	In this paper we show that in the current context of multimodal communication where texts and images are combined in social networks, visual information should be combined with texts in order to obtain more accurate emojiprediction models.
4	39	We explore the use of emojis in the social media platform Instagram.
10	24	Given that emojis may also mislead humans (Miller et al., 2017), the automated prediction of emojis may help to achieve better language understanding.
18	26	We cast the emoji prediction problem as a classification task: given an image or a text (or both inputs in the multimodal scenario) we select the most likely emoji that could be added to (thus used to label) such contents.
22	41	ResNet is a feedforward CNN that exploits “residual learning”, by bypassing two or more convolution layers (like similar previous approaches (Sermanet and LeCun, 2011)).
31	28	Given a set of N documents, the loss that the model attempts to minimize is the negative log-likelihood over the labels (in our case, the emojis): loss = − 1 N n=1∑ N en log(softmax (BAxn)) where en is the emoji included in the n-th Instagram post, represented as hot vector, and used as label.
38	13	In the first experiment (Section 4.2) we compare the FastText model with the state of the art on emoji classification (B-LSTM) by Barbieri et al. (2017).
42	17	We use 80% of our dataset (introduced in Section 2) for training, 10% to tune our models, and 10% for testing (selecting the sets randomly).
43	30	To model visual features we first finetune the ResNet (process described in Section 3.1) on the emoji prediction task, then extract the vectors from the input of the last fully connected layer (before the softmax).
48	13	In this comparison we used the same Twitter datasets.
51	25	We present the results of the three emoji classification tasks, using the visual, textual and multimodal features (see Table 2).
52	31	The emoji prediction task seems difficult by just using the image of the Instagram post (Visual), even if it largely outperforms the majority baseline3 and weighted random4.
53	14	We achieve better performances when we use feature embeddings extracted from the text.
54	26	The most interesting finding is that when we use a multimodal combination of visual and textual features, we get a nonnegligible improvement.
58	26	The emoji with highest F1 using the textual features is the most frequent one (0.62) and the US flag (0.52).
59	26	The latter seems easy to predict since it appears in specific contexts: when the word USA/America is used (or when American cities are referred, like #NYC).
60	25	The hardest emojis to predict by the text only system are the two gestures (0.12) and (0.13).
75	42	Detecting the correct emoji given an image is harder than a simple object recognition task, as the emoji choice depends on subjective emotions of the user who posted the image.
76	29	In Figure 2 we show the first four predictions of the CNN for three pictures, and where the network focuses (in red).
77	56	We can see that in the first example the network selects the smile with sunglasses because of the legs in the bottom of the image, the dog emoji is selected while focusing on the dog in the image, and the smiling emoji while focusing on the person in the back, who is lying on a hammock.
79	69	The same “praying” emoji is also selected when focusing on the luxury car in the third example, probably because the same emoji is used to express desire, i.e. “please, I want this awesome car”.
80	32	It is interesting to note that images can give context to textual messages like in the following Instagram posts: (1)“Love my new home ” (associated to a picture of a bright garden, outside) and (2) “I can’t believe it’s the first day of school!!!
81	107	I love being these boys’ mommy!!!!
82	29	#myboys #mommy ” (associated to picture of two boys wearing two blue shirts).
83	26	In both examples the textual system predicts .
84	12	While the multimodal system correctly predicts both of them: the blue color in the picture associated to (2) helps to change the color of the heart, and the sunny/bright picture of the garden in (1) helps to correctly predict .
100	10	We have shown that using a synergistic approach, thus relying on both textual and visual contents of social media posts, we can outperform state of the art unimodal approaches (based only on textual contents).
101	15	As future work, we plan to extend our models by considering the prediction of more than one emoji per Social Media post and also considering a bigger number of labels.
