32	1	By experiments, we demonstrate the effectiveness of Gibbs-OT for solving optimal transport with Coulomb cost (Benamou et al., 2016) and the Wasserstein non-negative matrix factorization (NMF) problem (Sandler & Lindenbaum, 2009; Rolet et al., 2016).
47	26	Definition 3.1 (Optimal Transportation, OT).
48	21	Let p 2 m1 ,q 2 m2 , where m is the set of m-dimensional simplex: m def.
51	14	The optimal transport cost between p and q with respect to M is W (p,q) def.
54	42	One may refer to Villani (2003) for the general background of the Kantorovich-Rubenstein duality.
56	38	Definition 3.2 (Dual Formulation of OT).
57	6	Let CM > 0, denote vector [g 1 , .
60	15	(2) Informally, for a sufficiently large CM (subject to p,q,M ), the LP problem Eq.
63	4	Then any optimal point f ⇤ = (g ⇤,h⇤) 2 ⌦⇤(M) constructs a (projected) subgradient such that g⇤ 2 @W/@p and h⇤ 2 @W/@q .
66	24	In order to constrain the feasible region to be bounded, we alternatively define ⌦ 0 (M)={f = [g;h] 2 ⌦(M) | g 1 = 0}.
67	11	(4) One can show that the maximization in ⌦(M) as Eq.
68	6	(3) is equivalent to the maximization in ⌦ 0 (M) because hp, m1i = hq, m2i.
73	5	The basic concept behind SA states that the samples from the Boltzmann distribution will eventually concentrate at the optimum set of its deriving problem (e.g. W (p,q)) as T !
74	14	However, since the Boltzmann distribution is often difficult to sample, a practical convergence rate remains mostly unsettled for specific MCMC methods.
78	8	= max 1jm2 ( CM + hj) , (8) hj < bUj(g) def.
80	4	Suppose f follows the Boltzmann distribution by Eq.
81	33	(5), gi’s are conditionally independent given h, and likewise hj’s are also conditionally independent given g. Furthermore, it is immediate from Eq.
87	28	,m 2 , let ( L(t)j := max1im1 ⇣ g(t 1)i Mi,j ⌘ h(t)j := L (t) j + ✓j · T (2t 1)/qj (12) 2.
89	2	But we have found in experiments that by calculating U (t) 1 and sampling g(t) 1 in Algorithm 1 according to Eq.
90	22	(13), one can still generate MCMC samples from ⌦(M) such that the energy quantity hp,gi hq,hi converges to the same distribution as that of MCMC samples from ⌦ 0 (M).
91	1	Therefore, we will not assume g 1 = 0 from now on and develop analysis solely for the unconstrained version of Gibbs-OT.
93	50	As T decreases along iterations, the 95% percentile band for sample f becomes thinner and thinner.
98	10	As a result, a well-accepted practice of SA for many complicated optimization problems is to empirically adjust cooling schedules, a strategy we take for our experiments.
102	8	Definition 4.2 (Notations for Auxiliary Statistics).
106	14	They can be redefined equivalently by specifying the transition probabilities p(zn+1|zn) for n = 1, .
107	56	, 2N , a.k.a., the conditional p.d.f.
108	71	One may notice that the alternative representation converts the Gibbs sampler to one whose structure is similar to a hidden Markov model, where the g,h chain is conditional independent given the U,L chain and has (factored) exponential emission distributions.
109	170	We will use this equivalent representation in Appendix A and develop analysis based on the U,L chain accordingly.
110	13	We now consider the function V (x,y) def.
112	25	Let V (Ut 0 ,Lt) be denoted by V (zt+t 0 ), where t0 = t or t 1.
