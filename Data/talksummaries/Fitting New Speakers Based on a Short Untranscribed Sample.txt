17	2	These experiments demonstrate that mimicking a new speaker does not require an optimization process and can be based on even a short sample.
21	4	The situation is not yet materially different for neural speech systems.
22	4	A critical look at the current neural methods reveals that the most striking voice qualities are obtained on single speaker models, trained on hours of carefully transcribed samples and that the literature on multispeaker systems does not focus on de-novo speakers.
23	2	The recent neural TTS systems include the Deep Voice systems DV1 (Arik et al., 2017b), DV2 (Arik et al., 2017a) and DV3 (Ping et al., 2018), WaveNet (Oord et al., 2016) and Parallel WaveNet (van den Oord et al., 2017), Char2Wav (Sotelo et al., 2017), Tacotron (Wang et al., 2017) and Tacotron2 (Shen et al., 2017), and VoiceLoop (Taigman et al., 2018).
27	4	With regards to output, there are also a few options in the literature: Tacotron creates spectrograms, which are inverted using the Griffin-Lim method.
30	1	DV1, DV2, DV3 and Tacotron2 employ WaveNet as neural vocoder to transform compact representations, such as spectrograms or vocoder features to raw audio.
32	3	Wavenet employs dilated convolutions.
33	1	The Tacotron method employs multiple RNNs, convolutions and a highway network (Srivastava et al., 2015).
34	2	Recently, Tacotron2 simplified the latter by replacing highway networks with RNNs and predicting a residual to improve the system output.
39	2	The reliance on World Vocoder is convenient, especially considering the landscape of open-source WaveNet implementations with regards to quality and efficiency.
41	2	Only three published neural systems are multi-speaker: DV2, DV3, and VoiceLoop1.
43	1	DV2 was also applied to an internal dataset of audiobooks with 447 speakers and DV3 to LibriSpeech (Panayotov et al., 2015) with 2484 speakers.
56	3	Each forward pass runs three sequential steps.
68	3	The neural multispeaker systems in the literature (Arik et al., 2017a; Ping et al., 2018; Taigman et al., 2018) are based on embedding the speakerâ€™s voice in some vector space.
69	1	This form of embedding opens the way to fitting a new speaker, using backpropagation, as was shown in (Taigman et al., 2018).
88	3	(ii) the accuracy of these systems is limited, especially in uncontrolled settings.
98	1	Finally, an affine projection followed by an L2 normalization is performed in order to obtain the embedding vector z.
116	2	The various parameters follow the implementation released by Taigman et al. (2018).
124	1	Since VoiceLoop is the only open implementation of a multispeaker contribution, and since it is the only contribution to demonstrate fitting to new speakers, we employ it as our baseline.
141	1	Specifically, for each pair, we compute the cosine distance between the activations of the last layer prior to the classifi- cation, which is of dimensionality 256.
148	2	Among the samples of each speaker, we use the existing splits of train and test.
180	1	We, therefore, report these on subsets of the speakers which are stratified by quality.
192	2	The Priming based method, presented in Sec.
208	2	This capability is based on a priming operator, and relies on a phenomenon we identify, the voice constancy property.
209	4	Second, by training on VCTK85 and then fitting on datasets with different characteristics and many more speakers, is becomes apparent that it is sufficient to train on a small population of 85 speakers in order to capture much of the variation in the general population.
210	3	Third, we demonstrate that a dynamic embedding, which is captured on-the-fly, is able to at least match learned embeddings.
211	4	This is surprising, and as our ablation analysis shows, stems from the losses we incorporate into the problem.
212	19	One of the losses employed, Lcycle, opens the way for training a TTS system in a semi-supervised way, in which some speakers are transcribed and some are not.
213	188	This is because it does not require that the audio generated by the system is identical to the input audio, only that the speaker identity is preserved.
214	183	This way, the same TTS system can be trained on many languages at once, including languages without suitable transcribed corpora.
