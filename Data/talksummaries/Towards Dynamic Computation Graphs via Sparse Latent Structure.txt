9	27	We propose here a new strategy for building dynamic computation graphs with latent structure, through sparse structure prediction.
11	10	Namely, our approach is the first that: A) is fully differentiable; B) supports latent structured variables; C) can marginalize over full global structures.
12	11	This contrasts with off-line and with reinforcement learning-based approaches, which satisfy B and C but not A; and with local marginal-based methods such as structured attention networks, which satisfy A and B, but not C. Key to our approach is the recently proposed SparseMAP inference (Niculae et al., 2018), which induces, for each data example, a very sparse posterior distribution over the possible structures, allowing us to compute the expected network output efficiently and explicitly in terms of a small, interpretable set of latent structures.
13	77	Our model can be trained end-to-end with gradient-based methods, without the need for policy exploration or sampling.
14	45	We demonstrate our strategy on inducing latent dependency TreeLSTMs, achieving competitive results on sentence classification, natural language inference, and reverse dictionary lookup.
16	99	Let x and y denote classifier inputs and outputs, and h ∈ H(x) a latent variable; for example, H(x) can be the set of possible dependency trees for x.
17	101	We would like to train a neural network to model p(y | x) := ∑ h∈H(x) pθ(h | x) pξ(y | h, x), (1) where pθ(h | x) is a structured-output parsing model that defines a distribution over trees, and pξ(y | h, x) is a classifier whose computation graph may depend freely and globally on the structure h (e.g., a TreeLSTM).
24	10	With negative entropy regularization, i.e., Ω(q) := ∑ h∈H(x) q(h) log q(h), we recover marginal inference, and the probability of a tree becomes (Wainwright and Jordan, 2008) pθ(h | x) ∝ exp(fθ(h;x)).
26	12	However, crucially, since exp(·) > 0, every tree is assigned strictly nonzero probability.
29	14	This is generally intractable, and even hard to approximate via sampling, even when pθ is tractable.
30	10	At the polar opposite, setting Ω(q) := 0 yields maximum a posteriori (MAP) inference (see Appendix A).MAP assigns a probability of 1 to the highest-scoring tree, and 0 to all others, yielding a very sparse pθ.
36	37	Situated between marginal inference and MAP inference, SparseMAP assigns nonzero probability to only a small set of plausible trees H̄ ⊂ H, of size at most equal to the number of arcs (Martins et al., 2015, Proposition 11).
54	8	We evaluate our approach on three natural language processing tasks: sentence classification, natural language inference, and reverse dictionary lookup.
57	22	Our baselines consist in extreme cases of dependency trees: where the parent of word i is word i+1 (resulting in a left-to-right sequential LSTM), and where all words are direct children of the root node (resulting in a flat additive model).
58	54	We also consider off-line dependency trees precomputed by Stanford CoreNLP (Manning et al., 2014).
62	9	We tune the learning rate on a log-grid, using a decay factor of 0.9 after every epoch at which the validation performance is not the best seen, and stop after five epochs without improvement.
65	10	We evaluate our models for sentence-level subjectivity classification (Pang and Lee, 2004) and for binary sentiment classification on the Stanford Sentiment Treebank (Socher et al., 2013).
75	13	We maximize the cosine similarity of the predicted vector with the embedding of the defined word.
77	13	Compared to the latent structure model of Yogatama et al. (2017), our model performs better on SNLI (80.5%) but worse on SST (86.5%).
81	33	The latent TreeLSTM model reaches the best accuracy on two out of the three datasets.
83	9	For context, we repeat the scores of the CKY-based latent TreeLSTM model of Maillard et al. (2017), as well as of the LSTM from Hill et al. (2016); these different-sized models are not entirely comparable.
84	16	We attribute our model’s performance to the latent parser’s flexibility, investigated below.
86	73	We find that our model, to maximize accuracy, prefers flat or nearly-flat trees, but not exclusively: the average posterior probability of the flat tree is 28.9%.
88	9	Syntax is not necessarily an optimal composition order for a latent TreeLSTM, as illustrated by the poor performance of the off-line parser (Table 1).
89	21	Consequently, our (fully unsupervised) latent structures tend to disagree with CoreNLP: the average probability of CoreNLP arcs is 5.8%; Williams et al. (2018) make related observations.
90	11	Indeed, some syntactic conventions may be questionable for recursive composition.
91	13	Figure 3 shows two examples where our model identifies a plausible symmetric composition order for coordinate structures: this analysis disagrees with CoreNLP, which uses the asymmetrical Stanford / UD convention of assigning the left-most conjunct as head (Nivre et al., 2016).
92	27	Assigning the conjunction as head instead seems preferable in a Child-Sum TreeLSTM.
94	30	Thanks to sparsity and autobatching, the actual slow-down is not problematic; moreover, as the model trains, the latent parser gets more confident, and for many unambiguous sentences there may be only one latent tree with nonzero probability.
98	47	In concurrent work, Peng et al. (2018) proposed an approximate MAP backward pass, relying on a relaxation and a gradient projection.
