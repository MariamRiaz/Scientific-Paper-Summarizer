0	32	Reinforcement learning (RL) is a powerful learning paradigm for sequential decision making (see, e.g., Bertsekas & Tsitsiklis, 1995; Sutton & Barto, 1998).
3	60	Policy evaluation estimates a value function that predicts expected cumulative reward the agent would receive by following a fixed policy starting at a certain state.
4	20	In addition to quantifying long-term values of states, which can be of interest on its own, value functions also provide important information for the agent to optimize its policy.
5	18	For example, policy-iteration algorithms iterate between policy-evaluation steps and policy-improvement steps, until a (near-)optimal policy is found (Bertsekas & Tsitsiklis, 1995; Lagoudakis & Parr, 2003).
8	13	These methods use the Bellman equation to bootstrap the estimation process.
10	22	In this paper, we study policy evaluation by minimizing the mean squared projected Bellman error (MSPBE) with linear approximation of the value function.
13	20	The finite-data regime makes it possible to solve policy evaluation more efficiently with recently developed fast optimization methods based on stochastic variance reduction, such as SVRG (Johnson & Zhang, 2013) and SAGA (Defazio et al., 2014).
40	9	Rd and approximate the value function by bV ⇡ (s) = (s)T ✓, where ✓ 2 Rd is the model parameter to be estimated.
41	35	Here, we want to find ✓ that minimizes the mean squared projected Bellman error, or MSPBE: MSPBE (✓) , 1 2 kbV ⇡ ⇧T⇡ bV ⇡k2 ⌅ , (2) where ⌅ is a diagonal matrix with diagonal elements being the stationary distribution over S induced by the policy ⇡, and ⇧ is the weighted projection matrix onto the linear space spanned by (1), .
42	17	, T (|S|)] is the matrix obtained by stacking the feature vectors row by row.
67	17	Nevertheless, we will show that the minimizing the EMMSPBE is equivalent to solving a convex-concave saddlepoint problem which actually possesses the desired finitesum structure.
71	8	Therefore, minimizing the EM-MSPBE is equivalent to solving the saddle-point problem (10), which is convex in the primal variable ✓ and concave in the dual variable w. Moreover, it has a finite-sum structure similar to (9).
104	28	Moreover, even if ⇢ > 0, it will be inefficient to solve problem (10) using primal-dual algorithms based on proximal mappings of the strongly convex and concave terms (e.g., Chambolle & Pock, 2011; Balamurugan & Bach, 2016).
113	13	In this section, we provide two stochastic variance reduction methods and show they achieve fast linear convergence.
117	23	3: for j = 1 to N do 4: Sample an index t j from {1, · · · , n} and do 5: Compute B tj (✓, w) and Btj (˜✓, w̃).
122	23	(17) Here, B tj (✓, w) contains the stochastic gradients at (✓, w) computed using the random sample with index t j , and B(˜✓, w̃) B tj ( ˜✓, w̃) is a term used to reduce the variance in B tj (✓, w) while keeping Btj(✓, w, ˜✓, w̃) an unbiased estimate of B(✓, w).
125	13	The second stochastic variance reduction method for policy evaluation is adapted from SAGA (Defazio et al., 2014); see Algorithm 3.
141	33	applies automatically to k✓ ✓ ?
151	32	⇤  ✏ is upper bounded by O ✓✓ n+ ( bC)L2 G 2 min (⇢I + bAT bC 1 bA) ◆ d log ⇣ 1 ✏ ⌘◆ .
157	9	In contrast, the linear convergence of SVRG and SAGA in Balamurugan & Bach (2016) requires the Lagrangian to be both strongly convex in ✓ and strongly concave in w. Moreover, in the policy evaluation problem, the strong concavity with respect to the dual variable w comes from a weighted quadratic norm (1/2)kwk b C , which does not admit an efficient proximal mapping as required by the proximal versions of SVRG and SAGA in Balamurugan & Bach (2016).
162	26	The upper part of the table lists algorithms whose complexity is linear in feature dimension d, including the two new algorithms presented in the previous section.
163	18	We can also apply GTD2 to a finite dataset with samples drawn uniformly at random with replacement.
166	12	However, as verified by our experiments, the bounds in the table show that our SVRG/SAGA-based algorithms are much faster as their effective condition numbers vanish when n becomes large.
197	12	However, it does not change the order of the total complexity for SVRG/SAGA.
202	13	We only report the results of each algorithm which correspond to the best-tuned step sizes; for SVRG we choose N = 2n.
203	24	In the first task, we consider a randomly generated MDP with 400 states and 10 actions (Dann et al., 2014).
204	10	The transition probabilities are defined as P (s0|a, s) / pa ss 0+10 5, where pa ss 0 ⇠ U [0, 1].
222	24	This work leads to several interesting directions for research.
223	46	First, we believe it is important to extend the stochastic variance reduction methods to nonlinear approximation paradigms (Bhatnagar et al., 2009), especially with deep neural networks.
224	38	Moreover, it remains an important open problem how to apply stochastic variance reduction techniques to policy optimization.
