{"sections": [{"text": "\u03a6(x) = inf z\u2208Rm\n||x\u2212 z||22 + h(||z||2)\nand h is an even and univariate function on the real line. Connections are drawn between \u03a6 and the Moreau envelope of h. A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works. Finally comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O( \u221a log n/n)."}, {"heading": "1. Introduction", "text": "The dictionary learning problem, also known as sparse coding, was initially studied in the context of Neuroscience (Olshausen & Field, 1997); the relevant literature has grown enormously since; see (Zhang et al., 2015) and references therein. The problem is described as follows: given set {Xi}ni=1 \u2282 Rm with n points sampled from an unknown fixed probability measure \u00b5, a dictionary matrixD \u2208 Rm\u00d7d is to be constructed so that any sample from \u00b5 can be approximated well by linear combinations of columns of D. The quality of approximation, for a given dictionary D, is measured by some function fD while D usually belongs\n1School of Electrical and Computer Engineering, Technical University of Crete, Greece. Correspondence to: Alexandros Georgogiannis <alexandrosgeorgogiannis@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nto some predefined family of matrices. From the statistical learning theory perspective, the aim is to minimize the population risk\nR(D) := \u222b fD(X)d\u00b5 = \u222b fDd\u00b5, (1)\nwhen the only accessible information is a set of n training samples, say {Xi}ni=1, usually independent and identically distributed. Notation X is used for random vectors sampled from \u00b5 and notation x for real vectors in Rm.\nThe empirical risk minimization principle (ERM) is a natural approach in search of the best dictionary (Vapnik, 1998). It suggests that since the only availiable information is the set of training samples, one should search for the matrix D\u0302n that minimizes the empirical risk\nRn(D) := 1\nn n\u2211 i=1 fD(Xi). (2)\nThe empirical estimate D\u0302n is not of much use unless |Rn(D\u0302n) \u2212 R(D\u0302n)| decreases as the number of samples n increases. Subsuming all computational difficulties on computing the global minimizing argument of (2), the problem addressed here is a \u201cgeneralization problem\u201d. Given the family D of all m\u00d7 d matrices with unit-norm columns, we design a loss function fD that measures the quality of approximation x ' Da and ask: Does the difference\n|R(D\u0302n)\u2212 inf D\u2208D R(D)| = \u2223\u2223\u2223\u2223\u222b fD\u0302nd\u00b5\u2212 infD\u2208D \u222b fDd\u00b5 \u2223\u2223\u2223\u2223 (3) decrease as the number of samples n increases, and if so, at what rate? Or even further, if R(D\u0302n) is close to infD\u2208DR(D), is D\u0302n close to the global minimizing argument of R(D)? Intuitively, the decrement of the absolute difference in expression (3) guarantees that by increasing the amount of data the population risk, with high probability, is within a very small distance of the optimal achievable gets arbitrarily close to one. The answers to the previous questions of course depend on the number of samples, the predefined family of dictionaries and the loss function.\nThe proposed loss functions in the literature of dictionary learning vary according to the application but it would not be an exaggeration to say that almost all of them may be\ndescribed by a function of the form:\nfD(x) := inf a\u2208Rd\n\u03a6(x\u2212Da) + g(a), (4)\nwith \u03a6 : Rm \u2192 [0,+\u221e) and g : Rd \u2192 [0,+\u221e]. This article focuses on the generalization properties of dictionary learning when \u03a6 has the form:\n\u03a6(x\u2212Da) := inf z\u2208Rm\n1 2 ||x\u2212Da\u2212 z||22 + h(||z||2). (5)\nFunction h takes values on [0,+\u221e] and is described in further detail later on. Definition (5) is not novel and has been used in many applications of sparse coding, robust linear regression and dictionary learning (Adler et al., 2015; Amini et al., 2014; Forero et al., 2015; 2017; Jiang et al., 2015; Liu et al., 2015; Zhao & Tan, 2017). Although there is no formal robustness analysis yet to justify the superiority of (5) over the common square Euclidean loss ||x\u2212Da||22, experimental evaluations in the previous applications suggest that this modification is a computationally \u201ccheap\u201d alternative, achieving better reconstruction error in some cases.\nAs can be seen from (4), if g is a sparsity promoting penalty then approximations that are linear combinations of a few columns of D are favored. The rationale behind the choice of h in (5) is not so obvious but if h satisfies a set of assumptions, then the following simplification holds true:\n\u03a6(x\u2212Da) := eh(||x\u2212Da||2). (6)\nHere, eh is a univariate continuous function with special name and properties, the so called Moreau envelope of h (Rockafellar & Wets, 2009). Interestingly enough, the epigraphical form of eh is completely determined by the generating function h. Roughly speaking, with a suitably chosen h, we can design loss functions fD able to ignore the influence of points x, the distance of which from their approximation Da is above a predefined threshold. The consistency results of this study should be regarded as complementary extensions\u2212and in some cases refinements\u2212of the generalization bounds in (Gribonval et al., 2015b) and (Vainsencher et al., 2011). Contrary to previous works, all bounds presented here are valid for the whole of space of dictionaries with unit-norm columns.\nIn Section 3 is considered the case where g is a separable function, that is, g is of the form g(a) = \u2211d i=1 g\u0302(ai), and g\u0302 : R\u2192 [0,+\u221e) is univariate, continuous, even, and strictly increasing with minimum value g\u0302(0) = 0. These assumptions are valid for many coordinate-separable regularizers, e.g., the lp-norms and variants of the logarithmic function. Let us point out here that if h = 0, using the results of Section 3 we revert to previously known bounds for the penalized squared Euclidean loss fD(x) = infa\u2208Rd (1/2)\u2016x\u2212Da\u201622 + g(a).\nSection 4 is an attempt to cover, beyond the class of strictly increasing penalties g\u0302 of Section 3, continuous and bounded\npenalties from robust statistics, such as the MCP or SCAD. This type of penalty functions have achieved widespread use, and to the best of our knowledge, the bounds presented here are among the first that consider them.\nHowever, the extended bounds of Section 4 turn out to be of limited applicability and do not work when g is the indicator function of all k-sparse vectors. To overcome this difficulty, in Section 5, we remove the continuity assumption from g and rely on combinatorial tools from VapnikChervonenkis (VC) theory in order to present bounds valid for any bounded, lower semicontinuous function g. Whenever possible, the sample complexity bounds presented here are compared to similar ones in literature. Next follows a brief overview of the relevant literature."}, {"heading": "1.1. Related Work and Contribution", "text": "The authors in (Gribonval et al., 2015b; Vainsencher et al., 2011) derive sample complexity bounds for the rate of convergence towards 0 of the absolute difference in (3) when \u03a6(x) = ||x||22, D is a general constraint set, and g(a) ranges from the lp-norms and characteristic functions of compact sets to the indicator function of non-negative vectors or k-sparse vectors. The results in (Maurer & Pontil, 2010) are independent of dimension m, as well as some results in (Vainsencher et al., 2011).\nA closer look on results of (Gribonval et al., 2015b) and (Vainsencher et al., 2011) concerning the finite case for dimension d, reveals that those are valid under joint assumptions on g and D. For instance, if g is the indicator function of k-sparse vectors in Rd, then the generalization bounds in (Vainsencher et al., 2011) are valid under an incoherence assumption on D while in (Gribonval et al., 2015b) under a \u201crestricted isometry\u201d-like property. General non-asymptotic results can be extracted from the previous analyses, as the case \u03a6(x) = \u03c9(||x||), for any convex function \u03c9 : R\u2192 [0,+\u221e) and any norm || \u00b7 || on Rm. In (Liu & Tao, 2016) authors focus on the l1-non-negative matrix factorization problem where \u03a6(x) = ||x||1 and g is the indicator function of the non-negative orthant in Rd.\nThe main contribution of our work is the addition of generalization bounds concerning loss functions that are combinations of Moreau envelopes with bounded and lower semicontinuous regularizers. Some results are refinements of previously known ones, meaning that a spurious assumption on dictionary D has been removed."}, {"heading": "2. Preliminaries and some Technical Remarks", "text": "This is mainly a technical section where we take a closer look at the loss function fD and describe the statistical framework for the analysis. The value of fD at point x \u2208 Rm, in light of equations (4) and (5), is expressed through\nthe solution of the minimization problem:\ninf a\u2208Rd  := \u03a6(x\u2212Da)\ufe37 \ufe38\ufe38 \ufe37 inf z\u2208Rm { 1 2 ||x\u2212Da\u2212 z||22 + h(||z||2) } +g(a) \ufe38 \ufe37\ufe37 \ufe38 fD(x) . (7) The close connection between \u03a6 and h is captured in Lemma 1 that, among others, gives a description of the set of points z \u2208 Rm that achieve the minimum in (5). Lemma 1. Let h : R \u2192 [0,+\u221e] be a lower semicontinuous (lsc) and even function with its restriction on [0,+\u221e) non-decreasing and h(0) = 0. Assume that the multivalued map Ph : R\u2192\u2192 R, defined as\nPh(t) := argminu\u2208R 1\n2 (t\u2212 u)2 + h(u), (8)\n(H1) is odd, i.e., Ph(\u2212t) = \u2212Ph(t), (H2) compact-valued, (H3) non-decreasing, (H4) has a closed graph and (H5) satisfies Ph(t) \u2264 t for all t \u2208 R. Then function \u03a6 in (5) becomes\n\u03a6(x\u2212Da) = eh(||x\u2212Da||2), (9)\nwhere eh : R\u2192 [0,+\u221e) is defined as\neh(t) := inf u\u2208R\n1 2 (t\u2212 u)2 + h(u), t \u2208 R (10)\nand is continuous with its restriction on [0,+\u221e) being nondecreasing. Furthermore, map Ph : Rm \u2192\u2192 Rm,\nPh(x\u2212Da) := argminz\u2208Rm 1\n2 ||x\u2212Da\u2212z||22 +h(||z||2),\n(11) is equivalently represented as\nPh(x\u2212Da) = x\u2212Da ||x\u2212Da||2 Ph(||x\u2212Da||2). (12)\nAccording to Lemma 1, if h satisfies a certain set of assumptions, then \u03a6 is equal to the composition of the Moreau envelope of h with the Euclidean norm. Although the restrictions surrounding h and its proximal map seem to be strict, the lemma is valid for a large number of h and Ph pairs; see Section 3.1 in (Antoniadis, 2007) for various examples. Hereafter, any univariate function h in this article satisfies assumptions (H1) through (H5).\nExample 1. The case of the l0-norm on R is an example that clearly describes the influence of h on the boundedness properties of \u03a6. Let h be the l0-(pseudo)norm on the real line defined as l0(t;\u03bb) = \u03bb 2\n2 1{t 6=0} for some \u03bb > 0. The values of 1{t 6=0} alternate between zero and one according to whether t 6= 0 or not. The l0-norm satisfies all\nassumptions of Lemma 1: it is even, non-decreasing and lower semicontinuous while its proximal map Pl0 equals Pl0(t) = argminu\u2208R 1 2 (t\u2212u) 2 + l0(u;\u03bb) and is defined as\nPl0(t) =  0, |t| < \u03bb, {0, t}, |t| = \u03bb, t, |t| > \u03bb.\n(13)\nNow function fD : Rm \u2192 [0,+\u221e) reads as\nfD(x) = inf a\u2208Rd  12 min{||x\u2212Da||22, \u03bb2}\ufe38 \ufe37\ufe37 \ufe38 := el0 (||x\u2212Da||2) +g(a)  . (14) Boundedness of el0 implies that whenever the distance ||x\u2212Da\u2217D(x)||2 between a point x and its best linear approximation Da\u2217D(x) is greater than the predefined value \u03bb, then el0(||x \u2212 Da\u2217D(x)||2) = \u03bb2/2; here a\u2217D(x) is the (possibly multivalued) map\na\u2217D(x) := argmin a\u2208Rd\n{ el0(||x\u2212Da||22) + g(a) } . (15)\nAs long as g is globally upper bounded by some M > 0, if ||x\u2212Da\u2217D(x)||2 > \u03bb and a\u2217D(x) is sufficiently large, then fD(x) = \u03bb\n2/2+M . Since the empirical optimal dictionary D\u0302n is defined through the minimization of the empirical risk Rn(D) in (2), and Rn is solely a function of D, points x for which f(x) = \u03bb2/2 + M have no influence on the estimation of D\u0302n, and in that sense are \u201coutliers\u201d.\nThe previous example is merely used to build some intuition behind the popularity of fidelity term (9) in the presence of \u201coutliers\u201d. As \u201coutliers\u201d are considered points, the distance of which from their approximation Da\u2217D(x) is larger than a predefined threshold, say \u03b3 > 0. Note that any function h with proximal map satisfying Ph(t) = t when |t| > \u03b3 behaves like the l0-norm in Example 1.\nRemark 1. The simple example described above may serve to anchor intuition, but it should be kept in mind that although we use the term \u201coutlier\u201d, this is rather a study that focuses on the generalization error of dictionary learning. We do not provide robustness analysis of dictionary learning, since this would require a detailed mathematical definition of the notion \u201coutlier\u201d. Robustness analysis results for Moreau envelope losses using notions from robust statistics, as the breakdown value, are provided in (Georgogiannis, 2016) for the generalized k-means problem; k-means is an unstructured dictionary learning problem\u2212as Dm\u00d7d does not have unit-norm columns\u2212with m d, h(t) = 0, and g(\u00b7) the indicator of the basis vectors in Rd.\nA robustness analysis different from the previous one has already been developed in (Gribonval et al., 2015a); the authors show that under coherence-based assumptions on D,\nit is highly probable that the empirical risk 1n \u2211n i=1 fD(Xi), when fD(x) = infa\u2208Rd 12 ||x \u2212 Da|| 2 2 + g(a), has a guaranteed empirical local minimum around the neighborhood of a population global minimum dictionary. A study motivated by the above references is of great interest and would fill the gap between theoretical and actual performance of dictionary learning algorithms using Moreau envelopes.\nNext is introduced the statistical learning framework. Denote as X , X1, X2, . . . , independent and identically distributed random vectors with values in a closed ball in Rm, say BRm(T ) with radius T centered at the origin, and denote as P\u0304 the set of all probability distributions \u00b5 on the Borel \u03c3-algebra B(BRm(T )) generated by this ball.1 The aim is to show that the family of functions\nFD = { fD(x) : Rd \u2192 R; D \u2208 D } (16)\nhas the uniform convergence of empirical means property on the measure space (BRm(T ),B(BRm(T )), \u00b5), \u00b5 \u2208 P\u0304 . Here D is the set of all m\u00d7 d real matrices with unit Euclideannorm columns and fD is of the form (4). The collection of functions FD has the uniform convergence of empirical means (UCEM) property if the following convergence\nP  supfD\u2208FD\ufe38 \ufe37\ufe37 \ufe38 supD\u2208D \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 1 n n\u2211 i=1 fD(Xi)\ufe38 \ufe37\ufe37 \ufe38 Rn(D) \u2212 \u222b fDd\u00b5\ufe38 \ufe37\ufe37 \ufe38 R(D) \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 > \u03b5  n\u2192\u221e\u2212\u2192 0\n(17) is valid for every positive number \u03b5 and probability measure \u00b5 \u2208 P\u0304 on BRm(T ) (Vidyasagar, 2002).2 This asymptotic result immediately answers the question raised in the introduction: if (17) holds true, then an application of inequality\nR(D\u0302n)\u2212 inf D\u2208D R(D) . sup D\u2208D |Rn(D)\u2212R(D)|\nassures that R(D\u0302n) tends to the optimal value infD\u2208DR(D) as the number of samples increases.\nIn most of our proofs, standard arguments from empirical processes theory are followed. In Sections 3 and 4 an appropriate form for h and g is chosen and then are used techniques based on either deterministic (Kolmogorov & S\u030cirjaev, 1993) or random \u03b5-covers of the function class FD (Gyo\u0308rfi et al., 2006); let us recall their definitions.\nDefinition 1 (\u03b5-cover). Let \u03b5 > 0 and let F be a class of functions from A \u2286 Rm to R. Every finite collection\n1 The Borel \u03c3-algebra B(Y ) of a subset Y of a metric space S is the one generated by B(Y ) = {Y \u2229 E : E \u2208 B(S)}. Thus the Borel \u03c3-algebra B(BRm(T )) is precisely the class of all subsets of BRm(T ) which are Borel sets in Rm (Folland, 2013).\n2Symbol P in (17) denotes the product measure \u00b5\u00d7\u221e1 on the product \u03c3-algebra \u2297\u221e 1 B(BRm(T )) (Folland, 2013).\nof functions f\u03031, . . . , f\u0303N : Rm \u2192 R , for which for each f \u2208 F there is a j(f) \u2208 {1, . . . , N} such that\n||f \u2212 f\u0303j ||\u221e := sup x\u2208A |f(x)\u2212 f\u0303j(x)| < \u03b5, (18)\nis called \u03b5-cover of F under the supremum norm.\nLet FD,\u03b5 = {f1, . . . , fN} be a \u03b5-cover of FD with respect to || \u00b7 ||\u221e. As intuitively expected, the fewer the balls needed to cover FD, the smaller the FD. Definition 2 (\u03b5-covering number). Let \u03b5 > 0 and let F be a class of functions from a set A \u2286 Rm to R. Let N (\u03b5,F , || \u00b7 ||\u221e) be the size of the smallest \u03b5-cover of F under the supremum norm in (18). If no finite \u03b5-cover exists, takeN (\u03b5,F , ||\u00b7||\u221e) =\u221e. ThenN (\u03b5,F , ||\u00b7||\u221e) is named the \u03b5-covering number of F , abbreviated to N\u221e(\u03b5,F ).\nThe method of proof used in Sections 3 and 4 to establish the UCEM property for FD when g is continuous is based on deterministic \u03b5-covers, basic exponential inequalities and the Borel-Cantelli lemma. Unfortunately, this approach does not work when g is the indicator function of all k-sparse vectors; see Section 5. To overcome this difficulty, we rely on tools from VC theory, such as the shatter coefficient of the family of subgraphs of a function class. Definition 3 (subgraphs of a function class). Consider a function class F with functions f : Rm \u2192 R+. The set\nF+ := { {(x, t) \u2208 Rm+1 : f(x) \u2265 t}; f \u2208 F } (19)\nis the collection of all subgraphs of functions f in F .\nA family of subgraphs is a family of sets for which the shatter coefficient and VC dimension are defined as follows. Definition 4 (shatter coefficient). Let A be a family of sets. For {x1, . . . , xn} \u2282 Rm, let NA(x1, . . . , xn) be the number of different sets in {{x1, . . . , xn} \u2229 A; A \u2208 A} . The n-th shatter coefficient s(A, n) of A is\ns(A, n) := max x1,...,xn NA(x1, . . . , xn).\nThe shatter coefficient is the maximal number of different subsets of n points that can be picked out by sets of A. Definition 5 (VC dimension). Let A be a collection of sets with |A| \u2265 2. The largest integer k \u2265 1 for which s(A, k) = 2k is denoted by VA and is called the VC dimension of the class A.\nIf for some hypothetical function class F the corresponding shatter coefficient s(F+, n) is a polynomial of degree b with respect to n, i.e., s(F+, n) = O(nb), then the popular Vapnik-Chervonenkis\u2019s inequality (Theorem 12.5 in (Devroye et al., 1997)) implies UCEM for F . Later on, in Section 5, we show that this is the case for s(F+D , n) as well, where F+D denotes the collection of all subgraphs of functions in FD with g the indicator of k-sparse vectors in Rd\u2212recall the definitions of fD and FD in (4) and (16)."}, {"heading": "3. The case of a separable, continuous, even, and strictly increasing g : Rd \u2192 [0,+\u221e)", "text": "In this section we prove the UCEM property for the function class FD in (16) when fD is defined as\nfD(x) := inf a\u2208Rd\n{eh(||x\u2212Da||2) + g(a)} (20)\nand g has the following form:\ng(a) = d\u2211 i=1 g\u0302(ai). (21)\nHere is assumed that g\u0302 : R\u2192 [0,+\u221e) is a univariate, continuous, even, and strictly increasing function on [0,+\u221e) with minimum value g\u0302(0) = 0. The aforementioned assumptions on g are valid for many coordinate-separable regularizers, e.g., the lp norms on Rd, g(a) = \u03bb||a||p, 0 < p < +\u221e for some \u03bb > 0, and the log penalty function g(a) = \u2211d i=1 \u03bb log(\u03b3+1) log(\u03b3|ai| + 1), \u03b3 > 0. From now on, a separable function of the previous form is called (strictly) increasing if for all i, g\u0302(ai) is (strictly) increasing as |ai| \u2192 +\u221e. The main result is the following theorem. Theorem 1. Let \u03b5 > 0 and consider the function class FD in (16) with fD : BRm(T ) \u2192 [0, eh(T )] and g : Rd \u2192 [0,+\u221e) defined as in (20) and (21) respectively. Then\nP { sup\nfD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 > \u03b5 }\n\u2264 2 ( 9dg\u0302\u22121(eh(T ))\n2\u03b5\n)md e \u2212 2n\u03b52 9eh(T ) 2 .\n(22) Furthermore,\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223\u2192 0 (n\u2192\u221e) (23) almost surely, for any \u00b5 \u2208 P\u0304 . Hence, the function class FD has the UCEM property with respect to P\u0304 .\nAn outline of Theorem\u2019s 1 proof is the following:\n1. We define map F that maps any m\u00d7 p matrix to some function of the form (20). Using appropriate metrics, F is shown to be globally Lipschitz.\n2. The Lipschitz continuity of F and the covering number of D generate an upper bound for N\u221e(\u03b5,FD).\n3. Standard theorems from the empirical process theory imply the concentration result in (22) and finally prove the UCEM property for the function class FD.\nThe above outline makes clear that the main difficulty in proving Theorem 1 is the verification of the Lipschitz continuity of map F . Let us mention that factor dg\u0302\n\u22121(eh(T )) 2\nappearing on the right hand side (rhs) of (22) is an upper bound for the Lipschitz constant of the aforementioned map.\nThere exist other approaches that do not require any form of continuity on F to prove the UCEM property for FD. However, theoretical questions regarding the existence of the optimal dictionary are answered quite easily if we manage to construct such a map. For example, as well known, a continuous map maps compact sets to compact sets. If F is continuous, the compactness of D implies the compactness of FD. This in turn implies the existence of the optimal solution f\u2217D of minimization problem inffD\u2208FD \u222b fDd\u00b5; indeed, the integral is a linear operator and FD is compact. Remark 2. Another theoretical question, of great importance for the measure theory enthusiasts, concerns the measurability of the supremum appearing on the left hand side (lhs) of (22). This is a random variable of which the measurability stems from total boundedness of FD with respect to the supremum norm ||f ||\u221e := sup{x:||x||2\u2264T} |f(x)|. Proposition 1. Assume a set up as the one in Theorem 1. Then for any \u03b4 > 0,\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a log(nd) n ) (24)\nwith probability at least 1\u2212 \u03b4.\nThe term log(d) in (24), responsible for the sub-optimality of the bound in case of convex Moreau envelopes, results from our proof method; similar bounds in the literature are of order O( \u221a log n/n) (Gribonval et al., 2015b; Vainsencher et al., 2011). This term is eliminated in Lemma 2 below to end up with a same order upper bound. The latter is in alignment with the sample complexity results presented in (Gribonval et al., 2015b) and (Liu & Tao, 2016) for the cases where fD(x) equals infa\u2208Rd 12 ||x \u2212Da|| 2 2 + g(a) and infa\u2208Rd 12 ||x\u2212Da||1 + g(a) respectively. Lemma 2. Let L > dg\u0302 \u22121(eh(T ))\n2 and define \u03b2 > 0 as \u03b2 := mdmax{log(6L \u221a 8), 1}. Assume that n satisfies condition\nn\nlog(n) \u2265 max\n{ 8, ( 1\n2 \u221a 8L\n)2 \u03b2 } (25)\nand consider the same set up as in Theorem 1. Then,\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 \u2264 2\u221a8 \u221a \u03b2 log n n\n+ 1\u221a 8\n\u221a \u03b2 + t\nn ,\n(26) with probability at least 1\u2212 2e\u2212t.\nThe rationale behind this lemma is to find conditions, under which for large values of the sample size n, an exponential tail for the error kicks in but without the term log(d) of inequality (24). Although the analysis seems finer, the result is valid only if the sample size satisfies the quite strict and complex inequality (25)."}, {"heading": "4. The case of a separable, continuous, even, and bounded g : Rd \u2192 [0,+\u221e)", "text": "Analysis of Section 3 covers a broad range of regularizers g but it does not cover popular penalty functions from robust statistics, like SCAD, gscad(a) = \u2211d i=1 g\u0302scad(ai) or MCP,\ngmcp(a) = \u2211d i=1 g\u0302mcp(ai) (Mazumder et al., 2012):\ng\u0302scad(t;\u03bb, \u03b3) =  \u03bbt, t \u2264 \u03bb \u03bb\u03b3t\u2212 12 (t 2+\u03bb2)\n\u03b3\u22121 , \u03bb < t \u2264 \u03b3\u03bb \u03bb2(\u03b32\u22121) 2(\u03b3\u22121) , t > \u03bb\u03b3, (27)\nand\ng\u0302mcp(t;\u03bb, \u03b3) =\n{ \u03bbt\u2212 t 2\n2\u03b3 , t \u2264 \u03bb 1 2\u03b3\u03bb 2, t > \u03b3\u03bb. (28)\nAlthough the previous univariate functions are continuous, even, and satisfy the assumptions of Lemma 1, they fail to satisfy the assumptions of Theorem 1 because they are bounded above and thus not strictly increasing.\nThis section is an attempt to extend the results of Section 3 and handle a very special case of coordinate-separable regularizers: those g(a) = \u2211d i=1 g\u0302(ai), where g\u0302 : R \u2192 [0,+\u221e) is not only continuous and symmetric around zero, but also strictly increasing up to some point in [0,+\u221e) and then constant. For this purpose, we require that g\u0302 satisfies the additional (strict) inequality\neh(T ) < sup t\u2208R g\u0302(t). (29)\nUnder assumption (29), all results presented in Section 3 remain valid; see the relevant discussion in Appendix A.5. Example 2 describes the impact of this assumption on penalty function g\u0302mcp while the same applies to g\u0302scad.\nExample 2. Let h be the l0-norm on the real line and g\u0302(a) = g\u0302mcp(a; \u03b3, \u03bb2), \u03bb2 > 0; recall the definition of the l0-norm on the real line: l0(t;\u03bb1) = \u03bb12 1{t 6=0}, \u03bb1 > 0 . In this case, the Moreau envelope is\nel0(t;\u03bb1) = 1\n2 min{t2, \u03bb21}\nand gmcp(a) = \u2211d i=1 g\u0302mcp(ai; \u03b3, \u03bb2). Now assumption (29) reads as\nsup t\u2208R\ng\u0302mcp(t;\u03bb2, \u03b3) > 1\n2 min{T 2, \u03bb21} (30)\nor after some simple algebraic calculations,\n1 2 \u03bb22\u03b3 > 1 2 min{T 2, \u03bb21} \u21d4 \u03bb2 >\n\u221a 1\n\u03b3 min{T 2, \u03bb21}.\n(31) Thus, function class FD in (16) with fD(x) defined as\nfD(x) = inf a\u2208Rd\n{ el0(||x\u2212Da||2) + d\u2211 i=1 g\u0302mcp(ai; \u03b3, \u03bb2) }\nhas the UCEM property only for pairs of values (\u03bb1, \u03bb2) with \u03bb2 > \u221a 1 \u03b3 min{T 2, \u03bb 2 1}.\nExample 2 reveals that the ease with which we extend the results of Section 3 has great impact on the diversity of functions g\u0302 that we could handle. In order to use the upper bounds in Proposition 1 or Lemma 2, our focus needs to be restricted on families FD where the rightmost inequality in (31) holds. This artificial restriction on the available pair of values (\u03bb1, \u03bb2) makes this extension quite useless; in many applications, when setting up \u03bb1 and \u03bb2, we search on a wider grid of values.\nIn the next section, we remove the continuity assumption from g and derive generalization bounds valid for any bounded lsc function, such as SCAD, MCP or the indicator function of all k-sparse vectors in Rd."}, {"heading": "5. The case of the indicator function of all", "text": "k-sparse vectors in Rd and its extension\nDenote as \u03a3k = {a \u2208 Rd : |{i : ai 6= 0}| = k} the set of all k-sparse vectors in Rd. The approach followed in Sections 3 and 4 to prove the UCEM property for FD heavily relies on the assumption that g is continuous. Consequently, it does not work for the function\ng(a) = { 0, if a \u2208 \u03a3k +\u221e, otherwise,\n(32)\nthe non-separable and lsc indicator function of all k-sparse vectors in Rd. Using combinatorial tools from VC theory, we remove the spurious condition on the coherence of D \u2208 D appearing in previous works (Gribonval et al., 2015b; Vainsencher et al., 2011) and prove the UCEM property when g is bounded and lsc. Starting the analysis with function (32), the results are then extended to cover any bounded lsc function on Rd with range in [0,+\u221e).\nNext is presented Proposition 2, a modification of Theorem 20 in (Vainsencher et al., 2011): it states that map F from metric space (D, || \u00b7 ||1,2) to metric space (FD, || \u00b7 ||\u221e),\nFD := { min a\u2208\u03a3k eh(||x\u2212Da||2); D \u2208 D } , (33)\nis not uniformly Lipschitz for any Lipschitz constant.3 This is the main reason we resign (ourselves) from previous proof techniques. Without an explicit upper bound for the Lipschitz constant of map F , we cannot infer a bound for the covering number of FD in terms of the one of D.\nProposition 2. Consider the family of functions FD in (33). Then, there exist \u03b3 > 0 and q \u2208 BRm(T ) such that for every \u03b5 > 0, there exist D,D\u2032 \u2208 D such that\nmax 1\u2264j\u2264d\n||D\u00b7,j \u2212D\u2032\u00b7,j ||2 \u2264 \u03b5 but |fD(q)\u2212 fD\u2032(q)| > \u03b3.\nIn other words, map F from D to FD with D \u2208 D 7\u2192 F (D) \u2208 FD is not globally Lipschitz.\nProposition 2 suggests that there are two ways to overcome the limitations when dealing with k-sparse vectors: either more restrictions shall be imposed on the class of dictionaries D or a different proof method has to be followed. The former approach was adopted by (Vainsencher et al., 2011) and (Gribonval et al., 2015b), who both use deterministic \u03b5-net arguments under an incoherence assumption on D and a lower RIP-property, respectively. In such a way, the authors restrict their analysis on a subspace of original space of all unit-norm column dictionaries.\nHere the latter approach is adopted: without additional assumptions on the dictionaries, standard tools from VC theory verify the UCEM property of FD. The main result is Proposition 3 which delivers an upper bound for s(F+D , n), the shatter coefficient of\nF+D := { {(x, t) \u2208 Rm+1 : fD(x) \u2265 t}; fD \u2208 FD } ;\n(34) the previous set collection is the family of all subgraphs of functions fD which belong to FD (as defined in (33)).\nProposition 3. The shatter coefficient s(F+D , n) of the collection of sets F+D , as defined in (34), is bounded above as\ns(F+D , n) \u2264 ( en \u03b1(m,d) )\u03b1(m,d) with \u03b1(m, d) independent of n and \u03b1(m, d) = ((m+ d)2 + 3(m+ d))/2 + 1.\nA direct use of Proposition\u2019s 3 bound in the popular VapnikChervonenkis\u2019s theorem (Theorem 12.5, (Devroye et al., 1997)) generates Theorem 2 and its byproduct Proposition 4. The latter characterizes the rate of convergence to zero of the difference of the sample average from the true mean of fD(X). All random variables appearing in Theorems 2, 3 and Proposition 4 below are assumed measurable.\nTheorem 2. Let fD : BRm(T ) \u2192 [0, eh(T )] for each fD 3 Although Proposition 2 has the same formulation as Theorem 20 of (Vainsencher et al., 2011), the latter cannot apply directly in our case except for k = 2. Proposition 2 clarifies through minor modifications what happens when k > 2.\nin the function class FD in (33) and let \u03b5 > 0. Then\nP { sup\nfD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 > }\n\u2264 8s(F+D , n)e \u2212 n\u03b52 32eh(T ) 2 .\n(35) Furthermore,\n\u221e\u2211 n=1 ( en \u03b1(m, d) )\u03b1(m,d) e \u2212 2n\u03b52 32eh(T ) 2 <\u221e (36)\nfor all \u03b5 > 0, and by the Borel-Cantelli lemma,\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223\u2192 0 (almost surely). (37)\nHence, function class FD has the UCEM property. Proposition 4. Assume the same setup as in Theorem 2 and let \u03b4 > 0. With probability at least 1\u2212 \u03b4, holds true that\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a log n n ) .\n(38)\nWhen fD(x) = eh(||x||2) and eh(t) = t2, the bounds for the absolute difference in the rhs of (38) in (Gribonval et al., 2015b) and (Vainsencher et al., 2011) are of order\nO (\u221a\nlogn n\n) and O (\u221a log( \u221a n)\nn\n) respectively.\nAlthough Proposition 4 is suboptimal compared to the latter, let us recall that Proposition 4 is valid for all dictionaries with unit-norm columns, in contrast to the last referenced bounds that do not cover the whole of space D. With slight modifications, Theorem 2 extends to Theorem 3 which covers any bounded lsc function g, including MCP or SCAD. Theorem 3. Let g : Rd \u2192 [0,+\u221e) bounded and lsc, and FD the function class with functions\nfD(x) := inf a\u2208Rd\neh(||x\u2212Da||2) + g(a). (39)\nLet \u03b5 > 0. Then\nP { sup\nfD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 > }\n\u2264 8s(F+D , n)e \u2212 n\u03b52 32eh(T ) 2 ,\n(40) where s(F+D , n) \u2264 ( en \u03b1(m,d) )\u03b1(m,d) and \u03b1(m, d) := ((m+ d)2 + 3(m+ d))/2 + 1. Furthermore, with probability at least 1\u2212 \u03b4, holds true that\nsup fD\u2208FD \u2223\u2223\u2223\u2223\u2223 1n n\u2211 i=1 fD(Xi)\u2212 \u222b fDd\u00b5 \u2223\u2223\u2223\u2223\u2223 \u2264 O (\u221a log n n ) .\n(41)"}, {"heading": "6. On the approximation error when m d", "text": "As already mentioned, our aim is to analyze the expected reconstruction error of the learned bases D\u0302n, R(D\u0302n) :=\u222b fD\u0302nd\u00b5, when D\u0302n is the (ERM)-estimator D\u0302n := argminD\u2208DRn(D). This reconstruction error decomposes into the estimation error est and the approximation error app as follows:\nR(D\u0302n) = R(D\u0302n)\u2212R(D\u2217)\ufe38 \ufe37\ufe37 \ufe38 := est +R(D\u2217)\ufe38 \ufe37\ufe37 \ufe38 := app , (42)\nwhere D\u2217 := argminD\u2208DR(D) is the optimal dictionary, the global minimizer of the population risk. The estimation error exists because D\u0302n is just an estimate for D\u2217. The approximation error measures the risk of restricting ourselves to D rather than to a larger family of matrices. The optimal choice for D\u0302n guarantees that both est and app are the smallest possible. The estimation error is bounded as\nest := R(D\u0302n)\u2212R(D\u2217) \u2264 2 sup D\u2208D |Rn(D)\u2212R(D)|.\n(43) In previous sections was proven that the rhs of (43) approaches zero as n \u2192 +\u221e and that, in view of (42), the reconstruction errorR(D\u0302n) is asymptotically equal to app. The approximation error does not depend on the sample size n; it is determined by the family of losses under study and the probability distribution of the data. In the k-sparse case, app is rarely zero, even for well behaved probability measures \u00b5. The authors in (Vovk, 2016), Section 24, show that the two objectives, of good data approximation and of sparsity of the combination vector a, are incompatible if the data distribution puts its mass far from any low dimensional subspace and in such cases app 6= 0.\nIn this section, assuming m d, app is considered a function of d. An upper bound for app as d\u2192 m, valid for any probability measure \u00b5 \u2208 P\u0304 , gives insights to the problem of approximating points in Rm with combinations of points lying on subspaces of dimension d. Following the approach in (Liu & Tao, 2016), we relate the optimal population risk R(D\u2217) to the quantization error of probability measure \u00b5.\nNext proposition is meaningful only in the case where g is the indicator function of special compact subsets of Rd, i.e., g(a) = \u03c7K(a) withK \u2282 Rd; \u03c7K(a) alternates between zero and infinity according to whether its actual argument belongs in K or not. Specifically, K is assumed to contain the basis vectors of the positive orthant. Assumptions (H1) to (H5) in Lemma 1 regarding h and its proximal map Ph remain valid, but is also required that\n(H6) Ph(t) = 0, when t \u2208 [\u2212\u03c4, \u03c4 ], (44)\nfor some predefined value \u03c4 > 0. These assumptions simplify the proof of Proposition 5: if they are true, then the\nMoreau envelope behaves like the quadratic function t2 in a neighborhood around zero. Although assumptions (H1) through (H6) may seem strict, they are valid for many univariate penalty functions and compact sets, such as the closed unit-norm balls in Rd. Proposition 5. Assume m d. Let the family of losses FD be defined as FD := {fD(x); D \u2208 D} with\nfD(x) := inf a\u2208Rd\neh(||x\u2212Da||2) + \u03c7K(a), (45)\nwhere \u03c7K is the indicator function of some compact setK \u2282 Rd that contains all basis vectors of the positive orthant, i.e., {ej}d1 \u2208 K and ej is the j-th column of the identity matrix.\nIf h satisfies the assumptions of Lemma 1 while its proximal map Ph satisfies assumptions (H1)-(H6), then for the approximation error it holds true that\nR(D\u2217) := inf D\u2208D\n\u222b fD(x)d\u00b5 \u2264 O(d\u22122/m). (46)\nThe bound in (46) depends on m and d. Despite being \u201cweak\u201d, as m\u22122/m \u2192 1, this upper bound provides an insight to the problem: when m is fixed, but sufficiently large, and d \u2192 m, the approximation error decreases as d increases, at rate O(d\u22122/m). Let us note here that the UCEM property for the family of risk functions FD as defined in Proposition 5 can be proved using elements from the proof of Theorem 1 in Section 3."}, {"heading": "7. Conclusions", "text": "This article is a theoretical analysis on the sample complexity of dictionary learning when the loss function to be minimized is the sum of the Moreau envelope of some univariate lsc function h on the real line and a regularization function g. We derive generalization bounds for a wide range of g, including the case of the indicator function of all k-sparse vectors. As a byproduct of this analysis is provided some intuition behind the popularity of loss functions under study in the context of \u201cgross outliers\u201d, that is, samples with arbitrary \u201clarge\u201d values. Finally, we comment on the approximation error of an ideal family of losses when the dimension m d, where d is the size of the dictionary. In the future, it would be interesting to characterize the differentiability properties of the losses under study. Such an analysis would have direct practical applications on the design of numerical optimization algorithms."}, {"heading": "Acknowledgements", "text": "The author thanks Athanasios P. Liavas and the anonymous reviewers for helpful comments and suggestions that improved the quality of the article."}], "year": 2018, "references": [{"title": "Sparse coding with anomaly detection", "authors": ["A. Adler", "M. Elad", "Y. Hel-Or", "E. Rivlin"], "venue": "Journal of Signal Processing Systems,", "year": 2015}, {"title": "VC dimension of ellipsoids", "authors": ["Y. Akama", "K. Irie"], "venue": "arXiv preprint arXiv:1109.4347,", "year": 2011}, {"title": "Outlier-aware dictionary learning for sparse representation", "authors": ["S. Amini", "M. Sadeghi", "M. Joneidi", "M. Babaie-Zadeh", "C. Jutten"], "venue": "In Machine Learning for Signal Processing (MLSP),", "year": 2014}, {"title": "Wavelet methods in statistics: Some recent developments and their applications", "authors": ["A. Antoniadis"], "venue": "Statistics Surveys,", "year": 2007}, {"title": "Quantization of probability distributions under norm-based distortion measures", "authors": ["S. Delattre", "S. Graf", "H. Luschgy", "G. Pages"], "venue": "Statistics & Decisions,", "year": 2004}, {"title": "A Probabilistic Theory of Pattern Recognition. Stochastic Modelling and Applied Probability", "authors": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "URL https://books.google. gr/books?id=uDgXoRkyWqQC", "year": 1997}, {"title": "Real analysis: modern techniques and their applications", "authors": ["G.B. Folland"], "year": 2013}, {"title": "Structured outlier models for robust dictionary learning", "authors": ["P.A. Forero", "S. Shafer", "J. Harguess"], "venue": "In Information Sciences and Systems (CISS),", "year": 2015}, {"title": "Sparsity-driven laplacian-regularized outlier identification for dictionary learning", "authors": ["P.A. Forero", "S. Shafer", "J.D. Harguess"], "venue": "IEEE Transactions on Signal Processing,", "year": 2017}, {"title": "Robust k-means: a theoretical revisit", "authors": ["A. Georgogiannis"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Sparse and spurious: dictionary learning with noise and outliers", "authors": ["R. Gribonval", "R. Jenatton", "F. Bach"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "Sample complexity of dictionary learning and other matrix factorizations", "authors": ["R. Gribonval", "R. Jenatton", "F. Bach", "M. Kleinsteuber", "M. Seibert"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "A distribution-free theory of nonparametric regression", "authors": ["L. Gy\u00f6rfi", "M. Kohler", "A. Krzyzak", "H. Walk"], "venue": "Springer Science & Business Media,", "year": 2006}, {"title": "Robust dictionary learning with capped l1-norm", "authors": ["W. Jiang", "F. Nie", "H. Huang"], "venue": "In IJCAI, pp", "year": 2015}, {"title": "Robust kernel dictionary learning using a whole sequence convergent algorithm", "authors": ["H. Liu", "J. Qin", "H. Cheng", "F. Sun"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "year": 2015}, {"title": "On the performance of manhattan nonnegative matrix factorization", "authors": ["T. Liu", "D. Tao"], "venue": "IEEE transactions on neural networks and learning systems,", "year": 2016}, {"title": "k-dimensional coding schemes in hilbert spaces", "authors": ["A. Maurer", "M. Pontil"], "venue": "IEEE Transactions on Information Theory,", "year": 2010}, {"title": "Sparsenet: Coordinate descent with nonconvex penalties", "authors": ["R. Mazumder", "J.H. Friedman", "T. Hastie"], "venue": "Journal of the American Statistical Association,", "year": 2012}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by V1", "authors": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision research,", "year": 1997}, {"title": "Convergence of stochastic processes", "authors": ["D. Pollard"], "venue": "Springer Science & Business Media,", "year": 1984}, {"title": "Variational analysis, volume 317", "authors": ["R.T. Rockafellar", "Wets", "R.J.-B"], "venue": "Springer Science & Business Media,", "year": 2009}, {"title": "The sample complexity of dictionary learning", "authors": ["D. Vainsencher", "S. Mannor", "A.M. Bruckstein"], "venue": "Journal of Machine Learning Research,", "year": 2011}, {"title": "A theory of learning and generalization", "authors": ["M. Vidyasagar"], "year": 2002}, {"title": "Measures of Complexity: Festschrift for Alexey Chervonenkis", "authors": ["V. Vovk"], "venue": "ISBN 3319357786,", "year": 2016}, {"title": "Minimizing nonconvex non-separable functions", "authors": ["Y. Yu", "X. Zheng", "M. Marchetti-Bowick", "E.P. Xing"], "venue": "In AISTATS,", "year": 2015}, {"title": "A survey of sparse representation: algorithms and applications", "authors": ["Z. Zhang", "Y. Xu", "J. Yang", "X. Li", "D. Zhang"], "venue": "IEEE access,", "year": 2015}, {"title": "Online nonnegative matrix factorization with outliers", "authors": ["R. Zhao", "V.Y.F. Tan"], "venue": "IEEE Transactions on Signal Processing,", "year": 2017}], "id": "SP:b8354caba43c0567e687d2e2a89a58a4d4a90e21", "authors": [{"name": "Alexandros Georgogiannis", "affiliations": []}], "abstractText": "This is a theoretical study on the sample complexity of dictionary learning with general type of reconstruction losses. The goal is to estimate a m \u00d7 d matrix D of unit-norm columns when the only available information is a set of training samples. Points x in R are subsequently approximated by the linear combination Da after solving the problem mina\u2208Rd \u03a6(x\u2212Da) + g(a) with function g being either an indicator function or a sparsity promoting regularizer. Here is considered the case where \u03a6(x) = inf z\u2208Rm ||x\u2212 z||2 + h(||z||2) and h is an even and univariate function on the real line. Connections are drawn between \u03a6 and the Moreau envelope of h. A new sample complexity result concerning the k-sparse dictionary problem removes the spurious condition regarding the coherence of D appearing in previous works. Finally comments are made on the approximation error of certain families of losses. The derived generalization bounds are of order O( \u221a log n/n).", "title": "The Generalization Error of Dictionary Learning with Moreau Envelopes"}