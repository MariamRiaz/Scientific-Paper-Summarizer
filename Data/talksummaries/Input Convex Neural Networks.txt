9	54	This is similar in nature to the structured prediction energy networks (SPENs) (Belanger & McCallum, 2016), which also use deep networks over the input and output spaces, with the difference being that in our setting f is convex in y, so the optimization can be performed globally.
10	26	Data imputation Similar to structured prediction but slightly more generic, if we are given some space Y we can learn a network f(y; θ) (removing the additional x inputs, though these can be added as well) that, given an example with some subset I missing, imputes the likely values of these variables by solving the optimization problem as above ŷI = argminyI f(yI , yĪ ; θ) This could be used e.g., in image inpainting where the goal is to fill in some arbitrary set of missing pixels given observed ones.
11	16	Continuous action reinforcement learning Given a reinforcement learning problem with potentially continuous state and action spaces S × A, we can model the (negative) Q function, −Q(s, a; θ) as an input convex neural network.
14	22	Our main contributions are: we propose the ICNN architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art.
15	21	Energy-based learning The interplay between inference, optimization, and structured prediction has a long history in neural networks.
35	19	Here we more formally present different ICNN architectures and prove their convexity properties given certain constraints on the parameter space.
36	60	Our chief claim is that the class of (full and partial) input convex models is rich and lets us capture complex joint models over the input to a network.
40	24	The central result on convexity of the network is the following: Proposition 1.
48	26	Other linear operators like convolutions can be included in ICNNs without changing the convexity properties.
50	19	In the experiment that follow, we will explore ICNNs with both fully connected and convolutional layers, and we provide more detail about these additional architectures in Section A of the supplement.
51	46	The FICNN provides joint convexity over the entire input to the function, which indeed may be a restriction on the allowable class of models.
52	62	Furthermore, this full joint convexity is unnecessary in settings like structured prediction where the neural network is used to build a joint model over an input and output example space and only convexity over the outputs is necessary.
67	46	Exact inference in ICNNs Although it is not a practical approach for solving the optimization tasks, the inference problem for the networks presented above (where the nonlinear are either ReLU or linear units) can be posed as as linear program.
75	58	An alternative approach to gradient descent is the bundle method (Smola et al., 2008), also known as the epigraph cutting plane approach, which iteratively optimizes a piecewise lower bound on the function given by the maximum over a set of first-order approximations.
76	16	However, as, the traditional bundle method is not well suited to our setting (we need to evaluate a number of gradients equal to the dimension of x, and solve a complex optimization problem at each step) we have developed a new optimization algorithm for this domain that we term the bundle entropy method.
104	23	Max-margin structured prediction.
105	21	Although maxmargin structured prediction is a simple and well-studied approach (Tsochantaridis et al., 2005; Taskar et al., 2005), in our experiences using these methods within an ICNN, we had substantial difficulty choosing the proper margin scaling term (especially for domains with continuousvalued outputs), or allowing for losses other than the hinge loss.
107	19	In our final proposed approach, that of argmin differentiation, we propose to directly minimize a loss function between true outputs and the outputs predicted by our model, where these predictions themselves are the result of an optimization problem.
119	14	The gradients ∇θf(x, yi; θ) are standard neural network gradients, and further, can be computed in the same forward/backward pass as we use to compute the gradients for the bundle entropy method.
134	23	As a baseline, we use a fully-connected neural network with batch normalization and ReLU activation functions.
139	28	SPENs obtain a macro-F1 score of 0.422 on this task (Belanger & McCallum, 2016) and pose an interesting comparison point to ICNNs as they have a similar (but not identical) deep structure that is nonconvex over the input space.
142	49	As a test of the system on a structured prediction task over a much more complex output space Y , we apply a convolutional PICNN to face completion on the sklearn version (Pedregosa et al., 2011) of the Olivetti data set (Samaria & Harter, 1994), which contains 400 64x64 grayscale images.
160	15	We compare to Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015) and Normalized Advantage Functions (NAF) (Gu et al., 2016) as state-of-the-art offpolicy learning baselines.4 Table 3 shows the maximum test reward achieved by the different algorithms on these tasks.
163	34	NAF poses a particularly interesting comparison point to ICNNs.
164	59	In particular, NAF decomposes the Q function in terms of the value function an an advantage function Q(s, a) = V (s) + A(s, a) where the advantage function is restricted to be concave quadratic in the actions, and thus always has a closed-form solution.
165	26	In a sense, this closely mirrors the setup of the PICNN architecture: like NAF, we have a separate non-convex path for the s variables, and an overall function that is convex in a; however, the distinction is that while NAF requires that the convex portion be quadratic, the ICNN architecture allows any convex functional form.
166	78	As our experiments show, this representational power does allow for better performance of the resulting system, though the trade-off, of course, is that determining the optimal action in an ICNN is substantially more computationally complex than for a quadratic.
167	36	This paper laid the groundwork for the input convex neural network model.
169	27	Since many existing models already fit into this overall framework (e.g., CRF models perform an optimization over an output space where parameters are given by the output of a neural network), the proposed method presents an extension where the entire inference procedure is “learned” along with the network itself, without the need for explicitly building typical structured prediction architectures.
170	24	This work explored only a small subset of the possible applications of these network, and the networks offer promising directions for many additional domains.
