28	14	An open problem is to combine our techniques with the techniques of (Chaudhuri et al., 2015) and establish convergence rates for general unlabeled distributions.
29	25	Another interesting line of future work is to come up with other, realistic noise models that apply to maximum likelihood estimation problems such as regression and logistic regression, and determine when active learning can help under these noise models.
40	11	In the homoscedastic setting (i.e. σ(x)2 = σ for all x in (1)), MLE is known to give minimax optimal rates1.
43	16	In the heteroscedastic setting, it is easy to show that the standard least squares estimator is consistent.
48	13	When the weights are known, it has been shown that the weighted estimator is the “correct” estimator to study; in particular, it is the minimum variance unbiased linear estimator (Theorem 10.7, Greene (2002)).
50	12	This raises two important questions for which we provide answers in the subsequent sections.
51	12	What are the rates of convergence of the maximum likelihood estimator for the heteroscedastic model when the noise model, aka, f∗ is unknown?
55	29	from the underlying Px, a label budget n ≤ m, and access to label oracle O that generates responses yi according to the heteroscedastic noise model (2), we want an estimator β̂ of the regression model parameter β∗ such that the estimation error is small, i.e. ∥β̂ − β∗∥2 ≤ O(ϵ).
62	13	Even in this arguably simple setting, the rates for passive and active learning are a priori not clear, and the exercise turns out to be non-trivial.
64	34	In the standard (passive) learning setting, we sample n instances uniformly from the set U and compute the maximum likelihood estimator given in (4) with weights set to wi = 1/⟨f∗,xi⟩2.
78	47	Thus, if the noise model is known, the weighted least squares estimator can give a factor of d improvement in convergence rate.
85	9	Let X ∈ Rn×d where the rows xi are sampled i.i.d.
89	19	We give a sketch of the proof here (See Appendix B.2 for details).
106	21	Note that O(1/n) is the error for 1-dimensional problem and is much better than d2/n2 we get from uniform sampling.
126	10	We first discuss the implications of using the estimated f̂ in order to obtain the generalized least square estimator given in (4) and then present the active learning solution.
128	64	We first get a good estimator of f∗ (as in Lemma 4), and then obtain the weighted least squares estimator: β̂ = (XT ŴX)−1XT Ŵy, where Ŵ is the diagonal matrix of inverse noise variances obtained using the estimate f̂ with a small additive offset γ.
129	23	The procedure is presented in Algorithm 3.
130	12	Algorithm 3 can be thought of as a special case of the well-known iterative weighted least squares (i.e. with just one iteration), that has been studied in the past (Carroll et al., 1988).
134	15	Their analysis is not directly applicable to us for reasons two-fold: (i) they focus on using a maximum likelihood estimate of the parameters in the heteroscedastic noise model, and does not apply to our noise model (2), and (ii) their analysis relies the noise being smooth (for obtaining tighter Taylor series approximation).
139	16	When γ = 0, we get the weighted least squares estimator analogous to the one used in Algorithm 1.
148	33	Our algorithm follows the strategy of using a single-round of interaction (in light of the analysis presented in the passive learning setting) to achieve a good estimate of the underlying β∗ akin to the active MLE estimation algorithm studied by Chaudhuri et al. (2015).
149	19	Let f̂ denote an estimator of f∗ satisfying ∥f̂ − f∗∥2 ≤ ∆.
150	21	For a given τ , let L denote a set of |L| ≥ 2d log d instances sampled from m unlabeled instances U , such that |⟨f̂ ,xi⟩| ≤ τ , for all xi ∈ L, and let yi denote their corresponding labels.
152	13	The bound in the above theorem recovers the known variance case discussed in Theorem 2, where the estimation error ∆2 = 0 and the choice of τ = 2nm .
153	29	Compared to the passive learning error bound in Theorem 3, we hope to get leverage — as long we can choose τ sufficiently small, and yet guarantee that the number of samples m2 in Step 4 of Algorithm 4 is sufficiently large.
154	17	The following theorem shows that this is indeed the case, and that the proposed active learning solution achieves optimal learning rate.
160	32	Unlike in the case when noise model was known (Theorem 2), here we can not do better even with infinite unlabeled examples.
171	19	In case of active learning, the bounds in Theorem 2, for the case when m ≥ n2, suggest that we get an error rate of ∥β∗ − β̂∥ = O( dn2 ).
173	39	Turning to the noise estimation setting for passive learning, we see in Figure 1 (c) that the estimation error of β∗ as well as f∗ decay as √ d/n (as suggested by Theorem 3); for active learning, we see in Figure 1 (d) that the estimation error of β∗ is noticeably better, in particular, better than that of f∗, and approaches 1/ √ n as n becomes larger than d2.
178	25	In Figure 1 (e), we see that active learning with noise estimation gives a significant reduction in RMSE early on for WINE QUALITY.
