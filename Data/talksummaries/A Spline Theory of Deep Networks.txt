5	4	In this paper, we build a rigorous bridge between DNs and approximation theory via spline functions and operators.
6	15	We prove that a large class of DNs — including convolutional neural networks (CNNs) (LeCun, 1998), residual networks (ResNets) (He et al., 2016; Targ et al., 2016), skip connection networks (Srivastava et al., 2015), fully connected networks (Pal & Mitra, 1992), recurrent neural networks (RNNs) (Graves, 2013), and beyond — can be written as spline operators.
10	21	The max-affine spline connection provides a powerful portal through which to view and analyze the inner workings of a DN using tools from approximation theory and functional analysis.
11	12	Here is a summary of our key contributions: [C1] We prove that a large class of DNs can be written as a composition of MASOs, from which it follows immediately that, conditioned on the input signal, the output of a DN is a simple affine transformation of the input.
12	8	We illustrate in Section 4 by deriving a closed-form expression for the input/output mapping of a CNN.
13	9	[C2] The affine mapping formula enables us to interpret a MASO DN as constructing a set of signal-dependent, classspecific templates against which the signal is compared via a simple inner product.
14	21	In Section 5 we relate DNs directly to the classical theory of optimal classification via matched filters and provide insights into the effects of data memorization (Zhang et al., 2016).
15	5	[C3] We propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal to each other.
16	38	In Section 6, we show that this leads to significantly improved classification performance and reduced overfitting on standard test data sets like CIFAR100 with no change to the DN architecture.
17	23	[C4] The partition of the input space induced by a MASO links DNs to the theory of vector quantization (VQ) and K-means clustering, which opens up a new geometric avenue to study how DNs cluster and organize signals in a hierarchical fashion.
19	181	[C5] Leveraging the fact that a DN considers two signals to be similar if they lie in the same MASO partition region, we develop a new signal distance in Section 7.3 that measures the difference between their partition encodings.
20	49	The distance is easily computed via backpropagation.
21	33	A number of appendices in the Supplementary Material (SM) contain the mathematical setup and proofs.
22	101	A significantly extended account of these events with numerous new results is available in (Balestriero & Baraniuk, 2018).
23	96	A deep network (DN) is an operator fΘ : RD → RC that maps an input signal1 x ∈ RD to an output prediction ŷ ∈ RC as fΘ : RD → RC .
24	50	All current DNs can be written as a composition of L intermediate mappings called layers fΘ(x) = ( f (L) θ(L) ◦ · · · ◦ f (1) θ(1) ) (x), (1) where Θ = { θ(1), .
25	103	, θ(L) } is the collection of the network’s parameters from each layer.
26	67	This composition of mappings is nonlinear and non-commutative, in general.
27	30	A DN layer at level ` is an operator f (`) θ(`) that takes as input the vector-valued signal z(`−1)(x) ∈ RD(`−1) and produces the vector-valued output z(`)(x) ∈ RD(`) .
31	3	The signals z(`)(x) are typically called feature maps; it is easy to see that z(`)(x) = ( f (`) θ(`) ◦ · · · ◦ f (1) θ(1) ) (x), ` ∈ {1, .
35	11	An activation operator applies a scalar nonlinear activation function σ independently to each entry of its input, as in[ f (`) σ ( z(`−1)(x) ) ] k := σ ( [z(`−1)(x)]k ) , k = 1, .
36	55	Nonlinearities are crucial to DNs, since otherwise the entire network would collapse to a single global affine transform.
37	40	Three popular activation functions are the rectified linear unit (ReLU) σReLU(u) := max(u, 0), the leaky ReLU σLReLU(u) := max(ηu, u), η > 0, and the absolute value σabs(u) := |u|.
38	23	These three functions are both piecewise affine and convex.
40	2	These two functions are neither piecewise affine nor convex.
41	2	A pooling operator subsamples its input to reduce its dimensionality according to a sub-sampling policy ρ applied over a collection of input indices {Rk}K (`) k=1 (typically a small patch), e.g., max pooling[ f (`) ρ ( z(`−1)(x) ) ] k := max d∈R(`)k [ z(`−1)(x) ] d , k = 1, .
42	16	See (Balestriero & Baraniuk, 2018) for the definitions of average pooling, channel pooling, skip connections, and recurrent layers.
43	9	A DN layer f (`) θ(`) comprises a single nonlinear DN operator (non-affine to be precise) composed with any preceding affine operators lying between it and the preceding nonlinear operator.
45	4	For example, in a standard CNN, there are two different layers types: i) convolution-activation and ii) max-pooling.
46	45	We form the prediction ŷ by feeding fΘ(x) through a final nonlinearity g : RD(L) → RD(L) as in ŷ = g(fΘ(x)).
