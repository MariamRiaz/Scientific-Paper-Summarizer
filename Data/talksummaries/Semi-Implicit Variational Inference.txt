1	17	It introduces variational distribution Q over the latent variables to approximate the posterior (Jordan et al., 1999), and its stochastic version is scalable to big data (Hoffman et al., 2013).
10	13	Thus, one often resorts to density ratio estimation, which, however, not only adds an additional level of complexity into each iteration of the optimization, but also is known to be a very difficult problem, especially in high-dimensional settings (Sugiyama et al., 2012).
16	29	With a flexible variational family and novel optimization, SIVI bridges the accuracy gap of posterior estimation between VI and Markov chain Monte Carlo (MCMC), which can accurately characterize the posterior using MCMC samples, as will be demonstrated in a variety of Bayesian inference tasks.
20	13	Since one may show log p(x) = ELBO + KL(q(z | )||p(z |x)), where ELBO = E z⇠q(z| )[log q(z| ) log p(x, z)], (1) minimizing KL(q(z | )||p(z |x)) is equivalent to maximizing the ELBO (Bishop & Tipping, 2000; Blei et al., 2017).
24	25	Marginalizing the intermediate variable out, we can view z as a random variable drawn from distribution family H indexed by variational parameter , expressed as H = h (z) : h (z) = R q(z | )q ( )d .
31	22	SIVI is related to the hierarchical variational model (Ranganath et al., 2016; Maaløe et al., 2016; Agakov & Barber, 2004) in using a hierarchical variational distribution, but, as discussed below, differs from it in allowing an implicit mixing distribution q ( ) and optimizing the variational parameter via an asymptotically exact surrogate ELBO.
32	56	Note as long as q ( ) can degenerate to delta function 0 ( ) for arbitrary 0, the semi-implicit variational family H is a strict expansion of the original Q = {q(z | 0)} family, that is, Q ✓ H. For MFVI that assumes q(z | ) = Q m q(zm | m), this expansion significantly helps restore the dependencies between zm if m are not imposed to be independent between each other.
33	15	While restricting q(z | ) to be explicit, SIVI introduces a mixing distribution q ( ) to enhance its representation power.
35	40	More specifically, an implicit distribution (Mohamed & Lakshminarayanan, 2016; Tran et al., 2017), consisting of a source of randomness q(✏) for ✏ 2 Rg and a deterministic transform T : Rg !
36	20	Rd, can be constructed as = T (✏), ✏ ⇠ q(✏), with PDF q ( ) = @ @ 1 .
38	20	When T (·) is chosen as a deep neural network, thanks to its high modeling capacity, q ( ) can be highly flexible and the dependencies between the elements of can be well captured.
40	32	Using implicit distributions with intractable PDF increases flexibility but substantially complicates the optimization problem for VI, due to the need to estimate log density ratios involving intractable PDFs, which is particularly challenging in high dimensions (Sugiyama et al., 2012).
43	39	To optimize the variational parameters of SIVI, below we first derive for the ELBO a lower bound, climbing which, however, could drive the mixing distribution q ( ) towards a point mass density.
54	19	A direct optimization of the lower bound L in (4), however, can suffer from degeneracy, as shown in the proposition below.
59	17	Note that BK 0, with BK = 0 if and only if K = 0 or q ( ) degenerates to a point mass density.
81	24	One may further consult Rainforth et al. (2018) to help select K, J , and ˜K for SIVI.
83	18	As for ⇠, if ⇠ 6= ;, one may learn it as in Algorithm 1, set it empirically, or fix it at the value learned by another algorithm such as MFVI.
84	30	In summary, SIVI constructs a flexible variational distribution by mixing a (potentially) implicit distribution with an explicit one, while maintaining tractable optimization via the use of an asymptotically exact surrogate ELBO.
86	32	In particular, for a conditionally conjugate exponential family model, MFVI has an analytic ELBO, and its variational distribution can be directly used as the q ⇠ (z | ) of SIVI.
87	33	As in Appendix A, introducing a density ratio as r ⇠, (z, ✏, ✏ (1:K) ) = q ⇠ (z |T (✏))) q ⇠ (z |T (✏))+ PK k=1 q⇠(z |T (✏(k))) K+1 , we approximate the gradient of LK with respect to as r LK ⇡ 1J PJ j=1 r E z⇠q ⇠ (z |T (✏j))[log q ⇠ (z |T (✏j)) p(x,z) ] +r log r ⇠, (zj , ✏j , ✏ (1:K) ) + [r log q ⇠ (zj |T (✏j))] log r⇠, (zj , ✏j , ✏(1:K)) , (11) where the first summation term is equivalent to the gradient of MFVI’s ELBO, both the second and third terms correct the restrcitions of q ⇠ (z |T (✏j)), and log r⇠, (z, ✏, ✏(1:K)) in the third term is expected to be small regardless of convergence, effectively mitigating the variance of score function gradient estimation that is usually high in basic black-box VI; r ⇠ LK can be approximated in the same manner.
112	15	We first show the expressiveness of SIVI by approximating various target distributions.
113	16	As listed in Table 1, the conditional layer of SIVI is chosen to be as simple as an isotropic Gaussian (or log-normal) distribution N (0, 20I).
142	22	For each test data xN+i, we calculate the predictive probabilities 1/(1 + e x T N+i j ) for all j and compute its sample mean, and sample standard deviation that measures the uncertainty of the predictive distribution p(yN+i = 1 |xN+i, {xi, yi}1,N ).
143	20	As shown in Figure 5, even with a full covariance matrix, the MVN variational distribution inferred by MFVI underestimates the uncertainty in out-of-sample prediction, let alone with a diagonal one, whereas SIVI, mixing the MVN with an MLP based implicit distribution, closely matches MCMC in uncertainty estimation.
145	21	Further examining the correlation coefficients of , shown in Figure 7, all the univariate marginals, shown in Figure 11 of Appendix D, and additional results, show in Figures 12-17 of Appendix D, it is revealed that SIVI well characterizes the posterior distribution of and is only slightly negatively affected if its explicit layer is restricted with a diagonal covariance matrix, whereas MFVI with a diagonal/full covariance matrix and SVGD all misrepresent uncertainty.
159	49	We use 55,000 for training and use the 10,000 observations in the testing set for performance evaluation.
160	29	Similar to existing VAEs, we choose Bernoulli units, linked to a fully-connected neural network with two 500-unit hidden layers, as the decoder.
162	36	In comparison to other VAEs with a comparable single-stochastic-layer decoder, SIVAE achieves state-of-the-art performance by mixing an MVN with an implicit distribution defined as in (12) to construct a flexible encoder, whose marginal variational distribution is no longer restricted to the MVN distribution.
165	28	By designing a surrogate evidence lower bound that is asymptotically exact, SIVI establishes an optimization problem amenable to gradient ascent, without compromising the expressiveness of its semi-implicit variational distribution.
166	17	Flexible but simple to optimize, SIVI approaches the accuracy of MCMC in quantifying posterior uncertainty in a wide variety of inference tasks, and is not constrained by conjugacy, often runs faster, and can generate iid posterior samples on the fly via the inferred stochastic variational inference network.
