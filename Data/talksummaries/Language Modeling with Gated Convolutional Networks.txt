0	43	Statistical language models estimate the probability distribution of a sequence of words by modeling the probability of the next word given preceding words, i.e. P (w0, .
2	20	Language models are a critical part of systems for speech recognition (Yu & Deng, 2014) and machine translation (Koehn, 2010).
6	19	The current state of the art for language modeling is based on long short term memory networks (LSTM; Hochreiter et al., 1997) which can theoretically model arbitrarily long dependencies.
18	59	We also evaluate the ability of our models to deal with long-range dependencies on the WikiText-103 benchmark for which the model is conditioned on an entire paragraph rather than a single sentence and we achieve a new state-of-the-art on this dataset (Merity et al., 2016).
20	29	In this paper we introduce a new neural language model that replaces the recurrent connections typically used in recurrent networks with gated temporal convolutions.
22	12	,hN ] of the context for each word w0, .
33	11	We address this by shifting the convolutional inputs to prevent the kernels from seeing future context (Oord et al., 2016a).
47	29	Without these gates, information could easily vanish through the transformations of each timestep.
49	67	Therefore, we consider models possessing solely output gates, which allow the network to control what information should be propagated through the hierarchy of layers.
50	28	We show this mechanism to be useful for language modeling as it allows the model to select which words or features are relevant for predicting the next word.
51	52	Parallel to our work, Oord et al. (2016b) have shown the effectiveness of an LSTM-style mechanism of the form tanh(X∗W+b)⊗σ(X∗V+c) for the convolutional modeling of images.
52	21	Later, Kalchbrenner et al. (2016) extended this mechanism with additional gates for use in translation and character-level language modeling.
57	15	In con- trast, the gradient of the gated linear unit ∇[X⊗ σ(X)] = ∇X⊗ σ(X) + X⊗ σ′(X)∇X (3) has a path ∇X ⊗ σ(X) without downscaling for the activated gating units in σ(X).
58	18	This can be thought of as a multiplicative skip connection which helps gradients flow through the layers.
59	80	We compare the different gating schemes experimentally in Section §5.2 and we find gated linear units allow for faster convergence to better perplexities.
63	20	The data is based on an English corpus of 30, 301, 028 sentences whose order has been shuffled.
64	32	Second, WikiText-103 is a smaller dataset of over 100M tokens with a vocabulary of about 200K words (Merity et al., 2016).
67	19	On the Google Billion Word corpus each sequence is a single sentence, while on WikiText-103 a sequence is an entire paragraph.
70	23	We implement our models in Torch (Collobert et al., 2011) and train on Tesla M40 GPUs.
73	11	The gradients are then summed using Nvidia NCCL.
74	16	The multi-GPU setup allowed us to train models with larger hidden units.
76	58	While the cost in terms of memory is storing another vector of the size of the parameters, it increases the speed of convergence significantly with minimal additional computation compared to standard stochastic gradient descent.
80	13	Gradient clipping is found using a spherical trust region ∆θ∗ = argmin s. t. ‖∆θ‖≤ f(θ) +∇fT∆θ = −max(‖∇f‖, ) ∇f ‖∇f‖ .
83	32	We found good hyper-parameter configurations by crossvalidating with random search on a validation set.
88	22	In terms of optimization, we initialize the layers of the model with the Kaiming initialization (He et al., 2015b), with the learning rate sampled uniformly in the interval [1., 2.
92	66	In this section, we compare strong LSTM and RNN models from the literature to our gated convolutional approach on two datasets.
101	14	Note that these results can be improved by either using mixtures of experts (Shazeer et al., 2017) or ensembles of these models.
102	24	Another relevant concern is if the GCNN’s fixed context size can thoroughly model long sequences.
103	72	On Google Bil- ∗appeared after submission lion Word, the average sentence length is quite short — only 20 words.
104	26	We evaluate on WikiText-103 to determine if the model can perform well on a dataset where much larger contexts are available.
