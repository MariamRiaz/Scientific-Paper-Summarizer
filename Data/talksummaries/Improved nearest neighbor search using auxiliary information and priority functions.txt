0	18	Nearest neighbor search is extensively used as a subroutine for k-nn classifier and many complex graph based methods in wide range of domains such as machine learning, computer vision, pattern recognition and robotics.
1	30	The basic problem of nearest neighbor search is as follows: given a set of n d-dimensional data points S = {x1, x2, .
2	29	, xn} ⊂ Rd and a query point q ∈ Rd, one needs to build a data structure using S , so that nearest point (when measured using appropriate distance metric) to q from S can be found quickly.
3	12	The naive linear time solution, that scans through each data point xi ∈ S, often becomes impractical for large n and d. Towards this end, in recent years there has been a conscious effort towards designing sub-linear time algorithms for solving this problem.
4	11	Most of these efforts can broadly be classified into two groups, namely, (a) tree based methods ((Bentley, 1975; Uhlmann, 1991; Ciaccia et al., 1997; Katayama & Satoh, 1997; Liu et al., 2004; Beygelzimer et al., 2006; Dasgupta & Sinha, 2013; Sinha, 2015; Sinha & Keivani, 2017; Babenko & Lempitsky, 2017)) and (b) methods based on hashing ((Gionis et al., 1999; Andoni & Indyk, 2008; Datar et al., 2004)).
5	19	Basic principle for both these approaches is to quickly retrieve a smaller subset S ′ ⊂ S and perform linear scan within S ′ to answer a nearest neighbor search query.
8	21	Nearest neighbor search methods that use space partition trees hierarchically partition the search space into a large number of regions corresponding to tree leaves, where each leaf contains only a small subset of the database points.
17	70	In comparison, a guided DFS search is often applied in metric trees (Uhlmann, 1991; Ciaccia et al., 1997; Omohundro, 1990), where a single space partition tree is used but data points are retrieved from multiple leaf nodes by following a depth first search strategy.
18	53	Typically, at each internal node, one of the two child nodes is visited first, if it is more likely (based on some heuristic) to contain the nearest neighbor, before visiting the other child node.
20	16	While guided DFS search strategy works reasonably well for low dimensional datasets (data dimension is less than 30) (Liu et al., 2004), their performance degrades with increasing data dimension and for high dimensional dataset their performance often becomes no better than linear scan of the entire database.
64	11	Each internal node of an RPT stores a random projection direction and a scalar split point.
65	28	Once query reaches an internal node, it is first projected onto the random projection direction stored at this internal node, and depending on whether the resulting 1-d projection lies to the left or right side of the stored split point, the corresponding branch (left or right) is taken.
67	15	It is shown in (Dasgupta & Sinha, 2015) that the probability that an RPT fails to find exact nearest neighbors of a query can be restricted to an arbitrary constant, if the following sufficient conditions are met: (a) the leaf nodes contain number of data points exponential in the intrinsic dimension of the dataset and (b) data points are drawn from a underlying probability distribution that satisfies doubling measure condition.
68	19	While the intrinsic dimension of real world datasets is often much smaller than its ambient dimension, accurately estimating intrinsic data dimension is often an extremely difficult task.
69	20	In addition, due to its exponential dependence, unless intrinsic data dimension is really low, sufficient leaf node size (to ensure small failure probability) can be very high yielding a large number of retrieve points.
74	30	In our first approach, we introduce a modified defeatist search strategy, where, at each internal node we store auxiliary information to compensate for the fact that while routing a query from root node to a leaf node, only one of the two branches is chosen at each internal node.
75	18	The stored auxiliary information at any internal node aims to compensate for the unvisited subtree rooted at this node by identifying a small set of candidate nearest neighbors that lie in this unvisited subtree.
76	46	Note that this small set of candidate nearest neighbor points otherwise would not be considered had we adopted the traditional defeatist search strategy.
78	30	A natural question that arises is, what kind of auxiliary information can we store to achieve this?
79	36	On one hand, we would like to ensure that auxiliary information does not increase space complexity of the data structure significantly, while on the other hand, we would like the candidate nearest neighbors identified at each internal node along the query routing path to be query dependent (so that not the same candidate nearest neighbors are used for every query), and therefore, this additional query dependent computation (for each query) needs to be performed quickly without significantly increasing overall query processing time.
81	47	, xn} ⊂ Rd, maximum number of data points in leaf node n0, auxiliary index size c, m independent random vectors {V1, .
88	15	Note that if we have an ordering of the distances of points from S to query q, then any one dimensional random projection has the property that upon projection, this ordering (of projected points) is perturbed locally near projected q but is preserved globally with high probability as shown below.
92	12	If q and points from S are projected onto a direction U chosen at random from a unit sphere, then for any 1 ≤ k < |S|, the probability that there exists a subset of k points from S that are all not more than |U>(q− x(1))| distance away from U>q upon projection is at most 1k ∑|S| i=1 ‖q−x(1)‖2 ‖q−x(i)‖2 .
95	60	In other words, with high probability, true nearest neighbor of any query will remain close to the query even after projection, since distance between two points does not increase upon projection, however points which were far away from query in original high dimension may come closer to the query upon projection.
98	41	At any internal node of an RPT, suppose the projected q lies on left side of the split point (so that left child node falls on the query routing path).
100	31	Therefore, to identify q’s true nearest neighbor, one possibility is to store actual c high dimensional points (where c is some pre-defined fixed number) which are closest c points (upon projection) to the right side of the split point as auxiliary information for this node.
104	10	To prune out these nearest neighbor false positives for each query, if we attempt to compute actual distance from q to these c points in original high dimensions and keep only closest points based on actual distance as candidate nearest neighbors, this extra computation, will increase query time for large d. To alleviate this, we rely on celebrated Johnson Lindenstrauss lemma (Johnson & Lindenstrauss, 1984) which says that if we use m = O ( log(c+1) 2 ) random projections then pairwise distance between c points and q are preserved in Rm within a multiplicative factor of (1± ) of the original high dimensional distance in Rd.
136	30	We use this idea to design our next priority function.
142	17	Ideally, if doppmin ≤ dsamemin , the priority score should increase and vice versa because one of the c points on the unexplored side is closer to the query compared to the c points on the same side of the query.
144	14	We note that, while a priority function similar to fpr1 has been proposed recently (Babenko & Lempitsky, 2017), fpr2 is new.
