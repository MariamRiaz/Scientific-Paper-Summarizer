0	46	Systems for automatic assessment of spontaneous spoken language proficiency (Fig.
1	21	1) are becoming increasingly important to meet the demand for English second language learning.
2	16	Such systems are able to provide throughput and consistency which are unachievable with human examiners.
4	14	In addition, candidates of the same skill level will have different accents, voices, mispronunciations, and sentence construction errors.
6	7	It is therefore impossible in practice to observe all these variants in training.
11	37	However, such models reject candidates based on whether they can be scored at all, rather than an automatic grader’s uncertainty 1 in its predictions.
19	15	Hence, a novel approach to explicitly model uncertainty is proposed in which the DDN is trained in a multitask fashion to model a low variance real data distribution and a high variance artificial data distribution which represents candidates with unseen characteristics.
27	15	Alternatively, a grader can be constructed using Deep Neural Networks (DNNs) which have a very flexible architecture and scale well to large data sets.
29	24	Uncertainty estimates for DNNs can be computed using a Monte-Carlo ensemble approximation to Eq.
31	8	Recent work by Gal and Ghahramani (2016) showed that MCD is equivalent to approximate variational inference in GPs, and can be used to yield meaningful uncertainty estimates for DNNs.
36	17	1, a DNN can be modified to produce a prediction of both a mean and a variance: µg(x) = fµ(x;M) σ2g(x) = fσ2(x;M) (7) (8) parametrising a normal distribution over grades conditioned on the input, similar to a GP.
39	12	The variance of the DDN represents the natural spread of grades at a given input.
44	39	Two normal distributions are constructed: a low-variance real (training) data distribution pD and a high-variance artificial data distribution pN, which models data outside the real training data region.
45	23	The DDN needs to model both distributions in a multi-task (MT) fashion.
46	17	The loss function for training the DDN with explicitly specified uncertainty is the expectation over the training data of the KL divergence between the distribution it parametrizes and both the real and artificial data distributions: L = Ex̂[KL(pD||p(g|x̂;M)] + α · Ex̃[KL(pN||p(g|x̃;M)] (9) where α is the multi-task weight.
48	26	First, a standard DDN M0 is trained, then a DDNM is instantiated using the parameters ofM0 and trained in a multi-task fashion.
49	10	The real data distribution pD is defined byM0 (Eq.
51	41	The target variance σ2(x̃) should depend on the similarity of x̃ to the training data.
52	47	Here, this variance is modelled by the squared normalized Euclidean distance from the mean of x̂, with a diagonal covariance matrix, scaled by a hyperparameter λ.
53	27	The artificial inputs x̃ need to be different to, but related to the real data x̂.
55	8	A simple approach to generating x̃ is to use a Factor Analysis (FA) (Murphy, 2012) model trained on x̂.
58	16	AUCRR = AUCvar AUCmax (12) As previously stated, the operating scenario is to use a model’s estimate of the uncertainty in its prediction to reject candidates to be assessed by human graders for high-stakes tests, maximizing the increase in performance while rejecting the least number of candidates.
60	6	As the rejection fraction is increased, model predictions are replaced with human scores in some particular order, increasing overall correlation with human graders.
62	8	The expected random performance curve is a straight line from the base predictive performance to 1.0, representing rejection in a random order.
63	47	The optimal rejection curve is constructed by rejecting predictions in order of decreasing mean square error relative to human graders.
64	12	A rejection curve derived from a model should sit between the random and optimal curves.
65	10	In this work, model rejection is in order of decreasing predicted variance.
66	38	The following metrics are used to assess and compare models: Pearson Correlation Coefficient (PCC) with human graders, the standard performance metric in assessment (Zechner et al., 2009; Higgins et al., 2011); 10% rejection PCC, which illustrates the predictive performance at a partic- ular operating point, i.e. rejecting 10% of candidates; and Area under a model’s rejection curve (AUC) (Fig 3).
67	76	However, AUC is influenced by the base PCC of a model, making it difficult to compare the rejection performance.
70	11	12), is the ratio of the areas under the actual (AUCvar) and optimal (AUCmax) rejection curves relative to the random rejection curve.
