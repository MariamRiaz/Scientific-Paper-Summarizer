8	10	The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties.
9	15	MAEGE requires no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammaticality.
10	52	For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each.
11	16	It then computes the correlation of the induced partial order with the metric-induced rankings.
12	3	MAEGE addresses many of the problems with existing methodology: • Human rankings yield low inter-rater and intra-rater agreement (§3).
13	12	Indeed, Choshen and Abend (2018a) show that while annotators often generate different corrections given a sentence, they generally agree on whether a correction is valid or not.
14	11	Unlike CHR, MAEGE bases its scores on human corrections, rather than on rankings.
15	36	• CHR uses system outputs to obtain human rankings, which may be misleading, as systems may share similar biases, thus neglecting to evaluate some types of valid corrections (§7).
17	116	• The difficulty in handling ties is addressed by only evaluating correction pairs where one contains a sub-set of the errors of the other, and is therefore clearly better.
18	36	• MAEGE uses established statistical tests for determining the significance of its results, thereby avoiding ad-hoc methodologies used in CHR to tackle potential biases in human rankings (§5, §6).
19	14	In experiments on the standard NUCLE test set (Dahlmeier et al., 2013), we find that MAEGE often disagrees with CHR as to the quality of existing metrics.
20	31	For example, we find that the standard GEC metric, M2, is a poor predictor of corpuslevel ranking, but a good predictor of sentencelevel pair-wise rankings.
21	152	The best predictor of corpus-level quality by MAEGE is the referenceless LT metric (Miłkowski, 2010; Napoles et al., 2016b), while of the reference-based metrics, GLEU (Napoles et al., 2015) fares best.
22	53	In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivities of the metrics to corrections of different types, which to our knowledge is a novel contribution of this work.
23	128	Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Section 7).
24	40	The importance of interpretability and detail in evaluation practices (as opposed to just providing bottom-line figures), has also been stressed in MT evaluation (e.g., Birch et al., 2016).
25	74	We turn to presenting the metrics we experiment with.
26	100	The standard practice in GEC evaluation is to define differences between the source and a correction (or a reference) as a set of edits (Dale et al., 2012).
27	126	An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type.
28	64	For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit results in “I want a book”.
29	39	Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others.
30	9	We denote the examined set of metrics with METRICS.
32	68	While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016).
35	77	Recently, it was updated to better address multiple references (Napoles et al., 2016a).
36	32	GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference.
