0	40	Topic models comprise a family of methods for uncovering latent structure in text corpora, and are widely used tools in the digital humanities, political science, and other related fields (Boyd-Graber et al., 2017).
2	60	In the real world, however, most documents have non-textual attributes such as author (Rosen-Zvi et al., 2004), timestamp (Blei and Lafferty, 2006), rating (McAuliffe and Blei, 2008), or ideology (Eisenstein et al., 2011; Nguyen et al., 2015b), which we refer to as metadata.
4	44	Two models of note are supervised LDA (SLDA; McAuliffe and Blei, 2008), which jointly models words and labels (e.g., ratings) as being generated from a latent representation, and sparse additive generative models (SAGE; Eisenstein et al., 2011), which assumes that observed covariates (e.g., author ideology) have a sparse effect on the relative probabilities of words given topics.
5	36	The structural topic model (STM; Roberts et al., 2014), which adds correlations between topics to SAGE, is also widely used, but like SAGE it is limited in the types of metadata it can efficiently make use of, and how that metadata is used.
7	46	The ability to create variations of LDA such as those listed above has been limited by the expertise needed to develop custom inference algorithms for each model.
9	47	In this work, we take advantage of recent advances in variational methods (Kingma and Welling, 2014; Rezende et al., 2014; Miao et al., 2016; Srivastava and Sutton, 2017) to facilitate approximate Bayesian inference without requiring model-specific derivations, and propose a general neural framework for topic models with metadata, SCHOLAR.1 SCHOLAR combines the abilities of SAGE and SLDA, and allows for easy exploration of the following options for customization: 1.
18	25	In presenting this particular model, we emphasize not only its ability to adapt to the characteristics of the data, but the extent to which the VAE approach to inference provides a powerful framework for latent variable modeling that suggests the possibility of many further extensions.
49	41	Instead of using a Dirichlet prior as in LDA, we employ a logistic normal prior on θ as in Srivastava and Sutton (2017) to facilitate inference (§3.2): we draw a latent variable, r,5 from a multivariate normal, and transform it to lie on the simplex using a softmax transform.6 The generative story is shown in Figure 1a and described in equations below: For each document i of length Ni: # Draw a latent representation on the simplex from a logistic normal prior: ri ∼ N (r | µ0(α), diag(σ20(α))) θi = softmax(ri) # Generate words, incorporating covariates: ηi = fg(θi, ci) For each word j in document i: wij ∼ p(w | softmax(ηi)) # Similarly generate labels: yi ∼ p(y | fy(θi, ci)), where p(w | softmax(ηi)) is a multinomial distribution and p(y | fy(θi, ci)) is a distribution appropriate to the data (e.g., multinomial for categorical labels).
55	26	The background is included to account for common words with approximately the same frequency across documents, meaning that the B∗ weights now represent both positive and negative deviations from this background.
59	29	(3) We can choose to ignore various parts of this model, if, for example, we don’t have any labels or observed covariates, or we don’t wish to use interactions or sparsity.8 Other generator networks could also be considered, with additional layers to represent more complex interactions, although this might involve some loss of interpretability.
65	37	As in conventional variational inference, we assume a variational approximation to the posterior, qΦ(ri | wi, ci,yi), and seek to minimize the KL divergence between it and the true posterior, p(ri | wi, ci,yi), where Φ is the set of variational parameters to be defined below.
67	46	(4) As in the original VAE, we will encode the parameters of our variational distributions using a shared multi-layer neural network.
69	36	Incorporating labels and covariates to the inference network used by Miao et al. (2016) and Srivastava and Sutton (2017), we use: πi = fe([Wxxi; Wcci; Wyyi]), (5) µi = Wµπi + bµ, (6) logσ2i = Wσπi + bσ, (7) where xi is a V -dimensional vector representing the counts of words in wi, and fe is a multilayer perceptron.
71	22	This approach means that the expectations in Equation 4 are intractable, but we can approximate them using sampling.
78	64	In addition to inferring latent topics, our model can both infer latent representations for new documents and predict their labels, the latter of which was the motivation for SLDA.
80	64	With the VAE framework, by contrast, the encoder network (Equations 5–7) can be used to directly estimate the posterior distribution for each test document, using only a forward pass (no iterative optimization or sampling).
81	35	If not using labels, we can use this approach directly, passing the word counts of new documents through the encoder to get a posterior qΦ(ri | wi, ci).
82	40	When we also include labels to be predicted, we can first train a fully-observed model, as above, then fix the decoder, and retrain the encoder without labels.
115	39	SAGE, by contrast, attains very high levels of sparsity, but at the cost of worse perplexity and coherence than LDA.
118	51	Adding regularization to encourage sparse topics has a similar effect as in SAGE, leading to worse perplexity and coherence, but it does create sparse topics.
119	27	Interestingly, initializing the encoder with pretrained word2vec embeddings, and not updating them returned a model with the best internal coherence of any model we considered for IMDB and Yahoo answers, and the second-best for 20 newsgroups.
120	34	The background term in our model does not have much effect on perplexity, but plays an important role in producing coherent topics; as in SAGE, the background can account for common words, so they are mostly absent among the most heavily weighted words in the topics.
128	34	Further, any neural network that is successful for text classification could be incorporated into fy and trained end-to-end along with topic discovery.
129	26	We demonstrate how our model might be used to explore an annotated corpus of articles about immigration, and adapt to different assumptions about the data.
131	51	We first consider using the annotations as a label, and train a joint model to infer topics relevant to the tone of the article (pro- or anti-immigration).
135	58	Table 3 shows a set of topics learned from the immigration data, along with the most highly-weighted words in the corresponding tone-topic interaction terms.
138	22	Finally, we incorporate both the tone annotations and the year of publication of each article, treating the former as a label and the latter as a covariate.
152	40	Our model demonstrates the tradeoff between perplexity, coherence, and sparsity, and outperforms SLDA in predicting document labels.
153	30	Furthermore, the flexibility of our model enables intriguing exploration of a text corpus on US immigration.
154	91	We believe that our model and code will facilitate rapid exploration of document collections with metadata.
