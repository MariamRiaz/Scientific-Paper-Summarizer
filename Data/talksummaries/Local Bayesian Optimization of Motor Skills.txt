0	88	Recent advances in deep reinforcement learning, supported by the ability of generating and processing large amounts of data, allowed impressive achievements such as playing Atari at human level (Mnih et al., 2015) or mastering the game of Go (Silver et al., 2016).
1	30	In robotics however, sample complexity is paramount as sample generation on physical systems cannot be sped up and can cause wear and damage to the robot when excessive (Kober et al., 2013).
2	40	Relying on a simulator to carry the learning will inevitably result in a reality gap, since mechanical forces such as stiction are hard to accurately model.
4	56	Bayesian optimization is best known as a black-box global optimizer (Brochu et al., 2010; Shahriari et al., 2016).
5	68	It was shown to be efficient for several function landscapes (Jones, 2001), real world scenarios such as the automatic tuning of machine learning algorithms (Bergstra et al., 2011; Snoek et al., 2012; Feurer et al., 2015) or robotics and control (Lizotte et al., 2007; Wilson et al., 2014; Calandra et al., 2016) and several of its variants have convergence guaranties to a global optimum (Vazquez & Bect, 2010; Bull, 2011).
10	47	Several recent approaches trying to scale Bayesian optimization to higher dimensions assume additional structure of the objective function.
13	16	In this paper, we assume prior knowledge on the location of the optimum—given by an initial solution and a confidence on the optimality thereof—and leverage Bayesian optimization in a local manner to improve over this solution.
14	17	We are especially interested in the application of our algorithm to the optimization of motor skills since i) evaluating the policy return is expensive on physical systems and will likely dominate the computational budget of the optimization process; as such, sample efficient algorithms such as Bayesian optimization are desirable ii) robotics applications are typically high dimensional and global optimization might be prohibitively expensive iii) an initial solution can often be obtained through the use of imitation learning (Argall et al., 2009) or by a preliminary optimization on a surrogate model such as a simulator.
40	13	(xN , yN )} of parameter-evaluation pairs and the goal is to minimize the cumulative regret 1N ∑ i f(x ?)
41	11	− yi for some global maximizer x?
43	24	Prior knowledge on an optimum’s location x?
46	16	In an RL context, an informative prior can often be obtained from human generated data or from a simulator.
47	22	Specifically, we assume that the mean µ0 of π0 is obtained by imitation learning if near-optimal demonstrations are available or by a preliminary optimization on a less accurate but inexpensive model of the system dy- namics.
53	11	1, where the prior πn(x) on the optimality of x is weighted by the likelihood p(x = x?|Dn) of x being optimal according to Dn.
60	31	The next subsections give a detailed presentation of both the search distribution update and the sampling procedure from p?n.
61	29	The search distribution in our algorithm is updated such as to minimize the KL divergence between πn and p?n.
70	15	(1) is impacted by three factors.
85	13	We now show that the optimization problem in our algorithm can be phrased as the optimization problem solved by MORE for the equality entropy constraint; while only a small modification of the dual minimization is required for the inequality entropy constraint.
99	42	From Proposition 1 and following the derivations in (Abdolmaleki et al., 2015), the search distribution πn+1 solution of the optimization problem in Sec.
114	16	Samples from p?n are generated by i) sampling a hyper-parameter vector φ from the posterior distribution p(φ|Dn) using slice sampling (Murray & Adams, 2010), ii) sampling a function from the GP posterior p(f̃ |Dn,φ) and iii) returning the argmax of f̃ .
120	53	The rational behind searching the argmax of f̃ in the vicinity of πn is that samples from πn are likely to have high f̃ value since πn is updated such that the KL divergence w.r.t.
121	39	The repeated process of drawing points from πn, drawing their value from the GP posterior and selecting the point with highest value will constitute a data set D?n containing local samples from p?n.
125	11	The function f is initially evaluated at a point x0 drawn from the prior distribution π0.
133	28	lower) in expectation than the best (resp.
136	79	We then compare our algorithm to two state-of-the-art model based optimizers: the global Bayesian optimizer and the local Model-Based Relative Entropy Search (Abdolmaleki et al., 2015).
137	120	The algorithms are compared on several continuous function benchmarks as well as a simulated robotics task.
138	10	Variants of our algorithm are first compared on randomly generated smooth 2 dimensional objective functions.
140	16	We chose to split the experimentation between the uni-modal and the multi-modal categories of the testbed.
141	17	The unimodal category is representative of the informed initialization hypothesis that only requires local improvements.
144	23	In what follows, we will refer to our algorithm as L-BayesOpt.
