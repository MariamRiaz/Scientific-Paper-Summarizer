1	36	According to Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), a complex text is composed of non-overlapping Elementary Discourse Units (EDUs), as shown in Table 1.
2	30	Segmenting text into such discourse units is a key step in discourse analysis (Marcu, 2000) and can benefit many downstream tasks, such as sentence compression (Sporleder and Lapata, 2005) or document summarization (Li et al., 2016).
3	80	Since EDUs are initially designed to be determined with lexical and syntactic clues (Carlson et al., 2001), existing methods for discourse segmentation usually design hand-crafted features to capture these clues (Feng and Hirst, 2014).
4	13	Especially, nearly all previous methods rely on syntactic parse trees to achieve good performance.
5	22	But extracting such features usually takes a long time, which contradicts the fundamental role of discourse segmentation and hinders its actual use.
6	19	Considering the great success of deep learning on many NLP tasks (Lu and Li, 2016), it’s a natural idea for us to design an end-to-end neural model that can segment texts fast and accurately.
7	21	The first challenge of applying neural methods to discourse segmentation is data insufficiency.
8	39	Due to the limited size of labeled data in existing corpus (Carlson et al., 2001), it’s quite hard to train a data-hungry neural model without any prior knowledge.
9	10	In fact, some traditional features, such as the POS tags or parse trees, naturally provide strong signals for identifying EDUs.
10	17	Removing them definitely increases the difficulty of learning an accurate model.
12	6	For example, to recognize the boundary between e3 and e4 in Table 1, our model has to be aware that e3 is an embedded clauses starting from “overlooking”, otherwise it could regard “San Fernando Valley” as the subject of e4.
19	8	In summary, the contributions of this work are as follows: • Our neural discourse segmentation model doesn’t rely on any syntactic features, while it can outperform other state-of-the-art systems and achieve significant speedup.
20	48	• To our knowledge, we are the first to transfer word representations learned from large corpus into discourse segmentation task and show that they can significantly alleviate the data insufficiency problem.
21	33	• Based on the nature of discourse segmentation, we propose a restricted attention mechanism , which enables the model to capture useful information within a neighborhood but ignore unnecessary faraway noises.
23	17	Figure 1 gives an overview of our segmentation model.
24	17	We will introduce the BiLSTM-CRF framework in Section 2.1, and describe the two key components of our model in Section 2.2, 2.3.
25	5	Conditional Random Fields (CRF) (Lafferty et al., 2001) is an effective method to sequence labeling problem and has been widely used in many NLP tasks (Sutton and McCallum, 2012).
36	16	To tackle this issue, we propose to leverage model learned from other large datasets, aiming that this transferred model has been well trained to encode text and capture useful signals.
37	13	Instead of training the transferred model by ourselves, in this paper, we adopt the ELMo word representations (Peters et al., 2018), which are derived from a bidirectional language model (BiLM) trained on one billion word benchmark corpus (Chelba et al., 2014).
40	11	Then we concatenate rt with the word embedding et, and take them as the input of Equation (1).
41	11	As we have introduced in Section 1, some EDU boundaries rely on relatively long-distance signals to recognize, while normal LSTM model is still weak at this.
42	16	Recently, self-attention mechanism, which relates different positions of a single sequence, has been successfully applied to many NLP tasks (Vaswani et al., 2017; Wang et al., 2017) and shows its superiority in capturing long dependency.
43	25	However, we found that most boundaries are actually only influenced by nearby EDUs, thereby forcing the model to attend to the whole sequence will bring in unnecessary noises.
44	11	Therefore, we propose a restricted self-attention mechanism, which only collects information from a fixed neighborhood.
45	12	To do this, we first compute the similarity between current word xi and each nearby word xj within a window: si,j = w T attn[hi,hj ,hi hj ] (4) Then the attention vector ai is computed as a weighted sum of nearby words: αi,j = esi,j∑K k=−K e si,i+k (5) ai = ∑K j=−K αi,i+khi+k (6) where hyper-parameter K is the window size.
46	11	This attention vector ai is then put into another BiLSTM layer together with hi in order to fuse the information: h̃t = BiLSTM(h̃t−1, [ht,at]) (7) We use h̃t as the new input to the CRF layer.
47	7	We conduct experiments on the RST Discourse Treebank (RST-DT) (Carlson et al., 2001).
57	6	Exponential moving average is applied to all trainable variables with a decay rate 0.9999.
59	10	The results of our model and other competing systems on the test set of RST-DT are shown in Table 2.
67	26	To further explore the influence of different components in our model, we also report the results of ablation experiments in Table 2.
