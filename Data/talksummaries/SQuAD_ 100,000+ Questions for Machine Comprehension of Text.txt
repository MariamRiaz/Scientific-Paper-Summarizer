0	107	Reading Comprehension (RC), or the ability to read text and then answer questions about it, is a challenging task for machines, requiring both understanding of natural language and knowledge about the world.
4	20	Historically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993).
6	128	To address the need for a large and high-quality reading comprehension dataset, we present the Stan- 2383 ford Question Answering Dataset v1.0 (SQuAD), freely available at https://stanford-qa.com, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage.
7	19	SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previous manually labeled RC datasets such as MCTest (Richardson et al., 2013).
57	29	Additionally, crowdworkers were encouraged to ask questions in their own words, without copying word phrases from the paragraph.
70	74	In Table 2, we can see dates and other numbers make up 19.8% of the data; 32.6% of the answers are proper nouns of three different types; 31.8% are common noun phrases answers; and the remaining 15.8% are made up of adjective phrases, verb phrases, clauses and other types.
71	30	Reasoning required to answer questions.
72	21	To get a better understanding of the reasoning required to answer the questions, we sampled 4 questions from each of the 48 articles in the development set, and then manually labeled the examples with the categories shown in Table 3.
73	317	The results show that all examples have some sort of lexical or syntactic divergence between the question and the answer in the passage.
75	39	Q: What department store is thought to be the first in the world?
78	21	We also develop an automatic method to quantify the syntactic divergence between a question and the sentence containing the answer.
79	63	This provides another way to measure the difficulty of a question and to stratify the dataset, which we return to in Section 6.3.
96	16	Among these, we select the best one using the sliding-window approach proposed in Richardson et al. (2013).
97	30	In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013).
98	41	Whereas Richardson et al. (2013) used the entire passage as the context of an answer, we used only the sentence containing the candidate answer for efficiency.
99	48	In our logistic regression model, we extract several types of features for each candidate answer.
100	16	We discretize each continuous feature into 10 equallysized buckets, building a total of 180 million features, most of which are lexicalized features or dependency tree path features.
103	32	Length features bias the model towards picking common lengths and positions for answer spans, while span word frequencies bias the model against uninformative words.
105	28	In addition to these basic features, we resolve lexical variation using lexicalized features, and syntactic variation using dependency tree path features.
112	17	This metric measures the percentage of predictions that match any one of the ground truth answers exactly.
116	37	We assess human performance on SQuAD’s development and test sets.
123	35	We note that the model is able to select the sentence containing the answer correctly with 79.3% accuracy; hence, the bulk of the difficulty lies in finding the exact span within the sentence.
124	36	In order to understand the features that are responsible for the performance of the logistic regression model, we perform a feature ablation where we remove one group of features from our model at a time.
125	39	The results, shown in Table 6, indicate that lexicalized and dependency tree path features are most important.
127	18	Additionally, we note that with lexicalized features, the model significantly overfits the training set; however, we found that increasing L2 regularization hurts performance on the development set.
130	53	The results (shown in Table 7) show that the model performs best on dates and other numbers, categories for which there are usually only a few plausible candidates, and most answers are single tokens.
138	72	Figure 5 shows that the more divergence there is, the lower the performance of the logistic regression model.
141	21	Towards the end goal of natural language understanding, we introduce the Stanford Question Answering Dataset, a large reading comprehension dataset on Wikipedia articles with crowdsourced question-answer pairs.
146	24	We expect that the remaining gap will be harder to close, but that such efforts will result in significant advances in reading comprehension.
147	42	Reproducibility All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.codalab.org/worksheets/ 0xd53d03a48ef64b329c16b9baf0f99b0c/ .
