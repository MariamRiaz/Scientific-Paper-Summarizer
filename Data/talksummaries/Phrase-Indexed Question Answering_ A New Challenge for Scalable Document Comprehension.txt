0	12	Extractive question answering (QA) is the task of selecting an answer phrase (span) to a question given an evidence document.
1	80	Due to the easiness of evaluation (compared to generative QA) and the fine-grainess of the answer (compared to sentence-level QA), it has become one of the most popular QA tasks, driven by massive new datasets such as SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017).
2	95	Current QA models heavily rely on explicitly learning the interaction between the evidence document and the question using neural attention mechanisms (Wang and Jiang, 2017; Xiong et al., 2017; Seo et al., 2017; Lee et al., 2016, inter alia), in which the model is fully aware of the question before or as it reads the document.
3	30	As a result, despite significant advances, they have not led to the standalone representation of document discourse which is never- ∗Most work done during internship with Google AI.
5	13	Furthermore, QA models that condition the document representation on a question have the practical scalability downside that the entire model should be re-applied on the same document for every question.
6	52	In this paper, we formalize a modular variant of the QA task, Phrase Indexed Question Answering (PIQA), that enforces complete independence between document encoder and question encoder (Figure 1).
7	71	In PIQA, all documents are processed independently of any question to generate phrase index vectors (blue nodes in the figure) for each answer candidate (left boxes in the figure).
9	76	Then, at inference time, the answer is obtained by retrieving the nearest indexed phrase vector to the query vector.
15	22	Moreover, there is still a large gap between the baselines and the unconstrained state of the art, showing that the task is yet far from being solved.
16	22	We have set up a leaderboard1for PIQA challenge and invite the research community to participate.
17	17	We currently support SQuAD and plan to expand to other datasets as well.
28	46	Extractive question answering is the task of obtaining the answer â to a questionQ = {q1 .
34	7	(1) So far, most competitive designs of Fθ(Q,D, a) make use of attention connections between the words in Q and D. As a result, these models cannot yield a query independent representation of the document D. It is subsequently not possible to independently assess the document understanding capability of the model.
38	42	We enforce the decomposability of Fθ into two exclusive functions Gθ(Q), Hθ(D, a) ∈ Rk.
42	36	Language understanding ability is widely associated with learning a good standalone representation of text (or its components such as phrases) independent of the end task (Bowman et al., 2015).
45	38	Therefore, PIQA constraint enforces evaluating research in document comprehension and phrase representation learning.
46	17	Models that adhere to the PIQA constraint only need to be run once for each document, regardless of the number of questions asked.
47	7	To answer a question, the model then just needs to encode the question and compare it to each of the answer candidates via the inner product in Equation 2.
52	23	For all (neural) baselines, we represent the words in D and Q with one of three embedding mechanisms: CharCNN (Kim, 2014) + GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018).
54	25	.qn}, where the forward & backward LSTM outputs are concatenated to get a single word representation, i.e. di,qi ∈ R2k where k is the hidden state size of LSTMs.
55	82	PIQA disallows cross-attention between document and question.
