26	24	In both cases, incorporating StingyCD+ leads to efficiency improvements, demonstrating that “stingy updates” are a ver- Algorithm 1 Coordinate descent for solving (P) initialize x(0) ← 0m; r(0) ← b for t = 1, 2, .
27	14	T do i← get next coordinate() δ ← max { −x(t−1)i , 〈Ai,r(t−1)〉−λ ‖Ai‖2 } x(t) ← x(t−1) + δei r(t) ← r(t−1) − δAi return x(T ) satile and effective tool for scaling CD algorithms.
32	14	Solving (P) results in a set of learned weights, which define a linear predictive model.
33	16	The right term in the objective—commonly written as λ ‖x‖1 for Lasso problems without the nonnegativity constraint—is a regularization term that encourages the weights to have small value.
38	26	In this paper, we propose an algorithm that exploits sparsity for efficient optimization.
39	34	Coordinate descent (CD) is a popular algorithm for solving (P).
46	23	Bottleneck operations are computing the dot product 〈 Ai, r (t−1)〉 and updating r(t).
49	19	In this case, if r(t−1) lies outside of the “active update” region Ai = {r : 〈Ai, r〉 − λ > 0} , meaning 〈 Ai, r (t−1)〉−λ ≤ 0, then (1) implies that δ = 0.
50	76	In this scenario, weight i equals zero at the beginning and end of iteration t. When solutions to (P) are sufficiently sparse, these “zero updates” account for the majority of iterations in naive CD algorithms.
51	23	Computing these updates is very wasteful!
52	24	Each zero update requires O(NNZ (Ai)) time yet results in no progress toward convergence.
80	43	First, during some iterations, StingyCD updates a reference residuals vector, rr ← r(t−1).
85	13	T do # Update reference residuals on occasion: if should update reference() then rr← r(t−1) τ ← compute thresholds(x(t−1)) q(t−1) ← 0 i← get next coordinate() if q(t−1) ≤ τi and x(t−1)i = 0 then # Skip update: x(t) ← x(t−1); r(t) ← r(t−1); q(t) ← q(t−1) continue # Perform coordinate update: δ ← max { −x(t−1)i , 〈Ai,r(t−1)〉−λ ‖Ai‖2 } x(t) ← x(t−1) + δei r(t) ← r(t−1) − δAi q(t) ← q(t−1) − 2δ 〈 Ai, r (t−1) − rr 〉 + δ2 ‖Ai‖2 return x(T ) function compute thresholds(x) initialize τ ← 0m for i ∈ [m] do gi ← 〈Ai,Ax− b〉+ λ τi ← sign (gi) g 2 i ‖Ai‖2 return τ The second change to CD is that StingyCD tracks the quantity q(t) = ∥∥r(t) − rr∥∥2.
93	52	The threshold τi is computed after each update to rr.
94	29	If rr /∈ Ai, the value of τi equals the squared distance between rr andAi.
97	14	In Algorithm 2, every skipped update would, if computed, result in δ = 0.
98	21	We prove Theorem 2.1 in Appendix A. Theorem 2.1 is useful because it guarantees that although StingyCD skips many updates, CD and StingyCD have identical weight vectors for all iterations (assuming each algorithm updates coordinates in the same order).
105	61	In this case, StingyCD requires O(NNZ (A)) time.
106	51	Note StingyCD requires no more computation than CD for nearly all iterations (and often much less).
108	69	To ensure updates to rr do not overly impact convergence times, we schedule reference updates so that StingyCD invests less than 20% of its time in updating rr.
117	19	StingyCD does not skip update t. For now, we also assume τi > −q(t−1) (otherwise we can guarantee r(t−1) ∈ Ai, which implies δ 6= 0).
121	15	This computation will be wasteful if U (t) is false.
123	49	Specifically, r(t−1) lies on the boundary of S(t), which is a ball with center rr and radius √ q(t−1).
129	27	Included in Appendix C, Theorem 3.2’s proof calculates the probability that r(t−1) ∈ Ai by dividing the area of Ai ∩ bd(S(t)) by that of bd(S(t)) (illustrated in Figure 2).
198	17	This section demonstrates the impact of StingyCD and StingyCD+ in practice.
199	48	We first compare these algorithms to CD and safe screening for Lasso problems.
237	14	The datasets used for this comparison are an educational performance dataset (kdda)3 and a loan default prediction task (lending club)4.
254	13	Currently this idea is limited to CD algorithms and, for the most part, objectives with quadratic losses.
255	49	However, it seems likely that similar ideas would apply in many other contexts.
256	41	For example, it could be useful to use stingy updates in distributed optimization algorithms in order to significantly reduce communication requirements.
