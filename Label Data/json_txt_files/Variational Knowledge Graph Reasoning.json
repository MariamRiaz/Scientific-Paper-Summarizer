{"sections": [{"heading": "1 Introduction", "text": "Large-scaled knowledge graph supports a lot of downstream natural language processing tasks like question answering, response generation, etc. However, there are large amount of important facts missing in existing KG, which has significantly limited the capability of KG\u2019s application. Therefore, automated reasoning, or the ability for computing systems to make new inferences from the observed evidence, has attracted lots of attention from the research community. In recent years,\nthere are surging interests in designing machine learning algorithms for complex reasoning tasks, especially in large knowledge graphs (KGs) where the countless entities and links have posed great challenges to traditional logic-based algorithms. Specifically, we situate our study in this large KG multi-hop reasoning scenario, where the goal is to design an automated inference model to complete the missing links between existing entities in large KGs. For examples, if the KG contains a fact like president(BarackObama, USA) and spouse(Michelle, BarackObama), then we would like the machines to complete the missing link livesIn(Michelle, USA) automatically. Systems for this task are essential to complex question answering applications.\nTo tackle the multi-hop link prediction problem, various approaches have been proposed. Some earlier works like PRA (Lao et al., 2011; Gardner et al., 2014, 2013) use bounded-depth random walk with restarts to obtain paths. More recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018), frame the path-finding problem as a Markov Decision Process (MDP) and utilize reinforcement learning (RL) to maximize the expected return. Another line of work along with ours are Chain-of-Reasoning (Das et al., 2016) and Compositional Reasoning (Neelakantan et al., 2015), which take multi-hop chains learned by PRA as input and aim to infer its relation.\nHere we frame the KG reasoning task as a two sub-steps, i.e. \u201cPath-Finding\u201d and \u201cPathReasoning\u201d. We found that most of the related research is only focused on one step, which leads to major drawbacks\u2014lack of interactions between these two steps. More specifically, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018) can be interpreted as enhancing the \u201cPath-Finding\u201d step while compositional reasoning (Neelakantan et al., 2015) and chains of rea-\nar X\niv :1\n80 3.\n06 58\n1v 3\n[ cs\n.A I]\n2 3\nO ct\n2 01\n8\nsoning (Das et al., 2016) can be interpreted as enhancing the \u201cPath-Reasoning\u201d step. DeepPath is trained to find paths more efficiently between two given entities while being agnostic to whether the entity pairs are positive or negative, whereas MINERVA learns to reach target nodes given an entityquery pair while being agnostic to the quality of the searched path1. In contrast, chains of reasoning and compositional reasoning only learn to predict relation given paths while being agnostic to the path-finding procedure. The lack of interaction prevents the model from understanding more diverse inputs and make the model very sensitive to noise and adversarial samples.\nIn order to increase the robustness of existing KG reasoning model and handle noisier environments, we propose to combine these two steps together as a whole from the perspective of the latent variable graphic model. This graphic model views the paths as discrete latent variables and relation as the observed variables with a given entity pair as the condition, thus the path-finding module can be viewed as a prior distribution to infer the underlying links in the KG. In contrast, the pathreasoning module can be viewed as the likelihood distribution, which classifies underlying links into multiple classes. With this assumption, we introduce an approximate posterior and design a variational auto-encoder (Kingma and Welling, 2013) algorithm to maximize the evidence lower-bound. This variational framework closely incorporates two modules into a unified framework and jointly train them together. By active cooperations and interactions, the path finder can take into account the value of searched path and resort to the more meaningful paths. Meanwhile, the path reasoner can receive more diverse paths from the path finder and generalizes better to unseen scenarios. Our contributions are three-fold:\n\u2022 We introduce a variational inference framework for KG reasoning, which tightly integrates the path-finding and path-reasoning processes to perform joint reasoning.\n\u2022 We have successfully leveraged negative samples into training and increase the robustness of existing KG reasoning model.\n\u2022 We show that our method can scale up to large KG and achieve state-of-the-art results\n1MINERVA assigns constant rewards to all paths reaching the destination while ignoring their qualities.\non two popular datasets.\nThe rest of the paper is organized as follow. In Section 2 we will outline related work on KG embedding, multi-hop reasoning, and variational auto-encoder. We describe our variational knowledge reasoner DIVA in Section 3. Experimental results are presented in Section 4, and we conclude in Section 5."}, {"heading": "2 Related Work", "text": ""}, {"heading": "2.1 Knowledge Graph Embeddings", "text": "Embedding methods to model multi-relation data from KGs have been extensively studied in recent years (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013; Lin et al., 2015; Trouillon et al., 2017). From a representation learning perspective, all these methods are trying to learn a projection from symbolic space to vector space. For each triple (es, r, ed) in the KG, various score functions can be defined using either vector or matrix operations. Although these embedding approaches have been successful capturing the semantics of KG symbols (entities and relations) and achieving impressive results on knowledge base completion tasks, most of them fail to model multi-hop relation paths, which are indispensable for more complex reasoning tasks. Besides, since all these models operate solely on latent space, their predictions are barely interpretable."}, {"heading": "2.2 Multi-Hop Reasoning", "text": "The Path-Ranking Algorithm (PRA) method is the first approach to use a random walk with restart mechanism to perform multi-hop reasoning. Later on, some research studies (Gardner et al., 2014, 2013) have revised the PRA algorithm to compute feature similarity in the vector space. These formula-based algorithms can create a large fan-out area, which potentially undermines the inference accuracy. To mitigate this problem, a Convolutional Neural Network(CNN)based model (Toutanova et al., 2015) has been proposed to perform multi-hop reasoning. Recently, DeepPath (Xiong et al., 2017) and MINERVA (Das et al., 2018) view the multi-hop reasoning problem as a Markov Decision Process, and leverages REINFORCE (Williams, 1992) to efficiently search for paths in large knowledge graph. These two methods are reported to achieve state-of-the-art results, however, these two models both use heuristic rewards to drive the policy\nsearch, which could make their models sensitive to noises and adversarial examples."}, {"heading": "2.3 Variational Auto-encoder", "text": "Variational Auto-Encoder (Kingma and Welling, 2013) is a very popular algorithm to perform approximate posterior inference in large-scale scenarios, especially in neural networks. Recently, VAE has been successfully applied to various complex machine learning tasks like image generation (Mansimov et al., 2015), machine translation (Zhang et al., 2016), sentence generation (Guu et al., 2017a) and question answering (Zhang et al., 2017). Zhang et al. (2017) is closest to ours, this paper proposes a variational framework to understand the variability of human language about entity referencing. In contrast, our model uses a variational framework to cope with the complex link connections in large KG. Unlike the previous research in VAE, both Zhang et al. (2017) and our model both use discrete variable as the latent representation to infer the semantics of given entity pairs. More specifically, we view the generation of relation as a stochastic process controlled by a latent representation, i.e. the connected multi-hop link existed in the KG. Though the potential link paths are discrete and countable, its amount is still very large and poses challenges to direct optimization. Therefore, we resort to variational auto-encoder as our approximation strategy."}, {"heading": "3 Our Approach", "text": ""}, {"heading": "3.1 Background", "text": "Here we formally define the background of our task. Let E be the set of entities and R be the set of relations. Then a KG is defined as a collection of triple facts (es, r, ed), where es, ed \u2208 E and r \u2208 R. We are particularly interested in the problem of relation inference, which seeks to answer the question in the format of (es, ?, ed), the problem setting is slightly different from standard link prediction to answer the question of (es, r, ?). Next, in order to tackle this classification problem, we assume that there is a latent representation for given entity pair in the KG, i.e. the collection of linked paths, these hidden variables can reveal the underlying semantics between these two entities. Therefore, the link classification problem can be decomposed into two modules \u2013 acquire underlying paths (Path Finder) and infer relation from la-\ntent representation (Path Reasoner).\nPath Finder The state-of-the-art approach (Xiong et al., 2017; Das et al., 2018) is to view this process as a Markov Decision Process (MDP). A tuple < S,A, P > is defined to represent the MDP, where S denotes the current state, e.g. the current node in the knowledge graph, A is the set of available actions, e.g. all the outgoing edges from the state, while P is the transition probability describing the state transition mechanism. In the knowledge graph, the transition of the state is deterministic, so we do not need to model the state transition P .\nPath Reasoner The common approach (Lao et al., 2011; Neelakantan et al., 2015; Das et al., 2016) is to encode the path as a feature vector and use a multi-class discriminator to predict the unknown relation. PRA (Lao et al., 2011) proposes to encode paths as binary features to learn a loglinear classifier, while (Das et al., 2016) applies recurrent neural network to recursively encode the paths into hidden features and uses vector similarity for classification."}, {"heading": "3.2 Variational KG Reasoner (DIVA)", "text": "Here we draw a schematic diagram of our model in Figure 1. Formally, we define the objective function for the general relation classification problem as follows:\nObj = \u2211\n(es,r,ed)\u2208D\nlog p(r|(es, ed))\n= \u2211\n(es,r,ed)\u2208D\nlog \u2211 L p\u03b8(L|(es, ed))p(r|L) (1)\nwhere D is the dataset, (es, r, ed) is the triple contained in the dataset, and L is the latent connecting paths. The evidence probability p(r|(es, ed)) can be written as the marginalization of the product of two terms over the latent space. However, this evidence probability is intractable since it requires summing over the whole latent link space. Therefore, we propose to maximize its variational lower bound as follows: ELBO = E\nL\u223cq\u03d5(L|r,(es,ed)) [log p\u03b8(r|L)]\u2212 DKL(q\u03d5(L|r, (es, ed))||p\u03b2(L|(es, ed))) (2)\nSpecifically, the ELBO (Kingma and Welling, 2013) is composed of three different terms \u2013 likelihood p\u03b8 ( r|L), prior p\u03b2 ( L|(es, et)), and posterior\nq\u03d5 ( L|(es, ed), r). In this paper, we use three neural network models to parameterize these terms\nand then follow (Kingma and Welling, 2013) to apply variational auto-encoder to maximize the approximate lower bound. We describe these three models in details below:\nPath Reasoner (Likelihood). Here we propose a path reasoner using Convolutional Neural Networks (CNN) (LeCun et al., 1995) and a feedforward neural network. This model takes path sequence L = {a1, e1, \u00b7 \u00b7 \u00b7 , ai, ei, \u00b7 \u00b7 \u00b7 an, en} to output a softmax probability over the relations set R, where ai denotes the i-th intermediate relation and ei denotes the i-th intermediate entity between the given entity pair. Here we first project them into embedding space and concatenate i-th relation embedding with i-th entity embedding as a combined vector, which we denote as {f1, f2, \u00b7 \u00b7 \u00b7 , fn} and fi \u2208 R2E . As shown in Figure 2, we pad the embedding sequence to a length of N . Then we design three convolution layers with window size of (1\u00d7 2E), (2\u00d7 2E), (3\u00d7 2E), input channel size 1 and filter size D. After the convolution layer, we use (N \u00d7 1), (N \u2212 1 \u00d7 1), (N \u2212 2 \u00d7 1) to max pool the convolution feature map. Finally, we concatenate the three vectors as a combined vector F \u2208 R3D. Finally, we use two-layered MLP with intermediate hidden size of M to output a softmax distribution over all the relations set R.\nF = f(f1, f2, \u00b7 \u00b7 \u00b7 , fN ) (3) p(r|L; \u03b8) = softmax(WrF + br) (4)\nwhere f denotes the convolution and max-pooling function applied to extract reasoning path feature F , and Wr, br denote the weights and bias for the output feed-forward neural network.\nPath Finder (Prior). Here we formulate the path finder p(L|(es, ed)) as an MDP problem, and recursively predict actions (an outgoing relationentity edge (a, e)) in every time step based on the previous history ht\u22121 as follows:\nct = ReLU(Wh[ht; ed] + bh) (5)\np((at+1, et+1)|ht, \u03b2) = softmax(Atct) (6)\nwhere the ht \u2208 RH denotes the history embedding, ed \u2208 RE denotes the entity embedding, At \u2208 R|A|\u00d72E is outgoing matrix which stacks the concatenated embeddings of all outgoing edges and |A| denotes the number of outgoing edge, we use Wh and bh to represent the weight and bias of the feed-forward neural network outputting feature vector ct \u2208 R2E . The history embedding ht is obtained using an LSTM network (Hochreiter and\nSchmidhuber, 1997) to encode all the previous decisions as follows:\nht = LSTM(ht\u22121, (at, et)) (7)\nAs shown in Figure 3, the LSTM-based path finder interacts with the KG in every time step and decides which outgoing edge (at+1, et+1) to follow, search procedure will terminate either the target node is reached or the maximum step is reached.\nApproximate Posterior. We formulate the posterior distribution q(L|(es, ed), r) following the similar architecture as the prior. The main difference lies in the fact that posterior approximator is aware of the relation r, therefore making more relevant decisions. The posterior borrows the history vector from finder as ht, while the feed-forward neural network is distinctive in that it takes the relation embedding also into account. Formally, we write its outgoing distribution as follows:\nut = ReLU(Whp[ht; ed; r] + bhp)\nq((at+1, et+1)|ht;\u03d5) = softmax(Atut) (8)\nwhere Whp and bhp denote the weight and bias for the feed-forward neural network."}, {"heading": "3.3 Optimization", "text": "In order to maximize the ELBO with respect to the neural network models described above, we follow VAE (Kingma and Welling, 2013) to interpret the negative ELBO as two separate losses and minimize these them jointly using a gradient descent:\nReconstruction Loss. Here we name the first term of negative ELBO as reconstruction loss:\nJR = E L\u223cq\u03d5(L|r,(es,ed))\n[\u2212 log p\u03b8(r|L)] (9)\nthis loss function is motivated to reconstruct the relationR from the latent variable L sampled from approximate posterior, optimizing this loss function jointly can not only help the approximate posterior to obtain paths unique to particular relation r, but also teaches the path reasoner to reason over multiple hops and predict the correct relation.\nKL-divergence Loss. We name the second term as KL-divergence loss:\nJKL = DKL(q\u03d5(L|r, es, ed)|p\u03b2(L|es, ed)) (10)\nthis loss function is motivated to push the prior distribution towards the posterior distribution. The\nintuition of this loss lies in the fact that an entity pair already implies their relation, therefore, we can teach the path finder to approach the approximate posterior as much as possible. During testtime when we have no knowledge about relation, we use path finder to replace posterior approximator to search for high-quality paths.\nDerivatives. We show the derivatives of the loss function with respect to three different models. For the approximate posterior, we re-weight the KL-diverge loss and design a joint loss function as follows:\nJ = JR + \u03bbKLJKL (11)\nwhere \u03bbKL is the re-weight factor to combine these two losses functions together. Formally, we write the derivative of posterior as follows:\n\u2202J \u2202\u03d5 = E L\u223cq\u03d5(L)) [\u2212 fre(L) \u2202 log q\u03d5(L|(es, ed), r) \u2202\u03d5 ] (12)\nwhere fre(L) = log p\u03b8 + \u03bbKL log p\u03b2 q\u03d5\ndenotes the probability assigned by path reasoner. In practice, we found that a large KL-regularizer log p\u03b2q\u03d5 causes severe instability during training, therefore we keep a low \u03bbKL value during our experiments 2. For the path reasoner, we also optimize its parameters \u03b8 with regard to the reconstruction as follows:\n\u2202JR \u2202\u03b8 = E L\u223cq\u03d5(L) \u2212 \u2202 log p\u03b8(r|L) \u2202\u03b8\n(13)\nFor the path finder, we optimize its parameters \u03b2 with regard to the KL-divergence to teach it to infuse the relation information into the found links.\n\u2202JKL \u2202\u03b2 = E L\u223cq\u03d5(L) \u2212 \u2202 log p\u03b2(L|(es, ed)) \u2202\u03b2 (14)\nTrain & Test During training time, in contrast to the preceding methods like Das et al. (2018); Xiong et al. (2017), we also exploit negative samples by introducing an pseudo \u201cn/a\u201d relation, which indicates \u201cno-relation\u201d between two entities. Therefore, we manage to decompose the data sample (eq, rq, [e\u22121 , e \u2212 2 , \u00b7 \u00b7 \u00b7 , e+n ]) into a series of tuples (eq, r\u2032q, ei), where r \u2032 q = rq for positive samples and r\u2032q = n/a for negative samples. During training, we alternatively update three submodules with SGD. During test, we apply the\n2we set \u03bbKL = 0 through our experiments.\nAlgorithm 1 The DIVA Algorithm. 1: procedure TRAINING & TESTING 2: Train: 3: for episode\u2190 1 to N do 4: Rollout K paths from posterior p\u03d5 5: if Train-Posterior then 6: \u03d5\u2190 \u03d5\u2212 \u03b7 \u00d7 \u2202Lr\u2202\u03d5 7: else if Train-Likelihood then 8: \u03b8 \u2190 \u03b8 \u2212 \u03b7 \u00d7 \u2202Lr\u2202\u03b8 9: else if Train-Prior then 10: \u03b2 \u2190 \u03b2 \u2212 \u03b7 \u00d7 \u2202LKL\u2202\u03b2 11: end if 12: end for 13: Test MAP: 14: Restore initial parameters \u03b8, \u03b2 15: Given sample (es, rq, (e1, e2, \u00b7 \u00b7 \u00b7 , en)) 16: Li \u2190 BeamSearch(p\u03b2(L|es, ei)) 17: Si \u2190 1|Li| \u2211 l\u2208Li p\u03b8(rq|l) 18: Sort Si and find positive rank ra+ 19: MAP \u2190 1 1+ra+ 20: end procedure\npath-finder to beam-search the top paths for all tuples and rank them based on the scores assign by path-reasoner. More specifically, we demonstrate the pseudo code in Algorithm 1."}, {"heading": "3.4 Discussion", "text": "We here interpret the update of the posterior approximator in equation Equation 12 as a special case of REINFORCE (Williams, 1992), where we use Monte-Carlo sampling to estimate the expected return log p\u03b8(r|L) for current posterior policy. This formula is very similar to DeepPath and MINERVA (Xiong et al., 2017; Das et al., 2018) in the sense that path-finding process is described as an exploration process to maximize the policy\u2019s long-term reward. Unlike these two models assigning heuristic rewards to the policy, our model assigns model-based reward log p\u03b8(r|L), which is known to be more sophisticated and considers more implicit factors to distinguish between good and bad paths. Besides, our update formula for path reasoner Equation 13 is also similar to chain-of-reasoning (Das et al., 2016), both models are aimed at maximizing the likelihood of relation given the multi-hop chain. However, our model is distinctive from theirs in a sense that the obtained paths are sampled from a dynamic policy, by exposing more diverse paths to the path reasoner, it can generalize to more conditions. By the\nactive interactions and collaborations of two models, DIVA is able to comprehend more complex inference scenarios and handle more noisy environments."}, {"heading": "4 Experiments", "text": "To evaluate the performance of DIVA, we explore the standard link prediction task on two differentsized KG datasets and compare with the state-ofthe-art algorithms. Link prediction is to rank a list of target entities (e\u22121 , e \u2212 2 , \u00b7 \u00b7 \u00b7 , e+n ) given a query entity eq and query relation rq. The dataset is arranged in the format of (eq, rq, [e\u22121 , e \u2212 2 , \u00b7 \u00b7 \u00b7 , e+n ]), and the evaluation score (Mean Averaged Precision, MAP) is based on the ranked position of the positive sample."}, {"heading": "4.1 Dataset and Setting", "text": "We perform experiments on two datasets, and the details of the statistics are described in Table 1. The samples of FB15k-237 (Toutanova et al., 2015) are sampled from FB15k (Bordes et al., 2013), here we follow DeepPath (Xiong et al., 2017) to select 20 relations including Sports, Locations, Film, etc. Our NELL dataset is downloaded from the released dataset3, which contains 12 relations for evaluation. Besides, both datasets contain negative samples obtained by using the PRA code released by Lao et al. (2011). For each query rq, we remove all the triples with rq and r\u22121q during reasoning. During training, we set number of rollouts to 20 for each training sample and update the posterior distribution using Monte-Carlo REINFORCE (Williams, 1992) algorithm. During testing, we use a beam of 5 to approximate the whole search space for path finder. We follow MINERVA (Das et al., 2018) to set the maximum reasoning length to 3, which lowers the burden for the path-reasoner model. For both datasets, we set the embedding size E to 200, the history embedding size H to 200, the convolution kernel feature size D to 128, we set the hidden size of MLP for both path finder and path reasoner to 400.\n3https://github.com/xwhan/DeepPath"}, {"heading": "4.2 Quantitative Results", "text": "We mainly compare with the embedding-based algorithms (Bordes et al., 2013; Lin et al., 2015; Ji et al., 2015; Wang et al., 2014), PRA (Lao et al., 2011), MINERVA (Das et al., 2018), DeepPath (Xiong et al., 2017) and Chain-ofReasoning (Das et al., 2016), besides, we also take our standalone CNN path-reasoner from DIVA. Besides, we also try to directly maximize the marginal likelihood p(r|es, ed) =\u2211\nL p(L|es, ed)p(r|L) using only the prior and likelihood model following MML (Guu et al., 2017b), which enables us to understand the superiority of introducing an approximate posterior. Here we first report our results for NELL-995 in Table 2, which is known to be a simple dataset and many existing algorithms already approach very significant accuracy. Then we test our methods in FB15k (Toutanova et al., 2015) and report our results in Table 3, which is much harder than NELL and arguably more relevant for real-world scenarios.\nBesides, we also evaluate our model on FB-15k 20-relation subset with HITS@N score. Since our model only deals with the relation classification\nproblem (es, ?, ed) with ed as input, so it\u2019s hard for us to directly compare with MINERVA (Das et al., 2018). However, here we compare with chainRNN (Das et al., 2016) and CNN Path-Reasoner model, the results are demonstrated as Table 4. Please note that the HITS@N score is computed against relation rather than entity.\nResult Analysis We can observe from the above tables Table 3 and Table 2 that our algorithm has significantly outperformed most of the existing algorithms and achieves a very similar result as MINERVA (Das et al., 2018) on NELL dataset and achieves state-of-the-art results on FB15k. We conclude that our method is able to deal with more complex reasoning scenarios and is more robust to the adversarial examples. Besides, we also observe that our CNN Path-Reasoner can outperform the RNN-Chain (Das et al., 2016) on both datasets, we speculate that it is due to the short lengths of reasoning chains, which can extract more useful information from the reasoning chain.\nFrom these two pie charts in Figure 5, we can observe that in NELL-995, very few errors are coming from the path reasoner since the path length is very small. A large proportion only contains a single hop. In contrast, most of the failures in the FB15k dataset are coming from the path reasoner, which fails to classify the multi-hop chain into correct relation. This analysis demonstrates that FB15k is much harder dataset and may be closer to real-life scenarios."}, {"heading": "4.3 Beam Size Trade-offs", "text": "Here we are especially interested in studying the impact of different beam sizes in the link prediction tasks. With larger beam size, the path finder can obtain more linking paths, meanwhile, more noises are introduced to pose greater challenges for the path reasoner to infer the relation. With smaller beam size, the path finder will struggle to find connecting paths between positive entity pairs, meanwhile eliminating many noisy links. Therefore, we first mainly summarize three different types and investigate their changing curve under different beam size conditions:\n1. No paths are found for positive samples, while paths are found for negative samples, which we denote as Neg>Pos=0.\n2. Both positive samples and negative samples found paths, but the reasoner assigns higher scores to negative samples, which we denote as Neg>Pos>0.\n3. Both negative and positive samples are not able to find paths in the knowledge graph,\nwhich we denote as Neg=Pos=0.\nWe draw the curves for MAP and error ratios in Figure 4 and we can easily observe the tradeoffs, we found that using beam size of 5 can balance the burden of path-finder and path-reasoner optimally, therefore we keep to this beam size for the all the experiments."}, {"heading": "4.4 Error Analysis", "text": "In order to investigate the bottleneck of DIVA, we take a subset from validation dataset to summarize the causes of different kinds of errors. Roughly, we classify errors into three categories, 1) KG noise: This error is caused by the KG itself, e.g some important relations are missing; some entities are duplicate; some nodes do not have valid outgoing edges. 2) Path-Finder error: This error is caused by the path finder, which fails to arrive destination. 3) Path-Reasoner error: This error is caused by the path reasoner to assign a higher score to negative paths. Here we draw two pie charts to demonstrate the sources of reasoning errors in two reasoning tasks."}, {"heading": "4.5 Failure Examples", "text": "We also show some failure samples in Table 5 to help understand where the errors are coming from. We can conclude that the \u201cduplicate entity\u201d and \u201cmissing entity\u201d problems are mainly caused by the knowledge graph or the dataset, and the link prediction model has limited capability to resolve that. In contrast, the \u201cwrong reasoning\u201d problem is mainly caused by the reasoning model itself and can be improved with better algorithms."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a novel variational inference framework for knowledge graph reasoning. In contrast to prior studies that use a random walk with restarts (Lao et al., 2011) and explicit reinforcement learning path finding (Xiong et al., 2017), we situate our study in the context of variational inference in latent variable probabilistic graphical models. Our framework seamlessly integrates the path-finding and path-reasoning processes in a unified probabilistic framework, leveraging the strength of neural network based representation learning methods. Empirically, we show that our method has achieved the state-of-the-art performances on two popular datasets."}, {"heading": "6 Acknowledgement", "text": "The authors would like to thank the anonymous reviewers for their thoughtful comments. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053 and NSF IIS 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein."}], "year": 2018, "references": [{"title": "Translating embeddings for modeling multirelational data", "authors": ["Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko."], "venue": "Advances in neural information processing systems. pages 2787\u20132795.", "year": 2013}, {"title": "Chains of reasoning over entities, relations, and text using recurrent neural networks", "authors": ["Rajarshi Das", "Arvind Neelakantan", "David Belanger", "Andrew McCallum."], "venue": "arXiv preprint arXiv:1607.01426 .", "year": 2016}, {"title": "Improving learning and inference in a large knowledge-base using latent syntactic cues", "authors": ["Matt Gardner", "Partha Pratim Talukdar", "Bryan Kisiel", "Tom M. Mitchell."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language", "year": 2013}, {"title": "Incorporating vector space similarity in random walk inference over knowledge bases", "authors": ["Matt Gardner", "Partha Pratim Talukdar", "Jayant Krishnamurthy", "Tom M. Mitchell."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Nat-", "year": 2014}, {"title": "Generating sentences by editing prototypes", "authors": ["Kelvin Guu", "Tatsunori B Hashimoto", "Yonatan Oren", "Percy Liang."], "venue": "arXiv preprint arXiv:1709.08878 .", "year": 2017}, {"title": "From language to programs: Bridging reinforcement learning and maximum marginal likelihood", "authors": ["Kelvin Guu", "Panupong Pasupat", "Evan Zheran Liu", "Percy Liang."], "venue": "arXiv preprint arXiv:1704.07926 .", "year": 2017}, {"title": "Long short-term memory", "authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation 9(8):1735\u20131780.", "year": 1997}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "authors": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao."], "venue": "ACL (1). pages 687\u2013696.", "year": 2015}, {"title": "Autoencoding variational bayes", "authors": ["Diederik P Kingma", "Max Welling."], "venue": "arXiv preprint arXiv:1312.6114 .", "year": 2013}, {"title": "Random walk inference and learning in a large scale knowledge base", "authors": ["Ni Lao", "Tom Mitchell", "William W Cohen."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,", "year": 2011}, {"title": "Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks 3361(10):1995", "authors": ["Yann LeCun", "Yoshua Bengio"], "year": 1995}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "authors": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu."], "venue": "AAAI. pages 2181\u20132187.", "year": 2015}, {"title": "Generating images from captions with attention", "authors": ["Elman Mansimov", "Emilio Parisotto", "Jimmy Lei Ba", "Ruslan Salakhutdinov."], "venue": "arXiv preprint arXiv:1511.02793 .", "year": 2015}, {"title": "Compositional vector space models for knowledge base inference", "authors": ["Arvind Neelakantan", "Benjamin Roth", "Andrew McCallum."], "venue": "2015 aaai spring symposium series.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "authors": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel."], "venue": "ICML. volume 11, pages 809\u2013816.", "year": 2011}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "authors": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng."], "venue": "Advances in neural information processing systems. pages 926\u2013934.", "year": 2013}, {"title": "Representing text for joint embedding of text and knowledge bases", "authors": ["Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon."], "venue": "EMNLP. volume 15, pages 1499\u20131509.", "year": 2015}, {"title": "Knowledge graph completion via complex tensor factorization", "authors": ["Th\u00e9o Trouillon", "Christopher R Dance", "Johannes Welbl", "Sebastian Riedel", "\u00c9ric Gaussier", "Guillaume Bouchard."], "venue": "arXiv preprint arXiv:1702.06879 .", "year": 2017}, {"title": "Knowledge graph embedding by translating on hyperplanes", "authors": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen."], "venue": "AAAI. pages 1112\u20131119.", "year": 2014}, {"title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "authors": ["Ronald J Williams."], "venue": "Machine learning 8(3-4):229\u2013256.", "year": 1992}, {"title": "Deeppath: A reinforcement learning method for knowledge graph reasoning", "authors": ["Wenhan Xiong", "Thien Hoang", "William Yang Wang."], "venue": "arXiv preprint arXiv:1707.06690 .", "year": 2017}, {"title": "Variational neural machine translation", "authors": ["Biao Zhang", "Deyi Xiong", "Jinsong Su", "Hong Duan", "Min Zhang."], "venue": "arXiv preprint arXiv:1605.07869 .", "year": 2016}, {"title": "Variational reasoning for question answering with knowledge graph", "authors": ["Yuyu Zhang", "Hanjun Dai", "Zornitsa Kozareva", "Alexander J Smola", "Le Song."], "venue": "arXiv preprint arXiv:1709.04071 .", "year": 2017}], "id": "SP:533badace91d4b33d2a61bba15fefcdfa00604e6", "authors": [{"name": "Wenhu Chen", "affiliations": []}, {"name": "Wenhan Xiong", "affiliations": []}, {"name": "Xifeng Yan", "affiliations": []}, {"name": "William Yang Wang", "affiliations": []}], "abstractText": "Inferring missing links in knowledge graphs (KG) has attracted a lot of attention from the research community. In this paper, we tackle a practical query answering task involving predicting the relation of a given entity pair. We frame this prediction problem as an inference problem in a probabilistic graphical model and aim at resolving it from a variational inference perspective. In order to model the relation between the query entity pair, we assume that there exists an underlying latent variable (paths connecting two nodes) in the KG, which carries the equivalent semantics of their relations. However, due to the intractability of connections in large KGs, we propose to use variation inference to maximize the evidence lower bound. More specifically, our framework (DIVA) is composed of three modules, i.e. a posterior approximator, a prior (path finder), and a likelihood (path reasoner). By using variational inference, we are able to incorporate them closely into a unified architecture and jointly optimize them to perform KG reasoning. With active interactions among these sub-modules, DIVA is better at handling noise and coping with more complex reasoning scenarios. In order to evaluate our method, we conduct the experiment of the link prediction task on multiple datasets and achieve state-of-the-art performances on both datasets.", "title": "Variational Knowledge Graph Reasoning"}