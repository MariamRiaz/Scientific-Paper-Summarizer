19	39	This neural tree decoding (NTD) model outperforms state-of-the-art models for MeSH tagging.
23	60	It induces an attention distribution over encoder states, which is used together with the current decoder state vector to inform which (if any) of its immediate children are applicable to the input text (Figure 1).
27	7	These are passed through an embedding layer, producing a sequence of word embeddings x (for clarity we omit a document index here), which are then passed through a GRU (Cho et al., 2014b) to obtain a sequence of hidden vectors h = {h0, · · · , h|x|−1}, where ht = GRU(xt, ht−1).
28	15	These are then passed to our neural tree decoder, which is responsible for tagging the encoded text with an arbitrary number of terms from the label tree, i.e., sequences in the structured output space.
29	21	This module traverses the label space top-down, beginning at the root, thus exploiting the concept hierarchy codified by the tree structure.
34	26	Given cn, we then estimate the probability that child label v is applicable to the current input text as a function of the decoder state vector (sn), the current context vector (cn) and the decoder parameters.
37	31	The set of hidden vectors induced by the encoder (corresponding to the inputs) are denoted by h, s is the hidden state of the decoder, and y is the reference label (this encodes a path in the output tree).
39	8	The advantage of using an RNN during decoding is that this allows the exploitation of learned, distributed hidden representations of partial tree paths, which inform nodewise attention and subsequent predictions.
40	20	Algorithm 1 RECURSIVETREEDECODING 1: function NODELOSS(n, h, s, y) 2: ln← 0 3: cn, sn← DEC(h, n, s) 4: for each child v ∈ children(n) do 5: ŷv ← σ(Wv · [sn, cn]) 6: pv ←∝ depth in tree 7: Bv ∼ Ber(pv) 8: if Bv then 9: ln← ln + L(ŷv, y) 10: if ŷv > τ then 11: ln← ln + NODELOSS(v, h, sn, y) return ln 12: function TRAIN(x, y, α, epochs) 13: θ ← INIT(θ) 14: e← 0 15: while e < epochs do 16: for each instance xi ∈ x do 17: hi ← ENC(xi) 18: s0 ← 0 19: li ← NODELOSS(ROOT, hi, s0, yi) 20: ∆θ ← BACKPROP(li) 21: θ ← θ + α∆θ 22: e← e+ 1 return θ Incurring loss for all nodes along the path specified by y would place a disproportionate amount of emphasis on correctly applying terms that are ‘higher’ in the ontology, as loss will be propagated for the initial predictions concerning the application of these and then also, due to recursive application, for all of their children (and so on).
41	14	Thus we only incur (and hence backpropagate) loss for a node v stochastically, according to a Bernoulli distribution B with parameter pv.
42	35	We set pv to be proportional to the depth of node v in the tree such that we are likely to incur larger loss for deeper (rarely occurring) nodes.
51	22	Our dataset comprises abstracts of articles describing randomized controlled trials (RCTs) from PubMed along with their MeSH terms.
52	10	The MeSH annotations were manually applied by professionals at the National Library of Medicine (NLM).
53	19	The label space underlying MeSH terms is codified by a publicly available ontology.5 We split this dataset into disjoint sets for training/development and final evaluation (Table 1).
54	7	We further separated the former into train, validation and development test subsets, to refine our approach.
61	22	Specifically, they train a regressor to predict k, the number of MeSH terms to be applied to an abstract.
72	18	However, these metrics are overly strict in the sense that a model will be penalized equally for all mistakes, regardless of whether they are nearby or far from the target in the label tree.
74	70	To quantify this, and to explore the extent to which explicitly decoding into the target label space yields improved predictions, we also consider a measure that we refer to as semantic distance (SD): SD = 1 |Y| ∑ u∈Y min v∈Ŷ dist(u, v) (3) where Y and Ŷ are the sets of target and predicted terms respectively, and dist is a function that returns the shortest distance between two nodes in the label ontology tree.
80	31	The proposed Neural Tree Decoding model with stochastic backpropagation (NTD-s) bests the most competitive baseline (LSSI) in F1 score by over 2 points.
81	19	To explore the effect of backpropagating loss from nodes in proportion to their depth in the ontology, we also include results for a deterministic variant that does not do this, NTD-d.
85	9	We observe a marked performance increase of ∼21% over the best performing baseline.
87	17	We developed a neural attentive sequence tree decoding model for structured multilabel classification where labels are drawn from a known ontology.
88	7	The proposed method can decode an input text into a tree of labels, effectively using the structure in the output space.
89	12	We demonstrated that this model outperformed SOTA approaches for the important task of tagging biomedical abstracts with Medical Subject Heading (MeSH) terms on a modestly sized training corpus.
91	24	One limitation of our model is that it is comparatively slow, due to having to traverse the tree structure during decoding.
92	36	Prediction speed may not be a major issue in practice, as articles on PubMed could be batch tagged nightly as they arrive.
94	91	For this reason we have here used a modest training set of ∼20k abstracts, which is smaller than corpora used in prior work on this task.
97	13	In future work we thus hope to apply this model to larger datasets, and to address the efficiency issue.
98	17	Concerning the latter, sibling subtrees may be traversed in parallel, conditioned on the hidden state of their parent.
99	37	Another promising direction would be to move to convolutional encoder and decoder architectures, designing the latter in a way similarly capitalizes on the label space tree structure.
