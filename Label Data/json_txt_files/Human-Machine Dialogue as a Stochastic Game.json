{"sections": [{"text": "Proceedings of the SIGDIAL 2015 Conference, pages 2\u201311, Prague, Czech Republic, 2-4 September 2015. c\u00a92015 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "In a Spoken Dialogue System (SDS), the Dialogue Manager (DM) is designed in order to implement a decision-making process (called strategy or policy) aiming at choosing the system interaction moves. The decision is taken according to the current interaction context which can rely on bad transcriptions and misunderstandings due to Automatic Speech Recognition (ASR) and Spoken Language Understanding (SLU) errors. Machine learning methods, such as Reinforcement Learning (RL) (Sutton and Barto, 1998), are now very popular to learn optimal dialogue policies under noisy conditions and inter-user variability (Levin and Pieraccini, 1997; Lemon and Pietquin, 2007; Laroche et al., 2010; Young et al., 2013). In this framework, the dialogue task is modeled as a (Partially Observable) Markov Decision Process ((PO)MDP), and the DM is an RL-agent learning an optimal policy. Yet, despite some rare exam-\nples, RL-based DMs only consider task-oriented dialogues and stationary (non-adapting) users.\nUnfortunately, (PO)MDP are restricted to model game-against-nature problems (Milnor, 1951). These are problems in which the learning agent evolves in an environment that doesn\u2019t change with time and acts in a totally disinterested manner. (PO)MDP-based dialogue modeling thus applies only if 1) the user doesn\u2019t modify his/her behavior along time (the strategy is learned for a stationary environment) and 2) the dialogue is task-oriented and requires the user and the machine to positively collaborate to achieve the user\u2019s goal.\nThe first assumption doesn\u2019t hold if the user adapts his/her behavior to the continuously improving performance of a learning DM. Some recent studies have tried to model this co-adaptation effect between a learning machine and a human (Chandramohan et al., 2012b) but this approach still considers the user and the machine as independent learning agents. Although there has already been some few attempts to model the \u201ccoevolution\u201d of human machine interfaces (Bourguin et al., 2001), this work doesn\u2019t extend to RL-based interfaces (automatically learning) and is not related to SDS.\nMore challenging situations do also arise when the common-goal assumption doesn\u2019t hold either, which is the case in many interesting applications such as negotiation (El Asri et al., 2014), serious games, e-learning, robotic co-workers etc. Especially, adapting the MDP paradigm to the case of negotiation dialogues has been the topic of recent works. In (Georgila et al., 2014), the authors model the problem of negotiation as a Multi-Agent Reinforcement Learning (MARL) problem. Yet, this approach relies on algorithms that are treat-\n2\ning the multi-player issue as a non-stationnarity problem (e.g. WoLF-PHC (Bowling and Veloso, 2002)). Each agent is assumed to keep a stable interaction policy for a time sufficiently long so that the other agent can learn it\u2019s current policy. Otherwise, there is no convergence guarantees. Another major issue with these works is that noise in the ASR or NLU results is not taken into account although this is a major reason for using stochastic dialogue models. In (Efstathiou and Lemon, 2014), the authors follow the same direction by considering both agents as acting in a stationary MDP.\nIn this paper, we propose a paradigm shift from the now state-of-the-art (PO)MDP model to Stochastic Games (Patek and Bertsekas, 1999) to model dialogue. This model extends the MDP paradigm to multi-player interactions and allows learning jointly the strategies of both agents (the user and the DM), which leads to the best system strategy in the face of the optimal user/adversary (in terms of his/her goal). This paradigm models both co-adaptation and possible non-cooperativness. Unlike models based on standard game theory (Caelen and Xuereb, 2011), Stochastic Games allow to learn from data. Especially, departing from recent results (Perolat et al., 2015), we show that the optimal strategy can be learned from batch data as for MDPs (Pietquin et al., 2011). This means that optimal negotiation policies can be learnt from non-optimal logged interactions. This new paradigm is also very different from MARL methods proposed in previous work (Chandramohan et al., 2012b; Georgila et al., 2014; Efstathiou and Lemon, 2014) since optimization is jointly performed instead of alternatively optimizing each agent, considering the other can stay stationary for a while. Although experiments are only concerned with purely adversarial tasks (Zero-Sum games), we show that it could be naturally extended to collaborative tasks (general sum games) (Prasad et al., 2015). Experiments show that an efficient strategy can be learned even under noisy conditions which is suitable for modeling realistic human-machine spoken dialogues."}, {"heading": "2 Markov Decision Processes and Reinforcement Learning", "text": "As said before, human-machine dialogue has been modeled as an (PO)MDP to make it suitable for automatic strategy learning (Levin and Pieraccini,\n1997; Young et al., 2013). In this framework, the dialogue is seen as a turn-taking process in which two agents (a user and a DM) interact through a noisy channel (ASR, NLU) to exchange information. Each agent has to take a decision about what to say next according to the dialogue context (also called dialogue state). In this section, MDPs (Puterman, 1994) and RL (Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1996) are briefly reviewed and formally defined which will help switching the Stochastic Games in Section 3."}, {"heading": "2.1 Markov Decision Processes", "text": "Definition 2.1. A Markov Decision Process (MDP) is a tuple \u3008S,A, T ,R, \u03b3\u3009 where: S is the discrete set of environment states, A the discrete set of actions, T : S \u00d7 A \u00d7 S \u2192 [0, 1] the state transition probability function andR : S\u00d7A \u2192 R the reward function. Finally, \u03b3 \u2208 [0, 1) is a discount factor.\nAt each time step, the RL-agent acts according to a policy \u03c0, which is either deterministic or stochastic. In the first case, \u03c0 is a mapping from state space to action space : \u03c0 : S \u2192 A, while in the latter, \u03c0 is a probability distribution on the state-action space \u03c0 : S \u00d7 A \u2192 [0, 1]. Policies are generally designed to maximize the value of each state, i.e. the expected discounted cumulative reward: \u2200s \u2208 S, V \u03c0(s) = E[ \u2211\u221e\nt=0 \u03b3 tr(st, \u03c0(st))|s0 = s]. Let V be the\nspace of all possible value functions. The optimal value function V \u2217 is the only value function such that: \u2200V \u2208 V,\u2200s \u2208 S, V \u2217 \u2265 V . The following result, proved in (Puterman, 1994), is fundamental in the study of MDPs: Theorem 2.1. Let M be an MDP. Its optimal value function V \u2217 exists, is unique and verifies:\n\u2200s \u2208 S, V \u2217(s) = max a\u2208A\n( r(s, a)\n+ \u2211 s\u2032\u2208S T (s, a, s\u2032)V \u2217(s\u2032) ) Furthermore, one can always find a deterministic policy \u03c0\u2217 inducing V \u2217. The function Q\u03c0 : (s, a) 7\u2192 r(s, a) +\u2211 s\u2032\u2208S T (s, a, s\u2032)V\u03c0(s\u2032) is called Q-function. We thus have: \u03c0\u2217(s) = argmaxaQ\u03c0\u2217(s, a) = argmaxaQ\u2217(s, a)."}, {"heading": "2.2 Reinforcement Learning", "text": "In many cases, transition and reward functions are unknown. It is thus not possible to compute values\nnor Q-Functions, the RL-agent learns an approximation by sampling through actual interactions with the environment. The set of techniques solving this problem is called Reinforcement Learning.\nFor instance the Q-Learning algorithm (Watkins and Dayan, 1992) approximates, at each time step, the optimal Q-Function and uses the following update rule:\nQt+1(st, at)\u2190 Qt(st, at) + \u03b1[rt+1(st, at) + \u03b3max\na Qt(st+1, a)\u2212Qt(st, at)]\nIt can been shown that, under the assumption that\u2211 \u03b1 = \u221e and \u2211\u03b12 < \u221e and that all states are visited infinitely often,Q-values converge towards the optimal ones. Thus, by taking at each state the action maximizing those values, one finds the optimal policy. There are batch algorithms solving the same problem among which Fitted-Q (Gordon, 1999; Ernst et al., 2005)."}, {"heading": "3 Stochastic Games", "text": "Stochastic Games (Filar and Vrieze, 1996; Neyman and Sorin, 2003), introduced in (Shapley, 1953), are a natural extension of MDPs to the Multi-Agent setting."}, {"heading": "3.1 Definitions", "text": "Definition 3.1. A discounted Stochastic Game (SG) is a tuple \u3008D,S,A, T ,R, \u03b3\u3009 where: D = {1, ..., n} represents the set of agents, S the discrete set of environment states, A = \u00d7i\u2208DAi the joint action set, where for all i = 1, ..., n, Ai is the discrete set of actions available to the ith agent, T : S \u00d7 A \u00d7 S \u2192 [0, 1] the state transition probability function, R = \u00d7i\u2208DRi the joint reward function, where for all i = 1, ..., n, Ri : S \u00d7A\u2192 R is the reward function of agent i. Finally, \u03b3 \u2208 [0, 1) is a discount factor.\nAn agent i chooses its actions according to some strategy \u03c3i, which is in the general case a probability distribution on i\u2019s state-action space. If the whole space of agents is considered, we speak about the joint strategy \u03c3. The notation \u03c3\u2212i represents the joint strategy of all agents except i.\nThis definition is general, every \u2019MDP\u2019 in which multiple agents interact may be interpreted as a Stochastic Game. It is therefore useful to introduce a taxonomy. A game where there are only two players and where the rewards are opposite (i.e. R1 = \u2212R2) is called Zero-Sum Game.\nConversely, a Purely Cooperative Game is a game where all the agents have the same reward (i.e. \u2200i \u2208 D,Ri = R). A game which is neither ZeroSum nor Purely Cooperative is said to be GeneralSum."}, {"heading": "3.2 Best Response", "text": "In all environments, agents learn by acting according to what has previously been learned. In other words, agents adapt to an environment. This is also valid in a multi-agent scenario, if agent i wants to learn about agent j, it will act according to what has previously been learned about j. But conversely, if j wants to learn about agent i, it will act according to what it knows about i. We say that agents co-adapt. Co-adapation is, due to this feedback loop, an intrinsically non-stationary process. An algorithm converges if it converges to stationary strategies.\nEach agent acts in order to maximize its expected discounted cumulative reward, also called the discounted value of the joint strategy \u03c3 in state s to player i : Vi(s,\u03c3) = E[ \u2211\u221e t=0 \u03b3\ntr(st,\u03c3)]. The Q-function is then defined as (Filar and Vrieze, 1996):\nQ(s,\u03c3, a) = R(s, a) + \u03b3 \u2211 s\u2032\u2208S T (s, a, s\u2032)V (s\u2032,\u03c3)\nThis value function depends on the opponents\u2019 strategies. It is therefore not possible to define in the general case a strategy optimal against every other strategy. A Best Response is an optimal strategy given the opponents ones.\nDefinition 3.2. Agent i plays a Best Response \u03c3i against the other players\u2019 joint strategy \u03c3\u2212i if \u03c3i is optimal given \u03c3\u2212i. We write \u03c3i \u2208 BR(\u03c3\u2212i).\nBest Response induces naturally the following definition:\nDefinition 3.3. The strategy profile {\u03c3i}i\u2208D is a Nash Equilibrium (NE) if for all i \u2208 D, we have \u03c3i \u2208 BR(\u03c3\u2212i).\nIt is interesting to notice that in a single-player game, Nash Equilibrium strategies match the optimal policies defined in the previous section.\nThe existence of Nash Equilibria in all discounted Stochastic Games is assured by the following theorem (Filar and Vrieze, 1996):\nTheorem 3.1. In a discounted Stochastic Game G, there exists a Nash Equilibrium in stationary strategies.\nTwo remarks need to be introduced here. First, nothing was said about uniqueness since in the general case, there are many Nash Equilibria. Equilibrium selection and tracking may be a big deal while working with SGs. Second, contrarily to the MDP case, there may be no deterministic Nash Equilibrium strategies (but only stochastic)."}, {"heading": "3.3 The Zero-Sum Case", "text": "There are two ways to consider a Zero-Sum Stochastic Game: one can see two agents aiming at maximizing two opposite Q-functions or one can also see only one Q-function, with the first agent (called the maximizer) aiming at maximizing it and the second one (the minimizer) aiming at minimizing it. One can prove (Patek and Bertsekas, 1999), that if both players follow those maximizing and minimizing strategies, the game will converge towards a Nash Equilibrium, which is the only one of the game. In this case, thanks to the Minmax theorem (Osborne and Rubinstein, 1994), the value of the game is (with player 1 maximizing and player 2 minimizing):\nV \u2217 = max \u03c31 min \u03c32 V (\u03c31, \u03c32)\n= min \u03c32 max \u03c31 V (\u03c31, \u03c32)\nAs we will see later, the existence of this unique value function for both player is helpful for finding efficient algorithms solving zero-sum SGs."}, {"heading": "4 Algorithms", "text": "Even if the field of Reinforcement Learning in Stochastic Games is still young and guaranteed Nash Equilibrium convergence with tractable algorithms is, according to our knowledge, still an open problem, many algorithms have however already been proposed (Bus\u0327oniu et al., 2008), all with strengths and weaknesses.\nReinforcement Learning techniques to solve Stochastic Games were first introduced in (Littman, 1994). In his paper, Littman presents minimax-Q, a variant of theQ-Learning algorithm for the zero-sum setting, which is guaranteed to converge to the Nash Equilibrium in self-play. He then extended his work in (Littman, 2001) with Friend-or-Foe Q-Learning (FFQ), an algorithm assured to converge, and converging to Nash Equilibria in purely cooperative or purely competitive settings. The authors of (Hu and Wellman, 2003) were the first to propose an algorithm for\ngeneral-sum Stochastic Games. Their algorithm, Nash-Q, is also a variant of Q-Learning able to allow the agents to reach a Nash Equilibrium under some restrictive conditions on the rewards\u2019 distribution. In the general case, they empirically proved that convergence was not guaranteed any more. (Zinkevich et al., 2006) proved by giving a counter-example that the Q-function does not contain enough information to converge towards a Nash Equilibrium in the general setting.\nFor any known Stochastic Game, the Stochastic Tracing Procedure algorithm (Herings and Peeters, 2000) finds a Nash Equilibrium of it. The algorithm proposed in (Akchurina, 2009) was the first learning algorithm converging to an approximate Nash Equilibrium in all settings (even with an unknown game). Equilibrium tracking is made here by solving at each iteration a system of ordinary differential equations. The algorithm has no guaranty to converge toward a Nash Equilibrium even however, it seems empirically to work. Finally, (Prasad et al., 2015) presented two algorithms converging towards a Nash Equilibrium in the General-Sum setting: one batch algorithm assuming the complete knowledge of the game and an on-line algorithm working with simulated transitions of the Stochastic Game.\nIn this paper we will use two algorithms which are reviewed hereafter: WoLF-PHC (Bowling and Veloso, 2002) and AGPI-Q (Perolat et al., 2015)."}, {"heading": "4.1 WoLF-PHC", "text": "WoLF-PHC is an extension of the Q-learning algorithm allowing probabilistic strategies. It considers independent agents evolving in an environment made non-stationary by the presence of the others. In such a setting, the aim of the agents is not to find a Nash Equilibrium (it is therefore not an SG algorithm) but to do as good as possible in this environment (and as a consequence, it may lead to a Nash Equilibrium). The algorithm is based on the following idea: convergence shall be facilitated if agents learn quickly to adapt when they are sub-optimal and learn slowly when they are near-optimal (in order to let the other agents adapt to this strategy). Q-values are updated as in Q-learning and the probability of selecting the best action is incrementally increased according to some (variable) learning rate \u03b4, which is decomposed into two learning rates \u03b4L and \u03b4W , with \u03b4L > \u03b4W . The\npolicy update is made according to \u03b4L while losing and to \u03b4W while winning.\nTo determine if an agent is losing or winning, the expected value of its actual strategy \u03c0, is compared to the expected value of the average policy \u03c0. Formally, an agent is winning if\u2211\na \u03c0(s, a)Q(s, a) > \u2211\na \u03c0(s, a)Q(s, a) and losing otherwise.\nIn the general case, convergence is not proven and it is even shown on some toy-examples that sometimes, the algorithm does not converge (Bowling and Veloso, 2002).\n4.2 AGPI-Q\nApproximate Generalized Policy Iteration-Q, or AGPI-Q (Perolat et al., 2015), is an extension of the Fitted-Q (Gordon, 1999; Ernst et al., 2005) algorithm solving Zero-Sum Stochastic Games in a batch setting. At the initialization step, N samples (s, a1, a2, r, s\u2032) and aQ-function (for instance, the null function) are given. The algorithm consists then in K iterations, each of them composed of two parts : a greedy part and an evaluation part. The algorithm provides then at each iteration a better approximation of the Q-function.\nLet j = (sj , aj , bj , rj , s\u2032j) be N collected samples. At time step k + 1, the greedy part consists of finding the maximizer\u2019s maxminimizing action a of the matrix game defined by Qjk(s\n\u2032j , aj , bj). In our case, a turn-based setting, this involves finding a maximum. Then, during the evaluation part, since the second agent plays a minimizing strategy, the following value is computed: Qj = r + \u03b3minbQ j k(s \u2032j , aj , b). At each iteration, the algorithm returns the Q-function Qk+1 fitting at best these values over some hypothesis space."}, {"heading": "5 Dialogue as a Stochastic Game", "text": "Dialogue is a multi-agent interaction and therefore, it shall be considered as such during the optimization process. If each agent (i.e. the user and the DM) has its own goals and takes its decisions to achieve them, it sounds natural to model it as an MDP. In traditional dialogue system studies, this is only done for one conversant over two. Since (Levin and Pieraccini, 1997; Singh et al., 1999), only the DM is encoded as an RL agent, despite rare exceptions (Chandramohan et al., 2011; Chandramohan et al., 2012b; Chandramohan et al., 2012a)). The user is rather considered as a stationary agent modeled as a Bayesian net-\nwork (Pietquin, 2006) or an agenda-based process (Schatzmann et al., 2007), leading to modeling errors (Schatztnann et al., 2005; Pietquin and Hastie, 2013).\nAt first sight, it seems reasonable to think that if two RL agents, previously trained to reach an optimal strategy, interact with each other, it would result in \u201doptimal\u201d dialogues. Yet, this assertion is wrong. Each agent would be optimal given the environment it\u2019s been trained on, but given another environment, nothing can be said about the learnt policy. Furthermore, if two DMs are trained together with traditional RL techniques, no convergence is guaranteed since, as seen above, nonstationarities emerge. Indeed, non-stationarity is not well managed by standard RL methods although some methods can deal with it (Geist et al., 2009; Daubigney et al., 2012) but adaptation might not be fast enough.\nJointly optimizing RL-agents in the framework of Stochastic Games finds a Nash Equilibrium. This guarantees both strategies to be optimal and this makes a fundamental difference with previous work (Chandramohan et al., 2012b; Georgila et al., 2014; Efstathiou and Lemon, 2014).\nIn the next section, we illustrate how dialogue may be modeled by a Stochastic Game, how transitions and reward functions depend on the policy of both agents. We propose now a Zero-Sum dialogue game where agents have to drive efficiently the dialogue to gather information quicker than their opponent. In this example, human user (Agent 1) and DM (Agent 2) are modeled with MDPs: each of them has a goal encoded into reward functions R1 and R2 (they may depend on the joint action)."}, {"heading": "5.1 A Zero-Sum Dialogue Game", "text": "The task involves two agents, each of them receives a random secret number and aims at guessing the other agent\u2019s number. They are adversaries: if one wins, the other one loses as much.\nTo find the secret number out, agents may perform one of the following actions: ask, answer, guess, ok, confirm and listen.\nDuring a dialogue turn, the agent asking the question is called the guesser and the one answering is the opponent. To retrieve information about the opponent\u2019s hidden number, the guesser may ask if this number is smaller or greater than some other number. The opponent is forced to answer\nthe truth. To show that it has understood the answer, the agent says ok and releases then the turn to its adversary, which endorses the guesser\u2019s role.\nAgents are not perfect, they can misunderstand what has been said. This simulates ASR and NLU errors arising in real SDSs. They have an indicator giving a hint about the probability of having well understood (a confidence level). They are however never certain and they may answer a wrong question, e.g. in the following exchange :\n- Is your secret number greater than x ? - My number is greater than y. When such an error arises, Agent 1 is allowed to ask another question instead of just saying ok. This punishment is harsh for the agent which misunderstood, it is almost as if it has to pass its turn. Another dialogue act is introduced to deal with such situations. If an agent is not sure, it may ask to confirm. In this case, Agent 1 may ask its question again. To avoid abuses, i.e. infinitely ask for a confirmation, this action induces a cost (and therefore a gain for the opponent).\nIf an agent thinks that it has found the number out, it can make a guess. If it was right, it wins (and therefore its opponent loses), otherwise, it loses (and its opponent wins).\nSince we model dialogue as a turn-based interaction and we will need to consider joint actions, we introduce the action listen corresponding to the empty action."}, {"heading": "6 Experimental Setting", "text": "Effects of the multi-agent setting are studied here through one special feature of the human-machine dialogue: the uncertainty management due to the dysfunctions of the ASR and the NLU. To promote simple algorithms, we ran our experiments on the zero-sum dialogue game presented above.\nOn this task, we compare three algorithms: QLearning, WoLF-PHC and AGPI-Q. Among those algorithms, only AGPI-Q is proved to converge towards a Nash Equilibrium in a Multi-Agent setting. Q-Learning and WoLF-PHC have however been used as Multi-Agent learning algorithm in a dialogue setting (English and Heeman, 2005; Georgila et al., 2014). Similarly to these papers, experiments will be done using simulation. We will show that, contrarily to AGPI-Q, they do not converge towards the Nash Equilibrium and therefore do not fit to the dialogue problem."}, {"heading": "6.1 Modeling ASR and NLU Confidence Estimation", "text": "One difficulty while working with Spoken Dialogue Systems is how can a DM deal with uncertainty resulting from ASR and NLU errors and reflected by their Confidence Scores. Those scores are not always a probability. The only assumption made here is that with a score lower (resp. greater) than 0.5, the probability to misunderstand the last utterance is greater (resp. lower) than 0.5. Since dialogues are simulated, the ASR and NLU confidence levels will be modeled the following way.\nEach agent owns some fixed Sentence Error Rate (SERi). With probability (1 \u2212 SERi), agent i receives each utterance undisrupted, while with probability SERi, this utterance is misunderstood and replaced by another one.\nA (\u2212\u221e,\u221e) score is then sampled according to a normal distribution centered in -1 for incorrect understanding and +1 for correct understanding. The (0,1) score is obtained by applying the sigmoid function f(x) = 11+exp(\u2212x) , to the (\u2212\u221e,\u221e) score.\nSince Q-Learning and WoLF-PHC are used in their tabular form, it was necessary to discretize this score. To have states where the agent is almost sure of having understood (or sure of having misunderstood), we discretized by splitting the score around the cut points 0.1, 0.5 and 0.9. By equity concerns, the same discretization was applied for the AGPI-Q algorithm."}, {"heading": "6.2 Task Modeling", "text": ""}, {"heading": "6.2.1 State Space", "text": "Consider two agents i and j. Their secret numbers are respectively m and n. To gather information about m, agent i asks if the secret number m is smaller or greater than some given number k. If agent j answers that m is greater (resp. smaller) than k, it will provides i a lower bound bi (resp. an upper bound b\u2032i) on m. Agent i\u2019s knowledge on m may be represented by the interval Ii = [bi, b\u2032i]. The probability of wining by making a guess is then given by p = 1\nb\u2032i\u2212bi+1 . Progress of agent i in the game may therefore measured by only ci = b\u2032i \u2212 bi + 1, the cardinal of Ii. At the beginning of the game, one has: Ii = Ij = [1, 5]. Since agents have to know the progress of the whole game, they both track ci and cj .\nTo take an action, an agent needs to remember who pronounced the last utterance, what was the\nlast utterance it heard and to what extent it believes that what it heard was what had been said.\nTo summarize, agents taking actions make their decision according to the following features: the last utterance, its trust in this utterance, who uttered it, its progress in the game and its opponent\u2019s progress. they do not need to track the whole range of possible secret numbers but only the cardinal of these sets. Dialogue turn, last action, confidence score, cardinal of possible numbers for both agents are thus the five state features. The state space thus contains 2 \u2217 5 \u2217 4 \u2217 5 \u2217 5 = 1000 states."}, {"heading": "6.2.2 Action Space", "text": "Agents are able to make one of the following actions: ask, answer, guess, confirm and listen. The actions ask, answer and guess need an argument: the number the agent wants to compare to. To learn quicker, we chose not to take a decision about this value. When an agent asks, it asks if the secret number is greater or smaller than the number in the middle of his range (this range is computed by the environment, it is not taken into account in the states). An agent answering says that her secret number is greater or smaller than the number it heard (which may be not the uttered number). An agent guessing proposes randomly a number in his range of possible values."}, {"heading": "6.2.3 Reward function", "text": "To define the reward function, we consider the maximizing player. It is its turn to play. If it is guessing the right number, it earns +1. If it asks for a confirmation, it earns \u22120.2. Therefore, it is never in its interest to block the dialogue by always asking for a confirmation (in the worst case, ie if second agent immediately wins, it earns \u22121 while if it infinitely blocks the dialogue, it earns \u22120.2\u2211\u221ek=0(\u03b32)k \u2248 \u22121.05 for \u03b3 = 0.9)."}, {"heading": "6.3 Training of the algorithms", "text": "To trainQ-Learning and WoLF-PHC, we followed the setup proposed in (Georgila et al., 2014). Both algorithms are trained in self-play by following an -greedy policy. Training is split into five epochs of 100000 dialogues. The exploration rate is set to 0.95 in the first epoch, 0.8 in the second, 0.5 in the third, 0.3 in the fourth and 0.1 in the fifth.\nThe parameters \u03b4L and \u03b4W of WoLF-PHC are set to \u03b4W = 0.05 and \u03b4L = 0.2. The ratio \u03b4L/\u03b4W = 4 assures an aggressive learning when losing.\nAs a batch RL algorithm, AGPI-Q requires samples. To generate them, we followed the setup proposed in (Pietquin et al., 2011). An optimal (or at least near) policy is first handcrafted. This policy is the following: an agent always asks for more information except when it or its opponent have enough information to make the right guess with probability 1. When the agent has to answer, it asks to confirm if its confidence score is below 0.5.\nAn -random policy is then designed. Agents make their decisions according the hand-crafted policy with probability and pick randomly actions with probability (1 \u2212 ). Tuples (s, a1, a2, r, s\u2032) are then gathered. We are then assured that the problem space is well-sampled and that there also exists samples giving the successful task completion reward. To ensure convergence, 75000 such dialogues are generated.\nTo keep the model as parameter-free as possible, CART trees are used as hypothesis space for the regression.\nEach algorithm is trained with the following SER values: 0, 0.1, 0.2, 0.3 and 0.4."}, {"heading": "6.4 Results", "text": "The decision in the game is made on only two points: when is the best moment to end the dialogue with the guess action and what is the best way to deal with uncertainty by the use of the confirm action. Average duration of dialogues and average number of confirm actions are therefore chosen as the feature characterizing the Nash Equilibrium. Both are calculated over 5000 dialogues. Figures 1 and 2 illustrate those results. Q-Learning dialogues\u2019 length decreases gradually with respect to an increasing SER (Figure 1). Figure 2 brings an explanation: Q-Learning agents do not learn to use the CONFIRM action. More, dialogue length is even not regular, proving that the algorithm did not converge to a \u2019stable\u2019 policy. Q-Learning is a slow algorithm and therefore, agents do not have enough time to face the non-stationarities of the multi-agent environment. Convergence is thus not possible.\nWoLF-PHC does not treat uncertainty too. Its number of confirm actions is by far the highest but stays constant. If the SDS asks for confirmation, even when there is no noise, it may be because being disadvantaged, it always loses, and\nwhile losing, its quick learning rate makes its strategy always changing. As previously said, convergence was not guaranteed.\nAGPI-Q is then the only algorithm providing robustness against noise. The length of dialogues and the number of confirm actions increase both gradually with the SER of the SDS. We are also assured by the theory that in this setting, no improvement is possible.\nIt is also interesting to note the emergence of non-trivial strategies coming from the interaction between the AGPI-Q agents. For instance, when both agents are almost at the end of the dialogue (ci = 2 for each agent), agents make guess. Even if they have very low chances of wining, agents make also guess when it is sure that the adversary will win at the next turn."}, {"heading": "7 Conclusion: Beyond the Zero-Sum Setting", "text": "We provided a rigorous framework for co-learning in Dialogue Systems allowing optimization for both conversants. Its efficiency was shown on a purely adversarial setting under noisy conditions and an extension to situations more general than the purely adversarial setting is now proposed."}, {"heading": "7.1 An appointment scheduling problem", "text": "The previous model considers only purely competitive scenarios. In this section, it is extended for the General-Sum case. We take as an example the task of scheduling the best appointment between two agents, where conversants have to interact to find an agreement.\nEach agent i has its own preferences about a slot in their agenda, they are encoded into some reward function Ri. At each turn, an agent proposes some slot k. Next turn, its interlocutor may propose another slot or accept this one. If it accepts, agent i earnsRi(k), it gets nothing otherwise. The conversation ends when an agent accepts an offered slot.\nAgents, which are not always perfect, can misunderstand the last offer. An action confirm is therefore introduced. If an agent thinks that the last offer was on the slot k\u2032 instead of the slot k, the outcome may be disastrous. An agent has thus always to find a trade-off between the uncertainty management on the last offer and its impatience, (due to the discount factor \u03b3 which penalizes long dialogues).\nHere, cooperation is implicit. Conversants are self-centered, they care only on their own value functions, but, since it depends on both actions, or more explicitly the opponent may refuse an offer, they have to take into account the opponent\u2019s behavior."}, {"heading": "7.2 Future work", "text": "In future, using General-Sum algorithms (Prasad et al., 2015), our framework will be applied on those much more complicated dialogue situations where cooperative and competitive phenomenon get mixed up in addition to the noisy conditions encountered in dialogue.\nThe long-term goal of this work is to use the model on a real data set in order to provide model of real interactions and designing adaptive SDS freeing ourselves from user modeling."}, {"heading": "Acknowledgement", "text": "This work has been partially funded by the French National Agency for Research (ANR) through the ContInt Project MaRDi (Man-Robot Dialogue) and by the French Ministry for Higher Education and Research (MESR)."}], "year": 2015, "references": [{"title": "Multiagent reinforcement learning: algorithm converging to nash equilibrium in general-sum discounted stochastic games", "authors": ["Natalia Akchurina."], "venue": "Proc. of AAMAS.", "year": 2009}, {"title": "NeuroDynamic Programming", "authors": ["Dimitri P. Bertsekas", "John Tsitsiklis."], "venue": "Athena Scientific.", "year": 1996}, {"title": "Beyond the interface: Co-evolution inside interactive systems - a proposal founded on activity theory", "authors": ["Gr\u00e9gory Bourguin", "Alain Derycke", "Jean-Claude Tarby."], "venue": "People and Computers XV-Interaction without Frontiers, pages 297\u2013310.", "year": 2001}, {"title": "Multiagent learning using a variable learning rate", "authors": ["Michael Bowling", "Manuela Veloso."], "venue": "Artificial Intelligence, 136(2):215\u2013250.", "year": 2002}, {"title": "A comprehensive survey of multiagent reinforcement learning", "authors": ["Lucian Bu\u015foniu", "Robert Babuska", "Bart De Schutter."], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156\u2013172.", "year": 2008}, {"title": "Dialogue et th\u00e9orie des jeux", "authors": ["Jean Caelen", "Anne Xuereb."], "venue": "Congr\u00e9s international SPeD.", "year": 2011}, {"title": "User Simulation in Dialogue Systems using Inverse Reinforcement Learning", "authors": ["Senthilkumar Chandramohan", "Matthieu Geist", "Fabrice Lef\u00e8vre", "Olivier Pietquin."], "venue": "Proc. of Interspeech.", "year": 2011}, {"title": "Behavior Specific User Simulation in Spoken Dialogue Systems", "authors": ["Senthilkumar Chandramohan", "Matthieu Geist", "Fabrice Lef\u00e8vre", "Olivier Pietquin."], "venue": "Proc. of ITG Conference on Speech Communication.", "year": 2012}, {"title": "Coadaptation in Spoken Dialogue Systems", "authors": ["Senthilkumar Chandramohan", "Matthieu Geist", "Fabrice Lef\u00e8vre", "Olivier Pietquin."], "venue": "Proc. of IWSDS.", "year": 2012}, {"title": "A comprehensive reinforcement learning framework for dialogue management optimization", "authors": ["Lucie Daubigney", "Matthieu Geist", "Senthilkumar Chandramohan", "Olivier Pietquin."], "venue": "IEEE Journal of Selected Topics in Signal Processing, 6(8):891\u2013902.", "year": 2012}, {"title": "Learning non-cooperative dialogue behaviours", "authors": ["Ioannis Efstathiou", "Oliver Lemon."], "venue": "Proc. of SIGDIAL.", "year": 2014}, {"title": "Dinasti : Dialogues with a negotiating appointment setting interface", "authors": ["Layla El Asri", "Romain Laroche", "Olivier Pietquin."], "venue": "Proc. of LREC.", "year": 2014}, {"title": "Learning mixed initiative dialog strategies by using reinforcement learning on both conversants", "authors": ["Michael S. English", "Peter A. Heeman."], "venue": "Proc. of HLT/EMNLP.", "year": 2005}, {"title": "Tree-based batch mode reinforcement learning", "authors": ["Damien Ernst", "Pierre Geurts", "Louis Wehenkel."], "venue": "pages 503\u2013556.", "year": 2005}, {"title": "Competitive Markov decision processes", "authors": ["Jerzy Filar", "Koos Vrieze."], "venue": "Springer.", "year": 1996}, {"title": "Tracking in reinforcement learning", "authors": ["Matthieu Geist", "Olivier Pietquin", "Gabriel Fricout."], "venue": "Proc. of ICONIP.", "year": 2009}, {"title": "Single-agent vs", "authors": ["Kallirroi Georgila", "Claire Nelson", "David Traum."], "venue": "multi-agent techniques for concurrent reinforcement learning of negotiation dialogue policies. In Proc. of ACL.", "year": 2014}, {"title": "Approximate Solutions to Markov Decision Processes", "authors": ["Geoffrey J. Gordon."], "venue": "Ph.D. thesis, Carnegie Melon University.", "year": 1999}, {"title": "Stationary equilibria in stochastic games: structure, selection and computation", "authors": ["P. Jean-Jacques Herings", "Ronald Peeters"], "year": 2000}, {"title": "Nash qlearning for general-sum stochastic games", "authors": ["Junling Hu", "Michael P. Wellman."], "venue": "Journal of Machine Learning Research, 4:1039\u20131069.", "year": 2003}, {"title": "Optimising a handcrafted dialogue system design", "authors": ["Romain Laroche", "Ghislain Putois", "Philippe Bretier."], "venue": "Proc. of Interspeech.", "year": 2010}, {"title": "Machine learning for spoken dialogue systems", "authors": ["Oliver Lemon", "Olivier Pietquin."], "venue": "Proc. of Interspeech.", "year": 2007}, {"title": "A stochastic model of computer-human interaction for learning dialogue strategies", "authors": ["Esther Levin", "Roberto Pieraccini."], "venue": "Proc. of Eurospeech.", "year": 1997}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "authors": ["Michael L. Littman."], "venue": "Proc. of ICML.", "year": 1994}, {"title": "Friend-or-foe q-learning in general-sum games", "authors": ["Michael L. Littman."], "venue": "Proc. of ICML.", "year": 2001}, {"title": "Games against nature", "authors": ["John Milnor."], "venue": "Technical report, RAND corporation.", "year": 1951}, {"title": "Stochastic games and applications, volume 570", "authors": ["Abraham Neyman", "Sylvain Sorin."], "venue": "Springer Science & Business Media.", "year": 2003}, {"title": "A course in game theory", "authors": ["Martin J. Osborne", "Ariel Rubinstein."], "venue": "MIT press.", "year": 1994}, {"title": "Stochastic shortest path games", "authors": ["Stephen D. Patek", "Dimitri P. Bertsekas."], "venue": "SIAM Journal on Control and Optimization, 37(3).", "year": 1999}, {"title": "Approximate dynamic programming for two-player zero-sum markov games", "authors": ["Julien Perolat", "Bilal Piot", "Bruno Scherrer", "Olivier Pietquin."], "venue": "Proc. of ICML.", "year": 2015}, {"title": "A survey on metrics for the evaluation of user simulations", "authors": ["Olivier Pietquin", "Helen Hastie."], "venue": "The knowledge engineering review, 28(01):59\u201373.", "year": 2013}, {"title": "Sampleefficient batch reinforcement learning for dialogue management optimization", "authors": ["Olivier Pietquin", "Matthieu Geist", "Senthilkumar Chandramohan", "Herv\u00e9 Frezza-Buet."], "venue": "ACM Transactions on Speech and Language Processing (TSLP), 7(3).", "year": 2011}, {"title": "Consistent goal-directed user model for realistic man-machine task-oriented spoken dialogue simulation", "authors": ["Olivier Pietquin."], "venue": "Proc of ICME.", "year": 2006}, {"title": "Algorithms for nash equilibria in general-sum stochastic games", "authors": ["H.L. Prasad", "L.A. Prashanth", "Shalabh Bhatnagar."], "venue": "Proc. of AAMAS.", "year": 2015}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "authors": ["Martin L. Puterman."], "venue": "John Wiley & Sons.", "year": 1994}, {"title": "Agenda-based user simulation for bootstrapping a pomdp dialogue system", "authors": ["Jost Schatzmann", "Blaise Thomson", "Karl Weilhammer", "Hui Ye", "Steve Young."], "venue": "Proc. of HLT.", "year": 2007}, {"title": "Effects of the user model on simulation-based learning of dialogue strategies", "authors": ["Jost. Schatztnann", "Matthew Stuttle", "Konrad Weilhammer", "Steve Young."], "venue": "Proc. of ASRU.", "year": 2005}, {"title": "Stochastic games", "authors": ["Lloyd Shapley."], "venue": "Proc. of the National Academy of Sciences of the United States of America, 39(10):1095\u20131100.", "year": 1953}, {"title": "Reinforcement learning for spoken dialogue systems", "authors": ["Satinder P. Singh", "Michael J. Kearns", "Diane J. Litman", "Marilyn A. Walker."], "venue": "Proc. of NIPS.", "year": 1999}, {"title": "Reinforcement learning: An introduction", "authors": ["Richard S. Sutton", "Andrew G. Barto."], "venue": "MIT press.", "year": 1998}, {"title": "Qlearning", "authors": ["Christopher Watkins", "Peter Dayan."], "venue": "Machine learning, 8(3-4):279\u2013292.", "year": 1992}, {"title": "Pomdp-based statistical spoken dialog systems: A review", "authors": ["Steve Young", "Milica Gasic", "Blaise Thomson", "Jason D. Williams."], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.", "year": 2013}, {"title": "Cyclic equilibria in markov games", "authors": ["Martin Zinkevich", "Amy Greenwald", "Michael L. Littman."], "venue": "Proc. of NIPS.", "year": 2006}], "id": "SP:08679aac2d98f95855465bd87883e28846bbab28", "authors": [{"name": "Merwan Barlier", "affiliations": []}, {"name": "Julien Perolat", "affiliations": []}, {"name": "Romain Laroche", "affiliations": []}, {"name": "Olivier Pietquin", "affiliations": []}], "abstractText": "In this paper, an original framework to model human-machine spoken dialogues is proposed to deal with co-adaptation between users and Spoken Dialogue Systems in non-cooperative tasks. The conversation is modeled as a Stochastic Game: both the user and the system have their own preferences but have to come up with an agreement to solve a non-cooperative task. They are jointly trained so the Dialogue Manager learns the optimal strategy against the best possible user. Results obtained by simulation show that non-trivial strategies are learned and that this framework is suitable for dialogue modeling.", "title": "Human-Machine Dialogue as a Stochastic Game"}