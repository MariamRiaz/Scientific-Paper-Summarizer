0,1,label2,summary_sentences
"Dependency parsing is a longstanding natural language processing task, with its outputs crucial to various downstream tasks including relation extraction (Schmitz et al., 2012; Angeli et al., 2015), language modeling (Gubbins and Vlachos, 2013), and natural logic inference (Bowman et al., 2016).
",1 Introduction,[0],[0]
"Attractive for their linear time complexity and amenability to conventional classification methods, transition-based dependency parsers have sparked much research interest recently.",1 Introduction,[0],[0]
"A transition-based parser makes sequential predictions of transitions between states under the restrictions of a transition system (Nivre, 2003).",1 Introduction,[0],[0]
"Transition-based parsers have been shown to excel at parsing shorter-range dependency structures, as well as languages where non-projective parses are less pervasive (McDonald and Nivre, 2007).
",1 Introduction,[0],[0]
"However, the transition systems employed in state-of-the-art dependency parsers usually define very local transitions.",1 Introduction,[0],[0]
"At each step, only one or two words are affected, with very local attachments made.",1 Introduction,[0],[0]
"As a result, distant attachments require long and not immediately obvious transition sequences (e.g., ate→chopsticks in Figure 1, which requires two transitions).",1 Introduction,[0],[0]
"This is further aggravated by the usually local lexical information leveraged to make transition predictions (Chen and Manning, 2014; Andor et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we introduce a novel transition system, arc-swift, which defines non-local transitions that directly induce attachments of distance up to n (n = the number of tokens in the sentence).",1 Introduction,[0],[0]
"Such an approach is connected to graph-based dependency parsing, in that it leverages pairwise scores between tokens in making parsing decisions (McDonald et al., 2005).
",1 Introduction,[0],[0]
We make two main contributions in this paper.,1 Introduction,[0],[0]
"Firstly, we introduce a novel transition system for dependency parsing, which alleviates the difficulty of distant attachments in previous systems by allowing direct attachments anywhere in the stack.",1 Introduction,[0],[0]
"Secondly, we compare parsers by the number of mistakes they make in common linguistic con-
ar X
iv :1
70 5.
04 43
4v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
2 M
ay 2
01 7
structions.",1 Introduction,[0],[0]
We show that arc-swift parsers reduce errors in attaching prepositional phrases and conjunctions compared to parsers using existing transition systems.,1 Introduction,[0],[0]
Transition-based dependency parsing is performed by predicting transitions between states (see Figure 1 for an example).,2 Transition-based Dependency Parsing,[0],[0]
"Parser states are usually written as (σ|i, j|β,A), where σ|i denotes the stack with token i on the top, j|β denotes the buffer with token j at its leftmost, and A the set of dependency arcs.",2 Transition-based Dependency Parsing,[0],[0]
"Given a state, the goal of a dependency parser is to predict a transition to a new state that would lead to the correct parse.",2 Transition-based Dependency Parsing,[0],[0]
"A transition system defines a set of transitions that are sound and complete for parsers, that is, every transition sequence would derive a well-formed parse tree, and every possible parse tree can also be derived from some transition sequence.1
Arc-standard (Nivre, 2004) is one of the first transition systems proposed for dependency parsing.",2 Transition-based Dependency Parsing,[0],[0]
"It defines three transitions: shift, left arc (LArc), and right arc (RArc) (see Figure 2 for definitions, same for the following transition systems), where all arc-inducing transitions operate on the stack.",2 Transition-based Dependency Parsing,[0],[0]
"This system builds the parse bottom-up, i.e., a constituent is only attached to its head after it has received all of its dependents.",2 Transition-based Dependency Parsing,[0],[0]
"A potential drawback is that during parsing, it is difficult to predict if a constituent has consumed all of its right dependents.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-eager (Nivre, 2003) remedies this drawback by defining arc-inducing transitions that operate between the stack and the buffer.",2 Transition-based Dependency Parsing,[0],[0]
"As a result, a constituent no longer needs to be complete
1We only focus on projective parses for the scope of this paper.
",2 Transition-based Dependency Parsing,[0],[0]
"before it can be attached to its head to the left, as a right arc doesn’t prevent the attached dependent from taking further dependents of its own.2 Kuhlmann et al. (2011) propose a hybrid system derived from a tabular parsing scheme, which they have shown both arc-standard and arc-eager can be derived from.",2 Transition-based Dependency Parsing,[0],[0]
"Arc-hybrid combines LArc from arc-eager and RArc from arc-standard to build dependencies bottom-up.
",2 Transition-based Dependency Parsing,[0],[0]
"3 Non-local Transitions with arc-swift
The traditional transition systems discussed in Section 2 only allow very local transitions affecting one or two words, which makes long-distance dependencies difficult to predict.",2 Transition-based Dependency Parsing,[0],[0]
"To illustrate the limitation of local transitions, consider parsing the following sentences:
I ate fish with ketchup.",2 Transition-based Dependency Parsing,[0],[0]
"I ate fish with chopsticks.
",2 Transition-based Dependency Parsing,[0],[0]
"The two sentences have almost identical structures, with the notable difference that the prepositional phrase is complementing the direct object in the first case, and the main verb in the second.
",2 Transition-based Dependency Parsing,[0],[0]
"For arc-standard and arc-hybrid, the parser would have to decide between Shift and RArc when the parser state is as shown in Figure 3a, where ? stands for either “ketchup” or “chopsticks”.3 Similarly, an arc-eager parser would deal with the state shown in Figure 3b.",2 Transition-based Dependency Parsing,[0],[0]
"Making the correct transition requires information about context words “ate” and “fish”, as well as “?”.
",2 Transition-based Dependency Parsing,[0],[0]
2A side-effect of arc-eager is that there is sometimes spurious ambiguity between Shift and Reduce transitions.,2 Transition-based Dependency Parsing,[0],[0]
"For the example in Figure 1, the first Reduce can be inserted before the third Shift without changing the correctness of the resulting parse, i.e., both are feasible at that time.
",2 Transition-based Dependency Parsing,[0],[0]
"3For this example, we assume that the sentence is being parsed into Universal Dependencies.
",2 Transition-based Dependency Parsing,[0],[0]
"Parsers employing traditional transition systems would usually incorporate more features about the context in the transition decision, or employ beam search during parsing (Chen and Manning, 2014; Andor et al., 2016).
",2 Transition-based Dependency Parsing,[0],[0]
"In contrast, inspired by graph-based parsers, we propose arc-swift, which defines non-local transitions as shown in Figure 2.",2 Transition-based Dependency Parsing,[0],[0]
"This allows direct comparison of different attachment points, and provides a direct solution to parsing the two example sentences.",2 Transition-based Dependency Parsing,[0],[0]
"When the arc-swift parser encounters a state identical to Figure 3b, it could directly compare transitions RArc[1] and RArc[2] instead of evaluating between local transitions.",2 Transition-based Dependency Parsing,[0],[0]
"This results in a direct attachment much like that in a graph-based parser, informed by lexical information about affinity of the pairs of words.
",2 Transition-based Dependency Parsing,[0],[0]
Arc-swift also bears much resemblance to arceager.,2 Transition-based Dependency Parsing,[0],[0]
"In fact, an LArc[k] transition can be viewed as k− 1 Reduce operations followed by one LArc in arc-eager, and similarly for RArc[k].",2 Transition-based Dependency Parsing,[0],[0]
"Reduce is no longer needed in arc-swift as it becomes part of LArc[k] and RArc[k], removing the ambiguity in derived transitions in arc-eager.",2 Transition-based Dependency Parsing,[0],[0]
"arc-swift is also equivalent to arc-eager in terms of soundness and completeness.4 A caveat is that the worst-case time complexity of arc-swift is O(n2) instead of O(n), which existing transition-based parsers enjoy.",2 Transition-based Dependency Parsing,[0],[0]
"However, in practice the runtime is nearly
4This is easy to show because in arc-eager, all Reduce transitions can be viewed as preparing for a later LArc or RArc transition.",2 Transition-based Dependency Parsing,[0],[0]
"We also note that similar to arc-eager transitions, arc-swift transitions must also satisfy certain pre-conditions.",2 Transition-based Dependency Parsing,[0],[0]
"Specifically, an RArc[k] transition requires that the top k − 1 elements in the stack are already attached; LArc[k] additionally requires that the k-th element is unattached, resulting in no more than one feasible LArc candidate for any parser state.
",2 Transition-based Dependency Parsing,[0],[0]
"linear, thanks to the usually small number of reducible tokens in the stack.",2 Transition-based Dependency Parsing,[0],[0]
"We use the Wall Street Journal portion of Penn Treebank with standard parsing splits (PTBSD), along with Universal Dependencies v1.3 (Nivre et al., 2016) (EN-UD).",4.1 Data and Model,[0],[0]
"PTB-SD is converted to Stanford Dependencies (De Marneffe and Manning, 2008) with CoreNLP 3.3.0 (Manning et al., 2014) following previous work.",4.1 Data and Model,[0],[0]
"We report labelled and unlabelled attachment scores (LAS/UAS), removing punctuation from all evaluations.
",4.1 Data and Model,[0],[0]
"Our model is very similar to that of (Kiperwasser and Goldberg, 2016), where features are extracted from tokens with bidirectional LSTMs, and concatenated for classification.",4.1 Data and Model,[0],[0]
"For the three traditional transition systems, features of the top 3 tokens on the stack and the leftmost token in the buffer are concatenated as classifier input.",4.1 Data and Model,[0],[0]
"For arc-swift, features of the head and dependent tokens for each arc-inducing transition are concatenated to compute scores for classification, and features of the leftmost buffer token is used for Shift.",4.1 Data and Model,[0],[0]
For other details we defer to Appendix A.,4.1 Data and Model,[0],[0]
The full specification of the model can also be found in our released code online at https://github.,4.1 Data and Model,[0],[0]
com/qipeng/arc-swift.,4.1 Data and Model,[0],[0]
"We use static oracles for all transition systems, and for arc-eager we implement oracles that always Shift/Reduce when ambiguity is present (arceager-S/R).",4.2 Results,[0],[0]
"We evaluate our parsers with greedy parsing (i.e., beam size 1).",4.2 Results,[0],[0]
"The results are shown in Table 1.5 Note that K&G 2016 is trained with a dynamic oracle (Goldberg and Nivre, 2012), Andor 2016 with a CRF-like loss, and both Andor 2016 and Weiss 2015 employed beam search (with sizes 32 and 8, respectively).
",4.2 Results,[0],[0]
"For each pair of the systems we implemented, we studied the statistical significance of their difference by performing a paired test with 10,000 bootstrap samples on PTB-SD.",4.2 Results,[0],[0]
"The resulting pvalues are analyzed with a 10-group BonferroniHolm test, with results shown in Table 2.",4.2 Results,[0],[0]
"We note
5In the interest of space, we abbreviate all transition systems (TS) as follows in tables: asw for arc-swift, asd for arcstandard, aeS/R for arc-eager-S/R, and ah for arc-hybrid.
",4.2 Results,[0],[0]
"that with almost the same implementation, arcswift parsers significantly outperform those using traditional transition systems.",4.2 Results,[0],[0]
We also analyzed the performance of parsers on attachments of different distances.,4.2 Results,[0],[0]
"As shown in Figure 4, arc-swift is equally accurate as existing systems for short dependencies, but is more robust for longer ones.
",4.2 Results,[0],[0]
"While arc-swift introduces direct long-distance transitions, it also shortens the overall sequence necessary to induce the same parse.",4.2 Results,[0],[0]
"A parser could potentially benefit from both factors: direct attachments could make an easier classification task, and shorter sequences limit the effect of error propagation.",4.2 Results,[0],[0]
"However, since the two effects are correlated in a transition system, precise attribution of the gain is out of the scope of this paper.
",4.2 Results,[0],[0]
Computational efficiency.,4.2 Results,[0],[0]
"We study the computational efficiency of the arc-swift parser by
6https://github.com/tensorflow/models/ blob/master/syntaxnet/g3doc/universal.md
comparing it to an arc-eager parser.",4.2 Results,[0],[0]
"On the PTBSD development set, the average transition sequence length per sentence of arc-swift is 77.5% of that of arc-eager.",4.2 Results,[0],[0]
"At each step of parsing, arc-swift needs to evaluate only about 1.24 times the number of transition candidates as arc-eager, which results in very similar runtime.",4.2 Results,[0],[0]
"In contrast, beam search with beam size 2 for arc-eager requires evaluating 4 times the number of transition candidates compared to greedy parsing, which results in a UAS 0.14% worse and LAS 0.22% worse for arc-eager compared to greedily decoded arcswift.",4.2 Results,[0],[0]
"We automatically extracted all labelled attachment errors by error type (incorrect attachment or relation), and categorized a few top parser errors by hand into linguistic constructions.",4.3 Linguistic Analysis,[0],[0]
"Results on PTB-SD are shown in Table 3.7 We note that the arc-swift parser improves accuracy on prepositional phrase (PP) and conjunction attachments, while it remains comparable to other parsers on other common errors.",4.3 Linguistic Analysis,[0],[0]
Analysis on EN-UD shows a similar trend.,4.3 Linguistic Analysis,[0],[0]
"As shown in the table, there are still many parser errors unaccounted for in our analysis.",4.3 Linguistic Analysis,[0],[0]
"We leave this to future work.
",4.3 Linguistic Analysis,[0],[0]
"7We notice that for some examples the parsers predicted a ccomp (complement clause) attachment to verbs “says” and “said”, where the CoreNLP output simply labelled the relation as dep (unspecified).",4.3 Linguistic Analysis,[0],[0]
For other examples the relation between the prepositions in “out of” is labelled as prep (preposition) instead of pcomp (prepositional complement).,4.3 Linguistic Analysis,[0],[0]
"We suspect this is due to the converter’s inability to handle certain corner cases, but further study is warranted.",4.3 Linguistic Analysis,[0],[0]
Previous work has also explored augmenting transition systems to facilitate longer-range attachments.,5 Related Work,[0],[0]
"Attardi (2006) extended the arcstandard system for non-projective parsing, with arc-inducing transitions that are very similar to those in arc-swift.",5 Related Work,[0],[0]
A notable difference is that their transitions retain tokens between the head and dependent.,5 Related Work,[0],[0]
"Fernández-González and GómezRodrı́guez (2012) augmented the arc-eager system with transitions that operate on the buffer, which shorten the transition sequence by reducing the number of Shift transitions needed.",5 Related Work,[0],[0]
"However, limited by the sparse feature-based classifiers used, both of these parsers just mentioned only allow direct attachments of distance up to 3 and 2, respectively.",5 Related Work,[0],[0]
"More recently, Sartorio et al. (2013) extended arc-standard with transitions that directly attach to left and right “spines” of the top two nodes in the stack.",5 Related Work,[0],[0]
"While this work shares very similar motivations as arc-swift, it requires additional data structures to keep track of the left and right spines of nodes.",5 Related Work,[0],[0]
"This transition system also introduces spurious ambiguity where multiple transition sequences could lead to the same correct parse, which necessitates easy-first training to achieve a more noticeable improvement over arcstandard.",5 Related Work,[0],[0]
"In contrast, arc-swift can be easily implemented given the parser state alone, and does not give rise to spurious ambiguity.
",5 Related Work,[0],[0]
"For a comprehensive study of transition systems for dependency parsing, we refer the reader to (Bohnet et al., 2016), which proposed a generalized framework that could derive all of the traditional transition systems we described by configuring the size of the active token set and the maximum arc length, among other control parameters.",5 Related Work,[0],[0]
"However, this framework does not cover
arc-swift in its original form, as the authors limit each of their transitions to reduce at most one token from the active token set (the buffer).",5 Related Work,[0],[0]
"On the other hand, the framework presented in (GómezRodrı́guez and Nivre, 2013) does not explicitly make this constraint, and therefore generalizes to arc-swift.",5 Related Work,[0],[0]
"However, we note that arc-swift still falls out of the scope of existing discussions in that work, by introducing multiple Reduces in a single transition.",5 Related Work,[0],[0]
"In this paper, we introduced arc-swift, a novel transition system for dependency parsing.",6 Conclusion,[0],[0]
We also performed linguistic analyses on parser outputs and showed arc-swift parsers reduce errors in conjunction and adverbial attachments compared to parsers using traditional transition systems.,6 Conclusion,[0],[0]
"We thank Timothy Dozat, Arun Chaganty, Danqi Chen, and the anonymous reviewers for helpful discussions.",Acknowledgments,[0],[0]
Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract,Acknowledgments,[0],[0]
No. FA8750-13-2-0040.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.",Acknowledgments,[0],[0]
"Our model setup is similar to that of (Kiperwasser and Goldberg, 2016)",A Model and Training Details,[0],[0]
(See Figure 5).,A Model and Training Details,[0],[0]
"We employ two blocks of bidirectional long short-term memory (BiLSTM) networks (Hochreiter and Schmidhuber, 1997) that share very similar structures, one for part-of-speech (POS) tagging, the other for parsing.",A Model and Training Details,[0],[0]
"Both BiLSTMs have 400 hidden units in each direction, and the output of both are concatenated and fed into a dense layer of rectified linear units (ReLU) before 32-dimensional representations are derived as classification features.",A Model and Training Details,[0],[0]
"As the input to the tagger BiLSTM, we represent words with 100-dimensional word embeddings, initialized with GloVe vectors (Pennington et al., 2014).8",A Model and Training Details,[0],[0]
"The output distribution of the tagger classifier is used to compute a weighted sum of 32- dimensional POS embeddings, which is then concatenated with the output of the tagger BiLSTM",A Model and Training Details,[0],[0]
(800-dimensional per token) as the input to the parser BiLSTM.,A Model and Training Details,[0],[0]
"For the parser BiLSTM, we use two separate sets of dense layers to derive a “head” and a “dependent” representation for each token.",A Model and Training Details,[0],[0]
"These representations are later merged according to the parser state to make transition predictions.
",A Model and Training Details,[0],[0]
"For traditional transition systems, we follow (Kiperwasser and Goldberg, 2016) by featurizing the top 3 tokens on the stack and the leftmost token in the buffer.",A Model and Training Details,[0],[0]
"To derive features for each token, we take its head representation vhead and dependent representation vdep, and perform the following biaffine combination
vfeat,i =",A Model and Training Details,[0],[0]
"[f(vhead, vdep)]i = ReLU ( v>headWivdep +",A Model and Training Details,[0],[0]
b,A Model and Training Details,[0],[0]
>,A Model and Training Details,[0],[0]
"i vhead
+ c",A Model and Training Details,[0],[0]
">i vdep + di ) (1)
where Wi ∈ R32×32, bi, ci ∈ R32, and di is a scalar for i = 1, . . .",A Model and Training Details,[0],[0]
", 32.",A Model and Training Details,[0],[0]
"The resulting 32- dimensional features are concatenated as the input
8We also kept the vectors of the top 400k words trained on Wikipedia and English Gigaword for a broader coverage of unseen words.
to a fixed-dimensional softmax classifier for transition decisions.
",A Model and Training Details,[0],[0]
"For arc-swift, we featurize for each arcinducing transition with the same composition function in Equation (1) with vhead of the head token and vdep of the dependent token of the arc to be induced.",A Model and Training Details,[0],[0]
"For Shift, we simply combine vhead and vdep of the leftmost token in the buffer with the biaffine combination, and obtain its score by computing the inner-product of the feature and a vector.",A Model and Training Details,[0],[0]
"At each step, the scores of all feasible transitions are normalized to a probability distribution by a softmax function.
",A Model and Training Details,[0],[0]
"In all of our experiments, the parsers are trained to maximize the log likelihood of the desired transition sequence, along with the tagger being trained to maximize the log likelihood of the correct POS tag for each token.
",A Model and Training Details,[0],[0]
"To train the parsers, we use the ADAM optimizer (Kingma and Ba, 2014), with β2 = 0.9, an initial learning rate of 0.001, and minibatches of size 32 sentences.",A Model and Training Details,[0],[0]
Parsers are trained for 10 passes through the dataset on PTB-SD.,A Model and Training Details,[0],[0]
We also find that annealing the learning rate by a factor of 0.5 for every pass after the 5th helped improve performance.,A Model and Training Details,[0],[0]
"For EN-UD, we train for 30 passes, and anneal the learning rate for every 3 passes after the 15th due to the smaller size of the dataset.",A Model and Training Details,[0],[0]
"For all of the biaffine combination layers and dense layers, we dropout their units with a small probability of 5%.",A Model and Training Details,[0],[0]
"Also during training time, we randomly replace 10% of the input words by an artificial 〈UNK〉 token, which is then used to replace
all unseen words in the development and test sets.",A Model and Training Details,[0],[0]
"Finally, we repeat each experiment with 3 independent random initializations, and use the average result for reporting and statistical significance tests.
",A Model and Training Details,[0],[0]
The code for the full specification of our models and aforementioned training details are available at https://github.com/qipeng/ arc-swift.,A Model and Training Details,[0],[0]
Transition-based dependency parsers often need sequences of local shift and reduce operations to produce certain attachments.,abstractText,[0],[0]
Correct individual decisions hence require global information about the sentence context and mistakes cause error propagation.,abstractText,[0],[0]
"This paper proposes a novel transition system, arc-swift, that enables direct attachments between tokens farther apart with a single transition.",abstractText,[0],[0]
This allows the parser to leverage lexical information more directly in transition decisions.,abstractText,[0],[0]
"Hence, arc-swift can achieve significantly better performance with a very small beam size.",abstractText,[0],[0]
Our parsers reduce error by 3.7–7.6% relative to those using existing transition systems on the Penn Treebank dependency parsing task and English Universal Dependencies.,abstractText,[0],[0]
Arc-swift: A Novel Transition System for Dependency Parsing,title,[0],[0]
"1 Are BLEU and Meaning Representation in Opposition?
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",text,[0],[0]
Deep learning has brought the possibility of automatically learning continuous representations of sentences.,1 Introduction,[0],[0]
"On the one hand, such representations can be geared towards particular tasks such as classifying the sentence in various aspects (e.g. sentiment, register, question type) or relating the sentence to other sentences (e.g. semantic similarity, paraphrasing, entailment).",1 Introduction,[0],[0]
"On the other hand, we can aim at “universal” sentence representations, that is representations performing reasonably well in a range of such tasks.
",1 Introduction,[0],[0]
"Regardless the evaluation criterion, the representations can be learned either in an unsupervised way (from simple, unannotated texts) or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type.",1 Introduction,[0],[0]
"A different approach is to obtain sentence representations from training neural machine translation models (Hill et al., 2016).
",1 Introduction,[0],[0]
"Since Hill et al. (2016), NMT has seen substantial advances in translation quality and it is thus
natural to ask how these improvements affect the learned representations.
",1 Introduction,[0],[0]
"One of the key technological changes was the introduction of “attention” (Bahdanau et al., 2014), making it even the very central component in the network (Vaswani et al., 2017).",1 Introduction,[0],[0]
Attention allows the NMT system to dynamically choose which parts of the source are most important when deciding on the current output token.,1 Introduction,[0],[0]
"As a consequence, there is no longer a static vector representation of the sentence available in the system.
",1 Introduction,[0],[0]
"In this paper, we remove this limitation by proposing a novel encoder-decoder architecture with a structured fixed-size representation of the input that still allows the decoder to explicitly focus on different parts of the input.",1 Introduction,[0],[0]
"In other words, our NMT system has both the capacity to attend to various parts of the input and to produce static representations of input sentences.
",1 Introduction,[0],[0]
"We train this architecture on English-to-German and English-to-Czech translation and evaluate the learned representations of English on a wide range of tasks in order to assess its performance in learning “universal” meaning representations.
",1 Introduction,[0],[0]
"In Section 2, we briefly review recent efforts in obtaining sentence representations.",1 Introduction,[0],[0]
"In Section 3, we introduce a number of variants of our novel architecture.",1 Introduction,[0],[0]
Section 4 describes some standard and our own methods for evaluating sentence representations.,1 Introduction,[0],[0]
Section 5 then provides experimental results: translation and representation quality.,1 Introduction,[0],[0]
The relation between the two is discussed in Section 6.,1 Introduction,[0],[0]
The properties of continuous sentence representations have always been of interest to researchers working on neural machine translation.,2 Related Work,[0],[0]
"In the first works on RNN sequence-to-sequence models, Cho et al. (2014) and Sutskever et al. (2014)
ar X
iv :1
80 5.
06 53
6v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
6 M
ay 2
01 8
2
provided visualizations of the phrase and sentence embedding spaces and observed that they reflect semantic and syntactic structure to some extent.",2 Related Work,[0],[0]
"Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classification tasks and by relating semantic similarity to closeness in the representation space.",2 Related Work,[0],[0]
"Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence- and word-level syntactic labels (e.g. tense, part of speech) and by generating syntax trees from these representations.",2 Related Work,[0],[0]
Schwenk and Douze (2017) aim to learn language-independent sentence representations using NMT systems with multiple source and target languages.,2 Related Work,[0],[0]
They do not consider the attention mechanism and evaluate primarily by similarity scores of the learned representations for similar sentences (within or across languages).,2 Related Work,[0],[0]
"Our proposed model architectures differ in (a) which encoder states are considered in subsequent processing, (b) how they are combined, and (c) how they are used in the decoder.",3 Model Architectures,[0],[0]
Table 1 summarizes all the examined configurations of RNN-based models.,3 Model Architectures,[0],[0]
"The first three (ATTN, FINAL, FINAL-CTX) correspond roughly to the standard sequence-to-sequence models, Bahdanau et al. (2014), Sutskever et al. (2014) and Cho et al. (2014), respectively.",3 Model Architectures,[0],[0]
"The last column (ATTN-ATTN) is our main proposed architecture: compound attention, described here in Section 3.1.",3 Model Architectures,[0],[0]
"In addition to RNN-based models, we experiment with the Transformer model, see Section 3.3.",3 Model Architectures,[0],[0]
Our compound attention model incorporates attention in both the encoder and the decoder.,3.1 Compound Attention,[0],[0]
Its architecture is shown in Fig. 1.,3.1 Compound Attention,[0],[0]
Encoder with inner attention.,3.1 Compound Attention,[0],[0]
"First, we process the input sequence x1, x2, . . .",3.1 Compound Attention,[0],[0]
", xT using a bidirectional recurrent network with gated recurrent units (GRU, Cho et al., 2014):",3.1 Compound Attention,[0],[0]
"−→ ht = −−→ GRU(xt, −−→ ht−1), ←− ht = ←−− GRU(xt, ←−− ht+1), ht =",3.1 Compound Attention,[0],[0]
"[ −→ ht , ←− ht ].
",3.1 Compound Attention,[0],[0]
3,3.1 Compound Attention,[0],[0]
"We denote by u the combined number of units in the two RNNs, i.e. the dimensionality of ht.",3.1 Compound Attention,[0],[0]
"Next, our goal is to combine the states (h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT )",3.1 Compound Attention,[0],[0]
= H of the encoder into a vector of fixed dimensionality that represents the entire sentence.,3.1 Compound Attention,[0],[0]
Traditional seq2seq models concatenate the final states of both encoder RNNs ( −→ hT and ←− h1) to obtain the sentence representation (denoted as FINAL in Table 1).,3.1 Compound Attention,[0],[0]
"Another option is to combine all encoder states using the average or maximum over time (Collobert and Weston, 2008; Schwenk and Douze, 2017) (AVGPOOL and MAXPOOL in Table 1 and following).",3.1 Compound Attention,[0],[0]
"We adopt an alternative approach, which is to use inner attention1 (Liu et al., 2016; Li et al., 2016) to compute several weighted averages of the encoder states (Lin et al., 2017).",3.1 Compound Attention,[0],[0]
"The main motivation for incorporating these multiple “views” of the state sequence is that it removes the need for the RNN cell to accumulate the representation of the whole sentence as it processes the input, and therefore it should have more capacity for modeling local dependencies.",3.1 Compound Attention,[0],[0]
"Specifically, we fix a number r, the number of attention heads, and compute an r×T matrixA of attention weights αjt, representing the importance of position t in the input for the jth attention head.",3.1 Compound Attention,[0],[0]
"We then use this matrix to compute r weighted sums of the encoder states, which become the rows of a new matrix M : M = AH.",3.1 Compound Attention,[0],[0]
(1) A vector representation of the source sentence (the “sentence embedding”) can be obtained by flattening the matrix M .,3.1 Compound Attention,[0],[0]
"In our experiments, we project the encoder states h1, h2, . . .",3.1 Compound Attention,[0],[0]
", hT down to a given dimensionality before applying Eq.",3.1 Compound Attention,[0],[0]
"(1), so that we can control the size of the representation.",3.1 Compound Attention,[0],[0]
"Following Lin et al. (2017), we compute the attention matrix by feeding the encoder states to a two-layer feed-forward network: A = softmax(U tanh(WH>)), (2) where W and U are weight matrices of dimensions d× u and r × d, respectively (d is the number of hidden units); the softmax function is applied along the second dimension, i.e. across the encoder states.",3.1 Compound Attention,[0],[0]
1Some papers call the same or similar approach selfattention or single-time attention.,3.1 Compound Attention,[0],[0]
Attentive decoder.,3.1 Compound Attention,[0],[0]
"In vanilla seq2seq models with a fixed-size sentence representation, the decoder is usually conditioned on this representation via the initial RNN state.",3.1 Compound Attention,[0],[0]
We propose to instead leverage the structured sentence embedding by applying attention to its components.,3.1 Compound Attention,[0],[0]
"This is no different from the classical attention mechanism used in NMT (Bahdanau et al., 2014), except that it acts on this fixed-size representation instead of the sequence of encoder states.",3.1 Compound Attention,[0],[0]
"In the ith decoding step, the attention mechanism computes a distribution {βij}rj=1 over the r components of the structured representation.",3.1 Compound Attention,[0],[0]
"This is then used to weight these components to obtain the context vector ci, which in turn is used to update the decoder state.",3.1 Compound Attention,[0],[0]
"Again, we can write this in matrix form as C = BM, (3) where B = (βij) T ′,r i=1,j=1 is the attention matrix and C = (ci, c2, . . .",3.1 Compound Attention,[0],[0]
", cT ′) are the context vectors.",3.1 Compound Attention,[0],[0]
Note that by combining Eqs.,3.1 Compound Attention,[0],[0]
"(1) and (3), we get C = (BA)H. (4) Hence, the composition of the encoder and decoder attentions (the “compound attention”) defines an implicit alignment between the source and the target sequence.",3.1 Compound Attention,[0],[0]
"From this viewpoint, our model can be regarded as a restriction of the conventional attention model.",3.1 Compound Attention,[0],[0]
"The decoder uses a conditional GRU cell (cGRUatt; Sennrich et al., 2017), which consists of two consecutively applied GRU blocks.",3.1 Compound Attention,[0],[0]
"The first block processes the previous target token yi−1, while the second block receives the context vector ci and predicts the next target token yi.",3.1 Compound Attention,[0],[0]
"Compared to the FINAL model, the compound attention architecture described in the previous section undoubtedly benefits from the fact that the decoder is presented with information from the encoder (i.e. the context vectors ci) in every decoding step.",3.2 Constant Context,[0],[0]
"To investigate this effect, we include baseline models where we replace all context vectors ci with the entire sentence embedding (indicated by the suffix “-CTX” in Table 1).",3.2 Constant Context,[0],[0]
"Specifically, we provide either the flattened matrixM (for models with inner attention; ATTN or ATTN-CTX), the final state of the encoder (FINAL-CTX), or the
4
result of mean- or max-pooling (*POOL-CTX) as a constant input to the decoder cell.",3.2 Constant Context,[0],[0]
"The Transformer (Vaswani et al., 2017) is a recently proposed model based entirely on feedforward layers and attention.",3.3 Transformer with Inner Attention,[0],[0]
"It consists of an encoder and a decoder, each with 6 layers, consisting of multi-head attention on the previous layer and a position-wise feed-forward network.",3.3 Transformer with Inner Attention,[0],[0]
"In order to introduce a fixed-size sentence representation into the model, we modify it by adding inner attention after the last encoder layer.",3.3 Transformer with Inner Attention,[0],[0]
The attention in the decoder then operates on the components of this representation (i.e. the rows of the matrix M ).,3.3 Transformer with Inner Attention,[0],[0]
This variation on the Transformer model corresponds to the ATTN-ATTN column in Table 1 and is therefore denoted TRF-ATTN-ATTN.,3.3 Transformer with Inner Attention,[0],[0]
"Continuous sentence representations can be evaluated in many ways, see e.g. Kiros et al. (2015), Conneau et al. (2017) or the RepEval workshops.2 We evaluate our learned representations with classification and similarity tasks from SentEval (Section 4.1) and by examining clusters of sentence paraphrase representations (Section 4.2).",4 Representation Evaluation,[0],[0]
"We perform evaluation on 10 classification and 7 similarity tasks using the SentEval3 (Conneau et al., 2017) evaluation tool.",4.1 SentEval,[0],[0]
This is a superset of the tasks from Kiros et al. (2015).,4.1 SentEval,[0],[0]
"2https://repeval2017.github.io/ 3https://github.com/facebookresearch/ SentEval/
Table 2 describes the classification tasks (number of classes, data size, task type and an example) and Table 3 lists the similarity tasks.",4.1 SentEval,[0],[0]
The similarity (relatedness) datasets contain pairs of sentences labeled with a real-valued similarity score.,4.1 SentEval,[0],[0]
"A given sentence representation model is evaluated either by learning to directly predict this score given the respective sentence embeddings (“regression”), or by computing the cosine similarity of the embeddings (“similarity”) without the need of any training.",4.1 SentEval,[0],[0]
"In both cases, Pearson and Spearman correlation of the predictions with the gold ratings is reported.",4.1 SentEval,[0],[0]
See Dolan et al. (2004) for details on MRPC and Hill et al. (2016) for the remaining tasks.,4.1 SentEval,[0],[0]
We also evaluate the representation of paraphrases.,4.2 Paraphrases,[0],[0]
We use two paraphrase sources for this purpose: COCO and HyTER Networks.,4.2 Paraphrases,[0],[0]
"COCO (Common Objects in Context; Lin et al., 2014) is an object recognition and image captioning dataset, containing 5 captions for each image.",4.2 Paraphrases,[0],[0]
We extracted the captions from its validation set to form a set of 5 × 5k = 25k sentences grouped by the source image.,4.2 Paraphrases,[0],[0]
The average sentence length is 11 tokens and the vocabulary size is 9k types.,4.2 Paraphrases,[0],[0]
"HyTER Networks (Dreyer and Marcu, 2014)
5 are large finite-state networks representing a subset of all possible English translations of 102 Arabic and 102 Chinese sentences.",4.2 Paraphrases,[0],[0]
"The networks were built by manually based on reference sentences in Arabic, Chinese and English.",4.2 Paraphrases,[0],[0]
Each network contains up to hundreds of thousands of possible translations of the given source sentence.,4.2 Paraphrases,[0],[0]
"We randomly generated 500 translations for each source sentence, obtaining a corpus of 102k sentences grouped into 204 clusters, each containing 500 paraphrases.",4.2 Paraphrases,[0],[0]
The average length of the 102k English sentences is 28 tokens and the vocabulary size is 11k token types.,4.2 Paraphrases,[0],[0]
"For every model, we encode each dataset to obtain a set of sentence embeddings with cluster labels.",4.2 Paraphrases,[0],[0]
We then compute the following metrics: Cluster classification accuracy (denoted “Cl”).,4.2 Paraphrases,[0],[0]
"We remove 1 point (COCO) or half of the points (HyTER) from each cluster, and fit an LDA classifier on the rest.",4.2 Paraphrases,[0],[0]
We then compute the accuracy of the classifier on the removed points.,4.2 Paraphrases,[0],[0]
Nearest-neighbor paraphrase retrieval accuracy (NN).,4.2 Paraphrases,[0],[0]
"For each point, we find its nearest neighbor according to cosine or L2 distance, and count how often the neighbor lies in the same cluster as the original point.",4.2 Paraphrases,[0],[0]
Inverse Davies-Bouldin index (iDB).,4.2 Paraphrases,[0],[0]
"The Davies-Bouldin index (Davies and Bouldin, 1979) measures cluster separation.",4.2 Paraphrases,[0],[0]
"For every pair of clusters, we compute the ratio Rij of their combined scatter (average L2 distance to the centroid) Si + Sj and the L2 distance of their centroids dij , then average the maximum values for all clusters: Rij = Si + Sj dij (5) DB = 1 N N∑ i=1",4.2 Paraphrases,[0],[0]
"max j 6=i Rij (6) The lower the DB index, the better the separation.",4.2 Paraphrases,[0],[0]
"To match with the rest of our metrics, we take its inverse: iDB = 1DB .",4.2 Paraphrases,[0],[0]
"We trained English-to-German and English-toCzech NMT models using Neural Monkey4 (Helcl and Libovický, 2017a).",5 Experimental Results,[0],[0]
"In the following, we distinguish these models using the code of the target language, i.e. de or cs. 4https://github.com/ufal/neuralmonkey",5 Experimental Results,[0],[0]
"The de models were trained on the Multi30K multilingual image caption dataset (Elliott et al., 2016), extended by Helcl and Libovický (2017b), who acquired additional parallel data using backtranslation (Sennrich et al., 2016) and perplexitybased selection (Yasuda et al., 2008).",5 Experimental Results,[0],[0]
"This extended dataset contains 410k sentence pairs, with the average sentence length of 12 ± 4 tokens in English.",5 Experimental Results,[0],[0]
We train each model for 20 epochs with the batch size of 32.,5 Experimental Results,[0],[0]
We truecased the training data as well as all data we evaluate on.,5 Experimental Results,[0],[0]
"For German, we employed Neural Monkey’s reversible pre-processing scheme, which expands contractions and performs morphological segmentation of determiners.",5 Experimental Results,[0],[0]
We used a vocabulary of at most 30k tokens for each language (no subword units).,5 Experimental Results,[0],[0]
"The cs models were trained on CzEng 1.7 (Bojar et al.,",5 Experimental Results,[0],[0]
"2016).5 We used byte-pair encoding (BPE) with a vocabulary of 30k sub-word units, shared for both languages.",5 Experimental Results,[0],[0]
"For English, the average sentence length is 15±19 BPE tokens and the original vocabulary size is 1.9M. We performed 1 training epoch with the batch size of 128 on the entire training section (57M sentence pairs).",5 Experimental Results,[0],[0]
"The datasets for both de and cs models come with their respective development and test sets of sentence pairs, which we use for the evaluation of translation quality.",5 Experimental Results,[0],[0]
(We use 1k randomly selected sentence pairs from CzEng 1.7 dtest as a development set.,5 Experimental Results,[0],[0]
"For evaluation, we use the entire etest.)",5 Experimental Results,[0],[0]
"We also evaluate the InferSent model6 (Conneau et al., 2017) as pre-trained on the natural language inference (NLI) task.",5 Experimental Results,[0],[0]
InferSent has been shown to achieve state-of-the-art results on the SentEval tasks.,5 Experimental Results,[0],[0]
"We also include a bag-ofwords baseline (GloVe-BOW) obtained by averaging GloVe7 word vectors (Pennington et al., 2014).",5 Experimental Results,[0],[0]
"We estimate translation quality of the various models using single-reference case-sensitive BLEU (Papineni et al., 2002) as implemented in Neural Monkey (the reference implementation is mteval-v13a.pl from NIST or Moses).",5.1 Translation Quality,[0],[0]
Tables 4 and 5 provide the results on the two datasets.,5.1 Translation Quality,[0],[0]
The cs dataset is much larger and the training takes much longer.,5.1 Translation Quality,[0],[0]
"We were thus able 5http://ufal.mff.cuni.cz/czeng/czeng17 6https://github.com/facebookresearch/ InferSent 7https://nlp.stanford.edu/projects/ glove/
6
to experiment with only a subset of the possible model configurations.",5.1 Translation Quality,[0],[0]
The columns “Size” and “Heads” specify the total size of sentence representation and the number of heads of encoder inner attention.,5.1 Translation Quality,[0],[0]
"In both cases, the best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention).",5.1 Translation Quality,[0],[0]
"The non-attentive FINAL Cho et al. is the worst, except cs-MAXPOOL.",5.1 Translation Quality,[0],[0]
"For 5 selected cs models, we also performed the WMT-style 5-way manual ranking on 200 sentence pairs.",5.1 Translation Quality,[0],[0]
The annotations are interpreted as simulated pairwise comparisons.,5.1 Translation Quality,[0],[0]
"For each model, the final score is the number of times the model was judged better than the other model in the pair.",5.1 Translation Quality,[0],[0]
Tied pairs are excluded.,5.1 Translation Quality,[0],[0]
"The results, shown in Table 5, confirm the automatic evaluation results.",5.1 Translation Quality,[0],[0]
"We also checked the relation between BLEU
and the number of heads and representation size.",5.1 Translation Quality,[0],[0]
"While there are many exceptions, the general tendency is that the larger the representation or the more heads, the higher the BLEU score.",5.1 Translation Quality,[0],[0]
The Pearson correlation between BLEU and the number of heads is 0.87 for cs and 0.31 for de.,5.1 Translation Quality,[0],[0]
"Due to the large number of SentEval tasks, we present the results abridged in two different ways: by reporting averages (Table 6) and by showing only the best models in comparison with other methods (Table 7).",5.2 SentEval,[0],[0]
The full results can be found in the supplementary material.,5.2 SentEval,[0],[0]
"Table 6 provides averages of the classification and similarity results, along with the results of selected tasks (SNLI, SICK-E).",5.2 SentEval,[0],[0]
"As the baseline for classifications tasks, we assign the most frequent class to all test examples.8",5.2 SentEval,[0],[0]
"The de models are generally worse, most likely due to the higher OOV rate and overall simplicity of the training sentences.",5.2 SentEval,[0],[0]
"On cs, we see a clear pattern that more heads hurt the performance.",5.2 SentEval,[0],[0]
The de set has more variations to consider but the results are less conclusive.,5.2 SentEval,[0],[0]
"For the similarity results, it is worth noting that cs-ATTN-ATTN performs very well with 1 attention head but fails miserably with more heads.",5.2 SentEval,[0],[0]
"Otherwise, the relation to the number of heads is less clear.",5.2 SentEval,[0],[0]
Table 7 compares our strongest models with other approaches on all tasks.,5.2 SentEval,[0],[0]
"Besides InferSent and GloVe-BOW, we include SkipThought as evaluated by Conneau et al. (2017), and the NMTbased embeddings by Hill et al. (2016) trained on the English-French WMT15 dataset (this is the best result reported by Hill et al. for NMT).",5.2 SentEval,[0],[0]
We see that the supervised InferSent clearly outperforms all other models in all tasks except for MRPC and TREC.,5.2 SentEval,[0],[0]
"Results by Hill et al. are always lower than our best setups, except MRPC.",5.2 SentEval,[0],[0]
"On classification tasks, our models are outperformed even by GloVe-BOW, except for the NLI tasks (SICK-E and SNLI) where cs-FINAL-CTX is better.",5.2 SentEval,[0],[0]
Table 6 also provides our measurements based on sentence paraphrases.,5.3 Paraphrase Scores,[0],[0]
"For paraphrase retrieval 8For MR, CR, SUBJ, and MPQA, where there is no distinct test set, the class is established on the whole collection.",5.3 Paraphrase Scores,[0],[0]
"For other tasks, the class is learned from the training set.
7
8
B L
E U
M R C R S U
B J
M P
Q A
S S
T 2
S S
T 5
T R
E C
M R
P C
S IC
K -E
S N
L I",5.3 Paraphrase Scores,[0],[0]
A v g,5.3 Paraphrase Scores,[0],[0]
A cc S IC K -R S T S B S T S 1 2 S T S 1 3 S T S 1 4 S T S 1 5 S T S 1 6 A v g S,5.3 Paraphrase Scores,[0],[0]
im H y -C l H y -N N H y -i D B C O -C,5.3 Paraphrase Scores,[0],[0]
l C O -N N C,5.3 Paraphrase Scores,[0],[0]
"O -i D B
BLEU MR CR
SUBJ MPQA
SST2 SST5 TREC MRPC
SICK-E SNLI AvgAcc SICK-R
STSB STS12 STS13",5.3 Paraphrase Scores,[0],[0]
"STS14 STS15 STS16 AvgSim Hy-Cl Hy-NN Hy-iDB CO-Cl CO-NN CO-iDB
−1.00 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00
Figure 2: Pearson correlations.",5.3 Paraphrase Scores,[0],[0]
"Upper triangle: de models, lower triangle: cs models.",5.3 Paraphrase Scores,[0],[0]
Positive values shown in shades of green.,5.3 Paraphrase Scores,[0],[0]
"For similarity tasks, only the Pearson (not Spearman) coefficient is represented.
",5.3 Paraphrase Scores,[0],[0]
"(NN), we found cosine distance to work better than L2 distance.",5.3 Paraphrase Scores,[0],[0]
"We therefore do not list or further consider L2-based results (except in the supplementary material).
",5.3 Paraphrase Scores,[0],[0]
"This evaluation seems less stable and discerning than the previous two, but we can again confirm the victory of InferSent followed by our nonattentive cs models.",5.3 Paraphrase Scores,[0],[0]
cs and de models are no longer clearly separated.,5.3 Paraphrase Scores,[0],[0]
"To assess the relation between the various measures of sentence representations and translation quality as estimated by BLEU, we plot a heatmap of Pearson correlations in Fig. 2.",6 Discussion,[0],[0]
"As one example, Fig. 3 details the cs models’ BLEU scores and AvgAcc (average of SentEval accuracies).
",6 Discussion,[0],[0]
"A good sign is that on the cs dataset, most metrics of representation are positively correlated (the pairwise Pearson correlation is 0.78± 0.32 on average), the outlier being TREC (−0.16±0.16 correlation with the other metrics on average)
",6 Discussion,[0],[0]
"On the other hand, most representation metrics correlate with BLEU negatively (−0.57±0.31) on cs.",6 Discussion,[0],[0]
"The pattern is less pronounced but still clear also on the de dataset.
",6 Discussion,[0],[0]
"A detailed understanding of what the learned
representations contain is difficult.",6 Discussion,[0],[0]
"We can only speculate that if the NMT model has some capability for following the source sentence superficially, it will use it and spend its capacity on closely matching the target sentences rather than on deriving some representation of meaning which would reflect e.g. semantic similarity.",6 Discussion,[0],[0]
We assume that this can be a direct consequence of NMT being trained for cross entropy: putting the exact word forms in exact positions as the target sentence requires.,6 Discussion,[0],[0]
"Performing well in single-reference BLEU is not an indication that the system understands the meaning but rather that it can maximize the chance of producing the n-grams required by the reference.
",6 Discussion,[0],[0]
"The negative correlation between the number of attention heads and the representation metrics from Fig. 3 (−0.81±0.12 for cs and−0.18±0.19 for de, on average) can be partly explained by the following observation.",6 Discussion,[0],[0]
We plotted the induced alignments (e.g. Fig. 4) and noticed that the heads tend to “divide” the sentence into segments.,6 Discussion,[0],[0]
"While one would hope that the segments correspond to some meaningful units of the sentence (e.g. subject, predicate, object), we failed to find any such interpretation for ATTN-ATTN and for cs models in general.",6 Discussion,[0],[0]
"Instead, the heads divide the source sentence more or less equidistantly, as documented by Fig. 5.",6 Discussion,[0],[0]
"Such a multi-headed sentence representation is then less fit for representing e.g. paraphrases where the subject and object swap their position due to passivization, because their representations are then accessed by different heads, and thus end up in different parts of the sentence embedding vector.
9
For de-ATTN-CTX models, we observed a much flatter distribution of attention weights for each head and, unlike in the other models, we were often able to identify a head focusing on the main verb.",6 Discussion,[0],[0]
"This difference between ATTN-ATTN and some ATTN-CTX models could be explained by the fact that in the former, the decoder is oblivious to the ordering of the heads (because of decoder attention), and hence it may not be useful for a given head to look for a specific syntactic or semantic role.",6 Discussion,[0],[0]
"We presented a novel variation of attentive NMT models (Bahdanau et al., 2014; Vaswani et al., 2017) that again provides a single meeting point with a continuous representation of the source sen-
tence.",7 Conclusion,[0],[0]
We evaluated these representations with a number of measures reflecting how well the meaning of the source sentence is captured.,7 Conclusion,[0],[0]
"While our proposed “compound attention” leads to translation quality not much worse than the fully attentive model, it generally does not perform well in the meaning representation.",7 Conclusion,[0],[0]
"Quite on the contrary, the better the BLEU score, the worse the meaning representation.",7 Conclusion,[0],[0]
"We believe that this observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score (or even worse, cross entropy) leads to systems that refuse to consider the meaning of the sentence.",7 Conclusion,[0],[0]
"This work has been supported by the grants 18-24210S of the Czech Science Foundation, SVV 260 453 and “Progress” Q18+Q48 of Charles University, and using language resources distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (projects LM2015071 and OP VVV VI CZ.02.1.01/0.0/0.0/16 013/0001781).
",Acknowledgement,[0],[0]
10,Acknowledgement,[0],[0]
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems.,abstractText,[0],[0]
The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted.,abstractText,[0],[0]
We propose several variations of the attentive NMT architecture bringing this meeting point back.,abstractText,[0],[0]
"Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.",abstractText,[0],[0]
Are BLEU and Meaning Representation in Opposition?,title,[0],[0]
"Argument mining consists of the automatic identification of argumentative structures in documents, a valuable task with applications in policy making, summarization, and education, among others.",1 Introduction,[0],[0]
The argument mining task includes the tightly-knit subproblems of classifying propositions into elementary unit types and detecting argumentative relations between the elementary units.,1 Introduction,[0],[0]
"The desired output is a document argumentation graph structure, such as the one in Figure 1, where propositions are denoted by letter subscripts, and the associated argumentation graph shows their types and support relations between them.
",1 Introduction,[0],[0]
"Most annotation and prediction efforts in argument mining have focused on tree or forest structures (Peldszus and Stede, 2015; Stab and Gurevych, 2016), constraining argument structures to form one or more trees.",1 Introduction,[0],[0]
"This makes the problem computationally easier by enabling the use of maximum spanning tree–style parsing ap-
proaches.",1 Introduction,[0],[0]
"However, argumentation in the wild can be less well-formed.",1 Introduction,[0],[0]
"The argument put forth in Figure 1, for instance, consists of two components: a simple tree structure and a more complex graph structure (c jointly supports b and d).",1 Introduction,[0],[0]
"In this work, we design a flexible and highly expressive structured prediction model for argument mining, jointly learning to classify elementary units (henceforth propositions) and to identify the argumentative relations between them (henceforth links).",1 Introduction,[0],[0]
"By formulating argument mining as inference in a factor graph (Kschischang et al., 2001), our model (described in Section 4) can account for correlations between the two tasks, can consider second order link structures (e.g., in Figure 1, c → b → a), and can impose arbitrary constraints (e.g., transitivity).
",1 Introduction,[0],[0]
"To parametrize our models, we evaluate two alternative directions: linear structured SVMs
1We describe proposition types (FACT, etc.)",1 Introduction,[0],[0]
"in Section 3.
ar X
iv :1
70 4.
06 86
9v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
3 A
pr 2
01 7
(Tsochantaridis et al., 2005), and recurrent neural networks with structured loss, extending (Kiperwasser and Goldberg, 2016).",1 Introduction,[0],[0]
"Interestingly, RNNs perform poorly when trained with classification losses, but become competitive with the featureengineered structured SVMs when trained within our proposed structured learning model.
",1 Introduction,[0],[0]
We evaluate our approach on two argument mining datasets.,1 Introduction,[0],[0]
"Firstly, on our new Cornell eRulemaking Corpus – CDCP,2 consisting of argument annotations on comments from an eRulemaking discussion forum, where links don’t always form trees (Figure 1 shows an abridged example comment, and Section 3 describes the dataset in more detail).",1 Introduction,[0],[0]
"Secondly, on the UKP argumentative essays v2 (henceforth UKP), where argument graphs are annotated strictly as multiple trees (Stab and Gurevych, 2016).",1 Introduction,[0],[0]
"In both cases, the results presented in Section 5 confirm that our models outperform unstructured baselines.",1 Introduction,[0],[0]
"On UKP, we improve link prediction over the best reported result in (Stab and Gurevych, 2016), which is based on integer linear programming postprocessing.",1 Introduction,[0],[0]
"For insight into the strengths and weaknesses of the proposed models, as well as into the differences between SVM and RNN parameterizations, we perform an error analysis in Section 5.1.",1 Introduction,[0],[0]
"To support argument mining research, we also release our Python implementation, Marseille.3",1 Introduction,[0],[0]
Our factor graph formulation draws from ideas previously used independently in parsing and argument mining.,2 Related work,[0],[0]
"In particular, maximum spanning tree (MST) methods for arc-factored dependency parsing have been successfully used by McDonald et al. (2005) and applied to argument mining with mixed results by Peldszus and Stede (2015).",2 Related work,[0],[0]
"As they are not designed for the task, MST parsers cannot directly handle proposition classification or model the correlation between proposition and link prediction—a limitation our model addresses.",2 Related work,[0],[0]
"Using RNN features in an MST parser with a structured loss was proposed by Kiperwasser and Goldberg (2016); their model can be seen as a particular case of our factor graph approach, limited to link prediction with a tree structure constraint.",2 Related work,[0],[0]
"Our models support multi-task learning for proposition classification, parameter-
2Dataset available at http://joonsuk.org.",2 Related work,[0],[0]
"3Available at https://github.com/vene/marseille.
",2 Related work,[0],[0]
"izing adjacent links with higher-order structures (e.g., c → b → a) and enforcing arbitrary constraints on the link structure, not limited to trees.",2 Related work,[0],[0]
"Such higher-order structures and logic constraints have been successfully used for dependency and semantic parsing by Martins et al. (2013) and Martins and Almeida (2014); to our knowledge we are the first to apply them to argument mining, as well as the first to parametrize them with neural networks.",2 Related work,[0],[0]
"Stab and Gurevych (2016) used an integer linear program to combine the output of independent proposition and link classifiers using a hand-crafted scoring formula, an approach similar to our baseline.",2 Related work,[0],[0]
"Our factor graph method can combine the two tasks in a more principled way, as it fully learns the correlation between the two tasks without relying on hand-crafted scoring, and therefore can readily be applied to other argumentation datasets.",2 Related work,[0],[0]
"Furthermore, our model can enforce the tree structure constraint, required on the UKP dataset, using MST cycle constraints used by Stab and Gurevych (2016), thanks to the AD3 inference algorithm (Martins et al., 2015).
",2 Related work,[0],[0]
"Sequence tagging has been applied to the related structured tasks of proposition identification and classification (Stab and Gurevych, 2016; Habernal and Gurevych, 2016; Park et al., 2015b); integrating such models is an important next step.",2 Related work,[0],[0]
"Meanwhile, a new direction in argument mining explores pointer networks (Potash et al., 2016); a promising method, currently lacking support for tree structures and domain-specific constraints.",2 Related work,[0],[0]
"We release a new argument mining dataset consisting of user comments about rule proposals regarding Consumer Debt Collection Practices (CDCP) by the Consumer Financial Protection Bureau collected from an eRulemaking website, http:// regulationroom.org.
Argumentation structures found in web discussion forums, such as the eRulemaking one we use, can be more free-form than the ones encountered in controlled, elicited writing such as (Peldszus and Stede, 2015).",3 Data,[0],[0]
"For this reason, we adopt the model proposed by Park et al. (2015a), which does not constrain links to form tree structures, but unrestricted directed graphs.",3 Data,[0],[0]
"Indeed, over 20% of the comments in our dataset exhibit local structures that would not be allowable in a tree.",3 Data,[0],[0]
"Possible link types are reason and evidence, and propo-
sition types are split into five fine-grained categories: POLICY and VALUE contain subjective judgements/interpretations, where only the former specifies a specific course of action to be taken.",3 Data,[0],[0]
"On the other hand, TESTIMONY and FACT do not contain subjective expressions, the former being about personal experience, or “anecdotal.”",3 Data,[0],[0]
"Lastly, REFERENCE covers URLs and citations, which are used to point to objective evidence in an online setting.
",3 Data,[0],[0]
"In comparison, the UKP dataset (Stab and Gurevych, 2016) only makes the syntactic distinction between CLAIM, MAJOR CLAIM, and PREMISE types, but it also includes attack links.",3 Data,[0],[0]
"The permissible link structure is stricter in UKP, with links constrained in annotation to form one or more disjoint directed trees within each paragraph.",3 Data,[0],[0]
"Also, since web arguments are not necessarily fully developed, our dataset has many argumentative propositions that are not in any argumentation relations.",3 Data,[0],[0]
"In fact, it isn’t unusual for comments to have no argumentative links at all: 28% of CDCP comments have no links, unlike UKP, where all essays have complete argument structures.",3 Data,[0],[0]
"Such comments with no links make the problem harder, emphasizing the importance of capturing the lack of argumentative support, not only its presence.",3 Data,[0],[0]
"Each user comment was annotated by two annotators, who independently annotated the boundaries and types of propositions, as well as the links among them.4 To produce the final corpus, a third annotator manually resolved the conflicts,5 and two automatic preprocessing steps were applied: we take the link transitive closure, and we remove a small number of nested propositions.6 The resulting dataset contains 731 comments, consisting of about 3800 sentences (≈4700 propositions) and 88k words.",3.1 Annotation results,[0],[0]
"Out of the 43k possible pairs of propositions, links are present between only 1300 (roughly 3%).",3.1 Annotation results,[0],[0]
"In comparison, UKP has fewer documents (402), but they are longer, with a total of 7100 sentences (6100 propositions) and 147k
4The annotators used the GATE annotation tool (Cunningham et al., 2011).
",3.1 Annotation results,[0],[0]
"5Inter-annotator agreement is measured with Krippendorf’s α (Krippendorff, 1980) with respect to elementary unit type (α=64.8%) and links (α=44.1%).",3.1 Annotation results,[0],[0]
"A separate paper describing the dataset is under preparation.
",3.1 Annotation results,[0],[0]
"6When two propositions overlap, we keep the one that results in losing the fewest links.",3.1 Annotation results,[0],[0]
"For generality, we release the dataset without this preprocessing, and include code to reproduce it; we believe that handling nested argumentative units is an important direction for further research.
words.",3.1 Annotation results,[0],[0]
"Since UKP links only occur within the same paragraph and propositions not connected to the argument are removed in a preprocessing step, link prediction is less imbalanced in UKP, with 3800 pairs of propositions being linked out of a total of 22k (17%).",3.1 Annotation results,[0],[0]
"We reserve a test set of 150 documents (973 propositions, 272 links) from CDCP, and use the provided 80-document test split from UKP (1266 propositions, 809 links).",3.1 Annotation results,[0],[0]
for argument mining,4 Structured learning,[0],[0]
"Binary and multi-class classification have been applied with some success to proposition and link prediction separately, but we seek a way to jointly learn the argument mining problem at the document level, to better model contextual dependencies and constraints.",4.1 Preliminaries,[0],[0]
"We therefore turn to structured learning, a framework that provides the desired level of expressivity.
",4.1 Preliminaries,[0],[0]
"In general, learning from a dataset of documents xi ∈ X and their associated labels yi ∈ Y involves seeking model parameters w that can “pick out” the best label under a scoring function f :
ŷ",4.1 Preliminaries,[0],[0]
":= argmaxy∈Y f(x, y;w).",4.1 Preliminaries,[0],[0]
"(1)
Unlike classification or regression, where X is usually a feature space Rd and Y ⊆ R (e.g., we predict an integer class index or a probability), in structured learning, more complex inputs and outputs are allowed.",4.1 Preliminaries,[0],[0]
"This makes the argmax in Equation 1 impossible to evaluate by enumeration, so it is desirable to find models that decompose over smaller units and dependencies between them; for instance, as factor graphs.",4.1 Preliminaries,[0],[0]
"In this section, we give a factor graph description of our proposed structured model for argument mining.",4.1 Preliminaries,[0],[0]
An input document is a string of words with proposition offsets delimited.,4.2 Model description,[0],[0]
"We denote the propositions in a document by {a, b, c, ...} and the possible directed link between a and b as a → b.",4.2 Model description,[0],[0]
The argument structure we seek to predict consists of the type of each proposition ya ∈ P and a binary label for each link ya→b ∈,4.2 Model description,[0],[0]
"R = {on, off}.7
7For simplicity and comparability, we follow Stab and Gurevych (2016) in using binary link labels even if links could be of different types.",4.2 Model description,[0],[0]
"This can be addressed in our model by incorporating “labeled link” factors.
",4.2 Model description,[0],[0]
The possible proposition types P differ for the two datasets; such differences are documented in Table 1.,4.2 Model description,[0],[0]
"As we describe the variables and factors constituting a document’s factor graph, we shall refer to Figure 2 for illustration.
",4.2 Model description,[0],[0]
Unary potentials.,4.2 Model description,[0],[0]
Each proposition a and each link a → b has a corresponding random variable in the factor graph (the circles in Figure 2).,4.2 Model description,[0],[0]
"To encode the model’s belief in each possible value for these variables, we parametrize the unary factors (gray boxes in Figure 2) with unary potentials: φ(a) ∈ R|P| is a score of ya for each possible proposition type.",4.2 Model description,[0],[0]
"Similarly, link unary potentials φ(a → b) ∈ R|R| are scores for ya→b being on/off.",4.2 Model description,[0],[0]
"Without any other factors, this would amount to independent classifiers for each task.
",4.2 Model description,[0],[0]
Compatibility factors.,4.2 Model description,[0],[0]
"For every possible link a → b, the variables (a, b, a → b) are bound by a dense factor scoring their joint assignment (the black boxes in Figure 2).",4.2 Model description,[0],[0]
"Such a factor could automatically learn to encourage links from compatible types (e.g., from TESTIMONY to POLICY) or discourage links between less compatible ones (e.g., from FACT to TESTIMONY).",4.2 Model description,[0],[0]
"In the simplest form, this factor would be parametrized as a tensor T ∈ R|P|×|P|×|R|, with tijk retaining the score of a source proposition of type i to be (k = on) or not to be (k = off) in a link with a proposition of type j. For more flexibility, we parametrize this factor with compatibility features depending
only on simple structure: tijk becomes a vector, and the score of configuration (i, j, k) is given by v>abtijk where vab consists of three binary features:
• bias: a constant value of 1, allowing T to learn a base score for a label configuration (i, j, k), as in the simple form above,
• adjacency: when there are no other propositions between the source and the target,
• order: when the source precedes the target.
",4.2 Model description,[0],[0]
Second order factors.,4.2 Model description,[0],[0]
Local argumentation graph structures such as a → b → c might be modeled better together rather than through separate link factors for a → b and b → c.,4.2 Model description,[0],[0]
"As in higher-order structured models for semantic and dependency parsing (Martins et al., 2013; Martins and Almeida, 2014), we implement three types of second order factors: grandparent (a → b → c), sibling (a ← b → c), and co-parent (a → b ← c).",4.2 Model description,[0],[0]
"Not all of these types of factors make sense on all datasets: as sibling structures cannot exist in directed trees, we don’t use sibling factors on UKP.",4.2 Model description,[0],[0]
"On CDCP, by transitivity, every grandparent structure implies a corresponding sibling, so it is sufficient to parametrize siblings.",4.2 Model description,[0],[0]
"This difference between datasets is emphasized in Figure 2, where one example of each type of factor is pictured on the right side of the graphs (orange boxes with curved edges): on CDCP we illustrate a coparent factor (top right) and a sibling factor (bot-
tom right), while on UKP we show a co-parent factor (top right) and a grandparent factor (bottom right).",4.2 Model description,[0],[0]
"We call these factors second order because they involve two link variables, scoring the joint assignment of both links being on.
",4.2 Model description,[0],[0]
Valid link structure.,4.2 Model description,[0],[0]
The global structure of argument links can be further constrained using domain knowledge.,4.2 Model description,[0],[0]
We implement this using constraint factors; these have no parameters and are denoted by empty boxes in Figure 2.,4.2 Model description,[0],[0]
"In general, well-formed arguments should be cycle-free.",4.2 Model description,[0],[0]
"In the UKP dataset, links form a directed forest and can never cross paragraphs.",4.2 Model description,[0],[0]
"This particular constraint can be expressed as a series of tree factors,8 one for each paragraph (the factor connected to all link variables in Figure 2).",4.2 Model description,[0],[0]
"In CDCP, links do not form a tree, but we use logic constraints to enforce transitivity (top left factor in Figure 2) and to prevent symmetry (bottom left); the logic formulas implemented by these factors are described in Table 1.",4.2 Model description,[0],[0]
"Together, the two constraints have the desirable side effect of preventing cycles.
",4.2 Model description,[0],[0]
Strict constraints.,4.2 Model description,[0],[0]
"We may include further domain-specific constraints into the model, to express certain disallowed configurations.",4.2 Model description,[0],[0]
"For instance, proposition types that appear in CDCP data can be ordered by the level of objectivity (Park et al., 2015a), as shown in Table 1.",4.2 Model description,[0],[0]
"In a wellformed argument, we would want to see links from more objective to equally or less objective propositions: it’s fine to provide FACT as reason for VALUE, but not the other way around.",4.2 Model description,[0],[0]
"While the training data sometimes violates this constraint, enforcing it might provide a useful inductive bias.
Inference.",4.2 Model description,[0],[0]
"The argmax in Equation 1 is a MAP over a factor graph with cycles and many overlapping factors, including logic factors.",4.2 Model description,[0],[0]
"While exact inference methods are generally unavailable, our setting is perfectly suited for the Alternating Directions Dual Decomposition (AD3) algorithm: approximate inference on expressive factor graphs with overlapping factors, logic constraints, and generic factors (e.g., directed tree factors) defined through maximization oracles (Martins et al., 2015).",4.2 Model description,[0],[0]
"When AD3 returns an integral solution, it is globally optimal, but when solutions are frac-
8A tree factor regards each bound variable as an edge in a graph and assigns −∞ scores to configurations that are not valid trees.",4.2 Model description,[0],[0]
"For inference, we can use maximum spanning arborescence algorithms such as Chu-Liu/Edmonds.
tional, several options are available.",4.2 Model description,[0],[0]
"At test time, for analysis, we retrieve exact solutions using the branch-and-bound method.",4.2 Model description,[0],[0]
"At training time, however, fractional solutions can be used as-is; this makes better use of each iteration and actually increases the ratio of integral solutions in future iterations, as well as at test time, as proven by Meshi et al. (2016).",4.2 Model description,[0],[0]
"We also find that after around 15 training iterations with fractional solutions, over 99% of inference calls are integral.
Learning.",4.2 Model description,[0],[0]
"We train the models by minimizing the structured hinge loss (Taskar et al., 2004):∑ (x,",4.2 Model description,[0],[0]
"y)∈D max y′∈Y (f(x, y′;w)",4.2 Model description,[0],[0]
+,4.2 Model description,[0],[0]
"ρ(y, y′))− f(x, y;w) (2) where ρ is a configurable misclassification cost.",4.2 Model description,[0],[0]
"The max in Equation 2 is not the same as the one used for prediction, in Equation 1.",4.2 Model description,[0],[0]
"However, when the cost function ρ decomposes over the variables, cost-augmented inference amounts to regular inference after augmenting the potentials accordingly.",4.2 Model description,[0],[0]
"We use a weighted Hamming cost:
ρ(y, ŷ)",4.2 Model description,[0],[0]
":= ∑ v ρ(yv)I[yv = ŷv]
where v is summed over all variables in a document {a} ∪ {a → b}, and ρ(yv) is a misclassification cost.",4.2 Model description,[0],[0]
"We assign uniform costs ρ to 1 for all mistakes except false-negative links, where we use higher cost proportional to the class imbalance in the training split, effectively giving more weight to positive links during training.",4.2 Model description,[0],[0]
"One option for parameterizing the potentials of the unary and higher-order factors is with linear models, using proposition, link, and higher-order features.",4.3 Argument structure SVM,[0],[0]
"This gives birth to a linear structured SVM (Tsochantaridis et al., 2005), which, when using l2 regularization, can be trained efficiently in the dual using the online block-coordinate FrankWolfe algorithm of Lacoste-Julien et al. (2013), as implemented in the pystruct library (Müller and Behnke, 2014).",4.3 Argument structure SVM,[0],[0]
"This algorithm is more convenient than subgradient methods, as it does not require tuning a learning rate parameter.
",4.3 Argument structure SVM,[0],[0]
Features.,4.3 Argument structure SVM,[0],[0]
"For unary proposition and link features, we faithfully follow Stab and Gurevych (2016, Tables 9 and 10): proposition features are
lexical (unigrams and dependency tuples), structural (token statistics and proposition location), indicators (from hand-crafted lexicons), contextual, syntactic (subclauses, depth, tense, modal, and POS), probability, discourse (Lin et al., 2014), and average GloVe embeddings (Pennington et al., 2014).",4.3 Argument structure SVM,[0],[0]
"Link features are lexical (unigrams), syntactic (POS and productions), structural (token statistics, proposition statistics and location features), hand-crafted indicators, discourse triples, PMI, and shared noun counts.
",4.3 Argument structure SVM,[0],[0]
"Our proposed higher-order factors for grandparent, co-parent, and sibling structures require features extracted from a proposition triplet a, b, c. In dependency and semantic parsing, higher-order factors capture relationships between words, so sparse indicator features can be efficiently used.",4.3 Argument structure SVM,[0],[0]
"In our case, since propositions consist of many words, BOW features may be too noisy and too dense; so for simplicity we again take a cue from the link-specific features used by Stab and Gurevych (2016).",4.3 Argument structure SVM,[0],[0]
"Our higher-order factor features are: same sentence indicators (for all 3 and for each pair), proposition order (one for each of the 6 possible orderings), Jaccard similarity (between all 3 and between each pair), presence of any shared nouns (between all 3 and between each pair), and shared noun ratios: nouns shared by all 3 divided by total nouns in each proposition and each pair, and shared nouns between each pair with respect to each proposition.",4.3 Argument structure SVM,[0],[0]
"Up to vocabulary size difference, our total feature dimensionality is approximately 7000 for propositions and 2100 for links.",4.3 Argument structure SVM,[0],[0]
"The number of second order features is 35.
Hyperparameters.",4.3 Argument structure SVM,[0],[0]
"We pick the SVM regularization parameter C ∈ {0.001, 0.003, 0.01, 0.03, 0.1, 0.3} by k-fold cross validation at document level, optimizing for the average between link and proposition F1 scores.",4.3 Argument structure SVM,[0],[0]
Neural network methods have proven effective for natural language problems even with minimalto-no feature engineering.,4.4 Argument structure RNN,[0],[0]
"Inspired by the use of LSTMs (Hochreiter and Schmidhuber, 1997) for MST dependency parsing by Kiperwasser and Goldberg (2016), we parametrize the potentials in our factor graph with an LSTM-based neural network,9 replacing MST inference with the more general AD3 algorithm, and using relaxed solutions for training when inference is inexact.
",4.4 Argument structure RNN,[0],[0]
"We extract embeddings of all words with a corpus frequency > 1, initialized with GloVe word vectors.",4.4 Argument structure RNN,[0],[0]
"We use a deep bidirectional LSTM to encode contextual information, representing a proposition a as the average of the LSTM outputs of its words, henceforth denoted ↔ a.
Proposition potentials.",4.4 Argument structure RNN,[0],[0]
"We apply a multi-layer perceptron (MLP) with rectified linear activations to each proposition, with all layer dimensions equal except the final output layer, which has size |P| and is not passed through any nonlinearities.
",4.4 Argument structure RNN,[0],[0]
Link potentials.,4.4 Argument structure RNN,[0],[0]
"To score a dependency a → b, Kiperwasser and Goldberg (2016) pass the concatenation",4.4 Argument structure RNN,[0],[0]
[ ↔ a; ↔ b ] through an MLP.,4.4 Argument structure RNN,[0],[0]
"After trying this, we found slightly better performance by first passing each proposition through a slot-specific
dense layer ( a := σsrc( ↔ a), b := σtrg( ↔ b) )
followed by a bilinear transformation:
φon(a→ b) := a > Wb+w>srca+w > trgb+ w",4.4 Argument structure RNN,[0],[0]
"(on) 0 .
",4.4 Argument structure RNN,[0],[0]
"Since the bilinear expression returns a scalar, but the link potentials must have a value for both the on and off states, we set the full potential to φ(a → b) := [φon(a → b), w(off)0 ] where w (off) 0 is a learned scalar bias.",4.4 Argument structure RNN,[0],[0]
"We initialize W to the diagonal identity matrix.
",4.4 Argument structure RNN,[0],[0]
"9We use the dynet library (Neubig et al., 2017).
",4.4 Argument structure RNN,[0],[0]
Second order potentials.,4.4 Argument structure RNN,[0],[0]
"Grandparent potentials φ(a → b → c) score two adjacent directed edges, in other words three propositions.",4.4 Argument structure RNN,[0],[0]
We again first pass each proposition representation through a slot-specific dense layer.,4.4 Argument structure RNN,[0],[0]
"We implement a multilinear scorer analogously to the link potentials:
φ(a→ b→ c) := ∑ i,j,k aibjckwijk
where W = (w)ijk is a third-order cube tensor.",4.4 Argument structure RNN,[0],[0]
"To reduce the large numbers of parameters, we implicitly represent W as a rank r tensor: wijk = ∑r s=1",4.4 Argument structure RNN,[0],[0]
u (1) is u (2),4.4 Argument structure RNN,[0],[0]
js u,4.4 Argument structure RNN,[0],[0]
(3) ks .,4.4 Argument structure RNN,[0],[0]
"Notably, this model captures only third-order interactions between the representation of the three propositions.",4.4 Argument structure RNN,[0],[0]
"To capture first-order “bias” terms, we could include slotspecific linear terms, e.g., w>a a; but to further capture quadratic backoff effects (for instance, if two propositions carry a strong signal of being siblings regardless of their parent), we would require quadratically many parameters.",4.4 Argument structure RNN,[0],[0]
"Instead of explicit lower-order terms, we propose augmenting a, b, and c with a constant feature of 1, which has approximately the same effect, while benefiting from the parameter sharing in the low-rank factorization; an effect described by Blondel et al. (2016).",4.4 Argument structure RNN,[0],[0]
"Siblings and co-parents factors are similarly parametrized with their own tensors.
",4.4 Argument structure RNN,[0],[0]
Hyperparameters.,4.4 Argument structure RNN,[0],[0]
"We perform grid search using k-fold document-level cross-validation, tuning the dropout probability in the dense MLP layers over {0.05, 0.1, 0.15, 0.2, 0.25} and the optimal number of passes over the training data over {10, 25, 50, 75, 100}.",4.4 Argument structure RNN,[0],[0]
"We use 2 layers for the LSTM and the proposition classifier, 128 hidden units in all layers, and a multilinear decomposition with rank r = 16, after preliminary CV runs.",4.4 Argument structure RNN,[0],[0]
We compare our proposed models to equivalent independent unary classifiers.,4.5 Baseline models,[0],[0]
"The unary-only version of a structured SVM is an l2-regularized linear SVM.10 For the RNN, we compute unary potentials in the same way as in the structured model, but apply independent hinge losses at each variable, instead of the global structured hinge loss.",4.5 Baseline models,[0],[0]
"Since the RNN weights are shared, this is a form of multi-task learning.",4.5 Baseline models,[0],[0]
"The baseline predictions can
10We train our SVM using SAGA (Defazio et al., 2014) in lightning (Blondel and Pedregosa, 2016).
be interpreted as unary potentials, therefore we can simply round their output to the highest scoring labels, or we can, alternatively, perform testtime inference, imposing the desired structure.",4.5 Baseline models,[0],[0]
We evaluate our proposed models on both datasets.,5 Results,[0],[0]
"For model selection and development we used kfold cross-validation at document level: on CDCP we set k = 3 to avoid small validation folds, while on UKP we follow Stab and Gurevych (2016) setting k = 5.",5 Results,[0],[0]
We compare our proposed structured learning systems (the linear structured SVM and the structured RNN) to the corresponding baseline versions.,5 Results,[0],[0]
"We organize our experiments in three incremental variants of our factor graph: basic, full, and strict, each with the following components:11
component basic full strict (baseline)
unaries X X X X compat.",5 Results,[0],[0]
factors X X X compat.,5 Results,[0],[0]
"features X X higher-order X X link structure X X X strict constraints X X
Following Stab and Gurevych (2016), we compute F1 scores at proposition and link level, and also report their average as a summary of overall performance.12 The results of a single prediction run on the test set are displayed in Table 2.",5 Results,[0],[0]
"The overall trend is that training using a structured objective is better than the baseline models, even when structured inference is applied on the baseline predictions.",5 Results,[0],[0]
"On UKP, for link prediction, the linear baseline can reach good performance when using inference, similar to the approach of Stab and Gurevych (2016), but the improvement in proposition prediction leads to higher overall F1 for the structured models.",5 Results,[0],[0]
"Meanwhile, on the more difficult CDCP setting, performing inference on the baseline output is not competitive.",5 Results,[0],[0]
"While feature engineering still outperforms our RNN model, we find that RNNs shine on proposition classification, especially on UKP, and that structured training can make them competitive, reducing their observed lag on link prediction (Katiyar and Cardie, 2016), possibly through mitigating class imbalance.
11Components are described in Section 4.",5 Results,[0],[0]
"The baselines with inference support only unaries and factors with no parameters, as indicated in the last column.
",5 Results,[0],[0]
"12For link F1 scores, however, we find it more intuitive to only consider retrieval of positive links rather than macroaveraged two-class scores.",5 Results,[0],[0]
Contribution of compatibility features.,5.1 Discussion and analysis,[0],[0]
The compatibility factor in our model can be visualized as conditional odds ratios given the source and target proposition types.,5.1 Discussion and analysis,[0],[0]
"Since there are only four possible configurations of the compatibility features, we can plot all cases in Figure 3, alongside the basic model.",5.1 Discussion and analysis,[0],[0]
"Not using compatibility features, the basic model can only learn whether certain configurations are more likely than others (e.g. a REFERENCE supporting another REFERENCE is unlikely, while a REFERENCE supporting a FACT is more likely; essentially a soft version of our domain-specific strict constraints.",5.1 Discussion and analysis,[0],[0]
"The full model with compatibility features is finer grained, capturing, for example, that links from REFERENCE to FACT are more likely when the reference comes after, or that links from VALUE to POLICY are extremely likely only when the two are adjacent.
",5.1 Discussion and analysis,[0],[0]
Proposition errors.,5.1 Discussion and analysis,[0],[0]
The confusion matrices in Figure 4 reveal that the most common confusion is misclassifying FACT as VALUE.,5.1 Discussion and analysis,[0],[0]
The strongest difference between the various models tested is that the RNN-based models make this error less often.,5.1 Discussion and analysis,[0],[0]
"For instance, in the proposition:
And the single most frequently used excuse of any debtor is “I didn’t receive the letter/invoice/statement”
the pronouns in the nested quote may be mistaken for subjectivity, leading to the structured SVMs
predictions of VALUE or TESTIMONY, while the basic structured RNN correctly classifies it as FACT.
",5.1 Discussion and analysis,[0],[0]
Link errors.,5.1 Discussion and analysis,[0],[0]
"While structured inference certainly helps baselines by preventing invalid structures such as cycles, it still depends on local decisions, losing to fully structured training in cases where joint proposition and link decisions are needed.",5.1 Discussion and analysis,[0],[0]
"For instance, in the following conclusion of an UKP essay, the annotators found no links:
In short, [ the individual should finance his or her education ]a because [ it is a personal choice.",5.1 Discussion and analysis,[0],[0]
"]b Otherwise, [ it would cause too much cost from taxpayers and the government.",5.1 Discussion and analysis,[0],[0]
"]c
Indeed, no reasons are provided, but baseline are misled by the connectives: the SVM baseline outputs that b and c are PREMISEs supporting the CLAIM a.",5.1 Discussion and analysis,[0],[0]
"The full structured SVM combines the two tasks and correctly recognizes the link structure.
",5.1 Discussion and analysis,[0],[0]
"Linear SVMs are still a very good baseline, but they tend to overgenerate links due to class imbalance, even if we use class weights during training.",5.1 Discussion and analysis,[0],[0]
"Surprisingly, RNNs are at the opposite end, being extremely conservative, and getting the highest precision among the models.",5.1 Discussion and analysis,[0],[0]
"On CDCP, where the number of true links is 272, the linear baseline with strict inference predicts 796 links with a precision of only 16%, while the strict structured RNN only predicts 52 links, with 33% precision; the example in Figure 5 illustrates this.",5.1 Discussion and analysis,[0],[0]
"In terms of higher-order structures, we find that using higherorder factors increases precision, at a cost in recall.
",5.1 Discussion and analysis,[0],[0]
"This is most beneficial for the 856 co-parent structures in the UKP test set: the full structured SVM has 53% F1, while the basic structured SVM and the basic baseline get 47% and 45% respectively.",5.1 Discussion and analysis,[0],[0]
"On CDCP, while higher-order factors help, performance on siblings and co-parents is below 10% F1 score.",5.1 Discussion and analysis,[0],[0]
This is likely due to link sparsity and suggests plenty of room for further development.,5.1 Discussion and analysis,[0],[0]
"We introduce an argumentation parsing model based on AD3 relaxed inference in expressive factor graphs, experimenting with both linear struc-
tured SVMs and structured RNNs, parametrized with higher-order factors and link structure constraints.",6 Conclusions and future work,[0],[0]
We demonstrate our model on a new argumentation mining dataset with more permissive argument structure annotation.,6 Conclusions and future work,[0],[0]
"Our model also achieves state-of-the-art link prediction performance on the UKP essays dataset.
",6 Conclusions and future work,[0],[0]
Future work.,6 Conclusions and future work,[0],[0]
"Stab and Gurevych (2016) found polynomial kernels useful for modeling feature interactions, but kernel structured SVMs scale poorly, we intend to investigate alternate ways to capture feature interactions.",6 Conclusions and future work,[0],[0]
"While we focus on monological argumentation, our model could be extended to dialogs, for which argumentation theory thoroughly motivates non-tree structures (Afantenos and Asher, 2014).",6 Conclusions and future work,[0],[0]
We are grateful to André,Acknowledgements,[0],[0]
"Martins, Andreas Müller, Arzoo Katyiar, Chenhao Tan, Felix Wu, Jack Hessel, Justine Zhang, Mathieu Blondel, Tianze Shi, Tobias Schnabel, and the rest of the Cornell NLP seminar for extremely helpful discussions.",Acknowledgements,[0],[0]
We thank the anonymous reviewers for their thorough and well-argued feedback.,Acknowledgements,[0],[0]
"We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure.",abstractText,[0],[0]
(This is the case in over 20% of the web comments dataset we release.),abstractText,[0],[0]
Our model jointly learns elementary unit type classification and argumentative relation prediction.,abstractText,[0],[0]
"Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions.",abstractText,[0],[0]
Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.,abstractText,[0],[0]
Argument Mining with Structured SVMs and RNNs,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 217–226, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,1 Introduction,[0],[0]
"The related corpora are often huge, covering any topic imaginable, thus providing novel opportunities to address a number of open questions about the structure of dialogue.",1 Introduction,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,1 Introduction,[0],[0]
"We build a new dataset of 109,074 posts on the topics gay marriage, gun control, death penalty and evolution.",1 Introduction,[0],[0]
"We frame our problem as consisting of two separate tasks:
• Argument Extraction: How can we extract argument segments in dialogue that clearly express a particular argument facet?
",1 Introduction,[0],[0]
• Argument,1 Introduction,[0],[0]
"Facet Similarity: How can we recognize that two argument segments are semantically similar, i.e. about the same facet of the argument?
",1 Introduction,[0],[0]
Consider for example the sample posts and responses in Fig. 1.,1 Introduction,[0],[0]
"Argument segments that are good targets for argument extraction are indicated, in their dialogic context, in bold.",1 Introduction,[0],[0]
"Given extracted segments, the argument facet similarity module should recognize that R3 and R4 paraphrase the same argument facet, namely that there is a strong relationship between the availability of guns and the murder rate.",1 Introduction,[0],[0]
"This paper addresses only the argument extraction task, as an important first step towards producing argument summaries that reflect the range and type of arguments being made,
217
on a topic, over time, by citizens in public forums.",1 Introduction,[0],[0]
"Our approach to the argument extraction task is driven by a novel hypothesis, the IMPLICIT MARKUP hypothesis.",1 Introduction,[0],[0]
"We posit that the arguments that are good candidates for extraction will be marked by cues (implicit markups) provided by the dialog conversants themselves, i.e. their choices about the surface realization of their arguments.",1 Introduction,[0],[0]
"We examine a number of theoretically motivated cues for extraction, that we expect to be domain-independent.",1 Introduction,[0],[0]
"We describe how we use these cues to sample from the corpus in a way that lets us test the impact of the hypothesized cues.
",1 Introduction,[0],[0]
Both the argument extraction and facet similarity tasks have strong similarities to other work in natural language processing.,1 Introduction,[0],[0]
Argument extraction resembles the sentence extraction phase of multi-document summarization.,1 Introduction,[0],[0]
"Facet similarity resembles semantic textual similarity and paraphrase recognition (Misra et al., 2015; Boltuzic and Šnajder, 2014; Conrad et al., 2012; Han et al., 2013; Agirre et al., 2012).",1 Introduction,[0],[0]
"Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015).
",1 Introduction,[0],[0]
"Sec. 2 describes our corpus of arguments, and describes the hypothesized markers of highquality argument segments.",1 Introduction,[0],[0]
"We sample from the corpus using these markers, and then annotate the extracted argument segments for ARGUMENT QUALITY.",1 Introduction,[0],[0]
Sec. 3.2 describes experiments to test whether: (1) we can predict argument quality; (2) our hypothesized cues are good indicators of argument quality; and (3) an argument quality predictor trained on one topic or a set of topics can be used on unseen topics.,1 Introduction,[0],[0]
The results in Sec. 4 show that we can predict argument quality with RRSE values as low as .73 for some topics.,1 Introduction,[0],[0]
"Cross-domain training combined with domainadaptation yields RRSE values for several topics as low as .72, when trained on topic independent features, however some topics are much more difficult.",1 Introduction,[0],[0]
We provide a comparison of our work to previous research and sum up in Sec. 5.,1 Introduction,[0],[0]
"We created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) and evolution (EV, 43624), by combining the Internet Argument Corpus (IAC) (Walker et al., 2012), with dialogues from http://www.createdebate.com/.
Our aim is to develop a method that can extract high quality arguments from a large corpus of argumentative dialogues, in a topic and domain-
independent way.",2 Corpus and Method,[0],[0]
It is important to note that arbitrarily selected utterances are unlikely to be high quality arguments.,2 Corpus and Method,[0],[0]
"Consider for example all the utterances in Fig. 1: many utterances are either not interpretable out of context, or fail to clearly frame an argument facet.",2 Corpus and Method,[0],[0]
Our IMPLICIT MARKUP hypothesis posits that arguments that are good candidates for extraction will be marked by cues from the surface realization of the arguments.,2 Corpus and Method,[0],[0]
We first describe different types of cues that we use to sample from the corpus in a way that lets us test their impact.,2 Corpus and Method,[0],[0]
"We then describe the MT HIT, and how we use our initial HIT results to refine our sampling process.",2 Corpus and Method,[0],[0]
"Table 2 presents the results of our sampling and annotation processes, which we will now explain in more detail.",2 Corpus and Method,[0],[0]
"The IMPLICIT MARKUP hypothesis is composed of several different sub-hypotheses as to how speakers in dialogue may mark argumentative structure.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Discourse Relation hypothesis suggests that the Arg1 and Arg2 of explicit SPECIFICATION, CONTRAST, CONCESSION and CONTINGENCY markers are more likely to contain good argumentative segments (Prasad et al., 2008).",2.1 Implicit Markup Hypothesis,[0],[0]
"In the case of explicit connectives, Arg2 is the argument to which the connective is syntactically bound, and Arg1 is the other argument.",2.1 Implicit Markup Hypothesis,[0],[0]
"For example, a CONTINGENCY relation is frequently marked by the lexical anchor If, as in R1 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A CONTRAST relation may mark a challenge to an opponent’s claim, what Ghosh et al. call callout-target argument pairs (Ghosh et al., 2014b; Maynard, 1985).",2.1 Implicit Markup Hypothesis,[0],[0]
"The CONTRAST relation is frequently marked by But, as in R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"A SPECIFICATION relation may indicate a focused detailed argument, as marked by First in R2 in Fig. 1 (Li and Nenkova, 2015).",2.1 Implicit Markup Hypothesis,[0],[0]
"We decided to extract only the Arg2, where the discourse argument is syntactically bound to the connective, since Arg1’s are more difficult to locate, especially in dialogue.",2.1 Implicit Markup Hypothesis,[0],[0]
"We began by extracting the Arg2’s for the connectives most strongly associated with these discourse relations over the whole corpus, and then once we saw what the most frequent connectives were in our corpus, we refined this selection to include only but, if, so, and first.",2.1 Implicit Markup Hypothesis,[0],[0]
"We sampled a roughly even distribution of sentences from each category as well as sentences without any discourse connectives, i.e. None.",2.1 Implicit Markup Hypothesis,[0],[0]
See Table.,2.1 Implicit Markup Hypothesis,[0],[0]
"2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Syntactic Properties hypothesis posits that syntactic properties of a clause may indicate good argument segments, such as being the main clause (Marcu, 1999), or the sentential complement of mental state or speech-act verbs, e.g. the SBAR
in you agree that SBAR as in P2 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
"Because these markers are not as frequent in our corpus, we do not test this with sampling: rather we test it as a feature as described in Sec. 3.2.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Dialogue Structure hypothesis suggests that position in the post or the relation to a verbatim quote could influence argument quality, e.g. being turn-initial in a response as exemplified by P2, R3 and R4 in Fig. 1.",2.1 Implicit Markup Hypothesis,[0],[0]
We indicate sampling by position in post with Starts:,2.1 Implicit Markup Hypothesis,[0],[0]
Yes/No in Table.,2.1 Implicit Markup Hypothesis,[0],[0]
2.,2.1 Implicit Markup Hypothesis,[0],[0]
Our corpora are drawn from websites that offer a “quoting affordance” in addition to a direct reply.,2.1 Implicit Markup Hypothesis,[0],[0]
"An example of a post from the IAC corpus utilizing this mechanism is shown in Table 1, where the quoted text is highlighted in blue and the response is directly below it.
",2.1 Implicit Markup Hypothesis,[0],[0]
"The Semantic Density hypothesis suggests that measures of rich content or SPECIFICITY will indicate good candidates for argument extraction (Louis and Nenkova, 2011).",2.1 Implicit Markup Hypothesis,[0],[0]
We initially posited that short sentences and sentences without any topic-specific words are less likely to be good.,2.1 Implicit Markup Hypothesis,[0],[0]
"For the topics gun control and gay marriage, we filtered sentences less than 4 words long, which removed about 8-9% of the sentences.",2.1 Implicit Markup Hypothesis,[0],[0]
"After collecting the argument quality annotations for these two topics and examining the distribution of scores (see Sec. 2.2 below), we developed an additional measure of semantic density that weights words in each candidate by its pointwise mutual information (PMI), and applied it to the evolution and death penalty.",2.1 Implicit Markup Hypothesis,[0],[0]
"Using the 26 topic annotations in the IAC, we calculate the PMI between every word in the corpus appearing more than 5 times and each topic.",2.1 Implicit Markup Hypothesis,[0],[0]
We only keep those sentences that have at least one word whose PMI is above our threshold of 0.1.,2.1 Implicit Markup Hypothesis,[0],[0]
"We determined this threshold by examining the values in gun control and gay marriage, such that at least 2/3 of the filtered sentences were in the bottom third of the argument quality score.",2.1 Implicit Markup Hypothesis,[0],[0]
"The PMI filter eliminates 39% of the sentences from death penalty (40% combined with the length filter) and 85% of the sentences from
evolution (87% combined with the length filter).",2.1 Implicit Markup Hypothesis,[0],[0]
Table 2 summarizes the results of our sampling procedure.,2.1 Implicit Markup Hypothesis,[0],[0]
"Overall our experiments are based on 5,374 sampled sentences, with roughly equal numbers over each topic, and equal numbers representing each of our hypotheses and their interactions.",2.1 Implicit Markup Hypothesis,[0],[0]
Table 8 in the Appendix provides example argument segments resulting from the sampling and annotation process.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Sometimes arguments are completely self contained, e.g. S1 to S8 in Table 8.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"In other cases, e.g. S9 to S16 we can guess what the argument is based on using world knowledge of the domain, but it is not explicitly stated or requires several steps of inference.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"For example, we might be able to infer the argument in S14 in Table 8, and the context in which it arose, even though it is not explicitly stated.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"Finally, there are cases where the user is not making an argument or the argument cannot be reconstructed without significantly more context, e.g. S21 in Table 8.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collect annotations for ARGUMENT QUALITY for all the sentences summarized in Table 2 on Amazon’s Mechanical Turk (AMT) platform.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Figure 3 in the Appendix illustrates the basic layout of the HIT.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
Each HIT consisted of 20 sentences on one topic which is indicated on the page.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The annotator first checked a box if the sentence expressed an argument, and then rated the argument quality using a continuous slider ranging from hard (0.0) to easy to interpret (1.0).
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We collected 7 annotations per sentence.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"All Turkers were required to pass our qualifier, have a HIT approval rating above 95%, and be located in the United States, Canada, Australia, or Great Britain.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The results of the sampling and annotation on the final annotated corpus are in Table 2.
","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We measured the inter-annotator agreement (IAA) of the binary annotations using Krippendorff’s α (Krippendorff, 2013) and the continuous values using the intraclass correlation coefficient (ICC) for each topic.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We found that annotators could not distinguish between phrases that did not express an argument and hard sentences.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
See examples and definitions in Fig. 3.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We therefore mapped unchecked sentences (i.e., non arguments) to zero argument quality.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"We then calculated the average pairwise ICC value for each rater between all Turkers with overlapping annotations, and removed the judgements of any Turker that did not have a positive ICC value.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
The ICC for each topic is shown in Table 2.,"2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The mean rating across the remaining annotators for each sentence was used as the gold standard for argument quality, with means in the Argument Quality (AQ) column of Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The effect of the sampling on
argument quality can be seen in Table 2.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
"The differences between gun control and gay marriage, and the other two topics is due to effective use of the semantic density filter, which shifted the distribution of the annotated data towards higher quality arguments as we intended.","2.2 Data Sampling, Annotation and Analysis",[0],[0]
We can now briefly validate some of the IMPLICIT MARKUP hypothesis using an ANOVA testing the effect of a connective and its position in post on argument quality.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Across all sentences in all topics, the presence of a connective is significant (p = 0.00).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Three connectives, if, but, and so, show significant differences in AQ from no-connective phrases (p = 0.00, 0.02, 0.00, respectively).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
First does not show a significant effect.,3.1 Implicit Markup Hypothesis Validation,[0],[0]
"The mean AQ scores for sentences marked by if, but, and so differ from that of a no-connective sentence by 0.11, 0.04, and 0.04, respectively.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"These numbers support our hypothesis that there are certain discourse connectives or cue words which can help to signal the existence of arguments, and they seem to suggest that the CONTINGENCY category may be most useful, but more research using more cue words is necessary to validate this suggestion.
",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"In addition to the presence of a connective, the dialogue structural position of being an initial sentence in a response post did not predict argument quality as we expected.",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"Response-initial sentences provide significantly lower quality arguments (p = 0.00), with response-initial sentences having an average AQ score 0.03 lower (0.40 vs. 0.43).",3.1 Implicit Markup Hypothesis Validation,[0],[0]
"We use 3 regression algorithms from the Java Statistical Analysis Toolkit1: Linear Least Squared Error (LLS), Ordinary Kriging (OK) and Support Vector Machines using a radial basis function kernel (SVM).",3.2 Argument Quality Regression,[0],[0]
A random 75% of the sentences of each domain were put into training/development and 25% into the held out test.,3.2 Argument Quality Regression,[0],[0]
Training involved a grid search over the hyper-parameters of each model2 and a subset (23-29 and the complete set) of the top N features whose values correlate best with the argument quality dependent variable (using Pearson’s).,3.2 Argument Quality Regression,[0],[0]
"The combined set of parameters and features that achieved the best mean squared error over a 5-fold cross validation on the training data was used to train the complete model.
",3.2 Argument Quality Regression,[0],[0]
"We also compare hand-curated feature sets that are motivated by our hypotheses to this simple
1https://github.com/EdwardRaff/JSAT 2We used the default parameters for LLS and OK and only
searched hyper-parameters for the SVM model.
feature selection method, and the performance of in-domain, cross-domain, and domain-adaptation training using “the frustratingly easy” approach (Daumé III, 2007).
",3.2 Argument Quality Regression,[0],[0]
We use our training and development data to develop a set of feature templates.,3.2 Argument Quality Regression,[0],[0]
"The features are real-valued and normalized between 0 and 1, based on the min and max values in the training data for each domain.",3.2 Argument Quality Regression,[0],[0]
If not stated otherwise the presence of a feature was represented by 1.0 and its absence by 0.0.,3.2 Argument Quality Regression,[0],[0]
We describe all the handcurated feature sets below.,3.2 Argument Quality Regression,[0],[0]
Semantic Density Features: Deictic Pronouns (DEI): The presence of anaphoric references are likely to inhibit the interpretation of an utterance.,3.2 Argument Quality Regression,[0],[0]
"These features count the deictic pronouns in the sentence, such as this, that and it.
",3.2 Argument Quality Regression,[0],[0]
"Sentence Length (SLEN): Short sentences, particularly those under 5 words, are usually hard to interpret without context and complex linguistic processing, such as resolving long distance discourse anaphora.",3.2 Argument Quality Regression,[0],[0]
"We thus include a single aggregate feature whose value is the number of words.
",3.2 Argument Quality Regression,[0],[0]
Word Length (WLEN): Sentences that clearly articulate an argument should generally contain words with a high information content.,3.2 Argument Quality Regression,[0],[0]
"Several studies show that word length is a surprisingly good indicator that outperforms more complex measures, such as rarity (Piantadosi et al., 2011).",3.2 Argument Quality Regression,[0],[0]
"Thus we include features based on word length, including the min, max, mean and median.",3.2 Argument Quality Regression,[0],[0]
"We also create a feature whose value is the count of words of lengths 1 to 20 (or longer).
",3.2 Argument Quality Regression,[0],[0]
"Speciteller (SPTL): We add a single aggregate feature from the result of Speciteller, a tool that assesses the specificity of a sentence in the range of 0 (least specific) to 1 (most specific) (Li and Nenkova, 2015; Louis and Nenkova, 2011).",3.2 Argument Quality Regression,[0],[0]
"High specificity should correlate with argument quality.
",3.2 Argument Quality Regression,[0],[0]
Kullback-Leibler Divergence (KLDiv): We expect that sentences on one topic domain will have different content than sentences outside the domain.,3.2 Argument Quality Regression,[0],[0]
"We built two trigram language models using the Berkeley LM toolkit (Pauls and Klein, 2011).",3.2 Argument Quality Regression,[0],[0]
"One (P) built from all the sentences in the IAC within the domain, excluding all sentences from the annotated dataset, and one (Q) built from all sentences in IAC outside the domain.",3.2 Argument Quality Regression,[0],[0]
"The KL Divergence is then computed using the discrete n-gram probabilities in the sentence from each model as in equation (1).
",3.2 Argument Quality Regression,[0],[0]
DKL(P ||Q),3.2 Argument Quality Regression,[0],[0]
"= ∑
i
P (i) ln P (i) Q(i)
(1)
",3.2 Argument Quality Regression,[0],[0]
"Lexical N-Grams (LNG): N-Grams are a standard feature that are often a difficult baseline to
beat.",3.2 Argument Quality Regression,[0],[0]
However they are not domain independent.,3.2 Argument Quality Regression,[0],[0]
We created a feature for every unigram and bigram in the sentence.,3.2 Argument Quality Regression,[0],[0]
The feature value was the inverse document frequency of that n-gram over all posts in the entire combined IAC plus CreateDebate corpus.,3.2 Argument Quality Regression,[0],[0]
Any n-gram seen less than 5 times was not included.,3.2 Argument Quality Regression,[0],[0]
"In addition to the specific lexical features a set of aggregate features were also generated that only considered summary statistics of the lexical feature values, for example the min, max and mean IDF values in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Discourse and Dialogue Features: We expect our features related to the discourse and dialogue hypotheses to be domain independent.
",3.2 Argument Quality Regression,[0],[0]
Discourse (DIS): We developed features based on discourse connectives found in the Penn Discourse Treebank as well as a set of additional connectives in our corpus that are related to dialogic discourse and not represented in the PDTB.,3.2 Argument Quality Regression,[0],[0]
We first determine if a discourse connective is present in the sentence.,3.2 Argument Quality Regression,[0],[0]
"If not, we create a NO CONNECTIVE feature with a value of 1.",3.2 Argument Quality Regression,[0],[0]
"Otherwise, we identify all connectives that are present.",3.2 Argument Quality Regression,[0],[0]
"For each of them, we derive a set of specific lexical features and a set of generic aggregate features.
",3.2 Argument Quality Regression,[0],[0]
The specific features make use of the lexical (String) and PDTB categories (Category) of the found connectives.,3.2 Argument Quality Regression,[0],[0]
We start by identifying the connective and whether it started the sentence or not (Location).,3.2 Argument Quality Regression,[0],[0]
"We then identify the connective’s most likely PDTB category based on the frequencies stated in the PDTB manual and all of its parent categories, for example but → CONTRAST → COMPARISON.",3.2 Argument Quality Regression,[0],[0]
The aggregate features only consider how many discourse connectives and if any of them started the sentence.,3.2 Argument Quality Regression,[0],[0]
"The templates are:
Specific:{Location}:{String} Specific:{Location}:{Category} Aggregate:{Location}:{Count}
",3.2 Argument Quality Regression,[0],[0]
"For example, the first sentence in Table 8 would generate the following features:
Specific:Starts:but Specific:Starts:Contrast
Specific:Starts:COMPARISON Aggregate:Starts:1 Aggregate:Any:1
Because our hypothesis about dialogue structure was disconfirmed by the results described in section 3.1, we did not develop a feature to independently test position in post.",3.2 Argument Quality Regression,[0],[0]
Rather the Discourse features only encode whether the discourse cue starts the post or not.,3.2 Argument Quality Regression,[0],[0]
"Syntactic Property Features: We also expect syntactic property features to generalize across domains.
",3.2 Argument Quality Regression,[0],[0]
Part-Of-Speech N-Grams (PNG): Lexical features require large amounts of training data and are likely to be topic-dependent.,3.2 Argument Quality Regression,[0],[0]
Part-of-speech tags are less sparse and and less likely to be topicspecific.,3.2 Argument Quality Regression,[0],[0]
"We created a feature for every unigram, bigram and trigram POS tag sequence in the sentence.",3.2 Argument Quality Regression,[0],[0]
"Each feature’s value was the relative frequency of the n-gram in the sentence.
",3.2 Argument Quality Regression,[0],[0]
Syntactic (SYN):,3.2 Argument Quality Regression,[0],[0]
"Certain syntactic structures may be used more frequently for expressing argumentative content, such as complex sentences with verbs that take clausal complements.",3.2 Argument Quality Regression,[0],[0]
"In CreateDebate, we found a number of phrases of the form",3.2 Argument Quality Regression,[0],[0]
"I <VERB> that <X>, such as I agree that, you said that, except that",3.2 Argument Quality Regression,[0],[0]
and I disagree because.,3.2 Argument Quality Regression,[0],[0]
"Thus we included two types of syntactic features: one for every internal node, excluding POS tags, of the parse tree (NODE) and another for each context free production rule (RULE) in the parse tree.",3.2 Argument Quality Regression,[0],[0]
"The feature value is the relative frequency of the node or rule within the sentence.
",3.2 Argument Quality Regression,[0],[0]
Meta Features: The 3 meta feature sets are: (1) all features except lexical n-grams (!LNG); (2) all features that use specific lexical or categorical information (SPFC); and (3) aggregate statistics (AGG) obtained from our feature extraction process.,3.2 Argument Quality Regression,[0],[0]
"The AGG set included features, such as sentence and word length, and summary statistics about the IDF values of lexical n-grams, but did not actually reference any lexical properties in the
feature name.",3.2 Argument Quality Regression,[0],[0]
We expect both !,3.2 Argument Quality Regression,[0],[0]
LNG and AGG to generalize across domains.,3.2 Argument Quality Regression,[0],[0]
"Sec. 4.1 presents the results of feature selection, which finds a large number of general features.",4 Results,[0],[0]
The results for argument quality prediction are in Secs.,4 Results,[0],[0]
4.2 and 4.3.,4 Results,[0],[0]
"Our standard training procedure (SEL) incorporates all the feature templates described in Sec. 3.2, which generates a total of 23,345 features.",4.1 Feature Selection,[0],[0]
It then performs a grid search over the model hyper-parameters and a subset of all the features using the simple feature selection technique described in section 3.2.,4.1 Feature Selection,[0],[0]
Table 3 shows the 10 features most correlated with the annotated quality value in the training data for the topics gun control and gay marriage.,4.1 Feature Selection,[0],[0]
"A few domain specific lexical items appear, but in general the top features tend to be non-lexical and relatively domain independent, such as part-of-speech tags and sentence specificity, as measured by Speciteller (Li and Nenkova, 2015; Louis and Nenkova, 2011).
",4.1 Feature Selection,[0],[0]
"Sentence length has the highest correlation with the target value in both topics, as does the node:root feature, inversely correlated with length.",4.1 Feature Selection,[0],[0]
"Therefore, in order to shift the quality distribution of the sample that we put out on MTurk for the death penalty or evolution topics, we applied a filter that removed all sentences shorter than 4 words.",4.1 Feature Selection,[0],[0]
"For these topics, domain specific features such as lexical n-grams are better predictors of argument quality.",4.1 Feature Selection,[0],[0]
"As discussed above, the PMI filter that was applied only to these two topics during sampling removed some shorter low quality sentences, which probably altered the predictive value of this feature in these domains.",4.1 Feature Selection,[0],[0]
"We first tested the performance of 3 regression algorithms using the training and testing data within each topic using 3 standard evaluation measures: R2, Root Mean Squared Error (RMSE) and Root
Relative Squared Error (RRSE).",4.2 In-Domain Training,[0],[0]
R2 estimates the amount of variability in the data that is explained by the model.,4.2 In-Domain Training,[0],[0]
Higher values indicate a better fit to the data.,4.2 In-Domain Training,[0],[0]
"The RMSE measures the average squared difference between predicted values and true values, which penalizes wrong answers more as the difference increases.",4.2 In-Domain Training,[0],[0]
"The RRSE is similar to RMSE, but is normalized by the squared error of a simple predictor that always guesses the mean target value in the test set.",4.2 In-Domain Training,[0],[0]
"Anything below a 1.0 indicates an improvement over the baseline.
",4.2 In-Domain Training,[0],[0]
"Table 4 shows that SVMs and OK perform the best, with better than baseline results for all topics.",4.2 In-Domain Training,[0],[0]
Performance for gun control and gay marriage are significantly better.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"Since SVM was nearly always the best model, we only report SVM results in what follows.
",4.2 In-Domain Training,[0],[0]
We also test the impact of our theoretically motivated features and domain specific features.,4.2 In-Domain Training,[0],[0]
The top half of Table 5 shows the RRSE for each feature set with darker cells indicating better performance.,4.2 In-Domain Training,[0],[0]
The feature acronyms are described in Sec 3.2.,4.2 In-Domain Training,[0],[0]
"When training and testing on the same domain, using lexical features leads to the best performance for all topics (SEL, LEX, LNG and SPFC).",4.2 In-Domain Training,[0],[0]
"However, we can obtain good performance on all of the topics without using any lexical information at all (!LNG, WLEN, PNG, and AGG), sometimes close to our best results.",4.2 In-Domain Training,[0],[0]
"Despite the high correlation to the target value, sentence specificity as a single feature does not outperform any other feature sets.",4.2 In-Domain Training,[0],[0]
"In general, we do better for gun control and gay marriage than for death penalty and evolution.",4.2 In-Domain Training,[0],[0]
"Since the length and domain specific words are important features in the trained models, it seems likely that the filtering process made it harder to learn a good function.
",4.2 In-Domain Training,[0],[0]
"The bottom half of Table 5 shows the results using training data from all other topics, when testing on one topic.",4.2 In-Domain Training,[0],[0]
"The best results for GC are significantly better for several feature sets (SEL,
LEX, LNG), In general the performance remains similar to the in-domain training, with some minor improvements over the best performing models.",4.2 In-Domain Training,[0],[0]
"These results suggest that having more data outweighs any negative consequences of domain specific properties.
",4.2 In-Domain Training,[0],[0]
"●
●
●
● ●
● ● ● ●
●
0.8
0.9
1.0
250 500 750 1000 1250
Number of Training Instances
R oo
t R el
at iv
e S
qu ar
ed E
rr or
Domain ● Gun Control
Gay Marriage Evolution Death Penalty
Figure 2:",4.2 In-Domain Training,[0],[0]
"Learning curves for each of the 4 topics with 95% confidence intervals.
",4.2 In-Domain Training,[0],[0]
We also examine the effect of training set size on performance given the best performing feature sets.,4.2 In-Domain Training,[0],[0]
See Fig. 2.,4.2 In-Domain Training,[0],[0]
"We randomly divided our entire dataset into an 80/20 training/testing split and trained incrementally larger models from the 80% using the default training procedure, which were then applied to the 20% testing data.",4.2 In-Domain Training,[0],[0]
"The plotted points are the mean value of repeating this process 10 times, with the shaded region showing the 95% confidence interval.",4.2 In-Domain Training,[0],[0]
"Although most gains are achieved within 500-750 training examples, all models are still trending downward, suggesting that more training data would be useful.
",4.2 In-Domain Training,[0],[0]
"Finally, our results are actually even better than they appear.",4.2 In-Domain Training,[0],[0]
"Our primary application requires extracting arguments at the high end of the scale (e.g., those above 0.8 or 0.9), but the bulk of our data is closer to the middle of the scale, so our regressors are conservative in assigning high or low
values.",4.2 In-Domain Training,[0],[0]
To demonstrate this point we split the predicted values for each topic into 5 quantiles.,4.2 In-Domain Training,[0],[0]
The RMSE for each of the quantiles and domains in Table 6 demonstrates that the lowest RMSE is obtained in the top quantile.,4.2 In-Domain Training,[0],[0]
To investigate whether learned models generalize across domains we also evaluate the performance of training with data from one domain and testing on another.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
The columns labeled CD in Table 7 summarize these results.,4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although cross domain training does not perform as well as in-domain training, we are able to achieve much better than baseline results between gun control and gay marriage for many of the feature sets and some other minor transferability for the other domains.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"Although lexical features (e.g., lexical n-grams) perform best in-domain, the best performing features across domains are all non-lexical, i.e. !",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"LNG, PNG and AGG.
",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"We then applied Daume’s “frustratingly easy domain adaptation” technique (DA), by transforming the original features into a new augmented feature space where, each feature, is transformed into a general feature and a domain specific feature, source or target, depending on the input domain (Daumé III, 2007).",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"The training data from both the source and target domains are used to train
the model, unlike the cross-domain experiments where only the source data is used.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"These results are given in the columns labeled DA in Table 7, which are on par with the best in-domain training results, with minor performance degradation on some gay marriage and gun control pairs, and slight improvements on the difficult death penalty and evolution topics.",4.3 Cross-Domain and Domain Adaptation,[0],[0]
"This paper addresses the Argument Extraction task in a framework whose long-term aim is to first extract arguments from online dialogues, and then use them to produce a summary of the different facets of an issue.",5 Discussion and Conclusions,[0],[0]
We have shown that we can find sentences that express clear arguments with RRSE values of .72 for gay marriage and gun control (Table 6) and .93 for death penalty and evolution (Table 8 cross domain with adaptation).,5 Discussion and Conclusions,[0],[0]
"These results show that sometimes the best quality predictors can be trained in a domain-independent way.
",5 Discussion and Conclusions,[0],[0]
"The two step method that we propose is different than much of the other work on argument mining, either for more formal texts or for social media, primarily because the bulk of previous work takes a supervised approach on a labelled topicspecific dataset (Conrad et al., 2012; Boltuzic and Šnajder, 2014; Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
Conrad & Wiebe developed a data set for supervised training of an argument mining system on weblogs and news about universal healthcare.,5 Discussion and Conclusions,[0],[0]
They separate the task into two components: one component identifies ARGUING SEGMENTS and the second component labels the segments with the relevant ARGUMENT TAGS.,5 Discussion and Conclusions,[0],[0]
Our argument extraction phase has the same goals as their first component.,5 Discussion and Conclusions,[0],[0]
"Boltuzic & Snajder also apply a supervised learning approach, producing arguments labelled with a concept similar to what we call FACETS.",5 Discussion and Conclusions,[0],[0]
"However they perform what we call argument extraction by hand, eliminating comments from com-
ment streams that they call “spam” (Boltuzic and Šnajder, 2014).",5 Discussion and Conclusions,[0],[0]
"Ghosh et al. also take a supervised approach, developing techniques for argument mining on online forums about technical topics and applying a theory of argument structure that is based on identifying TARGETS and CALLOUTS, where the callout attacks a target proposition in another speaker’s utterance (Ghosh et al., 2014b).",5 Discussion and Conclusions,[0],[0]
"However, their work does not attempt to discover high quality callouts and targets that can be understood out of context like we do.",5 Discussion and Conclusions,[0],[0]
"More recent work also attempts to do some aspects of argument mining in an unsupervised way (Boltuzic and Šnajder, 2015; Sobhani et al., 2015).",5 Discussion and Conclusions,[0],[0]
"However (Boltuzic and Šnajder, 2015) focus on the argument facet similarity task, using as input a corpus where the arguments have already been extracted.",5 Discussion and Conclusions,[0],[0]
"(Sobhani et al., 2015) present an architecture where arguments are first topic-labelled in a semi-supervised way, and then used for stance classification, however this approach treats the whole comment as the extracted argument, rather than attempting to pull out specific focused argument segments as we do here.
",5 Discussion and Conclusions,[0],[0]
A potential criticism of our approach is that we have no way to measure the recall of our argument extraction system.,5 Discussion and Conclusions,[0],[0]
However we do not think that this is a serious issue.,5 Discussion and Conclusions,[0],[0]
"Because we are only interested in determining the similarity between phrases that are high quality arguments and thus potential contributors to summaries of a specific facet for a specific topic, we believe that precision is more important than recall at this point in time.",5 Discussion and Conclusions,[0],[0]
"Also, given the redundancy of the arguments presented over thousands of posts on an issue it seems unlikely we would miss an important facet.",5 Discussion and Conclusions,[0],[0]
"Finally, a measure of recall applied to the facets of a topic may be irreconcilable with our notion that an argument does not have a limited, enumerable number of facets, and our belief that each facet is subject to judgements of granularity.",5 Discussion and Conclusions,[0],[0]
Fig. 3 shows how the Mechanical Turk hit was defined and the examples that were used in the qualification task.,6 Appendix,[0],[0]
"Table 8 illustrates the argument quality scale annotations collected from Mechanical Turk.
",6 Appendix,[0],[0]
We invite other researchers to improve upon our results.,6 Appendix,[0],[0]
Our corpus and the relevant annotated data is available at http://nldslab.soe.ucsc.edu/ arg-extraction/sigdial2015/.,6 Appendix,[0],[0]
This research is supported by National Science Foundation Grant CISE-IIS-RI #1302668.,7 Acknowledgements,[0],[0]
Online forums are now one of the primary venues for public dialogue on current social and political issues.,abstractText,[0],[0]
"The related corpora are often huge, covering any topic imaginable.",abstractText,[0],[0]
Our aim is to use these dialogue corpora to automatically discover the semantic aspects of arguments that conversants are making across multiple dialogues on a topic.,abstractText,[0],[0]
We frame this goal as consisting of two tasks: argument extraction and argument facet similarity.,abstractText,[0],[0]
"We focus here on the argument extraction task, and show that we can train regressors to predict the quality of extracted arguments with RRSE values as low as .73 for some topics.",abstractText,[0],[0]
"A secondary goal is to develop regressors that are topic independent: we report results of cross-domain training and domain-adaptation with RRSE values for several topics as low as .72, when trained on topic independent features.",abstractText,[0],[0]
Argument Mining: Extracting Arguments from Online Dialogue,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1558–1572 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role. By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",text,[0],[0]
"“We’d now like to open the floor to shorter speeches disguised as questions...”
– Steve Macone, New Yorker cartoon caption
Why do we ask questions?",1 Introduction,[0],[0]
"Perhaps we are seeking factual information that others hold, or maybe we are requesting a favor.",1 Introduction,[0],[0]
"Alternatively we could be simply making a rhetorical point, perhaps at the start of an academic paper.
",1 Introduction,[0],[0]
"Questions play a prominent role in social interactions (Goffman, 1976), performing a multitude of rhetorical functions that go beyond mere factual
information gathering (Kearsley, 1976).",1 Introduction,[0],[0]
"While the informational component of questions has been well-studied in the context of question-answering applications, there is relatively little computational work addressing the rhetorical and social role of these basic dialogic units.
",1 Introduction,[0],[0]
One domain where questions have a particularly salient rhetorical role is politics.,1 Introduction,[0],[0]
"The ability to question the actions and intentions of governments is a crucial part of democracy (Pitkin, 1967), particularly in parliamentary systems.",1 Introduction,[0],[0]
"Consequently, scholars have studied parliamentary questions in detail, in terms of their origins (Chester and Bowring, 1962), their institutionalization (Eggers and Spirling, 2014) and their importance for oversight (Proksch and Slapin, 2011).",1 Introduction,[0],[0]
"In particular, the United Kingdom’s House of Commons, renowned for theatrical questions periods, has been studied in some depth.",1 Introduction,[0],[0]
"However, those accounts are largely qualitative in nature (Bull and Wells, 2012; Bates et al., 2014).",1 Introduction,[0],[0]
The present work: methodology.,1 Introduction,[0],[0]
"In order to approach these problems computationally, we introduce an unsupervised framework to structure the space of questions according to their rhetorical role.",1 Introduction,[0],[0]
"First, we identify common ways in which questions are phrased.",1 Introduction,[0],[0]
"To this end, we automatically extract these recurring surface forms, or motifs, based on the lexico-syntactic structure of the questions posed (Section 4).",1 Introduction,[0],[0]
"To capture rhetorical aspects we then group these motifs according to their role, relying on the intuition that this role is encoded in the type of answer a question receives.",1 Introduction,[0],[0]
To operationalize this intuition we construct a latent question-answer space in which question motifs triggering similar answers are mapped to the same region (Section 5).,1 Introduction,[0],[0]
The present work: application.,1 Introduction,[0],[0]
"We apply this general framework to the political discourse that occurs during parliamentary question sessions in
1558
the British House of Commons, a new dataset which we make publicly available (Section 3).",1 Introduction,[0],[0]
"Our framework extracts intuitive question types ranging from narrow factual queries to pointed criticisms disguised as questions (Section 5, Table 1).",1 Introduction,[0],[0]
We validate our framework by aligning these types with prior understandings of parliamentary proceedings from the political science literature (Section 6).,1 Introduction,[0],[0]
"In particular, previous work (Bates et al., 2014) has categorized questions asked in Parliament according to the intentions of the asker (e.g., to help the answerer, or to adversarially put them on the spot); we find a clear, predictive mapping between these expert-coded categories and the induced typology.",1 Introduction,[0],[0]
"We further show that the types of questions specific legislators tend to ask vary with whether they are part of the governing or opposition party, consistent with wellestablished accounts of partisan differences (Cowley, 2002; Spirling and McLean, 2007; Eggers and Spirling, 2014).",1 Introduction,[0],[0]
"Concretely, government legislators exhibit a preference for overtly friendly questions, while the opposition slants towards more aggressive question types.
",1 Introduction,[0],[0]
We then apply our methodology to provide new insights into how a legislator’s questioning behavior varies with their career trajectory.,1 Introduction,[0],[0]
"The pressures faced by legislators at various stages in their career are cross-cutting, and multiple possible hypotheses emerge.",1 Introduction,[0],[0]
"Younger, more enthusiastic legislators may be motivated to ask harderhitting questions, but risk being passed over for future promotion if they are too combative (Cowley, 2002).",1 Introduction,[0],[0]
"Older legislators, whose opportunities for promotion are largely behind them and hence have “less to lose”, may act more aggressively (Benedetto and Hix, 2007); or simply seek a quiet path to retirement.",1 Introduction,[0],[0]
"Viewing each group’s behavior through the questions they ask brings evidence for the latter hypothesis that more tenured legislators are more aggressive, even when questioning their own leaders.",1 Introduction,[0],[0]
"In this way, their presence in the House of Commons, and their refusal to simply ‘keep their heads down’, facilitates a core component of democracy.",1 Introduction,[0],[0]
Question-answering.,2 Related Work,[0],[0]
"Computationally, questions have received considerable attention in the context of question-answering (QA) systems—for a survey see Gupta and Gupta (2012)—with an em-
phasis on understanding their information need (Harabagiu, 2008).",2 Related Work,[0],[0]
"Techniques have been developed to categorize questions based on the nature of these information needs in the context of the TREC QA challenge (Harabagiu et al., 2000), and to identify questions asking for similar information (Shtok et al., 2012; Zhang et al., 2017; Jeon et al., 2005); questions have also been classified by topic (Cao et al., 2010) and quality (Treude et al., 2011; Ravi et al., 2014).",2 Related Work,[0],[0]
"In contrast, our work is not concerned with the information need central to QA applications, and instead focuses on the rhetorical aspect of questions.
",2 Related Work,[0],[0]
Question types.,2 Related Work,[0],[0]
"To facilitate retrieval of frequently asked questions, Lytinen and Tomuro (2002) manually developed a typology of surface question forms (e.g., ‘what’- and ‘why’-questions) starting from Lehnerts’ conceptual question categories (Lehnert, 1978).",2 Related Work,[0],[0]
"Question types were also hand annotated for dialog-act labeling, distinguishing between yes-no, wh-, open-ended and rhetorical questions (Dhillon et al., 2004).",2 Related Work,[0],[0]
"To complement this line of work, this paper introduces a completely unsupervised methodology to automatically build a domain-tailored question typology, bypassing the need for human annotation.
",2 Related Work,[0],[0]
Pragmatic dimensions.,2 Related Work,[0],[0]
"One important pragmatic dimension of questions that has been previously studied computationally is their level of politeness (Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016); in the specific context of making requests, politeness was shown to correlate with the social status of the asker.",2 Related Work,[0],[0]
Sachdeva and Kumaraguru (2017) studied another rhetorical aspect by examining linguistic attributes distinguishing serviceable requests addressed to police on social media from general conversation.,2 Related Work,[0],[0]
"Previous research has also been directed at identifying rhetorical questions (Bhattasali et al., 2015) and understanding the motivations of their “askers” (Ranganath et al., 2016).",2 Related Work,[0],[0]
"Using the relationship between questions and answers, our work examines the rhetorical and social aspect of questions without predefining a pragmatic dimension and without relying on labeled data.",2 Related Work,[0],[0]
"We also complement these efforts in analyzing a broader range of situations in which questions may be posed without an information-seeking intent.
",2 Related Work,[0],[0]
Political discourse.,2 Related Work,[0],[0]
"Finally, our work contributes to a rapidly growing area of NLP applications to political domains (Monroe et al., 2008; Card et al.,
2016; Gonzalez-Bailon et al., 2010; Niculae et al., 2015; Grimmer and Stewart, 2013; Grimmer et al., 2012; Iyyer et al., 2014b, inter alia).",2 Related Work,[0],[0]
"Particularly relevant are applications to discourse in congressional and parliamentary settings (Thomas et al., 2006; Boydstun et al., 2014; Rheault et al., 2016).",2 Related Work,[0],[0]
"The bulk of our analysis focuses on the questions asked, and responses given during parliamentary question periods in the British House of Commons.",3 Data: Parliamentary Question Periods,[0],[0]
"Below, we provide a brief overview of key features of this political system in general, as well as a description of the question period setting.",3 Data: Parliamentary Question Periods,[0],[0]
Parliamentary systems.,3 Data: Parliamentary Question Periods,[0],[0]
"Legislators in the House of Commons (Members of Parliament, henceforth MPs or members) belong to two main voting and debating affiliations: a government party which controls the executive and holds a majority of the seats in the chamber, and a set of opposition parties.1",3 Data: Parliamentary Question Periods,[0],[0]
"The executive is headed by the Prime Minister (PM) and run by a cabinet of ministers, highranking government MPs responsible for various departments such as finance and education.",3 Data: Parliamentary Question Periods,[0],[0]
Question periods.,3 Data: Parliamentary Question Periods,[0],[0]
"The House of Commons holds weekly, moderated question periods, in which MPs of all affiliations take turns to ask questions to (and theoretically receive answers from) government ministers for each department regarding their specific domains.",3 Data: Parliamentary Question Periods,[0],[0]
Such events are a primary way in which legislators hold senior policy-makers responsible for their decisions.,3 Data: Parliamentary Question Periods,[0],[0]
"In practice, beyond narrow requests for information about specific policy points, MPs use their questions to critique or praise the government, or to self-promote; indeed, certain sessions, such as Questions to the Prime Minister, have gained renown for their partisan clashes, often fueled by the (mis)handling of a current crisis.",3 Data: Parliamentary Question Periods,[0],[0]
"The following question, asked to the Prime Minister by an opposition MP about contamination of the meat supply in 2013, encapsulates this odd mix of purposes:
“The Prime Minister is rightly shocked by the revelations that many food products contain 100% horse.",3 Data: Parliamentary Question Periods,[0],[0]
"Does he share my concern that, if tested, many of his answers may contain 100% bull?”2
1We use affiliation to refer broadly to the government and opposition roles, independent of the identity of the current government and opposition parties.",3 Data: Parliamentary Question Periods,[0],[0]
"In subsequent analysis we only consider the largest, “official” opposition party as the opposition.
",3 Data: Parliamentary Question Periods,[0],[0]
"2MPs almost always address each other in 3rd person.
",3 Data: Parliamentary Question Periods,[0],[0]
"The moderated, relatively rigid format of questions periods, along with the multifaceted array of underlying incentives and interpersonal relationships, yields a structurally controlled setting with a rich variety of social interactions, taking place in the realm of important policy discussions.",3 Data: Parliamentary Question Periods,[0],[0]
"This complexity makes question periods a particularly fruitful and consequential setting in which to study questions as social signals, and expand our understanding of their role beyond factual queries.",3 Data: Parliamentary Question Periods,[0],[0]
Dataset description.,3 Data: Parliamentary Question Periods,[0],[0]
"Our dataset covers question periods from May 1979 to December 2016, encompassing six different Prime Ministers.",3 Data: Parliamentary Question Periods,[0],[0]
"For each question period, we extract all questionanswer pairs, along with the identity of the asker and answerer.",3 Data: Parliamentary Question Periods,[0],[0]
"Because our focus here is on how questions are posed in a social setting, and not on the subsequent dialogue, we ignore questions which were tabled prior to the session, as well as any followup back-and-forth dialogue between the asker and answerer.
",3 Data: Parliamentary Question Periods,[0],[0]
"We augment this collection with metadata about each asker and answerer, including their political party, the time when they first took office, and whether they were serving as a minister at a given point in time.",3 Data: Parliamentary Question Periods,[0],[0]
"Such information is used to validate our methodology and interpret our results in light of the social context in which the questions were asked, described further in Sections 6 and 7.
",3 Data: Parliamentary Question Periods,[0],[0]
"In total there are 216,894 question-answer pairs in our data, occurring over 4,776 days and 6 prime-ministerships.",3 Data: Parliamentary Question Periods,[0],[0]
"The questions cover 1,975 different askers, 1,066 different answerers, and a variety of government departments with responsibilities ranging from defense to transport.",3 Data: Parliamentary Question Periods,[0],[0]
"We make this dataset publicly available, along with the code implementing our methodology, as part of the Cornell Conversational Analysis Toolkit.3",3 Data: Parliamentary Question Periods,[0],[0]
"The first component of our framework identifies lexico-syntactic phrasing patterns recurring in a collection of questions, which we call motifs.",4 Question Motifs,[0],[0]
"Intuitively, motifs constitute wordings commonly used to pose questions.",4 Question Motifs,[0],[0]
"To find motifs in a given collection, we first extract relevant fragments from each question.",4 Question Motifs,[0],[0]
"We then group sets of frequently co-occurring fragments into motifs.
",4 Question Motifs,[0],[0]
"3 https://github.com/CornellNLP/
Cornell-Conversational-Analysis-Toolkit
Question fragments.",4 Question Motifs,[0],[0]
Our goal is to find motifs which reflect functional characteristics of questions.,4 Question Motifs,[0],[0]
"Hence, we start by extracting the key fragments within a question which encapsulate its functional nature.",4 Question Motifs,[0],[0]
"Following the intuition that the bulk of this functional information is contained in the root of a question’s dependency parse along with its outgoing arcs (Iyyer et al., 2014a), we take the fragments of a question to be the root of its parse tree, along with each (root, child) pair.",4 Question Motifs,[0],[0]
"To capture cases when the operational word in the question is not connected to its root (such as “What...”), we also consider the initial unigram and bigram of a question as fragments.",4 Question Motifs,[0],[0]
"The following question has 5 fragments: what, what is, going→*, is←going and going→do.
",4 Question Motifs,[0],[0]
"(1) What is the minister going to do about ... ?
",4 Question Motifs,[0],[0]
"Because our goal is to capture topic-agnostic patterns, we ignore all fragments which contain a noun phrase (NP) or pronoun.",4 Question Motifs,[0],[0]
"NP subtrees are identified based on their outgoing dependencies to the root;4 in the event that an NP starts with a WHdeterminer (WDT), we consider (root, WDT) to be a fragment and drop the remainder of the NP.5
Finally, we note that some questions consist of multiple sub-questions (“What does the Minister think [...], and why [...]?”).",4 Question Motifs,[0],[0]
"For such questions, we recursively extract fragments from each child subtree in the same manner, starting from their roots.",4 Question Motifs,[0],[0]
From fragments to motifs.,4 Question Motifs,[0],[0]
We define motifs as sets of question fragments that frequently co-occur (in at least n questions).,4 Question Motifs,[0],[0]
"We find motifs by applying the apriori algorithm (Agrawal and Srikant, 1994) to find these common itemsets.",4 Question Motifs,[0],[0]
"This results in a collection of motifs M which correspond to different question phrasings.6 Examples of motifs are shown in Table 1.
",4 Question Motifs,[0],[0]
Motifs can identify phrasings to varying degrees of specificity.,4 Question Motifs,[0],[0]
"For example, the singleton motif {what is} corresponds to all questions starting with that bigram, while {what is, going→do} nar-
4We take as NPs subtrees connected to the root with the following: nsubj, nsubjpass, dobj, iobj, pobj, attr.
",4 Question Motifs,[0],[0]
"5In the particular case of the Parliament dataset, removing NPs also removes conventional, partisan address terms (e.g. “my hon. Friend”).
",4 Question Motifs,[0],[0]
"6In some cases, a pair of motifs almost always co-occurs in the same questions, making them redundant.",4 Question Motifs,[0],[0]
"We treat two motifs m1 and m2 as equivalent if, for some probability p, Pr(m1|m2) > p and Pr(m2|m1)",4 Question Motifs,[0],[0]
"> p; we keep the smaller of the two as the representative motif, or pick one of them arbitrarily if they are of equal sizes.
rows these down to questions also containing the fragment going→do.",4 Question Motifs,[0],[0]
"To model the specificity relation between motifs, we structure M as a directed acyclic graph where an edge points from a motif m1 to another motif m2 if the latter has exactly one more fragment in addition to those in m1, corresponding to a narrower set of phrasings.",4 Question Motifs,[0],[0]
Motif-representation of a question.,4 Question Motifs,[0],[0]
"Finally, a question q contains a motif if it includes all of the fragments comprising that motif.",4 Question Motifs,[0],[0]
"We can hence capture the phrasing of a given question q using the subset of motifs it contains, structured as the subgraphMq ⊂ M induced by this subset.",4 Question Motifs,[0],[0]
"This directed subgraph represents the question at multiple levels of specificity simultaneously; in particular, the set of sinks (i.e., nodes with outdegree 0; henceforth sink motifs) ofMq is the most finegrained way to specify the phrasing of q. For example {what is, is←going, going→do} is the only sink motif of the question in example (1); its entire subgraph is shown in Figure 3 in the appendix.",4 Question Motifs,[0],[0]
"The second component of our framework structures the space of questions according to their functional roles, thus going beyond the lexicosyntactic representation captured via motifs.",5 Latent Question Types,[0],[0]
The main intuition is that the nature of the answer that a question receives provides a good indication of its intention.,5 Latent Question Types,[0],[0]
"Therefore, if two questions are phrased differently but answered in similar ways, the parallels exhibited by their answers should reflect commonalities in the askers’ intentions.
",5 Latent Question Types,[0],[0]
"To operationalize this intuition, we first construct a latent space based on answers, and then map question motifs (Section 4) to the same space.",5 Latent Question Types,[0],[0]
"Using the resultant latent representations, we can then cluster questions with similar rhetorical functions, even if their surface forms are different.",5 Latent Question Types,[0],[0]
Constructing a space of answers.,5 Latent Question Types,[0],[0]
"In line with our focus on functional characterizations, we extract the fragments from each sentence of an answer, defined in the same way as question fragments.",5 Latent Question Types,[0],[0]
"We then construct a term-document matrix, where terms correspond to answer fragments, and documents correspond to individual answers in the corpus.",5 Latent Question Types,[0],[0]
"We filter out infrequent fragments occurring less than nA times, reweight the rows of this matrix with tf-idf reweighting, and scale to unit norm, producing a fragment-answer matrix A. We perform singular value decomposi-
tion on A and obtain a low-rank representation A ≈ Â = UASV TA , for some rank d, where rows of UA correspond to answer fragments and rows of VA correspond to answers.7
Latent projection of question motifs.",5 Latent Question Types,[0],[0]
We can draw a natural correspondence between a question motif m and answer term t if m occurs in a question whose answer contains t.,5 Latent Question Types,[0],[0]
"This enables us to compute representations of question motifs in the same space as Â. Concretely, we construct a motif-question matrix Q = (qij) where qij = 1 if motif i occurred in question j; we scale rows of Q to unit norm.",5 Latent Question Types,[0],[0]
"To represent Q in the latent answer space, we solve for Q̂ in Q = Q̂SV TA as Q̂ = QVAS−1, again scaling rows to unit norm.",5 Latent Question Types,[0],[0]
"Row i of Q̂ then gives a d-dimensional representation of motif i, denoted q̂i.
",5 Latent Question Types,[0],[0]
Grouping similar questions.,5 Latent Question Types,[0],[0]
"Finally, we identify question types—broad groups of similar motifs.",5 Latent Question Types,[0],[0]
"Intuitively, if two motifs mi and mj have vectors qi and qj which are close together, they elicit answers that are close in the latent space, so are functionally similar in this sense.",5 Latent Question Types,[0],[0]
"We use the KMeans algorithm (Pedregosa et al., 2011) to cluster motif vectors into k clusters; these clusters then constitute the desired set of question types.
",5 Latent Question Types,[0],[0]
"To determine the type of a particular question q∗, we transform it to a binary vector (q∗i ) where q∗i = 1 if motif i is a sink motif of q
∗; using only sink motifs at this stage allows us to characterize a question according to the most specific representation of its phrasing, thus avoiding spurious associations resulting from more general motifs.",5 Latent Question Types,[0],[0]
"We scale q∗, project it to the latent space as before, and assign the resultant projection q̂∗ to a cluster t, hence determining its type.
",5 Latent Question Types,[0],[0]
"Since question motifs and answer fragments have both been mapped to the same latent space (as rows of Q̂ and UA respectively), we can also assign each answer fragment to a question type.",5 Latent Question Types,[0],[0]
"This further facilitates interpretability through characterizing the answers commonly triggered by a particular type of question.
",5 Latent Question Types,[0],[0]
"7We experimented with grouping answer fragments into motifs as well, but found that most of the motifs produced were one fragment large.",5 Latent Question Types,[0],[0]
"While future work could focus more on understanding consistent phrasings of answers, we note that at least in our chosen corpus, answers are longer and encompass a much greater variation of possible phrasings.",5 Latent Question Types,[0],[0]
"We now apply our general framework to the particular setting of parliamentary question periods, structuring the space of questions posed within these sessions according to their rhetorical function.",6 Validation,[0],[0]
"To validate the induced typology, we quantitatively show that it recovers asker intentions in an expert-coded dataset, and qualitatively aligns with prior findings in the political science literature on parliamentary dynamics.",6 Validation,[0],[0]
Question types in Parliament.,6 Validation,[0],[0]
"We apply our motif extraction and question type induction pipeline to the questions in the parliamentary dataset.8 Over 90% of the questions in the dataset contain at least one of the resulting 2,817 motifs; in subsequent analyses we discard questions without a matching motif.",6 Validation,[0],[0]
"We apply our pipeline to the questions in the parliamentary dataset, and induce a typology of k = 8 question types to capture the rich array of questions represented in this space while preserving interpretability.
",6 Validation,[0],[0]
"Table 1 displays extracted types, along with example questions, answers, and motifs.9",6 Validation,[0],[0]
"The second author, a political scientist with domain expertise in the UK parliamentary setting, manually investigated each type and provided interpretable labels.",6 Validation,[0],[0]
"For example, in questions of type 4, the asker is aware that his main premise is supported by the minister, and thus will be met with a positive statement backing the thrust of the question; we call this the agreement cluster.",6 Validation,[0],[0]
"Types 6 and 7 are much more combative: in type 6 questions the asker explicitly attempts to force the minister to concede/accept a point that would undermine some government stance, while type 7 contains condemnatory questions that prompt the minister to justify a policy that is self-evidently bad in the eyes of the asker.",6 Validation,[0],[0]
"In contrast, type 2 constitutes tamer narrow queries that require the minister to simply report on non-partisan matters of policy.",6 Validation,[0],[0]
(Extended interpretations in the appendix.),6 Validation,[0],[0]
Quantitative validation.,6 Validation,[0],[0]
"We compare our output to a dataset of 1,256 questions asked to various Prime Ministers labeled by Bates et al. (2014)
8We consider questions to be sentences ending in question marks.",6 Validation,[0],[0]
"If an utterance consists of multiple questions, we extract fragments sets from each question separately, and take the motifs of the utterance to be the union of motifs of each component question.",6 Validation,[0],[0]
"We set n = 100, p = 0.9, nA = 100 and d = 25.",6 Validation,[0],[0]
"The choice of parameters was done via manual inspection of the dataset.
",6 Validation,[0],[0]
"9Each type contains a few hundred question motifs and answer fragments.
(also included in our data distribution).",6 Validation,[0],[0]
"Each question in this data is hand-coded by a domain expert with one of three labels indicating the rhetorical intention of the asker: compared to standard questions—denoting straightforward factual queries, helpful questions serve as prompts for the PM to talk favorably about their government, while unanswerable questions are effectively vehicles for delivering criticisms that the PM cannot respond to.",6 Validation,[0],[0]
Questions which are unanswered by the PM are also labeled.,6 Validation,[0],[0]
"If our framework captures meaningful rhetorical dimensions, we expect a given label to be over-represented in some of our induced types, and under-represented in others.
",6 Validation,[0],[0]
"Even though our clustering of questions is generated in an unsupervised fashion without any guidance from the coded rhetorical roles, we see that several of the types we discover closely align with these annotations.",6 Validation,[0],[0]
"In particular, helpful questions are highly associated with the agreement type (constituting 28% of questions of that type compared to 14% over the entire dataset; binomial test p < 0.01), reinforcing our interpretation
that this type captures MPs cheerleading their own government.",6 Validation,[0],[0]
"Conversely, unanswerable questions are frequently of the concede/accept type (20% in-type vs. 11% overall), while condemnatory questions are often unanswered (43% vs. 24% overall), suggesting that questions of these types have an increased tendency to be posed as aggressive criticisms packaged as questions.
",6 Validation,[0],[0]
"We also validate our framework in a prediction setting using these labels, in three binary classification tasks: distinguishing helpful vs. standard, unanswerable vs. standard, and unanswered vs. answered questions.",6 Validation,[0],[0]
"(In each task, we balance the two classes.)",6 Validation,[0],[0]
"To control for asker affiliation effects, we consider only questions asked by government MPs for the helpful task, and opposition questions in the unanswerable and unanswered tasks; we train on questions to Conservative PMs and evaluate on Labour PMs.10 For each setting, we train logistic regression classifiers; as
10These choices are motivated by the number of questions from each affiliation and party in the dataset (see appendix for further details on this dataset).
",6 Validation,[0],[0]
"features we compare the latent representation of each question to a unigram BOW baseline.11
In the unanswerable and unanswered tasks, we find that the BOW features do not perform significantly better than a random (50%) baseline.",6 Validation,[0],[0]
"However, the latent question features produced by our framework bring additional predictive signal and outperform the baseline when combined with BOW (binomial p < 0.05), achieving accuracies of 66% and 62% respectively (compared with 55% and 50% for BOW alone).",6 Validation,[0],[0]
"This suggests that our representation captures useful rhetorical information that, given our train-test split, generalizes across parties.",6 Validation,[0],[0]
"None of the models significantly outperform the random baseline on the helpful task, perhaps owing to the small data size.",6 Validation,[0],[0]
Qualitative validation: question partisanship.,6 Validation,[0],[0]
We additionally provide a qualitative validation of our framework by comparing the questionasking activity of government and oppositionaffiliated MPs—as viewed through the extracted question types—to well-established characterizations of these affiliations in the political science literature.,6 Validation,[0],[0]
"In particular, prior work has examined the bifurcation in behavior between government and opposition members, in their differing focus on various issues (Louwerse, 2012), and in settings such as roll call votes (Cowley, 2002;",6 Validation,[0],[0]
"Spirling and McLean, 2007; Eggers and Spirling, 2014).",6 Validation,[0],[0]
"Since government MPs are elected on the same party ticket and manifesto, they primarily act to sup-
11We used tf-idf reweighting and excluded unigrams occurring less than 5 times.
",6 Validation,[0],[0]
"port the government’s various policies and bolster the status of their cabinet, seldom airing disagreements publicly.",6 Validation,[0],[0]
"In contrast, opposition members tend to offer trenchant partisan criticism of government policies, seeking to destabilize the government’s relationship with its MPs and create negative press in the country at large.",6 Validation,[0],[0]
"In characterizing the question-asking activity of government and opposition MPs, this friendly vs. adversarial behavior should also be reflected in a rhetorical typology of questions.12
Concretely, to quantify the relationship between a particular question type t and asker affiliation P , we compute the log-odds ratio of type t questions asked by MPs in P , compared to MPs not in P .13
Figure 1A shows the resultant log-odds ratios of each question type for government and opposition members.",6 Validation,[0],[0]
"Notably, we see that agreement-type questions are significantly more likely to originate from government than from opposition MPs, while the opposite holds for concede/accept and condemnatory questions (binomial p < 10−4 for each, comparing within-type to overall proportions of questions from an affiliation).",6 Validation,[0],[0]
"No such
12While we induce the typology over our entire dataset, we perform all subsequent analyses on a filtered subset of 50,152 questions.",6 Validation,[0],[0]
"In particular, we omit utterances with multiple questions—i.e. multiple question marks—to ensure that we don’t confound effects arising from different co-occurring question types.",6 Validation,[0],[0]
Our filtering decisions are also determined by the availability of information about the asker and answerers’ roles in Parliament.,6 Validation,[0],[0]
"Further information about these filtering choices can be found in the appendix.
",6 Validation,[0],[0]
"13The log-odds values are not symmetric between government and opposition, because they includes questions asked by MPs not in the official opposition.
",6 Validation,[0],[0]
"slant is exhibited in the narrow factual type, further reinforcing the role of such questions as informational queries about relatively non-partisan issues.",6 Validation,[0],[0]
"These results strongly cohere with the “textbook” accounts of parliamentary activity in the literature, as well as our interpretation of these types as bolstering or antagonistic.
",6 Validation,[0],[0]
"Moreover, we find that the same MP shifts in her propensity for different question types as her affiliation changes.",6 Validation,[0],[0]
"When a new political party is elected into office, MPs who were previously in the opposition now belong to the government party, and vice versa.",6 Validation,[0],[0]
"Such a switch occurs within our data between the Major and Blair governments (Conservative to Labour, 1997), and between the Brown and Cameron governments (Labour to Conservative, 2010).",6 Validation,[0],[0]
"For both switches, we consider all MPs who asked at least 5 questions both before and after the switch, resulting in 88 members who became government MPs and 102 who became opposition MPs.",6 Validation,[0],[0]
"For an MP M we compute PM,t, their propensity for a question type t, as the proportion of questions they ask which are from t. Comparing PM,t before and after a switch, we replicate the key differences observed above—for instance, we find that former opposition MPs who become government MPs decrease in their propensity for condemnatory questions, while newly opposition MPs move in the other direction (Wilcoxon p < 0.001, Figure 1B).",6 Validation,[0],[0]
"This suggests that the general trends we observed before are driven by the shift in affiliation, and hence parliamentary role, of individual MPs.",6 Validation,[0],[0]
"We now apply our framework to gain further insights into the nature of political discourse in Parliament, focusing on how questioning behavior varies with a member’s tenure in the institution.",7 Career Trajectory Effects,[0],[0]
"As stated in the introduction, two alternative hypotheses arise: younger MPs may be more vigorously critical out of enthusiasm, but are potentially tempered by their stake in future promotion prospects compared to older members (Cowley, 2002, 2012).",7 Career Trajectory Effects,[0],[0]
"Alternatively, older MPs who have less at stake in terms of prospects of further promotion may ask more antagonistic questions.",7 Career Trajectory Effects,[0],[0]
"Throughout, young and old refer to tenure—i.e., how many years someone has served as an MP— rather than biological age.
",7 Career Trajectory Effects,[0],[0]
"In order to understand the extent to which young or old members contribute a specific type of question, for each question type t we compute the median tenure of askers of each question in t, and compare the median tenures of different question types, for each affiliation (Figure 2A).14 We see that among both affiliations, more aggressive questions tend to originate more from older members, reflected in significantly higher median tenures (for types 6 in both affiliations, and 7 in government MPs; Mann Whitney U test p < 0.001 comparing within-type median tenure with outside-type median tenure); whereas standard issue update questions tend to come from younger
14Median tenures for opposition members are generally higher; winning an election tends to result in more newlyelected and therefore younger MPs (Webb and Farrell, 1999).
",7 Career Trajectory Effects,[0],[0]
"members (p < 0.001, both affiliations).",7 Career Trajectory Effects,[0],[0]
"Notably, the disproportionate aggressiveness of older members manifests even among government MPs who direct these questions towards their own government.",7 Career Trajectory Effects,[0],[0]
"This supports the “less to lose” intuition, offering a rhetorical parallel to previous findings about the increased tendency to vote contrary to party lines from MPs with little chance of ministerial promotion (Benedetto and Hix, 2007).
",7 Career Trajectory Effects,[0],[0]
"Interestingly, we find that these differential preferences across member tenure also manifest at a finer granularity than simply less to more aggressive.",7 Career Trajectory Effects,[0],[0]
"For instance, younger opposition members tend to contribute more condemnatory questions compared to older members (Mann Whitney U test p < 0.01), who disproportionately favor concede/accept questions.",7 Career Trajectory Effects,[0],[0]
"While further work is needed to fully explain these differences, we speculate that they are potentially reflective of strategic attempts by younger MPs to signal traits that could facilitate future promotion, such as partisan loyalty (Kam, 2009).
",7 Career Trajectory Effects,[0],[0]
"To discount the possibility of these effects being solely driven by a few very prolific young or old MPs, we also consider a setting where type propensities are macroaveraged over MPs.",7 Career Trajectory Effects,[0],[0]
"For each affiliation we compare the cohort of younger MPs who are newly voted in at the 1997 and 2010 elections, with older MPs who have been in office prior to the election.15 We compute the type propensities of these two cohorts over the questions they asked during the subsequent parliamentary sitting, and replicate the tenure effects observed previously (Figure 2B).",7 Career Trajectory Effects,[0],[0]
"This suggests that these parliamentary career effects reflect behavioral changes at the level of individual MPs, whose incentives evolve over their tenure.",7 Career Trajectory Effects,[0],[0]
In this work we introduced an unsupervised framework for structuring the space of questions according to their rhetorical role.,8 Conclusion and Future Work,[0],[0]
"We instantiated and validated our approach in the domain of parliamentary question periods, and revealed new interactions between questioning behavior and career trajectories.
",8 Conclusion and Future Work,[0],[0]
We note that our methodology is not tied to a particular domain.,8 Conclusion and Future Work,[0],[0]
"It would be interesting to explore its potential in a variety of less structured do-
15This totals 272 new and 184 old government MPs, and 84 new and 179 old opposition MPs.
mains where questions likewise play a crucial role.",8 Conclusion and Future Work,[0],[0]
"For example, examining how interviewers in highprofile media settings (e.g., Frost on Nixon) can use their questions to elicit substantive responses from influential people would aid us in the broader normative goal of holding elites to account, by gaining a better understanding of what and how to ask, and what (not) to accept as an answer.
",8 Conclusion and Future Work,[0],[0]
"From a technical standpoint, future work could also augment the representation of questions and answers presently used in our framework, beyond our heuristic of using root arcs without noun phrases.",8 Conclusion and Future Work,[0],[0]
"Richer linguistic representations, as well as more judicious ways of weighting different fragments and motifs, could enable us to capture a wider range of possible surface and rhetorical forms, especially in settings where phrasings are potentially less structured by institutional conventions.",8 Conclusion and Future Work,[0],[0]
"Additionally, as with most unsupervised methods, our approach is limited by the need to hand-select parameters such as the number of clusters, and manually interpret the typology’s output.",8 Conclusion and Future Work,[0],[0]
"Having annotations of these corpora could better motivate the methodology and enable further evaluation and interpretation; we hope to encourage such annotation efforts by releasing the dataset.
",8 Conclusion and Future Work,[0],[0]
"Inevitably, drawing causal lessons from observational data is difficult.",8 Conclusion and Future Work,[0],[0]
"Moving forward, experimental tests of insights gathered through such explorations would enable us to establish causal effects of question-asking rhetoric, perhaps offering prescriptive insights into questioning strategies for objectives such as information-seeking (Dillman, 1978), request-making (Althoff et al., 2014; Mitra and Gilbert, 2014) and persuasion (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017).
",8 Conclusion and Future Work,[0],[0]
Acknowledgements.,8 Conclusion and Future Work,[0],[0]
"The first author thanks John Bercow, the Speaker of the House, for suggesting she “calm [her]self
down by taking up yoga” during the hectic deadline push
(https://youtu.be/AiAWdLAIj3c).",8 Conclusion and Future Work,[0],[0]
"The authors
thank the anonymous reviewers and Liye Fu for their com-
ments and for their helpful questions.",8 Conclusion and Future Work,[0],[0]
"We are grateful to the
organizers of the conference on New Directions in Text as
Data for fostering the inter-disciplinary collaboration that led
to this work, to Amber Boydstun and Philip Resnik for their
insights on questions in the political domain, and to Stephen
Bates, Peter Kerr and Christopher Byrne for sharing the la-
beled PMQ dataset.",8 Conclusion and Future Work,[0],[0]
"This research has been supported in part
by a Discovery and Innovation Research Seed Award from
the Office of the Vice Provost for Research at Cornell.",8 Conclusion and Future Work,[0],[0]
"Questions play a prominent role in social interactions, performing rhetorical functions that go beyond that of simple informational exchange.",abstractText,[0],[0]
"The surface form of a question can signal the intention and background of the person asking it, as well as the nature of their relation with the interlocutor.",abstractText,[0],[0]
"While the informational nature of questions has been extensively examined in the context of question-answering applications, their rhetorical aspects have been largely understudied.",abstractText,[0],[0]
"In this work we introduce an unsupervised methodology for extracting surface motifs that recur in questions, and for grouping them according to their latent rhetorical role.",abstractText,[0],[0]
"By applying this framework to the setting of question sessions in the UK parliament, we show that the resulting typology encodes key aspects of the political discourse—such as the bifurcation in questioning behavior between government and opposition parties—and reveals new insights into the effects of a legislator’s tenure and political career ambitions.",abstractText,[0],[0]
Asking too much? The rhetorical role of questions in political discourse,title,[0],[0]
"Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014).",1 Introduction,[0],[0]
"Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect.",1 Introduction,[0],[0]
"For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect
∗ Corresponding author.
",1 Introduction,[0],[0]
“service” is negative.,1 Introduction,[0],[0]
Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner.,1 Introduction,[0],[0]
"Representative approaches in literature include feature based Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a).",1 Introduction,[0],[0]
"Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.
",1 Introduction,[0],[0]
"Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect.",1 Introduction,[0],[0]
We believe that only some subset of context words are needed to infer the sentiment towards an aspect.,1 Introduction,[0],[0]
"For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed.",1 Introduction,[0],[0]
"Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word.",1 Introduction,[0],[0]
A desirable solution should be capable of explicitly capturing the importance of context words and using that information to build up features for the sentence after given an aspect word.,1 Introduction,[0],[0]
"Furthermore, a human asked to do this task will selectively focus on parts of the contexts, and acquire information where it is needed to build up an internal representation towards an aspect in his/her mind.",1 Introduction,[0],[0]
"ar X iv :1 60 5.
",1 Introduction,[0],[0]
"08 90
0v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
4 Se
p 20
In pursuit of this goal, we develop deep memory network for aspect level sentiment classification, which is inspired by the recent success of computational models with attention mechanism and explicit memory (Graves et al., 2014; Bahdanau et al., 2015; Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"Our approach is data-driven, computationally efficient and does not rely on syntactic parser or sentiment lexicon.",1 Introduction,[0],[0]
The approach consists of multiple computational layers with shared parameters.,1 Introduction,[0],[0]
"Each layer is a content- and location- based attention model, which first learns the importance/weight of each context word and then utilizes this information to calculate continuous text representation.",1 Introduction,[0],[0]
The text representation in the last layer is regarded as the feature for sentiment classification.,1 Introduction,[0],[0]
"As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification.
",1 Introduction,[0],[0]
"We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014).",1 Introduction,[0],[0]
"On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed.",1 Introduction,[0],[0]
"Lastly, we show that using multiple computational layers over external memory could achieve improved performance.",1 Introduction,[0],[0]
"Our approach is inspired by the recent success of memory network in question answering (Weston et al., 2014; Sukhbaatar et al., 2015).",2 Background: Memory Network,[0],[0]
"We describe the background on memory network in this part.
",2 Background: Memory Network,[0],[0]
Memory network is a general machine learning framework introduced by Weston et al. (2014).,2 Background: Memory Network,[0],[0]
"Its central idea is inference with a long-term memory component, which could be read, written to, and jointly learned with the goal of using it for prediction.",2 Background: Memory Network,[0],[0]
"Formally, a memory network consists of a memory m and four components I , G, O and R, where m is an array of objects such as an array of vectors.",2 Background: Memory Network,[0],[0]
"Among these four components, I converts input to internal feature representation, G updates old memories with new input, O generates an out-
put representation given a new input and the current memory state, R outputs a response based on the output representation.
",2 Background: Memory Network,[0],[0]
Let us take question answering as an example to explain the work flow of memory network.,2 Background: Memory Network,[0],[0]
"Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an answer, e.g. a word.",2 Background: Memory Network,[0],[0]
"During inference, I component reads one sentence si at a time and encodes it into a vector representation.",2 Background: Memory Network,[0],[0]
Then G component updates a piece of memory mi based on current sentence representation.,2 Background: Memory Network,[0],[0]
"After all sentences are processed, we get a memory matrix m which stores the semantics of these sentences, each row representing a sentence.",2 Background: Memory Network,[0],[0]
"Given a question q, memory network encodes it into vector representation eq, and then O component uses eq to select question related evidences from memory m and generates an output vector o. Finally, R component takes o as the input and outputs the final response.",2 Background: Memory Network,[0],[0]
It is worth noting that O component could consist of one or more computational layers (hops).,2 Background: Memory Network,[0],[0]
The intuition of utilizing multiple hops is that more abstractive evidences could be found based on previously extracted evidences.,2 Background: Memory Network,[0],[0]
"Sukhbaatar et al. (2015) demonstrate that multiple hops could uncover more abstractive evidences than single hop, and could yield improved results on question answering and language modeling.",2 Background: Memory Network,[0],[0]
"In this section, we describe the deep memory network approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
We first give the task definition.,3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Afterwards, we describe an overview of the approach before presenting the content- and location- based attention models in each computational layer.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Lastly, we describe the use of this approach for aspect level sentiment classification.",3 Deep Memory Network for Aspect Level Sentiment Classification,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} consisting of n words and an aspect word wi 1 occur-
1In practice, an aspect might be a multi word expression such as “battery life”.",3.1 Task Definition and Notation,[0],[0]
"For simplicity we still consider aspect as a single word in this definition.
ring in sentence s, aspect level sentiment classification aims at determining the sentiment polarity of sentence s towards the aspect wi.",3.1 Task Definition and Notation,[0],[0]
"For example, the sentiment polarity of sentence “great food but the service was dreadful!”",3.1 Task Definition and Notation,[0],[0]
"towards aspect “food” is positive, while the polarity towards aspect “service” is negative.",3.1 Task Definition and Notation,[0],[0]
"When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014).",3.1 Task Definition and Notation,[0],[0]
"All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V",3.1 Task Definition and Notation,[0],[0]
| is vocabulary size.,3.1 Task Definition and Notation,[0],[0]
"The word embedding of wi is notated as ei ∈ Rd×1, which is a column in the embedding matrix L.",3.1 Task Definition and Notation,[0],[0]
"We present an overview of the deep memory network for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
"Given a sentence s = {w1, w2, ..., wi, ...wn} and the aspect word wi, we map each word into its embedding vector.",3.2 An Overview of the Approach,[0],[0]
"These word vectors are separated into two parts, aspect representation and context representation.",3.2 An Overview of the Approach,[0],[0]
"If aspect is a single word like “food” or “service”, aspect representation is the embedding of aspect word.",3.2 An Overview of the Approach,[0],[0]
"For the case where aspect is multi word expression like “battery life”, aspect representation is an average of its constituting word vectors (Sun et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"To simplify the interpretation, we consider aspect as a single word wi.",3.2 An Overview of the Approach,[0],[0]
"Context word vectors {e1, e2 ...",3.2 An Overview of the Approach,[0],[0]
"ei−1, ei+1 ...",3.2 An Overview of the Approach,[0],[0]
"en} are stacked and regarded as the external memory m ∈ Rd×(n−1), where n is the sentence length.
",3.2 An Overview of the Approach,[0],[0]
"An illustration of our approach is given in Figure 1, which is inspired by the use of memory network in question answering (Sukhbaatar et al., 2015).",3.2 An Overview of the Approach,[0],[0]
"Our approach consists of multiple computational layers (hops), each of which contains an attention layer and a linear layer.",3.2 An Overview of the Approach,[0],[0]
"In the first computational layer (hop 1), we regard aspect vector as the input to adaptively select important evidences from memory m through attention layer.",3.2 An Overview of the Approach,[0],[0]
The output of attention layer and the linear transformation of aspect vector2 are summed and the result is considered as the input of next layer (hop 2).,3.2 An Overview of the Approach,[0],[0]
"In a similar way, we stack multiple hops and
2In preliminary experiments, we tried directly using aspect vector without a linear transformation, and found that adding a linear layer works slightly better.
run these steps multiple times, so that more abstractive evidences could be selected from the external memory m. The output vector in last hop is considered as the representation of sentence with regard to the aspect, and is further used as the feature for aspect level sentiment classification.
",3.2 An Overview of the Approach,[0],[0]
It is helpful to note that the parameters of attention and linear layers are shared in different hops.,3.2 An Overview of the Approach,[0],[0]
"Therefore, the model with one layer and the model with nine layers have the same number of parameters.",3.2 An Overview of the Approach,[0],[0]
We describe our attention model in this part.,3.3 Content Attention,[0],[0]
"The basic idea of attention mechanism is that it assigns a weight/importance to each lower position when computing an upper level representation (Bahdanau et al., 2015).",3.3 Content Attention,[0],[0]
"In this work, we use attention model to compute the representation of a sentence with regard to an aspect.",3.3 Content Attention,[0],[0]
The intuition is that context words do not contribute equally to the semantic meaning of a sentence.,3.3 Content Attention,[0],[0]
"Furthermore, the importance of a word should be different if we focus on different aspect.",3.3 Content Attention,[0],[0]
Let us again take the example of “great food but the service was dreadful!”.,3.3 Content Attention,[0],[0]
The context word “great” is more important than “dreadful” for aspect “food”.,3.3 Content Attention,[0],[0]
"On the contrary, “dreadful” is more important than “great” for aspect “service”.
",3.3 Content Attention,[0],[0]
"Taking an external memory m ∈ Rd×k and an aspect vector vaspect ∈ Rd×1 as input, the attention model outputs a continuous vector vec ∈ Rd×1.",3.3 Content Attention,[0],[0]
"The
output vector is computed as a weighted sum of each piece of memory in m, namely
vec = k∑
i=1
αimi (1)
where k is the memory size, αi ∈",3.3 Content Attention,[0],[0]
"[0, 1] is the weight of mi and ∑ i αi = 1.",3.3 Content Attention,[0],[0]
We implement a neural network based attention model.,3.3 Content Attention,[0],[0]
"For each piece of memory mi, we use a feed forward neural network to compute its semantic relatedness with the aspect.",3.3 Content Attention,[0],[0]
"The scoring function is calculated as follows, where Watt ∈ R1×2d and batt ∈ R1×1.
gi = tanh(Watt[mi; vaspect] + batt) (2)
After obtaining {g1, g2, ...",3.3 Content Attention,[0],[0]
"gk}, we feed them to a softmax function to calculate the final importance scores {α1, α2, ... αk}.
",3.3 Content Attention,[0],[0]
αi =,3.3 Content Attention,[0],[0]
"exp(gi)∑k j=1 exp(gj)
(3)
",3.3 Content Attention,[0],[0]
We believe that such an attention model has two advantages.,3.3 Content Attention,[0],[0]
One advantage is that this model could adaptively assign an importance score to each piece of memory mi according to its semantic relatedness with the aspect.,3.3 Content Attention,[0],[0]
"Another advantage is that this attention model is differentiable, so that it could be easily trained together with other components in an end-to-end fashion.",3.3 Content Attention,[0],[0]
We have described our neural attention framework and a content-based model in previous subsection.,3.4 Location Attention,[0],[0]
"However, the model mentioned above ignores the location information between context word and aspect.",3.4 Location Attention,[0],[0]
Such location information is helpful for an attention model because intuitively a context word closer to the aspect should be more important than a farther one.,3.4 Location Attention,[0],[0]
"In this work, we define the location of a context word as its absolute distance with the aspect in the original sentence sequence3.",3.4 Location Attention,[0],[0]
"On this basis, we study four strategies to encode the location information in the attention model.",3.4 Location Attention,[0],[0]
"The details are described below.
3The location of a context word could also be measured by its distance to the aspect along a syntactic path.",3.4 Location Attention,[0],[0]
"We leave this as a future work as we prefer to developing a purely data-driven approach without using external parsing results.
",3.4 Location Attention,[0],[0]
• Model 1.,3.4 Location Attention,[0],[0]
"Following Sukhbaatar et al. (2015), we calculate the memory vector mi with
mi = ei vi (4)
where means element-wise multiplication and vi ∈ Rd×1 is a location vector for word wi.",3.4 Location Attention,[0],[0]
"Every element in vi is calculated as follows,
vki = (1− li/n)− (k/d)(1− 2× li/n) (5)
where n is sentence length, k is the hop number and li is the location of wi. •",3.4 Location Attention,[0],[0]
Model 2.,3.4 Location Attention,[0],[0]
"This is a simplified version of Model 1, using the same location vector vi for wi in different hops.",3.4 Location Attention,[0],[0]
"Location vector vi is calculated as follows.
",3.4 Location Attention,[0],[0]
"vi = 1− li/n (6)
• Model 3.",3.4 Location Attention,[0],[0]
"We regard location vector vi as a parameter and compute a piece of memory with vector addition, namely
mi = ei + vi (7)
All the position vectors are stacked in a position embedding matrix, which is jointly learned with gradient descent.",3.4 Location Attention,[0],[0]
•,3.4 Location Attention,[0],[0]
Model 4.,3.4 Location Attention,[0],[0]
Location vectors are also regarded as parameters.,3.4 Location Attention,[0],[0]
"Different from Model 3, location representations are regarded as neural gates to control how many percent of word semantics is written into the memory.",3.4 Location Attention,[0],[0]
"We feed location vector vi to a sigmoid function σ, and calculatemi with element-wise multiplication:
mi = ei σ(vi) (8)",3.4 Location Attention,[0],[0]
"It is widely accepted that computational models that are composed of multiple processing layers have the ability to learn representations of data with multiple levels of abstraction (LeCun et al., 2015).",3.5 The Need for Multiple Hops,[0],[0]
"In this work, the attention layer in one layer is essentially a weighted average compositional function, which is not powerful enough to handle the sophisticated computationality like negation, intensification and contrary in language.",3.5 The Need for Multiple Hops,[0],[0]
Multiple computational layers allow the deep memory network to learn representations of text with multiple levels of abstraction.,3.5 The Need for Multiple Hops,[0],[0]
"Each layer/hop retrieves important context words,
and transforms the representation at previous level into a representation at a higher, slightly more abstract level.",3.5 The Need for Multiple Hops,[0],[0]
"With the composition of enough such transformations, very complex functions of sentence representation towards an aspect can be learned.",3.5 The Need for Multiple Hops,[0],[0]
"We regard the output vector in last hop as the feature, and feed it to a softmax layer for aspect level sentiment classification.",3.6 Aspect Level Sentiment Classification,[0],[0]
"The model is trained in a supervised manner by minimizing the cross entropy error of sentiment classification, whose loss function is given below, where T means all training instances, C is the collection of sentiment categories, (s, a) means a sentence-aspect pair.
",3.6 Aspect Level Sentiment Classification,[0],[0]
"loss = − ∑
(s,a)∈T ∑ c∈C P gc (s, a) · log(Pc(s, a))",3.6 Aspect Level Sentiment Classification,[0],[0]
"(9)
Pc(s, a) is the probability of predicting (s, a) as category c produced by our system.",3.6 Aspect Level Sentiment Classification,[0],[0]
"P gc (s, a) is 1 or 0, indicating whether the correct answer is c. We use back propagation to calculate the gradients of all the parameters, and update them with stochastic gradient descent.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We clamp the word embeddings with 300-dimensional Glove vectors (Pennington et al., 2014), which is trained from web data and the vocabulary size is 1.9M4.",3.6 Aspect Level Sentiment Classification,[0],[0]
"We randomize other parameters with uniform distribution U(−0.01, 0.01), and set the learning rate as 0.01.",3.6 Aspect Level Sentiment Classification,[0],[0]
We describe experimental settings and report empirical results in this section.,4 Experiment,[0],[0]
"We conduct experiments on two datasets from SemEval 2014 (Pontiki et al., 2014), one from laptop domain and another from restaurant domain.",4.1 Experimental Setting,[0],[0]
Statistics of the datasets are given in Table 1.,4.1 Experimental Setting,[0],[0]
"It is worth noting that the original dataset contains the fourth category - conflict, which means that a sentence expresses both positive and negative opinion towards an aspect.",4.1 Experimental Setting,[0],[0]
"We remove conflict category as the number of instances is very tiny, incorporating which
4Available at: http://nlp.stanford.edu/projects/glove/.
will make the dataset extremely unbalanced.",4.1 Experimental Setting,[0],[0]
Evaluation metric is classification accuracy.,4.1 Experimental Setting,[0],[0]
"We compare with the following baseline methods on both datasets.
",4.2 Comparison to Other Methods,[0],[0]
"(1) Majority is a basic baseline method, which assigns the majority sentiment label in training set to each instance in the test set.
",4.2 Comparison to Other Methods,[0],[0]
(2) Feature-based SVM performs state-of-the-art on aspect level sentiment classification.,4.2 Comparison to Other Methods,[0],[0]
"We compare with a top system using ngram features, parse features and lexicon features (Kiritchenko et al., 2014).
",4.2 Comparison to Other Methods,[0],[0]
"(3) We compare with three LSTM models (Tang et al., 2015a)).",4.2 Comparison to Other Methods,[0],[0]
"In LSTM, a LSTM based recurrent model is applied from the start to the end of a sentence, and the last hidden vector is used as the sentence representation.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM extends LSTM by taking into account of the aspect, and uses two LSTM networks, a forward one and a backward one, towards the aspect.",4.2 Comparison to Other Methods,[0],[0]
"TDLSTM+ATT extends TDLSTM by incorporating an attention mechanism (Bahdanau et al., 2015) over the hidden vectors.",4.2 Comparison to Other Methods,[0],[0]
"We use the same Glove word vectors for fair comparison.
",4.2 Comparison to Other Methods,[0],[0]
"(4) We also implement ContextAVG, a simplistic version of our approach.",4.2 Comparison to Other Methods,[0],[0]
Context word vectors are averaged and the result is added to the aspect vector.,4.2 Comparison to Other Methods,[0],[0]
"The output is fed to a softmax function.
",4.2 Comparison to Other Methods,[0],[0]
Experimental results are given in Table 2.,4.2 Comparison to Other Methods,[0],[0]
"Our approach using only content attention is abbreviated to MemNet (k), where k is the number of hops.",4.2 Comparison to Other Methods,[0],[0]
"We can find that feature-based SVM is an extremely strong performer and substantially outperforms other baseline methods, which demonstrates the importance of a powerful feature representation for aspect level sentiment classification.",4.2 Comparison to Other Methods,[0],[0]
"Among three recurrent models, TDLSTM performs better than LSTM, which indicates that taking into account of the aspect information is helpful.",4.2 Comparison to Other Methods,[0],[0]
"This is reason-
able as the sentiment polarity of a sentence towards different aspects (e.g. “food” and “service”) might be different.",4.2 Comparison to Other Methods,[0],[0]
It is somewhat disappointing that incorporating attention model over TDLSTM does not bring any improvement.,4.2 Comparison to Other Methods,[0],[0]
We consider that each hidden vector of TDLSTM encodes the semantics of word sequence until the current position.,4.2 Comparison to Other Methods,[0],[0]
"Therefore, the model of TDLSTM+ATT actually selects such mixed semantics of word sequence, which is weird and not an intuitive way to selectively focus on parts of contexts.",4.2 Comparison to Other Methods,[0],[0]
"Different from TDLSTM+ATT, the proposed memory network approach removes the recurrent calculator over word sequence and directly apply attention mechanism on context word representations.
",4.2 Comparison to Other Methods,[0],[0]
"We can also find that the performance of ContextAVG is very poor, which means that assigning the same weight/importance to all the context words is not an effective way.",4.2 Comparison to Other Methods,[0],[0]
"Among all our models from single hop to nine hops, we can observe that using more computational layers could generally lead to better performance, especially when the number of hops is less than six.",4.2 Comparison to Other Methods,[0],[0]
"The best performances are achieved when the model contains seven and nine hops, respectively.",4.2 Comparison to Other Methods,[0],[0]
"On both datasets, the proposed approach could obtain comparable accuracy compared to the state-of-art feature-based SVM system.",4.2 Comparison to Other Methods,[0],[0]
We study the runtime of recurrent neural models and the proposed deep memory network approach with different hops.,4.3 Runtime Analysis,[0],[0]
"We implement all these approaches based on the same neural network infrastructure, use the same 300-dimensional Glove word vectors, and run them on the same CPU server.
",4.3 Runtime Analysis,[0],[0]
The training time of each iteration on the restaurant dataset is given in Table 3.,4.3 Runtime Analysis,[0],[0]
"We can find that LSTM based recurrent models are indeed computationally expensive, which is caused by the complex operations in each LSTM unit along the word sequence.",4.3 Runtime Analysis,[0],[0]
"Instead, the memory network approach is simpler and evidently faster because it does not need recurrent calculators of sequence length.",4.3 Runtime Analysis,[0],[0]
Our approach with nine hops is almost 15 times faster than the basic LSTM model.,4.3 Runtime Analysis,[0],[0]
"As described in Section 3.4, we explore four strategies to integrate location information into the attention model.",4.4 Effects of Location Attention,[0],[0]
We incorporate each of them separately into the basic content-based attention model.,4.4 Effects of Location Attention,[0],[0]
It is helpful to restate that the difference between four location-based attention models lies in the usage of location vectors for context words.,4.4 Effects of Location Attention,[0],[0]
"In Model 1 and Model 2, the values of location vectors are fixed and calculated in a heuristic way.",4.4 Effects of Location Attention,[0],[0]
"In Model 3 and Model 4, location vectors are also regarded as the parameters and jointly learned along with other parameters in the deep memory network.
",4.4 Effects of Location Attention,[0],[0]
Figure 2 shows the classification accuracy of each attention model on the restaurant dataset.,4.4 Effects of Location Attention,[0],[0]
We can find that using multiple computational layers could consistently improve the classification accuracy in all these models.,4.4 Effects of Location Attention,[0],[0]
All these models perform comparably when the number of hops is larger than five.,4.4 Effects of Location Attention,[0],[0]
"Among these four location-based models, we prefer Model 2 as it is intuitive and has less computation cost without loss of accuracy.",4.4 Effects of Location Attention,[0],[0]
"We also find
that Model 4 is very sensitive to the choice of neural gate.",4.4 Effects of Location Attention,[0],[0]
Its classification accuracy decreases by almost 5 percentage when the sigmoid operation over location vector is removed.,4.4 Effects of Location Attention,[0],[0]
We visualize the attention weight of each context word to get a better understanding of the deep memory network approach.,4.5 Visualize Attention Models,[0],[0]
"The results of context-based model and location-based model (Model 2) are given in Table 4 and Table 5, respectively.
From Table 4(a), we can find that in the first hop the context words “great”, “but” and “dreadful” contribute equally to the aspect “service”.",4.5 Visualize Attention Models,[0],[0]
"While after the second hop, the weight of “dreadful” increases and finally the model correctly predict the polarity towards “service” as negative.",4.5 Visualize Attention Models,[0],[0]
This case shows the effects of multiple hops.,4.5 Visualize Attention Models,[0],[0]
"However, in Table 4(b), the content-based model also gives a larger weight to “dreadful” when the target we focus on is “food”.",4.5 Visualize Attention Models,[0],[0]
"As a result, the model incorrectly predicts the polarity towards “food” as negative.",4.5 Visualize Attention Models,[0],[0]
This phenomenon might be caused by the neglect of location information.,4.5 Visualize Attention Models,[0],[0]
"From Table 5(b), we can find that the weight
of “great” is increased when the location of context word is considered.",4.5 Visualize Attention Models,[0],[0]
"Accordingly, Model 2 predicts the correct sentiment label towards “food”.",4.5 Visualize Attention Models,[0],[0]
We believe that location-enhanced model captures both content and location information.,4.5 Visualize Attention Models,[0],[0]
"For instance, in Table 5(a) the closest context words of the aspect “service” are “the” and “was”, while “dreadful” has the largest weight.",4.5 Visualize Attention Models,[0],[0]
"We carry out an error analysis of our location enhanced model (Model 2) on the restaurant dataset, and find that most of the errors could be summarized as follows.",4.6 Error Analysis,[0],[0]
The first factor is noncompositional sentiment expression.,4.6 Error Analysis,[0],[0]
This model regards single context word as the basic computational unit and cannot handle this situation.,4.6 Error Analysis,[0],[0]
"An example is “dessert was also to die for!”, where the aspect is underlined.",4.6 Error Analysis,[0],[0]
"The sentiment expression is “die for”, whose meaning could not be composed from its constituents “die” and “for”.",4.6 Error Analysis,[0],[0]
"The second factor is complex aspect expression consisting of many words, such as “ask for the round corner table next to the large window.”",4.6 Error Analysis,[0],[0]
"This model represents an aspect expression by averaging its constituting word vectors, which could not well handle this situation.",4.6 Error Analysis,[0],[0]
"The third factor is sentimental relation between context words such as negation, comparison and condition.",4.6 Error Analysis,[0],[0]
"An example is “but dinner here is never disappointing, even if the prices are a bit over the top”.",4.6 Error Analysis,[0],[0]
We believe that this is caused by the weakness of weighted average compositional function in each hop.,4.6 Error Analysis,[0],[0]
There are also cases when comparative opinions are expressed such as “i ’ve had better japanese food at a mall food court”.,4.6 Error Analysis,[0],[0]
This work is connected to three research areas in natural language processing.,5 Related Work,[0],[0]
We briefly describe related studies in each area.,5 Related Work,[0],[0]
"Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014).",5.1 Aspect Level Sentiment Classification,[0],[0]
"Most existing works use machine learning algorithms, and build sentiment classifier from
sentences with manually annotated polarity labels.",5.1 Aspect Level Sentiment Classification,[0],[0]
One of the most successful approaches in literature is feature based SVM.,5.1 Aspect Level Sentiment Classification,[0],[0]
"Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014).",5.1 Aspect Level Sentiment Classification,[0],[0]
"In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data.",5.1 Aspect Level Sentiment Classification,[0],[0]
"However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect.",5.1 Aspect Level Sentiment Classification,[0],[0]
"It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015).",5.1 Aspect Level Sentiment Classification,[0],[0]
The aspect word in this work is given as a part of the input.,5.1 Aspect Level Sentiment Classification,[0],[0]
"In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892).",5.2 Compositionality in Vector Space,[0],[0]
Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector.,5.2 Compositionality in Vector Space,[0],[0]
Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases.,5.2 Compositionality in Vector Space,[0],[0]
"To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Schütze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015).",5.2 Compositionality in Vector Space,[0],[0]
"Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016).",5.2 Compositionality in Vector Space,[0],[0]
"Recently, there is a resurgence in computational models with attention mechanism and explicit mem-
ory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
"In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks.",5.3 Attention and Memory Networks,[0],[0]
"Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation.",5.3 Attention and Memory Networks,[0],[0]
"Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015).",5.3 Attention and Memory Networks,[0],[0]
We develop deep memory networks that capture importances of context words for aspect level sentiment classification.,6 Conclusion,[0],[0]
"Compared with recurrent neural models like LSTM, this approach is simpler and faster.",6 Conclusion,[0],[0]
"Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures.",6 Conclusion,[0],[0]
We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation.,6 Conclusion,[0],[0]
We also demonstrate that using multiple computational layers in memory network could obtain improved performance.,6 Conclusion,[0],[0]
Our potential future plans are incorporating sentence structure like parsing results into the deep memory network.,6 Conclusion,[0],[0]
We would especially want to thank Xiaodan Zhu for running their system on our setup.,Acknowledgments,[0],[0]
We greatly thank Yaming Sun for tremendously helpful discussions.,Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61632011 and No.61273321).",Acknowledgments,[0],[0]
We introduce a deep memory network for aspect level sentiment classification.,abstractText,[0],[0]
"Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect.",abstractText,[0],[0]
"Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory.",abstractText,[0],[0]
"Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures.",abstractText,[0],[0]
On both datasets we show that multiple computational layers could improve the performance.,abstractText,[0],[0]
"Moreover, our approach is also fast.",abstractText,[0],[0]
The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.,abstractText,[0],[0]
Aspect Level Sentiment Classification with Deep Memory Network,title,[0],[0]
"We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain. Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels. Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner. A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents. We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant. Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.1",text,[0],[0]
Many NLP problems are naturally multitask classification problems.,1 Introduction,[0],[0]
"For instance, values extracted for different fields from the same document are often dependent as they share the same context.",1 Introduction,[0],[0]
Existing systems rely on this dependence (transfer across fields) to improve accuracy.,1 Introduction,[0],[0]
"In this paper, we consider a version of this problem where there is a clear dependence between two tasks but annotations are available only for the source task.",1 Introduction,[0],[0]
"For example,
1The code is available at https://github.com/ yuanzh/aspect_adversarial.
",1 Introduction,[0],[0]
the target goal may be to classify pathology reports (shown in Figure 1) for the presence of lymph invasion but training data are available only for carcinoma in the same reports.,1 Introduction,[0],[0]
"We call this problem aspect transfer as the objective is to learn to classify examples differently, focusing on different aspects, without access to target aspect labels.",1 Introduction,[0],[0]
"Clearly, such transfer learning is possible only with auxiliary information relating the tasks together.
",1 Introduction,[0],[0]
The key challenge is to articulate and incorporate commonalities across the tasks.,1 Introduction,[0],[0]
"For instance, in classifying reviews of different products, sentiment words (referred to as pivots) can be shared across the products.",1 Introduction,[0],[0]
"This commonality enables one to align feature spaces across multiple products, enabling useful transfer (Blitzer et al., 2006).",1 Introduction,[0],[0]
"Similar properties hold in other contexts and beyond senti-
ar X
iv :1
70 1.
00 18
8v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
5 Se
p 20
ment analysis.",1 Introduction,[0],[0]
"Figure 1 shows that certain words and phrases like “identified”, which indicates the presence of a histological property, are applicable to both carcinoma and lymph invasion.",1 Introduction,[0],[0]
"Our method learns and relies on such shared indicators, and utilizes them for effective transfer.
",1 Introduction,[0],[0]
"The unique feature of our transfer problem is that both the source and the target classifiers operate over the same domain, i.e., the same examples.",1 Introduction,[0],[0]
"In this setting, traditional transfer methods will always predict the same label for both aspects and thus leading to failure.",1 Introduction,[0],[0]
"Instead of supplying the target classifier with direct training labels, our approach builds on a secondary relationship between the tasks using aspect-relevance annotations of sentences.",1 Introduction,[0],[0]
"These relevance annotations indicate a possibility that the answer could be found in a sentence, not what the answer is.",1 Introduction,[0],[0]
"One can often write simple keyword rules that identify sentence relevance to a particular aspect through representative terms, e.g., specific hormonal markers in the context of pathology reports.",1 Introduction,[0],[0]
"Annotations of this kind can be readily provided by domain experts, or extracted from medical literature such as codex rules in pathology (Pantanowitz et al., 2008).",1 Introduction,[0],[0]
We assume a small number of relevance annotations (rules) pertaining to both source and target aspects as a form of weak supervision.,1 Introduction,[0],[0]
"We use this sentence-level aspect relevance to learn how to encode the examples (e.g., pathology reports) from the point of view of the desired aspect.",1 Introduction,[0],[0]
"In our approach, we construct different aspect-dependent encodings of the same document by softly selecting sentences relevant to the aspect of interest.",1 Introduction,[0],[0]
"The key to effective transfer is how these encodings are aligned.
",1 Introduction,[0],[0]
"This encoding mechanism brings the problem closer to the realm of standard domain adaptation, where the derived aspect-specific representations are considered as different domains.",1 Introduction,[0],[0]
"Given these representations, our method learns a label classifier shared between the two domains.",1 Introduction,[0],[0]
"To ensure that it can be adjusted only based on the source class labels, and that it also reasonably applies to the target encodings, we must align the two sets of encoded examples.2 Learning this alignment is pos-
2This alignment or invariance is enforced on the level of sets, not individual reports; aspect-driven encoding of any specific report should remain substantially different for the two tasks since the encoded examples are passed on to the same classifier.
sible because, as discussed above, some keywords are directly transferable and can serve as anchors for constructing this invariant space.",1 Introduction,[0],[0]
"To learn this invariant representation, we introduce an adversarial domain classifier analogous to the recent successful use of adversarial training in computer vision (Ganin and Lempitsky, 2014).",1 Introduction,[0],[0]
The role of the domain classifier (adversary) is to learn to distinguish between the two types of encodings.,1 Introduction,[0],[0]
During training we update the encoder with an adversarial objective to cause the classifier to fail.,1 Introduction,[0],[0]
"The encoder therefore learns to eliminate aspect-specific information so that encodings look invariant (as sets) to the classifier, thus establishing aspect-invariance encodings and enabling transfer.",1 Introduction,[0],[0]
"All three components in our approach, 1) aspect-driven encoding, 2) classification of source labels, and 3) domain adversary, are trained jointly (concurrently) to complement and balance each other.
",1 Introduction,[0],[0]
Adversarial training of domain and label classifiers can be challenging to stabilize.,1 Introduction,[0],[0]
"In our setting, sentences are encoded with a convolutional model.",1 Introduction,[0],[0]
Feedback from adversarial training can be an unstable guide for how the sentences should be encoded.,1 Introduction,[0],[0]
"To address this issue, we incorporate an additional word-level auto-encoder reconstruction loss to ground the convolutional processing of sentences.",1 Introduction,[0],[0]
"We empirically demonstrate that this additional objective yields richer and more diversified feature representations, improving transfer.
",1 Introduction,[0],[0]
We evaluate our approach on pathology reports (aspect transfer) as well as on a more standard review dataset (domain adaptation).,1 Introduction,[0],[0]
"On the pathology dataset, we explore cross-aspect transfer across different types of breast disease.",1 Introduction,[0],[0]
"Specifically, we test on six adaptation tasks, consistently outperforming all other baselines.",1 Introduction,[0],[0]
"Overall, our full model achieves 27% and 20.2% absolute improvement arising from aspect-driven encoding and adversarial training respectively.",1 Introduction,[0],[0]
"Moreover, our unsupervised adaptation method is only 5.7% behind the accuracy of a supervised target model.",1 Introduction,[0],[0]
"On the review dataset, we test adaptations from hotel to restaurant reviews.",1 Introduction,[0],[0]
"Our model outperforms the marginalized denoising autoencoder (Chen et al., 2012) by 5%.",1 Introduction,[0],[0]
"Finally, we examine and illustrate the impact of individual components on the resulting performance.",1 Introduction,[0],[0]
"Domain Adaptation for Deep Learning Existing approaches commonly induce abstract representations without pulling apart different aspects in the same example, and therefore are likely to fail on the aspect transfer problem.",2 Related Work,[0],[0]
"The majority of these prior methods first learn a task-independent representation, and then train a label predictor (e.g. SVM) on this representation in a separate step.",2 Related Work,[0],[0]
"For example, earlier researches employ a shared autoencoder (Glorot et al., 2011; Chopra et al., 2013) to learn a cross-domain representation.",2 Related Work,[0],[0]
Chen et al. (2012) further improve and stabilize the representation learning by utilizing marginalized denoising autoencoders.,2 Related Work,[0],[0]
"Later, Zhou et al. (2016) propose to minimize domain-shift of the autoencoder in a linear data combination manner.",2 Related Work,[0],[0]
Other researches have focused on learning transferable representations in an end-to-end fashion.,2 Related Work,[0],[0]
"Examples include using transduction learning for object recognition (Sener et al., 2016) and using residual transfer networks for image classification (Long et al., 2016).",2 Related Work,[0],[0]
"In contrast, we use adversarial training to encourage learning domaininvariant features in a more explicit way.",2 Related Work,[0],[0]
Our approach offers another two advantages over prior work.,2 Related Work,[0],[0]
"First, we jointly optimize features with the final classification task while many previous works only learn task-independent features using autoencoders.",2 Related Work,[0],[0]
"Second, our model can handle traditional domain transfer as well as aspect transfer, while previous methods can only handle the former.
",2 Related Work,[0],[0]
Adversarial Learning in Vision and NLP Our approach closely relates to the idea of domainadversarial training.,2 Related Work,[0],[0]
"Adversarial networks were originally developed for image generation (Goodfellow et al., 2014; Makhzani et al., 2015; Springenberg, 2015; Radford et al., 2015; Taigman et al., 2016), and were later applied to domain adaptation in computer vision (Ganin and Lempitsky, 2014; Ganin et al., 2015; Bousmalis et al., 2016; Tzeng et al., 2014) and speech recognition (Shinohara, 2016).",2 Related Work,[0],[0]
The core idea of these approaches is to promote the emergence of invariant image features by optimizing the feature extractor as an adversary against the domain classifier.,2 Related Work,[0],[0]
"While Ganin et al. (2015) also apply this idea to sentiment analysis, their practical gains have remained limited.
",2 Related Work,[0],[0]
Our approach presents two main departures.,2 Related Work,[0],[0]
"In computer vision, adversarial learning has been used for transferring across domains, while our method can also handle aspect transfer.",2 Related Work,[0],[0]
"In addition, we introduce a reconstruction loss which results in more robust adversarial training.",2 Related Work,[0],[0]
"We believe that this formulation will benefit other applications of adversarial training, beyond the ones described in this paper.
",2 Related Work,[0],[0]
"Semi-supervised Learning with Keywords In our work, we use a small set of keywords as a source of weak supervision for aspect-relevance scoring.",2 Related Work,[0],[0]
"This relates to prior work on utilizing prototypes and seed words in semi-supervised learning (Haghighi and Klein, 2006; Grenager et al., 2005; Chang et al., 2007; Mann and McCallum, 2008; Jagarlamudi et al., 2012; Li et al., 2012; Eisenstein, 2017).",2 Related Work,[0],[0]
All these prior approaches utilize prototype annotations primarily targeting model bootstrapping but not for learning representations.,2 Related Work,[0],[0]
"In contrast, our model uses provided keywords to learn aspect-driven encoding of input examples.
",2 Related Work,[0],[0]
"Attention Mechanism in NLP One may view our aspect-relevance scorer as a sentence-level “semi-supervised attention”, in which relevant sentences receive more attention during feature extraction.",2 Related Work,[0],[0]
"While traditional attention-based models typically induce attention in an unsupervised manner, they have to rely on a large amount of labeled data for the target task (Bahdanau et al., 2014; Rush et al., 2015; Chen et al., 2015; Cheng et al., 2016; Xu et al., 2015; Xu and Saenko, 2015; Yang et al., 2015; Martins and Astudillo, 2016; Lei et al., 2016).",2 Related Work,[0],[0]
"Unlike these methods, our approach assumes no label annotations in the target domain.",2 Related Work,[0],[0]
"Other researches have focused on utilizing human-provided rationales as “supervised attention” to improve prediction (Zaidan et al., 2007; Marshall et al., 2015; Zhang et al., 2016; Brun et al., 2016).",2 Related Work,[0],[0]
"In contrast, our model only assumes access to a small set of keywords as a source of weak supervision.",2 Related Work,[0],[0]
"Moreover, all these prior approaches focus on in-domain classification.",2 Related Work,[0],[0]
"In this paper, however, we study the task in the context of domain adaptation.
",2 Related Work,[0],[0]
Multitask Learning,2 Related Work,[0],[0]
Existing multitask learning methods focus on the case where supervision is available for all tasks.,2 Related Work,[0],[0]
"A typical architecture involves using a shared encoder with a separate clas-
sifier for each task.",2 Related Work,[0],[0]
"(Caruana, 1998; Pan and Yang, 2010; Collobert and Weston, 2008; Liu et al., 2015; Bordes et al., 2012).",2 Related Work,[0],[0]
"In contrast, our work assumes labeled data only for the source aspect.",2 Related Work,[0],[0]
We train a single classifier for both aspects by learning aspectinvariant representation that enables the transfer.,2 Related Work,[0],[0]
We begin by formalizing aspect transfer with the idea of differentiating it from standard domain adaptation.,3 Problem Formulation,[0],[0]
"In our setup, we have two classification tasks called the source and the target tasks.",3 Problem Formulation,[0],[0]
"In contrast to source and target tasks in domain adaptation, both of these tasks are defined over the same set of examples (here documents, e.g., pathology reports).",3 Problem Formulation,[0],[0]
What differentiates the two classification tasks is that they pertain to different aspects in the examples.,3 Problem Formulation,[0],[0]
"If each training document were annotated with both the source and the target aspect labels, the problem would reduce to multi-label classification.",3 Problem Formulation,[0],[0]
"However, in our setting training labels are available only for the source aspect so the goal is to solve the target task without any associated training label.
",3 Problem Formulation,[0],[0]
"To fix notation, let d = {si}|d|i=1 be a document that consists of a sequence of |d| sentences si.",3 Problem Formulation,[0],[0]
"Given a document d, and the aspect of interest, we wish to predict the corresponding aspect-dependent class label y (e.g., y ∈ {−1, 1}).",3 Problem Formulation,[0],[0]
We assume that the set of possible labels are the same across aspects.,3 Problem Formulation,[0],[0]
We use ysl;k to denote the k-th coordinate of a one-hot vector indicating the correct training source aspect label for document dl.,3 Problem Formulation,[0],[0]
"Target aspect labels are not available during training.
",3 Problem Formulation,[0],[0]
"Beyond labeled documents for the source aspect {dl, ysl }l∈L, and shared unlabeled documents for source and target aspects {dl}l∈U , we assume further that we have relevance scores pertaining to each aspect.",3 Problem Formulation,[0],[0]
"The relevance is given per sentence, for some subset of sentences across the documents, and indicates the possibility that the answer for that document would be found in the sentence but without indicating which way the answer goes.",3 Problem Formulation,[0],[0]
"Relevance is always aspect dependent yet often easy to provide in the form of simple keyword rules.
",3 Problem Formulation,[0],[0]
"We use rai ∈ {0, 1} to denote the given relevance label pertaining to aspect a for sentence si.",3 Problem Formulation,[0],[0]
"Only a small subset of sentences in the training set have as-
sociated relevance labels.",3 Problem Formulation,[0],[0]
"Let R = {(a, l, i)} denote the index set of relevance labels such that if (a, l, i) ∈ R then aspect a’s relevance label ral,i is available for the ith sentence in document dl.",3 Problem Formulation,[0],[0]
In our case relevance labels arise from aspect-dependent keyword matches.,3 Problem Formulation,[0],[0]
"rai = 1 when the sentence contains any keywords pertaining to aspect a and rai = 0 if it has any keywords of other aspects.3 Separate subsets of relevance labels are available for each aspect as the keywords differ.
",3 Problem Formulation,[0],[0]
The transfer that is sought here is between two tasks over the same set of examples rather than between two different types of examples for the same task as in standard domain adaptation.,3 Problem Formulation,[0],[0]
"However, the two formulations can be reconciled if full relevance annotations are assumed to be available during training and testing.",3 Problem Formulation,[0],[0]
"In this scenario, we could simply lift the sets of relevant sentences from each document as new types of documents.",3 Problem Formulation,[0],[0]
"The goal would be then to learn to classify documents of type T (consisting of sentences relevant to the target aspect) based on having labels only for type S (source) documents, a standard domain adaptation task.",3 Problem Formulation,[0],[0]
"Our problem is more challenging as the aspect-relevance of sentences must be learned from limited annotations.
",3 Problem Formulation,[0],[0]
"Finally, we note that the aspect transfer problem and the method we develop to solve it work the same even when source and target documents are a priori different, something we will demonstrate later.",3 Problem Formulation,[0],[0]
Our model consists of three key components as shown in Figure 2.,4.1 Overview of our approach,[0],[0]
"Each document is encoded in a relevance weighted, aspect-dependent manner (green, left part of Figure 2) and classified using the label predictor (blue, top-right).",4.1 Overview of our approach,[0],[0]
"During training, the encoded documents are also passed on to the domain classifier (orange, bottom-right).",4.1 Overview of our approach,[0],[0]
"The role of the domain classifier, as the adversary, is to ensure that the aspect-dependent encodings of documents are distributionally matched.",4.1 Overview of our approach,[0],[0]
"This matching justifies the use of the same end-classifier to provide the predicted label regardless of the task (aspect).
",4.1 Overview of our approach,[0],[0]
"3rai = 1 if the sentence contains keywords pertaining to both aspect a and other aspects.
",4.1 Overview of our approach,[0],[0]
"r = 1.0
r = 0.0r = 0.9
To encode a document, the model first maps each sentence into a vector and then passes the vector to a scoring network to determine whether the sentence is relevant for the chosen aspect.",4.1 Overview of our approach,[0],[0]
These predicted relevance scores are used to obtain document vectors by taking relevance-weighted sum of the associated sentence vectors.,4.1 Overview of our approach,[0],[0]
"Thus, the manner in which the document vector is constructed is always aspectdependent due to the chosen relevance weights.
",4.1 Overview of our approach,[0],[0]
"During training, the resulting adjusted document vectors are consumed by the two classifiers.",4.1 Overview of our approach,[0],[0]
"The primary label classifier aims to predict the source labels (when available), while the domain classifier determines whether the document vector pertains to the source or target aspect, which is the label that we know by construction.",4.1 Overview of our approach,[0],[0]
"Furthermore, we jointly update the document encoder with a reverse of the gradient from the domain classifier, so that the encoder learns to induce document representations that fool the domain classifier.",4.1 Overview of our approach,[0],[0]
"The resulting encoded representations will be aspect-invariant, facilitating transfer.
",4.1 Overview of our approach,[0],[0]
Our adversarial training scheme uses all the training losses concurrently to adjust the model parameters.,4.1 Overview of our approach,[0],[0]
"During testing, we simply encode each test document in a target-aspect dependent manner, and apply the same label predictor.",4.1 Overview of our approach,[0],[0]
"We expect that the same label classifier does well on the target task since it solves the source task, and operates on relevance-weighted representations that are matched across the tasks.",4.1 Overview of our approach,[0],[0]
"While our method is designed to work in the extreme setting that the examples for the two aspects are the same, this is by no means a re-
quirement.",4.1 Overview of our approach,[0],[0]
"Our method will also work fine in the more traditional domain adaptation setting, which we will demonstrate later.",4.1 Overview of our approach,[0],[0]
Sentence embedding We apply a convolutional model illustrated in Figure 3 to each sentence si to obtain sentence-level vector embeddings xseni .,4.2 Components in detail,[0],[0]
"The use of RNNs or bi-LSTMs would result in more flexible sentence embeddings but based on our initial experiments, we did not observe any significant gains over the simpler CNNs.
",4.2 Components in detail,[0],[0]
We further ground the resulting sentence embeddings by including an additional word-level reconstruction step in the convolutional model.,4.2 Components in detail,[0],[0]
The purpose of this reconstruction step is to balance adversarial training signals propagating back from the domain classifier.,4.2 Components in detail,[0],[0]
"Specifically, it forces the sentence encoder to keep rich word-level information in contrast to adversarial training that seeks to eliminate aspect specific features.",4.2 Components in detail,[0],[0]
"We provide an empirical analysis of the impact of this reconstruction in the
experiment section (Section 7).",4.2 Components in detail,[0],[0]
"More concretely, we reconstruct word embedding from the corresponding convolutional layer, as shown in Figure 3.4 We use xi,j to denote the embedding of the j-th word in sentence si.",4.2 Components in detail,[0],[0]
"Let hi,j be the convolutional output when xi,j is at the center of the window.",4.2 Components in detail,[0],[0]
"We reconstruct xi,j by
x̂i,j = tanh(W chi,j + b c) (1)
where Wc and bc are parameters of the reconstruction layer.",4.2 Components in detail,[0],[0]
"The loss associated with the reconstruction for document d is
Lrec(d)",4.2 Components in detail,[0],[0]
= 1 n ∑,4.2 Components in detail,[0],[0]
"i,j ||x̂i,j − tanh(xi,j)||22 (2)
where n is the number of tokens in the document and indexes",4.2 Components in detail,[0],[0]
"i, j identify the sentence and word, respectively.",4.2 Components in detail,[0],[0]
"The overall reconstruction loss Lrec is obtained by summing over all labeled/unlabeled documents.
",4.2 Components in detail,[0],[0]
"Relevance prediction We use a small set of keyword rules to generate binary relevance labels, both positive (r = 1) and negative (r = 0).",4.2 Components in detail,[0],[0]
These labels represent the only supervision available to predict relevance.,4.2 Components in detail,[0],[0]
The prediction is made on the basis of the sentence vector xseni passed through a feedforward network with a ReLU output unit.,4.2 Components in detail,[0],[0]
The network has a single shared hidden layer and a separate output layer for each aspect.,4.2 Components in detail,[0],[0]
"Note that our relevance prediction network is trained as a non-negative regression model even though the available labels are binary, as relevance varies more on a linear rather than binary scale.
",4.2 Components in detail,[0],[0]
"Given relevance labels indexed by R = {(a, l, i)}, we minimize
Lrel = ∑
(a,l,i)∈R
( ral,i − r̂al,i )2 (3) where r̂al,i is the predicted (non-negative) relevance score pertaining to aspect a for the ith sentence in document dl, as shown in the left part of Figure 2.",4.2 Components in detail,[0],[0]
"ral,i, defined earlier, is the given binary (0/1) relevance label.",4.2 Components in detail,[0],[0]
"We use a score in [0, 1] scale because it can be naturally used as a weight for vector combinations, as shown next.
4",4.2 Components in detail,[0],[0]
"This process is omitted in Figure 2 for brevity.
",4.2 Components in detail,[0],[0]
"Document encoding The initial vector representation for each document such as dl is obtained as a relevance weighted combination of the associated sentence vectors, i.e.,
xdoc,al =
∑ i r̂
a l,i · xsenl,i∑ i r̂ a l,i
(4)
The resulting vector selectively encodes information from the sentences based on relevance to the focal aspect.
",4.2 Components in detail,[0],[0]
Transformation layer The manner in which document vectors arise from sentence vectors means that they will retain aspect-specific information that will hinder transfer across aspects.,4.2 Components in detail,[0],[0]
"To help remove non-transferable information, we add a transformation layer to map the initial document vectors xdoc,al to their domain invariant (as a set) versions, as shown in Figure 2.",4.2 Components in detail,[0],[0]
"Specifically, the transformed representation is given by xtr,al = W
trxdoc,al . Meanwhile, the transformation has to be strongly regularized lest the gradient from the adversary would wipe out all the document signal.",4.2 Components in detail,[0],[0]
"We add the following regularization term
Ωtr = λtr||Wtr",4.2 Components in detail,[0],[0]
"− I||2F (5)
to discourage significant deviation away from identity I. λtr is a regularization parameter that has to be set separately based on validation performance.",4.2 Components in detail,[0],[0]
"We show an empirical analysis of the impact of this transformation layer in Section 7.
",4.2 Components in detail,[0],[0]
"Primary label classifier As shown in the topright part of Figure 2, the classifier takes in the adjusted document representation as an input and predicts a probability distribution over the possible class labels.",4.2 Components in detail,[0],[0]
The classifier is a feed-forward network with a single hidden layer using ReLU activations and a softmax output layer over the possible class labels.,4.2 Components in detail,[0],[0]
Note that we train only one label classifier that is shared by both aspects.,4.2 Components in detail,[0],[0]
The classifier operates the same regardless of the aspect to which the document was encoded.,4.2 Components in detail,[0],[0]
"It must therefore be cooperatively learned together with the encodings.
",4.2 Components in detail,[0],[0]
Let p̂l;k denote the predicted probability of class k for document dl when the document is encoded from the point of view of the source aspect.,4.2 Components in detail,[0],[0]
"Recall that [ysl;1, . . .",4.2 Components in detail,[0],[0]
", y s l;m] is a one-hot vector for the correct
(given) source class label for document dl, hence also a distribution.",4.2 Components in detail,[0],[0]
"We use the cross-entropy loss for the label classifier
Llab = ∑ l∈L
[ −
m∑ k=1 ysl;k log p̂l;k
] (6)
Domain classifier As shown in the bottomright part of Figure 2, the domain classifier functions as an adversary to ensure that the documents encoded with respect to the source and target aspects look the same as sets of examples.",4.2 Components in detail,[0],[0]
The invariance is achieved when the domain classifier (as the adversary) fails to distinguish between the two.,4.2 Components in detail,[0],[0]
"Structurally, the domain classifier is a feed-forward network with a single ReLU hidden layer and a softmax output layer over the two aspect labels.
",4.2 Components in detail,[0],[0]
Let ya =,4.2 Components in detail,[0],[0]
"[ya1 , y a 2 ] denote the one-hot domain label vector for aspect a ∈ {s, t}.",4.2 Components in detail,[0],[0]
"In other words, ys =",4.2 Components in detail,[0],[0]
"[1, 0] and yt =",4.2 Components in detail,[0],[0]
"[0, 1].",4.2 Components in detail,[0],[0]
"We use q̂k(x tr,a l ) as the predicted probability that the domain label is k when the domain classifier receives xtr,al as the input.",4.2 Components in detail,[0],[0]
"The domain classifier is trained to minimize
Ldom = ∑
l∈L∪U ∑ a∈{s,t}
[ −
2∑ k=1 yak log q̂k(x tr,a l ) ]",4.2 Components in detail,[0],[0]
(7),4.2 Components in detail,[0],[0]
"We combine the individual component losses pertaining to word reconstruction, relevance labels, transformation layer regularization, source class labels, and domain adversary into an overall objective function
Lall = Lrec + Lrel + Ωtr + Llab − ρLdom (8)
which is minimized with respect to the model parameters except for the adversary (domain classifier).",4.3 Joint learning,[0],[0]
The adversary is maximizing the same objective with respect to its own parameters.,4.3 Joint learning,[0],[0]
The last term −ρLdom corresponds to the objective of causing the domain classifier to fail.,4.3 Joint learning,[0],[0]
"The proportionality constant ρ controls the impact of gradients from the adversary on the document representation; the adversary itself is always directly minimizing Ldom.
",4.3 Joint learning,[0],[0]
All the parameters are optimized jointly using standard backpropagation (concurrent for the adversary).,4.3 Joint learning,[0],[0]
"Each mini-batch is balanced by aspect, half
coming from the source, the other half from the target.",4.3 Joint learning,[0],[0]
All the loss functions except Llab make use of both labeled and unlabeled documents.,4.3 Joint learning,[0],[0]
"Additionally, it would be straightforward to add a loss term for target labels if they are available.",4.3 Joint learning,[0],[0]
"Pathology dataset This dataset contains 96.6k breast pathology reports collected from three hospitals (Yala et al., 2016).",5 Experimental Setup,[0],[0]
"A portion of this dataset is manually annotated with 20 categorical values, representing various aspects of breast disease.",5 Experimental Setup,[0],[0]
"In our experiments, we focus on four aspects related to carcinomas and atypias:",5 Experimental Setup,[0],[0]
"Ductal Carcinoma InSitu (DCIS), Lobular Carcinoma In-Situ (LCIS), Invasive Ductal Carcinoma (IDC) and Atypical Lobular Hyperplasia (ALH).",5 Experimental Setup,[0],[0]
Each aspect is annotated using binary labels.,5 Experimental Setup,[0],[0]
"We use 500 held out reports as our test set and use the rest of the labeled data as our training set: 23.8k reports for DCIS, 10.7k for LCIS, 22.9k for IDC, and 9.2k for ALH.",5 Experimental Setup,[0],[0]
"Table 1 summarizes statistics of the dataset.
",5 Experimental Setup,[0],[0]
We explore the adaptation problem from one aspect to another.,5 Experimental Setup,[0],[0]
"For example, we want to train a model on annotations of DCIS and apply it on LCIS.",5 Experimental Setup,[0],[0]
"For each aspect, we use up to three common names
as a source of supervision for learning the relevance scorer, as illustrated in Table 2.",5 Experimental Setup,[0],[0]
Note that the provided list is by no means exhaustive.,5 Experimental Setup,[0],[0]
"In fact Buckley et al. (2012) provide example of 60 different verbalizations of LCIS, not counting negations.
",5 Experimental Setup,[0],[0]
Review dataset Our second experiment is based on a domain transfer of sentiment classification.,5 Experimental Setup,[0],[0]
"For the source domain, we use the hotel review dataset introduced in previous work (Wang et al., 2010; Wang et al., 2011), and for the target domain, we use the restaurant review dataset from Yelp.5 Both datasets have ratings on a scale of 1 to 5 stars.",5 Experimental Setup,[0],[0]
"Following previous work (Blitzer et al., 2007), we label reviews with ratings > 3 as positive and those with ratings < 3 as negative, discarding the rest.",5 Experimental Setup,[0],[0]
"The hotel dataset includes a total of around 200k reviews collected from TripAdvisor,6 so we split 100k as labeled and the other 100k as unlabeled data.",5 Experimental Setup,[0],[0]
We randomly select 200k restaurant reviews as the unlabeled data in the target domain.,5 Experimental Setup,[0],[0]
Our test set consists of 2k reviews.,5 Experimental Setup,[0],[0]
"Table 1 summarizes the statistics of the review dataset.
",5 Experimental Setup,[0],[0]
"The hotel reviews naturally have ratings for six aspects, including value, room quality, checkin service, room service, cleanliness and location.",5 Experimental Setup,[0],[0]
We use the first five aspects because the sixth aspect location has positive labels for over 95% of the reviews and thus the trained model will suffer from the lack of negative examples.,5 Experimental Setup,[0],[0]
"The restaurant reviews, however, only have single ratings for an overall impression.",5 Experimental Setup,[0],[0]
"Therefore, we explore the task of adaptation from each of the five hotel aspects to the restaurant domain.",5 Experimental Setup,[0],[0]
The hotel reviews dataset also provides a total of 280 keywords for different aspects that are generated by the bootstrapping method used in Wang et al. (2010).,5 Experimental Setup,[0],[0]
"We use those keywords as supervision for learning the relevance scorer.
",5 Experimental Setup,[0],[0]
Baselines and our method We first compare against a linear SVM trained on the raw bagof-words representation of labeled data in source.,5 Experimental Setup,[0],[0]
"Second, we compare against our SourceOnly model that assumes no target domain data or keywords.",5 Experimental Setup,[0],[0]
It thus has no adversarial training or target aspect-relevance scoring.,5 Experimental Setup,[0],[0]
"Next we compare
5The restaurant portion of https://www.yelp.com/ dataset_challenge.
6https://www.tripadvisor.com/
with marginalized Stacked Denoising Autoencoders (mSDA) (Chen et al., 2012), a domain adaptation algorithm that outperforms both prior deep learning and shallow learning approaches.7
In the rest part of the paper, we name our method and its variants as AAN (Aspect-augmented Adversarial Networks).",5 Experimental Setup,[0],[0]
We compare against AANNA and AAN-NR that are our model variants without adversarial training and without aspectrelevance scoring respectively.,5 Experimental Setup,[0],[0]
Finally we include supervised models trained on the full set of In-Domain annotations as the performance upper bound.,5 Experimental Setup,[0],[0]
Table 3 summarizes the usage of labeled and unlabeled data in each domain as well as keyword rules by our model (AAN-Full) and different baselines.,5 Experimental Setup,[0],[0]
"Note that our model assumes the same set of data as the AAN-NA, AAN-NR and mSDA methods.
",5 Experimental Setup,[0],[0]
Implementation details,5 Experimental Setup,[0],[0]
"Following prior work (Ganin and Lempitsky, 2014), we gradually increase the adversarial strength ρ and decay the learning rate during training.",5 Experimental Setup,[0],[0]
"We also apply batch normalization (Ioffe and Szegedy, 2015) on the sentence encoder and apply dropout with ratio 0.2 on word embeddings and each hidden layer activation.",5 Experimental Setup,[0],[0]
"We set the hidden layer size to 150 and pick the transformation regularization weight λt = 0.1 for the pathol-
7We use the publicly available implementation provided by the authors at http://www.cse.wustl.edu/˜mchen/ code/mSDA.tar.",5 Experimental Setup,[0],[0]
"We use the hyper-parameters from the authors and their models have more parameters than ours.
ogy dataset and λt = 10.0 for the review dataset.",5 Experimental Setup,[0],[0]
"Table 4 summarizes the classification accuracy of different methods on the pathology dataset, including the results of twelve adaptation tasks.",6 Main Results,[0],[0]
Our full model (AAN-Full) consistently achieves the best performance on each task compared with other baselines and model variants.,6 Main Results,[0],[0]
"It is not surprising that SVM and mSDA perform poorly on this dataset because they only predict labels based on an overall feature representation of the input, and do not utilize weak supervision provided by aspect-specific keywords.",6 Main Results,[0],[0]
"As a reference, we also provide a performance upper bound by training our model on the full labeled set in the target domain, denoted as InDomain in the last column of Table 4.",6 Main Results,[0],[0]
"On average, the accuracy of our model (AAN-Full) is only 5.7% behind this upper bound.
",6 Main Results,[0],[0]
"Table 5 shows the adaptation results from each aspect in the hotel reviews to the overall ratings of
restaurant reviews.",6 Main Results,[0],[0]
"AAN-Full and AAN-NR are the two best performing systems on this review dataset, attaining around 5% improvement over the mSDA baseline.",6 Main Results,[0],[0]
"Below, we summarize our findings when comparing the full model with the two model variants AAN-NA and AAN-NR.
Impact of adversarial training We first focus on comparisons between AAN-Full and AAN-NA.",6 Main Results,[0],[0]
The only difference between the two models is that AAN-NA has no adversarial training.,6 Main Results,[0],[0]
"On the pathology dataset, our model significantly outperforms AAN-NA, yielding a 20.2% absolute average gain (see Table 4).",6 Main Results,[0],[0]
"On the review dataset, our model obtains 2.5% average improvement over AAN-NA.",6 Main Results,[0],[0]
"As shown in Table 5, the gains are more significant when training on room and checkin aspects, reaching 6.9% and 4.5%, respectively.
",6 Main Results,[0],[0]
"Impact of relevance scoring As shown in Table 4, the relevance scoring component plays a crucial role in classification on the pathology dataset.
0.0
w/o reconstruction with reconstruction
Our model achieves more than 27% improvement over AAN-NR.",6 Main Results,[0],[0]
This is because in general aspects have zero correlations to each other in pathology reports.,6 Main Results,[0],[0]
"Therefore, it is essential for the model to have the capacity of distinguishing across different aspects in order to succeed in this task.
",6 Main Results,[0],[0]
"On the review dataset, however, we observe that relevance scoring has no significant impact on performance.",6 Main Results,[0],[0]
"On average, AAN-NR actually outperforms AAN-Full by 0.9%.",6 Main Results,[0],[0]
This observation can be explained by the fact that different aspects in hotel reviews are highly correlated to each other.,6 Main Results,[0],[0]
"For example, the correlation between room quality and cleanliness is 0.81, much higher than aspect correlations in the pathology dataset.",6 Main Results,[0],[0]
"In other words, the sentiment is typically consistent across all sentences in a review, so that selecting aspect-specific sentences becomes unnecessary.",6 Main Results,[0],[0]
"Moreover, our supervision for the relevance scorer is weak and noisy because the aspect keywords are obtained in a semiautomatic way.",6 Main Results,[0],[0]
"Therefore, it is not surprising that AAN-NR sometimes delivers a better classification
accuracy than AAN-Full.",6 Main Results,[0],[0]
Impact of the reconstruction loss Table 6 summarizes the impact of the reconstruction loss on the model performance.,7 Analysis,[0],[0]
"For our full model (AANFull), adding the reconstruction loss yields an average of 5.0% gain on the pathology dataset and 5.6% on the review dataset.
",7 Analysis,[0],[0]
Restaurant Reviews • the fries were undercooked and thrown haphazardly into the sauce holder .,7 Analysis,[0],[0]
the shrimp was over cooked and just deepfried .,7 Analysis,[0],[0]
… even the water tasted weird .,7 Analysis,[0],[0]
"…
• i had the shrimp boil and it was very underseasoned .",7 Analysis,[0],[0]
much closer to bland than anything .,7 Analysis,[0],[0]
"…
• the room was old .",7 Analysis,[0],[0]
… we did n’t like the night shows at all .,7 Analysis,[0],[0]
"…
• however , the decor was just fair .",7 Analysis,[0],[0]
"… the doorknob to our bathroom door fell off , as well as the handle on the toilet .",7 Analysis,[0],[0]
… in the second bedroom it literally rained water from above .,7 Analysis,[0],[0]
• the room decor was not entirely modern .,7 Analysis,[0],[0]
"… we just had the run of the mill hotel room without a view .
",7 Analysis,[0],[0]
"• stay away from fresh vegetable like lettuce , etc .",7 Analysis,[0],[0]
"…
• rest room in this restaurant is very dirty .",7 Analysis,[0],[0]
"… • the only problem i had was that … i was very ill with what was suspected to be food poison • probably the noisiest room he could have given us in the whole hotel .
",7 Analysis,[0],[0]
"Nearest Hotel Reviews by Ours-Full Nearest Hotel Reviews by Ours-NA
To analyze the reasons behind this difference, consider Figure 4 that shows the heat maps of the learned document representations on the review dataset.",7 Analysis,[0],[0]
The top half of the matrices corresponds to input documents from the source domain and the bottom half corresponds to the target domain.,7 Analysis,[0],[0]
"Unlike the first matrix, the other two matrices have no significant difference between the two halves, indicating that adversarial training helps learning of domain-invariant representations.",7 Analysis,[0],[0]
"However, adversarial training also removes a lot of information from representations, as the second matrix is much more sparse than the first one.",7 Analysis,[0],[0]
The third matrix shows that adding reconstruction loss effectively addresses this sparsity issue.,7 Analysis,[0],[0]
Almost 85% entries of the second matrix have small values (< 10−6) while the sparsity is only about 30% for the third one.,7 Analysis,[0],[0]
"Moreover, the standard deviation of the third matrix is also ten times higher than the second one.",7 Analysis,[0],[0]
These comparisons demonstrate that the reconstruction loss function improves both the richness and diversity of the learned representations.,7 Analysis,[0],[0]
"Note that in the case of no adversarial training (AAN-NA), adding the reconstruction component has no clear effect.",7 Analysis,[0],[0]
"This is expected because the main motivation of adding this component is to achieve a more robust adversarial training.
",7 Analysis,[0],[0]
"Regularization on the transformation layer Table 7 shows the averaged accuracy with differ-
ent regularization weights λt in Equation 5.",7 Analysis,[0],[0]
We change λt to reflect different model variants.,7 Analysis,[0],[0]
"First, λt = ∞ corresponds to the removal of the transformation layer because the transformation is always identity in this case.",7 Analysis,[0],[0]
"Our model performs better than this variant on both datasets, yielding an average improvement of 9.8% on the pathology dataset and 2.1% on the review dataset.",7 Analysis,[0],[0]
This result indicates the importance of adding the transformation layer.,7 Analysis,[0],[0]
"Second, using zero regularization (λt = 0) also consistently results in inferior performance, such as 13.8% loss on the pathology dataset.",7 Analysis,[0],[0]
We hypothesize that zero regularization will dilute the effect from reconstruction because there is too much flexibility in transformation.,7 Analysis,[0],[0]
"As a result, the transformed representation will become sparse due to the adversarial training, leading to a performance loss.
",7 Analysis,[0],[0]
"Examples of neighboring reviews Finally, we illustrate in Figure 5 a case study on the characteristics of learned abstract representations by different models.",7 Analysis,[0],[0]
The first column shows an example restaurant review.,7 Analysis,[0],[0]
"Sentiment phrases in this example are mostly food-specific, such as “undercooked” and “tasted weird”.",7 Analysis,[0],[0]
"In the other two columns, we show example hotel reviews that are nearest neighbors to the restaurant reviews, measured by cosine similarity between their representations.",7 Analysis,[0],[0]
"In column 2, many sentiment phrases are specific for room quality, such as “old” and “rained water from above”.",7 Analysis,[0],[0]
"In column 3, however, most sentiment phrases are either common sentiment expressions (e.g. dirty) or food-related (e.g. food poison), even though the focus of the reviews is based on the room quality of hotels.",7 Analysis,[0],[0]
"This observation indicates that adversarial training (AAN-Full) successfully learns to eliminate domain-specific information and to map those domain-specific words into similar domain-invariant
representations.",7 Analysis,[0],[0]
"In contrast, AAN-NA only captures domain-invariant features from phrases that commonly present in both domains.
",7 Analysis,[0],[0]
"Impact of keyword rules Finally, Figure 6 shows the accuracy of our full model (y-axis) when trained with various amount of keyword rules for relevance learning (x-axis).",7 Analysis,[0],[0]
"As expected, the transfer accuracy drops significantly when using fewer rules on the pathology dataset (LCIS as source and ALH as target).",7 Analysis,[0],[0]
"In contrary, the accuracy on the review dataset (hotel service as source and restaurant as target) is not sensitive to the amount of used relevance rules.",7 Analysis,[0],[0]
This can be explained by the observation from Table 5 that the model without relevance scoring performs equally well as the full model due to the tight dependence in aspect labels.,7 Analysis,[0],[0]
"In this paper, we propose a novel aspect-augmented adversarial network for cross-aspect and crossdomain adaptation tasks.",8 Conclusions,[0],[0]
"Experimental results demonstrate that our approach successfully learns invariant representation from aspect-relevant fragments, yielding significant improvement over the mSDA baseline and our model variants.",8 Conclusions,[0],[0]
"The effectiveness of our approach suggests the potential application of adversarial networks to a broader range of NLP tasks for improved representation learning, such as machine translation and language generation.",8 Conclusions,[0],[0]
The authors acknowledge the support of the U.S. Army Research Office under grant number W911NF-10-1-0533.,Acknowledgments,[0],[0]
We thank the MIT NLP group and the TACL reviewers for their comments.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.",Acknowledgments,[0],[0]
We introduce a neural method for transfer learning between two (source and target) classification tasks or aspects over the same domain.,abstractText,[0],[0]
"Rather than training on target labels, we use a few keywords pertaining to source and target aspects indicating sentence relevance instead of document class labels.",abstractText,[0],[0]
Documents are encoded by learning to embed and softly select relevant sentences in an aspect-dependent manner.,abstractText,[0],[0]
"A shared classifier is trained on the source encoded documents and labels, and applied to target encoded documents.",abstractText,[0],[0]
"We ensure transfer through aspect-adversarial training so that encoded documents are, as sets, aspect-invariant.",abstractText,[0],[0]
"Experimental results demonstrate that our approach outperforms different baselines and model variants on two datasets, yielding an improvement of 27% on a pathology dataset and 5% on a review dataset.1",abstractText,[0],[0]
Aspect-augmented Adversarial Networks for Domain Adaptation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 115–124 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
115",text,[0],[0]
"Representing the meaning of a word is a prerequisite to solve many linguistic and non-linguistic problems, such as retrieving words with the same meaning, finding the most relevant images or sounds of a word and so on.",1 Introduction,[0],[0]
"In recent years we have seen a surge of interest in building computational models that represent word meanings from patterns of word co-occurrence in corpora (Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014; Clark, 2015; Wang et al., 2018b).",1 Introduction,[0],[0]
"However, word meaning is also tied to the physical world.",1 Introduction,[0],[0]
"Many behavioral studies suggest that human semantic representation is grounded in the external environment and sensorimotor experience (Landau et al., 1998; Barsalou, 2008).",1 Introduction,[0],[0]
"This has led to the development of multimodal representation models that utilize both textual and perceptual information (e.g., images, sounds).
",1 Introduction,[0],[0]
"As evidenced by a range of evaluations (Andrews et al., 2009; Bruni et al., 2014; Silberer
et al., 2016), multimodal models can learn better semantic word representations (a.k.a. embeddings) than text-based models.",1 Introduction,[0],[0]
"However, most existing models still have a number of drawbacks.",1 Introduction,[0],[0]
"First, they ignore the associations between modalities, and thus lack the ability of information transferring between modalities.",1 Introduction,[0],[0]
Consequently they cannot handle words without perceptual information.,1 Introduction,[0],[0]
"Second, they integrate textual and perceptual representations with simple concatenation, which is insufficient to effectively fuse information from various modalities.",1 Introduction,[0],[0]
"Third, they typically treat the representations from different modalities equally.",1 Introduction,[0],[0]
"This is inconsistent with many psychological findings that information from different modalities contributes differently to the meaning of words (Paivio, 1990; Anderson et al., 2017).
",1 Introduction,[0],[0]
"In this work, we introduce the associative multichannel autoencoder (AMA), a novel multimodal word representation model that addresses all the above issues.",1 Introduction,[0],[0]
"Our model is built upon the stacked autoencoder (Bengio et al., 2007) to learn semantic representations by integrating textual and perceptual inputs.",1 Introduction,[0],[0]
"Inspired by the re-constructive and associative nature of human memory, we propose two associative memory modules as extensions.",1 Introduction,[0],[0]
"One is to learn associations between modalities (e.g., associations between textual and visual features), so as to reconstruct corresponding perceptual information of concepts.",1 Introduction,[0],[0]
"The other is to learn associations between related concepts, by reconstructing embeddings of both target words and their associated words.",1 Introduction,[0],[0]
"Furthermore, we propose a gating mechanism to learn the importance weights of different modalities to each word.
",1 Introduction,[0],[0]
"To summarize, our main contributions in this work are two-fold:
• We present a novel associative multichannel autoencoder for multimodal word representation, which is capable of utilizing associations between different modalities and related
concepts, and assigning different importance weights to each modality according to different words.",1 Introduction,[0],[0]
"Results on six standard benchmarks demonstrate that our methods outperform strong unimodal baselines and state-ofthe-art multimodal models.
",1 Introduction,[0],[0]
"• Our model successfully integrates cognitive insights of the re-constructive and associative nature of semantic memory in humans, suggesting that rich information contained in human cognitive processing can be used to enhance NLP models.",1 Introduction,[0],[0]
"Furthermore, our results shed light on the fundamental questions of how to learn semantic representations, such as the plausibility of reconstructing perceptual information, associating related concepts and grounding word symbols to external environment.",1 Introduction,[0],[0]
"A large body of research evidences that human semantic memory is inherently re-constructive and associative (Collins and Loftus, 1975; Anderson and Bower, 2014).",2.1 Cognitive Grounding,[0],[0]
"That is, memories are not exact static copies of reality, but are rather reconstructed from their stimuli and associated concepts each time they are retrieved.",2.1 Cognitive Grounding,[0],[0]
"For example, when we see a dog, not only the concept itself, but also the corresponding perceptual information and associated words will be jointly activated and reconstructed.",2.1 Cognitive Grounding,[0],[0]
"Moreover, various theories state that the different sources of information contribute differently to the semantic representation of a concept (Wang et al., 2010; Ralph et al., 2017).",2.1 Cognitive Grounding,[0],[0]
"For instance, Dual Coding Theory (Hiscock, 1974) posits that concrete words are represented in the brain in terms of a perceptual and linguistic code, whereas abstract words are encoded only in the linguistic modality.
",2.1 Cognitive Grounding,[0],[0]
"In these respects, our method employs a retrieval and representation process analogous to that of humans, in which the retrieval of perceptual information and associated words is triggered and mediated by a linguistic input.",2.1 Cognitive Grounding,[0],[0]
The learned cross-modality mapping and reconstruction of associated words are inspired by the human mental model of associations between different modalities and related concepts.,2.1 Cognitive Grounding,[0],[0]
"Moreover, word meaning is tied to both linguistic and physical environment, and relies differently on each modality in-
puts (Wang et al., 2018a).",2.1 Cognitive Grounding,[0],[0]
These are also captured by our multimodal representation model.,2.1 Cognitive Grounding,[0],[0]
The existing multimodal representation models can be generally classified into two groups: 1) Jointly training models build multimodal representations with raw inputs of textual and perceptual resources.,2.2 Multimodal Models,[0],[0]
2),2.2 Multimodal Models,[0],[0]
Separate training models independently learn textual and perceptual representations and integrate them afterwards.,2.2 Multimodal Models,[0],[0]
"A class of models extends Latent Dirichlet Allocation (Blei et al., 2003) to jointly learn topic distributions from words and perceptual units (Andrews et al., 2009; Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013).",2.2.1 Jointly training models,[0],[0]
"Recently introduced work is an extension of the Skip-gram model (Mikolov et al., 2013).",2.2.1 Jointly training models,[0],[0]
"For instance, Hill and Korhonen (2014) propose a corpus fusion method that inserts the perceptual features of concepts in the training corpus, which is then used to train the Skip-gram model.",2.2.1 Jointly training models,[0],[0]
"Lazaridou et al. (2015) propose MMSkip model, which injects visual information in the process of learning textual representations by adding a max-margin objective to minimize the distance between textual and visual vectors.",2.2.1 Jointly training models,[0],[0]
"Kiela and Clark (2015) adopt the MMSkip to learn multimodal vectors with auditory perceptual inputs.
",2.2.1 Jointly training models,[0],[0]
These methods can implicitly propagate perceptual information to word representations and at the same time learn multimodal representations.,2.2.1 Jointly training models,[0],[0]
"However, they utilize raw text corpus in which words having perceptual information account for a small portion.",2.2.1 Jointly training models,[0],[0]
This weakens the effect of introducing perceptual information and consequently leads to the slight improvement of textual vectors.,2.2.1 Jointly training models,[0],[0]
The simplest approach is concatenation which fuses textual and visual vectors by concatenating them.,2.2.2 Separate training models,[0],[0]
"It has been proven to be effective in learning multimodal representations (Bruni et al., 2014; Hill et al., 2014; Collell et al., 2017).",2.2.2 Separate training models,[0],[0]
"Variations of this method employ transformation and dimension reduction on the concatenation result, including application of singular value decomposition (SVD) (Bruni et al., 2014) or canonical correlation analysis (CCA) (Hill et al., 2014).",2.2.2 Separate training models,[0],[0]
"There is also work using deep learning methods to project different modality inputs into a common
space, including restricted Boltzman machines (Ngiam et al., 2011; Srivastava and Salakhutdinov, 2012), autoencoders (Silberer and Lapata, 2014; Silberer et al., 2016), and recursive neural networks (Socher et al., 2013).",2.2.2 Separate training models,[0],[0]
"However, the above methods can only generate multimodal vectors of those words that have perceptual information, thus reducing multimodal vocabulary drastically.
",2.2.2 Separate training models,[0],[0]
An empirically superior model addresses this problem by predicting missing perceptual information firstly.,2.2.2 Separate training models,[0],[0]
"This includes Hill et al. (2014) who utilize the ridge regression method to learn a mapping matrix from textual modality to visual modality, and Collell et al. (2017) who employ a feedforward neural network to learn the mapping relation between textual vectors and visual vectors.",2.2.2 Separate training models,[0],[0]
"Applying the mapping function on textual representations, they obtain the predicted visual vectors for all words in textual vocabulary.",2.2.2 Separate training models,[0],[0]
Then they calculate multimodal representations by concatenating textual and predicted visual vectors.,2.2.2 Separate training models,[0],[0]
"However, the above methods learn separate mapping functions and fusion models, which are somewhat inelegant.",2.2.2 Separate training models,[0],[0]
"In this paper we employ a neural-network mapping function to integrate these two processes into a unified multimodal models.
",2.2.2 Separate training models,[0],[0]
"According to this classification, our method falls into the second group.",2.2.2 Separate training models,[0],[0]
"However, existing models ignore either the associative relations among modalities, associative relations among relative words, or the different contributions of each modality.",2.2.2 Separate training models,[0],[0]
This paper aims to integrate more perceptual information and the human-like associative memory into a unified multimodal model to learn better word representations.,2.2.2 Separate training models,[0],[0]
We first provide a brief description of the basic multichannel autoencoder for learning multimodal word representations (Figure 1).,3 Associative Multichannel Autoencoder,[0],[0]
Then we extend the model with two associative memory modules and a gating mechanism (Figure 2) in the next sections.,3 Associative Multichannel Autoencoder,[0],[0]
"An autoencoder is an unsupervised neural network which is trained to reconstruct a given input from its latent representation (Bengio, 2009).",3.1 Basic Mutichannel Autoencoder,[0],[0]
"In this work, we propose a variant of autoencoder called multichannel autoencoder, which maps multimodal inputs into a common space.
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"Our model extends the unimodal and bimodal autoencoder (Ngiam et al., 2011; Silberer and Lapata, 2014) to induce semantic representations integrating textual, visual and auditory information.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"As shown in Figure 1, our model first transforms input textual vector xt, visual vector xv and auditory vector xa to hidden representations:
ht = g(Wtxt +",3.1 Basic Mutichannel Autoencoder,[0],[0]
"bt)
hv",3.1 Basic Mutichannel Autoencoder,[0],[0]
"= g(Wvxv + bv)
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"ha = g(Waxa + ba).
",3.1 Basic Mutichannel Autoencoder,[0],[0]
"(1)
Then the hidden representations are concatenated together and mapped to a common space:
hm = g(Wm[ht;hv;ha",3.1 Basic Mutichannel Autoencoder,[0],[0]
] + bm).,3.1 Basic Mutichannel Autoencoder,[0],[0]
"(2)
The model is trained to reconstruct the hidden representations of the three modalities from the multimodal representation hm:
[ĥt; ĥv; ĥa] = g(W ′mhm + bm̂), (3)
and finally to reconstruct the original embeddings of textual, visual and auditory inputs:
x̂t = g(W ′t ĥt + bt̂)",3.1 Basic Mutichannel Autoencoder,[0],[0]
x̂v,3.1 Basic Mutichannel Autoencoder,[0],[0]
= g(W ′vĥv + bv̂) x̂a,3.1 Basic Mutichannel Autoencoder,[0],[0]
"= g(W ′aĥa + bâ),
(4)
where x̂t, x̂v, x̂a are the reconstruction of input vectors xt, xv, xa, and ĥt, ĥv, ĥa
are the reconstruction of hidden representations ht, hv, ha.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"The learning parameters {Wt,Wv,Wa,W ′t ,W ′v,W ′a,Wm,W ′m} are weight matrices, {bt, bv, ba, bt̂, bv̂, bâ, bm, bm̂} are bias vectors.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"Here [· ; ·] denotes the vector concatenation, and g denotes the non-linear function which we use tanh(·).
",3.1 Basic Mutichannel Autoencoder,[0],[0]
Training a single-layer autoencoder corresponds to optimizing the learning parameters to minimize the overall loss between inputs and their reconstructions.,3.1 Basic Mutichannel Autoencoder,[0],[0]
"Following (Vincent et al., 2010), we use squared loss:
min θ1 n∑ i=1",3.1 Basic Mutichannel Autoencoder,[0],[0]
"(||xit− x̂it||2+ ||xiv− x̂iv||2+ ||xia− x̂ia||2), (5) where i denotes the ith word, and the model parameters are θ1 = {Wt,Wv,Wa,Wm,W ′t ,W ′v, W ′a,W ′",3.1 Basic Mutichannel Autoencoder,[0],[0]
"m, bt, bv, ba, bm, bt̂, bv̂, bâ, bm̂}.
",3.1 Basic Mutichannel Autoencoder,[0],[0]
Autoencoders can be stacked to create deep networks.,3.1 Basic Mutichannel Autoencoder,[0],[0]
"To enhance the quality of semantic representations, we employ a stacked multichannel autoencoder, which is composed of multiple hidden layers that are stacked together.",3.1 Basic Mutichannel Autoencoder,[0],[0]
"In reality, the words that have corresponding images or sounds are only a small subset of the textual vocabulary.",3.2 Integrating Modality Associations,[0],[0]
"To obtain the perceptual vectors for each word, we need associations between modalities (i.e., text-to-vision and text-to-audition mapping functions), that transform the textual vectors into visual and auditory ones.",3.2 Integrating Modality Associations,[0],[0]
"Previous methods learn separate mapping functions and fusion models, which are somewhat inelegant.",3.2 Integrating Modality Associations,[0],[0]
"Here we employ a neural-network mapping function to incorporate this modality association module into multimodal models.
",3.2 Integrating Modality Associations,[0],[0]
Take text-to-vision mapping as an example.,3.2 Integrating Modality Associations,[0],[0]
"Suppose that T ∈ Rmt×nt is the textual representation containing mt words, V ∈ Rmv×nv is the visual representation containing mv ( mt) words, where nt and nv are dimensions of the textual and visual representations respectively.",3.2 Integrating Modality Associations,[0],[0]
The textual and visual representations of the ith concept are denoted as Ti and Vi respectively.,3.2 Integrating Modality Associations,[0],[0]
Our goal is to learn a mapping function f : g(WpT + bp) from textual to visual space such that the prediction f(Ti) is similar to the actual visual vector Vi.,3.2 Integrating Modality Associations,[0],[0]
"The set of visual representations along with their corresponding textual representations
image2vec
...
word2vec sound2vec
...
...",3.2 Integrating Modality Associations,[0],[0]
"Multimodal representations
dog
... ...",3.2 Integrating Modality Associations,[0],[0]
"......
.........
...... ...",3.2 Integrating Modality Associations,[0],[0]
"...... ...
... ...",3.2 Integrating Modality Associations,[0],[0]
"...
",3.2 Integrating Modality Associations,[0],[0]
"In case you need, we've collected the cutest small dog breeds to lif t your
mood.
",3.2 Integrating Modality Associations,[0],[0]
"There's nothing that cheers you up quite as fast as a cute dog doing something peculiar.
are used to learn the mapping function.",3.2 Integrating Modality Associations,[0],[0]
"To train the model, we employ a square loss:
min θ2 mv∑ i=1 ||f(Ti)− Vi||2, (6)
where the training parameters are θ2 = {Wp, bp}.",3.2 Integrating Modality Associations,[0],[0]
We adopt the same method to learn the text-toaudition mapping function.,3.2 Integrating Modality Associations,[0],[0]
Word associations are a proxy for an aspect of human semantic memory that is not sufficiently captured by the usual training objectives of multimodal models.,3.3 Integrating Word Associations,[0],[0]
Therefore we assume that incorporating the objective of word associations helps to learn better semantic representations.,3.3 Integrating Word Associations,[0],[0]
"To achieve this, we propose to reconstruct the vector of associated word from the corresponding multimodal semantic representation.",3.3 Integrating Word Associations,[0],[0]
"Specifically, in the decoding process we change the equation (3) to:
[ĥt, ĥv, ĥa, ĥasc] = g(W ′mhm + bm̂), (7)
and equation (4) to:
x̂t = g(W ′t ĥt + bt̂) x̂v",3.3 Integrating Word Associations,[0],[0]
= g(W ′vĥv + bv̂) x̂a,3.3 Integrating Word Associations,[0],[0]
"= g(W ′aĥa + bâ)
",3.3 Integrating Word Associations,[0],[0]
x̂asc =,3.3 Integrating Word Associations,[0],[0]
"g(Wascĥasc + basc).
(8)
To train the model, we add an additional objective function, which is the mean square error
between the embeddings of the associated word y and their re-constructive embeddings",3.3 Integrating Word Associations,[0],[0]
"x̂asc:
min θ3 n∑ i=1",3.3 Integrating Word Associations,[0],[0]
"||yi − x̂iasc||2, (9)
where yi and xi are the embeddings of a pair of associated words.",3.3 Integrating Word Associations,[0],[0]
"Here, y is the concatenation of three unimodal vectors [yt; yv; ya].",3.3 Integrating Word Associations,[0],[0]
"The parameters of word association module are θ3 = {Wt,Wv,Wa,Wm, Ŵm,Wasc, bt, bv, ba, bm, bm̂, basc}.",3.3 Integrating Word Associations,[0],[0]
This additional criterion drives the learning towards a semantic representation capable of reconstructing its associated representation.,3.3 Integrating Word Associations,[0],[0]
"Considering that the meaning of each word has different dependencies on textual and perceptual information, we propose the sample-specific gate to assign different weights to each modality according to different words.",3.4 Integrating a Gating Mechanism,[0],[0]
"The weight parameters are calculated by the following feed-forward neural networks:
gt = g(Wgtxt + bgt)
gv = g(Wgvxv + bgv)
",3.4 Integrating a Gating Mechanism,[0],[0]
"ga = g(Wgaxa + bga),
(10)
where gt, gv and ga are value or vector gate of textual, visual and auditory representations respectively.",3.4 Integrating a Gating Mechanism,[0],[0]
"For the value gate, Wgt, Wgv and Wga are vectors, and bgt, bgv and bga are value parameters.",3.4 Integrating a Gating Mechanism,[0],[0]
"For the vector gate, the parameters Wgt, Wgv and Wga are matrices, bgt, bgv and bga are vectors.",3.4 Integrating a Gating Mechanism,[0],[0]
"The value gate controls the importance weights of different input representations as a whole, whereas the vector gate can adjust the importance weights of each dimension of input representations.
",3.4 Integrating a Gating Mechanism,[0],[0]
"Finally, we compute element-wise multiplication of the textual, visual and auditory representations with their corresponding gates:
xgt = xt gt xgv",3.4 Integrating a Gating Mechanism,[0],[0]
"= xv gv xga = xa ga.
(11)
",3.4 Integrating a Gating Mechanism,[0],[0]
"The xgt, xgv and xga can be seen as the weighted textual, visual and auditory representations.",3.4 Integrating a Gating Mechanism,[0],[0]
The parameters of our gating mechanism is trained together with that of the proposed model.,3.4 Integrating a Gating Mechanism,[0],[0]
"To train the AMA model, we use overall objective function of equation (5) + (6) + (9).",3.5 Model Training,[0],[0]
"In the training phase, model inputs are textual vectors, the corresponding visual vectors, auditory vectors, and association words (Figure 2).",3.5 Model Training,[0],[0]
"In the testing phase, we only need textual inputs to generate multimodal word representations.",3.5 Model Training,[0],[0]
Textual vectors.,4.1 Datasets,[0],[0]
"We use 300-dimensional GloVe vectors1 which are trained on the Common Crawl corpus consisting of 840B tokens and a vocabulary of 2.2M words2.
Visual vectors.",4.1 Datasets,[0],[0]
"Our source of visual vectors are collected from ImageNet (Russakovsky et al., 2015) which covers a total of 21,841 WordNet synsets (Fellbaum, 1998) that have 14,197,122 images.",4.1 Datasets,[0],[0]
"For our experiments, we delete words with fewer than 50 images or words not in the Glove vectors, and sample at most 100 images for each word.",4.1 Datasets,[0],[0]
"To generate a visual vector for each word, we use the forward pass of a pre-trained VGGnet model3 and extract the hidden representation of the last layer as the feature vector.",4.1 Datasets,[0],[0]
Then we use averaged feature vectors of the multiple images corresponding to the same word.,4.1 Datasets,[0],[0]
"Finally, we get 8,048 visual vectors of 128 dimensions.
",4.1 Datasets,[0],[0]
Auditory vectors.,4.1 Datasets,[0],[0]
"For auditory data, we gather audio files from Freesound4, in which we select words with more than 10 audio files and sample at most 50 sounds for one word.",4.1 Datasets,[0],[0]
"To extract auditory features, we use the VGG-net model which is pretrained on Audioset5.",4.1 Datasets,[0],[0]
"The final auditory vectors are averaged feature vectors of multiple audios of the same word, which contains 9,988 words of 128 dimensions6.
",4.1 Datasets,[0],[0]
Word associations.,4.1 Datasets,[0],[0]
"We use the word association data collected by (De Deyne et al., 2016), in which each word pair is generated by at least
1http://nlp.stanford.edu/projects/ glove
2We have tried skip-gram vectors and get the same conclusions.
3http://www.vlfeat.org/matconvnet/ 4http://www.freesound.org/ 5https://research.google.com/audioset 6We build auditory vectors with the released code at: https://github.com/tensorflow/models/ tree/master/research/audioset
one subject7.",4.1 Datasets,[0],[0]
"This dataset includes mostly words with similar meaning (e.g., occasionally & sometimes, adored & loved, supervisor & boss) and related words (e.g., eruption & volcano, cortex & brain, umbrella & rain).",4.1 Datasets,[0],[0]
We calculate the association score for each word pair (cue word + target word) as: the number of person who generated the word pair divided by the total number of people who were presented with the cue word.,4.1 Datasets,[0],[0]
"For training, we select pairs of associated words above a threshold of 0.15 and delete those that are not in the Glove vocabulary, which results in 7,674 word association data sets8.",4.1 Datasets,[0],[0]
"For the development set, we randomly sample 5,000 word association collections together with their association scores.",4.1 Datasets,[0],[0]
"Our models are implemented with PyTorch (Paszke et al., 2017), optimized with Adam (Kingma and Ba, 2014).",4.2 Model Settings,[0],[0]
"We set the initial learning rate to 0.05, and batch size to 64.",4.2 Model Settings,[0],[0]
"We tune the number of layers over 1, 2, 3, the size of multimodal vectors over 100, 200, 300, and the size of each layer in textual channel over 300, 250, 200, 150, 100 and in visual/auditory channel over 128, 120, 90, 60.",4.2 Model Settings,[0],[0]
We train the model for 500 epochs and select the best parameters on the development set.,4.2 Model Settings,[0],[0]
"All models are trained for 3 times and the average results are reported in Table 1.
",4.2 Model Settings,[0],[0]
"To test the effect of each module, we separately train the following models: multichannel autoencoder with modality association (AMAM), with modality and word associations (AMAMW), with modality and word associations plus value/vector gate (AMA-MW-Gval/vec).
",4.2 Model Settings,[0],[0]
"For AMA-M model, we initialize the text-tovision and text-to-audition mapping functions with pre-trained mapping matrices, which are parameters of one-layer feed-forward neural networks.",4.2 Model Settings,[0],[0]
"The network uses input of the textual vectors, output of visual or auditory vectors, and is trained with SGD for 100 epochs.",4.2 Model Settings,[0],[0]
"We initialize the network biases as zeros and network weights with He-initialisation (He et al., 2015).",4.2 Model Settings,[0],[0]
"The best parameters of AMA-M model are 2 hidden layers, with textual channel size of 300, 250 and 150, visual/auditory channel size of 128,
7The dataset can be found at: https:// simondedeyne.me/data.
8We have done experiments with Synonyms (which are extracted from WordNet and PPDB corpora), and the results are not as good as using word associations.
90, 60.",4.2 Model Settings,[0],[0]
"For AMA-MW model, we use the best AMA-M model parameters as initialization, and train the model with word association data.",4.2 Model Settings,[0],[0]
"The optimal parameter of association channel size is 300, 350, 556 (or 428 for bimodal inputs).",4.2 Model Settings,[0],[0]
"For AMA-MW-Gval and AMA-MW-Gvec, we adopt the same training strategy as AMA-MW model.",4.2 Model Settings,[0],[0]
The code for training and evaluation can be found at: https://github.com/wangshaonan/ Associative-multichannel-autoencoder.,4.2 Model Settings,[0],[0]
"We test the baseline and proposed models on six standard evaluation benchmarks, covering two different tasks: (i) Semantic relatedness: Men-3000 (Bruni et al., 2014) and Wordrel-252 (Agirre et al., 2009); (ii) Semantic similarity: Simlex-999 (Hill et al., 2016), Semsim-7576 (Silberer and Lapata, 2014), Wordsim-203 and Simverb-3500 (Gerz et al., 2016).",5.1 Evaluation Tasks,[0],[0]
"All test sets contain a list of word pairs along with their subject ratings.
",5.1 Evaluation Tasks,[0],[0]
We employ Spearman’s correlation method to evaluate the performance of our models.,5.1 Evaluation Tasks,[0],[0]
"This method calculates the correlation coefficients between model predictions and subject ratings, in which the model prediction is the cosine similarity between semantic representations of two words.",5.1 Evaluation Tasks,[0],[0]
Most of existing multimodal models only utilize textual and visual modalities.,5.2 Baseline Multimodal Models,[0],[0]
"For fair comparison, we re-implement several representative systems with our own textual and visual vectors.",5.2 Baseline Multimodal Models,[0],[0]
"The Concatenation (CONC) model (Kiela and Bottou, 2014) is simple concatenation of normalized textual and visual vectors.",5.2 Baseline Multimodal Models,[0],[0]
"The Mapping (Collell et al., 2017) and Ridge (Hill et al., 2014) models first learn a mapping matrix from textual to visual modality using feed-forward neural network and ridge regression respectively.",5.2 Baseline Multimodal Models,[0],[0]
"After applying the mapping function on the textual vectors, they obtain the predicted visual vectors for all words in textual vocabulary.",5.2 Baseline Multimodal Models,[0],[0]
Then they concatenate the normalized textual and predicted visual vectors to get multimodal word representations.,5.2 Baseline Multimodal Models,[0],[0]
"The SVD (Bruni et al., 2014) and CCA (Hill et al., 2014) models first concatenate normalized textual and visual vectors, and then conduct SVD or CCA transformations on the concatenated vectors.
",5.2 Baseline Multimodal Models,[0],[0]
"For multimodal models with textual, visual and
0.58
0.6
0.62
0.64
0.66
100% 80% 60% 40% 20% A ve ra ge S pe ar m an 's co rr el at io ns
Percentage of association data
AMA-M(TV)
AMA-M(TVA)
auditory inputs, we implement CONC and Ridge as baseline models.",5.2 Baseline Multimodal Models,[0],[0]
"The trimodal CONC model simply concatenates normalized textual, visual and auditory vectors.",5.2 Baseline Multimodal Models,[0],[0]
The trimodal Ridge model first learns text-to-vision and text-to-audition mapping matrices with ridge regression method.,5.2 Baseline Multimodal Models,[0],[0]
Then it applies the mapping functions on the textual vectors to get the predicted visual and auditory vectors.,5.2 Baseline Multimodal Models,[0],[0]
"Fi ally, the normalized textual, predictedvisual and predicted-auditory vectors are concatenated to get the multimodal representations.
",5.2 Baseline Multimodal Models,[0],[0]
All above baseline models are implemented with Sklearn9.,5.2 Baseline Multimodal Models,[0],[0]
"Same as the proposed AMA model,
9http://scikit-learn.org/
the hyper-parameters of baseline models are tuned on the development set using Spearman’s correlation method.",5.2 Baseline Multimodal Models,[0],[0]
"In Ridge model, the optimal regularization parameter is 0.6.",5.2 Baseline Multimodal Models,[0],[0]
"The Mapping model is trained with SGD for maximum 100 epochs with early stopping, and the optimal learning rate is 0.001.",5.2 Baseline Multimodal Models,[0],[0]
The output dimension of SVD and CCA models are 300.,5.2 Baseline Multimodal Models,[0],[0]
"As shown in Table 1, we divide all models into six groups: (1) existing multimodal models (with textual and visual inputs) in which results are reprinted from Collell et al. (2017).",5.3 Results and Discussion,[0],[0]
"(2) Unimodal models with textual, (predicted) visual or (pre-
dicted) auditory inputs.",5.3 Results and Discussion,[0],[0]
(3) Our re-implementation of baseline bimodal models with textual and visual inputs (TV).,5.3 Results and Discussion,[0],[0]
(4) Our AMA models with textual and visual inputs.,5.3 Results and Discussion,[0],[0]
"(5) Our implementation of trimodal baseline models with textual, visual and auditory inputs (TVA).",5.3 Results and Discussion,[0],[0]
"(6) Our AMA model with textual, visual and auditory inputs.
",5.3 Results and Discussion,[0],[0]
"Overall performance Our AMA models (in group 4 and 6) clearly outperform their baseline unimodal and multimodal models (in group 2, 3 and 5).",5.3 Results and Discussion,[0],[0]
We use Wilcoxon signed-rank test to check if significant difference exists between two models.,5.3 Results and Discussion,[0],[0]
"Results show that our multimodal models perform significantly better (p < 0.05) than all baseline models.
",5.3 Results and Discussion,[0],[0]
"As shown clearly, our bimodal and trimodal AMA models achieve better performance than baselines in both V/A (visual or auditory, the testing data that have associated visual or auditory vectors) and ZS (zero-shot, the testing data that do not have associated visual or auditory vectors) region.",5.3 Results and Discussion,[0],[0]
"In other words, our models outperform baseline models on words with or without perceptual information.",5.3 Results and Discussion,[0],[0]
"The good results in ZS region also indicate that our models have good generalization capacity.
",5.3 Results and Discussion,[0],[0]
"Unimodal baselines As shown in group 2, the Glove vectors are much better than CNNvisual and CNN-auditory vectors, in which CNNauditory has the worst performance on capturing concept similarities.",5.3 Results and Discussion,[0],[0]
"Comparing with visual and auditory vectors, the predicted visual and auditory vectors achieve much better performance.",5.3 Results and Discussion,[0],[0]
"This indicates that the predicted vectors contain richer information than purely perceptual representations and are more useful for building semantic representations.
",5.3 Results and Discussion,[0],[0]
"Multimodal baselines For bimodal models (group 3), the CONC model that combines Glove and visual vectors performs worse than Glove on four out of six datasets, suggesting that simple concatenation might be suboptimal.",5.3 Results and Discussion,[0],[0]
"The Mapping and Ridge models, which combine Glove and predicted visual vectors, improve over Glove on five out of six datasets in ALL regions.",5.3 Results and Discussion,[0],[0]
This reinforces the conclusion that the predicted visual vectors are more useful in building multimodal models.,5.3 Results and Discussion,[0],[0]
The SVD model gets similar results as Ridge model.,5.3 Results and Discussion,[0],[0]
"The CCA model maps different modality inputs into a common space, achieving better results on some datasets and worse results on the others.
",5.3 Results and Discussion,[0],[0]
"The improvement on three benchmark tests shows the potential of mapping multimodal inputs into a common space.
",5.3 Results and Discussion,[0],[0]
The above results can also be observed in the trimodal CONC and Ridge models (group 5).,5.3 Results and Discussion,[0],[0]
"Overall, the trimodal models, which utilize additional auditory inputs, get slightly worse performance than bimodal models.",5.3 Results and Discussion,[0],[0]
This is partly caused by the fusion method of concatenation.,5.3 Results and Discussion,[0],[0]
Note that our proposed AMA models are more effective with trimodal inputs as shown in group 6.,5.3 Results and Discussion,[0],[0]
"Our multimodal models With either bimodal or trimodal inputs, the proposed AMA-M model outperforms all baseline models by a large margin.",5.3 Results and Discussion,[0],[0]
Specifically our AMA-M model achieves an relative improvement of 4.1% on average (4.5% with trimodal inputs) over the state-of-the-art Ridge model.,5.3 Results and Discussion,[0],[0]
This illustrates that our AMA models can productively combine textual and perceptual representations.,5.3 Results and Discussion,[0],[0]
"Moreover, our AMA-MW model, which employs word associations, achieves an average improvement of 1.5% (2.7% with trimodal inputs) over the AMA-M model.",5.3 Results and Discussion,[0],[0]
"That is to say, the representation ability of multimodal models can be clearly improved by learning associative relations between words.",5.3 Results and Discussion,[0],[0]
"Furthermore, the AMAMW-Gval model improves the AMA-MW model by 1.3% (0.3% with trimodal inputs) on average, illustrating that the gating mechanism (especially the value gate) helps to learn better semantic representations.
",5.3 Results and Discussion,[0],[0]
"In addition, we explore the effect of word association data size.",5.3 Results and Discussion,[0],[0]
"We find that the decrease of association data has no discernible effect on model performance: when using 100%, 80%, 60%, 40%, 20% of the data, the average results are 0.6479, 0.6409, 0.6361, 0.6430, 0.6458 in bimodal model.",5.3 Results and Discussion,[0],[0]
The same trend is observed in trimodal models.,5.3 Results and Discussion,[0],[0]
We have proposed a cognitively-inspired multimodal model — associative multichannel autoencoder — which utilizes the associations between modalities and related words to learn multimodal word representations.,6 Conclusions and Future Work,[0],[0]
"Performance improvement on six benchmark tests shows that our models can efficiently fuse different modality inputs and build better semantic representations.
",6 Conclusions and Future Work,[0],[0]
"Ultimately, the present paper sheds light on the fundamental questions of how to learn word meanings, such as the plausibility of reconstructing per-
ceptual information, associating related concepts and grounding word symbols to external environment.",6 Conclusions and Future Work,[0],[0]
We believe that one of the promising future directions is to learn from how humans learn and store semantic word representations to build a more effective computational model.,6 Conclusions and Future Work,[0],[0]
The research work descried in this paper has been supported by the National Key Research and Development Program of China under Grant No. 2017YFB1002103 and also supported by the Natural Science Foundation of China under Grant No. 61333018.,Acknowledgement,[0],[0]
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.,Acknowledgement,[0],[0]
"In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs.",abstractText,[0],[0]
"Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA).",abstractText,[0],[0]
"Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts.",abstractText,[0],[0]
Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings.,abstractText,[0],[0]
Using a gating mechanism our model assigns different weights to each modality according to the different concepts.,abstractText,[0],[0]
Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.,abstractText,[0],[0]
Associative Multichannel Autoencoder for Multimodal Word Representation,title,[0],[0]
"With the development of deep neural networks, including deep convolutional neural networks (CNN) (Krizhevsky et al., 2012), the ability to recognize images and languages has improved dramatically.",1. Inroduction,[0],[0]
Training deeplayered networks using a large number of labeled samples enables us to correctly categorize samples in diverse domains.,1. Inroduction,[0],[0]
"In addition, the transfer learning of a CNN has been utilized in many studies.",1. Inroduction,[0],[0]
"For object detection or segmentation, we can transfer the knowledge of a CNN trained using a large-scale dataset by fine-tuning it on a relatively small
1The University of Tokyo, Tokyo, Japan 2RIKEN, Japan.",1. Inroduction,[0],[0]
"Correspondence to: Kuniaki Saito <k-saito@mi.t.utokyo.ac.jp>, Yoshitaka Ushiku <ushiku@mi.t.u-tokyo.ac.jp>, Tatsuya Harada <harada@mi.t.u-tokyo.ac.jp>.
",1. Inroduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Inroduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Inroduction,[0],[0]
"dataset (Girshick et al., 2014; Long et al., 2015a).
",1. Inroduction,[0],[0]
"One of the problems inherent to neural networks is that, although such networks perform well on samples generated from the same distribution as the training samples, they may find it difficult to correctly recognize samples from different distributions at the test time.",1. Inroduction,[0],[0]
"An example of this is images collected from the Internet, which may come in abundance and are fully labeled.",1. Inroduction,[0],[0]
Such images have a distribution that differs from images taken from a camera.,1. Inroduction,[0],[0]
"Thus, a classifier that performs well on various domains is important for practical use.",1. Inroduction,[0],[0]
"To realize such a classifier, it is necessary to learn domain-invariantly discriminative representations.",1. Inroduction,[0],[0]
"However, acquiring such representations is not easy because it is often difficult to collect a large number of labeled samples, and because samples from different domains have domain-specific characteristics.
",1. Inroduction,[0],[0]
"In unsupervised domain adaptation, we try to train a classifier that works well on a target domain under the condition that we are provided labeled source samples and unlabeled target samples during training.",1. Inroduction,[0],[0]
Most of the previously developed deep domain adaptation methods operate mainly under the assumption that the adaptation can be realized by matching the distribution of features from different domains.,1. Inroduction,[0],[0]
"These methods have been aimed at obtaining domain-invariant features by minimizing the divergence between domains, as well as a category loss on the source domain (Ganin & Lempitsky, 2014; Long et al., 2015b; 2016).",1. Inroduction,[0],[0]
"However, as shown in (Ben-David et al., 2010), if a classifier that works well on both the source and the target domains does not exist, we theoretically cannot expect a discriminative classifier to be applicable to the target domain.",1. Inroduction,[0],[0]
"That is, even if the distributions are matched with the non-discriminative representations, the classifier may not work well on the target domain.",1. Inroduction,[0],[0]
"Because the direct learning discriminative representations for the target domain, in the absence of target labels, is considered very difficult, we propose assigning pseudo-labels to the target samples and training the target-specific networks as if they were true labels.
",1. Inroduction,[0],[0]
"Co-training and tri-training (Zhou & Li, 2005) leverage multiple classifiers to artificially label unlabeled samples and retrain the classifiers.",1. Inroduction,[0],[0]
"However, such methods do not assume labeling samples from different domains.",1. Inroduction,[0],[0]
"Because
our goal is to classify unlabeled target samples that have different characteristics from labeled source samples, we propose the use of asymmetric tri-training for unsupervised domain adaptation.",1. Inroduction,[0],[0]
"By asymmetric, we mean that we assign different roles to three different classifiers.
",1. Inroduction,[0],[0]
"In this paper, we propose a novel tri-training method for unsupervised domain adaptation, where we assign pseudolabels to unlabeled samples, and train the neural networks utilizing these samples.",1. Inroduction,[0],[0]
"As described in Fig. 1, two networks are used to label unlabeled target samples, and the remaining network is trained using the pseudo-labeled target samples.",1. Inroduction,[0],[0]
"We evaluated our method using digit classification tasks, traffic sign classification tasks, and sentiment analysis tasks using the Amazon Review dataset, and demonstrated its state-of-the-art performance for nearly all of the conducted experiments.",1. Inroduction,[0],[0]
"In particular, for the adaptation scenario, MNIST→SVHN, our method outperformed other methods by more than 10%.",1. Inroduction,[0],[0]
"A number of previous methods have attempted to realize adaptation by measuring the divergence between different domains (Ganin & Lempitsky, 2014; Long et al., 2015b; Li et al., 2016).",2. Related Work,[0],[0]
"Such methods are based on the theory proposed in (Ben-David et al., 2010), which states that the expected loss for a target domain is bounded by three terms: (i) the expected loss for the source domain, (ii) the domain divergence between the source and target, and (iii) the minimum value of a shared expected loss.",2. Related Work,[0],[0]
A shared expected loss indicates the sum of the loss on the source and target domains.,2. Related Work,[0],[0]
"Because the third term, which is usually considered to be very low, cannot be evaluated when labeled target samples are absent, most methods attempt to minimize the first and second terms.",2. Related Work,[0],[0]
"With regard to the training of deep architectures, the maximum mean discrepancy (MMD), or the loss of a domain classifier network, is utilized to measure the divergence corresponding to the second term (Gretton et al., 2012; Ganin & Lempitsky, 2014; Long et al., 2015b; 2016; Bousmalis et al., 2016).",2. Related Work,[0],[0]
"However, the third term is very important in training a CNN, which simultaneously extracts and recognizes the representations.",2. Related Work,[0],[0]
"The third term can easily become large when the representations are not discriminative for the target do-
main.",2. Related Work,[0],[0]
"Therefore, we focus on how to learn the targetdiscriminative representations to consider the third term.",2. Related Work,[0],[0]
"In (Long et al., 2016), the focus was on this point, and a target-specific classifier was constructed using a residual network structure.",2. Related Work,[0],[0]
"Differing from their method, we constructed a target-specific network by providing artificially labeled target samples.
",2. Related Work,[0],[0]
"Several transductive methods use a similarity of features to provide labels for unlabeled samples (Rohrbach et al., 2013; Khamis & Lampert, 2014).",2. Related Work,[0],[0]
"For unsupervised domain adaptation, in (Sener et al., 2016), a method was proposed to learn the labeling metrics by utilizing the k-nearest neighbors between unlabeled target samples and labeled source samples.",2. Related Work,[0],[0]
"In contrast to this method, our method explicitly and simply backpropagates the category loss for the target samples based on pseudo-labeled samples.
",2. Related Work,[0],[0]
"Many methods have proposed giving pseudo-labels to unlabeled samples by utilizing the predictions of a classifier and retraining it, including pseudo-labeled samples, a process called self-training.",2. Related Work,[0],[0]
"The underlying assumption of self-training is that one’s own high-confidence predictions are correct (Zhu, 2005).",2. Related Work,[0],[0]
"As the predictions are mostly correct, utilizing samples with high confidence will further improve the performance of the classifier.",2. Related Work,[0],[0]
"Co-training utilizes two classifiers, which have different views on one sample, to provide pseudo-labels (Blum & Mitchell, 1998; Tanha et al., 2011).",2. Related Work,[0],[0]
The unlabeled samples are then added to the training set if at least one classifier is confident regarding the predictions.,2. Related Work,[0],[0]
"The generalization capability of co-training is theoretically ensured (Balcan et al., 2004; Dasgupta et al., 2001) under certain assumptions, and applied to various tasks (Wan, 2009; Levin et al., 2003).",2. Related Work,[0],[0]
"In (Chen et al., 2011), the idea of co-training was incorporated into domain adaptation.",2. Related Work,[0],[0]
"Similar to co-training, tritraining uses the output of three different classifiers to provide pseudo-labels to unlabeled samples (Zhou & Li, 2005).",2. Related Work,[0],[0]
"Tri-training does not require partitioning features into different views; instead, tri-training initializes each classifier in a different manner.",2. Related Work,[0],[0]
"However, tri-training does not assume that the unlabeled samples follow different distributions from those the labeled ones are generated from.",2. Related Work,[0],[0]
"Hence, we developed a tri-training method for domain adaptation that utilizes three classifiers asymmetrically.
",2. Related Work,[0],[0]
"In (Lee, 2013), the effects of pseudo-labels on a neural network were investigated.",2. Related Work,[0],[0]
"The authors argued that the effect of training a classifier using pseudo-labels is equivalent to entropy regularization, thus leading to a low-density separation between classes.",2. Related Work,[0],[0]
"In our experiments, we observed that the target samples are separated in hidden features.",2. Related Work,[0],[0]
"In this section, we provide details of the proposed model for domain adaptation.",3. Method,[0],[0]
"We aim to construct a target-
specific network by utilizing pseudo-labeled target samples.",3. Method,[0],[0]
"Simultaneously, we expect two labeling networks to acquire target-discriminative representations and gradually increase the accuracy on the target domain.
",3. Method,[0],[0]
Our proposed network structure is shown in Fig. 2.,3. Method,[0],[0]
"Here, F denotes a network that outputs shared features from among three different networks, and F1 and F2 classify the features generated from F .",3. Method,[0],[0]
Their predictions are utilized to provide pseudo-labels.,3. Method,[0],[0]
"The classifier Ft classifies features generated from F , which is a target-specific network.",3. Method,[0],[0]
"Here, F1 and F2 learn from the source and pseudo-labeled target samples, and Ft learns only from the pseudo-labeled target samples.",3. Method,[0],[0]
"The shared network F learns from all gradients from F1, F2, and Ft.",3. Method,[0],[0]
"Without such a shared network, another option for the network architecture is training the three networks separately, although this is inefficient in terms of training and implementation.",3. Method,[0],[0]
"Furthermore, by building a shared network, F , F1, and F2 can also harness the target-discriminative representations learned through the feedback from Ft.
",3. Method,[0],[0]
"The set of source samples is defined as { (xi, yi) }ms i=1
∼ Xs, the unlabeled target set is { (xi) }mt i=1
∼ Xt, and the pseudo-labeled target set is { (xi, ŷi) }nt i=1",3. Method,[0],[0]
∼ Xtl.,3. Method,[0],[0]
"In existing studies (Chen et al., 2011) on co-training for domain adaptation, the given features are divided into separate parts, and considered to be different views.
",3.1. Loss for Multiview Features Network,[0],[0]
"Because we aim to label the target samples with high accuracy, we expect F1 and F2 to classify the samples based on different viewpoints.",3.1. Loss for Multiview Features Network,[0],[0]
"Therefore, we make a constraint for the weights of F1 and F2 to make their inputs different from each other.",3.1. Loss for Multiview Features Network,[0],[0]
"We add the term |W1TW2| to the cost function, where W1 and W2 denote fully connected layer
weights of F1 and F2, which are first applied to the feature F (xi).",3.1. Loss for Multiview Features Network,[0],[0]
"With this constraint, each network will learn from different features.",3.1. Loss for Multiview Features Network,[0],[0]
"The objective for the learning of F1 and F2 is defined as
E(θF , θF1 , θF2) = 1
n n∑ i=1",3.1. Loss for Multiview Features Network,[0],[0]
[ Ly(F1 ◦,3.1. Loss for Multiview Features Network,[0],[0]
"F (xi)), yi)
+ Ly(F2",3.1. Loss for Multiview Features Network,[0],[0]
◦,3.1. Loss for Multiview Features Network,[0],[0]
"(F (xi)), yi) ]",3.1. Loss for Multiview Features Network,[0],[0]
"+ λ|W1TW2|
(1)
where Ly denotes the standard softmax cross-entropy loss function.",3.1. Loss for Multiview Features Network,[0],[0]
We determined the trade-off parameter λ based on a validation split.,3.1. Loss for Multiview Features Network,[0],[0]
Pseudo-labeled target samples will provide targetdiscriminative information to the network.,3.2. Learning Procedure and Labeling Method,[0],[0]
"However, because they certainly contain false labels, we have to pick up reliable pseudo-labels, which our labeling and learning method is aimed at realizing.
",3.2. Learning Procedure and Labeling Method,[0],[0]
The entire training procedure of the network is shown in Algorithm 1.,3.2. Learning Procedure and Labeling Method,[0],[0]
"First, we train the entire network using the source training set Xs.",3.2. Learning Procedure and Labeling Method,[0],[0]
"Here, F1 and F2 are optimized through Eq.",3.2. Learning Procedure and Labeling Method,[0],[0]
"(1), and Ft is trained based on a standard category loss.",3.2. Learning Procedure and Labeling Method,[0],[0]
"After training on Xs, to provide pseudo-labels, we use the predictions of F1 and F2, namely, ŷ1, ŷ2 obtained from xk.",3.2. Learning Procedure and Labeling Method,[0],[0]
"When C1 and C2 denote the class that has the maximum predicted probability for ŷ1, ŷ2, we assign a pseudo-label to xk if the following two conditions are satisfied.",3.2. Learning Procedure and Labeling Method,[0],[0]
"First, we require C1 = C2 to provide pseudo-labels, which means the two different classifiers agree with the prediction.",3.2. Learning Procedure and Labeling Method,[0],[0]
"The second requirement is that the maximizing probability of ŷ1 or ŷ2 exceed the threshold parameter, which we set as 0.9 or 0.95 in the experiment.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We suppose that unless one of the two classifiers is confident of the prediction, the prediction is not reliable.",3.2. Learning Procedure and Labeling Method,[0],[0]
"If the two requirements are satisfied, ( xk, yk = C1 = C2 ) is added to Xtl.",3.2. Learning Procedure and Labeling Method,[0],[0]
"To prevent an overfitting to the pseudo-labels, we resample the candidate for labeling the samples in each step.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We set the number of initial candidates Ninit to 5,000.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We gradually increase the number of candidates Nt = K/20 ∗ n, where n denotes the number of all target samples, and K denotes the number of steps; in addition, we set the maximum number of pseudo-labeled candidates to 40,000.",3.2. Learning Procedure and Labeling Method,[0],[0]
We set K to 30 in the experiments.,3.2. Learning Procedure and Labeling Method,[0],[0]
"After the pseudo-labeled training set Xtl is composed, F, F1, and F2 are updated based on the objective in Eq.",3.2. Learning Procedure and Labeling Method,[0],[0]
(1) for the labeled training set L = Xs ∪ Xtl.,3.2. Learning Procedure and Labeling Method,[0],[0]
"Then, F and Ft are simply optimized based on the category loss for Xtl.
Discriminative representations will be learned by constructing a target-specific network trained only on the target samples.",3.2. Learning Procedure and Labeling Method,[0],[0]
"However, if only noisy pseudo-labeled samples are used for the training, the network may not learn any
Algorithm 1 iter denotes the iteration of the training.",3.2. Learning Procedure and Labeling Method,[0],[0]
The function Labeling indicates the labeling method.,3.2. Learning Procedure and Labeling Method,[0],[0]
"We assign pseudo-labels to samples when the predictions of F1 and F2 agree, and at least one of them is confident of their predictions.
",3.2. Learning Procedure and Labeling Method,[0],[0]
"Input: data Xs = { (xi, ti) }m i=1 , Xt = { (xj) }n j=1",3.2. Learning Procedure and Labeling Method,[0],[0]
"Xtl = ∅ for j = 1 to iter do
Train F, F1, F2, Ft with a mini-batch from the training set S
end for Nt = Ninit Xtl = Labeling(F, F1, F2,Xt, Nt) L = Xs ∪Xtl for K steps do
for j = 1 to iter do Train F, F1, F2 with mini-batch from training set L Train F, Ft with mini-batch from training set Xtl end for Xtl = ∅, Nt = K/20 ∗ n",3.2. Learning Procedure and Labeling Method,[0],[0]
"Xtl = Labeling(F, F1, F2,Xt, Nt) L = Xs ∪Xtl
end for
useful representations.",3.2. Learning Procedure and Labeling Method,[0],[0]
"We then use both the source samples and pseudo-labeled samples for the training of F, F1, and F2 to ensure the accuracy.",3.2. Learning Procedure and Labeling Method,[0],[0]
"In addition, as the learning proceeds, F will learn target-discriminative representations, resulting in an improvement in accuracy for F1 and F2.",3.2. Learning Procedure and Labeling Method,[0],[0]
This cycle will gradually enhance the accuracy in the target domain.,3.2. Learning Procedure and Labeling Method,[0],[0]
"Batch normalization (BN) (Ioffe & Szegedy, 2015), which whitens the output of the hidden layer in a CNN, is an effective technique for accelerating the training speed and enhancing the accuracy of the model.",3.3. Batch Normalization for Domain Adaptation,[0],[0]
"In addition, in domain adaptation, whitening the output of the hidden layer is effective in improving the performance, and makes the distribution in different domains similar (Sun et al., 2016; Li et al., 2016).
",3.3. Batch Normalization for Domain Adaptation,[0],[0]
The input samples of F1 and F2 include both pseudolabeled target samples and source samples.,3.3. Batch Normalization for Domain Adaptation,[0],[0]
Introducing BN will be useful for matching the distribution and improving the performance.,3.3. Batch Normalization for Domain Adaptation,[0],[0]
"We add BN layers to F, F1 and F2, which we detail in our supplementary material.",3.3. Batch Normalization for Domain Adaptation,[0],[0]
"In this section, we provide a theoretical analysis to our approach.",4. Analysis,[0],[0]
"First, we provide insight into existing theory, and then introduce a simple expansion of the theory related to
our method.",4. Analysis,[0],[0]
"The distribution of the source samples is denoted as S; that of the target samples, as T ; and that of the pseudo-labeled target samples, as Tl.
",4. Analysis,[0],[0]
"In (Ben-David et al., 2010), an equation was introduced showing that the upper bound of the expected error in the target domain depends on three terms, which include the divergence between different domains and the error of an ideal joint hypothesis.",4. Analysis,[0],[0]
"The divergence between the source and target domains, H∆H-distance, is defined as follows:
dH∆H(S, T )
",4. Analysis,[0],[0]
"= 2 sup (h,h′)∈H2 ∣∣∣ E",4. Analysis,[0],[0]
x∼S,4. Analysis,[0],[0]
[h(x) ̸=,4. Analysis,[0],[0]
h′(x)]− E,4. Analysis,[0],[0]
x∼T,4. Analysis,[0],[0]
[h(x) ̸= h′(x)],4. Analysis,[0],[0]
"∣∣∣
This distance is frequently used to measure the adaptability between different domains.
",4. Analysis,[0],[0]
"The ideal joint hypothesis is defined as h∗ = arg min
h∈H
( RS(h) + RT (h) ) , and its corresponding error is
C = RS(h ∗)",4. Analysis,[0],[0]
"+ RT (h ∗), where R denotes the expected error for each hypothesis.",4. Analysis,[0],[0]
"The theorem is as follows.
",4. Analysis,[0],[0]
Theorem 1.,4. Analysis,[0],[0]
"(Ben-David et al., 2010) Let H be the hypothesis class.",4. Analysis,[0],[0]
"Given two different domains, S and T , we have ∀h ∈ H,RT (h) ≤ RS(h)",4. Analysis,[0],[0]
"+ 1
2 dH∆H(S, T ) + C (2)
",4. Analysis,[0],[0]
"This theorem indicates that the expected error on the target domain is upper bounded by three terms: the expected error on the source domain, the domain divergence measured by the disagreement of the hypothesis, and the error of the ideal joint hypothesis.",4. Analysis,[0],[0]
"In an existing work (Ganin & Lempitsky, 2014; Long et al., 2015b), C was disregarded because it was considered to be negligible.",4. Analysis,[0],[0]
"If we are provided with fixed features, we do not need to consider this term because it is also fixed.",4. Analysis,[0],[0]
"However, if we assume that xs ∼ S and xt ∼ T are obtained from the last fully connected layer of the deep models, we should note that C is determined based on the output of the layer, as well as the necessity of considering this term.
",4. Analysis,[0],[0]
We consider the pseudo-labeled target sample distributions Tl given false labels at a ratio of ρ.,4. Analysis,[0],[0]
"The shared error of h∗ on S, Tl is denoted as C ′. The following inequality then holds:
∀h ∈ H,RT (h) ≤ RS(h)",4. Analysis,[0],[0]
"+ 1
2 dH∆H(S, T ) +",4. Analysis,[0],[0]
"C
≤ RS(h) + 1
2 dH∆H(S, T ) +",4. Analysis,[0],[0]
C ′,4. Analysis,[0],[0]
"+ ρ
(3)
We show a simple derivation of the inequality in the Supplementary materials section.",4. Analysis,[0],[0]
"In Theorem 1, we cannot measure C in the absence of labeled target samples.",4. Analysis,[0],[0]
We can evaluate and minimize it approximately using pseudolabels.,4. Analysis,[0],[0]
"Furthermore, when we consider the second term on the right-hand side, our method is expected to reduce
this term.",4. Analysis,[0],[0]
This term intuitively denotes the discrepancy between different domains in the disagreement of two classifiers.,4. Analysis,[0],[0]
"If we regard h and h′ as F1 and F2, respectively, E
x∼S",4. Analysis,[0],[0]
[h(x) ̸= h′(x)] should be very low because the training is based on the same labeled samples.,4. Analysis,[0],[0]
"Moreover, for the same reason, E
x∼T [h(x) ̸= h′(x)] is expected to be low, al-
though we use the training set Xtl instead of the genuine labeled target samples.",4. Analysis,[0],[0]
"Thus, our method considers both the second and third terms in Theorem 1.",4. Analysis,[0],[0]
We conducted extensive evaluations of our method on image datasets and a sentiment analysis dataset.,5. Experiment and Evaluation,[0],[0]
"We evaluated the accuracy of the target-specific networks.
",5. Experiment and Evaluation,[0],[0]
"Visual Domain Adaptation For visual domain adaptation, we conducted our evaluation on the digit and traffic sign datasets.",5. Experiment and Evaluation,[0],[0]
"The digit datasets include MNIST (LeCun et al., 1998), MNIST-M (Ganin & Lempitsky, 2014), Street View House Numbers (SVHN) (Netzer et al., 2011), and Synthetic Digits (SYN DIGITS)",5. Experiment and Evaluation,[0],[0]
"(Ganin & Lempitsky, 2014).",5. Experiment and Evaluation,[0],[0]
We further evaluated our method on traffic sign datasets including Synthetic Traffic Signs (SYN SIGNS),5. Experiment and Evaluation,[0],[0]
"(Moiseev et al., 2013) and the German Traffic Sign Recognition Benchmark (Stallkamp et al., 2011) (GTSRB).",5. Experiment and Evaluation,[0],[0]
"In total, five adaptation scenarios were evaluated during this experiment.",5. Experiment and Evaluation,[0],[0]
"Because the datasets used for evaluation are varied in previous studies, we extensively evaluated our method using these five scenarios.
",5. Experiment and Evaluation,[0],[0]
Many previous studies have evaluated the fine-tuning of pretrained networks using ImageNet.,5. Experiment and Evaluation,[0],[0]
This protocol assumes the existence of another source domain.,5. Experiment and Evaluation,[0],[0]
"In our work, we want to evaluate a situation in which we have access to only a single source domain and a single target domain.
",5. Experiment and Evaluation,[0],[0]
"Adaptation in Amazon Reviews To investigate its behavior on the language datasets, we evaluated our method on the Amazon Review dataset (Blitzer et al., 2006) through the same preprocessing used by (Chen et al., 2011; Ganin et al., 2016).",5. Experiment and Evaluation,[0],[0]
"The dataset contains reviews on four types of products: books, DVDs, electronics, and kitchen appliances.",5. Experiment and Evaluation,[0],[0]
We evaluated our method under 12 domain adaptation scenarios.,5. Experiment and Evaluation,[0],[0]
"The results are shown in Table 1.
",5. Experiment and Evaluation,[0],[0]
"Baseline Methods We compared our method with five methods for unsupervised domain adaptation, including state-of-the art methods in visual domain adaptation: Maximum Mean Discrepancy (MMD) (Long et al., 2015b), Domain Adversarial Neural Network (DANN) (Ganin & Lempitsky, 2014), Deep Reconstruction Classification Network (DRCN) (Ghifary et al., 2016), Domain Separation Network (DSN) (Bousmalis et al., 2016), and k-Nearest Neighbor based adaptation (kNN-Ad) (Sener et al., 2016).",5. Experiment and Evaluation,[0],[0]
"We cited the results of MMD from (Bousmalis et al., 2016).",5. Experiment and Evaluation,[0],[0]
"In addition, we compared our
method with CNN trained only on the source samples.",5. Experiment and Evaluation,[0],[0]
"We compared our method with Variational Fair AutoEncoder (VFAE) (Louizos et al., 2015) and DANN (Ganin et al., 2016) in our experiment on the Amazon Review dataset.",5. Experiment and Evaluation,[0],[0]
"In our experiments on the image datasets, we employed the architecture of CNN used in (Ganin & Lempitsky, 2014).",5.1. Implementation Detail,[0],[0]
"For a fair comparison, we separated the network at the hidden layer from which (Ganin & Lempitsky, 2014) constructed discriminator networks.",5.1. Implementation Detail,[0],[0]
"Therefore, when considering a single classifier, for example, F1 ◦ F , the architecture is identical to a previous work.",5.1. Implementation Detail,[0],[0]
"We also followed (Ganin & Lempitsky, 2014) with the other protocols.",5.1. Implementation Detail,[0],[0]
"Based on a validation, we set the threshold value for the labeling method as 0.95 in MNIST↔SVHN.",5.1. Implementation Detail,[0],[0]
"In other scenarios, we set it as 0.9.",5.1. Implementation Detail,[0],[0]
"We used MomentumSGD for optimization, and set the momentum as 0.9, whereas the learning rate was set 0.01.",5.1. Implementation Detail,[0],[0]
λ was set to 0.01 for all scenarios based on our validation.,5.1. Implementation Detail,[0],[0]
"In the Supplementary materials section, we provide details of the network architecture and the hyper-parameters.
",5.1. Implementation Detail,[0],[0]
"For our experiments on the Amazon Review dataset, we used a similar architecture to that used in (Ganin et al., 2016): with the sigmoid activated, one dense hidden layer with 50 hidden units, and a softmax output.",5.1. Implementation Detail,[0],[0]
We extended its architecture to our method similarly to that of the CNN.,5.1. Implementation Detail,[0],[0]
λ was set to 0.001 based on a validation.,5.1. Implementation Detail,[0],[0]
"Because the input is sparse, we used Adagrad (Duchi et al., 2011) for optimization.",5.1. Implementation Detail,[0],[0]
"We repeated this evaluation ten times, and reported the mean accuracy.",5.1. Implementation Detail,[0],[0]
"In Tables 1 and 3, we show the main results of our experiments.",5.2. Experimental Result,[0],[0]
"When training only using source samples, the effect of the BN is not clear, as shown in the Tables 1.",5.2. Experimental Result,[0],[0]
"However, for most of the image recognition experiments, the effect of the BN with our method is clear; at the same time, the effect of our method is also clear when we do not use a BN in the network architecture compared to the Source Only method.",5.2. Experimental Result,[0],[0]
The effect of the weight constraint is not obvious in other than MNIST→SVHN.,5.2. Experimental Result,[0],[0]
"This result indicates that we can obtain sufficiently different classifiers when initializing the layer parameters differently.
MNIST→MNIST-M First, we evaluated the adaptation between the hand-written digit dataset, MNIST, and its transformed dataset, MNIST-M. MNIST-M was composed by merging clips of a background from the BSDS500 datasets (Arbelaez et al., 2011).",5.2. Experimental Result,[0],[0]
"A patch was randomly taken from the images in BSDS500, and merged with the MNIST digits.",5.2. Experimental Result,[0],[0]
"From 59,001 target training samples, we randomly selected 1,000 labeled target samples as a validation split and tuned the hyper-parameters.
",5.2. Experimental Result,[0],[0]
Our method outperformed the other existing method by about 7%.,5.2. Experimental Result,[0],[0]
Visualization of the features in the last pooling layer is shown in Fig. 3(a)(b).,5.2. Experimental Result,[0],[0]
We observed that the red target samples are more dispersed when adaptation is achieved.,5.2. Experimental Result,[0],[0]
A comparison of the accuracy between the actual labeling accuracy on the target samples during the training and the test accuracy is shown in Fig. 4.,5.2. Experimental Result,[0],[0]
"The test accuracy is very low initially, but as the steps increase, the accuracy becomes closer to that of the labeling accuracy.",5.2. Experimental Result,[0],[0]
"With this adaptation, we can clearly see that the actual labeling accuracy gradually improves with the accuracy of the network.",5.2. Experimental Result,[0],[0]
SVHN↔MNIST We increased the gap between distributions during this experiment.,5.2. Experimental Result,[0],[0]
"We evaluated the adaptation between SVHN (Netzer et al., 2011) and MNIST in a tenclass classification problem.",5.2. Experimental Result,[0],[0]
"SVHN and MNIST have distinct appearances, and thus this adaptation is a challenging scenario, particularly in MNIST→SVHN.",5.2. Experimental Result,[0],[0]
"The images in SVHN are colored, and some contain multiple digits.",5.2. Experimental Result,[0],[0]
"Therefore, a classifier trained on SVHN is expected to perform well on MNIST, but the reverse is not true.",5.2. Experimental Result,[0],[0]
"MNIST does not include any samples containing multiple digits,
and most of the samples are centered in the images, and thus adaptation from MNIST to SVHN is rather difficult.",5.2. Experimental Result,[0],[0]
"In both settings, we use 1,000 labeled target samples to find the optimal hyperparameters.
",5.2. Experimental Result,[0],[0]
We evaluated our method under both adaptation scenarios and achieved a state-of-the-art performance for both datasets.,5.2. Experimental Result,[0],[0]
"In particular, for the adaptation MNIST→SVHN, our method outperformed the other methods by more than 10%.",5.2. Experimental Result,[0],[0]
"In Fig. 3(c)(d), the representations in MNIST→SVHN are visualized.",5.2. Experimental Result,[0],[0]
"Although the distributions seem to be separated between domains, the red SVHN samples become more discriminative when using our method compared with non-adapted embedding.",5.2. Experimental Result,[0],[0]
A comparison between the actual labeling method accuracy and the testing accuracy is also shown in Fig. 4(b)(c).,5.2. Experimental Result,[0],[0]
"In this figure, it can be seen that the labeling accuracy rapidly decreases during the initial adaptation stage.",5.2. Experimental Result,[0],[0]
"On the other hand, the testing accuracy continues to improve, and finally exceeds the labeling accuracy.",5.2. Experimental Result,[0],[0]
There are two questions regarding this interesting phenomenon.,5.2. Experimental Result,[0],[0]
"The first is why does the labeling method continue to decrease despite the increase in the
0 5 10 15 20 25 30
Number of steps
0.4
0.5
0.6
0.7
0.8
0.9
1
A c c u",5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(a) MNIST→MNIST-M
0 5 10 15 20 25 30
Number of steps
0.7
0.75
0.8
0.85
0.9
0.95
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(b) SVHN→MNIST
0 5 10 15 20 25 30
Number of steps
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u m
b e r
o f s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(c) MNIST→SVHN
0 5 10 15 20 25 30
Number of steps
0.84
0.86
0.88
0.9
0.92
0.94
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u
m b e
r o f
s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(d) SYNDIGITS→SVHN
0 5 10 15 20 25 30
Number of steps
0.75
0.8
0.85
0.9
0.95
1
A",5.2. Experimental Result,[0],[0]
c c u,5.2. Experimental Result,[0],[0]
"ra
c y
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
N u
m b e
r o f
s a m
p le
s
!10 4
Accuracy of labeling method Accuracy of learned network Number of labeled samples
(e) SYNSIGNS→GTSRB
0 5 10 15 20 25 30
Number of steps
0.65
0.7
0.75
0.8
0.85
",5.2. Experimental Result,[0],[0]
"0.9
A c c u",5.2. Experimental Result,[0],[0]
"ra
c y
Accuracy of Target Network Accuracy of Network1 Accuracy of Network2
(f) Comparision of accuracy of three networks on SVHN→MNIST (g) A-distance in MNIST→MNISTM
Figure 4.",5.2. Experimental Result,[0],[0]
(a) ∼ (e): Comparison of the actual accuracy of the pseudo-labels and the learned network accuracy during training.,5.2. Experimental Result,[0],[0]
"The blue curve indicates the pseudo-label accuracy, and the red curve is the learned network accuracy.",5.2. Experimental Result,[0],[0]
Note that the labeling accuracy is computed using (the number of correctly labeled samples)/(the number of labeled samples).,5.2. Experimental Result,[0],[0]
The green curve shows the number of labeled target samples in each step.,5.2. Experimental Result,[0],[0]
(f): Comparison of the accuracy of the three networks in our model.,5.2. Experimental Result,[0],[0]
The accuracy of the three networks improved almost simultaneously.,5.2. Experimental Result,[0],[0]
(g): Comparison of the A-distance of the different methods.,5.2. Experimental Result,[0],[0]
"Our model slightly reduced the divergence of the domain compared with the source-only trained CNN.
",5.2. Experimental Result,[0],[0]
testing accuracy?,5.2. Experimental Result,[0],[0]
"Target samples given pseudo-labels always include mistakenly labeled samples, whereas those given no labels are ignored in our method.",5.2. Experimental Result,[0],[0]
"Therefore, an error will be reinforced in the target samples included in the training set.",5.2. Experimental Result,[0],[0]
The second question is why does the test accuracy continue to increase despite the lower labeling accuracy?,5.2. Experimental Result,[0],[0]
"The assumed reason is that the network already acquires target discriminative representations during this phase, which can improve the accuracy when using source samples and correctly labeled target samples.
",5.2. Experimental Result,[0],[0]
"In Fig. 4(f), we show a comparison of the accuracy of the three networks F1, F2, and Ft in SVHN→MNIST.",5.2. Experimental Result,[0],[0]
The accuracy of these networks is nearly the same during every step.,5.2. Experimental Result,[0],[0]
The same situation was observed for the other scenarios.,5.2. Experimental Result,[0],[0]
"Based on this result, we can state that targetdiscriminative representations are shared in three networks.
SYN DIGITS→SVHN With this experiment, we aimed to address a common adaptation scenario from synthetic images to real images.",5.2. Experimental Result,[0],[0]
"The datasets of synthetic numbers (Ganin & Lempitsky, 2014) consist of 500,000 images generated from Windows fonts by varying the text, positioning, orientation, background and stroke colors, and the amount of blur.",5.2. Experimental Result,[0],[0]
"We used 479,400 source samples and 73,257 target samples for training, and 26,032 target samples for testing.",5.2. Experimental Result,[0],[0]
"In addition, we used 1,000 SVHN samples as the validation set.
",5.2. Experimental Result,[0],[0]
Our method also outperformed the other methods during this experiment.,5.2. Experimental Result,[0],[0]
"With this experiment, the effect of BN is not clear as compared with the other scenarios.",5.2. Experimental Result,[0],[0]
"The domain gap is considered small in this scenario, as the performance of the source-only classifier illustrates.",5.2. Experimental Result,[0],[0]
"In Fig. 4(d), although the labeling accuracy decreases, the accuracy of the learned network prediction improves, as in MNIST↔SVHN.
",5.2. Experimental Result,[0],[0]
SYN SIGNS→GTSRB,5.2. Experimental Result,[0],[0]
"This setting is similar to the previous one, adaptation from synthetic images to real images, but we have a larger number of classes, namely, 43 classes instead of ten.",5.2. Experimental Result,[0],[0]
"We used the SYN SIGNS dataset (Ganin & Lempitsky, 2014) for the source, and the GTSRB dataset (Stallkamp et al., 2011) for the target, which consist of real images of traffic signs.",5.2. Experimental Result,[0],[0]
"We randomly selected 31,367 samples for the target training samples and evaluated the accuracy on the remaining samples.",5.2. Experimental Result,[0],[0]
"A total of 3,000 labeled target samples were used for validation.
",5.2. Experimental Result,[0],[0]
"Under this scenario, our method outperformed the other methods, which indicates that our method is effective for the adaptation from synthesized images to real images with diverse classes.",5.2. Experimental Result,[0],[0]
"As shown in Fig. 4(e), the same tendency as in MNIST↔SVHN was observed for this adaptation scenario.
",5.2. Experimental Result,[0],[0]
Gradient Stop Experiment We evaluated the effects of a target-specific network using our method.,5.2. Experimental Result,[0],[0]
"We stopped the
gradient from the upper layer networks F1, F2, and Ft to examine the effect on Ft. Table 2 shows three scenarios, including the case in which we stopped the gradients from F1, F2, and Ft.
",5.2. Experimental Result,[0],[0]
"In the experiment on MNIST→MNIST-M, we assumed that only the backpropagation from F1 and F2 cannot construct discriminative representations for the target samples, and confirmed the effect of Ft.",5.2. Experimental Result,[0],[0]
"For the adaptation on MNIST→SVHN, the best performance was realized when F received all gradients from the upper networks.",5.2. Experimental Result,[0],[0]
Backwarding all gradients ensures both target-specific discriminative representations in difficult adaptations.,5.2. Experimental Result,[0],[0]
"In SYN SIGNS→GTSRB, backwarding only from Ft results in the worst performance because these domains are similar, and noisy pseudo-labeled samples worsen the performance.
",5.2. Experimental Result,[0],[0]
"A-distance Based on the theoretical results in (Ben-David et al., 2010), the A-distance is usually used as a measure of domain discrepancy.",5.2. Experimental Result,[0],[0]
The method of estimating the empirical A-distance is simple: We train a classifier to classify a domain from each domains’ feature.,5.2. Experimental Result,[0],[0]
"The approximate distance is then calculated as d̂A = 2(1 − 2ϵ), where ϵ is a generalization error of the classifier.",5.2. Experimental Result,[0],[0]
"We compared our method with the distribution matching methods, DANN and MMD.",5.2. Experimental Result,[0],[0]
We calculated the distance using the last pooling layer features.,5.2. Experimental Result,[0],[0]
"We followed the implementation of DANN (Ganin et al., 2016) for the training.",5.2. Experimental Result,[0],[0]
"For MMD training, we followed the implementation in (Bousmalis et al., 2016).",5.2. Experimental Result,[0],[0]
"In Fig. 4(g), the A-distance calculated from each CNN feature is shown.",5.2. Experimental Result,[0],[0]
We used a linear SVM to calculate the distance.,5.2. Experimental Result,[0],[0]
"From this graph, we can see that our method clearly reduces the A-distance compared with the CNN trained on only the source samples.",5.2. Experimental Result,[0],[0]
"In addition, when comparing the distribution matching methods against our own, although the former reduce the A-distance much more, our method shows a superior performance as shown in Table 1.
Semi-supervised domain adaptation We evaluated our model in a semi-supervised domain adaptation setting on MNIST→SVHN.",5.2. Experimental Result,[0],[0]
"We randomly selected the labeled target samples for each class, and reported the mean accuracy for
ten experiments.",5.2. Experimental Result,[0],[0]
The resulting accuracy was 58% on average when using ten labeled target samples per class.,5.2. Experimental Result,[0],[0]
We can see the effectiveness of our method in a semi-supervised setting.,5.2. Experimental Result,[0],[0]
"A detailed explanation of this is given in our Supplementary materials section.
",5.2. Experimental Result,[0],[0]
"Amazon Reviews The reviews were encoded in 5,000 dimensional vectors of bag-of-word unigrams and bigrams with binary labels.",5.2. Experimental Result,[0],[0]
Negative labels were attached to the samples if they were ranked with 1 to 3 stars.,5.2. Experimental Result,[0],[0]
Positive labels were attached if they were ranked with 4 or 5 stars.,5.2. Experimental Result,[0],[0]
"We used 2,000 labeled source samples and 2,000 unlabeled target samples for the training, and between 3,000 and 6,000 samples for the testing.",5.2. Experimental Result,[0],[0]
"We used 200 labeled target samples for validation.
",5.2. Experimental Result,[0],[0]
"Based on the results in Table 3, our method performed better than VFAE (Louizos et al., 2015) and DANN (Ganin et al., 2016) in nine out of twelve settings.",5.2. Experimental Result,[0],[0]
Our method was shown to be effective in learning a shallow network on different domains.,5.2. Experimental Result,[0],[0]
"In this paper, we proposed a novel asymmetric tri-training method for unsupervised domain adaptation, which is implemented in a simple manner.",6. Conclusion,[0],[0]
We aimed at learning discriminative representations by utilizing pseudo-labels assigned to unlabeled target samples.,6. Conclusion,[0],[0]
"We utilized three classifiers, two networks assigned pseudo-labels to unlabeled target samples, and the remaining network, which learned from them.",6. Conclusion,[0],[0]
"We evaluated our method regarding both domain adaptation for a visual recognition and a sentiment analysis, and the results show that we outperformed all other methods.",6. Conclusion,[0],[0]
"In particular, our method outperformed the other methods by more than 10% for MNIST→SVHN.",6. Conclusion,[0],[0]
"This work was partially funded by the ImPACT Program of the Council for Science, Technology, and Innovation (Cabinet Office, Government of Japan), and was partially supported by CREST, JST.",7. Acknowledgement,[0],[0]
It is important to apply models trained on a large number of labeled samples to different domains because collecting many labeled samples in various domains is expensive.,abstractText,[0],[0]
"To learn discriminative representations for the target domain, we assume that artificially labeling the target samples can result in a good representation.",abstractText,[0],[0]
"Tritraining leverages three classifiers equally to provide pseudo-labels to unlabeled samples; however, the method does not assume labeling samples generated from a different domain.",abstractText,[0],[0]
"In this paper, we propose the use of an asymmetric tritraining method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train the neural networks as if they are true labels.",abstractText,[0],[0]
"In our work, we use three networks asymmetrically, and by asymmetric, we mean that two networks are used to label unlabeled target samples, and one network is trained by the pseudo-labeled samples to obtain target-discriminative representations.",abstractText,[0],[0]
Our proposed method was shown to achieve a stateof-the-art performance on the benchmark digit recognition datasets for domain adaptation.,abstractText,[0],[0]
Asymmetric Tri-training for Unsupervised Domain Adaptation,title,[0],[0]
"Deep Neural Networks (DNN) have pushed the frontiers of many applications, such as speech recognition (Sak et al., 2014; Sercu et al., 2016), computer vision (Krizhevsky et al., 2012; He et al., 2016; Szegedy et al., 2016), and natural language processing (Mikolov et al., 2013; Bahdanau et al., 2014; Gehring et al., 2017).",1. Introduction,[0],[0]
"Part of the success of DNN should be attributed to the availability of big training data and powerful computational resources, which allow people to learn very deep and big DNN models in parallel
1University of Science and Technology of China 2School of Mathematical Sciences, Peking University 3Microsoft Research 4Academy of Mathematics and Systems Science, Chinese Academy of Sciences.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Taifeng Wang, Wei Chen <taifengw, wche@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"(Zhang et al., 2015; Chen & Huo, 2016; Chen et al., 2016).
",1. Introduction,[0],[0]
"Stochastic Gradient Descent (SGD) is a popular optimization algorithm to train neural networks (Bottou, 2012; Dean et al., 2012; Kingma & Ba, 2014).",1. Introduction,[0],[0]
"As for the parallelization of SGD algorithms (suppose we use M machines for the parallelization), one can choose to do it in either a synchronous or asynchronous way.",1. Introduction,[0],[0]
"In synchronous SGD (SSGD), local workers compute the gradients over their own mini-batches of data, and then add the gradients to the global model.",1. Introduction,[0],[0]
"By using a barrier, these workers wait for each other, and will not continue their local training until the gradients from all the M workers have been added to the global model.",1. Introduction,[0],[0]
It is clear that the training speed will be dragged by the slowest worker1.,1. Introduction,[0],[0]
"To improve the training efficiency, asynchronous SGD (ASGD) (Dean et al., 2012) has been adopted, with which no barrier is imposed, and each local worker continues its training process right after its gradient is added to the global model.",1. Introduction,[0],[0]
"Although ASGD can achieve faster speed due to no waiting overhead, it suffers from another problem which we call delayed gradient.",1. Introduction,[0],[0]
"That is, before a worker wants to add its gradient g(wt) (calculated based on the model snapshot wt) to the global model, several other workers may have already added their gradients and the global model has been updated to wt+τ (here τ is called the delay factor).",1. Introduction,[0],[0]
"Adding gradient of model wt to another model wt+τ does not make a mathematical sense, and the training trajectory may suffer from unexpected turbulence.",1. Introduction,[0],[0]
"This problem has been well known, and some researchers have analyzed its negative effect on the convergence speed (Lian et al., 2015; Avron et al., 2015).
",1. Introduction,[0],[0]
"In this paper, we propose a novel method, called Delay Compensated ASGD (or DC-ASGD for short), to tackle the problem of delayed gradients.",1. Introduction,[0],[0]
"For this purpose, we study the Taylor expansion of the gradient function g(wt+τ ) at wt.",1. Introduction,[0],[0]
"We find that the delayed gradient g(wt) is just the zero-order approximator of the correct gradient g(wt+τ ), and we can leverage more items in the Taylor expansion to achieve more accurate approximation of g(wt+τ ).",1. Introduction,[0],[0]
"However, this straightforward idea is practically non-trivial, be-
1Recently, people proposed to use additional backup workers (Chen et al., 2016) to tackle this problem.",1. Introduction,[0],[0]
"However, this solution requires redundant computation resources and relies on the assumption that the majority of workers train almost equally fast.
",1. Introduction,[0],[0]
cause even including the first-order derivative of the gradient g(wt+τ ),1. Introduction,[0],[0]
"will require the computation of the secondorder derivative of the original loss function (i.e., the Hessian matrix), which will introduce high computation and space complexity.",1. Introduction,[0],[0]
"To overcome this challenge, we propose a cheap yet effective approximator of the Hessian matrix, which can achieve a good trade-off between bias and variance of approximation, only based on previously available gradients (without the necessity of directly computing the Hessian matrix).
",1. Introduction,[0],[0]
DC-ASGD is similar to ASGD in the sense that no worker needs to wait for others.,1. Introduction,[0],[0]
"It differs from ASGD in that it does not directly add the local gradient to the global model, but compensates the delay in the local gradient by using the approximate Taylor expansion.",1. Introduction,[0],[0]
"By doing so, it maintains almost the same efficiency as ASGD and achieves much higher accuracy.",1. Introduction,[0],[0]
"Theoretically, we proved that DC-ASGD can converge at a rate of the same order with sequential SGD for non-convex neural networks, if the delay is upper bounded; and it is more tolerant on the delay than ASGD2.",1. Introduction,[0],[0]
"Empirically, we conducted experiments on both CIFAR-10 and ImageNet datasets.",1. Introduction,[0],[0]
"The results show that (1) as compared to SSGD and ASGD, DC-ASGD accelerated the convergence of the training process; (2) the accuracy of the model obtained by DC-ASGD within the same time period is very close to the accuracy obtained by sequential SGD.",1. Introduction,[0],[0]
"In this section, we introduce DNN and its parallel training through ASGD.
",2. Problem Setting,[0],[0]
"Given a multi-class classification problem, we denote X = Rd as the input space, Y = {1, ...,K} as the output space, and P as the joint distribution over X × Y .",2. Problem Setting,[0],[0]
"Here d denotes the dimension of the input space, and K denotes the number of categories in the output space.
",2. Problem Setting,[0],[0]
"We have a training set {(x1, y1), ..., (xS , yS)}, whose elements are i.i.d. sampled from X × Y according to distribution",2. Problem Setting,[0],[0]
P. Our goal is to learn a neural network model O ∈,2. Problem Setting,[0],[0]
F : X × Y → R parameterized by w ∈,2. Problem Setting,[0],[0]
Rn based on the training set.,2. Problem Setting,[0],[0]
"Specifically, the neural network models have hierarchical structures, in which each node conducts linear combination and non-linear activation over its connected nodes in the lower layer.",2. Problem Setting,[0],[0]
The parameters are the weights on the edges between two layers.,2. Problem Setting,[0],[0]
"The neural network model produces an output vector, i.e., (O(x, k;w); k ∈ Y) for each input x ∈ X , indicating its likelihoods of belonging to different categories.",2. Problem Setting,[0],[0]
"Because the underlying distribution P is unknown, a common way of learning the model is to minimize the empirical loss function.",2. Problem Setting,[0],[0]
"A widely-used loss function for deep neural networks is the cross-entropy loss,
2We also obtained similar results for the convex cases.",2. Problem Setting,[0],[0]
"Due to space restrictions, we put the corresponding theorems and proofs in the appendix.
which is defined as follows,
f(x, y;w) =",2. Problem Setting,[0],[0]
"− K∑
k=1
(I[y=k] log σk(x;w)).",2. Problem Setting,[0],[0]
"(1)
Here σk(x;w) = e O(x,k;w)∑K
k′=1 e O(x,k′;w) is the Softmax operator.
",2. Problem Setting,[0],[0]
"The objective is to optimize the empirical risk, defined as below,
F (w) = 1 S S∑ s=1 fs(w)",2. Problem Setting,[0],[0]
":= 1 S S∑ s=1 f(xs, ys;w).",2. Problem Setting,[0],[0]
"(2)
As mentioned in the introduction, ASGD is a widely-used approach to perform parallel training of neural networks.",2. Problem Setting,[0],[0]
"Although ASGD is highly efficient, it is well known to suffer from the problem of delayed gradient.",2. Problem Setting,[0],[0]
"To better illustrate this problem, let us have a close look at the training process of ASGD as shown in Figure 1.",2. Problem Setting,[0],[0]
"According to the figure, local worker m starts from wt, the snapshot of the global model at time t, calculates the local gradient g(wt), and then add this gradient back to the global model3.",2. Problem Setting,[0],[0]
"However, before this happens, some other τ workers may have already added their local gradients to the global model, the global model has been updated τ times and becomes wt+τ .",2. Problem Setting,[0],[0]
"The ASGD algorithm is blind to this situation, and simply adds the gradient g(wt) to the global model wt+τ , as follows.
wt+τ+1 = wt+τ",2. Problem Setting,[0],[0]
"− ηg(wt), (3)
where η is the learning rate.
",2. Problem Setting,[0],[0]
It is clear that the above update rule of ASGD is problematic (and inequivalent to that of sequential SGD): one actually adds a “delayed” gradient g(wt) to the current global model wt+τ .,2. Problem Setting,[0],[0]
"In contrast, the correct way is to update the global model wt+τ based on the gradient w.r.t.",2. Problem Setting,[0],[0]
wt+τ .,2. Problem Setting,[0],[0]
"This
3Actually, the local gradient is also related to the randomly sampled data (xit , yit).",2. Problem Setting,[0],[0]
"For simplicity, when there is no confusion, we will omit xit , yit in the notations.
problem of delayed gradient has been well known (Agarwal & Duchi, 2011; Recht et al., 2011; Lian et al., 2015; Avron et al., 2015), and many practical observations indicate that it usually costs ASGD more iterations to converge than sequential SGD, and sometimes, the converged model of ASGD cannot reach accuracy parity of sequential SGD, especially when the number of workers is large (Dean et al., 2012; Ho et al., 2013; Zhang et al., 2015).",2. Problem Setting,[0],[0]
"Researchers have tried to improve ASGD from different perspectives (Ho et al., 2013; McMahan & Streeter, 2014; Zhang et al., 2015; Sra et al., 2015; Mitliagkas et al., 2016), however, to the best of our knowledge, there is still no solution that can compensate the delayed gradient while keeping the high efficiency of ASGD.",2. Problem Setting,[0],[0]
This is exactly the motivation of our paper.,2. Problem Setting,[0],[0]
"As explained in the previous sections, ideally, the optimization algorithm should add gradient g(wt+τ ) to the global model wt+τ , however, ASGD adds a delayed version g(wt).",3. Delay Compensation using Taylor Expansion and Hessian Approximation,[0],[0]
"In this section, we propose a novel method to bridge this gap by using Taylor expansion and Hessian approximation.",3. Delay Compensation using Taylor Expansion and Hessian Approximation,[0],[0]
"The Taylor expansion of the gradient function g(wt+τ ) at wt can be written as follows (Folland, 2005),
g(wt+τ )",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
= g(wt)+∇g(wt)(wt+τ −wt)+O((wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"−wt)2)In, (4)
where ∇g denotes the matrix with the element gij = ∂2f",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
∂wi∂wj for i ∈,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
[n],3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
and j ∈,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"[n], (wt+τ − wt)2 =
(wt+τ,1−wt,1)α1 · · · (wt+τ,n−wt,n)αn with ∑n
i=1 αi = 2",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
and αi ∈ N,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"and In is a n-dimension vector with all the elements equal to 1.
",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
By comparing the above formula with Eqn.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"(3), we can immediately find that ASGD actually uses the zero-order item in Taylor expansion as its approximation to g(wt+τ ), and totally ignores all the higher-order terms ∇g(wt)(wt+τ",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt) + O((wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt)2)In.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
This is exactly the root cause of the problem of delayed gradient.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"With this insight, a straightforward and ideal method is to use the full Taylor expansion to compensate the delay.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"However, this is practically intractable, since it involves the sum of an infinite number of items.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"And even the simplest delay compensation, i.e., additionally keeping the first-order item in the Taylor expansion (which is shown below), is highly nontrivial,
g(wt+τ )",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
≈ g(wt),3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
+∇g(wt)(wt+τ,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
− wt).,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"(5)
This is because the first-order derivative of the gradient function g corresponds to the Hessian matrix of the original loss function f (e.g., cross entropy for neural net-
works), which is defined as Hf(w) =",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"[hij ]i,j=1,··· ,n where hij =
∂2f ∂wi∂wj (w).
",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"For a neural network model with millions of parameters (which is very common and may only be regarded as a medium-size network today), the corresponding Hessian matrix will contain trillions of elements.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
It is clearly very computationally and spatially expensive to obtain such a large matrix4.,3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"Fortunately, as shown in the next subsection, we find an easy-to-compute/store approximator to the Hessian matrix, which makes our proposal of delay compensation technically feasible.",3.1. Gradient Decomposition using Taylor Expansion,[0],[0]
"Computing the exact Hessian matrix is computationally and spatially expensive, especially for large models.",3.2. Approximation of Hessian Matrix,[0],[0]
"Alternatively, we want to find some approximators that are theoretically close to the Hessian matrix, but can be easily stored and computed without introducing additional complexity (i.e., just using what we already have during the previous training process).
",3.2. Approximation of Hessian Matrix,[0],[0]
"First, we show that the outer product of the gradients is an asymptotically unbiased estimation of the Hessian matrix.",3.2. Approximation of Hessian Matrix,[0],[0]
"Let us use G(wt) to denote the outer product matrix of the gradient at wt, i.e.,
G(wt) =",3.2. Approximation of Hessian Matrix,[0],[0]
"( ∂
∂w f(x, y,wt)
)( ∂
∂w f(x, y,wt)
)T .",3.2. Approximation of Hessian Matrix,[0],[0]
"(6)
Because the cross entropy loss is a negative log-likelihood with respect to the Softmax distribution of the model, i.e., P(Y = k|x,wt) , σk(x;wt), it is not difficult to obtain that the outer product of the gradient is an asymptotically unbiased estimation of Hessian, according to the two equivalent methods to calculate the fisher information matrix (Friedman et al., 2001)5:
ϵt , E(y|x,w∗)||G(wt)−H(wt)|| → 0, t → ∞. (7)
",3.2. Approximation of Hessian Matrix,[0],[0]
The assumption behind the above equivalence is that the underlying distribution equals the model distribution with parameter w∗ (or there is no approximation error of the NN hypothesis space) and the training model wt gradually converges to the optimal model w∗ along with the training process.,3.2. Approximation of Hessian Matrix,[0],[0]
"This assumption is reasonable considering the universal approximation property of DNN (Hornik, 1991) and the recent results on the optimality of the local optima of DNN (Choromanska et al., 2015; Kawaguchi, 2016).
",3.2. Approximation of Hessian Matrix,[0],[0]
"Second, we show that by further introducing a welldesigned weight to the outer product of the gradients, we
4Although Hessian-free methods were used in some previous works (Martens, 2010), they double the computation and communication for each local worker and are therefore not very feasible in practice.
",3.2. Approximation of Hessian Matrix,[0],[0]
"5In this paper, the norm of the matrix is Frobenius norm.
can achieve a better trade-off between bias and variance for the approximation.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Although the outer product of the gradients can achieve unbiased estimation to the Hessian matrix, it may induce high approximation error due to potentially large variance.",3.2. Approximation of Hessian Matrix,[0],[0]
"To further control the variance, we use mean square error (MSE) to measure the quality of an approximator, which is defined as follows,
mset(G) =",3.2. Approximation of Hessian Matrix,[0],[0]
"E(y|x,w∗)∥ ( G(wt)−H(wt) ) ||2.",3.2. Approximation of Hessian Matrix,[0],[0]
"(8)
We consider the following new approximator λG(wt) ∆ =",3.2. Approximation of Hessian Matrix,[0],[0]
"[ λgtij ] , and prove that with appropriately set λ, λG(wt) can lead to smaller MSE than G(wt), for arbitrary model wt during the training.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Theorem 3.1 Assume that the loss function is L1-Lipschitz, and for arbitrary k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[K], ∣∣∣ ∂σk∂wi ∣∣∣ ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[li, ui], |σk(x,w∗)σk(x,wt) | ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[α, β].",3.2. Approximation of Hessian Matrix,[0],[0]
If λ ∈,3.2. Approximation of Hessian Matrix,[0],[0]
"[0, 1] makes the following inequality holds,
K∑ k=1
1
σ3k(x,wt) ≥ 2C ( K∑ k=1
1
σk(x,wt)
)2 + 2L21ϵt  , (9) where C = maxi,j 11+λ ( uiujβ liljα )2, and the model wt converges
to the optimal model w∗, then mset(λG) ≤ mset(G).
",3.2. Approximation of Hessian Matrix,[0],[0]
"The following corollary gives simpler sufficient conditions for Theorem 3.1.
",3.2. Approximation of Hessian Matrix,[0],[0]
Corollary 3.2,3.2. Approximation of Hessian Matrix,[0],[0]
A sufficient condition for inequality (9) is ∃k0 ∈,3.2. Approximation of Hessian Matrix,[0],[0]
[K] such that σk0 ∈,3.2. Approximation of Hessian Matrix,[0],[0]
"[ 1− K−1 2C(K2+L21ϵt) , 1 ] .
",3.2. Approximation of Hessian Matrix,[0],[0]
"According to Corollary 3.2, we have the following discussions.",3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that, if wt converges to w∗, ϵt is a decreasing term and approaches 0.",3.2. Approximation of Hessian Matrix,[0],[0]
"Thus, ϵt can be upper bounded by a very small constant for large t. Therefore, the condition on σk(x,wt) is more likely to be satisfied when σk(x,wt) (∃k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
[K]) is close to 1.,3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that this is not a strong condition, since if σk(x,wt) (∀k ∈",3.2. Approximation of Hessian Matrix,[0],[0]
"[K]) is very small, the classification power of the corresponding neural network model will be very weak and not useful in practice.
",3.2. Approximation of Hessian Matrix,[0],[0]
"Third, to reduce the storage of the approximator λG(w), we adopt a widely-used diagonalization trick (Becker et al., 1988), which has shown promising empirical results.",3.2. Approximation of Hessian Matrix,[0],[0]
"To be specific, we only store the diagonal elements of the approximator λG(w) and make all the other elements to be zero.",3.2. Approximation of Hessian Matrix,[0],[0]
"We denote the refined approximator as Diag(λG(w)) and assume that the diagonalization error is upper bounded by ϵD , i.e., ||Diag(H(wt))",3.2. Approximation of Hessian Matrix,[0],[0]
− H(wt)|| ≤ ϵD .,3.2. Approximation of Hessian Matrix,[0],[0]
"We give a uniform upper bound of its MSE in the supplementary materials, from which we can see that λ plays a role of trading off variance and Lipschitz6.
4.",3.2. Approximation of Hessian Matrix,[0],[0]
Delay Compensated ASGD:,3.2. Approximation of Hessian Matrix,[0],[0]
"Algorithm Description
In Section 3, we have shown that Diag(λG(w)) is a cheap approximator of the Hessian matrix, with guaranteed approxi-
6See Lemma 3.1 in Supplementary.
",3.2. Approximation of Hessian Matrix,[0],[0]
Algorithm 1 DC-ASGD:,3.2. Approximation of Hessian Matrix,[0],[0]
"worker m repeat
Pull wt from the parameter server.",3.2. Approximation of Hessian Matrix,[0],[0]
Compute gradient gm = ∇fm(wt).,3.2. Approximation of Hessian Matrix,[0],[0]
"Push gm to the parameter server.
until forever
Algorithm 2 DC-ASGD: parameter server Input: learning rate η, variance control parameter λt.",3.2. Approximation of Hessian Matrix,[0],[0]
"Initialize: t = 0, w0 is initialized randomly, wbak(m) = w0, m ∈ {1, 2, · · · ,M} repeat
if receive “gm"" then wt+1 ← wt−η· ( gm+λtgm⊙gm⊙(wt−wbak(m)) )",3.2. Approximation of Hessian Matrix,[0],[0]
t←,3.2. Approximation of Hessian Matrix,[0],[0]
"t+ 1
else if receive “pull request” then wbak(m)← wt Send wt back to worker m.
end if until forever
mation accuracy.",3.2. Approximation of Hessian Matrix,[0],[0]
"In this section, we will use this approximator to compensate the gradient delay, and call the corresponding algorithm Delay-Compensated ASGD (DC-ASGD).",3.2. Approximation of Hessian Matrix,[0],[0]
"Since Diag(λG(w)) = λg(wt) ⊙ g(wt), where ⊙ indicates the element-wise product, the update rule for DC-ASGD can be written as follows:
wt+τ+1 = wt+τ",3.2. Approximation of Hessian Matrix,[0],[0]
− η (g(wt) + λg(wt)⊙,3.2. Approximation of Hessian Matrix,[0],[0]
"g(wt)⊙ (wt+τ − wt)) , (10)
We call g(wt) + λg(wt) ⊙ g(wt) ⊙ (wt+τ − wt) the delaycompensated gradient for ease of reference.
",3.2. Approximation of Hessian Matrix,[0],[0]
The flow of DC-ASGD is shown in Algorithms 1 and 2.,3.2. Approximation of Hessian Matrix,[0],[0]
Here we assume that DC-ASGD is implemented by using the parameter server framework (although it can also be implemented in other frameworks).,3.2. Approximation of Hessian Matrix,[0],[0]
"According to Algorithm 1, local worker m pulls the latest global model wt from the parameter server, computes its gradient gm and sends it back to the server.",3.2. Approximation of Hessian Matrix,[0],[0]
"According to Algorithm 2, the parameter server will store a backup model wbak(m) when worker m pulls wt.",3.2. Approximation of Hessian Matrix,[0],[0]
"When the delayed gradient gm calculated by worker m is received at time t, the parameter server updates the global model according to Eqn (10).
",3.2. Approximation of Hessian Matrix,[0],[0]
"Please note that as compared to ASGD, DC-ASGD has no extra communication cost and no extra computational requirement on the local workers.",3.2. Approximation of Hessian Matrix,[0],[0]
And the additional computations regarding Eqn(10) only introduce a lightweight overhead to the parameter server.,3.2. Approximation of Hessian Matrix,[0],[0]
"As for the space requirement, for each worker m ∈ {1, 2, · · · ,M}, the parameter server needs to additionally store a backup model wbak(m).",3.2. Approximation of Hessian Matrix,[0],[0]
"This is not a critical issue since the parameter server is usually implemented in a distributed manner, and the parameters and its backup version are stored in CPU-side memory which is usually far beyond the total parameter size.",3.2. Approximation of Hessian Matrix,[0],[0]
"In this case, the cost of DC-ASGD is quite similar to ASGD, which is also reflected by our experiments.
",3.2. Approximation of Hessian Matrix,[0],[0]
The Delay Compensation is not only applicable to ASGD but SSGD.,3.2. Approximation of Hessian Matrix,[0],[0]
"Recently a study on SSGD(Goyal et al., 2017) assumes
g(wt+j) ≈ g(wt) for j < M to make the updates from small and large mini-batch SGD similar, which can be immediately improved by applying delay-compensated gradient.",3.2. Approximation of Hessian Matrix,[0],[0]
Please check the detailed discussion in Supplementary.,3.2. Approximation of Hessian Matrix,[0],[0]
"In this section, we prove the convergence rate of DC-ASGD.",5. Convergence Analysis,[0],[0]
"Due to space restrictions, we only give the results for the non-convex case, and leave the results for the convex case (which is much easier) to the supplementary.
",5. Convergence Analysis,[0],[0]
"In order to present our main theorem, we need to introduce the following mild assumptions.
",5. Convergence Analysis,[0],[0]
"Assumption 1 (Smoothness): (Lian et al., 2015)(Recht et al., 2011)",5. Convergence Analysis,[0],[0]
The loss function is smooth w.r.t.,5. Convergence Analysis,[0],[0]
"the model parameter, and we use L1, L2, L3 to denote the upper bounds of the first, second, and third-order derivatives of the loss function.",5. Convergence Analysis,[0],[0]
"The activation function σk(w) is L-Lipschitz continuous.
",5. Convergence Analysis,[0],[0]
"Assumption 2 (Non-convexity): (Lee et al., 2016)",5. Convergence Analysis,[0],[0]
"The loss function is µ-strongly convex in a ball centered at each local optimum which is denoted as d(wloc, r) with radius r, and twice differential about w.
We also introduce some notations to simplify the presentation of our results, i.e.,
M = max k,wloc
|P(Y = k|x,wloc)− P(Y = k|x,w∗)| ,
H = max k,x,w ∣∣∣∣∂2P(Y = k|x,w)∂2w × 1P(Y = k|x,w) ∣∣∣∣ ,
",5. Convergence Analysis,[0],[0]
∀k ∈,5. Convergence Analysis,[0],[0]
"[K], x, w.
Actually, the non-convexity error ϵnc = HKM , which is defined as the upper bound of the difference between the prediction outputs of the local optima and the global optimum (Please see Lemma 5.1 in the supplementary materials).",5. Convergence Analysis,[0],[0]
We assume that the DC-ASGD search in the set ∥w,5. Convergence Analysis,[0],[0]
"− w′∥22 ≤ π2, ∀w,w′ and denote D0 = F (w1)−F (w∗), C2λ = (L23π2/2+2((1−λ)L21+ ϵD)2+ 2ϵ2nc), C̃2λ = 4T0 maxs=1,··· ,T0 ϵs 2 + 4θ2 log (T − T0) where
T0 ≥ O(1/r4), θ = 2HKLV L2µ2 √ 1 µ ( 1 + L2+λL 2 1 L2 τ ) .
",5. Convergence Analysis,[0],[0]
"With all the above, we have the following theorem.
",5. Convergence Analysis,[0],[0]
Theorem 5.1 Assume that Assumptions 1-2 hold.,5. Convergence Analysis,[0],[0]
"Set the learning rate η = √ 2D0
bTL2V 2 ,where b is the mini-batch size, and V is
the upper bound of the variance of the delay-compensated gradient.",5. Convergence Analysis,[0],[0]
"If T ≥ max{O(1/r4), 2D0bL2/V 2} and delay τ is upperbounded as below,
τ ≤",5. Convergence Analysis,[0],[0]
"min { L2γ
Cλ , γ Cλ ,
√ Tγ
C̃ , L2Tγ 4C̃
} , (11)
",5. Convergence Analysis,[0],[0]
"where γ = √ L2TV 2
2D0b , then DC-ASGD has the following ergodic
convergence rate,
min t={1,··· ,T}
E(∥∇F (wt)∥2) ≤ V √
2D0L2 bT , (12)
where T is the number of iteration, the expectation is taken with respect to the random sampling in SGD and the data distribution P (Y |x,w∗).
",5. Convergence Analysis,[0],[0]
"Proof Sketch7:
Step 1: We denote the delay-compensated gradient as gdcm (wt) where m ∈ {1, · · · , b} is the index of instances in the mini-batch and ∇Fh(wt) = ∇F (wt) + EH(wt)(wt+τ",5. Convergence Analysis,[0],[0]
− wt).,5. Convergence Analysis,[0],[0]
"According to Assumption 1, we have
EF (wt+τ+1)− F (wt+τ )
≤",5. Convergence Analysis,[0],[0]
− bηt+τ 2 ∥∇F (wt+τ )∥2,5. Convergence Analysis,[0],[0]
"+ ∥∥∥∥∥ b∑
m=1
Egdcm (wt) ∥∥∥∥∥ 2 
+bηt+τ ∥∥∥∥∥∇F",5. Convergence Analysis,[0],[0]
(wt+τ ),5. Convergence Analysis,[0],[0]
"− b∑
m=1
∇Fh(wt) ∥∥∥∥∥ 2
+bηt+τ ∥∥∥∥∥ b∑
m=1
Egdcm (wt)− b∑
m=1
Fh(wt) ∥∥∥∥∥ 2
+ η2t+τL2
2 E ∥∥∥∥∥ b∑
m=1
gdcm (wt) ∥∥∥∥∥ 2  .",5. Convergence Analysis,[0],[0]
"(13)
The term ∥∥∥∑bm=1",5. Convergence Analysis,[0],[0]
"Egdcm (wt)−∑bm=1 Fh(wt)∥∥∥2, measured by the expectation with respect to P(Y |x,w∗), is bounded by C2λ · ∥wt+τ",5. Convergence Analysis,[0],[0]
− wt∥2.,5. Convergence Analysis,[0],[0]
"The term
∥∥∥∇F (wt+τ )",5. Convergence Analysis,[0],[0]
"−∑bm=1 ∇Fh(wt)∥∥∥2 can be bounded by L
2 3 4 ∥wt+τ −wt∥4, which will be smaller than
∥wt+τ",5. Convergence Analysis,[0],[0]
− wt∥2,5. Convergence Analysis,[0],[0]
when ∥wt+τ,5. Convergence Analysis,[0],[0]
− wt∥ is small.,5. Convergence Analysis,[0],[0]
"Other terms which are related to the gradients can be further upper bounded by the smoothness property of the loss function.
",5. Convergence Analysis,[0],[0]
"Step 2: We proved that, under the non-convexity assumption, if ∥λg(wt) ⊙ g(wt)∥ ≤",5. Convergence Analysis,[0],[0]
"λL21, then when t > O(1/r4), ϵt ≤ θ √
1 t−T0 + ϵnc, where T0 = O(1/r4).",5. Convergence Analysis,[0],[0]
"That is, we can find a weaker condition for the decreasing of ϵt than that for wt → w∗.
Step 3:",5. Convergence Analysis,[0],[0]
"By plugging in the decreasing rate of ϵt in Step 1 and following a similar proof of the convergence rate of ASGD (Lian et al., 2015), we can get the result in the theorem.
",5. Convergence Analysis,[0],[0]
"Discussions:
(1)",5. Convergence Analysis,[0],[0]
"The above theorem shows that the convergence rate of DCASGD is in the order of O( V√
T ).",5. Convergence Analysis,[0],[0]
"Recall that the convergence rate
of ASGD is O( V1√ T ), where V1 is the variance for the delayed gradient g(wt).",5. Convergence Analysis,[0],[0]
"By simple calculation, V can be upper bounded by V1 + λV2, where V2 is the extra moments of the noise introduced by the delay compensation term.",5. Convergence Analysis,[0],[0]
Thus if we set λ ∈,5. Convergence Analysis,[0],[0]
"[0, V1/V2], DC-ASGD and ASGD will converge at the same rate.",5. Convergence Analysis,[0],[0]
"As the training process goes on, g(w) will become smaller.",5. Convergence Analysis,[0],[0]
"Compared with V1, V2 (composed by variance of g ⊙ g) will not be the dominant order and can be gradually neglected.",5. Convergence Analysis,[0],[0]
"As a result, the feasible range for λ is actually very large.
",5. Convergence Analysis,[0],[0]
"(2) Although DC-ASGD converges at the same rate with ASGD, its tolerance on the delay is much better if T ≥ max{C̃2, 4C̃/L2} and Cλ < min{L2, 1}.",5. Convergence Analysis,[0],[0]
The intuition for the condition on T is that larger T induces smaller step size η.,5. Convergence Analysis,[0],[0]
A small step size means that wt and wt+τ are close to each other.,5. Convergence Analysis,[0],[0]
"According to the upper bound of Taylor expansion series (Folland, 2005), we can see that delay compensated gradient will be more
7Please check the complete proof in the supplementary material.
",5. Convergence Analysis,[0],[0]
accurate than the delayed gradient used in ASGD.,5. Convergence Analysis,[0],[0]
"Since Cλ is related to the diagonalization error ϵD and the non-convexity error ϵnc, smaller ϵD and ϵnc will lead to looser conditions for the convergence.",5. Convergence Analysis,[0],[0]
"If these two error are sufficiently small (which is usually the case according to (Choromanska et al., 2015; Kawaguchi, 2016; LeCun, 1987)), the condition L2 > Cλ can be simplified as L2 > (1 − λ)L21 +",5. Convergence Analysis,[0],[0]
"L3π, which is easy to be satisfied with a small 1−λ.",5. Convergence Analysis,[0],[0]
"Assume that L2−L3π > 0, which is easily to be satisfied if the gradient is small (e.g. at the later stage of the training progress).",5. Convergence Analysis,[0],[0]
"Accordingly, we can obtain the feasible range for λ as λ ∈",5. Convergence Analysis,[0],[0]
"[1 − (L2 − L3π)/2L21, 1].",5. Convergence Analysis,[0],[0]
"λ can be regarded as a trade-off between the extra variance introduced by the delay-compensate term λg(wt)⊙ g(wt) and the bias in Hessian approximation.
",5. Convergence Analysis,[0],[0]
"(3) Actually ASGD is an extreme case for DC-ASGD, with λ = 0.",5. Convergence Analysis,[0],[0]
Another extreme case is with λ = 1.,5. Convergence Analysis,[0],[0]
"DC-ASGD prefers larger T and smaller π, which can lead to a faster speed-up and larger tolerant for delay.
",5. Convergence Analysis,[0],[0]
"Based on the above discussions, we have the following corollary, which indicates that DC-ASGD is superior to ASGD in most cases.
",5. Convergence Analysis,[0],[0]
"Corollary 5.2 Let C0 = max{C̃2, 4C̃/L2}, which is a constant.",5. Convergence Analysis,[0],[0]
If we choose λ ∈,5. Convergence Analysis,[0],[0]
"[ 1− L2−L3π L21 , 1 ] ∩ [0, V1/V2] ∩",5. Convergence Analysis,[0],[0]
"[0, 1] and the number of total iterations T ≥ C0, DC-ASGD will outperform ASGD by a factor of T/C0.",5. Convergence Analysis,[0],[0]
"In this section, we evaluate our proposed DC-ASGD algorithm.",6. Experiments,[0],[0]
"We used two datasets: CIFAR-10 (Hinton, 2007) and ImageNet ILSVRC 2013 (Russakovsky et al., 2015).",6. Experiments,[0],[0]
The experiments were conducted on a GPU cluster interconnected with InfiniBand.,6. Experiments,[0],[0]
Each node has four K40 Tesla GPU processors.,6. Experiments,[0],[0]
We treat each GPU as a separate local worker.,6. Experiments,[0],[0]
"For the DNN algorithm running on
each worker, we chose ResNet (He et al., 2016) since it produces the state-of-the-art accuracy in many image related tasks and its implementation is available through open-source projects8.",6. Experiments,[0],[0]
"For the parallelization of ResNet across machines, we leveraged an open-source parameter server9.
",6. Experiments,[0],[0]
We implemented DC-ASGD on this experimental platform.,6. Experiments,[0],[0]
"We have two versions of implementations, one sets λt = λ0 as a constant, and the other adaptively tunes λt using a moving average method proposed by (Tieleman & Hinton, 2012).",6. Experiments,[0],[0]
"Specifically, we first define a quantity called MeanSquare as follows,
MeanSquare(t) = m·MeanSquare(t−1)+(1−m)·g(wt)2, (14) where m is a constant taking value from [0, 1).",6. Experiments,[0],[0]
And then we divide the initial λ0 by √ MeanSquare(t),6. Experiments,[0],[0]
"+ ϵ, where ϵ = 10−7 for all our experiments",6. Experiments,[0],[0]
.,6. Experiments,[0],[0]
This adaptive method is adopted to reduce the variance among coordinates with historical gradient values.,6. Experiments,[0],[0]
"For ease of reference, we denote the first implementation as DCASGD-c (constant) and the second as DC-ASGD-a (adaptive).
",6. Experiments,[0],[0]
"In addition to DC-ASGD, we also implemented ASGD and SSGD, which have been used in many previous works as baselines (Dean et al., 2012; Chen et al., 2016; Das et al., 2016).",6. Experiments,[0],[0]
"Furthermore, for the experiments on CIFAR-10, we used the sequential SGD algorithm as a reference model to examine the accuracy of parallel algorithms.",6. Experiments,[0],[0]
"However, for the experiments on ImageNet, we were not able to show this reference because it simply took too long time for a single machine to finish the training10.",6. Experiments,[0],[0]
"For sake of fairness, all experiments started from the same randomly initial-
8https://github.com/KaimingHe/ deep-residual-networks
9http://www.dmtk.io/ 10We also implemented the momentum variants of these algorithms.",6. Experiments,[0],[0]
"The corresponding comparisons are very similar to those without momentum.
ized model, and used the same strategy for learning rate scheduling.",6. Experiments,[0],[0]
The data were repartitioned randomly onto the local workers every epoch.,6. Experiments,[0],[0]
The CIFAR-10 dataset consists of a training set of 50k images and a test set of 10k images in 10 classes.,6.1. Experimental Results on CIFAR-10,[0],[0]
We trained a 20-layer ResNet model on this dataset (without data augmentation).,6.1. Experimental Results on CIFAR-10,[0],[0]
"For all the algorithms under investigation, we performed training for 160 epochs, with a mini-batch size of 128, and an initial learning rate which was reduced by ten times after 80 and 120 epochs following the practice in (He et al., 2016).",6.1. Experimental Results on CIFAR-10,[0],[0]
"We performed grid search for the hyper-parameter and the best test performances are obtained by choosing the initial learning rate η = 0.5, λ0 = 0.04 for DC-ASGD-c, and λ0 = 2, m = 0.95 for DC-ASGD-a. We tried different numbers of local workers in our experiments: M = {1, 4, 8}.
",6.1. Experimental Results on CIFAR-10,[0],[0]
"First, we investigate the learning curves with fixed number of effective passes as shown in Figure 2.",6.1. Experimental Results on CIFAR-10,[0],[0]
"From the figure, we have the following observations: (1) Sequential SGD achieves the best accuracy, and its final test error is 8.65%.",6.1. Experimental Results on CIFAR-10,[0],[0]
(2) The test errors of ASGD and SSGD increase with respect to the number of local workers.,6.1. Experimental Results on CIFAR-10,[0],[0]
"In particular, when M = 4, ASGD and SSGD achieve test errors of 9.27% and 9.17% respectively; and when M = 8, their test errors become 10.26% and 10.10% respectively.",6.1. Experimental Results on CIFAR-10,[0],[0]
"These results are reasonable: ASGD suffers from delayed gradients which becomes more serious for a larger number of workers; SSGD increases the effective mini-batch size by M times, and enlarged mini-batch size usually affects the training performances of DNN.",6.1. Experimental Results on CIFAR-10,[0],[0]
"(3) For DC-ASGD, no matter which λt is used, its performance is significantly better than ASGD and SSGD, and catches up with sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
"For example, when M = 4, the test error of DC-ASGD-c is 8.67%, which is indistinguishable from sequential SGD, and the test error for DC-ASGD-a is 8.19%, which is even better than that achieved by sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
It is not by design that DC-ASGD can beat sequential SGD.,6.1. Experimental Results on CIFAR-10,[0],[0]
The test performance lift might be attributed to the regularization effect brought by the variance introduced by parallel training.,6.1. Experimental Results on CIFAR-10,[0],[0]
"When M = 8, DC-ASGD-c can reduce the test error to 9.27%, which is nearly 1% better than ASGD and SSGD, meanwhile the test error is 8.57% for DC-ASGD-a, which again slightly better than sequential SGD.
",6.1. Experimental Results on CIFAR-10,[0],[0]
We further compared the convergence speeds of different algorithms as shown in Figure 3.,6.1. Experimental Results on CIFAR-10,[0],[0]
"From this figure, we have the following observations: (1)",6.1. Experimental Results on CIFAR-10,[0],[0]
"Although the convergent point is not very good, ASGD runs indeed very fast, and achieves almost linear speed-up as compared to sequential SGD in terms of throughput.",6.1. Experimental Results on CIFAR-10,[0],[0]
(2) SSGD also runs faster than sequential SGD.,6.1. Experimental Results on CIFAR-10,[0],[0]
"However, due to the synchronization barrier, it is significantly slower than ASGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
(3) DC-ASGD achieves very good balance between accuracy and speed.,6.1. Experimental Results on CIFAR-10,[0],[0]
"On one hand, its converge speed is very similar to that of ASGD (although it involves a little more computational cost and
some memory cost when compensating the delay).",6.1. Experimental Results on CIFAR-10,[0],[0]
"On the other hand, its convergent point is as good as, or even better than that of sequential SGD.",6.1. Experimental Results on CIFAR-10,[0],[0]
The experiments results clearly demonstrate the effectiveness of our proposed delay compensation technologies11.,6.1. Experimental Results on CIFAR-10,[0],[0]
"In order to further verify our method on the large-scale setting, we conducted the experiment on the ImageNet dataset, which contains 1.28 million training images and 50k validation images in 1000 categories.",6.2. Experimental Results on ImageNet,[0],[0]
"We trained a 50-layer ResNet model (He et al., 2016) on this dataset.
",6.2. Experimental Results on ImageNet,[0],[0]
"According to the previous subsection, DC-ASGD-a seems to be better, therefore in this large-scale experiment, we only implemented DC-ASGD-a.",6.2. Experimental Results on ImageNet,[0],[0]
"For all algorithms in this experiment, we performed training for 120 epochs , with a mini-batch size of 32, and an initial learning rate reduced by ten times after every 30 epochs following the practice in (He et al., 2016).",6.2. Experimental Results on ImageNet,[0],[0]
"We did grid search for hyperparameter tuning and set the initial learning rate η = 0.1, λ0 = 2, m = 0.",6.2. Experimental Results on ImageNet,[0],[0]
"Since the training on the ImageNet dataset is very time consuming, we employed M = 16 GPU nodes in our experiments.",6.2. Experimental Results on ImageNet,[0],[0]
"The top-1 accuracies based on 1-crop testing of different algorithms are given in Figure 4.
",6.2. Experimental Results on ImageNet,[0],[0]
"11Please refer to the supplementary materials for the experiments on tuning the parameter λ.
",6.2. Experimental Results on ImageNet,[0],[0]
"According to the figure, we have the following observations: (1) After processing the same amount of training data, DC-ASGD always outperforms SSGD and ASGD.",6.2. Experimental Results on ImageNet,[0],[0]
"In particular, while the eventual test error achieved by ASGD and SSGD were 25.64% and 25.30% respectively, DC-ASGD achieved a lower error rate of 25.18%.",6.2. Experimental Results on ImageNet,[0],[0]
"Please note this time the accuracy of SSGD is quite good (which is consistent with a separate observation in (Chen et al., 2016)).",6.2. Experimental Results on ImageNet,[0],[0]
An explanation is that the training on ImageNet is less sensitive to the mini-batch size than that on CIFAR-10.,6.2. Experimental Results on ImageNet,[0],[0]
"(2) If we look at the learning curve with respect to wallclock time, SSGD is slowed down due to the synchronization barrier; ASGD and DC-ASGD have similar efficiency, once again indicating that the extra overhead for delay compensation introduced by DC-ASGD can almost be neglected in practice.",6.2. Experimental Results on ImageNet,[0],[0]
"Based on all our experiments, we can clearly see that DC-ASGD has outstanding performance in terms of both classification accuracy and convergence speed, which in return verifies the soundness of our proposed delay compensation technologies.",6.2. Experimental Results on ImageNet,[0],[0]
"In this paper, we have given a theoretical analysis on the problem of delayed gradients in the asynchronous parallelization of stochastic gradient descent (SGD) algorithms, and proposed a novel algorithm called Delay Compensated Asynchronous SGD (DC-ASGD) to tackle the problem.",7. Conclusion,[0],[0]
"We have evaluated DC-ASGD on CIFAR-10 and ImageNet datasets, and the results demonstrate that it can achieve better accuracy than both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.",7. Conclusion,[0],[0]
"As for the future work, we plan to test DCASGD on larger computer clusters, where with the increasing number of local workers, the delay will become more serious.",7. Conclusion,[0],[0]
"Furthermore, we will investigate the economical approximation of higher-order items in the Taylor expansion to achieve more effective delay compensation.",7. Conclusion,[0],[0]
This work is partially supported by the National Natural Science Foundation of China (Grant No. 61371192).,Acknowledgments,[0],[0]
"With the fast development of deep learning, it has become common to learn big neural networks using massive training data.",abstractText,[0],[0]
"Asynchronous Stochastic Gradient Descent (ASGD) is widely adopted to fulfill this task for its efficiency, which is, however, known to suffer from the problem of delayed gradients.",abstractText,[0],[0]
"That is, when a local worker adds its gradient to the global model, the global model may have been updated by other workers and this gradient becomes “delayed”.",abstractText,[0],[0]
"We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD.",abstractText,[0],[0]
This is achieved by leveraging Taylor expansion of the gradient function and efficient approximation to the Hessian matrix of the loss function.,abstractText,[0],[0]
We call the new algorithm,abstractText,[0],[0]
Delay Compensated ASGD (DCASGD).,abstractText,[0],[0]
"We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and the experimental results demonstrate that DC-ASGD outperforms both synchronous SGD and asynchronous SGD, and nearly approaches the performance of sequential SGD.",abstractText,[0],[0]
Asynchronous Stochastic Gradient Descent with Delay Compensation,title,[0],[0]
"√ N) (N being the total
number of iterations) and it can achieve a linear speedup under certain conditions. We perform several experiments on both synthetic and real datasets. The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.",text,[0],[0]
Quasi-Newton (QN) methods are powerful optimization techniques that are able to attain fast convergence rates by incorporating local geometric information through an approximation of the inverse of the Hessian matrix.,1. Introduction,[0],[0]
"The L-BFGS algorithm (Nocedal & Wright, 2006) is a wellknown limited-memory QN method that aims at solving the following optimization problem:
θ? = arg min θ∈Rd
{ U(θ) , NY∑ i=1",1. Introduction,[0],[0]
"Ui(θ) } , (1)
1LTCI, Télécom",1. Introduction,[0],[0]
"ParisTech, Université Paris-Saclay, 75013, Paris, France 2Department of Computer Science, Aalto University, Espoo, 02150, Finland 3Department of Computer Engineering, Boğaziçi",1. Introduction,[0],[0]
"University, 34342, Bebek, Istanbul, Turkey.",1. Introduction,[0],[0]
"Correspondence to: Umut Şimşekli <umut.simsekli@telecomparistech.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"whereU is a twice continuously differentiable function that can be convex or non-convex, and is often referred to as the empirical risk.",1. Introduction,[0],[0]
"In a typical machine learning context, a dataset Y with NY independent and identically distributed (i.i.d.)",1. Introduction,[0],[0]
"data points is considered, which renders the function U as a sum of NY different functions {Ui}NYi=1.
",1. Introduction,[0],[0]
"In large scale applications, the number of data points NY often becomes prohibitively large and therefore using a ‘batch’ L-BFGS algorithm becomes computationally infeasible.",1. Introduction,[0],[0]
"As a remedy, stochastic L-BGFS methods have been proposed (Byrd et al., 2016; Schraudolph et al., 2007; Moritz et al., 2016; Zhou et al., 2017; Yousefian et al., 2017; Zhao et al., 2017), which aim to reduce the computational requirements of L-BFGS by replacing ∇U (i.e. the full gradients that are required by L-BFGS) with some stochastic gradients that are computed on small subsets of the dataset.",1. Introduction,[0],[0]
"However, using stochastic gradients within LBFGS turns out to be a challenging task since it brings additional technical difficulties, which we will detail in Section 2.
",1. Introduction,[0],[0]
"In a very recent study, Berahas et al. (2016) proposed a parallel stochastic L-BFGS algorithm, called multi-batch L-BFGS (mb-L-BFGS), which is suitable for synchronous distributed architectures.",1. Introduction,[0],[0]
"This work illustrated that carrying out L-BFGS in a distributed setting introduces further theoretical and practical challenges; however, if these challenges are addressed, stochastic L-BFGS can be powerful in a distributed setting as well, and outperform conventional algorithms such as distributed stochastic gradient descent (SGD), as shown by their experimental results.
",1. Introduction,[0],[0]
"Despite the fact that synchronous parallel algorithms have clear advantages over serial optimization algorithms, the computational efficiency of synchronous algorithms is often limited by the overhead induced by the synchronization and coordination among the worker processes.",1. Introduction,[0],[0]
"Inspired by asynchronous parallel stochastic optimization techniques (Agarwal & Duchi, 2011; Lian et al., 2015; Zhang et al., 2015; Zhao & Li, 2015; Zheng et al., 2017), in this study, we propose an asynchronous parallel stochastic L-BFGS algorithm for large-scale non-convex optimization problems.",1. Introduction,[0],[0]
"The proposed approach aims at speeding up the synchronous algorithm presented in (Berahas et al., 2016) by allowing all the workers work independently from each
other and circumvent the inefficiencies caused by synchronization and coordination.
",1. Introduction,[0],[0]
Extending stochastic L-BFGS to asynchronous settings is a highly non-trivial task and brings several challenges.,1. Introduction,[0],[0]
"In our strategy, we first reformulate the optimization problem (1) as a sampling problem where the goal becomes drawing random samples from a distribution whose density is concentrated around θ?.",1. Introduction,[0],[0]
"We then build our algorithm upon the recent stochastic gradient Markov Chain Monte Carlo (SG-MCMC) techniques (Chen et al., 2015; 2016b) that have close connections with stochastic optimization techniques (Dalalyan, 2017; Raginsky et al., 2017; Zhang et al., 2017), and have proven successful in large-scale Bayesian machine learning.",1. Introduction,[0],[0]
We provide formal theoretical analysis and prove non-asymptotic guarantees for the proposed algorithm.,1. Introduction,[0],[0]
"Our theoretical results show that the proposed algorithm achieves an ergodic global convergence with rate O(1/ √ N), whereN denotes the total number of iterations.",1. Introduction,[0],[0]
"Our results further imply that the algorithm can achieve a linear speedup under ideal conditions.
",1. Introduction,[0],[0]
"For evaluating the proposed method, we conduct several experiments on synthetic and real datasets.",1. Introduction,[0],[0]
The experimental results support our theory: our experiments on a large-scale matrix factorization problem show that the proposed algorithm provides a significant speedup over the synchronous parallel L-BFGS algorithm.,1. Introduction,[0],[0]
"Preliminaries: As opposed to the classical optimization perspective, we look at the optimization problem (1) from a maximum a-posteriori (MAP) estimation point of view, where we consider θ as a random variable in Rd and θ?",2. Technical Background,[0],[0]
as the optimum of a Bayesian posterior whose density is given as p(θ|Y ) ∝ exp(−U(θ)),2. Technical Background,[0],[0]
", where Y ≡ {Y1, . . .",2. Technical Background,[0],[0]
", YNY } is a set of i.i.d. observed data points.",2. Technical Background,[0],[0]
"Within this context, U(θ) is often called the potential energy and defined as U(θ) = −[log p(θ) + ∑NY i=1",2. Technical Background,[0],[0]
"log p(Yi|θ)], where p(Yi|θ) is the likelihood function and p(θ) is the prior density.",2. Technical Background,[0],[0]
"In a classical optimization context, − log p(Yi|θ) would correspond to the data-loss and− log p(θ) would correspond to a regularization term.",2. Technical Background,[0],[0]
"Throughout this study, we will assume that the problem (1) has a unique solution in Rd.
",2. Technical Background,[0],[0]
"We define a stochastic gradient ∇Ũ(θ), that is an unbiased estimator of ∇U , as follows: ∇Ũ(θ) =",2. Technical Background,[0],[0]
−[∇ log p(θ) + NY,2. Technical Background,[0],[0]
"NΩ ∑ i∈Ω∇ log p(Yi|θ)], where Ω ⊂ {1, . . .",2. Technical Background,[0],[0]
", NY } denotes a random data subsample that is drawn with replacement, NΩ = |Ω| is the cardinality of Ω.",2. Technical Background,[0],[0]
"In the sequel, we will occasionally use the notation∇Ũn and∇ŨΩ to denote the stochastic gradient computed at iteration n of a given algorithm, or on a specific data subsample Ω, respectively.
",2. Technical Background,[0],[0]
The L-BFGS algorithm:,2. Technical Background,[0],[0]
"The L-BFGS algorithm itera-
tively applies the following equation in order to find the MAP estimate given in (1):
θn = θn−1 − hHn∇U(θn−1) (2)
where n denotes the iterations.",2. Technical Background,[0],[0]
"Here, Hn is an approximation to the inverse Hessian at θn−1 and is computed by using the M past values of the ‘iterate differences’ sn , θn − θn−1, and ‘gradient differences’ yn , ∇U(θn) − ∇U(θn−1).",2. Technical Background,[0],[0]
The collection of the iterate and gradient differences is called the L-BFGS memory.,2. Technical Background,[0],[0]
"The matrix-vector product Hn∇U(θn−1) is often implemented by using the two-loop recursion (Nocedal & Wright, 2006), which has linear time and space complexities O(Md).
",2. Technical Background,[0],[0]
"In order to achieve computational scalability, stochastic LBFGS algorithms replace ∇U with ∇Ũ .",2. Technical Background,[0],[0]
"This turns out to be problematic, since the gradient differences yn would be inconsistent, meaning that the stochastic gradients in different iterations will be computed on different data subsamples, i.e. Ωn−1 and Ωn.",2. Technical Background,[0],[0]
"On the other hand, in the presence of the stochastic gradients, L-BFGS is no longer guaranteed to produce positive definite approximations even in convex problems, therefore more considerations should be taken in order to make sure that Hn is positive definite.
",2. Technical Background,[0],[0]
Stochastic Gradient Markov Chain Monte Carlo:,2. Technical Background,[0],[0]
"Along with the recent advances in MCMC techniques, diffusion-based algorithms have become increasingly popular due to their applicability in large-scale machine learning applications.",2. Technical Background,[0],[0]
"These techniques, so called the Stochastic Gradient MCMC (SG-MCMC) algorithms, aim at generating samples from the posterior distribution p(θ|Y ) as opposed to finding the MAP estimate, and have strong connections with stochastic optimization techniques (Dalalyan, 2017).",2. Technical Background,[0],[0]
"In this line of work, Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011) is one of the pioneering algorithms and generates an approximate sample θn from p(θ|Y ) by iteratively applying the following update equation:
θn = θn−1 − h∇Ũn(θn−1) + √ 2h/βZn (3)
where h is the step-size and {Zn}Nn=1 is a collection of standard Gaussian random variables in Rd.",2. Technical Background,[0],[0]
"Here, β is called the inverse temperature: it is fixed to β = 1 in vanilla SGLD and when β 6= 1 the algorithm is called ‘tempered’.",2. Technical Background,[0],[0]
"In an algorithmic sense, SGLD is identical to SGD, except that it injects a Gaussian noise at each iteration and it coincides with SGD when β goes to infinity.
",2. Technical Background,[0],[0]
"SGLD has been extended in several directions (Ma et al., 2015; Chen et al., 2015; Şimşekli et al., 2016b; Şimşekli, 2017).",2. Technical Background,[0],[0]
"In (Şimşekli et al., 2016a), we proposed an LBFGS-based SGLD algorithm with O(M2d) computational complexity, which aimed to improve the convergence
speed of the vanilla SGLD.",2. Technical Background,[0],[0]
"We showed that a straightforward way of combining L-BFGS in SGLD would incur an undesired bias; however, the remedy to prevent this bias resulted in numerical instability, which would limit the applicability of the algorithm.",2. Technical Background,[0],[0]
"In other recent studies, SGLD has also been extended to synchronous (Ahn et al., 2014) and asynchronous (Chen et al., 2016b; Springenberg et al., 2016) distributed MCMC settings.
",2. Technical Background,[0],[0]
"SGLD can be seen as a discrete-time simulation of a continuous-time Markov process that is the solution of the following stochastic differential equation (SDE):
dθt = −∇U(θt)dt+ √ 2/βdWt, (4)
where Wt denotes the standard Brownian motion in Rd.",2. Technical Background,[0],[0]
"Under mild regularity conditions on U , the solution process (θt)t≥0 attains a unique stationary distribution with a density that is proportional to exp(−βU(θ))",2. Technical Background,[0],[0]
"(Roberts & Stramer, 2002).",2. Technical Background,[0],[0]
"An important property of this distribution is that, as β goes to infinity, this density concentrates around the global minimum of U(θ) (Hwang, 1980; Gelfand & Mitter, 1991).",2. Technical Background,[0],[0]
"Therefore, for large enough β, a random sample that is drawn for the stationary distribution of (θt)t≥0 would be close to θ?.",2. Technical Background,[0],[0]
"Due to this property, SG-MCMC methods have recently started drawing attention from the non-convex optimization community.",2. Technical Background,[0],[0]
Chen et al. (2016a) developed an annealed SG-MCMC algorithm for non-convex optimization and it was recently extended by Ye et al. (2017).,2. Technical Background,[0],[0]
"Raginsky et al. (2017) and Xu et al. (2017) provided finite-time guarantees for SGLD to find an ‘approximate’ global minimizer that is close to θ?, which imply that the additive Gaussian noise in SGLD can help the algorithm escape from poor local minima.",2. Technical Background,[0],[0]
"In a complementary study, Zhang et al. (2017) showed that SGLD enters a neighborhood of a local minimum of U(θ) in polynomial time, which shows that even if SGLD fails to find the global optimum, it will still find a point that is close to one of the local optima.",2. Technical Background,[0],[0]
"Even though these results showed that SG-MCMC is promising for optimization, it is still not clear how an asynchronous stochastic L-BFGS method could be developed within an SG-MCMC framework.",2. Technical Background,[0],[0]
"In this section, we propose a novel asynchronous L-BFGSbased (tempered) SG-MCMC algorithm that aims to provide an approximate optimum that is close to θ? by generating samples from a distribution that has a density that is proportional to exp(−βU(θ)).",3. Asynchronous Stochastic L-BFGS,[0],[0]
We call the proposed algorithm asynchronous parallel stochastic L-BFGS (as-LBFGS).,3. Asynchronous Stochastic L-BFGS,[0],[0]
Our method is suitable for both distributed and shared-memory settings.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"We will describe the algorithm only for the distributed setting; the shared-memory version is almost identical to the distributed version as long as the
updates are ensured to be atomic.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"We consider a classical asynchronous optimization architecture, which is composed of a master node, several worker nodes, and a data server.",3. Asynchronous Stochastic L-BFGS,[0],[0]
The main task of the master node is to maintain the newest iterate of the algorithm.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"At each iteration, the master node receives an additive update vector from a worker node, it adds this vector to the current iterate in order to obtain the next iterate, and then it sends the new iterate to the worker node which has sent the update vector.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"On the other hand, the worker nodes work in a completely asynchronous manner.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"A worker node receives the iterate from the master node, computes an update vector, and sends the update vector to the master node.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"However, since the iterate would be possibly modified by another worker node which runs asynchronously in the mean time, the update vector that is sent to the server will thus be computed on an old iterate, which causes both practical and theoretical challenges.",3. Asynchronous Stochastic L-BFGS,[0],[0]
Such updates are aptly called ‘delayed’ or ‘stale’.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"The full data is kept in the data server and we assume that all the workers have access to the data server.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"The proposed algorithm iteratively applies the following update equations in the master node:
un+1 = un + ∆un+1, θn+1 = θn + ∆θn+1, (5)
where n is the iteration index, un is called the momentum variable, and ∆un+1 and ∆θn+1 are the update vectors that are computed by the worker nodes.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"A worker node runs the following equations in order to compute the update vectors:
∆un+1 ,− h′Hn+1(θn−ln)∇Ũn+1(θn−ln)− γ′un−ln + √ 2h′γ′/βZn+1, (6)
∆θn+1 ,Hn+1(θn−ln)un−ln , (7)
where h′ is the step-size, γ′ > 0 is the friction parameter that determines the weight of the momentum, β is the inverse temperature, {Zn}n denotes standard Gaussian random variables, and Hn denotes the L-BFGS matrix at iteration n. Here,",3. Asynchronous Stochastic L-BFGS,[0],[0]
ln ≥ 0 denotes the ‘staleness’ of a particular update and measures the delay between the current update and the up-to-date iterate that is stored in the master node.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"We assume that the delays are bounded, i.e. maxn ln ≤",3. Asynchronous Stochastic L-BFGS,[0],[0]
"lmax < ∞. Note that the matrix-vector products have O(Md) time-space complexity.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Due to the asynchrony, the stochastic gradients and the L-BFGS matrices will be computed on the delayed variables θn−ln and un−ln .",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As opposed to the asynchronous stochastic gradient algorithms, where the main difficulty stems from the delayed gradients, our algorithm faces further challenges since it is not straightforward to obtain the gradient and iterate differences that are required for the LBFGS computations in an asynchronously parallel setting.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
Algorithm 1: as-L-BFGS:,3. Asynchronous Stochastic L-BFGS,[0],[0]
"Master node 1 input: θ0, u0 // Global iteration index 2 n← 0 3",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send (θ0, u0) to all the workers w = 1, . . .",3. Asynchronous Stochastic L-BFGS,[0],[0]
",W 4 while n < N",3. Asynchronous Stochastic L-BFGS,[0],[0]
"do 5 Receive (∆θn+1,∆un+1) from worker w
//",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Generate the new iterates
6 un+1 = un + ∆un+1, θn+1 = θn + ∆θn+1 7",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send the iterates (θn+1, un+1) to worker w 8 Set n← n+ 1
We propose the following approach for the computation of the L-BFGS matrices.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As opposed to the mb-L-BFGS algorithm, which uses a central L-BFGS memory (i.e. the collection of the gradient and iterate differences) that is stored in the master node, we let each worker have their own local L-BFGS memories since the master node would not be able to keep track of the gradient and iterate differences, which are received in an asynchronous manner.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"In our strategy, each worker updates its own L-BFGS memory right after sending the update vector to the master node.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"The overall algorithm is illustrated in Algorithms 1 and 2 (W denotes the number of workers).
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"In order to be able to have consistent gradient differences, each worker applies a multi-batch subsampling strategy that is similar to mb-L-BFGS.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"We divide the data subsample into two subsets, i.e. Ωn = {Sn, On} with NS , |Sn|, NO , |On|, and NΩ = NS +NO.",3. Asynchronous Stochastic L-BFGS,[0],[0]
Here the main idea is to chooseNS,3. Asynchronous Stochastic L-BFGS,[0],[0]
NO and useOn as an overlapping subset for the gradient differences.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"In this manner, in addition to the gradients that are computed on Sn andOn, we also perform an extra gradient computation on the previous overlapping subset, at the end of each iteration.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"As NO will be small, this extra cost will not be significant.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Finally, in order to ensure the L-BFGS matrices are positive definite, we use a ‘cautious’ update mechanism that is useful for non-convex settings (Li & Fukushima, 2001; Zhang & Sutton, 2011; Berahas et al., 2016) as shown in Algorithm 2.
",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Note that, in addition to asynchrony, the proposed algorithm also extends the current stochastic L-BFGS methods by introducing momentum.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"This brings two critical practical features: (i) without the existence of the momentum variables, the injected Gaussian noise must depend on the L-BFGS matrices, as shown in (Şimşekli et al., 2016a), which results in an algorithm withO(M2d) time complexity whereas our algorithm hasO(Md) time complexity, (ii) the use of the momentum significantly repairs the numerical instabilities caused by the asynchronous updates, since un inherently encapsulates a direction for θn, which provides additional information to the algorithm besides the gradients and L-BFGS computations.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Furthermore, in a
Algorithm 2: as-L-BFGS: Worker node (w) 1 input: M , γ, NS , NO (NΩ = NS +NO) //",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Local iteration index 2 i← 0 3 while the master node is running do 4 Receive (θn−ln , un−ln) from the master 5 Draw a subsample Ωn+1 = {Sn+1, On+1}
//",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Gradient computation
6 ∇Ũn+1(θn−ln) =",3. Asynchronous Stochastic L-BFGS,[0],[0]
"NO NΩ ∇ŨOn+1(θn−ln) + NSNΩ∇ŨSn+1(θn−ln)
7 Compute (∆θn+1,∆un+1) by (6) and (7) 8",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Send (∆θn+1,∆un+1) to the master
// Local variables for L-BFGS
9 θ̃i = θn−ln , Õi = On+1, g̃i = ∇ŨOn+1(θn−ln) 10 if i ≥ 1 then
// Compute the overlapping gradient
11 g′ = ∇ŨÕi−1(θ̃i) //",3. Asynchronous Stochastic L-BFGS,[0],[0]
Compute the L-BFGS variables 12 si = θ̃i,3. Asynchronous Stochastic L-BFGS,[0],[0]
"− θ̃i−1, yi = g′",3. Asynchronous Stochastic L-BFGS,[0],[0]
"− g̃i−1 // Cautious memory update 13 Add (si, yi) to the L-BFGS memory only if y>i si ≥ ‖si‖2 for some > 0",3. Asynchronous Stochastic L-BFGS,[0],[0]
14,3. Asynchronous Stochastic L-BFGS,[0],[0]
"Set i← i+ 1
very recent study (Loizou & Richtárik, 2017)",3. Asynchronous Stochastic L-BFGS,[0],[0]
the use of momentum variables has been shown to be useful in other second-order optimization methods.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"On the other hand, despite their advantages, the momentum variable also drifts apart the proposed algorithm from the original L-BFGS formulation.",3. Asynchronous Stochastic L-BFGS,[0],[0]
"However, even such approximate approaches have proven useful in various scenarios (Zhang & Sutton, 2011; Fu et al., 2016).",3. Asynchronous Stochastic L-BFGS,[0],[0]
"Also note that, when β → ∞, lmax = 0, and Hn(θ) = I for all n, the algorithm coincides with SGD with momentum.",3. Asynchronous Stochastic L-BFGS,[0],[0]
A more detailed illustration is given in the supplementary document.,3. Asynchronous Stochastic L-BFGS,[0],[0]
"In this section, we will provide non-asymptotic guarantees for the proposed algorithm.",4. Theoretical Analysis,[0],[0]
Our analysis strategy is different from the conventional analysis approaches for stochastic optimization and makes use of tools from analysis of SDEs.,4. Theoretical Analysis,[0],[0]
"In particular, we will first develop a continuoustime Markov process whose marginal stationary measure admits a density that is proportional to exp(−βU(θ)).",4. Theoretical Analysis,[0],[0]
Then we will show that (5)-(7) form an approximate EulerMaruyama integrator that approximately simulates this continuous process in discrete-time.,4. Theoretical Analysis,[0],[0]
"Finally, we will analyze this approximate numerical scheme and provide a non-asymptotic error bound.",4. Theoretical Analysis,[0],[0]
"All the proofs are given in the supplementary document.
",4. Theoretical Analysis,[0],[0]
"We start by considering the following stochastic dynamical system:
dpt= [ 1 β Γt(θt)−Ht(θt)∇θU(θt)− γpt ]",4. Theoretical Analysis,[0],[0]
"dt+ √ 2γ β dWt
dθt =Ht(θt)ptdt (8)
where pt ∈ Rd is also called the momentum variable,Ht(·) denotes the L-BFGS matrix at time t and Γt(·) is a vector that is defined as follows:[
Γt(θ) ]",4. Theoretical Analysis,[0],[0]
"i , d∑ j=1 ∂[Ht(θ)]ij ∂[θ]j , (9)
where [v]i denotes the ith component of a vector v and similarly [M ]ij denotes a single element of a matrix M .
",4. Theoretical Analysis,[0],[0]
"In order to analyze the invariant measure of the SDE defined in (8), we need certain conditions to hold.",4. Theoretical Analysis,[0],[0]
"First, we have two regularity assumptions on U and Ht:
H1.",4. Theoretical Analysis,[0],[0]
"The gradient of the potential is Lipschitz continuous, i.e. ‖∇θU(θ)−∇θU(θ′)‖ ≤ L‖θ",4. Theoretical Analysis,[0],[0]
"− θ′‖, ∀θ, θ′ ∈ Rd.",4. Theoretical Analysis,[0],[0]
H 2.,4. Theoretical Analysis,[0],[0]
"The L-BFGS matrices have bounded second-order derivatives and they are Lipschitz continuous, i.e. ‖Ht(θ)− Ht(θ ′)‖ ≤ LH‖θ",4. Theoretical Analysis,[0],[0]
"− θ′‖, ∀θ, θ′ ∈ Rd, t ≥ 0.
",4. Theoretical Analysis,[0],[0]
"The assumptions H1 and H2 are standard conditions in analysis of SDEs (Duan, 2015) and similar assumptions have also been considered in stochastic gradient (Moulines & Bach, 2011) and stochastic L-BFGS algorithms (Zhou et al., 2017).",4. Theoretical Analysis,[0],[0]
"Besides, H2 provides a direct control on the partial derivatives of Ht, which will be useful for analyzing the overall numerical scheme.",4. Theoretical Analysis,[0],[0]
"We now present our first result that establishes the invariant measure of the SDE (8).
",4. Theoretical Analysis,[0],[0]
Proposition 1.,4. Theoretical Analysis,[0],[0]
Assume that the conditions H1 and 2 hold.,4. Theoretical Analysis,[0],[0]
Let,4. Theoretical Analysis,[0],[0]
Xt =,4. Theoretical Analysis,[0],[0]
"[θ>t , p > t ] > ∈ R2d and (Xt)t≥0 be a Markov process that is a solution of the SDE given in (8).",4. Theoretical Analysis,[0],[0]
Then (Xt)t≥0 has a unique invariant measure π that admits a density ρ(X) ∝,4. Theoretical Analysis,[0],[0]
exp(−E(X)),4. Theoretical Analysis,[0],[0]
"with respect to the Lebesgue measure, where E is an energy function on the extended state space and is defined as: E(X) , βU(θ) + β2 p >",4. Theoretical Analysis,[0],[0]
"p.
",4. Theoretical Analysis,[0],[0]
"This result shows that, if the SDE (8) could be exactly simulated, the marginal distribution of the samples θt would converge to a measure πθ which has a density that is proportional to exp(−βU(θ)).",4. Theoretical Analysis,[0],[0]
"Therefore, for large enough β and t, θt would be close to the global optimum θ?.
",4. Theoretical Analysis,[0],[0]
"We note that when β = 1, the SDE (8) shares similarities with the SDEs presented in (Fu et al., 2016; Ma et al., 2015).",4. Theoretical Analysis,[0],[0]
"While the main difference being the usage of the tempering scheme, (Fu et al., 2016) further differs from our approach as it directly discard the term Γt since is in a Metropolis-Hastings framework, which is not adequate for large-scale applications.",4. Theoretical Analysis,[0],[0]
"On the other hand, the stochastic
gradient Riemannian Hamiltonian Monte Carlo algorithm given in (Ma et al., 2015), chooses Ht as the Fisher information matrix; a quantity that requires O(d2) space-time complexity and is not analytically available in general.
",4. Theoretical Analysis,[0],[0]
We will now show that the proposed algorithm (5)-(7) form an approximate method for simulating (8) in discrete-time.,4. Theoretical Analysis,[0],[0]
"For illustration, we first consider the Euler-Maruyama integrator for (8), given as follows:
pn+1 =",4. Theoretical Analysis,[0],[0]
pn,4. Theoretical Analysis,[0],[0]
"− hHn(θn)∇θU(θn)− hγpn + h
β Γn(θn) +",4. Theoretical Analysis,[0],[0]
"√
2hγ/βZn+1, (10) θn+1 = θn + hHn(θn)pn.",4. Theoretical Analysis,[0],[0]
"(11)
Here, the term (1/β)Γn introduces an additional computational burden and its importance is very insignificant (i.e. its magnitude is of order O(1/β) due to H2).",4. Theoretical Analysis,[0],[0]
"Therefore, we discard Γn, define un , hpn, γ′ , hγ, h′ , h2, and use these quantities in (10) and (11).",4. Theoretical Analysis,[0],[0]
"We then obtain the following re-parametrized Euler integrator:
un+1=un−h′Hn(θn)∇θU(θn)−γ′un+ √ 2h′γ′/βZn+1
θn+1=θn+Hn(θn)un
The detailed derivation is given in the supplementary document.",4. Theoretical Analysis,[0],[0]
"Finally, we replace∇U with the stochastic gradients, replace the variables θn and un with stale variables θn−ln and pn−ln in the update vectors, and obtain the ultimate update equations, given in (5).",4. Theoretical Analysis,[0],[0]
"Note that, due to the negligence of Γn, the proposed approach would require a large β and would not be suitable for classical posterior sampling settings, where β = 1.
",4. Theoretical Analysis,[0],[0]
"In this section, we will analyze the ergodic error E[ÛN",4. Theoretical Analysis,[0],[0]
"− U?], where we define ÛN , (1/N) ∑N n=1 U(θn) and U? , U(θ?).",4. Theoretical Analysis,[0],[0]
"This error resembles the bias of a statistical estimator; however, as opposed to the bias, it directly measures the expected discrepancy to the global optimum.",4. Theoretical Analysis,[0],[0]
"Similar ergodic error notions have been considered in the analysis of non-convex optimization methods (Lian et al., 2015; Chen et al., 2016a; Berahas et al., 2016).
",4. Theoretical Analysis,[0],[0]
"In our proof strategy, we decompose the error into two terms: E[ÛN −U?] = A1 +A2, whereA1 , E[ÛN − Ūβ",4. Theoretical Analysis,[0],[0]
],4. Theoretical Analysis,[0],[0]
"A2 , [Ūβ−U?] ≥ 0, and Ūβ , ∫ Rd U(θ)πθ(dθ).",4. Theoretical Analysis,[0],[0]
"We then upper-bound these terms separately.
",4. Theoretical Analysis,[0],[0]
"The term A1 turns out to be the bias of a statistical estimator, which we can analyze by using ideas from recent SGMCMC studies.",4. Theoretical Analysis,[0],[0]
"However, existing tools cannot be directly used because of the additional difficulties introduced by the L-BFGS matrices.",4. Theoretical Analysis,[0],[0]
"In order to bound A1, we first require the following smoothness and boundedness condition.
H3.",4. Theoretical Analysis,[0],[0]
"Let ψ be a functional that is the unique solution of a
Poisson equation that is defined as follows:
Lnψ(Xn) = U(θn)− Ūβ , (12)
where Xn =",4. Theoretical Analysis,[0],[0]
"[θ>n , p > n ]",4. Theoretical Analysis,[0],[0]
">, Ln is the generator of (8) at t = nh and is formally defined in the supplementary document.",4. Theoretical Analysis,[0],[0]
"The functional ψ and its up to third-order derivatives Dkψ are bounded by a function V (X), such that ‖Dkψ‖ ≤",4. Theoretical Analysis,[0],[0]
"CkV rk for k = 0, 1, 2, 3 and Ck, rk > 0.",4. Theoretical Analysis,[0],[0]
"Furthermore, supnEV
r(Xn) <∞ and V is smooth such that sups∈(0,1) V
r(sX + (1 − s)X ′) ≤",4. Theoretical Analysis,[0],[0]
C(V r(X) + V r(X ′)),4. Theoretical Analysis,[0],[0]
"for all X,X ′ ∈ R2d, r ≤ max 2rk, and C > 0.
Assumption H3 is also standard in SDE analysis and SGMCMC (Mattingly et al., 2010; Teh et al., 2016; Chen et al., 2015; Durmus et al., 2016) and gives us control over the weak error of the numerical integrator.",4. Theoretical Analysis,[0],[0]
We further require the following regularity conditions in order to have control over the error induced by the delayed stochastic gradients.,4. Theoretical Analysis,[0],[0]
H4.,4. Theoretical Analysis,[0],[0]
"The variance of the stochastic gradients is bounded, i.e. E‖∇θU(θ)−∇θŨ(θ)‖2 ≤ σ for some 0 <",4. Theoretical Analysis,[0],[0]
σ,4. Theoretical Analysis,[0],[0]
<∞. H5.,4. Theoretical Analysis,[0],[0]
"For a smooth and bounded function f , the remainder rLn,f (·) in the following Taylor expansion is bounded:
ehLnf(X) = f(X) + hLnf(X)",4. Theoretical Analysis,[0],[0]
"+ h2rLn,f (X).",4. Theoretical Analysis,[0],[0]
"(13)
The following lemma presents an upper-bound for A1.",4. Theoretical Analysis,[0],[0]
Lemma 1.,4. Theoretical Analysis,[0],[0]
Assume the conditions H1-5 hold.,4. Theoretical Analysis,[0],[0]
"We have the following bound for the bias:∣∣E[ÛN − Ūβ ]∣∣ = O( 1
Nh + max(lmax, 1)h+
1
β
) .",4. Theoretical Analysis,[0],[0]
"(14)
Here, the term 1/β in (14) appears due to the negligence of Γn.",4. Theoretical Analysis,[0],[0]
"In order to bound the second term A2, we follow a similar strategy to (Raginsky et al., 2017), where we use H 1 and the following moment condition on πθ.",4. Theoretical Analysis,[0],[0]
H6.,4. Theoretical Analysis,[0],[0]
The second-order moments of πθ are bounded and satisfies the following inequality: ∫ Rd ‖θ‖2πθ(dθ) ≤,4. Theoretical Analysis,[0],[0]
"Cββ , for some Cβ > max(βd/(2πe), de/L).
",4. Theoretical Analysis,[0],[0]
This assumption is mild since πθ concentrates around θ?,4. Theoretical Analysis,[0],[0]
as β tends to infinity.,4. Theoretical Analysis,[0],[0]
"The order 1/β is arbitrary, hence the assumption can be further relaxed.",4. Theoretical Analysis,[0],[0]
The following lemma establishes an upper-bound for A2.,4. Theoretical Analysis,[0],[0]
Lemma 2.,4. Theoretical Analysis,[0],[0]
"Under assumptions H1 and 6, the following bound holds: Ūβ − U? =",4. Theoretical Analysis,[0],[0]
"O(1/β).
",4. Theoretical Analysis,[0],[0]
"We now present our main result, which can be easily proven by combining Lemmas 1 and 2.",4. Theoretical Analysis,[0],[0]
Theorem 1.,4. Theoretical Analysis,[0],[0]
Assume that the conditions H1-6 hold.,4. Theoretical Analysis,[0],[0]
Then the ergodic error of the proposed algorithm is bounded as follows:∣∣EÛN,4. Theoretical Analysis,[0],[0]
− U?∣∣,4. Theoretical Analysis,[0],[0]
"= O( 1
Nh + max(1, lmax)h+
1
β
) .",4. Theoretical Analysis,[0],[0]
"(15)
More explicit constants and a discussion on the relation of the theorem to other recent theoretical results are provided in the supplementary document.
",4. Theoretical Analysis,[0],[0]
Theorem 1 provides a non-asymptotic guarantee for convergence to a point that is close to the global optimizer θ?,4. Theoretical Analysis,[0],[0]
"even when U is non-convex, thanks to the additive Gaussian noise.",4. Theoretical Analysis,[0],[0]
"The bound suggests an optimal rate of convergence of O(1/ √ N), which is in line with the current rates of the non-convex asynchronous algorithms (Lian et al., 2015).",4. Theoretical Analysis,[0],[0]
"Furthermore, if we assume that the total number of iterations N is a linear function of the number of workers, e.g. N = NWW , where NW is the number of iterations executed by a single worker, Theorem 1 implies that, in the ideal case, the proposed algorithm can achieve a linear speedup with increasing W , provided that lmax = O(1/(Nh2)).
",4. Theoretical Analysis,[0],[0]
"Despite their nice theoretical properties, it is well-known that tempered sampling approaches also often get stuck near a local minimum.",4. Theoretical Analysis,[0],[0]
"In our case, this behavior would be mainly due to the hidden constant in (14), which can be exponential in dimension d, as illustrated in (Raginsky et al., 2017) for SGLD.",4. Theoretical Analysis,[0],[0]
"On the other hand, Theorem 1 does not guarantee that the proposed algorithm will converge to a neighborhood of a local minimum; however, we believe that we can also prove local convergence guarantees by using the techniques provided in (Zhang et al., 2017; Tzen et al., 2018), which we leave as a future work.",4. Theoretical Analysis,[0],[0]
"The performance of asynchronous stochastic gradient methods has been evaluated in several studies, where the advantages and limitations have been illustrated in various scenarios, to name a few (Dean et al., 2012; Zhang et al., 2015; Zheng et al., 2017).",5. Experiments,[0],[0]
"In this study, we will explore the advantages of using L-BFGS in an asynchronous environment.",5. Experiments,[0],[0]
"In order to illustrate the advantages of asynchrony, we will compare as-L-BFGS with mb-L-BFGS (Berahas et al., 2016); and in order to illustrate the advantages that are brought by using higher-order geometric information, we will compare as-L-BFGS to asynchronous SGD (aSGD)",5. Experiments,[0],[0]
"(Lian et al., 2015).",5. Experiments,[0],[0]
"We will also explore the speedup behavior of as-L-BFGS for increasing W .
",5. Experiments,[0],[0]
"We conduct experiments on both synthetic and real
datasets.",5. Experiments,[0],[0]
"For real data experiments, we have implemented all the three algorithms in C++ by using a low-level message passing protocol for parallelism, namely the OpenMPI library.",5. Experiments,[0],[0]
This code can be used both in a distributed environment or a single computer with multiprocessors.,5. Experiments,[0],[0]
"For the experiments on synthetic data, we have implemented the algorithms in MATLAB, by developing a realistic discreteevent simulator.",5. Experiments,[0],[0]
"This simulated environment is particularly useful for understanding the behaviors of the algorithms in detail since we can explicitly control the computation time that is spent at the master or worker nodes, and the communication time between the nodes.",5. Experiments,[0],[0]
"This simulation strategy also enables us to explicitly control the variation among the computational powers of the worker nodes; a feature that is much harder to control in real distributed environments.
",5. Experiments,[0],[0]
Linear Gaussian model: We conduct our first set of experiments on synthetic data where we consider a rather simple convex quadratic problem whose optimum is analytically available.,5. Experiments,[0],[0]
"The problem is formulated as finding the MAP estimate of the following linear Gaussian probabilistic model:
θ ∼ N (0, I), Yi|θ ∼ N (a>i θ, σ2x), ∀i = 1, . . .",5. Experiments,[0],[0]
", NY .
",5. Experiments,[0],[0]
We assume that {an}Nn=1 and σ2x are known and we aim at computing θ?.,5. Experiments,[0],[0]
"For these experiments, we develop a parametric discrete event simulator that aims to simulate the algorithms in a controllable yet realistic way.",5. Experiments,[0],[0]
"The simulator simulates a distributed optimization algorithm once it is provided four parameters: (i) µm: the average computational time spent by the master node at each iteration, (ii) µw: the average computational time spent by a single worker at each iteration, (iii) σw: the standard deviation of the computational time spent by a single worker per iteration, and (iv) τ",5. Experiments,[0],[0]
: the time spent for communications per iteration.,5. Experiments,[0],[0]
All these parameters are in a generic base time unit.,5. Experiments,[0],[0]
"Once these parameters are provided for one of the three algorithms, the simulator simulates the (a)synchronous distributed algorithm by drawing random computation times from a log-normal distribution whose mean and variance is specified by µw and σ2w.",5. Experiments,[0],[0]
"Figure 1 illustrates a typical outcome of the real and the simulated implementations of as-L-BFGS, where we observe that the simulator is able to provide realistic simulations that can even very well reflect
the fluctuations of the algorithm.
",5. Experiments,[0],[0]
"In our first experiment, we set d = 100, σ2x = 10, NY = 600, we randomly generate and fix the vectors {an}n in such a way that there will be a strong correlation in the posterior distribution, and we finally generate a true θ and the observations Y by using the generative model.
",5. Experiments,[0],[0]
"For each algorithm, we fix µm, µs, and τs to realistic values and investigate the effect of the variation among the workers by comparing the running time of the algorithms for achieving ε-accuracy (i.e., (U(θn) − U?)/U? ≤ ε) for different values of σ2w when W = 40.",5. Experiments,[0],[0]
We repeat each experiment 100 times.,5. Experiments,[0],[0]
"In all our experiments, we have tried several values for the hyper-parameters of each algorithm and we report the best results.",5. Experiments,[0],[0]
"All the hyper-parameters are provided in the supplementary document.
",5. Experiments,[0],[0]
Figure 2 visualizes the results for the first experiment.,5. Experiments,[0],[0]
"We can observe that, for smaller values σ2w as-L-BFGS and mbL-BFGS perform similarly, where a-SGD requires more computational time to achieve ε-accuracy.",5. Experiments,[0],[0]
"However, as we increase the value of σ2w, mb-L-BFGS requires more computational time in order to be able to collect sufficient amount of stochastic gradients.",5. Experiments,[0],[0]
"The results show that both asynchronous algorithms turn out to be more robust to the variability of the computational power of the workers, where as-L-BFGS shows a better performance.
",5. Experiments,[0],[0]
"In our second experiment, we investigate the speedup behavior of as-L-BFGS within the simulated setting.",5. Experiments,[0],[0]
"In this setting, we consider a highly varying set of workers and set σ2w = 200 and vary the number of workers W .",5. Experiments,[0],[0]
"As illustrated in Figure 3, as W increases, lmax increases as well and the algorithm hence requires more iterations in order to achieve ε-accuracy, since a smaller step-size needs to be used.",5. Experiments,[0],[0]
"However, this increment in the number of iterations is compensated by the increased number of workers, as we observe that the required computational time gracefully decreases with increasing W .",5. Experiments,[0],[0]
"We observe a similar behavior for different values of σ2w, where the speedup is more prominent for smaller σ2w.
",5. Experiments,[0],[0]
Large-scale matrix factorization:,5. Experiments,[0],[0]
"In our next set of experiments, we consider a large-scale matrix factorization problem (Gemulla et al., 2011; Şimşekli",5. Experiments,[0],[0]
"et al., 2015; Şimşekli et al., 2017), where the goal is to obtain the
MAP solution of the following probabilistic model: Frk ∼ N (0, 1), Gks ∼ N (0, 1), Yrs|F,G ∼ N (∑ k FrkGks, 1 ) .",5. Experiments,[0],[0]
"Here, Y ∈ RR×S is the data matrix, and F ∈ RR×K and G ∈ RK×S are the factor matrices to be estimated.
",5. Experiments,[0],[0]
"In this context, we evaluate the algorithms on three largescale movie ratings datasets, namely MovieLens 1Million (ML-1M), 10Million (ML-10M), and 20Million (ML20M) (grouplens.org).",5. Experiments,[0],[0]
"The ML-1M dataset contains 1 million non-zero entries, where R = 3883 (movies) and",5. Experiments,[0],[0]
S = 6040 (users).,5. Experiments,[0],[0]
"The ML-10M dataset contains 10 million non-zero entries, resulting in a 10681 × 71567 data matrix.",5. Experiments,[0],[0]
"Finally, the ML-20M dataset contains 20 million ratings, resulting in a 27278 × 138493 data matrix.",5. Experiments,[0],[0]
"We have conducted these experiments on a cluster of more than 500 interconnected computers, each of which is equipped with variable quality CPUs and memories.",5. Experiments,[0],[0]
"In these experiments, we have found that the numerical stability is improved when Hn is replaced with (Hn + ρI) for small ρ > 0.",5. Experiments,[0],[0]
This small modification does not violate our theoretical results.,5. Experiments,[0],[0]
"The hyper-parameters are provided in the supplementary document.
",5. Experiments,[0],[0]
"Figure 4 shows the performance of the three algorithms on the MovieLens datasets in terms of the root-mean-squarederror (RMSE), which is a standard metric for recommendation systems, and the norm of the gradients through iterations.",5. Experiments,[0],[0]
"In these experiments, we set K = 5 for all the three datasets and we set the number of workers toW = 10.",5. Experiments,[0],[0]
"The results show that, in all datasets, as-L-BFGS provides a significant speedup over mb-L-BFGS thanks to asynchrony.",5. Experiments,[0],[0]
We can observe that even when the speed of convergence of mb-L-BFGS is comparable to a-SGD and as-L-BFGS (cf.,5. Experiments,[0],[0]
"the plots showing the norm of the gradients), the final RMSE yielded by mb-L-BFGS is poorer than the two other
methods, which is an indicator that the asynchronous algorithms are able to find a better local minimum.",5. Experiments,[0],[0]
"On the other hand, the asynchrony causes more fluctuations in asL-BFGS when compared to a-SGD.
",5. Experiments,[0],[0]
"As opposed to the synthetic data experiments, in all the three MovieLens datasets, we observe that as-L-BFGS provides a slight improvement in the convergence speed when compared to a-SGD.",5. Experiments,[0],[0]
This indicates that a-SGD is able to achieve a comparable convergence speed by taking more steps while as-L-BFGS is computing the matrix-vector products.,5. Experiments,[0],[0]
"However, this gap can be made larger by considering a more efficient, yet more sophisticated implementation for L-BFGS computations (Chen et al., 2014).
",5. Experiments,[0],[0]
"In our last experiment, we investigate the speedup properties of as-L-BFGS in the real distributed setting.",5. Experiments,[0],[0]
"In this experiment, we only consider the ML-1M dataset and run the as-L-BFGS algorithm for different number of workers.",5. Experiments,[0],[0]
Figure 5 illustrates the results of this experiment.,5. Experiments,[0],[0]
"As we increase W from 1 to 10, we obtain a decent speedup that is close to a linear speedup.",5. Experiments,[0],[0]
"However, when we set W = 20 the algorithm becomes unstable, since the term lmaxh in (15) dominates.",5. Experiments,[0],[0]
"Therefore, for W = 20 we need to decrease the step-size h, which requires the algorithm to be run for a longer amount of time in order to achieve the same error as we achieved when W was smaller.",5. Experiments,[0],[0]
"On the other hand, the algorithm achieves a linear speedup in terms of iterations; however, the corresponding result is provided in the supplementary document due to the space constraints.",5. Experiments,[0],[0]
"In this study, we proposed an asynchronous parallel L-BFGS algorithm for non-convex optimization.",6. Conclusion,[0],[0]
"We developed the algorithm within the SG-MCMC framework, where we reformulated the problem as sampling from a concentrated probability distribution.",6. Conclusion,[0],[0]
We proved non-asymptotic guarantees and showed that as-LBFGS achieves an ergodic global convergence with rate O(1/ √ N) and it can achieve a linear speedup.,6. Conclusion,[0],[0]
Our experiments supported our theory and showed that the proposed algorithm provides a significant speedup over the synchronous parallel L-BFGS algorithm.,6. Conclusion,[0],[0]
The authors would like to thank to Murat A. Erdoğdu for fruitful discussions.,Acknowledgments,[0],[0]
"This work is partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014), by the Scientific and Technological Research Council of Turkey (TÜBİTAK) grant number 116E580, and by the industrial chair Machine Learning for Big Data from Télécom ParisTech.",Acknowledgments,[0],[0]
"Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong potential in non-convex optimization, where local and global convergence guarantees can be shown under certain conditions.",abstractText,[0],[0]
"By building up on this recent theory, in this study, we develop an asynchronous-parallel stochastic L-BFGS algorithm for non-convex optimization.",abstractText,[0],[0]
The proposed algorithm is suitable for both distributed and shared-memory settings.,abstractText,[0],[0]
We provide formal theoretical analysis and show that the proposed method achieves an ergodic convergence rate of O(1/ √ N),abstractText,[0],[0]
(N being the total number of iterations) and it can achieve a linear speedup under certain conditions.,abstractText,[0],[0]
We perform several experiments on both synthetic and real datasets.,abstractText,[0],[0]
The results support our theory and show that the proposed algorithm provides a significant speedup over the recently proposed synchronous distributed L-BFGS algorithm.,abstractText,[0],[0]
Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 196–202 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2031",text,[0],[0]
"Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al., 2015) and other NLP tasks.
",1 Introduction,[0],[0]
All of the above applications of S2S learning make use of a single encoder.,1 Introduction,[0],[0]
"Depending on the modality, it can be either a recurrent neural network (RNN) for textual input data, or a convolutional network for images.
",1 Introduction,[0],[0]
"In this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder.",1 Introduction,[0],[0]
"We explore various strategies the decoder can employ to attend to the hidden states of the individual encoders.
",1 Introduction,[0],[0]
"The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight,
2016).",1 Introduction,[0],[0]
"In multimodal MT (MMT), where an image and its caption are on the input, we might expect the caption to be the primary source of information, whereas the image itself would only play a role in output disambiguation.",1 Introduction,[0],[0]
"In automatic post-editing (APE), where a sentence in a source language and its automatically generated translation are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.
",1 Introduction,[0],[0]
"We propose two interpretable attention strategies that take into account the roles of the individual source sequences explicitly—flat and hierarchical attention combination.
",1 Introduction,[0],[0]
"This paper is organized as follows: In Section 2, we review the attention mechanism in single-source S2S learning.",1 Introduction,[0],[0]
Section 3 introduces new attention combination strategies.,1 Introduction,[0],[0]
"In Section 4, we evaluate the proposed models on the MMT and APE tasks.",1 Introduction,[0],[0]
"We summarize the related work in Section 5, and conclude in Section 6.",1 Introduction,[0],[0]
The attention mechanism in S2S learning allows an RNN decoder to directly access information about the input each time before it emits a symbol.,2 Attentive S2S Learning,[0],[0]
"Inspired by content-based addressing in Neural Turing Machines (Graves et al., 2014), the attention mechanism estimates a probability distribution over the encoder hidden states in each decoding step.",2 Attentive S2S Learning,[0],[0]
"This distribution is used for computing the context vector—the weighted average of the encoder hidden states—as an additional input to the decoder.
",2 Attentive S2S Learning,[0],[0]
"The standard attention model as described by Bahdanau et al. (2014) defines the attention energies eij , attention distribution αij , and the con-
196
text vector ci in i-th decoder step as:
eij = v > a tanh(Wasi + Uahj), (1)
αij = exp(eij)∑Tx k=1 exp(eik) , (2)
ci =
Tx∑
j=1
αijhj .",2 Attentive S2S Learning,[0],[0]
"(3)
The trainable parameters Wa and Ua are projection matrices that transform the decoder and encoder states si and hj into a common vector space and va is a weight vector over the dimensions of this space.",2 Attentive S2S Learning,[0],[0]
Tx denotes the length of the input sequence.,2 Attentive S2S Learning,[0],[0]
"For the sake of clarity, bias terms (applied every time a vector is linearly projected using a weight matrix) are omitted.
",2 Attentive S2S Learning,[0],[0]
"Recently, Lu et al. (2016) introduced sentinel gate, an extension of the attentive RNN decoder with LSTM units (Hochreiter and Schmidhuber, 1997).",2 Attentive S2S Learning,[0],[0]
"We adapt the extension for gated recurrent units (GRU) (Cho et al., 2014), which we use in our experiments:
ψi = σ(Wyyi +Wssi−1) (4)
where Wy and Ws are trainable parameters, yi is the embedded decoder input, and si−1 is the previous decoder state.
",2 Attentive S2S Learning,[0],[0]
"Analogically to Equation 1, we compute a scalar energy term for the sentinel:
eψi = v > a tanh ( Wasi + U (ψ) a (ψi si) )",2 Attentive S2S Learning,[0],[0]
"(5)
where Wa, U (ψ) a are the projection matrices, va is the weight vector, and ψi si is the sentinel vector.",2 Attentive S2S Learning,[0],[0]
Note that the sentinel energy term does not depend on any hidden state of any encoder.,2 Attentive S2S Learning,[0],[0]
The sentinel vector is projected to the same vector space as the encoder state hj in Equation 1.,2 Attentive S2S Learning,[0],[0]
"The term eψi is added as an extra attention energy term to Equation 2 and the sentinel vector ψi si is used as the corresponding vector in the summation in Equation 3.
",2 Attentive S2S Learning,[0],[0]
This technique should allow the decoder to choose whether to attend to the encoder or to focus on its own state and act more like a language model.,2 Attentive S2S Learning,[0],[0]
This can be beneficial if the encoder does not contain much relevant information for the current decoding step.,2 Attentive S2S Learning,[0],[0]
"In S2S models with multiple encoders, the decoder needs to be able to combine the attention information collected from the encoders.
",3 Attention Combination,[0],[0]
"A widely adopted technique for combining multiple attention models in a decoder is concatenation of the context vectors c(1)i , . . .",3 Attention Combination,[0],[0]
", c (N)",3 Attention Combination,[0],[0]
"i (Zoph and Knight, 2016; Firat et al., 2016).",3 Attention Combination,[0],[0]
"As mentioned in Section 1, this setting forces the model to attend to each encoder independently and lets the attention combination to be resolved implicitly in the subsequent network layers.
",3 Attention Combination,[0],[0]
"In this section, we propose two alternative strategies of combining attentions from multiple encoders.",3 Attention Combination,[0],[0]
"We either let the decoder learn the αi distribution jointly over all encoder hidden states (flat attention combination) or factorize the distribution over individual encoders (hierarchical combination).
",3 Attention Combination,[0],[0]
Both of the alternatives allow us to explicitly compute distribution over the encoders and thus interpret how much attention is paid to each encoder at every decoding step.,3 Attention Combination,[0],[0]
Flat attention combination projects the hidden states of all encoders into a shared space and then computes an arbitrary distribution over the projections.,3.1 Flat Attention Combination,[0],[0]
"The difference between the concatenation of the context vectors and the flat attention combination is that the αi coefficients are computed jointly for all encoders:
α (k) ij =
exp(e (k) ij )
",3.1 Flat Attention Combination,[0],[0]
"∑N n=1 ∑T (n)x m=1 exp ( e (n) im ) (6)
",3.1 Flat Attention Combination,[0],[0]
where T (n)x is the length of the input sequence of the n-th encoder and e(k)ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step.,3.1 Flat Attention Combination,[0],[0]
These attention energies are computed as in Equation 1.,3.1 Flat Attention Combination,[0],[0]
"The parameters va andWa are shared among the encoders, and Ua is different for each encoder and serves as an encoder-specific projection of hidden states into a common vector space.
",3.1 Flat Attention Combination,[0],[0]
"The states of the individual encoders occupy different vector spaces and can have a different dimensionality, therefore the context vector cannot be computed as their weighted sum.",3.1 Flat Attention Combination,[0],[0]
"We project
them into a single space using linear projections:
ci = N∑
k=1
T (k) x∑
j=1
α (k) ij U (k) c h (k) j (7)
",3.1 Flat Attention Combination,[0],[0]
where U (k)c are additional trainable parameters.,3.1 Flat Attention Combination,[0],[0]
The matrices U (k)c project the hidden states into a common vector space.,3.1 Flat Attention Combination,[0],[0]
"This raises a question whether this space can be the same as the one that is projected into in the energy computation using matrices U (k)a in Equation 1, i.e., whether U (k) c",3.1 Flat Attention Combination,[0],[0]
= U (k) a .,3.1 Flat Attention Combination,[0],[0]
"In our experiments, we explore both options.",3.1 Flat Attention Combination,[0],[0]
We also try both adding and not adding the sentinel α(ψ)i,3.1 Flat Attention Combination,[0],[0]
U (ψ) c,3.1 Flat Attention Combination,[0],[0]
(ψi si) to the context vector.,3.1 Flat Attention Combination,[0],[0]
"The hierarchical attention combination model computes every context vector independently, similarly to the concatenation approach.",3.2 Hierarchical Attention Combination,[0],[0]
"Instead of concatenation, a second attention mechanism is constructed over the context vectors.
",3.2 Hierarchical Attention Combination,[0],[0]
"We divide the computation of the attention distribution into two steps: First, we compute the context vector for each encoder independently using Equation 3.",3.2 Hierarchical Attention Combination,[0],[0]
"Second, we project the context vectors (and optionally the sentinel) into a common space (Equation 8), we compute another distribution over the projected context vectors (Equation 9) and their corresponding weighted average (Equation 10):
e (k) i = v > b tanh(Wbsi + U (k) b c (k) i ), (8)
β (k) i =",3.2 Hierarchical Attention Combination,[0],[0]
exp(e (k) i ),3.2 Hierarchical Attention Combination,[0],[0]
"∑N
n=1 exp(e (n) i )
, (9)
ci = N∑
k=1
β (k)",3.2 Hierarchical Attention Combination,[0],[0]
i U (k) c c,3.2 Hierarchical Attention Combination,[0],[0]
"(k) i (10)
where c(k)i is the context vector of the k-th encoder, additional trainable parameters vb and Wb are shared for all encoders, and U (k)b and U (k) c are encoder-specific projection matrices, that can be set equal and shared, similarly to the case of flat attention combination.",3.2 Hierarchical Attention Combination,[0],[0]
"We evaluate the attention combination strategies presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-editing (Section 4.2).
",4 Experiments,[0],[0]
"The models were implemented using the Neural Monkey sequence-to-sequence learning toolkit (Helcl and Libovický, 2017).12 In both setups, we process the textual input with bidirectional GRU network (Cho et al., 2014) with 300 units in the hidden state in each direction and 300 units in embeddings.",4 Experiments,[0],[0]
"For the attention projection space, we use 500 hidden units.",4 Experiments,[0],[0]
"We optimize the network to minimize the output cross-entropy using the Adam algorithm (Kingma and Ba, 2014) with learning rate 10−4.",4 Experiments,[0],[0]
"The goal of multimodal translation (Specia et al., 2016) is to generate target-language image captions given both the image and its caption in the source language.
",4.1 Multimodal Translation,[0],[0]
"We train and evaluate the model on the Multi30k dataset (Elliott et al., 2016).",4.1 Multimodal Translation,[0],[0]
"It consists of 29,000 training instances (images together with English captions and their German translations), 1,014 validation instances, and 1,000 test instances.",4.1 Multimodal Translation,[0],[0]
"The results are evaluated using the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011).
",4.1 Multimodal Translation,[0],[0]
"In our model, the visual input is processed with a pre-trained VGG 16 network (Simonyan and Zisserman, 2014) without further fine-tuning.",4.1 Multimodal Translation,[0],[0]
"Atten-
",4.1 Multimodal Translation,[0],[0]
"1http://github.com/ufal/neuralmonkey 2The trained models can be downloaded from
http://ufallab.ms.mff.cuni.cz/ ˜libovicky/acl2017_att_models/
tion distribution over the visual input is computed from the last convolutional layer of the network.",4.1 Multimodal Translation,[0],[0]
"The decoder is an RNN with 500 conditional GRU units (Firat and Cho, 2016) in the recurrent layer.",4.1 Multimodal Translation,[0],[0]
"We use byte-pair encoding (Sennrich et al., 2016b) with a vocabulary of 20,000 subword units shared between the textual encoder and the decoder.
",4.1 Multimodal Translation,[0],[0]
The results of our experiments in multimodal MT are shown in Table 1.,4.1 Multimodal Translation,[0],[0]
"We achieved the best results using the hierarchical attention combination without the sentinel mechanism, which also showed the fastest convergence.",4.1 Multimodal Translation,[0],[0]
The flat combination strategy achieves similar results eventually.,4.1 Multimodal Translation,[0],[0]
Sharing the projections for energy and context vector computation does not improve over the concatenation baseline and slows the training almost prohibitively.,4.1 Multimodal Translation,[0],[0]
"Multimodal models were not able to surpass the textual baseline (BLEU 33.0).
",4.1 Multimodal Translation,[0],[0]
"Using the conditional GRU units brought an improvement of about 1.5 BLEU points on average, with the exception of the concatenation scenario where the performance dropped by almost 5 BLEU points.",4.1 Multimodal Translation,[0],[0]
"We hypothesize this is caused by the fact the model has to learn the implicit attention combination on multiple places – once in the output projection and three times inside the conditional GRU unit (Firat and Cho, 2016, Equations 10-12).",4.1 Multimodal Translation,[0],[0]
We thus report the scores of the introduced attention combination techniques trained with conditional GRU units and compare them with the concatenation baseline trained with plain GRU units.,4.1 Multimodal Translation,[0],[0]
"Automatic post-editing is a task of improving an automatically generated translation given the source sentence where the translation system is treated as a black box.
",4.2 Automatic MT Post-editing,[0],[0]
"We used the data from the WMT16 APE Task (Bojar et al., 2016; Turchi et al., 2016), which consists of 12,000 training, 2,000 validation, and 1,000 test sentence triplets from the IT domain.",4.2 Automatic MT Post-editing,[0],[0]
"Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.",4.2 Automatic MT Post-editing,[0],[0]
"In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sentences.",4.2 Automatic MT Post-editing,[0],[0]
"The results are evaluated using the humantargeted error rate (HTER) (Snover et al., 2006) and BLEU score (Papineni et al., 2002).
",4.2 Automatic MT Post-editing,[0],[0]
"Following Libovický et al. (2016), we encode the target sentence as a sequence of edit operations transforming the MT output into the reference.",4.2 Automatic MT Post-editing,[0],[0]
"By this technique, we prevent the model from paraphrasing the input sentences.",4.2 Automatic MT Post-editing,[0],[0]
The decoder is a GRU network with 300 hidden units.,4.2 Automatic MT Post-editing,[0],[0]
"Unlike in the MMT setup (Section 4.1), we do not use the conditional GRU because it is prone to overfitting on the small dataset we work with.
",4.2 Automatic MT Post-editing,[0],[0]
"The models were able to slightly, but significantly improve over the baseline – leaving the MT output as is (HTER 24.8).",4.2 Automatic MT Post-editing,[0],[0]
The differences between the attention combination strategies are not significant.,4.2 Automatic MT Post-editing,[0],[0]
"Attempts to use S2S models for APE are relatively rare (Bojar et al., 2016).",5 Related Work,[0],[0]
"Niehues et al. (2016) concatenate both inputs into one long sequence, which forces the encoder to be able to work with both source and target language.",5 Related Work,[0],[0]
"Their attention is then similar to our flat combination strategy; however, it can only be used for sequential data.
",5 Related Work,[0],[0]
"The best system from the WMT’16 competition (Junczys-Dowmunt and Grundkiewicz, 2016) trains two separate S2S models,",5 Related Work,[0],[0]
one translating from MT output to post-edited targets and the second one from source sentences to post-edited targets.,5 Related Work,[0],[0]
The decoders average their output distributions similarly to decoder ensembling.,5 Related Work,[0],[0]
"The biggest source of improvement in this state-of-theart posteditor came from additional training data generation, rather than from changes in the network architecture.
",5 Related Work,[0],[0]
Source: a man sleeping in a green room on a couch .,5 Related Work,[0],[0]
Reference:,5 Related Work,[0],[0]
ein Mann schläft in einem grünen Raum auf einem Sofa .,5 Related Work,[0],[0]
"Output with attention:
Caglayan et al. (2016) used an architecture very similar to ours for multimodal translation.",5 Related Work,[0],[0]
They made a strong assumption that the network can be trained in such a way that the hidden states of the encoder and the convolutional network occupy the same vector space and thus sum the context vectors from both modalities.,5 Related Work,[0],[0]
"In this way, their multimodal MT system (BLEU 27.82) remained far bellow the text-only setup (BLEU 32.50).
",5 Related Work,[0],[0]
New state-of-the-art results on the Multi30k dataset were achieved very recently by Calixto et al. (2017).,5 Related Work,[0],[0]
"The best-performing architecture uses the last fully-connected layer of VGG-19 network (Simonyan and Zisserman, 2014) as decoder initialization and only attends to the text encoder hidden states.",5 Related Work,[0],[0]
"With a stronger monomodal baseline (BLEU 33.7), their multimodal model achieved a BLEU score of 37.1.",5 Related Work,[0],[0]
"Similarly to Niehues et al. (2016) in the APE task, even further improvement was achieved by synthetically extending the dataset.",5 Related Work,[0],[0]
We introduced two new strategies of combining attention in a multi-source sequence-to-sequence setup.,6 Conclusions,[0],[0]
"Both methods are based on computing a joint distribution over hidden states of all encoders.
",6 Conclusions,[0],[0]
"We conducted experiments with the proposed strategies on multimodal translation and automatic post-editing tasks, and we showed that the flat and hierarchical attention combination can be applied to these tasks with maintaining competitive score to previously used techniques.
",6 Conclusions,[0],[0]
"Unlike the simple context vector concatenation, the introduced combination strategies can be used with the conditional GRU units in the decoder.",6 Conclusions,[0],[0]
"On top of that, the hierarchical combination strategy exhibits faster learning than than the other strategies.",6 Conclusions,[0],[0]
"We would like to thank Ondřej Dušek, Rudolf Rosa, Pavel Pecina, and Ondřej Bojar for a fruitful discussions and comments on the draft of the paper.
",Acknowledgments,[0],[0]
"This research has been funded by the Czech Science Foundation grant no. P103/12/G084, the EU grant no.",Acknowledgments,[0],[0]
"H2020-ICT-2014-1-645452 (QT21), and Charles University grant no. 52315/2014 and SVV project",Acknowledgments,[0],[0]
no.,Acknowledgments,[0],[0]
260 453.,Acknowledgments,[0],[0]
This work has been using language resources developed and/or stored and/or distributed by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project LM2010013).,Acknowledgments,[0],[0]
"Modeling attention in neural multi-source sequence-to-sequence learning remains a relatively unexplored area, despite its usefulness in tasks that incorporate multiple source languages or modalities.",abstractText,[0],[0]
"We propose two novel approaches to combine the outputs of attention mechanisms over each source sequence, flat and hierarchical.",abstractText,[0],[0]
We compare the proposed methods with existing techniques and present results of systematic evaluation of those methods on the WMT16 Multimodal Translation and Automatic Post-editing tasks.,abstractText,[0],[0]
We show that the proposed methods achieve competitive results on both tasks.,abstractText,[0],[0]
Attention Strategies for Multi-Source Sequence-to-Sequence Learning,title,[0],[0]
In typical machine learning problems like image classification it is assumed that an image clearly represents a category (a class).,1. Introduction,[0],[0]
"However, in many real-life applications multiple instances are observed and only a general statement of the category is given.",1. Introduction,[0],[0]
"This scenario is called multiple instance learning (MIL) (Dietterich et al., 1997; Maron & Lozano-Pérez, 1998) or, learning from weakly annotated data (Oquab et al., 2014).",1. Introduction,[0],[0]
"The problem of weakly annotated data is especially apparent in medical imaging (Quellec et al., 2017) (e.g., computational pathology, mammography or CT lung screening) where an image is typically described by a single label (benign/malignant) or a Region Of Interest (ROI) is roughly given.
",1. Introduction,[0],[0]
MIL deals with a bag of instances for which a single class label is assigned.,1. Introduction,[0],[0]
"Hence, the main goal of MIL is to learn a
*Equal contribution 1University of Amsterdam, the Netherlands.",1. Introduction,[0],[0]
"Correspondence to: Maximilian Ilse <m.ilse@uva.nl>, Jakub M. Tomczak <j.m.tomczak@uva.nl>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
model that predicts a bag label, e.g., a medical diagnosis.",1. Introduction,[0],[0]
"An additional challenge is to discover key instances (Liu et al., 2012), i.e., the instances that trigger the bag label.",1. Introduction,[0],[0]
In the medical domain the latter task is of great interest because of legal issues1 and its usefulness in clinical practice.,1. Introduction,[0],[0]
"In order to solve the primary task of a bag classification different methods are proposed, such as utilizing similarities among bags (Cheplygina et al., 2015b), embedding instances to a compact low-dimensional representation that is further fed to a bag-level classifier (Andrews et al., 2003; Chen et al., 2006), and combining responses of an instance-level classifier (Ramon & De Raedt, 2000; Raykar et al., 2008; Zhang et al., 2006).",1. Introduction,[0],[0]
Only the last approach is capable of providing interpretable results.,1. Introduction,[0],[0]
"However, it was shown that the instance level accuracy of such methods is low (Kandemir & Hamprecht, 2015) and in general there is a disagreement among MIL methods at the instance level (Cheplygina et al., 2015a).",1. Introduction,[0],[0]
"These issues call into question the usability of current MIL models for interpreting the final decision.
",1. Introduction,[0],[0]
"In this paper, we propose a new method that aims at incorporating interpretability to the MIL approach and increasing its flexibility.",1. Introduction,[0],[0]
We formulate the MIL model using the Bernoulli distribution for the bag label and train it by optimizing the log-likelihood function.,1. Introduction,[0],[0]
"We show that the application of the Fundamental Theorem of Symmetric Functions provides a general procedure for modeling the bag label probability (the bag score function) that consists of three steps: (i) a transformation of instances to a low-dimensional embedding, (ii) a permutation-invariant (symmetric) aggregation function, and (iii) a final transformation to the bag probability.",1. Introduction,[0],[0]
"We propose to parameterize all transformations using neural networks (i.e., a combination of convolutional and fully-connected layers), which increases the flexibility of the approach and allows to train the model in an endto-end manner by optimizing an unconstrained objective function.",1. Introduction,[0],[0]
"Last but not least, we propose to replace widelyused permutation-invariant operators such as the maximum operator max and the mean operator mean by a trainable weighted average where weights are given by a two-layered neural network.",1. Introduction,[0],[0]
"The two-layered neural network corre-
1According to the European Union General Data Protection Regulation (taking effect 2018), a user should have the right to obtain an explanation of the decision reached.
sponds to the attention mechanism (Bahdanau et al., 2014; Raffel & Ellis, 2015).",1. Introduction,[0],[0]
"Notably, the attention weights allow us to find key instances, which could be further used to highlight possible ROIs.",1. Introduction,[0],[0]
"In the experiments we show that our model is on a par with the best classical MIL methods on common benchmark MIL datasets, and that it outperforms other methods on a MNIST-based MIL problem as well as two real-life histopathology image datasets.",1. Introduction,[0],[0]
"Moreover, in the image datasets we provide empirical evidence that our model can indicate key instances.",1. Introduction,[0],[0]
"Problem formulation In the classical (binary) supervised learning problem one aims at finding a model that predicts a value of a target variable, y ∈ {0, 1}, for a given instance, x ∈ RD.",2.1. Multiple instance learning (MIL),[0],[0]
"In the case of the MIL problem, however, instead of a single instance there is a bag of instances, X = {x1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
",xK}, that exhibit neither dependency nor ordering among each other.",2.1. Multiple instance learning (MIL),[0],[0]
We assume that K could vary for different bags.,2.1. Multiple instance learning (MIL),[0],[0]
There is also a single binary label Y associated with the bag.,2.1. Multiple instance learning (MIL),[0],[0]
"Furthermore, we assume that individual labels exist for the instances within a bag, i.e., y1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
", yK and yk ∈ {0, 1}, for k = 1, . . .",2.1. Multiple instance learning (MIL),[0],[0]
",K, however, there is no access to those labels and they remain unknown during training.",2.1. Multiple instance learning (MIL),[0],[0]
"We can re-write the assumptions of the MIL problem in the following form:
Y =
{ 0, iff ∑ k yk = 0,
1, otherwise.",2.1. Multiple instance learning (MIL),[0],[0]
"(1)
These assumptions imply that a MIL model must be permutation-invariant.",2.1. Multiple instance learning (MIL),[0],[0]
"Further, the two statements could be re-formulated in a compact form using the maximum operator:
Y = max k {yk}.",2.1. Multiple instance learning (MIL),[0],[0]
"(2)
Learning a model that tries to optimize an objective based on the maximum over instance labels would be problematic at least for two reasons.",2.1. Multiple instance learning (MIL),[0],[0]
"First, all gradient-based learning methods would encounter issues with vanishing gradients.",2.1. Multiple instance learning (MIL),[0],[0]
"Second, this formulation is suitable only when an instancelevel classifier is used.
",2.1. Multiple instance learning (MIL),[0],[0]
"In order to make the learning problem easier, we propose to train a MIL model by optimizing the log-likelihood function where the bag label is distributed according to the Bernoulli distribution with the parameter θ(X) ∈",2.1. Multiple instance learning (MIL),[0],[0]
"[0, 1], i.e., the probability of Y = 1 given the bag of instances X .
MIL approaches In the MIL setting the bag probability θ(X) must be permutation-invariant since we assume neither ordering nor dependency of instances within a bag.",2.1. Multiple instance learning (MIL),[0],[0]
"Therefore, the MIL problem can be considered in terms of
a specific form of the Fundamental Theorem of Symmetric Functions with monomials given by the following theorem (Zaheer et al., 2017):
Theorem 1.",2.1. Multiple instance learning (MIL),[0],[0]
"A scoring function for a set of instances X , S(X) ∈ R, is a symmetric function (i.e., permutationinvariant to the elements in X), if and only if it can be decomposed in the following form:
S(X)",2.1. Multiple instance learning (MIL),[0],[0]
"= g ( ∑ x∈X f(x) ) , (3)
where f and g are suitable transformations.
",2.1. Multiple instance learning (MIL),[0],[0]
This theorem provides a general strategy for modeling the bag probability using the decomposition given in (3).,2.1. Multiple instance learning (MIL),[0],[0]
"A similar decomposition with max instead of sum is given by the following theorem (Qi et al., 2017):
Theorem 2.",2.1. Multiple instance learning (MIL),[0],[0]
"For any ε > 0, a Hausdorff continuous symmetric function S(X) ∈ R can be arbitrarily approximated by a function in the form g ( maxx∈X f(x) ) , where max is the element-wise vector maximum operator and f and g are continuous functions, that is:
|S(X)− g ( max x∈X f(x) )",2.1. Multiple instance learning (MIL),[0],[0]
| < ε.,2.1. Multiple instance learning (MIL),[0],[0]
"(4)
The difference between Theorems 1 and 2 is that the former is a universal decomposition while the latter provides an arbitrary approximation.",2.1. Multiple instance learning (MIL),[0],[0]
"Nonetheless, they both formulate a general three-step approach for classifying a bag of instances: (i) a transformation of instances using the function f , (ii) a combination of transformed instances using a symmetric (permutation-invariant) function σ, (iii) a transformation of combined instances transformed by f using a function g. Finally, the expressiveness of the score function relies on the choice of classes of functions for f and g.
In the MIL problem formulation the score function in both theorems is the probability θ(X) and the permutationinvariant function σ is referred to as the MIL pooling.",2.1. Multiple instance learning (MIL),[0],[0]
"The choice of functions f , g and σ determines a specific approach to modeling the label probability.",2.1. Multiple instance learning (MIL),[0],[0]
"For a given MIL operator there are two main MIL approaches:
(i)",2.1. Multiple instance learning (MIL),[0],[0]
The instance-level approach: The transformation f is an instance-level classifier that returns scores for each instance.,2.1. Multiple instance learning (MIL),[0],[0]
Then individual scores are aggregated by MIL pooling to obtain θ(X).,2.1. Multiple instance learning (MIL),[0],[0]
"The function g is the identity function.
",2.1. Multiple instance learning (MIL),[0],[0]
(ii),2.1. Multiple instance learning (MIL),[0],[0]
The embedding-level approach: The function f maps instances to a low-dimensional embedding.,2.1. Multiple instance learning (MIL),[0],[0]
MIL pooling is used to obtain a bag representation that is independent of the number of instances in the bag.,2.1. Multiple instance learning (MIL),[0],[0]
"The bag representation is further processed by a bag-level classifier to provide θ(X).
",2.1. Multiple instance learning (MIL),[0],[0]
"It is advocated in (Wang et al., 2016) that the latter approach is preferable in terms of the bag level classification performance.",2.1. Multiple instance learning (MIL),[0],[0]
"Since the individual labels are unknown, there is a threat that the instance-level classifier might be trained insufficiently and it introduces additional error to the final prediction.",2.1. Multiple instance learning (MIL),[0],[0]
The embedding-level approach determines a joint representation of a bag and therefore it does not introduce additional bias to the bag-level classifier.,2.1. Multiple instance learning (MIL),[0],[0]
"On the other hand, the instance-level approach provides a score that can be used to find key instances i.e., the instances that trigger the bag label.",2.1. Multiple instance learning (MIL),[0],[0]
Liu et al. (2012) were able to show that a model that is successfully detecting key instances is more likely to achieve better bag label predictions.,2.1. Multiple instance learning (MIL),[0],[0]
We will show how to modify the embedding-level approach to be interpretable by using a new MIL pooling.,2.1. Multiple instance learning (MIL),[0],[0]
"In classical MIL problems it is assumed that instances are represented by features that do not require further processing, i.e., f is the identity.",2.2. MIL with Neural Networks,[0],[0]
"However, for some tasks like image or text analysis additional steps of feature extraction are necessary.",2.2. MIL with Neural Networks,[0],[0]
"Additionally, Theorem 1 and 2 indicate that for a flexible enough class of functions we can model any permutation-invariant score function.",2.2. MIL with Neural Networks,[0],[0]
"Therefore, we consider a class of transformations that are parameterized by neural networks fψ(·) with parameters ψ that transform the k-th instance into a low-dimensional embedding, hk = fψ(xk), where hk ∈ H such that H =",2.2. MIL with Neural Networks,[0],[0]
"[0, 1] for the instance-based approach andH = RM for the embeddingbased approach.
",2.2. MIL with Neural Networks,[0],[0]
"Eventually, the parameter θ(X) is determined by a transformation",2.2. MIL with Neural Networks,[0],[0]
gφ :,2.2. MIL with Neural Networks,[0],[0]
HK,2.2. MIL with Neural Networks,[0],[0]
"→ [0, 1].",2.2. MIL with Neural Networks,[0],[0]
"In the instance-based approach the transformation gφ is simply the identity, while in the embedding-based approach it could be also parameterized by a neural network with parameters φ.",2.2. MIL with Neural Networks,[0],[0]
"The former approach is depicted in Figure 6(a) and the latter in Figure 6(b) in the Appendix.
",2.2. MIL with Neural Networks,[0],[0]
The idea of parameterizing all transformations using neural networks is very appealing because the whole approach can be arbitrarily flexible and it can be trained end-to-end by backpropagation.,2.2. MIL with Neural Networks,[0],[0]
The only restriction is that the MIL pooling must be differentiable.,2.2. MIL with Neural Networks,[0],[0]
The formulation of the MIL problem requires the MIL pooling σ to be permutation-invariant.,2.3. MIL pooling,[0],[0]
"As shown in Theorem 1 and 2, there are two MIL pooling operators that ensure the score function (i.e., the bag probability) to be a symmetric function, namely, the maximum operator:
∀m=1,...,M : zm = max k=1,...,K {hkm}, (5)
and the mean operator:2
z = 1
K K∑ k=1 hk.",2.3. MIL pooling,[0],[0]
"(6)
In fact, other operators could be used such as, the convex maximum operator (i.e., log-sum-exp) (Ramon & De Raedt, 2000), Integrated Segmentation and Recognition (Keeler et al., 1991), noisy-or (Maron & Lozano-Pérez, 1998) and noisy-and (Kraus et al., 2016).",2.3. MIL pooling,[0],[0]
"These MIL pooling operators could replace max in Theorem 2 and proofs would follow in a similar manner (see Supplementary in (Qi et al., 2017) for a detailed proof for the maximum operator).",2.3. MIL pooling,[0],[0]
"All of these operators are differentiable, hence, they could be easily used as a MIL pooling layer in a deep neural network architecture.",2.3. MIL pooling,[0],[0]
"All MIL pooling operators mentioned in the previous section have a clear disadvantage, namely, they are pre-defined and non-trainable.",2.4. Attention-based MIL pooling,[0],[0]
"For instance, the max-operator could be a good choice in the instance-based approach but it might be inappropriate for the embedding-based approach.",2.4. Attention-based MIL pooling,[0],[0]
"Similarly, the mean operator is definitely a bad MIL pooling to aggregate instance scores, although, it could succeed in calculating the bag representation.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, a flexible and adaptive MIL pooling could potentially achieve better results by adjusting to a task and data.",2.4. Attention-based MIL pooling,[0],[0]
"Ideally, such MIL pooling should also be interpretable, a trait that is missing in all operators mentioned in Section 2.3.
",2.4. Attention-based MIL pooling,[0],[0]
Attention mechanism We propose to use a weighted average of instances (low-dimensional embeddings) where weights are determined by a neural network.,2.4. Attention-based MIL pooling,[0],[0]
"Additionally, the weights must sum to 1 to be invariant to the size of a bag.",2.4. Attention-based MIL pooling,[0],[0]
The weighted average fulfills the requirements of the Theorem 1 where the weights together with the embeddings are part of the f function.,2.4. Attention-based MIL pooling,[0],[0]
"Let H = {h1, . . .",2.4. Attention-based MIL pooling,[0],[0]
",hK} be a bag of K embeddings, then we propose the following MIL pooling:
z = K∑ k=1 akhk, (7)
where:
ak = exp{w> tanh
( Vh>k ) }
K∑ j=1 exp{w> tanh",2.4. Attention-based MIL pooling,[0],[0]
"( Vh>j ) } , (8)
where w ∈ RL×1 and V ∈ RL×M are parameters.",2.4. Attention-based MIL pooling,[0],[0]
"Moreover, we utilize the hyperbolic tangent tanh(·) element-wise non-linearity to include both negative and positive values for proper gradient flow.",2.4. Attention-based MIL pooling,[0],[0]
"The proposed construction allows to discover (dis)similarities among instances.
",2.4. Attention-based MIL pooling,[0],[0]
"2Notice that the weight 1 K can be seen as a part of the f function.
",2.4. Attention-based MIL pooling,[0],[0]
"Interestingly, the proposed MIL pooling corresponds to a version of the attention mechanism (Lin et al., 2017; Raffel & Ellis, 2015).",2.4. Attention-based MIL pooling,[0],[0]
The main difference is that typically in the attention mechanism all instances are sequentially dependent while here we assume that all instances are independent.,2.4. Attention-based MIL pooling,[0],[0]
"Therefore, a naturally arising question is whether the attention mechanism could work without sequential dependencies among instances, and if it will not learn the mean operator.",2.4. Attention-based MIL pooling,[0],[0]
"We will address this issue in the experiments.
",2.4. Attention-based MIL pooling,[0],[0]
Gated attention mechanism,2.4. Attention-based MIL pooling,[0],[0]
"Furthermore, we notice that the tanh(·) non-linearity could be inefficient to learn complex relations.",2.4. Attention-based MIL pooling,[0],[0]
Our concern follows from the fact that tanh(x) is approximately linear for x ∈,2.4. Attention-based MIL pooling,[0],[0]
"[−1, 1], which could limit the final expressiveness of learned relations among instances.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, we propose to additionally use the gating mechanism (Dauphin et al., 2016) together with tanh(·) non-linearity that yields:
ak = exp{w>
( tanh ( Vh>k ) sigm",2.4. Attention-based MIL pooling,[0],[0]
"( Uh>k )) }
K∑ j=1 exp{w> ( tanh ( Vh>j ) sigm",2.4. Attention-based MIL pooling,[0],[0]
"( Uh>j )) } , (9)
where U ∈ RL×M are parameters, is an element-wise multiplication and sigm(·) is the sigmoid non-linearity.",2.4. Attention-based MIL pooling,[0],[0]
"The gating mechanism introduces a learnable non-linearity that potentially removes the troublesome linearity in tanh(·).
Flexibility In principle, the proposed attention-based MIL pooling allows to assign different weights to instances within a bag and hence the final representation of the bag could be highly informative for the bag-level classifier.",2.4. Attention-based MIL pooling,[0],[0]
"In other words, it should be able to find key instances.",2.4. Attention-based MIL pooling,[0],[0]
"Moreover, application of the attention-based MIL pooling together with the transformations f and g parameterized by neural networks makes the whole model fully differentiable and adaptive.",2.4. Attention-based MIL pooling,[0],[0]
These two facts make the proposed MIL pooling a potentially very flexible operator that could model an arbitrary permutation-invariant score function.,2.4. Attention-based MIL pooling,[0],[0]
"The proposed attention mechanism together with a deep MIL model is depicted in Figure 6(c) in the Appendix.
",2.4. Attention-based MIL pooling,[0],[0]
"Interpretability Ideally, in the case of a positive label (Y = 1), high attention weights should be assigned to instances that are likely to have label yk = 1 (key instances).",2.4. Attention-based MIL pooling,[0],[0]
"Namely, the attention mechanism allows to easily interpret the provided decision in terms of instance-level labels.",2.4. Attention-based MIL pooling,[0],[0]
"In fact, the attention network does not provide scores as the instance-based classifier does but it can be considered as a proxy to that.",2.4. Attention-based MIL pooling,[0],[0]
"The attention-based MIL pooling bridges the instance-level approach and the embedding-level approach.
",2.4. Attention-based MIL pooling,[0],[0]
"From the practical point of view, e.g., in the computational pathology, it is desirable to provide ROIs together with the final diagnosis to a doctor.",2.4. Attention-based MIL pooling,[0],[0]
"Therefore, the attention mechanism is potentially of great interest in practical applications.",2.4. Attention-based MIL pooling,[0],[0]
"MIL pooling Typically, MIL approaches utilize either the mean pooling or the max pooling, while the latter is mostly used (Feng & Zhou, 2017; Pinheiro & Collobert, 2015; Zhu et al., 2017).",3. Related work,[0],[0]
Both operators are non-trainable which potentially limits their applicability.,3. Related work,[0],[0]
"There are MIL pooling operators that contain global adaptive parameters, such as noisy-and (Kraus et al., 2016), however, their flexibility is restricted.",3. Related work,[0],[0]
"We propose a fully trainable MIL pooling that adapts to new instances.
",3. Related work,[0],[0]
MIL with neural networks In the classical work on MIL it is assumed that instances are represented by precomputed features and there is very little need to apply additional feature extraction.,3. Related work,[0],[0]
"Nevertheless, recent work on utilizing fully-connected neural networks in MIL shows that it could still be beneficial (Wang et al., 2016).",3. Related work,[0],[0]
"Similarly, in computer vision the idea of MIL combined with deep learning significantly improves final accuracy (Oquab et al., 2014).",3. Related work,[0],[0]
"In this paper, we follow this line of research since it allows to apply a flexible class of transformations that can be trained end-to-end by backpropagation.
MIL and attention The attention mechanism is widely used in deep learning for image captioning (Xu et al., 2015) or text analysis (Bahdanau et al., 2014; Lin et al., 2017).",3. Related work,[0],[0]
In the context of the MIL problem it has rarely been used and only in a very limited form.,3. Related work,[0],[0]
"In (Pappas & Popescu-Belis, 2014) an attention-based MIL was proposed but attention weights were trained as parameters of an auxiliary linear regression model.",3. Related work,[0],[0]
"This idea was further expanded and the linear regression model was replaced by a one-layer neural network with single output (Pappas & Popescu-Belis, 2017).",3. Related work,[0],[0]
"The attention-based MIL operator was used very recently in (Qi et al., 2017), however, the attention was calculated using the dot product and it performed worse than the max operator.",3. Related work,[0],[0]
"Here, we propose to use a two-layered neural network to learn the MIL operator and we show that it outperforms commonly used MIL pooling operators.
",3. Related work,[0],[0]
MIL for medical imaging The MIL seems to perfectly fit medical imaging where processing a whole image consisting of billions of pixels is computationally infeasible.,3. Related work,[0],[0]
"Moreover, in the medical domain it is very difficult to obtain pixel-level annotations, that drastically reduces number of available data.",3. Related work,[0],[0]
"Therefore, it is tempting to divide a medical image into smaller patches that could be further considered as a bag with a single label (Quellec et al., 2017).",3. Related work,[0],[0]
"This idea attracts a great interest in the computational histopathology where patches could correspond to cells that are believed to indicate malignant changes (Sirinukunwattana et al., 2016).",3. Related work,[0],[0]
"Different MIL approaches were used for histopathology data, such as, Gaussian processes (Kandemir et al., 2014; 2016) or a two-stage approach with neural networks and EM algorithm to determine instance classes (Hou et al., 2016).
",3. Related work,[0],[0]
"Other applications of MIL methods in medical imaging are mammography (nodule) classification (Zhu et al., 2017) and microscopy cell detection (Kraus et al., 2016).",3. Related work,[0],[0]
"In this paper, we show that the proposed attention-based deep MIL approach can be used not only to provide the final diagnosis but also to indicate ROIs in a histopathology slide.",3. Related work,[0],[0]
In the experiments we aim at evaluating the proposed approach: a MIL model parameterized with neural networks and a (gated) attention-based pooling layer (’Attention’ and ’Gated-Attention’).,4. Experiments,[0],[0]
"We evaluate our approach on a number of different MIL datasets: five MIL benchmark datasets (MUSK1, MUSK2, FOX, TIGER, ELEPHANT), an MNIST-based image dataset (MNIST-BAGS) and two reallife histopathology datasets (BREAST CANCER, COLON CANCER).",4. Experiments,[0],[0]
"We want to verify two research questions in the experiments: (i) whether our approach achieves the best performance or is comparable to the best performing method, (ii) if our method can provide interpretable results by using the attention weights that indicate key instances or ROIs.
",4. Experiments,[0],[0]
"In order to obtain a fair comparison we use a common evaluation methodology, i.e., 10-fold-cross-validation, and five repetitions per experiment.",4. Experiments,[0],[0]
In the case of MNIST-BAGS we use a fixed division into training and test set.,4. Experiments,[0],[0]
In order to create test bags we solely sampled images from the MNIST test set.,4. Experiments,[0],[0]
During training we only used images from the MNIST training set.,4. Experiments,[0],[0]
"For all experiments we use modified versions of models that have shown high classification performance on the individual datasets (Wang et al., 2016; LeCun et al., 1998; Sirinukunwattana et al., 2016).",4. Experiments,[0],[0]
The MIL pooling layers are either located before the last layer of the model (the embedded-based approach) or after last layer of the model (the instance-based approach).,4. Experiments,[0],[0]
If an attention-based MIL pooling layer is used the number of parameters in V was determined using a validation set.,4. Experiments,[0],[0]
"We tested the following dimensions (L): 64, 128 and 256.",4. Experiments,[0],[0]
The different dimensions only resulted in minor changes of the model’s performance.,4. Experiments,[0],[0]
For layers using the gated attention mechanism V and U have the same number of parameters.,4. Experiments,[0],[0]
"Finally, all layers were initialized according to Glorot & Bengio (2010) and biases were set to zero.
",4. Experiments,[0],[0]
We compare our approach to various MIL methods on MIL benchmark datasets.,4. Experiments,[0],[0]
On the image datasets our method is compared with instance-level and embedding-level neural networks and commonly used MIL pooling layers (max and mean).,4. Experiments,[0],[0]
"In the following, we are using ’Instance+max/mean’ and ’Embedding+max/mean’ to indicate networks that are build from convolutional layers and fully-connected layers.",4. Experiments,[0],[0]
"In contrast to networks purely build from fully-connected layers, referred to as ’mi-Net’ and ’MI-Net’ (Wang et al., 2016).
",4. Experiments,[0],[0]
"On MNIST-BAGS we include a SVM-based MIL model, called (MI-SVM).",4. Experiments,[0],[0]
We do not present results of MI-SVM on the histopathology datasets since we could not train (including hyperparameter search and five times 10-fold-crossvalidation procedure) the model in a reasonable amount of time.3,4. Experiments,[0],[0]
"In order to compare the bag level performance we use the following metrics: the classification accuracy, precision, recall, F-score, and the area under the receiver operating characteristic curve (AUC).",4. Experiments,[0],[0]
Details In the first experiment we aim at verifying whether our approach can compete with the best MIL methods on historically important benchmark datasets.,4.1. Classical MIL datasets,[0],[0]
"Since all five datasets contain precomputed features and only a small number of instances and bags, neural networks are most likely not well suited.",4.1. Classical MIL datasets,[0],[0]
First we predict drug activity (MUSK1 and MUSK2).,4.1. Classical MIL datasets,[0],[0]
A molecule has the desired drug effect if and only if one or more of its conformations bind to the target binding site.,4.1. Classical MIL datasets,[0],[0]
"Since molecules can adopt multiple shapes, a bag is made up of shapes belonging to the same molecule (Dietterich et al., 1997).",4.1. Classical MIL datasets,[0],[0]
"The three remaining datasets, ELEPHANT, FOX and TIGER, contain features extracted from images.",4.1. Classical MIL datasets,[0],[0]
Each bag consists of a set of segments of an image.,4.1. Classical MIL datasets,[0],[0]
"For each category, positive bags are images that contain the animal of interest, and negative bags are images that contain other animals (Andrews et al., 2003).",4.1. Classical MIL datasets,[0],[0]
"For detailed information on the number of bags, instances and features in each dataset see Section 6.3 in the Appendix.
",4.1. Classical MIL datasets,[0],[0]
"In our experiments we use the same architecture, optimizer and hyperparameters as in the MI-Net model (Wang et al., 2016).
Results and discussion The results of the experiment are 3Learning a single MI-SVM took approximately one week due to the large number of patches.
presented in Table 1.",4.1. Classical MIL datasets,[0],[0]
Our approaches (Attention and GatedAttention) are comparable with the best performing classical MIL methods (notice the standard error of the mean).,4.1. Classical MIL datasets,[0],[0]
Details The main disadvantage of the classical MIL benchmark datasets is that instances are represented by precomputed features.,4.2. MNIST-bags,[0],[0]
"In order to consider a more challenging scenario, we propose to investigate a dataset that is created using the well-known MNIST image dataset.",4.2. MNIST-bags,[0],[0]
A bag is made up of a random number of 28× 28 grayscale images taken from the MNIST dataset.,4.2. MNIST-bags,[0],[0]
The number of images in a bag is Gaussian-distributed and the closest integer value is taken.,4.2. MNIST-bags,[0],[0]
A bag is given a positive label if it contains one or more images with the label ’9’.,4.2. MNIST-bags,[0],[0]
We chose ’9’ since it can be easily mistaken with ’7’ or ’4’.,4.2. MNIST-bags,[0],[0]
We investigate the influence of the number of bags in the training set as well as the average number of instances per bag on the prediction performance.,4.2. MNIST-bags,[0],[0]
During evaluation we use a fixed number of 1000 test bags.,4.2. MNIST-bags,[0],[0]
"For all experiments a LeNet5 model is used (LeCun et al., 1998), see Table 8 and 9 in the Appendix.",4.2. MNIST-bags,[0],[0]
"The models are trained with the Adam optimization algorithm (Kingma & Ba, 2014).",4.2. MNIST-bags,[0],[0]
"We keep the default parameters for β1 and β2, see Table 10 in the Appendix.",4.2. MNIST-bags,[0],[0]
"In addition, we compare our method with a SVM-based MIL method (MI-SVM) (Andrews et al., 2003) that uses a Gaussian kernel on raw pixel features4.
",4.2. MNIST-bags,[0],[0]
"In the experiments we use different numbers of the mean bag size, namely, 10, 50 and 100, and the variance 2, 10, 20, respectively.",4.2. MNIST-bags,[0],[0]
"Moreover, we use varying numbers of training bags, i.e., 50, 100, 150, 200, 300, 400, 500.",4.2. MNIST-bags,[0],[0]
These different settings allow us to verify how different number of training bags and different number of instances influence MIL models.,4.2. MNIST-bags,[0],[0]
We compare instance-based and embedding-based approaches parameterized with a neural network (LeNet5) with mean and max MIL pooling.,4.2. MNIST-bags,[0],[0]
"We use AUC as the evaluation metric.
Results and discussion The results of AUC for the mean bag sizes equal to 10, 50 and 100 are presented in Figure 1, 2 and 3, respectively, and detailed results are given in the Appendix.",4.2. MNIST-bags,[0],[0]
"The findings of the experiment are the following: First, the proposed attention-based deep MIL approach performs much better than other methods in the small sample size regime.",4.2. MNIST-bags,[0],[0]
"Moreover, when there is a small effective size of the training set that corresponds to 50-150 bags for around 10 instances per bag (see Figure 1) or 50-100 bags in the case of on average 50 instances in a bag (see Figure 2), our method still achieves significantly higher AUC than all other methods.",4.2. MNIST-bags,[0],[0]
"Second, we notice that our approach is more flexible and obtained better results than the SVM-
4We use code provided with (Doran & Ray, 2014): https: //github.com/garydoranjr/misvm
based approach in all cases except large effective sample sizes (see Figure 3).",4.2. MNIST-bags,[0],[0]
"Third, the embedding-based models performed better than the instance-based models.",4.2. MNIST-bags,[0],[0]
"However, for a sufficient number of training images (number of training bags and training instances per bag) all models achieve very similar results.",4.2. MNIST-bags,[0],[0]
"Fourth, the mean operator performs significantly worse than the max operator.",4.2. MNIST-bags,[0],[0]
"However, the embedding-based model with the mean operator converged eventually to the best value but always later than the one with max.",4.2. MNIST-bags,[0],[0]
"See Section 6.4 in the Appendix for details.
",4.2. MNIST-bags,[0],[0]
The results of this experiment indicate that for a smallsample size regime our approach is preferable to others.,4.2. MNIST-bags,[0],[0]
"Since attention serves as a gradient update filter during backpropagation (Wang et al., 2017), instances with higher weights will contribute more to learning the encoder network of instances.",4.2. MNIST-bags,[0],[0]
This is especially important since medical imaging problems contain only a small number of cases.,4.2. MNIST-bags,[0],[0]
"In general, the more instances are in a bag the easier the MIL task becomes, since the MIL assumption states that every instance in a negative bag is negative.",4.2. MNIST-bags,[0],[0]
"For example, a negative bag of size 100 from the MNIST-bags dataset will include about 11 negative examples per class.
",4.2. MNIST-bags,[0],[0]
"Finally, we present an exemplary result of the attention mechanism in Figure 4.",4.2. MNIST-bags,[0],[0]
In this example a bag consists of 13 images.,4.2. MNIST-bags,[0],[0]
For each digit the corresponding attention weight is given by the trained network.,4.2. MNIST-bags,[0],[0]
The bag is properly predicted as positive and all nines are correctly highlighted.,4.2. MNIST-bags,[0],[0]
"Hence, the attention mechanism works as expected.",4.2. MNIST-bags,[0],[0]
More examples are given in the Appendix.,4.2. MNIST-bags,[0],[0]
Details An automatic detection of cancerous regions in hematoxylin and eosin (H&E) stained whole-slide images is a task with high clinical relevance.,4.3. Histopathology datasets,[0],[0]
"Current supervised approaches utilize pixel-level annotations (Litjens et al., 2017).",4.3. Histopathology datasets,[0],[0]
"However, data preparation requires large amount of time from pathologists which highly interferes with their daily routines.",4.3. Histopathology datasets,[0],[0]
"Hence, a successful solution working with weak labels would hold a great promise to reduce the workload of the pathologists.",4.3. Histopathology datasets,[0],[0]
"In the following, we perform two experiments on classifying weakly-labeled real-life histopathol-
ogy images of the breast cancer dataset (BREAST CANCER) (Gelasca et al., 2008) and the colon cancer dataset (COLON CANCER) (Sirinukunwattana et al., 2016).
",4.3. Histopathology datasets,[0],[0]
BREAST CANCER consists of 58 weakly labeled 896× 768 H&E images.,4.3. Histopathology datasets,[0],[0]
"An image is labeled malignant if it contains breast cancer cells, otherwise it is benign.",4.3. Histopathology datasets,[0],[0]
We divide every image into 32 × 32 patches.,4.3. Histopathology datasets,[0],[0]
This results in 672 patches per bag.,4.3. Histopathology datasets,[0],[0]
"A patch is discarded if it contains 75% or more of white pixels.
",4.3. Histopathology datasets,[0],[0]
COLON CANCER comprises 100 H&E images.,4.3. Histopathology datasets,[0],[0]
The images originate from a variety of tissue appearance from both normal and malignant regions.,4.3. Histopathology datasets,[0],[0]
For every image the majority of nuclei of each cell were marked.,4.3. Histopathology datasets,[0],[0]
"In total there are 22,444 nuclei with associated class label, i.e. epithelial, inflammatory, fibroblast, and miscellaneous.",4.3. Histopathology datasets,[0],[0]
A bag is composed of 27×27 patches.,4.3. Histopathology datasets,[0],[0]
"Furthermore, a bag is given a positive label if it contains one or more nuclei from the epithelial class.",4.3. Histopathology datasets,[0],[0]
"Tagging epithelial cells is highly relevant from a clinical point of view, since colon cancer originates from epithelial cells (Ricci-Vitiani et al., 2007).
",4.3. Histopathology datasets,[0],[0]
"For both datasets we use the model proposed in (Sirinukunwattana et al., 2016) for the transformation f .",4.3. Histopathology datasets,[0],[0]
"All models are trained with the Adam optimization algorithm (Kingma & Ba, 2014).",4.3. Histopathology datasets,[0],[0]
Due to the limited amount of data samples in both datasets we performed data augmentation to prevent overfitting.,4.3. Histopathology datasets,[0],[0]
"See the Appendix for further details.
Results and discussion We present results in Table 2 and 3 for BREAST CANCER and COLON CANCER, respectively.",4.3. Histopathology datasets,[0],[0]
"First, we notice that the obtained results confirm our findings in MNIST-BAGS experiment that our approach outperforms all other methods.",4.3. Histopathology datasets,[0],[0]
A trend that is especially visible in the small-sample size regime of the MNIST-BAGS.,4.3. Histopathology datasets,[0],[0]
"Surprisingly, the embedding-based method with the max pooling failed almost completely on BREAST CANCER but in general this dataset is difficult due to high variability of slides and small number of cases.",4.3. Histopathology datasets,[0],[0]
The proposed method is not only most accurate but it also received the highest recall.,4.3. Histopathology datasets,[0],[0]
High recall is especially important in the medical domain since false negatives could lead to severe consequences including patient fatality.,4.3. Histopathology datasets,[0],[0]
"We also notice that the gated-attention mechanism performs better than the plain attention mechanism on BREAST CANCER while these two behave similarly on COLON CANCER.
",4.3. Histopathology datasets,[0],[0]
"Eventually, we present the usefulness of the attention mechanism in providing ROIs.",4.3. Histopathology datasets,[0],[0]
In Figure 5 we show a histopathology image divided into patches containing (mostly) single cells.,4.3. Histopathology datasets,[0],[0]
We create a heatmap by multiplying patches by its corresponding attention weight.,4.3. Histopathology datasets,[0],[0]
"Although only image-level annotations are used during training, there is a substantial matching between the heatmap in Figure 5(d) and the ground truth in Figure 5(c).",4.3. Histopathology datasets,[0],[0]
"Additionally, we notice that
the instance-based classifier tends to select only a small subset of positive patches (see Figure 10(e) in Appendix) that confirms low instance accuracy of the instance-based approach discussed in (Kandemir & Hamprecht, 2015).",4.3. Histopathology datasets,[0],[0]
"For
more examples please see the Appendix.
",4.3. Histopathology datasets,[0],[0]
The obtained results again confirm that the proposed approach attains high predictive performance and allows to properly highlight ROIs.,4.3. Histopathology datasets,[0],[0]
"Moreover, the attention weights can be used to create a reliable heatmap.",4.3. Histopathology datasets,[0],[0]
"In this paper, we proposed a flexible and interpretable MIL approach that is fully parameterized by neural networks.",5. Conclusion,[0],[0]
We outlined the usefulness of deep learning for modeling a permutation-invariant bag score function in terms of the Fundamental Theorem of Symmetric Functions.,5. Conclusion,[0],[0]
"Moreover, we presented a trainable MIL pooling based on the (gated) attention mechanism.",5. Conclusion,[0],[0]
"We showed empirically on five MIL datasets, one image corpora and two real-life histopathology datasets that our method is on a par with the best performing methods or performs the best in terms of different evaluation metrics.",5. Conclusion,[0],[0]
"Additionally, we showed that our approach provides an interpretation of the decision by presenting ROIs, which is extremely important in many practical applications.
",5. Conclusion,[0],[0]
We strongly believe that the presented line of research is worth pursuing further.,5. Conclusion,[0],[0]
"Here we focused on a binary MIL problem, however, the multi-class MIL is more interesting and challenging (Feng & Zhou, 2017).",5. Conclusion,[0],[0]
"Moreover, in some applications it is worth to consider repulsion points (Scott et al., 2005), i.e., instances for which a bag is always negative, or assume dependencies among instances within a bag (Zhou et al., 2009).",5. Conclusion,[0],[0]
We leave investigating these issues for future research.,5. Conclusion,[0],[0]
"The authors are very grateful to Rianne van den Berg for insightful remarks and discussions.
",Acknowledgements,[0],[0]
Maximilian Ilse was funded by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek (Grant DLMedIa:,Acknowledgements,[0],[0]
"Deep Learning for Medical Image Analysis).
",Acknowledgements,[0],[0]
"Jakub Tomczak was funded by the European Commission within the Marie Skodowska-Curie Individual Fellowship (Grant No. 702666, ”Deep learning and Bayesian inference for medical imaging”).",Acknowledgements,[0],[0]
Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances.,abstractText,[0],[0]
"In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks.",abstractText,[0],[0]
"Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism.",abstractText,[0],[0]
"Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label.",abstractText,[0],[0]
We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.,abstractText,[0],[0]
Attention-based Deep Multiple Instance Learning,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 247–256, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Most of the sentiment analysis research focuses on sentiment classification which aims to determine whether the users attitude is positive, neutral or negative.",1 Introduction,[0],[0]
"There are two classes of mainstreaming sentiment classification algorithms: unsupervised methods which usually require a sentiment lexicon
(Taboada et al., 2011) and supervised methods (Pang et al., 2002) which require manually labeled data.",1 Introduction,[0],[0]
"However, both of these sentiment resources are unbalanced in different languages.",1 Introduction,[0],[0]
The sentiment lexicon or labeled data are rich in several languages such as English and are poor in others.,1 Introduction,[0],[0]
Manually building these resources for all the languages will be expensive and time-consuming.,1 Introduction,[0],[0]
Cross-lingual sentiment classification tackles the problem by trying to adapt the resources in one language to other languages.,1 Introduction,[0],[0]
"It can also be regarded as a special kind of cross-lingual text classification task.
",1 Introduction,[0],[0]
"Recently, there have been several bilingual representation learning methods such as (Hermann and Blunsom, 2014; Gouws et al., 2014) for cross-lingual sentiment or text classification which achieve promising results.",1 Introduction,[0],[0]
They try to learn a joint embedding space for different languages such that the training data in the source language can be directly applied to the test data in the target language.,1 Introduction,[0],[0]
"However, most of the studies only use simple functions, e.g. arithmetic average, to synthesize representations for larger text sequences.",1 Introduction,[0],[0]
"Some of them use more complicated compositional models such as the bi-gram non-linearity model in (Hermann and Blunsom, 2014) which also fail to capture the long distance dependencies in texts.
",1 Introduction,[0],[0]
"In this study, we propose an attention-based bilingual LSTM network for cross-lingual sentiment classification.",1 Introduction,[0],[0]
LSTMs have been proved to be very effective to model word sequences and are powerful to learn on data with long range temporal dependencies.,1 Introduction,[0],[0]
"After translating the training data into the target language using machine translation
247
tools, we use the bidirectional LSTM network to model the documents in both of the source and the target languages.",1 Introduction,[0],[0]
"The LSTMs show strong ability to capture the compositional semantics for the bilingual texts in our experiments.
",1 Introduction,[0],[0]
"For the traditional LSTM network, each word in the input document is treated with equal importance, which is reasonable for traditional text classification tasks.",1 Introduction,[0],[0]
"In this paper, we propose a hierarchical attention mechanism which enables our model to focus on certain part of the input document.",1 Introduction,[0],[0]
The motivation mainly comes from the following three observations: 1) the machine translation tool that we use to translate the documents will always introduce much noise for sentiment classification.,1 Introduction,[0],[0]
We hope that the attention mechanism can help to filter out these noises.,1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
"In each individual language, the sentiment of a document is usually decided by a relative small part of it.",1 Introduction,[0],[0]
"In a long review document, the user might discuss both the advantages and disadvantages of a product.",1 Introduction,[0],[0]
The sentiment will be confusing if we consider each sentence of the same contribution.,1 Introduction,[0],[0]
"For example, in the first review of Table 1, the first sentence reveals a negative sentiment towards the movie but the second one reveals a positive sentiment.",1 Introduction,[0],[0]
"As human readers, we can understand that the review is expressing a positive overall sentiment but it is hard for the sequence modeling algorithms including LSTM to capture.",1 Introduction,[0],[0]
3),1 Introduction,[0],[0]
"At the sentence level, it is important to focus on the sentiment signals such as the sentiment words.",1 Introduction,[0],[0]
"They are usually very decisive to determine the polarity even for a very long sentence, e.g. “easy” and “nice” in the second example of Table 1.
",1 Introduction,[0],[0]
"In sum, the main contributions of this study are summarized as follows:
1) We propose a bilingual LSTM network for
cross-lingual sentiment classification.",1 Introduction,[0],[0]
"Compared to the previous methods which only use weighted or arithmetic average of word embeddings to represent the document, LSTMs have obvious advantage to model the compositional semantics and to capture the long distance dependencies between words for bilingual texts.
",1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
We propose a hierarchical bilingual attention mechanism for our model.,1 Introduction,[0],[0]
"To the best of our knowledge, this is the first attention-based model designed for cross-lingual sentiment analysis.
",1 Introduction,[0],[0]
3),1 Introduction,[0],[0]
The proposed framework achieves good results on a benchmark dataset from a cross-language sentiment classification evaluation.,1 Introduction,[0],[0]
It outperforms the best team in the evaluation as well as several strong baseline methods.,1 Introduction,[0],[0]
"Sentiment analysis is the field of studying and analyzing peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012).",2 Related Work,[0],[0]
The most common task of sentiment analysis is polarity classification which arises with the emergence of customer reviews on the Internet.,2 Related Work,[0],[0]
Pang et al. (2002) used supervised learning methods and achieved promising results with simple unigram and bi-gram features.,2 Related Work,[0],[0]
"In subsequent research, more features and learning algorithms were tried for sentiment classification by a large number of researchers.",2 Related Work,[0],[0]
"Recently, the emerging of deep learning has also shed light on this area.",2 Related Work,[0],[0]
"Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), multi-channel convolutional neural networks (Kim, 2012), dynamic convolutional neural network (Blunsom et al., 2014) and tree structure LSTM (Tai et al., 2015).",2 Related Work,[0],[0]
"Very recently, Yang et al. (2016) proposed a similar hierarchical attention network based on GRU in the monolingual setting.",2 Related Work,[0],[0]
"Note that our work is independent with theirs and their study was released online after we submitted this study.
",2 Related Work,[0],[0]
"Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis
community which aims to solve the sentiment classification task from a cross-language view.",2 Related Work,[0],[0]
It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language.,2 Related Work,[0],[0]
Cross-lingual sentiment classification has been extensively studied in the very recent years.,2 Related Work,[0],[0]
Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier.,2 Related Work,[0],[0]
Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages.,2 Related Work,[0],[0]
"Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training.
",2 Related Work,[0],[0]
There have also been several studies addressing the task via multi-lingual text representation learning.,2 Related Work,[0],[0]
Xiao and Guo (2013) learned different representations for words in different languages.,2 Related Work,[0],[0]
Part of the word vector is shared among different languages and the rest is language-dependent.,2 Related Work,[0],[0]
"Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora.",2 Related Work,[0],[0]
Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences.,2 Related Work,[0],[0]
It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences.,2 Related Work,[0],[0]
Pham et al. (2015) extended the paragraph model into bilingual setting.,2 Related Work,[0],[0]
"Each pair of parallel sentences shares the same paragraph vector.
",2 Related Work,[0],[0]
"Compared to the existing studies, we propose to use the bilingual LSTM network to learn the document representations of reviews in each individual language.",2 Related Work,[0],[0]
It has obvious advantage to model the compositional semantics and to capture the long distance dependencies between words.,2 Related Work,[0],[0]
"Besides, we propose a hierarchical neural attention mechanism to capture the sentiment attention in each document.",2 Related Work,[0],[0]
The attention model helps to filter out the noise which is irrelevant to the overall sentiment.,2 Related Work,[0],[0]
Cross-language sentiment classification aims to use the training data in the source language to build a model which is adaptable for the test data in the target language.,3.1 Problem Definition,[0],[0]
"In our setting, we have labeled training data in English LEN = {xi, yi}Ni=1 , where xi is the review text and yi is the sentiment label vector.",3.1 Problem Definition,[0],[0]
"yi = (1, 0) represents the positive sentiment and yi = (0, 1) represents the negative sentiment.",3.1 Problem Definition,[0],[0]
"In the target language Chinese, we have the test data TCN = {xi}Ti=1 and unlabeled data UCN = {xi}Mi=1.",3.1 Problem Definition,[0],[0]
"The task is to use LEN and UCN to learn a model and classify the sentiment polarity for the review texts in TCN .
",3.1 Problem Definition,[0],[0]
"In our method, the labeled, unlabeled and test data are all translated into the other language using an online machine translation tool.",3.1 Problem Definition,[0],[0]
"In the subsequent part of the paper, we refer to a document and its corresponding translation in the other language as a pair of parallel documents.",3.1 Problem Definition,[0],[0]
"Recurrent neural network (RNN) (Rumelhart et al., 1988) is a special kind of feed-forward neural network which is useful for modeling time-sensitive sequences.",3.2 RNN and LSTM,[0],[0]
"At each time t, the model receives input from the current example and also from the hidden layer of the network’s previous state.",3.2 RNN and LSTM,[0],[0]
The output is calculated given the hidden state at that time stamp.,3.2 RNN and LSTM,[0],[0]
The recurrent connection makes the output at each time associated with all the previous inputs.,3.2 RNN and LSTM,[0],[0]
The vanilla RNN model has been considered to be difficult to train due to the well-known problem of vanishing and exploding gradients.,3.2 RNN and LSTM,[0],[0]
"The LSTM (Hochreiter and Schmidhuber, 1997) addresses the problem by re-parameterizing the RNN model.",3.2 RNN and LSTM,[0],[0]
The core idea of LSTM is introducing the “gates” to control the data flow in the recurrent neural unit.,3.2 RNN and LSTM,[0],[0]
The LSTM structure ensures that the gradient of the long-term dependencies cannot vanish.,3.2 RNN and LSTM,[0],[0]
The detailed architecture that we use in shown in Figure 1.,3.2 RNN and LSTM,[0],[0]
"In this study, we try to model the bilingual texts through the attention based LSTM network.",4 Framework,[0],[0]
"We first
describe the general architecture of the model and then describe the attention mechanism used in it.",4 Framework,[0],[0]
The general architecture of our approach is shown in Figure 2.,4.1 Architecture,[0],[0]
"For a pair of parallel documents xcn and xen, each of them is sent into the attention based
LSTM network.",4.1 Architecture,[0],[0]
The English-side and Chineseside architectures are the same but have different parameters.,4.1 Architecture,[0],[0]
We only show the Chinese-side network in the figure due to space limit.,4.1 Architecture,[0],[0]
The whole model is divided into four layers.,4.1 Architecture,[0],[0]
"In the input layer, the documents are represented as a word sequence where each position corresponds to a word vector from pre-trained word embeddings.",4.1 Architecture,[0],[0]
"In the LSTM layer, we get the high-level representation from a bidirectional LSTM network.",4.1 Architecture,[0],[0]
We use the hidden units from both the forward and backward LSTMs.,4.1 Architecture,[0],[0]
"In the document representation layer, we incorporate the attention model into the network and derive the final document representation.",4.1 Architecture,[0],[0]
"At the output layer, we concatenate the representations of the English and Chinese documents and use the softmax function to predict the sentiment label.
",4.1 Architecture,[0],[0]
Input Layer:,4.1 Architecture,[0],[0]
The input layer of the network is the word sequences in a document x which can be either Chinese or English.,4.1 Architecture,[0],[0]
"The document x contains several sentences {si}|x|i=1 and each sentence is composed of several words si = {wi,j}|si|j=1 .",4.1 Architecture,[0],[0]
"We represent each word in the document as a fixed-size vector from pre-trained word embeddings.
",4.1 Architecture,[0],[0]
LSTM Layer:,4.1 Architecture,[0],[0]
"In each individual language, we use bi-directional LSTMs to model the input sequences.",4.1 Architecture,[0],[0]
"In the bidirectional architecture, there are two layers of hidden nodes from two separate LSTMs.",4.1 Architecture,[0],[0]
The two LSTMs capture the dependencies in different directions.,4.1 Architecture,[0],[0]
"The first hidden layers have recurrent connections from the past words while second one’s direction of recurrent of connections is flipped, passing activation backwards in the texts.",4.1 Architecture,[0],[0]
"Therefore, in the LSTM layer, we can get the forward hidden state ~hi,j from the forward LSTM network and the backward hidden state ~hi,j from the backward LSTM network.",4.1 Architecture,[0],[0]
"We represent the final state at position (i, j), i.e. the j-th word in the i-th sentence of the document, with the concatenation of ~hi,j and ~hi,j .
hi,j = ~hi,j ‖ ~hi,j
It captures the compositional semantics in both directions of the word sequences.
",4.1 Architecture,[0],[0]
"Document Representation Layer:As described above, different parts of the document usually have different importance for the overall sentiment.",4.1 Architecture,[0],[0]
"Some
sentences or words can be decisive while the others are irrelevant.",4.1 Architecture,[0],[0]
"In this study, we use a hierarchical attention mechanism which assigns a real value score for each word and a real value score for each sentence.",4.1 Architecture,[0],[0]
"The detailed strategy of our attention model will be described in the next subsection.
",4.1 Architecture,[0],[0]
Suppose we have the sentence attention score Ai for each sentence,4.1 Architecture,[0],[0]
"si ∈ x, and the word attention score ai,j for each word wi,j ∈ si, both of the scores are normalized which satisfy the following equations,
∑
i
Ai = 1 and ∑
j
ai,j = 1
The sentence attention measures which sentence is more important for the overall sentiment while the word attention captures sentiment signals such as sentiment words in each sentence.",4.1 Architecture,[0],[0]
"Therefore, the document representation r for document x is calculated as follows,
r = ∑
i
[Ai · ∑
j
(ai,j · hi,j)]
Note that many LSTM based models represent the word sequences only using the hidden layer at the final node.",4.1 Architecture,[0],[0]
"In this study, the hidden states at all the positions are considered with different attention weights.",4.1 Architecture,[0],[0]
"We believe that, for document sentiment classification, focusing on some certain parts of the document will be effective to filter out the sentimentirrelevant noise.
",4.1 Architecture,[0],[0]
Output Layer:,4.1 Architecture,[0],[0]
"At the output layer, we need to predict the overall sentiment of the document.",4.1 Architecture,[0],[0]
"For each English document xen and its corresponding translation xcn, suppose the document representations of them are obtained in previous steps as ren and rcn, we simply concatenate them as the feature vector and use the softmax function to predict the final sentiment.
",4.1 Architecture,[0],[0]
ŷ = softmax(rcn ‖ ren),4.1 Architecture,[0],[0]
"For document-level sentiment classification task, we have shown that capturing both the sentence and word level attention is important.",4.2 Hierarchical Attention Mechanism,[0],[0]
"The general idea is inspired by previous works such as Bahdanau et
al. (2014) and Hermann et al. (2015) which have successfully applied the attention model to machine translation and question answering.",4.2 Hierarchical Attention Mechanism,[0],[0]
Bahdanau et al. (2014) incorporated the attention model into the sequence to sequence learning framework.,4.2 Hierarchical Attention Mechanism,[0],[0]
"During the decoding phase of the machine translation task, the attention model helps to find which input word should be “aligned” to the current output.",4.2 Hierarchical Attention Mechanism,[0],[0]
"In our case, the output of the model is not a sequence but only one sentiment vector.",4.2 Hierarchical Attention Mechanism,[0],[0]
"We hope to find the important units in the input sequence which are influential for the output.
",4.2 Hierarchical Attention Mechanism,[0],[0]
We propose to learn a hierarchical attention model jointly with the bilingual LSTM network.,4.2 Hierarchical Attention Mechanism,[0],[0]
The first level is the sentence attention model which measures which sentences are more important for the overall sentiment of a document.,4.2 Hierarchical Attention Mechanism,[0],[0]
"For each sentence si = {wi,j}|si|j=1 in the document, we represent the sentence via the final hidden state of the forward LSTM and the backward LSTM, i.e.
si = ~hi,|si| ‖ ~hi,1
",4.2 Hierarchical Attention Mechanism,[0],[0]
"We use a two-layer feed-forward neural network to predict the attention score of si
Âi = f(si; θs)
Ai = exp(Âi)∑ j exp(Âj)
where f denotes the two-layer feed-forward neural network and θs denotes the parameters in it.
",4.2 Hierarchical Attention Mechanism,[0],[0]
"At the word level, we represent each word wi,j using its word embedding and the hidden state of the bidirectional LSTM layer, i.e. hi,j .",4.2 Hierarchical Attention Mechanism,[0],[0]
"Similarly, we use a two-layer feed forward neural network to predict the attention score of wi,j ,
ei,j = wi,j ‖ ~hi,j ‖ ~hi,j
âi,j = f(ei,j ; θw)
ai,j = exp(âi,j)∑ j exp(âi,j)
where θw denotes the parameters for predicting word attention.",4.2 Hierarchical Attention Mechanism,[0],[0]
The proposed model is trained in a semi-supervised manner.,4.3 Training of the Proposed Model,[0],[0]
"In the supervised part, we use the cross entropy loss to minimize the sentiment prediction error between the output results and the gold standard labels,
L1 = ∑
(xen,xcn)
∑
i
−yi log(ŷi)
where xen and xcn are a pair of parallel documents in the training data, y is the gold-standard sentiment vector and ŷ is the predicted vector from our model.
",4.3 Training of the Proposed Model,[0],[0]
The unsupervised part tries to minimize the document representations between the parallel data.,4.3 Training of the Proposed Model,[0],[0]
"Following previous research, we simply measure the distance of two parallel documents via the Euclidean Distance,
L2 = ∑
(xen,xcn)
‖ren",4.3 Training of the Proposed Model,[0],[0]
"− rcn‖2
where xen and xcn are a pair of parallel documents from both the labeled and unlabeled data.
",4.3 Training of the Proposed Model,[0],[0]
"The final objective function is a weighted sum of L1 and L2,
L = L1 + α · L2 where α is the hyper-parameter controlling the weight.",4.3 Training of the Proposed Model,[0],[0]
"We use Adadelta (Zeiler, 2012) to update the parameters during training.",4.3 Training of the Proposed Model,[0],[0]
"It can dynamically adapt over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent.
",4.3 Training of the Proposed Model,[0],[0]
"In the test phase, the test document in TCN is sent into our model along with the corresponding machine translated text in TEN .",4.3 Training of the Proposed Model,[0],[0]
The final sentiment is predicted via a softmax function over the concatenated representation of the bilingual texts as described above.,4.3 Training of the Proposed Model,[0],[0]
"We use the dataset from the cross-language sentiment classification evaluation of NLP&CC 2013.1
1The dataset can be found at http://tcci.ccf.org.cn/conference/2013/index.html.",5.1 Dataset,[0],[0]
"NLP&CC is an annual conference specialized in the fields of Natural
The dataset contains reviews in three domains including book, DVD and music.",5.1 Dataset,[0],[0]
"In each domain, it has 2000 positive reviews and 2000 negative reviews in English for training and 4000 Chinese reviews for test.",5.1 Dataset,[0],[0]
"It also contains 44113, 17815 and 29678 unlabeled reviews for book, DVD and music respectively.",5.1 Dataset,[0],[0]
We use Google Translate2 to translate the labeled data to Chinese and translate the unlabeled data and test data to English.,5.2 Implementation Detail,[0],[0]
"All the texts are tokenized and converted into lower case.
",5.2 Implementation Detail,[0],[0]
"In the proposed framework, the dimensions of the word vectors and the hidden layers of LSTMs are set as 50.",5.2 Implementation Detail,[0],[0]
The initial word embeddings are trained on both the unlabeled and labeled reviews using word2vec in each individual language.,5.2 Implementation Detail,[0],[0]
The word vectors are fine-tuned during the training procedure.,5.2 Implementation Detail,[0],[0]
The hyper-parameter a is set to 0.2.,5.2 Implementation Detail,[0],[0]
The dropout rate is set to 0.5 to prevent overfitting.,5.2 Implementation Detail,[0],[0]
Ten percent of the training data are randomly selected as validation set.,5.2 Implementation Detail,[0],[0]
The training procedure is stopped when the prediction accuracy does not improve for 10 iterations.,5.2 Implementation Detail,[0],[0]
"We implement the framework based on theano (Bastien et al., 2012) and use a GTX 980TI graphic card for training.",5.2 Implementation Detail,[0],[0]
"To evaluate the performance of our model, we compared it with the following baseline methods:
LR and SVM:",5.3 Baselines and Results,[0],[0]
We use logistic regression and SVM to learn different classifiers based on the translated Chinese training data.,5.3 Baselines and Results,[0],[0]
"We simply use unigram features.
",5.3 Baselines and Results,[0],[0]
"MT-PV: Paragraph vector (Le and Mikolov, 2014) is considered as one of the state-of-the-art monolingual document modeling methods.",5.3 Baselines and Results,[0],[0]
We translate all the training data into Chinese and use paragraph vector to learn a vector representation for the training and test data.,5.3 Baselines and Results,[0],[0]
"A logistic regression classifier is used to predict the sentiment polarity.
",5.3 Baselines and Results,[0],[0]
Bi-PV: Pham et al. (2015) is one the state-ofthe-art bilingual document modeling methods.,5.3 Baselines and Results,[0],[0]
"It extends the paragraph vector into bilingual setting.
",5.3 Baselines and Results,[0],[0]
"Language Processing (NLP) and Chinese Computing (CC) organized by Chinese Computer Federation (CCF).
",5.3 Baselines and Results,[0],[0]
"2http://translate.google.com/
Each pair of parallel sentences in the training data shares the same vector representation.
",5.3 Baselines and Results,[0],[0]
BSWE:,5.3 Baselines and Results,[0],[0]
Zhou et al. (2015) proposed the bilingual sentiment word embedding algorithm based on denoising autoencoders.,5.3 Baselines and Results,[0],[0]
It learns the vector representations for 2000 sentiment words.,5.3 Baselines and Results,[0],[0]
"Each document is then represented by the sentiment words and the corresponding negation words in it.
",5.3 Baselines and Results,[0],[0]
H-Eval: Gui et al. (2013) got the highest performance in the NLP&CC 2013 cross-lingual sentiment classification evaluation.,5.3 Baselines and Results,[0],[0]
"It uses a mixed CLSC model by combining co-training and transfer learning strategies.
",5.3 Baselines and Results,[0],[0]
A-Eval:,5.3 Baselines and Results,[0],[0]
"This is the average performance of all the teams in the NLP&CC 2013 cross-lingual sentiment classification evaluation.
",5.3 Baselines and Results,[0],[0]
"The attention-based models EN-Attention, CNAttention and BI-Attention: Bi-Attention is the model described in the above sections which concatenate the document representations of the English side and the Chinese side texts.",5.3 Baselines and Results,[0],[0]
"EN-Attention only translates the Chinese test data into English and uses English-side attention model while CN-Attention only uses the Chinese side attention model.
",5.3 Baselines and Results,[0],[0]
Table 2 shows the cross-lingual sentiment classification accuracy of all the approaches.,5.3 Baselines and Results,[0],[0]
The first kind baseline algorithms are based on traditional bag-of-word features.,5.3 Baselines and Results,[0],[0]
SVM performs better than LR on book and DVD but gets much worse result on music.,5.3 Baselines and Results,[0],[0]
"The second kind baseline algorithms are based on deep learning methods which learn the vector representations for words or documents.
",5.3 Baselines and Results,[0],[0]
MT-PV achieves similar results with LR.,5.3 Baselines and Results,[0],[0]
Bi-PV improves the accuracy by about 0.03 using both the bilingual documents.,5.3 Baselines and Results,[0],[0]
"While MT-PV and BiPV directly learn document representations, BSWE learns the embedding for the words in a bilingual sentiment lexicon.",5.3 Baselines and Results,[0],[0]
"It gets higher accuracy than both Bi-PV and MT-PV which shows that the sentiment words are very important for this task.
",5.3 Baselines and Results,[0],[0]
Our attention based models achieve the highest prediction accuracy among all the approaches.,5.3 Baselines and Results,[0],[0]
The results show that CN-Attention always outperforms EN-Attention.,5.3 Baselines and Results,[0],[0]
The combination of the English-side and Chinese-side model brings improvement to both the book and music domains and yields the highest average prediction accuracy.,5.3 Baselines and Results,[0],[0]
The attention-based models outperform the algorithms using traditional features as well as the existing deep learning based methods.,5.3 Baselines and Results,[0],[0]
"Compared to the highest performance in the NLP&CC evaluation, we improve the average accuracy by about 0.05.",5.3 Baselines and Results,[0],[0]
"In this study, we propose a hierarchical attention mechanism to capture the sentiment-related information of each document.",5.4 Influence of the Attention Mechanism,[0],[0]
"In table 3, we show the results of models with different attention mechanisms.",5.4 Influence of the Attention Mechanism,[0],[0]
All the models are based on the bilingual bi-directional LSTM network as shown in Figure 2.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM is the basic bilingual bi-directional LSTM network.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM+SA considers only sentence-level attention while LSTM+WA considers only wordlevel attention.,5.4 Influence of the Attention Mechanism,[0],[0]
LSTM+HA combines both wordlevel and sentence-level attentions.,5.4 Influence of the Attention Mechanism,[0],[0]
"From the results, we can observe that LSTM+HA outperforms the other three methods, which proves the effectiveness of the hierarchical attention mechanism.",5.4 Influence of the Attention Mechanism,[0],[0]
"Besides, the word-level attention shows better performance than the sentence-level attention.
",5.4 Influence of the Attention Mechanism,[0],[0]
We also conduct a case study using the examples in Table 1.,5.4 Influence of the Attention Mechanism,[0],[0]
"We show the visualized word attention
using a heat map in Figure 3 by drawing the attention of each word in it.",5.4 Influence of the Attention Mechanism,[0],[0]
The darker color reveals higher attention scores while the lighter part has little importance.,5.4 Influence of the Attention Mechanism,[0],[0]
We can observe that our model successfully identifies the important units of the sentence.,5.4 Influence of the Attention Mechanism,[0],[0]
The sentiment word “easy” gets much higher attention score than the other words.,5.4 Influence of the Attention Mechanism,[0],[0]
The word “nice” gets the third highest score in the sentence right after the two “easy”.,5.4 Influence of the Attention Mechanism,[0],[0]
Note that our attention mechanism considers both the word embedding vector and the hidden state vectors.,5.4 Influence of the Attention Mechanism,[0],[0]
"Therefore, the same word “easy” gets different scores in different positions.",5.4 Influence of the Attention Mechanism,[0],[0]
"For the deep learning based methods, the initial word embeddings used as the inputs for the network usually play an important role.",5.5 Influence of the Word Embeddings,[0],[0]
"We study four different settings called rand, static, fine-tuned and multi-channel, respectively.",5.5 Influence of the Word Embeddings,[0],[0]
"In rand setting, the word embeddings are randomly initialized.",5.5 Influence of the Word Embeddings,[0],[0]
The static setting keeps initial embedding fixed while the fine-tuned setting learns a refined embedding during the training procedure.,5.5 Influence of the Word Embeddings,[0],[0]
Multi-channel is the combination of static and fine-tuned.,5.5 Influence of the Word Embeddings,[0],[0]
Two same word vectors are concatenated to represent each word.,5.5 Influence of the Word Embeddings,[0],[0]
"During the training procedure, half of it is fine-tuned while the rest is fixed.",5.5 Influence of the Word Embeddings,[0],[0]
"Note that finetuned is the embedding setting that we use in our model.
",5.5 Influence of the Word Embeddings,[0],[0]
Table 4 shows the performance of our model in these settings.,5.5 Influence of the Word Embeddings,[0],[0]
"Rand gets the lowest accuracy among
them.",5.5 Influence of the Word Embeddings,[0],[0]
"The fine-tuned word embeddings perform better than static which fits the results in previous study (Kim, 2012).",5.5 Influence of the Word Embeddings,[0],[0]
Multi-channel gets similar results with fine-tuned on DVD and music but is a bit lower on book.,5.5 Influence of the Word Embeddings,[0],[0]
We also find that using pre-trained word embeddings helps the model to converge much faster than random initialization.,5.5 Influence of the Word Embeddings,[0],[0]
"In our experiment, we set the size of the hidden layers in both the forward and backward LSTMs the same as the size of the input word vectors.",5.6 Influence of Vector Sizes,[0],[0]
"Therefore, the dimension of the document representation is twice of the word vector size.",5.6 Influence of Vector Sizes,[0],[0]
"In Figure 4, we show the performance of our model with different input vector sizes.",5.6 Influence of Vector Sizes,[0],[0]
"We use the vector size in the following set {10, 25, 50, 100, 150, 200}.",5.6 Influence of Vector Sizes,[0],[0]
"Note that the dimensions of all the units in the model also change with that.
",5.6 Influence of Vector Sizes,[0],[0]
We can observe from Figure 4 that the prediction accuracy for the book domain keeps steady when the vector size changes.,5.6 Influence of Vector Sizes,[0],[0]
"For DVD and music, the performance increases at the beginning and becomes stable after the vector size grows larger than 50.",5.6 Influence of Vector Sizes,[0],[0]
It shows that our model is robust to a wide range of vector sizes.,5.6 Influence of Vector Sizes,[0],[0]
"In this paper, we propose an attention based LSTM network for cross-language sentiment classification.",6 Conclusion,[0],[0]
We use the bilingual bi-directional LSTMs to model the word sequences in the source and target languages.,6 Conclusion,[0],[0]
"Based on the special characteristics of the sentiment classification task, we propose a hierarchical attention model which is jointly trained with the LSTM network.",6 Conclusion,[0],[0]
"The sentence level attention
enables us to find the key sentences in a document and the word level attention helps to capture the sentiment signals.",6 Conclusion,[0],[0]
The proposed model achieves promising results on a benchmark dataset using Chinese as the source language and English as the target language.,6 Conclusion,[0],[0]
It outperforms the best results in the NLPC&CC cross-language sentiment classification evaluation as well as several strong baselines.,6 Conclusion,[0],[0]
"In future work, we will evaluate the performance of our model on more datasets and more language pairs.",6 Conclusion,[0],[0]
The sentiment lexicon is also another kind of useful resource for classification.,6 Conclusion,[0],[0]
We will explore how to make full usages of these resources in the proposed framework.,6 Conclusion,[0],[0]
"The work was supported by National Natural Science Foundation of China (61331011), National HiTech Research and Development Program (863 Program) of China (2015AA015403, 2014AA015102) and IBM Global Faculty Award Program.",Acknowledgments,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
Xiaojun Wan is the corresponding author.,Acknowledgments,[0],[0]
Most of the state-of-the-art sentiment classification methods are based on supervised learning algorithms which require large amounts of manually labeled data.,abstractText,[0],[0]
"However, the labeled resources are usually imbalanced in different languages.",abstractText,[0],[0]
Cross-lingual sentiment classification tackles the problem by adapting the sentiment resources in a resource-rich language to resource-poor languages.,abstractText,[0],[0]
"In this study, we propose an attention-based bilingual representation learning model which learns the distributed semantics of the documents in both the source and the target languages.",abstractText,[0],[0]
"In each language, we use Long Short Term Memory (LSTM) network to model the documents, which has been proved to be very effective for word sequences.",abstractText,[0],[0]
"Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network.",abstractText,[0],[0]
The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive.,abstractText,[0],[0]
The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language.,abstractText,[0],[0]
Attention-based LSTM Network for Cross-Lingual Sentiment Classification,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 127–136, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
One major application of embodied spoken dialogue systems is to improve life for elderly people by providing companionship and social interaction.,1 Introduction,[0],[0]
"Several conversational robots have been designed for this specific purpose (Heerink et al., 2008; Sabelli et al., 2011; Iwamura et al., 2011).",1 Introduction,[0],[0]
A necessary feature of such a system is that it be an attentive listener.,1 Introduction,[0],[0]
This means providing feedback to the user as they are talking so that they feel some sort of rapport and engagement with the system.,1 Introduction,[0],[0]
"Humans can interact with attentive listeners
at any time, making them a useful tool for people such as the elderly.
",1 Introduction,[0],[0]
Our motivation is to create a robot which can function as an attentive listener.,1 Introduction,[0],[0]
"Towards this goal, we use the autonomous android named Erica.",1 Introduction,[0],[0]
Our long-term goal is for Erica to be able to participate in a conversation with a human user while displaying human-like speech and gesture.,1 Introduction,[0],[0]
"In this work we focus on integrating an attentive listener function into Erica and describe a new approach for this application.
",1 Introduction,[0],[0]
"The approaches to these kind of dialogue systems have focused mainly on backchanneling behavior and have been implemented in large-scale projects such as SimSensei (DeVault et al., 2014), Sensitive Artificial Listeners (Bevacqua et al., 2012) and active listening robots (Johansson et al., 2016).",1 Introduction,[0],[0]
"These systems are multimodal in nature, using human-like non-verbal behaviors to give feedback to the user.",1 Introduction,[0],[0]
"However, the backchannels are usually generated after the end of utterance and they do not necessarily create synchrony in the conversation (Kawahara et al., 2015).",1 Introduction,[0],[0]
"Moreover, the dialogue systems are still based on handcrafted keyword matching.",1 Introduction,[0],[0]
"This means that new lines of dialogue or extensions to new topics must be handcrafted, which becomes impractical.
",1 Introduction,[0],[0]
In this paper we present an approach to attentive listening which integrates continuous backchannels with responsive dialogue to user statements to maintain the flow of conversation.,1 Introduction,[0],[0]
We create a continuous prediction model which is perceived as being better than a model which predicts only after an IPU (inter-pausal unit) has been received from the automatic speech recognition (ASR) system.,1 Introduction,[0],[0]
"Meanwhile, the statement response system detects focus words of the user’s utterance and uses them to generate responses as a wh-question or by repeating it back to the user.",1 Introduction,[0],[0]
"We also introduce a novel approach to turn-taking which uses
127
backchannels and fillers to indicate confidence in taking the speaking turn.
",1 Introduction,[0],[0]
Our approach is not limited by the topic of conversation and no prior parameters about the conversation are required so it can be applied to open domain conversation.,1 Introduction,[0],[0]
"We also do not require perfect speech recognition accuracy, which has been identified as a limitation in other attentive listening systems (Bevacqua et al., 2012).",1 Introduction,[0],[0]
"Our system runs efficiently in real-time and can be flexibly integrated into a larger architecture, which we will also demonstrate through a conversational robot.
",1 Introduction,[0],[0]
The next section outlines the architecture of our attentive listener.,1 Introduction,[0],[0]
In Section 3 we describe in detail the major components of the attentive listener including results of evaluation experiments.,1 Introduction,[0],[0]
"We then implement this system into Erica as a proofof-concept in Section 4, before the conclusion of the paper.",1 Introduction,[0],[0]
"Our system is in Japanese, but English translations are used in the paper for clarity.",1 Introduction,[0],[0]
Figure 1 summarizes the components of attentive listening and the general system architecture.,2 System architecture,[0],[0]
"Inputs to the system are prosodic features, which is calculated continuously, and ASR results from the Japanese speech recognition system Julius (Lee et al., 2001).
",2 System architecture,[0],[0]
"We implement a dialogue act tagger which classifies an utterance into questions, statements or others such as greetings.",2 System architecture,[0],[0]
This is currently based on a support vector machine and is moving to a recurrent neural network.,2 System architecture,[0],[0]
Questions and others are handled by a separate module which will not be explained in this paper.,2 System architecture,[0],[0]
Statements are handled by a statement response component.,2 System architecture,[0],[0]
"The other two components in the attentive listener are a backchannel generator and a turn-taking model.
",2 System architecture,[0],[0]
"Backchannels are generated by one component, while the statement response component can generate different types of dialogue depending on the utterance of the user.",2 System architecture,[0],[0]
"As part of our NLP functionalities we have a focus word extractor trained by a conditional random field (Yoshino and Kawahara, 2015) which identifies the focus of an utterance.",2 System architecture,[0],[0]
"For example, the statement “Yesterday I ate curry.”",2 System architecture,[0],[0]
would produce a focus word of “curry”.,2 System architecture,[0],[0]
We then send this information to the statement response component which generates a question response “What kind of curry?”.,2 System architecture,[0],[0]
"Further details of the technical implementation are described in the
next section.",2 System architecture,[0],[0]
The process flow of the system is as follows.,2 System architecture,[0],[0]
The system performs continuous backchanneling behavior while listening to the speaker.,2 System architecture,[0],[0]
"At the same time, ASR results of the user are received.",2 System architecture,[0],[0]
"When the utterance unit is detected and its dialogue act is tagged as a statement, then a response is generated and then stored.",2 System architecture,[0],[0]
"However, a response is only actually output when the system predicts an appropriate time to take the turn.",2 System architecture,[0],[0]
This is because the user may wish to keep talking and the system should not interrupt.,2 System architecture,[0],[0]
"Thus, we can manage turn-taking more flexibly.
",2 System architecture,[0],[0]
"In summary, the three major components required for attentive listening are backchanneling, statement response and turn-taking.",2 System architecture,[0],[0]
In this section we describe the three major components of attentive listening.,3 Attentive listening components,[0],[0]
We evaluate each of these components individually.,3 Attentive listening components,[0],[0]
"Our goal is to increase rapport (Huang et al., 2011) with the user by showing that the system is interested in the content of the user’s speech.",3.1 Continuous backchannel generation,[0],[0]
"There have been many works on automatic backchannel generation, with most using prosodic features for either rule-based models (Ward and Tsukahara, 2000; Truong et al., 2010) or machine learning methods (Morency et al., 2008; Ozkan et al., 2010; Kawahara et al., 2015).
",3.1 Continuous backchannel generation,[0],[0]
"In this work we use a model in which backchanneling behavior occurs continuously during the speaker’s turn, not only at the end of an utterance.",3.1 Continuous backchannel generation,[0],[0]
We take a machine learning approach by implementing a logistic regression model to predict if a backchannel would occur 500ms into the future.,3.1 Continuous backchannel generation,[0],[0]
"We predict into the future rather than at the current time point, because in the real-time system Erica requires processing time to generate nodding and mouth movements that synchronize with her utterance.",3.1 Continuous backchannel generation,[0],[0]
We trained the model using a counseling corpus.,3.1 Continuous backchannel generation,[0],[0]
"This corpus consisted of eight one-to-one counseling sessions between a counselor and a student and were transcribed according to the guidelines of the Corpus of Spontaneous Japanese (CSJ) (Maekawa, 2003).
",3.1 Continuous backchannel generation,[0],[0]
"The model makes a prediction every 100ms by using windows of prosodic features of sizes 100, 200, 500, 1000 and 2000 milliseconds.",3.1 Continuous backchannel generation,[0],[0]
"For a win-
dow size s, feature extraction is conducted within windows every s milliseconds before the current time point, up to a maximum of 4s milliseconds.",3.1 Continuous backchannel generation,[0],[0]
"For example, for a time window of 100ms, prosodic features are calculated inside windows starting at 400, 300, 200 and 100 milliseconds before the current time point.",3.1 Continuous backchannel generation,[0],[0]
"The prosodic features are the mean, maximum, minimum, range and slope of the pitch and intensity.",3.1 Continuous backchannel generation,[0],[0]
"Finally, we add the durations of silence, voice activity, and overlap of the speaker and listener.
",3.1 Continuous backchannel generation,[0],[0]
We conducted two evaluations of the backchannel timing model.,3.1 Continuous backchannel generation,[0],[0]
The first is an objective evaluation of the precision and recall.,3.1 Continuous backchannel generation,[0],[0]
We used 8-fold cross validation and tested on individual sessions.,3.1 Continuous backchannel generation,[0],[0]
We compared against a baseline model which generated a backchannel after every IPU (Fixed) and an IPU-based model based on logistic regression which also predicted after every IPU using additional linguistic features (IPU-based).,3.1 Continuous backchannel generation,[0],[0]
"Our model showed that the most influential prosodic feature was the range and maximum intensity of the speech, with larger windows located just before the prediction point generally being more influential than other windows.",3.1 Continuous backchannel generation,[0],[0]
"Although we have no quantitative evidence, we propose that a reduction in the intensity of the speech provides an opportunity for the listener to produce a backchannel.",3.1 Continuous backchannel generation,[0],[0]
"The results are displayed in Table 1.
",3.1 Continuous backchannel generation,[0],[0]
We see that the time-based model performs better than the baseline and the IPU-based model with a high AUC and recall.,3.1 Continuous backchannel generation,[0],[0]
"The precision is fairly low, due to predicting a large number backchannels even though none in the corpus are found.
",3.1 Continuous backchannel generation,[0],[0]
We also conducted a subjective evaluation of this model by comparing against the same models as the objective evaluation.,3.1 Continuous backchannel generation,[0],[0]
"We also included an additional counselor condition, in which backchannels in the real corpus were substituted with the same recorded pattern.
",3.1 Continuous backchannel generation,[0],[0]
"Participants in the experiment listened to recorded segments from the counseling corpus, lasting around 30-40 seconds each.",3.1 Continuous backchannel generation,[0],[0]
We chose segments where the counselor acted as an attentive listener by only responding through the backchannels used in our model.,3.1 Continuous backchannel generation,[0],[0]
The counselor’s voice for backchannels was generated using a recorded pattern by a female voice actress.,3.1 Continuous backchannel generation,[0],[0]
We created the different conditions for each recording by applying our model directly to the audio signal of the speaker.,3.1 Continuous backchannel generation,[0],[0]
The audio channel of the counselor’s voice was separated and so could be removed.,3.1 Continuous backchannel generation,[0],[0]
"When the model determined that a backchannel should be generated at a timepoint, we manually inserted the backchannel pattern into the speaker’s channel using audio editing software, effectively replacing the counselor’s voice.
",3.1 Continuous backchannel generation,[0],[0]
Each condition was listened to twice by each participant through different recordings selected at random.,3.1 Continuous backchannel generation,[0],[0]
"Subjects rated each recording over five measures - naturalness and tempo of backchannels (Q1 and Q2), empathy and understanding (Q3 and Q4) and if the participant would like to talk with the counselor in the recording (Q5).",3.1 Continuous backchannel generation,[0],[0]
"Each measure was rated using a 7-point Likert scale.
",3.1 Continuous backchannel generation,[0],[0]
For analysis we conducted a repeated measures ANOVA with Bonferroni corrections.,3.1 Continuous backchannel generation,[0],[0]
"Results are
shown in Table 2.",3.1 Continuous backchannel generation,[0],[0]
"Our proposed model outperformed the baseline models and was comparable to the counselor condition.
",3.1 Continuous backchannel generation,[0],[0]
The results of both evaluations show the need for backchannel timing to be done continuously and not just at the end of utterances.,3.1 Continuous backchannel generation,[0],[0]
The statement response component is triggered for statements and outputs when the system takes a turn.,3.2 Statement response,[0],[0]
The purpose is to encourage the user to expand on what they have just said and extend the thread of the conversation.,3.2 Statement response,[0],[0]
The statement response tries to use a question phrase which repeats a word that the user has previously said.,3.2 Statement response,[0],[0]
"For example, if the user says “I will go to the beach.”, the statement response should generate a question such as “Which beach?”.",3.2 Statement response,[0],[0]
"It may also repeat the focus of the utterance back to the user to encourage elaboration, such as “The beach?”.
",3.2 Statement response,[0],[0]
Our approach uses wh-questions as a means to continue the conversation.,3.2 Statement response,[0],[0]
"From a linguistic perspective, they are described in question taxonomies by Graesser et al. (1994) and Nielsen et al. (2008) as concept completions (who, what, when, where) or feature specifications (what properties does X have?).",3.2 Statement response,[0],[0]
"We observe that listeners in everyday conversations use such phrases to get the speaker to provide more information.
",3.2 Statement response,[0],[0]
"From a technical perspective, there are two processes for the system.",3.2 Statement response,[0],[0]
The first process is to detect the focus word of the utterance.,3.2 Statement response,[0],[0]
The second is to correctly pair this with an appropriate whquestion word to form a meaningful question.,3.2 Statement response,[0],[0]
"The basic wh-question words are similar for both English and Japanese.
",3.2 Statement response,[0],[0]
"To detect the focus word we use a conditional random field classifier in previous work which uses part-of-speech tags and a phrase-level depen-
dency tree (Yoshino and Kawahara, 2015).",3.2 Statement response,[0],[0]
The model was trained with utterances from users interacting with two different dialogue systems.,3.2 Statement response,[0],[0]
"This corpus was then annotated to identify the focus phrases of sentences.
",3.2 Statement response,[0],[0]
We use a decision tree in Figure 2 to decide from one of four response types.,3.2 Statement response,[0],[0]
"If a focus phrase can be detected, we take each noun in the phrase, match them to a wh-question and select the pair with the maximum likelihood.",3.2 Statement response,[0],[0]
We used an ngram language model to compute the joint probability of the focus noun being associated with each question word.,3.2 Statement response,[0],[0]
"The corpus used is the Balanced Corpus of Contemporary Written Japanese, which contains 100 million words from written documents.",3.2 Statement response,[0],[0]
We then consider the maximum joint probability of this noun and a question word.,3.2 Statement response,[0],[0]
"If this is over a threshold Tf , then a question on the focus word is generated.",3.2 Statement response,[0],[0]
"If no question is generated, the focus noun is repeated with a rising tone.
",3.2 Statement response,[0],[0]
If no focus phrase is found we match the predicate of the utterance to a question word using the same method as above.,3.2 Statement response,[0],[0]
"If this is above a threshold Tp, then the response is a question on the predicate, otherwise a formulaic expression is generated as a fallback response.",3.2 Statement response,[0],[0]
"We provide examples of each of the response types in Table 3.
",3.2 Statement response,[0],[0]
We evaluated this component in two different ways.,3.2 Statement response,[0],[0]
"Firstly, we extracted dialogue from an existing chatting corpus created for Project Next’s NLP task1.",3.2 Statement response,[0],[0]
We selected 200 user statements from this corpus as a test set and applied the statement response system to them.,3.2 Statement response,[0],[0]
Two annotators then checked if the generated responses were appropriate.,3.2 Statement response,[0],[0]
"The results are shown in Table 4.
",3.2 Statement response,[0],[0]
The results showed that the algorithm could classify the statements reasonably well.,3.2 Statement response,[0],[0]
"However, in the case of a focus word being unable to be
1https://sites.google.com/ site/dialoguebreakdowndetection/ chat-dialogue-corpus
found correctly identifying a question word for a predicate is a challenge.
",3.2 Statement response,[0],[0]
"Next, we evaluated our statement response system by testing if it could reduce the number of fallback responses used by the system.",3.2 Statement response,[0],[0]
"We conducted this experiment with 22 participants, and gathered data on their utterances during a first-time meeting with Erica.",3.2 Statement response,[0],[0]
"In most cases the participants asked questions that could be answered by the system, but sometimes the users said statements for which the question-answering system could not formulate a response.",3.2 Statement response,[0],[0]
"In these cases a generic fallback response was generated.
",3.2 Statement response,[0],[0]
From the data we found that 39 out of 226 (17.2%) user utterances produced fallback responses.,3.2 Statement response,[0],[0]
We processed all these utterances offline through the statement response component.,3.2 Statement response,[0],[0]
"From these 39 statements, 19 (47.7%) result in a statement which could be categorized into either a question on focus, partial repeat, or a question on predicate.",3.2 Statement response,[0],[0]
"Furthermore, the generated responses were deemed to be coherent with the correct focus and question words being applied.",3.2 Statement response,[0],[0]
This would have continued the flow of conversation.,3.2 Statement response,[0],[0]
The goal of turn-taking is to manage the floor of the conversation.,3.3 Flexible turn-taking,[0],[0]
The system decides when it should take the turn using a decision model.,3.3 Flexible turn-taking,[0],[0]
One simple approach is to wait for a fixed duration of silence from the user before starting the speaking turn.,3.3 Flexible turn-taking,[0],[0]
"However, we have found this is highly user-dependent and very challenging when the user continues talking.",3.3 Flexible turn-taking,[0],[0]
"The major problem is that if the user has not finished their turn and the system begins speaking, they must then wait for the system’s utterance to finish.",3.3 Flexible turn-taking,[0],[0]
This disrupts the flow of the conversation and makes the user frustrated.,3.3 Flexible turn-taking,[0],[0]
"Solving this problem is not trivial so several works have attempted to develop a robust model for turn-taking (Raux and Eskenazi, 2009; Selfridge and Heeman, 2010; Ward et al., 2010).
",3.3 Flexible turn-taking,[0],[0]
"Figure 3 displays our approach towards turntaking behavior, rather than having to make a binary decision about whether or not to take the turn.",3.3 Flexible turn-taking,[0],[0]
"When the user has the floor and the system receives an ASR result, our model outputs a likelihood score between 0 and 1 that the system should take the turn.",3.3 Flexible turn-taking,[0],[0]
The actual likelihood score determines the system’s response.,3.3 Flexible turn-taking,[0],[0]
"The system has four possible responses - silence, generate a backchannel, generate a filler or take the turn by speaking.
",3.3 Flexible turn-taking,[0],[0]
The novelty of our approach is that we do not have to immediately take a turn based on a hard threshold.,3.3 Flexible turn-taking,[0],[0]
Backchannels encourage the user to continue speaking and signal that the system will not take the turn.,3.3 Flexible turn-taking,[0],[0]
"Fillers are known to indicate a willingness to take the turn (Clark and Tree, 2002; Ishi et al., 2006) and so are used to grab the turn from the user.",3.3 Flexible turn-taking,[0],[0]
"However, the user may still wish to continue speaking and if they do the system won’t grab the turn and so doesn’t interrupt the flow of
conversation.",3.3 Flexible turn-taking,[0],[0]
"To guarantee that Erica will eventually take the turn, we set a threshold for the user’s silence time and automatically take the turn once it elapses.
",3.3 Flexible turn-taking,[0],[0]
"To implement this system, we used a logistic regression model with the same features as our backchanneling model.",3.3 Flexible turn-taking,[0],[0]
We train using the same counseling corpus and features that were used for the backchanneling model.,3.3 Flexible turn-taking,[0],[0]
"We found 25% of the outputs within the corpus to be turn changes.
",3.3 Flexible turn-taking,[0],[0]
Our proposed model requires two likelihood score thresholds (T1 and T2) to decide whether or not to be silent (≤ T1) or take the turn (≥ T2).,3.3 Flexible turn-taking,[0],[0]
We set a threshold for deciding between backchannels and fillers to 0.5.,3.3 Flexible turn-taking,[0],[0]
"We determined T1 to be 0.45 and T2 to be 0.85 based on Figure 4, which displays the distributions of likelihood score for the two classes.
",3.3 Flexible turn-taking,[0],[0]
The performance of this model is shown in Table 5.,3.3 Flexible turn-taking,[0],[0]
We compared the proposed model to a logistic regression model with a single threshold at 0.5.,3.3 Flexible turn-taking,[0],[0]
"Results are shown in Table 5.
",3.3 Flexible turn-taking,[0],[0]
These two thresholds degrade the recall of turntaking ground-truth actions because the cases in between them are discarded.,3.3 Flexible turn-taking,[0],[0]
"However we improve the precision of taking the turn, which is critical in spoken dialogue systems, from 0.428 to 0.624.",3.3 Flexible turn-taking,[0],[0]
"The cases discarded in this stage will be recovered by uttering fillers or backchannels.
",3.3 Flexible turn-taking,[0],[0]
"Moreover, the ground-truth labels are based on actual turn-taking actions made by the human listener, and there should be more Transition Relevance Places (Sacks et al., 1974), where turntaking would be allowed.",3.3 Flexible turn-taking,[0],[0]
This should be addressed in future work.,3.3 Flexible turn-taking,[0],[0]
In this section we describe the overall system with the attentive listener being integrated into the conversational android Erica.,4 System,[0],[0]
Erica is an android robot that takes the appearance of a young woman.,4.1 ERICA,[0],[0]
Her purpose is to use conversation to play a variety of social roles.,4.1 ERICA,[0],[0]
"The physical realism of Erica necessitates that her conver-
sational behaviors are also human-like.",4.1 ERICA,[0],[0]
"Therefore our objective is not only to undertake natural language processing, but to also address a variety of conversational phenomena.
",4.1 ERICA,[0],[0]
The environment we create for Erica reduces the need to use a physical interface such as a handheld microphone or headset to have a conversation.,4.1 ERICA,[0],[0]
Instead we use a spherical microphone array placed on a table between Erica and the user.,4.1 ERICA,[0],[0]
"A photo of this environment is shown in Figure 5.
",4.1 ERICA,[0],[0]
"Based on the microphone array and the Kinect sensor, we are able to reliably determine the source of speech.",4.1 ERICA,[0],[0]
Erica only considers speech from a particular user and ignores unrelated noises such as ambient sounds and her own voice.,4.1 ERICA,[0],[0]
We conducted an initial evaluation of our system as a pilot study to demonstrate its appropriateness for attentive listening.,4.2 Pilot study,[0],[0]
We have observed from previous demonstrations that users often do not speak with Erica as if she is an attentive listener.,4.2 Pilot study,[0],[0]
"Rather, they simply ask Erica questions and wait for her answers.",4.2 Pilot study,[0],[0]
"To overcome this issue in order to evaluate the statement response system, we first provided the subjects with dialogue prompts in the form of scripts.",4.2 Pilot study,[0],[0]
This allowed users familiarize themselves with Erica for free conversation.,4.2 Pilot study,[0],[0]
"Two male graduate students were subjects in the experiment and interacted with Erica in these two different tasks.
",4.2 Pilot study,[0],[0]
The first task was to read from four conversational scripts of 3 to 5 turns each.,4.2 Pilot study,[0],[0]
"These scripts were not hand-crafted, but taken from a corpus of real attentive listening conversations with a Wizard-of-Oz controlled robot.",4.2 Pilot study,[0],[0]
Subjects were instructed to pause after each sentence in the script to wait for a statement response.,4.2 Pilot study,[0],[0]
"If Erica replied with a question they could answer it before con-
tinuing the scripted conversation.",4.2 Pilot study,[0],[0]
The second task was to speak with Erica freely while she did attentive listening.,4.2 Pilot study,[0],[0]
In this scenario the subjects talked freely on the subject of their favorite travel memories.,4.2 Pilot study,[0],[0]
They could end the conversation whenever they wished.,4.2 Pilot study,[0],[0]
"Statistics of the subjects’ turns are shown in Table 6.
",4.2 Pilot study,[0],[0]
We find that the subjects reading from the script had longer turns but the speaking rate was lower than for free talk.,4.2 Pilot study,[0],[0]
"In other words, script reading was slower and longer.",4.2 Pilot study,[0],[0]
"We also analyzed the distribution of response types generated from the system as shown in Table 7.
Backchannels were generated most frequently, while both questions on focus and formulaic expressions were the most common response types, with questions on focus words having the highest frequency in free conversation.",4.2 Pilot study,[0],[0]
Partial repeats had a much higher frequency in the scripts than in free conversation.,4.2 Pilot study,[0],[0]
"This is because the script readings were taken from conversations which used more complex sentences than the free talk, and focus nouns for which a suitable question word could not be reliably matched.",4.2 Pilot study,[0],[0]
We evaluated the system by asking 8 evaluators to listen to the recording of both the scripts and free conversation.,4.3 Subjective ratings,[0],[0]
"Each evaluator was assigned
one random script and both free conversations to evaluate.",4.3 Subjective ratings,[0],[0]
"The evaluators rated each of Erica’s backchannels and statement responses in terms of coherence (coherent, somewhat coherent, or incoherent) and timing (fast, appropriate, or slow).",4.3 Subjective ratings,[0],[0]
We used a majority vote to determine the overall rating of each speech act.,4.3 Subjective ratings,[0],[0]
"The ratings on the coherence of each statement are shown in Figure 6.
",4.3 Subjective ratings,[0],[0]
We see that the results are similar to the previous evaluation of the statement response system.,4.3 Subjective ratings,[0],[0]
"More than half of questions on focus words were coherent, although most of these were in response to the scripts.",4.3 Subjective ratings,[0],[0]
"Formulaic expressions were mostly coherent even though they were selected at random.
",4.3 Subjective ratings,[0],[0]
"Similarly, we categorized system utterances into backchannels or statements and analyzed timing.",4.3 Subjective ratings,[0],[0]
"The results are shown in Figure 7.
",4.3 Subjective ratings,[0],[0]
"We can see that while most backchannels have suitable timing, statement responses are slow due to the processing of the utterance that is required.",4.3 Subjective ratings,[0],[0]
Table 8 shows dialogue from a free talk conversation.,4.4 Generated dialogue,[0],[0]
"User utterances were punctuated by backchannels and the system is able to extract a focus noun or predicate and produce a coherent response.
",4.4 Generated dialogue,[0],[0]
"We also found that the system could produce a coherent response even in the case of ASR errors.
",4.4 Generated dialogue,[0],[0]
In one case the subject said “sakana tsuri wo shimashita (I went fishing.).”.,4.4 Generated dialogue,[0],[0]
"The ASR system generated “sakana wo sore wo sumashita”, which is nonsensical.",4.4 Generated dialogue,[0],[0]
"In this case, the word “fish” was successfully detected as the focus noun and a coherent response could be generated.",4.4 Generated dialogue,[0],[0]
We also examined 17 utterances determined to be incoherent (excluding backchannels and formulaic expressions) and analyzed the reasons for these.,4.5 Analysis of incoherent statements,[0],[0]
"Table 9 shows the sources of errors in the statement response with their associated frequencies.
",4.5 Analysis of incoherent statements,[0],[0]
Incorrect question word matching was found several times.,4.5 Analysis of incoherent statements,[0],[0]
"For example, the user said “Tokyo ni ryokou ni ittekimashita (I went on a trip to Tokyo)”, generating the reply “Donna Tokyo desu ka?",4.5 Analysis of incoherent statements,[0],[0]
(What kind of Tokyo?)”,4.5 Analysis of incoherent statements,[0],[0]
which does not make sense.,4.5 Analysis of incoherent statements,[0],[0]
Another source of error was the system detecting a focus noun or predicate which did not make sense.,4.5 Analysis of incoherent statements,[0],[0]
Repeated statements were also found.,4.5 Analysis of incoherent statements,[0],[0]
The subject had already explained something during the conversation but the system asked a question on it.,4.5 Analysis of incoherent statements,[0],[0]
This can be addressed by keeping a history of the dialogue.,4.5 Analysis of incoherent statements,[0],[0]
"The ASR word error rate was approximately 10% for both script reading and free talk, so was not a major issue.",4.5 Analysis of incoherent statements,[0],[0]
"In most cases, incorrect ASR results cannot be parsed and so a formulaic expression is produced.",4.5 Analysis of incoherent statements,[0],[0]
Our pilot study showed that our system is feasible with no technical failures.,4.6 Lessons from pilot study,[0],[0]
Backchannels can be generated at appropriate times.,4.6 Lessons from pilot study,[0],[0]
Coherent responses could be generated by the system and errors in Erica’s dialog can be addressed.,4.6 Lessons from pilot study,[0],[0]
"We chose third-party evaluations for this experiment due to the small sample size and also because the subjects could not evaluate specific utterances while they were using the system.
",4.6 Lessons from pilot study,[0],[0]
However we intend to conduct a more comprehensive study where the subjects evaluate their own interaction with Erica.,4.6 Lessons from pilot study,[0],[0]
"Subjects should engage in free talk, but we have found that motivating them to do so is not trivial.",4.6 Lessons from pilot study,[0],[0]
A reasonable metric for a full experiment is the subject’s willingness to continue the interaction with with Erica which indicates engagement with the system.,4.6 Lessons from pilot study,[0],[0]
We can also use more objective metrics such as the number and length of turns taken by the user.,4.6 Lessons from pilot study,[0],[0]
Our strategy of using fillers and backchannels to regulate turn-taking should also be evaluated.,4.6 Lessons from pilot study,[0],[0]
In this paper we described our approach towards creating an attentive listening system which is integrated inside the android Erica.,5 Conclusion and future work,[0],[0]
"The major components are backchannel generation, statement response system, and a turn-taking model.",5 Conclusion and future work,[0],[0]
We presented individual evaluations of each of these components and how they work together to form the attentive listening system.,5 Conclusion and future work,[0],[0]
We also conducted a pilot study to demonstrate the feasibility of the attentive listener.,5 Conclusion and future work,[0],[0]
We intend to conduct a full experiment with the system to discover if it is comparable to human conversational behavior.,5 Conclusion and future work,[0],[0]
"Our aim is for this system to be used in a practical setting, particularly with elderly people.",5 Conclusion and future work,[0],[0]
"This work was supported by JST ERATO Ishiguro Symbiotic Human-Robot Interaction program (Grant Number JPMJER1401), Japan.",Acknowledgements,[0],[0]
"Attentive listening systems are designed to let people, especially senior people, keep talking to maintain communication ability and mental health.",abstractText,[0],[0]
This paper addresses key components of an attentive listening system which encourages users to talk smoothly.,abstractText,[0],[0]
"First, we introduce continuous prediction of end-of-utterances and generation of backchannels, rather than generating backchannels after end-point detection of utterances.",abstractText,[0],[0]
This improves subjective evaluations of backchannels.,abstractText,[0],[0]
"Second, we propose an effective statement response mechanism which detects focus words and responds in the form of a question or partial repeat.",abstractText,[0],[0]
This can be applied to any statement.,abstractText,[0],[0]
"Moreover, a flexible turn-taking mechanism is designed which uses backchannels or fillers when the turnswitch is ambiguous.",abstractText,[0],[0]
These techniques are integrated into a humanoid robot to conduct attentive listening.,abstractText,[0],[0]
We test the feasibility of the system in a pilot experiment and show that it can produce coherent dialogues during conversation.,abstractText,[0],[0]
"Attentive listening system with backchanneling, response generation and flexible turn-taking",title,[0],[0]
Categorical distributions are fundamental to many areas of machine learning.,1. Introduction,[0],[0]
"Examples include classification (Gupta et al., 2014), language models (Bengio et al., 2006), recommendation systems (Marlin & Zemel, 2004), reinforcement learning (Sutton & Barto, 1998), and neural attention models (Bahdanau et al., 2015).",1. Introduction,[0],[0]
"They also play an important role in discrete choice models (McFadden, 1978).
",1. Introduction,[0],[0]
"A categorical is a die with K sides, a discrete random variable that takes on one of K unordered outcomes; a categorical distribution gives the probability of each possible outcome.",1. Introduction,[0],[0]
Categorical variables are challenging to use when there are many possible outcomes.,1. Introduction,[0],[0]
"Such large categoricals appear in common applications such as image classification
1University of Cambridge.",1. Introduction,[0],[0]
2Columbia University.,1. Introduction,[0],[0]
3Athens University of Economics and Business..,1. Introduction,[0],[0]
"Correspondence to: Francisco J. R. Ruiz <f.ruiz@eng.cam.ac.uk, f.ruiz@columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
with many classes, recommendation systems with many items, and language models over large vocabularies.",1. Introduction,[0],[0]
"In this paper, we develop a new method for fitting and using large categorical distributions.
",1. Introduction,[0],[0]
"The most common way to form a categorical is through the softmax transformation, which maps a K-vector of reals to a distribution of K outcomes.",1. Introduction,[0],[0]
Let ψ be a real-valued K-vector.,1. Introduction,[0],[0]
"The softmax transformation is
p(y",1. Introduction,[0],[0]
= k |ψ) = exp {ψk}∑ k′ exp {ψk′} .,1. Introduction,[0],[0]
"(1)
Note the softmax is not the only way to map real vectors to categorical distributions; for example, the multinomial probit (Albert & Chib, 1993) is an alternative.",1. Introduction,[0],[0]
"Also note that in many applications, such as in multiclass classification, the parameter ψk is a function of per-sample features",1. Introduction,[0],[0]
x.,1. Introduction,[0],[0]
"For example, a linear classifier forms a categorical over classes through a linear combination, ψk = w>k",1. Introduction,[0],[0]
"x.
We usually fit a categorical with maximum likelihood estimation or any other closely related strategy.",1. Introduction,[0],[0]
"Given a dataset y1:N of categorical data—each yn is one of K values—we aim to maximize the log likelihood,
Llog likelihood = N∑ n=1 log p(yn |ψ).",1. Introduction,[0],[0]
"(2)
Fitting this objective requires evaluating both the log probability and its gradient.
",1. Introduction,[0],[0]
Eqs.,1. Introduction,[0],[0]
1 and 2 reveal the challenge to using large categoricals.,1. Introduction,[0],[0]
Evaluating the log probability and evaluating its gradient are both O(K) operations.,1. Introduction,[0],[0]
"But this is not OK: most algorithms for fitting categoricals—for example, stochastic gradient ascent—require repeated evaluations of both gradients and probabilities.",1. Introduction,[0],[0]
"When K is large, these algorithms are prohibitively expensive.
",1. Introduction,[0],[0]
"Here we develop a method for fitting large categorical distributions, including the softmax but also more generally.",1. Introduction,[0],[0]
It is called augment and reduce (A&R).,1. Introduction,[0],[0]
"A&R rewrites the categorical distribution with an auxiliary variable ε,
p(y |ψ) = ∫ p(y, ε |ψ)dε.",1. Introduction,[0],[0]
"(3)
A&R then replaces the expensive log probability with a variational bound on the integral in Eq. 3.",1. Introduction,[0],[0]
"Using stochastic
variational methods (Hoffman et al., 2013), the cost to evaluate the bound (or its gradient) is far below O(K).
",1. Introduction,[0],[0]
"Because it relies on variational methods, A&R provides a lower bound on the marginal likelihood of the data.",1. Introduction,[0],[0]
"With this bound, we can embed A&R in a larger algorithm for fitting a categorical, e.g., a (stochastic) variational expectation maximization (VEM) algorithm (Beal, 2003).",1. Introduction,[0],[0]
"Though we focus on maximum likelihood, we can also use A&R in other algorithms that require log p(y |ψ) or its gradient, e.g., fully Bayesian approaches (Gelman et al., 2003) or the REINFORCE algorithm (Williams, 1992).
",1. Introduction,[0],[0]
We study A&R on linear classification tasks with up to 104 classes.,1. Introduction,[0],[0]
"On simulated and real data, we find that it provides accurate estimates of the categorical probabilities and gives better performance than existing approaches.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"There are many methods to reduce the cost of large categorical distributions, particularly under the softmax transformation.",1. Introduction,[0],[0]
"These include methods that approximate the exact computations (Gopal & Yang, 2013; Vijayanarasimhan et al., 2014), those that rely on sampling (Bengio & Sénécal, 2003; Mikolov et al., 2013; Devlin et al., 2014; Ji et al., 2016; Botev et al., 2017), those that use approximations and distributed computing (Grave et al., 2017), double-sum formulations (Raman et al., 2017; Fagan & Iyengar, 2018), and those that avail themselves of other techniques such as noise contrastive estimation (Smith & Jason, 2005; Gutmann & Hyvärinen, 2010) or random nearest neighbor search (Mussmann et al., 2017).
",1. Introduction,[0],[0]
Other methods change the model.,1. Introduction,[0],[0]
"They might replace the softmax transformation with a hierarchical or stickbreaking model (Kurzynski, 1988; Morin & Bengio, 2005; Tsoumakas et al., 2008; Beygelzimer et al., 2009; Dembczyński et al., 2010; Khan et al., 2012).",1. Introduction,[0],[0]
"These approaches can be successful, but the structure of the hierarchy may influence the learned probabilities.",1. Introduction,[0],[0]
"Other methods replace the softmax with a scalable spherical family of losses (Vincent et al., 2015; de Brébisson & Vincent, 2016).
",1. Introduction,[0],[0]
A&R is different from all of these techniques.,1. Introduction,[0],[0]
"Unlike many of them, it provides a lower bound on the log probability rather than an approximation.",1. Introduction,[0],[0]
The bound is useful because it can naturally be embedded in algorithms like stochastic VEM.,1. Introduction,[0],[0]
"Further, the A&R methodology applies to transformations beyond the softmax.",1. Introduction,[0],[0]
"In this paper, we study large categoricals via softmax, multinomial probit, and multinomial logistic.",1. Introduction,[0],[0]
A&R is the first scalable approach for the two latter models.,1. Introduction,[0],[0]
"It accelerates any transformation that can be recast as an additive noise model (e.g., Gumbel, 1954; Albert & Chib, 1993).
",1. Introduction,[0],[0]
"The approach that most closely relates to A&R is the one-vseach (OVE) bound of Titsias (2016), which is a lower bound of the softmax.",1. Introduction,[0],[0]
"Like the other related methods, it is narrower
than A&R in that it does not apply to transformations beyond the softmax.",1. Introduction,[0],[0]
We also empirically compare A&R to OVE in Section 4.,1. Introduction,[0],[0]
A&R provides a tighter lower bound and yields better predictive performance.,1. Introduction,[0],[0]
"We develop augment and reduce (A&R), a method for computing with large categorical random variables.
",2. Augment and Reduce,[0],[0]
The utility perspective.,2. Augment and Reduce,[0],[0]
"A&R uses the additive noise model perspective on the categorical, which we refer to as the utility perspective.",2. Augment and Reduce,[0],[0]
Define a mean utility ψk for each possible outcome k ∈,2. Augment and Reduce,[0],[0]
"{1, . . .",2. Augment and Reduce,[0],[0]
",K}.",2. Augment and Reduce,[0],[0]
"To draw a variable y from a categorical, we draw a zero-mean noise term εk for each possible outcome and then choose the value that maximizes the realized utility ψk + εk.",2. Augment and Reduce,[0],[0]
"This corresponds to the following process,
εk ∼ φ(·), k ∈ {1, . . .",2. Augment and Reduce,[0],[0]
",K}, y = argmax
k",2. Augment and Reduce,[0],[0]
"(ψk + εk) .
",2. Augment and Reduce,[0],[0]
"(4)
Note the errors εk are drawn fresh each time we draw a variable y.",2. Augment and Reduce,[0],[0]
"We assume that the errors are independent of each other, independent of the mean utility ψk, and identically distributed according to some distribution φ(·).
",2. Augment and Reduce,[0],[0]
Now consider the model where we marginalize the errors from Eq. 4.,2. Augment and Reduce,[0],[0]
"This results in a distribution p(y |ψ), a categorical that transforms ψ to the simplex.",2. Augment and Reduce,[0],[0]
"Depending on the distribution of the errors, this induces different transformations.",2. Augment and Reduce,[0],[0]
"For example, a standard Gumbel distribution recovers the softmax transformation; a standard Gaussian recovers the multinomial probit transformation; a standard logistic recovers the multinomial logistic transformation.
",2. Augment and Reduce,[0],[0]
"Typically, the mean utility ψk is a function of observed features x, e.g., ψk = x>wk in linear models or ψk = fwk(x) in non-linear settings.",2. Augment and Reduce,[0],[0]
"In both cases, wk are model parameters, relating the features to mean utilities.
",2. Augment and Reduce,[0],[0]
Let us focus momentarily on a linear classification problem under the softmax model.,2. Augment and Reduce,[0],[0]
"For each observation n, the mean utilities are ψnk = x>nwk and the random errors εnk are Gumbel distributed.",2. Augment and Reduce,[0],[0]
"After marginalizing out the errors, the probability that observation n is in class k is given by Eq. 1, p(yn = k |xn, w) ∝",2. Augment and Reduce,[0],[0]
exp{x>nwk}.,2. Augment and Reduce,[0],[0]
Fitting the classifier involves learning the weights wk that parameterize ψ.,2. Augment and Reduce,[0],[0]
"For example, maximum likelihood uses gradient ascent to maximize ∑ n log p(yn |xn, w) with respect to w.
Large categoricals.",2. Augment and Reduce,[0],[0]
"When the number of outcomes K is large, the normalizing constant of the softmax is a computational burden; it is O(K).",2. Augment and Reduce,[0],[0]
"Consequently, it is burdensome to calculate useful quantities like log p(yn |xn, w) and its gradient ∇w log p(yn |xn, w).",2. Augment and Reduce,[0],[0]
"As an ultimate consequence, maximum likelihood estimation is slow—it needs to evalu-
ate the gradient for each n at each iteration.
",2. Augment and Reduce,[0],[0]
Its difficulty scaling is not unique to the softmax.,2. Augment and Reduce,[0],[0]
Similar issues arise for the multinomial probit and multinomial logistic.,2. Augment and Reduce,[0],[0]
"With these transformations as well, evaluating likelihoods and related quantities is O(K).",2. Augment and Reduce,[0],[0]
We introduce A&R to relieve this burden.,2.1. Augment and reduce,[0],[0]
"A&R accelerates training in models with categorical distributions and a large number of outcomes.
",2.1. Augment and reduce,[0],[0]
"Rather than operating directly on the marginal p(y |ψ), A&R augments the model with one of the error terms and forms a joint p(y, ε |ψ).",2.1. Augment and reduce,[0],[0]
(We drop the subscript n to avoid cluttered notation.),2.1. Augment and reduce,[0],[0]
This augmented model has a desirable property: its log-joint is a sum over all the possible outcomes.,2.1. Augment and reduce,[0],[0]
A&R then reduces—it subsamples a subset of outcomes to construct estimates of the log-joint and its gradient.,2.1. Augment and reduce,[0],[0]
"As a result, its complexity relates to the size of the subsample, not the total number of outcomes K.
The augmented model.",2.1. Augment and reduce,[0],[0]
"Let φ(ε) be the distribution over the error terms, and Φ(ε) = ∫ ε −∞ φ(τ)dτ the corresponding cumulative distribution function (CDF).",2.1. Augment and reduce,[0],[0]
"The marginal probability of outcome k is the probability that its realized utility (ψk + εk) is greater than all others,
p(y",2.1. Augment and reduce,[0],[0]
"= k |ψ) = Pr (ψk + εk ≥ ψk′ + εk′ ∀k′ 6= k) .
",2.1. Augment and reduce,[0],[0]
"We write this probability as an integral over the kth error εk using the CDF of the other errors, p(y = k |ψ) = ∫",2.1. Augment and reduce,[0],[0]
"+∞ −∞ φ(εk) (∏ k′ 6=k ∫ εk+ψk−ψk′ −∞ φ(εk′)dεk′ ) dεk
= ∫",2.1. Augment and reduce,[0],[0]
+∞ −∞ φ(ε) (∏ k′ 6=k Φ(ε+ ψk,2.1. Augment and reduce,[0],[0]
− ψk′) ),2.1. Augment and reduce,[0],[0]
"dε. (5)
(We renamed the dummy variable εk as ε to avoid clutter.)",2.1. Augment and reduce,[0],[0]
"Eq. 5 is the same as found by Girolami & Rogers (2006) for the multinomial probit model, although we do not assume a Gaussian density φ(ε).",2.1. Augment and reduce,[0],[0]
"Rather, we only assume that we can evaluate both φ(ε) and Φ(ε).
",2.1. Augment and reduce,[0],[0]
"We derived Eq. 5 from the utility perspective, which encompasses many common models.",2.1. Augment and reduce,[0],[0]
"We obtain the softmax by choosing a standard Gumbel distribution for φ(ε), in which case Eqs. 1 and 5 are equivalent.",2.1. Augment and reduce,[0],[0]
"We obtain the multinomial probit by choosing a standard Gaussian distribution over the errors, and in this case the integral in Eq.",2.1. Augment and reduce,[0],[0]
5 does not have a closed form.,2.1. Augment and reduce,[0],[0]
"Similarly, we obtain the multinomial logistic by choosing a standard logistic distribution φ(ε).",2.1. Augment and reduce,[0],[0]
"What is important is that regardless of the model, the cost to compute the marginal probability p(y = k |ψ) is O(K).
",2.1. Augment and reduce,[0],[0]
"We now augment the model with the auxiliary latent variable ε to form the joint distribution p(y, ε |ψ),
p(y = k, ε |ψ) = φ(ε) ∏ k′ 6=k Φ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′).,2.1. Augment and reduce,[0],[0]
"(6)
This is a model that includes the kth error term from Eq. 4 but marginalizes out all the other errors.",2.1. Augment and reduce,[0],[0]
"By construction, marginalizing ε from Eq. 6 recovers the original model p(y |ψ) in Eq. 5.",2.1. Augment and reduce,[0],[0]
"Figure 1 illustrates this idea.
",2.1. Augment and reduce,[0],[0]
Riihimäki et al. (2013) used Eq. 6 in the nested expectation propagation for Gaussian process classification.,2.1. Augment and reduce,[0],[0]
"We use it to scale learning with categorical distributions.
",2.1. Augment and reduce,[0],[0]
The variational bound.,2.1. Augment and reduce,[0],[0]
The augmented model in Eq. 6 involves one latent variable ε.,2.1. Augment and reduce,[0],[0]
But our goal is to calculate the marginal log p(y |ψ) and its gradient.,2.1. Augment and reduce,[0],[0]
A&R derives a variational lower bound on log p(y |ψ) using the joint in Eq. 6.,2.1. Augment and reduce,[0],[0]
Define q(ε) to be a variational distribution on the auxiliary variable.,2.1. Augment and reduce,[0],[0]
"The bound is log p(y |ψ) ≥ L, where
L = Eq(ε) [ log p(y = k, ε |ψ)− log q(ε) ]",2.1. Augment and reduce,[0],[0]
"(7)
= Eq(ε) [ log φ(ε) + ∑ k′ 6=k logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
"− ψk′)− log q(ε) ] .
",2.1. Augment and reduce,[0],[0]
"In Eq. 7, L is the evidence lower bound (ELBO); it is tight when q(ε) is equal to the posterior of ε given y, p(ε | y, ψ)",2.1. Augment and reduce,[0],[0]
"(Jordan et al., 1999; Blei et al., 2017).
",2.1. Augment and reduce,[0],[0]
The ELBO contains a summation over the outcomes k′ 6=,2.1. Augment and reduce,[0],[0]
"k. A&R exploits this property to reduce complexity, as we describe below.",2.1. Augment and reduce,[0],[0]
"Next we show how to use the bound in a variational expectation maximization (VEM) procedure and we describe the reduce step of A&R.
Variational expectation maximization.",2.1. Augment and reduce,[0],[0]
"Consider again a linear classification task, where we have a dataset of features xn and labels yn ∈ {1, . . .",2.1. Augment and reduce,[0],[0]
",K} for n = 1, . . .",2.1. Augment and reduce,[0],[0]
", N .",2.1. Augment and reduce,[0],[0]
The mean utility for each observation n is ψnk =,2.1. Augment and reduce,[0],[0]
"w>k xn, and the goal is to learn the weights wk by maximizing the log likelihood ∑ n log p(yn |xn, w).
",2.1. Augment and reduce,[0],[0]
A&R replaces each term in the data log likelihood with its bound using Eq. 7.,2.1. Augment and reduce,[0],[0]
The objective becomes ∑ n L(n).,2.1. Augment and reduce,[0],[0]
Maximizing this objective requires an iterative process with two steps.,2.1. Augment and reduce,[0],[0]
"In one step, A&R optimizes the objective with respect to w.",2.1. Augment and reduce,[0],[0]
"In the other step, A&R optimizes each L(n) with respect to the variational distribution.",2.1. Augment and reduce,[0],[0]
"The resulting procedure takes the form of a VEM algorithm (Beal, 2003).
",2.1. Augment and reduce,[0],[0]
The VEM algorithm requires optimizing the ELBO with respect to w and the variational distributions.1,2.1. Augment and reduce,[0],[0]
This is challenging for two reasons.,2.1. Augment and reduce,[0],[0]
"First, the expectations in Eq. 7 might not be tractable.",2.1. Augment and reduce,[0],[0]
"Second, the cost to compute the gradients of Eq. 7 is still O(K).
",2.1. Augment and reduce,[0],[0]
Section 3 addresses these issues.,2.1. Augment and reduce,[0],[0]
"To sidestep the intractable expectations, A&R forms unbiased Monte Carlo estimates of the gradient of the ELBO.",2.1. Augment and reduce,[0],[0]
"To alleviate the computational complexity, A&R uses stochastic optimization, subsampling a set of outcomes k′.
Reduce by subsampling.",2.1. Augment and reduce,[0],[0]
The subsampling step in the VEM procedure is one of the key ideas behind A&R.,2.1. Augment and reduce,[0],[0]
Since Eq. 7 contains a summation over the outcomes k′,2.1. Augment and reduce,[0],[0]
6=,2.1. Augment and reduce,[0],[0]
"k, we can apply stochastic optimization techniques to obtain unbiased estimates of the ELBO and its gradient.
",2.1. Augment and reduce,[0],[0]
"More specifically, consider the gradient of the ELBO in Eq. 7 with respect to w (the parameters of ψ).",2.1. Augment and reduce,[0],[0]
"It is
∇wL = ∑ k′ 6=k Eq(ε) [ ∇w logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′),2.1. Augment and reduce,[0],[0]
"] .
",2.1. Augment and reduce,[0],[0]
"A&R estimates this by first randomly sampling a subset of outcomes S ⊆ {1, . . .",2.1. Augment and reduce,[0],[0]
",K} {k} of size |S|.",2.1. Augment and reduce,[0],[0]
"A&R then uses the outcomes in S to approximate the gradient,
∇̃wL = K − 1 |S| ∑ k′∈S Eq(ε) [ ∇w logΦ(ε+ ψk",2.1. Augment and reduce,[0],[0]
− ψk′),2.1. Augment and reduce,[0],[0]
"] .
",2.1. Augment and reduce,[0],[0]
"This is an unbiased estimator2 of the gradient ∇wL. Crucially, A&R only needs to iterate over |S| outcomes to obtain it, reducing the complexity to O(|S|).
",2.1. Augment and reduce,[0],[0]
The reduce step is also applicable to optimize the ELBO with respect to q(ε).,2.1. Augment and reduce,[0],[0]
Section 3 gives further details about the stochastic VEM procedure in different settings.,2.1. Augment and reduce,[0],[0]
"Here we provide the details to run the variational expectation maximization (VEM) algorithm for the softmax model (Sec-
1Note that maximizing the ELBO in Eq. 7 with respect to the distribution q(ε) is equivalent to minimizing the Kullback-Leibler divergence from q(ε) to the posterior p(ε | y, ψ).
",3. Algorithm Description,[0],[0]
2This is not the only way to construct an unbiased estimator.,3. Algorithm Description,[0],[0]
"Alternatively, we can draw the outcomes k′ using importance sampling, taking into account the frequency of each class.",3. Algorithm Description,[0],[0]
"We leave this for future work.
tion 3.1) and for more general models including the multinomial probit and multinomial logistic (Section 3.2).",3. Algorithm Description,[0],[0]
"These models only differ in the prior over the errors φ(ε).
",3. Algorithm Description,[0],[0]
Augment and reduce (A&R) is not limited to point-mass estimation of the parameters,3. Algorithm Description,[0],[0]
w.,3. Algorithm Description,[0],[0]
"It is straightforward to extend the algorithm to perform posterior inference on w via stochastic variational inference, but for simplicity we describe maximum likelihood estimation.",3. Algorithm Description,[0],[0]
"In the softmax model, the distribution over the error terms is a standard Gumbel (Gumbel, 1954),
φsoftmax(ε) = exp{−ε− e−ε}, Φsoftmax(ε) = exp{−e−ε}.
",3.1. Augment and Reduce for Softmax,[0],[0]
"In this model, the optimal distribution q?(ε), which achieves equality in the bound, has closed-form expression:
q?softmax(ε) = Gumbel(ε ; log η ?, 1), with η? = 1 + ∑ k′ 6=k e
ψk′−ψk .",3.1. Augment and Reduce for Softmax,[0],[0]
"However, even though q?softmax(ε) has an analytic form, its parameter η
? is computationally expensive to obtain because it involves a summation over K − 1 classes.",3.1. Augment and Reduce for Softmax,[0],[0]
"Instead, we set
qsoftmax(ε ; η) = Gumbel(ε ; log η, 1).
",3.1. Augment and Reduce for Softmax,[0],[0]
"Substituting this choice for qsoftmax(ε ; η) into Eq. 7 gives the following evidence lower bound (ELBO):
Lsoftmax = 1− log(η)− 1
η 1 + ∑ k′ 6=k eψk′−ψk  .",3.1. Augment and Reduce for Softmax,[0],[0]
"(8) Eq. 8 coincides with the log-concavity bound (Bouchard, 2007; Blei & Lafferty, 2007), although we have derived it from a completely different perspective.",3.1. Augment and Reduce for Softmax,[0],[0]
"This derivation allows us to optimize η efficiently, as we describe next.
",3.1. Augment and Reduce for Softmax,[0],[0]
"The Gumbel(ε ; log η, 1) is an exponential family distribution whose natural parameter is η.",3.1. Augment and Reduce for Softmax,[0],[0]
This allows us to use natural gradients in the stochastic inference procedure.,3.1. Augment and Reduce for Softmax,[0],[0]
"A&R iterates between a local step, in which we update η, and a global step, in which we update the parameters ψ.
",3.1. Augment and Reduce for Softmax,[0],[0]
"In the local step (E step), we optimize η by taking a step in the direction of the noisy natural gradient, yielding ηnew = (1 − α)ηold + αη̃.",3.1. Augment and Reduce for Softmax,[0],[0]
"Here, η̃ is an estimate of the optimal natural parameter, which we obtain using a random set of outcomes, i.e., η̃ = 1 + K−1|S| ∑ k′∈S e
ψk′−ψk , where S ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} {k}.",3.1. Augment and Reduce for Softmax,[0],[0]
"The parameter α is the step size; it must satisfy the Robbins-Monro conditions (Robbins & Monro, 1951; Hoffman et al., 2013).
",3.1. Augment and Reduce for Softmax,[0],[0]
"In the global step (M step), we take a gradient step with respect to w (the parameters of ψ), holding η fixed.",3.1. Augment and Reduce for Softmax,[0],[0]
"Similarly, we can estimate the gradient of Eq. 8 with complexity O(|S|) by leveraging stochastic optimization.
",3.1. Augment and Reduce for Softmax,[0],[0]
"Algorithm 1 Softmax A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and natural parameters for iteration t = 1, 2, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
", do # Sample minibatches: Sample a minibatch of data, B ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
", N} for n ∈ B do
Sample a set of labels, Sn ⊆ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} {yn} end for # Local step (E step): for n ∈ B do
Compute η̃n = 1 + K−1|S| ∑ k′∈Sn e ψnk′−ψnyn
Update natural param., ηn ← (1−α(t))ηn+α(t)η̃n end for # Global step (M step):",3.1. Augment and Reduce for Softmax,[0],[0]
"Set g = − N|B| K−1 |S| ∑ n∈B 1 ηn ∑ k′∈Sn∇we ψnk′−ψnyn
Gradient step on the weights, w ← w + ρ(t)g end for
Algorithm 1 summarizes the procedure for a classification task.",3.1. Augment and Reduce for Softmax,[0],[0]
"In this example, the dataset consists of N datapoints (xn, yn), where xn is a feature vector and yn ∈ {1, . . .",3.1. Augment and Reduce for Softmax,[0],[0]
",K} is the class label.",3.1. Augment and Reduce for Softmax,[0],[0]
"Each observation is associated with its parameters ψnk; e.g., ψnk = x>nwk.",3.1. Augment and Reduce for Softmax,[0],[0]
"We posit a softmax likelihood, and we wish to infer the weights via maximum likelihood using A&R.",3.1. Augment and Reduce for Softmax,[0],[0]
"Thus, the objective function is∑ n L (n) softmax.",3.1. Augment and Reduce for Softmax,[0],[0]
(It is straightforward to obtain the maximum a posteriori solution by adding a regularizer.),3.1. Augment and Reduce for Softmax,[0],[0]
"At each iteration, we process a random subset of observations as well as a random subset of classes for each one.
",3.1. Augment and Reduce for Softmax,[0],[0]
"Finally, note that we can perform posterior inference on the parameters w (instead of maximum likelihood) using A&R.",3.1. Augment and Reduce for Softmax,[0],[0]
"One way is to consider a variational distribution q(w) and take gradient steps with respect to the variational parameters of q(w) in the global step, using the reparameterization trick (Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014; Kingma & Welling, 2014) to approximate that gradient.",3.1. Augment and Reduce for Softmax,[0],[0]
"In the local step, we only need to evaluate the moment generating function, estimating the optimal natural parameter as η̃ = 1 + K−1|S| ∑ k′∈S Eq(w)",3.1. Augment and Reduce for Softmax,[0],[0]
[ eψk′−ψk ] .,3.1. Augment and Reduce for Softmax,[0],[0]
"For most models, the expectations of the ELBO in Eq. 7 are intractable, and there is no closed-form solution for the optimal variational distribution q?(ε).",3.2. Augment and Reduce for Other Models,[0],[0]
"Fortunately, we can apply A&R, using the reparameterization trick to build Monte Carlo estimates of the gradient of the ELBO with respect to the variational parameters (Rezende et al., 2014; Titsias & Lázaro-Gredilla, 2014; Kingma & Welling, 2014).
",3.2. Augment and Reduce for Other Models,[0],[0]
"More in detail, consider the variational distribution q(ε ; ν),
Algorithm 2 General A&R for classification Input: data (xn, yn), minibatch sizes |B| and |S| Output: weights w = {wk}Kk=1 Initialize all weights and local variational parameters for iteration t = 1, 2, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
", do # Sample minibatches: Sample a minibatch of data, B ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
", N} for n ∈ B do
Sample a set of labels, Sn ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
",K} {yn} end for # Local step (E step): for n ∈ B do
Sample auxiliary variable un ∼ q(rep)(un) Transform auxiliary variable, εn = T (un ; νn) Estimate the gradient ∇̃νnL(n)",3.2. Augment and Reduce for Other Models,[0],[0]
"(Eq. 9) Update variational param., νn ← νn+α(t)∇̃νnL(n)
end for # Global step (M step):",3.2. Augment and Reduce for Other Models,[0],[0]
"Sample εn ∼ q(εn ; νn) for all n ∈ B Set g= N|B| K−1 |S| ∑ n∈B ∑ k′∈Sn ∇wlogΦ(εn+ψnyn−ψnk′)
",3.2. Augment and Reduce for Other Models,[0],[0]
"Gradient step on the weights, w ← w + ρ(t)g end for
parameterized by some variational parameters ν.",3.2. Augment and Reduce for Other Models,[0],[0]
"We assume that this distribution is reparameterizable, i.e., we can sample from q(ε ; ν) by first sampling an auxiliary variable u ∼ q(rep)(u) and then setting ε = T (u ; ν).
",3.2. Augment and Reduce for Other Models,[0],[0]
"In the local step, we fit q(ε ; ν) by taking a gradient step of the ELBO with respect to the variational parameters ν.",3.2. Augment and Reduce for Other Models,[0],[0]
"Since the expectations in Eq. 7 are not tractable, we obtain Monte Carlo estimates by sampling ε from the variational distribution.",3.2. Augment and Reduce for Other Models,[0],[0]
"To sample ε, we sample u ∼ q(rep)(u) and set ε = T (u ; ν).",3.2. Augment and Reduce for Other Models,[0],[0]
"To alleviate the computational complexity, we apply the reduce step, sampling a random subset S ⊆ {1, . . .",3.2. Augment and Reduce for Other Models,[0],[0]
",K} {k} of outcomes.",3.2. Augment and Reduce for Other Models,[0],[0]
"We thus form a one-sample gradient estimator as
∇̃νL = ∇ε log p̃(y, ε |ψ)∇νT (u ; ν) +∇νH[q(ε ; ν)], (9) where H[q(ε ; ν)] is the entropy of the variational distribution,3 and log p̃(y, ε |ψ) is a log joint estimate,
log p̃(y, ε |ψ) = log φ(ε)+K",3.2. Augment and Reduce for Other Models,[0],[0]
"− 1 |S| ∑ k′∈S logΦ(ε+ψk−ψk′).
",3.2. Augment and Reduce for Other Models,[0],[0]
"In the global step, we estimate the gradient of the ELBO with respect to w. Following a similar approach, we obtain an unbiased one-sample gradient estimator as ∇̃wL = K−1 |S| ∑ k′∈S ∇w logΦ(ε+ ψk",3.2. Augment and Reduce for Other Models,[0],[0]
"− ψk′).
",3.2. Augment and Reduce for Other Models,[0],[0]
"Algorithm 2 summarizes the procedure to efficiently run
3We can estimate the gradient of the entropy when it is not available analytically.",3.2. Augment and Reduce for Other Models,[0],[0]
"Even when it is, the Monte Carlo estimator may have lower variance (Roeder et al., 2017).
maximum likelihood on a classification problem.",3.2. Augment and Reduce for Other Models,[0],[0]
"We subsample observations and classes at each iteration.
",3.2. Augment and Reduce for Other Models,[0],[0]
"Finally, note that we can perform posterior inference on the parameters w by positing a variational distribution q(w) and taking gradient steps with respect to the variational parameters of q(w) in the global step.",3.2. Augment and Reduce for Other Models,[0],[0]
"In this case, the reparameterization trick is needed in both the local and global step to obtain Monte Carlo estimates of the gradient.
",3.2. Augment and Reduce for Other Models,[0],[0]
"We now particularize A&R for the multinomial probit and multinomial logistic models.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R for multinomial probit.,3.2. Augment and Reduce for Other Models,[0],[0]
"Consider a standard Gaussian distribution over the error terms,
φprobit(ε) = 1√ 2π e− 1 2 ε 2 , Φprobit(ε) = ∫ ε",3.2. Augment and Reduce for Other Models,[0],[0]
"−∞ φprobit(τ)dτ.
",3.2. Augment and Reduce for Other Models,[0],[0]
"A&R chooses a Gaussian variational distribution qprobit(ε ; ν) = N (ε ; µ, σ2) and fits the variational parameters ν",3.2. Augment and Reduce for Other Models,[0],[0]
=,3.2. Augment and Reduce for Other Models,[0],[0]
"[µ, σ]>.",3.2. Augment and Reduce for Other Models,[0],[0]
"The Gaussian is reparameterizable in terms of a standard Gaussian, i.e., q(rep)probit(u) = N (u ; 0, 1).",3.2. Augment and Reduce for Other Models,[0],[0]
The transformation is ε = T (u ; ν) = µ + σu.,3.2. Augment and Reduce for Other Models,[0],[0]
"Thus, the gradients in Eq. 9 are ∇νT (u ; ν) =",3.2. Augment and Reduce for Other Models,[0],[0]
"[1, u]> and ∇νH[qprobit(ε ; ν)] = [0, 1/σ]>.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R for multinomial logistic.,3.2. Augment and Reduce for Other Models,[0],[0]
"Consider now a standard logistic distribution over the errors,
φlogistic(ε) = σ(ε)σ(−ε), Φlogistic(ε) = σ(ε),
where σ(ε) = 11+e−ε is the sigmoid function.",3.2. Augment and Reduce for Other Models,[0],[0]
(The logistic distribution has heavier tails than the Gaussian.),3.2. Augment and Reduce for Other Models,[0],[0]
"Under this model, the ELBO in Eq. 7 takes the form
Llogistic=Eq(ε) [ log
σ(ε)σ(−ε) q(ε) + ∑ k′ 6=k log σ(ε+ψk−ψk′) ] .
",3.2. Augment and Reduce for Other Models,[0],[0]
"Note the close resemblance between this expression and the one-vs-each (OVE) bound of Titsias (2016),
LOVE = ∑ k′ 6=k log σ(ψk − ψk′).",3.2. Augment and Reduce for Other Models,[0],[0]
"(10)
However, while the former is a bound on the multinomial logistic model, the OVE is a bound on the softmax.
",3.2. Augment and Reduce for Other Models,[0],[0]
A&R sets qlogistic(ε ; ν) = 1βσ ( ε−µ β ) σ,3.2. Augment and Reduce for Other Models,[0],[0]
"( − ε−µβ ) , a logistic distribution.",3.2. Augment and Reduce for Other Models,[0],[0]
The variational parameters are ν =,3.2. Augment and Reduce for Other Models,[0],[0]
"[µ, β]>.",3.2. Augment and Reduce for Other Models,[0],[0]
"The logistic distribution is reparameterizable, with q(rep)logistic(u) = σ(u)σ(−u) and transformation ε = T (u ; ν) = µ+ βu.",3.2. Augment and Reduce for Other Models,[0],[0]
The gradient of the entropy in Eq. 9 is ∇νH[qlogistic(ε ; ν)] =,3.2. Augment and Reduce for Other Models,[0],[0]
"[0, 1/β]>.",3.2. Augment and Reduce for Other Models,[0],[0]
We showcase augment and reduce (A&R) on a linear classification task.,4. Experiments,[0],[0]
"Our goal is to assess the predictive performance
of A&R in this classification task, to assess the quality of the marginal bound of the data, and to compare its complexity4 with existing approaches.
",4. Experiments,[0],[0]
"We run A&R for three different models of categorical distributions (softmax, multinomial probit, and multinomial logistic).5 For the softmax model, we compare A&R against the one-vs-each (OVE) bound (Titsias, 2016).",4. Experiments,[0],[0]
"Just like A&R, OVE is a rigorous lower bound on the marginal likelihood.",4. Experiments,[0],[0]
"It can also run on a single machine,6 and it has been shown to outperform other approaches.
",4. Experiments,[0],[0]
"For softmax, A&R runs nearly as fast as OVE but has better predictive performance and provides a tighter bound on the marginal likelihood than OVE.",4. Experiments,[0],[0]
"On two small datasets, the A&R bound closely reaches the marginal likelihood of exact softmax maximum likelihood estimation.
",4. Experiments,[0],[0]
We now describe the experimental settings.,4. Experiments,[0],[0]
"In Section 4.1, we analyze synthetic data and K = 104 classes.",4. Experiments,[0],[0]
"In Section 4.2, we analyze five real datasets.
",4. Experiments,[0],[0]
Experimental setup.,4. Experiments,[0],[0]
"We consider linear classification, where the mean utilities are ψnk =",4. Experiments,[0],[0]
w>k xn,4. Experiments,[0],[0]
+ w (0) k .,4. Experiments,[0],[0]
"We fit the model parameters (weights and biases) via maximum likelihood estimation, using stochastic gradient ascent.",4. Experiments,[0],[0]
"We initialize the weights and biases randomly, drawing from a Gaussian distribution with zero mean and standard deviation 0.1 (0.001 for the biases).",4. Experiments,[0],[0]
"For each experiment, we use the same initialization across all methods.
",4. Experiments,[0],[0]
Algorithms 1 and 2 require setting a step size schedule for ρ(t).,4. Experiments,[0],[0]
"We use the adaptive step size sequence proposed by Kucukelbir et al. (2017), which combines RMSPROP (Tieleman & Hinton, 2012) and Adagrad (Duchi et al., 2011).",4. Experiments,[0],[0]
"We set the step size using the default parameters, i.e.,
ρ(t) = ρ0 × t−1/2+10 −16",4. Experiments,[0],[0]
×,4. Experiments,[0],[0]
( 1 + √ s(t) ),4. Experiments,[0],[0]
"−1 ,
s(t)",4. Experiments,[0],[0]
"= 0.1(g(t))2 + 0.9s(t−1).
",4. Experiments,[0],[0]
We set ρ0 = 0.02 and we additionally decrease ρ0 by a factor of 0.9 every 2000 iterations.,4. Experiments,[0],[0]
"We use the same step size sequence for OVE.
",4. Experiments,[0],[0]
"We set the step size α(t) in Algorithm 1 as α(t) = (1+t)−0.9, the default values suggested by Hoffman et al. (2013).",4. Experiments,[0],[0]
"For the step size α(t) in Algorithm 2, we set α(t) = 0.01(1 + t)−0.9.",4. Experiments,[0],[0]
"For the multinomial logit and multinomial probit A&R, we parameterize the variational distributions in terms of their means µ and their unconstrained scale parameter γ, such that the scale parameter is log(1 + exp(γ)).
",4. Experiments,[0],[0]
4We focus on runtime cost.,4. Experiments,[0],[0]
"A&R requires O(N) memory storage capacity due to the local variational parameters.
",4. Experiments,[0],[0]
"5Code for A&R is available at https://github.com/ franrruiz/augment-reduce.
",4. Experiments,[0],[0]
"6A&R is amenable to an embarrassingly parallel algorithm, but we focus on single-core procedures.",4. Experiments,[0],[0]
We mimic the toy experiment of Titsias (2016) to assess how well A&R estimates the categorical probabilities.,4.1. Synthetic Dataset,[0],[0]
"We generate a dataset with 104 classes andN = 3×105 observations, each assigned label k with probability pk ∝ p̃2k, where each p̃k is randomly generated from a uniform distribution in [0, 1].",4.1. Synthetic Dataset,[0],[0]
"After generating the data, we have K = 9,035 effective classes (thus we use this value for K).",4.1. Synthetic Dataset,[0],[0]
"In this simple setting, there are no observed covariates xn.
",4.1. Synthetic Dataset,[0],[0]
We estimate the probabilities pk via maximum likelihood on the biases w(0)k .,4.1. Synthetic Dataset,[0],[0]
"We posit a softmax model, and we apply both the variational expectation maximization (VEM) in Section 3.1 and the OVE bound.",4.1. Synthetic Dataset,[0],[0]
"For both approaches, we choose a minibatch size of |B| = 500 observations and |S| = 100 classes, and we run 5× 105 iterations.
",4.1. Synthetic Dataset,[0],[0]
We run each approach on one CPU core.,4.1. Synthetic Dataset,[0],[0]
"On average, the wall-clock time per epoch (one epoch takes N/|B| = 600 iterations) is 0.196 minutes for softmax A&R and 0.189 minutes for OVE.",4.1. Synthetic Dataset,[0],[0]
"A&R is slightly slower because of the local step that OVE does not require; however, the bound on the marginal log likelihood is tighter (by orders of magnitude) for A&R than for OVE (−2.62×106 and−1.40×109, respectively).",4.1. Synthetic Dataset,[0],[0]
The estimated probabilities are similar for both methods: the average absolute error is 3.00 × 10−6 for A&R and 3.65 × 10−6 for OVE; the difference is not statistically significant.,4.1. Synthetic Dataset,[0],[0]
We now turn to real datasets.,4.2. Real Datasets,[0],[0]
"We consider MNIST and Bibtex (Katakis et al., 2008; Prabhu & Varma, 2014), where we can compare against the exact softmax.",4.2. Real Datasets,[0],[0]
"We also analyze Omniglot (Lake et al., 2015), EURLex-4K (Mencia & Furnkranz, 2008; Bhatia et al., 2015), and AmazonCat-13K (McAuley & Leskovec, 2013).7 Table 1 gives information about the structure of these datasets.
",4.2. Real Datasets,[0],[0]
We run each method for a fixed number of iterations.,4.2. Real Datasets,[0],[0]
We set the minibatch sizes |B| and |S| beforehand.,4.2. Real Datasets,[0],[0]
"The specific values for each dataset are also in Table 1.
Data preprocessing.",4.2. Real Datasets,[0],[0]
"For MNIST, we divide the pixel values by 255 so that the maximum value is one.",4.2. Real Datasets,[0],[0]
"For Omniglot, following other works in the literature (e.g., Burda et al., 2016), we resize the images to 28 × 28 pixels.",4.2. Real Datasets,[0],[0]
"For EURLex-4K and AmazonCat-13K, we normalize the covariates dividing by their maximum value.
",4.2. Real Datasets,[0],[0]
"Bibtex, EURLex-4K, and AmazonCat-13K are multi-class datasets, i.e., each observation may be assigned more than one label.",4.2. Real Datasets,[0],[0]
"Following Titsias (2016), we keep only the first non-zero label for each data point.",4.2. Real Datasets,[0],[0]
"See Table 1 for the resulting number of classes in each case.
",4.2. Real Datasets,[0],[0]
Evaluation.,4.2. Real Datasets,[0],[0]
"For the softmax, we compare A&R against the OVE bound.8",4.2. Real Datasets,[0],[0]
"We also compare against the exact softmax on MNIST and Bibtex, where the number of classes is small.",4.2. Real Datasets,[0],[0]
"For the multinomial probit and multinomial logistic models, we also report the predictive performance of A&R.
We evaluate performance with test log likelihood and accuracy.",4.2. Real Datasets,[0],[0]
"The accuracy is the fraction of correctly classified instances, assuming that we assign the most likely label (i.e., the one with the highest mean utility).",4.2. Real Datasets,[0],[0]
"To compute the test log likelihood, we use Eq. 1 for the softmax and Eq. 5 for the multinomial probit and multinomial logistic models.",4.2. Real Datasets,[0],[0]
"We approximate the integral in Eq. 5 with 1,000 samples using importance sampling (we use a Gaussian distribution with mean 5 and standard deviation 5 as a proposal).
",4.2. Real Datasets,[0],[0]
Results.,4.2. Real Datasets,[0],[0]
Table 2 shows the wall-clock time per epoch for each method and dataset.,4.2. Real Datasets,[0],[0]
"In general, softmax A&R is almost as fast as OVE because the extra local step can be performed efficiently without additional expensive operations.",4.2. Real Datasets,[0],[0]
It requires to evaluate exponential functions that can be reused in the global step.,4.2. Real Datasets,[0],[0]
"Multinomial probit A&R and multinomial
7MNIST is available at http://yann.lecun.com/ exdb/mnist.",4.2. Real Datasets,[0],[0]
Omniglot can be found at https://github.,4.2. Real Datasets,[0],[0]
com/brendenlake/omniglot.,4.2. Real Datasets,[0],[0]
"Bibtex, EURLex-4K, and AmazonCat-13K are available at http://manikvarma.org/ downloads/XC/XMLRepository.html.
",4.2. Real Datasets,[0],[0]
"8We also implemented the approach of Botev et al. (2017), but we do not report the results because it did not outperform OVE in terms of test log-likelihood on four out of the five considered datasets.",4.2. Real Datasets,[0],[0]
"On the fifth dataset, softmax A&R was still superior.
",4.2. Real Datasets,[0],[0]
Table 1.,4.2. Real Datasets,[0],[0]
Statistics and experimental settings of the considered datasets.,4.2. Real Datasets,[0],[0]
Ntrain and Ntest are the number of training and test data points.,4.2. Real Datasets,[0],[0]
The number of classes is the resulting value after the preprocessing step (see text).,4.2. Real Datasets,[0],[0]
"The minibatch sizes correspond to |B| and |S|, respectively.
",4.2. Real Datasets,[0],[0]
"dataset
MNIST Bibtex
Omniglot EURLex-4K
AmazonCat-13K
Ntrain Ntest covariates classes
60, 000 10, 000 784 10 4, 880 2, 413 1, 836 148 25, 968 6, 492 784 1, 623 15, 539 3, 809 5, 000 896 1, 186, 239 306, 782 203, 882 2,",4.2. Real Datasets,[0],[0]
"919
minibatch (obs.) minibatch (classes) iterations
500 1 35, 000 488 20 5, 000 541 50 45, 000 379 50 100, 000 1, 987 60 5, 970
Table 2.",4.2. Real Datasets,[0],[0]
Average time per epoch for each method and dataset.,4.2. Real Datasets,[0],[0]
Softmax A&R (Section 3.1) is almost as fast as OVE.,4.2. Real Datasets,[0],[0]
"The A&R approaches in Section 3.2 take longer because they require some additional computations, but they are still competitive.
",4.2. Real Datasets,[0],[0]
"dataset
MNIST Bibtex
Omniglot EURLex-4K
AmazonCat-13K
OVE (Titsias, 2016)
0.336 s 0.181 s 4.47 s 5.54 s 2.80 h
A&R [this paper] softmax multi. probit multi. logistic
0.337 s 0.431 s 0.511 s 0.188 s 0.244 s 0.246 s 4.65 s 5.63 s 5.57 s 5.65 s 6.46 s 6.23 s 2.80 h 2.82 h 2.91 h
logistic A&R are slightly slower because of the local step, but they are still competitive.
",4.2. Real Datasets,[0],[0]
"For the five datasets, Figure 2 shows the evolution of the evidence lower bound (ELBO) as a function of wall-clock time for the softmax A&R (Eq. 8), compared to the OVE (Eq. 10).",4.2. Real Datasets,[0],[0]
"For easier visualization, we plot a smoothed version of the bounds after applying a moving average window of size 100.",4.2. Real Datasets,[0],[0]
"(For AmazonCat-13K, we only compute the ELBO every 50 iterations and we use a window of size 5.)",4.2. Real Datasets,[0],[0]
"Softmax A&R provides a significantly tighter bound for most datasets (except for Bibtex, where the ELBO of A&R is close to the OVE bound).",4.2. Real Datasets,[0],[0]
"For MNIST and Bibtex, we also plot the marginal likelihood obtained after running maximum likelihood estimation on the exact softmax model.",4.2. Real Datasets,[0],[0]
"The ELBO of A&R nearly achieves this value.
",4.2. Real Datasets,[0],[0]
"Finally, Table 3 shows the predictive performance for all methods across all datasets.",4.2. Real Datasets,[0],[0]
We report test log likelihood and accuracy.,4.2. Real Datasets,[0],[0]
Softmax A&R outperforms OVE in both metrics on all but one dataset (except EURLex-4K).,4.2. Real Datasets,[0],[0]
"Although our goal is not to compare performance across different models, for completeness Table 3 also shows the predictive performance of multinomial probit A&R and multinomial logistic A&R.",4.2. Real Datasets,[0],[0]
"In general, softmax A&R provides the highest
test log likelihood, but multinomial probit A&R outperforms all other methods in EURLex-4K and AmazonCat-13K.",4.2. Real Datasets,[0],[0]
"Additionally, multinomial logistic A&R presents better predictive performance than OVE on Omniglot and Bibtex.",4.2. Real Datasets,[0],[0]
"We have introduced augment and reduce (A&R), a scalable method to fit models involving categorical distributions.",5. Conclusion,[0],[0]
"A&R is general and applicable to many models, including the softmax and the multinomial probit.",5. Conclusion,[0],[0]
"On classification tasks, we found that A&R outperforms state-of-the art algorithms with little extra computational cost.",5. Conclusion,[0],[0]
"This work was supported by ONR N00014-15-1-2209, ONR 133691-5102004, NIH 5100481-5500001084, NSF CCF1740833, the Alfred P. Sloan Foundation, the John Simon Guggenheim Foundation, Facebook, Amazon, and IBM.",Acknowledgements,[0],[0]
"Francisco J. R. Ruiz is supported by the EU Horizon 2020 programme (Marie Skłodowska-Curie Individual Fellowship, grant agreement 706760).",Acknowledgements,[0],[0]
We also thank Victor Elvira and Pablo Moreno for their comments and help.,Acknowledgements,[0],[0]
"Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems.",abstractText,[0],[0]
"However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes.",abstractText,[0],[0]
"To address this problem, we propose augment and reduce (A&R), a method to alleviate the computational complexity.",abstractText,[0],[0]
A&R uses two ideas: latent variable augmentation and stochastic variational inference.,abstractText,[0],[0]
It maximizes a lower bound on the marginal likelihood of the data.,abstractText,[0],[0]
"Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit.",abstractText,[0],[0]
"On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.",abstractText,[0],[0]
Augment and Reduce: Stochastic Inference for Large Categorical Distributions,title,[0],[0]
"The problem of learning mappings between domains from unpaired data has recently received increasing attention, especially in the context of image-to-image translation (Zhu et al., 2017a; Kim et al., 2017; Liu et al., 2017).",1. Introduction,[0],[0]
"This problem is important because, in some cases, paired information may be scarce or otherwise difficult to obtain.",1. Introduction,[0],[0]
"For example, consider tasks like face transfiguration (male to female), where obtaining explicit pairs would be difficult as it would require artistic authoring.",1. Introduction,[0],[0]
"An effective unsupervised model may help when learning from relatively few paired examples, as compared to training strictly from the paired examples.",1. Introduction,[0],[0]
"Intuitively, forcing inter-domain mappings to be (approximately) invertible by a model of limited capacity acts as a strong regularizer.
",1. Introduction,[0],[0]
"Motivated by the success of Generative Adversarial Networks (GANs) in image generation (Goodfellow et al., 2014; Radford et al., 2015), existing unsupervised mapping meth-
1Montreal Institute for Learning Algorithms (MILA), Canada.",1. Introduction,[0],[0]
"2Microsoft Research Montreal, Canada.",1. Introduction,[0],[0]
3CIFAR Fellow.,1. Introduction,[0],[0]
†Work,1. Introduction,[0],[0]
partly done at MSR Montreal.,1. Introduction,[0],[0]
Correspondence to: Amjad Almahairi,1. Introduction,[0],[0]
"<amjad.almahairi@umontreal.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"ods such as CycleGAN (Zhu et al., 2017a) learn a generator which produces images in one domain given images from the other.",1. Introduction,[0],[0]
"Without the use of pairing information, there are many possible mappings that could be inferred.",1. Introduction,[0],[0]
"To reduce the space of the possible mappings, these models are typically trained with a cycle-consistency constraint which enforces a strong connection across domains, by requiring that mapping an image from the source domain to the target domain and then back to source will result in the same starting image.",1. Introduction,[0],[0]
"This framework has been shown to learn convincing mappings across image domains and proved successful in a variety of related applications (Tung et al., 2017; Wolf et al., 2017; Hoffman et al., 2017).
",1. Introduction,[0],[0]
"One major limitation of CycleGAN is that it only learns one-to-one mappings, i.e. the model associates each input image with a single output image.",1. Introduction,[0],[0]
"We believe that most relationships across domains are more complex, and better characterized as many-to-many.",1. Introduction,[0],[0]
"For example, consider mapping silhouettes of shoes to images of shoes.",1. Introduction,[0],[0]
"While the mapping that CycleGAN learns can be superficially convincing (e.g. it produces a single reasonable shoe with a particular style), we would like to learn a mapping that can capture diversity of the output (e.g. produces multiple shoes with different styles).",1. Introduction,[0],[0]
The limits of one-to-one mappings are more dramatic when the source domain and target domain substantially differ.,1. Introduction,[0],[0]
"For instance, it would be difficult to learn a CycleGAN model when the two domains are descriptive facial attributes and images of faces.
",1. Introduction,[0],[0]
We propose a model for learning many-to-many mappings between domains from unpaired data.,1. Introduction,[0],[0]
"Specifically, we “augment” each domain with auxiliary latent variables and extend CycleGAN’s training procedure to the augmented spaces.",1. Introduction,[0],[0]
"The mappings in our model take as input a sample from the source domain and a latent variable, and output both a sample in the target domain and a latent variable (Fig. 1b).",1. Introduction,[0],[0]
"The learned mappings are one-to-one in the augmented space, but many-to-many in the original domains after marginalizing over the latent variables.
",1. Introduction,[0],[0]
Our contributions are as follows.,1. Introduction,[0],[0]
(i) We introduce the Augmented CycleGAN model for learning many-to-many mappings across domains in an unsupervised way.,1. Introduction,[0],[0]
(ii) We show that our model can learn mappings which produce a diverse set of outputs for each input.,1. Introduction,[0],[0]
"(iii) We show that our model can learn mappings across substantially different domains, and we apply it in a semi-supervised setting for mapping between faces and attributes with competitive results.",1. Introduction,[0],[0]
"Given two domains A and B, we assume there exists a mapping, potentially many-to-many, between their elements.",2.1. Problem Setting,[0],[0]
The objective is to recover this mapping using unpaired samples from distributions pd(a) and pd(b) in each domain.,2.1. Problem Setting,[0],[0]
This can be formulated as a conditional generative modeling task where we try to estimate the true conditionals p(a|b) and p(b|a) using samples from the true marginals.,2.1. Problem Setting,[0],[0]
"An important assumption here is that elements in domains A and B are highly dependent; otherwise, it is unlikely that the model would uncover a meaningful relationship without any pairing information.",2.1. Problem Setting,[0],[0]
"The CycleGAN model (Zhu et al., 2017a) estimates these conditionals using two mappings GAB : A 7→ B and GBA : B 7→ A, parameterized by neural networks, which satisfy the following constraints:
1.",2.2. CycleGAN Model,[0],[0]
"Marginal matching: The output of each mapping should match the empirical distribution of the target domain, when marginalized over the source domain.
2.",2.2. CycleGAN Model,[0],[0]
"Cycle-consistency: Mapping an element from one domain to the other, and then back, should produce a sample close to the original element.
",2.2. CycleGAN Model,[0],[0]
"Marginal matching in CycleGAN is achieved using the generative adversarial networks framework (GAN) (Goodfellow et al., 2014).",2.2. CycleGAN Model,[0],[0]
"Mappings GAB and GBA are given by neural networks trained to fool adversarial discriminators DB and
DA, respectively.",2.2. CycleGAN Model,[0],[0]
"Enforcing marginal matching on target domain B, marginalized over source domain A, involves minimizing an adversarial objective with respect to GAB :
LBGAN(GAB , DB) = E b∼pd(b)
[ logDB(b) ]",2.2. CycleGAN Model,[0],[0]
"+
E a∼pd(a)
[ log(1−DB(GAB(a)))",2.2. CycleGAN Model,[0],[0]
"] ,
(1)",2.2. CycleGAN Model,[0],[0]
while the discriminatorDB is trained to maximize it.,2.2. CycleGAN Model,[0],[0]
"A similar adversarial loss LAGAN(GBA, DA) is defined for marginal matching in the reverse direction.
",2.2. CycleGAN Model,[0],[0]
"Cycle-consistency enforces that, when starting from a sample a from A, the reconstruction a′ = GBA(GAB(a)) remains close to the original a. For image domains, closeness between a and a′ is typically measured with L1 or L2 norms.",2.2. CycleGAN Model,[0],[0]
"When using the L1 norm, cycle-consistency starting from A can be formulated as:
LACYC(GAB , GBA) = E a∼pd(a) ∥∥GBA(GAB(a))− a∥∥1.",2.2. CycleGAN Model,[0],[0]
"(2) And similarly for cycle-consistency starting from B. The full CycleGAN objective is given by:
LAGAN(GBA, DA) + LBGAN(GAB , DB) + γLACYC(GAB , GBA) + γLBCYC(GAB , GBA),
(3)
where γ is a hyper-parameter that balances between marginal matching and cycle-consistency.
",2.2. CycleGAN Model,[0],[0]
The success of CycleGAN can be attributed to the complementary roles of marginal matching and cycle-consistency in its objective.,2.2. CycleGAN Model,[0],[0]
Marginal matching encourages generating realistic samples in each domain.,2.2. CycleGAN Model,[0],[0]
Cycle-consistency encourages a tight relationship between domains.,2.2. CycleGAN Model,[0],[0]
"It may also help prevent multiple items from one domain mapping to a single item from the other, analogous to the troublesome mode collapse in adversarial generators (Li et al., 2017).",2.2. CycleGAN Model,[0],[0]
A fundamental weakness of the CycleGAN model is that it learns deterministic mappings.,2.3. Limitations of CycleGAN,[0],[0]
"In CycleGAN, and in other similar models (Kim et al., 2017; Yi et al., 2017), the conditionals between domains correspond to delta functions: p̂(a|b) = δ(GBA(b)) and p̂(b|a) = δ(GAB(a)), and cycleconsistency forces the learned mappings to be inverses of each other.",2.3. Limitations of CycleGAN,[0],[0]
"When faced with complex cross-domain relationships, this results in CycleGAN learning an arbitrary one-toone mapping instead of capturing the true, structured conditional distribution more faithfully.",2.3. Limitations of CycleGAN,[0],[0]
"Deterministic mappings are also an obstacle to optimizing cycle-consistency when the domains differ substantially in complexity, in which case mapping from one domain (e.g. class labels) to the other (e.g. real images) is generally one-to-many.",2.3. Limitations of CycleGAN,[0],[0]
"Next, we dis-
cuss how to extend CycleGAN to capture more expressive relationships across domains.",2.3. Limitations of CycleGAN,[0],[0]
A straightforward approach for extending CycleGAN to model many-to-many relationships is to equip it with stochastic mappings between A and B. Let Z be a latent space with a standard Gaussian prior p(z) over its elements.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
We define mappings GAB : A × Z 7→ B and GBA : B × Z 7→ A1.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"Each mapping takes as input a vector of auxiliary noise and a sample from the source domain, and generates a sample in the target domain.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"Therefore, by sampling different z ∼ p(z), we could in principle generate multiple b’s conditioned on the same a and vice-versa.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"We can write the marginal matching loss on domain B as:
LBGAN(GAB , DB) = E b∼pd(b)
[ logDB(b) ]",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"+
E a∼pd(a) z∼p(z)
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"[ log(1−DB(GAB(a, z))) ] .
(4) Cycle-consistency starting from A is now given by:
LACYC(GAB , GBA) = E a∼pd(a)
z1,z2∼p(z) ∥∥GBA(GAB(a, z1), z2)− a∥∥1 (5)
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
The full training loss is similar to the objective in Eqn. 3.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"We refer to this model as Stochastic CycleGAN.
",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In principle, stochastic mappings can model multi-modal conditionals, and hence generate a richer set of outputs than deterministic mappings.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"However, Stochastic CycleGAN suffers from a fundamental flaw: the cycle-consistency in Eq. 5 encourages the mappings to ignore the latent z. Specifically, the unimodality assumption implicit in the reconstruction error from Eq. 5 forces the mapping GBA to be manyto-one when cycling A→ B → A′, since any b generated for a given a must map to a′ = GBA(b, z)",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"≈ a, for all z.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"For the cycle B → A → B′, GAB is similarly forced to be many-to-one.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
The only way for to GBA and GAB to be both many-to-one and mutual inverses is if they collapse to being (roughly) one-to-one.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
We could possibly mitigate this degeneracy by introducing a VAE-like encoder and exchanging the L1 error in Eq. 5 for a more complex variational bound on conditional log-likelihood.,2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In the next section, we discuss an alternative approach to learning complex, stochastic mappings between domains.",2.4. CycleGAN with Stochastic Mappings,[0],[0]
"In order to learn many-to-many mappings across domains, we propose to learn to map between pairs of items (a, zb) ∈
1To avoid clutter in notation, we reuse the same symbols of deterministic mappings.
",3. Approach,[0],[0]
"A× Zb and (b, za) ∈ B × Za, where Za and Zb are latent spaces that capture any missing information when transforming an element from A to B, and vice-versa.",3. Approach,[0],[0]
"For example, when generating a female face (b ∈ B) which resembles a male face (a ∈ A), the latent code zb ∈",3. Approach,[0],[0]
"Zb can capture female face variations (e.g. hair length or style) independent from a. Similarly, za ∈ Za captures variations in a generated male face independent from the given female face.",3. Approach,[0],[0]
"This approach can be described as learning mappings between augmented spaces A× Zb and B × Za (Figure 1b); hence, we call it Augmented CycleGAN.",3. Approach,[0],[0]
"By learning to map a pair (a, zb) ∈",3. Approach,[0],[0]
"A × Zb to (b, za) ∈ B × Za, we can (i) learn a stochastic mapping from a to multiple items in B by sampling different zb ∈",3. Approach,[0],[0]
"Zb, and (ii) infer latent codes za containing information about a not captured in the generated b, which allows for doing proper reconstruction of a. As a result, we are able to optimize both marginal matching and cycle consistency while using stochastic mappings.",3. Approach,[0],[0]
We present details of our approach in the next sections.,3. Approach,[0],[0]
2,3. Approach,[0],[0]
Our proposed model has four components.,3.1. Augmented CycleGAN,[0],[0]
"First, the two mappings GAB : A × Zb 7→ B and GBA : B × Za 7→",3.1. Augmented CycleGAN,[0],[0]
"A, which are the conditional generators of items in each domain.",3.1. Augmented CycleGAN,[0],[0]
These models are similar to those used in Stochastic CycleGAN.,3.1. Augmented CycleGAN,[0],[0]
We also have two encoders EA :,3.1. Augmented CycleGAN,[0],[0]
A×B,3.1. Augmented CycleGAN,[0],[0]
"7→ Za and EB : A × B 7→ Zb, which enable optimization of cycle-consistency with stochastic, structured mappings.",3.1. Augmented CycleGAN,[0],[0]
All components are parameterized with neural networks – see Fig. 2.,3.1. Augmented CycleGAN,[0],[0]
We define mappings over augmented spaces in our model as follows.,3.1. Augmented CycleGAN,[0],[0]
"Let p(za) and p(zb) be standard Gaussian priors over Za and Zb, which are independent from pd(b) and pd(a).",3.1. Augmented CycleGAN,[0],[0]
"Given a pair (a, zb) ∼ pd(a)p(zb), we generate a pair (b̃, z̃a) as follows:
b̃ = GAB(a, zb), z̃a = EA(a, b̃).",3.1. Augmented CycleGAN,[0],[0]
"(6)
That is, we first generate a sample in domain B, then we use it along with a to generate latent code z̃a.",3.1. Augmented CycleGAN,[0],[0]
"Note here that by sampling different zb ∼ p(zb), we can generate multiple b̃’s conditioned on the same a. In addition, given the pair (a, b̃), we can recover information about a which is not captured in b̃, via z̃a.",3.1. Augmented CycleGAN,[0],[0]
"Similarly, given a pair (b, za) ∼ pd(b)p(za), we generate a pair (ã, z̃b) as follows:
ã = GBA(b, za), z̃b = EB(b, ã).",3.1. Augmented CycleGAN,[0],[0]
"(7)
Learning in Augmented CycleGAN follows a similar approach to CycleGAN – optimizing both marginal matching and cycle-consistency losses, albeit over augmented spaces.
",3.1. Augmented CycleGAN,[0],[0]
"2Our model captures many-to-many relationships because it captures both one-to-many and many-to-one: one item in A maps to many items in B, and many items in B map to one item in A (cycle).",3.1. Augmented CycleGAN,[0],[0]
"The same is true in the other direction.
",3.1. Augmented CycleGAN,[0],[0]
"Marginal Matching Loss We adopt an adversarial approach for marginal matching over B × Za where we use two independent discriminators DB and DZa to match generated pairs to real samples from the independent priors pd(b) and p(za), respectively.",3.1. Augmented CycleGAN,[0],[0]
Marginal matching loss over B is defined as in Eqn 4.,3.1. Augmented CycleGAN,[0],[0]
"Marginal matching over Za is given by:
LZaGAN(EA, GAB , DZa)",3.1. Augmented CycleGAN,[0],[0]
"= E za∼p(za)
",3.1. Augmented CycleGAN,[0],[0]
[ logDZa(za) ],3.1. Augmented CycleGAN,[0],[0]
"+
E a∼pd(a) zb∼p(zb)
",3.1. Augmented CycleGAN,[0],[0]
"[ log(1−DZa(z̃a)) ] ,
(8)
where z̃a is defined by Eqn 6.",3.1. Augmented CycleGAN,[0],[0]
"As in CycleGAN, the goal of marginal matching over B is to insure that generated samples b̃ are realistic.",3.1. Augmented CycleGAN,[0],[0]
"For latent codes z̃a, marginal matching acts as a regularizer for the encoder, encouraging the marginalized encoding distribution to match a simple prior p(za).",3.1. Augmented CycleGAN,[0],[0]
"This is similar to adversarial regularization of latent codes in adversarial autoencoders (Makhzani et al., 2016).",3.1. Augmented CycleGAN,[0],[0]
"We define similar losses LAGAN(GBA, DA) and LZbGAN(EB , GBA, DZb) for marginal matching over A×Zb.
",3.1. Augmented CycleGAN,[0],[0]
"Cycle Consistency Loss We define two cycle-consistency constraints in Augmented CycleGAN starting from each of the two augmented spaces, as shown in Fig. 2.",3.1. Augmented CycleGAN,[0],[0]
"In cycleconsistency starting from A × Zb, we ensure that given a pair (a, zb) ∼ pd(a)p(zb), the model is able to produce a faithful reconstruction of it after being mapped to (b̃, z̃a).",3.1. Augmented CycleGAN,[0],[0]
"This is achieved with two losses; first for reconstructing a ∼ pd(a):
LACYC(GAB , GBA, EA) = E a∼pd(a) zb∼p(zb)
",3.1. Augmented CycleGAN,[0],[0]
"∥∥a′ − a∥∥ 1 ,
b̃ = GAB(a, zb), z̃a = EA(a, b̃), a ′ = GBA(b̃, z̃a).",3.1. Augmented CycleGAN,[0],[0]
"(9)
The second is for reconstructing zb ∼ p(zb):
LZbCYC(GAB , EB) = E a∼pd(a) zb∼p(zb)",3.1. Augmented CycleGAN,[0],[0]
∥∥z′b,3.1. Augmented CycleGAN,[0],[0]
"− zb∥∥1, z′b = EB(a, b̃), b̃ = GAB(a, zb).",3.1. Augmented CycleGAN,[0],[0]
"(10)
These reconstruction costs represent an autoregressive decomposition of the basic CycleGAN cycle-consistency cost from Eq. 2, after extending it to the augmented domains.",3.1. Augmented CycleGAN,[0],[0]
"Specifically, we decompose the required reconstruction distribution p(b, za|a, zb) into the conditionals p(b|a, zb) and p(za|a, zb, b).
",3.1. Augmented CycleGAN,[0],[0]
"Just like in CycleGAN, the cycle loss in Eqn. 9 enforces the dependency of generated samples in B on samples of A. Thanks to the encoder EA, the model is able to reconstruct a because it can recover information loss in generated b̃ through z̃a.",3.1. Augmented CycleGAN,[0],[0]
"On the other hand, the cycle loss in Eqn. 10 enforces the dependency of a generated sample b̃ on the given latent code zb.",3.1. Augmented CycleGAN,[0],[0]
"In effect, it increases the mutual information between zb and b conditioned on a, i.e. I(b, zb|a) (Chen et al., 2016; Li et al., 2017).
",3.1. Augmented CycleGAN,[0],[0]
"Training Augmented CycleGAN in the direction A× Zb to B × Za is done by optimizing:
LBGAN(DB , GAB)",3.1. Augmented CycleGAN,[0],[0]
+,3.1. Augmented CycleGAN,[0],[0]
"L za GAN(DZa , EA, GAB) + γ1LACYC(GAB , GBA, EA) + γ2L zb CYC(GAB , EB), (11)
where γ1 and γ2 are a hyper-parameters used to balance objectives.",3.1. Augmented CycleGAN,[0],[0]
"We define a similar objective for the direction going from B × Za to A× Zb, and train the model on both objectives simultaneously.",3.1. Augmented CycleGAN,[0],[0]
"In cases where we have access to paired data, we can leverage it to train our model in a semi-supervised setting (Fig. 3).",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"Given pairs sampled from the true joint,
i.e. (a, b) ∼ pd(a, b), we can define a supervision cost for the mapping GAB as follows:
LASUP(GBA, EA) = E (a,b)∼pd(a,b) ∥∥GBA(b, z̃a)− a∥∥1, (12)
where z̃a = EA(a, b) infers a latent code which can produce a given b via GBA(b, z̃a).",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"We also apply an adversarial regularization cost on the encoder, in the form of Eqn. 8.",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
"Similar supervision and regularization costs can be defined for GBA and EB , respectively.",3.2. Semi-supervised Learning with Augmented CycleGAN,[0],[0]
We note here some design choices that we found important for training our stochastic mappings.,3.3. Modeling Stochastic Mappings,[0],[0]
We discuss architectural and training details further in Sec. 5.,3.3. Modeling Stochastic Mappings,[0],[0]
"In order to allow the latent codes to capture diversity in generated samples, we found it important to inject latent codes to layers of the network which are closer to the inputs.",3.3. Modeling Stochastic Mappings,[0],[0]
"This allows the injected codes to be processed with a larger number of remaining layers and therefore capture high-level variations of the output, as opposed to small pixel-level variations.",3.3. Modeling Stochastic Mappings,[0],[0]
"We also found that Conditional Normalization (CN) (Dumoulin et al.; Perez et al., 2017) for conditioning layers can be more effective than concatenation, which is more commonly used (Radford et al., 2015; Zhu et al., 2017b).",3.3. Modeling Stochastic Mappings,[0],[0]
"The basic idea of CN is to replace parameters of affine transformations in normalization layers (Ioffe & Szegedy, 2015) of a neural network with a learned function of the conditioning information.",3.3. Modeling Stochastic Mappings,[0],[0]
"We apply CN by learning two linear functions f and g which take a latent code z as input and output scale and shift parameters of normalization layers in intermediate layers, i.e. γ = f(z) and β = g(z).",3.3. Modeling Stochastic Mappings,[0],[0]
"When activations are normalized over spatial dimensions only, we get Conditional Instance Normalization (CIN), and when they are also normalized over batch dimension, we get Conditional Batch Normalization (CBN).",3.3. Modeling Stochastic Mappings,[0],[0]
"There has been a surge of interest recently in unsupervised learning of cross-domain mappings, especially for image translation tasks.",4. Related Work,[0],[0]
Previous attempts for image-to-image translation have unanimously relied on GANs to learn mappings that produce compelling images.,4. Related Work,[0],[0]
"In order to constrain learned mappings, some methods have relied on cycleconsistency based constraints similar to CycleGAN (Kim et al., 2017; Yi et al., 2017; Royer et al., 2017), while others relied on weight sharing constraints (Liu & Tuzel, 2016; Liu et al., 2017).",4. Related Work,[0],[0]
"However, the focus in all of these methods was on learning conditional image generators that produce single output images given the input image.",4. Related Work,[0],[0]
"Notably, Liu et al. (2015) propose to map inputs from both domains into a
shared latent space.",4. Related Work,[0],[0]
"This approach may constrain too much the space of learnable mappings, for example in cases where the domains differ substantially (class labels and images).
",4. Related Work,[0],[0]
"Unsupervised learning of mappings have also been addressed recently in language translation, especially for machine translation (Lample et al., 2017) and text style transfer (Shen et al., 2017).",4. Related Work,[0],[0]
These methods also rely on some notion of cycle-consistency over domains in order to constrain the learned mappings.,4. Related Work,[0],[0]
They rely heavily on the power of the RNN-based decoders to capture complex relationships across domains while we propose to use auxiliary latent variables.,4. Related Work,[0],[0]
"The two approaches may be synergistic, as it was recently suggested in (Gulrajani et al., 2016).
",4. Related Work,[0],[0]
"Recently, Zhu et al. (2017b) proposed the BiCycleGAN model for learning multi-modal mappings but in fully supervised setting.",4. Related Work,[0],[0]
"This model extends the pix2pix framework in (Isola et al., 2017) by learning a stochastic mapping from the source to the target, and shows interesting diversity in the generated samples.",4. Related Work,[0],[0]
"Several modeling choices in BiCycleGAN resemble our proposed model, including the use of stochastic mappings and an encoder to handle multi-modal targets.",4. Related Work,[0],[0]
"However, our approach focuses on unsupervised many-to-many mappings, which allows it to handle domains with no or very little paired data.",4. Related Work,[0],[0]
"We first study a one-to-many image translation task between edges (domain A) and photos of shoes (domain B).3 Training data is composed of almost 50K shoe images with corresponding edges (Yu & Grauman, 2014; Zhu et al., 2016; Isola et al., 2017), but as in previous approaches (e.g. (Kim et al., 2017)), we assume no pairing information while training unsupervised models.",5.1. Edges-to-Photos,[0],[0]
"Stochastic mappings in our Augmented CycleGAN (AugCGAN) model are based on ResNet conditional image generators of (Zhu et al., 2017a), where we inject noise with CIN to all intermediate layers.",5.1. Edges-to-Photos,[0],[0]
"As baselines, we train: CycleGAN, Stochastic CycleGAN (StochCGAN) and Triangle-GAN (∆-GAN) of (Gan et al., 2017) which share the same architectures and training procedure for fair comparison.",5.1. Edges-to-Photos,[0],[0]
"4
Quantitative Results First, we evaluate conditionals learned by each model by measuring the ability of the model of generating a specific edge-shoe pair from a test set.",5.1. Edges-to-Photos,[0],[0]
"We follow the same evaluation methodology adopted in (Metz et al., 2016; Xiang & Li, 2017), which opt for an
3 Public code available at: https://github.com/ aalmah/augmented_cyclegan
4∆-GAN architecture differs only in the two discriminators, which match conditionals/joints instead of marginals.
inference-via-optimization approach to estimate the reconstruction error of a specific shoe given an edge.",5.1. Edges-to-Photos,[0],[0]
"Specifically, given a trained model with mapping GAB and an edgeshoe pair (a, b) in the test set, we solve the optimization task z∗b = arg minzb ‖GAB(a, zb)−b‖1 and compute reconstruction error ‖GAB(a, z∗b )",5.1. Edges-to-Photos,[0],[0]
− b‖1.,5.1. Edges-to-Photos,[0],[0]
"Optimization is done with RMSProp as in (Xiang & Li, 2017).",5.1. Edges-to-Photos,[0],[0]
"We show the average errors over a predefined test set of 200 samples in Table 1 for: AugCGAN (unsupervised and semi-supervised with 10% paired data), unsupervised CycleGAN and StochCGAN, and a semi-supervised ∆-GAN, all sharing the same architecture.",5.1. Edges-to-Photos,[0],[0]
"Our unsupervised AugCGAN model outperforms all baselines including semi-supervised ∆-GAN, which indicates that reconstruction-based cycle-consistency is more effective in learning conditionals than the adversarial approach of ∆-GAN.",5.1. Edges-to-Photos,[0],[0]
"As expected, adding 10% supervision to AugCGAN improves shoe predictions further.",5.1. Edges-to-Photos,[0],[0]
"In addition, we evaluate edge predictions given real shoes from test set as well.",5.1. Edges-to-Photos,[0],[0]
"We report mean squared error (MSE) similar to (Gan et al., 2017), where we normalize over all edge pixels.",5.1. Edges-to-Photos,[0],[0]
"The ∆-GAN model with our architecture outperforms
the one reported in (Gan et al., 2017), but is outperformed by our unsupervised AugCGAN model.",5.1. Edges-to-Photos,[0],[0]
"Again, adding 10% supervision to AugCGAN reduces MSE even further.
",5.1. Edges-to-Photos,[0],[0]
Qualitative Results We qualitatively compare the mappings learned by our model AugCGAN and StochCGAN.,5.1. Edges-to-Photos,[0],[0]
"Fig. 6 shows generated images of shoes given an edge a ∼ pd(a) (row) and zb ∼ p(zb) (column) from both model, and",5.1. Edges-to-Photos,[0],[0]
Fig. 5 shows cycles starting from edges and shoes.,5.1. Edges-to-Photos,[0],[0]
Note that here the edges are sampled from the data distribution and not produced by the learnt stochastic mapping GBA.,5.1. Edges-to-Photos,[0],[0]
"In this case, both models can (i) generate diverse set of shoes with color variations mostly defined by zb, and (ii) perform reconstructions of both edges and shoes.
",5.1. Edges-to-Photos,[0],[0]
"While we expect our model to achieve these results, the fact that StochCGAN can reconstruct shoes perfectly without an inference model may seem at first surprising.",5.1. Edges-to-Photos,[0],[0]
"However, this can be explained by the “steganography” behavior of CycleGAN (Chu et al., 2017):",5.1. Edges-to-Photos,[0],[0]
"the model hides in the generated edge ã imperceptible information about a given shoe b (e.g. its color), in order to satisfy cycle-consistency without being
penalized by the discriminator on A. A good model of the true conditionals p(b|a), p(a|b) should reproduce the hidden joint distribution and consequently the marginals by alternatively sampling from conditionals.",5.1. Edges-to-Photos,[0],[0]
"Therefore, we examine the behavior of the models when edges are generated from the model itself (instead of the empirical data distribution).",5.1. Edges-to-Photos,[0],[0]
"In Fig. 7, we plot multiple generated shoes given an edge generated by the model, i.e. ã, and 5 different zb sampled from p(zb).",5.1. Edges-to-Photos,[0],[0]
"In StochCGAN, the mapping GBA(ã, zb) collapses to a deterministic function generating a single shoe for every zb.",5.1. Edges-to-Photos,[0],[0]
"This distinction between behaviour on real and synthetic data is undesirable, e.g. regularization benefits of using unpaired data may be reduced if the model slips into this regime switching style.",5.1. Edges-to-Photos,[0],[0]
"In AugCGAN, on the other hand, the mapping seem to closely capture the diversity in the conditional distribution of shoes given edges.",5.1. Edges-to-Photos,[0],[0]
"Furthermore, in Fig. 8, we run a Markov chain by generating from the learned mappings multiple times, starting from a real shoe.",5.1. Edges-to-Photos,[0],[0]
"Again AugCGAN produces diverse samples while StochCGAN seems to collapse to a single mode.
",5.1. Edges-to-Photos,[0],[0]
"We investigate “steganography” behavior in both AugCGAN and StochCGAN using a similar approach to (Chu et al., 2017), where we corrupt generated edges with noise sampled fromN (0, 2), and compute reconstruction error of shoes.",5.1. Edges-to-Photos,[0],[0]
Fig. 4 shows L1 reconstruction error as we increase .,5.1. Edges-to-Photos,[0],[0]
"AugCGAN seems more robust to corruption of edges than in StochCGAN, which confirms that information is being stored in the latent codes instead of being completely hidden in generated edges.",5.1. Edges-to-Photos,[0],[0]
We study another image translation task of translating between male and female faces.,5.2. Male-to-Female,[0],[0]
"Data is based on CelebA dataset (Liu et al., 2015) where we split it into two separate domains using provided attributes.",5.2. Male-to-Female,[0],[0]
"Several key features distinguish this task from other image-translation tasks: (i) there is no predefined correspondence in real data of each domain, (ii) the relationship is many-to-many between domains, as we can map a male to female face, and vice-versa, in many possible ways, and (iii) capturing realistic variations in generated faces requires transformations that go beyond simple color and texture changes.",5.2. Male-to-Female,[0],[0]
"The architecture of stochastic mappings are based on U-NET conditional image generators of (Isola et al., 2017), and again with noise injected to all intermediate layers.",5.2. Male-to-Female,[0],[0]
Fig. 9 shows results of applying our model to this task on 128 × 128 resolution CelebA images.,5.2. Male-to-Female,[0],[0]
We can see that our model depicts meaningful variations in generated faces without compromising their realistic appearance.,5.2. Male-to-Female,[0],[0]
"In Fig. 10 we show 64 × 64 generated samples in both domains from our model ((a) and (b)), and compare them to both: (c) our model but with noise injected noise only in last 3 layers of the GAB’s
network, and (d) StochCGAN with the same architecture.",5.2. Male-to-Female,[0],[0]
"We can see that in Fig. 10-(c) variations are very limited, which highlights the importance of processing latent code with multiple layers.",5.2. Male-to-Female,[0],[0]
"StochCGAN in this task produces almost no variations at all, which highlights the importance of proper optimization of cycle-consistency for capturing meaningful variations.",5.2. Male-to-Female,[0],[0]
"We verify these results quantitatively using LPIPS distance (Zhang et al., 2018), where we average distance between 1000 pairs of generated female faces (10 random pairs from 100 male faces).",5.2. Male-to-Female,[0],[0]
"AugCGAN (Fig. 10-(b)) achieves highest LPIPS diversity score with 0.108 ± 0.003, while AugCGAN with z in low-level layers (Fig. 10-(c)) gets 0.059 +/- 0.001, and finally StochCGAN (Fig. 10-(d)) gets 0.008 +/- 0.000, i.e. severe mode collapse.",5.2. Male-to-Female,[0],[0]
"In this task, we make use of the CelebA dataset in order map from descriptive facial attributes A to images of faces B and vice-versa.",5.3. Attributes-to-Faces,[0],[0]
We report both quantitative and qualitative results.,5.3. Attributes-to-Faces,[0],[0]
"For the quantitative results, we follow (Gan et al.,
2017) and test our models in a semi-supervised attribute prediction setting.",5.3. Attributes-to-Faces,[0],[0]
We let the model train on all the available data without the pairing information and only train with a small amount of paired data as described in Sec. 3.2.,5.3. Attributes-to-Faces,[0],[0]
We report Precision (P) and normalized Discounted Cumulative Gain (nDCG) as the two metrics for multi-label classification problems.,5.3. Attributes-to-Faces,[0],[0]
"As an additional baseline, we also train a supervised classifier (which has the same architecture as GBA) on the paired subset.",5.3. Attributes-to-Faces,[0],[0]
The results are reported in Table 3.,5.3. Attributes-to-Faces,[0],[0]
"In Fig. 11, we show some generation obtained from the model in the direction attributes to faces.",5.3. Attributes-to-Faces,[0],[0]
We can see that the model generates reasonable diverse faces for the same set of attributes.,5.3. Attributes-to-Faces,[0],[0]
In this paper we have introduced the Augmented CycleGAN model for learning many-to-many cross-domain mappings in unsupervised fashion.,6. Conclusion,[0],[0]
This model can learn stochastic mappings which leverage auxiliary noise to capture multimodal conditionals.,6. Conclusion,[0],[0]
Our experimental results verify quantitatively and qualitatively the effectiveness of our approach in image translation tasks.,6. Conclusion,[0],[0]
"Furthermore, we apply our model in a challenging task of learning to map across attributes and faces, and show that it can be used effectively in a semi-supervised learning setting.",6. Conclusion,[0],[0]
Authors would like to thank Zihang Dai for valuable discussions and feedback.,Acknowledgements,[0],[0]
We are also grateful for ICML anonymous reviewers for their comments.,Acknowledgements,[0],[0]
"Learning inter-domain mappings from unpaired data can improve performance in structured prediction tasks, such as image segmentation, by reducing the need for paired data.",abstractText,[0],[0]
"CycleGAN was recently proposed for this problem, but critically assumes the underlying inter-domain mapping is approximately deterministic and one-to-one.",abstractText,[0],[0]
"This assumption renders the model ineffective for tasks requiring flexible, many-to-many mappings.",abstractText,[0],[0]
"We propose a new model, called Augmented CycleGAN, which learns many-to-many mappings between domains.",abstractText,[0],[0]
We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.,abstractText,[0],[0]
Augmented CycleGAN: Learning Many-to-Many Mappings  from Unpaired Data,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1057–1068 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Social power is a difficult concept to define, but is often manifested in how we interact with one another.",1 Introduction,[0],[0]
"Understanding these manifestations is important not only to answer fundamental questions in social sciences about power and social interactions, but also to build computational models that can automatically infer social power structures from interactions.",1 Introduction,[0],[0]
"The availability and access to large digital repositories of naturally occurring social interactions and the advancements in natural language processing techniques in recent years have enabled researchers to perform large scale studies on linguistic correlates of power, such as words and phrases (Bramsen et al., 2011; Gilbert, 2012), linguistic coordination (DanescuNiculescu-Mizil et al., 2012), agenda control (Tay-
lor et al., 2012), and dialog structure (Prabhakaran and Rambow, 2014).
",1 Introduction,[0],[0]
Another area of research that has recently garnered interest within the NLP community is the modeling of author commitment in text.,1 Introduction,[0],[0]
"Initial studies in this area were done in processing hedges, uncertainty and lack of commitment, specifically focused on scientific text (Mercer et al., 2004; Di Marco et al., 2006; Farkas et al., 2010).",1 Introduction,[0],[0]
"More recently, researchers have also looked into capturing author commitment in nonscientific text, e.g., levels of factuality in newswire (Saurı́ and Pustejovsky, 2009), types of commitment of beliefs in a variety of genres including conversational text (Diab et al., 2009; Prabhakaran et al., 2015).",1 Introduction,[0],[0]
"These approaches are motivated from an information extraction perspective, for instance in aiding tasks such as knowledge base",1 Introduction,[0],[0]
"population.1 However, it has not been studied whether such sophisticated author commitment analysis can go beyond what is expressed in language and reveal the underlying social contexts in which language is exchanged.
",1 Introduction,[0],[0]
"In this paper, we bring together these two lines of research; we study how power relations correlate with the levels of commitment authors express in interactions.",1 Introduction,[0],[0]
"We use the power analysis framework built by Prabhakaran and Rambow (2014) to perform this study, and measure author commitment using the committed belief tagging framework introduced by (Diab et al., 2009) that distinguishes different types of beliefs expressed in text.",1 Introduction,[0],[0]
"Our contributions are two-fold — statistical analysis of author commitment in relation with power, and enrichment of lexical features with commitment labels to aid in computational prediction of power relations.",1 Introduction,[0],[0]
"In the first part, we find that au-
1The BeSt track of the 2017 TAC-KBP evaluation aimed at detecting the “belief and sentiment of an entity toward another entity, relation, or event” (http://www.cs. columbia.edu/˜rambow/best-eval-2017/).
1057
thor commitment is significantly correlated with the social power relations between their participants — subordinates use more instances of noncommitment, a finding that is in line with sociolinguistics studies in this area.",1 Introduction,[0],[0]
"We also find that subordinates use significantly more reported beliefs (i.e., attributing beliefs to other agents) than superiors.",1 Introduction,[0],[0]
"This is a new finding; to our knowledge, there has not been any sociolinguistics studies investigating this aspect of interaction in relation with power.",1 Introduction,[0],[0]
"In the second part, we present novel ways of incorporating the author commitment information into lexical features that can capture important distinctions in word meanings conveyed through the belief contexts in which they occur; distinctions that are lost in a model that conflates all occurrences of a word into one unit.
",1 Introduction,[0],[0]
We first describe the related work in computational power analysis and computational modeling of cognitive states in Section 2.,1 Introduction,[0],[0]
"In Section 3, we describe the power analysis framework we use.",1 Introduction,[0],[0]
"Section 4 formally defines the research questions we are investigating, and describes how we obtain the belief information.",1 Introduction,[0],[0]
"In Section 5, we present the statistical analysis of author commitment and power.",1 Introduction,[0],[0]
Section 6 presents the utility of enriching lexical features with belief labels in the context of automatic power prediction.,1 Introduction,[0],[0]
Section 7 concludes the paper and summarizes the results.,1 Introduction,[0],[0]
"The notion of belief that we use in this paper (Diab et al., 2009; Prabhakaran et al., 2015) is closely related to the notion of factuality that is captured in FactBank (Saurı́ and Pustejovsky, 2009).",2 Related Work,[0],[0]
"They capture three levels of factuality, certain (CT), probable (PB), and possible (PS), as well as the underspecified factuality (Uu).",2 Related Work,[0],[0]
"They also record the corresponding polarity values, and the source of the factuality assertions to distinguish between factuality assertions by the author and those by the agents/sources introduced by the author.",2 Related Work,[0],[0]
"While FactBank offers a finer granularity, they are annotated on newswire text.",2 Related Work,[0],[0]
"Hence, we use the corpus of belief annotations (Prabhakaran et al., 2015) that is obtained on online discussion forums, which is closer to our genre.
",2 Related Work,[0],[0]
Automatic hedge/uncertainty detection is a very closely related task to belief detection.,2 Related Work,[0],[0]
"The belief tagging framework we use aims to capture the cognitive states of authors, whereas hedges are lin-
guistic expressions that convey one of those cognitive states — non-committed beliefs.",2 Related Work,[0],[0]
Automatic hedge/uncertainty detection has generated active research in recent years within the NLP community.,2 Related Work,[0],[0]
"Early work in this area focused on detecting speculative language in scientific text (Mercer et al., 2004; Di Marco et al., 2006; Kilicoglu and Bergler, 2008).",2 Related Work,[0],[0]
"The open evaluation as part of the CoNLL shared task in 2010 to detect uncertainty and hedging in biomedical and Wikipedia text (Farkas et al., 2010) triggered further research on this problem in the general domain (Agarwal and Yu, 2010; Morante et al., 2010; Velldal et al., 2012; Choi et al., 2012).",2 Related Work,[0],[0]
Most of this work was aimed at formal scientific text in English.,2 Related Work,[0],[0]
"More recent work has tried to extend this work to other genres (Wei et al., 2013; Sanchez and Vogel, 2015) and languages (Velupillai, 2012; Vincze, 2014), as well as building general purpose hedge lexicons (Prokofieva and Hirschberg, 2014).",2 Related Work,[0],[0]
"In our work, we use the lexicons from (Prokofieva and Hirschberg, 2014) to capture hedges in text.
",2 Related Work,[0],[0]
"Sociolinguists have long studied the association between level of commitment and social contexts (Lakoff, 1973; O’Barr and Atkins, 1980; Hyland, 1998).",2 Related Work,[0],[0]
"A majority of this work studies gender differences in the use of hedges, triggered by the influential work by Robin Lakoff (Lakoff, 1973).",2 Related Work,[0],[0]
"She argued that women use linguistic strategies such as hedging and hesitations in order to adopt an unassertive communication style, which she terms “women’s language”.",2 Related Work,[0],[0]
"While many studies have found evidence to support Lakoff’s theory (e.g., (Crosby and Nyquist, 1977; Preisler, 1986; Carli, 1990)), there have also been contradictory findings (e.g., (O’Barr and Atkins, 1980))",2 Related Work,[0],[0]
"that link the difference in the use of hedges to other social factors (e.g., power).",2 Related Work,[0],[0]
"O’Barr and Atkins (1980) argue that the use of hedges is linked more to the social positions rather than gender, suggesting to rename “women’s language” to “powerless language”.",2 Related Work,[0],[0]
"In later work, O’Barr (1982) formalized the notion of powerless language, which formed the basis of many sociolinguistics studies on social power and communication.",2 Related Work,[0],[0]
O’Barr (1982) analyzed courtroom interactions and identified hedges and hesitations as some of the linguistic markers of “powerless” speech.,2 Related Work,[0],[0]
"However, there has not been any computational work which has looked into how power relations relate to the level of commitment expressed in text.",2 Related Work,[0],[0]
"In this paper, we use com-
putational power analysis to perform a large scale data-oriented study on how author commitment in text reveals the underlying power relations.
",2 Related Work,[0],[0]
"There is a large body of literature in the social sciences that studies power as a social construct (e.g., (French and Raven, 1959; Dahl, 1957; Emerson, 1962; Pfeffer, 1981; Wartenberg, 1990)) and how it relates to the ways people use language in social situations (e.g., (Bales et al., 1951; Bales, 1970; O’Barr, 1982; Van Dijk, 1989; Bourdieu and Thompson, 1991; Ng and Bradac, 1993; Fairclough, 2001; Locher, 2004)).",2 Related Work,[0],[0]
Recent years have seen growing interest in computationally analyzing and detecting power and influence from interactions.,2 Related Work,[0],[0]
"Early work in computational power analysis used social network analysis based approaches (Diesner and Carley, 2005; Shetty and Adibi, 2005; Creamer et al., 2009) or email traffic patterns (Namata et al., 2007).",2 Related Work,[0],[0]
"Using NLP to deduce social relations from online communication is a relatively new area of active research.
",2 Related Work,[0],[0]
"Bramsen et al. (2011) and Gilbert (2012) first applied NLP based techniques to predict power relations in Enron emails, approaching this task as a text classification problem using bag of words or ngram features.",2 Related Work,[0],[0]
"More recently, our work has used dialog structure features derived from deeper dialog act analysis for the task of power prediction in Enron emails (Prabhakaran and Rambow, 2014; Prabhakaran et al., 2012; Prabhakaran and Rambow, 2013).",2 Related Work,[0],[0]
"In this paper, We use the framework of (Prabhakaran and Rambow, 2014), but we analyze a novel aspect of interaction that has not been studied before — what level of commitment do the authors express in language.
",2 Related Work,[0],[0]
There has also been work on analyzing power in other genres of interactions.,2 Related Work,[0],[0]
Strzalkowski et al. (2010) and Taylor et al. (2012) concentrate on lower-level constructs called Language Uses such as agenda control to predict power in Wikipedia talk pages.,2 Related Work,[0],[0]
Danescu-Niculescu-Mizil et al. (2012) study how social power and linguistic coordination are correlated in Wikipedia interactions as well as Supreme Court hearings.,2 Related Work,[0],[0]
Bracewell et al. (2012) and Swayamdipta and Rambow (2012) try to identify pursuit of power in discussion forums.,2 Related Work,[0],[0]
"Biran et al. (2012) and Rosenthal (2014) study the problem of predicting influence in Wikipedia talk pages, blogs, and other online forums.",2 Related Work,[0],[0]
Prabhakaran et al. (2013) study manifestations of power of confidence in presidential debates.,2 Related Work,[0],[0]
"The focus of our study is to investigate whether the level of commitment participants express in their contributions in an interaction is related to the power relations they have with other participants, and how it can help in the problem of predicting social power.",3 Power in Workplace Email: Data and Analysis Framework,[0],[0]
"In this section, we introduce the power analysis framework as well as the data we use in this study.",3 Power in Workplace Email: Data and Analysis Framework,[0],[0]
"In order to model manifestations of power relations in interactions, we use our interaction analysis framework from (Prabhakaran and Rambow, 2014), where we introduced the problem of predicting organizational power relations between pairs of participants based on single email threads.",3.1 Problem,[0],[0]
"The problem is formally defined as follows: given an email thread t , and a related interacting participant pair (p1 , p2 ) in the thread, predict whether p1 is the superior or subordinate of p2 .",3.1 Problem,[0],[0]
"In this formulation, a related interacting participant pair (RIPP) is a pair of participants of the thread such that there is at least one message exchanged within the thread between them (in either direction) and that they are hierarchically related with a superior/subordinate relation.",3.1 Problem,[0],[0]
"We use the same dataset we used in (Prabhakaran and Rambow, 2014), which is a version of the Enron email corpus in which the thread structure of email messages is reconstructed (Yeh and Harnly, 2006), and enriched by Agarwal et al. (2012) with gold organizational power relations, manually determined using information from Enron organizational charts.",3.2 Data,[0],[0]
"The corpus captures dominance relations between 13,724 pairs of Enron employees.",3.2 Data,[0],[0]
"As in (Prabhakaran and Rambow, 2014), we use these dominance relation tuples to obtain gold labels for the superior or subordinate relationships between pairs of participants.",3.2 Data,[0],[0]
"We use the same train-test-dev split as in (Prabhakaran and Rambow, 2014).",3.2 Data,[0],[0]
We summarize the number of threads and related interacting participant pairs in each subset of the data in Table 1.,3.2 Data,[0],[0]
"Our first objective in this paper is to perform a large scale computational analysis of author com-
mitment and power relations.",4 Research Hypotheses,[0],[0]
"Specifically, we want to investigate whether the commitment authors express towards their contributions in organizational interactions is correlated with the power relations they have with other participants.",4 Research Hypotheses,[0],[0]
"Sociolinguistics studies have found some evidence to suggest that lack of commitment expressed through hedges and hesitations is associated with lower power status (O’Barr, 1982).",4 Research Hypotheses,[0],[0]
"However, in our study, we go beyond hedge word lists, and analyze different cognitive belief states expressed by authors using a belief tagging framework that takes into account the syntactic contexts within which propositions are expressed.",4 Research Hypotheses,[0],[0]
"We use the committed belief analysis framework introduced by (Diab et al., 2009; Prabhakaran et al., 2015) to model different levels of beliefs expressed in text.",4.1 Obtaining Belief Labels,[0],[0]
"Specifically, in this paper, we use the 4-way belief distinction — COMMITTEDBELIEF, NONCOMMITTEDBELIEF, REPORTEDBELIEF, and NONAPPLICABLE— introduced in (Prabhakaran et al., 2015).2 (Prabhakaran et al., 2015) presented a corpus of online discussion forums with over 850K words, annotating each propositional head in text with one of the four belief labels.",4.1 Obtaining Belief Labels,[0],[0]
"The paper also presented an automatic belief tagger trained on this data, which we use to obtain belief labels in our data.",4.1 Obtaining Belief Labels,[0],[0]
"We describe each belief label and our associated hypotheses below.
",4.1 Obtaining Belief Labels,[0],[0]
"Committed belief (CB): the writer strongly believes that the proposition is true, and wants the reader/hearer to believe that.",4.1 Obtaining Belief Labels,[0],[0]
"E.g.:
(1) a. John will submit the report.",4.1 Obtaining Belief Labels,[0],[0]
b.,4.1 Obtaining Belief Labels,[0],[0]
"I know that John is capable.
",4.1 Obtaining Belief Labels,[0],[0]
"2We also performed analysis and experiments using an earlier 3-way belief distinction proposed by (Diab et al., 2009), which also yielded similar findings.",4.1 Obtaining Belief Labels,[0],[0]
"We do not report the details of those analyses in this paper.
",4.1 Obtaining Belief Labels,[0],[0]
"As discussed earlier, lack of commitment in one’s writing/speech is identified as markers of powerless language.",4.1 Obtaining Belief Labels,[0],[0]
"We thus hypothesize:
H. 1.",4.1 Obtaining Belief Labels,[0],[0]
"Superiors use more instances of committed belief in their messages than subordinates.
",4.1 Obtaining Belief Labels,[0],[0]
"Non-committed belief (NCB): the writer explicitly identifies the proposition as something which he or she could believe, but he or she happens not to have a strong belief in, for example by using an epistemic modal auxiliary.",4.1 Obtaining Belief Labels,[0],[0]
"E.g.:
(2) a. John may submit the report.",4.1 Obtaining Belief Labels,[0],[0]
b.,4.1 Obtaining Belief Labels,[0],[0]
"I guess John is capable.
",4.1 Obtaining Belief Labels,[0],[0]
"This class captures a more semantic notion of non-commitment than hedges, since the belief annotation attempts to model the underlying meaning rather than language uses, and hence captures other linguistic means of expressing noncommittedness.",4.1 Obtaining Belief Labels,[0],[0]
"Following (O’Barr, 1982), we formulate the below hypothesis:
H. 2.",4.1 Obtaining Belief Labels,[0],[0]
"Subordinates use more instances of non committed belief in their messages than superiors.
",4.1 Obtaining Belief Labels,[0],[0]
Reported belief (ROB): the writer attributes belief (either committed or non-committed) to another person or group.,4.1 Obtaining Belief Labels,[0],[0]
"E.g.:
(3) a. Sara says John will submit the report.",4.1 Obtaining Belief Labels,[0],[0]
"b. Sara thinks John may be capable.
",4.1 Obtaining Belief Labels,[0],[0]
Note that this label is only applied when the writer’s own belief in the proposition is unclear.,4.1 Obtaining Belief Labels,[0],[0]
"For instance, if the first example above was Sara knows John will submit the report on-time, the writer is expressing commitment toward the proposition that John will submit the report and it will be labeled as committed belief rather than reported belief.",4.1 Obtaining Belief Labels,[0],[0]
Reported belief captures instances where the writer is in effect limiting his/her commitment towards what is stated by attributing the belief to someone else.,4.1 Obtaining Belief Labels,[0],[0]
"So, in line with our hypotheses for non-committed beliefs, we formulate the following hypothesis:
H. 3.",4.1 Obtaining Belief Labels,[0],[0]
"Subordinates use more instances of reported beliefs in their messages than superiors.
",4.1 Obtaining Belief Labels,[0],[0]
"Non-belief propositions (NA): – the writer expresses some other cognitive attitude toward the proposition, such as desire or intention (4a), or expressly states that he/she has no belief about the proposition (e.g., asking a question (4b)).",4.1 Obtaining Belief Labels,[0],[0]
"E.g.:
(4) a. I need John to submit the report.",4.1 Obtaining Belief Labels,[0],[0]
"b. Will John be capable?
",4.1 Obtaining Belief Labels,[0],[0]
"As per the above definition, requests for information (i.e., questions) and requests for actions are cases where the author is not expressing a belief about the proposition, but rather expressing the desire that some action be done.",4.1 Obtaining Belief Labels,[0],[0]
"In the study correlating power with dialog act tags (Prabhakaran and Rambow, 2014), we found that superiors issue significantly more requests than subordinates.",4.1 Obtaining Belief Labels,[0],[0]
"Hence, we expect the superiors to have significantly more non belief expressions in their messages, and formulate the following hypothesis:
H. 4.",4.1 Obtaining Belief Labels,[0],[0]
Superiors use more instances of non beliefs in their messages than subordinates.,4.1 Obtaining Belief Labels,[0],[0]
"NLP tools are imperfect and may produce errors, which poses a problem when using any NLP tool for sociolinguistic analysis.",4.2 Testing Belief Tagger Bias,[0],[0]
"More than the magnitude of error, we believe that whether the error is correlated with the social variable of interest (i.e., power) is more important; e.g., is the belieftagger more likely to find ROB false-positives in subordinates text?",4.2 Testing Belief Tagger Bias,[0],[0]
"To test whether this is the case, we performed manual belief annotation on around 500 propositional heads in our corpus.",4.2 Testing Belief Tagger Bias,[0],[0]
"Logistic regression test revealed that the belief-tagger is equally likely to make errors (both false-positives and false-negatives, for all four belief-labels) in sentences written by subordinates as superiors (the null hypothesis accepted at p > 0.05 for all eight tests).",4.2 Testing Belief Tagger Bias,[0],[0]
"Now that we have set up the analysis framework and research hypotheses, we present the statistical analysis of how superiors and subordinates differ in their relative use of expressions of commitment.",5 Statistical Analysis,[0],[0]
"For each participant of each pair of related interacting participants in our corpus, we aggregate each of the four belief tags:
• CBCount: number of propositional heads tagged as Committed Belief (CB) • NCBCount: number of propositional heads tagged as Non Committed Belief (NCB) •",5.1 Features,[0],[0]
"ROBCount: number of propositional heads tagged as Reported Belief (ROB)
•",5.1 Features,[0],[0]
NACount: number of propositional heads tagged as Non Belief (NA),5.1 Features,[0],[0]
"Our general hypothesis is that power relations do correlate with the level of commitment people express in their messages; i.e., at least one of H.1 - H.4 is true.",5.2 Hypotheses Testing,[0],[0]
"In this analysis, each participant of the pair (p1 , p2 ) is a data instance.",5.2 Hypotheses Testing,[0],[0]
"We exclude the instances for which a feature value is undefined.3
In order to test whether superiors and subordinates use different types of beliefs, we used a linear regression based analysis.",5.2 Hypotheses Testing,[0],[0]
"For each feature, we built a linear regression model predicting the feature value using power (i.e., superior vs. subordinate) as the independent variable.",5.2 Hypotheses Testing,[0],[0]
"Since verbosity of a participant can be highly correlated with each of these feature values (we found it to be highly correlated with subordinates (Prabhakaran and Rambow, 2014)), we added token count as a control variable to the linear regression.
",5.2 Hypotheses Testing,[0],[0]
"Our linear regression test revealed significant differences in NCB (b=-.095, t(-8.09), p<.001), ROB (b=-.083, t(-7.162), p<.001) and NA (b=.125, t(4.351), p<.001), and no significant difference in CB (b=.007, t(0.227), p=0.821).",5.2 Hypotheses Testing,[0],[0]
"Figure 1 pictorially demonstrates these results by plotting the difference between the mean values of each commitment feature (here normalized by token count) of superiors vs. subordinates, as a percentage of mean feature value of the corresponding commitment feature for superiors.",5.2 Hypotheses Testing,[0],[0]
Dark bars denote statistically significant differences.,5.2 Hypotheses Testing,[0],[0]
The results from our statistical analysis validate our original hypothesis that power relations do correlate with the level of commitment people express in their messages.,5.3 Interpretation of Findings,[0],[0]
"This finding remains statistically significant (p < 0.001) even after applying the Bonferroni correction for multiple testing.
",5.3 Interpretation of Findings,[0],[0]
The results on NCB confirm our hypothesis that subordinates use more non-committedness in their language.,5.3 Interpretation of Findings,[0],[0]
"Subordinates’ messages contain 48% more instances of non-committed belief than superiors’ messages, even after normalizing for the length of messages.",5.3 Interpretation of Findings,[0],[0]
"This is in line with prior sociolinguistics literature suggesting that people with
3These are instances corresponding to participants who did not send any messages in the thread (some of the pairs in the set of related interacting participant pairs only had oneway communication) or whose messages were empty (e.g., forwarding messages).
0.05.",5.3 Interpretation of Findings,[0],[0]
"(RD = (Mean(Subordinates)−Mean(Superiors))∗100Mean(Superiors) ).
less power tend to use less commitment, previously measured in terms of hedges.",5.3 Interpretation of Findings,[0],[0]
"However, in our work, we go beyond hedge dictionaries and use expressions of non-committedness that takes into account the syntactic configurations in which the words appear.
",5.3 Interpretation of Findings,[0],[0]
Another important finding is in terms of reported belief (ROB).,5.3 Interpretation of Findings,[0],[0]
Our results strongly verify the hypothesis H.3 that subordinates use significantly more reported beliefs than superiors.,5.3 Interpretation of Findings,[0],[0]
"In fact, it obtained the largest magnitude of relative difference (65.3% more) of all features we analyzed.",5.3 Interpretation of Findings,[0],[0]
"To our knowledge, ours is the first study that analyzed the manifestation of power in authors attributing beliefs to others.",5.3 Interpretation of Findings,[0],[0]
"Our results are in line with the finding in (Agarwal et al., 2014) that “if many more people get mentioned to a person then that person is the boss”, because as subordinates report other people’s beliefs to superiors, they are also likely to mention them.
",5.3 Interpretation of Findings,[0],[0]
The finding that superiors use more NAs confirms our hypothesis H.4.,5.3 Interpretation of Findings,[0],[0]
"As discussed earlier, this is expected since superiors issue more requests (as found by (Prabhakaran and Rambow, 2014)), the propositional heads of which would be tagged as NA by the belief tagger.",5.3 Interpretation of Findings,[0],[0]
"However, our hypothesis H.1 is proven false.",5.3 Interpretation of Findings,[0],[0]
"Being a superior or subordinate does not affect how often their messages contain CB, which suggests that power differences are manifested only in terms of lack of commitment.",5.3 Interpretation of Findings,[0],[0]
Our next step is to explore whether we can utilize the hedge and belief labels to improve the performance of an automatic power prediction system.,6 Commitment in Power Prediction,[0],[0]
"For this purpose, we use our POWERPRE-
DICTOR system (Prabhakaran and Rambow, 2014) that predicts the direction of power between a pair of related interacting participants in an email thread.",6 Commitment in Power Prediction,[0],[0]
"It uses a variety of linguistic and dialog structural features consisting of verbosity features (message count, message ratio, token count, token ratio, and tokens per message), positional features (initiator, first message position, last message position), thread structure features (number of all recipients and those in the To and CC fields of the email, reply rate, binary features denoting the adding and removing of other participants), dialog act features (request for action, request for information, providing information, and conventional), and overt displays of power, and lexical features (lemma ngrams, part-of-speech ngrams, and mixed ngrams, a version of lemma ngrams with open class words replaced with their part-ofspeech tags).",6 Commitment in Power Prediction,[0],[0]
"The feature sets are summarized in Table 2 ((Prabhakaran and Rambow, 2014) has a detailed description of these features).
",6 Commitment in Power Prediction,[0],[0]
"None of the features used in POWERPREDICTOR use information from the parse trees of sentences in the text However, in order to accurately obtain the belief labels, deep dependency parse based features are critical (Prabhakaran et al., 2010).",6 Commitment in Power Prediction,[0],[0]
We use the ClearTk wrapper for the Stanford CoreNLP pipeline to obtain the dependency parses of sentences in the email text.,6 Commitment in Power Prediction,[0],[0]
"To ensure an unified analysis framework, we also use the Stanford CoreNLP for tokenization, part-ofspeech tagging, and lemmatization steps, instead of OpenNLP.",6 Commitment in Power Prediction,[0],[0]
This change affects our analysis in two ways.,6 Commitment in Power Prediction,[0],[0]
"First, the source of part-of-speech tags and word lemmas is different from what was presented in the original system, which might affect the performance of the dialog act tagger and overt display of power tagger (DIA and ODP features).",6 Commitment in Power Prediction,[0],[0]
"Second, we had to exclude 117 threads (0.3%) from the corpus for which the Stanford CoreNLP failed to parse some sentences, resulting in the removal of 11 data points (0.2%), only one of which
was in the test set.",6 Commitment in Power Prediction,[0],[0]
"On randomly checking, we found that they contained non-parsable text such as dumps of large tables, system logs, or unedited dumps of large legal documents.
",6 Commitment in Power Prediction,[0],[0]
"In order to better interpret how the commitment features help in power prediction, we use a linear kernel SVM in our experiments.",6 Commitment in Power Prediction,[0],[0]
"Linear kernel SVMs are significantly faster than higher order SVMs, and our preliminary experiments revealed the performance gain by using a higher order SVM to be only marginal.",6 Commitment in Power Prediction,[0],[0]
"We use the best performing feature set from (Prabhakaran and Rambow, 2014) as a strong baseline for our experiments.",6 Commitment in Power Prediction,[0],[0]
This baseline feature set is the combination of thread structure features (THR) and lexical features (LEX).,6 Commitment in Power Prediction,[0],[0]
This baseline system obtained an accuracy of 68.8% in the development set.,6 Commitment in Power Prediction,[0],[0]
"Adding the belief label counts into the SVM directly as features will not yield much performance improvements, as signal in the aggregate counts would be minimal given the effect sizes of differences we find in Section 5.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"In this section, we investigate a more sophisticated way of incorporating the belief tags into the power prediction framework.",6.1 Belief Label Enriched Lexical Features,[0],[0]
Lexical features are very useful for the task of power prediction.,6.1 Belief Label Enriched Lexical Features,[0],[0]
"However, it is often hard to capture deeper syntactic/semantic contexts of words and phrases using ngram features.",6.1 Belief Label Enriched Lexical Features,[0],[0]
We hypothesize that incorporating belief tags into the ngrams will enrich the representation and will help disambiguate different usages of same words/phrases.,6.1 Belief Label Enriched Lexical Features,[0],[0]
"For example, let us consider two sentences: I need the report by tomorrow vs.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"If I need the report, I will let you know.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"The former is likely coming from a person who has power, whereas the latter does not give any such indication.",6.1 Belief Label Enriched Lexical Features,[0],[0]
Applying the belief tagger to these two sentences will result in I need(CB) the report ... and If I need(NA) the report ....,6.1 Belief Label Enriched Lexical Features,[0],[0]
"Capturing the difference between need(CB) vs. need(NA) will help the machine learning system to make the distinction between these two usages and in turn improve the power prediction performance.
",6.1 Belief Label Enriched Lexical Features,[0],[0]
"In building the ngram features, whenever we encounter a token that is assigned a belief tag, we append the belief tag to the corresponding lemma or part-of-speech tag in the ngram.",6.1 Belief Label Enriched Lexical Features,[0],[0]
We call it the Append version of corresponding ngram feature.,6.1 Belief Label Enriched Lexical Features,[0],[0]
"We summarize the different versions of each type of
ngram features below: • LN: the original word lemma ngram; e.g.,
i need the.",6.1 Belief Label Enriched Lexical Features,[0],[0]
• LNCBApnd:,6.1 Belief Label Enriched Lexical Features,[0],[0]
"word lemma ngram with appended
belief tags; e.g., i need(CB) the.",6.1 Belief Label Enriched Lexical Features,[0],[0]
•,6.1 Belief Label Enriched Lexical Features,[0],[0]
"PN: the original part-of-speech ngram; e.g.,
PRP VB DT. • PNCBApnd: part-of-speech ngram with ap-
pended belief tags; e.g., PRP VB(CB) DT. • MN: the original mixed ngram; e.g., i VB the.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"• MNCBApnd: mixed ngram with appended belief
tags; e.g., i VB(CB) the.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"In Table 3, we show the results obtained by incorporating the belief tags in this manner to the LEXICAL features of the original baseline feature set.",6.1 Belief Label Enriched Lexical Features,[0],[0]
The first row indicates the baseline results and the following rows show the impact of incorporating belief tags using the Append method.,6.1 Belief Label Enriched Lexical Features,[0],[0]
"While the Append version of both lemma ngrams and mixed ngrams improved the results, the Append version of part of speech ngrams reduced the results.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"The combination of best performing version of each type of ngram obtained slightly lower result than using the Append version of word ngram alone, which posted the overall best performance of 69.3%, a significant improvement (p<0.05) over not using any belief information.",6.1 Belief Label Enriched Lexical Features,[0],[0]
"We use the approximate randomization test (Yeh, 2000) for testing statistical significance of the improvement.
",6.1 Belief Label Enriched Lexical Features,[0],[0]
"Finally, we verified that our best performing feature sets obtain similar improvements in the unseen test set.",6.1 Belief Label Enriched Lexical Features,[0],[0]
The baseline system obtained 70.2% accuracy in the test set.,6.1 Belief Label Enriched Lexical Features,[0],[0]
The best performing configuration from Table 3 significantly improved this accuracy to 70.8%.,6.1 Belief Label Enriched Lexical Features,[0],[0]
The second best performing configuration of using the Append version of both word and mixed ngrams obtained only a small improvement upon the baseline in the test set.,6.1 Belief Label Enriched Lexical Features,[0],[0]
We inspect the feature weights assigned to the LNCBApnd version of lemma ngrams in our best performing model.,6.2 Word NGram Feature Analysis,[0],[0]
"Each lemma ngram that contains a propositional head (e.g., need) has four possible LNCBApnd ngram versions: need(CB), need(NCB), need(ROB), and need(NA).",6.2 Word NGram Feature Analysis,[0],[0]
"For each lemma ngram, we calculate the standard deviation of weights assigned to different LNCBApnd versions in the learned model as a measure of variation captured by incorporating belief tags into that ngram.4
Figure 2 shows the feature weights of different LNCBApnd versions of twenty five propositional heads whose lemma unigrams had the highest standard deviation.",6.2 Word NGram Feature Analysis,[0],[0]
"The y-axis lists propositional heads arranged in the decreasing order of standard deviation from bottom to top, while the x-axis denotes the feature weights.",6.2 Word NGram Feature Analysis,[0],[0]
"The markers distinguish the different LNCBApnd versions of each propositional head — square denotes COMMITTEDBE-
4Not all lemma ngrams have all four versions; we calculated standard deviation using the versions present.
",6.2 Word NGram Feature Analysis,[0],[0]
"LIEF, circle denotes NONCOMMITTEDBELIEF, triangle denotes REPORTEDBELIEF, and diamond denotes NONAPPLICABLE.",6.2 Word NGram Feature Analysis,[0],[0]
"The feature versions with negative weights are associated more with subordinates’ messages, whereas those with positive weights are associated more with superiors’ messages.",6.2 Word NGram Feature Analysis,[0],[0]
"Since NCB and ROB versions are rare, they rarely get high weights in the model.
",6.2 Word NGram Feature Analysis,[0],[0]
"We find that by incorporating belief labels into lexical features, we capture important distinctions in social meanings expressed through words that are lost in the regular lemma ngram formulation.",6.2 Word NGram Feature Analysis,[0],[0]
"For example, propositional heads such as know, need, hold, mean and want are indicators of power when they occur in CB contexts (e.g., i need ...), whereas their usages in NA contexts (e.g., do you need?, if i need..., etc.) are indicators of lack of power.",6.2 Word NGram Feature Analysis,[0],[0]
"In contrast, the CB version of attend, let, plan, could, check, discuss, and feel (e.g., i will attend/check/plan ...) are strongly associated with lack of power, while their NA versions (e.g., can you attend/check/plan?)",6.2 Word NGram Feature Analysis,[0],[0]
are indicators of power.,6.2 Word NGram Feature Analysis,[0],[0]
"In this paper, we made two major contributions.",7 Conclusion,[0],[0]
"First, we presented a large-scale data oriented analysis of how social power relations between participants of an interaction correlate with different types of author commitment in terms of their relative usage of hedges and different levels of beliefs — committed belief, non-committed belief, reported belief, and non-belief.",7 Conclusion,[0],[0]
"We found evidence that subordinates use significantly more propositional hedges than superiors, and that superiors and subordinates use significantly different proportions of different types of beliefs in their messages.",7 Conclusion,[0],[0]
"In particular, subordinates use significantly more non-committed beliefs than superiors.",7 Conclusion,[0],[0]
They also report others’ beliefs more often than superiors.,7 Conclusion,[0],[0]
"Second, we investigated different ways of incorporating the belief tag information into the machine learning system that automatically detects the direction of power between pairs of participants in an interaction.",7 Conclusion,[0],[0]
"We devised a sophisticated way of incorporating this information into the machine learning framework by appending the heads of propositions in lexical features with corresponding belief tags, demonstrating its utility in distinguishing social meanings expressed through the different belief contexts.
",7 Conclusion,[0],[0]
"This study is based on emails from a single corporation, at the beginning of the 21st century.",7 Conclusion,[0],[0]
Our findings on the correlation between author commitment and power may be reflective of the work culture that prevailed in that organization at the time when the emails were exchanged.,7 Conclusion,[0],[0]
It is important to replicate this study on emails from multiple organizations in order to assess whether these results generalize across board.,7 Conclusion,[0],[0]
"It is likely that behavior patterns are affected by factors such as ethnic culture (Cox et al., 1991) of the organization, and the kinds of conversations interactants engage in (for instance, co-operative vs. competitive behavior (Hill et al., 1992)).",7 Conclusion,[0],[0]
We intend to explore this line of inquiry in future work.,7 Conclusion,[0],[0]
This paper is partially based upon work supported by the DARPA DEFT program under a grant to Columbia University; all three co-authors were at Columbia University when portions of this work were performed.,Acknowledgments,[0],[0]
"The views expressed here are those of the author(s) and do not reflect the official policy or position of the Department of De-
fense or the U.S. Government.",Acknowledgments,[0],[0]
We thank Dan Jurafsky and the anonymous reviewers for their helpful feedback.,Acknowledgments,[0],[0]
"Understanding how social power structures affect the way we interact with one another is of great interest to social scientists who want to answer fundamental questions about human behavior, as well as to computer scientists who want to build automatic methods to infer the social contexts of interactions.",abstractText,[0],[0]
"In this paper, we employ advancements in extrapropositional semantics extraction within NLP to study how author commitment reflects the social context of an interactions.",abstractText,[0],[0]
"Specifically, we investigate whether the level of commitment expressed by individuals in an organizational interaction reflects the hierarchical power structures they are part of.",abstractText,[0],[0]
We find that subordinates use significantly more instances of non-commitment than superiors.,abstractText,[0],[0]
"More importantly, we also find that subordinates attribute propositions to other agents more often than superiors do — an aspect that has not been studied before.",abstractText,[0],[0]
"Finally, we show that enriching lexical features with commitment labels captures important distinctions in social meanings.",abstractText,[0],[0]
Author Commitment and Social Power: Automatic Belief Tagging to Infer the Social Context of Interactions,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 908–916, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Neural networks have proven to be highly effective at many tasks in natural language.,1 Introduction,[0],[0]
"For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014).",1 Introduction,[0],[0]
"However, neural networks can be complicated to design and train well.",1 Introduction,[0],[0]
"Many decisions need to be made, and performance can be highly dependent on making them correctly.",1 Introduction,[0],[0]
"Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments.
",1 Introduction,[0],[0]
"In this paper, we focus on the choice of the sizes of hidden layers.",1 Introduction,[0],[0]
"We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that
they can be removed from the network.",1 Introduction,[0],[0]
"Thus, after training with more units than necessary, a network is produced that has hidden layers correctly sized, saving both time and memory when actually putting the network to use.
",1 Introduction,[0],[0]
"Using a neural n-gram language model (Bengio et al., 2003), we are able to show that our novel auto-sizing method is able to learn models that are smaller than models trained without the method, while maintaining nearly the same perplexity.",1 Introduction,[0],[0]
"The method has only a single hyperparameter to adjust (as opposed to adjusting the sizes of each of the hidden layers), and we find that the same setting works consistently well across different training data sizes, vocabulary sizes, and n-gram sizes.",1 Introduction,[0],[0]
"In addition, we show that incorporating these models into a machine translation decoder still results in large BLEU point improvements.",1 Introduction,[0],[0]
The result is that fewer experiments are needed to obtain models that perform well and are correctly sized.,1 Introduction,[0],[0]
Language models are often used in natural language processing tasks involving generation of text.,2 Background,[0],[0]
"For instance, in machine translation, the language model helps to output fluent translations, and in speech recognition, the language model helps to disambiguate among possible utterances.
",2 Background,[0],[0]
"Current language models are usually n-gram models, which look at the previous (n− 1) words to predict the nth word in a sequence, based on (smoothed) counts of n-grams collected from training data.",2 Background,[0],[0]
"These models are simple but very effective in improving the performance of natural language systems.
",2 Background,[0],[0]
"However, n-gram models suffer from some limitations, such as data sparsity and memory usage.",2 Background,[0],[0]
"As an alternative, researchers have begun exploring the use of neural networks for language modeling.",2 Background,[0],[0]
"For modeling n-grams, the most common approach is the feedforward network of Bengio et
908
al. (2003), shown in Figure 1.",2 Background,[0],[0]
"Each node represents a unit or “neuron,” which has a real valued activation.",2 Background,[0],[0]
The units are organized into real-vector valued layers.,2 Background,[0],[0]
The activations at each layer are computed as follows.,2 Background,[0],[0]
(We assume n = 3; the generalization is easy.),2 Background,[0],[0]
"The two preceding words, w1, w2, are mapped into lowerdimensional word embeddings,
x1",2 Background,[0],[0]
= A:w1 x2 =,2 Background,[0],[0]
"A:w2
then passed through two hidden layers,
y = f(B1x1 +",2 Background,[0],[0]
B2x2 + b) z,2 Background,[0],[0]
"= f(Cy + c)
where f is an elementwise nonlinear activation (or transfer) function.",2 Background,[0],[0]
"Commonly used activation functions are the hyperbolic tangent, logistic function, and rectified linear units, to name a few.",2 Background,[0],[0]
"Finally, the result is mapped via a softmax to an output probability distribution,
P (wn | w1 · · ·wn−1) ∝",2 Background,[0],[0]
exp([Dz + d]wn).,2 Background,[0],[0]
"The parameters of the model are A, B1, B2, b, C, c, D, and d, which are learned by minimizing the negative log-likelihood of the the training data using stochastic gradient descent (also known as backpropagation) or variants.
",2 Background,[0],[0]
"Vaswani et al. (2013) showed that this model, with some improvements, can be used effectively during decoding in machine translation.",2 Background,[0],[0]
"In this paper, we use and extend their implementation.",2 Background,[0],[0]
Our method is focused on the challenge of choosing the number of units in the hidden layers of a feed-forward neural network.,3 Methods,[0],[0]
"The networks used for different tasks require different numbers of units, and the layers in a single network also require different numbers of units.",3 Methods,[0],[0]
"Choosing too few units can impair the performance of the network, and choosing too many units can lead to overfitting.",3 Methods,[0],[0]
"It can also slow down computations with the network, which can be a major concern for many applications such as integrating neural language models into a machine translation decoder.
",3 Methods,[0],[0]
Our method starts out with a large number of units in each layer and then jointly trains the network while pruning out individual units when possible.,3 Methods,[0],[0]
"The goal is to end up with a trained network
that also has the optimal number of units in each layer.
",3 Methods,[0],[0]
We do this by adding a regularizer to the objective function.,3 Methods,[0],[0]
"For simplicity, consider a single layer without bias, y = f(Wx).",3 Methods,[0],[0]
Let L(W) be the negative log-likelihood of the model.,3 Methods,[0],[0]
"Instead of minimizing L(W) alone, we want to minimize L(W)",3 Methods,[0],[0]
"+ λR(W), where R(W) is a convex regularizer.",3 Methods,[0],[0]
"The `1 norm, R(W) = ‖W‖1 =∑
i,j |Wij |, is a common choice for pushing parameters to zero, which can be useful for preventing overfitting and reducing model size.",3 Methods,[0],[0]
"However, we are interested not only in reducing the number of parameters but the number of units.",3 Methods,[0],[0]
"To do this, we need a different regularizer.
",3 Methods,[0],[0]
"We assume activation functions that satisfy f(0) = 0, such as the hyperbolic tangent or rectified linear unit (f(x) = max{0, x}).",3 Methods,[0],[0]
"Then, if we push the incoming weights of a unit yi to zero, that is, Wij = 0 for all j (as well as the bias, if any: bi = 0), then yi = f(0) = 0 is independent of the previous layers and contributes nothing to subsequent layers.",3 Methods,[0],[0]
So the unit can be removed without affecting the network at all.,3 Methods,[0],[0]
"Therefore, we need a regularizer that pushes all the incoming connection weights to a unit together towards zero.
",3 Methods,[0],[0]
"Here, we experiment with two, the `2,1 norm and the `∞,1 norm.1 The `2,1 norm on a ma-
1In the notation `p,q , the subscript p corresponds to the norm over each group of parameters, and q corresponds to the norm over the group norms.",3 Methods,[0],[0]
"Contrary to more common usage, in this paper, the groups are rows, not columns.
",3 Methods,[0],[0]
"trix W is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖2 = ∑ i ∑ j W 2ij  12 .,3 Methods,[0],[0]
"(1) (If there are biases bi, they should be included as well.)",3 Methods,[0],[0]
"This puts equal pressure on each row, but within each row, the larger values contribute more, and therefore there is more pressure on larger values towards zero.",3 Methods,[0],[0]
"The `∞,1 norm is
R(W) =",3 Methods,[0],[0]
∑ i ‖Wi:‖∞ = ∑ i max j |Wij |.,3 Methods,[0],[0]
"(2)
Again, this puts equal pressure on each row, but within each row, only the maximum value (or values) matter, and therefore the pressure towards zero is entirely on the maximum value(s).
",3 Methods,[0],[0]
Figure 2 visualizes the sparsity-inducing behavior of the two regularizers on a single row.,3 Methods,[0],[0]
Both have a sharp tip at the origin that encourages all the parameters in a row to become exactly zero.,3 Methods,[0],[0]
"However, this also means that sparsity-inducing regularizers are not differentiable at zero, making gradient-based optimization methods trickier to apply.",4 Optimization,[0],[0]
"The methods we use are discussed in detail elsewhere (Duchi et al., 2008; Duchi and Singer, 2009); in this section, we include a short description of these methods for completeness.",4 Optimization,[0],[0]
"Most work on learning with regularizers, including this work, can be thought of as instances of the proximal gradient method (Parikh and Boyd, 2014).",4.1 Proximal gradient method,[0],[0]
"Our objective function can be split into two parts, a convex and differentiable part (L) and a
convex but non-differentiable part (λR).",4.1 Proximal gradient method,[0],[0]
"In proximal gradient descent, we alternate between improving L alone and λR alone.",4.1 Proximal gradient method,[0],[0]
Let u be the parameter values from the previous iteration.,4.1 Proximal gradient method,[0],[0]
"We compute new parameter values w using:
v← u− η∇L(u) (3)
w← arg max w",4.1 Proximal gradient method,[0],[0]
( 1 2η ‖w − v‖2 + λR(w) ),4.1 Proximal gradient method,[0],[0]
"(4)
and repeat until convergence.",4.1 Proximal gradient method,[0],[0]
The first update is just a standard gradient descent update on L; the second is known as the proximal operator for λR and in many cases has a closed-form solution.,4.1 Proximal gradient method,[0],[0]
"In the rest of this section, we provide some justification for this method, and in Sections 4.2 and 4.3 we show how to compute the proximal operator for the `2 and `∞ norms.
",4.1 Proximal gradient method,[0],[0]
We can think of the gradient descent update (3) on L as follows.,4.1 Proximal gradient method,[0],[0]
"Approximate L around u by the tangent plane,
L̄(v) = L(u) +∇L(u)(v − u) (5)
and move v to minimize L̄, but don’t move it too far from u; that is, minimize
F (v) = 1",4.1 Proximal gradient method,[0],[0]
2η ‖v,4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(v).
",4.1 Proximal gradient method,[0],[0]
"Setting partial derivatives to zero, we get
∂F ∂v = 1 η
(v − u) +∇L(u) = 0 v = u− η∇L(u).
",4.1 Proximal gradient method,[0],[0]
"By a similar strategy, we can derive the second step (4).",4.1 Proximal gradient method,[0],[0]
"Again we want to move w to minimize the objective function, but don’t want to move it too far from u; that is, we want to minimize:
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− u‖2 + L̄(w) + λR(w).
",4.1 Proximal gradient method,[0],[0]
Note that we have not approximated R by a tangent plane.,4.1 Proximal gradient method,[0],[0]
We can simplify this by substituting in (3).,4.1 Proximal gradient method,[0],[0]
"The first term becomes
1 2η ‖w",4.1 Proximal gradient method,[0],[0]
− u‖2 = 1 2η ‖w,4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)‖2
= 1 2η ‖w − v‖2 −∇L(u)(w − v)
+ η
2 ‖∇L(u)‖2
and the second term becomes
L̄(w) = L(u) +∇L(u)(w − u) = L(u) +∇L(u)(w",4.1 Proximal gradient method,[0],[0]
"− v − η∇L(u)).
",4.1 Proximal gradient method,[0],[0]
"The ∇L(u)(w − v) terms cancel out, and we can ignore terms not involving w, giving
G(w) = 1 2η ‖w",4.1 Proximal gradient method,[0],[0]
"− v‖2 + λR(w) + const.
which is minimized by the update (4).",4.1 Proximal gradient method,[0],[0]
"Thus, we have split the optimization step into two easier steps: first, do the update for L (3), then do the update for λR (4).",4.1 Proximal gradient method,[0],[0]
The latter can often be done exactly (without approximating R by a tangent plane).,4.1 Proximal gradient method,[0],[0]
"We show next how to do this for the `2 and `∞ norms.
4.2 `2 and `2,1 regularization Since the `2,1 norm on matrices (1) is separable into the `2 norm of each row, we can treat each row separately.",4.1 Proximal gradient method,[0],[0]
"Thus, for simplicity, assume that we have a single row and want to minimize
G(w) = 1 2η ‖w − v‖2 +",4.1 Proximal gradient method,[0],[0]
"λ‖w‖+ const.
",4.1 Proximal gradient method,[0],[0]
"The minimum is either at w = 0 (the tip of the cone) or where the partial derivatives are zero (Figure 3):
∂G ∂w = 1 η (w − v) + λ w‖w‖",4.1 Proximal gradient method,[0],[0]
"= 0.
",4.1 Proximal gradient method,[0],[0]
"Clearly, w and v must have the same direction and differ only in magnitude, that is, w = α v‖v‖ .",4.1 Proximal gradient method,[0],[0]
"Substituting this into the above equation, we get the solution
α = ‖v‖",4.1 Proximal gradient method,[0],[0]
− ηλ.,4.1 Proximal gradient method,[0],[0]
"Therefore the update is
w = α v ‖v‖
α = max(0, ‖v‖ − ηλ).",4.1 Proximal gradient method,[0],[0]
"As above, since the `∞,1 norm on matrices (2) is separable into the `∞ norm of each row, we can treat each row separately; thus, we want to minimize
G(w) = 1 2η ‖w − v‖2 + λmax j |xj |+ const.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the solution can be characterized as: Decrease all of the maximal |xj | until the total decrease reaches ηλ or all the xj are zero.","4.3 `∞ and `∞,1 regularization",[0],[0]
"See Figure 4.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"If we pre-sort the |xj | in nonincreasing order, it’s easy to see how to compute this: for ρ = 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n, see if there is a value ξ ≤ xρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"such that decreasing all the x1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", xρ to ξ amounts to a total decrease of ηλ.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The largest ρ for which this is possible gives the correct solution.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"But this situation seems similar to another optimization problem, projection onto the `1-ball, which Duchi et al. (2008) solve in linear time without pre-sorting.","4.3 `∞ and `∞,1 regularization",[0],[0]
"In fact, the two problems can be solved by nearly identical algorithms, because they are convex conjugates of each other (Duchi and Singer, 2009; Bach et al., 2012).","4.3 `∞ and `∞,1 regularization",[0],[0]
"Intuitively, the `1 projection of v is exactly what is cut out by the `∞ proximal operator, and vice versa (Figure 4).
","4.3 `∞ and `∞,1 regularization",[0],[0]
Duchi et al.’s algorithm modified for the present problem is shown as Algorithm 1.,"4.3 `∞ and `∞,1 regularization",[0],[0]
It partitions the xj about a pivot element (line 6) and tests whether it and the elements to its left can be decreased to a value ξ such that the total decrease is δ (line 8).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"If so, it recursively searches the right side; if not, the
left side.","4.3 `∞ and `∞,1 regularization",[0],[0]
"At the conclusion of the algorithm, ρ is set to the largest value that passes the test (line 13), and finally the new xj are computed (line 16) – the only difference from Duchi et al.’s algorithm.
","4.3 `∞ and `∞,1 regularization",[0],[0]
This algorithm is asymptotically faster than that of Quattoni et al. (2009).,"4.3 `∞ and `∞,1 regularization",[0],[0]
"They reformulate `∞,1 regularization as a constrained optimization problem (in which the `∞,1 norm is bounded by µ) and provide a solution inO(n log n) time.","4.3 `∞ and `∞,1 regularization",[0],[0]
"The method shown here is simpler and faster because it can work on each row separately.
","4.3 `∞ and `∞,1 regularization",[0],[0]
"Algorithm 1 Linear-time algorithm for the proximal operator of the `∞ norm.
1: procedure UPDATE(w, δ) 2: lo, hi← 1, n 3: s← 0 4:","4.3 `∞ and `∞,1 regularization",[0],[0]
"while lo ≤ hi do 5: select md randomly from lo, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi 6: ρ← PARTITION(w, lo,md, hi) 7: ξ ← 1ρ","4.3 `∞ and `∞,1 regularization",[0],[0]
"( s+ ∑ρ i=lo |xi| − δ
) 8: if ξ ≤ |xρ| then 9: s← s+∑ρi=lo |xi|
10: lo← ρ+ 1 11: else 12: hi← ρ− 1 13: ρ← hi 14: ξ ← 1ρ (s− δ) 15: for i← 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", n","4.3 `∞ and `∞,1 regularization",[0],[0]
"do 16: xi ← min(max(xi,−ξ), ξ) 17: procedure PARTITION(w, lo,md, hi) 18: swap xlo and xmd 19: i← lo + 1 20: for j ← lo + 1, . . .","4.3 `∞ and `∞,1 regularization",[0],[0]
", hi do 21: if xj ≥ xlo then 22: swap xi and xj 23: i← i+ 1 24: swap xlo and xi−1 25: return i− 1","4.3 `∞ and `∞,1 regularization",[0],[0]
"We evaluate our model using the open-source NPLM toolkit released by Vaswani et al. (2013), extending it to use the additional regularizers as described in this paper.2 We use a vocabulary size of 100k and word embeddings with 50 dimensions.",5 Experiments,[0],[0]
"We use two hidden layers of rectified linear units (Nair and Hinton, 2010).
",5 Experiments,[0],[0]
"2These extensions have been contributed to the NPLM project.
",5 Experiments,[0],[0]
"We train neural language models (LMs) on two natural language corpora, Europarl v7 English and the AFP portion of English Gigaword 5.",5 Experiments,[0],[0]
"After tokenization, Europarl has 56M tokens and Gigaword AFP has 870M tokens.",5 Experiments,[0],[0]
"For both corpora, we hold out a validation set of 5,000 tokens.",5 Experiments,[0],[0]
"We train each model for 10 iterations over the training data.
",5 Experiments,[0],[0]
Our experiments break down into three parts.,5 Experiments,[0],[0]
"First, we look at the impact of our pruning method on perplexity of a held-out validation set, across a variety of settings.",5 Experiments,[0],[0]
"Second, we take a closer look at how the model evolves through the training process.",5 Experiments,[0],[0]
"Finally, we explore the downstream impact of our method on a statistical phrase-based machine translation system.",5 Experiments,[0],[0]
"We first look at the impact that the `∞,1 regularizer has on the perplexity of our validation set.",5.1 Evaluating perplexity and network size,[0],[0]
The main results are shown in Table 1.,5.1 Evaluating perplexity and network size,[0],[0]
"For λ ≤ 0.01, the regularizer seems to have little impact: no hidden units are pruned, and perplexity is also not affected.",5.1 Evaluating perplexity and network size,[0],[0]
"For λ = 1, on the other hand, most hidden units are pruned – apparently too many, since perplexity is worse.",5.1 Evaluating perplexity and network size,[0],[0]
"But for λ = 0.1, we see that we are able to prune out many hidden units: up to half of the first layer, with little impact on perplexity.",5.1 Evaluating perplexity and network size,[0],[0]
"We found this to be consistent across all our experiments, varying n-gram size, initial hidden layer size, and vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
Table 2 shows the same information for 5-gram models trained on the larger Gigaword AFP corpus.,5.1 Evaluating perplexity and network size,[0],[0]
"These numbers look very similar to those on Europarl: again λ = 0.1 works best, and, counter to expectation, even the final number of units is similar.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 3 shows the result of varying the vocabulary size: again λ = 0.1 works best, and, although it is not shown in the table, we also found that the final number of units did not depend strongly on the vocabulary size.
",5.1 Evaluating perplexity and network size,[0],[0]
"Table 4 shows results using the `2,1 norm (Europarl corpus, 5-grams, 100k vocabulary).",5.1 Evaluating perplexity and network size,[0],[0]
"Since this is a different regularizer, there isn’t any reason to expect that λ behaves the same way, and indeed, a smaller value of λ seems to work best.",5.1 Evaluating perplexity and network size,[0],[0]
We also studied the evolution of the network over the training process to gain some insights into how the method works.,5.2 A closer look at training,[0],[0]
"The first question we want to
answer is whether the method is simply removing units, or converging on an optimal number of units.",5.2 A closer look at training,[0],[0]
"Figure 5 suggests that it is a little of both: if we start with too many units (900 or 1000), the method converges to the same number regardless of how many extra units there were initially.",5.2 A closer look at training,[0],[0]
"But if we start with a smaller number of units, the method still prunes away about 50 units.
",5.2 A closer look at training,[0],[0]
"Next, we look at the behavior over time of different regularization strengths λ.",5.2 A closer look at training,[0],[0]
"We found that not only does λ = 1 prune out too many units, it does so at the very first iteration (Figure 6, above), perhaps prematurely.",5.2 A closer look at training,[0],[0]
"By contrast, the λ = 0.1 run prunes out units gradually.",5.2 A closer look at training,[0],[0]
"By plotting these curves together with perplexity (Figure 6, below), we can see that the λ = 0.1 run is fitting the model and pruning it at the same time, which seems preferable to fitting without any pruning (λ =
0.01) or pruning first and then fitting (λ = 1).",5.2 A closer look at training,[0],[0]
"We can also visualize the weight matrix itself over time (Figure 7), for λ = 0.1.",5.2 A closer look at training,[0],[0]
"It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune.",5.2 A closer look at training,[0],[0]
We also looked at the impact of our method on statistical machine translation systems.,5.3 Evaluating on machine translation,[0],[0]
"We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext.",5.3 Evaluating on machine translation,[0],[0]
We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data.,5.3 Evaluating on machine translation,[0],[0]
"Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regularization (λ = 0).
",5.3 Evaluating on machine translation,[0],[0]
We built our system using the newscommentary dataset v8.,5.3 Evaluating on machine translation,[0],[0]
We tuned our model using newstest13 and evaluated using newstest14.,5.3 Evaluating on machine translation,[0],[0]
"After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset, and 3,000 sentences each for the tuning and test sets.
",5.3 Evaluating on machine translation,[0],[0]
"Table 5 shows that the addition of a neural LM helps substantially over the baseline, with improvements of up to 2 BLEU.",5.3 Evaluating on machine translation,[0],[0]
"Using the Europarl model, the BLEU scores obtained without and with regularization were not significantly different (p ≥ 0.05), consistent with the negligible perplexity difference between these models.",5.3 Evaluating on machine translation,[0],[0]
"On the Gigaword AFP model, regularization did decrease the BLEU score by 0.3, consistent with the small perplexity increase of the regularized model.",5.3 Evaluating on machine translation,[0],[0]
"The decrease is statistically significant, but small compared with the overall benefit of adding a neural LM.",5.3 Evaluating on machine translation,[0],[0]
Researchers have been exploring the use of neural networks for language modeling for a long time.,6 Related Work,[0],[0]
Schmidhuber and Heil (1996) proposed a character n-gram model using neural networks which they used for text compression.,6 Related Work,[0],[0]
"Xu and Rudnicky (2000) proposed a word-based probability model using a softmax output layer trained using cross-entropy, but only for bigrams.",6 Related Work,[0],[0]
Bengio et al. (2003) defined a probabilistic word n-gram model and demonstrated improvements over conventional smoothed language models.,6 Related Work,[0],[0]
Mnih and Teh (2012) sped up training of log-bilinear language models through the use of noise-contrastive estimation (NCE).,6 Related Work,[0],[0]
"Vaswani et al. (2013) also used NCE to train the architecture of Bengio et al. (2003), and were able to integrate a largevocabulary language model directly into a machine translation decoder.",6 Related Work,[0],[0]
"Baltescu et al. (2014) describe a similar model, with extensions like a hierarchical softmax (based on Brown clustering) and direct n-gram features.
",6 Related Work,[0],[0]
"Beyond feed-forward neural network language models, researchers have explored using more complicated neural network architectures.",6 Related Work,[0],[0]
"RNNLM is an open-source implementation of a language model using recurrent neural networks (RNN) where connections between units can form directed cycles (Mikolov et al., 2011).",6 Related Work,[0],[0]
Sundermeyer et al. (2015) use the long-short term memory (LSTM) neural architecture to show a perplexity improvement over the RNNLM toolkit.,6 Related Work,[0],[0]
"In future work, we plan on exploring how our method could improve these more complicated neural models as well.
",6 Related Work,[0],[0]
Automatically limiting the size of neural networks is an old idea.,6 Related Work,[0],[0]
"The “Optimal Brain Damage” (OBD) technique (LeCun et al., 1989) computes a saliency based on the second derivative of the objective function with respect to each parameter.",6 Related Work,[0],[0]
"The parameters are then sorted by saliency, and the lowest-saliency parameters are pruned.",6 Related Work,[0],[0]
"The pruning process is separate from the training process, whereas regularization performs training and pruning simultaneously.",6 Related Work,[0],[0]
"Regularization in neural networks is also an old idea; for example, Nowland and Hinton (1992) mention both `22 and `0 regularization.",6 Related Work,[0],[0]
"Our method develops on this idea by using a mixed norm to prune units, rather than parameters.
",6 Related Work,[0],[0]
"Srivastava et al. introduce a method called dropout in which units are directly deactivated at random during training (Srivastava et al., 2014), which induces sparsity in the hidden unit activations.",6 Related Work,[0],[0]
"However, at the end of training, all units are reactivated, as the goal of dropout is to reduce overfitting, not to reduce network size.",6 Related Work,[0],[0]
"Thus, dropout and our method seem to be complementary.",6 Related Work,[0],[0]
"We have presented a method for auto-sizing a neural network during training by removing units using a `∞,1 regularizer.",7 Conclusion,[0],[0]
"This regularizer drives a unit’s input weights as a group down to zero, allowing the unit to be pruned.",7 Conclusion,[0],[0]
"We can thus prune units out of our network during training with minimal impact to held-out perplexity or downstream performance of a machine translation system.
",7 Conclusion,[0],[0]
"Our results showed empirically that the choice
of a regularization coefficient of 0.1 was robust to initial configuration parameters of initial network size, vocabulary size, n-gram order, and training corpus.",7 Conclusion,[0],[0]
"Furthermore, imposing a single regularizer on the objective function can tune all of the hidden layers of a network with one setting.",7 Conclusion,[0],[0]
"This reduces the need to conduct expensive, multi-dimensional grid searches in order to determine optimal sizes.
",7 Conclusion,[0],[0]
We have demonstrated the power and efficacy of this method on a feed-forward neural network for language modeling though experiments on perplexity and machine translation.,7 Conclusion,[0],[0]
"However, this method is general enough that it should be applicable to other domains, both inside natural language processing and outside.",7 Conclusion,[0],[0]
"As neural models become more pervasive in natural language processing, the ability to auto-size networks for fast experimentation and quick exploration will become increasingly important.",7 Conclusion,[0],[0]
"We would like to thank Tomer Levinboim, Antonios Anastasopoulos, and Ashish Vaswani for their helpful discussions, as well as the reviewers for their assistance and feedback.",Acknowledgments,[0],[0]
Neural networks have been shown to improve performance across a range of natural-language tasks.,abstractText,[0],[0]
"However, designing and training them can be complicated.",abstractText,[0],[0]
"Frequently, researchers resort to repeated experimentation to pick optimal settings.",abstractText,[0],[0]
"In this paper, we address the issue of choosing the correct number of units in hidden layers.",abstractText,[0],[0]
"We introduce a method for automatically adjusting network size by pruning out hidden units through `∞,1 and `2,1 regularization.",abstractText,[0],[0]
We apply this method to language modeling and demonstrate its ability to correctly choose the number of hidden units while maintaining perplexity.,abstractText,[0],[0]
We also include these models in a machine translation decoder and show that these smaller neural models maintain the significant improvements of their unpruned versions.,abstractText,[0],[0]
Auto-Sizing Neural Networks: With Applications to n-gram Language Models,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 725–731 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
725
Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq). The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation. Compared with the source content, the annotated summary is short and well written. Moreover, it shares the same meaning as the source content. In this work, we supervise the learning of the representation of the source content with that of the summary. In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq. Following previous work, we evaluate our model on a popular Chinese social media dataset. Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1",text,[0],[0]
Text summarization is to produce a brief summary of the main ideas of the text.,1 Introduction,[0],[0]
"Unlike extractive text summarization (Radev et al., 2004; Woodsend and Lapata, 2010; Cheng and Lapata, 2016), which selects words or word phrases from the source texts as the summary, abstractive text summarization learns a semantic representation to generate more human-like summaries.",1 Introduction,[0],[0]
"Recently, most models for abstractive text summarization are based on the sequence-to-sequence model, which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder.
1The code is available at https://github.com/ lancopku/superAE
The contents on the social media are long, and contain many errors, which come from spelling mistakes, informal expressions, and grammatical mistakes (Baldwin et al., 2013).",1 Introduction,[0],[0]
Large amount of errors in the contents cause great difficulties for text summarization.,1 Introduction,[0],[0]
"As for RNN-based Seq2Seq, it is difficult to compress a long sequence into an accurate representation (Li et al., 2015), because of the gradient vanishing and exploding problem.
",1 Introduction,[0],[0]
"Compared with the source content, it is easier to encode the representations of the summaries, which are short and manually selected.",1 Introduction,[0],[0]
"Since the source content and the summary share the same points, it is possible to supervise the learning of the semantic representation of the source content with that of the summary.
",1 Introduction,[0],[0]
"In this paper, we regard a summary autoencoder as an assistant supervisor of Seq2Seq.",1 Introduction,[0],[0]
"First, we train an autoencoder, which inputs and reconstructs the summaries, to obtain a better representation to generate the summaries.",1 Introduction,[0],[0]
"Then, we supervise the internal representation of Seq2Seq with that of autoencoder by minimizing the distance between two representations.",1 Introduction,[0],[0]
"Finally, we use adversarial learning to enhance the supervision.",1 Introduction,[0],[0]
"Following the previous work (Ma et al., 2017), We evaluate our proposed model on a Chinese social media dataset.",1 Introduction,[0],[0]
Experimental results show that our model outperforms the state-of-theart baseline models.,1 Introduction,[0],[0]
"More specifically, our model outperforms the Seq2Seq baseline by the score of 7.1 ROUGE-1, 6.1 ROUGE-2, and 7.0 ROUGE-L.",1 Introduction,[0],[0]
We introduce our proposed model in detail in this section.,2 Proposed Model,[0],[0]
"Given a summarization dataset that consists of N data samples, the ith data sample (xi, yi) con-
tains a source content xi = {x1, x2, ..., xM}, and a summary yi = {y1, y2, ..., yL}, while M is the number of the source words, and L is the number of the summary words.",2.1 Notation,[0],[0]
"At the training stage, we train the model to generate the summary y given the source content x.",2.1 Notation,[0],[0]
"At the test stage, the model decodes the predicted summary y′",2.1 Notation,[0],[0]
given the source content x.,2.1 Notation,[0],[0]
Figure 1 shows the architecture of our model.,2.2 Supervision with Autoencoder,[0],[0]
"At the training stage, the source content encoder compresses the input contents x into the internal representation zt with a Bi-LSTM encoder.",2.2 Supervision with Autoencoder,[0],[0]
"At the same time, the summary encoder compresses the reference summary y into the representation zs.",2.2 Supervision with Autoencoder,[0],[0]
Then both zt and zs are fed into a LSTM decoder to generate the summary.,2.2 Supervision with Autoencoder,[0],[0]
"Finally, the semantic representation of the source content is supervised by the summary.
",2.2 Supervision with Autoencoder,[0],[0]
"We implement the supervision by minimizing the distance between the semantic representations zt and zs, and this term in the loss function can be written as:
LS = λ
Nh d(zt, zs) (1)
where d(zt, zs) is a function which measures the distance between zs and zt.",2.2 Supervision with Autoencoder,[0],[0]
"λ is a tunable hyperparameter to balance the loss of the supervision and the other parts of the loss, and Nh is the number of the hidden unit to limit the magnitude of the distance function.",2.2 Supervision with Autoencoder,[0],[0]
We set λ = 0.3 based on the performance on the validation set.,2.2 Supervision with Autoencoder,[0],[0]
"The distance between two representations can be written as:
d(zt, zs) = ‖zt",2.2 Supervision with Autoencoder,[0],[0]
− zs‖2 (2),2.2 Supervision with Autoencoder,[0],[0]
We further enhance the supervision with the adversarial learning approach.,2.3 Adversarial Learning,[0],[0]
"As shown in Eq. 1, we use a fixed hyper-parameter as a weight to measure the strength of the supervision of the autoencoder.",2.3 Adversarial Learning,[0],[0]
"However, in the case when the source content and summary have high relevance, the strength of the supervision should be higher, and when the source content and summary has low relevance, the strength should be lower.",2.3 Adversarial Learning,[0],[0]
"In order to determine the strength of supervision more dynamically, we introduce the adversarial learning.",2.3 Adversarial Learning,[0],[0]
"More specifically, we regard the representation of the autoencoder as the “gold” representation, and that of the sequence-to-sequence as the “fake” representation.",2.3 Adversarial Learning,[0],[0]
"A model is trained to discriminate between the gold and fake representations, which is called a discriminator.",2.3 Adversarial Learning,[0],[0]
The discriminator tries to identify the two representations.,2.3 Adversarial Learning,[0],[0]
"On the contrary, the supervision, which minimizes the distance of the representations and makes them similar, tries to prevent the discriminator from making correct predictions.",2.3 Adversarial Learning,[0],[0]
"In this way, when the discriminator can distinguish the two representations (which means the source content and the summary has low relevance), the strength of supervision will be decreased, and when the discriminator fails to distinguish, the strength of supervision will be improved.
",2.3 Adversarial Learning,[0],[0]
"In implementation of the adversarial learning, the discriminator objective function can be written as:
LD(θD) =− logPθD(y = 1|zt)",2.3 Adversarial Learning,[0],[0]
"− logPθD(y = 0|zs)
(3)
where PθD(y = 1|z) is the probability that the discriminator identifies the vector z as the “gold” representation, while PθD(y = 0|z) is the probability that the vector z is identified as the “fake” representation, and θD is the parameters of the discrim-
inator.",2.3 Adversarial Learning,[0],[0]
"When minimizing the discriminator objective, we only train the parameters of the discriminator, while the rest of the parameters remains unchanged.
",2.3 Adversarial Learning,[0],[0]
"The supervision objective to be against the discriminator can be written as:
LG(θE) =",2.3 Adversarial Learning,[0],[0]
− logPθD(y = 0|zt),2.3 Adversarial Learning,[0],[0]
"− logPθD(y = 1|zs)
(4)
When minimizing the supervision objective, we only update the parameters of the encoders.",2.3 Adversarial Learning,[0],[0]
There are several parts of the objective functions to optimize in our models.,2.4 Loss Function and Training,[0],[0]
"The first part is the cross entropy losses of the sequence-to-sequence and the autoencoder:
LSeq2seq = − N∑ i=1",2.4 Loss Function and Training,[0],[0]
"pSeq2seq(yi|zs) (5)
LAE =",2.4 Loss Function and Training,[0],[0]
"− N∑ i=1 pAE(yi|zt) (6)
The second part is the L2 loss of the supervision, as written in Equation 1.",2.4 Loss Function and Training,[0],[0]
"The last part is the adversarial learning, which are Equation 3 and Equation 4.",2.4 Loss Function and Training,[0],[0]
"The sum of all these parts is the final loss function to optimize.
",2.4 Loss Function and Training,[0],[0]
"We use the Adam (Kingma and Ba, 2014) optimization method to train the model.",2.4 Loss Function and Training,[0],[0]
"For the hyper-parameters of Adam optimizer, we set the learning rate α = 0.001, two momentum parameters β1 = 0.9 and β2 = 0.999 respectively, and = 1 × 10−8.",2.4 Loss Function and Training,[0],[0]
"We clip the gradients (Pascanu et al., 2013) to the maximum norm of 10.0.",2.4 Loss Function and Training,[0],[0]
"Following the previous work (Ma et al., 2017), we evaluate our model on a popular Chinese social media dataset.",3 Experiments,[0],[0]
"We first introduce the datasets, evaluation metrics, and experimental details.",3 Experiments,[0],[0]
"Then, we compare our model with several state-of-the-art systems.",3 Experiments,[0],[0]
Large Scale Chinese Social Media Text Summarization Dataset (LCSTS) is constructed by Hu et al. (2015).,3.1 Dataset,[0],[0]
"The dataset consists of more than 2,400,000 text-summary pairs, constructed from a famous Chinese social media website called Sina Weibo.2 It is split into three parts, with 2,400,591
2http://weibo.com
pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III.",3.1 Dataset,[0],[0]
All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5.,3.1 Dataset,[0],[0]
"We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III.",3.1 Dataset,[0],[0]
"Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set.",3.1 Dataset,[0],[0]
"Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation.",3.2 Evaluation Metric,[0],[0]
"The metrics compare an automatically produced summary with the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS).",3.2 Evaluation Metric,[0],[0]
"Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results.",3.2 Evaluation Metric,[0],[0]
"The vocabularies are extracted from the training sets, and the source contents and the summaries share the same vocabularies.",3.3 Experimental Details,[0],[0]
"In order to alleviate the risk of word segmentation mistakes, we split the Chinese sentences into characters.",3.3 Experimental Details,[0],[0]
"We prune the vocabulary size to 4,000, which covers most of the common characters.
",3.3 Experimental Details,[0],[0]
We tune the hyper-parameters based on the ROUGE scores on the validation sets.,3.3 Experimental Details,[0],[0]
"We set the word embedding size and the hidden size to 512, and the number of LSTM layers is 2.",3.3 Experimental Details,[0],[0]
"The batch size is 64, and we do not use dropout (Srivastava et al., 2014) on this dataset.",3.3 Experimental Details,[0],[0]
"Following the previous work (Li et al., 2017), we implement the beam search, and set the beam size to 10.",3.3 Experimental Details,[0],[0]
"We compare our model with the following stateof-the-art baselines.
",3.4 Baselines,[0],[0]
"• RNN and RNN-cont are two sequence-tosequence baseline with GRU encoder and decoder, provided by Hu et al. (2015).",3.4 Baselines,[0],[0]
"The difference between them is that RNN-context has attention mechanism while RNN does not.
",3.4 Baselines,[0],[0]
"• RNN-dist (Chen et al., 2016) is a distractionbased neural model, which the attention
mechanism focuses on the different parts of the source content.
",3.4 Baselines,[0],[0]
"• CopyNet (Gu et al., 2016) incorporates a copy mechanism to allow parts of the generated summary are copied from the source content.
",3.4 Baselines,[0],[0]
"• SRB (Ma et al., 2017) is a sequence-tosequence based neural model with improving the semantic relevance between the input text and the output summary.
",3.4 Baselines,[0],[0]
"• DRGD (Li et al., 2017) is a deep recurrent generative decoder model, combining the decoder with a variational autoencoder.
",3.4 Baselines,[0],[0]
"• Seq2seq is our implementation of the sequence-to-sequence model with the attention mechanism, which has the same experimental setting as our model for fair comparison.",3.4 Baselines,[0],[0]
"For the purpose of simplicity, we denote our supervision with autoencoder model as superAE.",3.5 Results,[0],[0]
"We report the ROUGE F1 score of our model and the baseline models on the test sets.
",3.5 Results,[0],[0]
Table 1 summarizes the results of our superAE model and several baselines.,3.5 Results,[0],[0]
We first compare our model with Seq2Seq baseline.,3.5 Results,[0],[0]
"It shows that our
superAE model has a large improvement over the Seq2Seq baseline by 7.1 ROUGE-1, 6.1 ROUGE2, and 7.0 ROUGE-L, which demonstrates the efficiency of our model.",3.5 Results,[0],[0]
"Moreover, we compare our model with the recent summarization systems, which have been evaluated on the same training set and the test sets as ours.",3.5 Results,[0],[0]
Their results are directly reported in the referred articles.,3.5 Results,[0],[0]
"It shows that our superAE outperforms all of these models, with a relative gain of 2.2 ROUGE-1, 1.8 ROUGE-2, and 2.0 ROUGE-L. We also perform ablation study by removing the adversarial learning component, in order to show its contribution.",3.5 Results,[0],[0]
"It shows that the adversarial learning improves the performance of 1.5 ROUGE-1, 0.7 ROUGE-2, and 1.0 ROUGE-L.
We also give a summarization examples of our model.",3.5 Results,[0],[0]
"As shown in Table 3, the SeqSeq model captures the wrong meaning of the source content, and produces the summary that “China United Airlines exploded in the airport”.",3.5 Results,[0],[0]
"Our superAE model captures the correct points, so that the generated summary is close in meaning to the reference summary.",3.5 Results,[0],[0]
We want to analyze whether the internal text representation is improved by our superAE model.,3.6 Analysis of text representation,[0],[0]
"Since the text representation is abstractive and hard to evaluate, we translate the representation into a sentiment score with a sentiment classifier, and evaluate the quality of the representation by means of the sentiment accuracy.
",3.6 Analysis of text representation,[0],[0]
"We perform experiments on the Amazon Fine Foods Reviews Corpus (McAuley and Leskovec, 2013).",3.6 Analysis of text representation,[0],[0]
"The Amazon dataset contains users’ rating labels as well as the summary for the reviews, making it possible to train a classifier to predict the sentiment labels and a seq2seq model to generate summaries.",3.6 Analysis of text representation,[0],[0]
"First, we train the superAE model and
the seq2seq model with the text-summary pairs until convergence.",3.6 Analysis of text representation,[0],[0]
"Then, we transfer the encoders to a sentiment classifier, and train the classifier with fixing the parameters of the encoders.",3.6 Analysis of text representation,[0],[0]
The classifier is a simple feedforward neural network which maps the representation into the label distribution.,3.6 Analysis of text representation,[0],[0]
"Finally, we compute the accuracy of the predicted 2-class labels and 5-class labels.
",3.6 Analysis of text representation,[0],[0]
"As shown in Table 2, the seq2seq model achieves 80.7% and 65.1% accuracy of 2-class and 5-class, respectively.",3.6 Analysis of text representation,[0],[0]
Our superAE model outperforms the baselines with a large margin of 8.1% and 6.6%.,3.6 Analysis of text representation,[0],[0]
"Rush et al. (2015) first propose an abstractive based summarization model, which uses an attentive CNN encoder to compress texts and a neural network language model to generate summaries.",4 Related Work,[0],[0]
Chopra et al. (2016) explore a recurrent structure for abstractive summarization.,4 Related Work,[0],[0]
"To deal with out-of-vocabulary problem, Nallapati et al. (2016)
propose a generator-pointer model so that the decoder is able to generate words in source texts.",4 Related Work,[0],[0]
"Gu et al. (2016) also solved this issue by incorporating copying mechanism, allowing parts of the summaries are copied from the source contents.",4 Related Work,[0],[0]
"See et al. (2017) further discuss this problem, and incorporate the pointer-generator model with the coverage mechanism.",4 Related Work,[0],[0]
"Hu et al. (2015) build a large corpus of Chinese social media short text summarization, which is one of our benchmark datasets.",4 Related Work,[0],[0]
"Chen et al. (2016) introduce a distraction based neural model, which forces the attention mechanism to focus on the difference parts of the source inputs.",4 Related Work,[0],[0]
"Ma et al. (2017) propose a neural model to improve the semantic relevance between the source contents and the summaries.
",4 Related Work,[0],[0]
"Our work is also related to the sequence-tosequence model (Cho et al., 2014), and the autoencoder model (Bengio, 2009; Liou et al., 2008, 2014).",4 Related Work,[0],[0]
"Sequence-to-sequence model is one of the most successful generative neural model, and is widely applied in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), and other natural language processing tasks.",4 Related Work,[0],[0]
"Autoencoder (Bengio, 2009) is an artificial neural network used for unsupervised learning of efficient representation.",4 Related Work,[0],[0]
Neural attention model is first proposed by Bahdanau et al. (2014).,4 Related Work,[0],[0]
"We propose a novel model, in which the autoencoder is a supervisor of the sequence-to-sequence model, to learn a better internal representation for abstractive summarization.",5 Conclusion,[0],[0]
An adversarial learning approach is introduced to further improve the supervision of the autoencoder.,5 Conclusion,[0],[0]
"Experimental results show that our model outperforms the sequence-to-sequence baseline by a large margin, and achieves the state-of-the-art performances on a Chinese social media dataset.",5 Conclusion,[0],[0]
"Our work is supported by National Natural Science Foundation of China (No. 61433015, No. 61673028), National High Technology Research and Development Program of China (863 Program, No. 2015AA015404), and the National Thousand Young Talents Program.",Acknowledgements,[0],[0]
Xu Sun is the corresponding author of this paper.,Acknowledgements,[0],[0]
Most of the current abstractive text summarization models are based on the sequence-to-sequence model (Seq2Seq).,abstractText,[0],[0]
"The source content of social media is long and noisy, so it is difficult for Seq2Seq to learn an accurate semantic representation.",abstractText,[0],[0]
"Compared with the source content, the annotated summary is short and well written.",abstractText,[0],[0]
"Moreover, it shares the same meaning as the source content.",abstractText,[0],[0]
"In this work, we supervise the learning of the representation of the source content with that of the summary.",abstractText,[0],[0]
"In implementation, we regard a summary autoencoder as an assistant supervisor of Seq2Seq.",abstractText,[0],[0]
"Following previous work, we evaluate our model on a popular Chinese social media dataset.",abstractText,[0],[0]
Experimental results show that our model achieves the state-of-the-art performances on the benchmark dataset.1,abstractText,[0],[0]
Autoencoder as Assistant Supervisor: Improving Text Representation for Chinese Social Media Text Summarization,title,[0],[0]
"Over two decades ago, in The importance of starting small, Elman put forward the idea that a curriculum of progressively harder tasks could significantly accelerate a neural network’s training (Elman, 1993).",1. Introduction,[0],[0]
"However curriculum learning has only recently become prevalent in the field (e.g., Bengio et al., 2009), due in part to the greater complexity of problems now being considered.",1. Introduction,[0],[0]
"In particular, recent work on learning programs with neural networks has relied on curricula to scale up to longer or more complicated tasks (Sutskever and Zaremba, 2014; Reed and de Freitas, 2015; Graves et al., 2016).",1. Introduction,[0],[0]
"We expect this trend to continue as the scope of neural networks widens, with deep reinforcement learning providing fertile ground for structured learning.
",1. Introduction,[0],[0]
One reason for the slow adoption of curriculum learning is that it’s effectiveness is highly sensitive to the mode of progression through the tasks.,1. Introduction,[0],[0]
"One popular approach is to define a hand-chosen performance threshold for advance-
1DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Alex Graves <gravesa@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ment to the next task, along with a fixed probability of returning to earlier tasks, to prevent forgetting (Sutskever and Zaremba, 2014).",1. Introduction,[0],[0]
"However, as well as introducing hard-totune parameters, this poses problems for curricula where appropriate thresholds may be unknown or variable across tasks.",1. Introduction,[0],[0]
"More fundamentally, it presupposes that the tasks can be ordered by difficulty, when in reality they may vary along multiple axes of difficulty, or have no predefined order at all.
",1. Introduction,[0],[0]
"We propose to instead treat the decision about which task to study next as a stochastic policy, continuously adapted to optimise some notion of what Oudeyer et al. (2007) termed learning progress.",1. Introduction,[0],[0]
"Doing so brings us into contact with the intrinsic motivation literature (Barto, 2013), where various indicators of learning progress have been used as reward signals to encourage exploration, including compression progress (Schmidhuber, 1991), information acquisition (Storck et al., 1995), Bayesian surprise (Itti and Baldi, 2009), prediction gain (Bellemare et al., 2016) and variational information maximisation (Houthooft et al., 2016).",1. Introduction,[0],[0]
"We focus on variants of prediction gain, and also introduce a novel class of progress signals which we refer to as complexity gain.",1. Introduction,[0],[0]
"Derived from minimum description length principles, complexity gain equates acquisition of knowledge with an increase in effective information encoded in the network weights.
",1. Introduction,[0],[0]
"Given a progress signal that can be evaluated for each training example, we use a multi-armed bandit algorithm to find a stochastic policy over the tasks that maximises overall progress.",1. Introduction,[0],[0]
"The bandit is nonstationary because the behaviour of the network, and hence the optimal policy, evolves during training.",1. Introduction,[0],[0]
"We take inspiration from a previous work that modelled an adaptive student with a multiarmed bandit in the context of developmental learning (Lopes and Oudeyer, 2012; Clement et al., 2015).",1. Introduction,[0],[0]
"Another related area is the field of active learning, where similar gain signals have been used to guide decisions about which data point to label next (Settles, 2010).",1. Introduction,[0],[0]
"Lastly, there are parallels with recent work on using Bayesian optimisation to find the best order in which to train a word embedding network on a language corpus (Tsvetkov, 2016); however this differs from our work in that the ordering was entirely determined before each training run, rather than adaptively altered in response to the model’s progress.",1. Introduction,[0],[0]
"We consider supervised learning problems where target sequences (b1,b2, . . . )",2. Background,[0],[0]
"are conditionally modelled given their respective input sequences (a1,a2, . . . ).",2. Background,[0],[0]
"For convenience we suppose that the targets are drawn from a finite set B, noting, however, that our framework extends straightforwardly to continuous targets, with probability densities taking the place of probabilities.",2. Background,[0],[0]
"As is typical for neural networks, sequences may be grouped together in batches (b1:B ,a1:B) to accelerate training.",2. Background,[0],[0]
"The conditional probability output by the model is
p(b1:B |a1:B) = ∏",2. Background,[0],[0]
"i,j p(bij |bi1:j−1,ai1:j−1).
",2. Background,[0],[0]
"From here onwards, we consider each batch as a single example x from X := (A × B)N , and write p(x) := p(b1:B |a1:B) for its probability.",2. Background,[0],[0]
"Under this notation, a task is a distribution D over sequences from X .",2. Background,[0],[0]
"A curriculum is an ensemble of tasks D1, . . .",2. Background,[0],[0]
", DN , a sample is an example drawn from one of the tasks of the curriculum, and a syllabus is a time-varying sequence of distributions over tasks.",2. Background,[0],[0]
"We consider a neural network to be a probabilistic model pθ over X , whose parameters are denoted θ.",2. Background,[0],[0]
"The expected loss of the network on the kth task is
Lk(θ)",2. Background,[0],[0]
":= E x∼Dk L(x, θ),
where L(x, θ) := − log pθ(x) is the sample loss on x.",2. Background,[0],[0]
"Whenever unambiguous, we will simply denote the expected and sample losses by Lk and L(x) respectively.",2. Background,[0],[0]
We consider two related settings.,2.1. Curriculum Learning,[0],[0]
"In the multiple tasks setting, The goal is to perform as well as possible on all tasks in {Dk}; this is captured by the objective function
LMT := 1
N N∑ k=1",2.1. Curriculum Learning,[0],[0]
"Lk.
",2.1. Curriculum Learning,[0],[0]
"In the target task setting, we are only interested in minimizing the loss on the final task DN .",2.1. Curriculum Learning,[0],[0]
The other tasks then act as a series of stepping stones to the real problem.,2.1. Curriculum Learning,[0],[0]
The objective function in this setting is simply LTT := LN .,2.1. Curriculum Learning,[0],[0]
"We view a curriculum containing N tasks as an N -armed bandit (Bubeck and Cesa-Bianchi, 2012), and a syllabus as an adaptive policy which seeks to maximize payoffs from this bandit.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"In the bandit setting, an agent selects a sequence of arms (actions) a1 . . .",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"aT over T rounds of play (at ∈ {1, . . .",2.2. Adversarial Multi-Armed Bandits,[0],[0]
", N}).",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"After each round, the selected arm
yields a payoff rt; the payoffs for the other arms are not observed.
",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"The classic algorithm for adversarial bandits is Exp3 (Auer et al., 2002), which uses multiplicative weight updates to guarantee low regret with respect to the best arm.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"On round t, the agent selects an arm stochastically according to a policy πt.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"This policy is defined by a set of weights wt,i:
πEXP3t (i)",2.2. Adversarial Multi-Armed Bandits,[0],[0]
":= ewt,i∑N j=1 e wt,j .
",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"The weights are the sum of importance-sampled rewards: wt,i := η ∑ s<t r̃s,i r̃s,i := rsI[as=i] πs(i) .
",2.2. Adversarial Multi-Armed Bandits,[0],[0]
Exp3 acts so as to minimize regret with respect to the single best arm evaluated over the whole history.,2.2. Adversarial Multi-Armed Bandits,[0],[0]
"However, a common occurrence is for an arm to be optimal for a portion of the history, then another arm, and so on; the best strategy is then piecewise stationary.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"This is generally the case in our setting, as the expected reward for each task changes as the model learns.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"The Fixed Share method (Herbster and Warmuth, 1998) addresses this issue by using an -greedy strategy and mixing in the weights additively.",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"In the bandit setting, this is known as the Exp3.S algorithm (also by Auer et al. (2002)):
πEXP3.Pt (i) := (1− )πEXP3",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"t (i) +
N (1) wSt,i := log [ (1− αt) exp { wSt−1,i + ηr̃ β t−1,i } +
αt N",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"− 1 ∑ j 6=i exp { wSt−1,j + ηr̃ β t−1,j }]",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"wS1,i := 0 αt := t −1",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"r̃βs,i := rsI[as=i] + β
πs(i) .",2.2. Adversarial Multi-Armed Bandits,[0],[0]
"The appropriate step size η depends on the magnitudes of the rewards, which may not be known a priori.",2.3. Reward Scaling,[0],[0]
"The problem is particularly acute in our setting, where the magnitude depends strongly on the gain signal used to measure learning progress, as well as varying over time as the model learns.",2.3. Reward Scaling,[0],[0]
"To address this issue, we adaptively rescale all rewards to lie in the interval",2.3. Reward Scaling,[0],[0]
"[−1, 1] using the following procedure: Let Rt be the history of unscaled rewards up to time t, i.e. Rt = {r̂i}t−1i=1 .",2.3. Reward Scaling,[0],[0]
"Let qlot and qhit be quantiles of Rt, which we choose here to be the 20th and 80th percentiles respectively.",2.3. Reward Scaling,[0],[0]
"The scaled reward rt is obtained by clipping r̂t to the interval [qlot , q
hit ] and then linearly mapping the result to lie in [−1, 1]:
rt =  −1",2.3. Reward Scaling,[0],[0]
if r̂t < qlot 1 if r̂t > qhit 2(r̂t−qlot ) qhit−qlot,2.3. Reward Scaling,[0],[0]
− 1 otherwise.,2.3. Reward Scaling,[0],[0]
"(2)
Rather than keeping the entire history of rewards, we use reservoir sampling to maintain a representative sample, and compute approximate quantiles from this sample.",2.3. Reward Scaling,[0],[0]
These quantiles can be obtained in Θ(log|Rt|) time.,2.3. Reward Scaling,[0],[0]
Our goal is to use the policy output by Exp3.S as a syllabus for training our models.,3. Learning Progress Signals,[0],[0]
"Ideally we would like the policy to maximize the rate at which we minimize the loss, and the reward should reflect this rate – what Oudeyer et al. (2007) calls learning progress.",3. Learning Progress Signals,[0],[0]
"However, it usually is computationally undesirable or even impossible to measure the effect of a training sample on the target objective, and we therefore turn to surrogate measures of progress.",3. Learning Progress Signals,[0],[0]
"Broadly, these measures are either 1) loss-driven, in the sense that they equate progress with a decrease in some loss; or 2) complexity-driven, when they equate progress with an increase in model complexity.
",3. Learning Progress Signals,[0],[0]
"Training proceeds as follows: at each time t, we first sample a task index k ∼ πt.",3. Learning Progress Signals,[0],[0]
We then generate a sample from this task x ∼ Dk.,3. Learning Progress Signals,[0],[0]
"Note that each x is in general a batch of training sequences, and that in order to reduce noise in the gain signal we draw the whole batch from a single task.",3. Learning Progress Signals,[0],[0]
"We compute the chosen measure of learning progress ν then divide by the time τ(x) required to process the sample (since it is the rate of progress we are concerned with, and processing time may vary from task to task) to get the raw reward r̂ = ν/τ(x)",3. Learning Progress Signals,[0],[0]
"For the purposes of this work, τ(x) was simply the length of the longest input sequence in x; for other tasks or architectures a more complex calculation may be required.",3. Learning Progress Signals,[0],[0]
We then rescale r̂ into a reward rt ∈,3. Learning Progress Signals,[0],[0]
"[−1, 1], and provide it to Exp3.S.",3. Learning Progress Signals,[0],[0]
"The procedure is summarized as Algorithm 1.
",3. Learning Progress Signals,[0],[0]
"Algorithm 1 Automated Curriculum Learning
Initially: wi = 0 for i ∈",3. Learning Progress Signals,[0],[0]
"[N ]
for t = 1 . . .",3. Learning Progress Signals,[0],[0]
T do π(k) := (1− ),3. Learning Progress Signals,[0],[0]
e wk∑,3. Learning Progress Signals,[0],[0]
"i e wi
+ N Draw task index k from π Draw training batch x from Dk Train network pθ on x Compute learning progress ν",3. Learning Progress Signals,[0],[0]
(Sections 3.1 & 3.2),3. Learning Progress Signals,[0],[0]
Map r̂ = ν/τ(x) to r ∈,3. Learning Progress Signals,[0],[0]
"[−1, 1] (Section 2.3) Update wi with reward r using Exp3.S (1)
end for",3. Learning Progress Signals,[0],[0]
"We consider five loss-driven progress signals, all which compare the predictions made by the model before and after training on some sample x.",3.1. Loss-driven Progress,[0],[0]
"The first two signals we
present are instantaneous in the sense that they only depend on x. Such signals are appealing because they are typically cheaper to evaluate, and are agnostic about the overall goal of the curriculum.",3.1. Loss-driven Progress,[0],[0]
"The remaining three signals more directly measure the effect of training on the desired objective, but require an additional sample x′.",3.1. Loss-driven Progress,[0],[0]
"In what follows we denote the model parameters before and after training on x by θ and θ′ respectively.
",3.1. Loss-driven Progress,[0],[0]
Prediction gain (PG).,3.1. Loss-driven Progress,[0],[0]
"Prediction gain is defined as the instantaneous change in loss for a sample x, before and after training on x:
νPG := L(x, θ)− L(x, θ′).
",3.1. Loss-driven Progress,[0],[0]
"For Bayesian mixture models, prediction gain upper bounds the model’s information gain (Bellemare et al., 2016), and is therefore closely related to the Bayesian precept that learning is a change in posterior.
",3.1. Loss-driven Progress,[0],[0]
Gradient prediction gain (GPG).,3.1. Loss-driven Progress,[0],[0]
Computing prediction gain requires an additional forward pass.,3.1. Loss-driven Progress,[0],[0]
"When pθ is differentiable, an alternative is to consider the first-order Taylor series approximation to prediction gain:
L(x, θ′)",3.1. Loss-driven Progress,[0],[0]
"≈ L(x, θ) +",3.1. Loss-driven Progress,[0],[0]
"[∇L(x, θ)]>∆θ,
where ∆θ is the descent step.",3.1. Loss-driven Progress,[0],[0]
"Taking this step to be the negative gradient −∇θL(x, θ) we obtain the gradient prediction gain
νGPG",3.1. Loss-driven Progress,[0],[0]
:,3.1. Loss-driven Progress,[0],[0]
"= ‖∇L(x, θ)‖22.
",3.1. Loss-driven Progress,[0],[0]
"This measures the magnitude of the gradient vector, which has been used an indicator of salience in the active learning literature (Settles et al., 2008).",3.1. Loss-driven Progress,[0],[0]
"We show in Section 3.3 that gradient prediction gain is a biased estimate of true expected learning progress, and in particular favours tasks whose loss has higher variance.
",3.1. Loss-driven Progress,[0],[0]
Self prediction gain (SPG).,3.1. Loss-driven Progress,[0],[0]
"Prediction gain is a biased estimate of the change in Lk(θ), the expected loss on task k.",3.1. Loss-driven Progress,[0],[0]
"Having trained on x, we naturally expect the sample loss L(x, θ) to decrease, even though the loss at other points may increase.",3.1. Loss-driven Progress,[0],[0]
"Self prediction gain addresses this issue by sampling a second time from the same task and estimating progress on the new sample:
νSPG := L(x ′, θ)− L(x′, θ′)",3.1. Loss-driven Progress,[0],[0]
"x′ ∼ Dk.
",3.1. Loss-driven Progress,[0],[0]
Target prediction gain (TPG).,3.1. Loss-driven Progress,[0],[0]
"We can take the selfprediction gain idea further and evaluate directly on the loss of interest, which has has also been considered in active learning (Roy and Mccallum, 2001).",3.1. Loss-driven Progress,[0],[0]
"In the target task setting, this becomes
νTPG := L(x ′, θ)− L(x′, θ′) x′ ∼ DN .
",3.1. Loss-driven Progress,[0],[0]
"Although this might seem like the most accurate measure so far, it tends to suffer from high variance, and also runs counter to the premise that, early in training, the model cannot improve on the difficult target task and should instead train on a task that it can master.
",3.1. Loss-driven Progress,[0],[0]
Mean prediction gain (MPG).,3.1. Loss-driven Progress,[0],[0]
"Mean prediction gain is the analogue of target prediction gain in the multiple tasks setting, where it is natural to evaluate our progress across all tasks.",3.1. Loss-driven Progress,[0],[0]
"We write
νMPG := L(x ′, θ)− L(x′, θ′) x′ ∼ Dk, k ∼ UN ,
where UN denotes the uniform distribution over {1, . . .",3.1. Loss-driven Progress,[0],[0]
", N}.",3.1. Loss-driven Progress,[0],[0]
Mean prediction gain has additional variance from sampling an evaluation task k ∼ UN .,3.1. Loss-driven Progress,[0],[0]
"So far we have considered gains that gauge the network’s learning progress directly, by observing the change in its predictive ability.",3.2. Complexity-driven Progress,[0],[0]
We now turn to a novel set of gains that instead measure the rate at which the network’s complexity increases.,3.2. Complexity-driven Progress,[0],[0]
"These gains are underpinned by the Minimum Description Length principle (MDL; Rissanen, 1986; Grünwald, 2007): namely that in order to best generalise from a particular dataset, one should minimise the number of bits required to describe the model parameters plus the number of bits required for the model to describe the data.",3.2. Complexity-driven Progress,[0],[0]
The MDL principle makes it explicit that increasing the complexity of the model by a certain amount is only worthwhile if it reduces the data cost by a greater amount.,3.2. Complexity-driven Progress,[0],[0]
We would therefore expect the training examples that induce it to do so to correspond to salient data from which it is able to generalise.,3.2. Complexity-driven Progress,[0],[0]
"These examples are exactly what we seek when attempting to maximise learning progress.
",3.2. Complexity-driven Progress,[0],[0]
"MDL training for neural networks (Hinton and Van Camp, 1993) can be practically realised with stochastic variational inference (Graves, 2011; Kingma et al., 2015; Blundell et al., 2015).",3.2. Complexity-driven Progress,[0],[0]
"In this framework a variational posterior Pφ(θ) over the network weights is maintained during training, with a single weight sample drawn for each training example.",3.2. Complexity-driven Progress,[0],[0]
"The parameters φ of the posterior are optimised, rather than θ itself.",3.2. Complexity-driven Progress,[0],[0]
"The total loss is the expected log-loss of the training dataset1 (which in our case is the complete curriculum), plus the KL-divergence between the posterior and some fixed (Blundell et al., 2015) or adaptive (Graves, 2011)",3.2. Complexity-driven Progress,[0],[0]
"prior Qψ(θ):
LV I(φ, ψ) = KL(Pφ ‖Qψ)︸ ︷︷ ︸ model complexity
+ ∑
k ∑ x∈Dk E θ∼Pφ
L(x, θ)︸ ︷︷ ︸ data cost .
",3.2. Complexity-driven Progress,[0],[0]
"1MDL deals with sets rather than distributions; in this context we consider each Dk in the curriculum to be a dataset sampled from the task distribution, rather than the distribution itself
Following (Graves, 2011)",3.2. Complexity-driven Progress,[0],[0]
we used an adaptive prior with two free parameters: a mean and variance that are reused for every network weight.,3.2. Complexity-driven Progress,[0],[0]
Since we are using stochastic gradient descent we need to determine the per-sample loss for both the model complexity and the data.,3.2. Complexity-driven Progress,[0],[0]
"Defining S :=∑ k |Dk| as the total number of samples in the curriculum we obtain
LV I(x, φ, ψ) := 1
S KL(Pφ ‖Qψ) + E θ∼Pφ L(x, θ), (3)
with LV I(φ, ψ) = ∑",3.2. Complexity-driven Progress,[0],[0]
"k ∑ x∼Dk LV I(x, φ, ψ).",3.2. Complexity-driven Progress,[0],[0]
"Some of the curricula we consider are algorithmically generated, meaning that the total number of samples is undefined.",3.2. Complexity-driven Progress,[0],[0]
The treatment suggested by the MDL principle is to divide the complexity cost by the total number of samples generated so far.,3.2. Complexity-driven Progress,[0],[0]
However we simplified matters by setting S to a large constant that roughly matches the number of samples we expect to see during training.,3.2. Complexity-driven Progress,[0],[0]
"We used a diagonal Gaussian for both P and Q, allowing us to determine the complexity cost analytically:
KL(Pφ ‖Qψ) =",3.2. Complexity-driven Progress,[0],[0]
"(µφ − µψ)2 + σ2φ − σ2ψ
2σ2ψ +",3.2. Complexity-driven Progress,[0],[0]
"ln ( σψ σφ ) ,
where µφ, σ2φ and µψ, σ 2 ψ are the mean and variance vectors for Pφ and Qψ respectively.",3.2. Complexity-driven Progress,[0],[0]
"We adapted ψ with gradient descent along with φ, and the gradient of Eθ∼Pφ L(x, θ) with respect to φ was estimated using the reparameterisation trick2 (Kingma and Welling, 2013) with a single Monte-Carlo sample.",3.2. Complexity-driven Progress,[0],[0]
"The SoftPlus function y = ln(1 + ex) was used to ensure that the variances were positive (Blundell et al., 2015).
",3.2. Complexity-driven Progress,[0],[0]
Variational complexity gain (VCG).,3.2. Complexity-driven Progress,[0],[0]
"The increase of model complexity induced by a training example can be estimated from the change in complexity following a single parameter update from φ to φ′ and ψ to ψ′, yielding
νV CG := KL(Pφ′ ‖Qψ′)−KL(Pφ ‖Qψ)
",3.2. Complexity-driven Progress,[0],[0]
Gradient variational complexity gain (GVCG).,3.2. Complexity-driven Progress,[0],[0]
"As with prediction gain, we can derive a first order Taylor approximation using the direction of gradient descent:
KL(Pφ′ ‖Qψ′)",3.2. Complexity-driven Progress,[0],[0]
"≈ KL(Pφ ‖Qψ)
",3.2. Complexity-driven Progress,[0],[0]
"− [∇φ,ψKL(Pφ ‖Qψ)]>∇ψ,φLMDL(x, φ, ψ)
=⇒ νV",3.2. Complexity-driven Progress,[0],[0]
CG ≈ C,3.2. Complexity-driven Progress,[0],[0]
"− [∇φ,ψKL(Pφ ‖Qψ)]>∇φ E θ∼Pφ L(x, θ),
2The reparameterisation trick yields a better gradient estimator for the posterior variance than that used in (Graves, 2011), which requires either calculation of the diagonal of the Hessian, or a biased approximation using the empirical Fisher.",3.2. Complexity-driven Progress,[0],[0]
"The gradient estimator for the posterior mean is the same in both cases.
where C is a term that does not depend on x and is therefore irrelevant to the gain signal.",3.2. Complexity-driven Progress,[0],[0]
"We define gradient variational complexity gain as
νGV CG :=",3.2. Complexity-driven Progress,[0],[0]
"[∇φ,ψKL(Pφ ‖Qψ)]>∇φ E θ∼Pφ L(x, θ),
which is the directional derivative of the KL along the gradient descent direction.",3.2. Complexity-driven Progress,[0],[0]
"We believe that the linear approximation is more reliable here than for prediction gain, as the model complexity has less curvature than the loss surface.
",3.2. Complexity-driven Progress,[0],[0]
Relationship to VIME.,3.2. Complexity-driven Progress,[0],[0]
"Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016), uses a reward signal that is closely related to variational complexity gain.",3.2. Complexity-driven Progress,[0],[0]
"The difference is that while VIME measures the KL between the posterior before and after a step in parameter space, VCG considers the change in KL between the posterior and prior induced by the step.",3.2. Complexity-driven Progress,[0],[0]
"Therefore, while VIME looks for any change to the posterior, VCG focuses only on changes that alter the divergence from the prior.",3.2. Complexity-driven Progress,[0],[0]
"Further research will be needed to assess the relative merits of the two signals.
L2 gain (L2G).",3.2. Complexity-driven Progress,[0],[0]
"Variational inference tends to slow down learning, making it appealing to define a complexity-based progress signal applicable to more conventionally trained networks.",3.2. Complexity-driven Progress,[0],[0]
"Many of the standard neural network regularisation terms, such as Lp-norms, can be viewed as defining an upper bound on model description length (Graves, 2011).",3.2. Complexity-driven Progress,[0],[0]
We therefore hypothesize that the increase in regularisation cost will be indicative of the increase in model complexity.,3.2. Complexity-driven Progress,[0],[0]
"To test this hypothesis we consider training with a standard L2 regularisation term added to the loss:
LL2(x, θ) = L(x, θ) + α
2 ‖θ‖22 (4)
where α is an empirically chosen constant.",3.2. Complexity-driven Progress,[0],[0]
"In this case the complexity gain can be defined as
νL2G := ‖θ′‖22",3.2. Complexity-driven Progress,[0],[0]
"− ‖θ‖22 (5)
where we have dropped the α/2 term as the gain will anyway be rescaled to [−1, 1] before use.",3.2. Complexity-driven Progress,[0],[0]
"The corresponding first-order approximation is
νGL2G := [θ] >∇θL(x, θ) (6)
It is possible to calculate L2 gain for unregularized networks; however we found this an unreliable signal, presumably because the network has no incentive to decrease complexity when faced with uninformative data.",3.2. Complexity-driven Progress,[0],[0]
"Prediction gain, self prediction gain and gradient prediction gain are all closely related, but incur varying degrees of
bias and variance.",3.3. Prediction Gain Bias,[0],[0]
"We now present a formal analysis of the biases present in each, noting that a similar treatment can be applied to our complexity gains.
",3.3. Prediction Gain Bias,[0],[0]
"We first assume that the lossL is locally well-approximated by its first-order Taylor expansion:
L(x, θ′)",3.3. Prediction Gain Bias,[0],[0]
"≈ L(x, θ) +∇L(x, θ)>∆θ (7)
where ∆θ := θ′",3.3. Prediction Gain Bias,[0],[0]
− θ.,3.3. Prediction Gain Bias,[0],[0]
"For ease of exposition, we also suppose the network is trained with stochastic gradient descent (the same argument leads to similar conclusions for higherorder optimization methods):
∆θ := −α∇L(x, θ).",3.3. Prediction Gain Bias,[0],[0]
"(8)
We define the true expected learning progress as
ν",3.3. Prediction Gain Bias,[0],[0]
":= E x′∼D
[L(θ)− L(θ′)]",3.3. Prediction Gain Bias,[0],[0]
"= α ∥∥ E
x′∼D ∇L(x, θ) ∥∥2, with the identity following from (8) (recall that L(θ) = Ex L(θ)).",3.3. Prediction Gain Bias,[0],[0]
"The expected prediction gain is then
νPG = E x′∼D",3.3. Prediction Gain Bias,[0],[0]
"[L(x, θ)− L(x, θ′)]",3.3. Prediction Gain Bias,[0],[0]
"= α E x′∼D ∥∥∇L(x, θ)∥∥2.",3.3. Prediction Gain Bias,[0],[0]
"Defining
V ( ∇L(x, θ) )",3.3. Prediction Gain Bias,[0],[0]
":= E ∥∥∇L(x, θ)− E∇L(x′, θ)‖2, we find that prediction gain is the sum of two terms: true expected learning progress, plus the gradient variance:
νPG = ν + V ( ∇L(x, θ) ) .
",3.3. Prediction Gain Bias,[0],[0]
"We conclude that for equal learning progress, a prediction gain-based curriculum maximizes variance.",3.3. Prediction Gain Bias,[0],[0]
"The problem is made worse when using gradient prediction gain, which actually relies on the Taylor approximation (7).",3.3. Prediction Gain Bias,[0],[0]
"On the other hand, self prediction gain is an unbiased estimate of expected learning progress:
",3.3. Prediction Gain Bias,[0],[0]
"E x νSPG = E x,x′∼D",3.3. Prediction Gain Bias,[0],[0]
"[L(x′, θ)− L(x′, θ′)]",3.3. Prediction Gain Bias,[0],[0]
"= ν.
Naturally, its use of two samples results in higher variance than prediction gain, suggesting a bias-variance trade off between the two estimates.",3.3. Prediction Gain Bias,[0],[0]
"To test our approach, we applied all the gains defined in the previous section to three task suites: synthetic language modelling on text generated by n-gram models, repeat copy (Graves et al., 2014) and the bAbI tasks (Weston et al., 2015)
",4. Experiments,[0],[0]
"The network architecture was stacked unidirectional LSTM (Graves, 2013) for all experiments, and the training
loss was cross-entropy with either categorical targets and softmax output, or Bernoulli targets and sigmoid outputs, optimised by RMSProp with momentum (Tieleman, 2012; Graves, 2013), using a momentum of 0.9 and a learning rate of 10−5 unless specified otherwise.",4. Experiments,[0],[0]
"The parameters for the Exp3.S algorithm were η = 10−3, β = 0, = 0.05.",4. Experiments,[0],[0]
"For all experiments, one set of networks was trained with variational inference (VI) to test the variational complexity gain signals, and another set was trained with normal maximum likelihood (ML) for the other signals.",4. Experiments,[0],[0]
All experiments were repeated 10 times with different random initialisations of the network weights.,4. Experiments,[0],[0]
The α regularisation parameter from Eq.,4. Experiments,[0],[0]
(4) for the networks trained with L2 gain signals was 10−4 for all experiments.,4. Experiments,[0],[0]
"For all plots with a time axis, time is defined as the total number of input steps processed so far.",4. Experiments,[0],[0]
"In the absence of hand-designed curricula for these tasks, our performance benchmarks are 1) a fixed uniform policy over all the tasks and 2) directly training on the target task (where applicable).",4. Experiments,[0],[0]
All losses and error rates are measured on independent samples not used for training or reward calculation.,4. Experiments,[0],[0]
"For our first experiment, we trained character-level KneserNey n-gram models (Kneser and Ney, 1995) on the King James Bible data from the Canterbury corpus (Arnold and Bell, 1997), with the maximum depth parameter n ranging between 0 to 10.",4.1. N-Gram Language Modelling,[0],[0]
"We then used each model to generate a separate dataset of 1M characters, which we divided into disjoint sequences of 150 characters.",4.1. N-Gram Language Modelling,[0],[0]
"The first 50 characters of each sequence were used as burn-in context for the next 100, which the network was trained to predict.",4.1. N-Gram Language Modelling,[0],[0]
"The LSTM network had two layers of 512 cells, and the batch size was 32.
",4.1. N-Gram Language Modelling,[0],[0]
"An important characteristic of this dataset is that the amount of linguistic structure increases monotonically with n. Simultaneously, the entropy – and hence, minimum achievable loss – decreases almost monotonically in n. If we believe that learning progress should be higher for interesting data than for data that is difficult to predict, we would expect the gain signals to be drawn to higher n: they should favour structure over noise.",4.1. N-Gram Language Modelling,[0],[0]
"Note that in this case the curriculum is superfluous: the most efficient strategy for learning the 10-gram source is to directly train on it.
",4.1. N-Gram Language Modelling,[0],[0]
"Fig. 1 shows that most of the complexity-based gain signals from Section 3.2 (L2G, GL2G, GVCG) progress rapidly through the curriculum before focusing strongly on the 10- gram task (though interestingly, GVCG appears to revisit 0- gram later on in training).",4.1. N-Gram Language Modelling,[0],[0]
"The clarity of the result is striking, given that sequences generated from models beyond about 6-gram are difficult to distinguish by eye.",4.1. N-Gram Language Modelling,[0],[0]
"VCG follows a similar path, but with much less confidence, presum-
ably due to the increased noise.",4.1. N-Gram Language Modelling,[0],[0]
"The loss-based measures (PG, GPG, SPG, TG) also tend to move towards higher n, although more slowly and with less certainty.",4.1. N-Gram Language Modelling,[0],[0]
"Unlike the complexity gains, they tend to initially favour the lower-n tasks, which may be desirable as we would expect early learning to be more efficient with simpler data.",4.1. N-Gram Language Modelling,[0],[0]
"In the repeat copy task (Graves et al., 2014)",4.2. Repeat Copy,[0],[0]
"the network receives an input sequence of random bit vectors, and is then asked to output that sequence a given number of times.",4.2. Repeat Copy,[0],[0]
"The task has two main dimensions of difficulty: the length of the input sequence and the required number of repeats, both of which increase the demand on the models memory.",4.2. Repeat Copy,[0],[0]
"Neural Turing machines are able to learn a ‘for-loop’ like algorithm on simple examples that can directly generalise to much harder examples (Graves et al., 2014).",4.2. Repeat Copy,[0],[0]
"For LSTM networks without access to external memory, however, significant retraining is required to adapt to harder tasks.
",4.2. Repeat Copy,[0],[0]
"We devised a curriculum with both the sequence length and the number of repeats varying from 1 to 13, giving 169 tasks in all, with length 13, repeats 13 defined as the target task.",4.2. Repeat Copy,[0],[0]
"The LSTM network had a single layer of 512 cells,
and the batch size was 32.",4.2. Repeat Copy,[0],[0]
"As the data was generated online, the number of samples S in Eq.",4.2. Repeat Copy,[0],[0]
"(3) (the per-sample VI loss) was undefined; we arbitrarily set it to 169M (1M per task in the curriculum).
",4.2. Repeat Copy,[0],[0]
"Fig. 2 shows that GVCG solves the target task about twice as fast as uniform sampling for VI training, and that the PG, SPG and TPG gains are somewhat faster than uniform for ML training, especially in the early stages.",4.2. Repeat Copy,[0],[0]
From the entropy plots it is clear that these signals all lead to strongly non-uniform policies.,4.2. Repeat Copy,[0],[0]
"The VI complexity curves also demonstrate that GVCG yields significantly higher network complexity than uniform sampling, supporting our hypothesis that increased complexity correlates with learning progress.",4.2. Repeat Copy,[0],[0]
"Unlike GVCG, the VCG signal did not deviate far from a uniform policy.",4.2. Repeat Copy,[0],[0]
"L2G and particularly GPG and GL2G were much worse than uniform, suggesting that (1) the bias induced by the gradient approximation has a pernicious effect on learning and (2) that the increase in L2 norm is not a reliable measure of increased network complexity.",4.2. Repeat Copy,[0],[0]
"Training directly on the target task failed to learn
at all, underlining the necessity of curriculum learning for this problem.
",4.2. Repeat Copy,[0],[0]
"Fig. 3 reveals a consistent strategy for the GVCG syllabuses, first focusing on short sequences with high repeats, then long sequences with low repeats, thereby decoupling the two dimensions of difficulty.",4.2. Repeat Copy,[0],[0]
At each stage the loss is substantially reduced across many tasks that the policy does not focus on.,4.2. Repeat Copy,[0],[0]
"Crucially, this means that the network does not have to visit each of the 169 tasks to solve them all, and the syllabus is able to exploit this fact to more efficiently complete the curriculum.",4.2. Repeat Copy,[0],[0]
"The bAbI dataset (Weston et al., 2015) consists of 20 synthetic question-answering problems designed to probe the basic reasoning capabilities of machine learning models.",4.3. Babi,[0],[0]
"Although bAbI was not specifically designed for curriculum learning, some of the tasks follow a natural ordering of complexity (e.g. ‘Two Arg Relations’, ‘Three Arg Relations’) and all are based on a consistent probabilistic grammar, leading us to hope that an efficient syllabus could be found for learning the whole set.",4.3. Babi,[0],[0]
"The usual performance measure for bAbI is the number of tasks ‘completed’ by the model, where completion is defined as getting less than 5% of the test set questions wrong.
",4.3. Babi,[0],[0]
"The data representation followed (Graves et al., 2016), with each word in the observation and target sequences represented as a 1-hot vector, along with an extra binary channel to mark answer prompts.",4.3. Babi,[0],[0]
"The original datasets were small, with either 1K or 10K questions per task, so as to test generalisation from limited samples.",4.3. Babi,[0],[0]
"However LSTM is known to perform poorly in this setting (Sukhbaatar et al., 2015; Graves et al., 2016), and we wished to avoid the confounding effect of overfitting on curriculum learning.",4.3. Babi,[0],[0]
"We therefore used the bAbI code (Weston et al., 2015) to generate 1M stories (each containing one or more questions) for each of the 20 tasks.",4.3. Babi,[0],[0]
"With so many examples, we found that training and evaluation set performance were indistinguishable, and therefore report training performance only.",4.3. Babi,[0],[0]
"The LSTM network had two layer of 512 cells, the batch size was 16, and the RMSProp learning rate was 3× 10−5.
",4.3. Babi,[0],[0]
"Fig. 4 shows that prediction gain (PG) clearly improved on uniform sampling in terms of both learning speed and number of tasks completed; for self-prediction gain (SPG) the same benefits were visible, though less pronounced.",4.3. Babi,[0],[0]
The other gains were either roughly equal to or worse than uniform.,4.3. Babi,[0],[0]
"For variational inference training, GVCG was faster than uniform at first, then slightly worse later on, while VCG performed poorly for reasons that are unclear to us.",4.3. Babi,[0],[0]
"In general, training with variational inference appeared to hamper progress on the bAbI tasks.
",4.3. Babi,[0],[0]
Fig. 5 shows how the PG bandit accelerates the network’s progress by selectively focusing on specific tasks until completion.,4.3. Babi,[0],[0]
"For example, the bandit solves ‘Time Reasoning’ much faster than uniform sampling by concentrating on it early in training, and later focuses strongly on ‘Path Finding’ (one of the harder bAbI tasks) until completion.",4.3. Babi,[0],[0]
"Also noteworthy is the way the bandit progresses from ‘Single Supporting Fact’ to ‘three Supporting Facts’ in order (albeit while completing other tasks), showing that it can discover implicit orderings, and hence opportunities for efficient transfer, in an unsorted curriculum.",4.3. Babi,[0],[0]
"Our experiments suggest that using a stochastic syllabus to maximise learning progress can lead to significant gains
in curriculum learning efficiency, so long as a suitable progress signal is used.",5. Conclusion,[0],[0]
We note however that uniformly sampling from all tasks is a surprisingly strong benchmark.,5. Conclusion,[0],[0]
"We speculate that this is because learning is dominated by gradients from the tasks on which the network is making fastest progress, inducing a kind of implicit curriculum, albeit with the inefficiency of unnecessary samples.",5. Conclusion,[0],[0]
"For maximium likelihood training, we found prediction gain to be the most consistent signal, while for variational inference training, gradient variational complexity gain performed best.",5. Conclusion,[0],[0]
"Importantly, both are instantaneous, in the sense that they can be evaluated using only the samples used for training.",5. Conclusion,[0],[0]
"As well as being more efficient, this has broader applicability to tasks where external evaluation is difficult, and suggest that learning progress is best assessed on a local, rather than global basis.",5. Conclusion,[0],[0]
"The authors thank their colleagues at DeepMind for their excellent feedback, in particular Oriol Vinyals, Simon Osindero, and Guy Lever.",Acknowledgements,[0],[0]
"Last but not least, many thanks to Pierre-Yves Oudeyer for fruitful discussions that helped shape this work.",Acknowledgements,[0],[0]
"We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency.",abstractText,[0],[0]
"A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus.",abstractText,[0],[0]
"We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity.",abstractText,[0],[0]
"Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.",abstractText,[0],[0]
Automated Curriculum Learning for Neural Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 793–805 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1074",text,[0],[0]
Grammatical Error Correction (GEC) systems are often only evaluated in terms of overall performance because system hypotheses are not annotated.,1 Introduction,[0],[0]
"This can be misleading however, and a system that performs poorly overall may in fact outperform others at specific error types.",1 Introduction,[0],[0]
This is significant because a robust specialised system is actually more desirable than a mediocre general system.,1 Introduction,[0],[0]
"Without an error type analysis however, this information is completely unknown.
",1 Introduction,[0],[0]
The main aim of this paper is hence to rectify this situation and provide a method by which parallel error correction data can be automatically annotated with error type information.,1 Introduction,[0],[0]
"This not only facilitates error type evaluation, but can also be used to provide detailed error type feedback
to non-native learners.",1 Introduction,[0],[0]
"Given that different corpora are also annotated according to different standards, we also attempted to standardise existing datasets under a common error type framework.
",1 Introduction,[0],[0]
Our approach consists of two main steps.,1 Introduction,[0],[0]
"First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016) and second, we classify them according to a new, rule-based framework that relies solely on dataset-agnostic information such as lemma and part-of-speech.",1 Introduction,[0],[0]
"We demonstrate the value of our approach, which we call the ERRor ANnotation Toolkit (ERRANT)1, by carrying out a detailed error type analysis of each system in the CoNLL-2014 shared task on grammatical error correction (Ng et al., 2014).
",1 Introduction,[0],[0]
"It is worth mentioning that despite an increased interest in GEC evaluation in recent years (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015; Sakaguchi et al., 2016), ERRANT is the only toolkit currently capable of producing error types scores.",1 Introduction,[0],[0]
The first stage of automatic annotation is edit extraction.,2 Edit Extraction,[0],[0]
"Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits.",2 Edit Extraction,[0],[0]
"This is fundamentally an alignment problem:
1https://github.com/chrisjbryant/errant
793
The first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align parallel original and corrected sentences.",2 Edit Extraction,[0],[0]
"As the Levenshtein distance only aligns individual tokens however, they also merged all adjacent nonmatches in an effort to capture multi-token edits.",2 Edit Extraction,[0],[0]
"Xue and Hwa (2014) subsequently improved on Swanson and Yamangil’s work by training a maximum entropy classifier to predict whether edits should be merged or not.
",2 Edit Extraction,[0],[0]
"Most recently, Felice et al. (2016) proposed a new method of edit extraction using a linguistically-enhanced alignment algorithm supported by a set of merging rules.",2 Edit Extraction,[0],[0]
"More specifically, they incorporated various linguistic information, such as part-of-speech and lemma, into the cost function of the Damerau-Levenshtein2 algorithm to make it more likely that tokens with similar linguistic properties aligned.",2 Edit Extraction,[0],[0]
"This approach ultimately proved most effective at approximating human edits in several datasets (80-85% F1), and so we use it in the present study.",2 Edit Extraction,[0],[0]
"Having extracted the edits, the next step is to assign them error types.",3 Automatic Error Typing,[0],[0]
"While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora.",3 Automatic Error Typing,[0],[0]
"For example, a classifier trained on the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is unlikely to perform as well on the National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier and Ng, 2012) or vice versa, because both corpora have been annotated according to different standards (cf. Xue and Hwa (2014)).",3 Automatic Error Typing,[0],[0]
"Instead, a dataset-agnostic error type classifier is much more desirable.",3 Automatic Error Typing,[0],[0]
"To solve this problem, we took inspiration from Swanson and Yamangil’s (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags.",3.1 A Rule-Based Error Type Framework,[0],[0]
"We then added another rule to similarly differentiate between Missing, Unnecessary and Replace-
2Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB→BA
ment errors depending on whether tokens were inserted, deleted or substituted.",3.1 A Rule-Based Error Type Framework,[0],[0]
"Finally, we extended our approach to classify errors that are not well-characterised by POS, such as Spelling or Word Order, and ultimately assigned all error types based solely on automatically-obtained, objective properties of the data.
",3.1 A Rule-Based Error Type Framework,[0],[0]
"In total, we wrote roughly 50 rules.",3.1 A Rule-Based Error Type Framework,[0],[0]
"While many of them are very straightforward, significant attention was paid to discriminating between different kinds of verb errors.",3.1 A Rule-Based Error Type Framework,[0],[0]
"For example, despite all having the same correction, the following sentences contain different types of common learner errors:
(a) He IS asleep now.",3.1 A Rule-Based Error Type Framework,[0],[0]
[IS→ is]: orthography (b),3.1 A Rule-Based Error Type Framework,[0],[0]
He iss asleep now.,3.1 A Rule-Based Error Type Framework,[0],[0]
[iss→ is]: spelling (c),3.1 A Rule-Based Error Type Framework,[0],[0]
He has asleep now.,3.1 A Rule-Based Error Type Framework,[0],[0]
[has→ is]: verb (d) He being asleep now.,3.1 A Rule-Based Error Type Framework,[0],[0]
[being→ is]: form (e),3.1 A Rule-Based Error Type Framework,[0],[0]
He was asleep now.,3.1 A Rule-Based Error Type Framework,[0],[0]
[was→ is]: tense (f),3.1 A Rule-Based Error Type Framework,[0],[0]
He are asleep now.,3.1 A Rule-Based Error Type Framework,[0],[0]
"[are→ is]: SVA
To handle these cases, we hence wrote the following ordered rules:
1.",3.1 A Rule-Based Error Type Framework,[0],[0]
Are the lower case forms of both sides of the edit the same?,3.1 A Rule-Based Error Type Framework,[0],[0]
"(a)
2. Is the original token a real word?",3.1 A Rule-Based Error Type Framework,[0],[0]
"(b)
3.",3.1 A Rule-Based Error Type Framework,[0],[0]
Do both sides of the edit have the same lemma?,3.1 A Rule-Based Error Type Framework,[0],[0]
"(c)
4.",3.1 A Rule-Based Error Type Framework,[0],[0]
Is one side of the edit a gerund (VBG) or participle (VBN)?,3.1 A Rule-Based Error Type Framework,[0],[0]
"(d)
5.",3.1 A Rule-Based Error Type Framework,[0],[0]
Is one side of the edit in the past tense (VBD)?,3.1 A Rule-Based Error Type Framework,[0],[0]
"(e)
6.",3.1 A Rule-Based Error Type Framework,[0],[0]
Is one side of the edit in the 3rd person present tense (VBZ)?,3.1 A Rule-Based Error Type Framework,[0],[0]
"(f)
While the final three rules could certainly be reordered, we informally found the above sequence performed best during development.",3.1 A Rule-Based Error Type Framework,[0],[0]
"It is also worth mentioning that this is a somewhat simplified example and that there are additional rules to discriminate between auxiliary verbs, main verbs and multi verb expressions.",3.1 A Rule-Based Error Type Framework,[0],[0]
"Nevertheless, the above case exemplifies our approach, and a more complete description of all rules is provided with the software.",3.1 A Rule-Based Error Type Framework,[0],[0]
"One of the key strengths of a rule-based approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset independent and does not require labelled training data.",3.2 A Dataset-Agnostic Classifier,[0],[0]
"This is in contrast with machine learning approaches which not only learn dataset specific biases, but also presuppose the existence of sufficient quantities of training data.
",3.2 A Dataset-Agnostic Classifier,[0],[0]
A second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned a particular error category.,3.2 A Dataset-Agnostic Classifier,[0],[0]
"In contrast, human and machine learning classification decisions are often much less transparent.
",3.2 A Dataset-Agnostic Classifier,[0],[0]
"Finally, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.",3.2 A Dataset-Agnostic Classifier,[0],[0]
"The prerequisites for our rule-based classifier are that each token in both the original and corrected
sentence is POS tagged, lemmatized, stemmed and dependency parsed.",3.3 Automatic Markup,[0],[0]
"We use spaCy3 v1.7.3 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK.4 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy’s Penn Treebank style tags to the coarser set of Universal Dependency tags.5 We use the latest Hunspell GB-large word list6 to help classify non-word errors.",3.3 Automatic Markup,[0],[0]
The marked-up tokens in an edit span are then input to the classifier and an error type is returned.,3.3 Automatic Markup,[0],[0]
The complete list of 25 error types in our new framework is shown in Table 2.,3.4 Error Categories,[0],[0]
"Note that most of them can be prefixed with ‘M:’, ‘R:’ or ‘U:’, depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable
3https://spacy.io/ 4http://www.nltk.org/ 5http://universaldependencies.org/tagset-conversion/
en-penn-uposf.html 6https://sourceforge.net/projects/wordlist/files/speller/ 2017.01.22/
evaluation at different levels of granularity (see Appendix A for all valid combinations).",3.4 Error Categories,[0],[0]
"This means we can choose to evaluate, for example, only replacement errors (anything prefixed by ‘R:’), only noun errors (anything suffixed with ‘NOUN’) or only replacement noun errors (‘R:NOUN’).",3.4 Error Categories,[0],[0]
"This flexibility allows us to make more detailed observations about different aspects of system performance.
",3.4 Error Categories,[0],[0]
"One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could→ should] a tense error, when it might otherwise be considered a modal error.",3.4 Error Categories,[0],[0]
"The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can → should] and [has eaten → should eat], which might be considered a more complex combination of modal and tense error.",3.4 Error Categories,[0],[0]
"As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality.",3.4 Error Categories,[0],[0]
"As our new error scheme is based solely on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance.",3.5 Classifier Evaluation,[0],[0]
"For this reason, we instead carried out a small-scale manual evaluation, where we simply asked 5 GEC researchers to rate the appropriateness of the predicted error types for 200 randomly chosen edits in context (100 from FCE-test and 100 from CoNLL-2014) as “Good”, “Acceptable” or “Bad”.",3.5 Classifier Evaluation,[0],[0]
"“Good’ meant the chosen type was the most appropriate for the given edit, “Acceptable” meant the chosen type was appropriate, but probably not optimum, while “Bad” meant the chosen type was not appropriate for the edit.",3.5 Classifier Evaluation,[0],[0]
"Raters were warned that the edit boundaries had been determined automatically and hence might be unusual, but that they should focus on the appropriateness of the error type regardless of whether they agreed with the boundary or not.
",3.5 Classifier Evaluation,[0],[0]
"It is worth stating that the main purpose of this evaluation was not to evaluate the specific strengths and weaknesses of the classifier, but rather ascertain how well humans believed the predicted error types characterised each edit.",3.5 Classifier Evaluation,[0],[0]
"GEC is known to be a highly subjective task (Bryant and
Ng, 2015) and so we were more interested in overall judgements than specific disagreements.
",3.5 Classifier Evaluation,[0],[0]
The results from this evaluation are shown in Table 3.,3.5 Classifier Evaluation,[0],[0]
"Significantly, all 5 raters considered at least 95% of the predicted error types to be either “Good” or “Acceptable”, despite the degree of noise introduced by automatic edit extraction.",3.5 Classifier Evaluation,[0],[0]
"Furthermore, whenever raters judged an edit as “Bad”, this could usually be traced back to a POS or parse error; e.g. [ring → rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the POS tagger considered both sides of the edit nouns or verbs.",3.5 Classifier Evaluation,[0],[0]
"Interannotator agreement was also good at 0.724 κfree (Randolph, 2005).
",3.5 Classifier Evaluation,[0],[0]
"In contrast, although incomparable on account of the different metric and error scheme, the best results using machine learning were between 50- 70% F1 (Felice et al., 2016).",3.5 Classifier Evaluation,[0],[0]
"Ultimately however, we believe the high scores awarded by the raters validates the efficacy of our rule-based approach.",3.5 Classifier Evaluation,[0],[0]
"Having described how to automatically annotate parallel sentences with ERRANT, we now also have a method to annotate system hypotheses; this is the first step towards an error type evaluation.",4 Error Type Scoring,[0],[0]
"Since no scorer is currently capable of calculating error type performance however (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015), we instead built our own.
",4 Error Type Scoring,[0],[0]
"Fortunately, one benefit of explicitly annotating system hypotheses is that it makes evaluation much more straightforward.",4 Error Type Scoring,[0],[0]
"In particular, for each sentence, we only need to compare the edits in the hypothesis against the edits in each respective reference and measure the overlap.",4 Error Type Scoring,[0],[0]
"Any edit with the same span and correction in both files is hence a
true positive (TP), while unmatched edits in the hypothesis and references are false positives (FP) and false negatives (FN) respectively.",4 Error Type Scoring,[0],[0]
"These results can then be grouped by error type for the purposes of error type evaluation.
",4 Error Type Scoring,[0],[0]
"Finally, it is worth noting that this scorer is much simpler than other scorers in GEC which typically incorporate edit extraction or alignment directly into their algorithms.",4 Error Type Scoring,[0],[0]
"Our approach, on the other hand, treats edit extraction and evaluation as separate tasks.",4 Error Type Scoring,[0],[0]
"Before evaluating an automatically annotated hypothesis against its reference, we must also address another mismatch: namely that hypothesis edits must be extracted and classified automatically, while reference edits are typically extracted and classified manually using a different framework.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"Since evaluation is now reduced to a straightforward comparison between two files however, it is especially important that the hypothesis and references are both processed in the same way.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"For instance, a hypothesis edit [have eating → has eaten] will not match the reference edits [have → has] and [eating → eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.
",4.1 Gold Reference vs. Auto Reference,[0],[0]
"To solve this problem, we can reprocess the references in the same way as the hypotheses.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"In other words, we can apply ERRANT to the references such that each reference edit is subject to the same automatic extraction and classification criteria as each hypothesis edit.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"While it may seem unorthodox to discard gold reference information in favour of automatic reference information, this is necessary to minimise the difference between hypothesis and reference edits and also standardise error type annotations.
",4.1 Gold Reference vs. Auto Reference,[0],[0]
"To show that automatic references are feasible alternatives to gold references, we evaluated each team in the CoNLL-2014 shared task using both types of reference with the M2 scorer (Dahlmeier and Ng, 2012), the de facto standard of GEC evaluation, and our own scorer.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"Table 4 hence shows that there is little difference between the overall scores for each team, and we formally validated this hypothesis for precision, recall and F0.5 by means of bootstrap significance testing (Efron and Tibshirani, 1993).",4.1 Gold Reference vs. Auto Reference,[0],[0]
"Ultimately, we found no statistically significant difference
between automatic and gold references (1,000 iterations, p > .05) which leads us to conclude that our automatic references are qualitatively as good as human references.",4.1 Gold Reference vs. Auto Reference,[0],[0]
"Despite using the same metric, Table 4 also shows that the M2 scorer tends to produce slightly higher F0.5 scores than our own.",4.2 Comparison with the M2 Scorer,[0],[0]
"This initially led us to believe that our scorer was underestimating performance, but we subsequently found that instead the M2 scorer tends to overestimate performance (cf. Felice and Briscoe (2015) and Napoles et al. (2015)).
",4.2 Comparison with the M2 Scorer,[0],[0]
"In particular, given a choice between matching [have eating → has eaten] from Annotator 1 or [have → has] and [eating → eaten] from Annotator 2, the M2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one.",4.2 Comparison with the M2 Scorer,[0],[0]
"Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other,7 it merges them and treats them as one false positive; e.g. [is a cat → are a cats] is selected over [is→ are] and [cat → cats] even though these edits are best handled separately.",4.2 Comparison with the M2 Scorer,[0],[0]
"In other words, the M2 scorer exploits its dynamic edit boundary prediction to artificially maximise true positives and minimise false positives and hence produce slightly inflated scores.
",4.2 Comparison with the M2 Scorer,[0],[0]
7The distance is controlled by the max unchanged words parameter which is set to 2 by default.,4.2 Comparison with the M2 Scorer,[0],[0]
"To demonstrate the value of ERRANT, we applied it to the data produced in the CoNLL-2014 shared task (Ng et al., 2014).",5 CoNLL-2014 Shared Task Analysis,[0],[0]
"Specifically, we automatically annotated all the system hypotheses and official reference files.8",5 CoNLL-2014 Shared Task Analysis,[0],[0]
"Although ERRANT can be applied to any dataset of parallel sentences, we chose to evaluate on CoNLL-2014 because it represents the largest collection of publicly available GEC system output.",5 CoNLL-2014 Shared Task Analysis,[0],[0]
"For more information about the systems in CoNLL-2014, we refer the reader to the shared task paper.",5 CoNLL-2014 Shared Task Analysis,[0],[0]
"In our first category experiment, we simply investigated the performance of each system in terms of Missing, Replacement and Unnecessary edits.",5.1 Edit Operation,[0],[0]
"The results are shown in Table 5 with additional information in Appendix B, Table 10.
",5.1 Edit Operation,[0],[0]
"The most surprising result is that five teams (AMU, IPN, PKU, RAC, UFC) failed to correct any unnecessary token errors at all.",5.1 Edit Operation,[0],[0]
This is noteworthy because unnecessary token errors account for roughly 25% of all errors in the CoNLL-2014 test data and so failing to address them significantly limits a system’s maximum performance.,5.1 Edit Operation,[0],[0]
"While the reason for this is clear in some cases, e.g. UFC’s rule-based system was never designed to tackle unnecessary tokens (Gupta, 2014), it is less clear in others, e.g. there is no obvious reason why AMU’s SMT system failed to learn when
8http://www.comp.nus.edu.sg/∼nlp/conll14st.html
to delete tokens (Junczys-Dowmunt and Grundkiewicz, 2014).",5.1 Edit Operation,[0],[0]
"AMU’s result is especially remarkable given that their system still came 3rd overall despite this limitation.
",5.1 Edit Operation,[0],[0]
"In contrast, CUUI’s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB’s hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors.",5.1 Edit Operation,[0],[0]
"It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016).",5.1 Edit Operation,[0],[0]
"Table 6 shows precision, recall and F0.5 for each of the error types in our proposed framework for each team in CoNLL-2014.",5.2 General Error Types,[0],[0]
"As some error types are more common than others, we also provide the TP, FP and FN counts used to make this table in Appendix B, Table 11.
",5.2 General Error Types,[0],[0]
"Overall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU, who scored highest in 6 categories.",5.2 General Error Types,[0],[0]
"All but 3 teams (IITB, IPN and POST) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types.",5.2 General Error Types,[0],[0]
"Only CAMB attempted to correct at least 1 error from every category.
",5.2 General Error Types,[0],[0]
"Other interesting observations we can make from this table include:
• Despite the prevalence of spell checkers nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance.
",5.2 General Error Types,[0],[0]
"• Although several teams built specialised classifiers for DET and PREP errors, CAMB’s hybrid MT approach still outperformed them.",5.2 General Error Types,[0],[0]
"This might be because the classifiers were trained using a different error type framework however.
",5.2 General Error Types,[0],[0]
• CUUI’s classifiers significantly outperformed all other approaches at ORTH and VERB:FORM errors.,5.2 General Error Types,[0],[0]
"This suggests classifiers are well-suited to these error types.
",5.2 General Error Types,[0],[0]
"• Although UFC’s rule-based approach was the best at VERB:SVA errors, CUUI’s classifier was not very far behind.
",5.2 General Error Types,[0],[0]
"• Only AMU managed to correct any CONJ errors.
",5.2 General Error Types,[0],[0]
"• Content word errors (i.e. ADJ, ADV, NOUN and VERB) were unsurprisingly very difficult for all teams.",5.2 General Error Types,[0],[0]
"In addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail.",5.3 Detailed Error Types,[0],[0]
"For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation.",5.3 Detailed Error Types,[0],[0]
"Note that this is a representative example of detailed error type performance, as an analysis of all error type combinations for all teams would take up too much space.
",5.3 Detailed Error Types,[0],[0]
"While CAMB’s hybrid MT approach achieved a higher score than CUUI’s classifier overall, our more detailed evaluation reveals that CUUI actually outperformed CAMB at Replacement Determiner errors.",5.3 Detailed Error Types,[0],[0]
We also learn that CAMB scored twice as highly on M:DET and U:DET than it did on R:DET and that CUUI’s significantly higher U:DET recall was offset by a lower precision.,5.3 Detailed Error Types,[0],[0]
"Ultimately, this shows that even though one approach might be better than another overall, different approaches may still have complementary strengths.",5.3 Detailed Error Types,[0],[0]
Another benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size.,5.4 Multi Token Errors,[0],[0]
"Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that has at least two tokens on either side.",5.4 Multi Token Errors,[0],[0]
"In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).
",5.4 Multi Token Errors,[0],[0]
"In general, teams did not do well at multi-token edits.",5.4 Multi Token Errors,[0],[0]
"In fact only three teams achieved scores greater than 10% F0.5 and all of them used MT (AMU, CAMB, UMC).",5.4 Multi Token Errors,[0],[0]
"This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016).",5.4 Multi Token Errors,[0],[0]
"If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available.",5.4 Multi Token Errors,[0],[0]
Another important aspect of GEC that is seldom reported in the literature is that of error detection; i.e. the extent to which a system can identify erroneous tokens in text.,5.5 Detection vs. Correction,[0],[0]
"This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011).
",5.5 Detection vs. Correction,[0],[0]
Figure 1 hence shows how each team’s score for detection differed in relation to their score for correction.,5.5 Detection vs. Correction,[0],[0]
"While CAMB scored highest for detection overall, it is interesting to note that CUUI ultimately performed slightly better than CAMB at correction.",5.5 Detection vs. Correction,[0],[0]
This suggests CUUI was more successful at correcting the errors they detected than CAMB.,5.5 Detection vs. Correction,[0],[0]
"In contrast, IPN and PKU are notable for detecting significantly more errors than they were able to correct.",5.5 Detection vs. Correction,[0],[0]
"Nevertheless, a system’s ability to detect errors, even if it is unable to correct them, is still likely to be valuable information to a learner (Rei and Yannakoudakis, 2016).
",5.5 Detection vs. Correction,[0],[0]
"Finally, although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection.",5.5 Detection vs. Correction,[0],[0]
"In this paper, we described ERRANT, a grammatical ERRor ANnotation Toolkit designed to au-
tomatically annotate parallel error correction data with explicit edit spans and error type information.",6 Conclusion,[0],[0]
"ERRANT can be used to not only facilitate a detailed error type evaluation in GEC, but also to standardise existing error correction corpora and reduce annotator workload.",6 Conclusion,[0],[0]
"We release ERRANT with this paper.
",6 Conclusion,[0],[0]
Our approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits.,6 Conclusion,[0],[0]
"This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas.",6 Conclusion,[0],[0]
"A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either “Good” (85%) or “Acceptable” (10%).
",6 Conclusion,[0],[0]
We demonstrated the value of ERRANT by carrying out a detailed evaluation of system error type performance for all teams in the CoNLL2014 shared task on Grammatical Error Correction.,6 Conclusion,[0],[0]
We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.,6 Conclusion,[0],[0]
"Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated.",abstractText,[0],[0]
"To overcome this problem, we introduce ERRANT, a grammatical ERRor ANnotation Toolkit designed to automatically extract edits from parallel original and corrected sentences and classify them according to a new, dataset-agnostic, rulebased framework.",abstractText,[0],[0]
"This not only facilitates error type evaluation at different levels of granularity, but can also be used to reduce annotator workload and standardise existing GEC datasets.",abstractText,[0],[0]
"Human experts rated the automatic edits as “Good” or “Acceptable” in at least 95% of cases, so we applied ERRANT to the system output of the CoNLL-2014 shared task to carry out a detailed error type analysis for the first time.",abstractText,[0],[0]
Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction,title,[0],[0]
"Data analysis problems often involve pre-processing raw data, which is a tedious and time-demanding task due to several reasons: i) raw data is often unstructured and largescale; ii) it contains errors and missing values; and iii) documentation may be incomplete or not available.",1. Introduction,[0],[0]
"As a consequence, as the availability of data increases, so does the interest of the data science community to automate this process.",1. Introduction,[0],[0]
"In particular, there are a growing body of work which focuses on automating the different stages of data pre-processing, including data cleaning (Hellerstein, 2008), data wrangling (Kandel et al., 2011) and data integration and fusion (Dong & Srivastava, 2013).
",1. Introduction,[0],[0]
"The outcome of data pre-processing is commonly a structured dataset, in which the objects are described by a set of attributes.",1. Introduction,[0],[0]
"However, before being able to proceed with the predictive analytics step of the data analysis process, the data scientist often needs to identify which kind of variables (i.e., real-values, categorical, ordinal, etc.)",1. Introduction,[0],[0]
these attributes represent.,1. Introduction,[0],[0]
"This labeling of the data is necessary to select the appropriate machine learning approach to ex-
1University of Cambridge, Cambridge, United Kingdom; 2Uber AI Labs, San Francisco, California, USA.",1. Introduction,[0],[0]
"Correspondence to: Isabel Valera <miv24@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"plore, find patterns or make predictions on the data.",1. Introduction,[0],[0]
"As an example, a prediction task is solved differently depending on the kind of data to be predicted—e.g., while prediction on categorical variables is usually formulated as a classification task, in the case of ordinal variables it is formulated as an ordinal regression problem (Agresti, 2010).",1. Introduction,[0],[0]
"Moreover, different data types should be pre-processed and input differently in the predictive tool—e.g., categorical inputs are often transformed into as many binary inputs (which state whether the object belongs to a category or not) as number of categories; positive real inputs might be logtransformed, etc.
",1. Introduction,[0],[0]
"Information on the statistical data types in a dataset becomes particularly important in the context of statistical machine learning (Breiman, 2001), where the choice of a likelihood model appears as a main assumption.",1. Introduction,[0],[0]
"Although extensive work has focused on model selection (Ando, 2010; Burnham & Anderson, 2003), the likelihood model is usually assumed to be known and fixed.",1. Introduction,[0],[0]
"As an example, a common approach is to model continuous data as Gaussian variables, and discrete data as categorical variables.",1. Introduction,[0],[0]
"However, while extensive work has shown the advantages of capturing the statistical properties of the observed data in the likelihood model (Chu & Ghahramani, 2005a; Schmidt et al., 2009; Hilbe, 2011; Valera & Ghahramani, 2014), there still exists a lack of tools to automatically perform likelihood model selection, or equivalently to discover the most plausible statistical type of the variables in the data, directly from the data.
",1. Introduction,[0],[0]
"In this work, we aim to fill this gap by proposing a general and scalable Bayesian method to solve this task.",1. Introduction,[0],[0]
"The proposed method exploits the latent structure in the data to automatically distinguish among real-valued, positive realvalued and interval data as types of continuous variables, and among categorical, ordinal and count data as types of discrete variables.",1. Introduction,[0],[0]
"The proposed method is based on probabilistic modeling and exploits the following key ideas:
i)",1. Introduction,[0],[0]
There exists a latent structure in the data that capture the statistical dependencies among the different objects and attributes in the dataset.,1. Introduction,[0],[0]
"Here, as in standard latent feature modeling, we assume that we can capture this structure by a low-rank representation, such that conditioning on it, the likelihood model factorizes for both number of objects and attributes.
",1. Introduction,[0],[0]
ii),1. Introduction,[0],[0]
"The observation model for each attribute can be ex-
pressed as a mixture of likelihood models, one per each considered data type, where the inferred weight associated to a likelihood model captures the probability of the attribute belonging to the corresponding data type.
",1. Introduction,[0],[0]
We derive an efficient MCMC inference algorithm to jointly infer both the low-rank representation and the weight of each likelihood model for each attribute in the observed data.,1. Introduction,[0],[0]
"Our experimental results show that the proposed method accurately discovers the true data type of the variables in a dataset, and by doing so, it fits the data substantially better than modeling continuous data as Gaussian variables and discrete data as categorical variables.",1. Introduction,[0],[0]
"As stated above, the outcome of the pre-processing step of data analysis is a structured dataset, in which a set of objects are defined by a set of attributes, and our objective is to automatically discover which type of variables these attributes correspond to.",2. Problem Statement,[0],[0]
"In order to distinguish between discrete and continuous variables, we can apply simple logic rules, e.g.. count the number of unique values that the attribute takes and how many times we observe these attributes.",2. Problem Statement,[0],[0]
"Moreover, binary variables are invariant to the labeling of the categories, and therefore, both categorical and ordinal models are equivalent in this case.",2. Problem Statement,[0],[0]
"However, distinguishing among different types of discrete and continuous variables cannot be easily solved using simple heuristics.
",2. Problem Statement,[0],[0]
"In the context of continuous variables, given the finite size of observed datasets, it is complicated to identify whether a variable may take values in the entire real line, or only on an interval of it, e.g., (0,∞) or (θL, θH).",2. Problem Statement,[0],[0]
"In other words, due to the finite observation sample, we cannot distinguish whether the data distribution has an infinite tail that we have not observed, or its support is limited to an interval.",2. Problem Statement,[0],[0]
"As an illustrative example, Figures 2(d)&(f) in Section 4 show two data distributions that, although at a first sight look similar, correspond respectively to a Beta variable, which therefore takes values in the interval (0, 1), and a gamma variable, which takes values in (0,∞).",2. Problem Statement,[0],[0]
"In the context of discrete data, it is impossible to tell the difference between categorical and ordinal variables in isolation.",2. Problem Statement,[0],[0]
The presence of an order in the data only makes sense given a context.,2. Problem Statement,[0],[0]
"As an example, while colors in M&Ms usually do not present an order, colors in a traffic light clearly do.",2. Problem Statement,[0],[0]
"Similarly, we cannot easily distinguish between ordinal data (which take values in a finite ordered set) and count data (which take values in an infinite ordered set with equidistant values) due to two main reasons.",2. Problem Statement,[0],[0]
"First, similarly to continuous variables, since datasets contain a finite number of examples, it is difficult to tell whether we have observed the finite set of possible values of a variable, or simply a finite subsample of an infinite set.",2. Problem Statement,[0],[0]
"Second, we would
need access to exact information on whether its consecutive values are equidistant or not, however, this information depends on how the data have been gathered.",2. Problem Statement,[0],[0]
"For example, an attribute that collects information on “frequency of an action” will correspond to an ordinal variable if its categories belong to, e.g., {“never”, “sometimes”, “usually”, “often”}, and to a count variable if it takes values in {‘‘0 times per week”, “1 time per week”, . . .}.",2. Problem Statement,[0],[0]
"Previous work (Hernandez-Lobato et al., 2014) proposed to distinguish between categorical and ordinal data by comparing the model evidence and the predictive test loglikelihood of ordinal and categorical models.",2. Problem Statement,[0],[0]
"However, this approach can be only used to distinguish between ordinal and categorical data, and it does so by assuming that it has access to a real-valued variable that contains information about the presence of an ordering in the observed discrete (ordinal or categorical) variable.",2. Problem Statement,[0],[0]
"As a consequence, it cannot be easily generalizable to label the data type of all the variables (or attributes) in a dataset.",2. Problem Statement,[0],[0]
"In contrast, in this paper we proposed a general method that allows us to distinguish among real-valued, positive real-valued and interval data as types of continuous variables, and among categorical, ordinal and count data as types of discrete variables.",2. Problem Statement,[0],[0]
"Moreover, the general framework we present can be readily extended to other data types as needed.",2. Problem Statement,[0],[0]
"In this section, we introduce a Bayesian method to determine the statistical type of variable that corresponds to each of the attributes describing the objects in an observation matrix X. In particular, we propose a probabilistic model, in which we assume that there exists a low-rank representation of the data that captures its latent structure, and therefore, the statistical dependencies among its objects and attributes.",3. Methodology,[0],[0]
"In detail, we consider that each observation xdn can be explained by a K-length vector of latent variables zn =",3. Methodology,[0],[0]
"[zn1, . . .",3. Methodology,[0],[0]
", znK ] associated to the n-th object and a weighting vector bd =",3. Methodology,[0],[0]
"[bd1, . . .",3. Methodology,[0],[0]
", b d K ] (with K being the number of latent variables), whose elements bdk weight the contribution of k-th the latent feature to the d-th attribute in X. Then, given the latent low-rank representation of the data, the attributes describing the objects in a dataset are assumed to be independent, i.e.,
p(X|Z, {bd}Dd=1) = D∏ d=1 p(xd|Z, bd),
where we gather the latent feature vectors zn in a N ×K matrix Z. For convenience, here zn is a K-length row vector, while bd is a K-length column vector.",3. Methodology,[0],[0]
"The above model resembles standard latent feature models (Salakhutdinov & Mnih, 2007; Griffiths & Ghahramani, 2011), which assume known and fixed likelihood models p(xd|Z, bd).",3. Methodology,[0],[0]
"In contrast, in this paper we aim to infer the statistical data type
(or equivalently, the likelihood model) that better captures the distribution of each attribute in X. To this end, here we assume that the likelihood model of the d-th attribute in X is a mixture of likelihood functions such that
p(xd|Z, {bd`}`∈Ld) = ∑ `∈Ld wd` p`(x d|Z, bd` ),
where Ld is the set of possible types of variables (or equivalently, likelihood models) to be considered for this attribute, and the weight wd` captures the probability of the likelihood function ` in the d-th attribute of the observation matrix X. Note that, the above expression is a valid likelihood model as long as ∑ `∈Ld w d ` = 1 and each p`(x d|Z, bd` ,Ψd` ) is a normalized probability density function or probability mass function for, respectively, continuous and discrete variables.",3. Methodology,[0],[0]
"Hence, under the proposed model, which is is illustrated in Figure 1a, the likelihood factorizes as
p(X|Z, {bd`}) = D∏",3. Methodology,[0],[0]
"d=1 ∑ `∈Ld wd` p`(x d|Z, bd` ).",3. Methodology,[0],[0]
"(1)
We place a Dirichlet prior distribution on the likelihood weights wd =",3. Methodology,[0],[0]
"[wd` ]`∈Ld , and similarly to (Salakhutdinov & Mnih, 2007), assume that both the latent feature vectors zn and the weighting vectors bdj are Gaussian distributed with zero mean and covariance matrices σ2zI and σ 2 b",3. Methodology,[0],[0]
"I, respectively.",3. Methodology,[0],[0]
"Here, I denotes the identity matrix of size equal to the number of latent features K.
Moreover, we consider the following types of data for, respectively, continuous and discrete variables: • Continuous variables:
1.",3. Methodology,[0],[0]
"Real-valued data, which takes values in the real line, i.e., xdn ∈ <.",3. Methodology,[0],[0]
2.,3. Methodology,[0],[0]
"Positive real-valued data, which takes values in the positive real line, i.e., xdn ∈",3. Methodology,[0],[0]
<,3. Methodology,[0],[0]
+.,3. Methodology,[0],[0]
"3. Interval data, which takes values in an interval of the real line, i.e., xdn ∈ (θL, θH), where θL, θH ∈ < and θL ≤ θH .",3. Methodology,[0],[0]
• Discrete variables: 1.,3. Methodology,[0],[0]
"Categorical data, which takes values in a finite
unordered set, e.g., xdn ∈ {‘blue’, ‘red’, ‘black’}.",3. Methodology,[0],[0]
2.,3. Methodology,[0],[0]
"Ordinal data, which takes values in a finite or-
dered set, e.g., xdn ∈ {‘never’, ‘sometimes’, ‘often’, ‘usually’, ‘always’}.",3. Methodology,[0],[0]
3.,3. Methodology,[0],[0]
"Count data, which takes values in the natural numbers, i.e., xdn ∈ {0, . . .",3. Methodology,[0],[0]
",∞}.
",3. Methodology,[0],[0]
"We remark that the main goal of this paper is to determine the types of variables that better capture each attribute in the observed matrix X, which in our method translates to inferring the likelihood weights wd.",3. Methodology,[0],[0]
"However, solving this inference problem in an efficient way is a challenging task for several reasons.",3. Methodology,[0],[0]
"First, we need to jointly infer all the latent variables in the model, i.e., the low-rank representation
of the data (which includes the latent feature matrix Z and the corresponding weighting vectors {bd`}{`∈Ld|d=1,...,D}) and the likelihood weights {wd}Dd=1.",3. Methodology,[0],[0]
"Second, we need to do so given a heterogeneous (and non-conjugate) observation model, which combines D different likelihood models, corresponding each of them to a mixture of likelihood functions and coupled through the latent feature matrix Z. Additionally, these likelihood functions do not only correspond to either a probability density function or a probability mass function depending on whether we are dealing with a continuous or a discrete variable, but also each mixture combines likelihood functions with different supports.",3. Methodology,[0],[0]
"For example, while real-valued data lead to a likelihood function with the real line as support, interval data only accounts for a segment of the real line.",3. Methodology,[0],[0]
"Similarly, both categorical and ordinal data assume a finite support, while count data requires an infinite-support likelihood function.
",3. Methodology,[0],[0]
"In order to allow for efficient inference, we exploit the key idea in (Valera & Ghahramani, 2014) to propose an alternative and equivalent model representation (shown in Figure 1b), which efficiently deal with heterogeneous likelihood functions.",3. Methodology,[0],[0]
"In this alternative model representation, we include for each observation xdn as many Gaussian variables (or pseudo-observations) ydn` ∼ N (znbd` , σ2y) as the number of likelihood functions in Ld, and assume that there exists a transformation function over the variables ydn` which maps the real line< into the support of the likelihood function `, Ω`, i.e.,
f` : < 7→ Ω` y → x .",3. Methodology,[0],[0]
"(2)
Note that, if we condition on the pseudo-observations the latent variable model behaves as a conjugate Gaussian model, allowing for efficient inference of the latent feature matrix Z and the weighting vectors {bd`}.",3. Methodology,[0],[0]
"Additionally, we include a latent multinomial variable sdn ∼ Multinomial(wd) which indicates the type of variable (or likelihood function) that the observation xdn belongs to.",3. Methodology,[0],[0]
"Then, given sdn, we can obtain the observation x d n as
xdn = fsdn(y d nsdn + udn), (3)
where udn ∼ N (0, σ2u) is a noise variable.",3. Methodology,[0],[0]
We gather the likelihood assignments sdn in a N ×D matrix S.,3. Methodology,[0],[0]
"In this section, we provide the set of transformations to map from the Gaussian pseudo-observations ydn` into the types of data defined above, specifying also the six likelihood functions that our method will account for.",3.1. Likelihood functions,[0],[0]
"In the case of continuous variables, we assume that the mapping functions f` are continuous invertible and differentiable functions, such that we can obtain corresponding likelihood function (after integrating out the pseudoobservation ydn`) as
p`(x",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"d n|zn, bd` , sdn = `) = 1√ 2π(σ2y",3.1.1. CONTINUOUS VARIABLES,[0],[0]
+,3.1.1. CONTINUOUS VARIABLES,[0],[0]
σ,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"2 u) ∣∣∣∣ ddxdn f−1` (xdn) ∣∣∣∣
× exp {",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"− 1
2(σ2y + σ 2 u)
(f−1` (x d n)− znbd` )2
} ,
where f−1` is the inverse function of the transformation f`(·), i.e., f−1` (f`(v))",3.1.1. CONTINUOUS VARIABLES,[0],[0]
=,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"v. Next, we provide examples of mapping functions that allow us to account for real-valued, positive real-valued, and interval data.
1.",3.1.1. CONTINUOUS VARIABLES,[0],[0]
Real-valued Data.,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"In order to obtain real-valued observations, i.e., xdn ∈ <, we need a transformation over ydn that maps from the real numbers to the real numbers, i.e., f< : < → <.",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"The simplest case is to assume that x = f<(y + u) = y + u, and therefore, each observation is distributed as xdn ∼ N (znbd<, σ2y + σ2u).",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"Nevertheless, other mapping functions can be used, e.g., we will use in our experiments the transformation
x = f<(y",3.1.1. CONTINUOUS VARIABLES,[0],[0]
+ u) =,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"w(y + u) + µ,
where w and µ are parameters allowing attribute rescaling, and tuneable by the user.
",3.1.1. CONTINUOUS VARIABLES,[0],[0]
2.,3.1.1. CONTINUOUS VARIABLES,[0],[0]
Positive Real-valued Data.,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"As an example of a function that maps from the real numbers to the positive real numbers, i.e., f<+ : < 7→",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"<+, we consider
x = f<+(y + u) = log(1 + exp(w(y + u))).
",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"where w allows attribute rescaling.
3.",3.1.1. CONTINUOUS VARIABLES,[0],[0]
Interval Data.,3.1.1. CONTINUOUS VARIABLES,[0],[0]
"As an example of a function the maps from the real numbers into the interval (θL, θH), i.e., f<+ : < 7→ (θL, θH), we consider the transformation
x = fInt(y + u) = θH",3.1.1. CONTINUOUS VARIABLES,[0],[0]
"− θL
1 + exp(−w(y + u)) + θL,
where w, θL and θH are user hyperparameters.1 1In our experiments, we assume θL = arg minn(xdn)− and θH = arg maxn(x d n)+ , where → 0 is a user hyper-parameter.",3.1.1. CONTINUOUS VARIABLES,[0],[0]
We set the rescaling parameter w = 2/max(xd) for the three continuous data types.,3.1.1. CONTINUOUS VARIABLES,[0],[0]
1.,3.1.2. DISCRETE VARIABLES,[0],[0]
Categorical Data.,3.1.2. DISCRETE VARIABLES,[0],[0]
"Now we account for categorical observations, i.e., each observation xdn can take values in the unordered index set {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
", Rd}.",3.1.2. DISCRETE VARIABLES,[0],[0]
"Hence, assuming a multinomial probit model, we can write
x = fcat(y) = arg max r∈{1,...,Rd} y(r),
where in this case there are as many pseudo-observations as number of categories and each pseudo-observation can be sampled as ydncat(r) ∼ N (znbdcat(r), σ2y) where bdcat(r) denotes the K-length weighting vector, which weights the influence the latent features for a categorical observation xdn taking value r. Note that, under this likelihood model, we need one pseudo-observation ydncat(r) and a weighting vector bdcat(r) for each possible value of the observation r ∈ {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
", Rd}.",3.1.2. DISCRETE VARIABLES,[0],[0]
"Under the multinomial probit model, we can obtain the probability of xdn taking value r ∈ {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
", Rd} as (Girolami & Rogers, 2005)
pcat(x = r|zn, bdcat, sdn = cat)
= Ep(u)",3.1.2. DISCRETE VARIABLES,[0],[0]
"[ Rd∏ r′=1 r′ 6=r Φ ( u+ zn(b d cat(r)− bdcat(r′)) )] ,
where Φ(·) denotes the cumulative density function of the standard normal distribution and Ep(u)",3.1.2. DISCRETE VARIABLES,[0],[0]
"[·] denotes expectation with respect to the distribution p(u) = N (0, σ2y).",3.1.2. DISCRETE VARIABLES,[0],[0]
2. Ordinal Data.,3.1.2. DISCRETE VARIABLES,[0],[0]
"Consider ordinal data, in which each element xdn takes values in the ordered index set {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
", Rd}.",3.1.2. DISCRETE VARIABLES,[0],[0]
"Then, assuming an ordered probit model, we can write
xdn = ford(y d nord) =  1 if ydnord ≤ θd1 2 if θd1 < y d nord ≤",3.1.2. DISCRETE VARIABLES,[0],[0]
"θd2
...",3.1.2. DISCRETE VARIABLES,[0],[0]
Rd if θdRd−1,3.1.2. DISCRETE VARIABLES,[0],[0]
"< y d nord
where again ydnord is Gaussian distributed with mean znb d ord and variance σ 2 y , and θ d r for r ∈ {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
",",3.1.2. DISCRETE VARIABLES,[0],[0]
Rd − 1} are the thresholds that divide the real line into Rd regions.,3.1.2. DISCRETE VARIABLES,[0],[0]
"We assume the thresholds θdr are sequentially generated from the truncated Gaussian distribution θdr ∼ T N (0, σ2θ , θdr−1,∞), where θd0 = −∞ and θdRd = +∞. As opposed to the categorical case, now we have a unique weighting vector bdord and a unique Gaussian variable y d nord for each observation xdn, and the value of x d n is determined by the region in which ydnord falls.
",3.1.2. DISCRETE VARIABLES,[0],[0]
"Under the ordered probit model (Chu & Ghahramani, 2005b), the probability of each element xdn taking value r ∈ {1, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
", Rd} can be written as
pord(x d n = r|zn, bdord, sdn = ord)
= Φ
( θdr − znbdord
σy
)",3.1.2. DISCRETE VARIABLES,[0],[0]
"− Φ ( θdr−1 − znbdord
σy
) .
",3.1.2. DISCRETE VARIABLES,[0],[0]
3.,3.1.2. DISCRETE VARIABLES,[0],[0]
Count Data.,3.1.2. DISCRETE VARIABLES,[0],[0]
"In count data each observation xdn takes non-negative integer values, i.e., xdn ∈ {0, . . .",3.1.2. DISCRETE VARIABLES,[0],[0]
",∞}.",3.1.2. DISCRETE VARIABLES,[0],[0]
"Then, we assume
xdn = fcount(y d n) = bg(ydn)c,
where bvc returns the floor of v, that is the largest integer that does not exceed v, and g : < → <+ is a monotonic differentiable function, in our experiments g(y) = log(1 + exp(wy)).",3.1.2. DISCRETE VARIABLES,[0],[0]
"We can thus write the likelihood function as
pcount(x d n|zn, bdord, sdn = count) =
Φ
( g−1(xdn + 1)− znbdcount
σy
)",3.1.2. DISCRETE VARIABLES,[0],[0]
"− Φ ( g−1(xdn)− znbdcount
σy ) where g−1 : <+ → < is the inverse function of the transformation g(·).",3.1.2. DISCRETE VARIABLES,[0],[0]
"Here, we exploit the model representation in Figure 1b to derive an efficient inference algorithm that allows us to infer all the latent variables in the model, providing as output the likelihood weights wd, which determine the probability of the d-th attribute in X belonging to each of the above data types.",3.2. Inference Algorithm,[0],[0]
"Algorithm 1 summarizes the inference.
",3.2. Inference Algorithm,[0],[0]
Sampling low-rank decomposition.,3.2. Inference Algorithm,[0],[0]
"In order to sample the latent feature matrix Z and the associated weighting vectors {bd`}, we condition on the pseudo-observations such that we can efficiently sample the feature vectors as zn ∼ N ( µdz ,Σz ) , where
Σz = (∑d
d=1 ∑ `∈Ld b d ` (b d ` )",3.2. Inference Algorithm,[0],[0]
>,3.2. Inference Algorithm,[0],[0]
+ σ−2z,3.2. Inference Algorithm,[0],[0]
"I )−1 and µz =
Σz (∑N n ∑ `∈Ld b d `y d n` ) .",3.2. Inference Algorithm,[0],[0]
Note that this step involves a matrix inversion of size K (the number of latent features) per iteration of the algorithm.,3.2. Inference Algorithm,[0],[0]
"Similarly, the weighting vectors can be sampled as bd` ∼ N ( µd` ,Σb ) , where Σb =(
σ−2y Z >Z + σ−2b I )−1",3.2. Inference Algorithm,[0],[0]
and µd` = Σb (∑N n z,3.2. Inference Algorithm,[0],[0]
> n y d n` ) .,3.2. Inference Algorithm,[0],[0]
Since Σb is shared for all {bd`} with ` ∈ Ld and d = 1 . . .,3.2. Inference Algorithm,[0],[0]
", D, this step also involves one matrix inversions of size K per iteration of the algorithm.",3.2. Inference Algorithm,[0],[0]
Sampling pseudo-observations.,3.2. Inference Algorithm,[0],[0]
"Given the low-rank decomposition and the likelihood assignments S, we can sample each pseudo-observation ydn` from its prior distribution if sdn 6=",3.2. Inference Algorithm,[0],[0]
"`, and from its posterior distribution if sdn =",3.2. Inference Algorithm,[0],[0]
`.,3.2. Inference Algorithm,[0],[0]
"In the case of continuous variables, the posterior distribution of the pseudo-observation can be obtained as
p(ydn`|xdn, zn, bd` , sdn = `)",3.2. Inference Algorithm,[0],[0]
"= N ( ydn ∣∣∣∣µ̂y, σ̂2y) , where µ̂y = ( (znb d ` )
σ2y +
f−1` (x d n)
σ2u
) σ̂2y, and σ̂
2 y =(
1 σ2y + 1σ2u
)−1 .
",3.2. Inference Algorithm,[0],[0]
"In the case of discrete variables, the posterior distribution of the pseudo-observation can be computed as follows.
",3.2. Inference Algorithm,[0],[0]
Algorithm 1 Inference Algorithm.,3.2. Inference Algorithm,[0],[0]
"Input: X Initialize: S, {bd`} and {ydn`} 1: for each iteration do 2: Update Z given {bd`} and {ydn`}.",3.2. Inference Algorithm,[0],[0]
"3: for d = 1, . . .",3.2. Inference Algorithm,[0],[0]
", D do 4: for ` ∈",3.2. Inference Algorithm,[0],[0]
"Ld do 5: for n = 1, . . .",3.2. Inference Algorithm,[0],[0]
", N do 6:",3.2. Inference Algorithm,[0],[0]
"Sample {ydn`} given xdn, Z, {bd`} and sdn. 7: end for 8:",3.2. Inference Algorithm,[0],[0]
Sample {bd`} given Z and {ydn`} .,3.2. Inference Algorithm,[0],[0]
"9: for n = 1, . . .",3.2. Inference Algorithm,[0],[0]
", N do 10: Sample sdn given xdn, Z and {bd`}.",3.2. Inference Algorithm,[0],[0]
"11: end for 12: end for 13: Sample wd given S. 14: end for 15: end for Output: Likelihood weights wd.
1.",3.2. Inference Algorithm,[0],[0]
"For categorical observations: p(ydncat(r)|xdn = T, zn, bdcat, sdn = cat)
= { T N (znbdcat(r), σ2y,maxj 6=r(ydncat(j)),∞), r = T T N (znbdcat(r), σ2y,−∞, ydncat(T )), r 6= T
",3.2. Inference Algorithm,[0],[0]
"In words, if xdn = T = r we sample y d nr from a truncated Normal distribution with mean znb d cat(r), variance σ 2 y and truncated on the left by maxj 6=r(y d ncat(j)).",3.2. Inference Algorithm,[0],[0]
"Otherwise, we sample from a truncated Gaussian (with same mean and variance) truncated on the right by ydncat(r) with r = x d",3.2. Inference Algorithm,[0],[0]
n.,3.2. Inference Algorithm,[0],[0]
Note that sampling from the variables ydncat(r) corresponds to solve a multinomial probit regression problem.,3.2. Inference Algorithm,[0],[0]
"Hence, to achieve identifiability we assume, without loss of generality, that the regression function fRd(zn) is identically zero, and thus, we fix bd` (Rd) = 0.
2.",3.2. Inference Algorithm,[0],[0]
"For ordinal observations: p(ydnord|xdn = r, zn, bdord, sdn = ord) = T N",3.2. Inference Algorithm,[0],[0]
"(ydnord|znbdord, σ2y, θdr−1, θdr ).
",3.2. Inference Algorithm,[0],[0]
"Note that in this case, we also need to sample the values for the thresholds θdr with r = 1, . . .",3.2. Inference Algorithm,[0],[0]
", Rd − 1 as
p(θdr |ydnord) = T N",3.2. Inference Algorithm,[0],[0]
"(θdr |0, σ2θ , θmin, θmax), where θmin = max(θdr−1,maxn(y d nord|xdn = r)) and θmax = min(θ d r ,minn(y d nord|xdn = r+ 1)).",3.2. Inference Algorithm,[0],[0]
"In words, each θdr is constrained to be between θ",3.2. Inference Algorithm,[0],[0]
d r−1,3.2. Inference Algorithm,[0],[0]
"and θ d r+1, as well as to ensure that the pseudo-observations ydnord associated to the observations xdn = r and x d n = r+ 1 fall respectively at the left and at the right side of θdr .",3.2. Inference Algorithm,[0],[0]
"Since in this ordinal regression problem the thresholds {θr}Rdr=1 are unknown, we set θ1 to a fixed value in order to achieve identifiability.
3.",3.2. Inference Algorithm,[0],[0]
"For count observations: p(ydncount|xdn, zn, bdcount, sdn = count) = T N (ydncount|znbdcount, σ2y, g−1(xdn), g−1(xdn + 1)),
where g−1 : <+ → < is the inverse function of g, i.e., g−1(g(y))",3.2. Inference Algorithm,[0],[0]
"= y. Therefore, ydncount is sampled from a Gaussian truncated on the left by g−1(xdn) and on the right by g−1(xdn + 1).
",3.2. Inference Algorithm,[0],[0]
Sampling likelihood assignments.,3.2. Inference Algorithm,[0],[0]
"In order to improve the mixing properties of the sampler, when sampling {sdn} we integrate out the pseudo-observations {ydn`}.",3.2. Inference Algorithm,[0],[0]
"Then, the posterior probability of each observation being assigned to the likelihood model ` can be obtained as
p(sdn = `|wd,Z, {bd`}) = wd` p`(x d n|zn, bd` )∑
`′∈Ld w d `′ p`′(x d n|zn, bd`′)
.
",3.2. Inference Algorithm,[0],[0]
Sampling likelihood weights.,3.2. Inference Algorithm,[0],[0]
We assume the prior distribution on the vector wd to be a Dirichlet distribution with parameters {α`}`∈Ld .,3.2. Inference Algorithm,[0],[0]
"Then, by conjugacy, we can sample wd given the likelihood assignments S from a Dirichlet distribution with parameters {α` + ∑ n δ(s",3.2. Inference Algorithm,[0],[0]
"d n == `)}`∈Ld .
",3.2. Inference Algorithm,[0],[0]
Scalability.,3.2. Inference Algorithm,[0],[0]
"The overall complexity of Algorithm 1 is O(NDLmax + K3) per iteration, where N is the number of objects, D the number of attributes, Lmax the maximum number of considered data types (or likelihood models) and K the size of the low-rank representation.",3.2. Inference Algorithm,[0],[0]
"In all of our experiments, we ran the MCMC for 5000 iterations, which lasted 10-100 minutes depending on the dataset.",3.2. Inference Algorithm,[0],[0]
"In this section, we show that the proposed method is able to accurately discover the true statistical type of variables in synthetic datasets, where we have perfect knowledge of the distribution from which the data have been generated.
",4.1. Experiments on synthetic data,[0],[0]
"First, we focus on continuous variables by generating univariate datasets with 1, 000 observations sampled from a known probability density function, which corresponds to i) a Gaussian distribution when considering real-valued data; ii) a Gamma distribution for positive real-valued data; and iii) a (scaled) Beta distribution for interval data lying in the interval (0, θL) where θL takes values 0.1, 1 or 100.",4.1. Experiments on synthetic data,[0],[0]
"Figure 2 shows the distribution, by means of a boxplot,2 of the inferred likelihood weights wd for 10 independent simulations of Algorithm 1 with 500 iterations on 10 independent datasets generated with the parameters detailed in the figure.",4.1. Experiments on synthetic data,[0],[0]
"Reassuringly we observe that the proposed method identifies interval data as the most likely type of data for the three considered Beta distributions; moreover, as the tail of the Beta distribution increases, so does the weight given to the positive real-valued variables.",4.1. Experiments on synthetic data,[0],[0]
"This effect can be explained by the finite size of the dataset, since it is hard to determine whether the variable is limited to values smaller than θL, or we simply have not observed them in the finite set of observations.",4.1. Experiments on synthetic data,[0],[0]
A similar effect occurs when applying our method to data sampled from Gamma (Figure 2(e)-(h)),4.1. Experiments on synthetic data,[0],[0]
and Gaussian (Figure 2(i)-(l)) distributions.,4.1. Experiments on synthetic data,[0],[0]
"Here, we observe that in addition to, respectively, positive real-valued and real-valued data types, our model finds that the variable may also be of interval data type.",4.1. Experiments on synthetic data,[0],[0]
"This effect is larger for Gaussian variables, since in this example the Gaussian is a more heavy-tailed distribution than the Gamma.
",4.1. Experiments on synthetic data,[0],[0]
"2In a boxplot, the central mark is the median, the edges of the box are the 25th and 75th percentiles, the whiskers extend to the 10th and 90th percentiles.
",4.1. Experiments on synthetic data,[0],[0]
"Next, we study whether the proposed model is able to disambiguate among different discrete types of variables, particularly, among categorical, ordinal and count data.",4.1. Experiments on synthetic data,[0],[0]
"To this end, we generate three types of datasets of size 1, 000.",4.1. Experiments on synthetic data,[0],[0]
"In the first type we account for categorical data by sampling a multinomial variable with R categories, where the probability of the categories is sampled from a Dirichlet distribution.",4.1. Experiments on synthetic data,[0],[0]
"Then, for each category we sample a multidimensional Gaussian centroid that corresponds to the mean of the multivariate Gaussian observations that complete the dataset.",4.1. Experiments on synthetic data,[0],[0]
"To account for ordinal observations, we first sample the first variable in our dataset from a uniform distribution in the interval (0, R), which we randomly divide into R categories that correspond to the ordinal variable in our dataset.",4.1. Experiments on synthetic data,[0],[0]
"Finally, to account for count data we first generate a Gamma variable sampled from Γ(α, α/4), and then generate the counting variable in the dataset by taking the floor of the Gamma variable.",4.1. Experiments on synthetic data,[0],[0]
"For both categorical and ordinal data, we generate 10 independent datasets for each value of the number of categoriesR ∈ {3, . . .",4.1. Experiments on synthetic data,[0],[0]
", 10}, and for count data we generate another 10 datasets for each value of α ∈ {2, . . .",4.1. Experiments on synthetic data,[0],[0]
", 8}.",4.1. Experiments on synthetic data,[0],[0]
"Figure 3 summarizes the likelihood weights obtained for each type of datasets (i.e., for each type of discrete variable) after running on each dataset 10 independent simulations of Algorithm 1 with 500 iterations for different model complexity values, i.e., for different numbers of latent feature variables K = 1, . . .",4.1. Experiments on synthetic data,[0],[0]
", 10.",4.1. Experiments on synthetic data,[0],[0]
In this figure we can observe that we can accurately discover the true type of discrete variable robustly and independently of the assumed model complexity,4.1. Experiments on synthetic data,[0],[0]
"K. We also observe on
the top row of Figure 3(a)-(b) that i) as the number of categories R in the discrete variable decreases, the harder is to distinguish between ordinal and categorical data, i.e., to find out whether the data takes values in a ordered set or in an unordered set; and ii) as R in ordinal data increases, the ordinal variable is more likely to be identified as count data.",4.1. Experiments on synthetic data,[0],[0]
Both of these effects are intuitively sensible.,4.1. Experiments on synthetic data,[0],[0]
"In this section, we evaluate the performance of the proposed method on seven real datasets collected from the UCI machine learning repository (Lichman, 2013).",4.2. Experiments on real data,[0],[0]
"Table 1 summarizes theses datasets by providing the number of objects and attributes in the dataset, as well as how many of these attributes are discrete.
",4.2. Experiments on real data,[0],[0]
"In order to quantitatively evaluate the performance of the proposed method, we select at random 10% of the observations in each dataset as a held-out set and compare the predictive performance, in terms of average test log-likelihood per observation, of our method with a baseline method.",4.2. Experiments on real data,[0],[0]
The baseline method corresponds to a latent feature model in which all the continuous variables are modeled as realvalued data and the discrete variables as categorical data.,4.2. Experiments on real data,[0],[0]
"Figure 4 shows the obtained results for our method (solid line) and the baseline (dashed line) for several values of the model complexity (i.e., the number of latent features K) averaged over 10 independent runs of the corresponding inference algorithms.",4.2. Experiments on real data,[0],[0]
"Here, we observe that i) both methods provide robust results with respect to the number of variables K; and ii) our method clearly outperforms the baseline in all the datasets, except for the Student dataset where the baseline performs slightly better.",4.2. Experiments on real data,[0],[0]
"In other words, this figure shows that by taking into account the uncertainty in the statistical types of the variables, we provide a better fitting of the data.
",4.2. Experiments on real data,[0],[0]
"Additionally, Table 2 shows the list of (non-binary) attributes in the Adult and the German datasets together with the data types with larger inferred likelihood weights,3 i.e., the discovered statistical data types.",4.2. Experiments on real data,[0],[0]
"Here, the number in parenthesis corresponds to the observed number of categories in discrete data.",4.2. Experiments on real data,[0],[0]
The very heterogeneous nature of these datasets explains the substantial gain observed in Figure 4.,4.2. Experiments on real data,[0],[0]
"Moreover, Table 2 shows some expected results, e.g.,
3In cases in which two data types present very similar likelihood weights (< 10% difference), we display both of them.
",4.2. Experiments on real data,[0],[0]
"Number of Latent Variables (K)
Figure 4.",4.2. Experiments on real data,[0],[0]
[Real Data],4.2. Experiments on real data,[0],[0]
"Comparison between our model (solid) and the baseline (dashed) in terms of average test log-likelihood per observation evaluated on a held-out set containing 10% of the observations in each dataset.
",4.2. Experiments on real data,[0],[0]
"marital status and race are identified as categorical, while the age is of count data type for both datasets.",4.2. Experiments on real data,[0],[0]
"However, other results might seem surprising.",4.2. Experiments on real data,[0],[0]
"For example, the duration (in months), which one would expect it to be count data, is identified as ordinal; or the a priori categorical attributes native country and job are inferred to be ordinal.
",4.2. Experiments on real data,[0],[0]
"In order to better understand these results, we show the histograms of several variables in these datasets and the associated inferred likelihood weights.",4.2. Experiments on real data,[0],[0]
"Figure 5 shows the histograms of two continuous variables, length and weight of the Abalone dataset, which take only positive real values, but are assigned to different data types (respectively, to real-valued and positive real-valued data).",4.2. Experiments on real data,[0],[0]
"This can be explained by the fact that, while the distribution of the length presents large tails, the distribution of the weight is clearly truncated at zero.",4.2. Experiments on real data,[0],[0]
"Additionally, Figure 6(a)-(b) shows two discrete variables, the duration (in months) and the age in German data, which based on the documentation are expected to be count data.",4.2. Experiments on real data,[0],[0]
"However, our model assigns the duration to ordinal data.",4.2. Experiments on real data,[0],[0]
This result can be explained by the irregular distribution that this variable has.,4.2. Experiments on real data,[0],[0]
"In count data the distance between every two consecutive values should be roughly the same (there is the same distance from “1 pen” to “2 pens” as from “2 pens” to “3 pens”, that is 1 pen), resulting therefore in smooth probability mass functions.",4.2. Experiments on real data,[0],[0]
"We found in Figure 6(c)-(d) that while the number of credits and the job variables can be a priori thought as re-
spectively count and categorical data, they are both inferred to be ordinal data.",4.2. Experiments on real data,[0],[0]
"In the case of the number of credits, this can be explained by the small (finite) number of values that the variable takes, while in the case of the job, this assignment can be explained by the labels of its categories, i.e., {unskilled non-resident, unskilled resident, skilled employee and highly qualified employee}, which clearly represent an ordered set.
",4.2. Experiments on real data,[0],[0]
"From these results, we can conclude that i)",4.2. Experiments on real data,[0],[0]
"our model accurately discovers the true statistical type of the data, which might not be easily extracted from its documentation; and by doing so, ii) it provides a better fit of the data.",4.2. Experiments on real data,[0],[0]
"Moreover, apparent failures are in fact sensible when data histograms are carefully examined.",4.2. Experiments on real data,[0],[0]
"In this paper, we presented the first approach to automatically discover the statistical types of the variables in a dataset.",5. Conclusions,[0],[0]
"Our experiments showed that the proposed approach accurately infers the data type, or equivalently likelihood model, that best fits the data.
",5. Conclusions,[0],[0]
Our work opens many interesting avenues for future work.,5. Conclusions,[0],[0]
"For example, it would be interesting to extend the proposed method to account for other data types.",5. Conclusions,[0],[0]
"We would like to include directional data, also called circular data, which arise in a multitude of data-modelling contexts ranging from robotics to the social sciences (Navarro et al., 2016).",5. Conclusions,[0],[0]
"Moreover, since the proposed method can be seen as a likelihood selection method, it would be interesting to study how to incorporate our framework in any statistical machine learning tool, where the likelihood model, instead of being fixed a priori, would be inferred directly from the data jointly with the rest of the model parameters.",5. Conclusions,[0],[0]
"Isabel Valera acknowledges her Humboldt Research Fellowship for Postdoctoral Researchers, which funded this research during her stay at the Max Planck Institute for Software Systems.",Acknowledgement,[0],[0]
"Zoubin Ghahramani acknowledges support from the Alan Turing Institute (EPSRC Grant EP/N510129/1) and EPSRC Grant EP/N014162/1, and donations from Google and Microsoft Research.
",Acknowledgement,[0],[0]
"The code implementing the proposed method, as well as the scripts that reproduce the experiments presented in the paper, are publicly available at: https://github.com/ivaleraM/DataTypes",Acknowledgement,[0],[0]
"A common practice in statistics and machine learning is to assume that the statistical data types (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known.",abstractText,[0],[0]
"However, as the availability of realworld data increases, this assumption becomes too restrictive.",abstractText,[0],[0]
"Data are often heterogeneous, complex, and improperly or incompletely documented.",abstractText,[0],[0]
"Surprisingly, despite their practical importance, there is still a lack of tools to automatically discover the statistical types of, as well as appropriate likelihood (noise) models for, the variables in a dataset.",abstractText,[0],[0]
"In this paper, we fill this gap by proposing a Bayesian method, which accurately discovers the statistical data types in both synthetic and real data.",abstractText,[0],[0]
Automatic Discovery of the Statistical Types of Variables in a Dataset,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 662–666 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
662",text,[0],[0]
Simultaneous Interpretation (SI) is an inherently difficult task that carries significant cognitive and attentional burdens.,1 Introduction,[0],[0]
The role of the simultaneous interpreter is to accurately render the source speech in a given target language in a timely and precise manner.,1 Introduction,[0],[0]
"Interpreters employ a range of strategies, including generalization and summarization, to convey the source message as efficiently and reliably as possible (He et al., 2016).",1 Introduction,[0],[0]
"Unfortunately, the interpreter is pitched against the limits of human memory and stamina, and after only minutes of interpreting, the number of errors made by an interpreter begins to increase exponentially (Moser-Mercer et al., 1998).
",1 Introduction,[0],[0]
"1https://github.com/craigastewart/qe sim interp
We examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling.",1 Introduction,[0],[0]
"This has several immediate potential applications, one of which being in Computer-Assisted Interpretation (CAI).",1 Introduction,[0],[0]
"CAI is quickly gaining traction in the interpreting community, with software products such as InterpretBank (Fantinouli, 2016) deployed in interpreting booths to provide live and interactive terminology support.",1 Introduction,[0],[0]
"Figure 1(b) shows how this might work; both the interpreter and the CAI system receive the source message and the system displays assistive information in the form of terminology and informational support.
",1 Introduction,[0],[0]
"While this might improve the quality of interpreter output, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter (Fantinouli, 2018).",1 Introduction,[0],[0]
"Intuitively, the ideal level of support depends on current interpreter performance.",1 Introduction,[0],[0]
The system can minimize distraction by providing assistance only when an interpreter is struggling.,1 Introduction,[0],[0]
This level of support could be moderated appropriately if interpreter performance can be accurately predicted.,1 Introduction,[0],[0]
"Figure 1(c) demonstrates
how our proposed quality estimation (QE) system receives and evaluates interpreter output, allowing the CAI system to appropriately lower the amount of information passed to the interpreter, maximizing the quality of interpreter output.
",1 Introduction,[0],[0]
"As a concrete method for estimating interpreter performance, we turn to existing work on QE for machine translation (MT) systems (Specia et al., 2010, 2015), which takes in the source sentence and MT-generated outputs and estimates a measure of quality.",1 Introduction,[0],[0]
"In doing so, we arrive at two natural research questions:
1.",1 Introduction,[0],[0]
"Do existing methods for performing QE on MT output also allow for accurate estimation of interpreter performance, despite the inherent differences between MT and SI?
2.",1 Introduction,[0],[0]
"What unique aspects of the problem of interpreter performance estimation, such as the availability of prosody and other linguistic cues, can be exploited to further improve the accuracy of our predictions?
",1 Introduction,[0],[0]
"The remainder of the paper describes methods and experiments on English-Japanese (ENJA), English-French (EN-FR), and English-Italian (EN-IT) interpretation data attempting to answer these questions.",1 Introduction,[0],[0]
"Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU (Papineni et al., 2002) rely on the availability of reference translations to evaluate MT output quality, which aren’t always available.",2 Quality Estimation,[0],[0]
"As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation (Callison-Burch et al., 2012), using approaches ranging from linear classifiers (Ueffing and Ney, 2007; Luong et al., 2014) to neural models (Martins et al., 2016, 2017).
",2 Quality Estimation,[0],[0]
"QuEst++ (Specia et al., 2015) is a well-known QE pipeline that supports word-level, sentencelevel, and document-level QE.",2 Quality Estimation,[0],[0]
Its effectiveness and flexibility make it an attractive candidate for our proposed task.,2 Quality Estimation,[0],[0]
There are two main modules to QuEst++: a feature extractor and a learning module.,2 Quality Estimation,[0],[0]
"The feature extractor produces an intermediate representation of the source and translation in
a continuous feature vector.",2 Quality Estimation,[0],[0]
"The goal of the learning module, given a source and translation pair, is to predict the quality of the translation, either as a label or as a continuous value.",2 Quality Estimation,[0],[0]
This module is trained on example translations that have an assigned score (such as BLEU) and then predicts the score of a new example.,2 Quality Estimation,[0],[0]
QuEst++ offers a range of learning algorithms but defaults to Support Vector Regression for sentence-level QE.,2 Quality Estimation,[0],[0]
"The default, out-of-the-box, sentence-level feature set for QuEst++ includes seventeen features such as number of tokens in source/target utterances, average token length, n-gram frequency, etc.",3 Quality Estimation for Interpretation,[0],[0]
"(Specia et al., 2015).",3 Quality Estimation for Interpretation,[0],[0]
"While this feature set is effective for evaluation of MT output, SI output is inherently different—full of pauses, hesitations, paraphrases, re-orderings and repetitions.",3 Quality Estimation for Interpretation,[0],[0]
"In the following sections, we describe our methods to adapt QE to handle these phenomena.",3 Quality Estimation for Interpretation,[0],[0]
"To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter.
",3.1 Interpretation-specific Features,[0],[0]
Ratio of pauses/hesitations/incomplete words: Sridhar et al. (2013) propose that interpreters regularly use pauses to gain more time to think and as a cognitive strategy to manage memory constraints.,3.1 Interpretation-specific Features,[0],[0]
An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output.,3.1 Interpretation-specific Features,[0],[0]
"In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words.
",3.1 Interpretation-specific Features,[0],[0]
"Ratio of non-specific words: Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology (Sridhar et al., 2013), either to prevent redundancy or to ease cognitive load.",3.1 Interpretation-specific Features,[0],[0]
For example: “The chairman explained the proposal to the delegates” might be rendered in a target language as “he explained it to them.”,3.1 Interpretation-specific Features,[0],[0]
"To capture this, we include a feature that checks for words from a pre-determined seed list of pronouns and demonstrative adjectives.
",3.1 Interpretation-specific Features,[0],[0]
"Ratio of ‘quasi-’cognates: In related language pairs, often words of a similar root are orthographically similar, for example “artificial”(EN), “artificiel”(FR) and “artificiale”(IT).",3.1 Interpretation-specific Features,[0],[0]
"Likewise in
Japanese, words adapted from English are transcribed in katakana script to indicate their foreign origin.",3.1 Interpretation-specific Features,[0],[0]
"Transliterated words in interpreted speech could represent facilitated translation by language proximity, or an attempt to produce an approximation of a word that the interpreter did not know.",3.1 Interpretation-specific Features,[0],[0]
"We include a feature that counts the number of words that share at least 50% identical orthography (for EN, FR, IT) or are rendered in the interpreter transcript in katakana (JA).
",3.1 Interpretation-specific Features,[0],[0]
Ratio of number of words: We further include three features from the bank of features provided with QuEst++ that compare source and target length and the amount of transcribed punctuation.,3.1 Interpretation-specific Features,[0],[0]
"Information about utterance length makes sense in an interpreting scenario, given the aforementioned strategies of omission and compression of information.",3.1 Interpretation-specific Features,[0],[0]
"A list, for example, may be compressed to avoid redundancy or may be an erroneous omission (Barik, 1994).",3.1 Interpretation-specific Features,[0],[0]
"Novice interpreters are assessed for accuracy on the number of omissions, additions and the inaccurate renditions of lexical items and longer phrases (Altman, 1994), but recovery of content and correct terminology are highly valued.",3.2 Evaluation Metric,[0],[0]
"While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve.",3.2 Evaluation Metric,[0],[0]
One important design decision is which evaluation metric to target in our QE system.,3.2 Evaluation Metric,[0],[0]
"There is an abundance of evaluation metrics available for MT including WER (Su et al.), BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Denkowski and Lavie, 2014), all of which compare the similarity between reference translations and translations.",3.2 Evaluation Metric,[0],[0]
"Interpreter output is fundamentally different from any reference that we may use in evaluation because interpreters employ a range of economizing strategies such as segmentation, omission, generalization, and reformulation (Riccardi, 2005).",3.2 Evaluation Metric,[0],[0]
"As such, measuring interpretation quality by some metrics employed in MT such as BLEU can result in artificially low scores (Shimizu et al., 2013).",3.2 Evaluation Metric,[0],[0]
"To mitigate this, we use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and contentfunction word distinctions, and thus should be better equipped to deal with the disparity between MT and SI.",3.2 Evaluation Metric,[0],[0]
"Better handling of these divergences
for evaluation of interpreter output, or fine-grained evaluation based on measures from interpretation studies is an interesting direction for future work.",3.2 Evaluation Metric,[0],[0]
"For our EN-JA language data we train the pipeline on combined data from seven TED Talks taken from the NAIST TED SI corpus (Shimizu et al., 2013).",4 Data: Interpretation Corpora,[0],[0]
"This corpus provides human transcribed SI output from three interpreters of low, intermediate and high levels of proficiency denoted B-rank, Arank and S-rank respectively, with 559 utterances from each interpreter.",4 Data: Interpretation Corpora,[0],[0]
"The corpus also provides written translations of the source speech, which we use as reference translations when evaluating interpreter output using METEOR.
",4 Data: Interpretation Corpora,[0],[0]
"Our EN-FR and EN-IT data are drawn from the EPTIC corpus (Bernardini et al., 2016), which provides source and interpreter transcripts for speeches from the European Parliament (manually transcribed to include vocal expressions), as well as translations of transcripts of the source speech.",4 Data: Interpretation Corpora,[0],[0]
The EN-FR and EN-IT datasets contain 739 and 731 utterances respectively.,4 Data: Interpretation Corpora,[0],[0]
"While the EPTIC translations are accurate, they were created from an official transcript that differs significantly in register from the source speech.",4 Data: Interpretation Corpora,[0],[0]
"As a proxy for our experiments, we generated translations of the original speech using Google Translate, which resulted in much more qualitatively reliable METEOR scores than the EPTIC translations.",4 Data: Interpretation Corpora,[0],[0]
"To evaluate the quality of our QE system, we use the Pearson’s r correlation between the predicted and true METEOR for each language pair (Graham, 2015).",5 Interpreter Quality Experiments,[0],[0]
"As a baseline, we train QuEst++ on the out-of-the-box feature set (Section 2).
",5 Interpreter Quality Experiments,[0],[0]
"We use k-fold cross-validation individually on EN-JA, EN-FR, and EN-IT source-interpreter language pairs with a held-out development set and test set for each fold.",5 Interpreter Quality Experiments,[0],[0]
"For each experiment setting, we run the experiment for each fold (ten iterations for each set) and evaluate average Pearson’s r correlation on the development set.
",5 Interpreter Quality Experiments,[0],[0]
"In our baseline setting, we extract features based on the default QuEst++ sentence-level feature set (baseline).",5 Interpreter Quality Experiments,[0],[0]
"We ablate baseline features through cross-validation and remove features relating to bigram and trigram frequency and punctuation frequency in the source utterance, creating
a more effective trimmed model (trimmed).",5 Interpreter Quality Experiments,[0],[0]
"Subsequently, we add our interpreter features (Section 3.1) and arrive at our proposed model.",5 Interpreter Quality Experiments,[0],[0]
We then repeat each experiment using the test set data from each fold and compare the resulting average Pearson’s r scores.,5 Interpreter Quality Experiments,[0],[0]
"Table 1 shows our primary results comparing the baseline, trimmed, and proposed feature sets.",5.1 Results,[0],[0]
"Our first observation is that, even with the baseline feature set, QE obtains respectable correlation scores, proving feasible as a method to predict interpreter performance.",5.1 Results,[0],[0]
"Our trimmed feature set performs moderately better than the baseline for Japanese, and slightly under-performs for French and Italian.",5.1 Results,[0],[0]
"However, our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank) (+0.104), achieving its highest accuracy on the EN-FR dataset.",5.1 Results,[0],[0]
"Over all datasets, the gain of the proposed model is statistically significant at p < 0.05 by the pairwise bootstrap (Koehn, 2004).",5.1 Results,[0],[0]
We further present two analyses: ablation on the full feature set and a qualitative comparison.,5.2 Analysis,[0],[0]
"Table 2 iteratively reduces the feature set by first removing the ‘quasi-’cognate feature (w/o cog); specific words (w/o spec); pauses, hesitations, and incomplete words (w/o fill); and finally sentence length and punctuation differences (w/o length).
",5.2 Analysis,[0],[0]
Relative difference in utterance length appears to aid Japanese and French above other languages.,5.2 Analysis,[0],[0]
Cognates are particularly useful in EN-FR and EN-IT; this may be indicative of the corpus domain (European Parliament proceedings being rich in Latinate legalese) or of cognate frequency in those languages.,5.2 Analysis,[0],[0]
"In Japanese, cognates were
more indicative of quality for the more skilled interpreter (S-rank).",5.2 Analysis,[0],[0]
"While pauses and hesitations seem to aid the model in EN-FR and EN-IT, they appear to hinder EN-JA.
",5.2 Analysis,[0],[0]
Below is a qualitative EN-IT example with a METEOR score of 0.079 (being substantially lower than the average METEOR score across all datasets; 0.262).,5.2 Analysis,[0],[0]
"The baseline model prediction of its score was 0.127, and our proposed model, 0.066:
SOURCE: “Will the Parliament grant President Dilma Rousseff, on the very first occasion after her groundbaking groundbreaking election and for no sound formal reason, the kind of debate that we usually reserve for people like Mugabe?",5.2 Analysis,[0],[0]
"So, I ask you to remove Brazil from the agenda of the urgencies.”
INTERP: “Ehm il Parlamento... dopo le elezioni... daremdar spazio a un dibattito sul ehm sul caso per esempio del presidente Mugabe invece di mettere il Brasile all’ordine del giorno?”
",5.2 Analysis,[0],[0]
GLOSS: “Ehm the Parliament... after the elections...,5.2 Analysis,[0],[0]
"we’ll gi- will give way to a debate on the ehm on the case for example of President Mugabe instead of putting Brazil on the agenda?”
",5.2 Analysis,[0],[0]
"Our model can better capture the issues in this example because it has many interpretation specific qualities (pauses, compression, and omission).",5.2 Analysis,[0],[0]
This is an example in which a CAI system might offer assistance to an interpreter struggling to produce an accurate rendition.,5.2 Analysis,[0],[0]
"We introduce a novel and effective application of QE to evaluate interpreter output, which could be immediately applied to allow CAI systems to selectively offer assistance to struggling interpreters.",6 Conclusion,[0],[0]
"This work uses METEOR to evaluate interpreter output, but creation of fine-grained mea-
sures to evaluate various aspects of interpreter performance is an interesting avenue for future work.",6 Conclusion,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1748663 and Graduate Research Fellowship No. DGE1745016.,Acknowledgements,[0],[0]
"Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding.",abstractText,[0],[0]
"Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computerassisted interpretation interfaces or pedagogical tools.",abstractText,[0],[0]
We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output.,abstractText,[0],[0]
"In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.1",abstractText,[0],[0]
Automatic Estimation of Simultaneous Interpreter Performance,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1226–1236 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1226",text,[0],[0]
Automatic extraction of prominent information from text has always been a core problem in language research.,1 Introduction,[0],[0]
"While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities (Dunietz and Gillick, 2014) and events (Choubey et al., 2018).
",1 Introduction,[0],[0]
Events are important discourse units that form the backbone of our communication.,1 Introduction,[0],[0]
They play various roles in documents.,1 Introduction,[0],[0]
"Some are more central in discourse: connecting other entities and events, or providing key information of a story.",1 Introduction,[0],[0]
"Others are less relevant, but not easily identifiable by NLP systems.",1 Introduction,[0],[0]
Hence it is important to be able to quantify the “importance” of events.,1 Introduction,[0],[0]
"For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not.
",1 Introduction,[0],[0]
"Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015).",1 Introduction,[0],[0]
"Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018).",1 Introduction,[0],[0]
"Thus, proper models for finding important events are desired.
",1 Introduction,[0],[0]
"In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents.",1 Introduction,[0],[0]
"To build a salience detection model, one core observation is that salient discourse units are forming discourse relations.",1 Introduction,[0],[0]
"In Figure 1, the “trial” event is connected to many other events: “charge” is pressed before “trial”; “trial” is being “delayed”.
",1 Introduction,[0],[0]
We present two salience detection systems based on the observations.,1 Introduction,[0],[0]
First is a feature based learning to rank model.,1 Introduction,[0],[0]
"Beyond basic features like frequency and discourse location, we design features using cosine similarities among events and entities, to estimate the content organization (Grimes, 1975): how lexical meaning of elements relates to each other.",1 Introduction,[0],[0]
"Similarities from within-sentence or across the whole document are used to capture
interactions on both local and global aspects (§4).",1 Introduction,[0],[0]
"The model significantly outperforms a strong “Frequency” baseline in our experiments.
",1 Introduction,[0],[0]
"However, there are other discourse relations beyond lexical similarity.",1 Introduction,[0],[0]
"Figure 1 showcases some: the script relation (Schank and Abelson, 1977)1 between “charge” and “trial”, and the frame relation (Baker et al., 1998) between “attacks” and “trial” (“attacks” fills the “charges” role of “trial”).",1 Introduction,[0],[0]
"Since it is unclear which ones contribute more to salience, we design a Kernel based Centrality Estimation (KCE) model (§5) to capture salient specific interactions between discourse units automatically.
",1 Introduction,[0],[0]
"In KCE, discourse units are projected to embeddings, which are trained end-to-end towards the salience task to capture rich semantic information.",1 Introduction,[0],[0]
A set of soft-count kernels are trained to weigh salient specific latent relations between discourse units.,1 Introduction,[0],[0]
"With the capacity to model richer relations, KCE outperforms the feature-based model by a large margin (§7.1).",1 Introduction,[0],[0]
Our analysis shows that KCE is exploiting several relations between discourse units: including script and frames (Table 5).,1 Introduction,[0],[0]
"To further understand the nature of KCE, we conduct an intrusion test (§6.2), which requires a model to identify events from another document.",1 Introduction,[0],[0]
"The test shows salient events form tightly related groups with relations captured by KCE.
",1 Introduction,[0],[0]
The notion of salience is subjective and may vary from person to person.,1 Introduction,[0],[0]
"We follow the empirical approaches used in entity salience research (Dunietz and Gillick, 2014).",1 Introduction,[0],[0]
"We consider the summarization test: an event is considered salient if a summary written by a human is likely to include it, since events about the main content are more likely to appear in a summary.",1 Introduction,[0],[0]
"This approach allows us to create a large-scale corpus (§3).
",1 Introduction,[0],[0]
"In this paper, we make three main contributions.",1 Introduction,[0],[0]
"First, we present two event salience detection systems, which capture rich relations among discourse units.",1 Introduction,[0],[0]
"Second, we observe interesting connections between salience and various discourse relations (§7.1 and Table 5), implying potential research on these areas.",1 Introduction,[0],[0]
"Finally, we construct a large scale event salience corpus, providing a testbed for future research.",1 Introduction,[0],[0]
"Our code, dataset and models are publicly available2.
",1 Introduction,[0],[0]
"1Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”.
2https://github.com/hunterhector/",1 Introduction,[0],[0]
EventSalience,1 Introduction,[0],[0]
Events have been studied on many aspects due to their importance in language.,2 Related Work,[0],[0]
"To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016).
",2 Related Work,[0],[0]
"However, studies on event salience are premature.",2 Related Work,[0],[0]
"Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015).",2 Related Work,[0],[0]
"Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles.",2 Related Work,[0],[0]
"They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes.",2 Related Work,[0],[0]
"In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale.",2 Related Work,[0],[0]
"We also do not restrict to a single most important event per document.
",2 Related Work,[0],[0]
"There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018).",2 Related Work,[0],[0]
"In this work, we study the case for events.
",2 Related Work,[0],[0]
"Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976).",2 Related Work,[0],[0]
"Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999).",2 Related Work,[0],[0]
"Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004).",2 Related Work,[0],[0]
"Though sharing similar intuitions, our proposed models are designed to learn richer semantic relations in the embedding space.
",2 Related Work,[0],[0]
"Comparing to the traditional summarization task, we focus on events, which are at a different granularity.",2 Related Work,[0],[0]
Our experiments also unveil interesting phenomena among events and other discourse units.,2 Related Work,[0],[0]
"This section introduces our approach to construct a large-scale event salience corpus, including methods for finding event mentions and obtaining saliency labels.",3 The Event Salience Corpus,[0],[0]
"The studies are based on the Annotated New York Times corpus (Sandhaus, 2008), a newswire corpus with expert-written abstracts.",3 The Event Salience Corpus,[0],[0]
"Event Mention Annotation: Despite many annotation attempts on events (Pustejovsky et al., 2002; Brown et al., 2017), automatic labeling of them in general domain remains an open problem.",3.1 Automatic Corpus Creation,[0],[0]
Most of the previous work follows empirical approaches.,3.1 Automatic Corpus Creation,[0],[0]
"For example, Chambers and Jurafsky (2008) consider all verbs together with their subject and object as events.",3.1 Automatic Corpus Creation,[0],[0]
"Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998).
",3.1 Automatic Corpus Creation,[0],[0]
There are two main challenges in labeling event mentions.,3.1 Automatic Corpus Creation,[0],[0]
"First, we need to decide which lexical items are event triggers.",3.1 Automatic Corpus Creation,[0],[0]
"Second, we have to disambiguate the word sense to correctly identify events.",3.1 Automatic Corpus Creation,[0],[0]
"For example, the word “phone” can refer to an entity (a physical phone) or an event (a phone call event).",3.1 Automatic Corpus Creation,[0],[0]
We use FrameNet to solve these problems.,3.1 Automatic Corpus Creation,[0],[0]
"We first use a FrameNet based parser: Semafor (Das and Smith, 2011), to find and disambiguate triggers into frame classes.",3.1 Automatic Corpus Creation,[0],[0]
"We then use the FrameNet ontology to select event mentions.
",3.1 Automatic Corpus Creation,[0],[0]
"Our frame based selection method follows the Vendler classes (Vendler, 1957), a four way classification of eventuality: states, activities, accomplishments and achievements.",3.1 Automatic Corpus Creation,[0],[0]
"The last three classes involve state change, and are normally considered as events.",3.1 Automatic Corpus Creation,[0],[0]
"Following this, we create an “eventevoking frame” list using the following procedure:
1.",3.1 Automatic Corpus Creation,[0],[0]
We keep frames that are subframes of Event and Process in the FrameNet ontology.,3.1 Automatic Corpus Creation,[0],[0]
2.,3.1 Automatic Corpus Creation,[0],[0]
"We discard frames that are subframes of state, entity and attribute frames, such as Entity, Attributes, Locale, etc. 3.",3.1 Automatic Corpus Creation,[0],[0]
"We manually inspect frames that are not subframes of the above-mentioned ones (around 200) to keep event related ones (including subframes), such as Arson, Delivery, etc.
",3.1 Automatic Corpus Creation,[0],[0]
This gives us a total of 569 frames.,3.1 Automatic Corpus Creation,[0],[0]
We parse the documents with Semafor and consider predicates that trigger a frame in the list as candidates.,3.1 Automatic Corpus Creation,[0],[0]
"We finish the process by removing the light verbs3 and reporting events4 from the candidates, similar to previous research (Recasens et al., 2013).",3.1 Automatic Corpus Creation,[0],[0]
"Salience Labeling: For all articles with a human written abstract (around 664,911) in the New York
3Light verbs carry little semantic information: “appear”, “be”, “become”, “do”, “have”, “seem”, “",3.1 Automatic Corpus Creation,[0],[0]
"do”, “get”, “give”, “go”, “have”, “keep”, “make”, “put”, “set”, “take”.
",3.1 Automatic Corpus Creation,[0],[0]
"4Reporting verbs are normally associated with the narrator: “argue”, “claim”, “say”, “suggest”, “tell”.
",3.1 Automatic Corpus Creation,[0],[0]
"Times Annotated Corpus, we extract event mentions.",3.1 Automatic Corpus Creation,[0],[0]
We then label an event mention as salient if we can find its lemma in the corresponding abstract (Mitamura et al. (2015) showed that lemma matching is a strong baseline for event coreference.).,3.1 Automatic Corpus Creation,[0],[0]
"For example, in Figure 1, event mentions in bold and red are found in the abstract, thus labeled as salient.",3.1 Automatic Corpus Creation,[0],[0]
Data split is detailed in Table 1 and §6.,3.1 Automatic Corpus Creation,[0],[0]
"While the automatic method enables us to create a dataset at scale, it is important to understand the quality of the dataset.",3.2 Annotation Quality,[0],[0]
"For this purpose, we have conducted two small manual evaluation study.
",3.2 Annotation Quality,[0],[0]
Our lemma-based salience annotation method is based on the assumption that lemma matching being a strong detector for event coreference.,3.2 Annotation Quality,[0],[0]
"In order to validate this assumption, one of the authors manually examined 10 documents and identified 82 coreferential event mentions pairs between the text body and the abstract.",3.2 Annotation Quality,[0],[0]
"The automatic lemma rule identifies 72 such pairs: 64 of these matches human decision, producing a precision of 88.9% (64/72) and a recall of 78% (64/82).",3.2 Annotation Quality,[0],[0]
"There are 18 coreferential pairs missed by the rule.
",3.2 Annotation Quality,[0],[0]
The next question is: is an event really important if it is mentioned in the abstract?,3.2 Annotation Quality,[0],[0]
"Although prior work (Dunietz and Gillick, 2014) shows that the assumption to be valid for entities, we study the case for events.",3.2 Annotation Quality,[0],[0]
We asked two annotators to manually annotate 10 documents (around 300 events) using a 5-point Likert scale for salience.,3.2 Annotation Quality,[0],[0]
"We compute the agreement score using Cohen’s Kappa (Cohen, 1960).",3.2 Annotation Quality,[0],[0]
We find the task to be challenging for human: annotators don’t agree well on the 5-point scale (Cohens Kappa = 0.29).,3.2 Annotation Quality,[0],[0]
"However, if we collapse the scale to binary decisions, the Kappa between the annotators raises to 0.67.",3.2 Annotation Quality,[0],[0]
"Further, the Kappa between each annotator and automatic labels are 0.49 and 0.42 respectively.",3.2 Annotation Quality,[0],[0]
"These agreement scores are also close to those reported in the entity salience tasks (Dunietz and Gillick, 2014).
",3.2 Annotation Quality,[0],[0]
"While errors exist in the automatic annotation process inevitably, we find the error rate to be reasonable for a large-scale dataset.",3.2 Annotation Quality,[0],[0]
"Further, our study indicates the difficulties for human to rate on a finer scale of salience.",3.2 Annotation Quality,[0],[0]
We leave the investigation of continuous salience scores to future work.,3.2 Annotation Quality,[0],[0]
"This section presents the feature-based model, including the features and the learning process.",4 Feature-Based Event Salience Model,[0],[0]
Our features are summarized in Table 2.,4.1 Features,[0],[0]
Basic Discourse Features: We first use two basic features similar to Dunietz and Gillick (2014): Frequency and Sentence Location.,4.1 Features,[0],[0]
"Frequency is the lemma count of the mention’s syntactic head word (Manning et al., 2014).",4.1 Features,[0],[0]
"Sentence Location is the sentence index of the mention, since the first few sentences are normally more important.",4.1 Features,[0],[0]
"These two features are often used to estimate salience (Barzilay and Lapata, 2008; Vossen and Caselli, 2015).",4.1 Features,[0],[0]
"Content Features: We then design several lexical similarity features, to reflect Grimes’ content relatedness (Grimes, 1975).",4.1 Features,[0],[0]
"In addition to events, the relations between events and entities are also important.",4.1 Features,[0],[0]
"For example, Figure 1 shows some related entities in the legal domain, such as “prosecutors” and “court”.",4.1 Features,[0],[0]
"Ideally, they should help promote the salience status for event “trial”.
",4.1 Features,[0],[0]
"Lexical relations can be found both withinsentence (local) or across sentence (global) (Halliday and Hasan, 1976).",4.1 Features,[0],[0]
We compute the local part by averaging similarity scores from other units in the same sentence.,4.1 Features,[0],[0]
The global part is computed by averaging similarity scores from other units in the document.,4.1 Features,[0],[0]
"All similarity scores are computed using cosine similarities on pre-trained embeddings (Mikolov et al., 2013).
",4.1 Features,[0],[0]
"These lead to 3 content features: Event Voting, the average similarity to other events in the document; Entity Voting, the average similarity to entities in the document; Local Entity Voting, the average similarity to entities in the same sentence.",4.1 Features,[0],[0]
Local event voting is not used since a sentence often contains only 1 event.,4.1 Features,[0],[0]
"A Learning to Rank (LeToR) model (Liu, 2009) is used to combine the features.",4.2 Model,[0],[0]
"Let evi denote
the ith event in a document d.",4.2 Model,[0],[0]
"Its salience score is computed as:
f(evi, d) =",4.2 Model,[0],[0]
"Wf · F (evi, d) + b",4.2 Model,[0],[0]
"(1)
where F (evi, d) is the features for evi in d (Table 2); Wf and b are the parameters to learn.
",4.2 Model,[0],[0]
"The model is trained with pairwise loss:∑ ev+,ev−∈d max(0, 1− f(ev+, d) + f(ev−, d)), (2)
w.r.t.",4.2 Model,[0],[0]
"y(ev+, d) = +1 & y(ev−, d) = −1.
y(ei, d) = { +1, if ei is a salient entity in d, −1, otherwise.
where ev+ and ev− represent the salient and nonsalient events; y is the gold standard function.",4.2 Model,[0],[0]
Learning can be done by standard gradient methods.,4.2 Model,[0],[0]
"As discussed in §1, the salience of discourse units is reflected by rich relations beyond lexical similarities, for example, script (“charge” and “trial”) and frame (a “trial” of “attacks”).",5 Neural Event Salience Model,[0],[0]
"The relations between these words are specific to the salience task, thus difficult to be captured by raw cosine scores that are optimized for word similarities.",5 Neural Event Salience Model,[0],[0]
"In this section, we present a neural model to exploit the embedding space more effectively, in order to capture relations for event salience estimation.",5 Neural Event Salience Model,[0],[0]
"Inspired by the kernel ranking model (Xiong et al., 2017), we propose Kernel-based Centrality Estimation (KCE), to find and weight semantic relations of interests, in order to better estimate salience.
",5.1 Kernel-based Centrality Estimation,[0],[0]
"Formally, given a document d, the set of annotated events V = {ev1, . . .",5.1 Kernel-based Centrality Estimation,[0],[0]
evi . . .,5.1 Kernel-based Centrality Estimation,[0],[0]
", evn}, KCE first embed an event into vector space: evi
Emb−−−→ −→evi.",5.1 Kernel-based Centrality Estimation,[0],[0]
The embedding function is initialized with pretrained embeddings.,5.1 Kernel-based Centrality Estimation,[0],[0]
"It then extract K features for each evi:
ΦK(evi,V) = {φ1(−→evi,V), . . .",5.1 Kernel-based Centrality Estimation,[0],[0]
", (3) φk( −→evi,V), . . .",5.1 Kernel-based Centrality Estimation,[0],[0]
", φK(−→evi,V)},
φk( −→evi,V) = ∑ evj∈V exp
( − (cos( −→evi,−→evj)− µk)2
2σ2k
) .
",5.1 Kernel-based Centrality Estimation,[0],[0]
"(4)
φk( −→evi,V) is the k-th Gaussian kernel with mean µk and variance σ2k.",5.1 Kernel-based Centrality Estimation,[0],[0]
"It models the interactions between events in its kernel range defined by µk and σk. ΦK(evi,V) enforces multi-level interactions among events — relations that contribute similarly to salience are expected to be grouped into the same kernels.",5.1 Kernel-based Centrality Estimation,[0],[0]
Such interactions greatly improve the capacity of the model with negligible increase in the number of parameters.,5.1 Kernel-based Centrality Estimation,[0],[0]
"Empirical evidences (Xiong et al., 2017) have shown that kernels in this form are effective to learn weights for task-specific term pairs.
",5.1 Kernel-based Centrality Estimation,[0],[0]
"The final salience score is computed as:
f(evi, d) =",5.1 Kernel-based Centrality Estimation,[0],[0]
"Wv · ΦK(evi,V) + b, (5)
where Wv is learned to weight the contribution of the certain relations captured by each kernel.
",5.1 Kernel-based Centrality Estimation,[0],[0]
We then use the exact same learning objective as in equation (2).,5.1 Kernel-based Centrality Estimation,[0],[0]
"The pairwise loss is first backpropagated through the network to update the kernel weights Wv, assigning higher weights to relevant regions.",5.1 Kernel-based Centrality Estimation,[0],[0]
"Then the kernels use the gradients to update the embeddings, in order to capture the meaningful discourse relations for salience.
",5.1 Kernel-based Centrality Estimation,[0],[0]
"Since the features and KCE capture different aspects, combining them may give superior performance.",5.1 Kernel-based Centrality Estimation,[0],[0]
"This can be done by combining the two vectors in the final linear layer:
f(evi, d) =",5.1 Kernel-based Centrality Estimation,[0],[0]
"Wv · ΦK(evi,V) +",5.1 Kernel-based Centrality Estimation,[0],[0]
"Wf · F (evi, d) + b",5.1 Kernel-based Centrality Estimation,[0],[0]
(6),5.1 Kernel-based Centrality Estimation,[0],[0]
KCE is also used to model the relations between events and entities.,5.2 Integrating Entities into KCE,[0],[0]
"For example, in Figure 1, the entity “court” is a frame element of the event “trial”; “United States” is a frame element of the event “war”.",5.2 Integrating Entities into KCE,[0],[0]
It is not clear which pair contributes more to salience.,5.2 Integrating Entities into KCE,[0],[0]
"We again let KCE to learn it.
",5.2 Integrating Entities into KCE,[0],[0]
"Formally, let E be the list of entities in the document, i.e. E = {en1, . . .",5.2 Integrating Entities into KCE,[0],[0]
", eni, . . .",5.2 Integrating Entities into KCE,[0],[0]
", enn}, where
eni is the ith entity in document d. KCE extracts the kernel features about entity-event relations as follows:
ΦK(evi,E) = {φ1(−→evi,E), . . .",5.2 Integrating Entities into KCE,[0],[0]
", (7) φk( −→evi,E), . . .",5.2 Integrating Entities into KCE,[0],[0]
", φK(−→evi,E)},
φk( −→evi,E) = ∑ enj∈E",5.2 Integrating Entities into KCE,[0],[0]
"exp
( − (cos( −→evi,−→enj)− µk)2
2σ2k
) (8)
similarly, eni is embedded by: eni Emb−−−→ −→eni, which is initialized by pre-trained entity embeddings.
",5.2 Integrating Entities into KCE,[0],[0]
"We reach the full KCE model by combining all the vectors using a linear layer:
f(evi, d) =",5.2 Integrating Entities into KCE,[0],[0]
"We · ΦK(evi,E) +Wv · ΦK(evi,V) +",5.2 Integrating Entities into KCE,[0],[0]
"Wf · F (evi, d) + b",5.2 Integrating Entities into KCE,[0],[0]
"(9)
",5.2 Integrating Entities into KCE,[0],[0]
The model is again trained by equation (2).,5.2 Integrating Entities into KCE,[0],[0]
This section describes our experiment settings.,6 Experimental Methodology,[0],[0]
Dataset: We conduct our experiments on the salience corpus described in §3.,6.1 Event Salience Detection,[0],[0]
"Among the 664,911 articles with abstracts, we sample 10% of the data as the test set and then randomly leave out another 10% documents for development.",6.1 Event Salience Detection,[0],[0]
"Overall, there are 4359 distinct event lexical items, at a similar scale with previous work (Chambers and Jurafsky, 2008; Do et al., 2011).",6.1 Event Salience Detection,[0],[0]
The corpus statistics are summarized in Table 1.,6.1 Event Salience Detection,[0],[0]
Input: The inputs to models are the documents and the extracted events.,6.1 Event Salience Detection,[0],[0]
The models are required to rank the events from the most to least salience.,6.1 Event Salience Detection,[0],[0]
"Baselines: Three methods from previous researches are used as baselines: Frequency, Location and PageRank.",6.1 Event Salience Detection,[0],[0]
"The first two are often used
to simulate saliency (Barzilay and Lapata, 2008; Vossen and Caselli, 2015).",6.1 Event Salience Detection,[0],[0]
The Frequency baseline ranks events based on the count of the headword lemma; the Location baseline ranks events using the order of their appearances in discourse.,6.1 Event Salience Detection,[0],[0]
"Ties are broken randomly.
",6.1 Event Salience Detection,[0],[0]
"Similar to entity salience ranking with PageRank scores (Xiong et al., 2018), our PageRank baseline runs PageRank on a fully connected graph whose nodes are the events in documents.",6.1 Event Salience Detection,[0],[0]
The edges are weighted by the embedding similarities between event pairs.,6.1 Event Salience Detection,[0],[0]
"We conduct supervised PageRank on this graph, using the same pairwise loss setup as in KCE.",6.1 Event Salience Detection,[0],[0]
We report the best performance obtained by linearly combining Frequency with the scores obtained after a one-step random walk.,6.1 Event Salience Detection,[0],[0]
"Evaluation Metric: Since the importance of events is on a continuous scale, the boundary between “important” and “not important” is vague.",6.1 Event Salience Detection,[0],[0]
Hence we evaluate it as a ranking problem.,6.1 Event Salience Detection,[0],[0]
"The metrics are the precision and recall value at 1, 5 and 10 respectively.",6.1 Event Salience Detection,[0],[0]
It is adequate to stop at 10 since there are less than 9 salient events per document on average (Table 1).,6.1 Event Salience Detection,[0],[0]
We also report Area Under Curve (AUC).,6.1 Event Salience Detection,[0],[0]
Statistical significance values are tested by permutation (randomization) test with p < 0.05.,6.1 Event Salience Detection,[0],[0]
Implementation Details:,6.1 Event Salience Detection,[0],[0]
"We pre-trained word embeddings with 128 dimensions on the whole Annotated New York Times corpus using Word2Vec (Mikolov et al., 2013).",6.1 Event Salience Detection,[0],[0]
"Entities are extracted using the TagMe entity linking toolkit (Ferragina and Scaiella, 2010).",6.1 Event Salience Detection,[0],[0]
"Words or entities that appear only once in training are replaced with special “unknown” tokens.
",6.1 Event Salience Detection,[0],[0]
"The hyper-parameters of the KCE kernels follow previous literature (Xiong et al., 2017).",6.1 Event Salience Detection,[0],[0]
"There is one exact match kernel (µ = 1, σ = 1e−3) and ten soft-match kernels evenly distributed between (−1, 1), i.e. µ ∈ {−0.9,−0.7, . . .",6.1 Event Salience Detection,[0],[0]
", 0.9}, with the same σ = 0.1.
",6.1 Event Salience Detection,[0],[0]
"The parameters of the models are optimized by Adam (Kingma and Ba, 2015), with batch size 128.",6.1 Event Salience Detection,[0],[0]
The vectors of entities are initialized by the pre-trained embeddings.,6.1 Event Salience Detection,[0],[0]
Event embeddings are initialized by their headword embedding.,6.1 Event Salience Detection,[0],[0]
KCE is designed to estimate salience by modeling relations between discourse units.,6.2 The Event Intrusion Test: A Study,[0],[0]
"To better understand its behavior, we design the following event
intrusion test, following the word intrusion test used to assess topic model quality (Chang et al., 2009).",6.2 The Event Intrusion Test: A Study,[0],[0]
Event Intrusion Test:,6.2 The Event Intrusion Test: A Study,[0],[0]
"The test will present to a model a set of events, including: the origins, all events from one document; the intruders, some events from another document.",6.2 The Event Intrusion Test: A Study,[0],[0]
"Intuitively, if events inside a document are organized around the core content, a model capturing their relations well should easily identify the intruder(s).
",6.2 The Event Intrusion Test: A Study,[0],[0]
"Specifically, we take a bag of unordered events {O1, O2, . . .",6.2 The Event Intrusion Test: A Study,[0],[0]
", Op}, from a document O, as the origins.",6.2 The Event Intrusion Test: A Study,[0],[0]
"We insert into it intruders, events drawn from another document, I: {I1, I2, . . .",6.2 The Event Intrusion Test: A Study,[0],[0]
", Iq}.",6.2 The Event Intrusion Test: A Study,[0],[0]
"We ask a model to rank the mixed event set M = {O1, I1, O2, I2, . . .}.",6.2 The Event Intrusion Test: A Study,[0],[0]
We expect a model to rank the intruders Ii below the origins Oi.,6.2 The Event Intrusion Test: A Study,[0],[0]
"Intrusion Instances: From the development set, we randomly sample 15,000 origin and intruding document pairs.",6.2 The Event Intrusion Test: A Study,[0],[0]
"To simplify the analysis, we only take documents with at least 5 salient events.",6.2 The Event Intrusion Test: A Study,[0],[0]
"The intruder events, together with the entities in the same sentences, are added to the origin document.",6.2 The Event Intrusion Test: A Study,[0],[0]
"Metrics: AUC is used to quantify ranking quality, where events in O are positive and events in I are negative.",6.2 The Event Intrusion Test: A Study,[0],[0]
"To observe the ranking among the salient origins, we compute a separate AUC score between the intruders and the salient origins, denoted as SAAUC.",6.2 The Event Intrusion Test: A Study,[0],[0]
"In other words, SA-AUC is the AUC score on the list with non-salient origins removed.",6.2 The Event Intrusion Test: A Study,[0],[0]
"Experiments Details: We take the full KCE model to compute salient scores for events in the mixed event set M , which are directly used for ranking.",6.2 The Event Intrusion Test: A Study,[0],[0]
Frequency is recounted.,6.2 The Event Intrusion Test: A Study,[0],[0]
"All other features (Table 2) are set to 0 to emphasize the relational aspects,
We experiment with two settings: 1. adding only the salient intruders.",6.2 The Event Intrusion Test: A Study,[0],[0]
2. adding only the non-salient intruders.,6.2 The Event Intrusion Test: A Study,[0],[0]
"Under both settings, the intruders are added one by one, allowing us to observe the score change regarding the number of intruders added.",6.2 The Event Intrusion Test: A Study,[0],[0]
"For comparison, we add a Frequency baseline, that directly ranks events by the Frequency feature.",6.2 The Event Intrusion Test: A Study,[0],[0]
This section presents the evaluations and analyses.,7 Evaluation Results,[0],[0]
We summarize the main results in Table 3.,7.1 Event Salience Performance,[0],[0]
Baselines:,7.1 Event Salience Performance,[0],[0]
Frequency is the best performing baseline.,7.1 Event Salience Performance,[0],[0]
Its precision at 1 and 5 are higher than 40%.,7.1 Event Salience Performance,[0],[0]
"PageRank performs worse than Frequency on all
the precision and recall metrics.",7.1 Event Salience Performance,[0],[0]
"Location performs the worst.
",7.1 Event Salience Performance,[0],[0]
Feature Based: LeToR outperforms the baselines significantly on all metrics.,7.1 Event Salience Performance,[0],[0]
"Particularly, its P@1 value outperforms the Frequency baseline the most (4.64%), indicating a much better estimation on the most salient event.",7.1 Event Salience Performance,[0],[0]
"In terms of AUC, LeToR outperforms Frequency by a large margin (11.19% relative gain).
",7.1 Event Salience Performance,[0],[0]
"Feature Ablation: To understand the contribution of individual features, we conduct an ablation study of various feature settings in Table 4.",7.1 Event Salience Performance,[0],[0]
We gradually add feature groups to the Frequency baseline.,7.1 Event Salience Performance,[0],[0]
The combination of Location (sentence location) and Frequency almost sets the performance for the whole model.,7.1 Event Salience Performance,[0],[0]
Adding each voting feature individually produces mixed results.,7.1 Event Salience Performance,[0],[0]
"However, adding all voting features improves all metrics.",7.1 Event Salience Performance,[0],[0]
"Though the margin is small, 4 of them are statistically signifi-
cant over Frequency+Location.
",7.1 Event Salience Performance,[0],[0]
"Kernel Centrality Estimation: The KCE model further beats LeToR significantly on all metrics, by around 5% on AUC and precision values, and by around 10% on the recall values.",7.1 Event Salience Performance,[0],[0]
"Notably, the P@1 score is much higher, reaching 50%.",7.1 Event Salience Performance,[0],[0]
"The large relative gain on all the recall metrics and the high performance on precision show that KCE works really well on the top of the rank list.
",7.1 Event Salience Performance,[0],[0]
"Kernel Ablation: To understand the source of performance gain of KCE, we conduct an ablation study by removing its components: -E removes of entity kernels; -EF removes the entity kernels and the features.",7.1 Event Salience Performance,[0],[0]
We observe a performance drop in both cases.,7.1 Event Salience Performance,[0],[0]
"Without entities and features, the model only using event information still performs similarly to Frequency.",7.1 Event Salience Performance,[0],[0]
"The drops are also a reflection of the small number of events (≈ 60 per document) comparing to entities (≈ 200 per document).
",7.1 Event Salience Performance,[0],[0]
The study indicates that the relational signals and features contain different but both important information.,7.1 Event Salience Performance,[0],[0]
Discussion: The superior results of KCE demonstrate its effectiveness in predicting salience.,7.1 Event Salience Performance,[0],[0]
So what additional information does it capture?,7.1 Event Salience Performance,[0],[0]
We revisit the changes made by KCE: 1.,7.1 Event Salience Performance,[0],[0]
it adjusts the embeddings during training.,7.1 Event Salience Performance,[0],[0]
2.,7.1 Event Salience Performance,[0],[0]
it introduces weighted soft count kernels.,7.1 Event Salience Performance,[0],[0]
"However, the PageRank baseline also does embedding tuning but produces poor results, thus the second change should be crucial.",7.1 Event Salience Performance,[0],[0]
We plot the learned kernel weights of KCE in Figure 2.,7.1 Event Salience Performance,[0],[0]
"Surprisingly, the salient decisions are not linearly related, nor even positively correlated to the weights.",7.1 Event Salience Performance,[0],[0]
"In fact, besides the “Exact Match” bin, the highest absolute weights actually appear at 0.3 and -0.3.",7.1 Event Salience Performance,[0],[0]
"This implies that embedding similarities do not directly imply salience, breaking some assumptions of the feature based model and PageRank.
",7.1 Event Salience Performance,[0],[0]
Case Study: We inspect some pairs of events and entities in different kernels and list some examples in Table 5.,7.1 Event Salience Performance,[0],[0]
The pre-trained embeddings are changed a lot.,7.1 Event Salience Performance,[0],[0]
"Pairs of units with different
raw similarity values are now placed in the same bin.",7.1 Event Salience Performance,[0],[0]
"The pairs in Table 3 exhibit interesting types of relations: e.g.,“arrest-charge” and “attack-kill” form script-like chains; “911 attack” forms a quasiidentity relation (Recasens et al., 2010) with “attack”; “business” and “increase” are candidates as frame-argument structure.",7.1 Event Salience Performance,[0],[0]
"While these pairs have different raw cosine similarities, they are all useful in predicting salience.",7.1 Event Salience Performance,[0],[0]
"KCE learns to gather these relations into bins assigned with higher weights, which is not achieved by pure embedding based methods.",7.1 Event Salience Performance,[0],[0]
The KCE has changed the embedding space and the scoring functions significantly from the original space after training.,7.1 Event Salience Performance,[0],[0]
This partially explains why the raw voting features and PageRank are not as effective.,7.1 Event Salience Performance,[0],[0]
Figure 3 plots results of the intrusion test .,7.2 Intrusion Test Results,[0],[0]
The left figure shows the results of setting 1: adding nonsalient intruders.,7.2 Intrusion Test Results,[0],[0]
The right one shows the results of setting 2: adding salient intruders.,7.2 Intrusion Test Results,[0],[0]
"The AUC is 0.493 and the SA-AUC is 0.753 if all intruders are added.
",7.2 Intrusion Test Results,[0],[0]
The left figure shows that KCE successfully finds the non-salient intruders.,7.2 Intrusion Test Results,[0],[0]
The SA-AUC is higher than 0.8.,7.2 Intrusion Test Results,[0],[0]
"Yet the AUC scores, which include the rankings of non-salience events, are rather close to random.",7.2 Intrusion Test Results,[0],[0]
"This shows that the salient events in the origin documents form a more cohesive group, making them more robust against the intruders; the non-salient ones are not as cohesive.
",7.2 Intrusion Test Results,[0],[0]
"In both settings, KCE produces higher SA-AUC than Frequency at the first 30%.",7.2 Intrusion Test Results,[0],[0]
"However, in setting 2, KCE starts to produce lower SA-AUC than Frequency after 30%, then gradually drops to 0.5 (random).",7.2 Intrusion Test Results,[0],[0]
This phenomenon is expected since the asymmetry between origins and intruders allow KCE to distinguish them at the beginning.,7.2 Intrusion Test Results,[0],[0]
"When all intruders are added, KCE performs worse because it relies heavily on the relations, which can be also formed by the salient intruders.",7.2 Intrusion Test Results,[0],[0]
"This phenomenon is observed only on the salient intruders, which again confirms the cohesive relations are found among salient events.
",7.2 Intrusion Test Results,[0],[0]
"In conclusion, we observe that the salient events form tight groups connected by discourse relations while the non-salient events are not as related.",7.2 Intrusion Test Results,[0],[0]
"The observations imply that the main scripts in documents are mostly anchored by small groups of salient events (such as the “Trial” script in
Example 1).",7.2 Intrusion Test Results,[0],[0]
"Other events may serve as “backgrounds” (Cheung et al., 2013).",7.2 Intrusion Test Results,[0],[0]
"Similarly, Choubey et al. (2018) find that relations like event coreference and sequence are important for saliency.",7.2 Intrusion Test Results,[0],[0]
"We propose two salient detection models, based on lexical relatedness and semantic relations.",8 Conclusion,[0],[0]
"The feature-based model with lexical similarities is effective, but cannot capture semantic relations like scripts and frames.",8 Conclusion,[0],[0]
"The KCE model uses kernels and embeddings to capture these relations, thus outperforms the baselines and feature-based models significantly.",8 Conclusion,[0],[0]
All the results are tested on our newly created large-scale event salience dataset.,8 Conclusion,[0],[0]
"While the automatic method inevitably introduces noises to the dataset, the scale enables us to study complex event interactions, which is infeasible via costly expert labeling.
",8 Conclusion,[0],[0]
"Our case study shows that the salience model finds and utilize a variety of discourse relations: script chain (attack and kill), frame argument relation (business and increase), quasi-identity (911 attack and attack).",8 Conclusion,[0],[0]
Such complex relations are not as prominent in the raw word embedding space.,8 Conclusion,[0],[0]
The core message is that a salience detection module automatically discovers connections between salience and relations.,8 Conclusion,[0],[0]
"This goes beyond prior centering analysis work that focuses on lexical and syntax and provide a new semantic view from the script and frame perspective.
",8 Conclusion,[0],[0]
"In the intrusion test, we observe that the small number of salient events are forming tight connected groups.",8 Conclusion,[0],[0]
"While KCE captures these relations quite effectively, it can be confused by salient intrusion events.",8 Conclusion,[0],[0]
"The phenomenon indicates that the salient events are tightly connected, which form
the main scripts of documents.",8 Conclusion,[0],[0]
This paper empirically reveals many interesting connections between discourse phenomena and salience.,8 Conclusion,[0],[0]
The results also suggest that core script information may reside mostly in the salient events.,8 Conclusion,[0],[0]
"Limited by the data acquisition method, this paper only models discourse salience as binary decisions.",8 Conclusion,[0],[0]
"However, salience value may be continuous and may even have more than one aspects.",8 Conclusion,[0],[0]
"In the future, we plan to investigate these complex settings.",8 Conclusion,[0],[0]
"Another direction of study is large-scale semantic relation discovery, for example, frames and scripts, with a focus on salient discourse units.",8 Conclusion,[0],[0]
This research was supported by DARPA grant FA8750-18-2-0018 funded under the AIDA program and National Science Foundation (NSF) grant IIS-1422676.,Acknowledgement,[0],[0]
"Any opinions, findings, and conclusions in this paper are the authors and do not necessarily reflect the sponsors.",Acknowledgement,[0],[0]
We thank the anonymous reviewers whose suggestions helped clarify this paper.,Acknowledgement,[0],[0]
Identifying the salience (i.e. importance) of discourse units is an important task in language understanding.,abstractText,[0],[0]
"While events play important roles in text documents, little research exists on analyzing their saliency status.",abstractText,[0],[0]
This paper empirically studies the Event Salience task and proposes two salience detection models based on content similarities and discourse relations.,abstractText,[0],[0]
The first is a feature based salience model that incorporates similarities among discourse units.,abstractText,[0],[0]
The second is a neural model that captures more complex relations between discourse units.,abstractText,[0],[0]
"Tested on our new largescale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin.",abstractText,[0],[0]
"Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).",abstractText,[0],[0]
Automatic Event Salience Identification,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 306–316, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
306",text,[0],[0]
The concept of causality can be informally introduced as a relationship between two events e1 and e2 such that occurrence of e1 results in the occurrence of e2.,1 Introduction,[0],[0]
Curating causal relations from text documents help in automatically building causal networks which can be used for predictive tasks.,1 Introduction,[0],[0]
Expression of causality can be expressed within text documents in arbitrarily complex ways.,1 Introduction,[0],[0]
"For example, in the sentence “Aircel
files for bankruptcy over mounting financial troubles”, the event “mounting financial troubles” is causing the event “Aircel filed for bankruptcy.”",1 Introduction,[0],[0]
"In a more complicated scenario, “Company recalled some vehicles to fix loose bolts that could lead to engine stall” we can observe nested cause-effect pairs.",1 Introduction,[0],[0]
"Here, the effect “company recalled vehicle” is caused by the event “to fix loose bolts is not easy to extract.",1 Introduction,[0],[0]
"That the cause “loose bolts” could lead to engine stall”, is even more difficult to detect.
",1 Introduction,[0],[0]
"While there has been a considerable body of researchers working in the area whose work has been reviewed in section 2, there are many challenges that are still not properly addressed.",1 Introduction,[0],[0]
Most of the earlier approaches have considered rule based or traditional machine learning algorithms which heavily depend on careful feature engineering.,1 Introduction,[0],[0]
"Though one sees adoption of deep learning techniques for causality extraction, it is still considerably low compared to other text mining tasks.",1 Introduction,[0],[0]
This is largely due to the unavailability of adequate annotated data: the only available dataset for evaluation is the SEMEVAL-10 Task 8 which is woefully inadequate to train such deep models.,1 Introduction,[0],[0]
"There are challenges with annotations of this data also (Rehbein and Ruppenhofer, 2017).
",1 Introduction,[0],[0]
"Most of the existing extraction mechanisms look for single word representation of events within a sentence, thereby yielding wrong results.",1 Introduction,[0],[0]
"For example, in the sentence “The AIDS pandemic caused by the spread of HIV infection” the cause and effect are both multi-word phrases i.e. “spread of HIV infection” and ‘AIDS pandemic’.",1 Introduction,[0],[0]
"However, SEMEVAL 2010 annotated dataset for this task mentions the cause and effect as “infection” and “pandemic” only.",1 Introduction,[0],[0]
"In another example, “Infectious diseases or communicable diseases are caused by bacteria, viruses, and parasites.”",1 Introduction,[0],[0]
", the need to extract multiple causal as well as effect events is obvious.",1 Introduction,[0],[0]
"The example sentence in the first paragraph not only demonstrates the need to
extract phrases as events, but also highlights how complex such statements can be, often without the use of known causal connectives like “causes, because of, leads to, after, due to” etc. which have been traditionally exploited by the community.
",1 Introduction,[0],[0]
"In this work, we explore the use of bidirectional LSTMs that can learn to detect causal instances from sentences.",1 Introduction,[0],[0]
"To address the paucity of training data, we propose the use of additional linguistic feature embeddings, over and above the regular word embeddings.",1 Introduction,[0],[0]
"With the use of such linguistically-informed deep architecture, we avoid the task of complex feature engineering.
",1 Introduction,[0],[0]
A major contribution of this work is in developing annotated datasets with information curated from multiple sources spanning across different domains.,1 Introduction,[0],[0]
"To do this, we have collected news articles and generate annotations.",1 Introduction,[0],[0]
"Beside SEMEVAL dataset we have also used another available dataset that has annotated data about drugs and their adverse effect extracted from Medline (Gurulingappa et al., 2012).",1 Introduction,[0],[0]
"We have done intensive experimentations with parts of the dataset for training and testing which will be discussed in the following sections.
",1 Introduction,[0],[0]
Detection of causal relation from text has many analytical and predictive applications.,1 Introduction,[0],[0]
"Few of these are: detecting cause-effect relations in medical documents, learning about after effects of natural disasters, learning causes for safety related incidents etc..",1 Introduction,[0],[0]
"However to build a meaningful application that can detect an event from texts and predict its possible effects, there is a need to curate large volume of cause-effect event pairs.",1 Introduction,[0],[0]
"Further, similar events need to be grouped and generalized to super classes, over which the predictive framework can be built(Zhao et al., 2017).",1 Introduction,[0],[0]
"In this paper, we have proposed a k-means clustering of causal and effect events detected from text, using word vector representations.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
Section 2 summarizes challenges and related works on causality detection.,1 Introduction,[0],[0]
Section 3 presents the resource creation and the architecture of the proposed causality extraction framework.,1 Introduction,[0],[0]
Experiments and evaluation are detailed in Section 4.,1 Introduction,[0],[0]
"Finally, in section 5 we conclude the paper.",1 Introduction,[0],[0]
Identification of causality is not a trivial problem.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
Causation can occur in various forms.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Two common differentiations are made on: a) Marked and Unmarked causality and b) Implicit and Explicit causality (Blanco et al., 2008)(Hendrickx et al., 2009)(Sorgente et al., 2013).",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Marked Causality is where there is a linguistic signal of causation present.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"For example, “I attended the event because I was invited”.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Here, causality is marked by because.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
On the other hand in “Drive slowly.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"There are potholes”, causality is unmarked.
",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Explicit Causality is where both cause and effect are stated.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"For example, “The burst has been caused by water hammer pressure” has both cause and effect stated explicitly.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"However, “The car ran over his leg” does not have the effect of the accident explicitly stated.
",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Automatic extraction of cause-effect relations are primarily based on three different approaches namely, Linguistic rule based, supervised and unsupervised machine learning approaches.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Both SemEval-2007 (Girju et al., 2007) & 2010 (Hendrickx et al., 2009) had tasks aimed at identifying different relations from text, including CauseEffect relations.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Both tasks offered a corpus of annotated gold standard data to researchers.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"However, the task has primarily focused on extracting single word cause-effect pairs.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Early work in this area relied totally on hand-coded patterns.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"These were heavily dependent on both domain and linguistic knowledge, due to the nature of the patterns, and were hard to scale up.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"PROTEUS (Grishman, 1988) and COATIS (Garcia, 1997) were two early systems that used such non-statistical techniques.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"C.G Khoo carried out extensive development of this train of thought in a series of works (Khoo et al., 1998)",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"(Khoo et al., 2001), and eliminated a lot of the need for domain knowledge.
",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"A method of automatically identifying linguistic patterns that indicate causal relations and a semi-supervised method of validation of patterns obtained was proposed by (Girju et al., 2002).",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"In particular, this work introduced the usage of WordNet hierarchal classes, namely, human action, phenomenon, state, psychological feature and event, as a distinguishing feature.
",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Radinsky et al. in their work uses statistical inferencing combined with hierarchical clustering technique to predict future events from
news (Radinsky et al., 2012).",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Logistic regression was employed (Bui et al., 2010) to extract drugs (cause) and virus mutation (effect) occurrences from medical literature.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"The relatively untouched task of extracting implicit cause-effect from sentences was tackled by Ittoo et.al (Ittoo and Bouma, 2011).",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"More recently, Zhao et al. (Zhao et al., 2017) have proposed novel causality network embeddings for the abstract representation of causal events from News headlines.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Here, the authors have primarily used four common causal connectives namely, “because”, “after”, “because of” and “lead to” to extract causal mentions in news headlines and constructed a network of causal relations.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
The authors have proposed a novel generalization technique to represent “specific events” into more abstract form.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Finally, they proposed a dual cause-effect model that uses the causal network embeddings and optimize the margin based loss function to predict effect of a given cause.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Although the work is commendable, there are various factors that need to be addressed further.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"For example, construction of the causal network itself is a non trivial task.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Some of the linguistic challenges have already mentioned earlier in this section.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"Further, Zhao et al. worked with only unambiguous causal connectives.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"On the contrary causal connectives can be ambiguous also (Sorgente et al., 2013)",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"(Hendrickx et al., 2009)",2 Challenges in Causality Detection and the State of the Art,[0],[0]
"For example, from in “Profits from the sale were given to charity” implies causation of profits due to the sale, while from in “Sales profits increased from 1.2% to 2%” does not have any causality involved in it.",2 Challenges in Causality Detection and the State of the Art,[0],[0]
Analysis of such complex constructs are yet to be addressed.,2 Challenges in Causality Detection and the State of the Art,[0],[0]
"The overall architecture of our proposed approach is composed of three modules: a)Resource Creation b) Linguistic preprocessor and feature extractor, c) Classification model builder, and d) Prediction framework for cause/effect, built on the output of the classifier module.",3 Proposed Methodology,[0],[0]
Each of the individual modules are described in the following subsections.,3 Proposed Methodology,[0],[0]
Data Description:,3.1 Resource Creation,[0],[0]
In this section we will discuss about the following dataset used to develop and test our proposed models.,3.1 Resource Creation,[0],[0]
"1) Part of the SemEval 2010 Task 8 data set dealing with“Cause-Effect”
relation, which consists of 1331 sentences.",3.1 Resource Creation,[0],[0]
2),3.1 Resource Creation,[0],[0]
"The adverse drug effect (ADE) dataset (Gurulingappa et al., 2012) composed of 1000 sentences consisting of information about consumption of different drugs and their associated side effects.",3.1 Resource Creation,[0],[0]
"3)The BBC News Article dataset, created by the Trinity College Computer Science Department, containing news articles in five topical areas : business, sports, tech, entertainment and politics from 2004-2005 (Greene and Cunningham, 2006).",3.1 Resource Creation,[0],[0]
"We have considered 140 business news articles, containing approximately 1950 sentences.",3.1 Resource Creation,[0],[0]
"Out of this, around 500 sentences were found to contain causation.",3.1 Resource Creation,[0],[0]
4)Around 4500 analyst reports of a specific organization over a period of seven months is the fourth dataset that we have considered.,3.1 Resource Creation,[0],[0]
We have manually extracted all the sentences that contained causation.,3.1 Resource Creation,[0],[0]
5),3.1 Resource Creation,[0],[0]
"The Recall dataset 1 is a collection of 1050 recall news of different products.
",3.1 Resource Creation,[0],[0]
"The first two datasets, that is, SemEval and ADE datasets, are already publicly available.",3.1 Resource Creation,[0],[0]
"However, for the SemEval dataset we have extended the annotation to phrase-level causal relationships.",3.1 Resource Creation,[0],[0]
"Hence the fresh annotations of these existing data sets, as well as parts of the annotated Recall news and BBC news datasets, will be publicly shared with this paper.",3.1 Resource Creation,[0],[0]
"We could not share the analyst report dataset due to copyright and IPR issues.
",3.1 Resource Creation,[0],[0]
Preprocessing: We perform a number of preprocessing over the collected dataset.,3.1 Resource Creation,[0],[0]
The first stage of preprocessing involves identifying which sentences are probably candidates for cause-effect identification out of a body of text.,3.1 Resource Creation,[0],[0]
This involves looking for the presence of at least one causal connective in the sentence under consideration.,3.1 Resource Creation,[0],[0]
"Xuelan (Xuelan and Kennedy, 1992) reported a list of 130 causal connectives in English.",3.1 Resource Creation,[0],[0]
"To extend the list we follow methods similar to Girju (Girju, 2003) and Blanco (Blanco et al., 2008).",3.1 Resource Creation,[0],[0]
"We use Wordnet (University, 2010) as our lexical database.",3.1 Resource Creation,[0],[0]
"An entry of WordNet, whose gloss definition contains any of the terms in the exist-
1https://www.edmunds.com/recalls/
Table 2: Annotation Examples
Honda/E1 Motor/E1 Co./E1 is/E1 recalling/E1 Acura/E1 ILX/E1 and/E1 ILX/E1 Hybrid/E1 vehicles/E1 because/CC1 excessive/C1 headlight/C1 temperatures/C1 pose/C1 a/C1 fire/C1 risk/C1.
",3.1 Resource Creation,[0],[0]
Attrition/C1 of/C1 associates/C1 will/,3.1 Resource Creation,[0],[0]
"CC1 effect/CC1 scheduled/E1/C2 release/E1/C2 of/E1/C2 product/E1/C2 causing/CC2 high/E2 business/E2 impact/E2.
",3.1 Resource Creation,[0],[0]
"ing causal list, is included in the list as a possible causal connectives.",3.1 Resource Creation,[0],[0]
"Once we have a list of words, we further expand the list by adding common phrases with contain one or more of these words.",3.1 Resource Creation,[0],[0]
"For example, the seed word causes is extended to include phrases like “one of the main causes of”, “a leading cause of” etc.",3.1 Resource Creation,[0],[0]
This gives us an extended connective list of 310 words/phrases.,3.1 Resource Creation,[0],[0]
Table 3 shows a few examples of seed words and new terms added to the list.,3.1 Resource Creation,[0],[0]
"After preprocessing, we finally obtained a dataset of 8K sentences for annotation in terms of their cause, effect and causal connectives.
",3.1 Resource Creation,[0],[0]
The Annotation Process: The above sentences are presented to three expert annotators.,3.1 Resource Creation,[0],[0]
The experts were asked to complete the following two tasks.,3.1 Resource Creation,[0],[0]
"a) Identify whether a given sentence contains a causal event (either cause/effect) and b) Annotate each word in a sentence in terms of the four labels cause (C), effect(E), causal connectives(CC) and None.",3.1 Resource Creation,[0],[0]
"An illustration of the annotated dataset is depicted in Table 2.
",3.1 Resource Creation,[0],[0]
"In some of the candidate sentences, it is observed that a single sentence contains multiple cause-effect pairs, some of which are even chained together.",3.1 Resource Creation,[0],[0]
"In order to handle multiple instances of causality present in the same sentence, sentences are split into sub-sentences.",3.1 Resource Creation,[0],[0]
"e.g. “In developing countries four-fifths of all the illnesses are caused by water-borne diseases with diarrhoea being the leading cause of childhood death” (Hendrickx et al., 2009).",3.1 Resource Creation,[0],[0]
"This sentence has two distinct causes and their corresponding effects : four-fifths of all the illnesses are caused by water-borne diseases and diarrhoea being the leading cause of childhood death.
",3.1 Resource Creation,[0],[0]
We have also observed a number of cases where a single sentence contains a chain of causal events where a cause event e1 results the effect of another event e2 which in turn causes event e3.,3.1 Resource Creation,[0],[0]
In such cases e2 will be marked as both effect for e1 and cause for e3.,3.1 Resource Creation,[0],[0]
"For example, in “The reactor meltdown caused a chain reaction that destroyed all the towers in the network” (Hendrickx et al., 2009), there are two different causalities, chained
Word2Vec
Linguistic
feature vector
Word
Embedding
Vector
Bi−LSTM
Softmax
cause−effect tags
values for each
LSTM LSTM LSTM LSTM
LSTM LSTM LSTM LSTM
",3.1 Resource Creation,[0],[0]
"Out Out Out Out
Forward LSTM
Backward LSTM
Output Layers
Tag Scores
Best Tag Sequence
Word Embeddings
Additional Linguistic Features
Honda Recalled 150000 SUVs
Effect Effect Effect Effect
Figure 1: Overview of the bidirectional LSTM architecture for Cause-Effect relation extraction.
together: (1)The reactor meltdown caused a chain reaction and (2)a chain reaction that destroyed all the towers in the network.",3.1 Resource Creation,[0],[0]
The effect in the first case and the cause in the second is “A chained reaction”.,3.1 Resource Creation,[0],[0]
Similar example illustrated with an annotation is depicted in example (2) of Table 2.,3.1 Resource Creation,[0],[0]
"In order to extract all instances of causality present in a sentence, the sentence is divided into subsentences.",3.1 Resource Creation,[0],[0]
"We use openIE (Schmitz et al., 2012) to extract multiple relationships from the sentence, and then treat each relationship as a separate sentence.
",3.1 Resource Creation,[0],[0]
"Based on the given annotation scheme, each of the annotator received around 2500 sentences.",3.1 Resource Creation,[0],[0]
"Out of these, 2000 sentences are unique and rest 500 are overlapping.",3.1 Resource Creation,[0],[0]
"Using these 500 common sentences, we measure the inter annotator agreement of the annotation using the Fleiss Kappa (Fleiss and Paik, 1981) measure (κ).",3.1 Resource Creation,[0],[0]
"This is computed as κ = P̄−P̄e
1−P̄e .",3.1 Resource Creation,[0],[0]
"The factor 1 − P̄e gives the de-
gree of agreement that is attainable above chance, and P̄",3.1 Resource Creation,[0],[0]
− P̄e gives the degree of agreement actually achieved above chance.,3.1 Resource Creation,[0],[0]
We have achieved the inter annotator agreement to be around 0.63.,3.1 Resource Creation,[0],[0]
This implies that the expert annotated dataset is reliable to be used for further processing.,3.1 Resource Creation,[0],[0]
Some more examples of annotated sentences are elaborated in the appendix A.,3.1 Resource Creation,[0],[0]
There is a recent surge of interest in deep neural network based models that are based on continuous-space representation of the input and non-linear functions.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Thus, such models are capable of modeling complex patterns in data and since they do not depend on manual engineering of features, they can be applied to solve problems in an end-to-end fashion.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"On the other hand, such neural network models fails to consider the latent linguistic characteristics of a text that can play an important role in extraction of the relevant information.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Therefore, we have proposed a deep neural network model based on the bidirectional long-short term memory (LSTM) model (Hochreiter and Schmidhuber, 1997)",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"(Schmidhuber et al., 2006) that along with the word embeddings, utilizes different linguistic features within a text for the automatic classification of cause-effect relations.
",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"In identification of causal relationships from text, the surrounding context is of paramount information.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"While typical LSTMs allow the preceding elements to be considered as context for an element under scrutiny, we prefer to use bidirectional LSTMs (Bi-LSTM) networks (Graves et al., 2012) that are connected so that both future and past sequence context can be examined, i.e. both preceding and succeeding elements can be considered.
",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
The overview of the proposed model is depicted in Figure 1.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Corresponding to each input text, we determine the word embedding representation of each words of the text and the different linguistic feature embeddings.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
The input to the BiLSTM unit is an embedding vector (E)which is the composition of the word embedding representation (We) and the linguistic feature embeddings (Wl).,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"This is represented as −→ E = −→ We ⊗−→ Wl
Generating Word Embeddings: Pre-trained GloVe word vector representations of dimension 300 have been used for this work (Pennington et al., 2014).",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"GloVe is a relatively recent method
of obtaining vector representations of words and has been proven to be effective.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Along with the GloVe vector, the embedding vector of each word is appended with the vector formed from the linguistic features that has been described in the earlier section.
",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
Generating linguistic feature embeddings:,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Apart from the presence of causal connectives mentioned earlier, other features added to make our model linguistically informed are relevant lexical and syntactic features : Part of Speech(POS) tags (Manning et al., 2014), Universal Dependency relations (De Marneffe et al., 2006) and position in Verb/",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
Noun/ Prepositional Phrase structure.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"We have also used the semantic features as identified by Girju (Girju, 2003) - the nine Noun hierarchies (H(1) to H(9)) in WordNet namely, entity, psychological feature, abstraction, state, event, act, group, possession, and phenomenon.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"First, a single feature Primary Causal Class (PCC) is defined for a word wi.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"If wi ∈ Hi where Hi is any of the nine WordNet hierarchies, PCC = Hi, else PCC = null.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Another feature, Secondary Causal Class(SCC) is also defined.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
This takes value H(i),3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"if any WordNet synonym of the word belongs to H(i), and is Null otherwise.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"Further, we consider the dependency structure of the sentence, which gives us thatwi is dependent on word pi.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"In addition to the five features described above for wi, we also consider the same five features of pi as part of wis feature set.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"If wi is not dependent on any other word in the sentence, then the parent features are the same as the word features.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"An example of the linguistic feature selection can be found in appendix A.
Network Architecture: We use a k-layer BiRNN, composed of k Bi-RNNs stacked, where the output of each such unit is the input to the next unit (Irsoy and Cardie, 2014).",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
A two-layer stack of Bi-LSTMs is employed for the purpose of experiments.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"The model is trained with Adam optimizer (Kingma and Ba, 2014) and dropout layer with the dropout value of 0.5 for each Bi-RNN.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"The dropout layer reduces the problem of overfitting often seen in trained models by dropping
unit with connections to the neural network at random during the training process (Srivastava et al., 2014).",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"The model is fit over runs of 2000 epochs, with batch size of 128.",3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
The loss is calculated as a function of the mean cross entropy generated.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
Each Bi-LSTM has 256 hidden layers and 1 final dense layer with softmax activation as output.,3.2 The linguistically informed Bi-directional LSTM model,[0],[0]
"We have applied the proposed causal extraction technique over a large set of data from four different domains namely, Analyst Reports, Adverse Drug Effects, Business News and Product Recall News.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
We observe that a number of extracted causal events shows high degree of semantic similarity.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"For example, “Engine breakdown” and “Engine failure” represents the same semantic sense.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Therefore, we intend to group these events into clusters.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Accordingly, we device a novel algorithm to determine similar causal events.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
The algorithm follows the following steps: a) first identify the word embeddings of each constituent word of a causal event.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"The word embeddings are identified using the standard GloVe representations (Pennington et al., 2014).",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Apart from the word embeddings, we have also created phrase embeddings by computing a tensor product between the individual word embeddings.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"For example, given two causal events C1 = w1, w2..., wi and C2 = w′1, w ′ 2, ...w ′ j , where w1, w2, ...wk and w′1, w ′",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"2...w ′ k are the constituent word embeddings of the causal events C1, and C2 such that i 6= j, the phrase embedding P(w1, w2) is created by computing the tensor product of each adjacent word embedding pairs.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"This is represented as P (w1, w2) =",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
w1 ⊗ w2.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
Similar word and phrase embeddings are constructed for causal event C2.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Consequently, we define A and B as the number of word embeddings in C1 andC2 respectively.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Similarly,A′ and B′ are the number of phrase embeddings in C1 and C2 respectively.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Therefore, the similarity
S(C1, C2) = (S′ + S′′)
N1 +N2
The expressionsN1 andN2 impliesA∪B andA′∪ B′ respectively.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"S′ and S′′ are computed as: S′ =∑ ∀wi∈C1 Swi and S ′′ = ∑ ∀pi∈C1 Spi Where,
Swi = max∀w′j∈C2 (Sim(wi, w
′ j))
",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Spi = max∀p′j∈C2 (Sim(pi, p
′ j))
",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Again, p and p′ are the individual phrase embeddings in sentence C1 and C2 respectively.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Sim(x, y) is the cosine similarity between the two word vector wx and wy.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Based on the similarity score, we perform a k-means clustering to form clusters of similar causal events.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
We have used the Average silhouette method to identify number of clusters k.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
For the present work we obtained the value of k as 21.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"A partial network of a few representative clusters, as obtained from the vehicle Recall database, is shown in Figure 2.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"For each cluster, the size is given as number of phrases that constitute the cluster, and a few representative phrases of each cluster is also shown as reference.",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
The name of the cluster is chosen from the most common noun chunks present in the cluster.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"The network itself is shown as a directed graph, with edges directed from Cause to Effect, as edge weights being computed as the fraction of total occurrences of the cause that lead to the effect.
",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
"Following the method each cluster can be further represented by a verb-noun pair as proposed in (Zhao et al., 2017).",3.3 Causal Embeddings for Representing Similar Events,[0],[0]
For noisy clusters where no such generalization is possible are left out for the time being.,3.3 Causal Embeddings for Representing Similar Events,[0],[0]
We perform a number of different experiments to evaluate and compare the performance of our proposed system with the baseline systems.,4 Experiments and Results,[0],[0]
In general we classify the experiments into three different groups.,4 Experiments and Results,[0],[0]
Each group uses different techniques to identify causality in text.,4 Experiments and Results,[0],[0]
"Group-1 uses rule based method, group-2 uses a CRF based classification model, group-3 uses Bi-LSTM model and group-4 uses our proposed linguistically informed Bi-LSTM model.",4 Experiments and Results,[0],[0]
The outputs of the experiments are evaluated in terms of the five given datasets that are explained earlier.,4 Experiments and Results,[0],[0]
"Again, corresponding to each group, we define three different evaluation tasks.",4 Experiments and Results,[0],[0]
"The tasks are distinguished in terms of the way each datasets are divided for training, development and testing purposes.
",4 Experiments and Results,[0],[0]
"In Task-I, we took the five datasets separately and each dataset is divided into 80%, 10% and 10% for training, testing and development respectively.",4 Experiments and Results,[0],[0]
"The F1 scores obtained by each system on the datasets by this model are reported in Table 4 for identified Cause, Effect and Causal Connec-
tives.",4 Experiments and Results,[0],[0]
"In Task-II, we combine all the five datasets together and divide the training set, development set and test sets into 80%, 10% and 10% respectively.",4 Experiments and Results,[0],[0]
The division in dataset follows a five-fold manner.,4 Experiments and Results,[0],[0]
"Therefore, the 10% testing data in fold-1 is different from the 10% testing data in fold-2 or fold-3.",4 Experiments and Results,[0],[0]
"We compute the individual results and report the average of them.
",4 Experiments and Results,[0],[0]
"Finally, in Task-III, we train the model using one dataset and test it to other four models.",4 Experiments and Results,[0],[0]
"We conducted the experiments using the designated training portions of each dataset of BBC news, Recall News, Analyst Reports and SemEval individually to train the model and then tested all the sets on each resultant model.",4 Experiments and Results,[0],[0]
"Of these, the best results were seen to be from the model trained on the BBC dataset.
",4 Experiments and Results,[0],[0]
"From Table 4 we observe that in most of the
Cause Identification
Effect Identification
cases Bi-directional LSTM model along with the additional layer of linguistic features significantly reduces the false negative score and achieved a high true positive score thereby achieving a high F-measure.",4 Experiments and Results,[0],[0]
"For the project analyst report, BBC News, SEMEVAL and Recall news, we have achieved F-measures of around 66%, 73%, 79%, and 78% respectively which is best as compared to the other baseline methods.",4 Experiments and Results,[0],[0]
"For the ADE dataset, the CRF classifier performs better than the proposed deep learning techniques, at about 73%.",4 Experiments and Results,[0],[0]
"The inclusion of openIE as a sentence-splitter
Connective Identification
gave the most significant improvements in situations where the sentence structure was not overtly complicated, despite of the presence of multiple causal instances.",4 Experiments and Results,[0],[0]
"Hence, the SemEval and ADE dataset results gained most from it.",4 Experiments and Results,[0],[0]
"However, sentences from news sources often had a far more complicated structure than what OpenIE could resolve.",4 Experiments and Results,[0],[0]
The presence of descriptive clause along with valid cause/effect phrases made it difficult for the system to correctly identify and localize the valid phrases.,4 Experiments and Results,[0],[0]
"In fact, the system suffered when working with such sentences, even when there was just a single instances of causality present.",4 Experiments and Results,[0],[0]
"In the SemEval dataset, openIE usage led to identification of multiple causality in around 1/4th of the cases where multiple causality was indeed present.",4 Experiments and Results,[0],[0]
"However, in the BBC News dataset, this amount was barely 8% of all the sentences that contained multiple instances of causation.
",4 Experiments and Results,[0],[0]
"On an average, around 7% cases the system incorrectly predicted a cause/effect relation as valid which is actually not, whereas only 4% of the sentences were incorrectly identified as “Not an cause/effect” despite being marked as “cause/effect” by the experts.",4 Experiments and Results,[0],[0]
"The primary reason behind this is due to fact that most of the collected texts are noisy, as a result of which the dependency parser fails to parse the texts properly and thus returning incorrect linguistic feature values.",4 Experiments and Results,[0],[0]
"For ADE dataset, we observed that a large number of descriptions are written in languages other than English, as a result of which the classifier failed to predict correctly.",4 Experiments and Results,[0],[0]
Another source of error is the occurrence of incomplete sentences that restricts the classification engine to correctly label the descriptions.,4 Experiments and Results,[0],[0]
"Apart from labeling the cause and effect events, the proposed classifier also aims to label
the explicit causal connectives.",4 Experiments and Results,[0],[0]
Table 4 reports the results of the connective classification.,4 Experiments and Results,[0],[0]
We have observed that the proposed classification model is able to identify novel causal connectives that were previously not enlisted in the original causal connective list.,4 Experiments and Results,[0],[0]
We previously mentioned that existing schemes of having a single word represent cause and effect leads to a loss of information.,4 Experiments and Results,[0],[0]
"Just in the SemEval dataset, just 33% of the total corpus is such that their given single-word annotation effectively captures all the information about the causal event present in the sentence.",4 Experiments and Results,[0],[0]
Using our proposed methodology and extending the scheme to phrases give us the complete causal information in almost 60% of the sentences that were only partially covered previously.,4 Experiments and Results,[0],[0]
"However, we are able to somewhat quantify this observation only for the SemEval dataset, since the other datasets do not have a single-word gold standard annotation.",4 Experiments and Results,[0],[0]
"As discussed in section 2, ambiguous causatives are a big contributor to causality being identified when it is not actually present in the sentence.",4 Experiments and Results,[0],[0]
"Examples of some common ambiguous causal connectives, as well some of the novel connectives identified by the system (which were not present in our original list), are given in Appendix A.",4 Experiments and Results,[0],[0]
"In addition to the above results, Figures 3, 4 and 5 show the relative performances of models trained with the individual datasets and then tested on all the test sets (Task-III).",4 Experiments and Results,[0],[0]
"In this paper, we present a linguistically informed deep neural network architecture for the automatic extraction of cause-effect relations from text documents.",5 Conclusion,[0],[0]
Our proposed architecture uses word level embeddings and other linguistic features to detect causal events and their effects.,5 Conclusion,[0],[0]
We evaluate the performance of the proposed model with respect to a rule based classifier and a conditional random field (CRF) based supervised classifier.,5 Conclusion,[0],[0]
We find that the bi-directional LSTM model along with an additional linguistic layer performs much better than existing baseline systems.,5 Conclusion,[0],[0]
"Along with the extraction task another important contribution of this work is the development of new dataset annotated in terms of the cause-effect relations, which will be publicly shared with this paper for further research in this domain.",5 Conclusion,[0],[0]
"We use this section to elaborate on certain aspects of our work with the help of some more examples.
",A Appendix,[0],[0]
Table 6 shows the list of linguistic features constructed for each word of an example sentence.,A Appendix,[0],[0]
"W1-W6 are similarly features of the original word, which are, in order,part of speech tag, universal dependency tag, parent word id, phrase structure, primary causal class and secondary causal class.",A Appendix,[0],[0]
"Feature P is the parent word, and P1-P6 are the features of the parent word, similar to those described as W1-W6.",A Appendix,[0],[0]
"Finally, the last column is the label associated with the word.",A Appendix,[0],[0]
"C implies Cause, CN implies Causal Connective, E implies Effect, and N implies None.
",A Appendix,[0],[0]
Table 7 shows some more typical cases of causal sentences encountered and their respective annotations.,A Appendix,[0],[0]
"As explained, the four annotation labels are cause (C), effect(E), causal connectives(CC) and None(N).",A Appendix,[0],[0]
The second sentence contains an example of a phrase irrelevant to the actual causality that is present in the target sentence.,A Appendix,[0],[0]
"In the current work, preciseness of the solution is dependent on it correctly disregarding the irrelevant portion and identifying causality only in the rest of the sentence.",A Appendix,[0],[0]
"The third sentence, on
the other hand, shows an example of one of the more challenging scenarios of causality identification, i.e. in the absence of any explicit causal connective.",A Appendix,[0],[0]
"While the causality in the given sentence looks obvious to an observer, the challenge lies in the fact that there are possible grammatically and structurally similar sentences that do not contain causality.
",A Appendix,[0],[0]
Table 8 shows some common ambiguous causal connectives that identify sentences as causal even in the cases where they are not being used to identify causality.,A Appendix,[0],[0]
"To further emphasize on their ambiguity, we show, in parallel, examples where the same connectives imply causality.
",A Appendix,[0],[0]
Table 5 depicts a sample set of novel causal connectives identified by our system.,A Appendix,[0],[0]
In this paper we have proposed a linguistically informed recursive neural network architecture for automatic extraction of cause-effect relations from text.,abstractText,[0],[0]
These relations can be expressed in arbitrarily complex ways.,abstractText,[0],[0]
The architecture uses word level embeddings and other linguistic features to detect causal events and their effects mentioned within a sentence.,abstractText,[0],[0]
"The extracted events and their relations are used to build a causal-graph after clustering and appropriate generalization, which is then used for predictive purposes.",abstractText,[0],[0]
"We have evaluated the performance of the proposed extraction model with respect to two baseline systems,one a rule-based classifier, and the other a conditional random field (CRF) based supervised model.",abstractText,[0],[0]
"We have also compared our results with related work reported in the past by other authors on SEMEVAL data set, and found that the proposed bidirectional LSTM model enhanced with an additional linguistic layer performs better.",abstractText,[0],[0]
"We have also worked extensively on creating new annotated datasets from publicly available data, which we are willing to share with the community.",abstractText,[0],[0]
Automatic Extraction of Causal Relations from Text using Linguistically Informed Deep Neural Networks,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1098–1107, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"People use language to communicate not only facts, but also intentions, uncertain information and points of view.",1 Introduction,[0],[0]
"Modality can be broadly defined as a grammatical phenomenon used to express the speaker’s opinion or attitude towards a proposition (Lyons, 1977).",1 Introduction,[0],[0]
"Modality has also been defined as “the category of meaning used to talk about possibilities and necessities, essentially, states of affairs beyond the actual.”",1 Introduction,[0],[0]
"(Hacquard, 2011).",1 Introduction,[0],[0]
"Within computational linguistics, processing modality has proven useful for, among others, recognizing textual entailment (Snow et al., 2006; MacCartney et al., 2006), machine translation (Murata et al., 2005; Baker et al., 2012), and sentiment analysis (Wiebe et al., 2005).
",1 Introduction,[0],[0]
"In the absence of modality markers, it is understood that the author of a proposition agrees with it (Hengeveld and Mackenzie, 2008).",1 Introduction,[0],[0]
"Adding a modality marker—also referred to as cue—casts doubt on the truth of the proposition, e.g., Mary got a new job last week vs. Mary likely got a new job last week.",1 Introduction,[0],[0]
"Modality is surprisingly common (Morante
and Sporleder, 2012), and notoriously difficult to annotate and process automatically (Rubinstein et al., 2013; Vincze et al., 2011).",1 Introduction,[0],[0]
"In MEDLINE, 11% of sentences contain speculative language (Light et al., 2004) and in biomedical abstracts, 18% (Vincze et al., 2008).",1 Introduction,[0],[0]
Rubin (2006) reports that 59% of statements in 80 New York Times articles include epistemic modality.,1 Introduction,[0],[0]
"Despite modality being ubiquitous, there is not an agreed upon annotation schema.
",1 Introduction,[0],[0]
"In this paper, we extract implicit interpretations intuitively understood by humans when reading modal constructions.",1 Introduction,[0],[0]
We do not follow any specific theory of modality.,1 Introduction,[0],[0]
"Instead, we manipulate modal constructions to automatically generate potential interpretations, and then assign factuality scores to them.",1 Introduction,[0],[0]
"Consider statement (1) below:
1.",1 Introduction,[0],[0]
"John likely contracted the disease when a mouse bit him in the Adirondacks.
",1 Introduction,[0],[0]
"Even though likely syntactically attaches to contracted, a natural reading suggests that John contracted the disease is factual; the only bit of uncertain information is how (or when)",1 Introduction,[0],[0]
he contracted the disease.,1 Introduction,[0],[0]
"In other words, assuming that the author of statement (1) is truthful, event contracted occurred with AGENT John and THEME the disease, but the MANNER (or TIME) may not have been when a mouse bit him in the Adirondacks.
",1 Introduction,[0],[0]
A key feature of the work presented in this paper is that the interpretations extracted from modal constructions are not tied to any syntactic or semantic representation.,1 Introduction,[0],[0]
"Given modal constructions in plain text, we extract implicit interpretations in plain text, and these interpretations can be processed with any existing NLP pipeline.",1 Introduction,[0],[0]
"The main contributions of
1098
this paper are: (1) procedure to automatically generate potential interpretations from modal constructions; (2) annotations assessing the factuality of potential interpretations generated from OntoNotes;1 and (3) experimental results using several features.",1 Introduction,[0],[0]
"Theoretical works in philosophy and linguistics have studied modality for decades (Palmer, 2001; Jespersen, 1992).",2 Previous Work,[0],[0]
"Morante and Sporleder (2012) summarize some of these works and related phenomena, e.g., evidentiality, certainty, factuality, subjectivity.",2 Previous Work,[0],[0]
"There are several expressions that have modal meanings (Fintel, 2006), including auxiliaries (must, should, etc.), adverbs (perhaps, possibly, etc.) nouns (possibility, chance, etc.) adjectives (necessary, possible, etc.)",2 Previous Work,[0],[0]
"and conditionals (e.g., If the light is on, Sandy is home).",2 Previous Work,[0],[0]
"Most previous works in computational linguistics target modal adverbs (Rubinstein et al., 2013; Carretero and Zamorano-Mansilla, 2013; de Waard and Maat, 2012), and some also target other modal triggers such as reporting verbs (e.g., The evidence suggests that he caused the fire), references, or all verbs (Diab et al., 2009).",2 Previous Work,[0],[0]
"Following these previous works, we focus on modal adverbs.
",2 Previous Work,[0],[0]
"Beyond theoretical works, there are many proposals to annotate modality.",2 Previous Work,[0],[0]
"Doing so has proven challenging: following different annotations schemas on the same source text yields little overlap (Vincze et al., 2011), and Carretero and Zamorano-Mansilla (2013) present an analysis of disagreements when targeting modal adverbs.",2 Previous Work,[0],[0]
"Annotation schemas typically include 3 tasks: identifying modality triggers, their scopes, and sources (Quaresma et al., 2014; Sánchez and Vogel, 2015).",2 Previous Work,[0],[0]
"Many also classify the modality into several types (epistemic, circumstantial, ability, deontic, etc.) or a fine-grained taxonomy (Rubinstein et al., 2013; Nissim et al., 2013).",2 Previous Work,[0],[0]
"In this paper, we are not concerned with modeling modality per se, or classifying instances of modality into predefined classes or hierarchies.",2 Previous Work,[0],[0]
"Instead, we extract implicit interpretations from modal constructions in order to mirror intuitive readings.
",2 Previous Work,[0],[0]
"FactBank is probably the best-known corpus for event factuality (Saurı́ and Pustejovsky, 2009).",2 Previous Work,[0],[0]
"It was created following carefully crafted annotation
1Available at www.sanders.tech
guidelines and examples comprising 34 pages.2",2 Previous Work,[0],[0]
"The guidelines detail a manual normalization step to “identify the full event that needs to be assessed in terms of its factuality” (p. 12), and the annotation process includes identifying the sources that are assessing factuality (p. 15).",2 Previous Work,[0],[0]
de Marneffe et al. (2012) reannotate a subset of FactBank with factuality values from the reader’s perspective—they call it veridicality—using crowdsourcing.,2 Previous Work,[0],[0]
"Both FactBank and de Marneffe et al. (2012), rely on manual normalization to identify the eventuality whose factuality is being annotated.",2 Previous Work,[0],[0]
"Instead, we present an automated approach: we manipulate semantic roles and syntactic dependencies deterministically to generate several potential interpretations per modal construction, and then assess their factuality.
",2 Previous Work,[0],[0]
"Many other efforts expand on FactBank using crowdsourced annotations, different annotation schemas (usually simpler) or other domains.",2 Previous Work,[0],[0]
"Prabhakaran et al. (2012) use crowdsourcing to classify propositions into 5 modalities: ability, effort, intention, success and want.",2 Previous Work,[0],[0]
Soni et al. (2014) target the factuality of quotes (direct and indirect) in Twitter.,2 Previous Work,[0],[0]
Lee et al. (2015) detect events and assess factuality using easy-to-understand short instructions to crowdsource annotations.,2 Previous Work,[0],[0]
"Unlike us, they annotate factuality at the individual token level, where annotated tokens are deemed events by annotators.",2 Previous Work,[0],[0]
"Prabhakaran et al. (2015) define and annotate propositional heads with four categories: (1) non-belief propositions, or (2) committed, non-committed or reported belief.",2 Previous Work,[0],[0]
"Instead of assessing factuality only for propositional heads (usually verbs, one assessment per proposition), we do so for potential interpretations automatically generated by manipulating verbs and their arguments deterministically.
",2 Previous Work,[0],[0]
"All works cited in the previous two paragraphs either manually normalize text prior to assessing factuality—making automation from plain text impossible—or assess factuality for tokens deemed events (ordered, delay, agreed, etc.) or full propositions (a verb and all its arguments).",2 Previous Work,[0],[0]
"Unlike them, we automatically generate potential interpretations from a single modal construction—or, equivalently, automatically generate several normalizations—and then assess their factuality.
2https://catalog.ldc.upenn.edu/docs/ LDC2009T23/annotationGuidelines.pdf",2 Previous Work,[0],[0]
"We use the term modal construction to refer to verbargument structures modified by a modal adverb (possibly, probably, etc.).",3 Terminology and Background,[0],[0]
"We use the term implicit interpretation, or interpretation to save space, to refer to meaning intuitively understood by humans when reading a modal construction.",3 Terminology and Background,[0],[0]
Potential interpretations are interpretations automatically generated whose factuality has yet to be determined.,3 Terminology and Background,[0],[0]
"The factuality of an interpretation is a score indicating its likelihood—whether it is true, false or unknown given the modal construction.
",3 Terminology and Background,[0],[0]
"We work on top of OntoNotes (Hovy et al., 2006) because it includes text from several genres (news, broadcast and telephone conversations, weblogs, etc.) and includes part-of-speech tags, parse trees, PropBank-style semantic roles and other linguistic information.3",3 Terminology and Background,[0],[0]
"Very briefly, PropBank (Palmer et al., 2005) has two kinds of semantic roles: numbered roles (ARG0, ARG1, etc.), which are defined in verb-specific framesets, and argument modifiers (ARGM-TMP, ARGM-LOC, etc.), we refer the reader to the aforementioned reference, and the guidelines and framesets4 for more details.",3 Terminology and Background,[0],[0]
We transformed the parse trees in OntoNotes into syntactic dependencies using Stanford CoreNLP,3 Terminology and Background,[0],[0]
"(Manning et al., 2014).",3 Terminology and Background,[0],[0]
We define a two-step procedure to create a corpus of modal constructions and the implicit interpretations intuitively understood by humans when reading them.,4 Corpus Creation,[0],[0]
"First, we automatically generate potential interpretations from modal constructions by manipulating syntactic dependencies and semantic roles.",4 Corpus Creation,[0],[0]
"Second, we manually score potential interpretations according to their likelihood.",4 Corpus Creation,[0],[0]
These interpretations and scores are later used to learn how to score potential interpretations automatically (Section 6).,4 Corpus Creation,[0],[0]
Selecting Modal Constructions.,4.1 Generating Potential Interpretations,[0],[0]
"OntoNotes is a large corpus containing 63,918 sentences.",4.1 Generating Potential Interpretations,[0],[0]
Creating a corpus of interpretations for all modal constructions is outside the scope of this paper.,4.1 Generating Potential Interpretations,[0],[0]
"In order
3We use the CoNLL-2011 Shared Task distribution (Pradhan et al., 2011), http://conll.cemantix.org/2011/
4http://propbank.github.io/
to alleviate the annotation effort, we focus on selected modal constructions.",4.1 Generating Potential Interpretations,[0],[0]
"Specifically, we select verb-argument structures that have one ARGM-ADV or ARGM-MNR role, and that role is one of the following modal adverbs: certainly, clearly, definitely, likely, obviously, possibly, probably, surely, or unlikely.",4.1 Generating Potential Interpretations,[0],[0]
These adverbs are the most frequent that satisfy the above filter.,4.1 Generating Potential Interpretations,[0],[0]
"Additionally, we discard verbargument structures with to be as the main verb.",4.1 Generating Potential Interpretations,[0],[0]
These rules retrieve 324 modal constructions.,4.1 Generating Potential Interpretations,[0],[0]
Automatic Normalization.,4.1 Generating Potential Interpretations,[0],[0]
Modal constructions often occur in long multi-clause sentences.,4.1 Generating Potential Interpretations,[0],[0]
"In order to identify the eventuality from which potential interpretations should be generated, we automatically normalize the original sentence.",4.1 Generating Potential Interpretations,[0],[0]
Normalizing consists of a battery of deterministic steps implemented using syntactic dependencies and semantic roles.,4.1 Generating Potential Interpretations,[0],[0]
"In contrast with previous work (Section 2), our normalization is fully automated.",4.1 Generating Potential Interpretations,[0],[0]
"Hereafter, we use verb to refer to the main verb in the modal construction, adverb to the modal adverb, and sem roles to all semantic roles in the modal construction.
1.",4.1 Generating Potential Interpretations,[0],[0]
Remove adverb.,4.1 Generating Potential Interpretations,[0],[0]
2.,4.1 Generating Potential Interpretations,[0],[0]
"Convert negated verb-argument structures into
their positive counterparts.",4.1 Generating Potential Interpretations,[0],[0]
"We follow 3 steps inspired by the rules to form negation proposed by (Huddleston and Pullum, 2002): (a) Remove the negation mark by deleting the
token whose syntactic dependency is neg.",4.1 Generating Potential Interpretations,[0],[0]
"(b) Remove auxiliaries, expand contractions,
and fix third-person singular and past tense.",4.1 Generating Potential Interpretations,[0],[0]
"For example (before: after), doesn’t go: goes, didn’t go: went, won’t go: will go.",4.1 Generating Potential Interpretations,[0],[0]
"To implement this step, we loop through tokens whose head is the negated verb with dependency aux, and use a list of irregular verbs5 and grammar rules to convert to third-person singular and past tense based on orthographic patterns.",4.1 Generating Potential Interpretations,[0],[0]
(c) Rewrite negatively-oriented polaritysensitive items.,4.1 Generating Potential Interpretations,[0],[0]
"For example (before: after), anyone: someone, any longer: still, yet: already.",4.1 Generating Potential Interpretations,[0],[0]
at all: somewhat.,4.1 Generating Potential Interpretations,[0],[0]
"We use the correspondences between negatively-oriented and positively-
5https://en.wikipedia.org/wiki/English_ irregular_verbs
oriented polarity-sensitive items by (Huddleston and Pullum, 2002, pp. 831).
3.",4.1 Generating Potential Interpretations,[0],[0]
Fix modal verbs and tense.,4.1 Generating Potential Interpretations,[0],[0]
"If a modal verb (can, could, may, would, should, must, etc.) has as syntactic head verb, we transform the modal construction into past or future depending on the modal and tense of verb.",4.1 Generating Potential Interpretations,[0],[0]
"For example: could go: went, can go: will go, should have gone: went.",4.1 Generating Potential Interpretations,[0],[0]
We use the same grammar rules and list of irregular verbs as in Step (2b).,4.1 Generating Potential Interpretations,[0],[0]
4.,4.1 Generating Potential Interpretations,[0],[0]
Select relevant tokens.,4.1 Generating Potential Interpretations,[0],[0]
We remove all tokens in the original sentence except verb and tokens belonging to the roles in sem roles.,4.1 Generating Potential Interpretations,[0],[0]
"Additionally, we fix phrasal verbs by adding tokens with the part-of-speech tag RP whose syntactic head is verb and dependency type prt (semantic roles in OntoNotes are annotated for verb tokens, missing the preposition when verb is a phrasal verb would inadvertently change meaning).",4.1 Generating Potential Interpretations,[0],[0]
"We also add all tokens to the left of verb until we find the first token whose part-of-speech tag does not start with VB, MD, RB or EX (verbs, modals, adverbs and existential there).",4.1 Generating Potential Interpretations,[0],[0]
5.,4.1 Generating Potential Interpretations,[0],[0]
Generate additional normalizations.,4.1 Generating Potential Interpretations,[0],[0]
"If verb is
followed by TO + verb2 (e.g., want to go, like to play, intend to pass), we generate an additional normalization for verb2 after merging the semantic roles of verb and",4.1 Generating Potential Interpretations,[0],[0]
"verb2.
Table 1 exemplifies the automatic normalization step by step with 2 modal constructions.",4.1 Generating Potential Interpretations,[0],[0]
Generating Potential Interpretations in Plain Text.,4.1 Generating Potential Interpretations,[0],[0]
"Inspired by the rules Blanco and Sarabi (2016) used to generate interpretations from negation, we generate potential interpretations from modal constructions by toggling off combinations of roles in sem roles.",4.1 Generating Potential Interpretations,[0],[0]
"We consider numbered roles (ARG0– ARG5), and argument modifiers (ARGM-) ending in LOC, TMP, MNR, PRP, CAU, EXT, PRD or DIR.
",4.1 Generating Potential Interpretations,[0],[0]
Table 1 lists some potential interpretations generated from a sample modal construction.,4.1 Generating Potential Interpretations,[0],[0]
"The total number of potential interpretations for the 324 selected modal construction is 1,756 (average: 5.4).
",4.1 Generating Potential Interpretations,[0],[0]
We recognize that our procedure to generate implicit interpretations is unable to generate some useful interpretations.,4.1 Generating Potential Interpretations,[0],[0]
"For example, from This is [a person who]ARG1 [likely]ARGM-ADV [died]verb [on impact versus perhaps freezing to death]ARGM-MNR , we
generate This is a person who died {ARGM-MNR}, which is factual: the only uncertain information is the manner in which the person died.",4.1 Generating Potential Interpretations,[0],[0]
"Since we toggle off semantic roles of verb, our procedure is unable to generate A person died on impact and A person died freezing to death; the former interpretation would receive a higher factuality score than the latter.",4.1 Generating Potential Interpretations,[0],[0]
"We argue that automation is preferable, and reserve for future work generating interpretations that require splitting semantic roles.",4.1 Generating Potential Interpretations,[0],[0]
"After automatically generating potential interpretations, we collected manual annotations to determine their factuality.",4.2 Scoring Potential Interpretations,[0],[0]
"The annotation interface showed the original sentence containing the modal construction, the previous and next sentences as context, and no additional information.",4.2 Scoring Potential Interpretations,[0],[0]
"Following previous work (Saurı́ and Pustejovsky, 2009; de Marneffe et al., 2012), we found it useful not to restrict answers to yes or no, but to allow for degrees of certainty.",4.2 Scoring Potential Interpretations,[0],[0]
"Specifically, we asked “Given the 3 sentences above, do you believe that the statement [potential interpretation] below is true?”.",4.2 Scoring Potential Interpretations,[0],[0]
"Answers are a score ranging from −5 to 5, where −5 indicates Certainly no, 5 indicates Certainly yes, and the scores in between indicate a continuum of certainty (0 indicates unknown).
",4.2 Scoring Potential Interpretations,[0],[0]
"After pilot annotations, we examined disagreements and defined the following simple guidelines:
1.",4.2 Scoring Potential Interpretations,[0],[0]
"Context (previous sentence, target sentence, and next sentence) is taken into account.",4.2 Scoring Potential Interpretations,[0],[0]
2.,4.2 Scoring Potential Interpretations,[0],[0]
World knowledge available at the time the original sentence was authored—not new knowledge available after—is taken into account.,4.2 Scoring Potential Interpretations,[0],[0]
3.,4.2 Scoring Potential Interpretations,[0],[0]
"Semantic roles toggled off are replaced with a semantically related substitute (Turney and Pantel, 2010) for the original role, e.g., give: take, customer: sales associate.",4.2 Scoring Potential Interpretations,[0],[0]
"The total number of modal constructions selected is 324 and the number of potential interpretations automatically generated in 1,756 (average: 5.4 interpretation per modal construction).",5 Corpus Analysis,[0],[0]
39.4% of interpretations are scored with a high degree of certainty.,5 Corpus Analysis,[0],[0]
We define high certainty as a score below −3,5 Corpus Analysis,[0],[0]
"(interpretation is false) or larger than 3 (interpretation is
true).",5 Corpus Analysis,[0],[0]
"Importantly, on overage, modal constructions have 2.13 interpretations scored with high certainty, and 1.23 scored 3 or higher.",5 Corpus Analysis,[0],[0]
"In other words, on average, our procedure generates over 2 interpretation that are either true or false, and over 1 interpretation that is true per modal construction.
",5 Corpus Analysis,[0],[0]
Tables 2 and 3 present basic corpus statistics.,5 Corpus Analysis,[0],[0]
"The percentage of interpretations annotated with a score different than 0 depends greatly on the number of roles toggled off (Table 2): 0: 87.25%, 1: 48.50%, 2: 20.46%, 3: 5.83%.",5 Corpus Analysis,[0],[0]
Note that the number of roles toggled off does not significantly affect the mean score of interpretations not scored 0,5 Corpus Analysis,[0],[0]
"(Table 2, last 2 columns).",5 Corpus Analysis,[0],[0]
"Most interpretations have either ARG0 or ARG1 toggled off (Table 3), and the percentages of interpretations not scored zero range from 20% to 32.84% depending on the semantic role.",5 Corpus Analysis,[0],[0]
"Note that the average score of interpretations scored positively and negatively, however, does not depend on whether a semantic role is toggled off.",5 Corpus Analysis,[0],[0]
The annotation guidelines (Section 4.2) to score potential interpretations were defined after examining disagreements in pilot annotations.,5.1 Annotation Quality,[0],[0]
"After defining the guidelines, inter-annotator agreement was 0.92 on 18% of randomly selected interpretations.6 Agreement measures designed for categorical labels are unsuitable, as not all disagreements are equal, e.g., 4 vs. 5, -2 vs. 5.",5.1 Annotation Quality,[0],[0]
"Because of the high agreement and following previous work (Agirre et al., 2012), the rest of interpretations were annotated once.",5.1 Annotation Quality,[0],[0]
Table 4 presents annotation examples.,5.2 Annotation Examples,[0],[0]
"For each example, we include the original sentence containing a selected modal construction, its context (previous and next sentence) if helpful for scoring, and 2 automatically generated potential interpretations with their annotated scores.
",5.2 Annotation Examples,[0],[0]
Example (1) shows that context helps in determining the factuality of potential interpretations (item (1) in the guidelines).,5.2 Annotation Examples,[0],[0]
"After reading the three sen-
6We set an internal deadline of 3 days after agreeing on the guidelines, and we could annotate 18% of instances in that time.
",5.2 Annotation Examples,[0],[0]
"tences, it is clear that they are making wild statements, and are hoping to get attention for it.",5.2 Annotation Examples,[0],[0]
"Interpretation 1.1 removes adverb certainly and receives the highest score, 5.",5.2 Annotation Examples,[0],[0]
"Interpretation 1.2 is obtained after toggling off ARG1, and receives the lowest score, −5.",5.2 Annotation Examples,[0],[0]
"This low score is justified by item (3) in our annotation guidelines: replacing wild statements with a semantically (different but) related substitute, e.g., But they chose reasonable statements / good manners to get our attention and that of the international community, yields an unlikely interpretation.
",5.2 Annotation Examples,[0],[0]
"The interpretations in Example (2) show again the importance of context, and also exemplify item (2) in the annotation guidelines.",5.2 Annotation Examples,[0],[0]
"Interpretation 2.1, We will find them one day receives a high score (4/5), as given the context (and assuming that Rumsfeld is truthful), it is very likely that they will find the weapons of mass destruction, but it is not guaranteed.",5.2 Annotation Examples,[0],[0]
Note that annotators are not allowed to use the fact that the weapons were never found (item (2) in the guidelines).,5.2 Annotation Examples,[0],[0]
"In Interpretation 2.2, one day could be replaced with never / at no time or similar constructions, and doing so yields the opposite of the intended meaning (score: −3).",5.2 Annotation Examples,[0],[0]
"A possible descrip-
tion of these scores could be “almost certainly true” (4 out of 5), and “most probably false” (-3 out of -5).",5.2 Annotation Examples,[0],[0]
"We see scores as a continuum of certainty, but textual description may help understand the examples.
",5.2 Annotation Examples,[0],[0]
"Example (3) demonstrates the usefulness of the normalization process—specifically, Step 4, selecting relevant tokens—and the importance of replacing roles with semantically related substitutes (item (3) in the guidelines).",5.2 Annotation Examples,[0],[0]
"In interpretation 3.1, {ARG0} will act in the interests of the minority holders, ARG0 can be replaced with a company with several minority holders, yielding a valid interpretation scored 4 (out of 5).",5.2 Annotation Examples,[0],[0]
"Similarly, in interpretation 3.2, A company with a big majority holder will act {ARG1}, ARG1 can be replaced with in the interests of the big majority holder, yielding another valid interpretation also scored 4 (out of 5).
",5.2 Annotation Examples,[0],[0]
"Finally, Example (4) shows Step 5 in the automatic normalization procedure (Section 4).",5.2 Annotation Examples,[0],[0]
"By creating an additional verb-argument structure, we are able to differentiate between liking to do something (Interpretation 4.1, score 5/5) and actually doing that something (Interpretation 4.2, score 1/5).",5.2 Annotation Examples,[0],[0]
"In order to automatically score potential interpretations, we follow a standard supervised machine learning approach.",6 Learning to Score Potential Interpretations,[0],[0]
"Each potential interpretation becomes an instance, and we split modal constructions (and their potential interpretations) into training (80%) and test (20%).",6 Learning to Score Potential Interpretations,[0],[0]
"When splitting, we make sure that the amount of modal constructions for each adverb in each split is proportional, i.e., 80% of modal constructions with each adverb are in the train split and the rest in the test split.",6 Learning to Score Potential Interpretations,[0],[0]
"Splitting instances randomly would assign interpretations generated from the same modal construction to the train and test splits, and bias the results.
",6 Learning to Score Potential Interpretations,[0],[0]
"We trained a Support Vector Machine (SVM) for regression with RBF kernel using scikit-learn (Pedregosa et al., 2011), which uses LIBSVM (Chang and Lin, 2011).",6 Learning to Score Potential Interpretations,[0],[0]
"The SVM parameters (C and γ) were tuned using 10-fold cross-validation with the training set, and we report results using the test split.",6 Learning to Score Potential Interpretations,[0],[0]
The full set of features is detailed in Table 5.,6.1 Feature Selection,[0],[0]
Baseline features are simple features characterizing adverb and verb and we do not elaborate on them.,6.1 Feature Selection,[0],[0]
Adverb and verb features are extracted from the modal construction (constituent tree and semantic roles) and provide additional information about the modal construction.,6.1 Feature Selection,[0],[0]
"Interpretation features characterize the potential interpretation whose factuality is being scored, and are also derived from the constituent tree and semantic roles.
",6.1 Feature Selection,[0],[0]
"Most adverb and verb features are standard in semantic role labeling (Gildea and Jurafsky, 2002).",6.1 Feature Selection,[0],[0]
"We include the part-of-speech tags of the parent, and left and right siblings of adverb and verb, as well as their subcategorization, i.e., the concatenation of the sibling’s part-of-speech tags.",6.1 Feature Selection,[0],[0]
"We also include syntactic path between adverb and verb, and its length.",6.1 Feature Selection,[0],[0]
"Additionally, we include the common ancestor, i.e., the syntactic node of the lowest common node that is an ancestor of both adverb and verb, and use binary features to indicate whether each semantic role is present in the modal construction.
",6.1 Feature Selection,[0],[0]
"Finally, interpretation features characterize the semantic roles toggled off to generate the potential interpretation.",6.1 Feature Selection,[0],[0]
"We include the number of roles toggled off to generate the potential interpretation, and binary flags indicating which roles.",6.1 Feature Selection,[0],[0]
"Additionally, for each role toggled off, we include the distance from the verb (number of tokens), whether it occurs before or after the verb, the syntactic path to the verb and the length of the path.",6.1 Feature Selection,[0],[0]
"Table 6 details results obtained with test instances using several feature combinations derived from
gold linguistic information (POS tags, parse trees, semantic roles, etc.).",7 Experimental Results,[0],[0]
"Baseline and adverb and verb features, which characterize the modal construction from which potential interpretation are extracted, are virtually useless.",7 Experimental Results,[0],[0]
"They yield Pearson correlations of −0.029 and 0.025 individually, and −0.013 combined.",7 Experimental Results,[0],[0]
"These results suggest that the verb and adverb in the modal construction (word forms, syntactic paths, etc.) are insufficient to rank potential interpretations generated from the modal construction.
",7 Experimental Results,[0],[0]
"Interpretation features, which capture differences between potential interpretations being scored (number of roles toggled off, roles toggled off, etc.), obtain a modest Pearson correlation of 0.494.",7 Experimental Results,[0],[0]
"Combining interpretation features with other features proved detrimental, Pearson correlations are between 0.463 and 0.468.",7 Experimental Results,[0],[0]
Modality is a pervasive phenomenon used to talk about what is not factual.,8 Conclusions,[0],[0]
"In this paper, we have presented a methodology to extract implicit interpretations from modal constructions.",8 Conclusions,[0],[0]
"First, we automatically generate potential interpretations using syntactic dependencies and semantic roles, and then assign to them a factuality score.
",8 Conclusions,[0],[0]
The most important conclusion of the work presented here is that several interpretations automatically generated from a single modal construction often receive scores indicating high certainty.,8 Conclusions,[0],[0]
"Indeed, on average, modal constructions have 2.13 interpretations scored lower or equal than −3, or higher or equal than 3.",8 Conclusions,[0],[0]
"This contrast with previous work, which only assess factuality of one normalization per proposition.
",8 Conclusions,[0],[0]
Experimental results using supervised machine learning and relatively simple features show that the task is challenging but can be automated.,8 Conclusions,[0],[0]
"We believe better results could be obtained by incorporating features capturing knowledge in the context of the modal construction, including other clauses in the same sentence, and the previous and next sentences.",8 Conclusions,[0],[0]
"Another extension of the current work is to investigate a similar approach for other modality markers such as nouns (e.g., possibility, chance), adjectives (e.g.necessary, probable, ) and certain verbs (e.g., claim, suggests).",8 Conclusions,[0],[0]
This paper presents an approach to extract implicit interpretations from modal constructions.,abstractText,[0],[0]
"Importantly, our approach uses a deterministic procedure to normalize eventualities and generate potential interpretations.",abstractText,[0],[0]
An annotation effort demonstrates that these interpretations are intuitive to humans and most modal constructions convey at least one interpretation.,abstractText,[0],[0]
Experimental results show that the task is challenging but can be automated.,abstractText,[0],[0]
Automatic Extraction of Implicit Interpretations from Modal Constructions,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 71–81, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Convergence of behaviour is an important feature of Human-Human (H-H) interaction that occurs both at low-level (e.g., body postures, accent and speech rate, word choice, repetitions) and at highlevel (e.g., mental, emotional, cognitive)",1 Introduction,[0],[0]
"(Gallois et al., 2005).",1 Introduction,[0],[0]
"In particular, dialogue participants (DPs) automatically align their communicative behaviour at different linguistic levels including the lexical, syntactic and semantic ones (Pickering and Garrod, 2004).",1 Introduction,[0],[0]
"A key ability in dialogue is to be able to align (or not) to show a convergent, engaged behaviour or at the opposite a divergent one.",1 Introduction,[0],[0]
"Such convergent behaviour may facilitate successful task-oriented dialogues (Nenkova et al., 2008; Friedberg et al., 2012).",1 Introduction,[0],[0]
"Our goal is to provide a virtual agent with the ability to detect the alignment behaviour of its human interlocutor, as well as the ability to align with the user to enhance its believability, to increase interaction naturalness and to maintain user’s engagement (Yu et al., 2016).",1 Introduction,[0],[0]
"In this paper, we aim at providing measures characterising verbal alignment pro-
cesses based on repetitions between DPs.",1 Introduction,[0],[0]
"We propose a framework based on repetition at the lexical level which deals with textual dialogues (e.g., transcripts), along with automatic and generic measures indicating verbal alignment between interlocutors.",1 Introduction,[0],[0]
We offer a study that contrasts H-H and Human-Agent (H-A) dialogues on a negotiation task and show how our proposed measures can be used to quantify verbal alignment.,1 Introduction,[0],[0]
"We confirm quantitatively some predictions from previous literature regarding the strength and orientation of verbal alignment in Human-Machine Interaction (Branigan et al., 2010).
",1 Introduction,[0],[0]
Section 2 presents and discusses the related work.,1 Introduction,[0],[0]
Section 3 describes the proposed model and outlines its main features.,1 Introduction,[0],[0]
"Next, Section 4 presents the corpus-based experimentation protocol and states the main investigated hypotheses.",1 Introduction,[0],[0]
"Then, Section 5 presents the quantitative analysis and discusses the main results.",1 Introduction,[0],[0]
"Finally, Section 6 concludes this paper.",1 Introduction,[0],[0]
"When people are engaged in a dialogue there is evidence that their behaviours tend to converge (Gallois et al., 2005) and automatically align at several levels (Pickering and Garrod, 2004).",2 Related Work,[0],[0]
"This includes non-linguistic levels such as facial expressions and body postures as well as linguistic levels such as lexical, syntactic and semantic ones.",2 Related Work,[0],[0]
"In particular, alignment theory predicts the existence of patterns of repetition via a priming mechanism stating that “encountering an utterance that activates a particular representation makes it more likely that the person will subsequently produce an utterance that uses that representation” (Pickering and Garrod, 2004).",2 Related Work,[0],[0]
"Thus, DPs tend to reuse lexical as well as syntactic structure (Reitter et al., 2006; Ward and Litman, 2007).",2 Related Work,[0],[0]
"One consequence
71
of successful alignment at several levels between DPs is a certain repetitiveness in dialogue and the development of a lexicon of fixed expressions established during dialogue (Pickering and Garrod, 2004).",2 Related Work,[0],[0]
DPs tend to automatically establish and use fixed expressions that become dialogue routines via a process called “routinization”.,2 Related Work,[0],[0]
"Recent work argues that these patterns of repetition may be specific to task-oriented dialogues and do not generalise to ordinary conversation in H-H interactions (Healey et al., 2014).",2 Related Work,[0],[0]
"Here, we are specifically interested in verbal alignment in H-H and H-A task-oriented interactions.",2 Related Work,[0],[0]
"We use the term alignment to say that DPs converge at the lexical level by using the same words and expressions (e.g., by employing the expression “that’s not gonna work for me” to reject a proposition).
",2 Related Work,[0],[0]
"Studies point out evidence that lexical items and syntactic structures used by a system are subsequently adopted by users (Brennan, 1996; Stoyanchev and Stent, 2009; Parent and Eskenazi, 2010; Branigan et al., 2010).",2 Related Work,[0],[0]
"(Branigan et al., 2010) argue that linguistic alignment should occur in Human-Machine interaction.",2 Related Work,[0],[0]
"In particular, they outline the fact that the strength of alignment may be dependent on the human’s belief about the communicative capability of the machine.",2 Related Work,[0],[0]
"As such, alignment might be stronger from a human participant who believes that it might improve communication and understanding.",2 Related Work,[0],[0]
"In this work, we bring quantitative evidence supporting the fact that human align more with a virtual agent than with another human based on a study contrasting H-H and H-A interactions at the level of repetition of expressions.",2 Related Work,[0],[0]
"While previous studies have mainly focused on H-H dialogues, we offer in this work an analysis of verbal alignment in H-A dialogues based on a corpus.
",2 Related Work,[0],[0]
"Several studies aim at providing virtual agents with the ability to verbally align with the user in order to improve credibility, naturalness, and also to foster user engagement (Clavel et al., 2016).",2 Related Work,[0],[0]
"It involves high-level alignment such as politeness (De Jong et al., 2008) or aligning on appreciations (Campano et al., 2015).",2 Related Work,[0],[0]
"Work on convergence in the spoken dialogue system community has mainly focused on lexical entrainment, i.e. the tendency to use the same terms when DPs refer repeatedly to the same objects (Brennan and Clark, 1996).",2 Related Work,[0],[0]
"Several entrainment models have been proposed to let the system entrains to user utterances
(e.g., (Brockmann et al., 2005; Buschmeier et al., 2010; Hu et al., 2014; Lopes et al., 2015)).",2 Related Work,[0],[0]
These models are completely or partially rule-based and focus on specific aspects of entrainment.,2 Related Work,[0],[0]
"Recent work aims at introducing entrainment in a fully trainable natural language system by exploiting the preceding user utterance (Dušek and Jurcıcek, 2016).
",2 Related Work,[0],[0]
Several metrics have been employed to automatically measure linguistic alignment in written corpora.,2 Related Work,[0],[0]
"At the word or token levels, (Nenkova et al., 2008) quantify verbal alignment based on high-frequency words while (Campano et al., 2014) quantify verbal alignment based on vocabulary overlap between DPs.",2 Related Work,[0],[0]
"(Healey et al., 2014) compute similarity at the syntax and lexical levels on windows of a fixed number of turns.",2 Related Work,[0],[0]
"(Fusaroli and Tyln, 2016) employ (cross-)recurrence quantification analysis to quantify interactive alignment and interpersonal synergy at the lexical, prosodic and speech/pause levels.",2 Related Work,[0],[0]
"(Reitter et al., 2006; Ward and Litman, 2007) focus on regression models to study priming effects within a small window of time in single dialogues.",2 Related Work,[0],[0]
"(Stenchikova and Stent, 2007) use a frequencybased approach (Church, 2000) to measure adaptation between dialogues.",2 Related Work,[0],[0]
"In this paper, we propose global and speaker-specific measures based on the automatic construction of the expression lexicon built by the DPs.",2 Related Work,[0],[0]
An originality of our approach is to consider lexical patterns predicted by the routinization process of the interactive alignment theory.,2 Related Work,[0],[0]
These measures rely on efficient algorithms making an online usage in a dialogue system realistic.,2 Related Work,[0],[0]
They indicate both verbal alignment at the level of repetitions and the orientation of verbal alignment between DPs in single dialogues.,2 Related Work,[0],[0]
"To address the problem of detecting (possibly overlapping) repetitions between DPs, we propose a framework defining key features of repeated expressions, along with an efficient computational mean of building an expression lexicon.
",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"In this work, we define an expression as a surface text pattern at the utterance level that has been produced by both speakers in a dialogue.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"In other words, it is a contiguous sequence of tokens that appears in at least two utterances produced by two different speakers.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"An expression may be a single
token (e.g., “you”, “I”).",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"However, an expression should contain at least one non-punctuation token.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Thus, sequences like “?”, “!”, “,” are not expressions.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
An instance of an expression can either be free or constrained in a given utterance1.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
A free instance is an instance of an expression that appears in an utterance without being a subexpression of a larger expression.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
A constrained instance is an expression that appears in a turn as a subexpression of a larger expression.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
The initiator of the expression is the interlocutor that first produced an instance of the expression either in a free or constrained form.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Lastly, an expression is established as soon as the two following criteria are met: (i) the expression has been produced by both interlocutors (either in a free or constrained form), and (ii) the expression has been produced at least once in a free form.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
The first turn in which these criteria are all met is the establishment turn of the expression.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Eventually, the expression lexicon of a dialogue is the set of established expressions that appear in this dialogue.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Importantly, the expression lexicon contains all expressions that appear in a dialogue at least once in a free form.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Expressions that are always constrained (i.e. which instances are always a subpart of a larger expression) are discarded.
",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
Table 1 presents an excerpt of dialogue extracted from the corpus used in this work.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"In this example, “that’s not gonna work for me” is an expression initiated by A in turn 1 and established in turn 4.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"This expression is free in this excerpt, and it belongs to the expression lexicon.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Similarly, “work for” is an expression initiated by A in turn 1 and established in turn 2.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"It appears in a constrained form in the expression “that’s not gonna work for me” in turns 1 and 4, and in a free form in turn 2.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
It belongs to the expression lexicon.,3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"The expression “that’s not gonna” occurs in a constrained form in turns 1 and 4, and never occurs in a free form.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"This expression is never established (contrary to its parent expression “that’s not gonna work for me”) and thus is not included in the expression lexicon.
",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"The automatic extraction of expressions from a dialogue is an instance of sequential pattern mining (Mooney and Roddick, 2013) applied to textual dialogues.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"In this work, we follow a similar approach than (Dubuisson Duplessis et al., 2017)
",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"1This terminology is borrowed and adapted from the textual data analysis field and the notion of “repeated segment” (Lebart et al., 1997)
by employing a generalised suffix tree in order to solve the multiple common subsequence problem (MCSP) (Gusfield, 1997) to extract frequent surface text patterns between utterances, and then filtering patterns used by both DPs.",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
"Notably, the MCSP is solved in linear time with respect to the number of tokens in a dialogue (Gusfield, 1997).",3 Model: Expression-based Measures of Verbal Alignment,[0],[0]
An expression has a frequency which corresponds to the number of utterances in which the expression appears.,3.1 Properties of Expressions,[0],[0]
"For example, the expression “work for” has a frequency of 3 because it appears in utterance 1, 2 and 4.",3.1 Properties of Expressions,[0],[0]
"Next, the size of an expression is its number of tokens (e.g., expression “the clock” has size 2).",3.1 Properties of Expressions,[0],[0]
"Then, the span of an expression is the number of utterances between the first production and the last production of this expression in the dialogue (including the first and last utterances).",3.1 Properties of Expressions,[0],[0]
"The minimum span is 2, meaning the expression has been established in two adjacent utterances.",3.1 Properties of Expressions,[0],[0]
"For instance, the expression “the clock” has a span of 4 because it appears first in utterance 3 and last in utterance 6.",3.1 Properties of Expressions,[0],[0]
We derive the density of an expression which is given by the ratio between its frequency and its span.,3.1 Properties of Expressions,[0],[0]
"For instance, the density of the expression “well” is 0.5.",3.1 Properties of Expressions,[0],[0]
"Eventually, the priming of an expression is the number of repetitions of the expression by the initiator before being used by the other interlocutor (either in a free or constrained form).",3.1 Properties of Expressions,[0],[0]
"For example, the expression “well” has a priming of 2 because it is repeated by speaker A in utterance 1 and 5 before being established in utterance 6.",3.1 Properties of Expressions,[0],[0]
"Globally, we derive the following measures from the model: Expression lexicon size (ELS) the number of
items in the expression lexicon, i.e. the number of established expressions in the dialogue
Expression variety (EV) the expression lexicon size normalised by the total number of tokens in the dialogue.",3.2 Measures,[0],[0]
"It is given by: EV =
ELS # Tokens .",3.2 Measures,[0],[0]
This ratio indicates the variety of the expression lexicon relatively to the length of the dialogue.,3.2 Measures,[0],[0]
"The higher it is, the more there are different expressions established between DPs.
",3.2 Measures,[0],[0]
"Expression repetition (ER) the ratio of produced tokens belonging to an instance of an established expression, i.e. the ratio of tokens belonging to a repetition of an expression.",3.2 Measures,[0],[0]
"It is given by:
ER = # Tokens in an established expr.",3.2 Measures,[0],[0]
#,3.2 Measures,[0],[0]
"Tokens , ER ∈",3.2 Measures,[0],[0]
"[0, 1].",3.2 Measures,[0],[0]
"The higher the ER is, the more DPs dedicate tokens to the repetition of established expressions.",3.2 Measures,[0],[0]
"We also derive the following measures for each speaker S: Initiated expressions (IES) number of expres-
sions initiated by S (and further established) normalised by the expression lexicon size.",3.2 Measures,[0],[0]
"It is given by: IES = # Expr. initiated by S
ELS ,∀ S, IES ∈",3.2 Measures,[0],[0]
"[0, 1].",3.2 Measures,[0],[0]
"Note that in a dyadic dialogue involving speaker S1 and S2, IES1 + IES1 = 1.
Expression repetition (ERS) ratio of produced tokens belonging to an instance of an established expression, i.e. ratio of tokens belonging to a repetition of an expression.",3.2 Measures,[0],[0]
"It is given by: ERS = # Tokens from S in an established expr.
",3.2 Measures,[0],[0]
"# Tokens from S ,∀ S, ERS ∈",3.2 Measures,[0],[0]
"[0, 1] Eventually, we also consider a measure independent of the model: the Token Overlap (TO) which is the ratio of shared tokens between locutor S1 and locutor S2 in a dialogue.",3.2 Measures,[0],[0]
It is given by: TO = #(TokensS1∩TokensS2) #(TokensS1∪TokensS2) .,3.2 Measures,[0],[0]
"The higher is TO, the more vocabulary is shared between S1 and S2.",3.2 Measures,[0],[0]
"Our methodology aims at comparing quantitatively both H-H and H-A task-oriented corpora at
the level of the repetition of expressions.",4 Experimentation,[0],[0]
"The corpus of this study focuses on a negotiation task between two DPs and is detailed in (Gratch et al., 2016).",4.1 Negotiation Corpora,[0],[0]
"It focuses on a common abstraction of negotiation known as the multi-issue bargaining task (Kelley and Schenitzki, 1972).",4.1 Negotiation Corpora,[0],[0]
"Here, it requires two interlocutors to find an agreement over the amount of a product each player wishes to buy.",4.1 Negotiation Corpora,[0],[0]
"Each player receives some payoff for each possible agreement, usually unknown to the other party.",4.1 Negotiation Corpora,[0],[0]
Negotiation can take two structures in this scenario.,4.1 Negotiation Corpora,[0],[0]
The integrative structure represents a negotiation that can turn out to be a win-win for both players (if they realise through conversation that this is a cooperative negotiation).,4.1 Negotiation Corpora,[0],[0]
"On the other hand, the distributive negotiation represents a competitive (zero-sum) negotiation where players share the same interests in objects.",4.1 Negotiation Corpora,[0],[0]
"However, players do not know in advance and often assume a distributive negotiation (i.e. their opponent wants the same thing as them) rather than an integrative negotiation.",4.1 Negotiation Corpora,[0],[0]
This corpus can be broken down into two parts: a H-H corpus and a H-A corpus.,4.1 Negotiation Corpora,[0],[0]
"In both parts, people were given similar instructions, i.e. humans are told that they must negotiate with another player how to divide the contents of a storage locker filled with three classes of valuable items (such as records, lamps or painting).
",4.1 Negotiation Corpora,[0],[0]
"In the H-H corpus, pairs of people performed one negotiation which was either distributive or integrative in structure.",4.1 Negotiation Corpora,[0],[0]
"Independently, they were given information in the instructions that suggested the negotiation was integrative or distributive.",4.1 Negotiation Corpora,[0],[0]
"Note that this condition does not affect the results presented below.
",4.1 Negotiation Corpora,[0],[0]
"In the H-A corpus, the human participant engaged in two negotiations with two different virtual agents (a male called Brad and a female called Ellie).",4.1 Negotiation Corpora,[0],[0]
The first negotiation was a cooperative/integrative negotiation while the second was a competitive/distributive negotiation.,4.1 Negotiation Corpora,[0],[0]
The order of interaction with the agents (Brad-Ellie or EllieBrad) was randomly chosen.,4.1 Negotiation Corpora,[0],[0]
The interaction was framed.,4.1 Negotiation Corpora,[0],[0]
Half of the human participants was told they were interacting with an autonomous agent while the other half was told they were interacting with a human wizard (though the agent was always controlled by a wizard).,4.1 Negotiation Corpora,[0],[0]
"The Woz system controlling virtual agents has been designed to be
as natural as possible (DeVault et al., 2015).",4.1 Negotiation Corpora,[0],[0]
It involves low-level functions carried out automatically (such as the selection of gestures and expressions related to speech) and high-level decisions about verbal and non-verbal behaviour carried out by two wizards.,4.1 Negotiation Corpora,[0],[0]
"Notably, it includes a large number of possible utterances (more than 11,000) along with a specific interface enabling the human operator to rapidly select among those (DeVault et al., 2015).",4.1 Negotiation Corpora,[0],[0]
"For both virtual human agents, wizards were rather free but followed some guidelines.",4.1 Negotiation Corpora,[0],[0]
"First, the goal in both negotiations is for the agent to win.",4.1 Negotiation Corpora,[0],[0]
"Next, in the distributive condition, wizards were requested to be soft, polite and vague trying hard to get the human participant to make the first offer and avoiding revealing what they wanted (unless the human directly asks).",4.1 Negotiation Corpora,[0],[0]
"In the integrative condition, wizards could share preferences and were not requested to be vague.",4.1 Negotiation Corpora,[0],[0]
"However, they were requested to try getting the human share first and make the first offer.",4.1 Negotiation Corpora,[0],[0]
"Table 1 presents an excerpt from a competitive negotiation from the H-A corpus.
Figures about both corpora can be found in Table 2.",4.1 Negotiation Corpora,[0],[0]
"Globally, dialogues in both corpora contains more than 100 utterances.",4.1 Negotiation Corpora,[0],[0]
It shows that H-A dialogues are a bit shorter than H-H dialogues but still comparable.,4.1 Negotiation Corpora,[0],[0]
"Besides, utterances are shorter in terms of tokens in the H-A dialogues than in the H-H dialogues.",4.1 Negotiation Corpora,[0],[0]
"To investigate hypotheses stated in Section 4.3, we constituted two randomised corpora HHR and HAR respectively for the randomised version of the H-H corpus and the H-A corpus.",4.2 Randomised Corpora,[0],[0]
"This randomisation process is similar to the ones adopted
by various work investigating verbal alignment (e.g., (Ward and Litman, 2007), (Healey et al., 2014), (Fusaroli and Tyln, 2016)).",4.2 Randomised Corpora,[0],[0]
"To constitute the HHR corpus, the following process is performed for each dialogue of the initial corpus: each interlocutor’s real turns in sequence are interleaved with turns randomly chosen from the HH corpus.",4.2 Randomised Corpora,[0],[0]
A similar process is followed for the HAR corpus with the exception that each human turn is substituted by a random human turns from the H-A corpus when keeping the sequence of wizard turns; while each wizard turn is substituted by a random wizard turns from the H-A corpus when keeping the sequence of human turns.,4.2 Randomised Corpora,[0],[0]
"In all, two dialogues are generated by these processes for each original H-H/A dialogue (one for each locutor).",4.2 Randomised Corpora,[0],[0]
These surrogate corpora lack the coherence of dialogues in the H-H and H-A corpora.,4.2 Randomised Corpora,[0],[0]
"Indeed, utterances are no longer in their original relationship with their response utterances.",4.2 Randomised Corpora,[0],[0]
We thus expect to find reduced verbal alignment at the level of expressions in these corpora.,4.2 Randomised Corpora,[0],[0]
Our first hypothesis is that DPs should verbally align at the level of expressions in both the H-H corpus and the H-A corpus more than would be expected by chance.,4.3.1 “Above Chance” Hypotheses,[0],[0]
"This hypothesis can be expressed in the following way: routinization DPs should constitute a richer ex-
pression lexicon than they would by chance (this should be indicated by the EV measure)
repetition DPs should repeat expressions more often than chance (this should be indicated by the ER and the TO measures)",4.3.1 “Above Chance” Hypotheses,[0],[0]
"Following Branigan et al’s hypothesis (Branigan et al., 2010), we should expect more verbal alignment at the level of expressions in the H-A corpus than in the H-H corpus.",4.3.2 H-H VS H-A Hypotheses,[0],[0]
"Besides, we should expect more verbal alignment from the human participant than from the agent.",4.3.2 H-H VS H-A Hypotheses,[0],[0]
"Indeed, the human participant both has the ability to verbally align (contrary to the agent) and may be influenced by beliefs about the communicative limitations of the agent.",4.3.2 H-H VS H-A Hypotheses,[0],[0]
"This hypothesis can be expressed in the following way: routinization DPs should constitute a richer ex-
pression lexicon in the H-A corpus than in the H-H corpus (this should be indicated by the EV measure)
repetition DPs should dedicate more tokens to the repetition of established expressions in the H-A corpus than in the H-H corpus (this should be indicated by the ER and the TO measures)",4.3.2 H-H VS H-A Hypotheses,[0],[0]
orientation the human participant should repeat more expressions initiated by the agent than the other way around (this should be indicated by the IES and the ERS measures),4.3.2 H-H VS H-A Hypotheses,[0],[0]
"In this study, we also consider conditions that affects only the H-A corpus.",4.3.3 H-A-specific Hypotheses,[0],[0]
"First, interactions with the virtual agent were randomly “framed” meaning that, prior interactions, the human participant was either told that the agent was controlled by a human operator (72 dialogues) or that it was autonomous (82 dialogues).",4.3.3 H-A-specific Hypotheses,[0],[0]
"This condition affects the mediated component of verbal alignment i.e. the beliefs of the human participant about the communicative capabilities of the agent (e.g., in terms of understanding).",4.3.3 H-A-specific Hypotheses,[0],[0]
"This leads us to the following hypothesis: framing framing should impact verbal alignment
in the routinization, repetition and orientation aspects.
",4.3.3 H-A-specific Hypotheses,[0],[0]
"More specifically, “human” framing should lead to a more “human-like verbal alignment” while “agent” framing should lead to a “HMI-like verbal alignment” (Branigan et al., 2010).
",4.3.3 H-A-specific Hypotheses,[0],[0]
"Moreover, the human participants interacted with two versions of the virtual agent.",4.3.3 H-A-specific Hypotheses,[0],[0]
"One was Ellie, a female agent, while the other was Brad, a male agent.",4.3.3 H-A-specific Hypotheses,[0],[0]
Interaction order was random (BradEllie or Ellie-Brad).,4.3.3 H-A-specific Hypotheses,[0],[0]
"This condition leads us to the following hypothesis: gender gender matching (Male-Male or Female-
Female) or unmatching (Male-Female, Female-Male) should not impact verbal alignment Lastly, interactions involved two types of negotiations (integrative and distributive).",4.3.3 H-A-specific Hypotheses,[0],[0]
We study the impact of the negotiation type on the verbal alignment at the level of expressions.,4.3.3 H-A-specific Hypotheses,[0],[0]
"We compare the H-H and H-A corpora of real interactions to the surrogate HHR and HAR corpora to ensure that established expressions in the dialogues are actually due to the coherent sequence
of utterances and are not incidental.",5.1 Comparisons to the Surrogate Corpora,[0],[0]
We investigated whether DPs in the H-H corpus verbally align at the level of expressions more than would be expected by chance by comparing it to the HHR corpus (following hypotheses stated in Section 4.3.1).,5.1 Comparisons to the Surrogate Corpora,[0],[0]
"First, the expression variety is significantly higher for the H-H corpus (mean=0.118, std=0.023) than for the HHR corpus (mean=0.110, std=0.015).",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"Statistical difference is checked by a Wilcoxon rank sum test (U = 8951, p = 0.00051 < 0.001, r = 0.22)2.",5.1 Comparisons to the Surrogate Corpora,[0],[0]
This indicates that H-H interactions lead to a richer expression lexicon.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
"However, the expression repetition is not significantly different (p = 0.3446) between the H-H corpus (mean=0.436, std=0.107) and the HHR corpus (mean=0.420, std=0.108).",5.1 Comparisons to the Surrogate Corpora,[0],[0]
This means that the amount of tokens dedicated to the repetition of expressions is similar between the H-H corpus and the HHR corpus.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
An explanation of this may be that the dialogues happen in a closed domain on a specific task (negotiations of a set of objects) and thus in a constrained vocabulary.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
This inevitably leads random dialogues to include repetitions though in a lesser variety.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
"This is confirmed by the token overlap that is significantly higher for the H-H corpus (mean=0.316, std=0.073) than for the HHR corpus (mean=0.276, std=0.058) (U = 9468.5, p = 9.781 × 10−6 < 0.001, r = 0.28).",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"DPs share a richer vocabulary than what would happen by chance.
",5.1 Comparisons to the Surrogate Corpora,[0],[0]
We performed a similar analysis by comparing the H-A corpus and the HAR corpus.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
It turns out that both the expression lexicon variety and the expression repetition are significantly higher in the H-A corpus than in the HAR corpus.,5.1 Comparisons to the Surrogate Corpora,[0],[0]
"Indeed, the expression variety is significantly higher (U = 30126, p = 2.155 × 10−6 < 0.001, r = 0.22) for the H-A corpus (mean=0.134, std=0.022) than for the HAR corpus (mean=0.124, std=0.020).",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"Besides, the expression repetition is significantly higher (U = 28124, p = 0.0011 < 0.01, r = 0.15) for the H-A corpus (mean=0.416, std=0.086) than for the HAR corpus (mean=0.386, std=0.088).",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"This is comforted by the fact that the token overlap is significantly higher (U = 30164, p = 1.875×10−6 < 0.001, r = 0.22) for the H-A corpus (mean=0.322, std=0.06) than for the HAR corpus (mean=0.293, std=0.06).
",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"All in all, it turns out that both H-H and H-A di-
2For each test, we report the test statistics (U/W), the pvalue (p) and the effect size (r).
",5.1 Comparisons to the Surrogate Corpora,[0],[0]
alogues constitute a richer expression lexicon than they would by chance (routinization hypothesis).,5.1 Comparisons to the Surrogate Corpora,[0],[0]
"As for the repetition hypothesis, DPs clearly repeat expressions more often than chance in the HA corpus.",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"However, repetition in the H-H corpus is comparable to what would happen by chance in closed domain task-oriented dialogues.",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"All things considered, our indicators show that both corpora tends to verbally align at the level of shared expressions more than they would by chance.",5.1 Comparisons to the Surrogate Corpora,[0],[0]
"We compare verbal alignment at the expression level between the H-H corpus and the H-A corpus globally, per speaker and at the lexicon level.",5.2 Differences between H-H/A Interactions,[0],[0]
"It turns out that the expression variety is significantly lower for the H-H corpus (mean=0.118, std=0.023) than for the H-A corpus (mean=0.134, std=0.022).",5.2.1 Global Interaction Analysis,[0],[0]
"This is checked via a Wilcoxon rank sum test (U = 4056.5, p = 2.035×10−6 < 0.001, r = 0.31).",5.2.1 Global Interaction Analysis,[0],[0]
This indicates that DPs constitute a richer expression lexicon in the H-A corpus than in the H-H corpus.,5.2.1 Global Interaction Analysis,[0],[0]
"However, we noticed that there is no significant difference between the H-H corpus and the H-A corpus in terms of expression repetition and token overlap.",5.2.1 Global Interaction Analysis,[0],[0]
"Indeed, the expression repetition is not significantly different between the H-H corpus (mean=0.436, std=0.107) and the HA corpus (mean=0.416, std=0.086) by a Wilcoxon rank sum test (p = 0.1261).",5.2.1 Global Interaction Analysis,[0],[0]
"Besides, the token overlap is not significantly different between the H-H corpus (mean=0.316, std=0.073) and the HA corpus (mean=0.322, std=0.06) by a similar test (p = 0.6618).
",5.2.1 Global Interaction Analysis,[0],[0]
H-A interactions lead to a richer expression lexicon than the H-H interactions (routinization hypothesis).,5.2.1 Global Interaction Analysis,[0],[0]
This indicates more verbal alignment at the level of shared expressions in H-A dialogues.,5.2.1 Global Interaction Analysis,[0],[0]
"However, DPs do not dedicate more tokens to the repetition of established expressions in the H-A corpus than in the H-H corpus (repetition hyp.).",5.2.1 Global Interaction Analysis,[0],[0]
We investigated verbal alignment at the level of expressions by having a closer look at each speaker in a dialogue in terms of initiated expressions (IE) and expression repetition (ER).,5.2.2 Speaker Perspective Analysis,[0],[0]
"In the H-H corpus, both speakers play a symmetrical role at the level of expressions.",5.2.2 Speaker Perspective Analysis,[0],[0]
"First, they initiate a similar amount of expressions.",5.2.2 Speaker Perspective Analysis,[0],[0]
"Indeed, IES1 and
the IES2 are not significantly different (Wilcoxon signed rank test, p = 0.5978).",5.2.2 Speaker Perspective Analysis,[0],[0]
"Next, they dedicate the same amount of tokens to the repetition of expressions (see Figure 1).",5.2.2 Speaker Perspective Analysis,[0],[0]
"In fact, ERS1 and the ERS2 are not significantly different (p = 0.9875).
",5.2.2 Speaker Perspective Analysis,[0],[0]
"On the contrary, the H-A corpus shows an asymmetrical role at the level of expressions between the Woz and the human participant.",5.2.2 Speaker Perspective Analysis,[0],[0]
"First, the Woz initiates more expressions than the human participant.",5.2.2 Speaker Perspective Analysis,[0],[0]
"Indeed, IEWoz (mean=0.596, std=0.116) is significantly higher than IEH (mean=0.404, std=0.116) (Wilcoxon signed rank test, W = 10161, p < 2.2 × 10−16 < 0.001, r = 0.87).",5.2.2 Speaker Perspective Analysis,[0],[0]
"Then, the human participant dedicates more tokens to the repetition of an established expression than the Woz (see Figure 1).",5.2.2 Speaker Perspective Analysis,[0],[0]
"As a matter of fact, ERWoz (mean=0.347, std=0.104) is significantly lower than ERH (mean=0.492, std=0.086) (Wilcoxon signed rank test, W = 545, p < 2.2 × 10−16 < 0.001, r = 0.87).",5.2.2 Speaker Perspective Analysis,[0],[0]
"Notably, this asymmetry does not appear when considering the number of tokens produced by each speaker, i.e. the Woz and the human tend to produce the same amount of tokens.",5.2.2 Speaker Perspective Analysis,[0],[0]
"Indeed, there is not a significant difference in the proportion of tokens produced by the Woz (mean=0.483, std=0.134) and by the human participant (mean=0.517, std=0.134) (Wilcoxon signed rank test, p = 0.08067).",5.2.2 Speaker Perspective Analysis,[0],[0]
"Besides, a closer look at the shared vocabulary shows that there is not a significant difference in the proportion of vocabulary shared by the Woz (mean=0.4853, std=0.116) and by the human participant (mean=0.515, std=0.093)3 (Wilcoxon signed rank test, p = 0.08029).",5.2.2 Speaker Perspective Analysis,[0],[0]
"That is, globally, the Woz does not share more of its vocabulary than the human participants, and conversely.
",5.2.2 Speaker Perspective Analysis,[0],[0]
It turns out that verbal alignment at the level of shared expressions is symmetrical in the H-H corpus.,5.2.2 Speaker Perspective Analysis,[0],[0]
"On the contrary, it is asymmetrical in the H-A corpus (orientation hypothesis) where it indicates that the human participant verbally align more by (i) adopting more Woz-initiated expressions (than the Woz adopting Human-initiated expressions), and (ii) dedicating more tokens to the repetition of established expressions.",5.2.2 Speaker Perspective Analysis,[0],[0]
"Eventually, we took a closer look at the expression lexicon produced in the H-H corpus and the
3Relative shared vocabulary for S1is computed as follow:
SVS1= #(TokensS1∩TokensS2 )
#(TokensS1 )
H-A corpus.",5.2.3 Expression Lexicon Analysis,[0],[0]
"Regarding the size in tokens of the expressions, there is no significant difference between the two corpora (Wilcoxon rank sum test, p = 0.9897).",5.2.3 Expression Lexicon Analysis,[0],[0]
The majority of expressions contains less than 3 tokens.,5.2.3 Expression Lexicon Analysis,[0],[0]
"Around 70% of expressions are 1-token expressions, 20% are 2-token expressions, 5% are 3-token expressions, and the other 5% are 4-token and more expressions.
",5.2.3 Expression Lexicon Analysis,[0],[0]
"Considering the priming of an expression (i.e. the number of repetitions of the expression by the initiator before being used by the other interlocutor), most expressions have a priming of less than 3 repetitions in both corpora.",5.2.3 Expression Lexicon Analysis,[0],[0]
"However, there is a significant difference between the two corpora (Wilcoxon rank sum test, U = 57185000, p < 2.2 × 10−16 < 0.001).",5.2.3 Expression Lexicon Analysis,[0],[0]
The most striking one is about the proportion of 1-repetition priming expressions.,5.2.3 Expression Lexicon Analysis,[0],[0]
63% of expressions have a 1- repetition priming in the H-H corpus while it is higher in the H-A corpus at 72%.,5.2.3 Expression Lexicon Analysis,[0],[0]
20% of expressions have a 2-repetition priming in the H-H corpus while it is 17% in the H-A corpus.,5.2.3 Expression Lexicon Analysis,[0],[0]
"Lastly, 8% of the H-H expressions have a 3-repetition priming while it reaches 6% for the H-A corpus.",5.2.3 Expression Lexicon Analysis,[0],[0]
The main reason of the difference at the priming level may be found in the functions that serve expression repetition in the corpora.,5.2.3 Expression Lexicon Analysis,[0],[0]
This is supported by the study of the density of expressions (i.e. their ratio frequency/span) in both corpora.,5.2.3 Expression Lexicon Analysis,[0],[0]
"Expressions in the H-A corpus are denser (mean=0.174, std=0.238) than expressions in the H-H corpus
(mean=0.146, std=0.206).",5.2.3 Expression Lexicon Analysis,[0],[0]
"This difference is significant (Wilcoxon rank sum test, U = 45419000, p < 2.2× 10−16 < 0.001).",5.2.3 Expression Lexicon Analysis,[0],[0]
Expressions in the HA corpus tend to occur more frequently between their first and last appearance in the dialogue than in the H-H corpus.,5.2.3 Expression Lexicon Analysis,[0],[0]
We studied the impact of the “human operator” framing against the “AI” framing on the verbal alignment at the level of expressions.,5.3 Other Conditions in Human-Agent Interactions,[0],[0]
It turns out there is no difference in the variety of the expression lexicon between the two framing modes.,5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"Indeed, the expression variety is not significantly different between “human operator” framing (mean=0.131, std=0.023) and the “AI” framing (mean=0.136, std=0.021) (Wilcoxon rank sum test, p = 0.1338).",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
Study about repetition does not reveal any effect from the framing condition.,5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"As a matter of fact, the expression repetition is not significantly different between “human operator” framing (mean=0.423, std=0.087) and “AI” framing (mean=0.409, std=0.085) (p = 0.2915).",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"Similarly, no effect is found at the token overlap.",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"Besides, analyses on the expression initiation (EI) and the expression repetition at the speaker level (ERS) yield the same results than the entire H-A corpus i.e. the verbal alignment is asymmetrical between the agent and the human.",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"Contrary to our hypothesis, framing does not quantitatively impact verbal alignment at the level of expressions.
",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"A similar analysis at the gender mismatch or match between the human participant and the agent (Brad or Ellie) does not reveal any difference at the expression variety, expression repetition (globally or by speaker), token overlap, and expression initiation.",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"These analyses confirm our hypothesis that gender does not quantitatively impact verbal alignment at the level of expressions in our H-A corpus.
",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
It turns out that some significant differences exist between the two types of negotiation (integrative and distributive) in the HA corpus.,5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"First, distributive negotiation leads to longer dialogues in number of utterances (mean=144.3, std=58.757) than integrative negotiation (mean=82.5, std=41.09).",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"Despite this difference in dialogue length, the expression variety is similar between the integrative negotiations (mean=0.133, std=0.022) and the distributive ones
(mean=0.133, std=0.020) (Wilcoxon signed rank test, p = 0.9847).",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"However, a major difference can be observed at the expression repetition which is significantly higher for the distributive negotiations (mean=0.456, std=0.073) than for the integrative negotiations (mean=0.375, std=0.084) (W = 142, p = 7.665 × 10−10 < 0.001, r = 0.87).",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
"All in all, this indicates that participants align more at the level of expressions in competitive negotiations than in cooperative ones.",5.3 Other Conditions in Human-Agent Interactions,[0],[0]
This may be due to the fact that they need to verbally align more on (counter-)propositions in competitive negotiations.,5.3 Other Conditions in Human-Agent Interactions,[0],[0]
We have presented automatic and generic measures of verbal alignment based on an expression framework focusing on repetition between DPs at the level of surface of text utterances.,5.4 Discussion,[0],[0]
"This framework mainly takes into account lexical cues by building a lexicon of shared expressions emerging during dialogue, but also syntactic cues to the extent of expressions (other work on conversations report a strong correlation between lexical and syntactic cues regarding alignment (Healey et al., 2014)).",5.4 Discussion,[0],[0]
"The proposed measures make it possible to quantify the routinization process (via EV), the degree of repetition between DPs (via ER), and the orientation of the verbal alignment (via IES and ERS) at the level of expressions.",5.4 Discussion,[0],[0]
"Besides, these measures are based on efficient algorithms (Gusfield, 1997) that make it realistic to envision an online usage in a dialogue system.",5.4 Discussion,[0],[0]
They have made it possible to check quantitatively that verbal alignment was real in both H-H and H-A task-oriented interactions (i.e. it is not likely to happen randomly).,5.4 Discussion,[0],[0]
"Next, they have helped contrasting quantitatively H-H interactions from H-A interactions, showing that verbal alignment was symmetrical in H-H interactions while being asymmetrical in H-A (comforting previous hypotheses (Branigan et al., 2010)).",5.4 Discussion,[0],[0]
"Finally, we have observed that H-A verbal alignment was independent of the gender of the agent (male or female) and of the framing of the experiment (human operator VS AI).",5.4 Discussion,[0],[0]
"However, the proposed measures indicate more verbal alignment in competitive negotiations than in cooperative ones that may be due to the need to reach more agreements during competitive negotiations.
",5.4 Discussion,[0],[0]
"Nevertheless, this work is limited to automatically quantifying repetitions at the lexical level.
",5.4 Discussion,[0],[0]
"Hence, it does not take into account other aspects of alignment such as linguistic style (Niederhoffer and Pennebaker, 2002) or higher level such as concepts (Brennan and Clark, 1996).",5.4 Discussion,[0],[0]
"However, the alignment theory proposes that alignment “percolates” between levels.",5.4 Discussion,[0],[0]
"As such, alignment at the level of repetition of expressions indicate alignment at other levels to some extent.",5.4 Discussion,[0],[0]
"Besides, this work does not consider the functions behind repetition such as conveying the reception of a message, appraising a proposal, introducing a disagreement, complaining (Tannen, 2007; Schenkein, 1980).",5.4 Discussion,[0],[0]
A functional analysis could explain more in depth the differences between the H-H and the H-A corpora.,5.4 Discussion,[0],[0]
"Lastly, an interesting perspective would be to confirm these results on another corpora involving comparable H-H and H-A dialogues.",5.4 Discussion,[0],[0]
This paper has presented a framework based on expression repetition at the surface text of dialogue utterances involving automatic and computationally inexpensive measures.,6 Conclusion and Future Work,[0],[0]
These measures make it possible to quantitatively characterise the strength and orientation of verbal alignment between DPs in a task-oriented dialogue.,6 Conclusion and Future Work,[0],[0]
A promising perspective of this work lies in the exploitation of these measures to adapt and align the verbal communicative behaviour of a virtual agent.,6 Conclusion and Future Work,[0],[0]
This work was supported by the European project H2020 ARIA-VALUSPA and the French ANR project IMPRESSIONS (ANR-15-CE23-0023).,Acknowledgments,[0],[0]
"We warmly thank Jonathan Gratch and David DeVault for sharing the negotiation corpora, and Catherine Pelachaud for valuable and enriching discussions.",Acknowledgments,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
This work aims at characterising verbal alignment processes for improving virtual agent communicative capabilities.,abstractText,[0],[0]
We propose computationally inexpensive measures of verbal alignment based on expression repetition in dyadic textual dialogues.,abstractText,[0],[0]
"Using these measures, we present a contrastive study between Human-Human and Human-Agent dialogues on a negotiation task.",abstractText,[0],[0]
We exhibit quantitative differences in the strength and orientation of verbal alignment showing the ability of our approach to characterise important aspects of verbal alignment.,abstractText,[0],[0]
Automatic Measures to Characterise Verbal Alignment in Human-Agent Interaction,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1372–1382 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1372",text,[0],[0]
"Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks.",1 Introduction,[0],[0]
"Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017).",1 Introduction,[0],[0]
"Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology.
",1 Introduction,[0],[0]
"Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging.",1 Introduction,[0],[0]
"Other than the costs of compiling a sizable validation set, human rank-
ings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3).",1 Introduction,[0],[0]
"These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016).",1 Introduction,[0],[0]
"These conclusions have not yet been implemented in GEC, despite their relevance.",1 Introduction,[0],[0]
"In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies.
",1 Introduction,[0],[0]
"The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties.",1 Introduction,[0],[0]
"MAEGE requires no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammaticality.",1 Introduction,[0],[0]
"For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each.",1 Introduction,[0],[0]
"It then computes the correlation of the induced partial order with the metric-induced rankings.
",1 Introduction,[0],[0]
"MAEGE addresses many of the problems with existing methodology: • Human rankings yield low inter-rater and
intra-rater agreement (§3).",1 Introduction,[0],[0]
"Indeed, Choshen and Abend (2018a) show that while annotators often generate different corrections given a sentence, they generally agree on whether a correction is valid or not.",1 Introduction,[0],[0]
"Unlike CHR, MAEGE bases its scores on human corrections, rather than on rankings.
",1 Introduction,[0],[0]
"• CHR uses system outputs to obtain human rankings, which may be misleading, as systems may share similar biases, thus neglecting to evaluate some types of valid corrections (§7).",1 Introduction,[0],[0]
"MAEGE addresses this issue by systematically traversing an inclusive space of corrections.
",1 Introduction,[0],[0]
"• The difficulty in handling ties is addressed by only evaluating correction pairs where one contains a sub-set of the errors of the other, and is therefore clearly better.
",1 Introduction,[0],[0]
"• MAEGE uses established statistical tests for determining the significance of its results, thereby avoiding ad-hoc methodologies used in CHR to tackle potential biases in human rankings (§5, §6).
",1 Introduction,[0],[0]
"In experiments on the standard NUCLE test set (Dahlmeier et al., 2013), we find that MAEGE often disagrees with CHR as to the quality of existing metrics.",1 Introduction,[0],[0]
"For example, we find that the standard GEC metric, M2, is a poor predictor of corpuslevel ranking, but a good predictor of sentencelevel pair-wise rankings.",1 Introduction,[0],[0]
"The best predictor of corpus-level quality by MAEGE is the referenceless LT metric (Miłkowski, 2010; Napoles et al., 2016b), while of the reference-based metrics, GLEU (Napoles et al., 2015) fares best.
",1 Introduction,[0],[0]
"In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivities of the metrics to corrections of different types, which to our knowledge is a novel contribution of this work.",1 Introduction,[0],[0]
"Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Section 7).",1 Introduction,[0],[0]
"The importance of interpretability and detail in evaluation practices (as opposed to just providing bottom-line figures), has also been stressed in MT evaluation (e.g., Birch et al., 2016).",1 Introduction,[0],[0]
We turn to presenting the metrics we experiment with.,2 Examined Metrics,[0],[0]
"The standard practice in GEC evaluation is to define differences between the source and a correction (or a reference) as a set of edits (Dale et al., 2012).",2 Examined Metrics,[0],[0]
"An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type.",2 Examined Metrics,[0],[0]
"For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit
results in “I want a book”.",2 Examined Metrics,[0],[0]
"Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others.",2 Examined Metrics,[0],[0]
"We denote the examined set of metrics with METRICS.
BLEU.",2 Examined Metrics,[0],[0]
"BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns.",2 Examined Metrics,[0],[0]
"While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016).",2 Examined Metrics,[0],[0]
"We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014).
GLEU.",2 Examined Metrics,[0],[0]
"GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU.",2 Examined Metrics,[0],[0]
"Recently, it was updated to better address multiple references (Napoles et al., 2016a).",2 Examined Metrics,[0],[0]
"GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference.
iBLEU.",2 Examined Metrics,[0],[0]
"iBLEU (Sun and Zhou, 2012) was introduced to monolingual translation in order to balance BLEU, by averaging it with the BLEU score of the source and the output.",2 Examined Metrics,[0],[0]
"This yields a metric that rewards similarity to the source, and not only overlap with the reference:
iBLEU(S,R,O) = αBLEU(O,R)−(1−α)BLEU(O,S)
",2 Examined Metrics,[0],[0]
"We set α = 0.8 as suggested by Sun and Zhou.
",2 Examined Metrics,[0],[0]
"F -Score computes the overlap of edits to the source in the reference, and in the output.",2 Examined Metrics,[0],[0]
"As system edits can be constructed in multiple ways, the standard M2 scorer (Dahlmeier and Ng, 2012) computes the set of edits that yields the maximum F -score.",2 Examined Metrics,[0],[0]
"As M2 requires edits from the source to the reference, and as MAEGE generates new source sentences, we use an established protocol to automatically construct edits from pairs of strings (Felice et al., 2016; Bryant et al., 2017).",2 Examined Metrics,[0],[0]
The protocol was shown to produce similar M2 scores to those produced with manual edits.,2 Examined Metrics,[0],[0]
"Following common practice, we use the Precision-oriented F0.5.
SARI.",2 Examined Metrics,[0],[0]
"SARI (Xu et al., 2016) is a referencebased metric proposed for sentence simplification.
",2 Examined Metrics,[0],[0]
"SARI averages three scores, measuring the extent to which n-grams are correctly added to the source, deleted from it and retained in it.",2 Examined Metrics,[0],[0]
"Where multiple references are present, SARI’s score is determined not as the maximum single-reference score, but some averaging over them.",2 Examined Metrics,[0],[0]
"As this may lead to an unintuitive case, where a correction which is identical to the output gets a score of less than 1, we experiment with an additional metric, MAX-SARI, which coincides with SARI for a single reference, and computes the maximum singlereference SARI score for multiple-references.
",2 Examined Metrics,[0],[0]
Levenshtein Distance.,2 Examined Metrics,[0],[0]
"We use the Levenshtein distance (Kruskal and Sankoff, 1983), i.e., the number of character edits needed to convert one string to another, between the correction and its closest reference (MinLDO→R).",2 Examined Metrics,[0],[0]
"To enrich the discussion, we also report results with a measure of conservatism, LDS→O, i.e., the Levenshtein distance between the correction and the source.",2 Examined Metrics,[0],[0]
"Both distances are normalized by the number of characters in the second string (R,O respectively).",2 Examined Metrics,[0],[0]
"In order to convert these distance measures into measures of similarity, we report 1− LD(c1,c2)len(c1) .
",2 Examined Metrics,[0],[0]
"Grammaticality is a reference-less metric, which uses grammatical error detection tools to assess the grammaticality of GEC system outputs.",2 Examined Metrics,[0],[0]
"We use LT (Miłkowski, 2010), the best performing non-proprietary grammaticality metric (Napoles et al., 2016b).",2 Examined Metrics,[0],[0]
The detection tool at the base of LT can be much improved.,2 Examined Metrics,[0],[0]
"Indeed, Napoles et al. (2016b) reported that the proprietary tool they used detected 15 times more errors than LT.",2 Examined Metrics,[0],[0]
A sentence’s score is defined to be 1− #errors#tokens .,2 Examined Metrics,[0],[0]
"See (Asano et al., 2017; Choshen and Abend, 2018b) for additional reference-less measures, published concurrently with this work.
",2 Examined Metrics,[0],[0]
I-Measure.,2 Examined Metrics,[0],[0]
"I-Measure (Felice and Briscoe, 2015) is a weighted accuracy metric over tokens.",2 Examined Metrics,[0],[0]
I-measure rank determines whether a correction is better than the source and to what extent.,2 Examined Metrics,[0],[0]
"Unlike in this paper, I-measure assumes that every pair of intersecting edits (i.e., edits whose spans of tokens overlap) are alternating, and that non-intersecting edits are independent.",2 Examined Metrics,[0],[0]
"Consequently, where multiple references are present, it extends the set of references, by generating every possible combination of independent edits.",2 Examined Metrics,[0],[0]
"As the number of combinations is generally exponential in the number of references, the procedure can be severely inefficient.
",2 Examined Metrics,[0],[0]
"Indeed, a sentence in the test set has 3.5 billion references on average, where the median is 512 (See Figure 1).",2 Examined Metrics,[0],[0]
"I-measure can also be run without generating new references, but despite parallelization efforts, this version did not terminate after 140 CPU days, while the cumulative CPU time of the rest of the metrics was less than 1.5 days.",2 Examined Metrics,[0],[0]
Correlation with human rankings (CHR) is the standard methodology for assessing the validity of GEC metrics.,3 Human Ranking Experiments,[0],[0]
"While informative, human rankings are costly to produce, present low inter-rater agreement (shown for MT evaluation in (Bojar et al., 2011; Dras, 2015)), and introduce methodological difficulties that are hard to overcome.",3 Human Ranking Experiments,[0],[0]
"We begin by showing that existing sets of human rankings produce inconsistent results with respect to the quality of different metrics, and proceed by proposing an improved protocol for computing this correlation in the future.
",3 Human Ranking Experiments,[0],[0]
"There are two existing sets of human rankings for GEC that were compiled concurrently: GJG15 by Grundkiewicz et al. (2015), and NSPT15 by Napoles et al. (2015).",3 Human Ranking Experiments,[0],[0]
"Both sets are based on system outputs from the CoNLL 2014 (Ng et al., 2014) shared task, using sentences from the NUCLE test set.",3 Human Ranking Experiments,[0],[0]
We compute CHR against each.,3 Human Ranking Experiments,[0],[0]
"System-level correlations are computed by TrueSkill (Sakaguchi et al., 2014), which adopts its methodology from MT.1
1There’s a minor problem in the output of the NTHU system: a part of the input is given as sentence 39 and sentence 43 is missing.",3 Human Ranking Experiments,[0],[0]
"We corrected it to avoid unduly penalizing NTHU for all the sentences in this range.
",3 Human Ranking Experiments,[0],[0]
Table 1 shows CHR with Spearman ρ,3 Human Ranking Experiments,[0],[0]
(Pearson r shows similar trends).,3 Human Ranking Experiments,[0],[0]
"Results on the two datasets diverge considerably, despite their use of the same systems and corpus (albeit a different sub-set thereof).",3 Human Ranking Experiments,[0],[0]
"For example, BLEU receives a high positive correlation on GJG15, but a negative one on NSPT15; GLEU receives a correlation of 0.51 against GJG15 and 0.76 against NSPT15; and M2 ranges between 0.4 (GJG15) and 0.7 (NSPT15).",3 Human Ranking Experiments,[0],[0]
"In fact, this variance is already apparent in the published correlations of GLEU, e.g., Napoles et al. (2015) reported a ρ of 0.56 against NSPT15 and Napoles et al. (2016b) reported a ρ of 0.85 against GJG15.2",3 Human Ranking Experiments,[0],[0]
"This variance in the metrics’ scores is an example of the low agreement between human rankings, echoing similar findings in MT (Bojar et al., 2011; Lopez, 2012; Dras, 2015).
",3 Human Ranking Experiments,[0],[0]
"Another source of inconsistency in CHR is that the rankings are relative and sampled, so datasets rank different sets of outputs (Lopez, 2012).",3 Human Ranking Experiments,[0],[0]
"For example, if a system is judged against the best systems more often then others, it may unjustly receive a lower score.",3 Human Ranking Experiments,[0],[0]
"TrueSkill is the best known practice to tackle such issues (Bojar et al., 2014), but it produces a probabilistic corpus-level score, which can vary between runs (Sakaguchi et al., 2016).3 This makes CHR more difficult to interpret, compared to classic correlation coefficients.
",3 Human Ranking Experiments,[0],[0]
We conclude by proposing a practice for reporting CHR in future work.,3 Human Ranking Experiments,[0],[0]
"First, we combine both sets of human judgments to arrive at the statistically most powerful test.",3 Human Ranking Experiments,[0],[0]
"Second, we compute the metrics’ corpus-level rankings according to the same subset of sentences used for human rankings.",3 Human Ranking Experiments,[0],[0]
"The current practice of allowing metrics to rank systems based on their output on the entire CoNLL test set (while human rankings are only collected for a sub-set thereof), may bias the results due to potential non-uniform system performance on the test set.",3 Human Ranking Experiments,[0],[0]
We report CHR according to the proposed protocol in Table 1 (left column).,3 Human Ranking Experiments,[0],[0]
"In the following sections we present MAEGE an alternative methodology to CHR, which uses human corrections to induce more reliable and scalable rankings to compare metrics against.",4 Constructing Lattices of Corrections,[0],[0]
"We begin our presentation by detailing the method MAEGE
2The difference between our results and previously reported ones is probably due to a recent update in GLEU to better tackles multiple references (Napoles et al., 2016a).
",4 Constructing Lattices of Corrections,[0],[0]
"3The standard deviation of the results is about 0.02.
",4 Constructing Lattices of Corrections,[0],[0]
uses to generate source-correction pairs and a partial order between them.,4 Constructing Lattices of Corrections,[0],[0]
"MAEGE operates by using a corpus with gold annotation, given as edits, to generate lattices of corrections, each defined by a sub-set of the edits.",4 Constructing Lattices of Corrections,[0],[0]
"Within the lattice, every pair of sentences can be regarded as a potential source and a potential output.",4 Constructing Lattices of Corrections,[0],[0]
"We create sentence chains, in an increasing order of quality, taking a source sentence and applying edits in some order one after the other (see Figure 2 and 3).
",4 Constructing Lattices of Corrections,[0],[0]
"Formally, for each sentence s in the corpus and each annotation a, we have a set of typed edits edits(s, a) =",4 Constructing Lattices of Corrections,[0],[0]
"{e(1)s,a, . . .",4 Constructing Lattices of Corrections,[0],[0]
", e(ns,a)s,a } of size ns,a.",4 Constructing Lattices of Corrections,[0],[0]
"We call 2edits(s,a) the corrections lattice, and denote it with Es,a.",4 Constructing Lattices of Corrections,[0],[0]
"We call, s, the correction corresponding to ∅ the original.",4 Constructing Lattices of Corrections,[0],[0]
"We define a partial order relation between x, y ∈ Es,a such that x",4 Constructing Lattices of Corrections,[0],[0]
< y if x ⊂,4 Constructing Lattices of Corrections,[0],[0]
y.,4 Constructing Lattices of Corrections,[0],[0]
"This order relation is assumed to be the gold standard ranking between the corrections.
",4 Constructing Lattices of Corrections,[0],[0]
"For our experiments, we use the NUCLE test data (Ng et al., 2014).",4 Constructing Lattices of Corrections,[0],[0]
Each sentence is paired with two annotations.,4 Constructing Lattices of Corrections,[0],[0]
"The other eight available
references, produced by Bryant and Ng (2015), are used as references for the reference-based metrics.",4 Constructing Lattices of Corrections,[0],[0]
"Denote the set of references for s with Rs.
Sentences which require no correction according to at least one of the two annotations are discarded.",4 Constructing Lattices of Corrections,[0],[0]
"In 26 cases where two edit spans intersect in the same annotation (out of a total of about 40K edits), the edits are manually merged or split.",4 Constructing Lattices of Corrections,[0],[0]
"We conduct a corpus-level analysis, namely testing the ability of metrics to determine which corpus of corrections is of better quality.",5 Corpus-level Analysis,[0],[0]
"In practice, this procedure is used to rank systems based on their outputs on the test corpus.
",5 Corpus-level Analysis,[0],[0]
"In order to compile corpora corresponding to systems of different quality levels, we define sev-
eral corpus models, each applying a different expected number of edits to the original.",5 Corpus-level Analysis,[0],[0]
Models are denoted with the expected number of edits they apply to the original which is a positive number M ∈ R+.,5 Corpus-level Analysis,[0],[0]
"Given a corpus model M , we generate a corpus of corrections by traversing the original sentences, and for each sentence s uniformly sample an annotation a (i.e., a set of edits that results in a perfect correction), and the number of edits applied nedits, which is sampled from a clipped binomial probability with mean M and variance 0.9.",5 Corpus-level Analysis,[0],[0]
"Given nedits, we uniformly sample from the latticeEs,a a sub-set of edits of size nedits, and apply this set of edits to s. The corpus of M = 0 is the set of originals.
",5 Corpus-level Analysis,[0],[0]
"The corpus of source sentences, against which all other corpora are compared, is sampled by traversing the original sentences, and for each sentence s, uniformly sample an annotation a, and given s, a, uniformly sample a sentence fromEs,a.
",5 Corpus-level Analysis,[0],[0]
"Given a metric m ∈ METRICS, we compute its score for each sampled corpus.",5 Corpus-level Analysis,[0],[0]
"Where corpuslevel scores are not defined by the metrics themselves, we use the average sentence score instead.",5 Corpus-level Analysis,[0],[0]
"We compare the rankings induced by the scores of m and the ranking of systems according to their corpus model (i.e., systems that have a higher M should be ranked higher), and report the correlation between these rankings.",5 Corpus-level Analysis,[0],[0]
Setup.,5.1 Experiments,[0],[0]
"For each model, we sample one correction per NUCLE sentence, noting that it is possible to reduce the variance of the metrics’ corpuslevel scores by sampling more.",5.1 Experiments,[0],[0]
Corpus models of integer values between 0 and 10 are taken.,5.1 Experiments,[0],[0]
"We report Spearman ρ, commonly used for system-level rankings (Bojar et al., 2017).4
Results.",5.1 Experiments,[0],[0]
"Results, presented in Table 2 (left part), shows that LT correlates best with the rankings induced by MAEGE, where GLEU is second.",5.1 Experiments,[0],[0]
M2’s correlation is only 0.06.,5.1 Experiments,[0],[0]
"We note that the LT requires a complementary metric to penalize grammatical outputs that diverge in meaning from the source (Napoles et al., 2016b).",5.1 Experiments,[0],[0]
"See §8.
",5.1 Experiments,[0],[0]
"Comparing the metrics’ quality in corpus-level evaluation with their quality according to CHR (§3), we find they are often at odds.",5.1 Experiments,[0],[0]
"Figure 4 plots the Spearman correlation of the different metrics according to the two validation methodologies,
4Using Pearson correlation shows similar trends.
",5.1 Experiments,[0],[0]
"showing correlations are slightly correlated, but disagreements as to metric quality are frequent and substantial (e.g., with iBLEU or SARI).",5.1 Experiments,[0],[0]
"We proceed by presenting a method for assessing the correlation between metric-induced scores of corrections of the same sentence, and the scores given to these corrections by MAEGE.",6 Sentence-level Analysis,[0],[0]
"Given a sentence s and an annotation a, we sample a random permutation over the edits in edits(s, a).",6 Sentence-level Analysis,[0],[0]
"We denote the permutation with σ ∈ Sns,a , where Sns,a is the permutation group over {1, · · · , ns,a}.",6 Sentence-level Analysis,[0],[0]
"Given σ, we define a monotonic chain in Ei,j as:
chain(s, a, σ) =",6 Sentence-level Analysis,[0],[0]
"( ∅ < {e(σ(1))s,a } < {e(σ(1))s,a , e(σ(2))s,a } <
. . .",6 Sentence-level Analysis,[0],[0]
"< edits(s, a) )
",6 Sentence-level Analysis,[0],[0]
"For each chain, we uniformly sample one of its elements, mark it as the source, and denote it with src.",6 Sentence-level Analysis,[0],[0]
"In order to generate a set of chains, MAEGE
traverses the original sentences and annotations, and for each sentence-annotation pair, uniformly samples nch chains without repetition.",6 Sentence-level Analysis,[0],[0]
It then uniformly samples a source sentence from each chain.,6 Sentence-level Analysis,[0],[0]
"If the number of chains inEs,a is smaller than nch, MAEGE selects all the chains.
",6 Sentence-level Analysis,[0],[0]
"Given a metric m ∈ METRICS, we compute its score for every correction in each sampled chain against the sampled source and available references.",6 Sentence-level Analysis,[0],[0]
We compute the sentence-level correlation of the rankings induced by the scores of m and the rankings induced by <.,6 Sentence-level Analysis,[0],[0]
"For computing rank correlation (such as Spearman ρ or Kendall τ ), such a relative ranking is sufficient.
",6 Sentence-level Analysis,[0],[0]
"We report Kendall τ , which is only sensitive to the relative ranking of correction pairs within the same chain.",6 Sentence-level Analysis,[0],[0]
"Kendall is minimalistic in its assumptions, as it does not require numerical scores, but only assuming that < is well-motivated, i.e., that applying a set of valid edits is better in quality than applying only a subset of it.
",6 Sentence-level Analysis,[0],[0]
"As < is a partial order, and as Kendall τ is standardly defined over total orders, some modification is required.",6 Sentence-level Analysis,[0],[0]
"τ is a function of the number of compared pairs and of discongruent pairs (ordered differently in the compared rankings):
τ",6 Sentence-level Analysis,[0],[0]
"= 1− 2 |discongruent pairs| |all pairs| .
",6 Sentence-level Analysis,[0],[0]
"To compute these quantities, we extract all unique pairs of corrections that can be compared with < (i.e., one applies a sub-set of the edits of the other), and count the number of discongruent ones between the metric’s ranking and <.",6 Sentence-level Analysis,[0],[0]
Significance is modified accordingly.5 Spearman,6 Sentence-level Analysis,[0],[0]
"ρ is
5Code can be found in https://github.com/ borgr/EoE
less applicable in this setting, as it compares total orders whereas here we compare partial orders.
",6 Sentence-level Analysis,[0],[0]
"To compute linear correlation with Pearson r, we make the simplifying assumption that all edits contribute equally to the overall quality.",6 Sentence-level Analysis,[0],[0]
"Specifically, we assume that a perfect correction (i.e., the top of a chain) receives a score of 1.",6 Sentence-level Analysis,[0],[0]
"Each original sentence s (the bottom of a chain), for which there exists annotations a1, . . .",6 Sentence-level Analysis,[0],[0]
", an, receives a score of
1−min i |edits(s, ai)| |tokens(s)| .
",6 Sentence-level Analysis,[0],[0]
The scores of partial (non-perfect) corrections in each chain are linearly spaced between the score of the perfect correction and that of the original.,6 Sentence-level Analysis,[0],[0]
"This scoring system is well-defined, as a partial correction receives the same score according to all chains it is in, as all paths between a partial correction and the original have the same length.",6 Sentence-level Analysis,[0],[0]
Setup.,6.1 Experiments,[0],[0]
"We experiment with nch = 1, yielding 7936 sentences in 1312 chains (same as the number of original sentences in the NUCLE test set).",6.1 Experiments,[0],[0]
"We report the Pearson correlation over the scores of all sentences in all chains (r), and Kendall τ over all pairs of corrections within the same chain.
Results.",6.1 Experiments,[0],[0]
Results are presented in Table 2 (right part).,6.1 Experiments,[0],[0]
"No metric scores very high, neither according to Pearson r nor according to Kendall τ .",6.1 Experiments,[0],[0]
"iBLEU correlates best with < according to r, obtaining a correlation of 0.23, whereas LT fares best according to τ , obtaining 0.222.
",6.1 Experiments,[0],[0]
Results show a discrepancy between the low corpus-level and sentence-level r correlations of M2 and its high sentence-level τ .,6.1 Experiments,[0],[0]
"It seems that although M2 orders pairs of corrections well, its scores are not a linear function of MAEGE’s scores.",6.1 Experiments,[0],[0]
"This may be due to M2’s assignment of the minimal possible score to the source, regardless of its quality.",6.1 Experiments,[0],[0]
"M2 thus seems to predict well the relative quality of corrections of the same sentence, but to be less effective in yielding a globally coherent score (cf.",6.1 Experiments,[0],[0]
"Felice and Briscoe (2015)).
",6.1 Experiments,[0],[0]
"GLEU shows the inverse behaviour, failing to correctly order pairs of corrections of the same sentence, while managing to produce globally coherent scores.",6.1 Experiments,[0],[0]
"We test this hypothesis by computing the average difference in GLEU score between all pairs in the sampled chains, and find it to be slightly negative (-0.00025), which is in line with
GLEU’s small negative τ .",6.1 Experiments,[0],[0]
"On the other hand, plotting the GLEU scores of the originals grouped by the number of errors they contain, we find they correlate well (Figure 5), indicating that GLEU performs well in comparing the quality of corrections of different sentences.",6.1 Experiments,[0],[0]
Four sentences with considerably more errors than the others were considered outliers and removed.,6.1 Experiments,[0],[0]
MAEGE’s lattice can be used to analyze how the examined metrics reward corrections of errors of different types.,7 Metric Sensitivity by Error Type,[0],[0]
"For each edit type t, we denote with St the set of correction pairs from the lattice that only differ in an edit of type t. For each such pair (c, c′) and for each metric m, we compute the difference in the score assigned by m to c and c′.",7 Metric Sensitivity by Error Type,[0],[0]
"The average difference is denoted with ∆m,t.
∆m,t = 1 |St| ∑
(c,c′)∈St
[ m(src, c, R)−m(src, c′, R) ]",7 Metric Sensitivity by Error Type,[0],[0]
R is the corresponding reference set.,7 Metric Sensitivity by Error Type,[0],[0]
"A negative (positive) ∆m,t indicates that m penalizes (awards) valid corrections of type t.",7 Metric Sensitivity by Error Type,[0],[0]
Setup.,7.1 Experiments,[0],[0]
"We sample chains using the same sampling method as in §6, and uniformly sample a source from each chain.",7.1 Experiments,[0],[0]
"For each edit type t, we detect all pairs of corrections in the sampled chains that only differ in an edit of type t, and use them to compute ∆m,t. We use the set of 27 edit types given in the NUCLE corpus.
Results.",7.1 Experiments,[0],[0]
"Table 3 presents the results, showing that under all metrics, some edits types are penalized and others rewarded.",7.1 Experiments,[0],[0]
"iBLEU and LT penalize the least edit types, and GLEU penalizes the most, providing another perspective on GLEU’s negative Kendall τ",7.1 Experiments,[0],[0]
(§6).,7.1 Experiments,[0],[0]
Certain types are penalized by almost all metrics.,7.1 Experiments,[0],[0]
"One such type is Vm, wrong verb modality (e.g., “as they [∅ ; may] not want to know”).",7.1 Experiments,[0],[0]
"Another such type is Npos, a problem in noun possessive (e.g., “their [facebook’s ; Facebook] page”).",7.1 Experiments,[0],[0]
"Other types, such as Mec, mechanical (e.g., “[real-life ; real life]”), and V0, missing verb (e.g., “’Privacy’, this is the word that [∅; is] popular”), are often rewarded by the metrics.
",7.1 Experiments,[0],[0]
"In general, the tendency of reference-based metrics (the vast majority of GEC metrics) to penalize edits of various types suggests that many edit
types are under-represented in available reference sets.",7.1 Experiments,[0],[0]
"Automatic evaluation of systems that perform these edit types may, therefore, be unreliable.",7.1 Experiments,[0],[0]
"Moreover, not addressing these biases in the metrics may hinder progress in GEC.",7.1 Experiments,[0],[0]
"Indeed, M2 and GLEU, two of the most commonly used metrics, only award a small sub-set of edit types, thus offering no incentive for systems to improve performance on such types.6",7.1 Experiments,[0],[0]
We revisit the argument that using system outputs to perform metric validation poses a methodological difficulty.,8 Discussion,[0],[0]
"Indeed, as GEC systems are developed, trained and tested using available metrics, and as metrics tend to reward some correction types and penalize others (§7), it is possible that GEC development adjusts to the metrics, and neglects some error types.",8 Discussion,[0],[0]
"Resulting tendencies in GEC systems would then yield biased sets of outputs for human rankings, which in turn would result in biases in the validation process.
",8 Discussion,[0],[0]
"To make this concrete, GEC systems are often precision-oriented: trained to prefer not to correct than to invalidly correct.",8 Discussion,[0],[0]
"Indeed, Choshen and
6LDS→O tends to award valid corrections of almost all types.",8 Discussion,[0],[0]
"As source sentences are randomized across chains, this indicates that on average, corrections with more applied edits tend to be more similar to comparable corrections on the lattice.",8 Discussion,[0],[0]
"This is also reflected by the slightly positive sentencelevel correlation of LDS→O (§6).
",8 Discussion,[0],[0]
"Abend (2018a) show that modern systems tend to be highly conservative, often performing an order of magnitude fewer changes to the source than references do.",8 Discussion,[0],[0]
"Validating metrics on their ability to rank conservative system outputs (as is de facto the common practice) may produce a different picture of metric quality than when considering a more inclusive set of corrections.
",8 Discussion,[0],[0]
We use MAEGE to mimic a setting of ranking against precision-oriented outputs.,8 Discussion,[0],[0]
"To do so, we perform corpus-level and sentence-level analyses, but instead of randomly sampling a source, we invariably take the original sentence as the source.",8 Discussion,[0],[0]
"We thereby create a setting where all edits applied are valid (but not all valid edits are applied).
",8 Discussion,[0],[0]
"Comparing the results to the regular MAEGE correlation (Table 4), we find that LT remains reliable, while M2, that assumes the source receives the worst possible score, gains from this unbalanced setting.",8 Discussion,[0],[0]
"iBLEU drops, suggesting it may need to be retuned to this setting and give less weight toBLEU(O,S), thus becoming more like BLEU and GLEU.",8 Discussion,[0],[0]
"The most drastic change we see is in SARI and MAX-SARI, which flip their sign and present strong performance.",8 Discussion,[0],[0]
"Interestingly, the metrics that benefit from this precisionoriented setting in the corpus-level are the same metrics that perform better according to CHR than to MAEGE (Figure 4).",8 Discussion,[0],[0]
"This indicates the different trends produced by MAEGE and CHR, may result
from the latter’s use of precision-oriented outputs.
",8 Discussion,[0],[0]
Drawbacks.,8 Discussion,[0],[0]
Like any methodology MAEGE has its simplifying assumptions and drawbacks; we wish to make them explicit.,8 Discussion,[0],[0]
"First, any biases introduced in the generation of the test corpus are inherited by MAEGE (e.g., that edits are contiguous and independent of each other).",8 Discussion,[0],[0]
"Second, MAEGE does not include errors that a human will not perform but machines might, e.g., significantly altering the meaning of the source.",8 Discussion,[0],[0]
"This partially explains why LT, which measures grammaticality but not meaning preservation, excels in our experiments.",8 Discussion,[0],[0]
"Third, MAEGE’s scoring system (§6) assumes that all errors damage the score equally.",8 Discussion,[0],[0]
"While this assumption is made by GEC metrics, we believe it should be refined in future work by collecting user information.",8 Discussion,[0],[0]
"In this paper, we show how to leverage existing annotation in GEC for performing validation reliably.",9 Conclusion,[0],[0]
"We propose a new automatic methodology, MAEGE, which overcomes many of the shortcomings of the existing methodology.",9 Conclusion,[0],[0]
Experiments with MAEGE reveal a different picture of metric quality than previously reported.,9 Conclusion,[0],[0]
"Our analysis suggests that differences in observed metric quality are partly due to system outputs sharing consistent tendencies, notably their tendency to under-predict corrections.",9 Conclusion,[0],[0]
"As existing methodology ranks system outputs, these shared tendencies bias the validation process.",9 Conclusion,[0],[0]
"The difficulties in basing validation on system outputs may be applicable to other text-to-text generation tasks, a question we will explore in future work.",9 Conclusion,[0],[0]
"This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office.",Acknowledgments,[0],[0]
We thank Joel Tetreault and Courtney Napoles for helpful feedback and inspiring conversations.,Acknowledgments,[0],[0]
Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings.,abstractText,[0],[0]
"However, such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement.",abstractText,[0],[0]
"We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices.",abstractText,[0],[0]
"Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M2 metric fares poorly on corpus-level ranking.",abstractText,[0],[0]
"Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics.",abstractText,[0],[0]
Automatic Metric Validation for Grammatical Error Correction,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3143–3153 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3143",text,[0],[0]
Language is one of the most important forms of human intelligence and poetry is a concise and graceful art of human language.,1 Introduction,[0],[0]
"Across different countries, nationalities and cultures, poetry is always popular, having far-reaching influence on the development of human society.
",1 Introduction,[0],[0]
"In this work, we concentrate on automatic poetry generation.",1 Introduction,[0],[0]
"Besides the long-term goal of building artificial intelligence, research on this task could become the auxiliary tool to better analyse poetry and understand the internal mechanism of human writing.",1 Introduction,[0],[0]
"In addition, these generation
∗Corresponding author: sms@mail.tsinghua.edu.cn.
systems are also helpful for electronic entertainments and literary education.
",1 Introduction,[0],[0]
"In recent years, neural networks have proven to be powerful on poetry generation.",1 Introduction,[0],[0]
Some neural models are proposed and achieve significant improvement.,1 Introduction,[0],[0]
"However, existing models are all based on maximum likelihood estimation (MLE), which brings two substantial problems.",1 Introduction,[0],[0]
"First, MLE-based models tend to remember common patterns of the poetry corpus (Zhang et al., 2017), such as high-frequency bigrams and stop words, losing some diversity and innovation for generated poetry.",1 Introduction,[0],[0]
"Moreover, based on word-level likelihood, two kinds of loss-evaluation mismatch (Wiseman and Rush, 2016) arise.",1 Introduction,[0],[0]
One is evaluation granularity mismatch.,1 Introduction,[0],[0]
"When evaluating, human experts usually focus on sequence level (a poem line) or discourse level (a whole poem), while MLE optimizes word-level loss, which fails to hold a wider view of generated poems.",1 Introduction,[0],[0]
The other is criteria mismatch.,1 Introduction,[0],[0]
"Instead of the likelihood, humans usually evaluate poetry in terms of some criteria.",1 Introduction,[0],[0]
"In this work we focus on the main four criteria
(Manurung, 2003; Zhang and Lapata, 2014; Yan, 2016; Yi et al., 2017): fluency (are the lines fluent and well-formed?), coherence (is the poem as a whole coherent in meaning and theme?),",1 Introduction,[0],[0]
"meaningfulness (does the poem convey some certain messages?), overall quality (the reader’s general impression on the poem).",1 Introduction,[0],[0]
"This mismatch may make the model lean towards optimizing easier criteria, e.g., fluency, and ignore other ones.
",1 Introduction,[0],[0]
"To tackle these problems, we directly model the four aforementioned human evaluation criteria and use them as explicit rewards to guide gradient update by reinforcement learning.",1 Introduction,[0],[0]
"This is a criterion-driven training process, which motivates the model to generate poems with higher scores on these criteria.",1 Introduction,[0],[0]
"Besides, in writing theories, writing requires observing other learners (Bandura, 2001).",1 Introduction,[0],[0]
"It is also shown that writing is supported as an activity in which writers will learn from more experienced writers, such as other students, teachers, or authors (Prior, 2006).",1 Introduction,[0],[0]
Therefore it is necessary to equip generators with the ability of mutual learning and communication.,1 Introduction,[0],[0]
"Inspired by this, we propose a novel mutual reinforcement learning schema (Figure 1), where we simultaneously train two learners (generators).",1 Introduction,[0],[0]
"During the training process, one learner will learn not only from the teacher (rewarder) but also from the other.",1 Introduction,[0],[0]
"We will show this mutual learning-teaching process leads to better results.
",1 Introduction,[0],[0]
"In summary, our contributions are as follows:
• To the best of our knowledge, for the sake of tackling the loss-evaluation mismatch problem in poetry generation, we first utilize reinforcement learning to model and optimize human evaluation criteria.
",1 Introduction,[0],[0]
"• We propose a novel mutual reinforcement learning schema to further improve performance, which is transparent to model architectures.",1 Introduction,[0],[0]
"One can apply it to any poetry generation model.
",1 Introduction,[0],[0]
• We experiment on Chinese quatrains.,1 Introduction,[0],[0]
Both automatic and human evaluation results show that our method outperforms a strong basic method and the state-of-the-art model.,1 Introduction,[0],[0]
"As a desirable entry point of automatic analysing, understanding and generating literary text, the research on poetry generation has lasted for decades.
",2 Related Work,[0],[0]
"In recent twenty years, the models can be categorized into two main paradigms.
",2 Related Work,[0],[0]
The first one is based on statistical machine learning methods.,2 Related Work,[0],[0]
"Genetic algorithms (Manurung, 2003; Levy, 2001), Statistical Machine Translation (SMT) approaches (He et al., 2012; Jiang and Zhou, 2008) and Automatic Summarization approaches (Yan et al., 2013) are all adopted to generate poetry.
",2 Related Work,[0],[0]
"More recently, the second paradigm, neural network, has shown great advantages in this task, compared to statistical models.",2 Related Work,[0],[0]
"Recurrent Neural Network (RNN) is first used to generate Chinese quatrains by (Zhang and Lapata, 2014).",2 Related Work,[0],[0]
"To improve fluency and coherence, Zhang’s model needs to be interpolated with extra SMT features as shown in their paper.",2 Related Work,[0],[0]
"Focusing on coherence, some works (Yi et al., 2017; Wang et al., 2016a) use sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015) to generate poetry.",2 Related Work,[0],[0]
"Wang et al. (2016b) design a special Planning schema, which plans some sub-keywords in advance by a language model and then generates each line with the planned sub-keyword to improve coherence.",2 Related Work,[0],[0]
"Pursuing better overall quality, Yan (2016) proposes an iterative polishing schema to generate Chinese poetry, which refines the poem generated in one pass for several times.",2 Related Work,[0],[0]
"Aiming at enhancing meaningfulness, Ghazvininejad et al. (2016) extend user keywords to incorporate richer semantic information.",2 Related Work,[0],[0]
"Zhang et al. (2017) combine a neural memory, which saves hundreds of human-authored poems, with a sequence-to-sequence model to improve innovation of generated poems and achieve style transfer.
",2 Related Work,[0],[0]
These neural structures have made some progress and improved different aspects of generated poetry.,2 Related Work,[0],[0]
"Nevertheless, as discussed in Section 1, the two essential problems, lack of diversity and loss-evaluation mismatch, are still challenging resulting from MLE.",2 Related Work,[0],[0]
"Compared to further adjusting model structures, we believe a better solution is to design more reasonable optimization objectives.
",2 Related Work,[0],[0]
"Deep Reinforcement Learning (DRL) first shows its magic power in automatic game playing, such as Atari electronic games (Mnih et al., 2013) and the game of Go (Silver et al., 2016).",2 Related Work,[0],[0]
"Soon, DRL is used to playing text games (Narasimhan et al., 2015; He et al., 2016) and then applied to dialogue generation (Li et al., 2016b).
",2 Related Work,[0],[0]
"From the perspective of poetry education, the
teacher will judge student-created poems in terms of some specific criteria and guide the student to cover the shortage, which naturally accords with DRL process.",2 Related Work,[0],[0]
Therefore we take advantage of DRL.,2 Related Work,[0],[0]
"We design four automatic rewarders for the criteria, which act as the teacher.",2 Related Work,[0],[0]
"Furthermore, we train two generators and make them learn from each other, which imitates the mutual learning of students, as a step towards multi-agent DRL in literary text generation.",2 Related Work,[0],[0]
"We apply our method to a basic poetry generation model, which is pre-trained with MLE.",3.1 Basic Generation Model,[0],[0]
"Therefore, we first formalize our task and introduce this model.
",3.1 Basic Generation Model,[0],[0]
"The inputs are user topics specified by K keywords,W = {wk}Kk=1.",3.1 Basic Generation Model,[0],[0]
"The output is a poem consisting of n lines, P = L1, L2, · · · , Ln.",3.1 Basic Generation Model,[0],[0]
"Since we take the line-by-line generation process, the task can be converted to the generation of an i-th line given previous i-1 lines L1:i−1 andW .
",3.1 Basic Generation Model,[0],[0]
"We use GRU-based (Cho et al., 2014) sequenceto-sequence model.",3.1 Basic Generation Model,[0],[0]
"−→ h t , ←− h t and st represent the forward encoder, backward encoder and decoder hidden states respectively.",3.1 Basic Generation Model,[0],[0]
"For each topic word wk = c1, c2, · · · , cTk , we feed characters into the encoder and get the keyword representation vk =",3.1 Basic Generation Model,[0],[0]
[ −→ h Tk ; ←− h,3.1 Basic Generation Model,[0],[0]
"Tk ], where [;] means concatenation.",3.1 Basic Generation Model,[0],[0]
"Then we get the topic representation by1:
o = f",3.1 Basic Generation Model,[0],[0]
"( 1
K K∑ t=1 vk), (1)
where f defines a non-linear layer.",3.1 Basic Generation Model,[0],[0]
"Denote the generated i-th line in decoder, Y = (y1y2 . . .",3.1 Basic Generation Model,[0],[0]
yTi).,3.1 Basic Generation Model,[0],[0]
e(yt) is the word embedding of yt.,3.1 Basic Generation Model,[0],[0]
"The probability distribution of each yt to be generated in Li is calculated by:
st = GRU(st−1, [e(yt−1); o; gi−1]), (2) P (yt|y1:t−1, L1:i−1,W) = softmax(Wst), (3)
where W is the projection parameter.",3.1 Basic Generation Model,[0],[0]
"gi−1 is a global history vector, which records what has been generated so far and provides global-level information for the model.",3.1 Basic Generation Model,[0],[0]
"Once Li is generated, it is
1For brevity, we omit biases in all equations.
updated by a convolutional layer:
at = f([st; · · · ; st+d−1]), (4) gi = f(gi−1, ∑ t at), g0 = 0, (5)
where 0 is a vector with all 0-s and d is convolution window size.",3.1 Basic Generation Model,[0],[0]
"Then the basic model is pretrained by minimizing standard MLE loss:
LMLE(θ) =",3.1 Basic Generation Model,[0],[0]
"− M∑
m=1
logP (Pm|Wm; θ), (6)
where M is data size and θ is the parameter set to be trained.
",3.1 Basic Generation Model,[0],[0]
"This basic model is a modified version of (Yan, 2016).",3.1 Basic Generation Model,[0],[0]
"The main differences are that we replace vanilla RNN with GRU unit, use convolution to calculate the line representation rather than directly use the last decoder hidden state, and we remove the polishing schema to better obverse the influence of DRL itself.",3.1 Basic Generation Model,[0],[0]
"We select this model as our basic framework since it achieves satisfactory performance and the author has done thorough comparisons with other models, such as (Yan et al., 2013) and (Zhang and Lapata, 2014).",3.1 Basic Generation Model,[0],[0]
"Before presenting the single-learner version of our method (abbreviated as SRL), we first design corresponding automatic rewarders for the four human evaluation criteria.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
Fluency Rewarder.,3.2 Single-Learner Reinforcement Learning,[0],[0]
We use a neural language model to measure fluency.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"Given a poem line Li, higher probability Plm(Li) indicates the line is more likely to exist in the corpus and thus may be more fluent and well-formed.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"However, it’s inadvisable to directly use Plm(Li) as the reward, since over high probability may damage diversity and innovation.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"We expect moderate probabilities which fall into a reasonable range, neither too high nor too low.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Therefore, we define the fluency reward of a poem P as:
r(Li) = max(|Plm(Li)− µ| − δ1 ∗ σ, 0), (7)
R1(P) = 1
n n∑ i=1 exp(−r(Li)), (8)
where µ and σ are the mean value and standard deviation of Plm calculated over all training sets.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"δ1 is a hyper-parameter to control the range.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
Coherence Rewarder.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"For poetry, good coherence means each line Li should be coherent with
previous lines in a poem.",3.2 Single-Learner Reinforcement Learning,[0],[0]
We use Mutual Information (MI) to measure the coherence of Li and L1:i−1.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"As shown in (Li et al., 2016a), MI of two sentences, S1 and S2, can be calculated by:
MI(S1, S2) = logP (S2|S1)− λlogP (S2), (9)
where λ is used to regulate the weight of generic sentences.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Based on this, we calculate the coherence reward as:
MI(L1:i−1, Li) = logPseq2seq(Li|L1:i−1)",3.2 Single-Learner Reinforcement Learning,[0],[0]
"− λlogPlm(Li), (10)
R2(P) = 1
n− 1 n∑ i=2",3.2 Single-Learner Reinforcement Learning,[0],[0]
"MI(L1:i−1, Li),
(11)
where Pseq2seq is a GRU-based sequence-tosequence model, which takes the concatenation of previous i-1 lines as input, and predicts Li.",3.2 Single-Learner Reinforcement Learning,[0],[0]
A better choice is to use a dynamic λ instead of a static one.,3.2 Single-Learner Reinforcement Learning,[0],[0]
Here we directly set λ = exp(−r(Li)),3.2 Single-Learner Reinforcement Learning,[0],[0]
"+ 1, which gives smaller weights to lines with extreme language model probabilities.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
Meaningfulness Rewarder.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"In dialogue generation task, neural models are prone to generate generic sentences such as “I don’t know” (Li et al., 2016a; Serban et al., 2016).",3.2 Single-Learner Reinforcement Learning,[0],[0]
We observed similar issues in poetry generation.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"The basic model tends to generate some common and meaningless words, such as bu zhi (don’t know), he chu (where), and wu ren (no one).",3.2 Single-Learner Reinforcement Learning,[0],[0]
"It’s quite intractable to quantify the meaningfulness of a whole poem, but we find that TF-IDF values of human-authored poems are significantly higher than values of generated ones (Figure 2).",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Consequently, we utilize TF-IDF to motivate the model to generate more meaningful words.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"This is a simple and rough attempt, but it makes generated poems more “meaningful” from the readers perspective.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Direct use of TF-IDF leads to serious out-ofvocabulary (OOV) problem and high variance, because we need to sample poems during the training process of DRL, which causes many OOV words.",3.2 Single-Learner Reinforcement Learning,[0],[0]
Therefore we use another neural network to smooth TF-IDF values.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"In detail, we have:
R3(P) = 1
n n∑ i=1",3.2 Single-Learner Reinforcement Learning,[0],[0]
"F (Li), (12)
where F (Li) is a neural network which takes a line as input and predicts its estimated TF-IDF
value.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"For each line in training sets, we calculate standard TF-IDF values of all words and use the average as the line TF-IDF value.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Then we use them to train F (Li) with Huber loss.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
Overall Quality Rewarder.,3.2 Single-Learner Reinforcement Learning,[0],[0]
The three kinds of rewards above are all based on line-level.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"In fact, human experts will also focus on discourselevel to judge the overall quality of a poem, ignoring some minor defects.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"We train a neural classifier to classify a given poem (in terms of the concatenation of all lines) into three classes: computer-generated poetry (class 1), ordinary human-authored poetry (class 2) and masterpiece (class 3).",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Then we get the reward by:
R4(P)",3.2 Single-Learner Reinforcement Learning,[0],[0]
"= 3∑
",3.2 Single-Learner Reinforcement Learning,[0],[0]
"k=1
Pcl(k|P) ∗ k. (13)
",3.2 Single-Learner Reinforcement Learning,[0],[0]
This classifier should be as reliable as possible.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"Due to the limited amount of masterpieces, normal classifiers don’t work well.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Therefore we use an adversarial training based classifier (Miyato et al., 2017), which achieves F-1 0.96, 0.73, 0.76 for the three classes respectively on the validation set.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Based on these rewarders, the total reward is:
R(P) = 4∑
j=1
αj ∗ R̃j(P), (14)
where αj is the weight and the symbol ˜ means the four rewards are re-scaled to the same magnitude.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"As (Gulcehre et al., 2018), we reduce the variance by:
R ′",3.2 Single-Learner Reinforcement Learning,[0],[0]
(P) =,3.2 Single-Learner Reinforcement Learning,[0],[0]
"R(P)− bu√
σ2u + ϵ −B(P), (15)
where bu and σu are running average and standard deviation of R respectively.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"B(P) is a neural network trained with Huber loss, which takes a poem as input and predicts its estimated reward.
",3.2 Single-Learner Reinforcement Learning,[0],[0]
DRL Process.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"For brevity, we use Pg(·|W; θ) to represent a basic generator and use REINFORCE algorithm (Williams, 1992) to optimize the model, which minimizes:
LDRL(θ) =",3.2 Single-Learner Reinforcement Learning,[0],[0]
"− M∑
m=1
EP∼Pg(·|Wm;θ)(R ′",3.2 Single-Learner Reinforcement Learning,[0],[0]
"(P)).
",3.2 Single-Learner Reinforcement Learning,[0],[0]
"(16)
Training with solely Eq.(16) is unstable.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Lacking of original MLE supervisory signals, the
Algorithm 1 Global Mutual Learning 1: Set history reward lists V1 and V2 empty; 2: for number of iterations do 3: Sample batch (Wm,Pmg ) from training
data set; 4: for eachWm do 5: Sample Pm1 ∼ Pg(·|W
m; θ1); 6: Sample Pm2 ∼ Pg(·|W
m; θ2); 7: Add R(Pm1 ) to V1, R(Pm2 ) to V2 8: end for 9: Set LM (θ1)=L(θ1), LM (θ2)=L(θ2);
10: if mean value V2 > V1 ∗",3.2 Single-Learner Reinforcement Learning,[0],[0]
"(1 + δ3) then 11: LM (θ1)=L(θ1) +KL(Pg(θ2)||Pg(θ1)); 12: else if V1 > V2 ∗ (1 + δ3) then 13: LM (θ2)=L(θ2) +KL(Pg(θ1)||Pg(θ2)); 14: end if 15: Update θ1 with LM (θ1), θ2 with LM (θ2); 16: end for
model is easy to get lost and totally ignore the corresponding topics specified by W , leading to explosive increase of MLE loss.",3.2 Single-Learner Reinforcement Learning,[0],[0]
We use two steps to alleviate this issue.,3.2 Single-Learner Reinforcement Learning,[0],[0]
"The first one is the Teacher Forcing (Li et al., 2017).",3.2 Single-Learner Reinforcement Learning,[0],[0]
"For each W , we estimate E(R′(P))",3.2 Single-Learner Reinforcement Learning,[0],[0]
"by ns sampled poems, as well as the ground-truth Pg whose reward is set to max(R ′",3.2 Single-Learner Reinforcement Learning,[0],[0]
"(Pg), 0).",3.2 Single-Learner Reinforcement Learning,[0],[0]
"The second step is to combine MLE loss and DRL loss as:
L(θ) = (1− β) ∗ LMLE(θ)",3.2 Single-Learner Reinforcement Learning,[0],[0]
"+ β ∗ L̃DRL(θ), (17)
where ˜ means the DRL loss is re-scaled to the same magnitude with MLE loss.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"Ultimately, we use Eq.(17) to fine-tune the basic model.",3.2 Single-Learner Reinforcement Learning,[0],[0]
"As discussed in Section 1 & 2, to further improve the performance, we mimic the mutual writing learning activity by simultaneously training two generators defined as Pg(θ1) and Pg(θ2).",3.3 Mutual Reinforcement Learning,[0],[0]
"The two learners (generators) learns not only from the teacher (rewarders) but also from each other.
",3.3 Mutual Reinforcement Learning,[0],[0]
"From the perspective of machine learning, one generator may not explore the policy space sufficiently and thus is easy to get stuck in the local minima.",3.3 Mutual Reinforcement Learning,[0],[0]
Two generators can explore along different directions.,3.3 Mutual Reinforcement Learning,[0],[0]
"Once one generator finds a better path (higher reward), it can communicate with the other and lead it towards this path.",3.3 Mutual Reinforcement Learning,[0],[0]
"This process could also be considered as the ensemble of different generators during the training phase.
",3.3 Mutual Reinforcement Learning,[0],[0]
"We implement the Mutual Reinforcement Learning (abbreviated as MRL) by two methods.
",3.3 Mutual Reinforcement Learning,[0],[0]
Local MRL.,3.3 Mutual Reinforcement Learning,[0],[0]
The first one is a simple instancebased method.,3.3 Mutual Reinforcement Learning,[0],[0]
"For the same input, suppose P1, P2 are generated by Pg(θ1) and Pg(θ2) respectively.",3.3 Mutual Reinforcement Learning,[0],[0]
"If R(P1) > R(P2)∗(1+δ2) and R̃j(P1) > R̃j(P2) for all j, then Pg(θ2) usesP1 instead ofP2 to update itself in Eq.(16) and vice versa.",3.3 Mutual Reinforcement Learning,[0],[0]
"That is, if a learner creates a significantly better poem, then the other learner will learn it.",3.3 Mutual Reinforcement Learning,[0],[0]
"This process gives a generator more high-reward instances and allows it to explore larger space along a more proper direction so as to escape from the local minima.
Global MRL.",3.3 Mutual Reinforcement Learning,[0],[0]
"During the training process, we need to sample poems from the generator, and hence local MRL may cause high variance.",3.3 Mutual Reinforcement Learning,[0],[0]
"Instead of an instance, mutual learning can also be applied on the distribution level.",3.3 Mutual Reinforcement Learning,[0],[0]
We can pull the distribution of a generator towards that of the other by minimizing KL divergence of them.,3.3 Mutual Reinforcement Learning,[0],[0]
We detail this method in algorithm 1.,3.3 Mutual Reinforcement Learning,[0],[0]
"The inner thought is that if learner 1 is generally better than learner 2, that is, during the creating history, learner 1 achieves higher average rewards, then learner 2 should directly learn from learner 1, rather than learn the poem itself.",3.3 Mutual Reinforcement Learning,[0],[0]
"This process allows the generator to learn from long-period history and focus on a higher level.
",3.3 Mutual Reinforcement Learning,[0],[0]
"In practice, we combine these two methods by simultaneously communicating high-reward samples and using KL loss, which leads to the best testing rewards (Table 1).",3.3 Mutual Reinforcement Learning,[0],[0]
"Our corpus consists of three sets: 117,392 Chinese quatrains (CQ), 10,000 Chinese regulated verses (CRV) and 10,000 Chinese iambics (CI).",4.1 Data and Setups,[0],[0]
"As men-
tioned, we experiment on the generation of quatrain which is the most popular genre of Chinese poetry and accounts for the largest part of our corpus.",4.1 Data and Setups,[0],[0]
"From the three sets, we randomly select 10% for validation.",4.1 Data and Setups,[0],[0]
"From CQ, we select another 10% for testing.",4.1 Data and Setups,[0],[0]
"The rest are used for training.
",4.1 Data and Setups,[0],[0]
"For our model and baseline models, we run TextRank (Mihalcea and Tarau, 2004) on all training sets and then extract four keywords from each quatrain.",4.1 Data and Setups,[0],[0]
"Then we build four < keyword(s), poem > pairs for each quatrain using 1 to 4 keywords respectively, so as to enable the model to cope with different numbers of keywords.
",4.1 Data and Setups,[0],[0]
"For the models and rewarders, the sizes of word embedding and hidden state are 256 and 512 respectively.",4.1 Data and Setups,[0],[0]
History vector size is 512 and convolution window size d = 3.,4.1 Data and Setups,[0],[0]
The word embedding is initialized with pre-trained word2vec vectors.,4.1 Data and Setups,[0],[0]
We use tanh as the activation function.,4.1 Data and Setups,[0],[0]
"For other more configurations of the basic model, we directly follow (Yan, 2016).
",4.1 Data and Setups,[0],[0]
Plm and Pseq2seq are trained with the three sets.,4.1 Data and Setups,[0],[0]
"We train F (Li) and B(P) with the CQ, CRV and 120,000 generated poems.",4.1 Data and Setups,[0],[0]
"There are 9,465 masterpieces in CQ.",4.1 Data and Setups,[0],[0]
"We use these poems, together with 10,000 generated poems and 10,000 ordinary human-authored poems to train the classifier Pcl.",4.1 Data and Setups,[0],[0]
"For training rewarders, half of the generated poems are sampled and the other half are generated with beam search (beam size 20).",4.1 Data and Setups,[0],[0]
"For testing, all models generate poems with beam search.
",4.1 Data and Setups,[0],[0]
"We use Adam (Kingma and Ba, 2015) with shuffled mini-batches.",4.1 Data and Setups,[0],[0]
The batch size is 64 for MLE and 32 for DRL.,4.1 Data and Setups,[0],[0]
"For DRL, we random select batches to fine-tune the basic model.",4.1 Data and Setups,[0],[0]
"We set δ1 = 0.5, δ2 = 0.1, δ3 = 0.001, α1 = 0.25,
α2 = 0.31, α3 = 0.14, α4 = 0.30, ns = 4, and β = 0.7.
",4.1 Data and Setups,[0],[0]
"A key point for MRL is to give the two pretrained generators some diversity, which can be achieved by using different model structures or parameters.",4.1 Data and Setups,[0],[0]
Here we simply initialize the generators differently and train one of them for more epoches.,4.1 Data and Setups,[0],[0]
"We compare MRL2 (our model, with both local and global mutual learning), GT (ground-truth, namely human-authored poems), Base (the basic model described in Section 3.1) and Mem (Zhang et al., 2017).",4.2 Models for Comparisons,[0],[0]
"The Mem model is the current stateof-the-art model for Chinese quatrain generation, which also achieves the best innovation so far.",4.2 Models for Comparisons,[0],[0]
"Some previous models (He et al., 2012; Zhang and Lapata, 2014; Yan, 2016) adopt BLEU and perplexity as automatic evaluation metrics.",4.3 Automatic Evaluation,[0],[0]
"Nevertheless, as discussed in Section 1, word-level likelihood or n-gram matching will greatly diverge from human evaluation manner.",4.3 Automatic Evaluation,[0],[0]
"Therefore we dispense with them and automatically evaluate generated poems as follows:
Rewarder Scores.",4.3 Automatic Evaluation,[0],[0]
The four rewarder scores are objective and model-irrelevant metrics which approximate corresponding human criteria.,4.3 Automatic Evaluation,[0],[0]
"They
2Due to length limit, we only display the better of the two simultaneously trained generators.",4.3 Automatic Evaluation,[0],[0]
"Our source code will be available at https://github.com/XiaoyuanYi/MRLPoetry.
can reflect poetry quality to some extent.",4.3 Automatic Evaluation,[0],[0]
"As shown in Table 1, on each criterion, GT gets much higher rewards than all these models.",4.3 Automatic Evaluation,[0],[0]
"Compared to Base, MRL gets closer to GT and achieves 31% improvement on the weighted average reward.",4.3 Automatic Evaluation,[0],[0]
Mem outperforms Base on the criteria except for meaningfulness (R̃3).,4.3 Automatic Evaluation,[0],[0]
"This is mainly because Mem generates more distinct words (Table 2), but these words tend to concentrate on the highfrequency area, resulting in unsatisfactory TF-IDF reward.",4.3 Automatic Evaluation,[0],[0]
We also test different strategies of MRL.,4.3 Automatic Evaluation,[0],[0]
"With naive single-learner RL, the improvement is limited, only 14%.",4.3 Automatic Evaluation,[0],[0]
"With mutual RL, the improvement increases to 27%.",4.3 Automatic Evaluation,[0],[0]
Combining local MRL and global MRL leads to another 4% improvement.,4.3 Automatic Evaluation,[0],[0]
"The results demonstrate our explicit optimization (RL) is more effective than the implicit ones and MRL gets higher scores than SRL.
Diversity and Innovation.",4.3 Automatic Evaluation,[0],[0]
Poetry is a kind of literature text with high requirements on diversity and innovation.,4.3 Automatic Evaluation,[0],[0]
Users don’t expect the machine to always generate monotonous poems.,4.3 Automatic Evaluation,[0],[0]
"We evaluate innovation of generated poems by distinct bigram ratio as (Li et al., 2016b).",4.3 Automatic Evaluation,[0],[0]
"More novel generated bigrams can somewhat reflect higher innova-
tion.",4.3 Automatic Evaluation,[0],[0]
The diversity is measured by bigram-based average Jaccard similarity of each two generated poems.,4.3 Automatic Evaluation,[0],[0]
"Intuitively, a basic requirement for innovation is that, with different inputs, the generated poems should be different from each other.
",4.3 Automatic Evaluation,[0],[0]
"As shown in Table 2, Mem gets the highest bigram ratio, close to GT, benefiting from its specially designed structure for innovation.",4.3 Automatic Evaluation,[0],[0]
"Our MRL achieves 43% improvement over Base, comparable to Mem.",4.3 Automatic Evaluation,[0],[0]
We will show later this satisfactory performance may lie in the incorporation of TFIDF (Figure 2).,4.3 Automatic Evaluation,[0],[0]
"On Jaccard, MRL gets the best result due to the utilization of MI.",4.3 Automatic Evaluation,[0],[0]
"MI brings richer context-related information which can enhance diversity as shown in (Li et al., 2016a).",4.3 Automatic Evaluation,[0],[0]
"In fact, human-authored poems often contain strong diversity of personal emotion and experience.",4.3 Automatic Evaluation,[0],[0]
"Therefore, despite prominent improvement, there is still a large gap between MRL and GT.
TF-IDF Distribution.",4.3 Automatic Evaluation,[0],[0]
"As mentioned, the basic model tends to generate common and meaningless words.",4.3 Automatic Evaluation,[0],[0]
"Consequently, we use TF-IDF as one of the rewards.",4.3 Automatic Evaluation,[0],[0]
Figure 2 shows the TF-IDF distributions.,4.3 Automatic Evaluation,[0],[0]
"As we can see, Base generates poems with lower TF-IDF compared to GT, while MRL pulls the distribution towards that of GT, making the model generate more meaningful words and hence benefiting innovation and diversity.
",4.3 Automatic Evaluation,[0],[0]
Topic Distribution.,4.3 Automatic Evaluation,[0],[0]
"We run LDA (Blei et al., 2003) with 20 topics on the whole corpus and then inference the topic of each generated poem.",4.3 Automatic Evaluation,[0],[0]
Figure 3 gives the topic distributions.,4.3 Automatic Evaluation,[0],[0]
"Poems generated by Base center in a few topics, which again demonstrates the claim: MLE-based models tend to remember the common patterns.",4.3 Automatic Evaluation,[0],[0]
"In contrast, humanauthored poems spread on more topics.",4.3 Automatic Evaluation,[0],[0]
"After finetuning by our MRL method, the topic distribution shows better diversity and balance.",4.3 Automatic Evaluation,[0],[0]
"From the testing set, we randomly select 80 sets of keywords to generate poems with these mod-
els.",4.4 Human Evaluation,[0],[0]
"For GT, we select poems containing the given words.",4.4 Human Evaluation,[0],[0]
"Therefore, we obtain 320 quatrains (80*4).",4.4 Human Evaluation,[0],[0]
"We invite 12 experts on Chinese poetry to evaluate these poems in terms of the four criteria: fluency, coherence, meaningfulness and overall quality and each needs to be scored in a 5-point scale ranging from 1 to 5.",4.4 Human Evaluation,[0],[0]
"Since it’s tiring to evaluate all poems for one person, we randomly divide the 12 experts into three groups.",4.4 Human Evaluation,[0],[0]
Each group evaluates the randomly shuffled 320 poems (80 for each expert).,4.4 Human Evaluation,[0],[0]
"Then for each model, each poem, we get 3 scores on each criterion and we use the average to alleviate individual preference.
",4.4 Human Evaluation,[0],[0]
Table 3 gives human evaluation results.,4.4 Human Evaluation,[0],[0]
MRL achieves better results than the other two models.,4.4 Human Evaluation,[0],[0]
"Since fluency is quite easy to be optimized, our method gets close to human-authored poems on Fluency.",4.4 Human Evaluation,[0],[0]
The biggest gap between MRL and GT lies on Meaning.,4.4 Human Evaluation,[0],[0]
"It’s a complex criterion involving the use of words, topic, emotion expression and so on.",4.4 Human Evaluation,[0],[0]
"The utilization of TF-IDF does ameliorate the use of words on diversity and innovation, hence improving Meaningfulness to some extent, but there are still lots to do.",4.4 Human Evaluation,[0],[0]
In this section we give more discussions.,4.5 Further Analyses and Discussions,[0],[0]
Learning Curve.,4.5 Further Analyses and Discussions,[0],[0]
We show the learning curves of SRL and MRL in Figure 4.,4.5 Further Analyses and Discussions,[0],[0]
"As we can see, for SRL, the adequately pre-trained generator 2 al-
ways gets higher rewards than the other one during the DRL training process.",4.5 Further Analyses and Discussions,[0],[0]
"With the increase of training steps, the gap between their rewards gets larger.",4.5 Further Analyses and Discussions,[0],[0]
"After several hundred steps, rewards of the two generators converge.
",4.5 Further Analyses and Discussions,[0],[0]
"For MRL, generator 2 gets higher rewards at the beginning, but it is exceeded by generator 1 since generator 1 learns from it and keeps chasing.",4.5 Further Analyses and Discussions,[0],[0]
"Finally, the two generators converge to higher rewards compared to SRL.
",4.5 Further Analyses and Discussions,[0],[0]
Case Study.,4.5 Further Analyses and Discussions,[0],[0]
We show some generated poems in Figure 5.,4.5 Further Analyses and Discussions,[0],[0]
"The Base model generates two words, ‘sunset’ and ‘moon’ in poem (1), which appear together and thus cause the conflict of time.",4.5 Further Analyses and Discussions,[0],[0]
The word ‘fishing jetty’ is confusing without any necessary explanation in the context.,4.5 Further Analyses and Discussions,[0],[0]
"In contrast, poem (2) describes a clearer scene and expresses some emotion: a lonely man takes a boat from morning till night and then falls asleep solitarily.
",4.5 Further Analyses and Discussions,[0],[0]
"In poem (3), Mem generates some meaningful words, such as ‘phoenix tree’, ‘wild goose’ and ‘friend’.",4.5 Further Analyses and Discussions,[0],[0]
"However, there isn’t any clue to link them together, resulting in poor coherence.",4.5 Further Analyses and Discussions,[0],[0]
"On the contrary, things in poem (4) are tightly connected.",4.5 Further Analyses and Discussions,[0],[0]
"For example, ‘moonlight’ is related to ‘night’; ‘rain’, ‘frost’ and ‘dew’ are connected with ‘cold’.
",4.5 Further Analyses and Discussions,[0],[0]
Poem (5) expresses almost nothing.,4.5 Further Analyses and Discussions,[0],[0]
The first two lines seem to talk about the change of time.,4.5 Further Analyses and Discussions,[0],[0]
But the last two lines are almost unrelated to ‘time change’.,4.5 Further Analyses and Discussions,[0],[0]
"Poem (6) talks about an old poet, with the description of cheap wine, poem and dream, expressing something about life and time.",4.5 Further Analyses and Discussions,[0],[0]
"However, the human-authored poem (7) does much better.",4.5 Further Analyses and Discussions,[0],[0]
"It seems to describe a mosquito, but in fact, it’s a metaphor of the author himself.",4.5 Further Analyses and Discussions,[0],[0]
"In this work, we address two substantial problems in automatic poetry generation: lack of diversity, and loss-evaluation mismatch, which are caused by MLE-based neural models.",5 Conclusion and Future Work,[0],[0]
"To this end, we directly model the four widely used human evaluation criteria and design corresponding automatic rewarders.",5 Conclusion and Future Work,[0],[0]
We use these explicit rewards to guide gradient update by reinforcement learning.,5 Conclusion and Future Work,[0],[0]
"Furthermore, inspired by writing theories, we propose a novel mutual learning schema to further improve the performance.",5 Conclusion and Future Work,[0],[0]
"Mimicking the poetry learning activity, we simultaneously train two generators, which will not only be taught by the rewarders but also learn from each other.",5 Conclusion and Future Work,[0],[0]
"Experi-
",5 Conclusion and Future Work,[0],[0]
"mental results show our method achieves significant improvement both on automatic rewards and human evaluation scores, outperforming the current state-of-the-art model3.
",5 Conclusion and Future Work,[0],[0]
There are still lots to do.,5 Conclusion and Future Work,[0],[0]
Can we better model the meaningfulness of a whole poem?,5 Conclusion and Future Work,[0],[0]
"Can we quantify some other intractable criteria, e.g, poeticness?",5 Conclusion and Future Work,[0],[0]
"Besides, we only tried two learners in this work.",5 Conclusion and Future Work,[0],[0]
Would the collaboration of more learners lead to better results?,5 Conclusion and Future Work,[0],[0]
How to design the methods of communication among many generators?,5 Conclusion and Future Work,[0],[0]
We will explore these questions in the future.,5 Conclusion and Future Work,[0],[0]
"We would like to thank Cheng Yang, Jiannan Liang, Zhipeng Guo, Huimin Chen and anonymous reviewers for their insightful comments.",Acknowledgments,[0],[0]
This research is funded by the National 973 project (No. 2014CB340501).,Acknowledgments,[0],[0]
"It is also partially supported by the NExT++ project, the National Research Foundation, Prime Ministers Office, Singapore under its IRC@Singapore Funding Initiative.
3Our method will be incorporated into Jiuge, the THUNLP online poetry generation system, https:// jiuge.thunlp.cn.",Acknowledgments,[0],[0]
Poetry is one of the most beautiful forms of human language art.,abstractText,[0],[0]
"As a crucial step towards computer creativity, automatic poetry generation has drawn researchers’ attention for decades.",abstractText,[0],[0]
"In recent years, some neural models have made remarkable progress in this task.",abstractText,[0],[0]
"However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in lossevaluation mismatch.",abstractText,[0],[0]
"Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood.",abstractText,[0],[0]
"To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores.",abstractText,[0],[0]
"Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema.",abstractText,[0],[0]
We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance.,abstractText,[0],[0]
We experiment on Chinese poetry.,abstractText,[0],[0]
"Based on a strong basic model, our method achieves better results and outperforms the current state-of-theart method.",abstractText,[0],[0]
Automatic Poetry Generation with Mutual Reinforcement Learning,title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 381–392, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics
In this work, we focus on automatically recognizing social conversational strategies that in human conversation contribute to building, maintaining or sometimes destroying a budding relationship. These conversational strategies include self-disclosure, reference to shared experience, praise and violation of social norms. By including rich contextual features drawn from verbal, visual and vocal modalities of the speaker and interlocutor in the current and previous turn, we can successfully recognize these dialog phenomena with an accuracy of over 80% and kappa ranging from 60-80%. Our findings have been successfully integrated into an end-to-end socially aware dialog system, with implications for virtual agents that can use rapport between user and system to improve task-oriented assistance.",text,[0],[0]
"People pursue multiple conversational goals in dialog (Tracy and Coupland, 1990).",1 Introduction and Motivation,[0],[0]
"Contributions to a conversation can be divided into those that fulfill propositional functions, contributing informational content to the dialog; those that fulfill interactional functions, managing the conversational interaction; and those that fulfill interpersonal functions, managing the relationship between the interlocutors (Cassell and Bickmore, 2003; Fetzer, 2013).",1 Introduction and Motivation,[0],[0]
"In the category of talk that fulfills interpersonal goals are conversational strategies - units of discourse that are larger than speech acts (in fact, a single conversational strategy can span more than one turn in conversation), and that can achieve social goals.
",1 Introduction and Motivation,[0],[0]
"In this paper, we propose a technique to automatically recognize conversational strategies.",1 Introduction and Motivation,[0],[0]
"We demonstrate that these conversational strategies are most effectively recognized when verbal (linguistic), visual (nonverbal) and vocal (acoustic) features are all taken into account (and, in a demo paper published in this volume, we demonstrate that the results here can be effectively integrated into an end-to-end socially-aware dialog system).
",1 Introduction and Motivation,[0],[0]
"As naturalistic interactions with dialog systems increasingly become a part of people’s daily lives, it is important for these systems to advance their capabilities of not only conveying information and achieving smooth interaction, but also managing long-term relationships with people by building intimacy (Pecune et al., 2013) and rapport (Zhao et al., 2014), not just for the sake of companionship, but as an intrinsic part of successfully fulfilling collaborative tasks.
",1 Introduction and Motivation,[0],[0]
"Rapport, or the feeling of harmony and connection with another, is an important aspect of human interaction, with powerful effects in domains such as education (Ogan et al., 2012; Sinha and Cassell, 2015a; Sinha and Cassell, 2015b) and negotiation (Drolet and Morris, 2000).",1 Introduction and Motivation,[0],[0]
The central theme of our work is to develop a dialog system that can facilitate such interpersonal rapport with users over interactions in time.,1 Introduction and Motivation,[0],[0]
"Taking a step towards this goal, our prior work (Zhao et al., 2014) has developed a dyadic computational model that explains how interlocutors manage rapport through use of specific conversational strategies to fulfill the intermediate goals that lead to rapport - face management, mutual attentiveness, and coordination.
",1 Introduction and Motivation,[0],[0]
"Foundational work by (Spencer-Oatey, 2008) conceptualizes the interpersonal nature of face as a desire to be recognized for one’s social value and individual positive traits.",1 Introduction and Motivation,[0],[0]
"Face-boosting strategies such as praise serve to create increased selfesteem in the individual and increased interper-
381
sonal cohesiveness or rapport in the dyad (Zhao et al., 2014).",1 Introduction and Motivation,[0],[0]
"(Spencer-Oatey, 2008) also posits that over time, interlocutors intend to increase coordination by adhering to behavior expectations, which are guided by sociocultural norms in the initial stages of interaction and by interpersonally determined norms afterwards.",1 Introduction and Motivation,[0],[0]
"In these later stages, general norms may be purposely violated to accommodate the other’s behavioral expectations.
",1 Introduction and Motivation,[0],[0]
"Meanwhile, in the increasing trajectory of interpersonal closeness, referring to shared experience allows interlocutors to increase coordination by indexing common history and differentiating in-group and out-group individuals (Tajfel and Turner, 1979) (cementing the sense that the two are part of a group in ways that similar phenomena such as ”referring to shared interests” do not appear to).",1 Introduction and Motivation,[0],[0]
"To better learn about the other person mutual attentiveness plays an important role (TickleDegnen and Rosenthal, 1990).",1 Introduction and Motivation,[0],[0]
We have seen in our own corpora that mutual attentiveness is fulfilled by leading one’s interlocutors to provide information about themselves through the strategy of eliciting self-disclosure.,1 Introduction and Motivation,[0],[0]
"As the relationship proceeds and social distance decreases, these selfdisclosures become more intimate in nature.
",1 Introduction and Motivation,[0],[0]
"Motivated by this theoretical rationale and our prior empirical findings concerning the relationship between these conversational strategies and rapport (Sinha et al., 2015), in the current work, our goals are twofold:",1 Introduction and Motivation,[0],[0]
"Our theoretical question is to understand the nature of conversational strategies in greater detail, by correlating them with associated observable verbal, vocal and visual cues (section 5).",1 Introduction and Motivation,[0],[0]
"Our methodological question is then to use this understanding to automatically recognize these conversational strategies by leveraging statistical machine learning techniques (section 6).
",1 Introduction and Motivation,[0],[0]
We believe that the answers to these questions can contribute important insights into the nature of human dialog.,1 Introduction and Motivation,[0],[0]
"By the same token, we believe this work to be crucial if we wish to develop a socially-aware dialog system that can identify conversational strategy usage in real-time, assess its impact on rapport, and then produce an appropriate next conversational strategy as a follow-up to maintain or increase rapport in the service of improving the system’s ability to support the user’s goals.",1 Introduction and Motivation,[0],[0]
"(Papangelis et al., 2014).",1 Introduction and Motivation,[0],[0]
Below we describe related work that focuses on computational modeling of social conversational phenomena.,2 Related Work,[0],[0]
"For instance, (Wang et al., 2016) developed a model to measure self-disclosure in social networking sites by deploying emotional valence, social distance between the poster and other people and linguistic features such as those identified by the Linguistic Inquiry and Word Count program (LIWC) etc.",2 Related Work,[0],[0]
"While the features used here are quite interesting, this study relied only on the verbal aspects of talk, while we also include vocal and visual features.
",2 Related Work,[0],[0]
"Interesting prior work on quantifying social norm violation has taken a heavily data-driven focus (Danescu-Niculescu-Mizil et al., 2013b; Wang et al., 2016).",2 Related Work,[0],[0]
"For instance, (Danescu-NiculescuMizil et al., 2013b) trained a series of bigram language models to quantify the violation of social norms in users’ posts on an online community by leveraging cross-entropy value, or the deviation of word sequences predicted by the language model and their usage by the user.",2 Related Work,[0],[0]
"Another kind of social norm violation was examined by (Riloff et al., 2013), who developed a classifier to identify a specific type of sarcasm in tweets.",2 Related Work,[0],[0]
"They utilized a bootstrapping algorithm to automatically extract lists of positive sentiment phrases and negative situation phrases from given sarcastic tweets, which were in turn leveraged to recognize sarcasm in an SVM classifier.",2 Related Work,[0],[0]
"Experimental results showed the adequacy of their approach.
",2 Related Work,[0],[0]
"(Wang et al., 2012) investigated the different social functions of language as used by friends or strangers in teen peer-tutoring dialogs.",2 Related Work,[0],[0]
This work was able to successfully predict impoliteness and positivity in the next turn of the dialog.,2 Related Work,[0],[0]
Their success with both annotated and automatically extracted features suggests that a dialog system will be able to employ similar analyses to signal relationships with users.,2 Related Work,[0],[0]
"Other work, such as (Danescu-Niculescu-Mizil et al., 2013a) has developed computational frameworks to automatically classify requests along a scale of politeness.",2 Related Work,[0],[0]
"Politeness strategies such as requests, gratitude and greetings, as well as their specialized lexicons, were used as features to train a classifier.
",2 Related Work,[0],[0]
"In terms of hedges or indirect language, (Prokofieva and Hirschberg, 2014) proposed a preliminary approach to automatic detection, relying on a simple lexical-based search.",2 Related Work,[0],[0]
"Machine learn-
ing methods that go beyond keyword searches are a promising extension, as they may be able to better capture language used to hedge as a function of contextual usage.
",2 Related Work,[0],[0]
"However, a common limitation of the above work is its focus on only the verbal modality, while studies have shown conversational strategies to be associated with specific kinds of nonverbal behaviors.",2 Related Work,[0],[0]
"For instance, (Kang et al., 2012) discovered that head tilts and pauses were the strongest nonverbal cues to interpersonal intimacy.",2 Related Work,[0],[0]
"Unfortunately, here too only one modality was examined.",2 Related Work,[0],[0]
"While nonverbal behavioral correlates to intimacy in self-disclosure were modeled, the verbal and vocal modalities of the conversation was ignored.",2 Related Work,[0],[0]
"Computational work has also modeled rapport using only nonverbal information (Huang et al., 2011).",2 Related Work,[0],[0]
"In what follows we describe our approach to modeling social conversational phenomena, which relies on verbal, visual and vocal content to automatically recognize conversational strategies.",2 Related Work,[0],[0]
"Our models are trained on a peer tutoring corpus, which gives us the opportunity to look at conversational strategies as they are used in both a task and social context.",2 Related Work,[0],[0]
"Reciprocal peer tutoring data was collected from 12 American English-speaking dyads (6 friends and 6 strangers; 6 boys and 6 girls), with a mean age of 13 years, who interacted for 5 hourly sessions over as many weeks (a total of 60 sessions, and 5400 minutes of data), tutoring one another in algebra (Yu et al., 2013).",3 Study Context,[0],[0]
"Each session began with a period of getting to know one another, after which the first tutoring period started, followed by another small social interlude, a second tutoring period with role reversal between the tutor and tutee, and then the final social time.
",3 Study Context,[0],[0]
"Prior work demonstrates that peer tutoring is an effective paradigm that results in student learning (Sharpley et al., 1983), making this an effective context to study dyadic interaction with a concrete task outcome.",3 Study Context,[0],[0]
"Our student-student data, in addition, demonstrates that a tremendous amount of rapport-building takes place during the task of reciprocal tutoring (Sinha and Cassell, 2015b).",3 Study Context,[0],[0]
"We assessed our automatic recognition of conversational strategies against this corpus annotated
for those strategies (as well as other educational tutoring phenomena not discussed here).",4 Ground Truth,[0],[0]
"Interrater reliability (IRR) for the conversational strategy annotations, computed via Krippendorff’s alpha, was 0.75 for self-disclosure, 0.79 for reference to shared experience, 1.0 for praise and 0.75 for social norm violation.",4 Ground Truth,[0],[0]
"IRR for visual behavior was 0.89 for eye gaze, 0.75 for smile count (how many smiles occur), 0.64 for smile duration and 0.99 for head nod.",4 Ground Truth,[0],[0]
Below we discuss the definitions of each conversational strategy and nonverbal behavior that was annotated.,4 Ground Truth,[0],[0]
Self-Disclosure (SD): Self-disclosure refers to the conversational act of revealing aspects of oneself (personal private information) that otherwise would not be seen or known by the person being disclosed to (or would be difficult to see or know).,4.1 Coding Conversational Strategies,[0],[0]
"A lot of psychological literature talks about the ways people reveal facts about themselves as ways of building relationships, but we are the first to look at the role of self-disclosure during social and task interactions by the same dyad, particularly for adolescents engaged in reciprocal peer tutoring.",4.1 Coding Conversational Strategies,[0],[0]
"We coded for two sub-categories: (1) revealing the long-term aspects of oneself that one may feel are deep and true (e.g, “I love my pets”), (2) revealing one’s transgressive (forbidden or sociallyunacceptable) behaviors or actions, which may be a way of attempting to make the interlocutor feel better by disclosing one’s flaws (e.g, “I suck at linear equations”).
",4.1 Coding Conversational Strategies,[0],[0]
Referring to Shared Experience (SE): We differentiate between shared experience - an experience that the two interlocutors engage in or share with one another at the same time (such as ”that facebook post Cecily posted last week was wild!”) - from shared interests (such as ”you like Xbox games too?).,4.1 Coding Conversational Strategies,[0],[0]
"Shared experiences may index a shared community membership (even if a community of two), which can in turn build rapport.",4.1 Coding Conversational Strategies,[0],[0]
"We coded for shared experiences (e.g, going to the mall together last week).
",4.1 Coding Conversational Strategies,[0],[0]
"Praise (PR): We annotated both labeled praise (an expression of a positive evaluation of a specific attribute, behavior or product of the other; e.g, “great job with those negative numbers”), and unlabeled praise (a generic expression of positive evaluation, without a specific target;e.g, “Perfect”).
",4.1 Coding Conversational Strategies,[0],[0]
Violation of Social Norms (VSN): Social norm violations are behaviors or actions that go against general socially acceptable and stereotypical behaviors.,4.1 Coding Conversational Strategies,[0],[0]
"In a first pass, we coded whether an utterance was a social norm violation.",4.1 Coding Conversational Strategies,[0],[0]
"In a second pass, if a social norm violation, we differentiated: (1) breaking the conversational rules of the experiment (e.g. off-task talk during tutoring session, insulting the experimenter or the experiment, etc); (2) face threatening acts (e.g. criticizing, teasing, or insulting, etc); (3) referring to one’s own or the other person’s social norm violations or general social norm violations (e.g. referring to the need to get back to focusing on work, or to the other person being verbally annoying etc).",4.1 Coding Conversational Strategies,[0],[0]
"Social norms are culturally-specific, and so we judged a social norm violation by the impact it had on the listener (e.g. shock, specific reference to the behavior as a violation, etc.).",4.1 Coding Conversational Strategies,[0],[0]
"Social norm violations may signal that a dyad is becoming closer, and no longer feels the need to adhere to the norms of the larger community.",4.1 Coding Conversational Strategies,[0],[0]
Eye Gaze: Gaze for each participant was annotated individually.,4.2 Coding Visual Behaviors,[0],[0]
Front facing video for the individual participant was supplemented with a side camera view when needed.,4.2 Coding Visual Behaviors,[0],[0]
Audio was turned off so that words didn’t influence the annotation.,4.2 Coding Visual Behaviors,[0],[0]
"We coded (1) Gaze at the partner (gP), (2) Gaze at one’s own worksheet (gO), (3) Gaze at partner’s worksheet (gN), (4) Gaze elsewhere (gE).
",4.2 Coding Visual Behaviors,[0],[0]
Smile: A smile is defined by the elongation of the participant’s lips and rising of their cheeks (smiles will often be asymmetric).,4.2 Coding Visual Behaviors,[0],[0]
It is often accompanied by creases at the corner of the eyes.,4.2 Coding Visual Behaviors,[0],[0]
"Smiles have three parameters: rise, sustain, and decay (Hoque et al., 2011).",4.2 Coding Visual Behaviors,[0],[0]
"We annotated a smile from the beginning of the rise to the end of the decay.
",4.2 Coding Visual Behaviors,[0],[0]
Head Nod: We coded temporal intervals of head nod rather than individual nod - the beginning of the head moving up and down until the moment the head came to rest.,4.2 Coding Visual Behaviors,[0],[0]
"Our first objective, then, was to understand the nature of different conversational strategies (discussed in section 4) in greater detail.",5 Understanding Conversational Strategies,[0],[0]
"Towards this end, we first under-sampled the non-annotated
examples of self disclosure, shared experience, praise and social norm violation in order to create a balanced dataset of utterances.",5 Understanding Conversational Strategies,[0],[0]
The utterances chosen to reflect the non-annotated cases were randomly selected.,5 Understanding Conversational Strategies,[0],[0]
"We made sure to have a similar average utterance length for all annotated and nonannotated cases, to prevent conflation of results due to lower or higher opportunities for detection of multimodal features.",5 Understanding Conversational Strategies,[0],[0]
"The final corpus (selected from 60 interaction sessions) comprised of 1014 self disclosure and 1014 non-self disclosure, 184 shared experience and 184 non-shared experience, 167 praise and 167 non-praise, 7470 social norm violation and 7470 non-social norm violation.
",5 Understanding Conversational Strategies,[0],[0]
"Second, we explored observable verbal and vocal behaviors of interest that could potentially be associated with different conversational strategies, assessing whether the mean value of these features were significantly higher in utterances with a particular conversational strategy label than in ones with no label (two-tailed correlated samples t-test).",5 Understanding Conversational Strategies,[0],[0]
"Bonferroni correction was used to correct the p-values with respect to the number of features, because of multiple comparisons involved.",5 Understanding Conversational Strategies,[0],[0]
"Finally, for all significant results (p <0.05), we also calculated effect size via Cohen’s d to test for generalizability of results.
",5 Understanding Conversational Strategies,[0],[0]
"Third, for visual behaviors like smile, eye gaze, head nod, we binarized these features by denoting their presence (1) or absence (0) in one clause.",5 Understanding Conversational Strategies,[0],[0]
"If an individual shifts gaze during a particular spoke conversational strategy, we might have multiple types of eye gaze represented.",5 Understanding Conversational Strategies,[0],[0]
We performed 2 test to see whether the appearance of visual annotations were independent of whether the utterance belonged to a particular conversational strategy or not.,5 Understanding Conversational Strategies,[0],[0]
"For all significant 2 test statistics, odds ratio (o) was computed to explore co-occurrence likelihood.",5 Understanding Conversational Strategies,[0],[0]
"Majority of the features discussed in the subsequent sub-sections were drawn from qualitative observations and note-taking, during and after the formulation of our coding manuals.",5 Understanding Conversational Strategies,[0],[0]
"We used Linguistic Inquiry and Word Count (LIWC 2015) (Pennebaker et al., 2015) to quantify verbal cues of interest that were semantically associated with a broad range of psychological constructs and could be useful in distinguishing conversational strategies.",5.1 Verbal,[0],[0]
"The input to LIWC were conversational transcripts that had been tran-
scribed and segmented into syntactic clauses.
",5.1 Verbal,[0],[0]
"Self-disclosure: We observed personal concerns of students (sum of words identified as belonging to categories of work, leisure, home, money, religion and death etc) to be significantly higher, than in non self-disclosure utterances with a moderate effect size (d=0.44), signaling that students referred significantly more to their personal concerns during self-disclosure.",5.1 Verbal,[0],[0]
"Next, due to the fact that self-disclosures are often likely to comprise of emotional expressions when revealing one’s likes and dislikes (Sparrevohn and Rapee, 2009), we used the LIWC dictionary to capture words representative of negative emotions (d=0.32) and positive emotion words (d=0.18).",5.1 Verbal,[0],[0]
"Also, to formalize the intuition that when people reveal themselves in an authentic or honest way, they are more personal, humble, and vulnerable, the standardized LIWC summary variable of Authenticity (d=1.16) was taken into account.",5.1 Verbal,[0],[0]
"Finally, as expected, we found self-disclosure utterances had significantly higher usage of first person singular pronouns (d=1.62).
",5.1 Verbal,[0],[0]
"Reference to shared experience: We looked at three LIWC categories: (1) Affiliation drive, which comprises words signaling a need to affiliate such as ally, friend, social etc (d=0.92), (2) Time Orientation words, which capture past (mostly in ROE) , present (mostly in RIE) and future focus and comprises words such as ago, did, talked, today, is, now, may, will, soon etc (d=0.95).",5.1 Verbal,[0],[0]
"Such words are not only used by interlocutors to index commonality within a time frame (Enfield, 2013), but also to signal an increased need for affiliation with the conversational partner, perhaps to indicate common ground(Clark, 1996), (3) First person plural such as we, us, our etc.",5.1 Verbal,[0],[0]
"In line with expectations, this feature had high effect size (d=0.93), since interlocutors focused on both themselves and the conversational partner.
",5.1 Verbal,[0],[0]
"Praise: We looked at positive emotions (d=2.55), since praise is one form of verbal persuasion that increases the interlocutor’s confidence and boosts self efficacy (Bandura, 1994).",5.1 Verbal,[0],[0]
Most of the praise utterances in our dataset were not very specific or directed at the tutee’s performance or effort.,5.1 Verbal,[0],[0]
"Also, the LIWC standardized summary variable of Emotional Tone from LIWC was considered for the sake of completeness, which puts positive emotion and negative emotion dimensions into a single summary variable, such
that the higher the number, the more positive the tone (d=3.56).
",5.1 Verbal,[0],[0]
"Social norm violation: We looked at different categories of off-task talk from LIWC, such as social processes comprising words related to friends, family, male and female references (d=0.78), biological processes comprising words belonging to the categories of body, health etc (d=0.30) and personal concerns (d=0.24).",5.1 Verbal,[0],[0]
The effect sizes across these categories ranged from moderate to low.,5.1 Verbal,[0],[0]
"Next, we looked at usage of swearing words like fuck, damn, shit etc and found low effect size (d=0.13) for this category in utterances of social norm violation.",5.1 Verbal,[0],[0]
"For the LIWC category of anger (words such as hate, annoyed etc), the effect size was moderate (d=0.27).
",5.1 Verbal,[0],[0]
"In our qualitative analysis of social norm violation utterances, we had discovered interactions of students to be reflective of need for power, meaning attention to or awareness of relative status in a social setting (perhaps this could be a result of putting one student in the tutor role).",5.1 Verbal,[0],[0]
We formalized this intuition from the LIWC category of power drive that comprises words such as superior etc (d=0.18).,5.1 Verbal,[0],[0]
"Finally, based on prior work (Kacewicz et al., 2009) that found increased use of first-person plural to be a good predictor of higher status, and increased use of first-person singular to be a good predictor of lower status, we posited that when students violated social norms, they were more likely to freely make statements that involved others.",5.1 Verbal,[0],[0]
"However, the effect size for first-person plural usage in utterances of social norm violation was negligible (d=0.07).",5.1 Verbal,[0],[0]
Table 2 in the appendix provides complete set of results.,5.1 Verbal,[0],[0]
"In our qualitative observations, we noticed the variations of both pitch and loudness when interlocutors used different conversational strategies.",5.2 Vocal,[0],[0]
We were thus motivated to explore the mean difference of those low-level vocal descriptors as differentiators among the different conversational strategies.,5.2 Vocal,[0],[0]
"By using Open Smile (Eyben et al., 2010), we extracted two sets of basic features - for loudness features, pcm-loudness and its delta coefficient were tested; for pitch-based features, jitterLocal, jitterDDP, shimmerLocal, F0final and also their delta coefficients were tested.",5.2 Vocal,[0],[0]
pcmloudness represents the loudness as the normalised intensity raised to a power of 0.3.,5.2 Vocal,[0],[0]
"F0final is the
smoothed fundamental frequency contour.",5.2 Vocal,[0],[0]
JitterLocal is the frame-to-frame pitch period length deviations.,5.2 Vocal,[0],[0]
JitterDDP is the differential frame-toframe jitter.,5.2 Vocal,[0],[0]
"ShimmerLocal is the frame-to-frame amplitude deviations between pitch periods.
",5.2 Vocal,[0],[0]
Self-disclosure: We found a moderate effect size for pcm-loudness-sma-amean (d=0.26).,5.2 Vocal,[0],[0]
"Despite often becoming excited when disclosing things that they loved or liked, sometimes students also seemed to hesitate and spoke at a lower pitch when they revealed a transgressive act.",5.2 Vocal,[0],[0]
"However, the effect size for pitch was negligible.",5.2 Vocal,[0],[0]
"One potential reason for our results not aligning with hypothesis could be consideration of utterances with annotations of enduring states as well as transgressive acts together.
",5.2 Vocal,[0],[0]
"Reference to shared experience: We found a moderate negative effect size for the shimmerLocal-sma-amean (d=-0.32).
",5.2 Vocal,[0],[0]
"Praise: We found negative effect size for loudness (d=-0.51), meaning the speakers spoke in a lower voice when praising the interlocutor (mostly the tutee).",5.2 Vocal,[0],[0]
"We also found positive and moderate effect sizes for jitterLocal-sma-amean (d=0.45) and shimmerLocal-sma-amean (d=0.39).
",5.2 Vocal,[0],[0]
"Social norm violation: We found high effect sizes for pcm-loudness-sma-amean (d=0.72) and F0final-sma-amean (d=0.61) and interestingly, negative effect sizes for jitter (d=-0.09) and shimmer (d=-0.16).",5.2 Vocal,[0],[0]
"One potential reason could be that when student violate social norms, their behaviors are likely to become outliers compared to their normative behaviors.",5.2 Vocal,[0],[0]
"In fact, we noticed usage of “joking” tone of voice (Norrick, 2003) and pitch different than usual, to signal a social norm violation.",5.2 Vocal,[0],[0]
"When the content of the utterance was unaccepted by the social norms, students also tried to lower down their voice, which could be a way of hedging these violations.",5.2 Vocal,[0],[0]
Table 2 in the appendix provides complete set of results.,5.2 Vocal,[0],[0]
Computing the odds ratio o involved comparing the odds of occurrence of a non-verbal behavior for a pair of categories of a second variable (whether an utterance was a specific conversational strategy or not).,5.3 Visual,[0],[0]
"Overall, we found that that smile and gaze were significantly more likely to occur in utterances of self-disclosure (o(Smile)=1.67, o(gP)=2.39, o(gN)=0.498, o(gO)=0.29, o(gE)=2.8) compared to a non self-disclosure utterance.",5.3 Visual,[0],[0]
"A similar
trend was observed for reference to shared experience (o(Smile)=1.75, o(gP)=3.02, o(gN)=0.58, o(gO)=0.31, o(gE)=4.19) and social norm violation (o(Smile)=3.35, o(gP)=2.75, o(gN)=0.8, o(gO)=0.47, o(gE)=1.67) utterances, compared to utterances that did not belong to these categories.
",5.3 Visual,[0],[0]
"The high odds ratio for gP in these results suggests that an interlocutor was likely to gaze at their partner when using specific conversational strategies, signaling attention towards the interlocutor.",5.3 Visual,[0],[0]
The extremely high odds ratio for smiling behaviors during a social norm violation is also interesting.,5.3 Visual,[0],[0]
"However, for praise utterances, we did not find all kinds of gaze and smile to be more likely to occur than non-praise utterances.",5.3 Visual,[0],[0]
Only gazing at partner (o(gP)=0.44) or their worksheet (o(gN)=4.29) or gazing elsewhere (o(gE)=0.30) were among the non-verbals that were significantly greatly present in praise utterances.,5.3 Visual,[0],[0]
Table 3 in the appendix provides complete set of results for the speaker (as discussed above) and also for the listener.,5.3 Visual,[0],[0]
"In this section, our objective was to build a computational model for conversational strategy recognition.",6 Machine Learning Modeling,[0],[0]
"Towards this end, we first took each clause, or the smallest units that can express a complete proposition, as the prediction unit.",6 Machine Learning Modeling,[0],[0]
"Next, three sets of features were used as input.",6 Machine Learning Modeling,[0],[0]
"The first set f1 comprised verbal (LIWC), vocal and visual features of the speaker, informed from the qualitative and quantitative analysis as discussed above.",6 Machine Learning Modeling,[0],[0]
"While LIWC features helped in categorization of words used during usage of a particular conversational strategy, they did not capture contextual usage of words within the utterance.",6 Machine Learning Modeling,[0],[0]
"Thus, we also added bigrams, part of speech bigrams and wordpart of speech pairs from the speaker’s utterance.
",6 Machine Learning Modeling,[0],[0]
"In addition to the speaker’s behavior, we also added two sets of interlocutor behavior to capture the context around usage of a conversational strategy.",6 Machine Learning Modeling,[0],[0]
The feature set f2 comprised visual behaviors of the interlocutor (listener) in the current turn.,6 Machine Learning Modeling,[0],[0]
"The feature set f3 comprised verbal (bigrams, part of speech bigrams and word-part of speech pairs), vocal and visual features of the interlocutor in the previous turn.
",6 Machine Learning Modeling,[0],[0]
"Finally, early fusion was applied on these multimodal features (by concatenation) and L2 regularized logistic regression with 10-fold cross val-
idation was used as the machine learning algorithm, with rare threshold for feature extraction being set to 10 and performance evaluated using accuracy and kappa1 measures.",6 Machine Learning Modeling,[0],[0]
"The following table shows our comparison with other standard machine learning algorithms such as Support Vector Machine (SVM) and Naive Bayes (NB), where we found Logistic Regression (LR) to perform better in recognition of the four conversational strategies.",6 Machine Learning Modeling,[0],[0]
"In next sub-section, we therefore denote the feature weights derived from logistic regression in brackets to offer interpretability of results.",6 Machine Learning Modeling,[0],[0]
Self-Disclosure: We could successfully identify self-disclosure from non self-disclosure utterances with an accuracy of 85% and a kappa of 70%.,6.1 Results and Discussion,[0],[0]
"The top features from feature set f1 predictive of speakers disclosing themselves included gazing at partner (0.44), head nodding (0.24) and not gazing at their own worksheet (-0.60) or the interlocutor’s worksheet (-0.21).",6.1 Results and Discussion,[0],[0]
"Head nod is a way to emphasize what one is saying (Poggi et al., 2010), while gazing at the partner signals one’s attention.",6.1 Results and Discussion,[0],[0]
Higher usage of first person singular by the speaker (0.04) was also positively predictive of self-disclosure in the utterance.,6.1 Results and Discussion,[0],[0]
"The top features from feature set f2 predictive of speakers disclosing included listener behaviors such as head nodding (0.3) to communicate their attention (Schegloff, 1982), gazing elsewhere (0.12) or at the speaker (0.09) instead of gazing at their own worksheet (-0.89) or the speaker’s worksheet (- 0.27).",6.1 Results and Discussion,[0],[0]
"The top features from feature set f3 predictive of speakers disclosing included no smiling
1The discriminative ability over chance of a predictive model, for the target annotation, or the accuracy adjusted for chance
(-0.30),no head nodding (-0.15) and lower loudness in voice (-0.11) from the interlocutor in the last turn.
",6.1 Results and Discussion,[0],[0]
Reference to shared experience: We achieved an accuracy of 84% and kappa of 67% for prediction.,6.1 Results and Discussion,[0],[0]
"The top features from feature set f1 predictive of speakers referring to shared experience included not gazing at own worksheet (-0.66), partner’s worksheet (-0.40) or at the partner (-0.22), no smiling (-0.18) and having lower shimmer in voice (-0.26).",6.1 Results and Discussion,[0],[0]
"Instead, words signaling affiliation drive (0.07) and time orientation (0.06) from the speaker were deployed to index shared experience.",6.1 Results and Discussion,[0],[0]
"The top features from feature set f2 predictive of speakers using shared experience included listener behaviors such as smiling (0.53) perhaps to indicate appreciation towards the content of the talk, or encourage the speaker to go on (Niewiadomski et al., 2010).",6.1 Results and Discussion,[0],[0]
"Besides, the listener gazing elsewhere (0.50) or at the speaker (0.47), and neither gazing at own worksheet (-0.45) nor head nodding (-0.28) had strong predictive power.",6.1 Results and Discussion,[0],[0]
"The top features from feature set f3 predictive of speakers using shared experience included lower loudness in voice (-0.58), smiling (0.47), gazing elsewhere (0.59), at own worksheet (0.27) or at the partner (0.22) but not at partner’s worksheet (-0.40) from the interlocutor in the last turn.
",6.1 Results and Discussion,[0],[0]
"Praise: For praise, our computational model achieved an accuracy of 91% and kappa of 81%.",6.1 Results and Discussion,[0],[0]
"The top features from feature set f1 predictive of speakers using praise included gazing at partner’s worksheet (0.68) indicative of directing attention to the partner’s (perhaps the tutee’s) work, smiling (0.51), perhaps to mitigate the potential embarassment of praise (Niewiadomski et al., 2010) and head nodding (0.35) with a positive tone of voice (0.04), perhaps to emphasize the praise.",6.1 Results and Discussion,[0],[0]
"The top features from feature set f2 predictive of speakers using praise included listener behaviors such as head nodding (0.45) for backchanneling and acknowledgement and not gazing at partner’s worksheet (-1.06), elsewhere (-0.5) or at the partner (- 0.49).",6.1 Results and Discussion,[0],[0]
"The top features from feature set f3 predictive of speakers using praise included smiling (0.51), lower loudness in voice (-0.91) and overlap (-0.66) from the interlocutor in the last turn.
",6.1 Results and Discussion,[0],[0]
Violation of Social Norm:,6.1 Results and Discussion,[0],[0]
We achieved an accuracy of 80% and kappa of 61% for prediction.,6.1 Results and Discussion,[0],[0]
"The top features from feature set f1 predictive of speakers violating social norms included smiling
(0.40), gazing at partner (0.45) but not head nodding (-0.389).",6.1 Results and Discussion,[0],[0]
"(Keltner and Buswell, 1997) introduced a remedial account of embarrassment, emphasizing that smiles signal awareness of a social norm being violated and serve to provoke forgiveness from the interlocutor, in addition to being a hedging indicator.",6.1 Results and Discussion,[0],[0]
"(Kraut and Johnston, 1979) posited that smiling evolved from primate appeasement displays and is likely to occur when a person has violated a social norm.",6.1 Results and Discussion,[0],[0]
"The top features from feature set f2 predictive of speakers violating social norms included listener behaviors such as smiling (0.54), gazing at own worksheet (0.32) or at the partner’s (0.14).",6.1 Results and Discussion,[0],[0]
"The top features from feature set f3 predictive of speakers violating social norms included high loudness (0.86) and jitter in voice (0.50), lower shimmer in voice (-0.53), gazing at own worksheet (0.49) and no head nodding (-0.31) from the interlocutor in the last turn.",6.1 Results and Discussion,[0],[0]
"We began this paper indicating our interest in better understanding conversational strategies in and of themselves, and in employing automatic recognition of conversational strategies to improve interactive systems.",6.2 Implications,[0],[0]
"With respect to this first goal, because the current approach takes into account verbal, vocal and visual behaviors, it can identify regularities in social interaction processes that have not been identified by earlier work.",6.2 Implications,[0],[0]
"This becomes especially important as automatic behavioral analysis increasingly develops new real-time metrics to predict other kinds of conversational strategies related to interpersonal dynamics like politeness, sarcasm etc, that are not easily captured by observer-based labeling.",6.2 Implications,[0],[0]
"Similar benefits may accrue in other areas of automated human behavior understanding.
",6.2 Implications,[0],[0]
"With respect to interactive systems, these findings are applicable to building virtual peer tutors in whom rapport improves learning gains as it does for human-human tutors, training military personnel and police to build rapport with the communities in which they work, and trustworthy dialog systems for clinical decision support (DeVault et al., 2013).",6.2 Implications,[0],[0]
"Improved understanding of conversational strategy response pairs can help us better estimate the level of rapport at a given point in a dialog (Sinha et al., 2015; Zhao et al., 2016), which means that for the design of interactive systems, our work could help improve the capability of a natural language understanding module to capture
user’s interpersonal goals, such as those of building, maintaining or destroying rapport.
",6.2 Implications,[0],[0]
"More broadly, understanding of these particular ways of talking may also help us in building artificially intelligent systems that exhibit and evoke behaviors not just as conversationalists, but also as confidants to whom we can relay personal and emotional information with the expectation of acknowledgement, empathy and sympathy in response (Boden, 2010).",6.2 Implications,[0],[0]
"These social strategies improve the bond between interlocutors which, in turn, can improve the efficacy of their collaboration.",6.2 Implications,[0],[0]
"Efforts to experimentally generate interpersonal closeness (Aron et al., 1997) to achieve positive task and social outcomes depend on advances in moving beyond behavioral channels in isolation and leveraging the synergy and complementarity provided by multimodal human behaviors.",6.2 Implications,[0],[0]
"In this work, by performing quantitative analysis of our peer tutoring corpus followed by machine learning modeling, we learnt the discriminative power and generalizability of verbal, vocal and visual behaviors from both the speaker and listener, in distinguishing conversational strategy usage.
",7 Conclusion,[0],[0]
We found that interlocutors usually accompany the disclosure of personal information with head nods and mutual gaze.,7 Conclusion,[0],[0]
"When faced with such selfdisclosure listeners, on the other hand, often nod and avert their gaze .",7 Conclusion,[0],[0]
"When the conversational strategy of reference to shared experience is used, speakers are less likely to smile, and more likely to avert their gaze (Cassell et al., 2007).",7 Conclusion,[0],[0]
"Meanwhile, listeners smile to signal their coordination.",7 Conclusion,[0],[0]
"When speakers praise their partner, they direct their gaze to the interlocutor’s worksheet, smile and nod with a positive tone of voice.",7 Conclusion,[0],[0]
"Meanwhile, listeners simply smile, perhaps to mitigate the embarrassment of having been praised.
",7 Conclusion,[0],[0]
"Finally, speakers tend to gaze at their partner and smile when they violate a social norm, without nodding.",7 Conclusion,[0],[0]
"The listener, faced with a social norm violation, is likely to smile extensively (once again, most likely to mitigate the face threat of social norm violations such as teasing or insults).",7 Conclusion,[0],[0]
"Overall, these results present an interesting interplay of multimodal behaviors at work when speakers use conversational strategies to fulfil interpersonal goals in a dialog.
",7 Conclusion,[0],[0]
"These results have been integrated into a realtime end-to-end socially aware dialog system
(SARA)2 described in (Matsuyama et al., 2016) in this same volume.",7 Conclusion,[0],[0]
"SARA is capable of detecting conversational strategies, relying on the conversational strategies detected in order to accurately estimate rapport between the interlocutors, reasoning about how to respond to the intentions behind those particular behaviors, and generating appropriate social responses as a way of more effectively carrying out her task duties.",7 Conclusion,[0],[0]
"To our knowledge, SARA is the first socially-aware dialog system that relies on visual, verbal, and vocal cues to detect user social and task intent,and generates behaviors in those same channels to achieve her social and task goals.",7 Conclusion,[0],[0]
We acknowledge some methodological limitations in the current work.,8 Limitations and Future Work,[0],[0]
In the current work we undersampled the negative examples in order to make a balanced dataset.,8 Limitations and Future Work,[0],[0]
"For future work, we will work with corpora that have a more natural distribution and deal with the sparsity of the phenomena through machine learning methods.",8 Limitations and Future Work,[0],[0]
This will improve applicability to a real-time system where conversation strategies are likely to be less frequent than in our training dataset.,8 Limitations and Future Work,[0],[0]
"Moreover, in current work, we looked at individual modalities in isolation initially, and fused them later via a simple concatenation of feature vectors.",8 Limitations and Future Work,[0],[0]
Including sequentially occurring features may better exploit correlation and dependencies between features from different modalities.,8 Limitations and Future Work,[0],[0]
"As a next step, we have thus started to investigate the impact of temporal ordering of verbal and visual behaviors that lead to increased rapport (Zhao et al., 2016).
",8 Limitations and Future Work,[0],[0]
"In terms of future work, one concrete example of an application area where we are beginning to apply these findings is the domain of learning technologies.",8 Limitations and Future Work,[0],[0]
"While we know from research on dialog-based intelligent tutoring systems that conversations with such computer systems help students learn (Graesser, 2016), we also know that those students who are academically challenged, perhaps because under-represented in the fields they are trying to learn (Robinson et al., 2005), are most likely to need a social component to their learning interactions.",8 Limitations and Future Work,[0],[0]
Hence a major critique of existent intelligent tutoring systems is that they serve to fulfil only the task-goal of the interaction.,8 Limitations and Future Work,[0],[0]
"Traditionally (DMello and Graesser, 2013), this is instantiated via an expectation and misconception
2sociallyawarerobotassistant.net
tailored dialog directed towards the portions of learning content where student under-performance is noted, and simply blended with some motivational scaffolding.
",8 Limitations and Future Work,[0],[0]
"Despite significant advances in such conversational tutoring systems (Rus et al., 2013), we believe that future systems that provide intelligent support for tutoring via dialog should support the social as well as task nature of natural peer tutoring.",8 Limitations and Future Work,[0],[0]
"Because learning does not happen in a cultural or social void, it is important to think about how we can leverage dialog, the natural modality of pedagogy, to foster supportive relationships that make learning challenging, engaging and meaningful 3.
",8 Limitations and Future Work,[0],[0]
"We have also begun to use the social conversational strategies described here to complement the curriculum script in a traditional tutoring dialogue comprising knowledge-telling or knowledge-building utterances, shallow or deep question asking, hints and other forms of feedback.",8 Limitations and Future Work,[0],[0]
"We believe this is a step towards building SCEM-sensitive (social, cognitive, emotional and motivational) tutors (Graesser et al., 2010), and towards more accurate computational models of human interaction that will need to underlie those new kinds of intelligent tutors.
",8 Limitations and Future Work,[0],[0]
"Dialog systems that can recognize and use conversational strategies such as self-disclosure, reference to shared experience, and violation of social norms, are also part of a new genre of dialog system that departs from the rigid repetitive natural language generation templates of the olden days, and that can learn to speak with style.",8 Limitations and Future Work,[0],[0]
"It is conceivable that contemporary corpus-based approaches to NLG that introduce stylistic variation into a dialog (Wen et al., 2015) may one day learn on the user’s own conversational style, and entrain to it.",8 Limitations and Future Work,[0],[0]
"In a system like that, real-time recognition of conversational strategies like that demonstrated here could play an essential role.",8 Limitations and Future Work,[0],[0]
"Strategies (Section 5)
.",A Appendix: Complete Statistics for Understanding Conversational,[0],[0]
"In this work, we focus on automatically recognizing social conversational strategies that in human conversation contribute to building, maintaining or sometimes destroying a budding relationship.",abstractText,[0],[0]
"These conversational strategies include self-disclosure, reference to shared experience, praise and violation of social norms.",abstractText,[0],[0]
"By including rich contextual features drawn from verbal, visual and vocal modalities of the speaker and interlocutor in the current and previous turn, we can successfully recognize these dialog phenomena with an accuracy of over 80% and kappa ranging from 60-80%.",abstractText,[0],[0]
"Our findings have been successfully integrated into an end-to-end socially aware dialog system, with implications for virtual agents that can use rapport between user and system to improve task-oriented assistance.",abstractText,[0],[0]
Automatic Recognition of Conversational Strategies in the Service of a Socially-Aware Dialog System,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4797–4802 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4797",text,[0],[0]
"As the general quality of machine translation (MT) increases, there is a growing interest in improving the translation of specific linguistic phenomena.",1 Introduction,[0],[0]
"A case in point that has been studied in the context of both statistical (Hardmeier, 2014; Guillou, 2016; Loáiciga, 2017) and neural MT (Bawden et al., 2017; Voita et al., 2018) is that of pronominal anaphora.",1 Introduction,[0],[0]
"In the simplest case, translating anaphoric pronouns requires the generation of corresponding word forms respecting the grammatical constraints on agreement in the target language, as in the following English-French example, where the correct form of the pronoun in the second sentence varies depending on which of the (equally correct) translations of the word bicycle was used in the first:
(1) a. I have a bicycle.",1 Introduction,[0],[0]
It is red.,1 Introduction,[0],[0]
b. J’ai un vélo.,1 Introduction,[0],[0]
Il est rouge.,1 Introduction,[0],[0]
[ref] c. J’ai une bicyclette.,1 Introduction,[0],[0]
Elle est rouge.,1 Introduction,[0],[0]
"[MT]
However, the problem is more complex in practice because there is often no 1 : 1 correspondence between pronouns in two languages.",1 Introduction,[0],[0]
"This is easily demonstrated at the corpus level by observing that the number of pronouns varies significantly across languages in parallel texts (Mitkov
*Both authors contributed equally.
and Barbu, 2003), but it tends to be difficult to predict in individual cases.
",1 Introduction,[0],[0]
"In general MT research, significant progress was enabled by the invention of automatic evaluation metrics based on reference translations, such as BLEU (Papineni et al., 2002).",1 Introduction,[0],[0]
"Attempting to create a similar framework for efficient research, researchers have proposed automatic reference-based evaluation metrics specifically targeting pronoun translation: AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and PopescuBelis, 2017).",1 Introduction,[0],[0]
We study the performance of these metrics on a dataset of English-French translations and investigate to what extent automatic evaluation based on reference translations provides insights into how well an MT system handles pronouns.,1 Introduction,[0],[0]
"Our analysis clarifies the conceptual differences between AutoPRF and APT, uncovering weaknesses in both metrics, and investigates the effects of the alignment correction heuristics used in APT.",1 Introduction,[0],[0]
"By using the fine-grained PROTEST categories of pronoun function, we find that the accuracy of the automatic metrics varies across pronouns of different functions, suggesting that certain linguistic patterns are captured better in the automatic evaluation than others.",1 Introduction,[0],[0]
"We argue that fully automatic wide-coverage evaluation of this phenomenon is unlikely to drive research forward, as it misses essential parts of the problem despite achieving some correlation with human judgements.",1 Introduction,[0],[0]
"Instead, semiautomatic evaluation involving automatic identification of correct translations with high precision and low recall appears to be a more achievable goal.",1 Introduction,[0],[0]
Another more realistic option is a test suite evaluation with a very limited scope.,1 Introduction,[0],[0]
"Two reference-based automatic metrics of pronoun translation have been proposed in the literature.
",2 Pronoun Evaluation Metrics for MT,[0],[0]
"The first (Hardmeier and Federico, 2010) is a variant of precision, recall and F-score that measures the overlap of pronouns in the MT output with a reference translation.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"It lacks an official name, so we refer to it as AutoPRF following the terminology of the DiscoMT 2015 shared task (Hardmeier et al., 2015).",2 Pronoun Evaluation Metrics for MT,[0],[0]
"The scoring process relies on a word alignment between the source and the MT output, and between the source and the reference translation.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"For each input pronoun, it computes a clipped count (Papineni et al., 2002) of the overlap between the aligned tokens in the reference and the MT output.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"The clipped count of a given word is defined as the number of times it occurs in the MT output, limited by the number of times it occurs in the reference translation.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"The final metric is then calculated as the precision, recall and F-score based on these clipped counts.
",2 Pronoun Evaluation Metrics for MT,[0],[0]
Miculicich Werlen and Popescu-Belis (2017) propose a metric called Accuracy of Pronoun Translation (APT) that introduces several innovations over the previous work.,2 Pronoun Evaluation Metrics for MT,[0],[0]
"It is a variant of accuracy, so it counts, for each source pronoun, whether its translation can be considered correct, without considering multiple alignments.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"Since word alignment is problematic for pronouns, the authors propose an heuristic procedure to improve alignment quality.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"Finally, it introduces the notion of pronoun equivalence, assigning partial credit to pronoun translations that differ from the reference translation in specific ways deemed to be acceptable.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"In particular, it considers six possible cases when comparing the translation of a pronoun in MT output and the reference.",2 Pronoun Evaluation Metrics for MT,[0],[0]
"The pronouns may be: (1) identical, (2) equivalent, (3) different/incompatible, or there may be no translation in: (4) the MT output, (5) the reference, (6) either the MT output or the reference.",2 Pronoun Evaluation Metrics for MT,[0],[0]
Each of these cases may be assigned a weight between 0 and 1 to determine the level of correctness.,2 Pronoun Evaluation Metrics for MT,[0],[0]
"We study the behaviour of the two automatic metrics using the PROTEST test suite (Guillou and Hardmeier, 2016).",3 The PROTEST Dataset,[0],[0]
"The test suite comprises 250 hand-selected personal pronoun tokens taken from the DiscoMT2015.test dataset of TED talk transcriptions and translations (Hardmeier et al., 2016) and annotated according to the ParCor guidelines (Guillou et al., 2014).",3 The PROTEST Dataset,[0],[0]
"It is structured according to a linguistic typology motivated by work on func-
tional grammar by Dik (1978) and Halliday (2004).",3 The PROTEST Dataset,[0],[0]
"Pronouns are first categorised according to their function:
anaphoric: I have a bicycle.",3 The PROTEST Dataset,[0],[0]
It is red.,3 The PROTEST Dataset,[0],[0]
event:,3 The PROTEST Dataset,[0],[0]
He lost his job.,3 The PROTEST Dataset,[0],[0]
It was a shock.,3 The PROTEST Dataset,[0],[0]
pleonastic: It is raining.,3 The PROTEST Dataset,[0],[0]
addressee reference: You’re welcome.,3 The PROTEST Dataset,[0],[0]
"They are then subcategorised according to morphosyntactic criteria, whether the antecedent is a group noun, whether the ancedent is in the same or a different sentence, and whether an addressee reference pronoun refers to one or more specific people (deictic) or to people in general (generic).
",3 The PROTEST Dataset,[0],[0]
Our dataset contains human judgements on the performance of nine MT systems on the translation of the 250 pronouns in the PROTEST test suite.,3 The PROTEST Dataset,[0],[0]
"The systems include five submissions to the DiscoMT 2015 shared task on pronoun translation (Hardmeier et al., 2015) – four phrase-based SMT systems AUTO-POSTEDIT (Guillou, 2015), UU-HARDMEIER (Hardmeier et al., 2015), IDIAP (Luong et al., 2015), UU-TIEDEMANN (Tiedemann, 2015), a rule-based system ITS2 (Loáiciga and Wehrli, 2015), and the shared task baseline (also phrase-based SMT).",3 The PROTEST Dataset,[0],[0]
"Three NMT systems are included for comparison: LIMSI (Bawden et al., 2017), NYU (Jean et al., 2014), and YANDEX (Voita et al., 2018).
",3 The PROTEST Dataset,[0],[0]
"Manual evaluation was conducted using the PROTEST graphical user interface and accompanying guidelines (Hardmeier and Guillou, 2016).",3 The PROTEST Dataset,[0],[0]
The annotators were asked to make judgements (correct/incorrect) on the translations of the pronouns and antecedent heads whilst ignoring the correctness of other words (except in cases where it impacted the annotator’s ability to make a judgement).,3 The PROTEST Dataset,[0],[0]
"The annotations were carried out by two bilingual English-French speakers, both of whom are native speakers of French.",3 The PROTEST Dataset,[0],[0]
"Our human judgements differ in important ways from the human evaluation conducted for the same set of systems at DiscoMT 2015 (Hardmeier et al., 2015), which was carried out by non-native speakers over an unbalanced data sample using a gap-filling methodology.",3 The PROTEST Dataset,[0],[0]
"In the gap-filling task annotators are asked to select, from a predefined list (including an uninformative catch-all group “other”), those pronouns that could fill the pronoun translation slot.",3 The PROTEST Dataset,[0],[0]
"Unlike in the PROTEST evaluation, the pronoun translations were obscured in the MT output.",3 The PROTEST Dataset,[0],[0]
"This avoided priming the annotators with the output of
the candidate translation, but it occasionally caused valid translations to be rejected because they were missed by the annotator.",3 The PROTEST Dataset,[0],[0]
"There are three ways in which APT differs from AutoPRF: the scoring statistic, the alignment heuristic in APT, and the definition of pronoun equivalence.
",4 Accuracy versus Precision/Recall,[0],[0]
APT is a measure of accuracy: It reflects the proportion of source pronouns for which an acceptable translation was produced in the target.,4 Accuracy versus Precision/Recall,[0],[0]
"AutoPRF, by contrast, is a precision/recall metric on the basis of clipped counts.",4 Accuracy versus Precision/Recall,[0],[0]
"Hardmeier and Federico (2010) motivate the use of precision and recall by pointing out that word alignments are not 1 : 1, so each pronoun can be linked to multiple elements in the target language, both in the reference translation and in the MT output.",4 Accuracy versus Precision/Recall,[0],[0]
"Their metric is designed to account for all linked words in such cases.
",4 Accuracy versus Precision/Recall,[0],[0]
"To test the validity of this argument, we examined the subset of examples of 8 systems in our English-French dataset1 giving rise to a clipped count greater than one2 and found that these examples follow very specific patterns.",4 Accuracy versus Precision/Recall,[0],[0]
All 143 cases included exactly one personal pronoun.,4 Accuracy versus Precision/Recall,[0],[0]
"In 99 cases, the additional matched word was the complementiser que ‘that’.",4 Accuracy versus Precision/Recall,[0],[0]
"In 31 and 4 cases, respectively, it was a form of the auxiliary verbs avoir ‘to have’ and être",4 Accuracy versus Precision/Recall,[0],[0]
‘to be’.,4 Accuracy versus Precision/Recall,[0],[0]
One example matched both que and a form of être.,4 Accuracy versus Precision/Recall,[0],[0]
"Two had reflexive pronouns, and one an imperative verb form.",4 Accuracy versus Precision/Recall,[0],[0]
"With the possible exception of the two reflexive pronouns, none of this seems to be relevant to pronoun correctness.",4 Accuracy versus Precision/Recall,[0],[0]
We conclude that it is more reasonable to restrict the counts to a single pronominal item per example.,4 Accuracy versus Precision/Recall,[0],[0]
"With this additional restriction, however, the recall score of AutoPRF becomes equivalent to a version of APT without equivalent pronouns and alignment correction.",4 Accuracy versus Precision/Recall,[0],[0]
We therefore limit the remainder of our study to APT.,4 Accuracy versus Precision/Recall,[0],[0]
APT includes an heuristic alignment correction procedure to mitigate errors in the word alignment between a source-language text and its translation (reference or MT output).,5 Effects of Word Alignment,[0],[0]
"We ran experiments to
1Excluding the YANDEX system, which was added later.",5 Effects of Word Alignment,[0],[0]
"2A clipped count greater than one for a given pronoun translation indicates that the MT output and the reference translation aligned to this pronoun overlap in more than one token.
assess the correlation of APT with human judgements, with and without the alignment correction heuristics.
",5 Effects of Word Alignment,[0],[0]
Table 1 displays the APT results in both conditions and the proportion of pronouns in the PROTEST test suite marked as correctly translated.,5 Effects of Word Alignment,[0],[0]
"For better comparison with the PROTEST test suite results, we restricted APT to the pronouns in the test suite.",5 Effects of Word Alignment,[0],[0]
We used two different weight settings:3 APT-A uses weight 1 for identical matches and 0 for all other cases.,5 Effects of Word Alignment,[0],[0]
"APT-B uses weight 1 for identical matches, 0.5 for equivalent matches and 0 otherwise.
",5 Effects of Word Alignment,[0],[0]
There is little difference in the APT scores when we consider the use of alignment heuristics.,5 Effects of Word Alignment,[0],[0]
This is due to the small number of pronouns for which alignment improvements are applied for most systems (typically 0–12 per system).,5 Effects of Word Alignment,[0],[0]
The exception is the ITS2 system output for which 18 alignment improvements are made.,5 Effects of Word Alignment,[0],[0]
"For the following systems we observe a very small increase in APT score for each of the two weight settings we consider, when alignment heuristics are applied: UU-HARDMEIER (+0.8), ITS2 (+0.8), BASELINE (+0.8), YANDEX (+0.8), and NYU (+0.4).",5 Effects of Word Alignment,[0],[0]
"However, these small improvements are not sufficient to affect the system rankings.",5 Effects of Word Alignment,[0],[0]
"It seems, therefore, that the alignment heuristic has only a small impact on the validity of the score.
",5 Effects of Word Alignment,[0],[0]
"To assess differences in correlation with human judgment for pairs of APT settings, we run Williams’s significance test (Williams, 1959; Graham and Baldwin, 2014).",5 Effects of Word Alignment,[0],[0]
"The test reveals that differences in correlation between the various configurations of APT and human judgements are not statistically significant (p > 0.2 in all cases).
",5 Effects of Word Alignment,[0],[0]
3Personal recommendation by Lesly Miculicich Werlen.,5 Effects of Word Alignment,[0],[0]
"Like Miculicich Werlen and Popescu-Belis (2017), we use Pearson’s and Spearman’s correlation coefficients to assess the correlation between APT and our human judgements (Table 2).",6 Metric Accuracy per Category,[0],[0]
"Although APT does correlate with the human judgements over the PROTEST test suite, the correlation is weaker than that with the DiscoMT gap-filling evaluations reported in Miculicich Werlen and Popescu-Belis (2017).",6 Metric Accuracy per Category,[0],[0]
A Williams significance test reveals that the difference in correlation (for those systems common to both studies) is not statistically significant (p > 0.3).,6 Metric Accuracy per Category,[0],[0]
Table 1 also shows that the rankings induced from the PROTEST and APT scores are rather different.,6 Metric Accuracy per Category,[0],[0]
"The differences are due to the different ways in which the two metrics define pronoun correctness, and the different sources against which correctness is measured (reference translation vs. human judgement).
",6 Metric Accuracy per Category,[0],[0]
We also study how the results of APT (with alignment correction) interact with the categories in PROTEST.,6 Metric Accuracy per Category,[0],[0]
"We consider a pronoun to be measured as correct by APT if it is assigned case 1
(identical) or 2 (equivalent).",6 Metric Accuracy per Category,[0],[0]
"Likewise, a pronoun is considered incorrect if it is assigned case 3 (incompatible).",6 Metric Accuracy per Category,[0],[0]
"We compare the number of pronouns marked as correct/incorrect by APT and by the human judges, ignoring APT cases in which no judgement can be made: no translation of the pronoun in the MT output, reference or both, and pronouns for which the human judges were unable to make a judgement due to factors such as poor overall MT quality, incorrect word alignments, etc.",6 Metric Accuracy per Category,[0],[0]
"The results of this comparison are displayed in Table 3.
",6 Metric Accuracy per Category,[0],[0]
"At first glance, we can see that APT disagrees with the human judgements for almost a quarter (24.3%) of the assessed translations.",6 Metric Accuracy per Category,[0],[0]
The distribution of the disagreements over APT cases is very skewed and ranges from 8% for case 1 to 32% for case 2 and 49% for case 3.,6 Metric Accuracy per Category,[0],[0]
"In other words, APT identifies correct pronoun translations with good precision, but relatively low recall.",6 Metric Accuracy per Category,[0],[0]
"We can also see that APT rarely marks pronouns as equivalent (case 2).
",6 Metric Accuracy per Category,[0],[0]
Performance for anaphoric pronouns is mixed.,6 Metric Accuracy per Category,[0],[0]
"In general, there are three main problems affecting anaphoric pronouns (Table 4).",6 Metric Accuracy per Category,[0],[0]
"1) APT, which does not incorporate knowledge of anaphoric pronoun antecedents, does not consider pronoun-antecedent head agreement so many valid alternative translations involving personal pronouns are marked as incompatible (i.e. incorrect, case 3), but as correct by the human judges.",6 Metric Accuracy per Category,[0],[0]
"Consider the following example, in which the pronoun they is deemed correctly translated by the YANDEX system (according to the human judges) as it agrees in number and grammatical gender with the translation of the antecedent extraits (clips).",6 Metric Accuracy per Category,[0],[0]
"However, the pronoun translation ils is marked as incorrect by APT as it does not match the translation in the reference (elles).
",6 Metric Accuracy per Category,[0],[0]
SOURCE:,6 Metric Accuracy per Category,[0],[0]
"so what these two clips show is not just the devastating consequence of the disease, but they also tell us something about the shocking pace of the disease. .",6 Metric Accuracy per Category,[0],[0]
".
",6 Metric Accuracy per Category,[0],[0]
YANDEX:,6 Metric Accuracy per Category,[0],[0]
"donc ce que ces deux extraits[masc.,pl.]",6 Metric Accuracy per Category,[0],[0]
"montrent n’est pas seulement la conséquence dévastatrice de la maladie, mais ils[masc.",6 Metric Accuracy per Category,[0],[0]
pl.],6 Metric Accuracy per Category,[0],[0]
"nous disent aussi quelque chose sur le rythme choquant de la maladie. . .
",6 Metric Accuracy per Category,[0],[0]
REFERENCE:,6 Metric Accuracy per Category,[0],[0]
"ce que ces deux vidéos[fem.,pl.]",6 Metric Accuracy per Category,[0],[0]
"montrent, ce ne sont pas seulement les conséquences dramatiques de cette maladie, elles[fem.",6 Metric Accuracy per Category,[0],[0]
pl.],6 Metric Accuracy per Category,[0],[0]
nous montrent aussi la vitesse fulgurante de cette maladie. .,6 Metric Accuracy per Category,[0],[0]
".
",6 Metric Accuracy per Category,[0],[0]
2) Substitutions between pronouns are governed by much more complex rules than the simple pronoun equivalence mechanism in APT.,6 Metric Accuracy per Category,[0],[0]
"For example, the dictionary of pronouns used in APT lists il and ce as equivalent.",6 Metric Accuracy per Category,[0],[0]
"However, while il can often replace ce as a pleonastic pronoun in French, it has a much stronger tendency to be interpreted as anaphoric, rendering pleonastic use unacceptable if there is a salient masculine antecedent in the context.",6 Metric Accuracy per Category,[0],[0]
"3) APT does not consider the use of impersonal pronouns such as c’ in place of the feminine personal pronoun elle or the plural forms ils and elles.
",6 Metric Accuracy per Category,[0],[0]
Category V E,6 Metric Accuracy per Category,[0],[0]
"I O
Anaphoric intra-sent.",6 Metric Accuracy per Category,[0],[0]
subj.,6 Metric Accuracy per Category,[0],[0]
it 22 9 8 8 intra-sent.,6 Metric Accuracy per Category,[0],[0]
non,6 Metric Accuracy per Category,[0],[0]
-subj.,6 Metric Accuracy per Category,[0],[0]
it 16 – 1 2 inter-sent.,6 Metric Accuracy per Category,[0],[0]
subj.,6 Metric Accuracy per Category,[0],[0]
it 35 6 22 – inter-sent.,6 Metric Accuracy per Category,[0],[0]
non,6 Metric Accuracy per Category,[0],[0]
-,6 Metric Accuracy per Category,[0],[0]
subj.,6 Metric Accuracy per Category,[0],[0]
it – – – 13 intra-sent.,6 Metric Accuracy per Category,[0],[0]
they 25 – 3 9 inter-sent.,6 Metric Accuracy per Category,[0],[0]
they 22 – 3 22 singular they 40 – – 18 group it/,6 Metric Accuracy per Category,[0],[0]
"they 21 – – 10
Event it – 16 – 44
Pleonastic it – 11 – 35
V: Valid alternative translation",6 Metric Accuracy per Category,[0],[0]
"I: Impersonal translation E: Incorrect equivalence O: Other
Table 4: Common cases of disagreement for anaphoric, pleonastic, and event reference pronouns
As with anaphoric pronouns, APT incorrectly marks some pleonastic and event translations as equivalent, in disagreement with the human judges.",6 Metric Accuracy per Category,[0],[0]
"Other common errors arise from 1) the use of alternative translations marked as incompatible (i.e. incorrect) by APT but correct by the human judges, for example il (personal) in the MT output when the reference contained the impersonal pronoun cela or ça (30 cases for pleonastic, 7 for event), or 2) the presence of il in both the MT output and reference marked by APT as identical but by the human judges as incorrect (3 cases for pleonastic, 15 event).
",6 Metric Accuracy per Category,[0],[0]
"Some of these issues could be addressed by incorporating knowledge of pronoun function in the source language, of pronoun antecedents, and of the wider context of the translation surrounding the pronoun.",6 Metric Accuracy per Category,[0],[0]
"However, whilst we might be able to derive language-specific rules for some scenarios, it would be difficult to come up with more general or language-independent rules.",6 Metric Accuracy per Category,[0],[0]
"For example, il and ce can be anaphoric or pleonastic pronouns, but
il has a more referential character.",6 Metric Accuracy per Category,[0],[0]
Therefore in certain constructions that are strongly pleonastic (e.g. clefts) only ce is acceptable.,6 Metric Accuracy per Category,[0],[0]
"This rule would be specific to French, and would not cover other scenarios for the translation of pleonastic it.",6 Metric Accuracy per Category,[0],[0]
"Other issues include the use of pronouns in impersonal constructions such as il faut [one must/it takes] in which evaluation of the pronoun requires consideration of the whole expression, or transformations between active and passive voice, where the perspective of the pronouns changes.",6 Metric Accuracy per Category,[0],[0]
"Our analyses reveal that despite some correlation between APT and the human judgements, fully automatic wide-coverage evaluation of pronoun translation misses essential parts of the problem.",7 Conclusions,[0],[0]
"Comparison with human judgements shows that APT identifies good translations with relatively high precision, but fails to reward important patterns that pronoun-specific systems must strive to generate.",7 Conclusions,[0],[0]
"Instead of relying on fully automatic evaluation, our recommendation is to emphasise high precision in the automatic metrics and implement semiautomatic evaluation procedures that refer negative cases to a human evaluator, using available tools and methods (Hardmeier and Guillou, 2016).",7 Conclusions,[0],[0]
"Fully automatic evaluation of a very restricted scope may still be feasible using test suites designed for specific problems (Bawden et al., 2017).",7 Conclusions,[0],[0]
"We would like to thank our annotators, Marie Dubremetz and Miryam de Lhoneux, for their many hours of painstaking work, Lesly Miculicich Werlen for providing APT results for the DiscoMT 2015 systems, Elena Voita, Sébastien Jean, Stanislas Lauly and Rachel Bawden for providing the NMT system outputs, and the three anonymous reviewers.",Acknowledgements,[0],[0]
The annotation work was funded by the European Association for Machine Translation.,Acknowledgements,[0],[0]
The work carried out at The University of Edinburgh was funded by the ERC H2020 Advanced Fellowship GA 742137 SEMANTAX and a grant from The University of Edinburgh and Huawei Technologies.,Acknowledgements,[0],[0]
The work carried out at Uppsala University was funded by the Swedish Research Council under grant 2017-930.,Acknowledgements,[0],[0]
We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite.,abstractText,[0],[0]
"Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics.",abstractText,[0],[0]
"Instead, we recommend the use of semiautomatic metrics and test suites in place of fully automatic metrics.",abstractText,[0],[0]
Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 168–178 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1016
We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the most human-like amongst all evaluated.",text,[0],[0]
"Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints.",1 Introduction,[0],[0]
"As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult.",1 Introduction,[0],[0]
"In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.
",1 Introduction,[0],[0]
"Poetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem’s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows.",1 Introduction,[0],[0]
"These rules may describe aspects of the literary devices used, and are usually highly prescriptive.",1 Introduction,[0],[0]
"Examples of different forms of poetry are limericks, ballads and sonnets.",1 Introduction,[0],[0]
"Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines.",1 Introduction,[0],[0]
"Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud.
",1 Introduction,[0],[0]
Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context.,1 Introduction,[0],[0]
"Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015).",1 Introduction,[0],[0]
"Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form).
",1 Introduction,[0],[0]
We experiment with two novel methodologies for solving this task.,1 Introduction,[0],[0]
The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding.,1 Introduction,[0],[0]
"The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form.",1 Introduction,[0],[0]
"This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.
168",1 Introduction,[0],[0]
Automatic poetry generation is an important task due to the significant challenges involved.,2 Related Work,[0],[0]
"Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches.
",2 Related Work,[0],[0]
"Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013; Barbieri et al., 2012) and text mining (Netzer et al., 2009).",2 Related Work,[0],[0]
"These approaches are often inspired by how humans might generate poetry.
",2 Related Work,[0],[0]
"Statistical approaches, conversely, make no assumptions about the creative process.",2 Related Work,[0],[0]
"Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used to generate new poetic variants (Yi et al., 2016; Greene et al., 2010).",2 Related Work,[0],[0]
Neural language models have been increasingly applied to the task of poetry generation.,2 Related Work,[0],[0]
"The work of Zhang and Lapata (2014) is one such example, where they were able to outperform all other classical Chinese poetry generation systems with both manual and automatic evaluation.",2 Related Work,[0],[0]
Ghazvininejad et al. (2016) and Goyal et al. (2016) apply neural language models with regularising finite state machines.,2 Related Work,[0],[0]
"However, in the former case the rhythm of the output cannot be defined at sample time, and in the latter case the finite state machine is not trained on rhythm at all, as it is trained on dialogue acts.",2 Related Work,[0],[0]
"McGregor et al. (2016) construct a phonological model for generating prosodic texts, however there is no attempt to embed semantics into this model.",2 Related Work,[0],[0]
"Our first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content.",3 Phonetic-level Model,[0],[0]
Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols.,3 Phonetic-level Model,[0],[0]
"Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm.
",3 Phonetic-level Model,[0],[0]
"However, just training on a large corpus of poetry data is not enough.",3 Phonetic-level Model,[0],[0]
"Specifically, two problems need to be overcome.",3 Phonetic-level Model,[0],[0]
"1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes.
",3 Phonetic-level Model,[0],[0]
This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes.,3 Phonetic-level Model,[0],[0]
2),3 Phonetic-level Model,[0],[0]
"The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme.",3 Phonetic-level Model,[0],[0]
"It is therefore important to train the model on poetry which has its own internal consistency.
",3 Phonetic-level Model,[0],[0]
"Thus, the model comprises three steps: transliterating an orthographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.
",3 Phonetic-level Model,[0],[0]
"Phonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words.",3 Phonetic-level Model,[0],[0]
"These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1.",3 Phonetic-level Model,[0],[0]
The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones.,3 Phonetic-level Model,[0],[0]
"In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon.",3 Phonetic-level Model,[0],[0]
"Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters.
",3 Phonetic-level Model,[0],[0]
Although this approach offers full coverage over the training corpus—even for abbreviated words like ask’d and archaic words like renewest—it has several limitations.,3 Phonetic-level Model,[0],[0]
"Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.
",3 Phonetic-level Model,[0],[0]
"In addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol.",3 Phonetic-level Model,[0],[0]
"This makes
1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%.",3 Phonetic-level Model,[0],[0]
"Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.
decipherment, when converting back into an orthographic representation, much easier.",3 Phonetic-level Model,[0],[0]
"Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.
",3 Phonetic-level Model,[0],[0]
"Neural language model We train a Long-Short Term Memory network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus.",3 Phonetic-level Model,[0],[0]
The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes.,3 Phonetic-level Model,[0],[0]
"Specifically, we maximize a multinomial logistic regression objective over the final softmax prediction.",3 Phonetic-level Model,[0],[0]
"Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256.",3 Phonetic-level Model,[0],[0]
"We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form.",3 Phonetic-level Model,[0],[0]
This allows the network to learn features like rhyme even when spread over multiple lines.,3 Phonetic-level Model,[0],[0]
"Training is preemptively stopped at 25 epochs to prevent overfitting.
",3 Phonetic-level Model,[0],[0]
"Orthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes.",3 Phonetic-level Model,[0],[0]
"That is, we compute the most probable hypothesis word W given a phoneme sequence ρ:
arg maxi P ( Wi | ρ ) (1)
We can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings.",3 Phonetic-level Model,[0],[0]
"The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010).
",3 Phonetic-level Model,[0],[0]
"Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model.",3 Phonetic-level Model,[0],[0]
We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word.,3 Phonetic-level Model,[0],[0]
All pronunciations are naively considered equiprobable.,3 Phonetic-level Model,[0],[0]
We perform Viterbi decoding to find the most likely sequence of words.,3 Phonetic-level Model,[0],[0]
"This means finding the most likely word wt+1 given a
previous word sequence (wt−n, ..., wt).
arg maxwt+1 P ( wt+1 | w1, ... , wt ) (2)
",3 Phonetic-level Model,[0],[0]
"If a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that n maximises the n-gram frequency of the subsequences.
",3 Phonetic-level Model,[0],[0]
Output A popular form of poetry with strict internal structure is the sonnet.,3 Phonetic-level Model,[0],[0]
"Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010).",3 Phonetic-level Model,[0],[0]
"Since the 17,134 word tokens in Shakespeare’s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters.
",3 Phonetic-level Model,[0],[0]
An example of the output when training on this sonnets corpus is provided in Figure 1.,3 Phonetic-level Model,[0],[0]
"Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme.",3 Phonetic-level Model,[0],[0]
"As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters.",4 Constrained Character-level Model,[0],[0]
"However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry.",4 Constrained Character-level Model,[0],[0]
"That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry.",4 Constrained Character-level Model,[0],[0]
"Not only is this impractical, but in many cases no corpus of
adequate size even exists.",4 Constrained Character-level Model,[0],[0]
"Even when such poetic corpora are available, a new model must be trained for each type of poetry.",4 Constrained Character-level Model,[0],[0]
"This precludes tweaking the form of the output, which is important when generating poetry automatically.
",4 Constrained Character-level Model,[0],[0]
We now explore an alternative approach.,4 Constrained Character-level Model,[0],[0]
"Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model representing content, and a discriminative model representing form.",4 Constrained Character-level Model,[0],[0]
"This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.
",4 Constrained Character-level Model,[0],[0]
"Character Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content.",4 Constrained Character-level Model,[0],[0]
This approach offers several benefits over the word-level models that are prevalent in the literature.,4 Constrained Character-level Model,[0],[0]
"Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words.
",4 Constrained Character-level Model,[0],[0]
"As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus.",4 Constrained Character-level Model,[0],[0]
"Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources.",4 Constrained Character-level Model,[0],[0]
"This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online.",4 Constrained Character-level Model,[0],[0]
The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters.,4 Constrained Character-level Model,[0],[0]
"This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers, with embeddings in 128 dimensions.",4 Constrained Character-level Model,[0],[0]
"The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent.",4 Constrained Character-level Model,[0],[0]
"We attenuate the learning rate over time, and by 20 epochs the model converges.
",4 Constrained Character-level Model,[0],[0]
"Rhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such
as rhythm.",4 Constrained Character-level Model,[0],[0]
"Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features.",4 Constrained Character-level Model,[0],[0]
"As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus.
",4 Constrained Character-level Model,[0],[0]
"Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations for constructing a classifier.",4 Constrained Character-level Model,[0],[0]
"All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3.",4 Constrained Character-level Model,[0],[0]
"Furthermore, they are constructed from American English, meaning that British English may be misclassified.
",4 Constrained Character-level Model,[0],[0]
These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word.,4 Constrained Character-level Model,[0],[0]
"That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress.",4 Constrained Character-level Model,[0],[0]
This allows us to derive a syllablestress distribution.,4 Constrained Character-level Model,[0],[0]
"Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used.
",4 Constrained Character-level Model,[0],[0]
We represent each line of poetry as a cascade of Weighted Finite State Transducers (WFST).,4 Constrained Character-level Model,[0],[0]
A WFST is a finite-state automaton that maps between two sets of symbols.,4 Constrained Character-level Model,[0],[0]
"It is defined as an eight-tuple where ⟨Q, Σ, ρ, I, F, ∆, λ, p⟩:
Q : A set of states
Σ : An input alphabet of symbols
ρ : An output alphabet of symbols
I :",4 Constrained Character-level Model,[0],[0]
"A set of initial states
F : A set of final states, or sinks
∆ : A transition function mapping pairs of states and symbols to sets of states
λ : A set of weights for initial states
P : A set of weights for final states 3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91% when the following word is the (Greene et al., 2010)
",4 Constrained Character-level Model,[0],[0]
"A WFST assigns a probability (or weight, in the general case) to each path through it, going from an initial state to an end state.",4 Constrained Character-level Model,[0],[0]
"Every path corresponds to an input and output label sequence, and there can be many such paths for each sequence.
WFSTs are often used in a cascade, where a number of machines are executed in series, such that the output tape of one machine is the input tape for the next.",4 Constrained Character-level Model,[0],[0]
"Formally, a cascade is represented by the functional composition of several machines.
",4 Constrained Character-level Model,[0],[0]
"W (x, z) = A(x|y) ◦ B(y|z) ◦",4 Constrained Character-level Model,[0],[0]
"C(z) (3)
Where W (x, z) is defined as the ⊕ sum of the path probabilities through the cascade, and x and z are an input sequence and output sequence respectively.",4 Constrained Character-level Model,[0],[0]
"In the real semiring (where the product of probabilities are taken in series, and the sum of the probabilities are taken in parallel), we can rewrite the definition of weighted composition to produce the following:
W (x, z) = ⊕
y
A(x | y) ⊗ B(y | z) ⊗ C(z) (4)
",4 Constrained Character-level Model,[0],[0]
"As we are dealing with probabilities, this can be rewritten as:
P (x, z) = ∑
y
P (x | y)P (y | z)P (z) (5)
We can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word.",4 Constrained Character-level Model,[0],[0]
"Every word is represented by a single transducer.
",4 Constrained Character-level Model,[0],[0]
"In each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length4.",4 Constrained Character-level Model,[0],[0]
"We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set.",4 Constrained Character-level Model,[0],[0]
"We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades.",4 Constrained Character-level Model,[0],[0]
"In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2.",4 Constrained Character-level Model,[0],[0]
"We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.
4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).
",4 Constrained Character-level Model,[0],[0]
"× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×
Constraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model.",4 Constrained Character-level Model,[0],[0]
"To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word tokens in an intermediary buffer.",4 Constrained Character-level Model,[0],[0]
We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade.,4 Constrained Character-level Model,[0],[0]
"This defines the distribution of stress-patterns over our word tokens.
",4 Constrained Character-level Model,[0],[0]
"We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter.",4 Constrained Character-level Model,[0],[0]
"While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled.",4 Constrained Character-level Model,[0],[0]
The pronunciation model then returns the probability that the entire line is within the specified meter.,4 Constrained Character-level Model,[0],[0]
"If a new word is rejected by the classifier, the state of the network is rolled back to the last formulaically acceptable state of the line, removing the rejected word from memory.",4 Constrained Character-level Model,[0],[0]
The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier.,4 Constrained Character-level Model,[0],[0]
"By increasing the threshold, output focuses on form over content.",4 Constrained Character-level Model,[0],[0]
"Conversely, decreasing the criterion puts greater emphasis on content.",4 Constrained Character-level Model,[0],[0]
It is important for any generative poetry model to include themes and poetic devices.,4.1 Themes and Poetic devices,[0],[0]
One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices.,4.1 Themes and Poetic devices,[0],[0]
"To create a themed corpus about ‘love’, for instance, we would aggregate love poetry to train the model, which would thus learn an implicit representation of love.",4.1 Themes and Poetic devices,[0],[0]
"However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model.",4.1 Themes and Poetic devices,[0],[0]
"In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus.",4.1 Themes and Poetic devices,[0],[0]
"Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like ‘love’.",4.1 Themes and Poetic devices,[0],[0]
"This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.
",4.1 Themes and Poetic devices,[0],[0]
"Themes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model.",4.1 Themes and Poetic devices,[0],[0]
"First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013).",4.1 Themes and Poetic devices,[0],[0]
"For example, the theme winter might include thematic words frozen, cold, snow and frosty.",4.1 Themes and Poetic devices,[0],[0]
"We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of these character strings by their cosine similarity to the key word, plus a constant.",4.1 Themes and Poetic devices,[0],[0]
"Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.
",4.1 Themes and Poetic devices,[0],[0]
"Poetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration.",4.1 Themes and Poetic devices,[0],[0]
"Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the same heuristic to boost the probability of sampling character strings that have previously been sampled.",4.1 Themes and Poetic devices,[0],[0]
"That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word.",4.1 Themes and Poetic devices,[0],[0]
"After a word break character, we boost the probability that those characters will be sampled again in the softmax.",4.1 Themes and Poetic devices,[0],[0]
We only keep track of frequencies for a fixed number of time steps.,4.1 Themes and Poetic devices,[0],[0]
"By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration.",4.1 Themes and Poetic devices,[0],[0]
Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels).,4.1 Themes and Poetic devices,[0],[0]
"An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c.",4.1 Themes and Poetic devices,[0],[0]
"In order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways.",5 Evaluation,[0],[0]
"First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry.",5 Evaluation,[0],[0]
"Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry.",5 Evaluation,[0],[0]
"To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model.",5.1 Intrinsic evaluation,[0],[0]
The first set was sampled from the phonetic-level model trained on Iambic poetry.,5.1 Intrinsic evaluation,[0],[0]
"The second set was sampled from the characterlevel model, constrained to Iambic form.",5.1 Intrinsic evaluation,[0],[0]
"For com-
parison, and to act as a baseline, we also sampled from the unconstrained character model.
",5.1 Intrinsic evaluation,[0],[0]
"We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed.",5.1 Intrinsic evaluation,[0],[0]
"We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line.",5.1 Intrinsic evaluation,[0],[0]
"This was done by speaking each line aloud, and noting where the speaker put stresses.
",5.1 Intrinsic evaluation,[0],[0]
"As Table 1 shows, the constrained character level model generated the most formulaic poetry.",5.1 Intrinsic evaluation,[0],[0]
"Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors.",5.1 Intrinsic evaluation,[0],[0]
"We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry.
",5.1 Intrinsic evaluation,[0],[0]
"In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps).",5.1 Intrinsic evaluation,[0],[0]
Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5.,5.1 Intrinsic evaluation,[0],[0]
"Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction.",5.1 Intrinsic evaluation,[0],[0]
Coverage captures the percentage of in-vocabulary items.,5.1 Intrinsic evaluation,[0],[0]
The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words.,5.1 Intrinsic evaluation,[0],[0]
"The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes.
",5.1 Intrinsic evaluation,[0],[0]
"5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.",5.1 Intrinsic evaluation,[0],[0]
We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry.,5.2 Extrinsic evaluation,[0],[0]
As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e:,5.2 Extrinsic evaluation,[0],[0]
"the model generates good Iambic form, but not very good English), we only evaluate on the constrained characterlevel model.",5.2 Extrinsic evaluation,[0],[0]
"Poetry was generated with a variety of themes and poetic devices (see supplementary material).
",5.2 Extrinsic evaluation,[0],[0]
"The aim of the study was to determine whether participants could distinguish between human and machine-generated poetry, and if so to what extent.",5.2 Extrinsic evaluation,[0],[0]
"A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated.",5.2 Extrinsic evaluation,[0],[0]
"Participants were recruited from friends and people within poetry communities within the University of Cambridge, with an age range of 17 to 80, and a mean age of 29.",5.2 Extrinsic evaluation,[0],[0]
"Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.
",5.2 Extrinsic evaluation,[0],[0]
"In addition to the classification task, each participant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit).",5.2 Extrinsic evaluation,[0],[0]
We naively consider the overall quality of a poem to be the mean of these three measures.,5.2 Extrinsic evaluation,[0],[0]
"We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5.",5.2 Extrinsic evaluation,[0],[0]
"Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans.
",5.2 Extrinsic evaluation,[0],[0]
"To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948).",5.2 Extrinsic evaluation,[0],[0]
This formula represents readability as a function of the complexity of the input words.,5.2 Extrinsic evaluation,[0],[0]
We selected nine machine-generated poems with a high readability score.,5.2 Extrinsic evaluation,[0],[0]
"The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them.
",5.2 Extrinsic evaluation,[0],[0]
"For our human poems, we focused explicitly on poetry where greater consideration is placed on
6http://neuralpoetry.getforge.io/
prosodic elements like rhythm and rhyme than semantic content (known as “nonsense verse”).",5.2 Extrinsic evaluation,[0],[0]
"We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which eight were selected for the final comparison based on their comparable readability score.",5.2 Extrinsic evaluation,[0],[0]
"The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments.",5.2 Extrinsic evaluation,[0],[0]
An example of such a segment is shown in Figure 4d.,5.2 Extrinsic evaluation,[0],[0]
"The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts.
",5.2 Extrinsic evaluation,[0],[0]
"The performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3.",5.2 Extrinsic evaluation,[0],[0]
"For the human poems,
our group of participants guessed correctly that they were human 51.4% of the time.",5.2 Extrinsic evaluation,[0],[0]
"For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated.",5.2 Extrinsic evaluation,[0],[0]
"To determine whether our results were statistically significant, we performed a Chi2 test.",5.2 Extrinsic evaluation,[0],[0]
This resulted in a p-value of 0.718.,5.2 Extrinsic evaluation,[0],[0]
This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way.,5.2 Extrinsic evaluation,[0],[0]
"Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them.",5.2 Extrinsic evaluation,[0],[0]
"Interestingly, our results seem to suggest that our participants consider the generated poems to be more ‘human-like’ than those actually written by humans.",5.2 Extrinsic evaluation,[0],[0]
"In addition, the poem with the highest overall quality rating is a machine generated one.",5.2 Extrinsic evaluation,[0],[0]
"This shows that our approach was effective at generating high-quality rhythmic verse.
",5.2 Extrinsic evaluation,[0],[0]
It should be noted that the poems that were most ‘human-like’ and most aesthetic respectively were generated by the neural character model.,5.2 Extrinsic evaluation,[0],[0]
"Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form.",5.2 Extrinsic evaluation,[0],[0]
"All generated poems included in this evaluation can be found in the supplementary material, and our code is made available online7.
",5.2 Extrinsic evaluation,[0],[0]
7https://github.com/JackHopkins/ACLPoetry,5.2 Extrinsic evaluation,[0],[0]
Our contributions are twofold.,6 Conclusions,[0],[0]
"First, we developed a neural language model trained on a phonetic transliteration of poetic form and content.",6 Conclusions,[0],[0]
"Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse.",6 Conclusions,[0],[0]
"We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time.",6 Conclusions,[0],[0]
"This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices.
",6 Conclusions,[0],[0]
"An indistinguishability test, where participants were asked to classify a randomly selected set of human “nonsense verse” and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans.",6 Conclusions,[0],[0]
"In addition, the poems that were deemed most ‘humanlike’ and most aesthetic were both machinegenerated.
",6 Conclusions,[0],[0]
"In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.",6 Conclusions,[0],[0]
We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms.,abstractText,[0],[0]
The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry.,abstractText,[0],[0]
"This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration.",abstractText,[0],[0]
"The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form.",abstractText,[0],[0]
"By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes.",abstractText,[0],[0]
A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time.,abstractText,[0],[0]
"In addition, participants rated a machinegenerated poem to be the most human-like amongst all evaluated.",abstractText,[0],[0]
Automatically Generating Rhythmic Verse with Neural Networks,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1132–1142, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
and reasoning approach to automatically solving math word problems. A new meaning representation language is designed to bridge natural language text and math expressions. A CFG parser is implemented based on 9,600 semi-automatically created grammar rules. We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall.",text,[0],[0]
"Computers, since their creation, have exceeded human beings in (speed and accuracy of) mathematical calculation.",1 Introduction,[0],[0]
"However, it is still a big challenge nowadays to design algorithms to automatically solve even primary-school-level math word problems (i.e., math problems described in natural language).
",1 Introduction,[0],[0]
"Efforts to automatically solve math word problems date back to the 1960s (Bobrow, 1964a, b).",1 Introduction,[0],[0]
Previous work on this topic falls into two categories: symbolic approaches and statistical learning methods.,1 Introduction,[0],[0]
"In symbolic approaches (Bobrow, 1964a, b; Charniak, 1968; Bakman, 2007; Liguda & Pfeiffer, 2012), math problem sentences are transformed to certain structures by pattern matching or verb categorization.",1 Introduction,[0],[0]
Equations are then derived from the structures.,1 Introduction,[0],[0]
"Statistical learning methods are employed in two recent papers (Kushman et al., 2014; Hosseini et al., 2014).
suffer from two major shortcomings.",1 Introduction,[0],[0]
"First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad-hoc manner (refer to the related work section for more details).",1 Introduction,[0],[0]
"Second, surprisingly, they seldom report evaluation results about the effectiveness of the methods (except for some examples for demonstration purposes).",1 Introduction,[0],[0]
"For the small percentage of work with evaluation results available, it is unclear whether the patterns and rules are specially designed for specific sentences in a test set.
",1 Introduction,[0],[0]
"In this paper, we present a computer system called SigmaDolphin which automatically solves math word problems by semantic parsing and reasoning.",1 Introduction,[0],[0]
We design a meaning representation language called DOL (abbreviation of dolphin language) as the structured semantic representation of NL text.,1 Introduction,[0],[0]
A semantic parser is implemented to transform math problem text into DOL trees.,1 Introduction,[0],[0]
A reasoning module is included to derive math expressions from DOL trees and to calculate final answers.,1 Introduction,[0],[0]
"Our approach falls into the symbolic category, but makes improvements over previous symbolic methods in the following ways, ______________________________________",1 Introduction,[0],[0]
"* Work done while this author was an intern at Microsoft Research
1132
1)",1 Introduction,[0],[0]
"We introduce a systematic way of parsing
NL text, based on context-free grammar (CFG).
2)",1 Introduction,[0],[0]
Evaluation is enhanced in terms of both data set construction and evaluation mechanisms.,1 Introduction,[0],[0]
We split the problem set into a development set (called dev set) and a test set.,1 Introduction,[0],[0]
"Only the dev set is accessible during our algorithm design (especially in designing CFG rules and in implementing the parsing algorithm), which avoids over-tuning towards the test set.",1 Introduction,[0],[0]
"Three metrics (precision, recall, and F1) are employed to measure system performance from multiple perspectives, in contrast to all previous work (including the statistical ones) which only measures accuracy.
",1 Introduction,[0],[0]
"We target, in experiments, a subtype of word problems: number word problems (i.e., verbally expressed number problems, as shown in Figure 1).",1 Introduction,[0],[0]
"We hope to extend our techniques to handle general math word problems in the future.
",1 Introduction,[0],[0]
"We build a test set of over 1,500 problems and make a quantitative comparison with state-of-theart statistical methods.",1 Introduction,[0],[0]
Evaluation results show that our approach significantly outperforms baseline methods on our test set.,1 Introduction,[0],[0]
"Our system yields an extremely high precision of 95.4% and a reasonable recall of 60.2%, which shows promising application of our system in precision-critical situations.",1 Introduction,[0],[0]
Most previous work on automatic word problem solving is symbolic.,2.1 Math word problem solving,[0],[0]
"STUDENT (Bobrow, 1964a, b) handles algebraic problems by first transforming NL sentences into kernel sentences using a small set of transformation patterns.",2.1 Math word problem solving,[0],[0]
The kernel sentences are then transformed to math expressions by recursive use of pattern matching.,2.1 Math word problem solving,[0],[0]
"CARPS (Charniak, 1968, 1969) uses a similar approach to solve English rate problems.",2.1 Math word problem solving,[0],[0]
The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object.,2.1 Math word problem solving,[0],[0]
Liguda & Pfeiffer (2012) propose modeling math word problems with augmented semantic networks.,2.1 Math word problem solving,[0],[0]
"Addition/subtraction problems are studied most in early research (Briars & Larkin, 1984; Fletcher, 1985; Dellarosa, 1986; Bakman, 2007; Ma et al., 2010).",2.1 Math word problem solving,[0],[0]
"Please refer to Mukherjee & Garain (2008) for a review of symbolic approaches before 2008.
",2.1 Math word problem solving,[0],[0]
"1 http://www.wolframalpha.com
No empirical evaluation results are reported in most of the above work.",2.1 Math word problem solving,[0],[0]
Almost all of these approaches parse NL text by simply applying pattern matching rules in an ad-hoc manner.,2.1 Math word problem solving,[0],[0]
"For example, as mentioned in Bobrow (1964b), due to the pattern “($, AND $)”, the system would incorrectly divide “Tom has 2 apples, 3 bananas, and 4 pears.”",2.1 Math word problem solving,[0],[0]
"into two “sentences”: “Tom has 2 apples, 3 bananas.”",2.1 Math word problem solving,[0],[0]
"and “4 pears.”
",2.1 Math word problem solving,[0],[0]
"WolframAlpha1 shows some examples2 of automatically solving elementary math word problems, with technique details unknown to the general public.",2.1 Math word problem solving,[0],[0]
Other examples on the web site demonstrate a large coverage of short phrase queries on math and other domains.,2.1 Math word problem solving,[0],[0]
"By randomly selecting problems from our dataset and manually testing on their web site, we find that it fails to handle most problems in our problem collection.
",2.1 Math word problem solving,[0],[0]
Statistical learning methods have been proposed recently in two papers: Hosseini et al. (2014) solve single step or multi-step homogenous addition and subtraction problems by learning verb categories from the training data.,2.1 Math word problem solving,[0],[0]
"Kushman et al. (2014) can solve a wide range of word problems, given that the equation systems and solutions are attached to problems in the training set.",2.1 Math word problem solving,[0],[0]
The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines.,2.1 Math word problem solving,[0],[0]
There has been much work on analyzing the semantic structure of NL strings.,2.2 Semantic parsing,[0],[0]
"In semantic role labeling and frame-semantic parsing (Gildea & Jurafsky, 2002; Carreras & Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014), predicate-argument structures are discovered from text as their shallow semantic representation.",2.2 Semantic parsing,[0],[0]
"In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expressions.
",2.2 Semantic parsing,[0],[0]
"Another type of semantic parsing work (Zelle & Mooney, 1996; Zettlemoyer & Collins, 2005; Zettlemoyer & Collins, 2007; Wong & Mooney, 2007; Cai & Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013; Berant & Liang, 2014) maps NL text into logical forms by supervised or semi-supervised learning.",2.2 Semantic parsing,[0],[0]
"Some of them are based on or related to combinatory categorial grammar (CCG) (Steedman, 2000).",2.2 Semantic parsing,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) keeps richer semantic information than CCG and logical
2 https://www.wolframalpha.com/examples/ElementaryMath.html (bottom-right part)
forms.",2.2 Semantic parsing,[0],[0]
"In Section 3.1.4, we discuss the differences between DOL, AMR, and CCG, and explain why we choose DOL as the meaning representation language for math problem solving.",2.2 Semantic parsing,[0],[0]
"Consider the first problem in Figure 1 (written below for convenience), One number is 16 more than another.",3 Approach,[0],[0]
"If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two numbers.",3 Approach,[0],[0]
"Find the numbers.
",3 Approach,[0],[0]
"To automatically solve this problem, the computer system needs to figure out, somehow, that 1) two numbers x, y are demanded, and 2) they satisfy the equations below,
x = 16 + y
(2/3)x – y =",3 Approach,[0],[0]
"(x + y) / 4
(1)
(2)
To achieve this, reasoning must be performed based on common sense knowledge and the information provided by the source problem.",3 Approach,[0],[0]
"Given the difficulty of performing reasoning directly on unstructured and ambiguous natural language text, it is reasonable to transform the source text into a structured, less ambiguous representation.
",3 Approach,[0],[0]
"Our approach contains three modules: 1) A meaning representation language called DOL newly designed by us as the semantic
representation of natural language text.
",3 Approach,[0],[0]
"2) A semantic parser which transforms natural language sentences of a math problem
into DOL representation.
3)",3 Approach,[0],[0]
A reasoning module to derive math expressions from DOL representation.,3 Approach,[0],[0]
Every meaningful piece of NL text is represented in DOL as a semantic tree of various node types.,3.1 DOL: Meaning representation language,[0],[0]
Figure 2 shows the DOL representation of the second problem of Figure 1.,3.1 DOL: Meaning representation language,[0],[0]
"It contains two semantic trees, corresponding to the two sentences.",3.1 DOL: Meaning representation language,[0],[0]
"Node types of a DOL tree include constants, classes, and functions.",3.1.1 Node types,[0],[0]
"Each interim node of a tree is always a function; and each leaf node can be a constant, a class, or a zero-argument function.
",3.1.1 Node types,[0],[0]
Constants in DOL refer to specific objects in the world.,3.1.1 Node types,[0],[0]
"A constant can be a number (e.g., 3.57), a lexical string (like “New York”), or an entity.
",3.1.1 Node types,[0],[0]
Classes: An entity class refers to a category of entities sharing common semantic properties.,3.1.1 Node types,[0],[0]
"For example, all cities are represented by the class location.city; and math.number is a class for all numbers.",3.1.1 Node types,[0],[0]
"It is clear that,
3.14159 ∈ math.number city.new_york ∈",3.1.1 Node types,[0],[0]
"location.city
A class C1 is a sub-class (denoted by ⊆) of another class C2 if and only if every instance of C1 are in C2.",3.1.1 Node types,[0],[0]
"The following holds according to common sense knowledge,
math.number ⊆ math.expression person.pianist ⊆ person.performer
Template classes are classes with one or more parameters, just like template classes in C++.",3.1.1 Node types,[0],[0]
"The most important template class in DOL is
t.list<c,m,n>
where c is a class; m and n are integers.",3.1.1 Node types,[0],[0]
Each instance of this class is a list containing at least m and at most n elements of type c.,3.1.1 Node types,[0],[0]
"For example, each instance of t.list<math.number,2,+∞> is a list containing at least 2 numbers.
",3.1.1 Node types,[0],[0]
Functions are used in DOL as the major way to form larger language units from smaller ones.,3.1.1 Node types,[0],[0]
"A function is comprised of a name, a list of core arguments, and a return type.",3.1.1 Node types,[0],[0]
DOL enables function overloading (again borrowing ideas from programming languages).,3.1.1 Node types,[0],[0]
"That is, one function name can have multiple core-argument specifications.",3.1.1 Node types,[0],[0]
"Below are two specifications for fn.math.sum (which appears in the example of Figure 2).
",3.1.1 Node types,[0],[0]
"nf.math.sum!1:
$1: math.expression; $2: math.expression return type: math.expression return value: The sum of its arguments
nf.math.sum!2:
$1: t.list<math.expression,2,+∞> return type: math.expression return value: The sum of the elements in $1
Here “$1: math.expression” means the first ar-
gument has type math.expression.
",3.1.1 Node types,[0],[0]
"DOL supports three kinds of functions: noun functions, verb functions, and modifier functions.
",3.1.1 Node types,[0],[0]
Noun functions map entities to their properties or to other entities having specific relations with the argument(s).,3.1.1 Node types,[0],[0]
"For example, nf.math.sum maps math expressions to their sum.",3.1.1 Node types,[0],[0]
Noun functions are used to represent noun phrases in natural language text.,3.1.1 Node types,[0],[0]
"More noun functions are shown in Table 1.
",3.1.1 Node types,[0],[0]
"Among all noun functions, nf.list has a special important position due to its high frequency in DOL trees.",3.1.1 Node types,[0],[0]
"The function is specified below,
nf.list
$1: class; $2: math.number return type: t.list<$1> return value: An entity list with cardinality $2
and element type $1
For example nf.list(math.number,5) returns a list containing 5 elements of type math.number.",3.1.1 Node types,[0],[0]
"It is the semantic representation of “five numbers”.
",3.1.1 Node types,[0],[0]
Pronoun functions are special zero-argument noun functions.,3.1.1 Node types,[0],[0]
"Examples are nf.it (representing an already-mentioned entity or event) and nf.what (denoting an unknown entity or entity list).
",3.1.1 Node types,[0],[0]
Verb functions act as sentences or sub-sentences in DOL.,3.1.1 Node types,[0],[0]
"As an example, vf.be.equ (in Figure 2) is a verb function that has two arguments of the quantity type.
vf.be.equ
$1: quantity.generic; $2: quantity.generic return type:",3.1.1 Node types,[0],[0]
"t.vf Meaning: Two quantities $1 and $2 have the
same value
In addition to core arguments ($1, $2, etc.), many functions can take additional extended arguments as their modifiers.",3.1.1 Node types,[0],[0]
"Our last function type called modifier functions often take the role of extended arguments, to modify noun functions, verb functions, or other modifier functions.",3.1.1 Node types,[0],[0]
"Modifier functions are used in DOL as the semantic representation of adjectives, adverb phrases (including conjunctive adverb phrases), and prepositional phrases in natural languages.",3.1.1 Node types,[0],[0]
"In the example of Figure 2, the function mf.number.even modifies the noun function nf.list as its extended argument.",3.1.1 Node types,[0],[0]
Variables are assigned to DOL sub-trees for indicating the co-reference of sub-trees to entities and for facilitating the construction of logical forms and math expressions from DOL.,3.1.2 Entity variables,[0],[0]
"In Figure 2, the same variable v1 (meaning a variable with ID 1) is assigned to two sub-trees in the first sentence
and one sub-tree in the second sentence.",3.1.2 Entity variables,[0],[0]
Thus the three sub-trees refer to the same entity.,3.1.2 Entity variables,[0],[0]
DOL has some nice characteristics that are critical to building a high-precision math problem solving system.,3.1.3 Key features of DOL,[0],[0]
"That is why we invent DOL as our meaning representation language instead of employing an existing one.
",3.1.3 Key features of DOL,[0],[0]
"First, DOL is a strongly typed language.",3.1.3 Key features of DOL,[0],[0]
Every function has clearly defined argument types and a return type.,3.1.3 Key features of DOL,[0],[0]
"A valid DOL tree must satisfy the type-compatibility property:
Type-compatibility: The type of each child of a function node should match the corresponding argument type of the function.
",3.1.3 Key features of DOL,[0],[0]
"For example, in Figure 2, the return type of nf.math.power is math.expression, which matches the second argument of vf.be.equ.",3.1.3 Key features of DOL,[0],[0]
"However, the following two trees (yielded from the corresponding pieces of text) are invalid because they do not satisfy type-compatibility.
sum of 100 [unreasonable text] nf.math.sum!2(100)",3.1.3 Key features of DOL,[0],[0]
"[invalid DOL tree] sum of 3 and Jordan [unreasonable text] nf.math.sum!2({3, “Jordan”})",3.1.3 Key features of DOL,[0],[0]
"[invalid tree]
Second, we maintain in DOL an open-domain type system.",3.1.3 Key features of DOL,[0],[0]
The type system contains over 1000 manually verified classes and more automatically generated ones (refer to Section 3.2.1 for more details).,3.1.3 Key features of DOL,[0],[0]
Such a comprehensive type system makes it possible to define various kinds of functions and to perform type-compatibility checking.,3.1.3 Key features of DOL,[0],[0]
"In contrast, most previous semantic languages have at most 100+ types at the grammar level.",3.1.3 Key features of DOL,[0],[0]
"In addition, by introducing template classes, we avoid maintaining a lot of potentially duplicate types and reduce the type system management efforts.",3.1.3 Key features of DOL,[0],[0]
"To the best of our knowledge, template classes are not
available in other semantic representation languages.
",3.1.3 Key features of DOL,[0],[0]
"Third, DOL has built-in data structures like t.list and nf.list which greatly facilitate both function declaration and text representation (especially math text representation).",3.1.3 Key features of DOL,[0],[0]
"For example, the two variants of nf.math.sum (refer to Section 3.1.1 for their specifications) are enough to represent the following English phrases:
3 plus 5  nf.math.sum!1(3, 5) sum of 3, 5, 7, and 9  nf.math.sum!2(nf.list(3, 5, 7, 9)) sum of ten thousand numbers  nf.math.sum!2(nf.list(math.number,10000))
",3.1.3 Key features of DOL,[0],[0]
"Without t.list or nf.list, we would have to define a lot of overloaded functions for nf.math.sum to deal with different numbers of addends.",3.1.3 Key features of DOL,[0],[0]
"Among all meaning representation languages, AMR (Banarescu et al., 2013) is most similar to DOL.",3.1.4 Comparing with other languages,[0],[0]
"Their major differences are: First, they use very different mechanisms to represent noun phrases.",3.1.4 Comparing with other languages,[0],[0]
"In AMR, a sentence (e.g., “the boy destroyed the room”) and a noun phrase (e.g., “the boy’s destruction of the room”) can have the same representation.",3.1.4 Comparing with other languages,[0],[0]
"While in DOL, a sentence is always represented by a verb function; and a noun phrase is always a noun function or a constant.",3.1.4 Comparing with other languages,[0],[0]
"Second, DOL has a larger type system and is stricter in type compatibility checking.",3.1.4 Comparing with other languages,[0],[0]
"Third, DOL has template classes and built-in data structures like t.list and nf.list to facilitate the representation of math concepts.
",3.1.4 Comparing with other languages,[0],[0]
"CCG (Steedman, 2000) provides a transparent interface between syntax and semantics.",3.1.4 Comparing with other languages,[0],[0]
"In CCG, semantic information is defined on words (e.g., “λx.odd(x)” for “odd” and “λx.number(x)” for “number”).",3.1.4 Comparing with other languages,[0],[0]
"In contrast, DOL explicitly connects NL text patterns to semantic elements.",3.1.4 Comparing with other languages,[0],[0]
"For example, as shown in Table 2 (Section 3.2.1), one CFG grammar rule connects pattern “{$1} raised to the power of {$2}” to function nf.math.power.
",3.1.4 Comparing with other languages,[0],[0]
Logical forms are another way of meaning representation.,3.1.4 Comparing with other languages,[0],[0]
"We choose not to transform NL text directly to logical forms for two reasons: On one hand, state-of-the-art methods for mapping NL text into logical forms typically target short, onesentence queries in restricted domains.",3.1.4 Comparing with other languages,[0],[0]
"However, many math word problems are long and contain multiple sentences.",3.1.4 Comparing with other languages,[0],[0]
"On the other hand, variable-id assignment is a big issue in direct logical form construction for many math problems.",3.1.4 Comparing with other languages,[0],[0]
"Let’s use
the following problem (i.e., the first problem of Figure 1) to illustrate,
One number is 16 more than another.",3.1.4 Comparing with other languages,[0],[0]
"If the smaller number is subtracted from 2/3 of the larger, the result is 1/4 of the sum of the two numbers.",3.1.4 Comparing with other languages,[0],[0]
"Find the numbers.
",3.1.4 Comparing with other languages,[0],[0]
"For this problem, it is difficult to determine whether “the smaller number” refers to “one number” or “another” in directly constructing logical forms.",3.1.4 Comparing with other languages,[0],[0]
"It is therefore a challenge to construct a correct logical form for such kinds of problems.
",3.1.4 Comparing with other languages,[0],[0]
Our solution to the above challenge is assigning a new variable ID (which is different from the IDs of “one number” and “another”) and to delay the final variable-ID assignment to the reasoning stage.,3.1.4 Comparing with other languages,[0],[0]
"To enable this mechanism, the meaning representation language should support a lazy variable ID assignment and keep as much information (e.g., determiners, plurals, modifiers) from the noun phrases as possible.",3.1.4 Comparing with other languages,[0],[0]
"DOL is a language that always keeps the structure information of phrases, whether or not it has been assigned a variable ID.
",3.1.4 Comparing with other languages,[0],[0]
"In summary, compared with other languages, DOL has some unique features which make it more suitable for our math problem solving scenario.",3.1.4 Comparing with other languages,[0],[0]
"Our parsing algorithm is based on context-free grammar (CFG) (Chomsky, 1956; Backus, 1959; Jurafsky & Martin, 2000), a commonly used mathematical system for modeling constituent structure in natural languages.
",3.2 Semantic Parsing,[0],[0]
"3.2.1 CFG for connecting DOL and NL
The core part of a CFG is the set of grammar rules.",3.2 Semantic Parsing,[0],[0]
"Example English grammar rules for build-
ing syntactic parsers include “S → NP VP”, “NP →
CD | DT NN | NP PP”, etc.",3.2 Semantic Parsing,[0],[0]
Table 2 shows some example CFG rules in our system for mapping DOL nodes to natural language word sequences.,3.2 Semantic Parsing,[0],[0]
"The left side of each rule is a DOL element (a function, class, or constant); and the right side is a sequence of words and arguments.",3.2 Semantic Parsing,[0],[0]
"The grammar rules are consumed by our parser for building DOL trees from NL text.
",3.2 Semantic Parsing,[0],[0]
"So far there are 9,600 grammar rules in our system.",3.2 Semantic Parsing,[0],[0]
"For every DOL node type, the lexicon and grammar rules are constructed together in a semiautomatic way.",3.2 Semantic Parsing,[0],[0]
"Math-related classes, functions, and constants and their grammar rules are manually built by referring to text books and online tu-
torials.",3.2 Semantic Parsing,[0],[0]
About 35 classes and 200 functions are obtained in this way.,3.2 Semantic Parsing,[0],[0]
"Additional instances of each element type are constructed in the ways below.
",3.2 Semantic Parsing,[0],[0]
"Classes: Additional classes and grammar rules are obtained from two data sources: Freebase 3 types, and automatically extracted lexical semantic data.",3.2 Semantic Parsing,[0],[0]
"By treating Freebase types as DOL classes and the mapping from types to lexical names as grammar rules, we get the first version of grammar for classes.",3.2 Semantic Parsing,[0],[0]
"To improve coverage, we run a term peer similarity and hypernym extraction algorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer-similarity graph and a collection of is-a pairs.",3.2 Semantic Parsing,[0],[0]
"An is-a pair example is (Megan Fox, actress), where “Megan Fox” and “actress” are instance and type names respectively.",3.2 Semantic Parsing,[0],[0]
"In our peer similarity graph, “Megan Fox” and “Britney Spears” have a high similarity score.",3.2 Semantic Parsing,[0],[0]
The peer similarity graph is used to clean the is-a data collection (with the idea that peer terms often share some common type names).,3.2 Semantic Parsing,[0],[0]
"Given the cleaned isa data, we sort the type names by weight and manually create classes for top-1000 type names.",3.2 Semantic Parsing,[0],[0]
"For example, create a class person.actress and add a grammar rule “person.actress → actress”.",3.2 Semantic Parsing,[0],[0]
"For the other 2000 type names in the top 3000, we create classes and rules automatically, in the form of “class.",3.2 Semantic Parsing,[0],[0]
"TN → TN”, where TN is a type name.",3.2 Semantic Parsing,[0],[0]
"For example, create rule “class.succulent → succulent” for name “succulent”.
",3.2 Semantic Parsing,[0],[0]
"3 Freebase: http://www.freebase.com/
Functions: Additional noun functions are automatically created from Freebase properties and attribute extraction results (Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and isa extraction results.",3.2 Semantic Parsing,[0],[0]
We have over 50 manually defined math-related verb functions.,3.2 Semantic Parsing,[0],[0]
"Our future plan is automatically generating verb functions from databases like PropBank (Kingsbury & Palmer, 2002), FrameNet (Fillmore et al., 2003), and VerbNet4 (Schuler, 2005).",3.2 Semantic Parsing,[0],[0]
"Additional modifier functions are automatically created from an English adjective and adverb list, in the form of “mf.adj.",3.2 Semantic Parsing,[0],[0]
TN → TN” and “mf.adv.,3.2 Semantic Parsing,[0],[0]
"TN → TN” where TN is the name of an adjective or adverb.
",3.2 Semantic Parsing,[0],[0]
"3.2.2 Parsing
Parsing for CFG is a well-studied topic with lots of algorithms invented (Kasami, 1965; Earley, 1970).",3.2 Semantic Parsing,[0],[0]
The core idea behind almost all the algorithms is exploiting dynamic programming to achieve efficient search through the space of possible parse trees.,3.2 Semantic Parsing,[0],[0]
"For syntactic parsing, a wellknown serious problem is ambiguity: the appearance of many syntactically correct but semantically unreasonable parse trees.",3.2 Semantic Parsing,[0],[0]
"Modern syntactic parsers reply on statistical information to reduce
4 VerbNet: http://verbs.colorado.edu/~mpalmer/projects/verbnet.html
ambiguity.",3.2 Semantic Parsing,[0],[0]
"They are often based on probabilistic CFGs (PCFGs) or probabilistic lexicalized CFGs trained on hand-labeled TreeBanks.
",3.2 Semantic Parsing,[0],[0]
"With the new set of DOL-NL grammar rules (examples in Table 2) and the type-compatibility property (Section 3.1.3), ambiguity can hopefully be greatly reduced, because semantically unreasonable parsing often results in invalid DOL trees.
",3.2 Semantic Parsing,[0],[0]
"We implement a top-down parser for our new CFG of Section 3.2.1, following the Earley algorithm (Earley, 1970).",3.2 Semantic Parsing,[0],[0]
No probabilistic information is attached in the grammar rules because no Treebanks are available for learning statistical probabilities for the new CFG.,3.2 Semantic Parsing,[0],[0]
Figure 3 shows the parse tree returned by our parser when processing a simple sentence.,3.2 Semantic Parsing,[0],[0]
The DOL tree can be obtained by removing the dotted lines (corresponding to the non-argument part in the right side of the grammar rules).,3.2 Semantic Parsing,[0],[0]
"A traditional syntactic parse tree is shown in Figure 4 for reference.
",3.2 Semantic Parsing,[0],[0]
"During parsing, a score is calculated for each DOL node.",3.2 Semantic Parsing,[0],[0]
"The score of a tree T is the weighted average of the scores of its sub-trees,
𝑺(𝑻) =
∑ 𝑳(𝑻𝒊) ∙ 𝑺(𝑻𝒊) 𝒌 𝒊=𝟏
∑ 𝑳(𝑻𝒊) 𝒌 𝒊=𝟏
∙ 𝒑(𝑻)",3.2 Semantic Parsing,[0],[0]
"(3)
where 𝑇𝑖 is a sub-tree, and 𝐿(𝑇𝑖) is the number of words to which the sub-tree corresponds in the original text.",3.2 Semantic Parsing,[0],[0]
"If the type-compatibility property for T is satisfied, 𝑝(𝑇)=1; otherwise 𝑝(𝑇)=0.",3.2 Semantic Parsing,[0],[0]
"All leaf nodes are assigned a score of 1.0, except for pure lexical string nodes (which are used as named entity names).",3.2 Semantic Parsing,[0],[0]
"The score of a lexical string node is set to 1/(1+𝜇n), where n is the number of words in the node, and 𝜇 (=0.2 in experiments) is a parameter whose value does not have much impact on parsing results.",3.2 Semantic Parsing,[0],[0]
"Such a score function encourages interpreting a word sequence with our grammar than treating it as an entity name.
",3.2 Semantic Parsing,[0],[0]
"Among all candidate DOL trees yielded during parsing, we return the one with the highest score as the final parsing result.",3.2 Semantic Parsing,[0],[0]
A null tree is returned if the highest score is zero.,3.2 Semantic Parsing,[0],[0]
The reasoning module is responsible for deriving math expressions from DOL trees and calculating problem answers by solving equation systems.,3.3 Reasoning,[0],[0]
Math expressions have different definitions in different contexts.,3.3 Reasoning,[0],[0]
"In some definitions, equations and inequations are excluded from math expressions.",3.3 Reasoning,[0],[0]
"In this paper, equations and inequations (like “a=b” and “ax+b>0”) are called s-expressions because they represent mathematical sentences,
while other math expressions (like “x+5”) are named n-expressions since they are essentially noun phrases.",3.3 Reasoning,[0],[0]
"Our definition of “math expressions” therefore includes both n-expressions and s-expressions.
",3.3 Reasoning,[0],[0]
Different types of nodes may generate different types of math expressions.,3.3 Reasoning,[0],[0]
"In most cases, s-expressions are derived from verb function nodes and modifier function nodes, while n-expressions are generated from constants and noun function nodes.",3.3 Reasoning,[0],[0]
"For example, the s-expression “9+x=314” can be derived from the DOL tree of Figure 3, if variable x represents the integer.",3.3 Reasoning,[0],[0]
"In the same Figure, The n-expression “9+x” is derived from the left sub-tree.
",3.3 Reasoning,[0],[0]
The pseudo-codes of our math expression derivation algorithm are shown in Figure 5.,3.3 Reasoning,[0],[0]
"The algorithm generates the math expression for a DOL tree T by first calling the expression derivation procedure of sub-trees, and then applying the semantic interpretation of T. All the s-expressions derived so far are stored in an expression list named XL.
",3.3 Reasoning,[0],[0]
The semantic interpretation of DOL nodes plays a critical role in the algorithm.,3.3 Reasoning,[0],[0]
Table 3 shows some example interpretations of some representative DOL functions.,3.3 Reasoning,[0],[0]
"In the table, $1, $2 etc. are function arguments, and $↑ for a modifier node denotes the node which the modifier modifies.",3.3 Reasoning,[0],[0]
So far the semantic interpretations are built manually.,3.3 Reasoning,[0],[0]
"Please note that it is not necessary to make semantic interpretations for every DOL
node in solving number word problems.",3.3 Reasoning,[0],[0]
"For example, most class nodes and many adverb nodes can have null interpretations at the moment.",3.3 Reasoning,[0],[0]
"Datasets: Our problem collection5 contains 1,878 math number word problems, collected from two web sites: algebra.com6 (a web site for users to post math problems and get help from tutors) and answers.yahoo.com7.",4.1 Experimental setup,[0],[0]
Problems on both sites are organized into categories.,4.1 Experimental setup,[0],[0]
"For algebra.com, problems are randomly sampled from the number word problems category; for answers.yahoo.com, we first randomly sample an initial set of problems from the math category and then ask human annotators to manually choose number word problems from them.",4.1 Experimental setup,[0],[0]
"Math equations 8 and answers to the problems are manually added by human annotators.
",4.1 Experimental setup,[0],[0]
We randomly split the dataset into a dev set (for algorithm design and debugging) and a test set.,4.1 Experimental setup,[0],[0]
More subsets are extracted to meet the requirements of the baseline methods (see below).,4.1 Experimental setup,[0],[0]
"Table 4 shows the statistics of the datasets.
",4.1 Experimental setup,[0],[0]
"Baseline methods: We compare our approach with two baselines: KAZB (Kushman et al., 2014) and BasicSim.
",4.1 Experimental setup,[0],[0]
KAZB is a learning-based statistical method which solves a problem by mapping it to one of the equation templates determined by the annotated equations in the training data.,4.1 Experimental setup,[0],[0]
"We run the ALLEQ version of their algorithm since it performs much better than the other two (i.e., 5EQ and 5EQ+ANS).",4.1 Experimental setup,[0],[0]
Their codes support only linear equations and require that there are at least two problems for each equation template (otherwise an exception will be thrown).,4.1 Experimental setup,[0],[0]
"By choosing problems from the collection that meet these requirements, we build a sub-dataset called LinearT2.",4.1 Experimental setup,[0],[0]
"In the dataset of KAZB, each equation template corresponds to at least 6 problems.",4.1 Experimental setup,[0],[0]
"So we form another sub-dataset called LinearT6 by removing from the test set the problems for which the associated equation template appears less than 6 times.
",4.1 Experimental setup,[0],[0]
"BasicSim is a simple statistical method which works by computing the similarities between a testing problem and those in the training set, and then applying the equations of the most similar problem.",4.1 Experimental setup,[0],[0]
"This method has similar performance
5 Available from http://research.microsoft.com/en-us/projects/dolphin/ 6 http://www.algebra.com
with KAZB on their dataset, but does not have the two limitations mentioned above.",4.1 Experimental setup,[0],[0]
"Therefore we adopt it as the second baseline.
",4.1 Experimental setup,[0],[0]
"For both baselines, experiments are conducted using 5-fold cross-validation with the dev set always included in the training data.",4.1 Experimental setup,[0],[0]
"In other words, we always use the dev set and 4/5 of the test set as training data for each fold.
",4.1 Experimental setup,[0],[0]
Evaluation metrics:,4.1 Experimental setup,[0],[0]
Evaluation is performed in the setting that a system can choose NOT to answer all problems in the test set.,4.1 Experimental setup,[0],[0]
"In other words, one has the flexibility of generating answers only when she knows how to solve it or she is confident about her answer.",4.1 Experimental setup,[0],[0]
"In this setting, the following three metrics are adopted in reporting evaluation results (assuming, in a test set of size n, a system generates answers for m problems, where k of them are correct):
Precision: k/m Recall (or coverage): k/n",4.1 Experimental setup,[0],[0]
F1: 2PR/(P+R) = 2k/(m+n),4.1 Experimental setup,[0],[0]
"The Overall evaluation results are summarized in Table 5, where “Dolphin” represents our approach.",4.2 Experimental results,[0],[0]
"The results show that our approach significantly outperforms (with p<<0.01 according to two-tailed t-test) the two baselines on every test set, in terms of precision, recall, and F-measure.",4.2 Experimental results,[0],[0]
Our approach achieves a particularly high precision of 95%.,4.2 Experimental results,[0],[0]
"That means once an answer is provided by our approach, it has a very high probability of being correct.
",4.2 Experimental results,[0],[0]
Please note that our grammar rules and parsing algorithm are NOT tuned for the evaluation data.,4.2 Experimental results,[0],[0]
"Only the dev set is referred to in system building.
",4.2 Experimental results,[0],[0]
"7 https://answers.yahoo.com/ 8 Math equations are used in the baseline approaches as part of training data.
",4.2 Experimental results,[0],[0]
"Since the baselines generate results for all problems, the precision, recall, and F1 are all the same for each dataset.
",4.2 Experimental results,[0],[0]
"The reason for such a high precision is that, by transforming NL text to DOL trees, the system “understands” the problem (or has structured and accurate information about quantity relations).",4.2 Experimental results,[0],[0]
Therefore it is more likely to generate correct results than statistical methods who simply “guess” according to features.,4.2 Experimental results,[0],[0]
"By examining the problems in the dev set that we cannot generate answers, we find that most of them are due to empty parsing results.
",4.2 Experimental results,[0],[0]
"On the other hand, statistical approaches have the advantage of generating answers without understanding the semantic meaning of problems (as long as there are similar problems in the training data).",4.2 Experimental results,[0],[0]
"So they are able to handle (with probably low precision) problems that are complex in terms of language and logic.
",4.2 Experimental results,[0],[0]
Please pay attention that our experimental results reported here are on number word problems.,4.2 Experimental results,[0],[0]
"General math word problems are much harder to our approach because the entity types, properties, relations, and actions contained in general word problems are much larger in quantity and more complex in quality.",4.2 Experimental results,[0],[0]
We are working on extending our approach to general math word problems.,4.2 Experimental results,[0],[0]
"Now our DOL language and CFG grammar already have a good coverage on common entity types, but the coverage on properties, relations, and actions is quite limited.",4.2 Experimental results,[0],[0]
"As a result, our parser fails to parse many sentences in general math word problems because they contain properties, relations or actions that are unknown to our system.",4.2 Experimental results,[0],[0]
"We also observe that sometimes we are able to parse a problem successfully, but cannot derive math expressions in the reasoning stage.",4.2 Experimental results,[0],[0]
This is often because some relations or actions in the problem are not modeled appropriately.,4.2 Experimental results,[0],[0]
"As future work, we plan to extend our DOL lexicon and
grammar to improve the coverage of properties, relations, and actions.",4.2 Experimental results,[0],[0]
We also plan to study the mechanism of modeling relations and actions.,4.2 Experimental results,[0],[0]
We proposed a semantic parsing and reasoning approach to automatically solve math number word problems.,5 Conclusion,[0],[0]
We have designed a new meaning representation language DOL to bridge NL text and math expressions.,5 Conclusion,[0],[0]
A CFG parser is implemented to parse NL text to DOL trees.,5 Conclusion,[0],[0]
"A reasoning module is implemented to derive math expressions from DOL trees, by applying the semantic interpretation of DOL nodes.",5 Conclusion,[0],[0]
"We achieve a high precision and a reasonable recall on our test set of over 1,500 problems.",5 Conclusion,[0],[0]
We hope to extend our techniques to handling general math word problems and to other domains (like physics and chemistry) in the future.,5 Conclusion,[0],[0]
We would like to thank the annotators for their efforts in assigning math equations and answers to the problems in our dataset.,Acknowledgments,[0],[0]
"Thanks to the anonymous reviewers for their helpful comments and suggestions.
",Acknowledgments,[0],[0]
"Reference
J.W. Backus. 1959.",Acknowledgments,[0],[0]
"The syntax and semantics of the
proposed international algebraic language of the Zurich ACM-GAMM conference.",Acknowledgments,[0],[0]
"Proceedings of the International Conference on Information Processing, 1959.
",Acknowledgments,[0],[0]
Y. Bakman.,Acknowledgments,[0],[0]
2007.,Acknowledgments,[0],[0]
"Robust understanding of word prob-
lems with extraneous information.",Acknowledgments,[0],[0]
http://arxiv.org/ abs/math/0701393.,Acknowledgments,[0],[0]
"Accessed Feb. 2nd, 2015.
",Acknowledgments,[0],[0]
"C. Baker, M. Ellsworth, and K. Erk. 2007.",Acknowledgments,[0],[0]
"SemEval-
2007 Task 19: Frame semantic structure extraction.",Acknowledgments,[0],[0]
"In Proceedings of SemEval.
",Acknowledgments,[0],[0]
"L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K.
Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider. 2013.",Acknowledgments,[0],[0]
Abstract meaning representation for sembanking.,Acknowledgments,[0],[0]
"In Proc. of the Linguistic Annotation Workshop and Interoperability with Discourse.
",Acknowledgments,[0],[0]
"J. Berant, A. Chou, R. Frostig, and P. Liang. 2013.",Acknowledgments,[0],[0]
"Se-
mantic parsing on Freebase from question-answer pairs.",Acknowledgments,[0],[0]
"In Empirical Methods in Natural Language Processing (EMNLP).
",Acknowledgments,[0],[0]
J. Berant and P. Liang. 2014.,Acknowledgments,[0],[0]
"Semantic Parsing via Par-
aphrasing.",Acknowledgments,[0],[0]
"In ACL'2014.
",Acknowledgments,[0],[0]
D.G. Bobrow. 1964a.,Acknowledgments,[0],[0]
"Natural language input for a
computer problem solving system.",Acknowledgments,[0],[0]
"Report MACTR-1, Project MAC, MIT, Cambridge, June
D.G. Bobrow. 1964b.",Acknowledgments,[0],[0]
"Natural language input for a
computer problem solving system.",Acknowledgments,[0],[0]
Ph.D.,Acknowledgments,[0],[0]
"Thesis, Department of Mathematics, MIT, Cambridge
D.L. Briars, J.H. Larkin. 1984.",Acknowledgments,[0],[0]
"An integrated model of
skill in solving elementary word problems.",Acknowledgments,[0],[0]
"Cognition and Instruction, 1984, 1 (3) 245-296.
",Acknowledgments,[0],[0]
Q. Cai and A. Yates.,Acknowledgments,[0],[0]
2013.,Acknowledgments,[0],[0]
"Large-scale semantic pars-
ing via schema matching and lexicon extension.",Acknowledgments,[0],[0]
"In Association for Computational Linguistics (ACL).
X. Carreras.",Acknowledgments,[0],[0]
and L. Marquez. 2004.,Acknowledgments,[0],[0]
"Introduction to the
CoNLL-2004 shared task: Semantic role labeling.",Acknowledgments,[0],[0]
"In Proceedings of CoNLL.
E. Charniak. 1968.",Acknowledgments,[0],[0]
"CARPS: a program which solves
calculus word problems.",Acknowledgments,[0],[0]
"Report MAC-TR-51, Project MAC, MIT, Cambridge, July
E. Charniak. 1969.",Acknowledgments,[0],[0]
"Computer solution of calculus word
problems.",Acknowledgments,[0],[0]
In Proceedings of international joint conference on artificial intelligence.,Acknowledgments,[0],[0]
"Washington, DC, pp 303–316
N. Chomsky. 1956.",Acknowledgments,[0],[0]
"Three models for the description of
language.",Acknowledgments,[0],[0]
"Information Theory, IRE Transactions on, 2(3), 113-124.
S. Clark, and J. Curran. 2007.",Acknowledgments,[0],[0]
"Wide-coverage efficient
statistical parsing with CCG and log-linear models.",Acknowledgments,[0],[0]
"Computational Linguistics, 33(4):493-552.
D. Das, D. Chen, A.F.T. Martins, N. Schneider and
N.A. Smith. 2014.",Acknowledgments,[0],[0]
Frame-Semantic Parsing.,Acknowledgments,[0],[0]
"Computational Linguistics 40:1, pages 9-56
D. Dellarosa.",Acknowledgments,[0],[0]
1986.,Acknowledgments,[0],[0]
"A computer simulation of chil-
dren’s arithmetic word problem solving.",Acknowledgments,[0],[0]
"Behavior Research Methods, Instruments, & Computers, 18:147–154
V. Durme, T. Qian, and L. Schubert. 2008.",Acknowledgments,[0],[0]
"Class-
driven attribute extraction.",Acknowledgments,[0],[0]
"In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pp. 921-928.",Acknowledgments,[0],[0]
"Association for Computational Linguistics, 2008.
",Acknowledgments,[0],[0]
J. Earley.,Acknowledgments,[0],[0]
1970.,Acknowledgments,[0],[0]
"An efficient context-free parsing algo-
rithm.",Acknowledgments,[0],[0]
"Communications of the ACM, 13(2), 94-102.
",Acknowledgments,[0],[0]
"C.J. Fillmore, C.R. Johnson, and M.R. Petruck. 2003.
",Acknowledgments,[0],[0]
Background to FrameNet.,Acknowledgments,[0],[0]
"International Journal of Lexicography, 16(3).
",Acknowledgments,[0],[0]
C.R. Fletcher.,Acknowledgments,[0],[0]
1985.,Acknowledgments,[0],[0]
"Understanding and solving arith-
metic word problems: a computer simulation.",Acknowledgments,[0],[0]
"Behavior Research Methods, Instruments, & Computers, 17:565–571
D. Gildea, and D. Jurafsky. 2002.",Acknowledgments,[0],[0]
"Automatic labeling
of semantic roles.",Acknowledgments,[0],[0]
"Computational Linguistics, 28(3).
",Acknowledgments,[0],[0]
M. Hearst. 1992.,Acknowledgments,[0],[0]
"Automatic Acquisition of Hyponyms
from Large Text Corpora.",Acknowledgments,[0],[0]
"In Fourteenth International Conference on Computational Linguistics, Nantes, France.
M.J. Hosseini, H. Hajishirzi, O. Etzioni, and N. Kush-
man.",Acknowledgments,[0],[0]
2014.,Acknowledgments,[0],[0]
Learning to Solve Arithmetic Word Problems with Verb Categorization.,Acknowledgments,[0],[0]
"In EMNLP’2014.
D. Jurafsky, and J.H. Martin. 2000.",Acknowledgments,[0],[0]
"Speech & language
processing.",Acknowledgments,[0],[0]
"Pearson Education India.
",Acknowledgments,[0],[0]
T. Kasami.,Acknowledgments,[0],[0]
1965.,Acknowledgments,[0],[0]
"An efficient recognition and syntax-
analysis algorithm for context-free languages (Technical report).",Acknowledgments,[0],[0]
AFCRL.,Acknowledgments,[0],[0]
"65-758.
",Acknowledgments,[0],[0]
"P. Kingsbury, and M. Palmer. 2002.",Acknowledgments,[0],[0]
"From TreeBank to
PropBank.",Acknowledgments,[0],[0]
"In Proceedings of LREC.
N. Kushman, Y. Artzi, L. Zettlemoyer, and R. Barzi-
lay.",Acknowledgments,[0],[0]
2014.,Acknowledgments,[0],[0]
Learning to automatically solve algebra word problems.,Acknowledgments,[0],[0]
"In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer.
2013.",Acknowledgments,[0],[0]
Scaling semantic parsers with on-the-fly ontology matching.,Acknowledgments,[0],[0]
"In Empirical Methods in Natural Language Processing (EMNLP).
",Acknowledgments,[0],[0]
"I. Lev, B. MacCartney, C. Manning, and R. Levy.
2004.",Acknowledgments,[0],[0]
Solving logic puzzles: From robust processing to precise semantics.,Acknowledgments,[0],[0]
In Proceedings of the Workshop on Text Meaning and Interpretation.,Acknowledgments,[0],[0]
"Association for Computational Linguistics.
C. Liguda, T. Pfeiffer. 2012.",Acknowledgments,[0],[0]
"Modeling Math Word
Problems with Augmented Semantic Networks.",Acknowledgments,[0],[0]
"NLDB’2012, pp. 247-252.
Y. Ma, Y. Zhou, G. Cui, R. Yun, R. Huang.",Acknowledgments,[0],[0]
"2010.
",Acknowledgments,[0],[0]
Frame-based calculus of solving arithmetic multistep addition and subtraction word problems.,Acknowledgments,[0],[0]
"In International Workshop on Education Technology and Computer Science, vol. 2, pp. 476–479.
L. Marquez, X. Carreras, K.C. Litkowski, and S. Ste-
venson. 2008.",Acknowledgments,[0],[0]
Semantic role labeling: an introduction to the special issue.,Acknowledgments,[0],[0]
"Computational Linguistics, 34(2).
",Acknowledgments,[0],[0]
A. Mukherjee and U. Garain. 2008.,Acknowledgments,[0],[0]
"A review of meth-
ods for automatic understanding of natural language mathematical problems.",Acknowledgments,[0],[0]
"Artificial Intelligence Review, 29(2).
",Acknowledgments,[0],[0]
"M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006.",Acknowledgments,[0],[0]
Organizing and searching the world wide web of facts-step one: the one-million fact extraction challenge.,Acknowledgments,[0],[0]
"In AAAI (Vol. 6, pp. 1400-1405).
K.K. Schuler. 2005.",Acknowledgments,[0],[0]
VerbNet:,Acknowledgments,[0],[0]
"A broad-coverage, com-
prehensive verb lexicon.",Acknowledgments,[0],[0]
Dissertation.,Acknowledgments,[0],[0]
"http://repository.upenn.edu/dissertations/AAI3179808
S. Shi, H. Zhang, X. Yuan, and J.-R. Wen. 2010.",Acknowledgments,[0],[0]
"Cor-
pus-based semantic class mining: distributional vs. pattern-based approaches.",Acknowledgments,[0],[0]
"In Proceedings of the 23rd International Conference on Computational Linguistics, pages 993–1001.",Acknowledgments,[0],[0]
"Association for Computational Linguistics.
M. Steedman. 2000.",Acknowledgments,[0],[0]
The Syntactic Process.,Acknowledgments,[0],[0]
"The MIT
Press.
",Acknowledgments,[0],[0]
Y. W. Wong and R. J. Mooney. 2007.,Acknowledgments,[0],[0]
"Learning syn-
chronous grammars for semantic parsing with lambda calculus.",Acknowledgments,[0],[0]
"In Association for Computational Linguistics (ACL), pages 960–967.
M. Zelle and R.J. Mooney. 1996.",Acknowledgments,[0],[0]
"Learning to parse da-
tabase queries using inductive logic proramming.",Acknowledgments,[0],[0]
"In Association for the Advancement of Artificial Intelligence (AAAI), pages 1050–1055.
L.S. Zettlemoyer and M. Collins. 2005.",Acknowledgments,[0],[0]
"Learning to
map sentences to logical form: Structured classification with probabilistic categorial grammars.",Acknowledgments,[0],[0]
"In Uncertainty in Artificial Intelligence (UAI), pages 658–666.
L.S. Zettlemoyer and M. Collins. 2007.",Acknowledgments,[0],[0]
"Online Learn-
ing of Relaxed CCG Grammars for Parsing to Logical Form.",Acknowledgments,[0],[0]
"In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).
",Acknowledgments,[0],[0]
"F. Zhang, S. Shi, J. Liu, S. Sun, and C.-Y. Lin. 2011.
",Acknowledgments,[0],[0]
Nonlinear evidence fusion and propagation for hyponymy relation mining.,Acknowledgments,[0],[0]
"In ACL, volume 11, pages 1159–1168.",Acknowledgments,[0],[0]
This paper presents a semantic parsing and reasoning approach to automatically solving math word problems.,abstractText,[0],[0]
A new meaning representation language is designed to bridge natural language text and math expressions.,abstractText,[0],[0]
"A CFG parser is implemented based on 9,600 semi-automatically created grammar rules.",abstractText,[0],[0]
"We conduct experiments on a test set of over 1,500 number word problems (i.e., verbally expressed number problems) and yield 95.4% precision and 60.2% recall.",abstractText,[0],[0]
Automatically Solving Number Word Problems by Semantic Parsing and Reasoning,title,[0],[0]
"In clinical medicine, prognosis refers to the risk of future health outcomes in patients with given features.",1. Introduction,[1.0],"['In clinical medicine, prognosis refers to the risk of future health outcomes in patients with given features.']"
"Prognostic research aims at building actionable predictive models that can inform clinicians about future course of patients’
1University of California, Los Angeles, USA 2University of Oxford, Oxford, UK 3Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Ahmed M. Alaa <ahmedmalaa@ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
clinical conditions in order to guide screening and therapeutic decisions.",1. Introduction,[0],[0]
"With the recent abundance of data linkages, electronic health records, and bio-repositories, clinical researchers have become aware that the value conferred by big, heterogeneous clinical data can only be realized with prognostic models based on flexible machine learning (ML) approaches.",1. Introduction,[1.0],"['With the recent abundance of data linkages, electronic health records, and bio-repositories, clinical researchers have become aware that the value conferred by big, heterogeneous clinical data can only be realized with prognostic models based on flexible machine learning (ML) approaches.']"
"There is, however, a concerning gap between the potential and actual utilization of ML in prognostic research; the reason being that clinicians with no expertise in data science find it hard to manually design and tune ML pipelines (Luo et al., 2017).
",1. Introduction,[0.9999999864528896],"['There is, however, a concerning gap between the potential and actual utilization of ML in prognostic research; the reason being that clinicians with no expertise in data science find it hard to manually design and tune ML pipelines (Luo et al., 2017).']"
"To fill this gap, we developed AUTOPROGNOSIS, an automated ML (AutoML) framework tailored for clinical prognostic modeling.",1. Introduction,[1.0],"['To fill this gap, we developed AUTOPROGNOSIS, an automated ML (AutoML) framework tailored for clinical prognostic modeling.']"
"AUTOPROGNOSIS takes as an input data from a patient cohort, and uses such data to automatically configure ML pipelines.",1. Introduction,[1.0],"['AUTOPROGNOSIS takes as an input data from a patient cohort, and uses such data to automatically configure ML pipelines.']"
"Every ML pipeline comprises all stages of prognostic modeling: missing data imputation, feature preprocessing, prediction, and calibration.",1. Introduction,[1.0],"['Every ML pipeline comprises all stages of prognostic modeling: missing data imputation, feature preprocessing, prediction, and calibration.']"
"The system handles different types of clinical data, including longitudinal and survival (time-to-event) data, and automatically explains its predictions to the clinicians via an “interpreter” module which outputs clinically interpretable associations between patients’ features and predicted risk strata.",1. Introduction,[1.0],"['The system handles different types of clinical data, including longitudinal and survival (time-to-event) data, and automatically explains its predictions to the clinicians via an “interpreter” module which outputs clinically interpretable associations between patients’ features and predicted risk strata.']"
"An overview of the system is provided in Figure 1.
",1. Introduction,[0],[0]
"The core component of AUTOPROGNOSIS is an algorithm for configuring ML pipelines using Bayesian optimization (BO) (Snoek et al., 2012).",1. Introduction,[1.0],"['The core component of AUTOPROGNOSIS is an algorithm for configuring ML pipelines using Bayesian optimization (BO) (Snoek et al., 2012).']"
"Our BO algorithm models the pipelines’ performances as a black-box function, the input to which is a “pipeline configuration”, i.e. a selection of algorithms and hyperparameter settings, and the output of which is the performance (predictive accuracy) achieved by such a configuration.",1. Introduction,[0],[0]
We implement BO with a Gaussian process (GP) prior on the black-box function.,1. Introduction,[1.0],['We implement BO with a Gaussian process (GP) prior on the black-box function.']
"To deal with the high-dimensionality of the pipeline configuration space, we capitalize on the fact that for a given dataset, the performance of one ML algorithm may not be correlated with that of another algorithm.",1. Introduction,[1.0],"['To deal with the high-dimensionality of the pipeline configuration space, we capitalize on the fact that for a given dataset, the performance of one ML algorithm may not be correlated with that of another algorithm.']"
"For instance, it may be the case that the observed empirical performance of logistic regression on a given dataset does not tell us much information about how a neural network would perform on the same dataset.",1. Introduction,[1.0],"['For instance, it may be the case that the observed empirical performance of logistic regression on a given dataset does not tell us much information about how a neural network would perform on the same dataset.']"
"In such a case, both algorithms should not share the same GP prior, but should rather be
modeled independently.",1. Introduction,[1.000000033473783],"['In such a case, both algorithms should not share the same GP prior, but should rather be modeled independently.']"
Our BO learns such a decomposition of algorithms from data in order to break down the high-dimensional optimization problem into a set of lowerdimensional sub-problems.,1. Introduction,[1.0],['Our BO learns such a decomposition of algorithms from data in order to break down the high-dimensional optimization problem into a set of lowerdimensional sub-problems.']
"We model the decomposition of algorithms via an additive kernel with a Dirichlet prior on its structure, and learn the decomposition from data in concurrence with the BO iterations.",1. Introduction,[1.0],"['We model the decomposition of algorithms via an additive kernel with a Dirichlet prior on its structure, and learn the decomposition from data in concurrence with the BO iterations.']"
"We also propose a batched (parallelized) version of the BO procedure, along with a computationally efficient algorithm for maximizing the BO acquisition function.
",1. Introduction,[0],[0]
AUTOPROGNOSIS follows a principled Bayesian approach in all of its components.,1. Introduction,[1.0],['AUTOPROGNOSIS follows a principled Bayesian approach in all of its components.']
"The system implements post-hoc construction of pipeline ensembles via Bayesian model averaging, and implements a meta-learning algorithm that utilizes data from external cohorts of “similar” patients using an empirical Bayes method.",1. Introduction,[1.0],"['The system implements post-hoc construction of pipeline ensembles via Bayesian model averaging, and implements a meta-learning algorithm that utilizes data from external cohorts of “similar” patients using an empirical Bayes method.']"
"In order to resolve the tension between accuracy and interpretability, which is crucial for clinical decision-making (Cabitza et al., 2017), the system presents the clinicians with a rule-based approximation for the learned ML pipeline by mining for logical associations between patients’ features and the model’s predicted risk strata using a Bayesian associative classifier (Agrawal et al., 1993; Kruschke, 2008).
",1. Introduction,[0],[0]
"We conclude the paper by conducting a set of experiments on multiple patient cohorts representing various aspects of cardiovascular patient care, and show that prognostic models learned by AUTOPROGNOSIS outperform widely used clinical risk scores and existing AutoML frameworks.
",1. Introduction,[1.000000013583077],"['We conclude the paper by conducting a set of experiments on multiple patient cohorts representing various aspects of cardiovascular patient care, and show that prognostic models learned by AUTOPROGNOSIS outperform widely used clinical risk scores and existing AutoML frameworks.']"
"Related work: To the best of our knowledge, none of the existing AutoML frameworks, such as AUTO-WEKA (Kotthoff et al., 2016), AUTO-SKLEARN (Feurer et al., 2015), and TPOT (Olson & Moore, 2016) use principled GP-based BO to configure ML pipelines.",1. Introduction,[0],[0]
All of the existing frameworks model the sparsity of the pipelines’ hyperparameter space via frequentist tree-based structures.,1. Introduction,[0],[0]
"Both AUTO-WEKA and AUTO-SKLEARN use BO, but through tree-based heuristics, such as random forest models and tree Parzen estimators,
whereas TPOT uses a tree-based genetic programming algorithm.",1. Introduction,[0],[0]
Previous works have refrained from using principled GP-based BO because of its statistical and computational complexity in high-dimensional hyperparameter spaces.,1. Introduction,[0],[0]
"Our algorithm makes principled, high-dimensional GP-based BO possible by learning a sparse additive kernel decomposition for the GP prior.",1. Introduction,[0],[0]
"This approach confers many advantages as it captures the uncertainty about the sparsity structure of the GP prior, and allows for principled approaches for (Bayesian) meta-learning and ensemble construction that are organically connected to the BO procedure.",1. Introduction,[0],[0]
"In Section 5, we compare the performance of AUTOPROGNOSIS with that of AUTO-WEKA, AUTO-SKLEARN, and TPOT, demonstrating the superiority of our algorithm.
",1. Introduction,[0],[0]
Various previous works have addressed the problem of high-dimensional GP-based BO.,1. Introduction,[0],[0]
"(Wang et al., 2013) identifies a low-dimensional effective subspace for the black-box function via random embedding.",1. Introduction,[0],[0]
"However, in the AutoML setup, this approach cannot incorporate our prior knowledge about dependencies between the different hyperparameters (we know the sets of hyperparameters that are “activated” upon selecting an algorithm (Hutter et al., 2011)).",1. Introduction,[0],[0]
"This prior knowledge was captured by the Arc-kernel proposed in (Swersky et al., 2014), and similarly in (Jenatton et al., 2017), where a BO algorithm for domains with treestructured dependencies was proposed.",1. Introduction,[0],[0]
"Unfortunately, both methods require full prior knowledge of the dependencies between the hyperparameters, and hence cannot be used when jointly configuring hyperparameters across multiple algorithms, since the correlations of the performances of different algorithms are not known a priori.",1. Introduction,[0],[0]
"(Bergstra et al., 2011) proposed a naı̈ve approach that defines an independent GP for every set of hyperparameters that belong to the same algorithm.",1. Introduction,[0],[0]
"Since it does not share any information between the different algorithms, this approach would require trying all combinations of algorithms in a pipeline exhaustively.",1. Introduction,[0],[0]
"(In our system, there are 4,800 possible pipelines.)",1. Introduction,[0],[0]
"Our model solves the problems above via a data-driven kernel decomposition, through which only relevant groups of hyperparameters share a common GP prior, thereby balancing the trade-off between “information sharing” among hyperparameters and statistical efficiency.",1. Introduction,[0],[0]
"Consider a dataset D = {(xi, yi)}ni=1 for a cohort of n patients, with xi being patient i’s features, and yi being the patient’s clinical endpoint.",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"AUTOPROGNOSIS takes D as an input, and outputs an automatically configured prognostic model which predicts the patients’ risks, along with “explanations” for the predicted risk strata.",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"This Section provides an overview of the components of AUTOPROGNOSIS;
a schematic depiction of the system is shown in Figure 2.
",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The core component of AUTOPROGNOSIS is an algorithm that automatically configures ML pipelines, where every pipeline comprises algorithms for missing data imputation ( ), feature preprocessing (♣), prediction (•), and calibration (⋆).",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
Table 1 lists the baseline algorithms adopted by the system in all the stages of a pipeline.,2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The imputation and calibration stages are particularly important for clinical prognostic modeling (Blaha, 2016), and are not supported in existing AutoML frameworks.",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The total number of hyperparameters in AUTOPROGNOSIS is 106, which is less than those of AUTO-WEKA (786) and AUTO-SKLEARN (110).",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
The pipeline configuration algorithm uses Bayesian optimization to estimate the performance of different pipeline configurations in a scalable fashion by learning a structured kernel decomposition that identifies algorithms with correlated performance.,2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"Details of the Bayesian optimization algorithm are provided in Sections 3 and 5.
",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"In order to cope with the diverse nature of clinical data and health outcomes, AUTOPROGNOSIS pipelines are enriched with three modes of operation: (a) classification mode, (b) temporal mode, and (c) survival mode.",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The classification mode handles datasets with binary clinical outcomes (Yoon et al., 2017).",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"In this mode, the baseline predictive models include all algorithms in the scikit-learn library (Pedregosa et al., 2011), in addition to other powerful algorithms, such as XGBoost (Chen & Guestrin, 2016).",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The temporal mode handles longitudinal and time series data (Alaa et al., 2017) by applying the classification algorithms above on data residing in a sliding window within the time series, which we parametrize by the sequence time (Hripcsak et al., 2015).",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The survival mode handles time-to-event data, and involves all the classification algorithms above, in addition to survival models such as Cox proportional hazards model and survival forests (Ishwaran et al., 2008), and models for multiple competing risks (Fine & Gray, 1999).
",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"The meta-learning module is a pre-processing step that is used to warmstart BO using data from external cohorts,
whereas the ensemble construction and interpreter modules post-process the BO outputs.",2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
All of the three module run with a relatively low computational burden.,2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
Details of the three modules are provided in Sections 4 and 5.,2. AUTOPROGNOSIS: A Practical System for Automated Clinical Prognostic Modeling,[0],[0]
"Let (Ad,Af ,Ap,Ac) be the sets of all missing data imputation, feature processing, prediction, and calibration algorithms considered in AUTOPROGNOSIS (Table 1), respectively.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"A pipeline P is a tuple of the form: P = (Ad, Af , Ap, Ac) where Av ∈ Av, ∀v ∈ {d, f, p, c}.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
The space of all possible pipelines is given by P = Ad ×Af ×Ap ×Ac.,3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"Thus, a pipeline is a selection of algorithms from the elements of Table 1.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"An exemplary pipeline can be specified as follows: P = {MICE,PCA,Random Forest, Sigmoid}.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"The total number of pipelines in AUTOPROGNOSIS is |P| = 4, 800.
",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
The specification of a pipeline configuration is completed by determining the hyperparameters of its constituting algorithms.,3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"The space of hyperparameter configurations for a pipeline is Θ = Θd×Θf ×Θp×Θc, where Θv = ∪aΘav , for v ∈ {d, f, p, c}, with Θav being the space of hyperparameters associated with the ath algorithm in Av.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"Thus, a pipeline configuration Pθ ∈ PΘ is a selection of algorithms P ∈ P , and hyperparameter settings θ ∈ Θ; PΘ is the space of all possible pipeline configurations.",3. Pipeline Configuration via Bayesian Optimization with Structured Kernels,[0],[0]
"The main goal of AUTOPROGNOSIS is to identify the best pipeline configuration P ∗θ∗ ∈ PΘ for a given patient cohort D via J-fold cross-validation as follows:
P ∗θ∗ ∈",3.1. The Pipeline Selection & Configuration Problem,[0],[0]
argmaxPθ∈PΘ 1J ∑J i=1,3.1. The Pipeline Selection & Configuration Problem,[0],[0]
"L(Pθ;D (i) train,D (i) valid), (1)
where L is a given accuracy metric (AUC-ROC, c-index, etc), D(i)train and D (i) valid are training and validation splits of D
in the ith fold.",3.1. The Pipeline Selection & Configuration Problem,[0],[0]
The optimization problem in (1) is dubbed the Pipeline Selection and Configuration Problem (PSCP).,3.1. The Pipeline Selection & Configuration Problem,[0],[0]
"The PSCP can be thought of as a generalization for the combined algorithm selection and hyperparameter optimization (CASH) problem in (Feurer et al., 2015; Kotthoff et al., 2016), which maximizes an objective with respect to selections of single algorithms from the set Ap, rather than selections of full-fledged pipelines from PΘ.",3.1. The Pipeline Selection & Configuration Problem,[0],[0]
"The objective in (1) has no analytic form, and hence we treat the PSCP as a black-box optimization problem.",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"In particular, we assume that 1
J ∑J i=1",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"L(Pθ;D (i) train,D (i) valid) is a
noisy version of a black-box function f :",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"Λ → R, were Λ = Θ× P, and use BO to search for the pipeline configuration P ∗θ∗",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
that maximizes the black-box function f(.),3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"(Snoek et al., 2012).",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"The BO algorithm specifies a Gaussian process (GP) prior on f(.) as follows:
f ∼ GP(µ(Λ), k(Λ,Λ′)), (2)
where µ(Λ) is the mean function, encoding the expected performance of different pipeline, and k(Λ,Λ′) is the covariance kernel (Rasmussen & Williams, 2006), encoding the similarity between the different pipelines.",3.2. Solving the PSCP via Bayesian Optimization,[0],[0]
"The function f is defined over the D-dimensional space Λ, where D = dim(Λ) is given by
D = dim(P) + ∑ v∈{d,f,p,c} ∑ a∈Avdim(Θ a v).",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"(3)
In AUTOPROGNOSIS, the domain Λ is high-dimensional, with D = 106.",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
(The dimensionality of Λ can be calculated by summing up the number of pipeline stages and the number of hyperparameters in Table 1.),3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"High-dimensionality renders standard GP-based BO infeasible as both the sample complexity of nonparametric estimation and the computational complexity of maximizing the acquisition function are exponential in D (Györfi et al., 2006; Kandasamy
et al., 2015).",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"For this reason, existing AutoML frameworks have refrained from using GP priors, and relied instead on scalable tree-based heuristics (Feurer et al., 2015; Kotthoff et al., 2016).",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"Despite its superior performance, recent empirical findings have shown that plain-vanilla GP-based BO is feasible only for problems with D ≤ 10 (Wang et al., 2013).",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"Thus, the deployment of GP-based BO has been limited to hyperparameter optimization for single, predefined ML models via tools such as Google’s Visier and HyperTune (Golovin et al., 2017).",3.3. Bayesian Optimization via Structured Kernels,[0],[0]
AUTOPROGNOSIS overcomes this challenge by leveraging the structure of the PSCP problem as we show in what follows.,3.3. Bayesian Optimization via Structured Kernels,[0],[0]
"The key idea of our BO algorithm is that for a given dataset, the performance of a given group of algorithms may not be informative of the performance of another group of algorithms.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"Since the kernel k(Λ,Λ′) encodes the correlations between the performances of the different pipeline configurations, the underlying “informativeness” structure that relates the different hyperparameters can be expressed via the following sparse additive kernel decomposition:
k(Λ,Λ′) = ∑M
m=1km(Λ (m),Λ′ (m) ), (4)
where Λ(m) ∈ Λ(m), ∀m ∈ {1, . .",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
".,M}, with {Λ(m)}m being a set of disjoint subspaces of Λ. (That is, ∪mΛ(m) = Λ, and Λ(m) ∩ Λ(m
′) = ∅.)",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"The subspaces are assigned mutually exclusive subsets of the dimensions of Λ, so that∑
mdim(Λ (m))",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"= D. The structure of the kernel in (4) is unknown a priori, and needs to be learned from data.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"The kernel decomposition breaks down f as follows:
f(Λ) = ∑M
m=1fm(Λ (m)).",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"(5)
The additively sparse structure in (4) gives rise to a statistically efficient BO procedure.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"That is, if f is γ-smooth, then our additive kernels reduce sample complexity from O(n −γ 2γ+D ) to O(n −γ 2γ+Dm ), where Dm is the maximum number of dimensions in any subspace (Raskutti et al.,
2009; Yang et al., 2015).",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"(Similar improvements hold for the cumulative regret (Kandasamy et al., 2015).)
",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"Each subspace Λ(m) ⊂ Λ contains the hyperparameters of algorithms with correlated performances, whereas algorithms residing in two different subspaces Λ(m) and Λ(m ′) have uncorrelated performances.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
Since a hyperparameter in Θ is only relevant to f(.),3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"when the corresponding algorithm in P is selected (Hutter et al., 2009), then the decomposition {Λ(m)}m must ensure that all the hyperparameters of the same algorithm are bundled together in the same subspace.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
This a priori knowledge about the “conditional relevance” of the dimensions of Λ makes it easier to learn the kernel decomposition from data.,3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"Figure 3 provides an illustration for an exemplary subspace decomposition for the hyperparameters of a set of prediction, feature processing and imputation algorithms.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"Since the structured kernel in (4) is not fully specified a priori, we propose an algorithm to learn it from the data in the next Section.",3.3.1. THE STRUCTURE OF THE PSCP PROBLEM,[0],[0]
"AUTOPROGNOSIS uses a Bayesian approach to learn the subspace decomposition {Λ(m)}m in concurrence with the BO procedure, where the following Dirichlet-Multinomial prior is placed on the structured kernel (Wang et al., 2017):
α ∼ Dirichlet(M,γ), zv,a ∼ Multi(α), (6)
∀a ∈ Av, v ∈ {d, f, p, c}, where γ = {γm}m is the parameter of a Dirichlet prior, α = {αm}m are the Multinomial mixing proportions, and zv,a is an indicator variable that determines the subspace to which the ath algorithm in Av belongs.",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
The kernel decomposition in (4) is learned by updating the posterior distribution of {Λ(m)}m in every iteration of the BO procedure.,3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"The posterior distribution over
the variables {zv,a}v,a and α is given by:
P(z, α |Ht, γ) ∝",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"P(Ht | z)P(z |α)P(α, γ), (7)
where z = {zv,a : ∀a ∈ Av,∀v ∈ {d, f, p, c}}, and Ht is the history of evaluations of the black-box function up to iteration t. Since the variables {zv,a}v,",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"a are sufficient statistics for the subspace decomposition, the posterior over {Λ(m)}m is fully specified by (7) marginalized over α, which can be evaluated using Gibbs sampling as follows:
P(zv,a = m | z/{zv,a},Ht) ∝",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"P(Ht | z) (|A(m)v |+ γm),
where P(Ht | z) is the GP likelihood under the kernel induced by z.",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"The Gibbs sampler is implemented via the Gumble-Max trick (Maddison et al., 2014) as follows:
ωm i.i.d∼ Gumbel(0, 1), m ∈ {1, . .",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
".,M}, (8)
zv,a∼ argmaxm P(Ht | z, zv,a = m)(|A(m)v |+ γm) + ωm.",3.3.2. STRUCTURED KERNEL LEARNING,[0],[0]
"The BO procedure solves the PSCP problem by exploring the performances of a sequence of pipelines {P 1θ1 , P 2 θ2 , . . .}",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
until it (hopefully) converges to the optimal pipeline P ∗θ∗ .,3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"In every iteration t, BO picks a pipeline to evaluate using an acquisition function A(Pθ;Ht) that balances between exploration and exploitation.",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"AUTOPROGNOSIS deploys a 2- step batched (parallelized) exploration scheme that picks B pipelines for evaluation at every iteration t as follows:
Step 1: Select the frequentist kernel decomposition {Λ̂ (m)}m that maximizes the posterior P(z |Ht).
",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"Step 2: Select the B pipelines {P bθ }Bb=1 with the highest values for the acquisition function {A(P bθ ;Ht)}Bb=1, such that each pipeline P bθ , b ∈ {1, . .",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
".,",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"B}, involves a distinct prediction algorithm from a distinct subspace in {Λ̂(m)}m.
",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"We use the well-known Upper Confidence Bound (UCB) as acquisition function (Snoek et al., 2012).",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
The decomposition in (5) offers an exponential speed up in the overall computational complexity of Step 2 since the UCB acquisition function is maximized separately for every (lowdimensional) component fm; this reduces the number of computations from to O(n−D) to O(n−Dm).,3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"The batched implementation is advantageous since sequential evaluations of f(.) are time consuming as it involves training the selected ML algorithms.
",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
Step 2 in the algorithm above encourages exploration as follows.,3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"In every iteration t, we select a “diverse” batch of pipelines for which every pipeline is representative of a distinct subspace in {Λ̂(m)}m.",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"The batch selection scheme above encourages diverse exploration without the need for sampling pipelines via a determinantal point process with an exponential complexity as in (Kathuria et al., 2016; Nikolov, 2015; Wang et al., 2017).",3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
We also devise an efficient backward induction algorithm that exploits the structure of a pipeline to maximize the acquisition function efficiently.,3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
(Details are provided in the supplement.),3.3.3. EXPLORATION VIA DIVERSE BATCH SELECTION,[0],[0]
"In this Section, we discuss the details of the ensemble Construction and meta-learning modules; details of the interpreter module are provided in the next Section.",4. Ensemble Construction & Meta-learning,[0],[0]
"The frequentist approach to pipeline configuration is to pick the pipeline with the best observed performance from the set {P 1θ1 , . .",4.1. Post-hoc Ensemble Construction,[0],[0]
".,",4.1. Post-hoc Ensemble Construction,[0],[0]
P t θt} explored by the BO algorithm in Section 3.3.3.,4.1. Post-hoc Ensemble Construction,[0],[0]
"However, such an approach does not capture the uncertainty in the pipelines’ performances, and wastefully throws away t− 1 of the evaluated pipelines.",4.1. Post-hoc Ensemble Construction,[0],[0]
"On the contrary, AUTOPROGNOSIS makes use of all such pipelines via post-hoc Bayesian model averaging, where it creates an ensemble of weighted pipelines ∑ i wiP i θi .",4.1. Post-hoc Ensemble Construction,[0],[0]
"Model averaging is particularly useful in cohorts with small sample sizes, where large uncertainty about the pipelines’ performances would render frequentist solutions unreliable.
",4.1. Post-hoc Ensemble Construction,[0],[0]
"The ensemble weight wi = P(P i ∗
θi ∗ = P iθi |Ht) is the poste-
rior probability of P iθ being the best performing pipeline:
wi= ∑ z P(P i∗",4.1. Post-hoc Ensemble Construction,[0],[0]
θi ∗,4.1. Post-hoc Ensemble Construction,[0],[0]
"= P iθi | z,Ht) ·",4.1. Post-hoc Ensemble Construction,[0],[0]
"P(z |Ht), (9)
where i∗ is the pipeline configuration with the best (true) generalization performance.",4.1. Post-hoc Ensemble Construction,[0],[0]
"The weights in (9) are computed by Monte Carlo sampling of kernel decompositions via the posterior P(z |Ht), and then sampling the pipelines’ performances from the posterior f | z,Ht.",4.1. Post-hoc Ensemble Construction,[0],[0]
"Note that, unlike the ensemble builder of AUTOSKLEARN (Feurer et al., 2015), the weights in (9) account for correlations between
different pipelines, and hence it penalizes combinations of “similar” pipelines even if they are performing well.",4.1. Post-hoc Ensemble Construction,[0],[0]
"Moreover, our post-hoc approach allows building ensembles without requiring extra hyperparameters: in AUTOWEKA, ensemble construction requires a 5-fold increase in the number of hyperparameters (Kotthoff et al., 2016).",4.1. Post-hoc Ensemble Construction,[0],[0]
"The Bayesian model used for solving the PSCP problem in Section 3 can be summarized as follows:
f ∼ GP(µ, k | z), z ∼ Multi(α), α ∼ Dirichlet(M,γ).
",4.2. Meta-learning via Empirical Bayes,[0],[0]
"The speed of convergence of BO depends on the calibration of the prior’s hyperparameters (M,γ, µ, k).",4.2. Meta-learning via Empirical Bayes,[0],[0]
An agnostic prior would require many iterations to converge to satisfactory pipeline configurations.,4.2. Meta-learning via Empirical Bayes,[0],[0]
"To warmstart the BO procedure for a new cohort D, we incorporate prior information obtained from previous runs of AUTOPROGNOSIS on a repository of K complementary cohorts {D1, . .",4.2. Meta-learning via Empirical Bayes,[0],[0]
".,DK}.",4.2. Meta-learning via Empirical Bayes,[0],[0]
"Our meta-learning approach combines {H1t1 , . .",4.2. Meta-learning via Empirical Bayes,[0],[0]
".,H",4.2. Meta-learning via Empirical Bayes,[0],[0]
"M tK} (optimizer runs on the K complementary cohorts) with the data in D to obtain an empirical Bayes estimate (M̂, γ̂, µ̂, k̂).
",4.2. Meta-learning via Empirical Bayes,[0],[0]
Our approach to meta-learning works as follows.,4.2. Meta-learning via Empirical Bayes,[0],[0]
"For every complementary dataset Dk, we create a set of 55 metafeatures M(Dk), 40 of which are statistical meta-features (e.g. number of features, size of data, class imbalance, etc), and the remaining 15 are clinical meta-features (e.g. lab tests, vital signs, ICD-10 codes, diagnoses, etc).",4.2. Meta-learning via Empirical Bayes,[0],[0]
"For every complementary dataset in Dj , we optimize the hyperparameters (M̂j , γ̂j , µ̂j , k̂j) via marginal likelihood maximization.",4.2. Meta-learning via Empirical Bayes,[0],[0]
"For a new cohort D, we compute a set of weights {ηj}j , with ηj = ℓj/ ∑ k ℓk, where ℓj = ∥M(D)−M(Dj)∥1, and calibrate its prior (M,γ, µ, k) by setting it to be the average of the estimates (M̂j , γ̂j , µ̂j , k̂j), weighted by {ηj}j .
",4.2. Meta-learning via Empirical Bayes,[0],[0]
"Existing methods for meta-learning focus only on identifying well-performing pipelines from other datasets, and use them for initializing the optimization procedure (Brazdil et al., 2008; Feurer et al., 2015).",4.2. Meta-learning via Empirical Bayes,[0],[0]
Conceptualizing metalearning as an empirical Bayes calibration procedure allows the transfer of a much richer set of information across datasets.,4.2. Meta-learning via Empirical Bayes,[0],[0]
"Through the method described above, AUTOPROGNOSIS can import information on the smoothness of the black-box function (k), the similarities among baseline algorithms (γ,M), and the expected pipelines’ performances (µ).",4.2. Meta-learning via Empirical Bayes,[0],[0]
"This improves not only the initialization of the BO procedure, but also the mechanism by which it explores the pipelines’ design space.",4.2. Meta-learning via Empirical Bayes,[0],[0]
"In this section, we assess the ability of AUTOPROGNOSIS to automatically make the right prognostic modeling choices
when confronted with a variety of clinical datasets with different meta-features.",5. Evaluation of AUTOPROGNOSIS,[0],[0]
"We conducted experiments on 9 cardiovascular cohorts that correspond to the following aspects of patient care:
• Preventive care: We considered a major cohort for preventive cardiology: the Meta-analysis Global Group in Chronic heart failure database (MAGGIC), which holds data for 46,817 patients gathered from multiple clinical studies (Wong et al., 2014).
",5.1. Cardiovascular Disease Cohorts,[0],[0]
"• Heart transplant wait-list management: We extracted data from the United Network for Organ Sharing (UNOS) database, which holds information on all heart transplants conducted in the US between the years 1985 to 2015.",5.1. Cardiovascular Disease Cohorts,[0],[0]
"Cohort UNOS-I is a pre-transplant population of 36,329 cardiac patients who were enrolled in a transplant wait-list.
",5.1. Cardiovascular Disease Cohorts,[0],[0]
"• Post-transplant follow-up: Cohort UNOS-II is a posttransplant population of 60,400 patients in the US who underwent a transplant between the years 1985 to 2015.
",5.1. Cardiovascular Disease Cohorts,[0],[0]
"• Cardiovascular comorbidities: We extracted 6 cohorts from the Surveillance, Epidemiology, and End Results (SEER) cancer registries, which cover approximately 28% of the US population (Yoo & Coughlin, 2018).",5.1. Cardiovascular Disease Cohorts,[0],[0]
"We predict cardiac deaths in patients diagnosed with breast cancer (SEER-I), colorectal cancer (SEER-II), Leukemia (SEERIII), respiratory cancers (SEER-IV), digestive system cancer (SEER-V), and urinary system cancer (SEER-VI).
",5.1. Cardiovascular Disease Cohorts,[0],[0]
"The first three groups of datasets (colored in red) were collected for cohorts of patients diagnosed with (or at risk for) cardiac diseases, and so they shared a set of meta-features, including a large number of cardiac risk factors, low censoring rate, and moderate class imbalance.",5.1. Cardiovascular Disease Cohorts,[0],[0]
"The last group of datasets (colored in blue) was collected for cohorts of
cancer patients for whom cardiac diseases are potential comorbidities.",5.1. Cardiovascular Disease Cohorts,[0],[0]
"These datasets shared a different set of metafeatures, including a small number of cardiac risk factors, high censoring rate, and severe class imbalance.",5.1. Cardiovascular Disease Cohorts,[0],[0]
Our experiments will demonstrate the ability of AUTOPROGNOSIS to adapt its modeling choices to these different clinical setups.,5.1. Cardiovascular Disease Cohorts,[0],[0]
Table 2 shows the performance of various competing prognostic modeling approaches evaluated in terms of the area under receiver operating characteristic curve (AUC-ROC) with 5-fold cross-validation1.,5.2. Performance of AUTOPROGNOSIS,[0],[0]
"We compared the performance of AUTOPROGNOSIS with the clinical risk scores used for predicting prognosis in each cohort (MAGGIC score in MAGGIC and UNOS-I (Wong et al., 2014) and IMPACT score in UNOS-II (Weiss et al., 2011)).",5.2. Performance of AUTOPROGNOSIS,[0],[0]
"We also compared with various AutoML frameworks, including AUTO-WEKA (Kotthoff et al., 2016), AUTO-SKLEARN (Feurer et al., 2015), and TPOT (Olson & Moore, 2016).",5.2. Performance of AUTOPROGNOSIS,[0],[0]
"Finally, we compared with a standard Cox proportional hazards (Cox PH) model, which is the model most commonly used in clinical prognostic research.
",5.2. Performance of AUTOPROGNOSIS,[0],[0]
Table 2 demonstrates the superiority of AUTOPROGNOSIS to all the competing models on all the cohorts under consideration.,5.2. Performance of AUTOPROGNOSIS,[0],[0]
This reflects the robustness of our system since the 9 cohorts had very different characteristics.,5.2. Performance of AUTOPROGNOSIS,[0],[0]
"In many experiments, the learned kernel decomposition reflected an intuitive clustering of algorithms by the similarity of their structure.",5.2. Performance of AUTOPROGNOSIS,[0],[0]
"For instance, Figure 4 shows one subspace in the frequentist decomposition learned by AUTOPROGNOSIS over the BO iterations for the MAGGIC cohorts.",5.2. Performance of AUTOPROGNOSIS,[0],[0]
"We can see that all ensemble methods in the imputation and prediction stages that use decision-trees as their base learners were lumped together in the same subspace.
",5.2. Performance of AUTOPROGNOSIS,[0],[0]
1All algorithms were allowed to run for a maximum of 10 hours to ensure a fair comparison.,5.2. Performance of AUTOPROGNOSIS,[0],[0]
"Albeit accurate, models built by AUTOPROGNOSIS would generally be hard for a clinician to “interpret”.",5.3. The “Interpreter”,[0],[0]
"To address this issue, AUTOPROGNOSIS deploys an interpreter module (see Figure 2) that takes as an input the learned model for a given cohort, in addition to a set of actionable risk strata R, and outputs an “explanation” for its predictions in terms of a set of logical association rules of the form:
C1 ∧ C2 ∧ . . .",5.3. The “Interpreter”,[0],[0]
"∧ Cl(r) =⇒ r, ∀r ∈ R, (10)
",5.3. The “Interpreter”,[0],[0]
"where {C1, . .",5.3. The “Interpreter”,[0],[0]
".,",5.3. The “Interpreter”,[0],[0]
Cl(r)} is a set of Boolean conditions associated with risk stratum r.,5.3. The “Interpreter”,[0],[0]
"The association rules are obtained via a Bayesian associative classifier (Ma & Liu, 1998; Agrawal et al., 1993; Kruschke, 2008; Luo, 2016), with a prior over association rules, and a posterior computed based on target labels that correspond to the outputs of the learned model discretized via the strata in R. The Bayesian approach allows incorporating prior knowledge (from clinical literature) about “likely” association rules.
",5.3. The “Interpreter”,[0],[0]
We report one example for an explanation provided by the interpreter module based on our experiments on the MAGGIC cohort.,5.3. The “Interpreter”,[0],[0]
"For this cohort, the standard risk score exhibited a low AUC-ROC for patients with Type-2 Diabetes.",5.3. The “Interpreter”,[0],[0]
"On the contrary, AUTOPROGNOSIS performed almost equally well in the two subgroups.",5.3. The “Interpreter”,[0],[0]
The interpreter provided an explanation for the improved predictions through the following association rule: Diabetic ∧ Lipid-lowering ∧ (Age ≥ 40) =⇒ High risk None of these risk factors were included in the standard guidelines.,5.3. The “Interpreter”,[0],[0]
"That is, the interpreter indicates that a better stratification, with new risk factors such the usage of lipidlowering drugs, is possible for diabetic patients.",5.3. The “Interpreter”,[0],[0]
Clinicians can use the interpreter as a data-driven hypothesis generator that prompts new risk factors and strata for subsequent research.,5.3. The “Interpreter”,[0],[0]
"We split up Table 2 into 2 groups of columns: group 1 (left) contains cohorts obtained from cardiology studies, whereas group 2 (right) contains cohorts obtained from cancer studies, with cardiac secondary outcomes.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"As mentioned earlier, the two groups had different meta-features.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
We tracked the modeling choices made by vanilla AUTOPROGNOSIS (no ensembles or meta-learning) in both groups (“best predictor” row in Table 2).,5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"For all datasets in group 2, AUTOPROGNOSIS decided that survival modeling (using Cox PH model or survival forests) is the right model.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"This is because, with the high prevalence of censored time-to-event data, survival models are more data-efficient than operating on binarized survival labels and removing patients lost to follow-up.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"When given richer datasets with a large number of relevant features, low rates of censoring and moderate imbalance (group 1), AUTOPROGNOSIS spent more iterations navigating ML classifiers, and learned that an algorithm like AdaBoost is a better choice for a dataset like UNOS-I. Such a (non-intuitive) choice would have not been possibly identified by a clinical researcher; researchers typically use the Cox PH model, which on the UNOS-I cohort provides an inferior performance.
",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"Meta-learning was implemented via leave-one-dataset-out validation: we run vanilla AUTOPROGNOSIS on all of the 9 cohorts, and then for every cohort, we use the other 9 cohorts as the complementary datasets used to implement the meta-learning algorithm.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"Since the pool of complementary cohorts contained 5 datasets for cardiovascular comorbidities, meta-learning was most useful for group 2 datasets as they all had very similar meta-features.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"With meta-learning, AUTOPROGNOSIS had a strong prior on survival models for group 2 datasets, and hence it converges quickly to a decision on using a survival model having observed the dataset’s meta-features.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"Ensemble construction was most useful for the MAGGIC and UNOS cohorts, since those datasets had more complex hypotheses to learn.
",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
Clinical researchers often ask the question: when should I use machine learning for my prognostic study?,5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
The answer depends on the nature of the dataset involved.,5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"As we have see in Table 2, a simple Cox model may in some cases be sufficient to issue accurate predictions.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
The metalearning module in AUTOPROGNOSIS can act as a clairvoyant that tells whether ML models would add value to a given prognostic study without even training any model.,5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
"That is, by looking at the “meta-learned” GP prior calibrated by a new dataset’s meta-features, we can see whether the prior assigns high scores to ML models compared to a simple Cox model, and hence decide on whether ML has gains to offer for such a dataset.",5.4. Learning to Pick the Right Model and AUTOPROGNOSIS as a Clairvoyant,[0],[0]
The authors would like to thank the reviewers for their helpful comments.,Acknowledgements,[0],[0]
"The research presented in this paper was supported by the Office of Naval Research (ONR) and the NSF (Grant number: ECCS1462245, ECCS1533983, and ECCS1407712).",Acknowledgements,[0],[0]
Clinical prognostic models derived from largescale healthcare data can inform critical diagnostic and therapeutic decisions.,abstractText,[0],[0]
"To enable off-theshelf usage of machine learning (ML) in prognostic research, we developed AUTOPROGNOSIS: a system for automating the design of predictive modeling pipelines tailored for clinical prognosis.",abstractText,[0],[0]
AUTOPROGNOSIS optimizes ensembles of pipeline configurations efficiently using a novel batched Bayesian optimization (BO) algorithm that learns a low-dimensional decomposition of the pipelines’ high-dimensional hyperparameter space in concurrence with the BO procedure.,abstractText,[0],[0]
"This is achieved by modeling the pipelines’ performances as a black-box function with a Gaussian process prior, and modeling the “similarities” between the pipelines’ baseline algorithms via a sparse additive kernel with a Dirichlet prior.",abstractText,[0],[0]
Meta-learning is used to warmstart BO with external data from “similar” patient cohorts by calibrating the priors using an algorithm that mimics the empirical Bayes method.,abstractText,[0],[0]
The system automatically explains its predictions by presenting the clinicians with logical association rules that link patients’ features to predicted risk strata.,abstractText,[0],[0]
We demonstrate the utility of AUTOPROGNOSIS using 9 major patient cohorts representing various aspects of cardiovascular patient care.,abstractText,[0],[0]
AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning,title,[0],[0]
