8	33	As a first step towards understanding such relations, we introduce the Equation Parsing task - given a sentence expressing a mathematical relation, the goal is to generate an equation representing the relation, and to map the variables in the equation to their corresponding noun phrases.
9	27	To keep the problem tractable, in this paper we restrict the final output equation form to have at most two (possibly coreferent) variables, and assume that each quantity mentioned in the sentence can be used at most once in the final equation.1 In example 1, the gold output of an equation parse should be V1 = 3 × V2, with V1 = “Emanuel’s campaign contributions” and V2 = “those of his opponents put together”.
10	41	The task can be seen as a form of semantic parsing (Goldwasser and Roth, 2011; Kwiatkowski et al., 2013) where instead of mapping a sentence to a logical form, we want to map it to an equation.
14	27	Moreover, in difference from semantic parsing into logical forms, in Equation Parsing multiple phrases in the text could correspond to the same variable, and identical phrases in the text could correspond to multiple variables.
15	55	We call the problem of mapping noun phrases to variables the problem of grounding variables.
16	30	Grounding is challenging for various reasons, key among them are that: (i) The text often does not mention “variables” explicitly, e.g., the sentence in example 3 describes a mathematical relation between the speed of bird and the speed of wind, without mentioning “speed” explicitly.
17	21	(ii) Sometimes, multiple noun phrases could refer to the same variable.
19	19	On the other hand, the same noun phrase might refer to multiple variables, as in example 4, where the noun phrase “two numbers” refer to two variables.
57	28	For example, for the sentence in Fig 1, the spans “Twice”, “25”, and “triple” generate quantity triggers, whereas “a number” and “the same number” generate variable triggers, with label V1.
62	21	Note that there can be multiple valid trigger lists.
65	62	Equation Tree An equation tree of a sentence x is a binary tree whose leaves constitute the trigger list of x, and internal nodes (except the root) are labeled with one of the following operations – addition, subtraction, multiplication, division.
68	24	An equation tree is a natural representation for an equation.
78	19	In other words, the span of the left child and the right child cannot intersect in a projective equation tree2.
86	25	Therefore, we develop an algorithmic approach for predicting projective equation trees, and show empirically that it compares favourably with ones which do not make the projective assumption.
129	29	The tree prediction module receives the trigger list predicted by the previous two modules, and the goal is to create an equation tree using the trigger list as the leaves of that tree.
130	32	The input x is the sentence and the trigger list, and the output y is the equation tree representing the relation described in the sentence.
131	57	We assume that the output will be a projective equation tree.
134	24	The projectivity assumption implies that the final equation tree can be generated by combining only adjacent nodes, once the set of leaves is sorted based on Span-Start(·) values.
136	49	A natural approach to further reduce the output space is to conform to the projective structure of the syntactic parse of the sentence.
137	29	However, we found this to adversely affect performance, due to the poor performance of syntactic parser on equation data.
138	323	Lexicon To bootstrap the equation parsing process, we developed a high precision lexicon to translate mathematical expressions to operations and orders, like “sum of A and B” translates to “A+B”, “A minus B” translates to “A-B”, etc.
172	29	The average number of mention annotations per sentence was 1.74.
176	21	In each case, we also report accuracy by removing each feature group, one at a time.
177	71	In addition, for equation tree prediction, we also show the effect of lexicon, projectivity, conforming to syntactic parse constraints, and using lexicon as features instead of hard constraints.
179	24	In this section, we evaluate the performance of our system on the overall equation parsing task.
180	29	We report Equation Accuracy - the fraction of sentences for which the system got the equation correct, and Equation+Grounding Accuracy - the fraction of sentences for which the system got both the equation and the grounding of variables correct.
183	34	We also compare with SPF (Artzi and Zettlemoyer, 2013), a publicly available semantic parser, which can learn from sentence-logical form pairs.
187	46	We attribute this to the inability of SPF to handle overlapping mentions (like in Example 4), as well as its approach of parsing the whole sentence to the final output form.
190	24	Our approach, in contrast to SPF, can handle overlapping mentions, selects triggers from text, and parses the trigger list to form equations.
191	30	For variable trigger list prediction, around 25% of the errors were due to the predictor choosing a span which is contained within the correct span, e.g., when the target noun phrase is “The cost of a child’s ticket”, our predictor chose only “child’s ticket”.
