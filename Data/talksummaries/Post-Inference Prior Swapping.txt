1	42	For example, • Conjugate priors yield posteriors with a known parametric form and therefore allow for non-iterative, exact inference (Diaconis et al., 1979).
3	16	• Simple parametric priors allow for computationally cheap density queries, maximization, and sampling, which can reduce costs in iterative inference algorithms (e.g. Metropolis-Hastings (Metropolis et al., 1953), gradient-based MCMC (Neal, 2011), or sequential Monte Carlo (Doucet et al., 2000)).
6	18	This leads to the main question of this paper: for a given model, is it possible to use any convenient false prior to infer a false posterior, and afterwards, given any target prior of interest, efficiently and accurately infer the associated target posterior?
11	23	Note that most standard inference algorithms are iterative and data-dependent: parameter updates at each iteration involve data, and the computational cost or quality of each update depends on the amount of data used.
14	36	Prior swapping uses the pre-inferred false posterior to perform efficient updates that do not depend on the data, and thus proceeds very quickly.
15	63	We therefore advocate breaking difficult inference problems into two easier steps: first, do inference using the most computationally convenient prior for a given model, and then, for all future priors of interest, use prior swapping.
25	17	We are interested in the following task: given a false posterior inference result (i.e. samples from pf (θ|xn), or some exact or approximate PDF), choose an arbitrary target prior π(θ) and efficiently sample from the associated target posterior p(θ|xn)—or, more generally, compute an expectation µh = Ep [h(θ)] for some test function h(θ) with respect to the target posterior.
28	32	In importance sampling (IS), samples from an importance distribution are used to estimate the expectation of a test function with respect to a target distribution.
44	25	Suppose we’d like to estimate the posterior expectation under a Laplace target prior, with mean 10 and variance 1, for test function h(θ) = θ (i.e. an estimate of the target posterior mean).
45	98	We draw T false posterior samples {θ̃t}Tt=1 ∼ pf (θ|xn), compute weights w(θ̃t) and IS estimate µ̂ISh , and compare it with the true expectation µh.
48	17	Since we know pf (θ|xn) is normal, we can compute a lower bound on the number of false posterior samples T that would be needed for the expected estimate to be within δ of µh.
51	14	Note that this bound actually has nothing to do with the parametric form of π(θ)—it is based solely on the normal false posterior, and its distance to the target posterior mean µh.
52	15	However, even if this distance was small, the importance estimate would still have infinite variance due to the Laplace target prior.
62	23	(5) Note that if p̃f (θ) = pf (θ|xn), then ps(θ) = p(θ|xn).
63	30	However, depending on how we represent p̃f (θ), ps(θ) can have a much simpler analytic representation than p(θ|xn), which is typically defined via a likelihood function (i.e. a function of the data) and causes inference algorithms to have costs that scale with the data size n. Specifically, we will only use low-complexity p̃f (θ) that can be evaluated in constant time with respect to the data size n. Our general strategy is to use ps(θ) as a surrogate for p(θ|xn) in standard MCMC or optimization procedures, to yield data-independent algorithms with constant cost per iteration.
64	20	Intuitively, the likelihood information is captured by the false posterior—we make use of this instead of the likelihood function, which is costly to evaluate.
66	19	For example, we evaluate a function proportional to p(θ|xn) in Metropolis-Hastings (MH) (Metropolis et al., 1953), and ∇θ log p(θ|xn) in gradient-based MCMC methods (such as Langevin dynamics (LD) (Rossky et al., 1978) and Hamiltonian Monte Carlo (HMC) (Neal, 2011)) and in optimization procedures that yield a MAP point estimate.
72	28	2, we draw T samples {θt}Tt=1 ∼ ps(θ), compute a sample estimate µ̂PSh = 1 T ∑T t=1 θt, and compare it with the true value µh.
74	18	The previous method is only applicable if our false posterior inference result is a PDF p̃f (θ) (such as in closed-form inference or variational approximations).
123	22	Given Tf samples {θ̃t} Tf t=1 ∼ pf (θ|xn), we write the semiparametric false posterior estimate as p̃spf (θ) = 1 Tf Tf∑ t=1 [ 1 bd K ( ‖θ − θ̃t‖ b ) p̃αf (θ) p̃αf (θ̃t) ] , (9) where K denotes a probability density kernel, with bandwidth b, where b→ 0 as Tf →∞ (see (Wasserman, 2006) for details on probability density kernels and bandwidth selection).
136	16	• False posterior IS: IS using samples from pf (θ|xn).
151	25	Our second set of experiments are on Bayesian logistic regression models, which we write as yi ∼ Bern(pi), pi = logistic(Xiθ), θ ∼ π, i = 1,...,n. which we will pair with both heavy tailed priors and a hierarchical target prior π = N (0, α−1I), α ∼ Gamma(γ, 1).
165	18	In plot (b) we reduce the variance of the target prior; while this hurts the accuracy of false posterior IS, prior swapping still quickly converges to zero error.
168	17	Many latent variable models in machine learning—such as mixture models, topic models, probabilistic matrix factorization, and others—involve a set of latent factors (e.g. components or topics).
177	14	(c) Prior swapping with a diversity-promoting target prior on an LDA topic model (Simple English Wikipedia corpus) to separate redundant topic clusters; the top 6 words per topic are shown.
181	40	Hence, we propose the following strategy: first, assign a prior for the factor parameters that allows for collapsed Gibbs sampling; afterwards, reconstruct the factor samples and apply prior swapping for more complex relational priors over the factors.
187	14	‡https://simple.wikipedia.org/ In Fig.
189	20	In (a) we show inferred posteriors over GMM components for a number of relational target priors, which we define in (b).
191	65	Here, we show two topic clusters (“geography” and “family”) in pf (θ|xn), which are separated into distinct, yet thematically-similar, topics after prior swapping.
194	49	We have argued and shown empirically that this strategy is effective even when the false and target posteriors are quite dissimilar.
