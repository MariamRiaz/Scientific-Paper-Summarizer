0	44	Multilingual word embeddings have attracted a lot of attention in recent times.
1	37	In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012).
5	19	However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages.
16	80	Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary.
21	36	Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space.
45	48	Following this observation, we propose to use the output dictionary in Algorithm 1 as the input of the same system in a self-learning fashion which, assuming that the output dictionary was indeed better than the original one, should serve to learn a better mapping and, consequently, an even better dictionary the second time.
46	46	The process can then be repeated iteratively to obtain a hopefully better mapping and dictionary each time until some convergence criterion is met.
55	20	Among the more efficient exact alternatives, we decide to adopt the one by Artetxe et al. (2016) for its simplicity and good results as reported in their paper.
80	27	In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as Dinu et al. (2015).
83	16	In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.
101	28	In contrast, our proposed method obtains very competitive results for all dictionaries, with a difference of only 1-2 points between the full dictionary and both the 25 entry dictionary and the numerals dictionary in all three languages.
102	35	Figure 2 shows the curve of the English-Italian accuracy for different seed dictionary sizes, confirming this trend.
106	23	In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by Luong et al. (2015), which is the state-of-the-art for bilingual word embeddings based on parallel corpora (Upadhyay et al., 2016)6.
110	33	But, most importantly, it does not suffer from any significant degradation for using smaller dictionaries and, in fact, our method gets better results using the 25 entry dictionary or the numeral list as the only bilingual evidence than any of the baseline systems using much richer resources.
111	35	The relatively poor results of Luong et al. (2015) can be attributed to the fact that the dictionary based methods make use of much bigger monolingual corpora, while methods based on parallel corpora are restricted to smaller corpora.
114	42	All in all, our experiments show that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone.
123	26	The reasoning for how the self-learning framework is optimizing this objective is as follows.
128	30	Thanks to this, our self-learning algorithm is guaranteed to converge to a local optimum of the above global objective, behaving like an alternating optimization algorithm for it.
130	44	For that reason, it should be possible to use a random initialization instead of a small seed dictionary.
131	52	However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough.
135	16	Regarding random initialization, the figure shows that the algorithm gets stuck in a poor local optimum of the objective function, which is the reason of the bad performance (0% accuracy) on bilingual lexicon induction, but the proposed optimization objective itself seems to be adequate.
137	22	We first repeated our experiments on English-Italian bilingual lexicon induction for 5 different dictionaries of 25 entries, obtaining an average accuracy of 38.15% and a standard deviation of only 0.75%.
141	28	For that reason, it is likely that better methods to tackle this optimization problem would allow learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research.
142	43	So as to better understand the behavior of our system, we performed an error analysis of its output in English-Italian bilingual lexicon induction when starting with the 5,000 entry, the 25 entry and the numeral dictionaries in comparison with the baseline method of Artetxe et al. (2016) with the 5,000 entry dictionary.
143	18	For that purpose, we took 100 random examples from the test set in the [1-5K] frequency bin, another 100 from the [5K20K] frequency bin and 30 from the [100K-200K] frequency bin, and manually analyzed each of the errors made by all the 4 different variants.
146	20	This phenomenon is considerably more pronounced in the first frequency bins, which already have a much higher accuracy according to the gold standard.
152	34	We also observe a few errors that are related to multiwords or collocations (e.g. carrierâ†’ aereo, presumably related to the multiword air carrier / linea aerea), as well as some rare word that is repeated across many translations (Ferruzzi), which could be attributed to the hubness problem (Dinu et al., 2015; Lazaridou et al., 2015).
158	30	In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all.
159	19	In addition to that, we would like to explore non-linear transformations (Lu et al., 2015) and alternative dictionary induction methods (Dinu et al., 2015; Smith et al., 2017).
160	55	Finally, we would like to apply our model in the decipherment scenario (Dou et al., 2015).
