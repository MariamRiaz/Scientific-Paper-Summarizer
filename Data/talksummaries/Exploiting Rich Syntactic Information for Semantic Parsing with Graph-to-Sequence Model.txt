8	17	Intuitively, by incorporating such additional information, the encoder could produce a more meaningful and robust sentence representation.
9	10	The combination of these features (i.e., sequence + trees) forms a general graph structure (see Figure 1).
10	13	This inspires us to apply a graph encoder to produce a representation of a graph-structured input.
11	6	The graph encoder also has the advantages that it could simultaneously encode all types of syntactic contexts, and incorporate multiple types of syntactic structures in a unified way.
13	68	We then employ a novel graph-to-sequence (Graph2Seq) model (Xu et al., 2018), which consists of a graph encoder and a sequence decoder, to learn the representation of the syntactic graph (see §3).
14	12	Specifically, the graph encoder learns the representation of each node by aggregating information from its K-hop neighbors.
21	4	We represent three types of syntactic features, i.e., word order, dependency parse and constituency parse, as the syntactic graph (see Figure 1).
22	29	Previous neural semantic parsers mainly use these features by building a SeqLSTM that works on the word sequence.
24	1	In order to capture the forward and backward contextual information, we link these nodes in two directions, that is, from left to right and from right to left.
28	1	Similar to the dependency parse, the constituency parse represents the phrase structure, which is also important to the semantic parsing task.
30	17	Motivated by this observation, we add the non-terminal nodes of the constituent tree and the edges describing their parent-child relationships into the syntactic graph.
31	66	After building the syntactic graph for the input text, we employ a novel graph-to-sequence model (Xu et al., 2018), which includes a graph encoder and a sequence decoder with attention mechanism, to map the syntactic graph to the logical form.
33	46	Next, the sequence decoder takes the graph embedding as the initial hidden state, and calculates the attention over all node embeddings on the encoder side to generate logical forms.
34	16	Note that this graph encoder does not explicitly encode the edge label information, therefore, for each labeled edge, we add a node whose text attribute is the edge’s label.
40	24	The resulted vector is fed into a fully connected layer with nonlinear activation function σ, which updates the forward representation of v, hkv`, to be used at the next iteration; (5) We update the backward representation of v, hkva using the similar procedure as introduced in step (3) and (4) except that operating on the backward representations; (6) We repeat steps (3)∼(5) K times, and the concatenation of the final forward and backward representations is used as the final representation of v. Since the neighbor information from different hops may have different impacts on the node embedding, we learn a distinct aggregator at each iteration.
41	15	We feed the obtained node embeddings into a fully-connected neural network, and apply the element-wise max-pooling operation on all node embeddings.
48	19	We evaluate our model on three datasets: Jobs640, a set of 640 queries to a database of job listings; Geo880, a set of 880 queries to a database of U.S. geography; and ATIS, a set of 5,410 queries to a flight booking system.
56	27	For the graph encoder, the hop size K is set to 10, the non-linearity function σ is implemented as ReLU (Glorot et al., 2011), the parameters of the aggregators are randomly initialized.
59	31	Our model achieves competitive performance on Jobs640, ATIS and Geo880.
60	118	Our work is the first to use both multiple trees and the word sequence for semantic parsing, and it outperforms the Seq2Seq model reported in Dong and Lapata (2016), which only uses limited syntactic information.
62	63	To deal with the graph input, the BASELINE decomposes the graph embedding to two steps and applies different types of encoders sequentially: (1) a SeqLSTM to extract word order features, which results in word embeddings, Wseq; (2) two TreeLSTMs (Tai et al., 2015) to extract the dependency tree and constituency features while taking Wseq as initial word embeddings.
63	11	The resulted word embeddings and nonterminal node embeddings (from TreeLSTMs) are then fed into a sequence decoder.
64	142	We can see that our model significantly outperforms the BASELINE.
