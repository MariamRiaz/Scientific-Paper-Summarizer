1	35	A function f : 2N → R is submodular if it exhibits a diminishing returns property.
2	43	That is, for any S ⊆ T ⊆ N and any a /∈ T , the function respects: fS(a) ≥ fT (a) where fH(x) denotes the marginal contribution of an element x ∈ N to a set H ⊆ N , i.e. fH(x) = f(H ∪ x) − f(H).
3	69	Many fundamental measures such as entropy, diversity, and clustering can be modeled as submodular functions, and as a result submodular optimization is heavily studied in machine learning for well over a decade now.
4	52	It is well known that for the problem of maximizing a monotone (S ⊆ T =⇒ f(S) ≤ f(T )) submodular function under a cardinality constraint, the celebrated greedy algorithm which iteratively adds the element whose marginal contribution is maximal, obtains an approximation guarantee of 1−1/e (Nemhauser et al., 1978).
8	32	Mirzasoleiman et al. show that STOCHASTICGREEDY gives an approximation guarantee which is arbitrarily close to 1 − 1/e in expectation, and does very well in practice (Mirzasoleiman et al., 2015; Lucic et al., 2016).
9	68	Variants of this algorithm are used in clustering (Malioutov et al., 2016), sparsification (Lindgren et al., 2016), Gaussian RBF kernels (Sharma et al., 2015), sensing (Li et al., 2016), and social data analysis (Zhuang et al., 2016).
11	59	One can model this uncertainty with a probability distribution D: at every iteration, a value ξ ∼ D is being sampled, and the greedy algorithm adds an element whose marginal contribution is a ξ-approximation to the maximal marginal contribution.
12	49	In general, we refer to an algorithm that iteratively adds an element whose marginal contribution is approximately optimal in expectation as a stochastic greedy algorithm.
13	99	It is easy to show that stochastic greedy algorithms give an approximation ratio of 1 − 1/eµ in expectation, where µ is the mean of the distribution modeling the uncertainty.
14	30	This however is a weak guarantee as it leaves a non-negligible likelihood that the algorithm terminates with a solution with a poor approximation guarantee.
15	18	Indeed, as we later show, there are cases where stochastic greedy algorithms have desirable guarantees in expectation, but with constant probability have arbitrarily bad approximation guarantees.
16	11	We prove the following results: • Optimization under cardinality constraints.
17	68	For the problem of maximizing a monotone submodular function under cardinality constraint k, with uncertainty distributionD with expectation µ, we show that for any ε ∈ [0, 1], when k ≥ 1µε2 a stochastic greedy algorithm obtains an approximation of 1 − 1/e(1−ε)µ w.p.
18	12	Furthermore, we prove that this bound is optimal by showing that for any δ > 0 no algorithm can obtain an approximation ratio better than 1− 1/e(1−ε)µ w.p.
19	20	For the special case in which the function is modular, we prove an improved bound of (1− ε)µ w.p.
21	17	To further study the difference between guarantees that occur in expectation and guarantees which appear w.h.p, we study a generalization, where the greedy algorithm is used to maximize a monotone submodular function under intersection of matroid constraints, where a distribution D with mean µ generates uncertainty.
24	25	at least 1−µ−o(1), implying that in general stochastic greedy algorithms cannot obtain high probability guarantees under general matroid constraints; • Stochastic local search.
25	57	Finally, a natural alternative to greedy is local search.
26	37	We show that even for cardinality constraints local search performs poorly, and does not give any meaningful approximation guarantees when there is probabilistic uncertainty about the quality of the elements.
27	38	We contrast this with the case where there is deterministic uncertainty about the quality of the elements, in which we get an approximation ratio of (1 + 1/µ)−1, where µ is our uncertainty.
28	29	This implies that local search does not enjoy the same robustness guarantees of the greedy algorithms under uncertainty of the quality of elements.
29	6	The above results have several immediate consequences: • Fast algorithms for submodular optimization.
30	46	Our analysis applies to the STOCHASTIC-GREEDY algorithm (Mirzasoleiman et al., 2015), thus showing its approximation guarantee holds with high probability, implying it is the optimal algorithm in terms of running time and approximation guarantee for the problem of maximizing a submodular function under a cardinality constraint; 1 The same guarantee holds for the variants studied in (Malioutov et al., 2016; Lindgren et al., 2016; Sharma et al., 2015; Li et al., 2016); • Submodular optimization under noise.
33	10	We describe the results for general monotone submodular functions in Section 2, and the improved analysis for modular functions in Section 3.
34	25	In Section 4 we consider the more general problem of maximizing a submodular function under matroid constraints, and show the inapproximabilities of stochastic local search algorithms in Section 5.
40	8	In the stochastic version, the algorithm may no longer add the element whose marginal contribution is largest.
45	61	Let S be the set of k elements selected by a stochastic greedy algorithm s.t.
46	84	in each iteration i ∈ [k] the algorithm selects an element whose marginal contribution is an ξi approximation to the marginal contribution of the element with the largest marginal contribution at that stage, and let µ̂ = 1k ∑k i=1 ξi.
47	44	, ai}, and let the optimal solution be O, i.e. O ∈ argmaxT :|T |≤k f(T ).
48	27	Note that for any i < k we have that: f(Si+1)− f(Si) = fSi(ai+1) ≥ ξi+1 max o∈O fSi(o) ≥ ξi+1 k fSi(O) = ξi+1 k (f(O ∪ Si)− f(Si)) ≥ ξi+1 k (f(O)− f(Si)) Rearranging, we get: f(Si+1) ≥ ξi+1 k ( f(O)− f(Si) ) + f(Si) (1) We will show by induction that in every iteration i ∈ [k]: f(Si) ≥ 1− i∏ j=1 ( 1− ξj k ) f(O) The base case is when i = 1, and S0 = ∅ and by (1): f(S1) ≥ ξi+1 k ( f(O)− f(S0) ) = 1− ( 1− ξ1 k ) f(O) For a general iteration i+1, applying (1) for iteration i+1 and using the inductive hypothesis: f(Si+1) ≥ ξi+1 k f(O) + ( 1− ξi+1 k ) f(Si) ≥ ξi+1 k f(O) + ( 1− ξi+1 k )1− i∏ j=1 ( 1− ξj k ) f(O) = 1− i+1∏ j=1 ( 1− ξj k ) f(O) Since 1− x ≤ e−x, the above inequality implies: f(S) = f(Sk) = 1− k∏ j=1 ( 1− ξj k ) f(O) ≥ ( 1− e− 1k ∑k i=1 ξi ) f(O) = ( 1− e−µ̂ ) f(O).
