23	3	Since learning from scratch is difficult for REINFORCE, we combine it with an iterative max- imum likelihood (ML) training process, where beam search is used to find pseudo-gold programs, which are then used to augment the objective of REINFORCE.
29	4	Before diving into details, we define the semantic parsing task: given a knowledge base K, and a question x = (w1, w2, ..., wm), produce a program or logical form z that when executed against K generates the right answer y.
36	3	We adopt a Lisp interpreter as the “computer”.
37	2	A program C is a list of expressions (c1...cN ), where each expression is either a special token “Return” indicating the end of the program, or a list of tokens enclosed by parentheses “(FA1...AK)”.
50	6	The interpreter prunes the “programmer”’s search space by orders of magnitude, and enables learning from weak supervision on a large KB.
52	3	We base our programmer on a standard seq2seq model with attention, but extend it with a key-variable memory that allows the model to learn to represent and refer to program variables (Figure 2).
54	3	We used a 1-layer GRU (Cho et al., 2014) for both the encoder and decoder.
55	5	Given a sequence of words w1, w2...wm, each word wt is mapped to an embedding qt (embedding details are in Section 3).
65	4	During decoding, when a full expression is generated (i.e., the decoder generates “)”), it gets executed, and the result is stored as the value of a new variable in the “computer”.
75	3	Therefore, we base our training procedure on REINFORCE (Williams, 1992; Norouzi et al., 2016).
76	5	When the reward signal is sparse and the search space is large, it is common to utilize some full supervision to pre-train REINFORCE (Silver et al., 2016).
96	2	Iterative ML If we had gold programs, we could directly optimize their likelihood.
110	9	For example, differentiating PARENTSOF vs. SIBLINGSOF vs. CHILDRENOF can be challenging.
122	4	On top of imitation learning, our approach is related to the common practice in reinforcement learning (Schaul et al., 2016) to replay rare successful experiences to reduce the training variance and improve training efficiency.
136	5	Following (Yih et al., 2015) we used the last publicly available snapshot of Freebase (Bollacker et al., 2008).
146	2	The embedding is constructed by concatenating the average of word embeddings in the domain and type name to the average of word embeddings in the property name.
149	13	The dimension of encoder hidden state, decoder hidden state and key embeddings are all 50.
158	3	Because the dataset is small and some relations are only used once in the whole training set, we train the model on the entire training set for 200 iterations with the best hyperparameters.
159	5	Then we train the model with learning rate decay until convergence.
161	4	Since decoding needs to query the knowledge base (KB) constantly, the speed bottleneck for training is decoding.
162	7	We address this problem in our implementation by partitioning the dataset, and using multiple decoders in parallel to handle each partition.
181	2	Table 3 compares augmented REINFORCE, REINFORCE, and iterative ML on the validation set.
189	2	Given the small size of the dataset, overfitting is a major problem for training neural network models.
192	8	Among the programs generated by the model, a significant portion (36.7%) uses more than one expression.
193	6	From Table 6, we can see that the performance doesn’t decrease much as the composi- tional depth increases, indicating that the model is effective at capturing compositionality.
196	3	Error analysis Error analysis on the validation set shows two main sources of errors: 1.
224	6	It integrates neural networks with a symbolic nondifferentiable computer to support abstract, scalable and precise operations through a friendly neural computer interface.
226	34	Because the interpreter is non-differentiable and to directly optimize the task reward, we apply REINFORCE and use pseudo-gold programs found by an iterative ML training process to bootstrap training.
227	171	NSM achieves new state-of-the-art results on a challenging semantic parsing dataset with weak supervision, and significantly closes the gap between weak and full supervision.
228	165	It is trained endto-end, and does not require any feature engineering or domain-specific knowledge.
