6	18	Models based on the encoder-decoder framework have shown success in image captioning.
7	11	According to the pivot representation, they can be lancopku/simNet roughly categorized into models based on visual information (Vinyals et al., 2015; Chen and Zitnick, 2015; Mao et al., 2014; Karpathy and Li, 2015, 2017), and models based on conceptual information (Fang et al., 2015; You et al., 2016; Wu et al., 2016).
8	17	The later explicitly provides the visual words (e.g. dog, sit, red) to the decoder instead of the image features, and is more effective in image captioning according to the evaluation on benchmark datasets.
12	42	In contrast, models based on the visual information often are accurate in details but have difficulty in describing the image comprehensively and tend to only describe a subregion.
13	49	In this work, we get the best of both worlds and integrate visual attention and semantic attention for generating captions that are both detailed and comprehensive.
14	55	We propose a Stepwise ImageTopic Merging Network as the decoder to guide the information flow between the image and the extracted topics.
19	26	Overall, the main contributions of this work are: • We propose a novel approach that can effectively merge the information in the image and the topics to generate cohesive captions that are both detailed and comprehensive.
45	20	Then, we introduce the proposed stepwise image-topic merging decoder in detail.
59	17	Different from existing work that uses all the most frequent words in the captions as valid semantic concepts or visual words, we only include the object words (nouns) in the topic word list.
71	11	αt ∈ Rk is the attentive weights of V and the attentive visual input zt ∈ Rg is calculated as zt = V αt (4) The visual input zt and the embedding of the previous output word yt−1 are the input of the LSTM.
77	15	The attentive visual output is further transformed to rt = tanh(W s,zz̃t),W s,z ∈ Re×g, which is of the same dimension as the topic word embedding to simplify the following procedure.
81	13	It should be beneficial to provide the attentive visual information when selecting topics.
84	18	βt ∈ Rm is the weight of the topics, from which the attentive conceptual output qt ∈ Re is calculated: qt = Tβt (12) The topic attention qt and the hidden state ht are combined as the contextual information st: st = tanh(W s,qqt +W s,hht) (13) where W s,q ∈ Re×e,W s,h ∈ Re×d are learnable parameters.
85	33	Merging Gate We have prepared both the visual information rt and the contextual information st.
86	11	It is not reasonable to treat the two kinds of information equally when the decoder generates different types of words.
88	14	However, when generating object words (e.g., people, table), st is more important.
89	32	We introduce a novel score-based merging mechanism to make the model adaptively learn to adjust the balance: γt = σ(S(st)− S(rt)) (14) ct = γtst + (1− γt)rt (15) where σ is the sigmoid function, γt ∈ [0, 1] indicates how important the topic attention is compared to the visual attention, and S is the scoring function.
92	14	Finally, the output word is generated by: yt ∼ pt = softmax(W p,cct) (18) where each value of pt ∈ R|D| is a probability indicating how likely the corresponding word in vocabulary D is the current output word.
102	14	We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE.
107	23	In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015).
122	18	We compare our approach with various representative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-theart on the two datasets in comparable settings.
125	13	Moreover, our model overpasses the state-of-theart with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments (Anderson et al., 2016).
137	14	In this section, we analyze the contribution of each component in the proposed approach, and give examples to show the strength and the potential improvements of the model.
153	15	Combining the input attention and the output attention can further improve the results, especially in color and size descriptions.
154	13	Topic Attention As expected, compared with visual attention, the topic attention is better at identifying objects but worse at identifying attributes.
159	14	However, directly combining them also causes lower scores in attributes, color, count, and size, showing that the advantages are not fully made use of.
160	26	The most dramatic improvements come from applying the merging gate to the combined attention, showing that the proposed balance mechanism can adaptively combine the two kinds of information and is essential to the overall performance.
173	15	When generating chair, the topic attention is focused on a wrong object bed, while the visual attention attends correctly to the chair, and especially the output attention attends to the armrest.
186	30	To our knowledge, we are the first to combine the visual and the semantic attention to achieve substantial improvements.
195	23	The SPICE metrics are only available at the leaderboard on the COCO dataset website5, which, unfortunately, has not been updated for more than a year.
