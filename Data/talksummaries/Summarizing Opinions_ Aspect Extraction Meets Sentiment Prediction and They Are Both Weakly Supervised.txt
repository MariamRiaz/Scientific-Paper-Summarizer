16	63	Our contributions in this work are three-fold: a novel neural framework for the identification and extraction of salient customer opinions that combines aspect and sentiment information and does not require unrealistic amounts of supervision; the introduction of an opinion summarization dataset which consists of Amazon reviews from six product domains, and includes development and test sets with gold standard aspect annotations, salience labels, and multi-document extractive summaries; a large-scale user study on the quality of the final summaries paired with automatic evaluations for each stage in the summarization pipeline (aspects, extraction accuracy, final summaries).
39	22	For every product e, the corpus contains a set of reviews Re = {ri}|Re|i=1 expressing customers’ opinions.
53	25	We also assume that segments conveying highly positive or negative sentiment are more likely to present informative opinions compared to neutral ones, a claim supported by previous work (Angelidis and Lapata, 2018).
58	29	The model learns a segment-level aspect predictor without supervision by attempting to reconstruct the input segment’s encoding as a linear combination of aspect embeddings.
66	54	The segment’s vector is then reconstructed as the weighted sum of aspect embeddings: rs = A Tpasps .
67	23	(5) The model is trained by minimizing a reconstruction loss Jr(θ) that uses randomly sampled segments n1, n2, .
68	98	, nkn as negative examples: 2 Jr(θ) = ∑ s∈C kn∑ i=1 max(0, 1− rsvs + rsvni) (6) ABAE is essentially a neural topic model; it discovers topics which will hopefully map to aspects, without any preconceptions about the aspects themselves, a feature shared with most previous LDA-style aspect extraction approaches (Titov and McDonald, 2008a; He et al., 2017; Mukherjee and Liu, 2012).
70	17	This requires a many-to-one mapping between discovered topics and genuine aspects which is performed manually.
77	24	Figure 2 (top) depicts four television aspects (image, sound, connectivity and price) and three of their seeds in word embedding space.
78	19	MATE replaces ABAE’s aspect dictionary with multiple seed matrices {A1,A2, .
94	25	For example, the words colors and crisp, in the segment “The colors are perfectly crisp” should be sufficient to infer that the seg- ment comes from a television review, whereas the words keys and type in the segment “The keys feel great to type on” are more representative of the keyboard domain.
104	33	We identify segment polarity with the recently proposed Multiple Instance Learning Network model (MILNET; Angelidis and Lapata 2018).
115	29	, p(aK)s 〉 and polarities pols, form the opinion set Oe = {(s,As, pols)}s∈Re for every product e ∈ EC.
119	38	Opinion Selection The final step towards producing summaries is to discard potentially redundant opinions, something that is not taken into account by our salience scoring method.
123	17	We start with the highest ranked opinion, and keep adding opinions to the final summary one by one, unless the cosine similarity between the candidate segment and any segment already included in the summary is lower than 0.5.
124	29	We created OPOSUM, a new dataset for the training and evaluation of Opinion Summarization models which contains Amazon reviews from six product domains: Laptop Bags, Bluetooth Headsets, Boots, Keyboards, Televisions, and Vacuums.
139	33	Again, using the Kappa coefficient, agreement among annotators was K = 0.51 (N = 8,175, k = 3).4 In the second stage, annotators were shown the salient segments they identified (for every product) and asked to create a final extractive summary by choosing opinions based on their popularity, fluency and clarity, while avoiding redundancy and staying under a budget of 100 words.
153	33	Our MATE model and its multitask counterpart (MATE+MT) were compared against a majority baseline and two ABAE variants: vanilla ABAE, where aspect matrix A is initialized using k-means centroids and fine-tuned during training; and ABAEinit, where rows of A are fixed to the centroids of respective seed embeddings.
156	22	Our models outperform both variants of ABAE across domains.
161	16	The first phase of our opinion extraction annotation provides us with binary salience labels, which we use as gold standard to evaluate system opinion rankings.
165	15	The combined use of polarity and aspect information improves the retrieval of salient opinions across domains, as all model variants that use our salience formula of Equation (12) outperform the MILNET- and aspect-only baselines.
169	49	Opinion Summaries We now turn to the summarization task itself, where we compare our best performing model (MILNET+MATE+MT), with and without a redundancy filter (RD), against the following methods: a baseline that selects segments randomly; a Lead baseline that only selects the leading segments from each review; SumBasic, a generic frequency-based extractive summarizer (Nenkova and Vanderwende, 2005); LexRank, a generic graph-based extractive summarizer (Erkan and Radev, 2004); Opinosis, a graph-based abstractive summarizer that is designed for opinion summarization (Ganesan et al., 2010).
179	19	Every triplet was shown to three crowdworkers, who were asked to decide which summary was best and which one was worst according to four criteria: Informativeness (How much useful information about the product does the summary provide?
183	25	For every criterion, a system’s score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst (Orme, 2009).
191	50	Lastly, the abstractive summary of Opinosis does a good job of capturing opinions about specific aspects but lacks in fluency, as it produces grammatical errors.
192	24	For additional system outputs, see supplementary material.
195	20	We evaluated our weakly supervised models on a new opinion summarization corpus across three subtasks, namely aspect identification, salient opinion extraction, and summary generation.
196	44	Our approach delivered significant improvements over strong baselines in each of the subtasks, while a large-scale judgment elicitation study showed that crowdworkers favor our summarizer over competitive extractive and abstractive systems.
197	113	In the future, we plan to develop a more integrated approach where aspects and sentiment orientation are jointly identified, and work with additional languages and domains.
198	133	We would also like to develop methods for abstractive opinion summarization using weak supervision signals.
