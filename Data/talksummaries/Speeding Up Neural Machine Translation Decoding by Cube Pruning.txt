0	20	Neural machine translation (NMT) has shown promising results and drawn more attention recently (Kalchbrenner and Blunsom, 2013; Cho et al., 2014b; Bahdanau et al., 2015; Gehring et al., 2017a,b; Vaswani et al., 2017).
1	16	A widely used architecture is the attention-based encoder-decoder framework (Cho et al., 2014b; Bahdanau et al., 2015) which assumes there is a common semantic space between the source and target language pairs.
3	44	To generate a target word, a probability distribution over the target vocabulary is drawn based on the attention over the entire source sequence and the target information rolled by another RNN.
4	43	At the training time, the decoder is forced to generate the ground truth sentence, while at inference, it needs to employ the beam search algorithm to search through a constrained space due to the huge search space.
8	13	The second reason is that large vocabulary on target side is employed to avoid unknown words (UNKs), which leads to a large number of normalization factors for the softmax operation when drawing the probability distribution.
10	8	In this paper, we borrow ideas from phrasebased and syntax-based machine translation where cube pruning has been successfully applied to speed up the decoding (Chiang, 2007; Huang and Chiang, 2007).
55	10	On CPUs, the most expensive computational time cost was caused by the softmax operation over the entire target vocabulary2.
56	40	In order to avoid the time-consuming normalization operation in testing, we introduced self-normalization (denoted as SN) into the training.
69	9	The constraint we used here is that items being merged in the previous beam should have the same target words.
71	13	Elements in the sub-cube are sorted by previous accumulated NLL along the columns (the first dimension of beam size) and by the approximate predictions along the rows (the second dimension of vocabulary size).
78	9	Different from the naive beam search, we first group items in the previous beam into two sub-cubes C1 and C2 in term of the target word yj−1.
81	16	For each sub-cube, we use the first state vector in each sub-cube as the approximate one to produce the next probability distribution and the next state.
82	38	At beginning, each upperleft corner element in each sub-cube is pushed into a minimum heap, after popping minimum element from the heap, we calculate and restore the exact NLL of the element, then push the right and lower ones alongside the minimum element into heap.
86	15	We refer above algorithm as the naive cube pruning algorithm (called NCP)
90	19	Besides, Each sub-cube only requires one forward calculation.
97	10	The lengths of the sentences on both sides were limited up to 50 tokens, then actually 1.11M sentence pairs were left with 25.0M Chinese words and 27.0M English words.
115	12	NBS-SN: Naive Beam Search without SN NBS+SN: Naive Beam Search with SN NCP-SN: Cube Pruning without SN NCP+SN: Cube Pruning with SN ACP-SN: Accelerated Cube Pruning without SN ACP+SN: Accelerated Cube Pruning with SN
116	22	We first give the definition of the Average Merging Rate (denoted as AMR).
118	13	We used the pre-trained model to translate the test dataset on a single GeForce GTX TITAN X GPU server.
123	12	Comparing the curves in the Figure 3, we could observe that the naive beam search does not conduct any merging operation in the whole searching process, while the average merging rate in the cube pruning almost grows as the beam size increases.
134	17	Similar to the experiments conducted on GPUs, we also translated the whole MT03 test dataset on the CPU server by using all six search strategies under different beam sizes.
135	15	The trends of the BLEU scores over those strategies are shown in Figure 5.
136	10	The proposed search methods gain the similar superiority on CPUs to that on GPUs, and the decoding speed is obviously slower than that on GPUs.
137	16	From the Figure 5a, we can also clearly see that, compared with the NBS-SN, NCP-SN only speeds up the decoder a little, ACP-SN produces much more acceleration.
139	17	The self-normalization made the ACP strategy faster than the baseline by about 3.5×, in which condition the NBS+SN got the best BLEU score 38.05 with beam size 30 while the ACP+SN achieved the highest score 38.12 with beam size 30.
140	10	The results could be observed in Figure 5b.
142	11	Thus, the accelerated cube pruning with self-normalization could improve the search quality and efficiency stably.
146	8	Figure 7a and 7b show that the accelerated cube pruning algorithm speeds up the decoding by about 4.2× on CPU server with the beam size 40.
157	15	For each step in beam search, we grouped similar candidates in previous beam into one or more equivalence class(es), and bad hypotheses were pruned out.
160	38	Also, the translation precision could be the same or even better in both situations.
