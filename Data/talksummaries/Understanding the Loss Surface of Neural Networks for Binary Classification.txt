28	36	Conclusions are presented in Section 6.
30	11	Given an input vector x of dimension d, we consider a neural network with L layers for binary classification.
34	44	Therefore, the output of the network f : Rd → R can be expressed by f(x;θ) = W⊤L σ ( ...σ(W⊤1 x+ b1) + bL−1 ) + bL, where θ denotes all parameters in the neural network.
40	10	Let D = {(xi, yi)}ni=1 denote a dataset with n samples, each independently drawn from the distribution PX×Y .
43	68	We define the training error (also called the misclassification error) R̂n(θ) as the misclassification rate of the neural network f(x;θ) on the datasetD, i.e., R̂n(θ) = 1 n n∑ i=1 I{yi ̸= sgn(f(xi;θ))}, where I{·} is the indicator function.
44	32	The training error R̂n measures the classification performance of the network f on the finite samples in the dataset D.
46	18	We first introduce several important conditions in order to derive the main results, and we will provide further discussions on these conditions in the next section.
48	15	Assumption 1 (Loss function) Let ℓp : R → R denote a loss function satisfying the following conditions: (1) ℓp is a surrogate loss function, i.e., ℓp(z) ≥ I{z ≥ 0} for all z ∈ R, where I(·) denotes the indicator function; (2) ℓp has continuous derivatives up to order p onR; (3) ℓp is nondecreasing (i.e., ℓ′p(z) ≥ 0 for all z ∈ R) and there exists a positive constant z0 such that ℓ′p(z) = 0 iff z ≤ −z0.
61	9	Assumption 3 assumes that the positive and negative samples are not located on the same linear subspace.
62	16	Previous works (Belhumeur et al., 1997; Chennubhotla & Jepson, 2001; Cootes et al., 2001; Belhumeur et al., 1997) have observed that some classes of natural images (e.g., images of faces, handwritten digits, etc) can be reconstructed from lower-dimensional representations.
66	8	Assumption 4 (Network architecture) Assume that the neural network f is a single-layered neural network, or more generally, has shortcut-like connections shown in Fig 1 (b), where fS is a single layer network and fD is a feedforward network.
77	10	the parameters θS in the network fS up to a sufficiently high order and allow us to use Taylor expansion in the analysis.
81	10	which can be used in the network fS : softplus neuron, i.e., σ(z) = log2(1 + e z), quadratic neuron, i.e, σ(z) = z2, etc.
84	11	Now we present the following theorem to show that when assumptions 1-5 are satisfied, every local minimum of the empirical loss function has zero training error if the number of neurons in the network fS are chosen appropriately.
86	7	Assume that samples in the dataset D = {(xi, yi)}ni=1, n ≥ 1 are independently drawn from the distribution PX×Y .
88	7	If θ∗ = (θ∗S ,θ ∗ D) is a local minimum of the loss function L̂n(θS ,θD; p) and p ≥ 6, then R̂n(θ∗S ,θ∗D) = 0 holds with probability one.
98	31	We note here that although the result is stronger with quadratic neurons, it does not imply that the quadratic neuron has advantages over the other types of neurons (e.g., softplus neuron, etc).
102	9	As shown in Theorem 1, when the data distribution satisfies Assumption 2 and 3, every local minimum of the empirical loss has zero training error.
104	10	Therefore, to provide a complementary result to Theorem 1, we consider the case where the data distribution is linearly separable.
105	11	Before presenting the result, we first present the following assumption on the data distribution.
107	21	In Theorem 2, we will show that when the samples drawn from the data distribution are linearly separable, and the network has a shortcut-like connection shown in Figure 1, all local minima of the empirical loss function have zero training errors if the type of the neuron in the network fS are chosen appropriately.
110	7	Assume that the single layer network fS has M ≥ 1 neurons and neurons σ in the network fS are twice differentiable and satisfy σ′(z) > 0 for all z ∈ R. If θ∗ = (θ∗S ,θ∗D) is a local minimum of the loss function L̂n(θS ,θD; p), p ≥ 3, then R̂n(θ ∗ S ,θ ∗ D) = 0 holds with probability one.
111	9	Remark: Similar to Proposition 1, Theorem 2 does not require the number of neurons to be in scale with the number of samples.
112	9	In fact, we make a weaker assumption here: the single layer network fS only needs to have at least one neuron, in contrast to at least r neurons required by Proposition 1.
119	24	In the following, for each class of neurons, we show whether the main results hold and provide counterexamples if certain conditions in the main results are violated.
134	7	When the shortcut-like connections (i.e., the network fS in Figure 1(b)) are removed, the network architecture can be viewed as a standard feedforward neural network.
135	19	We provide a counterexample to show that, for a feedforward network with ReLU neurons, even if the other conditions in Theorem 1 or 2 are satisfied, the empirical loss functions is likely to have a local minimum with non-zero training error.
136	24	In other words, neither Theorem 1 nor 2 holds when the shortcut-like connections are removed.
137	21	Proposition 3 Suppose that assumption 1 is satisfied.
138	14	Assume that the feedforward network f(x;θ) has at least one hidden layer and at least one neuron in each hidden layer.
