39	19	However, replacing this tailored analysis with the more generic “advanced composition theorem” for differential privacy (Dwork et al., 2010), one still obtains useful results for the more general class of “low-sensitivity” submodular functions.
52	15	This phenomenon is analogous to how stochastic variants of gradient descent are more naturally suited to providing differential privacy than their deterministic counterparts (Song et al., 2013; Bassily et al., 2014).
55	20	For instance, one may wish to maximize a submodular function f(S) subject to S ∈ I for an arbitrary matroid I, or subject to S being contained in an intersection of p matroids (more generally, a p-extendible system).
61	21	For instance, one can replace the exponential mechanism in a black-box manner with the “large margin mechanism” of Chaudhuri et al. (2014) to obtain error bounds that replace the explicit dependence on log |V | in Table 1 with a term that may be significantly smaller for real datasets.
66	14	Suppose each dataset D is associated to a set function fD : 2V → R. The manner in which fD depends on D will be application-specific, but it is assumed that the association between D and fD is public information.
69	12	If for every dataset D = (x1, .
76	13	The sensitivity of a set function fD : 2V → R (depending on a dataset D) with respect to a constraint C ⊆ 2V is defined as max D∼D′ max S∈C |fD(S)− fD′(S)|.
120	12	A slight modification of Algorithm 1 gives a unified algorithm for privately maximizing a monotone submodular function subject to matroid and pextendible system constraints, presented as Algorithm 2.
122	8	Let δ > 0 and let ε0 ≥ 0 be such that ε = 2 · ε0 · (e − 1) ln(3e/δ) ≤ 1.
129	9	It also provides (ε, δ)-differential privacy for every δ > 0 with ε = r(I)ε2/2 + ε · √ 2r(I) ln(1/δ).
144	9	This makes our “subsample-greedy” algorithm the fastest algorithm for maximizing a general submodular function subject to a cardinality constraint (albeit with slightly worse approximation guarantees).
147	8	Assume that V is augmented by enough “dummy elements” to ensure that |V |/k is an integer; each dummy element u is defined so that fD(S ∪ {u}) = fD(S) for every set S. We also explicitly account for an additional set U of k dummy elements, and ensure that at least one appears in every subsample.
148	7	Private “Subsample-Greedy” SGO Input: Submodular function fD : 2V → R, dataset D, cardinality constraint k, privacy parameters ε0, δ0 Output: Size k subset of V 1.
150	11	Return Sk with all dummy elements removed Theorem 11.
151	16	Suppose fD : 2V → R has sensitivity λ.
156	13	Let fD : 2V → R be any submodular function.
159	9	The accuracy guarantee of the exponential mechanism can be pessimistic on datasets where q(·, D) exhibits additional structure.
160	10	For example, suppose that when the elements of V are sorted so that q(v1, D) ≥ q(v2, D) ≥ · · · ≥ q(v|V |, D), there exists an ` such that q(v1, D) q(v`+1, D).
161	11	Then only the top ` ground set items are relevant to the optimization problem, so running the exponential mechanism on these should maintain differential privacy, but with error proportional to ln ` rather than to ln |V |.
180	31	• LGrid is a set of 33 locations spread evenly across Manhattan in a grid-like manner.
181	51	We define a utility function M(i, j) to be the normalized Manhattan distance between a pickup location i and the waiting location j.
192	14	Figures 1(c) and (d) show how the utility of the EM-based and LMM-based algorithms vary with the privacy parameter .
199	23	For example, for the LPopular set of locations, the Empire State Building is close to the New York Public Library, the Soho Grand Hotel is close to NYU, and the Grand Army Plaza is close to the UN Headquarters.
206	8	We analyze a dataset created from a combination of National Health Examination Surveys ranging from 2007 to 2014 (NHANESDataset).
207	11	There are n = 23, 876 individuals in the dataset with information on whether or not they have diabetes, along with m = 23 other potentially related binary health features.
210	9	Mutual information takes the form: I(Y ;X) = ∑ y∈Y ∑ x∈X p(x, y) log2 ( p(x, y) p(x)p(y) ) .
212	12	, Xk) takes the form p(y, x1, .
218	7	We run 1,000 simulations with = 1.0 and δ = 2−20.
230	25	This work was supported by DARPA Young Faculty Award (D16AP00046), Simons-Berkeley fellowship, and ERC StG 307036.
231	22	This work was done in part while Amin Karbasi and Andreas Krause were visiting the Simons Institute for the Theory of Computing.
