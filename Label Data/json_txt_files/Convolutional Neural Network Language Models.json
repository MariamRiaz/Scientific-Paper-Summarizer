{"sections": [{"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1153\u20131162, Austin, Texas, November 1-5, 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Convolutional Neural Networks (CNNs) are the family of neural network models that feature a type of layer known as the convolutional layer. This layer can extract features by convolving a learnable filter (or kernel) along different positions of a vectorial input.\nCNNs have been successfully applied in Computer Vision in many different tasks, including ob-\nject recognition, scene parsing, and action recognition (Gu et al., 2015), but they have received less attention in NLP. They have been somewhat explored in static classification tasks where the model is provided with a full linguistic unit as input (e.g. a sentence) and classes are treated as independent of each other. Examples of this are sentence or document classification for tasks such as Sentiment Analysis or Topic Categorization (Kalchbrenner et al., 2014; Kim, 2014), sentence matching (Hu et al., 2014), and relation extraction (Nguyen and Grishman, 2015). However, their application to sequential prediction tasks, where the input is construed to be part of a sequence (for example, language modeling or POS tagging), has been rather limited (with exceptions, such as Collobert et al. (2011)). The main contribution of this paper is a systematic evaluation of CNNs in the context of a prominent sequential prediction task, namely, language modeling.\nStatistical language models are a crucial component in many NLP applications, such as Automatic Speech Recognition, Machine Translation, and Information Retrieval. Here, we study the problem under the standard formulation of learning to predict the upcoming token given its previous context. One successful approach to this problem relies on counting the number of occurrences of n-grams while using smoothing and back-off techniques to estimate the probability of an upcoming word (Kneser and Ney, 1995). However, since each individual word is treated independently of the others, n-gram models fail to capture semantic relations between words. In contrast, neural network language models (Bengio et al., 2006) learn to predict the up-\n1153\ncoming word given the previous context while embedding the vocabulary in a continuous space that can represent the similarity structure between words. Both feed-forward (Schwenk, 2007) and recurrent neural networks (Mikolov et al., 2010) have been shown to outperform n-gram models in various setups (Mikolov et al., 2010; Hai Son et al., 2011). These two types of neural networks make different architectural decisions. Recurrent networks take one token at a time together with a hidden \u201cmemory\u201d vector as input and produce a prediction and an updated hidden vector for the next time step. In contrast, feed-forward language models take as input the last n tokens, where n is a fixed window size, and use them jointly to predict the upcoming word.\nIn this paper we define and explore CNN-based language models and compare them with both feedforward and recurrent neural networks. Our results show a 11-26% perplexity reduction of the CNN with respect to the feed-forward language model, comparable or higher performance compared to similarly-sized recurrent models, and lower performance with respect to larger, state-of-the-art recurrent language models (LSTMs as trained in Zaremba et al. (2014)).\nOur second contribution is an analysis of the kind of information learned by the CNN, showing that the network learns to extract a combination of grammatical, semantic, and topical information from tokens of all across the input window, even those that are the farthest from the target."}, {"heading": "2 Related Work", "text": "Convolutional Neural Networks (CNNs) were originally designed to deal with hierarchical representation in Computer Vision (LeCun and Bengio, 1995). Deep convolutional networks have been successfully applied in image classification and understanding (Simonyan and Zisserman, 2014; He et al., 2015). In such systems the convolutional kernels learn to detect visual features at both local and more abstract levels.\nIn NLP, CNNs have been mainly applied to static classification tasks for discovering latent structures in text. Kim (2014) uses a CNN to tackle sentence classification, with competitive results. The same work also introduces kernels with varying window\nsizes to learn complementary features at different aggregation levels. Kalchbrenner et al. (2014) propose a convolutional architecture for sentence representation that vertically stacks multiple convolution layers, each of which can learn independent convolution kernels. CNNs with similar structures have also been applied to other classification tasks, such as semantic matching (Hu et al., 2014), relation extraction (Nguyen and Grishman, 2015), and information retrieval (Shen et al., 2014). In contrast, Collobert et al. (2011) explore a CNN architecture to solve various sequential and non-sequential NLP tasks such as part-of-speech tagging, named entity recognition and also language modeling. This is perhaps the work that is closest to ours in the existing literature. However, their model differs from ours in that it uses a max-pooling layer that picks the most activated feature across time, thus ignoring temporal information, whereas we explicitly avoid doing so. More importantly, the language models trained in that work are only evaluated through downstream tasks and through the quality of the learned word embeddings, but not on the sequence prediction task itself, as we do here.\nBesides being applied to word-based sequences, the convolutional layers have also been used to model sequences at the character level. Kim et al. (2015) propose a recurrent language model that replaces the word-indexed projection matrix with a convolution layer fed with the character sequence that constitutes each word to find morphological patterns. The main difference between that work and ours is that we consider words as the smallest linguistic unit, and thus apply the convolutional layer at the word level.\nStatistical language modeling, the task we tackle, differs from most of the tasks where CNNs have been applied before in multiple ways. First, the input typically consists of incomplete sequences of words rather than complete sentences. Second, as a classification problem, it features an extremely large number of classes (the words in a large vocabulary). Finally, temporal information, which can be safely discarded in many settings with little impact in performance, is critical here: An n-gram appearing close to the predicted word may be more informative, or yield different information, than the same n-gram appearing several tokens earlier."}, {"heading": "3 Models", "text": "Our model is constructed by extending a feedforward language model (FFLM) with convolutional layers. In what follows, we first explain the implementation of the base FFLM and then describe the CNN model that we study."}, {"heading": "3.1 Baseline FFLM", "text": "Our baseline feed-forward language model (FFLM) is almost identical to the original model proposed by Bengio et al. (2006), with only slight changes to push its performance as high as we can, producing a very strong baseline. In particular, we extend it with highway layers and use Dropout as regularization. The model is illustrated in Figure 1 and works as follows. First, each word in the input n-gram is mapped to a low-dimensional vector (viz. embedding) though a shared lookup table. Next, these word vectors are concatenated and fed to a highway layer (Srivastava et al., 2015). Highway layers improve the gradient flow of the network by computing as output a convex combination between its input (called the carry) and a traditional non-linear transformation of it (called the transform). As a result, if there is a neuron whose gradient cannot flow through the transform component (e.g., because the activation is zero), it can still receive the back-propagation update signal through the carry gate. We empirically observed the usage of a single highway layer to significantly improve the performance of the model. Even though a systematic evaluation of this aspect is beyond the scope of the current paper, our empirical results demonstrate that the resulting model is a very competitive one (see Section 4).\nFinally, a softmax layer computes the model prediction for the upcoming word. We use ReLU for all non-linear activations, and Dropout (Hinton et al., 2012) is applied between each hidden layer."}, {"heading": "3.2 CNN and variants", "text": "The proposed CNN network is produced by injecting a convolutional layer right after the words in the input are projected to their embeddings (Figure 2). Rather than being concatenated into a long vector, the embeddings xi \u2208 Rk are concatenated transversally producing a matrix x1:n \u2208 Rn\u00d7k, where n is\nthe size of the input and k is the embedding size. This matrix is fed to a time-delayed layer, which convolves a sliding window of w input vectors centered on each word vector using a parameter matrix W \u2208 Rw\u00d7k. Convolution is performed by taking the dot-product between the kernel matrix W and each sub-matrix xi\u2212w/2:i+w/2 resulting in a scalar value for each position i in input context. This value represents how much the words encompassed by the window match the feature represented by the filter W . A ReLU activation function is applied subsequently so negative activations are discarded. This operation is repeated multiple times using various kernel matrices W , learning different features independently. We tie the number of learned kernels to be the same as the embedding dimensionality k, such that the output of this stage will be another matrix of dimensions n \u00d7 k containing the activations for each kernel at each time step. The number of kernels was tied to the embedding size for two reasons, one practical, namely, to limit the hyper parameter search, one methodological, namely, to keep the network structure identical to that of the baseline feed-forward model.\nNext, we add a batch normalization stage immediately after the convolutional output, which facilitates learning by addressing the internal covariate\nshift problem and regularizing the learned representations (Ioffe and Szegedy, 2015).\nFinally, this feature matrix is directly fed into a fully connected layer that can project the extracted features into a lower-dimensional representation. This is different from previous work, where a max-over-time pooling operation was used to find the most activated feature in the time series. Our choice is motivated by the fact that the max pooling operator loses the specific position where the feature was detected, which is important for word prediction.\nAfter this initial convolutional layer, the network proceeds identically to the FFNN by feeding the produced features into a highway layer, and then, to a softmax output.\nThis is our basic CNN architecture. We also experiment with three expansions to the basic model, as follows. First, we generalize the CNN by extending the shallow linear kernels with deeper multilayer perceptrons, in what is called a MLP Convolution (MLPConv) structure (Lin et al., 2013). This allows the network to produce non-linear filters, and it has achieved state-of-the-art performance in object recognition while reducing the number of total layers compared to other mainstream networks. Concretely, we implement MLPConv networks by using another convolutional layer with a 1 \u00d7 1 kernel on top of the convolutional layer output. This results in an architecture that is exactly equivalent to sliding a one-hidden-layer MLP over the input. Notably, we do not include the global pooling layer in the original Network-in-Network structure (Lin et al., 2013).\nSecond, we explore stacking convolutional layers on top of each other (Multi-layer CNN or MLCNN) to connect the local features into broader regional representations, as commonly done in computer vision. While this proved to be useful for sentence representation (Kalchbrenner et al., 2014), here we have found it to be rather harmful for language modeling, as shown in Section 4. It is important to note that, in ML-CNN experiments, we stack convolutions with the same kernel size and number of kernels on top of each other, which is to be distinguished from the MLPConv that refers to the deeper structure in each CNN layer mentioned above.\nFinally, we consider combining features learned through different kernel sizes (COM), as depicted in\nFigure 3. For example, we can have a combination of kernels that learn filters over 3-grams with others that learn over 5-grams. This is achieved simply by applying in parallel two or more sets of kernels to the input and concatenating their respective outputs (Kim, 2014)."}, {"heading": "4 Experiments", "text": "We evaluate our model on three English corpora of different sizes and genres, the first two of which have been used for language modeling evaluation before. The Penn Treebank contains one million words of newspaper text with 10K words in the vocabulary. We reuse the preprocessing and training/test/validation division from Mikolov et\nal. (2014). Europarl-NC is a 64-million word corpus that was developed for a Machine Translation shared task (Bojar et al., 2015), combining Europarl data (from parliamentary debates in the European Union) and News Commentary data. We preprocessed the corpus with tokenization and true-casing tools from the Moses toolkit (Koehn et al., 2007). The vocabulary is composed of words that occur at least 3 times in the training set and contains approximately 60K words. We use the validation and test set of the MT shared task. Finally, we took a subset of the ukWaC corpus, which was constructed by crawling UK websites (Baroni et al., 2009). The training subset contains 200 million words and the vocabulary consists of the 200K words that appear more than 5 times in the training subset. The validation and test sets are different subsets of the ukWaC corpus, both containing 120K words. We preprocessed the data similarly to what we did for Europarl-NC.\nWe train our models using Stochastic Gradient Descent (SGD), which is relatively simple to tune compared to other optimization methods that involve additional hyper parameters (such as alpha in RMSprop) while being still fast and effective. SGD is commonly used in similar work (Devlin et al., 2014; Zaremba et al., 2014; Sukhbaatar et al., 2015). The learning rate is kept fixed during a single epoch, but we reduce it by a fixed proportion every time the validation perplexity increases by the end of the epoch. The values for learning rate, learning rate shrinking and mini-batch sizes as well as context size are fixed once and for all based on insights drawn from previous work (Hai Son et al., 2011; Sukhbaatar et al., 2015; Devlin et al., 2014) as well as experimentation with the Penn Treebank validation set.\nSpecifically, the learning rate is set to 0.05, with mini-batch size of 128 (we do not take the average of loss over the batch, and the training set is shuffled). We multiply the learning rate by 0.5 every time we shrink it and clip the gradients if their norm is larger than 12. The network parameters are initialized randomly on a range from -0.01 to 0.01 and the context size is set to 16. In Section 6 we show that this large context window is fully exploited.\nFor the base FFNN and CNN we varied embedding sizes (and thus, number of kernels) k = 128, 256. For k = 128 we explore the simple CNN,\nincrementally adding MLPConv and COM variations (in that order) and, alternatively, using a MLCNN. For k = 256, we only explore the former three alternatives (i.e. all but the ML-CNN). For the kernel size, we set it to w = 3 words for the simple CNN (out of options 3, 5, 7, 9), whereas for the COM variant we use w = 3 and 5, based on experimentation on PTB. However, we observed the models to be generally robust to this parameter. Dropout rates are tuned specifically for each combination of model and dataset based on the validation perplexity. We also add small dropout (p = 0.05\u20130.15) when we train the networks on the smaller corpus (Penn Treebank).\nThe experimental results for recurrent neural network language models, such as Recurrent Neural Networks (RNN) and Long-Short Term Memory models (LSTM), on the Penn Treebank are quoted from previous work; for Europarl-NC, we train our own models (we also report the performance of these in-house trained RNN and LSTM models on the Penn Treebank for reference). Specifically, we train LSTMs with embedding size k = 256 and number of layers L = 2 as well as k = 512 with L = 1, 2. We train one RNN with k = 512 and L = 2. To train these models, we use the published source code from Zaremba et al. (2014). Our own models are also implemented in Torch7 for easier comparison.1 Finally, we selected the best performing convolutional and recurrent language models on Europarl-NC and the Baseline FFLM to be evaluated on the ukWaC corpus.\nFor all models trained on Europarl-NC and ukWaC, we speed up training by approximating the softmax with Noise Contrastive Estimation (NCE) (Gutmann and Hyva\u0308rinen, 2010), with the parameters being set following previous work (Chen et al., 2015). Concretely, for each predicted word, we sample 10 words from the unigram distribution, and the normalization factor is such that lnZ = 9. 2\nFor comparison, we also implemented a simpler version of the FFNN without dropout and highway layers (Bengio et al., 2006). These networks have two hidden layers (Arisoy et al., 2012) with the size\n1Available at https://github.com/quanpn90/NCE CNNLM. 2We also experimented with Hierarchical Softmax (Mikolov et al., 2011) and found out that the NCE gave better performance in terms of speed and perplexity.\nof 2 times the embedding size (k), thus having the same number of parameters as our baseline."}, {"heading": "5 Results", "text": "Our experimental results are summarized in Table 1. First of all, we can see that, even though the FFNN gives a very competitive performance,3 the addition of convolutional layers clearly improves it even further. Concretely, we observe a solid 11-26% reduction of perplexity compared to the feed-forward network after using MLP Convolution, depending on the setup and corpus. CNN alone yields a sizable improvement (5-24%), while MLPConv, in line with our expectations, adds another approximately 2-5% reduction in perplexity. A final (smaller) improvement comes from combining kernels of size 3 and 5, which can be attributed to a more expressive model that can learn patterns of n-grams of different sizes. In contrast to the successful two variants above, the multi-layer CNN did not help in better capturing the regularities of text, but rather the opposite: the more convolutional layers were stacked, the worse the performance. This also stands in contrast to the tradition of convolutional networks in Computer Vision, where using very deep convolutional neural networks is key to having better models. Deep convolution for text representation is in contrast rather rare, and to our knowledge it has only been successfuly applied to sentence representation (Kalchbrenner et al., 2014). We conjecture that the reason why deep CNNs may not be so effective for language could be the effect of the convolution on the data: The convolution output for an image is akin to a new, more abstract image, which yet again can be subject to new convolution operations, whereas the textual counterpart may no longer have the same properties, in the relevant aspects, as the original linguistic input.\nRegarding the comparison with a stronger LSTM, our models can perform competitively under the same embedding dimension (e.g. see k = 256 of k = 512) on the first two datasets. However, the LSTM can be easily scaled using larger models, as shown in Zaremba et al. (2014), which gives the\n3In our experiments, increasing the number of fully connected layers of the FFNN is harmful. Two hidden layers with highway connections is the best setting we could find.\nbest known results to date. This is not an option for our model, which heavily overfits with large hidden layers (around 1000) even with very large dropout values. Furthermore, the experiments on the larger ukWaC corpus show an even clearer advantage for the LSTM, which seems to be more efficient at harnessing this volume of data, than in the case of the two smaller corpora.\nTo sum up, we have established that the results of our CNN model are well above those of simple feed forward networks and recurrent neural networks. While they are below state of the art LSTMs, they are able to perform competitively with them for small and moderate-size models. Scaling to larger sizes may be today the main roadblock for CNNs to reach the same performances as large LSTMs in language modeling."}, {"heading": "6 Model Analysis", "text": "In what follows, we obtain insights into the inner workings of the CNN by looking into the linguistic patterns that the kernels learn to extract and also studying the temporal information extracted by the network in relation to its prediction capacity.\nLearned patterns To get some insight into the kind of patterns that each kernel is learning to detect, we fed trigrams from the validation set of the Penn Treebank to each of the kernels, and extracted the ones that most highly activated the kernel, similarly to what was done in Kalchbrenner et al. (2014). Some examples are shown in Figure 4. Since the word windows are made of embeddings, we can expect patterns with similar embeddings to have close activation outputs. This is borne out in the analysis: The kernels specialize in distinct features of the data, including more syntactic-semantic constructions (cf. the \u201ccomparative kernel\u201d including as . . . as patterns, but also of more than) and more lexical or topical features (cf. the \u201cending-in-month-name\u201d kernel). Even in the more lexicalized features, however, we see linguistic regularities at different levels being condensed in a single kernel: For instance, the \u201cspokesman\u201d kernel detects phrases consisting of an indefinite determiner, a company name (or the word company itself) and the word \u201cspokesman\u201d. We hypothesize that the convolutional layer adds an \u201cI identify one specific feature, but at a high level of\nabstraction\u201d dimension to a feed-forward neural network, similarly to what has been observed in image classification (Krizhevsky et al., 2012). Temporal information To the best of our knowledge, the longest context used in feed-forward language models is 10 tokens (Hai Son et al., 2012),\nwhere no significant change in terms of perplexity was observed for bigger context sizes, even though in that work only same-sentence contexts were considered. In our experiments, we use a larger context size of 16 while removing the sentence boundary limit (as commonly done in n-gram language models) such that the network can take into account the words in the previous sentences.\nTo analyze whether all this information was effectively used, we took our best model, the CNN+MLPConv+COM model with embedding size of 256 (fifth line of second block in Table 1), and we identified the weights in the model that map the convolutional output (of size n \u00d7 k) to a lower dimensional vector (the \u201cmapping\u201d layer in Figure 2). Recall that the output of the convolutional layer is a matrix indexed by time step and kernel index containing the activation of the kernel when convolved with a window of text centered around the word at the given time step. Thus, output units of the above mentioned mapping predicate over an ensemble of kernel activations for each time step. We can identify the patterns that they learn to detect by extracting the time-kernel combinations for which they have positive weights (since we have ReLU activations, negative weights are equivalent to ignoring a feature). First, we asked ourselves whether these units tend to be more focused on the time steps closer to the target or not. To test this, we calculated the sum of the positive weights for each position in time using an average of the mappings that correspond to each output unit. The results are shown in\nFigure 5. As could be expected, positions that are close to the token to be predicted have many active units (local context is very informative; see positions 2-4). However, surprisingly, positions that are actually far from the target are also quite active. It seems like the CNN is putting quite a lot of effort on characterizing long-range dependencies.\nNext, we checked that the information extracted from the positions that are far in the past are actually used for prediction. To measure this, we artificially lesioned the network so it would only read the features from a given range of time steps (words in the context). To lesion the network we manually masked the weights of the mapping that focus on times outside of the target range by setting them to zero. We started using only the word closest to the final position and sequentially unmasked earlier positions until the full context was used again. The result of this experiment is presented in Figure 6, and it confirms our previous observation that positions that are the farthest away contribute to the predictions of the model. The perplexity drops dramatically as the first positions are unmasked, and then decreases more slowly, approximately in the form of a power law (f(x) \u221d x\u22120.9). Even though the effect is smaller, the last few positions still contribute to the final perplexity."}, {"heading": "7 Conclusion", "text": "In this work, we have investigated the potential of Convolutional Neural Networks for one prominent NLP task, language modeling, a sequential predic-\ntion task. We incorporate a CNN layer on top of a strong feed-forward model enhanced with modern techniques like Highway Layers and Dropout. Our results show a solid 11-26% reduction in perplexity with respect to the feed-forward model across three corpora of different sizes and genres when the model uses MLP Convolution and combines kernels of different window sizes. However, even without these additions we show CNNs to effectively learn language patterns that allow it to significantly decrease the model perplexity.\nIn our view, this improvement responds to two key properties of CNNs, highlighted in the analysis. First, as we have shown, they are able to integrate information from larger context windows, using information from words that are as far as 16 positions away from the predicted word. Second, as we have qualitatively shown, the kernels learn to detect specific patterns at a high level of abstraction. This is analogous to the role of convolutions in Computer Vision. The analogy, however, has limits; for instance, a deeper model stacking convolution layers harms performance in language modeling, while it greatly helps in Computer Vision. We conjecture that this is due to the differences in the nature of visual vs. linguistic data. The convolution creates sort of abstract images that still retain significant properties of images. When applied to language, it detects important textual features but distorts the input, such that it is not text anymore.\nAs for recurrent models, even if our model outperforms RNNs, it is well below state-of-the-art LSTMs. Since CNNs are quite different in nature, we believe that a fruitful line of future research could focus on integrating the convolutional layer into a recurrent structure for language modeling, as well as other sequential problems, perhaps capturing the best of both worlds."}, {"heading": "Acknowledgments", "text": "We thank Marco Baroni and three anonymous reviewers for fruitful feedback. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES) and the\nErasmus Mundus Scholarship for Joint Master Programs. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used in our research."}], "year": 2016, "references": [{"title": "Deep neural network language models", "authors": ["Ebru Arisoy", "Tara N Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran."], "venue": "Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the Ngram Model? On the Future of Language Modeling", "year": 2012}, {"title": "The WaCky wide web: A collection of very large linguistically processed webcrawled corpora", "authors": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta."], "venue": "Language Resources and Evaluation, 43(3):209\u2013226.", "year": 2009}, {"title": "Neural probabilistic language models", "authors": ["Yoshua Bengio", "Holger Schwenk", "Jean-S\u00e9bastien Sen\u00e9cal", "Fr\u00e9deric Morin", "Jean-Luc Gauvain."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "year": 2006}, {"title": "Findings of the 2015 workshop", "authors": ["Ond\u0159ej Bojar", "Rajen Chatterjee", "Christian Federmann", "Barry Haddow", "Matthias Huck", "Chris Hokamp", "Philipp Koehn", "Varvara Logacheva", "Christof Monz", "Matteo Negri", "Matt Post", "Carolina Scarton", "Lucia Specia", "Marco Turchi"], "year": 2015}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "authors": ["Xie Chen", "Xunying Liu", "Mark JF Gales", "Philip C Woodland."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "The Journal of Machine Learning Research, 12:2493\u2013 2537.", "year": 2011}, {"title": "Fast and robust neural network joint models for statistical machine translation", "authors": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul."], "venue": "ACL (1), pages 1370\u20131380. Citeseer.", "year": 2014}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "authors": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "AISTATS, volume 1, page 6.", "year": 2010}, {"title": "Structured output layer neural network language model", "authors": ["Le Hai Son", "Ilya Oparin", "Alexandre Allauzen", "Jean-Luc Gauvain", "Fran\u00e7ois Yvon."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5524\u20135527. IEEE.", "year": 2011}, {"title": "Measuring the influence of long range dependencies with neural network language models", "authors": ["Le Hai Son", "Alexandre Allauzen", "Fran\u00e7ois Yvon."], "venue": "Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Fu-", "year": 2012}, {"title": "Deep residual learning for image recognition", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "authors": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "CoRR, abs/1207.0580.", "year": 2012}, {"title": "Convolutional neural network architectures for matching natural language sentences", "authors": ["Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen."], "venue": "Advances in Neural Information Processing Systems, pages 2042\u2013 2050.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "authors": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint arXiv:1502.03167.", "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "authors": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD,", "year": 2014}, {"title": "Character-aware neural language models", "authors": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush."], "venue": "CoRR.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "authors": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "year": 2014}, {"title": "Improved backing-off for m-gram language modeling", "authors": ["Reinhard Kneser", "Hermann Ney."], "venue": "Acoustics, Speech, and Signal Processing, 1995. ICASSP-", "year": 1995}, {"title": "Moses: Open source toolkit for statistical machine translation", "authors": ["Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "year": 2012}, {"title": "Convolutional networks for images, speech, and time series", "authors": ["Yann LeCun", "Yoshua Bengio."], "venue": "The handbook of brain theory and neural networks, 3361(10):1995.", "year": 1995}, {"title": "Network in network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan."], "venue": "arXiv preprint arXiv:1312.4400.", "year": 2013}, {"title": "Recurrent neural network based language model", "authors": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "INTERSPEECH, volume 2, page 3.", "year": 2010}, {"title": "Extensions of recurrent neural network language model", "authors": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,", "year": 2011}, {"title": "Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753", "authors": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Michael Mathieu", "Marc\u2019Aurelio Ranzato"], "year": 2014}, {"title": "Relation extraction: Perspective from convolutional neural networks", "authors": ["Thien Huu Nguyen", "Ralph Grishman."], "venue": "Proceedings of NAACL-HLT, pages 39\u201348.", "year": 2015}, {"title": "Continuous space language models", "authors": ["Holger Schwenk."], "venue": "Computer Speech & Language, 21(3):492\u2013 518.", "year": 2007}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "authors": ["Yelong Shen", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Gr\u00e9goire Mesnil."], "venue": "Proceedings of the 23rd ACM International Conference on Conference on Information and", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "authors": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "year": 2014}, {"title": "Highway networks", "authors": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "CoRR, abs/1505.00387.", "year": 2015}, {"title": "End-to-end memory networks", "authors": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Recurrent neural network regularization", "authors": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329. 1162", "year": 2014}], "id": "SP:b0c5a4f8c833a4bcbe96f17a1e9323a5d46a02f4", "authors": [{"name": "Ngoc-Quan Pham", "affiliations": []}, {"name": "German Kruszewski", "affiliations": []}, {"name": "Gemma Boleda", "affiliations": []}], "abstractText": "Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26% better absolute performance than feed-forward neural language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target.", "title": "Convolutional Neural Network Language Models"}