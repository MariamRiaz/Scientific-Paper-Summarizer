0	67	Every public speech involving a large audience can be seen as a game of coordination (Asch, 1951): at each moment, each individual member of the audience must decide in a split second whether to applaud at what has just been said.
1	117	Applause is a potentially risky action: if an individual spontaneously claps but no one joins in, they suffer some negative social cost; the game is to judge from their own private information and content of the speech whether the rest of the audience will applaud at the same time they do.
3	43	the content of the message; b.)
4	53	their delivery (so that changes in pitch, duration and gaze signal salient moments for which applause may be licensed); and c.) the verbal design of the message—those rhetorical strategies that speakers use to signal that applause is welcome (Atkinson, 1984; Heritage and Greatbatch, 1986).
6	41	While past work has focused on these elements in isolation (Guerini et al., 2015; Liu et al., 2017) or for related problems such as laughter detection (Purandare and Litman, 2006; Chen and Lee, 2017; Bertero and Fung, 2016), we find that developing a holistic model encompassing all three aspects yields the most robust predictor of applause.
37	25	We downloaded about 500 speeches from presidential candidates, vice presidential candidates, or former presidents, collecting audio files and transcripts that were tagged in the categories “Campaign 2016” and “Speech” and which took place between 12/01/2015 and 12/01/2016.
43	15	We used logistic regression on the standard set of MFCC features and found similar results on the PennSound data to the reported classification accuracy of 99.4%.
45	23	Due to variation in the nature of applause in a crowd (sometimes we observe examples of isolated clapping and cheering, mixed laughter and applause, or applause interrupting the speaker), some ambiguity is inherent among the labels.
48	16	To match the identified segments of applause in the audio files with the relevant text from the transcriptions, we ran forced alignment using the Kaldi Toolkit (Povey et al., 2011).
58	21	Given this set of utterances, we paired each utterance with a “positive” or “negative” label, determined by whether applause occurred within 1.5 seconds of the end of the utterance.
77	26	Delta features (local approximations to derivatives) are commonly used in speech recognition and audio classification systems (Povey et al., 2011).
78	30	In a discourse, either highly similar or drastically different neighboring pairs of utterances may indicate dramatic moments.
79	38	We operationalize these features by explicitly adding a delta measurement for every feature in our model, which captures the difference between every feature at time t and the same feature at time t − 1.
85	12	ATTRIBUTION CONTRAST “He won’t win, but I’ll vote for him anyway” he said.
88	39	In our work, we parse the rhetorical structure of the extracted sequence of phrases using the RST parser of Ji and Eisenstein (2014).
93	13	In order to further operationalize the notion of predictability of applause, we measure the number of rhetorical phrases that a given discourse segment brings to closure.
95	12	This tree spans 10 elementary discourse units; each non-terminal node is annotated with the span of the subtree rooted at that node (so the root spans all ten EDUs, while its left child spans only the first five).
104	44	Combination of features from Guerini, Liu, and Audio.
109	21	We use logistic regression with `2 regularization for this experiment, with hyperparameters chosen through cross-validation on the training data.
110	52	We run 10-fold cross validation for each speaker, and leave-one-out cross validation for those speakers with fewer than 10 speeches (we exclude Rick Santorum from this experiment because we have only one speech from him), with whole speeches divided across folds so that no utterances from the same speech ever appear in both training and test sets.
111	38	Reported results aggregate the predictions across all speakers to calculate the final accuracies.
112	92	We choose utterances (or sequences of utterances) that directly precede applause as positive examples, pairing each one with a negative example randomly chosen from the same speech.
113	36	Since we use different amounts of data for each speaker, we are not able to compare accuracies across all speakers, but we can see that some speakers are significantly easier to model: for example, our best model reaches 0.719 accuracy on Bernie Sanders but only 0.660 on Donald Trump.
135	14	On February 2, 2016, presidential candidate Jeb Bush spoke to a crowd in New Hampshire a week before their state primary.
136	14	His speech ended with the following: So here’s my pledge to you.
139	18	Does our model recover this true intention?
141	32	The strongest features are again lexical (this country, commander in chief ), a LIWC focus on the future (elicited by will), and an RST PURPOSE relation (evoked by to get back in the business of creating a more peaceful world).
143	76	We introduce several new features designed to capture elements of tension and release in public performance, including rhetorical contrast, closure, repetition and movement across speech segments; while each of these features in isolation is able to predict applause to varying degree and comport with our prior understanding of their utility, we find that lexicalized features are among the strongest source of information in determining applause; while audiences react to many dimensions of a speaker’s style, the words they use—as slogan, stock phrases, and indicators of more complex rhetorical functions like moral valuations and imperatives—matter most.
144	45	As detailed in previous work (Liu et al., 2017; Haider et al., 2017; Clement and McLaughlin, 2016), understanding and identifying climactic moments in speeches can be useful for a variety of reasons, including learning to give better talks, automatically summarizing videos and transcripts, and analyzing social dynamics within crowds.
145	74	One additional interesting application of this work is to bring to the surface occasions where a speaker uses typical applause-seeking devices but does not receive applause (the “Please Clap” moments); we leave to future work identifying the reverse, when speakers receive applause without invoking common techniques (for example, to identify instances of claques paid to clap).
