6	36	To demonstrate this, we first show that LSTMs can be seen as a combination of two recurrent models: (1) an S-RNN, and (2) an element-wise weighted sum of the S-RNN’s outputs over time, which is implicitly computed by the gates.
19	11	We argue that the LSTM should be interpreted as a hybrid of two distinct recurrent architectures: (1) the S-RNN which provides multiplicative connections across timesteps, and (2) the memory cell which provides additive connections across timesteps.
22	11	,xn} be the sequence of input vectors, {h1, .
27	46	Evaluating the need for multiplicative recurrent connections in the content layer is the focus of this work.
28	16	The content layer is passed to the memory cell, which decides which parts of it to store.
29	19	Memory Cell (Equations 3-5) The memory cell ct is controlled by two gates.
30	11	The input gate it controls what part of the content (c̃t) is written to the memory, while the forget gate ft controls what part of the memory is deleted by filtering the previous state of the memory (ct−1).
32	45	Output Layer (Equations 6-7) The output layer ht passes the memory cell through a tanh activation function and uses an output gate ot to read selectively from the squashed memory cell.
35	26	It is possible to show that it implicitly computes an element-wise weighted sum of all the previous content layers by expanding the recurrence relation in Equation 5: ct = it ◦ c̃t + ft ◦ ct−1 = t∑ j=0 ( ij ◦ t∏ k=j+1 fk ) ◦ c̃j = t∑ j=0 wtj ◦ c̃j (8) Each weight wtj is a product of the input gate ij (when its respective input c̃j was read) and every subsequent forget gate fk.
38	11	However, constrained function spaces are also less expressive, and a natural question is whether these models will work well for NLP problems that involve understanding context.
39	105	We hypothesize that the memory cell (which computes weighted sums) can function as a standalone contextualizer.
42	38	The modeling power of LSTMs is commonly assumed to derive from the S-RNN in the content layer, with the rest of the model acting as a learning aid to bypass the vanishing gradient problem.
44	21	To test whether the memory cell has enough modeling power of its own, we take an LSTM and replace the S-RNN in the content layer from Equation 2 with a simple linear transformation (c̃t = Wcxxt) creating the LSTM – S-RNN model.
50	17	In this model, the only recurrence is the additive connection in the memory cell; it has no multiplicative recurrent connections at all.
66	14	In our experiments, we use the existing hyperparameters and only replace the LSTMs with the simplified architectures.
67	15	Machine Translation For machine translation, we used OpenNMT (Klein et al., 2017) to train English to German translation models on the multimodal benchmarks from WMT 2016 (used in OpenNMT’s readme file).
74	14	In contrast, removing the S-RNN makes little to no difference in the final performance, suggesting that the memory cell alone is largely responsible for the success of LSTMs in NLP.
75	18	Even after removing every multiplicative recurrence from the memory cell itself, the model’s performance remains well above the vanilla S- RNN’s, and falls within the standard deviation of an LSTM’s on some tasks (see Table 3).
77	15	As a corollary, this result also suggests that a weighted sum of context words, while mathematically simple, is a powerful model of contextual information.
81	21	After simplifying the content layer and removing the output gate (LSTM – S-RNN – OUT), the model’s computation can be expressed as a weighted sum of context-independent functions of the inputs (Equation 9 in Section 3.1).
82	32	This formula abstracts over both the simplified LSTM and the family of attention mechanisms, and through this lens, the memory cell’s computation can be seen as a “cousin” of self-attention.
87	9	Second, the weighted sum is accumulated with a dynamic program.
92	11	These variants compute locally normalized distributions via a product of sigmoids rather than globally normalized distributions via a single softmax.
104	12	Results across four major NLP tasks (language modeling, question answering, dependency parsing, and machine translation) indicate that LSTMs suffer little to no performance loss when removing the S-RNN.
124	11	The top-right triangle indicates weights from the forward direction, and the bottom-left triangle indicates from the backward direction.
125	89	For syntax, we see a significantly different pattern.
126	31	Function words that are useful for determining syntax are more likely to be remembered.
127	59	Weights on head words are also likely to persist until the end of a constituent.
128	47	This illustration provides only a glimpse into what the model is capturing, and perhaps future, more detailed visualizations that take the individual dimensions into account can provide further insight into what LSTMs are learning in practice.
129	71	739 Language model weights Dependency parser weights The hym n was sun g at my first inau gura l chu rch serv ice as gov erno r The hymn was sung at my first inaugural church service as governor The hym n was sun g at my first inau gura l chu rch serv ice as gov erno r The hymn was sung at my first inaugural church service as governor US troo ps ther e clas hed with gue rrilla s in a figh t that left one Iraq i dea d US troops there clashed with guerrillas in a fight that left one Iraqi dead US troo ps ther e clas hed with gue rrilla s in a figh t that left one Iraq i dea d US troops there clashed with guerrillas in a fight that left one Iraqi dead
