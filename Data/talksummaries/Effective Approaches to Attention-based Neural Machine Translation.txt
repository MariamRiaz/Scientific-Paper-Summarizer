0	64	Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015).
7	17	In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015).
19	66	, ym.3 A basic form of NMT consists of two components: (a) an encoder which computes a representation s for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as: log p(y|x) = ∑m j=1 log p (yj|y<j , s) (1) A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the recent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.
21	31	On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder.
24	31	On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.
42	13	On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht−1 → at → ct → ht, which, in turn, goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.
46	68	In their work, soft attention refers to the global attention approach in which weights are placed “softly” over all patches in the source image.
47	15	The hard attention, on the other hand, selects one patch of the image to attend to at a time.
51	94	In concrete details, the model first generates an aligned position pt for each target word at time t. The context vector ct is then derived as a weighted average over the set of source hidden states within the window [pt−D, pt+D];D is empirically selected.8 Unlike the global approach, the local alignment vector at is now fixed-dimensional, i.e., ∈ R2D+1.
54	46	The alignment vector at is defined according to Eq.
55	37	(7).9 Predictive alignment (local-p) – instead of assuming monotonic alignments, our model predicts an aligned position as follows: pt = S · sigmoid(v⊤p tanh(Wpht)), (10) Wp and vp are the model parameters which will be learned to predict positions.
56	34	S is the source sentence length.
57	36	To favor alignment points near pt, we place a Gaussian distribution centered around pt .
67	14	Whereas, in standard MT, a coverage set is often maintained during the translation process to keep track of which source words have been translated.
69	39	To address that, we propose an inputfeeding approach in which attentional vectors h̃t are concatenated with inputs at the next time steps as illustrated in Figure 4.11 The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.
108	32	Using a better alignment function, the content-based dot product one, together with dropout yields another gain of +2.7 BLEU.
117	30	As demonstrated in Figure 6, our attentional models are more effective than the other non-attentional model in handling long sentences: the translation quality does not degrade as sentences become longer.
118	133	Our best model (the blue + curve) outperforms all other systems in all length buckets.
121	26	However, results in Table 4 do give us some idea about different choices.
122	189	The location-based function does not learn good alignments: the global (location) model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions.14 For content-based functions, our implementation of concat does not yield good performances and more analysis should be done to understand the reason.15 It is interesting to observe that dot works well for the global attention and general is better for the local attention.
123	183	Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.
125	139	While (Bahdanau et al., 2015) visualized alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole.
126	21	In contrast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric.
127	20	Given the gold alignment data provided by RWTH for 508 English-German Europarl sentences, we “force” decode our attentional models to produce translations that match the references.
130	36	The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007).
134	66	Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations.
135	49	We also observed an interesting case in the second English-German example, which requires translating the doubly-negated phrase, “not incompatible”.
136	16	The attentional model correctly produces “nicht .
137	15	unvereinbar”; whereas the non-attentional model generates “nicht vereinbar”, meaning “not compatible”.18 The attentional model also demonstrates its superiority in translating long sentences as in the last example.
138	64	In this paper, we propose two simple and effective attentional mechanisms for neural machine translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time.
