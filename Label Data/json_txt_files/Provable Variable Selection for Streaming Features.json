{"sections": [{"heading": "1. Introduction", "text": "For retailers, brick-and-mortar stores and internet-based stores, various recommendation methods are proposed in an attempt to sell products. The recommendation model is usually updated in a timely manner or it includes new valuable features of products which are not previously available. For example, during the Apple WWDC 2018 keynote, Apple has introduced new features of their platforms to fight \u201cfingerprinting\u201d, a technique which tracks users based on identifying computers. With the available of new features, a feature selection model is employed to determine whether the new features will drive sales of products in the future\n1Cornell University, New York, NY 10021, USA. 2Rutgers University, Piscataway, NJ 08854, USA. 3Baidu Research, Bellevue, WA 98004, USA. Jing Wang <jiw2026@med.cornell.edu>, Ping Li <pingli98@gmail.com>. Correspondence to: Jie Shen <js2007@rutgers.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nand only related features will be included in the recommendation model. Hence, in real-world applications, features are usually revealed in a continuous stream. It is necessary to evaluate new features immediately and output intermediate result. The feature evaluation process in a stream is called online feature selection (Perkins & Theiler, 2003; Zhou et al., 2005; Wu et al., 2010). We first formulate this problem.\nSuppose that there are n samples but initially we do not observe all of the features. We call the sequence a1,a2, \u00b7 \u00b7 \u00b7 \u2208 Rn is a feature stream, with each ai \u2208 Rn being the ith feature, or the ith covariate of n samples. Note that in our setting, the feature ai is revealed at time stamp i. If ai is selected, we update the observation matrixA as follows:\nA\u2190 [A \u03b8iai] . (1.1)\nwhere the parameter \u03b8i 6= 0 is chosen in an online manner.\nIn the literature, a large number of online methods have been proposed based on statistical measurements or optimization techniques (Perkins & Theiler, 2003; Zhou et al., 2005; Wang et al., 2015). For example, Perkins & Theiler (2003) added a new feature which contributes to a predictor learning and optimization analysis into the model. Zhou et al. (2005) proposed an adaptive complexity penalty method to evaluate a new feature based on its p-value. Wu et al. (2010) utilized the Markov blanket to measure the relationship between a new feature and the selected feature subset. Yet successful, most of the results in this line of research are empirical in nature.\nOn the other hand, feature selection method can be categorized into either supervised or unsupervised. For instance, Shen & Li (2017) recently proposed a non-convex supervised approach for variable selection with favorable iteration complexity. Unsupervised methods, however, are with great practical importance to many areas such as Cardiology, as annotated data is usually precious and limited due to genetic privacy issue and the medical background requirement for annotators.\nSummary of Contributions. In this paper, we consider the high-dimensional regime that the number of features is much larger than the sample size, and the features are revealed in an online manner. We propose an unsupervised algorithm termed Online leverage scores for Feature Selection\n(OFS). Our main technique is to approximately compute the broadly used leverage score in each iteration, and determine the importance of each feature in real time. We prove that the reduced feature space is a good approximation to the original one in some sense to be clarified. Furthermore, we apply k-means clustering on the set of selected features, and show that the clustering performance does not degrade a lot. Computationally, our algorithm enjoys low time complexity and little memory usage, which makes it a perfect fit for big data analytics."}, {"heading": "2. Related Work", "text": "Feature selection is a primary technique in machine learning to address \u201cthe curse of dimensionality\u201d. In the last decades, a number of methods have been proposed (Guyon & Elisseeff, 2003; Donoho & Jin, 2008). In this section, we give a brief review of existing approaches in terms of batch situation and online situation.\nBatch Methods. Existing batch feature selection methods can be roughly divided to unsupervised, supervised and semi-supervised approaches. The supervised methods utilize the target variable to guide the feature evaluation process, such as Fisher score, Least Absolute Shrinkage and Selection Operator (Lasso) (Tibshirani, 1996) and minimum Redundancy Maximum Relevance (Peng et al., 2005). Unsupervised feature selection methods mainly depend on latent data distribution analysis (He et al., 2005), such as spectral analysis (Zhao & Liu, 2007; Cai et al., 2010) and Kullback-Leibler Divergence between neighborhood distributions (Wei & Philip, 2016). The semi-supervised feature selection algorithms make benefits of both aforementioned approaches, such as combining Gaussian Field and Harmonic functions (Kong & Yu, 2010; Zhu et al., 2003).\nFeature selection methods are also characterized as wrapper, embedded and filter model. The wrapper model evaluates feature subsets by their performance on a specific algorithm, such as SVM or Naive Bayes for classification tasks (Forman, 2003) and k-means for clustering tasks (Guyon et al., 2002; Xu et al., 2014). The embedded model seeks the desired feature subset by solving a regularized optimization objective function with certain constraints (Zhang, 2009; Yang & Xu, 2013). Examples of this approach include Least Angle Regression (Efron et al., 2004) and group Lasso (Zhang et al., 2016). The optimization process forces most coefficients small or exact zero. The features corresponding to nonzero coefficients are selected.\nThe filter model utilizes certain statistical measurements, such as the Hilbert-Schmidt Independence Criterion (HSIC), leverage score (Boutsidis et al., 2009) and kernel-based measures of independence (Chen et al., 2017). Specifically, the statistical leverage score is an important measurement\nfor unsupervised feature selection. It characterizes the outstanding features that have more affect towards the result of a statistical procedure. There are multiple variants of the statistical leverage score, such as the normalized leverage score (Boutsidis et al., 2009), the truncated version of leverage score (Gittens & Mahoney, 2013) and the kernel ridge leverage score (Alaoui & Mahoney, 2015). The ridge leverage score is used to select features for k-means clustering (Boutsidis et al., 2009) and has proved to attain (2 + )- approximate partition. Specifically, the ridge leverage score of the ith column of data matrix A \u2208 Rn\u00d7d is defined as (Alaoui & Mahoney, 2015),\nli = a > i (AA > + \u03bbI)\u22121ai, (2.1)\nwhere \u03bb > 0 is a parameter, I \u2208 Rn\u00d7n is the identity matrix. However, it is expensive as it requires O ( n3 + n2d ) running time and O (nd) memory storage. A number of recent papers focus on sampling some columns of A and approximate the linear kernel ofA (Li et al., 2013; Alaoui & Mahoney, 2015; Cohen et al., 2016; Musco & Musco, 2017). However, none of these techniques have been applied for feature selection of streaming features.\nOnline Methods. Motivated by the fact that features are available in a stream in real-world applications, online feature selection has attracted a lot of attention (Perkins & Theiler, 2003; Zhou et al., 2005; Wu et al., 2010; Wang et al., 2013). The batch-mode algorithms cannot handle this situation well as the global feature space is required in advance. Examples of online feature selection approaches either utilize statistical measurements, such as alpha-investing (Zhou et al., 2005) and mutual information (Wu et al., 2010) or rely on optimization techniques, such as stochastic gradient grafting (Perkins & Theiler, 2003; Wang et al., 2015). All existing mentioned methods come with no theoretical guarantees of the selected feature subset for clustering task."}, {"heading": "2.1. Notation", "text": "We use bold lower-case letters, e.g. v \u2208 Rd to denote a column vector. \u2016v\u20162 is used to denote the `2-norm of the vector. Capital letters such asX are used to denote matrices, and its transpose is denoted byX>. The capital letter In\u00d7n is reserved for the identity matrix where n indicates its size. For an invertible matrix X , we write its inverse as X\u22121. Otherwise, we use X\u2020 for the pseudoinverse. For a square matrix X , we write its trace as Tr (X), which is the sum of its diagonal elements. The ith column and jth row of the matrix X are denoted by xi and (xj)>, respectively. Suppose that the rank of matrixX \u2208 Rn\u00d7m is k \u2264 min{m,n}. The singular value decomposition of X\nis given by\nX = [ u1, \u00b7 \u00b7 \u00b7 ,uk ] \u03c31 . . . \u03c3k  v > 1 ... v>k  where the singular values in descending order \u03c31 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3k > 0, U = [u1, \u00b7 \u00b7 \u00b7 ,uk] \u2208 Rn\u00d7k contains the left singular vectors and V = [v1, \u00b7 \u00b7 \u00b7 ,vk] contains the right singular vectors. In this paper, we will use the Frobe-\nnius norm \u2016X\u2016F := \u221a\u2211k i=1 \u03c3 2 i and the spectral norm \u2016X\u20162 := max1\u2264i\u2264k \u03c3i = \u03c31.\nFor a sequence of random variablesX1,X2, . . . , we write Ej\u22121 [Xj ] for the expectation of Xj conditioning on {X1, . . . ,Xj\u22121}."}, {"heading": "3. Main Results", "text": "In this section, we propose an online algorithm for feature selection, where the goal is to approximate the original data with much fewer attributes in some sense. To the end, we make use of the leverage score that, from a high level, reflects the importance of each feature.\nSuppose that the data matrix isA \u2208 Rn\u00d7d, i.e., n samples lying in a d-dimensional ambient space. The statistical leverage score of the ith column (i.e., feature) of A is defined as\nl\u2217i = a > i (AA >)\u2020ai. (3.1)\nIt is well known that sampling an n\u00d7O ( \u22122n log n ) matrix\nA\u0303 with probabilities proportional to the respective leverage scores yields a (1 + )-spectral approximation toA (Spielman & Srivastava, 2011), in the sense that for all x\u2225\u2225\u2225A\u0303>x\u2225\u2225\u2225 2 \u2248 \u2225\u2225\u2225A>x\u2225\u2225\u2225 2 , or more precisely (1\u2212 )x>AA>x \u2264 x>A\u0303A\u0303>x \u2264 (1 + )x>AA>x, or\n(1\u2212 )AA> A\u0303A\u0303> (1 + )AA>.\nIn the online setting, however, we are not able to access all the data to compute the leverage score. The key idea of our algorithm is that when a new feature arrives, we approximate its leverage score based on the obtained features, which can further be used to guide the selection process.\nTo be more concrete, at time stamp i, suppose the observed data matrix is A\u0303i\u22121 and the new feature ai is revealed, we need to determine whether ai is kept or discarded. A natural way for the sake is to compute the approximate leverage score of ai as follows:\nli = a > i (A\u0303i\u22121A\u0303 > i\u22121) \u2020ai. (3.2)\nAlgorithm 1 Online Feature Selection Require: Initial data matrix A\u03030, sampling rate c =\n8 \u22122 log n, approximation parameter \u2208 (0, 1). 1: for i = 1, \u00b7 \u00b7 \u00b7 do 2: Reveal the ith feature ai. 3: Compute the online leverage score\nl\u0303i = min((1 + )a > i (A\u0303i\u22121A\u0303 > i\u22121) \u2020ai, 1).\n4: Compute the probability,\npi = min(cl\u0303i, 1).\n5: With probability pi, update\nA\u0303i = [A\u0303i\u22121 ai/ \u221a pi].\nOtherwise,\nA\u0303i = A\u0303i\u22121.\n6: end for\nIntuitively, if A\u0303i\u22121 is a good approximation to A, li indicates the importance of ai as l\u2217i does. And what we will show is that, it is the case after we reveal a few attributes.\nIt is known that if the entire feature space is available, each leverage score is upper bounded by 1. However the estimates based on A\u0303i\u22121 can be arbitrary because A\u0303i\u22121 is a submatrix of A which leads to A\u0303i\u22121A\u0303 > i\u22121 AA\n>. For our analysis, we technically require that each li is not larger than 1. Hence, we will make use of a modified quantity\nl\u0303i = min ( (1 + )a>i (A\u0303i\u22121A\u0303 > i\u22121) \u2020ai, 1 ) . (3.3)\nNote that > 0 is some pre-defined accuracy parameter, and the above suggests we are using a conservative estimate of the leverage score. To see this, consider A\u0303i\u22121 = A, then l\u0303i \u2265 l\u2217i . It is essential in the online setting in that we may lose many important features with an aggressive strategy.\nThen, the sampling probability is computed as pi = min ( 8 \u22122 log n \u00b7 l\u0303i, 1 ) . (3.4)\nWith the scaling factor of l\u0303i above, it is not hard to see that for a small approximation error , one has to select the current feature with high probability, which conforms the intuition \u2013 an exact estimation of A requires selecting all the features. We summarize our method in Algorithm 1."}, {"heading": "3.1. Analysis", "text": "We first show that with high probability, the data matrix produced by our algorithm is a good approximation toA.\nTheorem 1. Consider Algorithm 1. Let A\u0303 be the output when it terminates. It holds with high probability that\n(1\u2212 )AA> A\u0303A\u0303> (1 + )AA>.\nProof. Let Ai = (a1 a2 . . .ai). Define Y 0 as the zero matrix and for all i \u2265 1, let\nY i\u22121 = (AA >)\u2020/2(A\u0303i\u22121A\u0303 > i\u22121\u2212Ai\u22121A > i\u22121)(AA >)\u2020/2.\nLet ui = (AA>)\u2020/2ai. If \u2016Y i\u22121\u20162 \u2265 , we set Xi = 0. Otherwise, set\nXi = { (1/pi \u2212 1)uiu>i , if ai is sampled in A\u0303, \u2212uiu>i , otherwise.\nThus,Xi = Y i \u2212 Y i\u22121.\nConsider the case \u2016Y i\u22121\u20162 < . We get\nl\u0303i = min((1 + )a > i (A\u0303iA\u0303 > i ) \u2020ai, 1)\n\u2265 min((1 + )a>i (AiA > i + AA >)\u2020ai, 1) \u2265 min((1 + )a>i ((1 + )(AA >))\u2020ai, 1) = a>i (AA >)\u2020ai = u>i ui.\nThus, pi \u2265 min(cu>i ui, 1). If pi = 1, then Xi = 0. Otherwise, we have pi \u2265 cu>i ui. Moreover, we get\n\u2016Xi\u20162 \u2264 1/c\nand\nEi\u22121 [ X2i ]\npi \u00b7 (1/pi \u2212 1)2(uiu>i )2 + (1\u2212 pi) \u00b7 (uiu>i )2\n= (uiu > i ) 2/pi\nuiu>i /c.\nLetW i = \u2211i k=1 Ek\u22121 [ X2k ] . We have\n\u2016W i\u20162 \u2264 \u2225\u2225\u2225\u2225\u2225 i\u2211\nk=1\nuiu > i /c \u2225\u2225\u2225\u2225\u2225 2 \u2264 1/c.\nApplying Lemma 4 gives Pr(\u2016Y n\u20162 \u2265 ) \u2264 n \u00b7 exp ( \u2212 2/2 1/c+ /(3c) ) \u2264 n \u00b7 exp(\u2212c 2/4) = 1/n.\nThis implies that with high probability\u2225\u2225\u2225(AA>)\u2020/2(A\u0303A\u0303>)(AA>)\u2020/2 \u2212 I\u2225\u2225\u2225 2 \u2264 .\nWe thus have\n(1\u2212 )AA> A\u0303A\u0303 > (1 + )AA>,\ncompleting the theorem.\nNow we turn to control the number of features selected by Algorithm 1. We will use the result in (Cohen et al., 2015) shown below.\nLemma 1. Let A be an n \u00d7 d matrix, \u2208 (0, 1), c = 1/ , l\u03031, \u00b7 \u00b7 \u00b7 , l\u0303d be over-estimated leverage scores, i.e., l\u0303i \u2265 a>i (AA\n>)\u2020ai for all 1 \u2264 i \u2264 d. Let pi = min{cl\u0303i, 1}. Construct A\u0303 by independently sampling each column ai of A with probability pi and rescale it by 1/ \u221a pi if it is included in A\u0303. Then, with high probability, A\u0303 is the (1+ )- spectral approximation ofA and the number of columns in A\u0303 is O ( \u22122 \u2211d i=1 l\u0303i log n ) .\nBy Lemma 1, in order to control the number of selected features, we need to bound the sum of online leverage scores.\nLemma 2. After running Algorithm 1, it holds with high probability that\nd\u2211 i=1 l\u0303i = O (n log(\u2016A\u20162)) .\nProof. We define\n\u03b4i = log det(A\u0303iA\u0303 > i )\u2212 log det(A\u0303i\u22121A\u0303 > i\u22121).\nThe sum of \u03b4i can be bounded by the logarithm of the ratio of the determinants of A\u0303A\u0303 > . By the matrix determinant lemma, we have\nEi\u22121 [ exp(l\u0303i/8\u2212 \u03b4i) ] = pi \u00b7 eli/8(1 + a>i (A\u0303i\u22121A\u0303 > i\u22121) \u22121ai/pi) \u22121\n+ (1\u2212 pi) \u00b7 eli/8\n\u2264 (1 + li/4) \u00b7 (pi(1 + a>i (A\u0303i\u22121A\u0303 > i\u22121) \u22121ai/pi) \u22121\n+ 1\u2212 pi).\nIf cl\u0303i < 1, we have pi = cl\u0303i and Ei\u22121 [ exp(l\u0303i/8\u2212 \u03b4i) ] \u2264 cl\u0303i \u00b7 (1 + li/4)(1 + 1/((1 + )c))\u22121 + (1\u2212 cl\u0303i)\u00b7 (1 + li/4)\n= (1 + li/4)(cli(1 + 1/((1 + )c)) \u22121 + 1\u2212 cli)\n\u2264 (1 + li/4)(1 + cli(1\u2212 1/(4c)\u2212 1)) = (1 + li/4)(1\u2212 li/4) \u2264 1.\nOtherwise, pi = 1 and we have\nEi\u22121 [ exp(l\u0303i/8\u2212 \u03b4i) ] \u2264 (1 + li/4)(1 +A>i (A\u0303 > i\u22121A\u0303i\u22121 + \u03bbI) \u22121Ai) \u22121 \u2264 (1 + li/4)(1 + li)\u22121 \u2264 1.\nWe now analyze the expected product of exp(l\u0303i/8\u2212\u03b4i) over the first k steps. For k \u2265 1 we have\nE [ exp ( k\u2211 i=1 l\u0303i/8\u2212 \u03b4i )] \u2264 E [ exp ( k\u22121\u2211 i=1 l\u0303i/8\u2212 \u03b4i )] ,\nand so by induction on k,\nE [ exp ( d\u2211 i=1 l\u0303i/8\u2212 \u03b4i )] \u2264 1.\nHence by Markov\u2019s inequality,\nPr ( d\u2211 i=1 l\u0303i > 8n+ 8 d\u2211 i=1 \u03b4i ) \u2264 e\u2212n.\nUsing Theorem 1, with high probability, we have\nA\u0303A\u0303 > (1 + )AA>,\nimplying that\ndet(A\u0303A\u0303 > ) \u2264 (1 + )n(\u2016A\u201622) n,\nlog det(A\u0303A\u0303 > ) \u2264 n(1 + log(\u2016A\u201622)).\nBy the definition of \u03b4i, it holds with high probability that\nd\u2211 i=1 \u03b4i = log det(A\u0303 > A\u0303+ \u03bbI)\u2212 n\n\u2264 n(1 + log(\u2016A\u201622)\u2212 1) = n(log(\u2016A\u201622)).\nAnd with high probability,\nd\u2211 i=1 l\u0303i \u2264 8n+ 8 d\u2211 i=1 \u03b4i\n\u2264 8n+ 8n log(\u2016A\u201622) = O ( n log(\u2016A\u201622) ) = O (n log(\u2016A\u20162)) .\nThe proof is complete.\nThus Lemma 1 and 2 imply that Algorithm 1 selects O ( \u22122n log d log(\u2016A\u20162) ) features with high probability.\nTime Complexity. The running time of Algorithm 1 is dominated by the online leverage score computation in Step 3, which is O ( n3 ) . In the case that A\u0303i\u22121 is a Laplacian\nmatrix, Step 3 can be implemented inO ( d log2 n ) time by a fast graph Laplacian solver with the Johnson-Lindenstrauss lemma, as stated in (Koutis et al., 2016).\nMemory Cost. The memory cost for leverage score computation is significantly reduced from O (nd) to O ( n2 log n ) (storage of A\u0303i). This follows from the analysis of Lemma 2 which states that when the algorithm terminates, only O ( \u22122n log n log(\u2016A\u20162) ) features will be selected. Note that this paper considers the regime where n d, such as the number of patients with rare diseases n and the length of their gene expressions d, or the batch size in neural networks n and the corresponding dimension of feature space d. Hence our online implementation is order of magnitude more efficient. It leads to practical values of our algorithm for learning tasks, such as clustering.\n3.2. Application to k-Means Clustering\nWe explore the performance of matrix A\u0303 returned by Algorithm 1 when it is used for k-means clustering. We first recall the k-means clustering problem.\nFormally, k-means clustering seeks to partition the data matrix A \u2208 Rn\u00d7d into k clusters {C1, \u00b7 \u00b7 \u00b7 , Ck} to minimize the distance between data points and its closest center {\u00b51, \u00b7 \u00b7 \u00b7 ,\u00b5k} (Awasthi et al., 2010):\nmin \u00b51,...,\u00b5k k\u2211 i=1 \u2211 j\u2208Ci \u2225\u2225aj \u2212 \u00b5i\u2225\u222522 , (3.5) where \u00b5i be the center of data points in Ci. It is known that k-means clustering is an instance of low-rank approximation (Boutsidis et al., 2009). To see this, we construct an n\u00d7 k matrix X as the cluster indicator matrix. Then for each solution {\u00b5i}ki=1 of (3.5), we will assign a cluster label, say xj \u2208 {1, 2, . . . , k}, to each sample aj . We set Xij = 1/ \u221a |Cj | if ai belongs to Cj , and 0 otherwise. In this way, the ith row ofXX>A is actually the average of the points with label i, i.e., the center \u00b5i of the ith class. Hence, from the discussion, we may rewrite (3.5) as follows:\nmin X k\u2211 i=1 \u2211 j\u2208Ci \u2225\u2225\u2225aj \u2212 (XX>A)i\u2225\u2225\u22252 2 .\nMore compactly, we aim to solve\nmin X \u2225\u2225\u2225A\u2212XX>A\u2225\u2225\u22252 F .\nSee (Ostrovsky et al., 2006) for a more detailed discussion.\nLet the indicator matrix X\u2217 \u2208 Rn\u00d7k denote the optimal partition onA, i.e.,\nX\u2217 = argmin X \u2225\u2225\u2225A\u2212XX>A\u2225\u2225\u22252 F . (3.6)\nWe first investigate how the cluster indicator matrixX over A\u0303 is deviated from the optimum. The following lemma provides the bound of the k-means objective function value on A\u0303.\nLemma 3. Suppose that A\u0303 is the matrix returned by Algorithm 1, then\n(1\u2212 ) \u2225\u2225\u2225A\u2212XX>A\u2225\u2225\u22252 F \u2264 \u2225\u2225\u2225A\u0303\u2212XX>A\u0303\u2225\u2225\u22252 F\n\u2264 (1 + ) \u2225\u2225\u2225A\u2212XX>A\u2225\u2225\u22252\nF ,\nwhen is the parameter of Algorithm 1.\nProof. Using the notation Y = I \u2212XX>, we can rewrite the objective function of k-means based on the data matrices A and A\u0303 as\u2225\u2225\u2225A\u2212XX>A\u2225\u2225\u22252\nF = \u2016Y A\u20162F = Tr\n( Y AA>Y ) ,\u2225\u2225\u2225A\u0303\u2212XX>A\u0303\u2225\u2225\u22252 F = \u2225\u2225\u2225Y A\u0303\u2225\u2225\u22252 F = Tr ( Y A\u0303A\u0303 > Y ) .\nNote that\nTr ( Y A\u0303A\u0303 > Y ) = Tr ( n\u2211 i=1 y>i A\u0303A\u0303 > yi ) ,\nwhere yi is the ith column of Y . Then by the spectral bound onAA> in Theorem 1, we immediately get\n(1\u2212 )Tr ( Y AA>Y ) \u2264 Tr ( Y A\u0303A\u0303 > Y )\n\u2264 (1 + )Tr ( Y AA>Y ) .\nPlugging Y = I \u2212XX> into the above inequalities completes the proof.\nNow we show that A\u0303 is also a good approximation toA.\nTheorem 2. Suppose that A\u0303 is returned by Algorithm 1. Let X\u0303\u2217 = argmin \u2225\u2225\u2225A\u0303\u2212XX>A\u0303\u2225\u2225\u22252\nF . Then given \u2208 (0, 1),\nwe can get\u2225\u2225\u2225A\u2212 X\u0303\u2217X\u0303>\u2217 A\u2225\u2225\u22252 F \u2264 1 + 1\u2212 \u00b7 \u2225\u2225\u2225A\u2212X\u2217X>\u2217 A\u2225\u2225\u22252 F .\nProof. Using Lemma 3, we have (1\u2212 ) \u2225\u2225\u2225A\u2212 X\u0303\u2217X\u0303>\u2217 A\u2225\u2225\u22252 F \u2264 \u2225\u2225\u2225A\u0303\u2212 X\u0303\u2217X\u0303>\u2217 A\u0303\u2225\u2225\u22252\nF ,\u2225\u2225\u2225A\u0303\u2212X\u2217X>\u2217 A\u0303\u2225\u2225\u22252\nF \u2264 (1 + ) \u2225\u2225\u2225A\u2212X\u2217X>\u2217 A\u2225\u2225\u22252 F .\nOn the other hand, by the optimality of X\u0303\u2217 for A\u0303, we have\u2225\u2225\u2225A\u0303\u2212 X\u0303\u2217X\u0303>\u2217 A\u0303\u2225\u2225\u22252 F \u2264 \u2225\u2225\u2225A\u0303\u2212X\u2217X>\u2217 A\u0303\u2225\u2225\u22252 F .\nCombining the above inequalities, we have\u2225\u2225\u2225A\u2212 X\u0303\u2217X\u0303>\u2217 A\u2225\u2225\u22252 F \u2264 1 + 1\u2212 \u00b7 \u2225\u2225\u2225A\u2212X\u2217X>\u2217 A\u2225\u2225\u22252 F .\nThe proof is complete.\nTheorem 2 implies that if X\u0303\u2217 is an optimal solution for A\u0303, then it also preserves an (1 + )-approximation forA. We compare our algorithm with existing dimension reduction methods for k-means clustering as shown in Table 1."}, {"heading": "4. Experiments", "text": "This section describes an empirical study of the efficacy and efficiency of our algorithm. We first elaborate the experimental settings.\nData Sets. We perform the experiments on 6 realistic data sets, including USPS1, AR2, COIL203, CIFAR-104, MNIST5 and ORL6. The summary of them is shown in Table 2.\nComparative Methods. We compare our algorithm with state-of-the-art feature selection approaches, including supervised model, for instance, alpha-investing (Alpha) (Zhou et al., 2005), as well as unsupervised model, e.g., \u03bb-ridge leverage score (LevS) (Alaoui & Mahoney, 2015) and Laplacian score (LapS) (He et al., 2005). We also compare to sparse random projection (SEC) (Liu et al., 2017) which is particularly designed for k-means clustering.\nPipeline. After running our method and the baselines above, we obtain a reduced set of features. Then we feed it to the standard k-means clustering that is available in Matlab. We also report the clustering result based on the original set of features, and we simply denote it by k-means.\nResults. We report the clustering accuracy against the number of selected features in Figure 1. We can see that our algorithm achieves competitive performance with other batch methods. For example, our algorithm outperforms all the\n1https://archive.ics.uci.edu/ml/datasets. html\n2http://www2.ece.ohio-state.edu/\u02dcaleix/ ARdatabase.html\n3http://www.cs.columbia.edu/CAVE/ software/softlib/coil-20.php\n4https://www.cs.toronto.edu/\u02dckriz/cifar. html\n5http://yann.lecun.com/exdb/mnist/ 6http://www.cl.cam.ac.uk/research/dtg/\nattarchive/facedatabase.html\nbaseline methods on COIL20, CIFAR-10 and ORL when the number of selected features varies from 10 to 500. The clustering performance on our selected subset even outperforms the one with all available features.\nComputational Efficiency. We illustrate the running time in Table 3. In terms of efficiency, our algorithm outperforms most of the comparative methods. This is not surprising in that for batch methods, they often update the model with all the data while we process them one by one. For example, on the CIFAR-10 data set, Laplacian score requires 2 minutes\nfor feature selection because the computation of the graph matrix based on global feature space is expensive. Our algorithm, in contrast, only requires a few seconds. The reason is that in each iteration, we operate with a skinny matrix A\u0303 instead of the whole data matrixA."}, {"heading": "5. Conclusion", "text": "In this paper, we have proposed an online feature selection for k-means clustering. For features in a stream, we approx-\nimate its leverage score in an online manner and perform feature selection based on such an inexact score. We provide theoretical guarantee that our unsupervised approach produces an accurate estimation based on the original space. Moreover, in the high-dimensional regime the algorithm is computationally efficient and consumes little memory. Perhaps more importantly, our algorithm is capable of addressing streaming data which makes it a perfect fit for large-scale learning systems. In addition, we extend the analysis to the k-means clustering problem, and provably show that with the set of features reduced by our approach, we are still able to obtain a near-optimal solution to the original k-means problem. The extensive empirical study matches perfectly our analysis."}, {"heading": "Acknowledgements", "text": "Jing Wang, Jie Shen and Ping Li are supported by NSFBigdata-1419210 and NSF-III-1360971. Jing Wang is supported by grants from the Dalio Foundation. The work was done when Jing Wang was a postdoc at Rutgers University."}, {"heading": "A. Technical Lemmas", "text": "We provide a technical lemma which is due to (Tropp et al., 2011).\nLemma 4. Let Y 0,Y 1, . . . ,Y n be a matrix martingale that are self-adjoint matrices with dimension d, and let X1, . . . ,Xn be such that Xk = Y k \u2212 Y k\u22121 for all 1 \u2264 k \u2264 n. Assume\n\u2016Xk\u20162 \u2264 R, almost surely for all k.\nDefine the predictable quadratic variation process\nW k := k\u2211 j=1 Ej\u22121 [ X2j ] for all k,\nwhere Ej\u22121 [ X2j ]\ndenotes the expectation ofX2j conditioning onX1, \u00b7 \u00b7 \u00b7 ,Xj\u22121. Then, for all > 0 and \u03c32 > 0,\nPr ( \u2016Y n\u20162 \u2265 and \u2016W n\u20162 \u2264 \u03c3 2 )\n\u2264 d \u00b7 exp ( \u2212 2/2\n\u03c32 +R /3\n) ."}], "year": 2018, "references": [{"title": "Fast randomized kernel ridge regression with statistical guarantees", "authors": ["A. Alaoui", "M.W. Mahoney"], "venue": "In Proceedings of the 29th Annual Conference on Neural Information Processing Systems,", "year": 2015}, {"title": "Stability yields a PTAS for k-median and k-means clustering", "authors": ["P. Awasthi", "A. Blum", "O. Sheffet"], "venue": "In Proceedings of the 51st Annual IEEE Symposium on Foundations of Computer Science,", "year": 2010}, {"title": "Unsupervised feature selection for the k-means clustering problem", "authors": ["C. Boutsidis", "P. Drineas", "M.W. Mahoney"], "venue": "In Proceedings of the 23rd Annual Conference on Neural Information Processing Systems,", "year": 2009}, {"title": "Randomized dimensionality reduction for k-means clustering", "authors": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "Unsupervised feature selection for multi-cluster data", "authors": ["D. Cai", "C. Zhang", "X. He"], "venue": "In Proceedings of the 16th ACM International Conference on Knowledge Discovery and Data Mining,", "year": 2010}, {"title": "Kernel feature selection via conditional covariance minimization", "authors": ["J. Chen", "M. Stern", "M.J. Wainwright", "M.I. Jordan"], "venue": "In Proceedings of the 31st Annual Conference on Neural Information Processing Systems,", "year": 2017}, {"title": "Uniform sampling for matrix approximation", "authors": ["M.B. Cohen", "Y.T. Lee", "C. Musco", "R. Peng", "A. Sidford"], "venue": "In Proceedings of the 6th Conference on Innovations in Theoretical Computer Science,", "year": 2015}, {"title": "Online row sampling", "authors": ["M.B. Cohen", "C. Musco", "J. Pachocki"], "venue": "In Proceedings of the 19th International Workshop on Approximation Algorithms for Combinatorial Optimization Problems,", "year": 2016}, {"title": "Higher criticism thresholding: Optimal feature selection when useful features are rare and weak", "authors": ["D. Donoho", "J. Jin"], "venue": "Proceedings of the National Academy of Sciences,", "year": 2008}, {"title": "Least angle regression", "authors": ["B. Efron", "T. Hastie", "I. Johnstone", "R Tibshirani"], "venue": "Annals of Statistics,", "year": 2004}, {"title": "An extensive empirical study of feature selection metrics for text classification", "authors": ["G. Forman"], "venue": "Journal of Machine Learning Research,", "year": 2003}, {"title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "authors": ["A. Gittens", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "An introduction to variable and feature selection", "authors": ["I. Guyon", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "year": 2003}, {"title": "Gene selection for cancer classification using support vector machines", "authors": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning,", "year": 2002}, {"title": "Laplacian score for feature selection", "authors": ["X. He", "D. Cai", "P. Niyogi"], "venue": "In Proceedings of the 19th Annual Conference on Neural Information Processing Systems,", "year": 2005}, {"title": "Semi-supervised feature selection for graph classification", "authors": ["X. Kong", "P.S. Yu"], "venue": "In Proceedings of the 16th ACM International Conference on Knowledge Discovery and Data Mining,", "year": 2010}, {"title": "Faster spectral sparsification and numerical algorithms for SDD matrices", "authors": ["I. Koutis", "A. Levin", "R. Peng"], "venue": "ACM Transactions on Algorithms,", "year": 2016}, {"title": "Iterative row sampling", "authors": ["M. Li", "G.L. Miller", "R. Peng"], "venue": "In Proceedings of the 54th Annual IEEE Symposium on Foundations of Computer Science,", "year": 2013}, {"title": "Sparse embedded k-means clustering", "authors": ["W. Liu", "X. Shen", "I. Tsang"], "venue": "In Proceedings of the 31st Annual Conference on Neural Information Processing Systems,", "year": 2017}, {"title": "Recursive sampling for the Nystr\u00f6m method", "authors": ["C. Musco"], "venue": "In Proceedings of the 31st Annual Conference on Neural Information Processing Systems,", "year": 2017}, {"title": "The effectiveness of lloyd-type methods for the kmeans problem", "authors": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science,", "year": 2006}, {"title": "Feature selection based on mutual information criteria of max-dependency, maxrelevance, and min-redundancy", "authors": ["H. Peng", "F. Long", "C. Ding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2005}, {"title": "Online feature selection using grafting", "authors": ["S. Perkins", "J. Theiler"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "year": 2003}, {"title": "Preconditioned data sparsification for big data with applications to PCA and k-means", "authors": ["F. Pourkamali-Anaraki", "S. Becker"], "venue": "IEEE Transactions on Information Theory,", "year": 2017}, {"title": "On the iteration complexity of support recovery via hard thresholding pursuit", "authors": ["J. Shen", "P. Li"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Graph sparsification by effective resistances", "authors": ["D.A. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing,", "year": 1913}, {"title": "Freedman\u2019s inequality for matrix martingales", "authors": ["Tropp", "J. A"], "venue": "Electronic Communications in Probability,", "year": 2011}, {"title": "Online group feature selection", "authors": ["J. Wang", "Zhao", "Z.-Q", "X. Hu", "Cheung", "Y.-M", "M. Wang", "X. Wu"], "venue": "In Proceedings of the 23rd International Joint Conference on Artificial Intelligence,", "year": 2013}, {"title": "Online feature selection with group structure analysis", "authors": ["J. Wang", "M. Wang", "P. Li", "L. Liu", "Z. Zhao", "X. Hu", "X. Wu"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2015}, {"title": "Unsupervised feature selection by preserving stochastic neighbors", "authors": ["X. Wei", "S.Y. Philip"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "Online streaming feature selection", "authors": ["X. Wu", "K. Yu", "H. Wang", "W. Ding"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "year": 2010}, {"title": "Gradient boosted feature selection", "authors": ["Z. Xu", "G. Huang", "K.Q. Weinberger", "A.X. Zheng"], "venue": "In Proceedings of the 20th ACM International Conference on Knowledge Discovery and Data Mining, pp", "year": 2014}, {"title": "A unified robust regression model for lasso-like algorithms", "authors": ["W. Yang", "H. Xu"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "year": 2013}, {"title": "On the consistency of feature selection using greedy least squares regression", "authors": ["T. Zhang"], "venue": "Journal of Machine Learning Research,", "year": 2009}, {"title": "On the consistency of feature selection with lasso for non-linear targets", "authors": ["Y. Zhang", "S. Ray", "E.W. Guo"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Spectral feature selection for supervised and unsupervised learning", "authors": ["Z. Zhao", "H. Liu"], "venue": "In Proceedings of the 24th International Conference on Machine Learning,", "year": 2007}, {"title": "Streaming feature selection using alpha-investing", "authors": ["J. Zhou", "D. Foster", "R. Stine", "L. Ungar"], "venue": "In Proceedings of the 11st ACM International Conference on Knowledge Discovery and Data Mining, pp", "year": 2005}, {"title": "Semisupervised learning using gaussian fields and harmonic functions", "authors": ["X. Zhu", "Z. Ghahramani", "J.D. Lafferty"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "year": 2003}], "id": "SP:577d9bb32df3b9d9b327663ff91fd6805ced821b", "authors": [{"name": "Jing Wang", "affiliations": []}, {"name": "Jie Shen", "affiliations": []}, {"name": "Ping Li", "affiliations": []}], "abstractText": "In large-scale machine learning applications and high-dimensional statistics, it is ubiquitous to address a considerable number of features among which many are redundant. As a remedy, online feature selection has attracted increasing attention in recent years. It sequentially reveals features and evaluates the importance of them. Though online feature selection has proven an elegant methodology, it is usually challenging to carry out a rigorous theoretical characterization. In this work, we propose a provable online feature selection algorithm that utilizes the online leverage score. The selected features are then fed to k-means clustering, making the clustering step memory and computationally efficient. We prove that with high probability, performing k-means clustering based on the selected feature space does not deviate far from the optimal clustering using the original data. The empirical results on realworld data sets demonstrate the effectiveness of our algorithm.", "title": "Provable Variable Selection for Streaming Features"}