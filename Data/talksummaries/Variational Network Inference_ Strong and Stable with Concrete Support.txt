23	40	The variance of the network signal, for example, is of the same order as that of the network-independent parameters, and therefore robust to changes in the network-dependent distributions.
25	23	We benchmark our approach against the state of the art on three very different and challenging problems: discovering brain functional connectivity, modeling property prices in Sydney, and understanding regulation in the yeast genome.
27	18	Furthermore, when investigating the full yeast genome regulation, our qualitative analyses show that even in a large network (up to 38,000,000+ arcs), our technique is able to recover both high-level and low-level knowledge that is strikingly consistent with the previous literature and hints on original findings.
39	37	Given a dataset D of vector-valued observations Y = {yi}Ni=1 and their corresponding times1 {ti}Ni=1 from N nodes in a network, our goal is to infer the existence and strength of the arcs between the nodes.
41	14	Let yi(t) be the output of node i at time t, yi(t) = fi(t) + it, it ∼ N(0, σ2y), (1) where σ2y is the observation-noise variance.
50	21	Our main inference task is to estimate the posterior over the network parameters p(A,W|D).
51	21	To this end, by exploiting the closeness of GPs under linear operators, we will first show in §3.1 the exact expression for the (conditional) marginal likelihood p(Y|A,W) obtained when marginalizing the latent functions.
52	12	Furthermore, by establishing a relationship of our model to multi-task learning (Rakitsch et al., 2013; Bonilla et al., 2008), we show how to compute this marginal likelihood efficiently.
53	16	Subsequently, due to the highly nonlinear dependence of p(Y|A,W) on A,W, we will approximate the posterior over these network parameters using variational inference in §3.2.
54	9	Let us denote the values of all latent functions fi(t) at time t with f(t) = [f1(t), .
61	36	Let us assume synchronized observations, i.e. that observations for all nodes lie on a grid in time, t = 1, .
68	18	In our case, the nodes in the network can be seen as the tasks in a multi-task GP model and are associated with a task-dependent covariance Kf , which is fully determined by the parameters of the network A,W.
77	20	The main difficulties of computing the log-marginal likelihood above are the calculation of the logdeterminant of an n dimensional matrix, as well as solving an n-dimensional system of linear equations.
81	14	To give some intuition behind such derivations, the main idea is to “factorout” the noise matrix σ2fE+σ 2 yI from the covariance matrix Σy and then apply properties of the Kronecker product.
87	21	For non-trivial models and approximate posteriors, the expectations required in the objective above are analytically intractable.
88	13	Modern variational inference methods estimate Lelbo and its gradients using Monte Carlo samples and the re-parameterization trick (see e.g. Kingma & Welling, 2014; Rezende et al., 2014).
90	45	Furthermore, since the re-parameterization trick cannot be applied to discrete distributions, we use a continuous relaxation of discrete random variables known as the Concrete distribution (Maddison et al., 2016; Jang et al., 2016).
92	17	Analogously to Maddison et al. (2016), we also relax our priors and estimate the log-probabilities in Lkl using: log q(Aij) = log λc − λcaij + logαij − 2 log(1 + exp(−λcaij + logαij)), (13) and similarly for p(Aij).
93	10	Having relaxed our discrete variables, we proceed with optimization of the Lelbo in eq.
118	9	Theorem 2 For any value of the parameters of the concrete distributions (λc ≥ 0, αij ≥ 0 (i 6= j)), any σ2y > 0, |Lell| ∞.
119	23	The complete proofs of these theorems are given in the supplement, §II.1, §II.2.
120	15	Since we get stability for free, where other popular approaches need to make assumptions to get it, one might ask what more we can get under assumptions that would look alike.
122	60	What we now show is that under similar assumptions, we do not just get stability for inference, we make it numerically easy with high probability, and this holds for a sampling model (M) more general than ours, meaning that one could make alternative choices to the concrete distributions we use and yet keep the same property: (M ) (∀i, j) (i) weight Wij is picked as N(µij , σ2ij) (µij ∈ R, σij > 0), and (ii) adjacency Aij is picked as Bern(ρij) with ρij ∼ V, where V is any random variable with support in [0, 1] (letting pij def = E[ρij ]).
123	37	To get our result, we need two functions that aggregate the complete network signal coming from (or to) each node, U, S : {1, 2, ..., 2N} → R+, defined as: U(i) def = 2 N · { ∑ j pij(µ 2 ij + σ 2 ij) if i ≤ N∑ j pji∗(µ 2 ji∗ + σ 2 ji∗) otherwise , S(i) def = 1 N · { ∑ j µ 2 ij + σ 2 ij if i ≤ N∑ j µ 2 ji∗ + σ 2 ji∗ otherwise , with i∗ def= i−N .
124	16	Let us now state our main result.
127	25	Under sampling model M , suppose that max i U(i) ∈ [ maxi S(i) Nγ , 1 100N2 ] .
128	43	(16) Hence, if the non-network dependent “signal” is not flat (say, λ↓(Kt), σ2y are above machine zero), then it is in fact pretty easy to sample Lell over most of its support.
131	23	We also remark that we do not face the sparsity constraints of the model of Linderman & Adams (2014), such as a mandatory sparsity increase with N .
132	16	Theorem 3 is a direct consequence of another Theorem which can be roughly summarized as: "modulo an assumption on the network-dependent parameters, the covariance of observations is of the same order as the (co)variance of the network independent component plus that of the noise."
134	18	This can also be viewed as a balance achieved on the second-order moments, between the network-dependent “signal” versus the one which is not network-dependent.
