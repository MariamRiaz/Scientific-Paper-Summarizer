0	20	Recent successes of deep learning go beyond achieving state-of-the-art results in research benchmarks, and push the frontiers in some of the most challenging real world applications such as speech recognition (Hinton et al., 2012), image recognition (Krizhevsky et al., 2012; Szegedy et al., 2015), and machine translation (Wu et al., 2016).
10	67	The bridge between them is a new form of neural network distillation (Hinton et al., 2015), which we refer to as Probability Density Distillation, where a trained WaveNet model is used as a teacher for a feedforward IAF model.
14	15	Autoregressive networks model the joint distribution of highdimensional data as a product of conditional distributions using the probabilistic chain-rule: p(x) = ∏ t p(xt|x<t,θ), where xt is the t-th variable of x and θ are the parameters of the autoregressive model.
16	21	WaveNet (van den Oord et al., 2016a) is a convolutional autoregressive model which produces all p(xt|x<t) in one forward pass, by making use of causal—or masked— convolutions (van den Oord et al., 2016c; Germain et al., 2015).
18	21	At generation time, however, the waveform has to be synthesised in a sequential fashion as xt must be sampled first in order to obtain x>t.
19	79	Due to this nature, real time (or faster) synthesis with a fully autoregressive system is challenging.
20	21	While sampling speed is not a significant issue for offline generation, it is essential for real-word applications.
22	94	Raw audio data is typically very high-dimensional (e.g. 16,000 samples per second for 16kHz audio), and contains complex, hierarchical structures spanning many thousands of time steps, such as words in speech or melodies in music.
24	60	WaveNet avoids this constraint by using dilated causal convolutions, which allow the receptive field to grow exponentially with depth.
31	16	Unlike previous versions of WaveNet (van den Oord et al., 2016a), where 8-bit (µ-law or PCM) audio was modelled with a 256-way categorical distribution, we increased the fidelity by modelling 16-bit audio.
41	16	For all normalizing flows the transformation f is chosen so that it is invertible and its Jacobian determinant is easy to compute.
58	18	Although inverse-autoregressive flows (IAFs) and autoregressive models can in principle model the same distributions (Chen et al., 2016), they have different inductive biases and may vary greatly in their capacity to model certain processes.
67	16	When the KL divergence becomes zero, the student distribution has fully recovered the teacher’s distribution.
84	58	For every sample x we draw from the student pS we can compute all pT (xt|x<t) in parallel with the teacher and then evaluate H(pS(xt|z<t), pT (xt|x<t)) very efficiently by drawing multiple different samples xt from pS(xt|z<t) for each timestep.
85	27	This unbiased estimator has a much lower variance than naively evaluating the sample under the teacher with Equation 9.
86	19	We parameterise the teacher’s output distribution pT (xt|x<t) as a mixture of logistics distribution (Salimans et al., 2017), which allows the loss term ln pT (xt|x<t) to be differentiable with respect to both xt and x<t.
93	23	This effect is not due to adversarial behaviour on the part of the teacher, but rather is a fundamental property of the data distribution which the teacher has approximated.
94	21	As an example consider the simple case where we have audio from a white random noise source: the distribution at every timestep is N (0, 1), regardless of the samples at previous timesteps.
95	17	White noise has a very specific and perceptually recognizable sound: a continual hiss.
96	23	The MAP estimate of this data distribution, and thus of any generative model that matches it well, recovers the distribution mode, which is 0 at every timestep: i.e. complete silence.
100	33	Furthermore, if one changes the representation of the data (e.g., by nonlinearly pre-processing the audio signal), then the MAP estimate changes, unlike the KL-divergence in Equation 6, which is invariant to the coordinate system.
101	74	Training with Probability Density Distillation alone might not sufficiently constrain the student to generate high quality audio streams.
102	69	Therefore, we also introduce additional loss functions to guide the student distribution towards the desired output space.
103	98	The first additional loss we propose is the power loss, which ensures that the power in different frequency bands of the speech are on average used as much as in human speech.
104	38	The power loss helps to avoid the student from collapsing to a high-entropy WaveNet-mode, such as whispering.
105	18	The power-loss is defined as: ||φ(g(z, c))− φ(y)||2, (16) where (y, c) is an example with conditioning from the training set, φ(x) = |STFT(x)|2 and STFT stands for the ShortTerm Fourier Transform.
106	20	We found that φ(x) can be averaged over time before taking the Euclidean distance with little difference in effect, which means it is the average power for various frequencies that is important.
112	17	The latter produced better results in our experiments.
113	18	Finally, we also introduce a contrastive distillation loss as follows: DKL ( PS(c1) ∣∣∣∣∣∣PT (c1))−γDKL(PS(c1)∣∣∣∣∣∣PT c2)), (17) which minimises the KL-divergence between the teacher and student when both are conditioned on the same information c1 (e.g., linguistic features, speaker ID, .
117	23	The contrastive loss penalises waveforms that have high likelihood regardless of the conditioning vector.
