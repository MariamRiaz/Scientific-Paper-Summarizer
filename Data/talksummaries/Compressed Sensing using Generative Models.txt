0	18	Compressive or compressed sensing is the problem of reconstructing an unknown vector x⇤ 2 Rn after observing m < n linear measurements of its entries, possibly with added noise: y = Ax⇤ + ⌘, where A 2 Rm⇥n is called the measurement matrix and ⌘ 2 Rm is noise.
1	13	Even without noise, this is an underdetermined system of linear equations, so recovery is impossible unless we make an assumption on the structure Code for experiments in the paper can be found at: https://github.com/AshishBora/csgm of the unknown vector x⇤.
2	47	We need to assume that the unknown vector is “natural,” or “simple,” in some applicationdependent way.
3	53	The most common structural assumption is that the vector x⇤ is k-sparse in some known basis (or approximately k-sparse).
4	95	Finding the sparsest solution to an underdetermined system of linear equations is NP-hard, but still convex optimization can provably recover the true sparse vector x⇤ if the matrix A satisfies conditions such as the Restricted Isometry Property (RIP) or the related Restricted Eigenvalue Condition (REC) (Tibshirani, 1996; Candes et al., 2006; Donoho, 2006; Bickel et al., 2009).
5	86	The problem is also called high-dimensional sparse linear regression and there is vast literature on establishing conditions for different recovery algorithms, different assumptions on the design of A and generalizations of RIP and REC for other structures, see e.g. (Bickel et al., 2009; Negahban et al., 2009; Agarwal et al., 2010; Loh & Wainwright, 2011; Bach et al., 2012).
6	43	This significant interest is justified since a large number of applications can be expressed as recovering an unknown vector from noisy linear measurements.
7	129	For example, many tomography problems can be expressed in this framework: x⇤ is the unknown true tomographic image and the linear measurements are obtained by x-ray or other physical sensing system that produces sums or more general linear projections of the unknown pixels.
8	32	Compressed sensing has been studied extensively for medical applications including computed tomography (CT) (Chen et al., 2008), rapid MRI (Lustig et al., 2007) and neuronal spike train recovery (Hegde et al., 2009).
9	34	Another impressive application is the “single pixel camera” (Duarte et al., 2008), where digital micro-mirrors provide linear combinations to a single pixel sensor that then uses compressed sensing reconstruction algorithms to reconstruct an image.
10	5	These results have been extended by combining sparsity with additional structural assumptions (Baraniuk et al., 2010; Hegde et al., 2015), and by generalizations such as translating sparse vectors into low-rank matrices (Negahban et al., 2009; Bach et al., 2012; Foygel & Mackey, 2014).
11	16	These results can improve performance when the structural assumptions fit the sensed signals.
12	33	Other works perform “dictionary learning,” seeking overcomplete bases where the data is more sparse (see (Chen & Needell, 2016) and refer- ences therein).
14	12	Recently, several neural network based generative models such as variational autoencoders (VAEs) (Kingma & Welling, 2013) and generative adversarial networks (GANs) (Goodfellow et al., 2014) have found success at modeling data distributions.
17	34	We can therefore use any pre-trained generator to approximately capture the notion of a vector being “natural” in our domain: the generator defines a probability distribution over vectors in sample space and tries to assign higher probability to more likely vectors, for the dataset it has been trained on.
18	118	We expect that vectors “natural” to our domain will be close to some point in the support of this distribution, i.e., in the range of G. Our Contributions: We present an algorithm that uses generative models for compressed sensing.
19	83	Our algorithm simply uses gradient descent to optimize the representation z 2 Rk such that the corresponding image G(z) has small measurement error kAG(z) yk2 2 .
20	79	While this is a nonconvex objective to optimize, we empirically find that gradient descent works well, and the results can significantly outperform Lasso with relatively few measurements.
21	88	We obtain theoretical results showing that, as long as gradient descent finds a good approximate solution to our objective, our output G(z) will be almost as close to the true x⇤ as the closest possible point in the range of G. The proof is based on a generalization of the Restricted Eigenvalue Condition (REC) that we call the SetRestricted Eigenvalue Condition (S-REC).
24	273	Specifically, for d-layer neural networks such as VAEs and GANs, we show that O(kd log n) Gaussian measurements suffice to guarantee good reconstruction with high probability.
25	49	One result, for ReLU-based networks, is the following: Theorem 1.1.
26	10	Rn be a generative model from a d-layer neural network using ReLU activations.
27	260	Let A 2 Rm⇥n be a random Gaussian matrix for m = O(kd log n), scaled so A i,j ⇠ N(0, 1/m).
28	72	For any x⇤ 2 Rn and any observation y = Ax⇤+ ⌘, let bz minimize ky AG(z)k 2 to within additive ✏ of the optimum.
