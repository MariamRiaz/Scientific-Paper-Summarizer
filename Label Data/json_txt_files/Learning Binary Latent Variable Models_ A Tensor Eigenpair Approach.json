{"sections": [{"heading": "1. Introduction", "text": "In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1. The hidden layer, h = (h1, . . . , hd), consists of d binary random variables with an unknown joint distribution Ph : {0, 1}d \u2192 [0, 1]. The observed vector x \u2208 Rm with m \u2265 d features is modeled as\nx = W>h+ \u03c3\u03be, (1)\nwhereW \u2208 Rd\u00d7m is an unknown weight matrix assumed to be full rank d. Here, \u03c3 \u2265 0 is the noise level and \u03be is an additive noise vector independent of h, whose m coordinates are all i.i.d. zero mean and unit variance random variables.\n1Dept. of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel. 2Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel. 3Program of Applied Mathematics, Yale University, New Haven, CT 06511, USA. Correspondence to: Ariel Jaffe <ariel.jaffe@weizmann.ac.il>, Roi Weiss <roi.weiss@weizmann.ac.il>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nFor simplicity we assume it is Gaussian, though our method can be modified to handle other noise distributions.\nThe model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997). A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012). G-RBMs were used, e.g., in modeling human motion (Taylor et al., 2007) and natural image patches (Melchior et al., 2017).\nGiven n i.i.d. samples x1, . . . ,xn from model (1), the goal is to estimate the weight matrixW . A common approach for learning W is by maximum likelihood. As this function is non-convex, common optimization schemes include the EM algorithm and alternating least squares (ALS). In addition, several works developed iterative methods specialized to GRBMs (Hinton, 2010; Cho et al., 2011). All these methods, however, often lack consistency guarantees and may not be well suited for large datasets due to their potential slow convergence. This is not surprising, as learning W under model (1) is believed to be computationally hard; see for example Mossel & Roch (2005).\nOver the past years, several works considered variants and specific instances of model (1) under additional assumptions on the distribution Ph or on the weight matrix W . For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyva\u0308rinen et al., 2004). In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyva\u0308rinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014). Similarly, when the hidden units are mutually exclusive, namely Ph has support h \u2208 {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means. Efficient and consistent algorithms were derived for this case as well (Moitra & Valiant, 2010; Anandkumar et al., 2012a;b; Hsu & Kakade, 2013). Among those, most relevant to this work\nare orthogonal tensor decomposition methods (Anandkumar et al., 2014). Interestingly, these methods can learn some additional latent models, with hidden units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models (Arabshahi & Anandkumar, 2017).\nLearning W given the observed data {xj}nj=1 can also be viewed as a noisy matrix factorization problem. If W is known to be non-negative, then various non-negative matrix factorization methods can be used. Moreover, under appropriate conditions, some of these methods were proven to be computationally efficient and consistent (Donoho & Stodden, 2004; Arora et al., 2012). For general full rank W , the matrix factorization method in Slawski et al. (2013) (SHL) exactly recovers W when \u03c3 = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and has no consistency guarantees when \u03c3 > 0.\nA tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed data. We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n \u2212 12 ) for any noise level \u03c3 \u2265 0.\nThe non-degeneracy conditions we pose are significantly weaker than those required by the previous tensor decomposition methods mentioned above. In particular, their assumptions and resulting methods can be viewed as specific cases of our more general approach.\nSimilarly to the matrix factorization method in Slawski et al. (2013), our algorithm has runtime linear in n, polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime is spent on computing the eigenpairs of a d\u00d7 d\u00d7 d tensor. Practically, our method, implemented without any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC. Furthermore, the overall runtime can be significantly reduced, since the step of computing the tensor eigenpairs can be embarrassingly parallelized.\nPaper outline In the next section we give necessary background on tensor eigenpairs. In Section 3 we introduce our\nmethod in the case \u03c3 = 0. The case \u03c3 \u2265 0 is treated in Section 4. Experiments with our method and comparison to other approaches appear in Section 5. All proofs are deferred to the supplementary material."}, {"heading": "2. Preliminaries", "text": "Notation Denote [d] = {1, . . . , d} and ei as the i-th unit vector. We slightly abuse notation and view a matrixW also as the set of its columns, namely w \u2208 W is some column of W and span(W ) is the span of all its columns. The unit sphere is denoted by Sd\u22121 = {u \u2208 Rd : \u2016u\u2016 = 1}.\nA tensor T \u2208 Rd\u00d7d\u00d7d is symmetric if Tijk = T\u03c0(i,j,k) for all permutations \u03c0 of i, j, k. Here, we consider only symmetric tensors. T can also be seen as a multi-linear operator: for matrices W 1,W 2,W 3 with W i \u2208 Rd\u00d7di , the tensor-mode product, denoted T (W 1,W 2,W 3), is a d1 \u00d7 d2 \u00d7 d3 tensor whose (i1, i2, i3)-th entry is\u2211\nj1,j2,j3\u2208[d]\nW 1j1i1W 2 j2i2W 3 j3i3Tj1j2j3 .\nTensor eigenpairs Several types of eigenpairs of a tensor have been proposed. Here, we consider the following definition, termed Z-eigenpairs by Qi (2005) and l2-eigenpairs by Lim (2005). Henceforth we just call them eigenpairs.\nDefinition 1. (u, \u03bb) \u2208 Rd \u00d7 R is an eigenpair of T if\nT (I,u,u) = \u03bbu and \u2016u\u2016 = 1. (2)\nNote that if (u, \u03bb) is an eigenpair then the eigenvalue is simply \u03bb = T (u,u,u). In addition, (\u2212u,\u2212\u03bb) is also an eigenpair. Following common practice we treat these two pairs as one and make the convention that \u03bb \u2265 0.\nIn contrast to the matrix case, the number of eigenvalues {\u03bb} of a tensor T \u2208 Rd\u00d7d\u00d7d can be much larger than d. As shown by Cartwright & Sturmfels (2013), for a d\u00d7 d\u00d7 d tensor, there can be at most 2d \u2212 1 of them. With precise definitions appearing in Cartwright & Sturmfels (2013), for a generic tensor, all its eigenvalues have multiplicity one and the number of eigenpairs {(u, \u03bb)} is at most 2d \u2212 1.\nIn principle, enumerating the set of all eigenpairs of a general symmetric tensor is a #P problem (Hillar & Lim, 2013). Nevertheless, several methods have been proposed for computing at least some eigenpairs, including iterative higherorder power methods (Kolda & Mayo, 2011; 2014), homotopy continuation (Chen et al., 2016), semidefinite programming (Cui et al., 2014), and iterative Newton-based methods (Jaffe et al., 2017; Guo et al., 2017). We conclude this section with the definition of Newton-stable eigenpairs (Jaffe et al., 2017) which are most relevant to our work.\nNewton-stable eigenpairs Equivalently to (2), eigenpairs of T can also be characterized by the function g : Rd \u2192 Rd,\ng(u) = T (I,u,u)\u2212 T (u,u,u) \u00b7 u. (3)\nIt is easy to verify that a pair (u, \u03bb) with \u2016u\u2016 = 1 is an eigenpair of T if and only if g(u) = 0 and \u03bb = T (u,u,u). The stability of an eigenpair is determined by its Jacobian matrix \u2207g(u) \u2208 Rd\u00d7d, more precisely, by its projection into the d\u2212 1 dimensional subspace orthogonal to u. Formally, let Lu \u2208 Rd\u00d7(d\u22121) be a matrix with d\u2212 1 orthonormal columns that span the subspace orthogonal to u and define the (d\u2212 1)\u00d7 (d\u2212 1) projected Jacobian matrix\nJp(u) = L > u\u2207g(u)Lu. (4)\nDefinition 2. An eigenpair (u, \u03bb) of T \u2208 Rd\u00d7d\u00d7d is Newton-stable if the matrix Jp(u) has full rank d\u2212 1.\nThe homotopy continuation method in Chen et al. (2016) is guaranteed to compute all the Newton-stable eigenpairs of a tensor. Alternatively, all the Newton-stable eigenpairs can be computed by the iterative orthogonal Newton correction method (O\u2013NCM) in Jaffe et al. (2017) as these are the attracting fixed points for this algorithm. Moreover, O\u2013NCM converges to any Newton-stable eigenpair at a quadratic rate given a sufficiently close initial guess. Finally, for a generic tensor, all its eigenpairs are Newton-stable."}, {"heading": "3. Learning in the noiseless case", "text": "To motivate our approach for estimating the matrix W it is instructive to first consider the ideal noiseless case where \u03c3 = 0. In this case, model (1) takes the form x = W>h. Our problem then becomes that of factorizing the observed matrix X = [x1, . . . ,xn] \u2208 Rm\u00d7n of n samples into a product of real and binary low-rank matrices,1\nFind W \u2208 Rd\u00d7m, H \u2208 {0, 1}d\u00d7n s.t. X = W>H. (5)\nTo be able to recover W we first need conditions under which the decomposition of X into W and H is unique. Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore this degeneracy. A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid. Formally, H \u2208 {0, 1}d\u00d7n is rigid if any non-trivial linear combination of its rows yields a non-binary vector: \u2200u 6= 0,\nu>H \u2208 {0, 1}n \u21d4 u \u2208 {ei}di=1. (6)\nCondition (6) is satisfied, for example, when the columns of H include ei and ei + ej for all i 6= j \u2208 [d]. If there\n1Note that this is different from the problem known as \u201cBoolean matrix factorization\u201d, where X and W are assumed to be binary as well; see Miettinen & Vreeken (2014) and references therein.\nexists a positive constant p0 > 0 such that Ph(ei) \u2265 p0 and Ph(ei + ej) \u2265 p0, then for a sample size n > 2 log(d)/p0 the matrix H is rigid with high probability.\nThe following proposition, similar in nature to the (affine constrained) uniqueness guarantee in Slawski et al. (2013), shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary constraints.\nProposition 1. Let X = W>H with H \u2208 {0, 1}d\u00d7n rigid and W \u2208 Rd\u00d7m full rank with m \u2265 d. Let W \u2020 \u2208 Rm\u00d7d be the unique right pseudo-inverse of W so WW \u2020 = Id. Then W and H are unique and for all v \u2208 span(X) \\ {0},\nv>X \u2208 {0, 1}n \u21d4 v \u2208W \u2020. (7)\nHence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of finding the unique set W \u2020 = {v\u22171 , . . . ,v\u2217d} \u2286 span(X) of d non-zero vectors that satisfy the binary constraints v\u2217i >X \u2208 {0, 1}n. The weight matrix is then W = (W \u2020)\u2020.\nAlgorithm outline We recover W \u2020 via a two step procedure. First, a finite set V = {v1,v2, . . . } \u2286 span(X) of candidate vectors is computed with a guarantee that W \u2020 \u2286 V . Specifically, V is computed from the set of eigenpairs of a d\u00d7 d\u00d7 d tensor, constructed from the low order moments of X . Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v \u2208 V that satisfy v>X \u2208 {0, 1}n.\nBefore describing the two steps in more detail we first state the additional non-degeneracy conditions we pose. To this end, denote the unknown first, second, and third order moments of the latent binary vector h by\np = E[h] \u2208 Rd, C = E[h\u2297 h] \u2208 Rd\u00d7d, C = E[h\u2297 h\u2297 h] \u2208 Rd\u00d7d\u00d7d. (8)\nNon-degeneracy conditions We assume the following:\n(I) H is rigid.\n(II) rank(2C(I, I, ei)\u2212 C) = d for all i \u2208 [d].\nCondition (I) implies that both rank(HH>) = d and rank(C) = d. This in turn implies pi = E[hi] > 0 for all i \u2208 [d] and that at most one variable hi has pi = 1. Such an \u201calways on\u201d variable can model a fixed bias to x. As far as we know, condition (II) is new and its nature will become clear shortly.\nWe now describe each step of our algorithm in more detail.\nComputing the candidate set To compute a set V that is guaranteed to include the columns of W \u2020 we make use of\nthe second and third order moments of x,\nM = E[x\u2297 x] \u2208 Rm\u00d7m, M = E[x\u2297 x\u2297 x] \u2208 Rm\u00d7m\u00d7m.\n(9)\nGiven a large number of samples n 1, these can be easily and accurately estimated from the sample X . For simplicity, in this section we consider the population setting where n \u2192 \u221e, so M andM are known exactly. M andM are related to the unknown second and third order moments of h in (8) via (Anandkumar et al., 2014)\nM = W>CW, M = C(W,W,W ). (10)\nSince both C and W are full rank, the number of latent units can be deduced by rank(M) = d. Since C is positive definite, there is a whitening matrix K \u2208 Rm\u00d7d such that\nK>MK = Id. (11)\nSuch a K can be computed, for example, by an eigendecomposition of M . Although K is not unique, any K \u2286 span(M) that satisfies (11) suffices for our purpose. Define the d\u00d7 d\u00d7 d lower dimensional whitened tensor\nW =M(K,K,K). (12)\nDenote the set of eigenpairs ofW by\nU = {(u, \u03bb) \u2208 Sd\u22121 \u00d7 R+ :W(I,u,u) = \u03bbu}. (13)\nOur set of candidates is then\nV = {Ku/\u03bb : (u, \u03bb) \u2208 U with \u03bb \u2265 1} \u2286 Rm. (14)\nThe following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W \u2020. Lemma 1. LetW be the tensor in (12) corresponding to model (1) with \u03c3 = 0 and let V be as in (14). If condition (I) holds then W \u2020 \u2286 V . In particular, each (ui, \u03bbi) in the set of d relevant eigenpairs\nU\u2217 = {(u, \u03bb) \u2208 U : Ku/\u03bb \u2208W \u2020} (15)\nhas the eigenvalue \u03bbi = 1/ \u221a pi \u2265 1 where pi = E[hi] > 0.\nComputing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W \u2020 by first calculating the set U of eigenpairs ofW . Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013). Moreover, besides the d columns of W \u2020, the set V in (14) may contain many spurious candidates, as the number of eigenpairs of W is typically O(2d) d (Cartwright & Sturmfels, 2013).\nNevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs of a tensor under appropriate stability conditions. The following lemma highlights the importance of condition (II) for the stability of the eigenpairs in U\u2217. Note that conditions (I)-(II) do not depend on W , but only on the distribution of h.\nLemma 2. Let W be the whitened tensor in (12) corresponding to model (1) with \u03c3 = 0. If conditions (I)-(II) hold, then all (u, \u03bb) \u2208 U\u2217 are Newton-stable eigenpairs ofW .\nHence, under conditions (I)-(II), the homotopy method in Chen et al. (2016), or alternatively the O\u2013NCM with a sufficiently large number of random initializations (Jaffe et al., 2017), are guaranteed to compute a candidate set V which includes all the columns of W \u2020. The next step is to extract W \u2020 out of V .\nFiltering As suggested by Eq. (7) we select the subset of vectors V\u0304 \u2286 V that satisfy the binary constraints,\nV\u0304 = {v \u2208 V : vTX \u2208 {0, 1}n}. (16)\nIndeed, under condition (I), Proposition 1 implies that V\u0304 = W \u2020 and the weight matrix is thus W = V\u0304 \u2020.\nAlgorithm 2 in Appendix C summarizes our method for the noiseless case and has the following recovery guarantee. Theorem 1. Let X be a matrix of n samples from model (1) with \u03c3 = 0. If conditions (I)-(II) hold, then the above method recovers W exactly.\nWe note that when \u03c3 = 0 and conditions (I)-(II) hold for the empirical latent moments C\u0302 and C\u0302 (rather than C and C), the above procedure exactly recovers W when M and M are replaced by their finite sample estimates. The matrix factorization method SHL in Slawski et al. (2013) also exactly recovers W in the case \u03c3 = 0. While its runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach. This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d \u00d7 d \u00d7 d tensor. However, SHL does not take advantage of the large number of samples n, since only m\u00d7 d sub-matrices of the m\u00d7n sample matrix X are used for constructing its candidate set. Indeed, in the noisy case where \u03c3 > 0, SHL has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels of noise. In the next section we derive a modified version of our method that consistently estimates W for any noise level \u03c3 \u2265 0."}, {"heading": "4. Learning in the presence of noise", "text": "The method in Section 3 to estimateW is clearly inadequate when \u03c3 > 0. However, we now show that by making several adjustments, the two steps of computing the candidate set and its filtering can be both made robust to noise, yielding a consistent estimator of W for any \u03c3 \u2265 0.\nComputing the candidate set As in the case \u03c3 = 0, our goal in the first step is to compute a finite candidate set\nV\u03c3 \u2286 Rm that is guaranteed to contain accurate estimates for the d columns of W \u2020. To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment \u00b5 = E[x] and define the following noise corrected moments,\nM\u03c3 = M \u2212 \u03c32Im,\nM\u03c3 = M\u2212 \u03c32 m\u2211 i=1 ( \u00b5\u2297 ei \u2297 ei\n+ ei \u2297 \u00b5\u2297 ei + ei \u2297 ei \u2297 \u00b5 ) .\n(17)\nBy assumption, the noise satisfies E[\u03be3i ] = 0. Thus, similarly to the moment equations in (10), the modified moments in (17) are related to these ofh by (Anandkumar et al., 2014)\nM\u03c3 = W >CW, M\u03c3 = C(W,W,W ). (18)\nHence, if M\u03c3 andM\u03c3 were known exactly, a candidate set V\u03c3 that contains W \u2020 could be obtained exactly as in the noiseless case, but with M andM replaced with M\u03c3 and M\u03c3; namely, first calculate the whitening matrix K\u03c3 such that K>\u03c3M\u03c3K\u03c3 = Id and then compute the eigenpairs of the population whitened tensor\nW\u03c3 =M\u03c3(K\u03c3,K\u03c3,K\u03c3). (19)\nIn practice, \u03c32, d, \u00b5,M andM are all unknown and need to be estimated from the sample matrix X . Assuming m > d, the parameters \u03c32 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009). For simplicity, we assume they are known exactly. Similarly, \u00b5, M ,M are consistently estimated by their empirical means, \u00b5\u0302, M\u0302 , and M\u0302. So, after computing the plugin estimates K\u0302\u03c3 such that K\u0302>\u03c3 M\u0302\u03c3K\u0302\u03c3 = Id and W\u0302\u03c3 = M\u0302\u03c3(K\u0302\u03c3, K\u0302\u03c3, K\u0302\u03c3), we compute the set U\u0302\u03c3 of eigenpairs of W\u0302\u03c3 and for some small 0 < \u03c4 = O(n\u2212 1 2 ) take our candidate set as\nV\u0302\u03c3 = {K\u0302\u03c3u/\u03bb : (u, \u03bb) \u2208 U\u0302\u03c3 with \u03bb \u2265 1\u2212\u03c4}. (20)\nThe following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations. Namely, for perturbations of order \u03b4 1 inW\u03c3 and K\u03c3 , the method computes a candidate set V\u0302\u03c3 that contains a subset of d vectors that are O(\u03b4) close to the columns of W \u2020. Furthermore, these d vectors all correspond to Newton-stable eigenpairs of the perturbed tensor and are \u2126(1) separated from the other candidates in V\u0302\u03c3 .\nLemma 3. Let K\u03c3,W\u03c3 be the population quantities in (19) and let K\u0302\u03c3, W\u0302\u03c3 be their perturbed versions, inducing the candidate set V\u0302\u03c3 in (20). If conditions (I)-(II) hold, then there are c, \u03b40, \u03b41 > 0 such that for all 0 \u2264 \u03b4 \u2264 \u03b40 the following holds: If the perturbed versions satisfy\nmax{\u2016W\u0302\u03c3 \u2212W\u03c3\u2016F , \u2016K\u0302\u03c3 \u2212K\u03c3\u2016F } \u2264 \u03b4, (21)\nthen any v\u2217 \u2208W \u2020 has a unique v\u0302 \u2208 V\u0302\u03c3 such that\n\u2016v\u0302 \u2212 v\u2217\u2016 \u2264 c\u03b4. (22)\nMoreover, v\u0302 corresponds to a Newton-stable eigenpair of W\u0302\u03c3 with eigenvalue \u03bb \u2265 1\u2212 c\u03b4 and for all v\u0303 \u2208 V\u0302\u03c3 \\ {v\u0302},\n\u2016v\u0303 \u2212 v\u2217\u2016 \u2265 \u03b41 > 2c\u03b4. (23)\nThe proof is based on the implicit function theorem (Hubbard & Hubbard, 2015); small perturbations to a tensor result in small perturbations to its Newton-stable eigenpairs.\nNow, by the delta method, the plugin estimates K\u0302\u03c3 and W\u0302\u03c3 are both OP (n\u2212 1 2 ) close to their population quantities,\n\u2016K\u0302\u03c3 \u2212K\u03c3\u2016F = OP (n \u2212 12 ),\n\u2016W\u0302\u03c3 \u2212W\u03c3\u2016F = OP (n \u2212 12 ).\n(24)\nBy (24), we have that (21) holds with \u03b4 = OP (n\u2212 1 2 ). Hence, by Lemma 3, the eigenpairs of W\u0302\u03c3 provide a candidate set V\u0302\u03c3 that contains d vectors that are OP (n\u2212 1 2 ) close to the columns of W \u2020. In addition, any irrelevant candidate is \u2126P (1) far away from W \u2020. As we show next, these properties ensure that with high probability the d relevant candidates can be identified in V\u0302\u03c3 .\nFiltering Given the candidate set V\u0302\u03c3 computed in the first step, our goal now is to find a set V\u0304\u03c3 \u2286 V\u0302\u03c3 of d vectors that accurately estimate the d columns of W \u2020. To simplify the theoretical analysis, we assume the filtering step is done using a sample X of size n that is independent of V\u0302\u03c3. This can be achieved by first splitting a given sample of size 2n into two sets of size n, one for each step.\nRecall that for x from model (1) and any v \u2208 Rm,\nv>x = v>W>h+ \u03c3v>\u03be. (25)\nObviously, when \u03c3 > 0, the filtering procedure in (16) for the noiseless case is inadequate, as typically no v\u2217 \u2208 W \u2020 will exactly satisfy v\u2217>X \u2208 {0, 1}n. Nevertheless, we expect that for a sufficiently small noise level \u03c3, any v \u2208 V\u0302\u03c3 that is close to some v\u2217 \u2208 W \u2020 will result in v>X that is close to being binary, while any v sufficiently far from W \u2020 will result in v>X that is far from being binary. A natural measure for how v>X is \u201cfar from being binary\u201d, similar to the one used for filtering in Slawski et al. (2013), is simply its deviation from its binary rounding,\nmin b\u2208{0,1}n\n\u2016vTX \u2212 b\u20162\nn\u2016v\u20162 . (26)\nEq. (26) works extremely well for small \u03c3, but fails for high noise levels. Here we instead propose a filtering procedure based on the classical Kolmogorov-Smirnov goodness of fit\ntest (Lehmann & Romano, 2006). As we show below, this approach gives consistent estimates of W for any \u03c3 > 0.\nBefore describing the test, we first introduce the probabilistic analogue of the rigidity condition (6). For any u \u2208 Rd, define its corresponding expected binary rounding error,\nr(u) = Eh\u223cPh [\nmin b\u2208{0,1}\n(u>h\u2212 b)2 ] .\nClearly, r(0) = 0 and r(ei) = 0 for all i \u2208 [d]. We pose the following expected rigidity condition: for all u 6= 0,\nr(u) = 0 \u21d4 u \u2208 {ei}di=1. (27)\nAnalogously to the deterministic rigidity condition in (6), condition (27) is satisfied, for example, when Ph(ei) > 0 and Ph(ei + ej) > 0 for all i 6= j \u2208 [d].\nTo introduce our filtering test, recall that under model (1), \u03be \u223c N (0, Im). Hence, for any fixed v, the random variable v>x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),\nv>x \u223c \u2211\nh\u2208{0,1}d Ph(h) \u00b7 N (v>W>h, \u03c32\u2016v\u20162). (28)\nDenote the cumulative distribution function of v>x by Fv . For general v, this mixture may have up to 2d distinct components. However, for v\u2217 \u2208W \u2020, it reduces to a mixture of two components with means at 0 and 1. More precisely, for any candidate v with corresponding eigenvalue \u03bb(v) \u2265 1, define the GMM with two components\n(1\u2212 1\u03bb(v)2 ) \u00b7 N (0, \u03c3 2\u2016v\u20162) + 1\u03bb(v)2 \u00b7 N (1, \u03c3 2\u2016v\u20162). (29)\nDenote its cumulative distribution function by Gv. The following lemma shows that under condition (27), Gv fully characterizes the columns of W \u2020.\nLemma 4. Let K\u03c3,W\u03c3 be the population quantities in (19) and let V\u03c3 be the set of population candidates as computed from the eigenpairs of W\u03c3. If conditions (I)-(II) and the expected rigidity condition (27) hold, then for any v \u2208 V\u03c3 with corresponding eigenvalue \u03bb(v),\nFv = Gv \u21d4 v \u2208W \u2020.\nGiven the empirical candidate set V\u0302\u03c3, Lemma 4 suggests ranking all v\u0302 \u2208 V\u0302\u03c3 according to their goodness of fit to Gv\u0302 and taking the d candidates with the best fit. More precisely, given a sample X = [x1, . . . ,xn] that is independent of V\u0302\u03c3 , for each candidate v\u0302 \u2208 V\u0302\u03c3 we compute the empirical cumulative distribution function, F\u0302v\u0302(t) = 1n \u2211n j=1 1{v\u0302>xj \u2264 t}, t \u2208 R, and calculate its Kolmogorov-Smirnov score\n\u2206n(v\u0302) = sup t\u2208R |F\u0302v\u0302(t)\u2212Gv\u0302(t)|. (30)\nAlgorithm 1 Estimate W when \u03c3 > 0 and n <\u221e Input: sample matrix X \u2208 Rm\u00d7n and 0 < \u03c4 1\n1: estimate number of hidden units d and noise level \u03c32 2: compute empirical moments \u00b5\u0302, M\u0302 and M\u0302 and plugin moments M\u0302\u03c3 and M\u0302\u03c3 of (17) 3: compute K\u0302\u03c3 such that K\u0302>\u03c3 M\u0302\u03c3K\u0302\u03c3 = Id 4: construct W\u0302\u03c3 = M\u0302\u03c3(K\u0302\u03c3, K\u0302\u03c3, K\u0302\u03c3) 5: compute the set U\u0302\u03c3 of eigenpairs of W\u0302\u03c3 6: compute the candidate set V\u0302\u03c3 in (20) 7: for each v\u0302 \u2208 V\u0302\u03c3 compute its KS score \u2206n(v\u0302) in (30) 8: select V\u0304\u03c3 \u2286 V\u0302\u03c3 of d vectors with smallest \u2206n(v\u0302) 9: return the pseudo-inverse W\u0302 = V\u0304 \u2020\u03c3\nOur estimator V\u0304\u03c3 \u2286 V\u0302\u03c3 for W \u2020 is then the set of d vectors with the smallest scores \u2206n(v\u0302). The estimator for W is the pseudo-inverse, W\u0302 = V\u0304 \u2020\u03c3 .\nThe following lemma shows that for sufficiently large n, \u2206n(v\u0302) accurately distinguishes between v\u0302 \u2208 V\u0302\u03c3 that are close to the columns of W \u2020 from these that are not.\nLemma 5. Let v\u2217 \u2208 W \u2020 and v\u0302(1), v\u0302(2), . . . a sequence of random vectors such that \u2016v\u0302(n) \u2212 v\u2217\u2016 = OP (n\n\u2212 12 ). Then, \u2206n(v\u0302(n)) = oP (1). In contrast, if minv\u2217\u2208W \u2020 \u2016v\u0302(n) \u2212 v\u2217\u2016 = \u2126P (1), then \u2206n(v\u0302(n)) = \u2126P (1), provided the expected rigidity condition (27) holds.\nLemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example Lehmann & Romano (2006); Billingsley (2013).\nAlgorithm 1 summarizes our method for estimating W in the general case where \u03c3 > 0 and n < \u221e. The following theorem establishes its consistency.\nTheorem 2. Let x1, . . . ,xn be n i.i.d. samples from model (1). If conditions (I)-(II) and the expected rigidity condition (27) hold, then the estimator W\u0302 computed by Algorithm 1 is consistent, achieving the parametric rate,\nW\u0302 = W +OP (n \u2212 12 ).\nRuntime The runtime of Algorithm 1 is composed of three main parts. First, O(nm3) operations are needed to compute all the relevant moments from the data and to construct the d\u00d7 d\u00d7 d whitened tensor W\u0302\u03c3 . The most time consuming task is computing the eigenpairs of W\u0302\u03c3, which can be done by either the homotopy method or O\u2013NCM. Currently, no runtime guarantees are available for either of these methods. In practice, since there are O(2d) eigenpairs, these methods spend O(2d \u00b7 poly(d)) operations in total. Finally, since there are O(2d) candidates and each KS test takes O(dn) operations (Gonzalez et al., 1977), the filtering procedure runtime is O(d2dn).\nPower-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact that the set UN of Newton-stable eigenpairs ofW\u03c3 is typically O(2d). However, in some cases, the set U\u2217 of d relevant eigenpairs has additional structure so that a smaller candidate set may be computed instead of UN . Consider the subset UP \u2286 UN of power-stable eigenpairs ofW\u03c3: Definition 3. An eigenpair (u, \u03bb) is power-stable if its projected Jacobian Jp(u) is either positive or negative definite.\nTypically, the number of power-stable eigenpairs is significantly smaller than the number of Newton-stable eigenpairs.2 In addition, UP can be computed by the shifted higher-order power method (Kolda & Mayo, 2011; 2014).\nSimilarly to Lemma 2, one can show that UP is guaranteed to contain U\u2217 whenever the following stronger version of condition (II) holds: for all (ui, \u03bbi) \u2208 U\u2217, the matrix\n(WKLui) >(2C(I, I, ei)\u2212 C)(WKLui) (31)\nis either positive-definite or negative-definite.\nAs an example, consider the case where Ph has the support h \u2208 Id. Then model (1) corresponds to a GMM with d spherical components with linearly independent means. In this case, bothC and C are diagonal with p on their diagonal. Thus, the matrices in (31) take the form \u2212L>ei diag(p)Lei , which are all negative-definite when p > 0. In fact, in this case, W\u03c3 has an orthogonal CP decomposition and the d orthogonal eigenpairs in U\u2217 are the only negative-definite power-stable eigenpairs ofW\u03c3 (Anandkumar et al., 2014). Similarly, when Ph is a product distribution, the same orthogonal structure appears if the centered moments of x are used instead of M andM. As shown in Anandkumar et al. (2014), the power method, accompanied with a deflation procedure, decomposes an orthogonal tensor in polynomial time, thus implying an efficient algorithm in these cases. However, under the much weaker conditions we pose on Ph, the relevant eigenpairs in U\u2217 are not necessarily powerstable and the CP decomposition ofW\u03c3 does not necessarily include U\u2217."}, {"heading": "5. Experiments", "text": "We demonstrate our method in three scenarios: (I) simulations from the exact binary model (1); (II) learning a common population genetic admixture model; (III) learning the proportion matrix of a cell mixture from DNA methylation levels. Due to lack of space, (III) is deferred to Appendix N. Code to reproduce the simulation results can be found at https://github.com/arJaffe/ BinaryLatentVariables.\n2We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d."}, {"heading": "5.1. Simulations", "text": "We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and Gaussian noise \u03be \u223c N (0, Im). The m columns of W were drawn uniformly from the unit sphere Sd\u22121. Fixing a mean vector a \u2208 Rd and a covariance matrix R \u2208 Rd\u00d7d, each hidden vector h was generated independently by first drawing r \u223c N (a, R) and then taking its binary rounding.\nFigure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and \u03c3 (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.\nAs one can see, as opposed to SHL, our method is consistent for \u03c3 > 0 and achieves an error rate O(n\u2212 1 2 ) corresponding to a slope of \u22121 in the upper panel of Fig. 2. In addition, as seen in the lower panel of Fig. 2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate. Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error\n3Code from https://sites.google.com/site/ slawskimartin/code. For each realization X , we made 50 runs of SHL and chose H,W minimizing \u2016X \u2212W>H\u2016F .\nfor high noise levels. A comparison between the runtime of SHL and the spectral method appears in Appendix I."}, {"heading": "5.2. Population genetic admixture", "text": "We present an application of our method to a fundamental problem in population genetics, known as admixture (see Fig. 3). Admixture refers to the mixing of d \u2265 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008). The observed data X is an m\u00d7 n matrix where m is the number of modern \u201cadmixed\u201d individuals and n is the number of relevant locations in their DNA, known as SNPs. Each SNP corresponds to two alleles and individuals may have different alleles. Fixing a reference allele for each location, Xij takes values in {0, 12 , 1} according to the number of reference alleles appearing in the genotype of individual i \u2208 [m] at locus j \u2208 [n].\nGiven the genotypes X , an important problem in population genetics is to estimate the following two quantities. The allele frequency matrix H \u2208 [0, 1]d\u00d7n whose entry Hkj is the frequency of the reference allele at locus j \u2208 [n] in ancestral population k \u2208 [d]; and the admixture proportion matrix W \u2208 [0, 1]d\u00d7m whose columns sum to 1 and its entry Wki is the proportion of individual i\u2019s genome that was inherited from population k.\nA common model for X in terms of W and H is to assume that the number of alleles 2Xij \u2208 {0, 1, 2} is the sum of two i.i.d. Bernoulli random variables with success probability Fij = \u2211d k=1WkiHkj , namely, Xij |H \u223c 1 2 \u00b7 Binomial(2, Fij). Note that under this model\nE[X|H] = F = W>H. (32)\nAlthough (32) has similar form to model (1), there are two main differences; the noise is not normally distributed and the matrix H is non-binary. Yet, the binary model (1) serves as a good approximation whenever various alleles are rare in some populations but abundant in others. Specifically, for ancestral populations that have been long separated, some alleles may become fixed in one population (i.e., reach frequency of 1) while being totally absent in others.\nSimulating genetic admixture We followed a standard simulation scheme apllied, for example, in Xue et al. (2017); Gravel (2012); Price et al. (2009). First, using SCRM (Staab et al., 2015), we simulated d = 3 ancestral populations separated for 4000 generations and generated the genomes of 40 individuals for each. H was then computed as the frequency of the reference alleles in each population. Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter \u03b1 \u2265 0. Finally, the genomes of m = 50 admixed individuals were generated as mosaics of genomic segments of individuals from the ancestral populations with proportions W . The mosaic nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs (Xue et al., 2017). A detailed description is in Appendix M.\nWe compare our algorithm to two methods. The first is Admixture (Alexander et al., 2009), one of the most widely used algorithms in population genetics, which aims to maximize the likelihood of X . The second is the recently proposed spectral method ALStructure (Cabreros & Storey, 2017), where an estimation of span(W>) via Chen & Storey (2015) is followed by constrained ALS iterations of W and H . For our method, two modification are needed for Algorithm 1. First, since the distribution ofXij\u2212wTi hj is not Gaussian, the corrected moments M\u0302\u03c3,M\u0302\u03c3 as calculated by (17) do not satisfy (18). Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details. In addition, the filtering process described in Section 4 is no longer valid. However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.\nFigure 4 compares the results of the 3 methods for \u03b1 = 0.1, 1, 10. The spectral method outperforms Admixture and ALStructure for \u03b1 = 1, 10 and performs similarly to Admixture for \u03b1 = 0.1.\nAcknowledgements This research was funded in part by NIH Grant 1R01HG008383-01A1."}, {"heading": "A. Proof of Proposition 1", "text": "Uniqueness of the factorization readily follows from (7) so we proceed to prove (7). First note that span(X) = span(W>) = span(W \u2020). Since W is full rank, we have WW \u2020 = Id. Hence,\n(W \u2020)>X = (WW \u2020)>H = H \u2208 {0, 1}d\u00d7n.\nSo any v\u2217 \u2208 W \u2020 satisfies the binary constraint v\u2217>X \u2208 {0, 1}n. For the other direction, let v \u2208 span(X) \\ {0} be such that v>X \u2208 {0, 1}n. Since v>X = (Wv)>H , the rigidity condition (6) implies Wv \u2208 {ei}di=1. Since W is full rank and v \u2208 span(W \u2020), v must be a column of W \u2020."}, {"heading": "B. Proof of Lemma 1", "text": "Since the vector h is binary, its second and third order moments are related as follows. For all i, j \u2208 [d],\nCiij = Ciji = Cjii = E[h2ihj ] = E[hihj ] = Cij . (33)\nSince W is full rank, WW \u2020 = Id. Hence, applying W \u2020 multi-linearly on the moment equations in (10) we obtain\nC = (W \u2020)>MW \u2020,\nC = M(W \u2020,W \u2020,W \u2020).\nThus, the equality in (33) is equivalent to\n[M(W \u2020,W \u2020,W \u2020)]iij = [(W \u2020)>MW \u2020]ij . (34)\nLet Y \u2217 \u2208 Rd\u00d7d be the full rank matrix that satisfies W \u2020 = KY \u2217 where K is the whitening matrix in (11). Then,\nM(W \u2020,W \u2020,W \u2020) = M(KY \u2217,KY \u2217,KY \u2217) = W(Y \u2217, Y \u2217, Y \u2217)\nwhereW is the whitened tensor in (12). Similarly, by (11),\n(W \u2020)>MW \u2020 = (Y \u2217)>(KTMK)(Y \u2217) = (Y \u2217)>Y \u2217.\nInserting these into (34), the matrix Y \u2217 must satisfy\n[W(Y \u2217, Y \u2217, Y \u2217)]iij = [(Y \u2217)>(Y \u2217)]ij , \u2200i, j \u2208 [d]. (35)\nThe following lemma, proved in Appendix H, shows that Eq. (35) is nothing but a tensor eigen-problem. Specifically, the columns of Y \u2217, up to scaling, are eigenvectors ofW . Lemma 6. Let W \u2208 Rd\u00d7d\u00d7d be an arbitrary symmetric tensor. Then, a matrix Y = [y1, . . . ,yd] \u2208 Rd\u00d7d of rank d satisfies (35) if and only if for all k \u2208 [d], yk = uk/\u03bbk, where (uk, \u03bbk)dk=1 are d eigenpairs of W with linearly independent {uk}dk=1.\nBy Lemma 6, the set of scaled eigenpairs {y = u/\u03bb} of W is guaranteed to contain the d columns of Y \u2217. Since W \u2020 = KY \u2217, the set {Ky} is guaranteed to contain W \u2020.\nTo show that each y = u/\u03bb \u2208 Y \u2217 has \u03bb \u2265 1, note that the vector Ky is a column of W \u2020, so WKy = ei for some i \u2208 [d]. Hence, by the definition of the whitened tensor (12) and the moment equation (10),\nW(y,y,y) = M(Ky,Ky,Ky) = C(WKy,WKy,WKy) = C(ei, ei, ei) = Ciii = E[hi] \u2264 1.\nOn the other hand, since (u, \u03bb) is an eigenpair ofW with eigenvalue \u03bb =W(u,u,u),\nW(y,y,y) = 1 \u03bb3 W(u,u,u) = 1 \u03bb2 .\nBy convention, \u03bb \u2265 0. Hence, \u03bb = 1/ \u221a E[hi] \u2265 1,\nconcluding the proof."}, {"heading": "C. Recovery algorithm - noiseless case", "text": "Algorithm 2 Recover W when \u03c3 = 0 Input: sample matrix X\n1: estimate second and third order moments M ,M 2: set d = rank(M) 3: compute K \u2286 span(M) such that K>MK = Id 4: compute whitened tensorW =M(K,K,K) 5: compute the set U of eigenpairs ofW 6: compute the candidate set V in (14) 7: filter V\u0304 = {v \u2208 V : v>X \u2208 {0, 1}n} 8: return the pseudo-inverse W = V\u0304 \u2020"}, {"heading": "D. Proof of Lemma 2", "text": "Let (u, \u03bb) \u2208 U\u2217 be an eigenpair of W such that v\u2217 = Ku/\u03bb \u2208W \u2020. To show Newton-stability we need to show that under conditions (I)-(II) the projected Jacobian matrix Jp(u) = L > u\u2207g(u)Lu in (4) is full rank d\u2212 1.\nThe Jacobian matrix\u2207g(u) is\n\u2207g(u) = 2W(I, I,u)\u2212 3uW(I,u,u)>\n\u2212W(u,u,u)Id = 2W(I, I,u)\u2212 3\u03bbuu> \u2212 \u03bbId. (36)\nSince L>uu = 0, the second term in (36) does not contribute to Jp(u). For the first term in (36), by (12) and (10),\nW(I, I,u) =M(K,K,Ku) = C(WK,WK,WKu).\nSince v\u2217 = Ku/\u03bb is a column of W \u2020, WKu = \u03bbei for some i \u2208 [d]. Thus,\nW(I, I,u) = \u03bbC(WK,WK, ei) = \u03bbK>W>C(I, I, ei)WK.\nFor the third term in (36), by the definition of K in (11),\nId = K >MK = K>W>CWK.\nPutting the last two equalities in (36) and applying the projection Lu we obtain\nJp(u) = L > u\u2207g(u)Lu\n= \u03bbL>uK >W>(2C(I, I, ei)\u2212 C)WKLu.\nSince \u03bb \u2265 1 and W and K are full rank, condition (II) implies that Jp(u) is full rank as well. Thus, (u, \u03bb) is a Newton-stable eigenpair ofW ."}, {"heading": "E. Proof of Lemma 3", "text": "Lemma 3 follows from the following lemma which establishes the stability of Newton-stable eigenpairs of a tensor W to small perturbations W\u0303 =W + \u2206W . Lemma 7. Let (u, \u03bb) be a Newton-stable eigenpair ofW with \u03bb \u2265 1. There are c1, c2, \u03b50 > 0 such that for all sufficiently small \u03b5 > 0 the following holds. For any W\u0303 such that \u2016W\u0303 \u2212W\u2016F \u2264 \u03b5 there exists a unique eigenpair (u\u0303, \u03bb\u0303) of W\u0303 such that\n\u2016u\u2212 u\u0303\u2016 \u2264 c1\u03b5 and |\u03bb\u0303\u2212 \u03bb| \u2264 c2\u03b5.\nIn addition, (u\u0303, \u03bb\u0303) is Newton-stable and any other eigenvector v\u0303 of W\u0303 satisfies \u2016v\u0303 \u2212 u\u2016 \u2265 \u03b50.\nProof of Lemma 7. For a tensor T \u2208 Rd\u00d7d\u00d7d let t \u2208 Rs be the vector of s = d3 entries {Tijk}. Define the function Q : Rd+s \u2192 Rd by\nQ(v, t) = T (I,v,v)\u2212 T (v,v,v) \u00b7 v.\nNote that for any t \u2208 Rs and (v, \u03b2) \u2208 Rd \u00d7 R with v 6= 0 and \u03b2 6= 0, we have thatQ(v, t) = 0 if and only if (v, \u03b2) is an eigenpair of t with eigenvalue \u03b2 = T (v,v,v).4 Denote the gradients ofQ with respect to v and t by\nA(v, t) = \u2207vQ(v, t) \u2208 Rd\u00d7d, B(v, t) = \u2207tQ(v, t) \u2208 Rd\u00d7s.\nLet w \u2208 Rs be the vectorization of W and let (u, \u03bb) \u2208 Sd\u22121 \u00d7 R+ be a Newton-stable eigenpair of w with \u03bb \u2265 1. Since u is Newton-stable and \u03bb > 0, A(u,w) is invertible. In addition, the following (d+ s)\u00d7 (d+ s) matrix is invertible,\nD(u,w) =\n( A(u,w) B(u,w)\n0 Is\n) .\n4This does not precisely hold when \u03b2 = 0 since Q(v, t) = 0 does not imply \u2016v\u2016 = 1 in this case, but only that v is proportional to an eigenvector.\nLet \u03b3D = 1/\u2016D(u,w)\u22121\u2016 > 0 be the smallest singular value of D(u,w) and let LD < \u221e be the Lipschitz constant of \u2207Q(v, t) = [A(v, t), B(v, t)] \u2208 Rd\u00d7(d+s) in a small neighborhood of (u,w), namely, \u2200(v, t), (v\u0303, t\u0303) in the neighborhood,\n\u2016\u2207Q(v, t)\u2212\u2207Q(v\u0303, t\u0303)\u2016 \u2264 LD\u2016(v, t)\u2212 (v\u0303, t\u0303)\u2016.\nLet B\u03b5(w) \u2282 Rs be the ball of radius \u03b5 centered at w. Then by the implicit function theorem (Hubbard & Hubbard, 2015), for any \u03b5 \u2264 \u03b51 := \u03b32D/(2LD), there exists a unique continuously differentiable mapping u\u0303 : B\u03b5(w)\u2192 B2\u03b5/\u03b3D (u) such that Q(u\u0303(w\u0303), w\u0303) = 0 for all w\u0303 \u2208 B\u03b5(w). In other words, for any w\u0303 such that \u2016w\u0303 \u2212w\u2016 \u2264 \u03b5, there exist a unique vector u\u0303 in all B2\u03b5/\u03b3D (u) that is an eigenpair of w\u0303. Equivalently, for W\u0303 such that \u2016W\u0303 \u2212W\u2016F \u2264 \u03b5, there exists a unique eigenvector u\u0303 of W\u0303 such that\n\u2016u\u0303\u2212 u\u2016 \u2264 2\u03b5/\u03b3D := c1\u03b5. (37)\nThe bound on |\u03bb\u0303 \u2212 \u03bb| readily follows from (37). Indeed, let q : Rd+s \u2192 R be q(v, t) = T (v,v,v) and let L\u03bb be the Lipschitz constant of q in the neighborhood of (u,w). Then,\n|\u03bb\u0303\u2212 \u03bb| = |q(u\u0303, w\u0303)\u2212 q(u,w)| \u2264 L\u03bb \u221a \u2016u\u0303\u2212 u\u20162 + \u2016w\u0303 \u2212w\u20162\n\u2264 L\u03bb \u221a 2\n\u03b3D + 1 \u00b7 \u03b5 := c2\u03b5.\nAs for the Newton-stability of u\u0303, let r : Rd+s \u2192 R+ be r(v, t) = 1/\u2016A(v, t)\u22121\u2016, the minimal singular value of A(v, t). Since (u, \u03bb) is a Newton-stable eigenpair of w, \u2203\u03b3A > 0 such that r(u,w) \u2265 \u03b3A. Let L\u03b3 be the Lipschitz constant of r(v, t) in the neighborhood (Golub & Van Loan, 2012). Then, for \u03b5 \u2264 \u03b52 := \u03b3/(2L\u03b3), we have r(u\u0303, w\u0303) \u2265 \u03b3A/2 > 0, so (u\u0303, \u03bb\u0303) is a Newton-stable eigenpair of w\u0303.\nFinally, we show that any other eigenvector v\u0303 of W\u0303 is apart from u. Since u\u0303 is Newton-stable, there exists \u03b50 > 0 such that \u2016v\u0303 \u2212 u\u0303\u2016 \u2265 2\u03b50 for any other eigenvector v\u0303. Hence, for \u03b5 \u2264 \u03b50,\n\u2016v\u0303 \u2212 u\u2016 \u2265 \u2223\u2223\u2016v\u0303 \u2212 u\u0303\u2016 \u2212 \u2016u\u0303\u2212 u\u2016\u2223\u2223 \u2265 \u2016v\u0303 \u2212 u\u0303\u2016 \u2212 \u03b5 \u2265 \u03b50.\nTaking \u03b5 \u2264 min{\u03b50, \u03b51, \u03b52} and c1, c2, \u03b50 as above concludes the proof of the lemma.\nLastly, for completeness, we show that \u03b3D \u2265 \u03b3A\u221a \u03b32A+d .\n\u03b3\u22121D = \u2016D(u,w) \u22121\u2016 \u2264 \u221a \u2016A(u,w)\u22121\u20162(1 + \u2016B(u,w)\u20162) + \u2016Is\u20162\n\u2264 \u221a 1 + 1 + \u2016B(v,w)\u20162\n\u03b32A . (38)\nTo bound \u2016B(u,w)\u2016, note thatQ(u,w) is linear in w and its i-th entry is given by\n[Q(u,w)]i = \u2211 k,l wiklukul \u2212 ( \u2211 j,k,l wjklujukul)ui.\nThus, the d\u00d7m matrix B(u,w) has entries\n[B(u,w)]i,(jkl) = [\u2207wQ(u,w)]i,(jkl) = (\u03b4ij\u2212uiuj)ukul,\nwhich is independent of w. Recalling that \u2016u\u2016 = 1,\n\u2016B(u)\u20162 \u2264 \u2016B(u)\u20162F = d\u2211\ni,j,k,l=1\n(\u03b4ij \u2212 uiuj)2u2ku2l\n= d\u2211 i,j=1 (\u03b42ij \u2212 2\u03b4ijuiuj + u2iu2j ) = d\u2212 1.\nPutting this bound in (38), we obtain \u03b3D \u2265 \u03b3A\u221a \u03b32A+d ."}, {"heading": "F. Proof of Lemma 4", "text": "Let v\u2217 \u2208 W \u2020. Then \u2203i \u2208 [d] such that v\u2217>W>h = hi \u2208 {0, 1}. Hence, by (28), the c.d.f. Fv\u2217 of v\u2217>x corresponds to the two component GMM\n(1\u2212 pi) \u00b7 N (0, \u03c32\u2016v\u2217\u20162) + pi \u00b7 N (1, \u03c32\u2016v\u2217\u20162).\nBy Lemma 1 we have pi = 1/\u03bb(v\u2217)2. Thus, Fv\u2217 = Gv\u2217 .\nFor the other direction, let v \u2208 V\u03c3 \\W \u2020. Since W is full rank, the d-dimensional vector u> = v>W> /\u2208 {e>i }di=1. Moreover, by Eq. (23) of Lemma 3,\ninf v\u2208V\u03c3\\W \u2020 min v\u2217\u2208W \u2020\n\u2016v \u2212 v\u2217\u2016 \u2265 \u03b41 > 0.\nHence, there exists \u03b50 > 0 such that\nmin i\u2208[d] \u2016u\u2212 ei\u2016 \u2265 \u03b50.\nSo by the expected rigidity condition (27), there exists \u03b70 > 0 such that r(u) \u2265 \u03b70. It follows that Fv has a component with mean that is bounded away from both 0 and 1 and thus Fv 6= Gv . In particular, there exists \u03b71 > 0 such that\nsup t\u2208R |Fv(t)\u2212Gv(t)| \u2265 \u03b71."}, {"heading": "G. Proof of Lemma 5", "text": "Recall that our sample of size 2n was split into two separate parts each of size n. The first n samples were used to estimate the tensor eigenvectors, and the last n samples to estimate the empirical cdf\u2019s of their projections onto the eigenvectors.\nFor any v\u0302 that is close to a vector v, we bound \u2206n(v\u0302) = \u2016F\u0302v\u0302 \u2212Gv\u0302\u2016\u221e by the triangle inequality,\n\u2016F\u0302v\u0302 \u2212Gv\u0302\u2016\u221e \u2264 \u2016F\u0302v\u0302 \u2212 Fv\u0302\u2016\u221e + \u2016Fv\u0302 \u2212 Fv\u2016\u221e (39) + \u2016Fv \u2212Gv\u2016\u221e + \u2016Gv \u2212Gv\u0302\u2016\u221e.\nWe now consider each of the four terms separately, starting with the first one. Since \u03c3 > 0, the cdf Fv\u0302 : R \u2192 [0, 1] is continuous and the distribution of \u2016F\u0302v\u0302 \u2212 Fv\u0302\u2016\u221e is independent of v\u0302. Then, by the Dvoretzky-Kiefer-Wolfowitz inequality, \u2016F\u0302v\u0302 \u2212 Fv\u0302\u2016\u221e is w.h.p. of order O(1/ \u221a n) for any v\u0302, and in particular tends to zero as n\u2192 0.\nAs for the second term, write v\u0302 = v + \u03b7. Then,\nv\u0302>x = v>x+ \u03b7>x.\nRecall that x = W>h + \u03c3\u03be. Hence, |\u03b7>x| \u2264 \u2016W\u20162 \u221a d\u2016\u03b7\u2016 + \u03c3|\u03b7>\u03be|. The term \u03b7>\u03be is simply a zero mean Gaussian random variable with standard deviation \u03c3\u2016\u03b7\u2016. So, there exists Kn > \u221a d\u2016W\u20162 + \u03c3n1/3 such that with probability tending to one as n\u2192\u221e, for all n samples xj \u2208 X , |\u03b7>xj | \u2264 Kn\u2016\u03b7\u2016. Thus, |v\u0302>x \u2212 v>x| can be bounded by Kn\u2016v\u0302 \u2212 v\u2016. This, in turn, implies that\n\u2016Fv\u0302 \u2212 Fv\u2016\u221e \u2264 LKn\u2016v\u0302 \u2212 v\u2016,\nwhere L = maxt F \u2032v(t), which is finite for any \u03c3 > 0. Now, suppose the sequence v\u0302(n) converges to some v at rate OP (1/ \u221a n). Since Kn grows much more slowly with n, this term tends to zero.\nLet us next consider the fourth term, and leave the third term to the end. Here note that Gv is continuous in its parameter v. So if the sequence v\u0302(n) converges to some v, then this term tends to zero.\nFinally, consider the third term. If the limiting vector v belongs to the correct set, namely v\u2217 \u2208W \u2020, then Fv = Gv , and thus overall \u2016F\u0302v\u0302 \u2212Gv\u0302\u2016\u221e tends to zero as required.\nIn contrast, if v\u0302 converges to a vector v /\u2208W \u2020, then instead of Eq. (39) we invoke the following inequality:\n\u2016F\u0302v\u0302 \u2212Gv\u0302\u2016\u221e \u2265 \u2016Fv \u2212Gv\u2016\u221e \u2212 \u2016Fv \u2212 Fv\u0302\u2016\u221e \u2212\u2016Fv\u0302 \u2212 F\u0302v\u0302\u2016\u221e \u2212 \u2016Gv\u0302 \u2212Gv\u2016\u221e.\nHere \u2016Fv \u2212Gv\u2016\u221e is strictly larger than zero whereas the three other remaining terms tend to zero as n \u2192 \u221e as above."}, {"heading": "H. Proof of Lemma 6", "text": "Multiplying (35) from the right by the full rank matrix Y \u22121 we obtain the equations\n[W(Y, Y, I)]iij = [Y >]ij , \u2200i, j \u2208 [d].\nNote that for all i \u2208 [d],\n[W(Y, Y, I)]iij = [W(yi,yi, I)]j .\nSinceW is symmetric, we thus have\nW(I,yi,yi) = yi, \u2200i \u2208 [d].\nWriting yi = ui/\u03bbi we obtain the eigenpair equation\nW(I,ui,ui) = \u03bbiui, \u2200i \u2208 [d].\nThe other direction readily follows from the definition of eigenpairs."}, {"heading": "I. Simulation runtime", "text": "Figure 5 shows the simulation runtime of the spectral approach and that of SHL vs. the number of samples n. The setup is similar to the one described in section 5. The runtime of SHL increases linearly with n, as expected. For our spectral method, for lower values of n the dominant factor is the computation of tensor eigenvectors, which does not depend on n. For large n, the dominant factor is the computation of the correlation tensor, linear in n."}, {"heading": "J. Matrix and tensor denoising", "text": "In Algorithm 1, we modify the diagonal elements of M,M by (17). This modification is suited for additive Gaussian noise, but is not applicable for the case where X = binomial(2,WTH). Instead, we implemented a method derived in (Jain & Oh, 2014) for a similar setup.\nFirst, we treat the diagonal elements of M\u03c3 as missing data, and complete them with the following iterative steps. (i) compute the first d eigenpairs {vi, \u03bbi} of R(k); and (ii) update the diagonal elements by R(k+1)jj = ( \u2211 i \u03bbiviv > i )jj .\nNext, instead of computingM\u03c3 via (17) and thenW\u03c3 via (19), we compute W\u03c3 directly by solving the following system of linear equations. Let K\u2020 be the pseudo-inverse\nmatrix of K, and P\u2126(T ) denote a masking operation over the tensor T such that,\nP\u2126(T ) = { Tijk i 6= j 6= k 0 o.w\nWe estimateW by the following minimization problem,\nW\u0302 = argmin W\n\u2016P\u2126 ( W(K\u2020,K\u2020,K\u2020) ) \u2212 P\u2126(M)\u20162F\nThis method depends only on the off-diagonal elements of M and M and hence is applicable whenever E[X|H] = WTH and the noise has bounded variance."}, {"heading": "K. Adding a weighted least square step to the spectral method", "text": "In section 5, we compare the results of algorithm 1 with and without an additional single weighted least square step. Given an estimate W\u0302 , for each observed instance xj we calculate the conditional likelihood L(xj |h) for the 2d possible binary vectors h \u2208 {0, 1}d,\nL(xj |h) = 1\u221a\n2\u03c0\u03c32 exp\n( \u2212 \u2016xj \u2212 W\u0302Th\u20162 / (2\u03c32) ) For each instance xj , we keep the top K = 6 vectors h1j , . . . ,hKj with the highest likelihood. Let \u03a0 \u2208 [0, 1]K\u00d7n be a weight matrix such that \u03a0kj is proportional to L(xj |hkj), and \u2211 k \u03a0kj = 1 for all j. The new estimate W\u0302wls is the minimizer of the weighted least square problem,\nW\u0302wls = argmin W n\u2211 j=1 K\u2211 k=1 \u03a0kj\u2016xj \u2212WThkj\u20162."}, {"heading": "L. Alternating least squares for W and H", "text": "In section 5, we compare the results of the spectral approach to the following ALS iterations, with a random starting point.\nW (k) = argmin W\u2208Rd\u00d7m \u2016X \u2212WTH(k\u22121)\u20162F\nH\u0302(k) = argmin H\u2208Rd\u00d7n \u2016X \u2212 (W (k))TH\u20162F\nH(k) = argmin H\u2208{0,1}d\u00d7n \u2016H \u2212 H\u0302(k)\u20162F ,"}, {"heading": "M. Genetic admixture simulations", "text": "The simulated admixture data was generated via the following steps:\n1. We used SCRM (Staab et al., 2015) to simulate a split between d = 3 ancestral populations, with separation\ntime of 4000 generations. The simulator generated 40 chromosomes of length 250 \u00b7 106 for each of the three populations. The simulation parameters were determined as N0 = 104 effective population size, 10\u22128 mutation rate (per base pair per generation), and 10\u22128 recombination rate (per base pair per generation).\n2. We sampled the proportion matrix W from a Dirichlet distribution with parameter \u03b1.\n3. Two chromosomes of length 250 \u00b7 106 were created for each of the m = 50 admixed individuals with the following steps: (i) An ancestral population was sampled according to W, say, population hA. (ii) One of the 40 chromosomes was sampled from hA, say hA(k) (iii) A block length l was sampled from an exponential distribution with rate 20 per Morgan corresponding admixture event happening 20 generations ago (in our case, 1 Morgan was 108 base pairs). (iv) A block of length l was copied from chromosome hA(k) to the corresponding locations in the new admixed chromosome. We repeated steps (i)-(iv) until completion of the chromosome.\nN. Analysis of DNA methylation data The dataset is part of the supplementary material of (Houseman et al., 2012). The observed matrix X \u2208\n[0, 1]m\u00d7n consists of m = 12 blood samples, each with the DNA methylation measurements in n = 500 sites (called CpGs).\nThe statistical model for X is similar to that of Eq. (1). We assume that each blood cell is a mixture of d = 4 cell types, with unknown proportions. The latent variables h correspond to the presence or absence of methylation in each site for the 4 cell types. Given the DNA methylation array, the task is to estimate the proportion matrix W .\nThe upper and lower panels of Figure 6 correspond to the real and estimated mixture matrix. For comparison, we performed the steps described in detail for this dataset in Slawski et al. (2013, Section 4). For both methods we measured the l1 distance between the real and estimated mixture matrices,\n1\nmn \u2211 ij |Wij \u2212 W\u0302ij |\nThe l1 distance were equal to 0.003 for SHL and 0.00056 for the spectral approach."}], "year": 2018, "references": [{"title": "Fast modelbased estimation of ancestry in unrelated individuals", "authors": ["D. Alexander", "J. Novembre", "K. Lange"], "venue": "Genome research,", "year": 2009}, {"title": "A spectral algorithm for latent dirichlet allocation", "authors": ["A. Anandkumar", "D. Foster", "D. Hsu", "S. Kakade", "Y. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "A method of moments for mixture models and hidden markov models", "authors": ["A. Anandkumar", "D. Hsu", "S. Kakade"], "venue": "In COLT,", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "authors": ["A. Anandkumar", "R. Ge", "D. Hsu", "S. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Spectral methods for correlated topic models", "authors": ["F. Arabshahi", "A. Anandkumar"], "venue": "In Artificial Intelligence and Statistics,", "year": 2017}, {"title": "Computing a nonnegative matrix factorization\u2013provably", "authors": ["S. Arora", "R. Ge", "R. Kannan", "A. Moitra"], "venue": "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,", "year": 2012}, {"title": "Overlapping clustering: A review", "authors": ["S. Baadel", "F. Thabtah", "J. Lu"], "venue": "In SAI Computing Conference (SAI),", "year": 2016}, {"title": "Model-based overlapping clustering", "authors": ["A. Banerjee", "C. Krumpelman", "J. Ghosh", "S. Basu", "R. Mooney"], "venue": "In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining,", "year": 2005}, {"title": "Multifunctional proteins revealed by overlapping clustering in protein interaction", "authors": ["E. Becker", "B. Robisson", "C. Chapple", "A. Gu\u00e9noche", "C. Brun"], "venue": "network. Bioinformatics,", "year": 2011}, {"title": "Convergence of probability measures", "authors": ["P. Billingsley"], "year": 2013}, {"title": "A nonparametric estimator of population structure unifying admixture models and principal components analysis", "authors": ["I. Cabreros", "J. Storey"], "venue": "bioRxiv, pp", "year": 2017}, {"title": "The number of eigenvalues of a tensor", "authors": ["D. Cartwright", "B. Sturmfels"], "venue": "Linear algebra and its applications,", "year": 2013}, {"title": "Computing tensor eigenvalues via homotopy methods", "authors": ["L. Chen", "L. Han", "L. Zhou"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2016}, {"title": "Consistent estimation of lowdimensional latent structure in high-dimensional data", "authors": ["X. Chen", "J. Storey"], "venue": "arXiv preprint arXiv:1510.03497,", "year": 2015}, {"title": "Improved learning of GaussianBernoulli restricted Boltzmann machines", "authors": ["Cho", "Ilin", "Raiko"], "venue": "Artificial Neural Networks and Machine Learning\u2013ICANN", "year": 2011}, {"title": "All real eigenvalues of symmetric tensors", "authors": ["C. Cui", "Y. Dai", "J. Nie"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2014}, {"title": "When does non-negative matrix factorization give a correct decomposition into parts? In Advances in neural information processing", "authors": ["D. Donoho", "V. Stodden"], "year": 2004}, {"title": "Learning linear transformations", "authors": ["A. Frieze", "M. Jerrum", "R. Kannan"], "venue": "In Foundations of Computer Science,", "year": 1996}, {"title": "An efficient algorithm for the Kolmogorov-Smirnov and Lilliefors tests", "authors": ["T. Gonzalez", "S. Sahni", "W. Franta"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "year": 1977}, {"title": "Population genetics models of local ancestry", "authors": ["S. Gravel"], "venue": "Genetics, 191(2):607\u2013619,", "year": 2012}, {"title": "A modified Newton iteration for finding nonnegative Z-eigenpairs of a nonnegative tensor", "authors": ["C. Guo", "W. Lin", "C. Liu"], "venue": "arXiv preprint arXiv:1705.07487,", "year": 2017}, {"title": "Most tensor problems are np-hard", "authors": ["C. Hillar", "L. Lim"], "venue": "Journal of the ACM (JACM),", "year": 2013}, {"title": "A practical guide to training restricted", "authors": ["G. Hinton"], "venue": "Boltzmann machines. Momentum,", "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks. science", "authors": ["G. Hinton", "R. Salakhutdinov"], "year": 2006}, {"title": "Dna methylation arrays as surrogate measures of cell mixture distribution", "authors": ["E. Houseman", "P. Accomando", "D. Koestler", "B. Christensen", "C. Marsit", "H. Nelson", "J. Wiencke", "K. Kelsey"], "venue": "BMC bioinformatics,", "year": 2012}, {"title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions", "authors": ["D. Hsu", "S. Kakade"], "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,", "year": 2013}, {"title": "Vector calculus, linear algebra, and differential forms: a unified approach", "authors": ["J. Hubbard", "B. Hubbard"], "venue": "Matrix Editions,", "year": 2015}, {"title": "Independent component analysis, volume 46", "authors": ["A. Hyv\u00e4rinen", "J. Karhunen", "E. Oja"], "year": 2004}, {"title": "Newton correction methods for computing real eigenpairs of symmetric tensors", "authors": ["A. Jaffe", "R. Weiss", "B. Nadler"], "venue": "arXiv preprint arXiv:1706.02132,", "year": 2017}, {"title": "Learning mixtures of discrete product distributions using spectral decompositions", "authors": ["P. Jain", "S. Oh"], "venue": "In Conference on Learning Theory, pp", "year": 2014}, {"title": "Shifted power method for computing tensor eigenpairs", "authors": ["T. Kolda", "J. Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2011}, {"title": "An adaptive shifted power method for computing generalized tensor eigenpairs", "authors": ["T. Kolda", "J. Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2014}, {"title": "Non-parametric detection of the number of signals: Hypothesis testing and random matrix theory", "authors": ["S. Kritchman", "B. Nadler"], "venue": "IEEE Transactions on Signal Processing,", "year": 2009}, {"title": "Testing statistical hypotheses", "authors": ["E. Lehmann", "J. Romano"], "venue": "Springer Science & Business Media,", "year": 2006}, {"title": "Worldwide human relationships inferred from genome-wide patterns of variation", "authors": ["J. Li", "D. Absher", "H. Tang", "A. Southwick", "A. Casto", "S. Ramachandran", "H. Cann", "G. Barsh", "M. Feldman", "L Cavalli-Sforza"], "year": 2008}, {"title": "Singular values and eigenvalues of tensors: a variational approach", "authors": ["L. Lim"], "venue": "In Computational Advances in MultiSensor Adaptive Processing,", "year": 2005}, {"title": "Gaussian-binary restricted Boltzmann machines for modeling natural image statistics", "authors": ["J. Melchior", "N. Wang", "L. Wiskott"], "venue": "PloS one,", "year": 2017}, {"title": "Mdl4bmf: Minimum description length for boolean matrix factorization", "authors": ["P. Miettinen", "J. Vreeken"], "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "year": 2014}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "authors": ["A. Moitra", "G. Valiant"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2010}, {"title": "Learning nonsingular phylogenies and hidden markov models", "authors": ["E. Mossel", "S. Roch"], "venue": "In Proceedings of the thirtyseventh annual ACM symposium on Theory of computing,", "year": 2005}, {"title": "Sensitive detection of chromosomal segments of distinct ancestry in admixed populations", "authors": ["A. Price", "A. Tandon", "N. Patterson", "K. Barnes", "N. Rafaels", "I. Ruczinski", "T. Beaty", "R. Mathias", "D. Reich", "S. Myers"], "venue": "PLoS genetics,", "year": 2009}, {"title": "Inference of population structure using multilocus genotype", "authors": ["J. Pritchard", "M. Stephens", "P. Donnelly"], "venue": "data. Genetics,", "year": 2000}, {"title": "Eigenvalues of a real supersymmetric tensor", "authors": ["L. Qi"], "venue": "Journal of Symbolic Computation,", "year": 2005}, {"title": "Monotonic convergence of fixedpoint algorithms for ica", "authors": ["P. Regalia", "E. Kofidis"], "venue": "IEEE Transactions on Neural Networks,", "year": 2003}, {"title": "Decomposing gene expression into cellular processes", "authors": ["E. Segal", "A. Battle", "D. Koller"], "venue": "In Proceedings of the Pacific Symposium on Biocomputing,", "year": 2002}, {"title": "Super-exponential methods for blind deconvolution", "authors": ["O. Shalvi", "E. Weinstein"], "venue": "IEEE Transactions on Information Theory,", "year": 1993}, {"title": "Matrix factorization with binary components", "authors": ["M. Slawski", "M. Hein", "P. Lutsik"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "scrm: efficiently simulating long sequences using the approximated coalescent with recombination", "authors": ["Staab", "Zhu", "Metzler", "Lunter"], "year": 2015}, {"title": "Modeling human motion using binary latent variables", "authors": ["G. Taylor", "G. Hinton", "S. Roweis"], "venue": "In Advances in neural information processing systems,", "year": 2007}, {"title": "Analytical method for blind binary signal separation", "authors": ["Van der Veen", "A.-J"], "venue": "IEEE Transactions on Signal Processing,", "year": 1997}, {"title": "An analysis of gaussian-binary restricted Boltzmann machines for natural images", "authors": ["N. Wang", "J. Melchior", "L. Wiskott"], "venue": "In ESANN,", "year": 2012}, {"title": "The time and place of european admixture in ashkenazi jewish history", "authors": ["J. Xue", "T. Lencz", "A. Darvasi", "I. Peer", "S. Carmi"], "venue": "PLoS genetics,", "year": 2017}, {"title": "Let B\u03b5(w) \u2282 R be the ball of radius \u03b5 centered at w. Then by the implicit function theorem (Hubbard", "authors": ["\u2016\u2207Q(v", "t)\u2212\u2207Q(\u1e7d", "t\u0303)\u2016 \u2264 LD\u2016(v", "t)\u2212 (\u1e7d"], "year": 2015}], "id": "SP:a27ec8782e7dfc94965ff4e6a50a854f822b102b", "authors": [{"name": "Ariel Jaffe", "affiliations": []}, {"name": "Roi Weiss", "affiliations": []}, {"name": "Shai Carmi", "affiliations": []}, {"name": "Yuval Kluger", "affiliations": []}, {"name": "Boaz Nadler", "affiliations": []}], "abstractText": "Latent variable models with hidden binary units appear in various applications. Learning such models, in particular in the presence of noise, is a challenging computational problem. In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data. We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate. Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive. We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.", "title": "Learning Binary Latent Variable Models: A Tensor Eigenpair Approach"}