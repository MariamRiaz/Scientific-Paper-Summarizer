4	14	DBSCAN aims at discovering clusters which turn out to be the high-density regions of the dataset.
5	40	It defines a point as a core-point if there are at least minPts sample points in its εradius neighborhood.
6	45	The points within the ε-radius neighborhood of a core-point are said to be directly reachable from that core-point.
7	7	Then, a point q is reachable from a core-point p if there exists a path from q to p where each point is directly reachable from the next point.
13	15	Using recent developments in topological data analysis along with some tools we develop in this paper, we show that it is now possible to analyze the original procedure.
14	77	The clusters DBSCAN aims at discovering can be viewed as approximations of the connected components of the level sets {x : f(x) ≥ λ} where f is the density and λ is some density level.
16	18	Here, the density level λ is known to the algorithm while the density remains unknown.
17	70	Density level set estimation has been studied extensively.
18	12	e.g., (Carmichael et al., 1968; Hartigan, 1975; Polonik, 1995; Cuevas & Fraiman, 1997; Walther, 1997; Tsybakov et al., 1997; Baıllo et al., 2001; Cadre, 2006; Willett & Nowak, 2007; Biau et al., 2008; Rigollet & Vert, 2009; Maier et al., 2009; Singh et al., 2009; Rinaldo & Wasserman, 2010; Steinwart, 2011; Rinaldo et al., 2012; Steinwart et al., 2015; Chen et al., 2016; Jiang, 2017).
21	6	Also, unlike much of the existing work, we show that DBSCAN can also recover the connected components of the level sets separately and bijectively.
23	9	The latter has been heavily used for cluster-tree estimation (Chaudhuri & Dasgupta, 2010; Stuetzle & Nugent, 2010; Kpotufe & von Luxburg, 2011; Chaudhuri et al., 2014; Jiang & Kpotufe, 2017) and in this paper we adapt some of these ideas for ε-neighborhood graphs.
24	8	Cluster-tree estimation aims at discovering the hierarchical tree structure of the connected-components as the levels vary.
25	29	Balakrishnan et al. (2013) extends results by Chaudhuri & Dasgupta (2010) to the setting where the data lies on a lower dimensional manifold and provide consistency results depending on the lower dimension and independent of the ambient dimension.
29	10	Dasgupta & Kpotufe (2014) gives us optimal highprobability finite-sample k-NN density estimation bounds which hold uniformly; this is key to obtaining optimal level-set estimation rates under the Hausdorff error.
31	21	These metrics are considerably weaker than the Hausdorff metric; the latter is a uniform guarantee.
38	21	We extend the k-NN density estimation results of Dasgupta & Kpotufe (2014) to the manifold case, as the bulk our analysis is about the more general case that the data lies on a manifold.
39	37	Density-based procedures perform poorly in high-dimensions since the number of samples required increases exponentially in the dimension– the so called curse of dimensionality.
40	14	Thus, the consequences of handling the manifold case are of practical significance.
41	20	Since the estimation rates we obtain depend only on the intrinsic dimension, it explains why DBSCAN can do well in high dimensions if the data has low intrinsic dimension (i.e. the manifold hypothesis).
48	8	To solve for the unknown dimension, we use an estimator from Farahmand et al. (2007), which we show to have considerably better finite-sample behavior than previously thought.
55	16	This is significant because these graphs can be shown to estimate density level sets.
56	11	• Section 4 introduces the manifold setting and provides supporting results including k-nearest neighbor density estimation bounds (Lemma 5 and Lemma 6) that are useful later on.
58	24	• Section 6 shows how one can apply DBSCAN a second time to remove false clusters from the first application, thus completing a bijection between the estimates and the true clusters (Theorem 2).
61	6	• Section 8 gives the result when the data lives in RD without the manifold assumption.
76	14	Such an interpretation of DBSCAN has been given in previous works such as Campello et al. (2015).
82	9	Next we show that K = {x ∈ C : rk(x) ≤ ε}.
83	16	Suppose there exists core-point x ∈ C but x /∈ K and let y ∈ K. By Lemma 1, there exists core-point c ∈ C such that all points in C are directly reachable from c. Then there exists a path of core-points from x to c with pairwise edges of length at most ε.
84	15	Thus there exists such a path of core-points from x to y, which means that x, y are in the same CC of G(minPts, ε), contradicting the assumption that x /∈ K and y ∈ K. Thus, in fact K = {x ∈ C : rk(x) ≤ ε}.
88	26	The difference is that they use a kernel density estimator instead of a k-NN density estimator and study the convergence properties under different settings.
89	47	We make the following regularity assumptions which are standard among works on manifold learning e.g. (Baraniuk & Wakin, 2009; Genovese et al., 2012; Balakrishnan et al., 2013).
