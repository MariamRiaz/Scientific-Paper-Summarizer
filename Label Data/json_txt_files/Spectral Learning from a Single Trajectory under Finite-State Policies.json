{"sections": [{"heading": "1. Introduction", "text": "Spectral methods of moments are a powerful tool for designing provably correct learning algorithms for latent variable models. Successful applications of this approach include polynomial-time algorithms for learning topic models (Anandkumar et al., 2012; 2014), hidden Markov models (Hsu et al., 2012; Siddiqi et al., 2010; Anandkumar et al., 2014), mixtures of Gaussians (Anandkumar et al., 2014; Hsu & Kakade, 2013), predictive state representations (Boots et al., 2011; Hamilton et al., 2014; Bacon et al., 2015; Langer et al., 2016), weighted automata (Bailly, 2011; Balle et al., 2011; Balle & Mohri, 2012; Balle et al., 2014a;b; Glaude & Pietquin, 2016), and weighted contextfree grammars (Bailly et al., 2010; Cohen et al., 2013; 2014). All these methods can be split into two classes depending on which spectral decomposition they rely on. The first class includes algorithms based on an Singular Value Decomposition (SVD) decomposition of a matrix contain-\n1Amazon Research, Cambridge, UK (work done at Lancaster University) 2Inria Lille - Nord Europe, Villeneuve d\u2019Ascq, France. Correspondence to: Borja Balle <pigem@amazon.co.uk>, Odalric-Ambrym Maillard <odalric.maillard@inria.fr>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ning (estimated) moments of the target distribution (e.g. Hsu et al. (2012); Boots et al. (2011); Balle et al. (2014a)). The other class includes algorithms relying on symmetric tensor decomposition methods (e.g. Anandkumar et al. (2014); Hsu & Kakade (2013)). The advantage of tensor methods is that their output is always a proper probabilistic model. On the other hand, SVD methods, which do not always output a probabilistic model, provide learning algorithms for models which are provably non-learnable using tensor methods. A notable example is the class of stochastic weighted automata that do not admit a probabilistic parametrization (Jaeger, 2000; Denis & Esposito, 2008).\nNatural language processing (NLP) and reinforcement learning (RL) are the two main application domains of spectral learning. For example, SVD methods for learning weighted context-free grammars have proved very successful in language-related problems (Cohen et al., 2013; Luque et al., 2012). In the context of RL, efficient SVD methods for learning predictive state representations were proposed in (Boots et al., 2011; Hamilton et al., 2014). A recent application of tensor methods to RL is given in (Azizzadenesheli et al., 2016), where the authors use a spectral algorithm to obtain a PAC-RL learning result for POMDP under memory-less policies. All these results have in common that they provide learning algorithms for models over sequences. However, there is a fundamental difference between the nature of data in NLP and RL. With the exception of a few problems, most of NLP \u201csafely\u201d relies on the assumption that i.i.d. data from the target distribution is available. In RL, however, the most general scenario assumes that the learner can only collect a single continuous trajectory of data while all existing analysis of the SVD method for sequential models1 rely on the i.i.d. assumption (Hsu et al., 2012; Balle & Mohri, 2015; Glaude & Pietquin, 2016). Regarding tensor methods, (Azizzadenesheli et al., 2016) gave the first analysis under dependent data satisfying certain mixing conditions.\nThe purpose of this paper is to provide the first rigorous analyses of spectral SVD methods for learning sequential models from non-i.i.d. data. We provide efficient algorithms with provable guarantees for learning several sequential models from a single trajectory. Specifically, we\n1See (Thon & Jaeger, 2015) for a survey of sequential models learnable with SVD methods.\nconsider three models: probabilistic automata, stochastic weighted automata, and PSRs under control from a finitestate policy. The first two results extend existing results in the literature for i.i.d. data (Hsu et al., 2012; Balle et al., 2014a). The last result is an analog of the environment learning result for POMDP \u2013 not the whole RL result \u2013 of (Azizzadenesheli et al., 2016), with the difference that our analysis provides guarantees under a much larger set of policies (finite-state, as opposed to memory-less). This result can also be interpreted as an extension of the batchbased PSR learning algorithm from (Boots et al., 2011) to the non-i.i.d. case, although they do not provide finitesample guarantees. Our analysis is especially relevant since the single trajectory spectral algorithm we analyze has been used previously without an explicit instantiation or analysis. For example, (Kulesza et al., 2015; Shaban et al., 2015) present experiments with datasets containing single or few long trajectories which are broken into short subsequences and given as input to an spectral algorithm designed for i.i.d. data. A more detailed list of our contributions is as follows:\n(1) A single-trajectory spectral-learning algorithm for probabilistic automata whose sample complexity depends on the mixing properties of the target automaton (Section 3).\n(2) An extension of this result showing that the same algorithm also learns stochastic weighted automata (Section 4). In this case the analysis is more involved, and requires a novel notion of mixing for stochastic weighted automata and tools from the theory of linearinvariant cones (Berman & Plemmons, 1994).\n(3) A generalization of the algorithm that learns reactive PSR controlled by a finite-state stochastic policy (Section 5). We provide for this algorithm finite-sample bounds under a simple exploration assumption.\nThe most important tool in our analysis is a concentration inequality for functions of dependent random variables. These inequalities depend on the mixing coefficients of the underlying process. We provide technical estimates for the relevant mixing coefficients in each of the three cases listed above. Our goal for future work is to extend (3) to prove a PAC-RL for PSR under finite-state policies. We also think that the tools we develop to prove (2) can be used to improve the sample complexity of algorithms for learning stochastic weighted automata in the i.i.d. case.\nIn Section 2, we start by recalling several facts about weighted automata, spectral learning, and mixing that will play a role in the sequel. For space reasons, most of our proofs are deferred to the Supplementary Material."}, {"heading": "2. Background", "text": "Let \u03a3 be a finite alphabet, \u03a3? denote the set of words of finite length on \u03a3, \u03a3\u03c9 the set of all infinite words on \u03a3, and be the empty word. Given two sets of words U ,V \u2282 \u03a3? we write U \u00b7 V to denote the set of words {uv|u\u2208U , v\u2208V} obtained by concatenating all words in U with all words in V . Let P(\u03a3\u03c9) be the set of probability distributions over \u03a3\u03c9 . A member \u03c1 \u2208 P(\u03a3\u03c9) is called a stochastic process and a random infinite word \u03be\u223c\u03c1 is called a trajectory.\nWeighted and probabilistic automata A weighted finite automaton (WFA) with n states is a tuple A = \u3008\u03b1, \u03b2, {A\u03c3}\u03c3\u2208\u03a3\u3009 where \u03b1, \u03b2\u2208Rn are vectors of initial and final weights respectively, and A\u03c3 \u2208Rn\u00d7n are matrices of transition weights. A weighted automaton A computes a function fA : \u03a3? \u2192 R given by fA(w) = \u03b1>Aw\u03b2 where Aw = Aw1 \u00b7 \u00b7 \u00b7Awt for w = w1 \u00b7 \u00b7 \u00b7wt. A WFA is minimal if there does not exist another WFA with less states computing the same function. A WFA A is stochastic if there exists a stochastic process \u03c1 such that for every w \u2208 \u03a3?, fA(w) = P[\u03be \u2208 w\u03a3\u03c9]; that is, A provides a representation for the probabilities of prefixes under the distribution of \u03c1. A weighted automaton is irreducible if the labelled directed graph with n vertices obtained by adding a transition from i to j with label \u03c3 whenever A\u03c3(i, j) 6= 0 is strongly connected. It can be shown that irreducibility implies minimality, and that almost all WFA are irreducible in the sense that the set of irreducible WFA are dense on the set of all WFA (Balle et al., 2017).\nA probabilistic finite automaton (PFA) is a stochastic WFA A = \u3008\u03b1, \u03b2, {A\u03c3}\u3009 where the weights have a probabilistic interpretation. Namely, \u03b1 is a probability distribution over [n], A\u03c3(i, j) is the probability of emitting symbol \u03c3 and transitioning to state j starting from state i, and \u03b2(i) = 1 for all i \u2208 [n]. It is immediate to check that a PFA satisfying these conditions induces a stochastic process. However not all stochastic WFA admit an equivalent PFA (Jaeger, 2000; Denis & Esposito, 2008).\nIf A is a PFA, then the matrix A = \u2211 \u03c3\u2208\u03a3A\u03c3 yields the Markov kernel A(i, j) = P[j | i] on the state space [n] after marginalizing over the observations. It is easily checked that A is row-stochastic, and thus A\u03b2 = \u03b2. Furthermore, for every distribution \u03b10 \u2208 Rn over [n] we have \u03b1>0 A = \u03b11 for some other probability distribution \u03b11 over [n]. In the case of PFA irreducibility coincides with the usual concept of irreducibility of the Markov chain induced by A.\nHankel matrices and spectral learning The Hankel matrix of a function f : \u03a3? \u2192 R is the infinite matrix Hf \u2208 R\u03a3\n?\u00d7\u03a3? with entries Hf (u, v) = f(uv). Given finite sets U ,V \u2282 \u03a3?, HU,Vf \u2208RU\u00d7V denotes the restriction of matrix Hf to prefixes in U and suffixes in V . Fliess\u2019 theorem (Fliess, 1974) states that a Hankel matrix\nHf has finite rank n if and only if there exists a WFA A with n states such that f =fA. This implies that a WFA A with n states is minimal if and only if n = rank(HfA). The spectral learning algorithm for WFA (Balle et al., 2014a) provides a mechanism for recovering such a WFA from a finite sub-block HU,Vf of Hf such that: 1) \u2208 U \u2229 V , 2) there exists a set U \u2032 such that U = U \u2032 \u222a (\u222a\u03c3\u2208\u03a3U \u2032\u03c3), 3) rank(Hf ) = rank(H U \u2032,V f ). A pair (U ,V) that satisfies these conditions is called a complete basis for f . The pseudo-code of this algorithm is given below:\nAlgorithm 1: Spectral Learning for WFA Input: number of states n, Hankel matrix HU,V\nFind U \u2032 such that U = U \u2032 \u222a (\u222a\u03c3\u2208\u03a3U \u2032\u03c3) Let H = HU \u2032,V Compute the rank n SVD H \u2248 UDV > Let hV = H{ },V and take \u03b1 = V >hV Let hU \u2032 = HU\n\u2032,{ } and take \u03b2 = D\u22121U>hU \u2032 foreach \u03c3 \u2208 \u03a3 do\nLet H\u03c3 = HU \u2032\u03c3,V and take A\u03c3 = D\u22121U>H\u03c3V\nreturn A = \u3008\u03b1, \u03b2, {A\u03c3}\u3009\nThe main strength of Algorithm 1 is its robustness to noise. Specifically, if only an approximation H\u0302U,V of the Hankel matrix is known, then the error between the target automaton A and the automaton A\u0302 learned from H\u0302U,V can be controlled in terms of the error \u2016HU,V \u2212 H\u0302U,V\u20162; see (Hsu et al., 2012) for a proof in the HMM case and (Balle, 2013) for a proof in the general WFA case. These tedious but now standard arguments readily reduce the problem of learning WFA via spectral learning to that of estimating the corresponding Hankel matrix.\nClassical applications of spectral learning assume one has access to i.i.d. samples from a stochastic process \u03c1. In this setting one can obtain a sample S = (\u03be(1), . . . , \u03be(N)) containing N finite-length trajectories from \u03c1, and use them to estimate a Hankel matrix H\u0302U,VS as follows:\nH\u0302U,VS (u, v) = 1\nN N\u2211 i=1 I{\u03be(i) \u2208 uv\u03a3\u03c9} .\nIf \u03c1 = \u03c1A for some stochastic WFA, then obviously ES [H\u0302U,VS ] = H U,V fA and a large sample size N will provide a good approximation H\u0302U,VS of H U,V fA\n. Explicit concentration bounds for Hankel matrices bounding the error \u2016HU,VfA \u2212 H\u0302 U,V S \u20162 can be found in (Denis et al., 2016).\nIn this paper we consider the more challenging setup where we only have access to a sample S = {\u03be} of size N = 1 from \u03c1. In particular, we show it is possible to replace the empirical average above by a Ce\u0301saro average and still use the spectral learning algorithm to recover the transition ma-\ntrices of a stochastic WFA. To obtain a finite-sample analysis of this single-trajectory learning algorithm we prove concentration results for Ce\u0301saro averages of Hankel matrices. Our analysis relies on concentration inequalities for functions of dependent random variables which depend on mixing properties of the underlying process.\nMixing and concentration Let \u03c1 \u2208 P(\u03a3\u03c9) be a stochastic process and \u03be = x1x2 \u00b7 \u00b7 \u00b7 a random word drawn from \u03c1. For 1 6 s < t 6 T and u \u2208 \u03a3s we let \u03c1t:T (\u00b7|u) denote the distribution of xt \u00b7 \u00b7 \u00b7xT conditioned on x1 \u00b7 \u00b7 \u00b7xs = u. With this notation we define the quantity\n\u03b7t(u, \u03c3, \u03c3 \u2032) = \u2016\u03c1t:T (\u00b7|u\u03c3)\u2212 \u03c1t:T (\u00b7|u\u03c3\u2032)\u2016TV\nfor any u \u2208 \u03a3s\u22121, and \u03c3, \u03c3\u2032 \u2208 \u03a3. Then the \u03b7-mixing coefficients of \u03c1 at horizon T are given by\n\u03b7s,t = sup u\u2208\u03a3s\u22121,\u03c3,\u03c3\u2032\u2208\u03a3\n\u03b7t(u, \u03c3, \u03c3 \u2032) .\nMixing coefficients are useful in establishing concentration properties of functions of dependent random variables. The Lipschitz constant of a function g : \u03a3T \u2192 R with respect to the Hamming distance is defined as\n\u2016g\u2016Lip = sup |g(w)\u2212 g(w\u2032)| ,\nwhere the supremum is taken over all pairs of words w,w\u2032 \u2208 \u03a3T differing in exactly one symbol. The following theorem proved in (Chazottes et al., 2007; Kontorovich et al., 2008) provides a concentration inequality for Lipschitz functions of weakly dependent random variables.\nTheorem 1 Let \u03c1\u2208P(\u03a3\u03c9) and \u03be=x1x2 \u00b7 \u00b7 \u00b7\u223c\u03c1. Suppose g : \u03a3T\u2192R satisfies \u2016g\u2016Lip61 and let Z=g(x1, . . . , xT ). Let \u03b7\u03c1 = 1+max1<s<T \u2211T t=s+1 \u03b7s,t, where \u03b7s,t are the \u03b7-mixing coefficients of \u03c1 at horizon T . Then the following holds for any \u03b5 > 0:\nP\u03be [Z \u2212 EZ > \u03b5T ] 6 exp ( \u22122\u03b52T \u03b72\u03c1 ) ,\nwith an identical bound for the other tail.\nTheorem 1 shows that the mixing coefficient \u03b7\u03c1 is a key quantity in order to control the concentration of a function of dependent variables. In fact, upper-bounding \u03b7\u03c1 in terms of geometric ergodicity coefficients of a latent variable stochastic process enables (Kontorovich & Weiss, 2014) to analyze the concentration of functions of HMMs and (Azizzadenesheli et al., 2016) to provide PAC guarantees for an RL algorithm for POMDP based on spectral tensor decompositions. Our Lemma 2 uses a similar but more refined bounding strategy that directly applies when the transition and observation processes are not conditionally independent. Lemma 4 refines this strategy further to control \u03b7\u03c1 for stochastic WFA (for which there may be no underlying Markov stochastic process in general). To the best of our knowledge this yields the first concentration results for the challenging setting of stochastic WFA."}, {"heading": "3. Single-Trajectory Spectral Learning of PFA", "text": "In this section we focus on the problem of learning the transition structure of a PFA A using single trajectory is generated by A. We provide a spectral learning algorithm for this problem and a finite-sample analysis consisting of a concentration bound for the error on the Hankel matrix estimated by the algorithm. We assume the learner has access to a single infinite-length trajectory \u03be \u223c \u03c1A that is progressively uncovered. The algorithm uses a length t prefix from \u03be to estimate a Hankel matrix whose entries are Ce\u0301saro averages. This Hankel matrix is then processed by the usual spectral learning algorithm to recover an approximation to an automaton with transition weights equivalent to those of A. We want to analyze the quality of the model learned by the algorithm after observing the first t symbols from \u03be.\nWe start by showing that Ce\u0301saro averages provide a consistent mechanism for learning the transition structure of A. Then we proceed to analyze the accuracy of the Hankel estimation step. As discussed in Section 2, this is enough to obtain finite-sample bounds for learning PFA. The general case of stochastic WFA is considered in Section 4."}, {"heading": "3.1. Learning with Ce\u0301saro Averages is Consistent", "text": "Let A = \u3008\u03b1, \u03b2, {A\u03c3}\u3009 be a PFA computing a function fA : \u03a3?\u2192R and defining a stochastic process \u03c1A\u2208P(\u03a3\u03c9). For convenience we drop the subscript and just write f and \u03c1. Since we only have access to a single trajectory \u03be from \u03c1we cannot obtain an approximation of the Hankel matrix for f by averaging over multiple i.i.d. trajectories. Instead, we compute Ce\u0301saro averages over the trajectory \u03be to obtain a Hankel matrix whose expectation is related to A as follows. For any t \u2208 N let f\u0304t : \u03a3? \u2192 R be the function given by f\u0304t(w) = (1/t) \u2211t\u22121 s=0 f(\u03a3\nsw), where f(\u03a3sw) =\u2211 u\u2208\u03a3s f(uw). We shall sometimes write fs(w) = f(\u03a3sw). Using the definition of the function computed by a WFA it is easy to see that\u2211\nu\u2208\u03a3s f(uw) = \u2211 u\u2208\u03a3s \u03b1>AuAw\u03b2 = \u03b1 >AsAw\u03b2 ,\nwhere A= \u2211 \u03c3 A\u03c3 is the Markov kernel on the state space\nof A. Thus, introducing \u03b1\u0304>t = (1/t) \u2211t\u22121 s=0 \u03b1\n>As we get f\u0304t(w) = \u03b1\u0304 > t Aw\u03b2. Since \u03b1 is a probability distribution, A is a Markov kernel, and probability distributions are closed by convex combinations, then \u03b1\u0304t is also a probability distribution over [n]. Thus, we have just proved the following:\nLemma 1 (Consistency) The Ce\u0301saro average of f over t steps, f\u0304t, is computed by the probabilistic automaton A\u0304t = \u3008\u03b1\u0304t, \u03b2, {A\u03c3}\u3009. In particular, A and A\u0304t have the same number of states and the same transition probability matrices. Furthermore, if A is irreducible then A\u0304t is minimal.\nThe irreducibility claim follows from (Balle et al., 2017).\nFor convenience, in the sequel we write H\u0304U,Vt for the (U ,V)-block of the Hankel matrix HfA\u0304t .\nRemark 1 The irreducible condition simply ensures there is a unique stationary distribution, and that the Hankel matrix of A\u0304t has the same rank as the Hankel matrix of A (otherwise it could be smaller)."}, {"heading": "3.2. Spectral Learning Algorithm", "text": "Algorithm 2 describes the estimation of the empirical Hankel matrix H\u0302U,Vt,\u03be from the first t+L symbols of a single trajectory using the corresponding Ce\u0301saro averages. To avoid cumbersome notations, in the sequel we may drop super and subscripts when not needed and write H\u0302t or H\u0302 when U , V , and \u03be are clear from the context. Note that by Lemma 1 the expectation E[H\u0302] over \u03be\u223c\u03c1 is equal to the Hankel matrix H\u0304t of the function f\u0304t computed by the PFA A\u0304t.\nAlgorithm 2: Single Trajectory Spectral Learning (Generative Case)\nInput: number of states n, length t, prefixes U \u2282 \u03a3?, suffixes V \u2282 \u03a3?\nLet L = maxw\u2208U\u00b7V |w| Sample trajectory \u03be = x1x2 \u00b7 \u00b7 \u00b7xt+L \u00b7 \u00b7 \u00b7 \u223c \u03c1 foreach u \u2208 U and v \u2208 V do\nLet H\u0302(u, v) = 1t \u2211t\u22121 s=0 I{xs+1:s+|uv| = uv}\nApply the spectral algorithm to H\u0302 with rank n"}, {"heading": "3.3. Concentration Results", "text": "Now we proceed to analyze the error H\u0302t \u2212 H\u0304t in the Hankel matrix estimation inside Algorithm 2. In particular, we provide concentration bounds that depend on the length t, the mixing coefficient \u03b7\u03c1 of the process \u03c1, and the structure of the basis (U ,V). The main result of this section is the matrix-wise concentration bound Theorem 3 where we control the spectral norm of the error matrix. For comparison we also provide a simpler entry-wise bound and recall the equivalent matrix-wise bound in the i.i.d. setting.\nBefore trying to bound the concentration of the errors using Theorem 1 we need to analyze the mixing coefficient of the process generated by a PFA. This is the goal of the following result, whose proof is provided in Appendix A.\nLemma 2 (\u03b7-mixing for PFA) Let A be PFA and assume that it is (C, \u03b8)-geometrically mixing in the sense that for some constants C > 0, \u03b8 \u2208 (0, 1) we have\n\u2200t \u2208 N, \u00b5At = sup \u03b1,\u03b1\u2032 \u2016\u03b1At \u2212 \u03b1\u2032At\u20161 \u2016\u03b1\u2212 \u03b1\u2032\u20161 6 C\u03b8t ,\nwhere the supremum is over all probability vectors. Then we have \u03b7\u03c1A 6 C/(\u03b8(1\u2212 \u03b8)).\nRemark 2 A sufficient condition for the geometric control of \u00b5At is that A admits a spectral gap. In this case \u03b8 can be chosen to be the modulus of the second eigenvalue |\u03bb2(A)| < 1 of the transition kernel A.\nBefore the main result of this section we provide a concentration result for each individual entry of the estimated Hankel matrix as a warmup (see Appendix D).\nTheorem 2 (Single-trajectory, entry-wise) Let A be a (C, \u03b8)-geometrically mixing PFA and \u03be \u223c \u03c1A a trajectory of observations. Then for any u\u2208U , v\u2208V and \u03b4\u2208(0, 1),\nP [ H\u0302U,Vt,\u03be (u, v)\u2212H\u0304 U,V t (u, v) >\n|uv|C \u03b8(1\u2212 \u03b8)\n\u221a( 1 + |uv| \u2212 1\nt\n) log(1/\u03b4) 2t ] 6 \u03b4 ,\nwith an identical bound for the other tail.\nA naive way to handle the concentration of the whole Hankel matrix is to control the Frobenius norm \u2016H\u0302t\u2212 H\u0304t\u2016F by taking a union bound over all entries using Theorem 2. However, the resulting concentration bound would scale as\u221a |U||V|. To have better dependency with the dimension (the matrix has dimension |U| \u00d7 |V|) can split the empirical Hankel matrix H\u0302 into blocks containing strings of the same length (as suggested by the dependence of the bound above on |uv|). We thus introduce the maximal length L = maxw\u2208U\u00b7V |w|, and the set U` = {u \u2208 U : |u| = `} for any ` \u2208 N. We use these to define the quantity nU = |{` \u2208 [0, L] : |U`| > 0}|, and introduce likewise V`, nV with obvious definitions. With this notation we can now state the main result of this section.\nTheorem 3 (Single-trajectory, matrix-wise) Let A be as in Theorem 2. Let m = \u2211 u\u2208U,v\u2208V f\u0304t(uv) be the probability mass and d = min{|U||V|, 2nUnV} be the effective dimension. Then, for all \u03b4 \u2208 (0, 1) we have\nP [ \u2016H\u0302U,Vt,\u03be \u2212H\u0304 U,V t \u20162 > ( \u221a L+ \u221a 2C\n1\u2212 \u03b8\n)\u221a 2m\nt\n+ 2LC\n\u03b8(1\u2212\u03b8)\n\u221a( 1+\nL\u22121 t )d ln(1/\u03b4) 2t ] 6 \u03b4 .\nRemark 3 Note that quantity nUnV in d can be exponentially smaller than |U||V|. Indeed, for U = V = \u03a36L/2 we have |U||V| = \u0398(|\u03a3|L) while nUnV = \u0398(L2).\nFor comparison, we recall a state-of-the-art concentration bound for estimating the Hankel matrix of a stochastic language2 from N i.i.d. trajectories.\n2A stochastic language is a probability distribution over \u03a3?.\nTheorem 4 (Theorem 7 in (Denis et al., 2014)) Let A be a stochastic WFA with stopping probabilities and S = (\u03be(1), . . . , \u03be(N)) be an i.i.d. sample of size N from the distribution \u03c1A \u2208 P(\u03a3?). Let m = \u2211 u\u2208U,v\u2208V fA(uv). Then, for all c > 0 we have\nP [ \u2016H\u0302U,VS \u2212H U,V fA \u20162 > \u221a 2cm\nN +\n2c\n3N\n] 6\n2c\nec \u2212 c\u2212 1 ."}, {"heading": "3.4. Sketch of the Proof of Theorem 3", "text": "In this section we sketch the main steps of the proof of Theorem 3 (the full proof is given in Appendices A and D). We focus on highlighting the main difficulties and paving the path for the extension of Theorem 3 to stochastic WFA given in Section 4.\nThe key of the proof is to study the function g(\u03be) = \u2016H\u0302U,Vt,\u03be \u2212 H\u0304 U,V t \u20162, in view of applying Theorem 1. To this end, we first control the \u03b7-mixing coefficients using Lemma 2. The next step is to control the Lipschitz constant \u2016g\u2016Lip. This part is not very difficult and we derive after a few careful steps the bound \u2016g\u2016Lip 6 L \u221a d/t.\nThe second and most interesting part of the proof is about the control of E[g(\u03be)]. Let us give some more details.\nDecomposition step We control \u2016H\u0302U,Vt \u2212 H\u0304 U,V t \u20162 by its Frobenius norm and get\nE[\u2016H\u0302U,Vt \u2212H\u0304 U,V t \u20162]2 6 \u2211 w\u2208U\u00b7V |w|U,VE[(f\u0302t(w)\u2212f\u0304t(w))2] ,\nwhere we introduced |w|U,V = |{(u, v) \u2208 U \u00d7 V : uv = w}|, and f\u0302t(w) = 1t \u2211t s=1bs(w) using the shorthand notation bs(w) = I{xs . . . xs+|w|\u22121 = w}. Also, f\u0304t(w) = E[f\u0302t(w)] = 1t \u2211t s=1 fs(w), where fs(w) = \u03c1A(\u03a3 s\u22121w\u03a3\u03c9). This implies that we have a sum of variances, where each of the terms can be written as\nE[(f\u0302t(w)\u2212 f\u0304t(w))2] =\n1 t2 E ( t\u2211 s=1 bs(w) )2\u2212 1 t2 ( t\u2211 s=1 fs(w) )2 .\nSlicing step An important observation is that each probability term satisfies fs(w) = \u03b1>As\u22121Aw\u03b2 because of the PFA assumption on \u03c1A. Furthermore, it follows from A being a PFA that \u2211 |w|=l fs(w) = 1 for all s and l. This suggests that we group the terms in the sum over W = U \u00b7V by length, so we writeWl =W\u2229\u03a3l and define Ll = maxw\u2208Wl |w|U,V the maximum number of ways to write a string of length l inW as a concatenation of a prefix in U and a suffix in V . Note that we always have Ll 6 l+1.\nA few more steps lead to the following bound\nE[\u2016H\u0302U,Vt \u2212 H\u0304 U,V t \u20162]2 6 (1)\n1\nt2 \u221e\u2211 l=0 \u2211 w\u2208Wl |w|U,V [ t\u2211 s=1 (1\u2212fs(w))fs(w)\n+2 \u2211\n16s<s\u20326t\n( E[bs(w)bs\u2032(w)]\u2212fs(w)fs\u2032(w) )] .\nWe control the first term in (1) using \u221e\u2211 l=0 \u2211 w\u2208Wl |w|U,V t\u2211 s=1 (1\u2212fs(w))fs(w)6 t \u2211 u\u2208U,v\u2208V f\u0304t(uv) .\nCross terms Regarding the remaining \u201ccross\u201d-term in (1) we fix w \u2208 Wl and obtain the equation\nE[bs(w)bs\u2032(w)]\u2212 fs(w)fs\u2032(w)\n= \u03b1>s\u22121 ( As \u2032\u2212s w \u2212Aw\u03b2\u03b1>s\u2032\u22121Aw ) \u03b2 , (2)\nwhere we introduced the vectors \u03b1>s =\u03b1 >As and transition matrixAs \u2032\u2212s w = \u2211 x\u2208\u03a3s\u2032\u2212sw Ax corresponding to the \u201cevent\u201d \u03a3s \u2032\u2212s w =w\u03a3 s\u2032\u2212s \u2229 \u03a3s\u2032\u2212sw. We now discuss two cases.\nFirst control If s\u2032\u2212 s < l, we use the simplifying fact that \u03a3s \u2032\u2212s w \u2282 w\u03a3s \u2032\u2212s to upper bound (2) by\n\u03b1>s\u22121Aw ( As \u2032\u2212s \u2212 \u03b2\u03b1>s\u2032\u22121Aw ) \u03b2\n= fs(w)(1\u2212 fs\u2032(w)) 6 fs(w) .\nSecond control When s\u2032\u2212s > |w| = l we have \u03a3s\u2032\u2212sw = w\u03a3s \u2032\u2212s\u2212lw and As \u2032\u2212s w =AwA\ns\u2032\u2212s\u2212lAw. Thus, we rewrite (2) and bound it using Ho\u0308lder\u2019s inequality as follows:\n\u03b1>s\u22121Aw ( As \u2032\u2212s\u2212l\u2212\u03b2\u03b1>s\u2032\u22121 ) Aw\u03b2 6 (3)\n\u2016\u03b1>s\u22121Aw\u20161\u2016As \u2032\u2212s\u2212l \u2212 \u03b2\u03b1>s\u2032\u22121\u2016\u221e\u2016Aw\u03b2\u2016\u221e .\nUsing Lemma 6 in Appendix A we bound the induced norm as \u2016As\u2032\u2212s\u2212l \u2212 \u03b2\u03b1>s\u2032\u22121\u2016\u221e 6 2\u00b5As\u2032\u2212s\u2212l, where \u00b5At is the mixing coefficient defined in Lemma 2. Also, it holds that \u2016Aw\u03b2\u2016\u221e 6 1. Finally, since \u03b1>s\u22121Aw is a sub-distribution over states, we have the key equalities\u2211 w\u2208Wl |w|U,V\u2016\u03b1>s\u22121Aw\u20161 = \u2211 w\u2208Wl |w|U,V\u03b1>s\u22121Aw\u03b2 (4)\n= \u2211 w\u2208Wl |w|U,Vfs(w) = \u2211 u\u2208U,v\u2208V:uv\u2208Wl fs(uv) .\nThe proof is concluded by collecting the previous bounds, plugging them into (1), and using Lemma 2 to get\nE[g(\u03be)]2 6 (\n2L\u2212 1 + 4C 1\u2212 \u03b8\n) m\nt . (5)"}, {"heading": "4. Extension to Stochastic WFA", "text": "We now generalize the results in previous section to the case where the distribution over \u03be is generated by a stochastic weighted automaton that might not have a probabilistic representation. The key observation is that Algorithm 2 can learn stochastic WFA without any change, and the consistency result in Lemma 1 extends verbatim to stochastic WFA. However, the proof of the concentration bound in Theorem 3 requires further insights into the mixing properties of stochastic WFA. Before describing the changes required in the proof, we discuss some important geometric properties of stochastic WFA."}, {"heading": "4.1. The State-Space Geometry of SWFA", "text": "Recall that a stochastic WFA (SWFA) A = \u3008\u03b1, \u03b2, {A\u03c3}\u3009 defines a stochastic process \u03c1A and computes a function fA such that fA(w) = P[\u03be \u2208 w\u03a3\u03c9], where \u03be \u223c \u03c1A. It is immediate to check that this implies that the weights of A satisfy the properties: (i) \u03b1>Ax\u03b2 > 0 for all x \u2208 \u03a3?, and (ii) \u03b1>At\u03b2 = \u2211 |w|=t \u03b1\n>Aw\u03b2 = 1 for all t > 0, where A = \u2211 \u03c3\u2208\u03a3A\u03c3 . Without loss of generality we assume throughout this section that A is a minimal SWFA of dimension n, meaning that any SWFA computing the same probability distribution than A must have dimension at least n. Importantly, the weights in \u03b1, \u03b2, and A\u03c3 are not required to be non-negative in this definition. Nonetheless, it follows from these properties that \u03b2 is an eigenvector of A of eigenvalue 1 exactly like in the case of PFA. We now introduce further facts about the geometry of SWFA.\nA minimal SWFA A is naturally associated with a proper (i.e. pointed, closed, and solid) cone in K\u2282Rn called the backward cone (Jaeger, 2000), and characterized by the following properties: 1) \u03b2\u2208K, 2) A\u03c3K\u2286K for all \u03c3\u2208\u03a3, and 3) \u03b1>v>0 for all v\u2208K. Condition 2) says that every transition matrix A\u03c3 leaves K invariant, and in particular the backward vector Aw\u03b2 belongs to K for all w \u2208 \u03a3?. The vector of final weights \u03b2 plays a singular role in the geometry of the state space of a SWFA. This follows from facts about the theory of invariant cones (Berman & Plemmons, 1994) which provides a generalization of the classical Perron\u2013Frobenius theory of non-negative matrices to arbitrary matrices. We recall from (Berman & Plemmons, 1994) that a norm on Rn can be associated with every vector in the interior of K. In particular, we will take the norm associated with the final weights \u03b2 \u2208 K. This norm, denoted by \u2016 \u00b7 \u2016\u03b2 , is completely determined by its unit ball B\u03b2 = {v \u2208 Rn : \u2212\u03b2 6K v 6K \u03b2}, where u 6K v means v \u2212 u\u2208K. In particular, \u2016v\u2016\u03b2 = inf{r > 0 : v \u2208 rB\u03b2}. Induced and dual norms are derived from \u2016 \u00b7 \u2016\u03b2 as usual. When A is a PFA one can takeK to be the cone of vectors in Rn with non-negative entries, in which case \u03b2 = (1, . . . , 1) and \u2016 \u00b7 \u2016\u03b2 reduces to \u2016 \u00b7 \u2016\u221e (Berman & Plemmons, 1994). The following result shows that \u2016 \u00b7 \u2016\u03b2 provides the right\ngeneralization to SWFA of the norm \u2016\u00b7\u2016\u221e used in the Second control step of the proof for PFA (see Appendix B).\nLemma 3 For any w \u2208 \u03a3?: (i) \u2016Aw\u03b2\u2016\u03b2 6 1, and (ii) \u2016\u03b1>Aw\u2016\u03b2,\u2217 = \u03b1>Aw\u03b2.\nIt is also natural to consider mixing coefficients for stochastic processes generated by SWFA in terms of the dual \u03b2norm. This provides a direct analog to Lemma 2 for PFA:\nLemma 4 (\u03b7-mixing for SWFA) Let A be SWFA and assume that it is (C, \u03b8)-geometrically mixing in the sense that for some C > 0, \u03b8 \u2208 (0, 1),\n\u00b5At = sup \u03b10,\u03b11:\u03b1>0 \u03b2=\u03b1 > 1 \u03b2=1 \u2016\u03b1>0 At \u2212 \u03b1>1 At\u2016\u03b2,? \u2016\u03b10 \u2212 \u03b11\u2016\u03b2,? 6 C\u03b8t .\nThen the \u03b7-mixing coefficient satisfies \u03b7\u03c1A 6 C/\u03b8(1\u2212 \u03b8).\nRemark 4 A sufficient condition for the geometric control of \u00b5At is that A admits a spectral gap. In this case \u03b8 can be chosen to be the modulus of the second eigenvalue |\u03bb2(A)|<1 of A. Another sufficient condition is that \u03b8=\u03b3\u03b2(A)<1, where\n\u03b3\u03b2(A) = sup { ||A\u03bd||\u03b2,? ||\u03bd||\u03b2,? : \u03bd s.t. ||\u03bd||\u03b2,? 6= 0, \u03bd>\u03b2 = 0 } ."}, {"heading": "4.2. Concentration of Hankel Matrices for SWFA", "text": "We are now ready to extend the proof of Theorem 3 to SWFA. Using that both PFA and SWFA define probability distributions over prefixes it follows that any argument in Section 3.4 that only appeals to the function computed by the automaton can remain unchanged. Therefore, the only arguments that need to be revisited are described in the Second control step. In particular, we must provide versions of (3) and (4) for SWFA.\nRecalling that Ho\u0308lder\u2019s inequality can be applied with any pair of dual norms, we start by replacing the norms \u2016 \u00b7 \u2016\u221e and \u2016 \u00b7 \u20161 in (3) with the cone-norms \u2016 \u00b7 \u2016\u03b2 and \u2016 \u00b7 \u2016\u03b2,? respectively. Next we use Lemma 3 to obtain, for any w \u2208 \u03a3?, the bound \u2016Aw\u03b2\u2016\u03b2 6 1 and the equation \u2016\u03b1>Aw\u2016\u03b2,\u2217 = \u03b1>Aw\u03b2 which are direct analogs of the results used for PFA. Then it only remains to relate the \u03b2-norm of As\n\u2032\u2212s\u2212l \u2212 \u03b2\u03b1>s\u2032\u22121 to the mixing coefficients \u00b5At . Applying Lemma 8 in Appendix A yields \u2016As\u2032\u2212s\u2212l \u2212 \u03b2\u03b1>s\u2032\u22121\u2016\u03b2 6 2\u00b5As\u2032\u2212s\u2212l. Thus we obtain for SWFA exactly the same concentration result that we obtained for empirical Hankel matrices estimated from a single trajectory of observations generated by a PFA.\nTheorem 5 (Single-trajectory, SWFA) Let A be a SWFA that is (C, \u03b8)-geometrically mixing with the definition in Lemma 4. Then the concentration bound in Theorem 3 also holds for trajectories \u03be \u223c \u03c1A."}, {"heading": "5. The Controlled Case", "text": "This section describes the final contribution of the paper: a generalization of our analysis of spectral learning from a single trajectory the case of dynamical systems under finite-state control. We consider discrete-time dynamical systems with finite set of observations O and finite set of actions A, and let \u03a3 =O\u00d7A. We assume the learner has access to a single trajectory \u03be=(ot, at)t>1 in \u03a3\u03c9 . The trajectory is generated by coupling an environment defining a distribution over observations conditioned on actions and a policy defining a distribution over actions conditioned on observations. Assuming the joint action-observation distribution can be represented by a stochastic WFA is equivalent to saying that the environment corresponds to a POMDP or PSR, and the policy has finite memory. To fix some notation we assume the environment is represented by a conditional3 stochastic WFA A = \u3008\u03b1, \u03b2, {A\u03c3}\u3009 with n states. This implies the semantics fA(w) = P[o1 \u00b7 \u00b7 \u00b7 ot|a1 \u00b7 \u00b7 \u00b7 at] for the function computed by A, where w = w1 \u00b7 \u00b7 \u00b7wt with wi = (oi, au). For any w \u2208 \u03a3? we shall write wA = a1 \u00b7 \u00b7 \u00b7 at andwO = o1 \u00b7 \u00b7 \u00b7 ot. We also assume there is a stochastic policy \u03c0 represented by a conditional PFA A\u03c0 = \u3008\u03b1\u03c0, \u03b2\u03c0, {\u03a0\u03c3}\u3009 with k states; that is, fA\u03c0 (w) = \u03c0(w\nA|wO) = P[a1 \u00b7 \u00b7 \u00b7 at|o1 \u00b7 \u00b7 \u00b7 ot]. In particular, A\u03c0 represents a stochastic policy that starts in a state s1 \u2208 [k] sampled according to \u03b1\u03c0(i) = P[s1 = i], and at each time step samples an action and changes state according to \u03a0o,a(i, j) = P[st+1 = j, at = a|ot = o, st = i]. The trajectory \u03be observed by the learner is generated by the stochastic process \u03c1 \u2208 P(\u03a3\u03c9) obtained by coupling A and A\u03c0 . A standard construction in the theory of weighted automata (Berstel & Reutenauer, 1988) shows that this process can be computed by the product automaton B = A\u2297A\u03c0 = \u3008\u03b1\u2297, \u03b2\u2297, {B\u03c3}\u3009, where \u03b1\u2297 = \u03b1\u2297\u03b1\u03c0 , \u03b2\u2297 = \u03b2 \u2297 \u03b2\u03c0 , and Bo,a = Ao,a \u2297\u03a0o,a. It is easy to verify that B is a stochastic WFA with nk states computing the function fB(w) = fA(w)fA\u03c0 (w) = P[\u03be \u2208 w\u03a3\u03c9]. At this point, the spectral algorithm from Section 4 could be used to learn B directly from a trajectory \u03be\u223c \u03c1B. However, since the agent interacting with environment A knows the policy \u03c0, we would like to leverage this information to learn directly a model of the environment. This approach is formalized in Algorithm 3, which provides a singletrajectory version of the algorithm in (Bowling et al., 2006) for learning PSR from non-blind policies with i.i.d. data. The main difference with Algorithm 2 is that in the reactive case we need a smoothing parameter \u03ba that will prevent the entries in the empirical Hankel matrix H\u0302 to grow unboundedly plus that the policy \u03c0 satisfies an exploration assumption. \u03ba plays a similar role in our analysis as the smoothing parameter introduced in (Denis et al., 2014) for learning\n3Such WFA are also called reactive predictive state representations in the RL literature.\nAlgorithm 3: Single Trajectory Spectral Learning (Reactive Case)\nInput: number of states n, length t, U ,V\u2282(A\u00d7O)?, policy \u03c0, smoothing coefficient \u03ba\nLet L = maxw\u2208U\u00b7V |w| Sample trajectory o1a1o2a2 \u00b7 \u00b7 \u00b7 ot+Lat+L using \u03c0 foreach u \u2208 U and v \u2208 V do\nLet H\u0302(u, v) = 1 t \u2211t\u22121 s=0 I{os+1as+1\u00b7\u00b7\u00b7os+|uv|as+|uv|=uv} \u03bas\u03c0(a1\u00b7\u00b7\u00b7as+|uv||o1\u00b7\u00b7\u00b7os+|uv|)\nApply the spectral algorithm to H\u0302 with rank n\nstochastic languages from factor estimates in the i.i.d. case. The difference is that in our case the smoothing parameter must satisfy \u03ba\u03b5 > 1, where \u03b5 is the exploration probability of the policy \u03c0 provided by the following assumption.\nAssumption 1 (Exploration) There exists some \u03b5 > 0 such that for each w \u2208 \u03a3? the policy \u03c0 satisfies \u03c0(wA|wO) > \u03b5|w|. In particular, at every time step each action a \u2208 A is picked with probability at least \u03b5.\nBefore moving to the next section, where we provide finitesample concentration results for the Hankel matrix estimated by Algorithm 3, we show that Algorithm 3 is consistent, that is it learns in expectation a WFA whose transition matrices are equivalent to those of the environment A. The proof of the following lemma is provided in Appendix E.\nLemma 5 The Hankel matrix H\u0302 = H\u0302U,Vt,\u03be computed in Algorithm 3 satisfies E[H\u0302U,Vt,\u03be ] = H\u0303 U,V t , where H\u0303 U,V t is a block of the Hankel matrix corresponding to the stochastic WFA A\u0303t=\u3008\u03b1\u0303t, \u03b2, {A\u03c3}\u3009 where we introduced the modified vector \u03b1\u0303t=(1/t) \u2211t\u22121 s=0 \u03b1\n>(A/\u03ba)s. We denote by f\u0303t the function computed by A\u0303t."}, {"heading": "5.1. Concentration Results", "text": "Broadly speaking, a concentration bound for the estimation error \u2016H\u0302U,Vt,\u03be \u2212H\u0303 U,V t \u20162 can be obtained by following a proof strategy similar to the ones used in Theorems 3 and 5. However, almost all the bounds used in the previous proofs need to be reworked to account for (i) the effect of the extra dependencies introduced by the policy \u03c0, and (ii) the fact that the target automaton A to be learned is not a stochastic WFA in the sense of Section 4 but rather a conditional stochastic WFA.\nPoint (i) is addressed in our proof by introducing a \u201cnormalized\u201d reference process \u03c1A\u0304 corresponding to the coupling A\u0304 = A \u2297 Aunif between the environment A and the uniform random policy that at each step takes each action independently with probability 1/|A|. Assuming the smoothing parameter satisfies \u03ba\u03b5> 1 for some exploration\nparameter \u03b5 (cf. Assumption 1), then 1/\u03ba 6 1/|A|. This observation is used, for example, to bound some variance terms in E[g(\u03be)] by replace occurrences of f\u0303t with f\u0303unift , the function computed by taking the Ce\u0301saro average of the first t steps of A\u0304. Ultimately, this makes our bound depend not only on the mixing properties of \u03c1B, but also on those of the normalized process \u03c1A\u0304 induced by the SWFA A\u0304. Incidentally, this argument is also used to address point (ii): by bounding quantities involving \u03ba by quantities computed by a SWFA we can use again the arguments sketched in Section 4.1.\nPursuing the ideas above, and assuming that A\u0304 is (C\u0304, \u03b8\u0304)geometrically mixing, we obtain the following bound which can be compared to the one in (5):\nE[g(\u03be)]2 6 m\u0303\nt\u03b5L(1\u2212 1/(\u03ba\u03b5)2) +\n2m\u0304\nt\u03b52L\n( L+ C\u0304\n1\u2212 \u03b8\u0304\n) ,\nwhere L = maxw\u2208U\u00b7V |w|, m\u0303 = \u2211 u\u2208U,v\u2208V f\u0303t(uv), and\nm\u0304 = \u2211 u\u2208U,v\u2208V f\u0303 unif t (uv).\nTheorem 6 Suppose that B is (C, \u03b8)-geometrically mixing and A\u0304 is (C\u0304, \u03b8\u0304)-geometrically mixing. Suppose \u03c0 satisfies Assumption 1 and the smoothing coefficient \u03ba satisfies \u03ba\u03b5 > 1. Let d = \u2211 w\u2208U\u00b7V |w|U,V , and define L, m\u0303, m\u0304 as above. Then for any \u03b4 \u2208 (0, 1) we have\nP [ \u2016H\u0302U,Vt,\u03be \u2212 H\u0303 U,V t \u20162 > \u221a m\u0303\nt\u03b5L(1\u2212 \u03ba\u22122\u03b5\u22122) +\u221a\n2m\u0304\nt\u03b52L\n( L+ C\u0304\n1\u2212\u03b8\u0304\n) +\nC\n\u03b8(1\u2212\u03b8)\u03b5L\n\u221a 2d ln(1/\u03b4)\nt\n] 6 \u03b4 .\nOn a final note we remark that the dependence on \u03b5L might be unavoidable due to inherent increase in variance produced by importance sampling estimators."}, {"heading": "6. Conclusion", "text": "We present the first rigorous analysis of single-trajectory SVD-based spectral learning algorithms for sequential models with latent variables. Our analysis highlights the role of mixing properties of WFA and their relation with the geometry of the underlying state space. In the controlled case we obtain a result for control with finite-state policies, a much more general class than previously considered memoryless policies. In future work we will use our results to get upper confidence bounds on the predictions made by the learned environment with the goal of solving the full RL problem for PSR with complex control policies."}, {"heading": "Acknowledgements", "text": "O.-A. M. acknowledges the support of the French Agence Nationale de la Recherche (ANR), under grant ANR-16CE40-0002 (project BADASS)."}], "year": 2017, "references": [{"title": "A spectral algorithm for latent dirichlet allocation", "authors": ["Anandkumar", "Anima", "Foster", "Dean P", "Hsu", "Daniel J", "Kakade", "Sham M", "Liu", "Yi-Kai"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Tensor decompositions for learning latent variable models", "authors": ["Anandkumar", "Animashree", "Ge", "Rong", "Hsu", "Daniel J", "Kakade", "Sham M", "Telgarsky", "Matus"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Reinforcement learning of pomdps using spectral methods", "authors": ["Azizzadenesheli", "Kamyar", "Lazaric", "Alessandro", "Anandkumar", "Animashree"], "venue": "In 29th Annual Conference on Learning Theory,", "year": 2016}, {"title": "Learning and planning with timing information in markov decision processes", "authors": ["Bacon", "Pierre-Luc", "Balle", "Borja", "Precup", "Doina"], "venue": "In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence,", "year": 2015}, {"title": "A spectral approach for probabilistic grammatical inference on trees", "authors": ["R. Bailly", "A. Habrard", "F. Denis"], "venue": "In Algorithmic Learning Theory, pp", "year": 2010}, {"title": "Quadratic weighted automata: Spectral algorithm and likelihood maximization", "authors": ["Bailly", "Rapha\u00ebl"], "venue": "In Proceedings of the 3rd Asian Conference on Machine Learning,", "year": 2011}, {"title": "Learning Finite-State Machines: Algorithmic and Statistical Aspects", "authors": ["B. Balle"], "venue": "PhD thesis, Universitat Polite\u0300cnica de Catalunya,", "year": 2013}, {"title": "Spectral learning of general weighted automata via constrained matrix completion", "authors": ["Balle", "Borja", "Mohri", "Mehryar"], "venue": "In Advances in neural information processing systems,", "year": 2012}, {"title": "Learning weighted automata", "authors": ["Balle", "Borja", "Mohri", "Mehryar"], "venue": "In International Conference on Algebraic Informatics,", "year": 2015}, {"title": "A spectral learning algorithm for finite state transducers", "authors": ["Balle", "Borja", "Quattoni", "Ariadna", "Carreras", "Xavier"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "year": 2011}, {"title": "Spectral learning of weighted automata", "authors": ["Balle", "Borja", "Carreras", "Xavier", "Luque", "Franco M", "Quattoni", "Ariadna"], "venue": "Machine learning,", "year": 2014}, {"title": "Methods of moments for learning stochastic languages: Unified presentation and empirical comparison", "authors": ["Balle", "Borja", "Hamilton", "William L", "Pineau", "Joelle"], "venue": "In ICML,", "year": 2014}, {"title": "Bisimulation metrics for weighted automata", "authors": ["Balle", "Borja", "Gourdeau", "Pascale", "Panangaden", "Prakash"], "venue": "In 44rd International Colloquium on Automata, Languages, and Programming,", "year": 2017}, {"title": "Nonnegative matrices in the mathematical sciences", "authors": ["Berman", "Abraham", "Plemmons", "Robert J"], "year": 1994}, {"title": "Rational Series and Their Languages", "authors": ["Berstel", "Jean", "Reutenauer", "Christophe"], "year": 1988}, {"title": "Closing the learning-planning loop with predictive state representations", "authors": ["Boots", "Byron", "Siddiqi", "Sajid M", "Gordon", "Geoffrey J"], "venue": "The International Journal of Robotics Research,", "year": 2011}, {"title": "Learning predictive state representations using non-blind policies", "authors": ["Bowling", "Michael", "McCracken", "Peter", "James", "Neufeld", "Wilkinson", "Dana"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "year": 2006}, {"title": "Concentration inequalities for random fields via coupling", "authors": ["Chazottes", "J-R", "Collet", "Pierre", "K\u00fclske", "Christof", "Redig", "Frank"], "venue": "Probability Theory and Related Fields,", "year": 2007}, {"title": "Experiments with spectral learning of latentvariable PCFGs", "authors": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "In Proceedings of NAACL,", "year": 2013}, {"title": "Spectral learning of latent-variable PCFGs: Algorithms and sample complexity", "authors": ["S.B. Cohen", "K. Stratos", "M. Collins", "D.P. Foster", "L. Ungar"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Dimension-free concentration bounds on hankel matrices for spectral learning", "authors": ["Denis", "Fran\u00e7ois", "Gybels", "Mattias", "Habrard", "Amaury"], "venue": "Journal of Machine Learning Research,", "year": 2016}, {"title": "On rational stochastic languages", "authors": ["Denis", "Fran\u00e7ois", "Esposito", "Yann"], "venue": "Fundamenta Informaticae,", "year": 2008}, {"title": "Dimension-free concentration bounds on hankel matrices for spectral learning", "authors": ["Denis", "Fran\u00e7ois", "Gybels", "Mattias", "Habrard", "Amaury"], "venue": "In ICML, pp", "year": 2014}, {"title": "Matrices de Hankel", "authors": ["M. Fliess"], "venue": "Journal de Mathe\u0301matiques Pures et Applique\u0301es,", "year": 1974}, {"title": "Pac learning of probabilistic automaton based on the method of moments", "authors": ["Glaude", "Hadrien", "Pietquin", "Olivier"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Efficient learning and planning with compressed predictive states", "authors": ["Hamilton", "William L", "Fard", "Mahdi Milani", "Pineau", "Joelle"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Learning mixtures of spherical Gaussians: moment methods and spectral decompositions", "authors": ["Hsu", "Daniel", "Kakade", "Sham M"], "venue": "In Innovations in Theoretical Computer Science,", "year": 2013}, {"title": "A spectral algorithm for learning hidden markov models", "authors": ["Hsu", "Daniel", "Kakade", "Sham M", "Zhang", "Tong"], "venue": "Journal of Computer and System Sciences,", "year": 2012}, {"title": "Observable operator models for discrete stochastic time series", "authors": ["Jaeger", "Herbert"], "venue": "Neural Computation,", "year": 2000}, {"title": "Uniform chernoff and dvoretzky-kiefer-wolfowitz-type inequalities for markov chains and related processes", "authors": ["Kontorovich", "Aryeh", "Weiss", "Roi"], "venue": "Journal of Applied Probability,", "year": 2014}, {"title": "Concentration inequalities for dependent random variables via the martingale method", "authors": ["Kontorovich", "Leonid Aryeh", "Ramanan", "Kavita"], "venue": "The Annals of Probability,", "year": 2008}, {"title": "Geometric ergodicity and the spectral gap of non-reversible markov chains", "authors": ["Kontoyiannis", "Ioannis", "Meyn", "Sean P"], "venue": "Probability Theory and Related Fields,", "year": 2012}, {"title": "Low-rank spectral learning with weighted loss functions", "authors": ["Kulesza", "Alex", "Jiang", "Nan", "Singh", "Satinder"], "venue": "In Artificial Intelligence and Statistics,", "year": 2015}, {"title": "Learning multi-step predictive state representations", "authors": ["Langer", "Lucas", "Balle", "Borja", "Precup", "Doina"], "venue": "In Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence,", "year": 2016}, {"title": "Convergence rates for markov chains", "authors": ["Rosenthal", "Jeffrey S"], "venue": "Siam Review,", "year": 1995}, {"title": "Learning latent variable models by improving spectral solutions with exterior point method", "authors": ["Shaban", "Amirreza", "Farajtabar", "Mehrdad", "Xie", "Bo", "Song", "Le", "Boots", "Byron"], "venue": "In UAI, pp", "year": 2015}, {"title": "Links between multiplicity automata, observable operator models and predictive state representations\u2013a unified learning framework", "authors": ["Thon", "Michael", "Jaeger", "Herbert"], "venue": "Journal of Machine Learning Research,", "year": 2015}], "id": "SP:7e2991cf378fae265b9a0719d154d18354df70ce", "authors": [{"name": "Borja Balle", "affiliations": []}, {"name": "Odalric-Ambrym Maillard", "affiliations": []}], "abstractText": "We present spectral methods of moments for learning sequential models from a single trajectory, in stark contrast with the classical literature that assumes the availability of multiple i.i.d. trajectories. Our approach leverages an efficient SVD-based learning algorithm for weighted automata and provides the first rigorous analysis for learning many important models using dependent data. We state and analyze the algorithm under three increasingly difficult scenarios: probabilistic automata, stochastic weighted automata, and reactive predictive state representations controlled by a finite-state policy. Our proofs include novel tools for studying mixing properties of stochastic weighted automata.", "title": "Spectral Learning from a Single Trajectory under Finite-State Policies"}