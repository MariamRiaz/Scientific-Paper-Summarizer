13	37	To address the above challenges, we aim at creating a QA dataset that requires reasoning over multiple documents, and does so in natural language, without constraining itself to an existing knowledge base or knowledge schema.
14	47	We also want it to provide the system with strong supervision about what text the answer is actually derived from, to help guide systems to perform meaningful and explainable reasoning.
16	43	HOTPOTQA is collected by crowdsourcing based on Wikipedia articles, where crowd workers are shown multiple supporting context documents and asked explicitly to come up with questions requiring reasoning about all of the documents.
17	31	This ensures it covers multi-hop questions that are more natural, and are not designed with any pre-existing knowledge base schema in mind.
18	35	Moreover, we also ask the crowd workers to provide the supporting facts they use to answer the question, which we also provide as part of the dataset (see Figure 1 for an example).
19	53	We have carefully designed a data collection pipeline for HOTPOTQA, since the collection of high-quality multi-hop questions is nontrivial.
20	17	We hope that this pipeline also sheds light on future work in this direction.
23	12	The main goal of our work is to collect a diverse and explainable question answering dataset that requires multi-hop reasoning.
46	22	In addition to questions collected using bridge entities, we also collect another type of multi-hop questions— comparison questions.
47	13	The main idea is that comparing two entities from the same category usually results in interesting multi-hop questions, e.g., “Who has played for more NBA teams, Michael Jordan or Kobe Bryant?” To facilitate collecting this type of question, we manually curate 42 lists of similar entities (denoted as L) from Wikipedia.3 To generate candidate paragraph pairs, we randomly sample two paragraphs from the same list and present them to the crowd worker.
94	20	We further sample 100 examples from the dataset, and present the types of answers in Table 2.
96	19	We find that a majority of the questions are about entities in the articles (68%), and a non-negligible amount of questions also ask about various properties like date (9%) and other descriptive properties such as numbers (8%) and adjectives (4%).
97	43	We also sampled 100 examples from the dev and test sets and manually classified the types of reasoning required to answer each question.
100	14	A majority of sampled questions (42%) require chain reasoning (Type I in the table), where the reader must first identify a bridge entity before the second hop can be answered by filling in the bridge.
105	11	Here, to answer the question, one could find the set of all entities that satisfy each of the properties mentioned, and take an intersection to arrive at the final answer.
127	38	In the full wiki setting, to enable efficient tfidf retrieval among 5,000,000+ wiki paragraphs, given a question we first return a candidate pool of at most 5,000 paragraphs using an inverted-indexbased filtering strategy6 and then select the top 10 paragraphs in the pool as the final candidates using bigram tf-idf.7 Retrieval performance is shown in Table 5.
132	12	The second set features joint metrics that combine the evaluation of answer spans and supporting facts as follows.
137	19	The performance of our model on the benchmark settings is reported in Table 4, where all numbers are obtained with strong supervision over supporting facts.
141	19	We also investigate the explainability of our model by measuring supporting fact prediction performance.
148	12	Combined with the retrieval performance in Table 5, we believe that the deterioration in the full wiki setting in Table 4 is largely due to the difficulty of retrieving both entities.
156	35	To establish human performance on our dataset, we randomly sampled 1,000 examples from the dev and test sets, and had at least three additional Turkers provide answers and supporting facts for these examples.
160	68	If the baseline model were provided with the correct supporting paragraphs to begin with, it achieves parity with the crowd worker in finding supporting facts, but still falls short at finding the actual answer.
178	45	We present HOTPOTQA, a large-scale question answering dataset aimed at facilitating the development of QA systems capable of performing explainable, multi-hop reasoning over diverse natural language.
188	17	To make sure the sampled candidate paragraph pairs are intuitive for crowd workers to ask high-quality multi-hop questions about, we manually curate 591 categories from the lists of popular pages by WikiProject.9 For each category, we sample (a, b) pairs from the graph G where b is in the considered category, and manually check whether a multi-hop question can be asked given the pair (a, b).
194	18	A.3 Crowd Worker Interface Our crowd worker interface is based on ParlAI (Miller et al., 2017), an open-source project that facilitates the development of dialog systems and data collection with a dialog interface.
195	34	We adapt ParlAI for collecting question answer pairs by converting the collection workflow into a systemoriented dialog.
199	28	Besides being diverse in terms of types as is show in the main text, questions also vary greatly in length, indicating different levels of complexity and details covered.
202	67	For some of the question q, its corresponding gold para- Algorithm 2 Inverted Index Filtering Strategy Input: question text q, control threshold N , ngram-toWikidoc inverted index D Inintialize: Extract unigram + bigram set rq from q Ncand = +∞ Cgram = 0 while Ncands > N do Cgram = Cgram + 1 Set Soverlap to be an empty dictionary for w ∈ rq do for d ∈ D[w] do if d not in Soverlap then Soverlap[d] = 1 else Soverlap[d] = Soverlap[d] + 1 end if end for end for Scand = ∅ for d in Soverlap do if Soverlap[d] ≥ Cgram then Scand = Scand ∪ {d} end if end for Ncands = |Scand| end while return Scand graphs may not be included in the output candidate pool Scand, we set such missing gold paragraph’s rank as |Scand|+1 during the evaluation, so MAP and Mean Rank reported in this paper are upper bounds of their true values.
203	32	C.2 Compare train-medium Split to Hard Ones Table 9 shows the comparison between trainmedium split and hard examples like dev and test under retrieval metrics in full wiki setting.
204	36	As we can see, the performance gap between trainmedium split and its dev/test is close, which implies that train-medium split has a similar level of difficulty as hard examples under the full wiki setting in which a retrieval model is necessary as the first processing step.
