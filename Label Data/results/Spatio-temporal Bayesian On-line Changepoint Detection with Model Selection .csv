0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914–1925 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1914",text,[0],[0]
Deep learning models work best when trained on large amounts of labeled data.,1 Introduction,[0],[0]
"However, acquiring labels is costly, motivating the need for effective semi-supervised learning techniques that leverage unlabeled examples.",1 Introduction,[0],[0]
"A widely successful semi-supervised learning strategy for neural NLP is pre-training word vectors (Mikolov et al., 2013).",1 Introduction,[0],[0]
"More recent work trains a Bi-LSTM sentence encoder to do language modeling and then incorporates its context-sensitive representations into supervised models (Dai and Le, 2015; Peters et al.,
1Code will be made available at https: //github.com/tensorflow/models/tree/ master/research/cvt_text
2018).",1 Introduction,[0],[0]
"Such pre-training methods perform unsupervised representation learning on a large corpus of unlabeled data followed by supervised training.
",1 Introduction,[0],[0]
A key disadvantage of pre-training is that the first representation learning phase does not take advantage of labeled data – the model attempts to learn generally effective representations rather than ones that are targeted towards a particular task.,1 Introduction,[0],[0]
Older semi-supervised learning algorithms like self-training do not suffer from this problem because they continually learn about a task on a mix of labeled and unlabeled data.,1 Introduction,[0],[0]
"Selftraining has historically been effective for NLP (Yarowsky, 1995; McClosky et al., 2006), but is less commonly used with neural models.",1 Introduction,[0],[0]
"This paper presents Cross-View Training (CVT), a new self-training algorithm that works well for neural sequence models.
",1 Introduction,[0],[0]
"In self-training, the model learns as normal on labeled examples.",1 Introduction,[0],[0]
"On unlabeled examples, the model acts as both a teacher that makes predictions about the examples and a student that is trained on those predictions.",1 Introduction,[0],[0]
"Although this process has shown value for some tasks, it is somewhat tautological: the model already produces the predictions it is being trained on.",1 Introduction,[0],[0]
"Recent research on computer vision addresses this by adding noise to the student’s input, training the model so it is robust to input perturbations (Sajjadi et al., 2016; Wei et al., 2018).",1 Introduction,[0],[0]
"However, applying noise is difficult for discrete inputs like text.
",1 Introduction,[0],[0]
"As a solution, we take inspiration from multiview learning (Blum and Mitchell, 1998; Xu et al., 2013) and train the model to produce consistent predictions across different views of the input.",1 Introduction,[0],[0]
"Instead of only training the full model as a student, CVT adds auxiliary prediction modules – neural networks that transform vector representations into predictions – to the model and also trains them as students.",1 Introduction,[0],[0]
"The input to each student prediction module is a subset of the model’s intermediate rep-
resentations corresponding to a restricted view of the input example.",1 Introduction,[0],[0]
"For example, one auxiliary prediction module for sequence tagging is attached to only the “forward” LSTM in the model’s first BiLSTM layer, so it makes predictions without seeing any tokens to the right of the current one.
",1 Introduction,[0],[0]
CVT works by improving the model’s representation learning.,1 Introduction,[0],[0]
"The auxiliary prediction modules can learn from the full model’s predictions because the full model has a better, unrestricted view of the input.",1 Introduction,[0],[0]
"As the auxiliary modules learn to make accurate predictions despite their restricted views of the input, they improve the quality of the representations they are built on top of.",1 Introduction,[0],[0]
"This in turn improves the full model, which uses the same shared representations.",1 Introduction,[0],[0]
"In short, our method combines the idea of representation learning on unlabeled data with classic self-training.
",1 Introduction,[0],[0]
"CVT can be applied to a variety of tasks and neural architectures, but we focus on sequence modeling tasks where the prediction modules are attached to a shared Bi-LSTM encoder.",1 Introduction,[0],[0]
"We propose auxiliary prediction modules that work well for sequence taggers, graph-based dependency parsers, and sequence-to-sequence models.",1 Introduction,[0],[0]
"We evaluate our approach on English dependency parsing, combinatory categorial grammar supertagging, named entity recognition, partof-speech tagging, and text chunking, as well as English to Vietnamese machine translation.",1 Introduction,[0],[0]
CVT improves over previously published results on all these tasks.,1 Introduction,[0],[0]
"Furthermore, CVT can easily and effectively be combined with multi-task learning: we just add additional prediction modules for the different tasks on top of the shared Bi-LSTM encoder.",1 Introduction,[0],[0]
Training a unified model to jointly perform all of the tasks except machine translation improves results (outperforming a multi-task ELMo model) while decreasing the total training time.,1 Introduction,[0],[0]
We first present Cross-View Training and describe how it can be combined effectively with multi-task learning.,2 Cross-View Training,[0],[0]
See Figure 1 for an overview of the training method.,2 Cross-View Training,[0],[0]
"Let Dl = {(x1, y1), (x2, y2), ..., (xN , yN )} represent a labeled dataset and Dul = {x1, x2, ..., xM} represent an unlabeled dataset We use pθ(y|xi) to denote the output distribution over classes pro-
duced by the model with parameters θ on input xi.",2.1 Method,[0],[0]
"During CVT, the model alternates learning on a minibatch of labeled examples and learning on a minibatch of unlabeled examples.",2.1 Method,[0],[0]
"For labeled examples, CVT uses standard cross-entropy loss:
Lsup(θ) = 1 |Dl| ∑
xi,yi∈Dl
CE(yi, pθ(y|xi))
",2.1 Method,[0],[0]
"CVT adds k auxiliary prediction modules to the model, which are used when learning on unlabeled examples.",2.1 Method,[0],[0]
"A prediction module is usually a small neural network (e.g., a hidden layer followed by a softmax layer).",2.1 Method,[0],[0]
"Each one takes as input an intermediate representation hj(xi) produced by the model (e.g., the outputs of one of the LSTMs in a Bi-LSTM model).",2.1 Method,[0],[0]
It outputs a distribution over labels pjθ(y|xi).,2.1 Method,[0],[0]
"Each h
j is chosen such that it only uses a part of the input xi; the particular choice
can depend on the task and model architecture.",2.1 Method,[0],[0]
We propose variants for several tasks in Section 3.,2.1 Method,[0],[0]
"The auxiliary prediction modules are only used during training; the test-time prediction come from the primary prediction module that produces pθ.
",2.1 Method,[0],[0]
"On an unlabeled example, the model first produces soft targets pθ(y|xi) by performing inference.",2.1 Method,[0],[0]
"CVT trains the auxiliary prediction modules to match the primary prediction module on the unlabeled data by minimizing LCVT(θ) = 1|Dul| ∑ xi∈Dul ∑k j=1D(pθ(y|xi), p j θ(y|xi))
where D is a distance function between probability distributions (we use KL divergence).",2.1 Method,[0],[0]
"We hold the primary module’s prediction pθ(y|xi) fixed during training (i.e., we do not back-propagate through it) so the auxiliary modules learn to imitate the primary one, but not vice versa.",2.1 Method,[0],[0]
CVT works by enhancing the model’s representation learning.,2.1 Method,[0],[0]
"As the auxiliary modules train, the representations they take as input improve so they are useful for making predictions even when some of the model’s inputs are not available.",2.1 Method,[0],[0]
"This in turn improves the primary prediction module, which is built on top of the same shared representations.
",2.1 Method,[0],[0]
"We combine the supervised and CVT losses into the total loss, L = Lsup + LCVT, and minimize it with stochastic gradient descent.",2.1 Method,[0],[0]
"In particular, we alternate minimizing Lsup over a minibatch of labeled examples and minimizing LCVT over a minibatch of unlabeled examples.
",2.1 Method,[0],[0]
"For most neural networks, adding a few additional prediction modules is computationally cheap compared to the portion of the model building up representations (such as an RNN or CNN).",2.1 Method,[0],[0]
Therefore our method contributes little overhead to training time over other self-training approaches for most tasks.,2.1 Method,[0],[0]
CVT does not change inference time or the number of parameters in the fullytrained model because the auxiliary prediction modules are only used during training.,2.1 Method,[0],[0]
CVT can easily be combined with multi-task learning by adding additional prediction modules for the other tasks on top of the shared Bi-LSTM encoder.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"During supervised learning, we randomly select a task and then update Lsup using a minibatch of labeled data for that task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"When learning on the unlabeled data, we optimize LCVT
jointly across all tasks at once, first running inference with all the primary prediction modules and then learning from the predictions with all the auxiliary prediction modules.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"As before, the model alternates training on minibatches of labeled and unlabeled examples.
",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Examples labeled across many tasks are useful for multi-task systems to learn from, but most datasets are only labeled with one task.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
A benefit of multi-task CVT is that the model creates (artificial) all-tasks-labeled examples from unlabeled data.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
This significantly improves the model’s data efficiency and training time.,2.2 Combining CVT with Multi-Task Learning,[0],[0]
"Since running prediction modules is computationally cheap, computing LCVT is not much slower for many tasks than it is for a single one.",2.2 Combining CVT with Multi-Task Learning,[0.9598916849543875],"['Since not even a regular grid is decomposable, this is problematic for spatial data.']"
"However, we find the all-tasks-labeled examples substantially speed up model convergence.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
"For example, our model trained on six tasks takes about three times as long to converge as the average model trained on one task, a 2x decrease in total training time.",2.2 Combining CVT with Multi-Task Learning,[0],[0]
CVT relies on auxiliary prediction modules that have restricted views of the input.,3 Cross-View Training Models,[0],[0]
"In this section, we describe specific constructions of the auxiliary prediction modules that are effective for sequence tagging, dependency parsing, and sequence-tosequence learning.",3 Cross-View Training Models,[0],[0]
"All of our models use a two-layer CNN-BiLSTM (Chiu and Nichols, 2016; Ma and Hovy, 2016) sentence encoder.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
It takes as input a sequence of words xi =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[x1i , x 2 i , ..., x T i ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"First, each word is represented as the sum of an embedding vector and the output of a character-level Convolutional Neural Network, resulting in a sequence of vectors v =",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[v1, v2, ..., vT ].",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The encoder applies a twolayer bidirectional LSTM (Graves and Schmidhuber, 2005) to these representations.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The first layer runs a Long Short-Term Memory unit (Hochreiter and Schmidhuber, 1997) in the forward direction (taking vt as input at each step t) and the backward direction (taking vT−t+1 at each step) to produce vector sequences [ −→ h 11, −→ h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"−→ h T1 ] and [ ←− h 11, ←− h 21, ...",3.1 Bi-LSTM Sentence Encoder,[0],[0]
←− h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
The output of the Bi-LSTM is the concatenation of these vectors: h1 =,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"[ −→ h 11 ⊕←−
h 11, ..., −→ h T1 ⊕ ←−",3.1 Bi-LSTM Sentence Encoder,[0],[0]
h T1 ].,3.1 Bi-LSTM Sentence Encoder,[0],[0]
"The second Bi-LSTM layer
works the same, producing outputs h2, except it takes h1 as input instead of v.",3.1 Bi-LSTM Sentence Encoder,[0],[0]
"In sequence tagging, each token xti has a corresponding label yti .",3.2 CVT for Sequence Tagging,[0],[0]
"The primary prediction module for sequence tagging produces a probability distribution over classes for the tth label using a onehidden-layer neural network applied to the corresponding encoder outputs:
p(yt|xi) = NN(ht1 ⊕ ht2) = softmax(U · ReLU(W (ht1 ⊕ ht2))",3.2 CVT for Sequence Tagging,[0],[0]
"+ b)
",3.2 CVT for Sequence Tagging,[0],[0]
"The auxiliary prediction modules take −→ h 1(xi) and ←− h 1(xi), the outputs of the forward and backward LSTMs in the first2 Bi-LSTM layer, as inputs.",3.2 CVT for Sequence Tagging,[0],[0]
"We add the following four auxiliary prediction modules to the model (see Figure 2):
pfwdθ (y t|xi)",3.2 CVT for Sequence Tagging,[0],[0]
= NNfwd( −→ h t1(xi)),3.2 CVT for Sequence Tagging,[0],[0]
pbwdθ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
"NNbwd( ←− h t1(xi))
",3.2 CVT for Sequence Tagging,[0],[0]
pfutureθ,3.2 CVT for Sequence Tagging,[0],[0]
"(y t|xi) = NNfuture( −→ h t−11 (xi))
",3.2 CVT for Sequence Tagging,[0],[0]
p past θ (y t|xi) =,3.2 CVT for Sequence Tagging,[0],[0]
NNpast( ←− h,3.2 CVT for Sequence Tagging,[0],[0]
"t+11 (xi))
The “forward” module makes each prediction without seeing the right context of the current token.",3.2 CVT for Sequence Tagging,[0],[0]
The “future” module makes each prediction without the right context or the current token itself.,3.2 CVT for Sequence Tagging,[0],[0]
"Therefore it works like a neural language model that, instead of predicting which token comes next in the sequence, predicts which class of token comes next.",3.2 CVT for Sequence Tagging,[0],[0]
The “backward” and “past” modules are analogous.,3.2 CVT for Sequence Tagging,[0],[0]
"In a dependency parse, words in a sentence are treated as nodes in a graph.",3.3 CVT for Dependency Parsing,[0],[0]
"Typed directed edges connect the words, forming a tree structure describing the syntactic structure of the sentence.",3.3 CVT for Dependency Parsing,[0],[0]
"In particular, each word xti in a sentence",3.3 CVT for Dependency Parsing,[0],[0]
"xi = x 1 i , ..., x T i receives exactly one in-going edge (u, t, r) going from word xui (called the “head”) to it (the “dependent”) of type r (the “relation”).",3.3 CVT for Dependency Parsing,[0],[0]
We use a graph-based dependency parser similar to the one from Dozat and Manning (2017).,3.3 CVT for Dependency Parsing,[0],[0]
This treats dependency parsing as a classification task where the goal is to predict which in-going edge yti =,3.3 CVT for Dependency Parsing,[0],[0]
"(u, t, r) connects to each word x t i.
First, the representations produced by the encoder for the candidate head and dependent are
2Modules taking inputs from the second Bi-LSTM layer would not have restricted views because information about the whole sentence gets propagated through the first layer.
",3.3 CVT for Dependency Parsing,[0],[0]
"LSTM LSTM ŷfuture  ŷfwd  ŷ   ŷbwd  ŷpast  Backward LSTM Forward LSTM Predict LSTM LSTM LSTM LSTM Auxiliary Prediction Modules Primary Prediction Module
passed through separate hidden layers.",3.3 CVT for Dependency Parsing,[0],[0]
A bilinear classifier applied to these representations produces a score for each candidate edge.,3.3 CVT for Dependency Parsing,[0],[0]
"Lastly, these scores are passed through a softmax layer to produce probabilities.",3.3 CVT for Dependency Parsing,[0],[0]
"Mathematically, the probability of an edge is given as:
pθ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es(h u 1 (xi)⊕hu2 (xi),ht1(xi)⊕ht2(xi),r)
where s is the scoring function:
s(z1, z2, r) = ReLU(Wheadz1 + bhead)(Wr",3.3 CVT for Dependency Parsing,[0],[0]
"+W )
",3.3 CVT for Dependency Parsing,[0],[0]
"ReLU(Wdepz2 + bdep)
",3.3 CVT for Dependency Parsing,[0],[0]
The bilinear classifier uses a weight matrix Wr specific to the candidate relation as well as a weight matrix W shared across all relations.,3.3 CVT for Dependency Parsing,[0],[0]
"Note that unlike in most prior work, our dependency parser only takes words as inputs, not words and part-of-speech tags.
",3.3 CVT for Dependency Parsing,[0],[0]
"We add four auxiliary prediction modules to our model for cross-view training:
pfwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-fwd(
−→ h u1 (xi), −→ h t1(xi),r)
pfwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es fwd-bwd(
−→ h u1 (xi), ←− h t1(xi),r)
pbwd-fwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-fwd(
←− h u1 (xi), −→ h t1(xi),r)
pbwd-bwdθ ((u, t, r)|xi) ∝",3.3 CVT for Dependency Parsing,[0],[0]
"es bwd-bwd(
←− h u1 (xi), ←− h t1(xi),r)
",3.3 CVT for Dependency Parsing,[0],[0]
Each one has some missing context (not seeing either the preceding or following words) for the candidate head and candidate dependent.,3.3 CVT for Dependency Parsing,[0],[0]
"We use an encoder-decoder sequence-to-sequence model with attention (Sutskever et al., 2014; Bahdanau et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Each example consists of an input (source) sequence xi = x1i , ..., x T i and output (target) sequence yi = y1i , ..., y K i .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The encoder’s representations are passed into an LSTM decoder using a bilinear attention mechanism (Luong et al., 2015).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"In particular, at each time step t the decoder computes an attention distribution over source sequence hidden states as αj ∝ eh
jWαh̄t where h̄t is the decoder’s current hidden state.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The source hidden states weighted by the attention distribution form a context vector: ct = ∑ j αjh
j .",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Next, the context vector and current hidden state are combined into an attention vector at = tanh(Wa[ct, ht]).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Lastly, a softmax layer predicts the next token in the output sequence: p(yti |y<ti , xi) = softmax(Wsat).
",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
We add two auxiliary decoders when applying CVT.,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The auxiliary decoders share embedding and LSTM parameters with the primary decoder, but have different parameters for the attention mechanisms and softmax layers.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"For the first one, we restrict its view of the input by applying attention dropout, randomly zeroing out a fraction of its attention weights.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"The second one is trained to predict the next word in the target sequence rather than the current one: pfutureθ (y t i |y<ti , xi) = softmax(W futures a future t−1 ).",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Since there is no target sequence for unlabeled examples, we cannot apply teacher forcing to get an output distribution over the vocabulary from the primary decoder at each time step.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"Instead, we produce hard targets for the auxiliary modules by running the primary decoder with beam search on the input sequence.",3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
This idea has previously been applied to sequence-level knowledge distillation by Kim and Rush (2016).,3.4 CVT for Sequence-to-Sequence Learning,[0],[0]
"We compare Cross-View Training against several strong baselines on seven tasks:
Combinatory Categorial Grammar (CCG)",4 Experiments,[0],[0]
"Supertagging: We use data from CCGBank (Hockenmaier and Steedman, 2007).
",4 Experiments,[0],[0]
"Text Chunking: We use the CoNLL-2000 data (Tjong Kim Sang and Buchholz, 2000).
",4 Experiments,[0],[0]
"Named Entity Recognition (NER): We use the CoNLL-2003 data (Tjong Kim Sang and De Meulder, 2003).
",4 Experiments,[0],[0]
"Fine-Grained NER (FGN): We use the OntoNotes (Hovy et al., 2006) dataset.
",4 Experiments,[0],[0]
Part-of-Speech (POS),4 Experiments,[0],[0]
"Tagging: We use the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993).
",4 Experiments,[0],[0]
"Dependency Parsing: We use the Penn Treebank converted to Stanford Dependencies version 3.3.0.
Machine Translation: We use the EnglishVietnamese translation dataset from IWSLT 2015 (Cettolo et al., 2015).",4 Experiments,[0],[0]
"We report (tokenized) BLEU scores on the tst2013 test set.
",4 Experiments,[0],[0]
"We use the 1 Billion Word Language Model Benchmark (Chelba et al., 2014) as a pool of unlabeled sentences for semi-supervised learning.",4 Experiments,[0],[0]
"We apply dropout during training, but not when running the primary prediction module to produce soft targets on unlabeled examples.",4.1 Model Details and Baselines,[0],[0]
"In addition to the auxiliary prediction modules listed in Section 3, we find it slightly improves results to add another one that sees the whole input rather than a subset (but unlike the primary prediction module, does have dropout applied to its representations).",4.1 Model Details and Baselines,[0],[0]
"Unless indicated otherwise, our models have LSTMs with 1024-sized hidden states and 512-sized projection layers.",4.1 Model Details and Baselines,[0],[0]
See the supplementary material for full training details and hyperparameters.,4.1 Model Details and Baselines,[0],[0]
"We compare CVT with the following other semi-supervised learning algorithms:
Word Dropout.",4.1 Model Details and Baselines,[0],[0]
"In this method, we only train the primary prediction module.",4.1 Model Details and Baselines,[0],[0]
"When acting as a teacher it is run as normal, but when acting as a student, we randomly replace some of the input words with a REMOVED token.",4.1 Model Details and Baselines,[0],[0]
This is similar to CVT in that it exposes the model to a restricted view of the input.,4.1 Model Details and Baselines,[0],[0]
"However, it is less data efficient.",4.1 Model Details and Baselines,[0],[0]
"By carefully designing the auxiliary prediction modules, it is possible to train the auxiliary prediction modules to match the primary one across many different views of the input a once, rather than just one view at a time.
",4.1 Model Details and Baselines,[0],[0]
Virtual Adversarial Training (VAT).,4.1 Model Details and Baselines,[0],[0]
"VAT (Miyato et al., 2016) works like word dropout, but adds noise to the word embeddings of the student instead of dropping out words.",4.1 Model Details and Baselines,[0],[0]
"Notably, the noise is chosen adversarially so it most changes the model’s prediction.",4.1 Model Details and Baselines,[0],[0]
"This method was applied successfully to semi-supervised text classification
by Miyato et al. (2017).
ELMo.",4.1 Model Details and Baselines,[0],[0]
ELMo incorporates the representations from a large separately-trained language model into a task-specific model.,4.1 Model Details and Baselines,[0],[0]
Our implementaiton follows Peters et al. (2018).,4.1 Model Details and Baselines,[0],[0]
"When combining ELMo with multi-task learning, we allow each task to learn its own weights for the ELMo embeddings going into each prediction module.",4.1 Model Details and Baselines,[0],[0]
We found applying dropout to the ELMo embeddings was crucial for achieving good performance.,4.1 Model Details and Baselines,[0],[0]
Results are shown in Table 1.,4.2 Results,[0],[0]
CVT on its own outperforms or is comparable to the best previously published results on all tasks.,4.2 Results,[0],[0]
"Figure 3 shows an example win for CVT over supervised learning.
",4.2 Results,[0],[0]
"Of the prior results listed in Table 1, only TagLM and ELMo are semi-supervised.",4.2 Results,[0],[0]
These methods first train an enormous language model on unlabeled data and incorporate the representations produced by the language model into a supervised classifier.,4.2 Results,[0],[0]
"Our base models use 1024 hidden units in their LSTMs (compared to 4096 in ELMo), require fewer training steps (around one pass over the billion-word benchmark rather than
many passes), and do not require a pipelined training procedure.",4.2 Results,[0],[0]
"Therefore, although they perform on par with ELMo, they are faster and simpler to train.",4.2 Results,[0],[0]
Increasing the size of our CVT+Multitask model so it has 4096 units in its LSTMs like ELMo improves results further so they are significantly better than the ELMo+Multi-task ones.,4.2 Results,[0],[0]
"We suspect there could be further gains from combining our method with language model pre-training, which we leave for future work.
",4.2 Results,[0],[0]
CVT + Multi-Task.,4.2 Results,[0],[0]
We train a single sharedencoder CVT model to perform all of the tasks except machine translation (as it is quite different and requires more training time than the other ones).,4.2 Results,[0],[0]
"Multi-task learning improves results on all of the tasks except fine-grained NER, sometimes by large margins.",4.2 Results,[0],[0]
Prior work on many-task NLP such as Hashimoto et al. (2017) uses complicated architectures and training algorithms.,4.2 Results,[0],[0]
"Our result shows that simple parameter sharing can be enough for effective many-task learning when the model is big and trained on a large amount of data.
",4.2 Results,[0],[0]
"Interestingly, multi-task learning works better in conjunction with CVT than with ELMo.",4.2 Results,[0],[0]
"We hypothesize that the ELMo models quickly fit to the data primarily using the ELMo vectors, which perhaps hinders the model from learning effective representations that transfer across tasks.",4.2 Results,[0],[0]
"We also believe CVT alleviates the danger of the model “forgetting” one task while training on the other ones, a well-known problem in many-task learning (Kirkpatrick et al., 2017).",4.2 Results,[0],[0]
"During multi-task CVT, the model makes predictions about unlabeled examples across all tasks, creating (artificial) all-tasks-labeled examples, so the model does not only see one task at a time.",4.2 Results,[0],[0]
"In fact, multi-task learning plus self training is similar to the Learning without Forgetting algorithm (Li and Hoiem, 2016), which trains the model to keep its predictions on an old task unchanged when learning a new task.",4.2 Results,[0],[0]
"To test the value of all-tasks-labeled examples, we trained a multi-task CVT model that only computes LCVT on one task at a time (chosen randomly for each unlabeled minibatch) instead of for all tasks in parallel.",4.2 Results,[0],[0]
"The one-at-a-time model performs substantially worse (see Table 2).
",4.2 Results,[0],[0]
Model Generalization.,4.2 Results,[0],[0]
"In order to evaluate how our models generalize to the dev set from the train set, we plot the dev vs. train accuracy for our different methods as they learn (see Figure 4).",4.2 Results,[0],[0]
"Both CVT and multi-task learning improve model generalization: for the same train accuracy, the models get better dev accuracy than purely supervised learning.",4.2 Results,[0],[0]
"Interestingly, CVT continues to improve
in dev set accuracy while close to 100% train accuracy for CCG, Chunking, and NER, perhaps because the model is still learning from unlabeled data even when it has completely fit to the train set.",4.2 Results,[0],[0]
We also show results for a smaller multi-task + CVT model.,4.2 Results,[0],[0]
"Although it generalizes at least as well as the larger one, it halts making progress on the train set earlier.",4.2 Results,[0],[0]
"This suggests it is important to use sufficiently large neural networks for multitask learning: otherwise the model does not have the capacity to fit to all the training data.
",4.2 Results,[0],[0]
Auxiliary Prediction Module Ablation.,4.2 Results,[0],[0]
We briefly explore which auxiliary prediction modules are more important for the sequence tagging tasks in Table 3.,4.2 Results,[0],[0]
"We find that both kinds of auxiliary prediction modules improve performance, but that the future and past modules improve results more than the forward and backward ones, perhaps because they see a more restricted and challenging view of the input.
",4.2 Results,[0],[0]
Training Models on Small Datasets.,4.2 Results,[0],[0]
"We explore how CVT scales with dataset size by varying the amount of training data the model has ac-
cess to.",4.2 Results,[0],[0]
"Unsurprisingly, the improvement of CVT over purely supervised learning grows larger as the amount of labeled data decreases (see Figure 5, left).",4.2 Results,[0],[0]
"Using only 25% of the labeled data, our approach already performs as well or better than a fully supervised model using 100% of the training data, demonstrating that CVT is particularly useful on small datasets.
",4.2 Results,[0],[0]
Training Larger Models.,4.2 Results,[0],[0]
"Most sequence taggers and dependency parsers in prior work use small LSTMs (hidden state sizes of around 300) because larger models yield little to no gains in performance (Reimers and Gurevych, 2017).",4.2 Results,[0],[0]
We found our own supervised approaches also do not benefit greatly from increasing the model size.,4.2 Results,[0],[0]
"In contrast, when using CVT accuracy scales better with model size (see Figure 5, right).",4.2 Results,[0],[0]
"This finding suggests the appropriate semi-supervised learning methods may enable the development of larger, more sophisticated models for NLP tasks with limited amounts of labeled data.
",4.2 Results,[0],[0]
Generalizable Representations.,4.2 Results,[0],[0]
"Lastly, we explore training the CVT+multi-task model on five tasks, freezing the encoder, and then only training a prediction module on the sixth task.",4.2 Results,[0],[0]
This tests whether the encoder’s representations generalize to a new task not seen during its training.,4.2 Results,[0],[0]
Only training the prediction module is very fast because (1) the encoder (which is by far the slowest part of the model) has to be run over each example only once and (2) we do not back-propagate into the encoder.,4.2 Results,[0],[0]
"Results are shown in Table 4.
",4.2 Results,[0],[0]
"Training only a prediction module on top of multi-task representations works remarkably well,
outperforming ELMo embeddings and sometimes even a vanilla supervised model, showing the multi-task model is building up effective representations for language.",4.2 Results,[0],[0]
"In particular, the representations could be used like skip-thought vectors (Kiros et al., 2015) to quickly train models on new tasks without slow representation learning.",4.2 Results,[0],[0]
Unsupervised Representation Learning.,5 Related Work,[0],[0]
"Early approaches to deep semi-supervised learning pretrain neural models on unlabeled data, which has been successful for applications in computer vision (Jarrett et al., 2009; LeCun et al., 2010) and NLP.",5 Related Work,[0],[0]
"Particularly noteworthy for NLP are algorithms for learning effective word embeddings (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) and language model pretraining (Dai and Le, 2015; Ramachandran et al., 2017; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018).",5 Related Work,[0],[0]
"Pre-training on other tasks such as machine translation has also been studied (McCann et al., 2017).",5 Related Work,[0],[0]
"Other approaches train
“thought vectors” representing sentences through unsupervised (Kiros et al., 2015; Hill et al., 2016) or supervised (Conneau et al., 2017) learning.
",5 Related Work,[0],[0]
Self-Training.,5 Related Work,[0],[0]
"One of the earliest approaches to semi-supervised learning is self-training (Scudder, 1965), which has been successfully applied to NLP tasks such as word-sense disambiguation (Yarowsky, 1995) and parsing (McClosky et al., 2006).",5 Related Work,[0],[0]
"In each round of training, the classifier, acting as a “teacher,” labels some of the unlabeled data and adds it to the training set.",5 Related Work,[0],[0]
"Then, acting as a “student,” it is retrained on the new training set.",5 Related Work,[0],[0]
"Many recent approaches (including the consistentency regularization methods discussed below and our own method) train the student with soft targets from the teacher’s output distribution rather than a hard label, making the procedure more akin to knowledge distillation (Hinton et al., 2015).",5 Related Work,[0],[0]
"It is also possible to use multiple models or prediction modules for the teacher, such as in tri-training (Zhou and Li, 2005; Ruder and Plank, 2018).
",5 Related Work,[0],[0]
Consistency Regularization.,5 Related Work,[0],[0]
"Recent works add noise (e.g., drawn from a Gaussian distribution) or apply stochastic transformations (e.g., horizontally flipping an image) to the student’s inputs.",5 Related Work,[0],[0]
"This trains the model to give consistent predictions to nearby data points, encouraging distributional smoothness in the model.",5 Related Work,[0],[0]
"Consistency regularization has been very successful for computer vision applications (Bachman et al., 2014; Laine and Aila, 2017; Tarvainen and Valpola, 2017).",5 Related Work,[0],[0]
"However, stochastic input alterations are more difficult to apply to discrete data like text, making consistency regularization less used for natural language processing.",5 Related Work,[0],[0]
"One solution is to add noise to the model’s word embeddings (Miyato et al., 2017); we compare against this approach in our experiments.",5 Related Work,[0],[0]
"CVT is easily applicable to text because it does not require changing the student’s inputs.
",5 Related Work,[0],[0]
Multi-View Learning.,5 Related Work,[0],[0]
"Multi-view learning on data where features can be separated into distinct subsets has been well studied (Xu et al., 2013).",5 Related Work,[0],[0]
"Particularly relevant are co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Belkin, 2005), which trains two models with disjoint views of the input.",5 Related Work,[0],[0]
"On unlabeled data, each one acts as a “teacher” for the other model.",5 Related Work,[0],[0]
"In contrast to these methods, our approach trains a single unified model where auxiliary prediction modules see different, but not necessarily indepen-
dent views of the input.
",5 Related Work,[0],[0]
Self Supervision.,5 Related Work,[0],[0]
Self-supervised learning methods train auxiliary prediction modules on tasks where performance can be measured without human-provided labels.,5 Related Work,[0],[0]
"Recent work has jointly trained image classifiers with tasks like relative position and colorization (Doersch and Zisserman, 2017), sequence taggers with language modeling (Rei, 2017), and reinforcement learning agents with predicting changes in the environment (Jaderberg et al., 2017).",5 Related Work,[0],[0]
"Unlike these approaches, our auxiliary losses are based on self-labeling, not labels deterministically constructed from the input.
",5 Related Work,[0],[0]
Multi-Task Learning.,5 Related Work,[0],[0]
"There has been extensive prior work on multi-task learning (Caruana, 1997; Ruder, 2017).",5 Related Work,[0],[0]
"For NLP, most work has focused on a small number of closely related tasks (Luong et al., 2016; Zhang and Weiss, 2016; Søgaard and Goldberg, 2016; Peng et al., 2017).",5 Related Work,[0],[0]
Manytask systems are less commonly developed.,5 Related Work,[0],[0]
"Collobert and Weston (2008) propose a many-task system sharing word embeddings between the tasks, Hashimoto et al. (2017) train a many-task model where the tasks are arranged hierarchically according to their linguistic level, and Subramanian et al. (2018) train a shared-encoder many-task model for the purpose of learning better sentence representations for use in downstream tasks, not for improving results on the original tasks.",5 Related Work,[0],[0]
"We propose Cross-View Training, a new method for semi-supervised learning.",6 Conclusion,[0],[0]
"Our approach allows models to effectively leverage their own predictions on unlabeled data, training them to produce effective representations that yield accurate predictions even when some of the input is not available.",6 Conclusion,[0],[0]
"We achieve excellent results across seven NLP tasks, especially when CVT is combined with multi-task learning.",6 Conclusion,[0],[0]
"We thank Abi See, Christopher Clark, He He, Peng Qi, Reid Pryzant, Yuaho Zhang, and the anonymous reviewers for their thoughtful comments and suggestions.",Acknowledgements,[0],[0]
We thank Takeru Miyato for help with his virtual adversarial training code and Emma Strubell for answering our questions about OntoNotes NER.,Acknowledgements,[0],[0]
Kevin is supported by a Google PhD Fellowship.,Acknowledgements,[0],[0]
"Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text.",abstractText,[0],[0]
"However, the supervised models only learn from taskspecific labeled data during the main training phase.",abstractText,[0],[0]
"We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data.",abstractText,[0],[0]
"On labeled examples, standard supervised learning is used.",abstractText,[0],[0]
"On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input.",abstractText,[0.954635211396052],"['We show this using the example of the subprime mortgage financial crisis: While the RLD of Saatçi et al. (2010) identified only 2 CPS with ground truth and a third unlabelled one during the height of the crisis, BOCPDMS detects a large number of CPS corresponding to ground truths, see Fig.']"
"Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model.",abstractText,[0],[0]
"Moreover, we show that CVT is particularly effective when combined with multitask learning.",abstractText,[0],[0]
"We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.1",abstractText,[0],[0]
Semi-Supervised Sequence Modeling with Cross-View Training,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2009–2019 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2009",text,[0],[0]
"User geolocation, the task of identifying the “home” location of a user, is an integral component of many applications ranging from public health monitoring (Paul and Dredze, 2011; Chon et al., 2015; Yepes et al., 2015) and regional studies of sentiment, to real-time emergency awareness systems (De Longueville et al., 2009; Sakaki et al., 2010), which use social media as an implicit information resource about people.
",1 Introduction,[0],[0]
"Social media services such as Twitter rely on IP addresses, WiFi footprints, and GPS data to geolocate users.",1 Introduction,[0],[0]
"Third-party service providers don’t have easy access to such information, and have to rely on public sources of geolocation information such as the profile location field, which is noisy and difficult to map to a location (Hecht et al., 2011), or geotagged tweets, which are publicly available for only 1% of tweets (Cheng et al., 2010; Morstatter et al., 2013).",1 Introduction,[0],[0]
"The scarcity of publicly available
location information motivates predictive user geolocation from information such as tweet text and social interaction data.
",1 Introduction,[0],[0]
"Most previous work on user geolocation takes the form of either supervised text-based approaches (Wing and Baldridge, 2011; Han et al., 2012) relying on the geographical variation of language use, or graph-based semi-supervised label propagation relying on location homophily in user–user interactions (Davis Jr et al., 2011; Jurgens, 2013).
",1 Introduction,[0],[0]
Both text and network views are critical in geolocating users.,1 Introduction,[0],[0]
"Some users post a lot of local content, but their social network is lacking or is not representative of their location; for them, text is the dominant view for geolocation.",1 Introduction,[0],[0]
"Other users have many local social interactions, and mostly use social media to read other people’s comments, and for interacting with friends.",1 Introduction,[0],[0]
Single-view learning would fail to accurately geolocate these users if the more information-rich view is not present.,1 Introduction,[0],[0]
"There has been some work that uses both the text and network views, but it either completely ignores unlabelled data (Li et al., 2012a; Miura et al., 2017), or just uses unlabelled data in the network view (Rahimi et al., 2015b; Do et al., 2017).",1 Introduction,[0],[0]
"Given that the 1% of geotagged tweets is often used for supervision, it is crucial for geolocation models to be able to leverage unlabelled data, and to perform well under a minimal supervision scenario.
",1 Introduction,[0],[0]
"In this paper, we propose GCN, an end-to-end user geolocation model based on Graph Convolutional Networks (Kipf and Welling, 2017) that jointly learns from text and network information to classify a user timeline into a location.",1 Introduction,[0],[0]
"Our contributions are: (1) we evaluate our model under a minimal supervision scenario which is close to real world applications and show that GCN outperforms two strong baselines; (2) given sufficient supervision, we show that GCN is competitive, although the much simpler MLP-TXT+NET outper-
forms state-of-the-art models; and (3) we show that highway gates play a significant role in controlling the amount of useful neighbourhood smoothing in GCN.1",1 Introduction,[0],[0]
"We propose a transductive multiview geolocation model, GCN, using Graph Convolutional Networks (“GCN”: Kipf and Welling (2017)).",2 Model,[0],[0]
"We also introduce two multiview baselines: MLP-TXT+NET based on concatenation of text and network, and DCCA based on Deep Canonical Correlation Analysis (Andrew et al., 2013).",2 Model,[0],[0]
"Let X ∈ R|U |×|V | be the text view, consisting of the bag of words for each user in U using vocabulary V , and A ∈ 1|U",2.1 Multivew Geolocation,[0],[0]
"|×|U | be the network view, encoding user–user interactions.",2.1 Multivew Geolocation,[0],[0]
"We partition U = US ∪ UH into a supervised and heldout (unlabelled) set, US and UH , respectively.",2.1 Multivew Geolocation,[0],[0]
"The goal is to infer the location of unlabelled samples YU , given the location of labelled samples YS , where each location is encoded as a one-hot classification label, yi ∈ 1c with c being the number of target regions.
2.2 GCN GCN defines a neural network model f(X,A) with each layer:
Â = D̃− 1 2 (A+ λI)D̃− 1 2 H(l+1) = σ",2.1 Multivew Geolocation,[0],[0]
"( ÂH(l)W (l) + b ) ,
(1)
where D̃ is the degree matrix of A + λI; hyperparameter λ controls the weight of a node against its neighbourhood, which is set to 1 in the original model (Kipf and Welling, 2017); H0 = X and the din × dout matrix W (l) and dout ×",2.1 Multivew Geolocation,[0],[0]
1 matrix b are trainable layer parameters; and σ is an arbitrary nonlinearity.,2.1 Multivew Geolocation,[0],[0]
"The first layer takes an average of each sample and its immediate neighbours (labelled and unlabelled) using weights in Â, and performs a linear transformation using W and b followed by a nonlinear activation function (σ).",2.1 Multivew Geolocation,[0],[0]
"In other words, for user ui, the output of layer l is computed by:
~hl+1i = σ",2.1 Multivew Geolocation,[0],[0]
"(∑ j∈nhood(i) Âij~h l jW l + bl ) , (2)
1Code and data available at https://github.com/ afshinrahimi/geographconv
Highway GCN:
Highway GCN: ,
Output GCN:
X = BoWtext
Â
Â
Â tanh
tanh
softmax
H0
H1
Hl−1
Hl
predict location: ŷ
W l−1, bl−1, W l−1h , b l−1 h
W 1, b1, W 1h , b 1 h
W l, bl
Figure 1: The architecture of GCN geolocation model with layer-wise highway gates (W ih, b i h).",2.1 Multivew Geolocation,[0],[0]
"GCN is applied to a BoW model of user content over the @-mention graph to predict user location.
where W l and bl are learnable layer parameters, and nhood(i) indicates the neighbours of user ui.",2.1 Multivew Geolocation,[0],[0]
Each extra layer in GCN extends the neighbourhood over which a sample is smoothed.,2.1 Multivew Geolocation,[0],[0]
"For example a GCN with 3 layers smooths each sample with its neighbours up to 3 hops away, which is beneficial if location homophily extends to a neighbourhood of this size.",2.1 Multivew Geolocation,[0],[0]
"Expanding the neighbourhood for label propagation by adding multiple GCN layers can improve geolocation by accessing information from friends that are multiple hops away, but it might also lead to propagation of noisy information to users from an exponentially increasing number of expanded neighbourhood members.",2.2.1 Highway GCN,[0],[0]
"To control the required balance of how much neighbourhood information should be passed to a node, we use layer-wise gates similar to highway networks.",2.2.1 Highway GCN,[0],[0]
"In highway networks (Srivastava et al., 2015), the output of a layer is summed with its input with gating weights T (~hl):
T (~hl) = σ",2.2.1 Highway GCN,[0],[0]
( W lt ~hl + blt ) ~hl+1,2.2.1 Highway GCN,[0],[0]
= ~hl+1 ◦,2.2.1 Highway GCN,[0],[0]
T (~hl) +,2.2.1 Highway GCN,[0],[0]
"~hl ◦ (1− T (~hl)) , (3)
where ~hl is the incoming input to layer l + 1, (W lt , b l t) are gating weights and bias variables, ◦ is elementwise multiplication, and σ is the Sigmoid function.
",2.2.1 Highway GCN,[0],[0]
"2.3 DCCA Given two views X and Â (from Equation 1) of data samples, CCA (Hotelling, 1936), and its deep version (DCCA) (Andrew et al., 2013) learn functions f1(X) and f2(Â) such that the correlation between the output of the two functions is maximised:
ρ = corr(f1(X), f2(Â)) .",2.2.1 Highway GCN,[0],[0]
"(4)
The resulting representations of f1(X) and f2(Â) are the compressed representations of the two views where the uncorrelated noise between them is reduced.",2.2.1 Highway GCN,[0],[0]
"The new representations ideally represent user communities for the network view, and the language model of that community for the text view, and their concatenation is a multiview representation of data, which can be used as input for other tasks.
",2.2.1 Highway GCN,[0],[0]
"In DCCA, the two views are first projected to a lower dimensionality using a separate multilayer perceptron for each view (the f1 and f2 functions of Equation 4), the output of which is used to estimate the CCA cost:
maximise: tr(W T1 Σ12W2) subject to: W T1 Σ11W1 = W T 2 Σ22W2 =",2.2.1 Highway GCN,[0],[0]
"I (5)
where Σ11 and Σ22 are the covariances of the two outputs, and Σ12 is the cross-covariance.",2.2.1 Highway GCN,[0],[0]
"The weights W1 and W2 are the linear projections of the MLP outputs, which are used in estimating the CCA cost.",2.2.1 Highway GCN,[0],[0]
"The optimisation problem is solved by SVD, and the error is backpropagated to train the parameters of the two MLPs and the final linear projections.",2.2.1 Highway GCN,[0],[0]
"After training, the two networks are used to predict new projections for unseen data.",2.2.1 Highway GCN,[0],[0]
"The two projections of unseen data — the outputs of the two networks — are then concatenated to form a multiview sample representation, as shown in Figure 2.",2.2.1 Highway GCN,[0],[0]
"We use three existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), (2) TWITTER-US (Roller et al., 2012), and (3) TWITTER-WORLD (Han et al., 2012).",3.1 Data,[0],[0]
These datasets have been used widely for training and evaluation of geolocation models.,3.1 Data,[0],[0]
"They are all pre-partitioned into training, development and test
sets.",3.1 Data,[0],[0]
"Each user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet in the case of GEOTEXT and TWITTER-US, and the centre of the closest city in the case of TWITTER-WORLD.",3.1 Data,[0],[0]
"GEOTEXT and TWITTER-US cover the continental US, and TWITTER-WORLD covers the whole world, with 9k, 449k and 1.3m users, respectively.",3.1 Data,[0],[0]
"The labels are the discretised geographical coordinates of the training points using a k-d tree following Roller et al. (2012), with the number of labels equal to 129, 256, and 930 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.1 Data,[0],[0]
"We build matrix Â as in Equation 1 using the collapsed @-mention graph between users, where two users are connected (Aij = 1) if one mentions the other, or they co-mention another user.",3.2 Constructing the Views,[0],[0]
"The text view is a BoW model of user content with binary term frequency, inverse document frequency, and l2 normalisation of samples.",3.2 Constructing the Views,[0],[0]
"For GCN, we use highway layers to control the amount of neighbourhood information passed to a node.",3.3 Model Selection,[0],[0]
"We use 3 layers in GCN with size 300, 600, 900 for GEOTEXT, TWITTER-US and TWITTERWORLD respectively.",3.3 Model Selection,[0],[0]
"Note that the final softmax layer is also graph convolutional, which sets the radius of the averaging neighbourhood to 4.",3.3 Model Selection,[0],[0]
"The
k-d tree bucket size hyperparameter which controls the maximum number of users in each cluster is set to 50, 2400, and 2400 for the respective datasets, based on tuning over the validation set.",3.3 Model Selection,[0],[0]
"The architecture of GCN-LP is similar, with the difference that the text view is set to zero.",3.3 Model Selection,[0],[0]
"In DCCA, for the unsupervised networks we use a single sigmoid hidden layer with size 1000 and a linear output layer with size 500 for the three datasets.",3.3 Model Selection,[0],[0]
"The loss function is CCA loss, which maximises the output correlations.",3.3 Model Selection,[0],[0]
"The supervised multilayer perceptron has one hidden layer with size 300, 600, 1000 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively, which we set by tuning over the development sets.",3.3 Model Selection,[0],[0]
"We evaluate the models using Median error, Mean error, and Acc@161, accuracy of predicting a user within 161km or 100 miles from the known location.",3.3 Model Selection,[0],[0]
"We also compare DCCA and GCN with two baselines:
GCN-LP is based on GCN, but for input, instead of text-based features , we use one-hot encoding of a user’s neighbours, which are then convolved with their k-hop neighbours using the GCN.",3.4 Baselines,[0],[0]
"This approach is similar to label propagation in smoothing the label distribution of a user with that of its neighbours, but uses graph convolutional networks which have extra layer parameters, and also a gating mechanism to control the smoothing neighbourhood radius.",3.4 Baselines,[0],[0]
"Note that for unlabelled samples, the predicted labels are used for input after training accuracy reaches 0.2.
",3.4 Baselines,[0],[0]
"MLP-TXT+NET is a simple transductive supervised model based on a single layer multilayer perceptron where the input to the network is the concatenation of the text view X , the user content’s bag-of-words and Â (Equation 1), which represents the network view as a vector input.",3.4 Baselines,[0],[0]
"For the hidden layer we use a ReLU nonlinearity, and sizes 300, 600, and 600 for GEOTEXT, TWITTER-US, and TWITTER-WORLD, respectively.",3.4 Baselines,[0],[0]
"Deep CCA and GCN are able to provide an unsupervised data representation in different ways.
",4.1 Representation,[0],[0]
"Deep CCA takes the two text-based and networkbased views, and finds deep non-linear transformations that result in maximum correlation between the two views (Andrew et al., 2013).",4.1 Representation,[0],[0]
"The representations can be visualised using t-SNE, where we hope that samples with the same label are clustered together.",4.1 Representation,[0],[0]
"GCN, on the other hand, uses graph convolution.",4.1 Representation,[0],[0]
The representations of 50 samples from each of 4 randomly chosen labels of GEOTEXT are shown in Figure 3.,4.1 Representation,[0],[0]
"As shown, Deep CCA seems to slightly improve the representations from pure concatenation of the two views.",4.1 Representation,[0],[0]
"GCN, on the other hand, substantially improves the representations.",4.1 Representation,[0],[0]
"Further application of GCN results in more samples clumping together, which might be desirable when there is strong homophily.",4.1 Representation,[0],[0]
"To achieve good performance in supervised tasks, often large amounts of labelled data are required, which is a big challenge for Twitter geolocation, where only a small fraction of the data is geotagged (about 1%).",4.2 Labelled Data Size,[0],[0]
The scarcity of supervision indicates the importance of semi-supervised learning where unlabelled (e.g. non-geotagged) tweets are used for training.,4.2 Labelled Data Size,[0],[0]
"The three models we propose (MLP-TXT+NET, DCCA, and GCN) are all transductive semi-supervised models that use unlabelled data, however, they are different in terms of how much labelled data they require to achieve acceptable performance.",4.2 Labelled Data Size,[0],[0]
"Given that in a real-world scenario, only a small fraction of data is geotagged, we conduct an experiment to analyse the effect of labelled samples on the performance of the three geolocation models.",4.2 Labelled Data Size,[0],[0]
"We provided the three models with different fractions of samples that are labelled (in terms of % of dataset samples) while using the remainder as unlabelled data, and analysed their Median error performance over the development set of GEOTEXT, TWITTER-US, and TWITTER-WORLD.",4.2 Labelled Data Size,[0],[0]
"Note that the text and network view, and the development set, remain fixed for all the experiments.",4.2 Labelled Data Size,[0],[0]
"As shown in Figure 4, when the fraction of labelled samples is less than 10% of all the samples, GCN and DCCA outperform MLP-TXT+NET, as a result of having fewer parameters, and therefore, lower supervision requirement to optimise them.",4.2 Labelled Data Size,[0],[0]
"When enough training data is available (e.g. more than 20% of all the samples), GCN and MLP-TXT+NET clearly outperform DCCA, possibly as a result of directly modelling the
interactions between network and text views.",4.2 Labelled Data Size,[0],[0]
"When all the training samples of the two larger datasets (95% and 98% for TWITTER-US and TWITTERWORLD, respectively) are available to the models, MLP-TXT+NET outperforms GCN.",4.2 Labelled Data Size,[0],[0]
Note that the number of parameters increases from DCCA to GCN and to MLP-TXT+NET.,4.2 Labelled Data Size,[0],[0]
"In 1% for GEOTEXT, DCCA outperforms GCN as a result of having fewer parameters and just a few labelled samples, insufficient to train the parameters of GCN.",4.2 Labelled Data Size,[0],[0]
"Adding more layers to GCN expands the graph neighbourhood within which the user features are averaged, and so might introduce noise, and consequently decrease accuracy as shown in Figure 5 when no gates are used.",4.3 Highway Gates,[0],[0]
"We see that by adding highway network gates, the performance of GCN slightly improves until three layers are added, but then by adding more layers the performance doesn’t change that much as gates are allowing the layer inputs to pass through the network without
much change.",4.3 Highway Gates,[0],[0]
The performance peaks at 4 layers which is compatible with the distribution of shortest path lengths shown in Figure 6.,4.3 Highway Gates,[0],[0]
"The performance of the three proposed models (MLP-TXT+NET, DCCA and GCN) is shown in Table 1.",4.4 Performance,[0],[0]
"The models are also compared with supervised text-based methods (Wing and Baldridge, 2014; Cha et al., 2015; Rahimi et al., 2017b), a network-based method (Rahimi et al., 2015a) and GCN-LP, and also joint text and network models (Rahimi et al., 2017b; Do et al., 2017; Miura et al., 2017).",4.4 Performance,[0],[0]
"MLP-TXT+NET and GCN outperform all the text- or network-only models, and also the hybrid model of Rahimi et al. (2017b), indicating that joint modelling of text and network features is important.",4.4 Performance,[0],[0]
"MLP-TXT+NET is competitive with Do et al. (2017), outperforming it on larger datasets, and underperforming on GEO-
TEXT.",4.4 Performance,[0],[0]
"However, it’s difficult to make a fair comparison as they use timezone data in their feature set.",4.4 Performance,[0],[0]
"MLP-TXT+NET outperforms GCN over TWITTERUS and TWITTER-WORLD, which are very large, and have large amounts of labelled data.",4.4 Performance,[0],[0]
In a scenario with little supervision (1% of the total samples are labelled),4.4 Performance,[0],[0]
"DCCA and GCN clearly outperform MLP-TXT+NET, as they have fewer pa-
rameters.",4.4 Performance,[0],[0]
"Except for Acc@161 over GEOTEXT where the number of labelled samples in the minimal supervision scenario is very low, GCN outperforms DCCA by a large margin, indicating that for a medium dataset where only 1% of samples are labelled (as happens in random samples of Twitter) GCN is superior to MLP-TXT+NET and DCCA, consistent with Section 4.2.",4.4 Performance,[0],[0]
"Both MLP-TXT+NET and GCN achieve state of the art results compared
to network-only, text-only, and hybrid models.",4.4 Performance,[0],[0]
"The network-based GCN-LP model, which does label propagation using Graph Convolutional Networks, outperforms Rahimi et al. (2015a), which is based on location propagation using Modified Adsorption (Talukdar and Crammer, 2009), possibly because the label propagation in GCN is parametrised.",4.4 Performance,[0],[0]
"Although the performance of MLP-TXT+NET is better than GCN and DCCA when a large amount of labelled data is available (Table 1), under a scenario where little labelled data is available (1% of data), DCCA and GCN outperform MLP-TXT+NET, mainly because the number of parameters in MLP-TXT+NET grows with the number of samples, and is much larger than GCN and DCCA.",4.5 Error Analysis,[0],[0]
"GCN outperforms DCCA and MLP-TXT+NET using 1% of data, however, the distribution of errors in the development set of TWITTER-US indicates higher error for smaller states such as Rhode Island (RI), Iowa (IA), North Dakota (ND), and Idaho (ID), which is simply because the number of labelled samples in those states is insufficient.
",4.5 Error Analysis,[0],[0]
"Although we evaluate geolocation models with Median, Mean, and Acc@161, it doesn’t mean that the distribution of errors is uniform over all locations.",4.5 Error Analysis,[0],[0]
"Big cities often attract more local online discussions, making the geolocation of users in those areas simpler.",4.5 Error Analysis,[0],[0]
"For example users in LA are more likely to talk about LA-related issues such as their sport teams, Hollywood or local events than users in the state of Rhode Island (RI), which lacks large sport teams or major events.",4.5 Error Analysis,[0],[0]
"It is also possible that people in less densely populated areas are further apart from each other, and therefore, as a result of discretisation fall in different clusters.",4.5 Error Analysis,[0],[0]
The non,4.5 Error Analysis,[0],[0]
"-uniformity in local discussions results in lower geolocation performance in less densely populated areas like Midwest U.S., and higher performance in densely populated areas such as NYC and LA as shown in Figure 7.",4.5 Error Analysis,[0],[0]
"The geographical distribution of error for GCN, DCCA and MLP-TXT+NET under the minimal supervision scenario is shown in the supplementary material.
",4.5 Error Analysis,[0],[0]
"To get a better picture of misclassification between states, we built a confusion matrix based on known state and predicted state for development users of TWITTER-US using GCN using only 1% of labelled data.",4.5 Error Analysis,[0],[0]
"There is a tendency for users to be wrongly predicted to be in CA, NY, TX, and surpris-
ingly OH.",4.5 Error Analysis,[0],[0]
"Particularly users from states such as TX, AZ, CO, and NV, which are located close to CA, are wrongly predicted to be in CA, and users from NJ, PA, and MA are misclassified as being in NY.",4.5 Error Analysis,[0],[0]
The same goes for OH and TX where users from neighbouring smaller states are misclassified to be there.,4.5 Error Analysis,[0],[0]
"Users from CA and NY are also misclassified between the two states, which might be the result of business and entertainment connections that exist between NYC and LA/SF.",4.5 Error Analysis,[0],[0]
"Interestingly, there are a number of misclassifications to FL for users from CA, NY, and TX, which might be the effect of users vacationing or retiring to FL.",4.5 Error Analysis,[0],[0]
The full confusion matrix between the U.S. states is provided in the supplementary material.,4.5 Error Analysis,[0],[0]
"In Table 2, local terms of a few regions detected by GCN under minimal supervision are shown.",4.6 Local Terms,[0],[0]
The terms that were present in the labelled data are excluded to show how graph convolutions over the social graph have extended the vocabulary.,4.6 Local Terms,[0],[0]
"For example, in case of Seattle, #goseahawks is an important term not present in the 1% labelled data but present in the unlabelled data.",4.6 Local Terms,[0],[0]
The convolution over the social graph is able to utilise such terms that don’t exist in the labelled data.,4.6 Local Terms,[0],[0]
"Previous work on user geolocation can be broadly divided into text-based, network-based and multiview approaches.
",5 Related Work,[0],[0]
Text-based geolocation uses the geographical bias in language use to infer the location of users.,5 Related Work,[0],[0]
"There are three main text-based approaches to geolocation: (1) gazetteer-based models which map geographical references in text to location, but ignore non-geographical references and vernacular uses of language (Rauch et al., 2003; Amitay et al., 2004; Lieberman et al., 2010); (2) geographical topic models that learn region-specific topics, but don’t scale to the magnitude of social media (Eisenstein et al., 2010; Hong et al., 2012; Ahmed et al., 2013); and (3) supervised models which are often framed as text classification (Serdyukov et al., 2009; Wing and Baldridge, 2011; Roller et al., 2012; Han et al., 2014) or text regression (Iso et al., 2017; Rahimi et al., 2017a).",5 Related Work,[0],[0]
"Supervised models scale well and can achieve good performance with sufficient supervision, which is not available in a real world scenario.
",5 Related Work,[0],[0]
Network-based methods leverage the location homophily assumption: nearby users are more likely to befriend and interact with each other.,5 Related Work,[0],[0]
"There are four main network-based geolocation approaches: distance-based, supervised classification, graph-based label propagation, and node embedding methods.",5 Related Work,[0],[0]
"Distance-based methods model the probability of friendship given the distance (Backstrom et al., 2010; McGee et al., 2013; Gu et al., 2012; Kong et al., 2014), supervised models use neighbourhood features to classify a user into a location (Rout et al., 2013; Malmi et al., 2015), and graph-based label-propagation models propagate the location information through the user–user graph to estimate unknown labels (Davis Jr et al., 2011; Jurgens, 2013; Compton et al., 2014).",5 Related Work,[0],[0]
"Node embedding methods build heterogeneous graphs between user–user, user–location and location– location, and learn an embedding space to minimise the distance of connected nodes, and maximise the distance of disconnected nodes.",5 Related Work,[0],[0]
"The embeddings are then used in supervised models for geolocation (Wang et al., 2017).",5 Related Work,[0],[0]
Network-based models fail to geolocate disconnected users: Jurgens et al. (2015) couldn’t geolocation 37% of users as a result of disconnectedness.,5 Related Work,[0],[0]
"Previous work on hybrid text and network methods can be broadly categorised into three main approaches: (1) incorporating text-based information such as toponyms or locations predicted from a textbased model as auxiliary nodes into the user–user graph, which is then used in network-based models (Li et al., 2012a,b; Rahimi et al., 2015b,a); (2) ensembling separately trained text- and networkbased models (Gu et al., 2012; Ren et al., 2012; Jayasinghe et al., 2016; Ribeiro and Pappa, 2017); and (3) jointly learning geolocation from several information sources such as text and network information (Miura et al., 2017; Do et al., 2017), which can capture the complementary information in text and network views, and also model the interactions between the two.",5 Related Work,[0],[0]
"None of the previous
multiview approaches — with the exception of Li et al. (2012a) and Li et al. (2012b) that only use toponyms — effectively uses unlabelled data in the text view, and use only the unlabelled information of the network view via the user–user graph.
",5 Related Work,[0],[0]
"There are three main shortcomings in the previous work on user geolocation that we address in this paper: (1) with the exception of few recent works (Miura et al., 2017; Do et al., 2017), previous models don’t jointly exploit both text and network information, and therefore the interaction between text and network views is not modelled; (2) the unlabelled data in both text and network views is not effectively exploited, which is crucial given the small amounts of available supervision; and (3) previous models are rarely evaluated under a minimal supervision scenario, a scenario which reflects real world conditions.",5 Related Work,[0],[0]
"We proposed GCN, DCCA and MLP-TXT+NET, three multiview, transductive, semi-supervised geolocation models, which use text and network information to infer user location in a joint setting.",6 Conclusion,[0],[0]
"We showed that joint modelling of text and network information outperforms network-only, text-only, and hybrid geolocation models as a result of modelling the interaction between text and network information.",6 Conclusion,[0],[0]
We also showed that GCN and DCCA are able to perform well under a minimal supervision scenario similar to real world applications by effectively using unlabelled data.,6 Conclusion,[0],[0]
"We ignored the context in which users interact with each other, and assumed all the connections to hold location homophily.",6 Conclusion,[0],[0]
"In future work, we are interested in modelling the extent to which a social interaction is caused by geographical proximity (e.g. using user–user gates).",6 Conclusion,[0],[0]
Social media user geolocation is vital to many applications such as event detection.,abstractText,[0],[0]
"In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context.",abstractText,[0],[0]
"We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the stateof-the-art over three benchmark geolocation datasets when sufficient supervision is available.",abstractText,[0],[0]
"We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines.",abstractText,[0],[0]
We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.,abstractText,[0],[0]
Semi-supervised User Geolocation via Graph Convolutional Networks,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 97–102 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2016",text,[0],[0]
"Automated text simplification (ATS) tries to automatically transform (syntactically, lexically and/or semantically) complex sentences into their simpler variants without significantly altering the original meaning.",1 Introduction,[0],[0]
"It has attracted much attention recently as it could make texts more accessible to wider audiences (Aluı́sio and Gasperin, 2010; Saggion et al., 2015), and used as a pre-processing step, improve performances of various NLP tasks and systems (Vickrey and Koller, 2008; Evans, 2011; Štajner and Popović, 2016).
",1 Introduction,[0],[0]
"However, the state-of-the-art ATS systems still do not reach satisfying performances and require some human post-editing (Štajner and Popović, 2016).",1 Introduction,[0],[0]
"While the best supervised approaches generally lead to grammatical output with preserved original meaning, they are overcautious, making almost no changes to the input sentences (Specia, 2010; Štajner et al., 2015), probably due to the limited size or bad quality of parallel TS corpora used for training.",1 Introduction,[0],[0]
"The largest existing sentence-aligned TS dataset for English is the English Wikipedia – Simple English Wikipedia
(EW–SEW) dataset, which contains 160-280,000 sentence pairs, depending on whether we want to model only traditional sentence rewritings or also to model content reduction and stronger paraphrasing (Hwang et al., 2015).",1 Introduction,[0],[0]
"For Spanish, the largest existing parallel TS corpus contains only 1,000 sentence pairs thus impeding the use of fully supervised approaches.",1 Introduction,[0],[0]
"The best unsupervised lexical simplification (LS) systems for English which leverage word-embeddings (Glavaš and Štajner, 2015; Paetzold and Specia, 2016) seem to perform more lexical substitutions but at the cost of having less grammatical output and more often changed meaning.",1 Introduction,[0],[0]
"However, there have been no direct comparisons of supervised and unsupervised state-of-the-art approaches so far.
",1 Introduction,[0],[0]
"The Newsela corpora1 offers over 2,000 original news articles in English and around 250 in Spanish, manually simplified to 3–4 different complexity levels following strict guidelines (Xu et al., 2015).",1 Introduction,[0],[0]
"Although it was suggested that it has better quality than the EW–SEW corpus (Xu et al., 2015), Newsela has not yet been used for training end-to-end ATS systems, due to the lack of its sentence (and paragraph) alignments.",1 Introduction,[0],[0]
"Such alignments, between various text complexity levels, would offer large training datasets for modelling different levels of simplification, i.e. ‘mild’ simplifications (using the alignments from the neighbouring levels) and ‘heavy’ simplifications (using the alignments of level pairs: 0–3, 0–4, 1–4).
Contributions.",1 Introduction,[0],[0]
"We: (1) provide several methods for paragraph- and sentence alignment of parallel texts, and for assessing similarity level between pairs of text snippets, as freely avail-
1Freely available: https://newsela.com/data/
97
able software;2 (2) compare the performances of lexically- and semantically-based alignment methods across various text complexity levels; (3) test the hypothesis that the original order of information is preserved during manual simplification (Bott and Saggion, 2011) by offering customized MST-LIS alignment strategy (Section 3.1); and (4) show that the new sentence-alignments lead to the state-of-the-art ATS systems even in a basic phrase-based statistical machine translation (PBSMT) approach to text simplifications.",1 Introduction,[0],[0]
"The current state-of-the-art systems for automatic sentence-alignment of original and manually simplified texts are the GSWN method (Hwang et al., 2015) used for sentence-alignment of original and simple English Wikipedia, and the HMMbased method (Bott and Saggion, 2011) used for sentence-alignment of the Spanish Simplext corpus (Saggion et al., 2015).
",2 Related Work,[0],[0]
The HMM-based method can be applied to any language as it does not require any languagespecific resources.,2 Related Work,[0],[0]
"It is based on two hypotheses: (H1) that the original order of information is preserved, and (H2) that every ‘simple’ sentence has at least one corresponding ‘original’ sentence (it can have more than one in the case of ‘n-1’ or ‘nm’ alignments).
",2 Related Work,[0],[0]
"As Simple Wikipedia does not represent direct simplification of the ‘original’ Wikipedia articles (‘simple’ articles were written independently of the ‘original’ ones), GSWN method does not assume H1 or H2.",2 Related Work,[0],[0]
"The main limitations of this method are that it only allows for ‘1-1’ sentence alignments – which is very restricting for TS as it does not allow for sentence splitting (‘1-n’), and summarisation and compression (‘n-1’ and ‘n-m’) alignments – and it is language-dependent as it requires English Wiktionary.
",2 Related Work,[0],[0]
"Unlike the GSWN method, all the methods we apply are language-independent, resource-light and allow for ‘1-n’, ‘n-1’, and ‘n-m’ alignments.",2 Related Work,[0],[0]
"Similar to the HMM-method, our methods assume the hypothesis H2.",2 Related Work,[0],[0]
"We provide them in both variants, using the hypothesis H1 and without it (Section 3.1).
",2 Related Work,[0],[0]
2https://github.com/neosyon/ SimpTextAlign,2 Related Work,[0],[0]
"Having a set of ‘simple’ text snippets S and a set of ‘complex’ text snippets C, we offer two strategies (Section 3.1) to obtain the alignments (si, cj), where si ∈ S, cj ∈",3 Approach,[0],[0]
C.,3 Approach,[0],[0]
"Each alignment strategy, in turn, can use one of the three methods (Section 3.2) to calculate similarity scores between text snippets (either paragraphs or sentences).",3 Approach,[0],[0]
"Most Similar Text (MST): Given one of the similarity methods (Section 3.2), MST compares similarity scores of all possible pairs (si, cj), and aligns each si ∈ S with the closest one in C. MST with Longest Increasing Sequence (MSTLIS): MST-LIS uses the hypothesis H1.",3.1 Alignment strategies,[0],[0]
"It first uses the MST strategy, and then postprocess the output by extracting – from all obtained alignments – only those alignments li ∈ L, which contain the longest increasing sequence of offsets jk in C. In order to allow for ‘1–n’ alignments (i.e. sentence splitting), we allow for repeated offsets of C (‘complex’ text snippets) in L. The ‘simple’ text snippets not contained in L are included in the set U of unaligned snippets.",3.1 Alignment strategies,[0],[0]
"Finally, we align each um ∈ U by restricting the search space in C to those offsets of ‘complex’ text snippets that correspond to the previous and the next aligned ‘simple’ snippets.",3.1 Alignment strategies,[0],[0]
"For instance, if L = {(s1, c4), (s3, c7)} and U = {s2}, then the search space for the alignments of s2 is reduced to {c4...c7}.",3.1 Alignment strategies,[0],[0]
"We denote this strategy with an ‘*’ in the results (Table 2), e.g. C3G*.",3.1 Alignment strategies,[0],[0]
C3G: We employ the Character N -Gram,3.2 Similarity Methods,[0],[0]
"(CNG) (Mcnamee and Mayfield, 2004) similarity model (for n = 3) with log TF-IDF weighting (Salton and McGill, 1986) and compare vectors using the cosine similarity.",3.2 Similarity Methods,[0],[0]
WAVG:,3.2 Similarity Methods,[0],[0]
"We use the continuous skip-gram model (Mikolov et al., 2013b) of the TensorFlow toolkit3 to process the whole English Wikipedia and generate continuous representations of its words.4 For each text snippet, we average its word vectors to obtain a single representation of its content as this setting has shown good results
3https://www.tensorflow.org/ 4We use 300-dimensional vectors, context windows of size 10, and 20 negative words for each sample, in all our continuous word-based models.
",3.2 Similarity Methods,[0],[0]
"in other NLP tasks (e.g. for selecting out-of-thelist words (Mikolov et al., 2013a)).",3.2 Similarity Methods,[0],[0]
"Finally, the similarity between text snippets is estimated using the cosine similarity.",3.2 Similarity Methods,[0],[0]
CWASA:,3.2 Similarity Methods,[0],[0]
"We employ the Continuous Word Alignment-based Similarity Analysis (CWASA) model (Franco-Salvador et al., 2016), which finds the optimal word alignment by computing cosine similarity between continuous representations of all words (instead of averaging word vectors as in the case of WAVG).",3.2 Similarity Methods,[0],[0]
"It was originally proposed for plagiarism detection with excellent results, especially for longer text snippets.",3.2 Similarity Methods,[0],[0]
"To compare the performances of different alignment methods, we randomly selected 10 original texts (Level 0) and their corresponding simpler versions at Levels 1, 3 and 4.",4 Manual Evaluation,[0],[0]
"Instead of creating a ‘gold standard’ and then automatically evaluating the performances, we asked two annotators to rate each pair of automatically aligned paragraphs and sentences – by each of the possible six alignment methods and the HMM-based method (Bott and Saggion, 2011) – for three pairs of text complexity levels (0–1, 0–4, and 3–4) on a 0–2 scale, where: 0 – no semantic overlap in the content; 1 – partial semantic overlap (partial matches); 2 – same semantic content (good matches).",4 Manual Evaluation,[0],[0]
"This resulted in a total of 1526 paragraph- and 1086 sentence-alignments for the 0–1 pairs, and 1218 paragraph- and 1266 sentence-alignments for the 0–4 and 3–4 pairs.",4 Manual Evaluation,[0],[0]
"In the context of TS, both good- and partial matches
are important.",4 Manual Evaluation,[0],[0]
"While full semantic overlap models full paraphrases (‘1-1’ alignments), partial overlap models sentence splitting (“1-n” alignments), deleting irrelevant sentence parts, adding explanations, or summarizing (‘n-m’ alignments).",4 Manual Evaluation,[0],[0]
"Several examples of full and partial matches from the EW–SEW dataset (Hwang et al., 2015) are given in Table 1.
",4 Manual Evaluation,[0],[0]
"We expect that the automatic-alignment task is the easiest between the 0–1 text complexity levels, and much more difficult between the 0-4 levels (Level 4 is obtained after four stages of simplification and thus contains stronger paraphrases and less lexical overlap with Level 0 than Level 1 has).",4 Manual Evaluation,[0],[0]
"We also explore whether the task is equally difficult whenever we align two neighbouring levels, or the difficulty of the task depends on the level complexity (0–1 vs. 3–4).",4 Manual Evaluation,[0],[0]
"The obtained interannotator agreement, weighted Cohen’s κ (on 400 double-annotated instances) was between 0.71 and 0.74 depending on the task and levels.
",4 Manual Evaluation,[0],[0]
"The results of the manual analysis (Table 2) showed that: (1) all applied methods significantly (p < 0.001) outperformed the HMM method on both paragraph- and sentence-alignment tasks;5 (2) the methods which do not assume hypothesis H1 (C3G, CWASA, and WAVG) led to (not significantly) higher percentage of correct alignments than their counterparts which do assume
5Although some of our methods share the same percentage of good+partial matches with the HMM method on the paragraph-alignment 0–1 task, there is still significant difference in the obtained scores (in some cases, our methods led to good matches whereas the HMM only led to partial matches).
",4 Manual Evaluation,[0],[0]
"H1 (C3G*, CWASA*, WAVG*); (3) the difference in the performances of the lexical approach (C3G) and semantic approaches (CWASA and WAVG) was significant only in the 0–4 sentencealignment task, where CWASA performed significantly worse (p < 0.001) than the other two methods, and in the 0–4 paragraph-alignment task, where WAVG performed significantly worse than C3G; (4) the 2-step C3G alignment-method (C3G-2s), which first aligns paragraphs using the best paragraph-alignment method (C3G) and then within each paragraph align sentences with the best sentence-alignment method (C3G), led to more good+partial alignments than the ‘direct’ sentence-alignment C3G method.",4 Manual Evaluation,[0],[0]
"Finally, we test our new English Newsela (C3G2s) sentence-alignments (both for the neighbouring levels – neighb.",5 Extrinsic Evaluation,[0],[0]
"and for all levels – all) and Newsela sentence-alignments for neighboring levels obtained with HMM-method6 (Bott and Saggion, 2011) in the ATS task using standard PBSMT models7 in the Moses toolkit (Koehn et al., 2007).",5 Extrinsic Evaluation,[0],[0]
"We vary the training dataset and the corpus used to build language models (LMs), while keeping always the same 2,000 sentence pairs for tuning (Xu et al., 2016) and the first 70 sentence
6Given that the performance of the HMM-method was poor for non-neighboring levels (Table 2).
",5 Extrinsic Evaluation,[0],[0]
"7GIZA++ implementation of the IBM word alignment model 4 (Och and Ney, 2003), refinement and phraseextraction heuristics (Koehn et al., 2003), the minimum error rate training (Och, 2003) for tuning, and 5-gram LMs with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002).
pairs of their test set8 for our human evaluation.",5 Extrinsic Evaluation,[0],[0]
"Using that particular test set allow us to compare our (PBSMT) systems with the output of the stateof-the-art syntax-based MT (SBMT) system for TS (Xu et al., 2016) which is not freely available.",5 Extrinsic Evaluation,[0],[0]
"We compare: (1) the performance of the standard PBSMT model which uses only the already available EW–SEW dataset (Hwang et al., 2015) with the performances of the same PBSMT models but this time using the combination of the EW–SEW dataset and our newly-created Newsela datasets; (2) the latter PBSMT models (which use both EW–SEW and new Newsela datasets) against the state-of-the-art supervised ATS system (Xu et al., 2016), and one of the recently proposed unsupervised lexical simplification systems, the LightLS system (Glavaš and Štajner, 2015).9
We perform three types of human evaluation on the outputs of all systems.",5 Extrinsic Evaluation,[0],[0]
"First, we count the total number of changes made by each system (Total), counting the change of a whole phrase (e.g. “become defunct” → “was dissolved”) as one change.",5 Extrinsic Evaluation,[0],[0]
"We mark as Correct those changes that preserve the original meaning and grammaticality of the sentence (assessed by two native English speakers) and, at the same time, make the sentence easier to understand (assessed by two non-native fluent English speakers).10 Second, three native English speakers rate the grammaticality (G) and meaning preservation (M) of each sentence with at least one change on a 1–5 Likert scale (1 – very bad; 5 – very good).",5 Extrinsic Evaluation,[0],[0]
"Third, the three nonnative fluent English speakers were shown original (reference) sentences and target (output) sentences (one pair at the time) and asked whether the target sentence is: +2 – much simpler; +1 – somewhat simpler; 0 – equally difficult; -1 – somewhat more difficult; -2 – much more difficult, than the reference sentence.",5 Extrinsic Evaluation,[0],[0]
"While the correctness of changes takes into account the influence of each individual change on grammaticality, meaning and simplicity of a sentence, the Scores (G and M) and Rank (S) take into account the mutual influence of all changes within a sentence.
",5 Extrinsic Evaluation,[0],[0]
"Adding our sentence-aligned Newsela corpus
8Both freely available from: https://github.com/ cocoxu/simplification/
9We use the output of the original SBMT (Xu et al., 2016) and LightLS (Glavaš and Štajner, 2015) systems, obtained from the authors.
",5 Extrinsic Evaluation,[0],[0]
"10Those cases in which the two annotators did not agree are additionally evaluated by a third annotator to obtain majority.
",5 Extrinsic Evaluation,[0],[0]
(either neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2l or all C3G-2l) to the currently best sentence-aligned Wiki corpus (Hwang et al., 2015) in a standard PBSMT setup significantly11 improves grammaticality (G) and meaning preservation (M), and increases the percentage of correct changes (Table 3).",5 Extrinsic Evaluation,[0],[0]
"It also significantly outperforms the state-of-the-art ATS systems by simplicity rankings (S), meaning preservation (M), and number of correct changes (Correct), while achieving almost equally good grammaticality (G).
",5 Extrinsic Evaluation,[0],[0]
The level of simplification applied in the training dataset (Newsela neighb.,5 Extrinsic Evaluation,[0],[0]
"C3G-2s vs. Newsela all C3G-2s) significantly influences G and M scores.
",5 Extrinsic Evaluation,[0],[0]
"The use of the HMM-method for aligning Newsela (instead of ours) lead to significantly worse simplifications by all five criteria.
",5 Extrinsic Evaluation,[0],[0]
"11Wilcoxon’s signed rank test, p < 0.001.
",5 Extrinsic Evaluation,[0],[0]
An example of the outputs of different ATS systems is presented in Table 4.,5 Extrinsic Evaluation,[0],[0]
"We provided several methods for paragraphand sentence-alignment from parallel TS corpora, made the software publicly available, and showed that the use of the new sentence-aligned (freely available) Newsela dataset leads to state-of-the-art ATS systems even in a basic PBSMT setup.",6 Conclusions,[0],[0]
"We also showed that lexically-based C3G method is superior to semantically-based methods (CWASA and WAVG) in aligning paraphraphs and sentences with ‘heavy’ simplifications (0–4 alignments), and that 2-step sentence alignment (aligning first paragraphs and then sentences within the paragraphs) lead to more correct alignments than the ‘direct’ sentence alignment.",6 Conclusions,[0],[0]
"This work has been partially supported by the SFB 884 on the Political Economy of Reforms at the University of Mannheim (project C4), funded by the German Research Foundation (DFG), and also by the SomEMBED TIN2015-71147-C2-1-P MINECO research project.",Acknowledgments,[0],[0]
We provide several methods for sentencealignment of texts with different complexity levels.,abstractText,[0],[0]
"Using the best of them, we sentence-align the Newsela corpora, thus providing large training materials for automatic text simplification (ATS) systems.",abstractText,[0],[0]
"We show that using this dataset, even the standard phrase-based statistical machine translation models for ATS can outperform the state-of-the-art ATS systems.",abstractText,[0],[0]
Sentence Alignment Methods for Improving Text Simplification Systems,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 360–368, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence.,1 Introduction,[0],[0]
"Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011, to name a few).
",1 Introduction,[0],[0]
Existing compression systems heavily use syntactic information to minimize chances of introducing grammatical mistakes in the output.,1 Introduction,[0],[0]
"A common approach is to use only some syntactic information (Jing, 2000; Clarke & Lapata, 2008,
among others) or use syntactic features as signals in a statistical model (McDonald, 2006).",1 Introduction,[0],[0]
"It is probably even more common to operate on syntactic trees directly (dependency or constituency) and generate compressions by pruning them (Knight & Marcu, 2000; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013, among others).",1 Introduction,[0],[0]
"Unfortunately, this makes such systems vulnerable to error propagation as there is no way to recover from an incorrect parse tree.",1 Introduction,[0],[0]
"With the state-of-the-art parsing systems achieving about 91 points in labeled attachment accuracy (Zhang & McDonald, 2014), the problem is not a negligible one.",1 Introduction,[0],[0]
"To our knowledge, there is no competitive compression system so far which does not require any linguistic preprocessing but tokenization.
",1 Introduction,[0],[0]
In this paper we research the following question: can a robust compression model be built which only uses tokens and has no access to syntactic or other linguistic information?,1 Introduction,[0],[0]
"While phenomena like long-distance relations may seem to make generation of grammatically correct compressions impossible, we are going to present an evidence to the contrary.",1 Introduction,[0],[0]
"In particular, we will present a model which benefits from the very recent advances in deep learning and uses word embeddings and Long Short Term Memory models (LSTMs) to output surprisingly readable and informative compressions.",1 Introduction,[0],[0]
"Trained on a corpus of less than two million automatically extracted parallel sentences and using a standard tool to obtain word embeddings, in its best and most simple configuration it achieves 4.5 points out of 5 in readability and 3.8 points in informativeness in an extensive evaluation with human judges.",1 Introduction,[0],[0]
"We believe that this is an important result as it may suggest a new direction for sentence compression research which is less tied to modeling linguistic
360
structures, especially syntactic ones, than the compression work so far.
",1 Introduction,[0],[0]
The paper is organized as follows: Section 3 presents a competitive baseline which implements the system of McDonald (2006) for large training sets.,1 Introduction,[0],[0]
The LSTM model and its three configurations are introduced in Section 4.,1 Introduction,[0],[0]
The evaluation set-up and a discussion on wins and losses with examples are presented in Section 5 which is followed by the conclusions.,1 Introduction,[0],[0]
"The problem formulation we adopt in this paper is very simple: for every token in the input sentence we ask whether it should be kept or dropped, which translates into a sequence labeling problem with just two labels: one and zero.",2 Related Work,[0],[0]
"The deletion approach is a standard one in compression research, although the problem is often formulated over the syntactic structure and not the raw token sequence.",2 Related Work,[0],[0]
"That is, one usually drops constituents or prunes dependency edges (Jing, 2000; Knight & Marcu, 2000; McDonald, 2006; Clarke & Lapata, 2008; Berg-Kirkpatrick et al., 2011; Filippova & Altun, 2013).",2 Related Work,[0],[0]
"Thus, the relation to existing compression work is that we also use the deletion approach.
",2 Related Work,[0],[0]
Recent advances in machine learning made it possible to escape the typical paradigm of mapping a fixed dimensional input to a fixed dimensional output to mapping an input sequence onto an output sequence.,2 Related Work,[0],[0]
"Even though many of these models were proposed more than a decade ago, it is not until recently that they have empirically been shown to perform well.",2 Related Work,[0],[0]
"Indeed, core problems in natural language processing such as translation (Cho et al., 2014; Sutskever et al., 2014; Luong et al., 2014), parsing (Vinyals et al., 2014), image captioning (Vinyals et al., 2015; Xu et al., 2015), or learning to execute small programs (Zaremba & Sutskever, 2014) employed virtually the same principles—the use of Recurrent Neural Networks (RNNs).",2 Related Work,[0],[0]
"Thus, with regard to this line of research, our work comes closest to the recent machine translation work.",2 Related Work,[0],[0]
"An important difference is that we do not aim at building a model that generates compressions directly but rather a model which generates a sequence of deletion decisions.
",2 Related Work,[0],[0]
"A more complex translation model is also conceivable and may significantly advance work on compression by paraphrasing, of which there have
not been many examples yet (Cohn & Lapata, 2008).",2 Related Work,[0],[0]
"However, in this paper our goal is to demonstrate that a simple but robust deletionbased system can be built without using any linguistic features other than token boundaries.",2 Related Work,[0],[0]
We leave experiments with paraphrasing models to future work.,2 Related Work,[0],[0]
We compare our model against the system of McDonald (2006) which also formulates sentence compression as a binary sequence labeling problem.,3 Baseline,[0],[0]
"In contrast to our proposal, it makes use of a large set of syntactic features which are treated as soft evidence.",3 Baseline,[0],[0]
The presence or absence of these features is treated as signals which do not condition the output that the model can produce.,3 Baseline,[0],[0]
"Therefore the model is robust against noise present in the precomputed syntactic structures of the input sentences.
",3 Baseline,[0],[0]
The system was implemented based on the description by McDonald (2006) with two changes which were necessary due to the large size of the training data set used for model fitting.,3 Baseline,[0],[0]
"The first change was related to the learning procedure and the second one to the family of features used.
",3 Baseline,[0],[0]
"Regarding the learning procedure, the original model uses a large-margin learning framework, namely MIRA (Crammer & Singer, 2003), but with some minor changes as presented by McDonald et al. (2005).",3 Baseline,[0],[0]
"In this set-up, online learning is performed, and at each step an optimization procedure is made where K constraints are included, which correspond to the top-K solutions for a given training observation.",3 Baseline,[0],[0]
"This optimization step is equivalent to a Quadratic Programming problem if K > 1, which is time-costly to solve, and therefore not adequate for the large amount of data we used for training the model.",3 Baseline,[0],[0]
"Furthermore, in his publication McDonald states clearly that different values of K did not actually have a major impact on the final performance of the model.",3 Baseline,[0],[0]
"Consequently, and for the sake of being able to successfully train the model with largescale data, the learning procedure is implemented as a distributed structured perceptron with iterative parameter mixing (McDonald et al., 2010), where each shard is processed with MIRA and K is set to 1.
Setting K = 1 will only affect the weight update described on line 4 of Figure 3 of McDonald
(2006), which is now expressed as:
w(i+1) ← w(i)",3 Baseline,[0],[0]
"+ τ × eyt,y′ where τ = max ( 0, L(yt,y′)−w · eyt,y′ ||eyt,y′ ||2 )
eyt,y′ =",3 Baseline,[0],[0]
"F (xt,yt)− F (xt,y′) y′",3 Baseline,[0],[0]
= best(x; w(i)),3 Baseline,[0],[0]
"F (x,y) = |y|∑ j=2 f(x, I(yj−1), I(yj))
",3 Baseline,[0.9513959264286622],"['(4) Next, define the growth- and changepoint probabilities as p(y1:t, rt = rt−1 + 1,mt) = fmt(yt|y1:(t−1), rt)p(y1:(t−1), rt−1,mt−1)× (5a) (1−H(rt))q(mt−1|y1:(t−1), rt), p(y1:t, rt = 0,mt) = fmt(yt|y1:(t−1), rt)q(mt)× (5b)∑ mt−1 ∑ rt−1 { H(rt−1 + 1)p(y1:(t−1), rt−1,mt−1) } .']"
The second change concerns the feature set used.,3 Baseline,[0],[0]
"While McDonald’s original model contains deep syntactic features coming from both dependency and constituency parse trees, we use only dependency-based features.",3 Baseline,[0],[0]
"Additionally, and to better compare the baseline with the LSTM models, we have included as an optional feature a 256-dimension embedding-vector representation of each input word and its syntactic parent.",3 Baseline,[0],[0]
"The vectors are pre-trained using the Skipgram model1 (Mikolov et al., 2013).",3 Baseline,[0],[0]
"Ultimately, our implementation of McDonald’s model contained 463,614 individual features, summarized in three categories: • PoS features: Joint PoS tags of selected to-
kens.",3 Baseline,[0],[0]
"Unigram, bigram and trigram PoS context of selected and dropped tokens.",3 Baseline,[0],[0]
All the previous features conjoined with one indicating if the last two selected tokens are adjacent.,3 Baseline,[0],[0]
•,3 Baseline,[0],[0]
"Deep syntactic features: Dependency labels
of taken and dropped tokens and their parent dependencies.",3 Baseline,[0],[0]
"Boolean features indicating syntactic relations between selected tokens (i.e., siblings, parents, leaves, etc.).",3 Baseline,[0],[0]
Dependency label of the least common ancestor in the dependency tree between a batch of dropped tokens.,3 Baseline,[0],[0]
All the previous features conjoined with the PoS tag of the involved tokens.,3 Baseline,[0],[0]
"• Word features: Boolean features indicating
if a group of dropped nodes contain a complete or incomplete parenthesization.",3 Baseline,[0],[0]
Wordembedding vectors of selected and dropped tokens and their syntactic parents.,3 Baseline,[0],[0]
"The model is fitted over ten epochs on the whole training data, and for model selection a small development set consisting of 5,000 previously unseen sentences is used (none of them belonging to
1https://code.google.com/p/word2vec/
the evaluation set).",3 Baseline,[0],[0]
The automated metric used for this selection was accuracy@1 which is the proportion of golden compressions which could be fully reproduced.,3 Baseline,[0],[0]
The performance on the development set plateaus when getting close to the last epoch.,3 Baseline,[0],[0]
Our approach is largely based on the sequence to sequence paradigm proposed in Sutskever et al. (2014).,4 The LSTM model,[0],[0]
We train a model that maximizes the probability of the correct output given the input sentence.,4 The LSTM model,[0],[0]
"Concretely, for each training pair (X,Y ), we will learn a parametric model (with parameters θ), by solving the following optimization problem:
θ∗ = arg max θ ∑ X,Y log p(Y |X; θ) (1)
where the sum is assumed to be over all training examples.",4 The LSTM model,[0],[0]
"To model the probability p, we use the same architecture described by Sutskever et al. (2014).",4 The LSTM model,[0],[0]
"In particular, we use a RNN based on the Long Short Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997), designed to avoid vanishing gradients and to remember some long-distance dependences from the input sequence.",4 The LSTM model,[0],[0]
Figure 1 shows a basic LSTM architecture.,4 The LSTM model,[0],[0]
"The RNN is fed with input words Xi (one at a time), until we feed a special symbol “GO”.",4 The LSTM model,[0],[0]
"It is now a common practice (Sutskever et al., 2014; Li & Jurafsky, 2015) to start feeding the input in reversed order, as it has been shown to perform better empirically.",4 The LSTM model,[0],[0]
"During the first pass over the input, the network is expected to learn a compact, distributed representation of the input sentence, which will allow it to start generating the right predictions when the second pass starts, after the “GO” symbol is read.
",4 The LSTM model,[0],[0]
"We can apply the chain rule to decompose Equation (1) as follows:
p(Y |X; θ) = T∏ t=1 p(Yt|Y1, . . .",4 The LSTM model,[0],[0]
", Yt−1, X; θ) (2)
noting that we made no independence assumptions.",4 The LSTM model,[0],[0]
"Once we find the optimal θ∗, we construct our estimated compression Ŷ as:
Ŷ = arg max Y
p(Y |X; θ∗) (3)
LSTM cell: Let us review the sequence-tosequence LSTM model.",4 The LSTM model,[0],[0]
The Long Short Term Memory model of Hochreiter & Schmidhuber (1997) is defined as follows.,4 The LSTM model,[0],[0]
"Let xt, ht, and mt be the input, control state, and memory state at timestep t. Then, given a sequence of inputs (x1, . . .",4 The LSTM model,[0],[0]
", xT ), the LSTM computes the h-sequence (h1, . . .",4 The LSTM model,[0],[0]
", hT ) and the m-sequence (m1, . . .",4 The LSTM model,[0],[0]
",mT ) as follows
it =",4 The LSTM model,[0],[0]
"sigm(W1xt +W2ht−1) i′t = tanh(W3xt +W4ht−1) ft = sigm(W5xt +W6ht−1) ot = sigm(W7xt +W8ht−1) mt = mt−1 ft + it i′t ht = mt ot
The operator denotes element-wise multiplication, the matrices W1, . . .",4 The LSTM model,[0],[0]
",W8 and the vector h0 are the parameters of the model, and all the nonlinearities are computed element-wise.
",4 The LSTM model,[0],[0]
Stochastic gradient descent is used to maximize the training objective (Eq. (1)),4 The LSTM model,[0],[0]
w.r.t.,4 The LSTM model,[0],[0]
"all the LSTM parameters.
",4 The LSTM model,[0],[0]
Network architecture:,4 The LSTM model,[0],[0]
In these experiments we have used the architecture depicted in Figure 3.,4 The LSTM model,[0],[0]
"Following Vinyals et al. (2014), we have used three stacked LSTM layers to allow the upper layers to learn higher-order representations of the input, interleaved with dropout layers to prevent overfitting (Srivastava et al., 2014).",4 The LSTM model,[0],[0]
"The output layer is a SoftMax classifier that predicts, after the “GO” symbol is read, one of the following three
labels: 1, if a word is to be retained in the compression, 0 if a word is to be deleted, or EOS, which is the output label used for the “GO” input and the end-of-sentence final period.
",4 The LSTM model,[0],[0]
"Input representation: In the simplest implementation, that we call LSTM, the input layer has 259 dimensions.",4 The LSTM model,[0],[0]
"The first 256 contain the embedding-vector representation of the current in-
put word, pre-trained using the Skipgram model2",4 The LSTM model,[0],[0]
"(Mikolov et al., 2013).",4 The LSTM model,[0],[0]
"The final three dimensions contain a one-hot-spot representation of the goldstandard label of the previous word (during training), or the generated label of the previous word (during decoding).
",4 The LSTM model,[0],[0]
"For the LSTM+PAR architecture we first parse the input sentence, and then we provide as input, for each input word, the embedding-vector representation of that word and its parent word in the dependency tree.",4 The LSTM model,[0],[0]
"If the current input is the root node, then a special parent embedding is constructed with all nodes set to zero except for one node.",4 The LSTM model,[0],[0]
In these settings we want to test the hypothesis whether knowledge about the parent node can be useful to decide if the current constituent is relevant or not for the compression.,4 The LSTM model,[0],[0]
The dimensionality of the input layer in this case is 515.,4 The LSTM model,[0],[0]
"Similarly to McDonald (2006), syntax is used here as a soft feature in the model.
",4 The LSTM model,[0],[0]
"For the LSTM+PAR+PRES architecture, we again parse the input sentence, and use a 518-sized embedding vector, that includes: • The embedding vector for the current word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The embedding vector for the parent word
(256 dimensions).",4 The LSTM model,[0],[0]
"• The label predicted for the last word (3 di-
mensions).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has 2https://code.google.com/p/word2vec/
already been seen and kept in the compression (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word has
already been seen but discarded (1 dimension).",4 The LSTM model,[0],[0]
"• A bit indicating whether the parent word
comes later in the input (1 dimension).
",4 The LSTM model,[0],[0]
Decoding: Eq. (3) involves searching through all possible output sequences (given X).,4 The LSTM model,[0],[0]
"Contrary to the baseline, in the case of LSTMs the complete previous history is taken into account for each prediction and we cannot simplify Eq.",4 The LSTM model,[0],[0]
(2) with a Markov assumption.,4 The LSTM model,[0],[0]
"Therefore, the search space at decoding time is exponential on the length of the input, and we have used a beam-search procedure as described in Figure 2.
",4 The LSTM model,[0],[0]
"Fixed parameters: For training, we unfold the network 120 times and make sure that none of our training instances is longer than that.",4 The LSTM model,[0],[0]
"The learning rate is initialized at 2, with a decay factor of 0.96 every 300,000 traning steps.",4 The LSTM model,[0],[0]
The dropping probability for the dropout layers is 0.2.,4 The LSTM model,[0],[0]
The number of nodes in each LSTM layer is always identical to the number of nodes in the input layer.,4 The LSTM model,[0],[0]
We have not tuned these parameters nor the number of stacked layers.,4 The LSTM model,[0],[0]
Both the LSTM systems we introduced and the baseline require a training set of a considerable size.,5.1 Data,[0],[0]
"In particular, the LSTM model uses 256- dimensional embeddings of token sequences and cannot be expected to perform well if trained on a thousand parallel sentences, which is the size of the commonly used data sets (Knight & Marcu, 2000; Clarke & Lapata, 2006).",5.1 Data,[0],[0]
"Following the method of Filippova & Altun (2013), we collect a much larger corpus of about two million parallel sentence-compression instances from the news where every compression is a subsequence of tokens from the input.",5.1 Data,[0],[0]
"For testing, we use the publicly released set of 10,000 sentence-compression pairs3.",5.1 Data,[0],[0]
"We take the first 200 sentences from this set for the manual evaluation with human raters, and the first 1,000 sentences for the automatic evaluation.",5.1 Data,[0],[0]
We evaluate the baseline and our systems on the 200-sentence test set in an experiment with human raters.,5.2 Experiments,[0],[0]
The raters were asked to rate readability and informativeness of compressions given the input which are the standard evaluation metrics for compression.,5.2 Experiments,[0],[0]
"The former covers the grammatical correctness, comprehensibility and fluency of the output while the latter measures the amount of important content preserved in the compression.
",5.2 Experiments,[0],[0]
"Additionally, for experiments on the development set, we used two metrics for automatic evaluation: per-sentence accuracy (i.e., how many compressions could be fully reproduced) and word-based F1-score.",5.2 Experiments,[0],[0]
The latter differs from the RASP-based relation F-score by Riezler et al. (2003) in that we simply compute the recall and precision in terms of tokens kept in the golden and the generated compressions.,5.2 Experiments,[0],[0]
"We report these results for completeness although it is the results of the human evaluation from which we draw our conclusions.
",5.2 Experiments,[0],[0]
Compression ratio:,5.2 Experiments,[0],[0]
The three versions of our system (LSTM*) and the baseline (MIRA) have comparable compression ratios (CR) which are defined as the length of the compression in characters divided over the sentence length.,5.2 Experiments,[0],[0]
"Since the
3http://storage.googleapis.com/ sentencecomp/compressiondata.json
ratios are very close, a comparison of the systems’ scores is justified (Napoles et al., 2011).
",5.2 Experiments,[0],[0]
"Automatic evaluation: A total of 1,000 sentence pairs from the test set4 were used in the automatic evaluation.",5.2 Experiments,[0],[0]
"The results are summarized in Table 1.
",5.2 Experiments,[0],[0]
"There is a significant difference in performance of the MIRA baseline and the LSTM models, both in terms of F1-score and in accuracy.",5.2 Experiments,[0],[0]
More than 30% of golden compressions could be fully regenerated by the LSTM systems which is in sharp contrast with the 20% of MIRA.,5.2 Experiments,[0],[0]
"The differences in F-score between the three versions of LSTM are not significant, all scores are close to 0.81.
",5.2 Experiments,[0],[0]
"Evaluation with humans: The first 200 sentences from the set of 1,000 used in the automatic evaluation were compressed by each of the four systems.",5.2 Experiments,[0],[0]
"Every sentence-compression pair was rated by three raters who were asked to select a rating on a five-point Likert scale, ranging from one to five.",5.2 Experiments,[0],[0]
In very few cases (around 1%),5.2 Experiments,[0],[0]
"the ratings were inconclusive (i.e., 1, 3, 5 were given to the same pair) and had to be skipped.",5.2 Experiments,[0],[0]
"Table 2 summarizes the results.
",5.2 Experiments,[0],[0]
The results indicate that the LSTM models produce more readable and more informative compressions.,5.2 Experiments,[0],[0]
"Interestingly, there is no benefit in using the syntactic information, at least not with
4We used the very first 1,000 instances.
",5.2 Experiments,[0],[0]
the amount of parallel data we had at our disposal.,5.2 Experiments,[0],[0]
"The simple LSTM model which only uses token embeddings to generate a sequence of deletion decisions significantly outperforms the baseline which was given not only embeddings but also syntactic and other features.
",5.2 Experiments,[0],[0]
Discussion: What are the wins and losses of the LSTM systems?,5.2 Experiments,[0],[0]
Figure 4 presents some of the evaluated sentence-compression pairs.,5.2 Experiments,[0],[0]
"In terms of readability, the basic LSTM system performed surprisingly well.",5.2 Experiments,[0],[0]
Only in a few cases (out of 200) did it get an average score of two or three.,5.2 Experiments,[0],[0]
"Sentences which pose difficulty to the model are the ones with quotes, intervening commas, or other uncommon punctuation patterns.",5.2 Experiments,[0],[0]
"For example, in the second sentence in Figure 4, if one removes from the input the age modifiers and the preceding commas, the words and Chris Martin are not
dropped and the output compression is grammatical, preserving both conjoined elements.
",5.2 Experiments,[0],[0]
"With regard to informativeness, the difficult cases are those where there is very little to be removed and where the model still removed more than a half to achieve the compression ratio it observed in the training data.",5.2 Experiments,[0],[0]
"For example, the only part that can be removed from the fourth sentence in Figure 4 is the modifier of police, everything else being important content.",5.2 Experiments,[0],[0]
"Similarly, in the fifth sentence the context of the event must be retained in the compression for the event to be interpreted correctly.
",5.2 Experiments,[0],[0]
"Arguably, such cases would also be difficult for other systems.",5.2 Experiments,[0],[0]
"In particular, recognizing when the context is crucial is a problem that can be solved only by including deep semantic and discourse features which has not been attempted yet.",5.2 Experiments,[0],[0]
"And
sentences with quotes (direct speech, a song or a book title, etc.) are challenging for parsers which in turn provide important signals for most compression systems.
",5.2 Experiments,[0],[0]
The bottom of Figure 4 contains examples of good compressions.,5.2 Experiments,[0],[0]
"Even though for a significant number of input sentences the compression was a continuous subsequence of tokens, there are many discontinuous compressions.",5.2 Experiments,[0],[0]
"In particular, the LSTM model learned to drop appositions, no matter how long they are, temporal expressions, optional modifiers, introductory clauses, etc.
",5.2 Experiments,[0],[0]
"Our understanding of why the extended model (LSTM+PAR+PRES) performed worse in the human evlauation than the base model is that, in the absence of syntactic features, the basic LSTM learned a model of syntax useful for compression, while LSTM++, which was given syntactic information, learned to optimize for the particular way the ”golden” set was created (tree pruning).",5.2 Experiments,[0],[0]
"While the automatic evaluation penalized all deviations from the single golden variant, in human evals there was no penalty for readable alternatives.",5.2 Experiments,[0],[0]
"We presented, to our knowledge, a first attempt at building a competitive compression system which is given no linguistic features from the input.",6 Conclusions,[0],[0]
"The two important components of the system are (1) word embeddings, which can be obtained by anyone either pre-trained, or by running word2vec on a large corpus, and (2) an LSTM model which draws on the very recent advances in research on RNNs.",6 Conclusions,[0],[0]
"The training data of about two million sentence-compression pairs was collected automatically from the Internet.
",6 Conclusions,[0],[0]
Our results clearly indicate that a compression model which is not given syntactic information explicitly in the form of features may still achieve competitive performance.,6 Conclusions,[0],[0]
The high readability and informativeness scores assigned by human raters support this claim.,6 Conclusions,[0],[0]
"In the future, we are planning to experiment with more “interesting” paraphrasing models which translate the input not into a zero-one sequence but into words.",6 Conclusions,[0],[0]
"We present an LSTM approach to deletion-based sentence compression where the task is to translate a sentence into a sequence of zeros and ones, corresponding to token deletion decisions.",abstractText,[0],[0]
"We demonstrate that even the most basic version of the system, which is given no syntactic information (no PoS or NE tags, or dependencies) or desired compression length, performs surprisingly well: around 30% of the compressions from a large test set could be regenerated.",abstractText,[0],[0]
We compare the LSTM system with a competitive baseline which is trained on the same amount of data but is additionally provided with all kinds of linguistic features.,abstractText,[0],[0]
In an experiment with human raters the LSTMbased model outperforms the baseline achieving 4.5 in readability and 3.8 in informativeness.,abstractText,[0],[0]
Sentence Compression by Deletion with LSTMs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2453",text,[0],[0]
Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency.,1 Introduction,[0],[0]
"The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001).
",1 Introduction,[0],[0]
"The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013).",1 Introduction,[0],[0]
"Irrespective of how
1Publicly available for download at https://github. com/Jmallins/MOSS
the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words.",1 Introduction,[0],[0]
"More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016).",1 Introduction,[0],[0]
Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997).,1 Introduction,[0],[0]
"Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015).
",1 Introduction,[0],[0]
"Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g., part-of-speech tags, parse trees).",1 Introduction,[0],[0]
"In order to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller.",1 Introduction,[0],[0]
"For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products.",1 Introduction,[0],[0]
"Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences).",1 Introduction,[0],[0]
Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles.,1 Introduction,[0],[0]
"More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts).
",1 Introduction,[0],[0]
"2Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million.
",1 Introduction,[0],[0]
"Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015).",1 Introduction,[0],[0]
"As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics).",1 Introduction,[0],[0]
"And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions.
",1 Introduction,[0],[0]
Our work addresses the paucity of data for sentence compression models.,1 Introduction,[0],[0]
"We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements).",1 Introduction,[0],[0]
"We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016).",1 Introduction,[0],[0]
"Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.",1 Introduction,[0],[0]
"We also demonstrate that models trained on multilingual data perform well out-of-domain.
",1 Introduction,[0],[0]
"Although our approach does not employ compression corpora for training, for evaluation purposes, we create MOSS, a new Multilingual Compression dataset for English, French, and German.",1 Introduction,[0],[0]
"MOSS is a parallel corpus containing documents from the European parliament proceedings, TED talks, news commentaries, and the EU bookshop.",1 Introduction,[0],[0]
"Each document is written in English, French, and German, and compressed by native speakers of the respective language who process a document at a time.",1 Introduction,[0],[0]
"We obtain five compressions per document leading to 2,000 long-short sentence pairs per language.",1 Introduction,[0],[0]
"Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010)",1 Introduction,[0],[0]
"our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents.
",1 Introduction,[0],[0]
There has been relatively little interest in compressing languages other than English.,1 Introduction,[0],[0]
"A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese.",1 Introduction,[0],[0]
"There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language.",1 Introduction,[0],[0]
"Overall, there are no standardized datasets in languages other than English, either for training or testing.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is available in the bilingual parallel data; and the release of a multilingual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling.",1 Introduction,[0],[0]
"In our pivot-based sentence compression model an input sequence is first translated into a foreign language, and then back into the source language.",2 Pivot-based Neural Compression,[0],[0]
"Unlike previous paraphrasing pivoting models (Mallinson et al., 2017), we parameterize our translation models with a length feature, which allows us to produce compressed output.",2 Pivot-based Neural Compression,[0],[0]
"We define two models, performing compression in one step or alternatively in two steps which affords more flexibility in model output.",2 Pivot-based Neural Compression,[0],[0]
"In the neural encoder-decoder framework for MT (Bahdanau et al., 2015; Sutskever et al., 2014), an encoder takes in a source X =",2.1 NMT Background,[0],[0]
"(x1, ...,xTx) of length Tx and the decoder generates a target sequence (y1, ...,yTy) of length Ty.",2.1 NMT Background,[0],[0]
"Let hi be the hidden state of the source symbol at position i, obtained by concatenating the forward and backward encoder RNN hidden states, hi =",2.1 NMT Background,[0],[0]
[ −→ hi ; ←−,2.1 NMT Background,[0],[0]
hi ].,2.1 NMT Background,[0],[0]
"We deviate from previous work (Bahdanau et al., 2015; Sutskever et al., 2014) in that we initialize the decoder with the average of the hidden states, following Sennrich et al. (2017):
s0 = tanh(Winit ∑Txi=1",2.1 NMT Background,[0],[0]
"hi
Tx ) (1)
where Winit is a learnt parameter.",2.1 NMT Background,[0],[0]
"Our decoder is a conditional recurrent neural network, specifically a gated recurrent unit (GRU, Cho et al., 2014) with attention, which we denote as cGRUatt .",2.1 NMT Background,[0],[0]
"cGRUatt takes as input the previous hidden state s j−1, the source annotations C = h1, ...,hTx , and the previously decoded symbol y j−1 in order to update its hidden state s j, which is used to decode symbol y j at position j:
s j = cGRUatt(s j−1,y j−1,C)",2.1 NMT Background,[0],[0]
"(2)
cGRUatt consists of three components.",2.1 NMT Background,[0],[0]
The first combines the previously decoded symbol y j−1 and the previous hidden state s j−1 to generate an intermediate representation s′j.,2.1 NMT Background,[0],[0]
"The attention mechanism, AT T , inputs the entire context set C along with intermediate hidden state s′j in order to compute the context vector c j:
c j = AT T (C,s′j) =",2.1 NMT Background,[0],[0]
"Tx
∑ i αi jhi (3)
",2.1 NMT Background,[0],[0]
"αi j = exp(ei j)
∑Txk=1 exp(ek j) (4)
ei j = f",2.1 NMT Background,[0],[0]
"(s′j,hi) (5)
Where αi j is the normalized alignment weight between the source symbol at position i and the target symbol at position j, and f is a feedfoward neural network.
",2.1 NMT Background,[0],[0]
"Finally, we generate s j, the hidden state of cGRUatt , by using the intermediate representation s′j and",2.1 NMT Background,[0],[0]
"the context vector c j. Given s j, y j−1, and c j",2.1 NMT Background,[0],[0]
"the output probability p(y j|s j,y j−1,c j) is computed using a feedforward neural network with a softmax activation.",2.1 NMT Background,[0],[0]
"We define the probability of sequence y as:
P(y|x;θ) =",2.1 NMT Background,[0],[0]
"Ty
∏ j=1 p(y j|s j,y j−1,c j) (6)",2.1 NMT Background,[0],[0]
"To be able to produce compressed sentences, we parameterize our model with a length vector which allows to control the output length.",2.2 Length Control,[0],[0]
"Our approach is similar to the LenInit model of Kikuchi et al. (2016), however we use a GRU instead of an LSTM.",2.2 Length Control,[0],[0]
"The hidden state of the decoder consists of the average of the encoder’s hidden states but also a length vector LV , a learnt parameter, which is scaled by the desired target length Ty′ .",2.2 Length Control,[0],[0]
"We therefore rewrite Equation (1) as follows:
s′0 = tanh ( Winit [∑Txi=1 hi
Tx ;LV ·Ty′
]) (7)
As such we now define our model as:
P(y|x,Ty′ ;θ) (8)
During training, the target length is set to Ty′ = Ty.",2.2 Length Control,[0],[0]
"However, at test time, the target length generally varies according to the domain, genre, and language at hand.",2.2 Length Control,[0],[0]
We determine the target length experimentally based on a small validation set.,2.2 Length Control,[0],[0]
"Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language.",2.3 Pivoting,[0],[0]
"The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016).
",2.3 Pivoting,[0],[0]
"We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form.",2.3 Pivoting,[0],[0]
"We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1, ..., fK}.",2.3 Pivoting,[0],[0]
"The probability of generating compression y = y1...yTy is decomposed as:
P(y|x) =",2.3 Pivoting,[0],[0]
"Fx
∑ f
P(y| f ; −→ θ ) ·P( f |x; ←− θ ) (9)
which we approximate as the tokenwise weighted average of the pivots:
P(y|x)≈ Ty
∏ j=1
Fx ∑ f P(y j|y< j, f )P( f |x) (10)
where y< j = y1, ...y j .",2.3 Pivoting,[0],[0]
"To ensure a probability distribution, we normalize the K-best list Fx, such that the translation probabilities sum to one.",2.3 Pivoting,[0],[0]
We use beam search to decode tokens by conditioning on multiple pivoting sentences.,2.3 Pivoting,[0],[0]
"The results with the best decoding scores are considered candidate compressions.
",2.3 Pivoting,[0],[0]
"To ensure the model produces compressed output, we extend the pivoting approach in two ways.",2.3 Pivoting,[0],[0]
"In single step compression, one of the translation
models is parameterized with length information:
P(y|x,Ty′)",2.3 Pivoting,[0],[0]
"≈ F
∑ f
P(y| f ,Ty′ ; −→ θ ) · P( f |x; ←− θ )
",2.3 Pivoting,[0],[0]
"In dual-step compression, we parameterize both translation models with length information:
P(y|x,Ty′ ,Ty′′)≈ F
∑ f
P(y| f ,Ty′ ;",2.3 Pivoting,[0],[0]
"−→ θ )·P( f |x,Ty′′ ; ←− θ )
",2.3 Pivoting,[0],[0]
"We find that dual-compression performs better when the system is expected to drastically compress the source sentence (e.g., in a headline generation task).",2.3 Pivoting,[0],[0]
Imposing a high compression ratio from the start tends to produce unintelligible text.,2.3 Pivoting,[0],[0]
"The model attempts to reduce the length of the source at all costs, even at the expense of being semantically faithful to the input.",2.3 Pivoting,[0],[0]
"Performing two moderate compressions in succession reduces both length and content conservatively and as a result produces more meaningful text.
",2.3 Pivoting,[0],[0]
In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions.,2.3 Pivoting,[0],[0]
We show the output of a single-step compression model on three languages initialized with varying compression rates3,2.3 Pivoting,[0],[0]
(see Section 4 for details on how the models were trained and tested).,2.3 Pivoting,[0],[0]
"The compression rate (CR) is used to determine length parameter of Equation (8):
Ty′ =",2.3 Pivoting,[0],[0]
"Tx ·CR (11)
",2.3 Pivoting,[0],[0]
"The figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source
3The term refers to the percentage of words retained from the source sentence in the compression.
language (Mallinson et al., 2017).",2.3 Pivoting,[0],[0]
We can see that the majority of sentences are generated with length close to the desired compression rate.,2.3 Pivoting,[0],[0]
"For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French.",3 The MOSS Dataset,[0],[0]
The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre.,3 The MOSS Dataset,[0],[0]
"We sampled five documents each from:
1.",3 The MOSS Dataset,[0],[0]
"Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment).
2.",3 The MOSS Dataset,[0],[0]
"The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues.
3.",3 The MOSS Dataset,[0],[0]
"The EU bookshop corpus (Skadiņš et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel.
4.",3 The MOSS Dataset,[0],[0]
"The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs).
",3 The MOSS Dataset,[0],[0]
We obtained compressions using the Crowdflower platform.,3 The MOSS Dataset,[0],[0]
Crowdworkers were given instructions that explained the task and defined sentence compression with the aid of examples.,3 The MOSS Dataset,[0],[0]
"They
were asked to compress while preserving the most important information, ensuring the sentences remained grammatical and meaning preserving.",3 The MOSS Dataset,[0],[0]
"Annotators were encouraged to use any rewriting operations that seemed appropriate, e.g., to delete words, add new words, substitute them, or reorder them.",3 The MOSS Dataset,[0],[0]
"Annotation proceeded on a document-bydocument basis, line-by-line.",3 The MOSS Dataset,[0],[0]
Crowdworkers compressed the first twenty lines of each document and we elicited five compression per document.,3 The MOSS Dataset,[0],[0]
"Example compressions are shown in Table 1.
",3 The MOSS Dataset,[0],[0]
Table 2 presents various statistics on our corpus.,3 The MOSS Dataset,[0],[0]
"As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between.",3 The MOSS Dataset,[0],[0]
"We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances.",3 The MOSS Dataset,[0],[0]
"Overall, French speakers seem more conservative when shortening sentences compared to English and German.",3 The MOSS Dataset,[0],[0]
"In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED).",3 The MOSS Dataset,[0],[0]
"We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure com-
monly used to automatically evaluate the quality of machine translation output.",3 The MOSS Dataset,[0],[0]
We used TER to compute the (average) number of edits required to change a long sentence to shorter output.,3 The MOSS Dataset,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences.",3 The MOSS Dataset,[0],[0]
We observe that crowdworkers perform a fair amount of rewriting across corpora and languages.,3 The MOSS Dataset,[0],[0]
"The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions.",3 The MOSS Dataset,[0],[0]
"Neural Machine Translation Training Nematus (Sennrich et al., 2017) was used as the machine translation system for all our experiments.",4 Experimental Setup,[0],[0]
We generally used the default settings and training procedures as specified within Nematus.,4 Experimental Setup,[0],[0]
"All networks have a hidden layer size of 1,000, and an embedding layer size of 512.",4 Experimental Setup,[0],[0]
"In addition, layer normalization (Ba et al., 2016) was used.",4 Experimental Setup,[0],[0]
"During training, we used ADAM (Kingma and Ba, 2014), a minibatch size of 80, and the training set was reshuffled between epochs.",4 Experimental Setup,[0],[0]
"We also employed early stopping.
",4 Experimental Setup,[0],[0]
"We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19).",4 Experimental Setup,[0],[0]
German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task.,4 Experimental Setup,[0],[0]
"The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively.",4 Experimental Setup,[0],[0]
"We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems.",4 Experimental Setup,[0],[0]
"The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007).",4 Experimental Setup,[0],[0]
"Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b).",4 Experimental Setup,[0],[0]
"The BPE operations are shared between language directions.
",4 Experimental Setup,[0],[0]
We experimented with various model variants using one or multiple pivots.,4 Experimental Setup,[0],[0]
The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs).,4 Experimental Setup,[0],[0]
"Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2.
",4 Experimental Setup,[0],[0]
"4BLEU scores were calculated using mteval-v13a.pl.
",4 Experimental Setup,[0],[0]
"Comparison Systems We compared our model against ABS, a sequence-to-sequence attentionbased model, developed by Rush et al. (2015).",4 Experimental Setup,[0],[0]
"This model was trained on a monolingual dataset extracted from the Annotated English Gigaword corpus (Napoles et al., 2011).",4 Experimental Setup,[0],[0]
The dataset consists of approximately 4 million pairs of the first sentence from each source document and its headline.,4 Experimental Setup,[0],[0]
"We also trained LenInit (Kikuchi et al., 2016) on the same corpus which is conceptually similar to ABS but additionally controls the output length using a length embedding vector (as described in Section 2.2).5",4 Experimental Setup,[0],[0]
"Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale.",4 Experimental Setup,[0],[0]
An obvious workaround is to translate Gigaword to French and German and then train compression models on the translated data.,4 Experimental Setup,[0],[0]
"As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German.",4 Experimental Setup,[0],[0]
"Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set.",4 Experimental Setup,[0],[0]
"MOSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006).",5 Results,[0],[0]
"These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1).",5 Results,[0],[0]
"We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples.
",5 Results,[0],[0]
Table 3(a) reports results on English with a model which controls the output length (L) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10).,5 Results,[0],[0]
We experimented with French (fr) or German (de) as pivot languages.,5 Results,[0],[0]
All pivot-based models perform compression in a single step (see Section 2.3).,5 Results,[0],[0]
"Dual-step compres-
5We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016).
",5 Results,[0],[0]
"6We add a begin-of-sentence marker at the start of the candidate and reference sentences.
",5 Results,[0],[0]
sion obtained inferior results which we omit for the sake of brevity.,5 Results,[0],[0]
"As can be seen, models which use a single pivot are better than those using multiple ones (German is better than French; see SPde vs SP f r).",5 Results,[0],[0]
"More pivots might introduce noise at the expense of translation quality.
",5 Results,[0],[0]
"Overall, pivot-based models outperform ABS and LenInit.",5 Results,[0],[0]
This is perhaps to be expected since these models are tested on out of domain data with different vocabulary and writing conventions; MOSS does not contain any newspaper articles.,5 Results,[0],[0]
"Unfortunately, it is not possible to train ABS and LenInt on in-domain data as compression data only exists for the headlines-first sentences pairs.",5 Results,[0],[0]
"As an upper bound, we also report how well humans agree with each other, treating one (randomly selected) reference as system output and computing how it agrees with the rest (row Gold in Table 3).",5 Results,[0],[0]
"All models lag significantly behind human performance on this task.
",5 Results,[0],[0]
"Tables 3(b) and 3(c) report results on French and German, respectively.",5 Results,[0],[0]
"For these languages, we obtained best results with English as pivot, using a single-step compression model.",5 Results,[0],[0]
"ABS and LenInit perform poorly when trained directly on translations of Gigaword into French and German; their performance improves considerably when they are trained on the Gigaword and used to compress English translations of French or German (ABSen, LenIniten).",5 Results,[0],[0]
"Again, we observe that our models (SPL ,en, MPL ,en) outperform the comparison systems across all metrics and that using a single pivot yields better compressions.",5 Results,[0],[0]
Example compressions are given in Table 4 where we show output produced by ABS and SP for each language (see the supplementary material for more examples).,5 Results,[0],[0]
"Finally, notice that automatic scores for the prefix baseline across languages are misleadingly high, since it simply repeats the source sentence up to a fixed length without performing any rewriting.
",5 Results,[0],[0]
We also elicited human judgments through the Crowdflower platform.,5 Results,[0],[0]
We asked crowdworkers to rate the grammaticality of the target compressions and whether they preserved the most important information from the source.,5 Results,[0],[0]
"In both cases, they used a five-point rating scale where a high number indicates better performance.",5 Results,[0],[0]
"We randomly selected 25 sentences from each corpus from the test portion of MOSS, i.e., 100 long-short sentence pairs per language.",5 Results,[0],[0]
"We compared compressions generated by our model (SPL ), with ABS models for the three languages, the prefix baseline, and (randomly selected) gold-standard reference (Ref) compressions from MOSS.",5 Results,[0],[0]
All systems used the length parameter to allow comparisons with approximately the same compression rates.,5 Results,[0],[0]
We collected five ratings per compression.,5 Results,[0],[0]
Our results are summarized in Table 5.,5 Results,[0],[0]
"We show mean ratings for grammaticality (Gram), importance (Imp) and their combination (column Avg).",5 Results,[0],[0]
"Across languages our model (SPL) significantly (p < 0.05) outperforms comparison systems (Pfix, ABS) on both dimensions of grammaticality and importance (significance tests were performed using a student t-test).",5 Results,[0],[0]
"All systems are significantly worse (p < 0.05) than the human reference compressions.
",5 Results,[0],[0]
"Finally, in Table 6 we analyze the output of our best model (SPL ) using the same statistics we applied to the human compressions (see Table 2).",5 Results,[0],[0]
"As can be seen, the model generally compressess more aggressively and applies more ed-
its than the crowdworkers (both compression rates and TER scores are higher for all three languages).",5 Results,[0],[0]
"Although the rate of deletions is similar to humans, insertions, substitutions and shifts happen to a greater extent for our model, indicating that it performs a good amount of paraphrasing.
",5 Results,[0],[0]
"DUC-2004 Evaluation Besides MOSS, we evaluated our model on the benchmark DUC-2004 task-1 dataset.",5 Results,[0],[0]
"In this task, the aim is to create a very short summary (75 bytes) for a document.",5 Results,[0],[0]
The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries.,5 Results,[0],[0]
"We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary.",5 Results,[0],[0]
"To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries.
",5 Results,[0],[0]
Our results are shown in Table 7.,5 Results,[0],[0]
"To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE-L).9",5 Results,[0],[0]
We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants.,5 Results,[0],[0]
"We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword.",5 Results,[0],[0]
"ABS+ applies minimum error rate (MERT) training as a copy-
7Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35
8Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70
9We used ROUGE version 1.5.5 with the original DUC-2004 ROUGE parameters.
",5 Results,[0],[0]
ing mechanism.,5 Results,[0],[0]
"LenEmb and LenInit include a length parameter (Kikuchi et al., 2016), whereas RAS uses a specialized recurrent neural network architecture (Elman, 1990).",5 Results,[0.9583060593579984],"['For instance, m ∈M could be a GP (Saatçi et al., 2010), a time-deterministic regression (Fearnhead, 2005) or a mixture distribution (Caron et al., 2012).']"
We also report how well DUC-2004 abstractors agree with each other (row Gold in Table 7).,5 Results,[0],[0]
"Example compressions are given in Table 8, where we show output produced by SPL ,de and a human reference (see the supplementary material for further examples).
",5 Results,[0],[0]
Using automatic metrics we see that our model generally performs worse compared to these systems and that German is the best pivot for English.,5 Results,[0],[0]
"Although the objective of this paper is not to obtain state-of-the-art scores on this evaluation set, it is interesting to see that our model is able to compress out-of-domain.",5 Results,[0],[0]
"We do not have access to headline-first sentence pairs, while all comparison systems do.",5 Results,[0],[0]
We also elicited human judgments on the compressions of 100 lead sentences whose documents were randomly selected from the DUC-2004 test set.,5 Results,[0],[0]
"We compared the prefix baseline, our model (SPL ,de), ABS+ (Rush et al., 2015), LenEmb (Kikuchi et al., 2016), Topiary (Zajic et al., 2004), and a randomly selected reference.",5 Results,[0],[0]
Topiary came top in almost all measures in the DUC-2004 evaluation; it first compresses the lead sentence using linguistically motivated heuristics and then enhances it with topic keywords.,5 Results,[0],[0]
"Crowdworkers rated grammaticality and importance, using a five-point scale; we collected five ratings per compression.
",5 Results,[0],[0]
As shown in Table 9 ABS+ has the lead with our system following suit.,5 Results,[0],[0]
"In terms of grammaticality, ABS+ and SPL ,de are not significantly different from the gold standard or from each other (Pfix, Topiary, and LenEmb are significantly worse than Gold; p < 0.05).",5 Results,[0],[0]
"In terms of importance, pairwise differences between systems and the gold standard are not significant.",5 Results,[0],[0]
"Overall, we observe that SPL ,de performs comparably to ABS+ even though it was
not trained on any compression specific data.",5 Results,[0],[0]
Inspection of system output reveals that our model performs more paraphrasing than comparison systems (a conclusion also confirmed by the statistics in Table 6).,5 Results,[0],[0]
In this paper we have shown that multilingual corpora can be used to bootstrap compression models across languages and text genres.,6 Conclusions,[0],[0]
Our approach adapts existing neural machine translation machinery to the compression task coupled with methods which decode the output to a desired length.,6 Conclusions,[0],[0]
"An interesting direction for future work would be to train our model using reinforcement learning (Ranzato et al., 2016; Zhang and Lapata, 2017) in order to control the compression output more directly.",6 Conclusions,[0],[0]
"Moreover, although we do not use any direct supervision in our experiments, it would be interesting to incorporate it as a means of domain adaptation (Cheng et al., 2016).
",6 Conclusions,[0],[0]
Acknowledgments The authors gratefully acknowledge the support of the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1; Mallinson) and the European Research Council (award number 681760; Lapata).,6 Conclusions,[0],[0]
In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models.,abstractText,[0],[0]
Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length.,abstractText,[0],[0]
Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data.,abstractText,[0],[0]
"We release1 MOSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.",abstractText,[0],[0]
Sentence Compression for Arbitrary Languages via Multilingual Pivoting,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 584–594 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"The main goal of sentence simplification is to reduce the linguistic complexity of text, while still retaining its original information and meaning.",1 Introduction,[0],[0]
"The simplification task has been the subject of several modeling efforts in recent years due to its relevance for NLP applications and individuals alike (Siddharthan, 2014; Shardlow, 2014).",1 Introduction,[0],[0]
"For instance, a simplification component could be used as a preprocessing step to improve the performance of parsers (Chandrasekar et al., 1996), summarizers (Beigman Klebanov et al., 2004), and semantic role labelers (Vickrey and Koller, 2008; Woodsend and Lapata, 2014).",1 Introduction,[0.9517304858966462],"['Unlike previous extensions of the al- gorithm (e.g. Adams & MacKay, 2007; Saatçi et al., 2010), this avoids having to guess a single best model a priori.']"
"Automatic simplification would also benefit people with low-literacy skills (Watanabe et al., 2009), such as children and
1Our code and data are publicly available at https:// github.com/XingxingZhang/dress.
non-native speakers as well as individuals with autism (Evans et al., 2014), aphasia (Carroll et al., 1999), or dyslexia (Rello et al., 2013).
",1 Introduction,[0],[0]
"The most prevalent rewrite operations which give rise to simplified text include substituting rare words with more common words or phrases, rendering syntactically complex structures simpler, and deleting elements of the original text (Siddharthan, 2014).",1 Introduction,[0],[0]
Earlier work focused on individual aspects of the simplification problem.,1 Introduction,[0],[0]
"For example, several systems performed syntactic simplification only, using rules aimed at sentence splitting (Carroll et al., 1999; Chandrasekar et al., 1996; Vickrey and Koller, 2008; Siddharthan, 2004) while others turned to lexical simplification by substituting difficult words with more common WordNet synonyms or paraphrases (Devlin, 1999; Inui et al., 2003; Kaji et al., 2002).
",1 Introduction,[0],[0]
Recent approaches view the simplification process more holistically as a monolingual textto-text generation task borrowing ideas from statistical machine translation.,1 Introduction,[0],[0]
Simplification rewrites are learned automatically from examples of complex-simple sentences extracted from online resources such as the ordinary and simple English Wikipedia.,1 Introduction,[0],[0]
"For example, Zhu et al. (2010) draw inspiration from syntax-based translation and propose a model similar to Yamada and Knight (2001) which additionally performs simplification-specific rewrite operations (e.g., sentence splitting).",1 Introduction,[0],[0]
"Woodsend and Lapata (2011) formulate simplification in the framework of Quasi-synchronous grammar (Smith and Eisner, 2006) and use integer linear programming to score the candidate translations/simplifications.",1 Introduction,[0],[0]
"Wubben et al. (2012) propose a two-stage model: initially, a standard phrase-based machine translation (PBMT) model is trained on complex-simple sentence pairs.",1 Introduction,[0],[0]
"During inference, the K-best outputs of the PBMT model are reranked according
584
to their dis-similarity to the (complex) input sentence.",1 Introduction,[0],[0]
The hybrid model developed in Narayan and Gardent (2014) also operates in two phases.,1 Introduction,[0],[0]
"Initially, a probabilistic model performs sentence splitting and deletion operations over discourse representation structures assigned by Boxer (Curran et al., 2007).",1 Introduction,[0],[0]
The resulting sentences are further simplified by a model similar to Wubben et al. (2012).,1 Introduction,[0],[0]
"Xu et al. (2016) train a syntax-based machine translation model on a large scale paraphrase dataset (Ganitkevitch et al., 2013) using simplification-specific objective functions and features to encourage simpler output.
",1 Introduction,[0],[0]
"In this paper we propose a simplification model which draws on insights from neural machine translation (Bahdanau et al., 2015; Sutskever et al., 2014).",1 Introduction,[0],[0]
Central to this approach is an encoderdecoder architecture implemented by recurrent neural networks.,1 Introduction,[0],[0]
The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence.,1 Introduction,[0],[0]
"Although our model uses the encoder-decoder architecture as its backbone, it must also meet constraints imposed by the simplification task itself, i.e., the predicted output must be simpler, preserve the meaning of the input, and grammatical.",1 Introduction,[0],[0]
"To incorporate this knowledge, the model is trained in a reinforcement learning framework (Williams, 1992): it explores the space of possible simplifications while learning to maximize an expected reward function that encourages outputs which meet simplificationspecific constraints.",1 Introduction,[0],[0]
"Reinforcement learning has been previously applied to extractive summarization (Ryang and Abekawa, 2012), information extraction (Narasimhan et al., 2016), dialogue generation (Li et al., 2016), machine translation, and image caption generation (Ranzato et al., 2016).
",1 Introduction,[0],[0]
"We evaluate our system on three publicly available datasets collated automatically from Wikipedia (Zhu et al., 2010; Woodsend and Lapata, 2011) and human-authored news articles (Xu et al., 2015b).",1 Introduction,[0],[0]
We experimentally show that the reinforcement learning framework is the key to successful generation of simplified text bringing significant improvements over strong simplification models across datasets.,1 Introduction,[0],[0]
"We will first define a basic encoder-decoder model for sentence simplification and then explain how to embed it in a reinforcement learning
framework.",2 Neural Encoder-Decoder Model,[0],[0]
Given a (complex) source sentence X =,2 Neural Encoder-Decoder Model,[0],[0]
"(x1, x2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", x|X|), our model learns to predict its simplified target Y = (y1, y2, . . .",2 Neural Encoder-Decoder Model,[0],[0]
", y|Y |).",2 Neural Encoder-Decoder Model,[0],[0]
"Inferring the target Y given the sourceX is a typical sequence to sequence learning problem, which can be modeled with attention-based encoderdecoder models (Bahdanau et al., 2015; Luong et al., 2015).",2 Neural Encoder-Decoder Model,[0],[0]
"Sentence simplification is slightly different from related sequence transduction tasks (e.g., compression) in that it can involve splitting operations.",2 Neural Encoder-Decoder Model,[0],[0]
"For example, a long source sentence (In 1883, Faur married Marie Fremiet, with whom he had two sons.)",2 Neural Encoder-Decoder Model,[0],[0]
"can be simplified as two sentences (In 1883, Faur married Marie Fremiet.",2 Neural Encoder-Decoder Model,[0],[0]
They had two sons.).,2 Neural Encoder-Decoder Model,[0],[0]
"Nevertheless, we still view the target as a sequence, i.e., two or more sequences concatenated with full stops.
",2 Neural Encoder-Decoder Model,[0],[0]
The encoder-decoder model has two parts (see left hand side in Figure 1).,2 Neural Encoder-Decoder Model,[0],[0]
"The encoder transforms the source sentence X into a sequence of hidden states (hS1 ,h S 2 , . . .",2 Neural Encoder-Decoder Model,[0],[0]
",h S |X|)",2 Neural Encoder-Decoder Model,[0],[0]
"with a Long Short-Term Memory Network (LSTM; Hochreiter and Schmidhuber 1997), while the decoder uses another LSTM to generate one word yt+1 at a time in the simplified target Y .",2 Neural Encoder-Decoder Model,[0],[0]
"Generation is conditioned on all previously generated words y1:t and a dynamically created context vector ct, which encodes the source sentence:
P (Y |X) = |Y |∏ t=1 P (yt|y1:t−1, X) (1)
P (yt+1|y1:t, X) = softmax(g(hTt , ct))",2 Neural Encoder-Decoder Model,[0],[0]
"(2)
where g(·) is a one-hidden-layer neural network with the following parametrization:
g(hTt , ct) =",2 Neural Encoder-Decoder Model,[0],[0]
"Wo tanh(Uhh T t + Whct) (3)
where Wo ∈ R|V |×d, Uh ∈ Rd×d, and Wh ∈ Rd×d; |V",2 Neural Encoder-Decoder Model,[0],[0]
| is the output vocabulary size and d the hidden unit size.,2 Neural Encoder-Decoder Model,[0],[0]
"hTt is the hidden state of the decoder LSTM which summarizes y1:t, i.e., what has been generated so far:
hTt = LSTM(yt,h T t−1) (4)
",2 Neural Encoder-Decoder Model,[0],[0]
"The dynamic context vector ct is the weighted sum of the hidden states of the source sentence:
ct = |X|∑ i=1",2 Neural Encoder-Decoder Model,[0],[0]
"αtihSi (5)
whose weights αti are determined by an attention mechanism:
αti = exp(hTt · hSi )∑ i exp(h T t · hSi )
(6)
where · is the dot product between two vectors.",2 Neural Encoder-Decoder Model,[0],[0]
We use the dot product here mainly for efficiency reasons; alternative ways to compute attention scores have been proposed in the literature and we refer the interested reader to Luong et al. (2015).,2 Neural Encoder-Decoder Model,[0],[0]
The model sketched above is usually trained by minimizing the negative log-likelihood of the training source-target pairs.,2 Neural Encoder-Decoder Model,[0],[0]
"In this section we present DRESS, our Deep REinforcement Sentence Simplification model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Despite successful application in numerous sequence transduction tasks (Jean et al., 2015; Chopra et al., 2016; Xu et al., 2015a), a vanilla encoder-decoder model is not ideal for sentence simplification.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"Although a number of rewrite operations (e.g., copying, deletion, substitution, word reordering) can be used to simplify text, copying is by far the most common.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
We empirically found that 73% of the target words are copied from the source in the Newsela dataset.,3 Reinforcement Learning for Sentence Simplification,[0],[0]
This number further increases to 83% when considering Wikipedia-based datasets (we provide details on these datasets in Section 5).,3 Reinforcement Learning for Sentence Simplification,[0],[0]
"As a result, a generic encoder-decoder model learns to copy all too well at the expense of other rewrite operations, often parroting back the source or making only a few trivial changes.
",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"To encourage a wider variety of rewrite operations while remaining fluent and faithful to the meaning of the source, we employ a reinforcement learning framework (see Figure 1).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"We view the encoder-decoder model as an agent which first reads the source sentence X; then at each step, it takes an action ŷt ∈ V (where V is the output vocabulary) according to a policy PRL(ŷt|ŷ1:t−1, X) (see Equation (2)).",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The agent continues to take actions until it produces an End Of Sentence (EOS) token yielding the action sequence Ŷ = (ŷ1, ŷ2, . . .",3 Reinforcement Learning for Sentence Simplification,[0],[0]
", ŷ|Ŷ |), which is also the simplified output of our model.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"A reward r is then received and the REINFORCE algorithm (Williams, 1992) is used to update the agent.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"In the following, we first introduce our reward and then present the details of the REINFORCE algorithm.",3 Reinforcement Learning for Sentence Simplification,[0],[0]
"The reward r(Ŷ ) for system output Ŷ is the weighted sum of the three components aimed at capturing key aspects of the target output, namely simplicity, relevance, and fluency:
r(Ŷ ) = λS rS + λR rR + λF",3.1 Reward,[0],[0]
"rF (7)
where λS , λR, λF ∈",3.1 Reward,[0],[0]
"[0, 1]; r(Ŷ ) is a shorthand for r(X,Y, Ŷ )",3.1 Reward,[0],[0]
"whereX is the source, Y the reference (or target), and Ŷ the system output.",3.1 Reward,[0],[0]
"rS , rR, and rF are shorthands for simplicity rS(X,Y, Ŷ ), relevance rR(X, Ŷ ), and fluency rF (Ŷ ).",3.1 Reward,[0],[0]
"We provide details for each reward summand below.
",3.1 Reward,[0],[0]
"Simplicity To encourage the model to apply a wide range of simplification operations, we use SARI (Xu et al., 2016), a recently proposed metric which compares System output Against References and against the Input sentence.",3.1 Reward,[0],[0]
"SARI is the arithmetic average of n-gram precision and recall of three rewrite operations: addition, copying, and deletion.",3.1 Reward,[0],[0]
It rewards addition operations where system output was not in the input but occurred in the references.,3.1 Reward,[0],[0]
"Analogously, it rewards words retained/deleted in both the system output and the references.",3.1 Reward,[0],[0]
"In experimental evaluation Xu et al. (2016) demonstrate that SARI correlates well with human judgments of simplicity, whilst correctly rewarding systems that both make changes and simplify the input.
",3.1 Reward,[0],[0]
One caveat with using SARI as a reward is the fact that it relies on the availability of multiple references which are rare for sentence simplification.,3.1 Reward,[0],[0]
"Xu et al. (2016) provide eight references for 2,350 sentences, but these are primarily for system tuning and evaluation rather than training.",3.1 Reward,[0],[0]
The majority of existing simplification datasets (see Section 5 for details) have a single reference for each source sentence.,3.1 Reward,[0],[0]
"Moreover, they are unavoidably noisy as they are mostly constructed automatically, e.g., by aligning sentences from the ordinary and simple English Wikipedias.",3.1 Reward,[0],[0]
"When relying solely on a single reference, SARI will try to reward accidental",3.1 Reward,[0],[0]
n-grams that should never have occurred in it.,3.1 Reward,[0],[0]
"To countenance the effect of noise, we apply SARI(X, Ŷ , Y ) in the expected direction, with X as the source, Ŷ the system output, and Y the reference as well as in the reverse direction with Y as the system output and Ŷ as the reference.",3.1 Reward,[0],[0]
"Assuming our system can produce reasonably good simplifications, by swapping the output
and the reference, reverse SARI can be used to estimate how good a reference is with respect to the system output.",3.1 Reward,[0],[0]
"Our first reward is therefore the weighted sum of SARI and reverse SARI:
rS=β SARI(X, Ŷ , Y )+(1−β)",3.1 Reward,[0],[0]
"SARI(X,Y, Ŷ ) (8)
Relevance",3.1 Reward,[0],[0]
"While the simplicity-based reward rS tries to encourage the model to make changes, the relevance reward rR ensures that the generated sentences preserve the meaning of the source.",3.1 Reward,[0],[0]
We use an LSTM sentence encoder to convert the source X and the predicted target Ŷ into two vectors qX and qŶ .,3.1 Reward,[0],[0]
"The relevance reward rR is simply the cosine similarity between these two vectors:
rR = cos(qX ,qŶ )",3.1 Reward,[0],[0]
= qX · qŶ ||qX || ||qŶ,3.1 Reward,[0],[0]
"||
(9)
",3.1 Reward,[0],[0]
We use a sequence auto-encoder (SAE; Dai and Le 2015) to train the LSTM sentence encoder on both the complex and simple sentences.,3.1 Reward,[0],[0]
"Specifically, the SAE uses sentence X =",3.1 Reward,[0],[0]
"(x1, . . .",3.1 Reward,[0],[0]
", x|X|) to infer itself via an encoder-decoder model (without an attention mechanism).",3.1 Reward,[0],[0]
"Firstly, an encoder LSTM convertsX into a sequence of hidden states (h1, . . .",3.1 Reward,[0],[0]
",h|X|).",3.1 Reward,[0],[0]
"Then, we use h|X| to initialize the hidden state of the decoder LSTM and recover/generate X one word at a time.
",3.1 Reward,[0],[0]
"Fluency Xu et al. (2016) observe that SARI correlates less with fluency compared to other metrics such as BLEU (Papineni et al., 2002).",3.1 Reward,[0],[0]
The fluency reward rF models the well-formedness of the generated sentences explicitly.,3.1 Reward,[0],[0]
"It is the normalized sentence probability assigned by an LSTM
language model trained on simple sentences:
rF = exp  1 |Ŷ | |Ŷ |∑ i=1",3.1 Reward,[0],[0]
"logPLM (ŷi|ŷ0:i−1)  (10)
",3.1 Reward,[0],[0]
We take the exponential of Ŷ ’s perplexity to ensure that rF ∈,3.1 Reward,[0],[0]
"[0, 1] as is the case with rS and rR.",3.1 Reward,[0],[0]
The goal of the REINFORCE algorithm is to find an agent that maximizes the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The training loss for one sequence is its negative expected reward:
L(θ) = −E(ŷ1,...,ŷ|Ŷ |)∼PRL(·|X)[r(ŷ1, . .",3.2 The REINFORCE Algorithm,[0],[0]
"., ŷ|Ŷ",3.2 The REINFORCE Algorithm,[0],[0]
"|)]
where PRL is our policy, i.e., the distribution produced by the encoder-decoder model (see Equation(2)) and r(·) is the reward function of an action sequence Ŷ = (ŷ1, . . .",3.2 The REINFORCE Algorithm,[0],[0]
", ŷ|Ŷ |), i.e., a generated simplification.",3.2 The REINFORCE Algorithm,[0],[0]
"Unfortunately, computing the expectation term is prohibitive, since there is an infinite number of possible action sequences.",3.2 The REINFORCE Algorithm,[0],[0]
"In practice, we approximate this expectation with a single sample from the distribution of PLR(·|X).",3.2 The REINFORCE Algorithm,[0],[0]
We refer to Williams (1992) for the full derivation of the gradients.,3.2 The REINFORCE Algorithm,[0],[0]
The gradient of L(θ) is: ∇L(θ) ≈∑|Ŷ,3.2 The REINFORCE Algorithm,[0],[0]
"|
t=1∇ logPRL(ŷt|ŷ1:t−1, X)[r(ŷ1:|Ŷ |)− bt]
To reduce the variance of gradients, we also introduce a baseline linear regression model bt to estimate the expected future reward at time t (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
bt takes the concatenation of hTt and ct as input and outputs a real value as the expected reward.,3.2 The REINFORCE Algorithm,[0],[0]
"The parameters of the regressor are
trained by minimizing mean squared error.",3.2 The REINFORCE Algorithm,[0],[0]
"We do not back-propagate this error to hTt or ct during training (Ranzato et al., 2016).",3.2 The REINFORCE Algorithm,[0],[0]
"Presented in its original form, the REINFORCE algorithm starts learning with a random policy.",3.3 Learning,[0],[0]
"This assumption can make model training challenging for generation tasks like ours with large vocabularies (i.e., action spaces).",3.3 Learning,[0],[0]
"We address this issue by pre-training our agent (i.e., the encoderdecoder model) with a negative log-likelihood objective (see Section 2), making sure it can produce reasonable simplifications, thereby starting off with a policy which is better than random.",3.3 Learning,[0],[0]
"We follow prior work (Ranzato et al., 2016) in adopting a curriculum learning strategy.",3.3 Learning,[0],[0]
"In the beginning of training, we give little freedom to our agent allowing it to predict the last few words for each target sentence.",3.3 Learning,[0],[0]
"For every target sequence, we use negative log-likelihood to train the first L (initially, L = 24) tokens and apply the reinforcement learning algorithm to the (L + 1)th tokens onwards.",3.3 Learning,[0],[0]
"Every two epochs, we set L = L− 3 and the training terminates when L is 0.",3.3 Learning,[0],[0]
"Lexical substitution, the replacement of complex words with simpler alternatives, is an integral part of sentence simplification (Specia et al., 2012).",4 Lexical Simplification,[0],[0]
The model presented so far learns lexical substitution and other rewrite operations jointly.,4 Lexical Simplification,[0],[0]
"In some cases, words are predicted because they seem natural in the their context, but are poor substitutes for the content of the complex sentence.",4 Lexical Simplification,[0],[0]
"To countenance this, we learn lexical simplifications explicitly and integrate them with our reinforcement learning-based model.
",4 Lexical Simplification,[0],[0]
"We use an pre-trained encoder-decoder model (which is trained on a parallel corpus of complex and simple sentences) to obtain probabilistic word alignments, aka attention scores (see αt in Equation (6)).",4 Lexical Simplification,[0],[0]
Let X =,4 Lexical Simplification,[0],[0]
"(x1, x2, . . .",4 Lexical Simplification,[0],[0]
", x|X|) denote a source sentence and Y = (y1, y2, . . .",4 Lexical Simplification,[0],[0]
", y|Y |) a target sentence.",4 Lexical Simplification,[0],[0]
"We convert X into |X| hidden states (v1,v2, . . .",4 Lexical Simplification,[0],[0]
",v|X|) with an LSTM.",4 Lexical Simplification,[0],[0]
Note that vt ∈ Rd×1 corresponds to the context dependent representation of xt.,4 Lexical Simplification,[0],[0]
"Let αt denote the alignment scores αt1, αt2, . . .",4 Lexical Simplification,[0],[0]
", αt|X|.",4 Lexical Simplification,[0],[0]
"The lexical simplification probability of yt given the source sentence
and the alignment scores is:
PLS(yt|X,αt) =",4 Lexical Simplification,[0],[0]
"softmax(Wl st) (11)
where Wl ∈ R|V |×d and st represents the source:
st = |X|∑ i=1",4 Lexical Simplification,[0],[0]
"αtivi (12)
",4 Lexical Simplification,[0],[0]
"The lexical simplification model on its own encourages lexical substitutions, without taking into account what has been generated so far (i.e., y1:t−1) and as a result fluency could be compromised.",4 Lexical Simplification,[0],[0]
"A straightforward solution is to integrate lexical simplification with our reinforcement learning trained model (Section 3) using linear interpolation, where η ∈",4 Lexical Simplification,[0],[0]
"[0, 1]:
P (yt|y1:t−1, X) = (1− η)PRL(yt|y1:t−1, X) +",4 Lexical Simplification,[0],[0]
"η PLS(yt|X,αt) (13)",4 Lexical Simplification,[0],[0]
In this section we present our experimental setup for assessing the performance of the simplification model described above.,5 Experimental Setup,[0],[0]
"We give details on our datasets, model training, evaluation protocol, and the systems used for comparison.
",5 Experimental Setup,[0],[0]
Datasets We conducted experiments on three simplification datasets.,5 Experimental Setup,[0],[0]
"WikiSmall (Zhu et al., 2010) is a parallel corpus which has been extensively used as a benchmark for evaluating text simplification systems (Wubben et al., 2012; Woodsend and Lapata, 2011; Narayan and Gardent, 2014; Zhu et al., 2010).",5 Experimental Setup,[0],[0]
It contains automatically aligned complex and simple sentences from the ordinary and simple English Wikipedias.,5 Experimental Setup,[0],[0]
The test set consists of 100 complex-simple sentence pairs.,5 Experimental Setup,[0],[0]
"The training set contains 89,042 sentence pairs (after removing duplicates and test sentences).",5 Experimental Setup,[0],[0]
"We randomly sampled 205 pairs for development and used the remaining sentences for training.
",5 Experimental Setup,[0],[0]
"We also constructed WikiLarge, a larger Wikipedia corpus by combining previously created simplification corpora.",5 Experimental Setup,[0],[0]
"Specifically, we aggregated the aligned sentence pairs in Kauchak (2013), the aligned and revision sentence pairs in Woodsend and Lapata (2011), and Zhu’s (2010) WikiSmall dataset described above.",5 Experimental Setup,[0],[0]
We used the development and test sets created in Xu et al. (2016).,5 Experimental Setup,[0],[0]
These are complex sentences taken from WikiSmall paired with simplifications provided by Amazon Mechanical Turk workers.,5 Experimental Setup,[0],[0]
"The dataset
contains 8 (reference) simplifications for 2,359 sentences partitioned into 2,000 for development and 359 for testing.",5 Experimental Setup,[0],[0]
"After removing duplicates and sentences in development and test sets, the resulting training set contains 296,402 sentence pairs.
",5 Experimental Setup,[0],[0]
"Our third dataset is Newsela, a corpus collated by Xu et al. (2015b) who argue that Wikipediabased resources are suboptimal due to the automatic sentence alignment which unavoidably introduces errors, and their uniform writing style which leads to systems that generalize poorly.",5 Experimental Setup,[0],[0]
"Newsela2 consists of 1,130 news articles, each rewritten four times by professional editors for children at different grade levels (0 is the most complex level and 4 is simplest).",5 Experimental Setup,[0],[0]
Xu et al. (2015b) provide multiple aligned complex-simple pairs within each article.,5 Experimental Setup,[0],[0]
"We removed sentence pairs corresponding to levels 0–1, 1–2, and 2–3, since they were too similar to each other.",5 Experimental Setup,[0],[0]
"The first 1,070 documents were used for training (94,208 sentence pairs), the next 30 documents for development (1,129 sentence pairs) and the last 30 documents for testing (",5 Experimental Setup,[0],[0]
"1,076 sentence pairs).3 We are not aware of any published results on this dataset.
",5 Experimental Setup,[0],[0]
Training Details,5 Experimental Setup,[0],[0]
We trained our models on an Nvidia GPU card.,5 Experimental Setup,[0],[0]
We used the same hyperparameters across datasets.,5 Experimental Setup,[0],[0]
"We first trained an encoder-decoder model, and then performed reinforcement learning training (Section 3), and trained the lexical simplification model (Section 4).",5 Experimental Setup,[0],[0]
"Encoder-decoder parameters were uniformly initialized to [−0.1, 0.1].",5 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) to optimize the model with learning rate 0.001; the first momentum coefficient was set to 0.9 and the second momentum coefficient to 0.999.",5 Experimental Setup,[0],[0]
"The gradient was rescaled when the norm exceeded 5 (Pascanu et al., 2013).",5 Experimental Setup,[0],[0]
Both encoder and decoder LSTMs have two layers with 256 hidden neurons in each layer.,5 Experimental Setup,[0],[0]
"We regularized all LSTMs with a dropout rate of 0.2 (Zaremba et al., 2014).",5 Experimental Setup,[0],[0]
"We initialized the encoder and decoder word embedding matrices with 300 dimensional Glove vectors (Pennington et al., 2014).
",5 Experimental Setup,[0],[0]
"During reinforcement training, we used plain stochastic gradient descent with a learning rate of 0.01.",5 Experimental Setup,[0],[0]
"We set β = 0.1, λS = 1, λR = 0.25 and λF = 0.5.4 Training details for the lexical
2https://newsela.com 3If a sentence has multiple references in the development or test set, we use the reference with highest simplicity level.",5 Experimental Setup,[0],[0]
"4Weights were tuned on the development set of the Newsela dataset and kept fixed for the other two datasets.
simplification model are identical to the encoderdecoder model except that word embedding matrices were randomly initialized.",5 Experimental Setup,[0],[0]
"The weight of the lexical simplification model was set to η = 0.1.
",5 Experimental Setup,[0],[0]
"To reduce vocabulary size, named entities were tagged with the Stanford CoreNLP (Manning et al., 2014) and anonymized with a NE@N token, where NE ∈ {PER,LOC,ORG,MISC} and N indicates NE@N is the N -th distinct NE typed entity.",5 Experimental Setup,[0],[0]
"For example, “John and Bob are . . .",5 Experimental Setup,[0],[0]
” becomes “PER@1 and PER@2 are . . .,5 Experimental Setup,[0],[0]
”.,5 Experimental Setup,[0],[0]
"At test time, we de-anonymize NE@N tokens in the output by looking them up in their source sentences.",5 Experimental Setup,[0],[0]
"Note that the de-anonymization may fail, but the chance is small (around 2% of the time on the Newsela development set).",5 Experimental Setup,[0],[0]
We replaced words occurring three times or less in the training set with UNK.,5 Experimental Setup,[0],[0]
"At test time, when our models predict UNK, we adopt the UNK replacement method proposed in Jean et al. (2015).
",5 Experimental Setup,[0],[0]
"Evaluation Following previous work (Woodsend and Lapata, 2011; Xu et al., 2016)",5 Experimental Setup,[0],[0]
we evaluated system output automatically adopting metrics widely used in the simplification literature.,5 Experimental Setup,[0],[0]
"Specifically, we used BLEU5 (Papineni et al., 2002) to assess the degree to which generated simplifications differed from gold standard references and the Flesch-Kincaid Grade Level index (FKGL; Kincaid et al. 1975) to measure the readability of the output (lower FKGL6 implies simpler output).",5 Experimental Setup,[0],[0]
"In addition, we used SARI (Xu et al., 2016), which evaluates the quality of the output by comparing it against the source and reference simplifications.7 BLEU, FKGL, and SARI are all measured at corpus-level.",5 Experimental Setup,[0],[0]
We also evaluated system output by eliciting human judgments via Amazon’s Mechanical Turk.,5 Experimental Setup,[0],[0]
"Specifically (selfreported) native English speakers were asked to rate simplifications on three dimensions: Fluency (is the output grammatical and well formed?),",5 Experimental Setup,[0],[0]
Adequacy (to what extent is the meaning expressed in the original sentence preserved in the output?) and Simplicity (is the output simpler than the original sentence?).,5 Experimental Setup,[0],[0]
"All ratings were obtained using a five point Likert scale.
",5 Experimental Setup,[0],[0]
Comparison Systems We compared our model against several systems previously proposed in the literature.,5 Experimental Setup,[0],[0]
"These include PBMT-R, a mono-
5With the default mtevalv13a.pl settings.",5 Experimental Setup,[0],[0]
6FKGL implementation at http://goo.gl/OHP7k3.,5 Experimental Setup,[0],[0]
"7We used he implementation of SARI in Xu et al. (2016).
",5 Experimental Setup,[0],[0]
"lingual phrase-based machine translation system with a reranking post-processing step8 (Wubben et al., 2012) and Hybrid, a model which first performs sentence splitting and deletion operations over discourse representation structures and then further simplifies sentences with PBMT-R (Narayan and Gardent, 2014).",5 Experimental Setup,[0],[0]
Hybrid9 is state of the art on the WikiSmall dataset.,5 Experimental Setup,[0],[0]
"Comparisons with SBMT-SARI, a syntax-based translation model trained on PPDB (Ganitkevitch et al., 2013) and tuned with SARI (Xu et al., 2016), are problematic due to the size of PPDB which is considerably larger than any of the datasets used in this work (it contains 106 million sentence pairs with 2 billion words).",5 Experimental Setup,[0],[0]
"Nevertheless, we compare10 against SBMT-SARI, but only models trained on Wikilarge, our largest dataset.",5 Experimental Setup,[0],[0]
"Since Newsela contains high quality simplifications created by professional editors, we performed the bulk of our experiments on this dataset.",6 Results,[0],[0]
"Specifically, we set out to answer two questions: (a) which neural model performs best and (b) how do neural models which are resource lean and do not have access to linguistic annotations fare against more traditional systems.",6 Results,[0],[0]
"We therefore compared the basic attention-based encoder-
8We made a good-faith effort to re-implement their system following closely the details in Wubben et al. (2012).
",6 Results,[0],[0]
"9We are grateful to Shashi Narayan for running his system on our three datasets.
",6 Results,[0],[0]
"10The output of SBMT-SARI is publicly available.
",6 Results,[0],[0]
"decoder model (EncDecA), with the deep reinforcement learning model (DRESS; Section 3), and a linear combination of DRESS and the lexical simplification model (DRESS-LS; Section 4).",6 Results,[0],[0]
"Neural models were further compared against two strong baselines, PBMT-R and Hybrid.",6 Results,[0],[0]
"Table 3 shows example output of all models on the Newsela dataset.
",6 Results,[0],[0]
The top block in Table 1 summarizes the results of our automatic evaluation.,6 Results,[0],[0]
"As can be seen, all neural models obtain higher BLEU, lower FKGL and higher SARI compared to PBMT-R. Hybrid has the lowest FKGL and highest SARI.",6 Results,[0],[0]
"Compared to EncDecA, DRESS scores lower on FKGL and higher on SARI, which indicates that the model has indeed learned to optimize the reward function which includes SARI.",6 Results,[0],[0]
"Integrating lexical simplification (DRESS-LS) yields better BLEU, but slightly worse FKGL and SARI.
",6 Results,[0],[0]
The results of our human evaluation are presented in the top block of Table 2.,6 Results,[0],[0]
We elicited judgments for 100 randomly sampled test sentences.,6 Results,[0],[0]
"Aside from comparing system output (PBMT-R, Hybrid, EncDecA, DRESS, and DRESS-LS), we also elicited ratings for the gold standard Reference as an upper bound.",6 Results,[0],[0]
"We report results for Fluency, Adequacy, and Simplicity individually and in combination (All is the average rating of the three dimensions).",6 Results,[0],[0]
"As can be seen, DRESS and DRESS-LS outperform PBMT-R and
Hybrid on Fluency, Simplicity, and overall.",6 Results,[0],[0]
"The fact that neural models (EncDecA, DRESS and DRESS-LS) fare well on Fluency, is perhaps not surprising given the recent success of LSTMs in language modeling and neural machine translation (Zaremba et al., 2014; Jean et al., 2015).
",6 Results,[0],[0]
Neural models obtain worse ratings on Adequacy but are closest to the human references on this dimension.,6 Results,[0],[0]
"DRESS-LS (and DRESS) are significantly better (p < 0.01) on Simplicity than EncDecA, PBMT-R, and Hybrid which indicates that our reinforcement learning based model is effective at creating simpler output.",6 Results,[0],[0]
Combined ratings (All) for DRESS-LS are significantly different compared to the other models but not to DRESS and the Reference.,6 Results,[0],[0]
"Nevertheless, integration of the lexical simplification model boosts performance as ratings increase almost across the board (Simplicity is slightly worse).",6 Results,[0],[0]
"Returning to our original questions, we find that neural models are more fluent than comparison systems, while performing non-trivial rewrite operations (see the SARI
scores in Table 1) which yield simpler output (see the Simplicity column in Table 2).",6 Results,[0],[0]
"Based on our judgment elicitation study, neural models trained with reinforcement learning perform best, with DRESS-LS having a slight advantage.
",6 Results,[0],[0]
We further analyzed model performance by computing various statistics on the simplified output.,6 Results,[0],[0]
We measured average sentence length and the degree to which DRESS and comparison systems perform rewriting operations.,6 Results,[0],[0]
"We approximated the latter with Translation Error Rate (TER; Snover et al. 2006), a measure commonly used to automatically evaluate the quality of machine translation output.",6 Results,[0],[0]
We used TER to compute the (average) number of edits required to change an original complex sentence to simpler output.,6 Results,[0],[0]
"We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert complex to simple sentences.
",6 Results,[0],[0]
"As shown in Table 4, Hybrid obtains the highest TER, followed by our models (DRESS and
DRESS-LS), which indicates that they actively perform rewriting.",6 Results,[0],[0]
"Perhaps Hybrid is too aggressive when simplifying a sentence, it obtains low Fluency and Adequacy scores in human evaluation (Table 2).",6 Results,[0],[0]
"There is a strong correlation between sentence length and number of deletion operations (i.e., more deleteions lead to shorter sentences) and PBMT-R performs very few deletions.",6 Results,[0],[0]
"Overall, reinforcement learning encourages deletion (see DRESS and DRESS-LS), while performing a reasonable amount of additional operations (e.g., substitutions and shifts) compared to EncDecA and PBMT-R.
The middle blocks in Tables 1 and 2 report results on the WikiSmall dataset.",6 Results,[0],[0]
FKGL and SARI follow a similar pattern as on Newsela.,6 Results,[0],[0]
"BLEU scores for PBMT-R, Hybrid, and EncDecA are much higher compared to DRESS and DRESS-LS.",6 Results,[0],[0]
"Hybrid obtains best BLEU and SARI scores, while DRESS and DRESS-LS do very well on FKGL.",6 Results,[0],[0]
"In human evaluation, we elicited judgments on the entire WikiSmall test set (100 sentences).",6 Results,[0],[0]
"We compared DRESS-LS, with PBMT-R, Hybrid, and gold standard Reference simplifications.",6 Results,[0],[0]
"As human experiments are time consuming and expensive, we did not include other neural models besides DRESS-LS based on our Newsela study which showed that EncDecA is inferior to variants trained with reinforcement learning and that DRESS-LS is the better performing model (however, we do compare all models in Table 1).",6 Results,[0],[0]
"DRESS-LS is significantly better on Simplicity than PBMT-R, Hybrid, and the Reference.",6 Results,[0],[0]
It performs on par with PBMT-R on Fluency and worse on Adequacy (but still closer to the human Reference than PBMT-R or Hybrid).,6 Results,[0],[0]
"When combining all ratings (All in Table 2), DRESS-LS is significantly better than PBMT-R, Hybrid, and the Reference.
",6 Results,[0],[0]
The bottom blocks in Tables 1 and 2 report results on Wikilarge.,6 Results,[0],[0]
"We compared our models with PBMT-R, Hybrid, and SBMT-SARI (Xu et al., 2016).",6 Results,[0],[0]
The FKGL follows a similar pattern as in the previous datasets.,6 Results,[0],[0]
"PBMT-R and our models are best in terms of BLEU while SBMT-SARI outperforms all other systems on SARI.11 Because there are 8 references for each complex sentence in the test set, BLEU scores are much higher compared to Newsela and WikiSmall.",6 Results,[0],[0]
"In human evaluation, we again elicited judgments for 100 randomly sampled test sentences.",6 Results,[0],[0]
We randomly selected one of the 8 references as the Reference upper bound.,6 Results,[0],[0]
"On Simplicity, DRESS-LS is significantly better than all comparison systems, except Hybrid.",6 Results,[0],[0]
"On Adequacy, it is better than Hybrid but significantly worse than other comparison systems.",6 Results,[0],[0]
"On Fluency, it is on par with PBMT-R12 but better than Hybrid and SBMT-SARI.",6 Results,[0],[0]
On All dimension DRESS-LS significantly outperforms all comparison systems.,6 Results,[0],[0]
"We developed a reinforcement learning-based text simplification model, which can jointly model simplicity, grammaticality, and semantic fidelity to the input.",7 Conclusions,[0],[0]
We also proposed a lexical simplification component that further boosts performance.,7 Conclusions,[0],[0]
"Overall, we find that reinforcement learning offers a great means to inject prior knowledge to the simplification task achieving good results across three datasets.",7 Conclusions,[0],[0]
"In the future, we would like to explicitly model sentence splitting and simplify entire documents (rather than individual sentences).",7 Conclusions,[0],[0]
"Beyond sentence simplification, the reinforcement learning framework presented here is potentially applicable to generation tasks such as sentence compression (Chopra et al., 2016), generation of programming code (Ling et al., 2016), or poems (Zhang and Lapata, 2014).
",7 Conclusions,[0],[0]
"Acknowledgments We would like to thank Li Dong, Jianpeng Cheng, Shashi Narayan and the EMNLP reviewers for their valuable feedback.",7 Conclusions,[0],[0]
We are also grateful to Shashi Narayan for supplying us with the output of his system and Wei Xu for her help with this work.,7 Conclusions,[0],[0]
"The authors acknowledge the support of the European Research Council (award number 681760).
",7 Conclusions,[0],[0]
"11BLEU and SARI scores reported in Xu et al. (2016) are 72.36 and 37.91, and measured at sentence-level.
",7 Conclusions,[0],[0]
12We used more data to train PBMT-R and maybe that is why PBMT-R performs better than Xu et al. (2016) reported.,7 Conclusions,[0],[0]
Sentence simplification aims to make sentences easier to read and understand.,abstractText,[0],[0]
Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences.,abstractText,[0],[0]
We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework.,abstractText,[0],[0]
"Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input.",abstractText,[0],[0]
Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.1,abstractText,[0],[0]
Sentence Simplification with Deep Reinforcement Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1156–1168 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Sentences with gapping (Ross, 1970) such as Paul likes coffee and Mary tea are characterized by having one or more conjuncts that contain multiple arguments or modifiers of an elided predicate.",1 Introduction,[0],[0]
"In this example, the predicate likes is elided for the relation Mary likes tea.",1 Introduction,[0],[0]
"While these sentences appear relatively infrequently in most written texts, they are often used to convey a lot of factual information that is highly relevant for language understanding (NLU) tasks such as open information extraction and semantic parsing.",1 Introduction,[0],[0]
"For example, consider the following sentence from the WSJ portion of the Penn Treebank (Marcus et al., 1993).
",1 Introduction,[0],[0]
"(1) Unemployment has reached 27.6% in Azerbaijan, 25.7% in Tadzhikistan, 22.8% in Uzbekistan, 18.8% in Turkmenia, 18% in Armenia and 16.3% in Kirgizia, [...]
To extract the information about unemployment rates in the various countries, an NLU system has to identify that the percentages indicate unemployment rates and the locational modifiers indicate the corresponding country.",1 Introduction,[0],[0]
"Given only this sentence, or this sentence and a strict surface syntax representation that does not indicate elided predicates, this is a challenging task.",1 Introduction,[0],[0]
"However, given a dependency graph that reconstructs the elided predicate for each conjunct, the problem becomes much easier and methods developed to extract information from dependency trees of clauses with canonical structures are much more likely to extract the correct information from a gapped clause.
",1 Introduction,[0],[0]
"While gapping constructions receive a lot of attention in the theoretical syntax literature (e.g., Ross 1970; Jackendoff 1971; Steedman 1990; Coppock 2001; Osborne 2006; Johnson 2014; Toosarvandani 2016; Kubota and Levine 2016), they have been almost entirely neglected by the NLP community so far.",1 Introduction,[0],[0]
"The Penn Treebank explicitly annotates gapping constructions, by coindexing arguments in the clause with a predicate and the clause with the gap, but these co-indices are not included in the standard parsing metrics
1156
and almost all parsers ignore them.1",1 Introduction,[0],[0]
"Despite the sophisticated analysis of gapping within CCG (Steedman, 1990), sentences with gapping were deemed too difficult to represent within the CCGBank (Hockenmaier and Steedman, 2007).",1 Introduction,[0],[0]
"Similarly the treebanks for the Semantic Dependencies Shared Task (Oepen et al., 2015) exclude all sentences from the Wall Street Journal that contain gapping.",1 Introduction,[0],[0]
"Finally, while the tectogrammatical layer of the Prague Dependency Treebank (Bejček et al., 2013) as well as the enhanced Universal Dependencies (UD) representation (Nivre et al., 2016) provide an analysis with reconstructed nodes for gapping constructions, there exist no methods to automatically parse to these representations.
",1 Introduction,[0],[0]
"Here, we provide the first careful analysis of parsing of gapping constructions, and we present two methods for reconstructing elided predicates in sentences with gapping within the UD framework.",1 Introduction,[0],[0]
"As illustrated in Figure 1, we first parse to a dependency tree and then reconstruct the elided material.",1 Introduction,[0],[0]
The methods differ in how much information is encoded in the dependency tree.,1 Introduction,[0],[0]
"The first method adapts an existing procedure for parsing sentences with elided function words (Seeker et al., 2012), which uses composite labels that can be deterministically turned into dependency graphs in most cases.",1 Introduction,[0],[0]
"The second method is a novel procedure that relies on the parser only to identify a gap, and then employs an unsupervised method to reconstruct the elided predicates and reattach the arguments to the reconstructed predicate.",1 Introduction,[0],[0]
We find that both methods can reconstruct elided predicates with very high accuracy from gold standard dependency trees.,1 Introduction,[0],[0]
"When applied to the output of a parser, which often fails to identify gapping, our methods achieve a sentence-level accuracy of 32% and 34%, significantly outperforming the recently proposed constituent parser by Kummerfeld and Klein (2017).",1 Introduction,[0],[0]
"Gapping constructions in English come in many forms that can be broadly classified as follows.
",2.1 Gapping constructions,[0],[0]
"1 To the best of our knowledge, the parser by Kummerfeld and Klein (2017) is the only parser that tries to output the co-indexing of constituents in clauses with gapping but they lack an explicit evaluation of their co-indexing prediction accuracy.
(2) Single predicate gaps: John bought books, and Mary flowers.
(3) Contiguous predicate-argument gap (including ACCs): Eve gave flowers to Al and Sue to Paul.",2.1 Gapping constructions,[0],[0]
"Eve gave a CD to Al and roses to Sue.
(4) Non-contiguous predicate-argument gap: Arizona elected Goldwater Senator, and Pennsylvania Schwelker .
",2.1 Gapping constructions,[0],[0]
"(Jackendoff, 1971)
(5) Verb cluster gap: I want to try to begin to write a novel and
...",2.1 Gapping constructions,[0],[0]
Mary a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to write a play.
...",2.1 Gapping constructions,[0],[0]
Mary to begin to write a play. ...,2.1 Gapping constructions,[0],[0]
"Mary to try to begin to write a play.
",2.1 Gapping constructions,[0],[0]
"(Ross, 1970)
",2.1 Gapping constructions,[0],[0]
The defining characteristic of gapping constructions is that there is a clause that lacks a predicate (the gap) but still contains two or more arguments or modifiers of the elided predicate (the remnants or orphans).,2.1 Gapping constructions,[0],[0]
"In most cases, the remnants have a corresponding argument or modifier (the correspondent) in the clause with the overt predicate.
",2.1 Gapping constructions,[0],[0]
These types of gapping also make up the majority of attested constructions in other languages.,2.1 Gapping constructions,[0],[0]
"However, Wyngaerd (2007) notes that Dutch permits gaps in relative clauses, and Farudi (2013) notes that Farsi permits gaps in finite embedded clauses even if the overt predicate is not embedded.2",2.1 Gapping constructions,[0],[0]
"We work within the UD framework, which aims to provide cross-linguistically consistent dependency annotations that are useful for NLP tasks.",2.2 Target representation,[0],[0]
"UD defines two types of representation: the basic UD representation which is a strict surface syntax dependency tree and the enhanced UD representation (Schuster and Manning, 2016) which may be a graph instead of a tree and may contain additional nodes.",2.2 Target representation,[0],[0]
"The analysis of gapping in the enhanced representation makes use of copy nodes for elided predicates and additional edges for elided arguments, which we both try to automatically reconstruct in this paper.",2.2 Target representation,[0],[0]
"In the simple case in which only one predicate was elided, there is exactly one
2See Johnson (2014) or Schuster et al. (2017) for a more comprehensive overview of cross-linguistically attested gapping constructions.
copy node for the elided predicate, which leads to a structure that is identical to the structure of the same sentence without a gap.3
John bought books and Mary bought′ flowers
nsubj",2.2 Target representation,[0],[0]
"obj cc nsubj
conj
obj
If a clause contains a more complex gap, the enhanced representation contains copies for all content words that are required to attach the remnants.
... and Mary wanted′ try′ begin′ write′",2.2 Target representation,[0],[0]
"a play
cc conj
xcomp xcomp xcomp det
obj
The motivation behind this analysis is that the semantically empty markers to are not needed for interpreting the sentence and minimizing the number of copy nodes leads to less complex graphs.
",2.2 Target representation,[0],[0]
"Finally, if a core argument was elided along with the predicate, we introduce additional dependencies between the copy nodes and the shared arguments, as for example, the open clausal complement (xcomp) dependency between the copy node and Senator in the following example.
",2.2 Target representation,[0],[0]
"AZ elected G. Senator and PA elected′ S.
nsubj",2.2 Target representation,[0],[0]
"obj
xcomp cc
nsubj
conj
obj
xcomp
The rationale for not copying all arguments is again to keep the graph simple, while still encoding all relations between content words.",2.2 Target representation,[0],[0]
"Arguments can be arbitrarily complex and it seems misguided to copy entire subtrees of arguments which, e.g., could contain multiple adverbial clauses.",2.2 Target representation,[0],[0]
Note that linking to existing nodes would not work in the case of verb clusters because they do not satisfy the subtree constraint.,2.2 Target representation,[0],[0]
"Our first method adapts one of the procedures by Seeker et al. (2012), which represents gaps in dependency trees by attaching dependents of an elided predicate with composite relations.",3.1 Composite relations,[0],[0]
"These relations represent the dependency path that would
3To enhance the readability of our examples, we place the copy node in the sentence where the elided predicate would have been pronounced.",3.1 Composite relations,[0],[0]
"However, as linear order typically does not matter for extracting information with dependency patterns, our procedures only try to recover the structure of canonical sentences but not their linear order.
have existed if nothing had been elided.",3.1 Composite relations,[0],[0]
"For example, in the following sentence, the verb bought, which would have been attached to the head of the first conjunct with a conj relation, was elided from the second conjunct and hence all nodes that would have depended on the elided verb, are attached to the first conjunct using a composite relation consisting of conj and the type of argument.
",3.1 Composite relations,[0],[0]
"John bought books and Mary flowers
nsubj",3.1 Composite relations,[0],[0]
"obj
conj>cc
conj>nsubj
conj>obj
The major advantage of this approach is that the dependency tree contains information about the types of arguments and so it should be straightforward to turn dependency trees of this form into enhanced UD graphs.",3.1 Composite relations,[0],[0]
"For most dependency trees, one can obtain the enhanced UD graph by splitting the composite relations into its atomic parts and inserting copy nodes at the splitting points.4
At the same time, this approach comes with the drawback of drastically increasing the label space.",3.1 Composite relations,[0],[0]
"For sentences with more complex gaps as in (5), one has to use composite relations that consist of more than two atomic relations and theoretically, the number of composite relations is unbounded:
... and Mary a play
det
conj>xcomp>xcomp>xcomp>obj
conj>nsubj
conj>cc",3.1 Composite relations,[0],[0]
"Our second method also uses a two-step approach to resolve gaps, but compared to the previous method, it puts less work on the parser.",3.2 Orphan procedure,[0],[0]
"We first parse sentences to the basic UD v2 representation, which analyzes gapping constructions as follows.",3.2 Orphan procedure,[0],[0]
One remnant is promoted to be the head of the clause and all other remnants are attached to the promoted phrase.,3.2 Orphan procedure,[0],[0]
"For example, in this sentence, the subject of the second clause, Mary, is the head of the clause and the other remnant, flowers, is attached to Mary with the special orphan relation:
John bought books and Mary flowers
nsubj",3.2 Orphan procedure,[0],[0]
"obj cc conj orphan
4Note that this representation does not indicate conjunct boundaries, and for sentences with multiple gapped conjuncts, it is thus unclear how many copy nodes are required.
",3.2 Orphan procedure,[0],[0]
"This analysis can also be used for more complex gaps, as in the example with a gap that consists of a chain of non-finite embedded verbs in (5).
... and Mary a play
cc
conj
det
orphan
When parsing to this representation, the parser only has to identify that there is a gap but does not have to recover the elided material or determine the type of remnants.",3.2 Orphan procedure,[0],[0]
"As a second step, we use an unsupervised procedure to determine which nodes to copy and how and where to attach the remnants.",3.2 Orphan procedure,[0],[0]
"In developing this procedure, we made use of the fact that in the vast majority of cases, all arguments and modifiers that are expressed in gapped conjunct are also expressed in the full conjunct.",3.2 Orphan procedure,[0],[0]
The problem of determining which nodes to copy and which relations to use can thus be reduced to the problem of aligning arguments in the gapped conjunct to arguments in the full conjunct.,3.2 Orphan procedure,[0],[0]
"We apply the following procedure to all sentences that contain at least one orphan relation.
1.",3.2 Orphan procedure,[0],[0]
"Create a list F of arguments of the head of the full conjunct by considering all core argument dependents of the conjunct’s head as well as clausal and nominal non-core dependents, and adverbial modifiers.
2.",3.2 Orphan procedure,[0],[0]
"Create a list G of arguments in the gapped conjunct that contains the head of the gapped conjunct and all its orphan dependents.
3.",3.2 Orphan procedure,[0],[0]
"Find the highest-scoring monotonic alignment of arguments in G to arguments in F .
4.",3.2 Orphan procedure,[0],[0]
"Copy the head of the full conjunct and attach the copy node c to the head of the full conjunct with the original relation of the head of the gapped conjunct (usually conj).
5.",3.2 Orphan procedure,[0],[0]
"For each argument g ∈ G that has been aligned to f ∈ F , attach g to c with the same relation as the parent relation of f , e.g., if f is attached to the head of the full conjunct with an nsubj relation, also attach g to c with an nsubj relation.",3.2 Orphan procedure,[0],[0]
"Attach arguments g′ ∈ G that were not aligned to any token in F to c using the general dep relation.
6.",3.2 Orphan procedure,[0],[0]
"For each copy node c, add dependencies to all core arguments of the original node which do not have a corresponding remnant in the gapped clause.",3.2 Orphan procedure,[0],[0]
"For example, if the full conjunct contains a subject, an object, and an
oblique modifier but the clause with the gap, only a subject and an oblique modifier, add an object dependency between the copy node and the object in the full conjunct.
",3.2 Orphan procedure,[0],[0]
"A crucial step is the third step, determining the highest-scoring alignment.",3.2 Orphan procedure,[0],[0]
"This can be done straightforwardly with the sequence alignment algorithm by Needleman and Wunsch (1970) if one defines a similarity function sim(g, f) that returns a similarity score between the arguments g and f .",3.2 Orphan procedure,[0],[0]
"We defined sim based on the intuitions that often, parallel arguments are of the same syntactic category, that they are introduced by the same function words (e.g., the same preposition), and that they are closely related in meaning.",3.2 Orphan procedure,[0],[0]
"The first intuition can be captured by penalizing mismatching POS tags, and the other two by computing the distance between argument embeddings.",3.2 Orphan procedure,[0],[0]
We compute these embeddings by averaging over the 100- dim.,3.2 Orphan procedure,[0],[0]
"pretrained GloVe (Pennington et al., 2014) embeddings for each token in the argument.",3.2 Orphan procedure,[0],[0]
"Given the POS tags tg and tf and the argument embeddings vg and vf , sim is defined as follows.5
sim(g, f) = −‖vg",3.2 Orphan procedure,[0],[0]
− vf‖2 + 1,3.2 Orphan procedure,[0],[0]
"[tg = tf ] × pos_mismatch_penalty
We set pos_mismatch_penalty, a parameter that penalizes mismatching POS tags, to −2.6
This procedure can be used for almost all sentences with gapping constructions.",3.2 Orphan procedure,[0],[0]
"However, if parts of an argument were elided along with the main predicate, it can become necessary to copy multiple nodes.",3.2 Orphan procedure,[0],[0]
We therefore consider the alignment not only between complete arguments in the full clause and the gapped clause but also between partial arguments in the full clause and the complete arguments in the gapped clause.,3.2 Orphan procedure,[0],[0]
"For example, for the sentence “Mary wants to write a play and Sue a book” the complete arguments of the full clause are {Mary, to write a play} and the arguments of the gapped clause are {Sue, a book}.",3.2 Orphan procedure,[0],[0]
"In this case, we also consider the partial arguments {Mary, a play} and if the arguments of the gapped
5As suggested by one of the reviewers, we also ran a posthoc experiment with a simpler similarity score function without the embedding distance term, which only takes into account whether the POS tags match.",3.2 Orphan procedure,[0],[0]
"We found that quantitatively, the embeddings do not lead to significant better scores on the test set according to our metrics but qualitatively, they lead to better results for the examples with verb cluster gaps.
6We optimized this parameter on the training set by trying integer values from −1 to −15.
conjunct align better to the partial arguments, we use this alignment.",3.2 Orphan procedure,[0],[0]
"However, now that the token write is part of the dependency path between want and play, we also have to make a copy of write to reconstruct the UD graph of the gapped clause.",3.2 Orphan procedure,[0],[0]
Both methods rely on a dependency parser followed by a post-processing step.,4 Experiments,[0],[0]
We evaluated the individual steps and the end-to-end performance.,4 Experiments,[0],[0]
"We used the UD English Web Treebank v2.1 (henceforth EWT; Silveira et al., 2014; Nivre et al., 2017) for training and evaluating parsers.",4.1 Data,[0],[0]
"As the treebank is relatively small and therefore only contains very few sentences with gapping, we also extracted gapping constructions from the WSJ and Brown portions of the PTB (Marcus et al., 1993) and the GENIA corpus (Ohta et al., 2002).",4.1 Data,[0],[0]
"Further, we copied sentences from the Wikipedia page on gapping7 and from published papers on gapping.",4.1 Data,[0],[0]
"The sentences in the EWT already contain annotations with the orphan relation and copy nodes for the enhanced representation, and we manually added both of these annotations for the remaining examples.",4.1 Data,[0],[0]
"The composite relations can
7https://en.wikipedia.org/wiki/Gapping, accessed on Aug 24, 2017.
be automatically obtained from the enhanced representation by removing the copy nodes and concatenating the dependency labels, which we did to build the training and test corpus for the composite relation procedure.",4.1 Data,[0],[0]
"Table 1 shows properties of the data splits of the original treebank, the additional sentences with gapping, and their combination; Table 2 shows the number of sentences in our corpus for each of the gap types.",4.1 Data,[0],[0]
Parser We used the parser by Dozat and Manning (2017) for parsing to the two different intermediate dependency representations.,4.2 Parsing experiments,[0],[0]
"This parser is a graph-based parser (McDonald et al., 2005) that uses a biLSTM to compute token representations and then uses a multi-layer perceptron with biaffine attention to compute arc and label scores.
",4.2 Parsing experiments,[0],[0]
"Setup We trained the parser on the COMBINED training corpus with gold tokenization, and predicted fine-grained and universal part-of-speech tags, for which we used the tagger by Dozat et al. (2017).",4.2 Parsing experiments,[0],[0]
We trained the tagger on the COMBINED training corpus.,4.2 Parsing experiments,[0],[0]
"As pre-trained embeddings, we used the word2vec (Mikolov et al., 2013) embeddings that were provided for the CoNLL 2017 Shared Task (Zeman et al., 2017), and we used the same hyperparameters as Dozat et al. (2017).
",4.2 Parsing experiments,[0],[0]
Evaluation We evaluated the parseability of the two dependency representations using labeled and unlabeled attachment scores (LAS and UAS).,4.2 Parsing experiments,[0],[0]
"Further, to specifically evaluate how well parsers are able to parse gapping constructions according to the two annotation schemes, we also computed the LAS and UAS just for the head tokens of remnants (LASg and UASg).",4.2 Parsing experiments,[0],[0]
"For all our metrics, we excluded punctuation tokens.",4.2 Parsing experiments,[0],[0]
"To determine sta-
tistical significance of pairwise comparisons, we performed two-tailed approximate randomization tests (Noreen, 1989; Yeh, 2000) with an adapted version of the sigf package (Padó, 2006).
",4.2 Parsing experiments,[0],[0]
Results Table 3 shows the overall parsing results on the development and test sets of the two treebanks.,4.2 Parsing experiments,[0],[0]
"There was no significant difference between the parser that was trained on the UD representation (ORPHAN) and the parser trained on the composite representation (COMPOSITE) when tested on the EWT data sets, which is not surprising considering that there is just one sentence with gapping each in the development and the test split.",4.2 Parsing experiments,[0],[0]
"When evaluated on the GAPPING datasets, the ORPHAN parser performs significantly better (p < 0.01) in terms of labeled attachment score, which suggests that the parser trained on the COMPOSITE representation is indeed struggling with the greatly increased label space.",4.2 Parsing experiments,[0],[0]
This is further confirmed by the attachment scores of the head tokens of remnants (Table 4).,4.2 Parsing experiments,[0],[0]
The labeled attachment score of remnants is significantly higher for the ORPHAN parser than for the COMPOSITE parser.,4.2 Parsing experiments,[0.956584007999656],['This could lower the computational cost for the case where |M| is significantly larger than the number of threads.']
"Further, the unlabeled attachment score on the test set is also higher for the ORPHAN parser, which suggests that the COMPOSITE parser is sometimes struggling with finding the right attachment for the
multiple long-distance composite dependencies.",4.2 Parsing experiments,[0],[0]
Our second set of experiments concerns the recovery of the elided material and the reattachment of the orphans.,4.3 Recovery experiments,[0],[0]
We conducted two experiments: an oracle experiment that used gold standard dependency trees and an end-to-end experiment that used the output of the parser as input.,4.3 Recovery experiments,[0],[0]
"For all experiments, we used the COMBINED treebank.
",4.3 Recovery experiments,[0],[0]
"Evaluation Here, we evaluated dependency graphs and therefore used the labeled and unlabeled precision and recall metrics.",4.3 Recovery experiments,[0],[0]
"However, as our two procedures are only changing the attachment of orphans, we only computed these metrics for copy nodes and their dependents.",4.3 Recovery experiments,[0],[0]
"Further, we excluded punctuation and coordinating conjunctions as their attachment is usually trivial and including them would inflate scores.",4.3 Recovery experiments,[0],[0]
"Lastly, we computed the sentence-level accuracy for all sentences with gapping.",4.3 Recovery experiments,[0],[0]
"For this metric, we considered a sentence to be correct if all copy nodes and their dependents of a sentence were attached to the correct head with the correct label.
",4.3 Recovery experiments,[0],[0]
Oracle results The top part of Table 5 shows the results for the oracle experiment.,4.3 Recovery experiments,[0],[0]
Both methods are able to reconstruct the elided material and the canonical clause structure from gold dependency trees with high accuracy.,4.3 Recovery experiments,[0],[0]
"This was expected for the COMPOSITE procedure, which can make use of the composite relations in the dependency trees, but less so for the ORPHAN procedure which has to recover the structure and the types of relations.",4.3 Recovery experiments,[0],[0]
"The two methods work equally well in terms of all metrics except for the sentence-level accuracy, which is significantly higher for the COMPOSITE procedure.",4.3 Recovery experiments,[0],[0]
This difference is caused by a difference in the types of mistakes.,4.3 Recovery experiments,[0],[0]
All errors of the COMPOSITE procedure are of a structural nature and stem from copying the wrong number of nodes while the dependency labels are always correct because they are part of the dependency tree.,4.3 Recovery experiments,[0],[0]
"The majority of errors of the ORPHAN procedure stem from incorrect dependency labels, and these mistakes are scattered across more examples, which leads to the lower sentence-level accuracy.
",4.3 Recovery experiments,[0],[0]
End-to-end results The middle part of Table 5 shows the results for the end-to-end experiment.,4.3 Recovery experiments,[0],[0]
"The performance of both methods is considerably lower than in the oracle experiment, which is pri-
marily driven by the much lower recall.",4.3 Recovery experiments,[0],[0]
"Both methods assume that the parser detects the existence of a gap and if the parser fails to do so, neither method attempts to reconstruct the elided material.",4.3 Recovery experiments,[0],[0]
"In general, precision tends to be a bit higher for the ORPHAN procedure whereas recall tends to be a bit higher for the COMPOSITE method but overall and in terms of sentence-level accuracy both methods seem to perform equally well.
",4.3 Recovery experiments,[0],[0]
"Error analysis For both methods, the primary issue is low recall, which is a result of parsing errors.",4.3 Recovery experiments,[0.9529982233893037],"['Thirdly, modelling errors as contemporaneous is attractive for low-frequency data where the resolution of temporal effects is coarse, but the situation reverses for high-frequency data.']"
"When the parser correctly predicts the orphan relation, the main sources of error for the ORPHAN procedure are missing correspondents for remnants (e.g., [for good] has no correspondent in They had left the company, many for good) or that the types of argument of the remnant and its correspondent differ (e.g., in She was convicted of selling unregistered securities in Florida and of unlawful phone calls in Ohio,",4.3 Recovery experiments,[0],[0]
"[of selling unregistered securities] is an adverbial clause whereas [of unlawful phone calls] is an oblique modifier).
",4.3 Recovery experiments,[0],[0]
"Apart from the cases where the COMPOSITE procedure leads to an incorrect structure, the remaining errors are all caused by the parser predicting the wrong composite relation.",4.3 Recovery experiments,[0],[0]
Kummerfeld and Klein (henceforth K&K; 2017) recently proposed a one-endpoint-crossing graph parser that is able to directly parse to PTB-style trees with traces.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
They also briefly discuss gapping constructions and their parser tries to output the co-indexing that is used for gapping constructions in the PTB.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The EWT and all the sentences that we took from the WSJ, Brown, and GENIA treebanks already come with constituency tree annotations, and we manually annotated the remaining sentences according to the PTB guide-
lines (Bies et al., 1995).",4.4 Comparison to Kummerfeld and Klein,[0],[0]
This allowed us to train the K&K parser with exactly the same set of sentences that we used in our previous experiments.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"As this parser outputs constituency trees, we could not compute dependency graph metrics for this method.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For the sentence-level accuracy, we considered an example to be correct if a) each argument in the gapped conjunct was the child of a single constituent node, which in return was the sibling of the full clause/verb phrase, and b) the coindexing of each argument in the gapped conjunct was correct.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"For example, the following bracketing would be considered correct despite the incorrect internal structure of the first conjunct: [S[S[NP-1 Al ] likes [NP-2 coffee ]] and [S[NP=1 Sue",4.4 Comparison to Kummerfeld and Klein,[0],[0]
],4.4 Comparison to Kummerfeld and Klein,[0],[0]
"[NP=2 tea ]]]
The last row of Table 5 shows the results of the K&K parser.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
The parser failed to output the correct constituency structure or co-indexing for every single example in the development and test sets.,4.4 Comparison to Kummerfeld and Klein,[0],[0]
"The parser struggled in particular with outputting the correct co-indices: For 32.5% of the test sentences with gapping, the bracketing of the gapped clause was correct but one or more of the co-indices were missing from the output.
",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Overall these results suggest that our dependency-based approach is much more reliable at identifying gapping constructions than the parser by K&K, which, in their defense, was optimized to output traces for other phenomena.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
"Our method is also faster and took only seconds to parse the test set, while the K&K parser took several hours.",4.4 Comparison to Kummerfeld and Klein,[0],[0]
One of the appeals of the ORPHAN procedure is that it can be easily applied to other languages even if there exist no annotated enhanced dependency graphs.8,5 Resolving gaps in other languages,[0],[0]
"On the one hand, this is because
8There is no theoretical reason that would prevent one from using the COMPOSITE procedure for other languages
our method does not make use of lexical information, and on the other hand, this is because we developed our method on top of the UD annotation scheme, which has already been applied to many languages and for which many treebanks exist.
",5 Resolving gaps in other languages,[0],[0]
"Currently, all treebanks but the English one lack copy nodes for gapping constructions and many of them incorrectly use the orphan relation (Droganova and Zeman, 2017) and therefore we could not evaluate our method on a large variety of languages.",5 Resolving gaps in other languages,[0],[0]
"In order to demonstrate that our method can be applied to other languages, we therefore did a case study on the Swedish UD treebank.",5 Resolving gaps in other languages,[0],[0]
"The Swedish UD treebank is an automatic conversion from a section of the Talbanken (Einarsson, 1976) with extensive manual corrections.",5 Resolving gaps in other languages,[0],[0]
"While the treebank is overall of high quality, we noticed conversion errors that led to incorrect uses of the orphan relation in 11 of the 29 sentences with orphan relations, which we excluded from our evaluation.",5 Resolving gaps in other languages,[0],[0]
We applied our gapping resolution procedure without any modifications to the remaining 18 sentences.,5 Resolving gaps in other languages,[0],[0]
We used the Swedish word2vec embeddings that were prepared for the CoNLL 2017 Shared Task.,5 Resolving gaps in other languages,[0],[0]
"Our method correctly predicts the insertion of 29 copy nodes and is able to predict the correct structure of the enhanced representation in all cases, including complex ones with elided verb clusters such as the example in Figure 2.",5 Resolving gaps in other languages,[0],[0]
"It also predicts the correct dependency label for 108/110 relations, leading to a labeled precision and labeled recall of 98.18%, which are both higher than the English numbers despite the fact that we optimized our procedure for English.",5 Resolving gaps in other languages,[0],[0]
"The main reason for the higher performance seems to be that many of the Swedish examples come from informational texts from public organizations, which are more likely to be written to be clear and unambiguous.",5 Resolving gaps in other languages,[0],[0]
"Further, the Swedish data does not contain challenging examples from the linguistic literature.
",5 Resolving gaps in other languages,[0],[0]
"As Swedish is a Germanic language like English and thus shares many structural properties, we cannot conclude that our method is applicable to any language based on just this experiment.",5 Resolving gaps in other languages,[0],[0]
"However, given that our method does not rely on language-specific structural patterns, we expect it to work well for a wide range of languages.
",5 Resolving gaps in other languages,[0],[0]
"but given that UD treebanks are annotated with orphan relations, using the the COMPOSITE procedure would require additional manual annotations in practice.",5 Resolving gaps in other languages,[0],[0]
"Gapping constructions have been little studied in NLP, but several approaches (e.g., Dukes and Habash 2011; Simkó and Vincze 2017) parse to dependency trees with empty nodes.",6 Related work,[0],[0]
"Seeker et al. (2012) compared three ways of parsing with empty heads: adding a transition that inserts empty nodes, using composite relation labels for nodes that depend on an elided node, and pre-inserting empties before parsing.",6 Related work,[0],[0]
These papers all focus on recovering nodes for elided function words such as auxiliaries; none of them attempt to recover and resolve the content word elisions of gapping.,6 Related work,[0],[0]
"Ficler and Goldberg (2016) modified PTB annotations of argument-cluster coordinations (ACCs), i.e., gapping constructions with two post-verbal orphan phrases, which make up a subset of the gapping constructions in the PTB.",6 Related work,[0],[0]
"While the modified annotation style leads to higher parsing accuracy of ACCs, it is specific to ACCs and does not generalize to other gapping constructions.",6 Related work,[0],[0]
"Moreover, they did not reconstruct gapped ACC clauses.",6 Related work,[0],[0]
"Traditional grammarbased chart parsers (Kay, 1980; Klein and Manning, 2001) did handle empty nodes and so could in principle provide a parse of gapping sentences though additional mechanisms would be needed for reconstruction.",6 Related work,[0],[0]
"In practice, though, dealing with gapping in a grammar-based framework is not straightforward and can lead to a combinatorial explosion that slows down parsing in general, as has been noted for the English Resource Grammar (Flickinger, 2017, p.c.) and for an HPSG implementation for Norwegian (Haugereid, 2017).",6 Related work,[0],[0]
"The grammar-based parser built with augmented transition networks (Woods, 1970) provided an extension in the form of the SYSCONJ operation (Woods, 1973) to parse some gapping constructions, but also this approach lacked explicit reconstruction mechanisms and provided only limited coverage.
",6 Related work,[0],[0]
"There also exists a long line of work on postprocessing surface-syntax constituency trees to recover traces in the PTB (Johnson, 2002; Levy and Manning, 2004; Campbell, 2004; Gabbard et al., 2006), pre-processing sentences such that they contain tokens for traces before parsing (Dienes and Dubey, 2003b), or directly parsing sentences to either PTB-style trees with empty elements or pre-processed trees that can be deterministically converted to PTB-style trees (Collins,
1997; Dienes and Dubey, 2003a; Schmid, 2006; Cai et al., 2011; Hayashi and Nagata, 2016; Kato and Matsubara, 2016; Kummerfeld and Klein, 2017).",6 Related work,[0],[0]
"However, all of these works are primarily concerned with recovering traces for phenomena such as Wh-movement or control and raising constructions and, with the exception of Kummerfeld and Klein (2017), none of these works attempt to output the co-indexing that is used for analyzing gapping constructions.",6 Related work,[0],[0]
"And again, none of these works try to reconstruct elided material.
",6 Related work,[0],[0]
"Lastly, several methods have been proposed for resolving other forms of ellipsis, including VP ellipsis (Hardt, 1997; Nielsen, 2004; Lappin, 2005; McShane and Babkin, 2016) and sluicing (Anand and Hardt, 2016) but none of these methods consider gapping constructions.",6 Related work,[0],[0]
We presented two methods to recover elided predicates in sentences with gapping.,7 Conclusion,[0],[0]
Our experiments suggest that both methods work equally well in a realistic end-to-end setting.,7 Conclusion,[0],[0]
"While in general, recall is still low, the oracle experiments suggest that both methods can recover elided predicates from correct dependency trees, which suggests that as parsers become more and more accurate, the gap recovery accuracy should also increase.
",7 Conclusion,[0],[0]
We also demonstrated that our method can be used to automatically add the enhanced UD representation to UD treebanks in other languages than English.,7 Conclusion,[0],[0]
"Apart from being useful in a parsing pipeline, we therefore also expect our method to be useful for building enhanced UD treebanks.
",7 Conclusion,[0],[0]
"Reproducibility
All data, pre-trained models, system outputs as well as a package for running the enhancement procedure are available from https:// github.com/sebschu/naacl-gapping.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their thoughtful feedback.,Acknowledgments,[0],[0]
"Also thanks to Vera Gribanova and Boris Harizanov for continuous feedback throughout this project, and to Matthew Lamm for help with annotating the data.",Acknowledgments,[0],[0]
"This work was supported in part by gifts from Google, Inc. and IPSoft, Inc.",Acknowledgments,[0],[0]
The first author is also supported by a Goodan Family Graduate Fellowship.,Acknowledgments,[0],[0]
"Sentences with gapping, such as Paul likes coffee and Mary tea, lack an overt predicate to indicate the relation between two or more arguments.",abstractText,[0],[0]
"Surface syntax representations of such sentences are often produced poorly by parsers, and even if correct, not well suited to downstream natural language understanding tasks such as relation extraction that are typically designed to extract information from sentences with canonical clause structure.",abstractText,[0],[0]
"In this paper, we present two methods for parsing to a Universal Dependencies graph representation that explicitly encodes the elided material with additional nodes and edges.",abstractText,[0],[0]
We find that both methods can reconstruct elided material from dependency trees with high accuracy when the parser correctly predicts the existence of a gap.,abstractText,[0],[0]
We further demonstrate that one of our methods can be applied to other languages based on a case study on Swedish.,abstractText,[0],[0]
Sentences with Gapping: Parsing and Reconstructing Elided Predicates,title,[0],[0]
"Sentiment-oriented relation extraction (Choi et al., 2006) is concerned with recognizing sentiment polarities and comparative relations between entities from natural language text.",1 Introduction,[0],[0]
Identifying such relations often requires syntactic and semantic analysis at both sentence and phrase level.,1 Introduction,[0],[0]
"Most prior work on sentiment analysis consider either i) subjective sentence detection (Yu and Kübler, 2011), ii) polarity classification (Johansson and Moschitti, 2011; Wilson et al., 2005), or iii) comparative relation identification (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",1 Introduction,[0],[0]
"In practice, however, differ-
ent types of sentiment-oriented relations frequently coexist in documents.",1 Introduction,[0],[0]
"In particular, we found that more than 38% of the sentences in our test corpus contain more than one type of relations.",1 Introduction,[0],[0]
The isolated analysis approach is inappropriate because i) it sacrifices acuracy by ignoring the intricate interplay among different types of relations; ii) it could lead to conflicting predictions such as estimating a relation candidate as both negative and comparative.,1 Introduction,[0],[0]
"Therefore, in this paper, we identify instances of both sentiment polarities and comparative relations for entities of interest simultaneously.",1 Introduction,[0],[0]
"We assume that all the mentions of entities and attributes are given, and entities are disambiguated.",1 Introduction,[0],[0]
"It is a widely used assumption when evaluating a module in a pipeline system that the outputs of preceding modules are error-free.
",1 Introduction,[0],[0]
"To the best of our knowledge, the only existing system capable of extracting both comparisons and sentiment polarities is a rule-based system proposed by Ding et al. (2009).",1 Introduction,[0],[0]
We argue that it is better to tackle the task by using a unified model with structured outputs.,1 Introduction,[0],[0]
It allows us to consider a set of correlated relation instances jointly and characterize their interaction through a set of soft and hard constraints.,1 Introduction,[0],[0]
"For example, we can encode constraints to discourage an attribute to participate in a polarity relation and a comparative relation at the same time.",1 Introduction,[0],[0]
"As a result, the system extracts a set of correlated instances of sentiment-oriented relations from a given sentence.",1 Introduction,[0],[0]
"For example, with the sentence about the camera Canon 7D, “The sensor is great, but the price is higher than Nikon D7000.”",1 Introduction,[0],[0]
"the expected output is positive(Canon 7D, sensor)
155
Transactions of the Association for Computational Linguistics, 2 (2014) 155–168.",1 Introduction,[0],[0]
Action Editor: Janyce Wiebe.,1 Introduction,[0],[0]
Submitted 6/2013; Revised 11/2013; Published 4/2014.,1 Introduction,[0],[0]
"c©2014 Association for Computational Linguistics.
and preferred(Nikon D7000, Canon 7D, textitprice).
",1 Introduction,[0],[0]
"However, constructing a fully annotated training corpus for this task is labor-intensive and requires strong linguistic background.",1 Introduction,[0],[0]
"We minimize this overhead by applying a simplified annotation scheme, in which annotators mark mentions of entities and attributes, disambiguate the entities, and label instances of relations for each sentence.",1 Introduction,[0],[0]
"Based on the new scheme, we have created a small Sentiment Relation Graph (SRG) corpus for the domains of cameras and movies, which significantly differs from the corpora used in prior work (Wei and Gulla, 2010; Kessler et al., 2010; Toprak et al., 2010; Wiebe et al., 2005; Hu and Liu, 2004) in the following ways: i) both sentiment polarities and comparative relations are annotated; ii) all mentioned entities are disambiguated; and iii) no subjective expressions are annotated, unless they are part of entity mentions.
",1 Introduction,[0],[0]
The new annotation scheme raises a new challenge for learning algorithms in that they need to automatically find textual evidences for each annotated relation during training.,1 Introduction,[0],[0]
"For example, with the sentence “I like the Rebel a little better, but that is another price jump”, simply assigning a sentimentbearing expression to the nearest relation candidate is insufficient, especially when the sentiment is not explicitly expressed.
",1 Introduction,[0],[0]
"In this paper, we propose SENTI-LSSVM, a latent structural SVM based model for sentiment-oriented relation extraction.",1 Introduction,[0],[0]
"SENTI-LSSVM is applied to find the most likely set of the relation instances expressed in a given sentence, where the latent variables are used to assign the most appropriate textual evidences to the respective instances.
",1 Introduction,[0],[0]
"In summary, the contributions of this paper are the following:
• We propose SENTI-LSSVM: the first unified statistical model with the capability of extracting instances of both binary and ternary sentimentoriented relations.
",1 Introduction,[0],[0]
"• We design a task-specific integer linear programming (ILP) formulation for inference.
",1 Introduction,[0],[0]
"• We construct a new SRG corpus as a valuable asset for the evaluation of sentiment relation
extraction.
",1 Introduction,[0],[0]
"• We conduct extensive experiments with online reviews and forum posts, showing that SENTI-LSSVM model can effectively learn from a training corpus without explicitly annotated subjective expressions and that its performance significantly outperforms state-of-the-art systems.",1 Introduction,[0],[0]
"There are ample works on analyzing sentiment polarities and entity comparisons, but the majority of them studied the two tasks in isolation.
",2 Related Work,[0],[0]
Most prior approaches for fine-grained sentiment analysis focus on polarity classification.,2 Related Work,[0],[0]
"Supervised approaches on expression-level analysis require the annotation of sentiment-bearing expressions as training data (Jin et al., 2009; Choi and Cardie, 2010; Johansson and Moschitti, 2011; Yessenalina and Cardie, 2011; Wei and Gulla, 2010).",2 Related Work,[0],[0]
"However, the corresponding annotation process is time-consuming.",2 Related Work,[0],[0]
"Although sentence-level annotations are easier to obtain, the analysis at this level cannot cope with sentences conveying relations of multiple types (McDonald et al., 2007; Täckström and McDonald, 2011; Socher et al., 2012).",2 Related Work,[0],[0]
"Lexiconbased approaches require no training data (Ku et al., 2006; Kim and Hovy, 2006; Godbole et al., 2007; Ding et al., 2008; Popescu and Etzioni, 2005; Liu et al., 2005) but suffer from inferior performance (Wilson et al., 2005; Qu et al., 2012).",2 Related Work,[0],[0]
"In contrast, our method requires no annotation of sentiment-bearing expressions for training and can predict both sentiment polarities and comparative relations.
",2 Related Work,[0],[0]
"Sentiment-oriented comparative relations have been studied in the context of user-generated discourse (Jindal and Liu, 2006; Ganapathibhotla and Liu, 2008).",2 Related Work,[0],[0]
Approaches rely on linguistically motivated rules and assume the existence of independent keywords in sentences which indicate comparative relations.,2 Related Work,[0],[0]
"Therefore, these methods fall short of extracting comparative relations based on domain dependent information.
",2 Related Work,[0],[0]
Both Johansson and Moschitti (2011) and Wu et al. (2011) formulate fine-grained sentiment analysis as a learning problem with structured outputs.,2 Related Work,[0],[0]
"However, they focus only on polarity classification
of expressions and require annotation of sentimentbearing expressions for training as well.
While ILP has been previously applied for inference in sentiment analysis (Choi and Cardie, 2009; Somasundaran and Wiebe, 2009; Wu et al., 2011), our task requires a complete ILP reformulation due to 1) the absence of annotated sentiment expressions and 2) the constraints imposed by the joint extraction of both sentiment polarity and comparative relations.",2 Related Work,[0],[0]
This section gives an overview of the whole system for extracting sentiment-oriented relation instances.,3 System Overview,[0.961773796870215],['This provides an informative summary of the model posterior dispersion.']
"Prior to presenting the system architecture, we introduce the essential concepts and the definitions of two kinds of directed hypergraphs as the representation of correlated relation instances extracted from sentences.",3 System Overview,[0],[0]
Entity.,3.1 Concepts and Definitions,[0],[0]
"An entity is an abstract or concrete thing, which needs not be of material existence.",3.1 Concepts and Definitions,[0],[0]
An entity in this paper refers to either a product or a brand.,3.1 Concepts and Definitions,[0],[0]
Attribute.,3.1 Concepts and Definitions,[0],[0]
"An attribute is an object closely associated with or belonging to an entity, such as the lens of digital camera.",3.1 Concepts and Definitions,[0],[0]
Sentiment-Oriented Relation.,3.1 Concepts and Definitions,[0],[0]
"A sentimentoriented relation is either a sentiment polarity or a comparative relation, defined on tuples of entities and attributes.",3.1 Concepts and Definitions,[0],[0]
"A sentiment polarity relation conveys either a positive or a negative attitude towards entities or their attributes, whereas a comparative relation indicates the preference of one entity over the other entity w.r.t.",3.1 Concepts and Definitions,[0],[0]
an attribute.,3.1 Concepts and Definitions,[0],[0]
Relation Instance.,3.1 Concepts and Definitions,[0],[0]
"An instance of sentiment polarity takes the form r(entity, attribute) with r ∈ {positive, negative}, such as positive(Canon 7D, sensor).",3.1 Concepts and Definitions,[0],[0]
"The polarity instances expressed in the form of unary relations, such as “Nikon D7000 is excellent.”, are denoted as binary relations r(entity, whole), where the attribute whole indicates the entity as a whole.",3.1 Concepts and Definitions,[0],[0]
"In contrast, an instance of comparative relation is in the form of preferred{entity, entity, attribute}, e.g. preferred(Canon 7D, Nikon D7000, price).",3.1 Concepts and Definitions,[0],[0]
"For brevity, we refer to an instance set of sentiment-oriented relations extracted from a
sentence as an sSoR. To represent the instances of the remaining relations, we represent them as other{entity, attribute}, such as textitpartOf{wheel, car}.",3.1 Concepts and Definitions,[0],[0]
These relations include objective relations and the subjective relations other than sentimentoriented relations.,3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Instances.,3.1 Concepts and Definitions,[0],[0]
A mentionbased relation instance refers to a tuple of entity mentions with a certain relation.,3.1 Concepts and Definitions,[0],[0]
"This concept is introduced as the representation of instances in a sentence by replacing entities with the corresponding entity mentions, such as positive(“Canon SD880i”, “wide angle view”).
",3.1 Concepts and Definitions,[0],[0]
Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
A mention-based relation graph (or MRG ) represents a collection of mention-based relation instances expressed in a sentence.,3.1 Concepts and Definitions,[0],[0]
"As illustrated in Figure 1, an MRG is a directed hypergraph G = 〈M,E〉 with a vertex set M and an edge set E. A vertex mi ∈ M denotes a mention of an entity or an attribute occurring either within the sentence or in its context.",3.1 Concepts and Definitions,[0],[0]
We say that a mention is from the context if it is mentioned in the previous sentence or is an attribute implied in the current sentence.,3.1 Concepts and Definitions,[0],[0]
"An instance of a binary relation in an MRG takes the form of a binary edge el = (mi,ma), where mi and ma denote an entity mention and an attribute mention respectively, and the type l ∈ {positive, negative, other}.",3.1 Concepts and Definitions,[0],[0]
"A ternary edge el indicating comparative relation is represented as el = (mi,mj ,ma), where two entity mentions mi and mj are compared with respect to the attribute mention ma.",3.1 Concepts and Definitions,[0],[0]
"We define the type l ∈ {better,worse} to indicate two possible directions of the relation and assume mi occurs before mj .",3.1 Concepts and Definitions,[0],[0]
"As a result, we have a set L of five relation types: positive, negative, better, worse or other.",3.1 Concepts and Definitions,[0],[0]
"According to these definitions, the annotations in the SRG corpus are actually MRGs and disambiguated entities.",3.1 Concepts and Definitions,[0],[0]
"If there are multiple mentions referring to the same entity, annotators are asked to choose the
most obvious one because it saves annotation time and is less demanding for the entity recognition and diambiguation modules.
",3.1 Concepts and Definitions,[0],[0]
Evidentiary Mention-Based Relation Graph.,3.1 Concepts and Definitions,[0],[0]
"An evidentiary mention-based relation graph, coined eMRG , extends an MRG by associating each edge with a textual evidence to support the corresponding relation assertions (see Figure 2).",3.1 Concepts and Definitions,[0],[0]
"Consequently, an edge in an eMRG is denoted by a pair (a, c), where a represents a mention-based relation instance and c is the associated textual evidence.",3.1 Concepts and Definitions,[0],[0]
It is also referred to as an evidentiary edge.,3.1 Concepts and Definitions,[0],[0]
"represented as el = (mi,mj ,ma), an MRG as an evidentiary MRG (eMRG) and the edges of eMRGs as evidentiary edges, as shown in Figure 2.",3.1 Concepts and Definitions,[0],[0]
"As illustrated by Figure 3, at the core of our system is the SENTI-LSSVM model, which extracts sets
of mention-based relationships in the form of eMRGs from sentences.",3.2 System Architecture,[0],[0]
"For a given sentence with known entity mentions, we select all possible mention sets as relation candidates, where each set includes at least one entity mention.",3.2 System Architecture,[0],[0]
Then we associate each relation candidate with a set of constituents or the whole sentence as the textual evidence candidates (cf. Section 6.1).,3.2 System Architecture,[0],[0]
"Subsequently, the inference component aims to find the most likely eMRG from all possible combinations of mention-based relation instances and their textual evidences (cf. Section 6.2).",3.2 System Architecture,[0],[0]
The representation eMRG is chosen because it characterizes exactly the model outputs by letting each edge correspond to an instance of mention-based relation and the associated textual evidence.,3.2 System Architecture,[0],[0]
"Finally, the model parameters of this model are learned by an online algorithm (cf. Section 7).
",3.2 System Architecture,[0],[0]
"Since instance sets of sentiment-oriented relations (sSoRs) are the expected outputs, we can obtain sSoRs from MRGs by using a simple rule-based algorithm.",3.2 System Architecture,[0],[0]
The algorithm essentially maps the mentions from an MRG into entities and attributes in an sSoR and label the corresponding tuples with the relation types of the edges from an MRG.,3.2 System Architecture,[0],[0]
"For instances of comparative relation, the label better or worse is mapped to the relation type preferred.",3.2 System Architecture,[0],[0]
The task of sentiment-oriented relation extraction is to determine the most likely sSoR in a sentence.,4 SENTI-LSSVM Model,[0],[0]
"Since sSoRs are derived from the corresponding MRGs as described in Section 3, the task is reduced to find the most likely MRG for each sentence.",4 SENTI-LSSVM Model,[0],[0]
"Since an MRG is created by assigning relation types to a subset of all relation candidates, which are possible tuples of mentions with unknown relation types, the number of MRGs can be extremely high.
",4 SENTI-LSSVM Model,[0],[0]
"To tackle the task, one solution is to employ an edge-factored linear model in the framework of structural SVM (Martins et al., 2009; Tsochantaridis et al., 2004).",4 SENTI-LSSVM Model,[0],[0]
"The model suggests that a bag of features should be specified for each relation candidate, and then the model predicts the most likely candidate sets along with their relation types to form the optimal MRGs.",4 SENTI-LSSVM Model,[0],[0]
"As we observed, for a relation candidate, the most informative features are the words near its entity mentions in the original text.",4 SENTI-LSSVM Model,[0],[0]
"How-
ever, if we represent a candidate by all these words, it is very likely that the instances of different relation types share overly similar features, because a mention is often involved in more than one relation candidate, as shown in Figure 2.",4 SENTI-LSSVM Model,[0],[0]
"As a consequence, the instances of different relations represented by overly similar features can easily confuse the learning algorithm.",4 SENTI-LSSVM Model,[0],[0]
"Thus, it is critical to select proper constituents or sentences as textual evidences for each relation candidate in both training and testing.
",4 SENTI-LSSVM Model,[0],[0]
"Consequently, we divide the task of sentimentoriented relation extraction into two subtasks : i) identifying the most likely MRGs; ii) assigning proper textual evidences to each edge of MRGs to support their relation assertions.",4 SENTI-LSSVM Model,[0],[0]
It is desirable to carry out the two subtasks jointly as these two subtasks could enhance each other.,4 SENTI-LSSVM Model,[0],[0]
"First, the identification of relation types requires proper textual evidences; second, the soft and hard constraints imposed by the correlated relation instances facilitate the recognition of the corresponding textual evidences.",4 SENTI-LSSVM Model,[0],[0]
"Since the eMRGs are created by attaching every MRG with a set of textual evidences, tackling the two subtasks simultaneously is equivalent to selecting the most likely eMRG from a set of eMRG candidates.",4 SENTI-LSSVM Model,[0],[0]
"It is challenging because our SRG corpus does not contain any annotation of textual evidences.
",4 SENTI-LSSVM Model,[0],[0]
"Formally, let X denote the set of all available sentences, and we define y ∈ Y(x)(x ∈ X ) as the set of labeled edges of an MRG and Y = ∪x∈XY(x).",4 SENTI-LSSVM Model,[0],[0]
"Since the assignments of textual evidences are not observed, an assignment of evidences to y is denoted by a latent variable h ∈ H(x) and H = ∪x∈XH(x).",4 SENTI-LSSVM Model,[0],[0]
"Then (y, h) corresponds to an eMRG, and (a, c) ∈ (y, h) is a labeled edge a attached with a textual evidence c.",4 SENTI-LSSVM Model,[0],[0]
"Given a labeled dataset D = {(x1, y1), ..., (xn, yn)} ∈ (X × Y)n, we aim to learn a discriminant function f : X",4 SENTI-LSSVM Model,[0],[0]
"→ Y×H that outputs the optimal eMRG (y, h) ∈ Y(x)×H(x) for a given sentence x.
Due to the introduction of latent variables, we adopt the latent structural SVM (Yu and Joachims, 2009) for structural classification.",4 SENTI-LSSVM Model,[0],[0]
"Our discriminant function is defined as
f(x) =",4 SENTI-LSSVM Model,[0],[0]
"argmax(y,h)∈Y(x)×H(x)β >Φ(x, y, h) (1)
where Φ(x, y, h) is the feature function of an eMRG (y, h) and β is the corresponding weight vector.
",4 SENTI-LSSVM Model,[0],[0]
"To ensure tractability, we also employ edge-based factorization for our model.",4 SENTI-LSSVM Model,[0],[0]
"Let Mp denote a set of entity mentions and yr(mi) be a set of edges labeled with sentiment-oriented relations incident to mi, the factorization of Φ(x, y, h) is given as
Φ(x, y, h) = ∑
(a,c)∈(y,h) Φe(x, a, c) + (2)
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ Φc(a, a
′)
where Φe(x, a, c) is a local edge feature function for a labeled edge a attached with a textual evidence c and Φc(a, a′) is a feature function capturing cooccurrence of two labeled edges ami and a ′ mi incident to an entity mention mi.",4 SENTI-LSSVM Model,[0],[0]
"The following features are used in the feature functions (Equation 2):
Unigrams: As mentioned before, a textual evidence attached to an edge in MRG is either a word, phrase or sentence.",5 Feature Space,[0],[0]
"We consider all lemmatized unigrams in the textual evidence as unigram features.
",5 Feature Space,[0],[0]
"Context: Since web users usually express related sentiments about the same entity across sentence boundaries, we describe the sentiment flow using a set of contextual binary features.",5 Feature Space,[0],[0]
"For example, if entity A is mentioned in both the previous sentence and the current sentence, a set of contextual binary features are used to indicate all possible combinations of the current and the previous mentioned sentimentoriented relations regarding to entity A.
Co-occurrence: We have mentioned the cooccurrence feature in Equation 2, indicated by Φc(a, a
′).",5 Feature Space,[0],[0]
It captures the co-occurrence of two labeled edges incident to the same entity mention.,5 Feature Space,[0],[0]
"Note that the co-occurrence feature function is considered only if there is a contrast conjunction such as “but” between the non-shared entity mentions incident to the two labeled edges.
",5 Feature Space,[0],[0]
"Senti-predictors: Following the idea of (Qu et al., 2012), we encode the prediction results from the rule-based phrase-level multi-relation predictor (Ding et al., 2009) and from the bag-of-opinions predictor (Qu et al., 2010) as features based on the textual evidence.",5 Feature Space,[0],[0]
"The output of the first predictor is an integer value, while the output of the second predictor is a sentiment relation, such as “positive”,
“negative”, “better” or “worse”.",5 Feature Space,[0],[0]
"We map the relational outputs into integer values and then encode the outputs from both predictors as senti-predictor features.
",5 Feature Space,[0],[0]
Others: The commonly used part-of-speech tags are also included as features.,5 Feature Space,[0],[0]
"Moreover, for an edge candidate, a set of binary features are used to denote the types of the edge and its entity mentions.",5 Feature Space,[0],[0]
"For instance, a binary feature indicates whether an edge is a binary edge related to an entity mentioned in context.",5 Feature Space,[0],[0]
"To characterize the syntactic dependencies between two adjacent entity mentions, we use the path in the dependency tree between the heads of the corresponding constituents, the number of words and other mentions in-between as features.",5 Feature Space,[0],[0]
"Additionally, if the textual evidence is a constituent, its feature w.r.t.",5 Feature Space,[0],[0]
an edge is the dependency path to the closest mention of the edge that does not overlap with this constituent.,5 Feature Space,[0],[0]
"In order to find the best eMRG for a given sentence with a well trained model, we need to determine the most likely relation type for each relation candidate and support the corresponding assertions with proper textual evidences.",6 Structural Inference,[0],[0]
We formulate this task as an Integer Linear Programming (ILP).,6 Structural Inference,[0],[0]
"Instead of considering all constituents of a sentence, we empirically select a subset as textual evidences for each relation candidate.",6 Structural Inference,[0],[0]
"Textual evidences are selected based on the constituent trees of sentences parsed by the Stanford parser (Klein and Manning, 2003).",6.1 Textual Evidence Candidates Selection,[0],[0]
"For each mention in a sentence, we first locate a constituent in the tree with the maximal overlap by Jaccard similarity.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Starting from this constituent, we consider two types of candidates: type I candidates are constituents at the highest level which contain neither any word of another mention nor any contrast conjunctions such as “but”; type II candidates are constituents at the highest level which cover exactly two mentions of an edge and do not overlap with any other mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting an entity mention and an attribute mention, we consider a type I candidate starting from the attribute men-
tion.",6.1 Textual Evidence Candidates Selection,[0],[0]
"For a binary edge connecting two entity mentions, we consider type I candidates starting from both mentions.",6.1 Textual Evidence Candidates Selection,[0],[0]
"Moreover, for a comparative ternary edge, we consider both type I and type II candidates starting from the attribute mention.",6.1 Textual Evidence Candidates Selection,[0],[0]
This strategy is based on our observation that these candidates often cover the most important information w.r.t.,6.1 Textual Evidence Candidates Selection,[0],[0]
the covered entity mentions.,6.1 Textual Evidence Candidates Selection,[0],[0]
"We formulate the inference problem of finding the best eMRG as an ILP problem due to its convenient integration of both soft and hard constraints.
",6.2 ILP Formulation,[0],[0]
"Given the model parameters β, we reformulate the score of an eMRG in the discriminant function (1) as follows,
β>Φ(x, y, h) = ∑
(a,c)∈(y,h) saczac +
∑
mi∈Mp
∑
a,a′∈yr(mi),a 6=a′ saa′zaa′
where sac = β>Φe(x, a, c) denotes the score of a labeled edge a attached with a textual evidence c, saa′ = β
>Φc(a, a′) is the edge co-occurrence score, the binary variable zac indicates the presence or absence of the corresponding edge, and zaa′ indicates if two edges co-occurr.",6.2 ILP Formulation,[0],[0]
"As not every edge set can form an eMRG, we require that a valid eMRG should satisfy a set of linear constraints, which form our constraint space.",6.2 ILP Formulation,[0],[0]
"Then function (1) is equivalent to
max z∈B
s>z + µzd
s.t.",6.2 ILP Formulation,[0],[0]
A   z η,6.2 ILP Formulation,[0],[0]
τ   ≤,6.2 ILP Formulation,[0],[0]
"d
z,η, τ ∈ B
where B = 2S with S = {0, 1}, and η and τ are auxiliary binary variables that help define the constraint space.",6.2 ILP Formulation,[0],[0]
"The above optimization problem takes exactly the form of an ILP because both the constraints and the objective function are linear, and all variables take only integer values.
",6.2 ILP Formulation,[0],[0]
"In the following, we consider two types of constraint space, 1) an eMRG with only binary edges and 2) an eMRG with both binary and ternary edges.
eMRG with only Binary Edges: An eMRG has only binary edges if a sentence contains no attribute mention or at most one entity mention.",6.2 ILP Formulation,[0],[0]
We expect that each edge has only one relation type and is supported by a single textual evidence.,6.2 ILP Formulation,[0],[0]
"To facilitate the formulation of constraints, we introduce ηel to denote the presence or absence of a labeled edge el, and ηec to indicate if a textual evidence c is assigned to an unlabeled edge e. Then the binary variable for the corresponding evidentiary edge zelc = ηec ∧ ηel , where the ILP formulation of conjunction can be found in (Martins et al., 2009).
",6.2 ILP Formulation,[0],[0]
Let Ce denote the set of textual evidence candidates of an unlabeled edge e.,6.2 ILP Formulation,[0],[0]
"The constraint of at most one textual evidence per edge is formulated as:
∑
c∈Ce ηec ≤ 1 (3)
",6.2 ILP Formulation,[0],[0]
"Once a textual evidence is assigned to an edge, their relation labels should match and the number of labeled edges must agree with the number of attached textual evidences.",6.2 ILP Formulation,[0],[0]
"Further, we assume that a textual evidence c conveys at most one relation so that an evidence will not be assigned to the relations of different types, which is the main problem for the structural SVM based model.",6.2 ILP Formulation,[0],[0]
"Let ηcl indicate that the textual evidence c is labeled by the relation type l. The corresponding constraints are expressed as,
∑ l∈Le ηel = ∑ c∈Ce ηec; zelc ≤ ηcl; ∑ l∈L ηcl ≤ 1
where Le denotes the set of all possible labels for an unlabeled edge e, and L is the set of all relation types of MRGs (cf. Section 3).
",6.2 ILP Formulation,[0],[0]
"In order to avoid a textual evidence being overly reused by multiple relation candidates, we first penalize the assignment of a textual evidence c to a labeled edge a by associating the corresponding zac with a fixed negative cost −µ in the objective function.",6.2 ILP Formulation,[0],[0]
"Then the selection of one textual evidence per edge a is encouraged by associating µ to zdc in the objective function, where zdc = ∨ e∈Sc ηec and Sc is the set of edges that the textual evidence c serves as a candidate.",6.2 ILP Formulation,[0],[0]
"The disjunction zdc is expressed as:
zdc ≥ ηe, e ∈",6.2 ILP Formulation,[0],[0]
"Sc zdc ≤ ∑
e∈Sc ηe
This soft constraint not only encourages one textual evidence per edge, but also keeps it eligible for multiple assignments.
",6.2 ILP Formulation,[0],[0]
"For any two labeled edge a and a′ incident to the same entity mention, the edge-to-edge cooccurrence is described by zca,a′ = za",6.2 ILP Formulation,[0],[0]
"∧ za′ .
eMRG with both Binary and Ternary Edges: If there are more than one entity mentions and at least one attribute mention in a sentence, an eMRG can potentially have both binary and ternary edges.",6.2 ILP Formulation,[0],[0]
"In this case, we assume that each mention of attributes can participate either in binary relations or in ternary relations.",6.2 ILP Formulation,[0],[0]
"The assumption holds in more than 99.9% of the sentences in our SRG corpus, thus we describe it as a set of hard constraints.",6.2 ILP Formulation,[0],[0]
"Geometrically, the assumption can be visualized as the selection between two alternative structures incident to the same attribute mention, as shown in Figure 4.",6.2 ILP Formulation,[0],[0]
"Note that, in the binary edge structure, we include not only the edges incident to the attribute mention but also the edge between the two entity mentions.
",6.2 ILP Formulation,[0],[0]
Let Sbmi be the set of all possible labeled edges in a binary edge structure of an attribute mention mi.,6.2 ILP Formulation,[0],[0]
Variable τ,6.2 ILP Formulation,[0],[0]
"bmi = ∨ el∈Sbmi
ηel indicates whether the attribute mention is associated with a binary edge structure or not.",6.2 ILP Formulation,[0],[0]
"In the same manner, we use τ tmi = ∨ el∈Stmi
ηel to indicate the association of the an attribute mention mi with an ternary edge structure from the set of all incident ternary edges Stmi .",6.2 ILP Formulation,[0],[0]
"The selection between two alternative structures is
formulated as τ bmi + τ",6.2 ILP Formulation,[0],[0]
t mi = 1.,6.2 ILP Formulation,[0],[0]
"As this influences only the edges incident to an attribute mention, we keep all the constraints introduced in the previous section unchanged except for constraint (3), which is modified as
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"bmi ;
∑ c∈Ce ηec ≤ τ",6.2 ILP Formulation,[0],[0]
"tmi
Therefore, we can have either binary edges or ternary edges for an attribute mention.",6.2 ILP Formulation,[0],[0]
"Given a set of training sentences D = {(x1, y1), . . .",7 Learning Model Parameters,[0],[0]
", (xn, yn)}, the best weight vector β of the discriminant function (1) is found by solving the following optimization problem:
min β
1
n
n∑
i=1
[ max (ŷ,ĥ)∈Y(x)×H(x)
(β>Φ(x, ŷ, ĥ)+δ(ĥ, ŷ, y))
",7 Learning Model Parameters,[0],[0]
"− max h̄∈H(x)
β>Φ(x, y, h̄)]",7 Learning Model Parameters,[0],[0]
"+ ρ|β|] (4)
where δ(ĥ, ŷ, y) is a loss function measuring the discrepancies between an eMRG (y, h̄) with gold standard edge labels y and an eMRG (ŷ, ĥ) with inferred labeled edges ŷ and textual evidences ĥ. Due to the sparse nature of the lexical features, we apply L1 regularizer to the weight vector β, and the degree of sparsity is controlled by the hyperparameter ρ.
",7 Learning Model Parameters,[0],[0]
"Since the L1 norm in the above optimization problem is not differentiable at zero, we apply the online forward-backward splitting (FOBOS) algorithm (Duchi and Singer, 2009).",7 Learning Model Parameters,[0],[0]
"It requires two steps for updating the weight vector β by using a single training sentence x on each iteration t.
βt+ 12 = βt",7 Learning Model Parameters,[0],[0]
"− εt∆t
βt+1",7 Learning Model Parameters,[0],[0]
"= arg min β
1 2 ‖β",7 Learning Model Parameters,[0],[0]
"− βt‖2 + εtρ|β|
where ∆t is the subgradient computed without considering the L1 norm and εt is the learning rate.",7 Learning Model Parameters,[0],[0]
"For a labeled sentence x, ∆t = Φ(x, ŷ∗, ĥ∗) − Φ(x, y, h̄∗), where the feature functions of the corresponding eMRGs are inferred by solving (ŷ∗, ĥ∗) = arg max(ĥ,ŷ)∈H(x)×Y(x)[β >Φ(x, ŷ, ĥ) +",7 Learning Model Parameters,[0],[0]
"δ(ĥ, ŷ, y)] and (y, h̄∗) = arg maxh̄∈H(x) β",7 Learning Model Parameters,[0],[0]
>,7 Learning Model Parameters,[0],[0]
"Φ(x, y, h̄), as indicated in the optimization problem (4).
",7 Learning Model Parameters,[0],[0]
The former inference problem is similar to the one we considered in the previous section except the inclusion of the loss function.,7 Learning Model Parameters,[0],[0]
"We incorporate the loss function into the ILP formulation by defining the loss between an MRG (y, h) and a gold standard MRG as the sum of per-edge costs.",7 Learning Model Parameters,[0],[0]
"In our experiments, we consider a positive cost ϕ for each wrongly labeled edge a, so that if an edge a has a different label from the gold standard, we add ϕ to the coefficient sac of the corresponding variable zac in the objective function of the ILP formulation.
",7 Learning Model Parameters,[0],[0]
"In addition, since the non-positive weights of edge labels in the initial learning phrase often lead to eMRGs with many unlabeled edges, which harms the learning performance, we fix it by adding a constraint for the minimal number of labeled edges in an eMRG, ∑
a∈A
∑
c∈Ca ηac ≥ ζ (5)
where A is the set of all labeled edge candidates and ζ denotes the minimal number of labeled edges.
",7 Learning Model Parameters,[0],[0]
"Empirically, the best way to determine ζ is to make it equal to the maximal number of labeled edges in an eMRG with the restriction that a textual evidence can be assigned to at most one edge.",7 Learning Model Parameters,[0],[0]
"By considering all the edge candidates A and all the textual evidence candidates C as two vertex sets in a bipartite graph Ĝ = 〈V = (A,C), E〉 (with edges in E indicating which textual evidence can be assigned to which edge), ζ corresponds to exactly the size of a maximum matching of the bipartite graph1.
",7 Learning Model Parameters,[0],[0]
"To find the optimal eMRG (y, h̄∗), for the gold label k of each edge, we consider the following set of constraints for inference since the labels of the edges are known for the training data,
∑ c∈Ce ηec ≤ 1; ηec ≤ lck ∑ k′∈L lck′ ≤ 1; ∑ e∈Sc ηec ≤ 1
",7 Learning Model Parameters,[0],[0]
"We include also the soft constraints, which avoid a textual evidence being overly reused by multiple relations, and the constraints similar to (5) to ensure a minimal number of labeled edges and a minimal number of sentiment-oriented relations.
",7 Learning Model Parameters,[0],[0]
"1It is computed by the Hopcroft-Karp algorithm (Hopcroft and Karp, 1973) in our implementation.",7 Learning Model Parameters,[0],[0]
"For evaluation we constructed the SRG corpus, which in total consists of 1686 manually annotated online reviews and forum posts in the digital camera and movie domains2.",8 SRG Corpus,[0],[0]
"For each domain, we maintain a set of attributes and a list of entity names.
",8 SRG Corpus,[0],[0]
The annotation scheme for the sentiment representation asserts minimal linguistic knowledge from our annotators.,8 SRG Corpus,[0],[0]
"By focusing on the meanings of the sentences, the annotators make decisions based on their language intuition, not restricted by specific syntactic structures.",8 SRG Corpus,[0],[0]
"Taking the example in Figure 2, the annotators only need to mark the mentions of entities and attributes from both the sentences and the context, disambiguate them, and label (“Canon 7D”, “Nikon D7000”, price) as worse and (“Canon 7D”, “sensor”) as positive, whereas in prior work, people have annotated the sentiment-bearing expressions such as “great” and link them to the respective relation instances as well.",8 SRG Corpus,[0],[0]
"This also enables them to annotate instances of both sentiment polarity and comparative relaton, which are conveyed by not only explicit sentiment-bearing expressions like “excellent performance”, but also factual expressions implying evaluations such as “The 7V has 10x optical zoom and the 9V has 16x.”.
",8 SRG Corpus,[0],[0]
14 annotators participated in the annotation project.,8 SRG Corpus,[0],[0]
"After a short training period, annotators worked on randomly assigned documents one at a time.",8 SRG Corpus,[0],[0]
"For product reviews, the system lists all relevant information about the entity and the predefined attributes.",8 SRG Corpus,[0],[0]
"For forum posts, the system shows only the attribute list.",8 SRG Corpus,[0],[0]
"For each sentence in a document, the annotator first determines if it refers to an entity of interest.",8 SRG Corpus,[0],[0]
"If not, the sentence is marked
2The 107 camera reviews are from bestbuy.com and Amazon.com; the 667 camera forum posts are downloaded from forum.digitalcamerareview.com; the 138 movie reviews and 774 forum posts are from imdb.com and boards.ie respectively
as off-topic.",8 SRG Corpus,[0],[0]
"Otherwise, the annotator will identify the most obvious mentions, disambiguate them, and mark the MRGs.",8 SRG Corpus,[0],[0]
"We evaluate the inter-annotator agreement on sSoRs in terms of Cohen’s Kappa (κ) (Cohen, 1968).",8 SRG Corpus,[0],[0]
"An average Kappa value of 0.698 was achieved on a randomly selected set consisting of 412 sentences.
",8 SRG Corpus,[0],[0]
Table 1 shows the corpus distribution after normalizing them into sSoRs.,8 SRG Corpus,[0],[0]
Camera forum posts contain the largest proportion of comparisons because they are mainly about the recommendation of digital cameras.,8 SRG Corpus,[0],[0]
"In contrast, web users are much less interested in comparing movies, in both reviews and forums.",8 SRG Corpus,[0],[0]
"In all subsets, positive relations play a dominant role since web users intend to express more positive attitudes online than negative ones (Pang and Lee, 2007).",8 SRG Corpus,[0],[0]
This section describes the empirical evaluation of SENTI-LSSVM together with two competitive baselines on the SRG corpus.,9 Experiments,[0],[0]
"We implemented a rule-based baseline (DINGRULE) and a structural SVM (Tsochantaridis et al., 2004) baseline (SENTI-SSVM) for comparison.",9.1 Experimental Setup,[0],[0]
"The former system extends the work of Ding et al. (2009), which designed several linguisticallymotivated rules based on a sentiment polarity lexicon for relation identification and assumes there is only one type of sentiment relation in a sentence.",9.1 Experimental Setup,[0],[0]
"In our implementation, we keep all the rules of (Ding et al., 2009) and add one phrase-level rule when there are more than one mention in a sentence.",9.1 Experimental Setup,[0],[0]
The additional rule assigns sentiment-bearing words and negators to its nearest relation candidates based on the absolute surface distance between the words and the corresponding mentions.,9.1 Experimental Setup,[0],[0]
"In this case, the phraselevel sentiment-oriented relations depend only on the assigned sentiment words and negators.",9.1 Experimental Setup,[0],[0]
The latter system is based on a structural SVM and does not consider the assignment of textual evidences to relation instances during inference.,9.1 Experimental Setup,[0],[0]
"The textual features of a relation candidate are all lexical and sentiment predictor features within a surface distance of four words from the mentions of the candidate.
",9.1 Experimental Setup,[0],[0]
"Thus, this baseline does not need the inference constraints of SENTI-LSSVM for the selection of textual evidences.",9.1 Experimental Setup,[0],[0]
"To gain more insights into the model, we also evaluate the contribution of individual features of SENTI-LSSVM.",9.1 Experimental Setup,[0],[0]
"In addition, to show if identifying sentiment polarities and comparative relations jointly works better than tackling each task on its own, we train SENTI-LSSVM for each task separately and combine their predictions according to compatibility rules and the corresponding graph scores.
",9.1 Experimental Setup,[0],[0]
"For each domain and text genre, we withheld 15% documents for development and use the remaining for cross validation.",9.1 Experimental Setup,[0],[0]
The hyperparameters of all systems are tuned on the development datasets.,9.1 Experimental Setup,[0],[0]
"For all experiments of SENTI-LSSVM, we use ρ = 0.0001 for the L1 regularizer in Eq.(4) and ϕ = 0.05 for the loss function; and for SENTI-SSVM, ρ = 0.0001 and ϕ = 0.01.",9.1 Experimental Setup,[0],[0]
"Since the relation type of off-topic sentences is certainly other, we evaluate all systems with 5-fold cross-validation only on the on-topic sentences in the evaluation dataset.",9.1 Experimental Setup,[0],[0]
"Since the same sSoR can have several equivalent MRGs and the relation type other is not of our interest, we evaluate the sSoRs in terms of precision, recall and F-measure.",9.1 Experimental Setup,[0],[0]
All reported numbers are averages over the 5 folds.,9.1 Experimental Setup,[0],[0]
Table 2 shows the complete results of all systems.,9.2 Results,[0],[0]
Here our model SENTI-LSSVM outperformed all baselines in terms of the average F-measure scores and recalls by a large margin.,9.2 Results,[0],[0]
The F-measure on movie reviews is about 14% over the best baseline.,9.2 Results,[0],[0]
The rule-based system has higher precision than recall in most cases.,9.2 Results,[0],[0]
"However, simply increasing the coverage of the domain independent sentiment polarity lexicon might lead to worse performance (Taboada et al., 2011) because many sentiment oriented relations are conveyed by domain dependent expressions and factual expressions implying evaluations, such as “This camera does not have manual control.”",9.2 Results,[0],[0]
"Compared to DING-RULE, SENTI-SSVM performs better in the camera domain but worse for the movies due to many misclassification of negative relation instances as other.",9.2 Results,[0],[0]
It also wrongly predicted more positive instances as other than SENTI-LSSVM.,9.2 Results,[0],[0]
"We found that the recalls of these instances are low because they often have overly similar features with the instances of the type
other linking to the same mentions.",9.2 Results,[0],[0]
The problem gets worse in the movie domain since i),9.2 Results,[0],[0]
many sentences contain no explicit sentiment-bearing words; ii) the prior polarity of the sentiment-bearing words do not agree with their contextual polarity in the sentences.,9.2 Results,[0],[0]
Consider the following example from a forum post about the movie “Superman Returns”: “Have a look at Superman: the Animated Series or Justice League Unlimited . . .,9.2 Results,[0],[0]
that is how the characters of Superman and Lex Luthor should be.”.,9.2 Results,[0],[0]
"In contrast, our model minimizes the overlapping features by assigning them to the most likely relation candidates.",9.2 Results,[0],[0]
This leads to significantly better performance.,9.2 Results,[0],[0]
"Although SENTI-SSVM has low recall for both positive and negative relations, it achieves the highest recall for the comparative relation among all systems in the movie domain and camera reviews.",9.2 Results,[0],[0]
"Since less than 1% of all instances are for comparative relations in these document sets and all models are trained to optimize the overall accuracy, SENTILSSVM intends to trade off the minority class for the overall better performance.",9.2 Results,[0],[0]
"This advantage disappears on the camera forum posts, where the number of instances of comparative relation is 12 times more than that in the other data sets.
",9.2 Results,[0],[0]
All systems perform better in predicting positive relations than the negative ones.,9.2 Results,[0],[0]
"This corresponds well to the empirical findings in (Wilson, 2008) that people intend to use more complex expressions for negative sentiments than their affirmative counterparts.",9.2 Results,[0],[0]
It is also in accordance with the distribution of these relations in our SRG corpus which is randomly sampled from the online documents.,9.2 Results,[0],[0]
"For learning systems, it can also be explained by the fact that the training data for positive relations are considerably more than those for negative ones.",9.2 Results,[0],[0]
"The comparative relation is the hardest one to process since we found that many corresponding expressions do not contain explicit keywords for comparison.
",9.2 Results,[0],[0]
"To understand the performance of the key feature groups in our model better, we remove each group from the full SENTI-LSSVM system and evaluate the variations with movie reviews and camera forum posts, which have relatively balanced distribution of relation types.",9.2 Results,[0],[0]
"As shown in Table 3, the features from the sentiment predictors make significant contributions for both datasets.",9.2 Results,[0],[0]
"The different drops of the performance indicate that the po-
larities predicted by rules are more consistent in camera forum posts than in movie reviews.",9.2 Results,[0],[0]
Due to the complexity of expressions in the movie reviews our model cannot benefit from the unigram features but these features are a good compensation for the sentiment predictor features in camera forum posts.,9.2 Results,[0],[0]
The sharp drop by removing the context features from our model on movie reviews indicates that the sentiments in movie reviews depend highly on the relations of the previous sentences.,9.2 Results,[0],[0]
"In contrast, the sentiment-oriented relations of the previous sentences could be a reason of overfitting for camera forum data.",9.2 Results,[0],[0]
The edge co-occurrence features do not play an important role in our model since the number of co-occurred sentiment-oriented relations in the sentences with contrast conjunctions like “but” is small.,9.2 Results,[0],[0]
"However, we found that allowing the co-occurrence of any sentiment-oriented relations would harm the performance of the model.
",9.2 Results,[0],[0]
"In addition, our experiments showed that the sep-
arated approach, which trains a model for sentiment polarities and comparative relations respectively, leads to a decrease by almost 1% in terms of the F-measure averaged over all four datasets.",9.2 Results,[0],[0]
"The largest drop of F-measure is 3% on camera forum posts, since this dataset contains the largest proportion of comparative relations.",9.2 Results,[0],[0]
We found that the errors are increased when the trained models make conflicting predictions.,9.2 Results,[0],[0]
"In this case, the joint approach can take all factors into account and make more consistent decisions than the separated approaches.",9.2 Results,[0],[0]
We proposed SENTI-LSSVM model for extracting instances of both sentiment polarities and comparative relations.,10 Conclusion,[0],[0]
"For evaluating and training the model, we created an SRG corpus by using a lightweight annotation scheme.",10 Conclusion,[0],[0]
We showed that our model can automatically find textual evidences to support its relation predictions and achieves significantly better F-measure scores than alternative state-of-the-art methods.,10 Conclusion,[0],[0]
Extracting instances of sentiment-oriented relations from user-generated web documents is important for online marketing analysis.,abstractText,[0],[0]
"Unlike previous work, we formulate this extraction task as a structured prediction problem and design the corresponding inference as an integer linear program.",abstractText,[0],[0]
"Our latent structural SVM based model can learn from training corpora that do not contain explicit annotations of sentiment-bearing expressions, and it can simultaneously recognize instances of both binary (polarity) and ternary (comparative) relations with regard to entity mentions of interest.",abstractText,[0],[0]
The empirical evaluation shows that our approach significantly outperforms stateof-the-art systems across domains (cameras and movies) and across genres (reviews and forum posts).,abstractText,[0],[0]
The gold standard corpus that we built will also be a valuable resource for the community.,abstractText,[0],[0]
Senti-LSSVM: Sentiment-Oriented Multi-Relation Extraction with Latent Structural SVM,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3654–3663 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3654",text,[0],[0]
"Sentiment analysis, a.k.a. opinion mining, is a task which aims to identify the user sentiment orientation of a product/brand/service by monitoring the online textual data, e.g., reviews and social media messages.",1 Introduction,[0],[0]
"It has attracted huge attention in both academic and industrial communities due to its widespread applications, such like recommendation (Zhang et al., 2014) and social media mining (Chambers et al., 2015).",1 Introduction,[0],[0]
"As the fundamental component in sentiment analysis, sentiment classification mainly classifies the sentiment polarity as positive or negative, and has been well-studied from both sentence-level (Kim and Hovy, 2004) and document-level (Xu et al., 2016).
",1 Introduction,[0],[0]
"∗Corresponding author
Recently, a new QA-style reviewing form, namely “customer questions & answers”, has become increasingly popular on the giant ecommerce platforms, e.g., Amazon and Taobao.",1 Introduction,[0],[0]
"In this new form, a potential customer asks question(s) about the target product/service while other experienced user(s) can provide answer(s).",1 Introduction,[0],[0]
"With the widespread of such QA-style reviews, users find a different channel to efficiently explore rich and useful information, and service providers and scholars are paying more attention to its specific characteristics comparing with traditional reviews (Wachsmuth et al., 2014; Zhou et al., 2015a).",1 Introduction,[0],[0]
"Comparing to the traditional reviews, the QA style reviews can be more informative and convincing.",1 Introduction,[0],[0]
"More importantly, because answer providers are randomly picked from the users who already purchased the target item, this new form of review can be more reliable and trustful.
",1 Introduction,[0],[0]
"Regarding QA-style sentiment analysis, one straightforward method is to directly employ an existing sentiment classification approach that works well on traditional reviews, such as RNN (Nguyen and Shirai, 2015) and LSTM (Chen et al., 2016).",1 Introduction,[0],[0]
"However, because of the significant differences between QA-style and classical reviews, existing review mining algorithms, e.g., text-based sentiment analysis/classification, should not be di-
rectly applied to this new kind of QA-style data.",1 Introduction,[0],[0]
"More detailed reasons can be found as the followings.
",1 Introduction,[0],[0]
"First, in QA-style text, the question and answer text are more likely to be two parallel units rather than a sequence form.",1 Introduction,[0],[0]
"On the one hand, for instance, in Figure 1, sentence “It’s a nice phone with high-quality screen.”",1 Introduction,[0],[0]
in Answer 1 actually does not follow sentence “How is the battery?”,1 Introduction,[0],[0]
"in Question 1 , but corresponds to sentence “Is the screen clear?”",1 Introduction,[0],[0]
in Question 1.,1 Introduction,[0],[0]
"Therefore, when the question text and answer text are presented as two units in a sequence, it is rather difficult to capture the relationship between the question and its corresponding answer due to the possible long distance between them.",1 Introduction,[0],[0]
"On the other hand, there often exists both positive and negative sentiments in answer text according to different parts of question, and this specific case should be categorized as another category named conflict.",1 Introduction,[0],[0]
"For instance, in Figure 1, Answer 1",1 Introduction,[0],[0]
“It’s a nice phone with high-quality screen.,1 Introduction,[0],[0]
But the battery is not durable.”,1 Introduction,[0],[0]
is a conflict answer to Question 1.,1 Introduction,[0],[0]
"However, when this answer text is considered as a sequence, it is highly possible to be predicted as the category of positive or negative rather than conflict.",1 Introduction,[0],[0]
"In order to address these problems, a more appropriate approach is to segment both the question and answer text into some parallel sentences, and then construct the [Q-sentence, A-sentence] units in each QA text pair to detect in-depth sentiment information.
",1 Introduction,[0],[0]
"Second, although the main sentiment polarity is usually expressed from the answer text, the question text could also carry important sentiment tips to predict the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
"For instance, in Figure 1, we could hardly estimate the sentiment polarity solely based on Answer 2.",1 Introduction,[0],[0]
"However, when we take Question 2, “Is the sun cream really effective?”, into consideration, it can be easier to label this QA text pair with a negative tag.",1 Introduction,[0],[0]
"In this study, we propose an approach to match the sentences inside the question and answer text bidirectionally.
",1 Introduction,[0],[0]
"Third, in each QA text pair, the importance degrees of different [Q-sentence, A-sentence] units can be different.",1 Introduction,[0],[0]
"For instance, in Figure 1, the [Qsentence, A-sentence] unit, i.e., sentence “Summer is coming, I’m afraid of getting darker.”",1 Introduction,[0],[0]
"in Answer 2 and sentence “No, just depending on my own experience.”",1 Introduction,[0],[0]
"in Question 2, makes tiny contribution to imply the sentiment polarity for the
QA text pair.",1 Introduction,[0],[0]
"Therefore, a well-behaved network approach should consider the importance degrees of different [Q-sentence, A-sentence] units for predicting the sentiment polarity of a QA text pair.
",1 Introduction,[0],[0]
The contribution of this paper is twofold.,1 Introduction,[0],[0]
"First, we propose a novel problem, QA-style sentiment analysis, and build a large-scale annotated corpus tailed for this task.",1 Introduction,[0],[0]
The dataset is released to motivate future investigations for this track of research.,1 Introduction,[0],[0]
"Second, we propose a hierarchical matching network model to address the challenges of QA-style sentiment classification.",1 Introduction,[0],[0]
"Specifically, we first segment both the question and answer text into sentences and construct the [Q-sentence, A-sentence] units for each QA text pair.",1 Introduction,[0],[0]
"Then, by using a QA bidirectional matching layer, we encode each [Q-sentence, A-sentence] unit for exploring sentiment information.",1 Introduction,[0],[0]
"Finally, the self-matching attention layer in the model can capture the importance of these [Q-sentence, A-sentence] matching vectors obtained from QA bidirectional matching layer, which could effectively refine the evidence for inferring the sentiment polarity of a QA text pair.",1 Introduction,[0],[0]
Experimental results show that the proposed approach significantly outperforms several strong baselines for QA-style sentiment classification.,1 Introduction,[0],[0]
Sentiment classification has become a hot research field in NLP since the pioneering work by Pang et al. (2002).,2 Related Work,[0],[0]
"In general, the research on traditional sentiment classification has been carried out in different text levels, such like word-level, documentlevel and aspect-level.
",2 Related Work,[0],[0]
Word-level sentiment classification has been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
Some early studies have devoted their efforts to predicting the sentiment polarity of a word with different learning models and resources.,2 Related Work,[0],[0]
Turney (2002) proposed an approach to predicting the sentiment polarity of words by calculating Pointwise Mutual Information (PMI) values between the seed words and the search hits.,2 Related Work,[0],[0]
"Hassan and Radev (2010) and Hassan et al. (2011) applied a Markov random walk model to determine the word polarities with a large word relatedness graph, and the synonyms and hypernyms in WordNet (Miller, 1995).",2 Related Work,[0],[0]
"More recently, some studies aim to learn better word embedding of a word rather than its polarity.",2 Related Work,[0],[0]
"Tang et al. (2014) developed three neural networks to learn word em-
bedding by incorporating sentiment polarities of text in loss functions.",2 Related Work,[0],[0]
"Zhou et al. (2015b) employed both unsupervised and supervised neural networks to learn bilingual sentiment word embedding.
",2 Related Work,[0],[0]
Document-level sentiment classification has also been studied in a long period in the research community of sentiment analysis.,2 Related Work,[0],[0]
"On one hand, many early studies have been devoted their efforts to various of aspects on learning approaches, such as supervised learning (Pang et al., 2002; Riloff et al., 2006), semi-supervised learning (Li et al., 2010; Xia et al., 2015; Li et al., 2015), and domain adaptation (Blitzer et al., 2007; He et al., 2011).",2 Related Work,[0],[0]
"On the other hand, many recent studies employ deep learning approaches to enhance the performances in sentiment classification.",2 Related Work,[0],[0]
Tang et al. (2015) proposed a user-product neural network to incorporate both user and product information for sentiment classification.,2 Related Work,[0],[0]
Xu et al. (2016) proposed a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts.,2 Related Work,[0],[0]
"More recently, Long et al. (2017) proposed a novel attention model, namely cognition-based attention, for sentiment classification.
",2 Related Work,[0],[0]
Aspect-level sentiment classification is a relatively new research area in the research community of sentiment analysis and it is a fine-grained classification task.,2 Related Work,[0],[0]
"Recently, Wang et al. (2016) proposed an attention-based LSTM neural network to aspect-level sentiment classification by exploring the connection between an aspect and the content of a sentence.",2 Related Work,[0],[0]
Tang et al. (2016) proposed a deep memory network with multiple attention-based computational layers to improve the performance.,2 Related Work,[0],[0]
"Wang et al. (2018) proposed a hierarchical attention network to explore both word-level and clause-level sentiment information towards a target aspect.
",2 Related Work,[0],[0]
"Unlike all the prior studies, this paper focuses on a very different kind of text representation, i.e., QA-style text level, for sentiment classification.",2 Related Work,[0],[0]
"To the best of our knowledge, this is the first attempt to perform sentiment classification on this text level.",2 Related Work,[0],[0]
"We collect QA text pairs from “Asking All” in Taobao (Alibaba)1, which is the world’s biggest ecommerce company.",3 Data Collection and Annotation,[0],[0]
"The QA text pairs are mainly from Beauty, Shoe and Electronic domains and each domain contains 10,000 QA text pairs.
",3 Data Collection and Annotation,[0],[0]
"We define four sentiment-related categories, i.e., positive, negative, conflict (both positive and negative sentiment) and neutral (neither positive nor negative sentiment).",3 Data Collection and Annotation,[0],[0]
"To guarantee a high annotation agreement, we propose some annotation guidelines after several times of annotation processes on a small size of data.",3 Data Collection and Annotation,[0],[0]
"Then, we ask more coders to annotate the whole data set according to these annotation guidelines.
",3 Data Collection and Annotation,[0],[0]
The annotation guidelines contain two main groups.,3 Data Collection and Annotation,[0],[0]
"One contains the guidelines which aim to distinguish the categories of neutral and nonneutral, i.e., (a) A QA text pair in which the question and the answer do not match is annotated as a neutral sample.",3 Data Collection and Annotation,[0],[0]
"In this type of samples, the answer does not reply to the question correctly.",3 Data Collection and Annotation,[0],[0]
"E1 is an example of this type where the question talks about the screen while the answer talks about the battery.
",3 Data Collection and Annotation,[0],[0]
E1: Q: Is the screen clear?,3 Data Collection and Annotation,[0],[0]
"A: The battery life is decent.
",3 Data Collection and Annotation,[0],[0]
(b) A QA text pair with an unknown or uncertain answer is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E2 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E2: Q: What about these sneakers?,3 Data Collection and Annotation,[0],[0]
"A: I don’t know, I bought it for my dad.
",3 Data Collection and Annotation,[0],[0]
(c),3 Data Collection and Annotation,[0],[0]
A QA text pair with only objective description is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"E3 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E3: Q: What’s the operation system of the phone?,3 Data Collection and Annotation,[0],[0]
"A: Android.
",3 Data Collection and Annotation,[0],[0]
(d),3 Data Collection and Annotation,[0],[0]
A QA text pair which compares two different products is annotated as a neutral sample.,3 Data Collection and Annotation,[0],[0]
"In this type of samples, two products are involved and it
1https://www.taobao.com/
is sometimes difficult to tell the sentiment orientation of one product.",3 Data Collection and Annotation,[0],[0]
"E4 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E4: Q: How about this phone when compared to iPhone 6s?,3 Data Collection and Annotation,[0],[0]
"A: It’s up to you, and they’re not comparable.
",3 Data Collection and Annotation,[0],[0]
"The other group contains the guidelines which aim to distinguish the categories of positive and negative, i.e., (e) If the answer text contains sentimental expressions to question like “disappointed”, “terrible”, and so on, we annotate it as negative.",3 Data Collection and Annotation,[0],[0]
"E5 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E5: Q: How is the rock climbing shoe?,3 Data Collection and Annotation,[0],[0]
"A: I am so disappointed, my feet felt hurt when I wore them.
",3 Data Collection and Annotation,[0],[0]
(f),3 Data Collection and Annotation,[0],[0]
"If the answer text contains sentimental expressions to question like “perfect”, “satisfied”, and so on, we annotate it as positive.",3 Data Collection and Annotation,[0],[0]
"E6 is an example of this type.
",3 Data Collection and Annotation,[0],[0]
E6: Q: How about the fragrance?,3 Data Collection and Annotation,[0],[0]
"A: I am so satisfied, it smells distinctive.
",3 Data Collection and Annotation,[0],[0]
"(g) If we cannot confirm the polarity of a QA text pair only depending on answer text, we annotate the polarity according to both the question and answer text.",3 Data Collection and Annotation,[0],[0]
"For instance, E7 is an example with positive polarity, while E8 is an example with negative polarity.
",3 Data Collection and Annotation,[0],[0]
E7: Q: Will the phone get hot when gaming?,3 Data Collection and Annotation,[0],[0]
"A: No.
E8: Q: Is the sun cream really economic?",3 Data Collection and Annotation,[0],[0]
"A: No.
We assign two annotators to annotate each QA text pair, and the Kappa consistency check value of the annotation is 0.84.",3 Data Collection and Annotation,[0],[0]
"When annotators cannot reach an agreement, an expert will make the final decision, ensuring the quality of data annotation.",3 Data Collection and Annotation,[0],[0]
Table 1 shows the category distribution of the corpus.,3 Data Collection and Annotation,[0],[0]
"To motivate other scholars to investigate this novel but important task, we share the data via Github2.",3 Data Collection and Annotation,[0],[0]
"In this section, we introduce the proposed hierarchical matching network approach for QAstyle sentiment classification.",4 Methodology,[0],[0]
Figure 2 depicts the overview of the proposed approach.,4 Methodology,[0],[0]
Word Encoding Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After sentence segmentation, the question text in a QA text pair contains N sentences, SQi represents the i-th sentence in the question text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Similarly, the answer text in this QA text pair contains M sentences, SAj represents the j-th sentence in the answer text.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then construct [Q-sentence, A-sentence] units by pairing one sentence in the question text and one sentence in the answer text, and we obtain N*M [Q-sentence, A-sentence] units at last.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given a [SQi , SAj ] unit in this QA text pair, i.e., Q-sentence SQi with words wi,n, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈
2https://github.com/clshenNLP/QASC/
[1, Ni] and A-sentence SAj with words wj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ], we first convert the words to their respective word embeddings (xi,n ∈ Rd, i ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, N ], n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1, Ni] and xj,m, j ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,M ],m ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[1,Mj ]).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"We then use Bi-directional LSTM (namely Bi-LSTM), which can efficiently make use of past features (via forward states) and future features (via backward states) for a specific time step, to get contextual representations of SQi and SAj individually.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
The representation of each word is formed by concatenating the forward and backward hidden states.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For simplicity, we note contextual representation of SQi asHQi , and contextual representation of SAj as HAj respectively:
HQi =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hi,1, hi,2, ..., hi,n, ..., hi,Ni ] (1)
HAj =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[hj,1, hj,2, ..., hj,m, ..., hj,Mj ] (2)
where hi,n ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Rd ′
denotes the word representation in SQi at time step n, hj,m ∈ Rd ′ denotes the word representation in SAj at time step m, and d ′ is the dimensionality of word representation.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
QA Bidirectional Matching Layer:,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"General neural network could not capture sentiment matching information in a [SQi , SAj ] unit well.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"For the sake of solving this problem, we introduce the QA bidirectional matching layer to encapsulate the clues and interactions between SQi and SAj synchronously (Tay et al., 2017; McCann et al., 2017).",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Figure 3 depicts the detail architecture of QA bidirectional matching mechanism.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Specifically, we first calculate the bidirectional pair-wise matching matrix by using the fol-
lowing formula:
D[i,j] = (HQi)",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"> · (HAj ) (3)
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"where D[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"RNi×Mj denotes the bidirectional matching matrix for the [SQi , SAj ] unit.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Each element in D[i,j] is the score that measures how well the word in SQi semantically matches the word in SAj and vice versa.
",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Given the bidirectional matching matrix D[i,j], we use attention mechanism (Yang et al., 2016; Cui et al., 2017) to mine the sentiment matching information between question and answer from two directions, which could be seen as an Answerto-Question attention and a Question-to-Answer attention as follows.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
•,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Answer-to-Question Attention: We employ row-wise operations to compute the attention weight vector αr[i,j] as follows:
U r[i,j] = tanh(Wr ·D >",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j]) (4)
αr[i,j] = softmax(w > r · U r[i,j]) (5)
where αr[i,j] ∈ R Ni is the Answer-to-Question attention weight vector regarding the importance degrees of all words in Q-sentence SQi , Wr ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
Rd′×Mj and wr ∈ Rd ′ are weight matrices.,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After computing the Answer-to-Question attention weight vector, we can get the Answer-to-Question matching vector V r[i,j] ∈ R d′ as follows:
V r[i,j] = (HQi) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α r,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (6)
• Question-to-Answer Attention: Simultaneously, we employ column-wise operations to calculate the attention weight vector αc[i,j] as follows:
U c[i,j] = tanh(Wc ·D[i,j]) (7) αc[i,j] =",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"softmax(w > c · U c[i,j]) (8)
where αc[i,j] ∈",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"R Mj is the Question-to-Answer attention weight vector regarding the importance degrees of all words in A-sentence SAj , Wc ∈ Rd′×Ni and wc ∈ Rd ′ are weight matrices.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"After calculating the Question-to-Answer attention weight vector, we can get the Question-to-Answer matching vector V c[i,j] ∈ R d′ as follows:
V c[i,j] = (HAj ) ·",4.1 QA Bidirectional Matching Mechanism,[0],[0]
α c,4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (9)
Then, we combine Answer-to-Question and Question-to-Answer matching vectors to represent
the final bidirectional matching vector of the [SQi , SAj ] unit:
V[i,j] = V r",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] ⊕ V c",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"[i,j] (10)
where ⊕ denotes the concatenate operator, and V[i,j] denotes the bidirectional matching vector which integrates SQi and SAj with each other.",4.1 QA Bidirectional Matching Mechanism,[0],[0]
"Through the QA bidirectional matching layer, informative bidirectional matching vectors are generated to pinpoint the sentiment matching information in each [Q-sentence, A-sentence] unit.",4.2 Self-Matching Attention Mechanism,[0],[0]
"Intuitively, each matching vector for [Q-sentence, Asentence] unit holds different importance to a QA text pair.",4.2 Self-Matching Attention Mechanism,[0],[0]
"To better aggregate the evidence from these vectors for inferring the sentiment polarity of the QA text pair, we propose a self-matching attention layer, matching these informative vectors against themselves.",4.2 Self-Matching Attention Mechanism,[0],[0]
Self-Matching Attention Layer:,4.2 Self-Matching Attention Mechanism,[0],[0]
"As aforementioned, we have obtained N*M bidirectional matching vectors through QA bidirectional matching layer, then we calculate the attention weight vector α with these matching vectors by following formulas:
V =",4.2 Self-Matching Attention Mechanism,[0],[0]
"[V[1,1], V[1,2], ..., V[i,j], ..., V[N,M ]] (11)
",4.2 Self-Matching Attention Mechanism,[0],[0]
U = tanh(Wh · V ) (12) α =,4.2 Self-Matching Attention Mechanism,[0],[0]
"softmax(w>h · U) (13)
where α is the attention weight vector which measures the importance of these matching vectors, Wh and wh are the weight matrices.
",4.2 Self-Matching Attention Mechanism,[0],[0]
"Finally, we can get the QA text pair representation R as follows:
R = V · α (14)",4.2 Self-Matching Attention Mechanism,[0],[0]
QA text pair representationR is a high level representation which can be used for classification.,4.3 Classification Model,[0],[0]
"In our approach, we feed R to a softmax classifier:
p = softmax(Wl",4.3 Classification Model,[0],[0]
"·R+ bl) (15)
where p is a set of predicted distribution of the sentiment categories, i.e., positive, negative, neutral, and conflict.",4.3 Classification Model,[0],[0]
"Wl is the weight matrix and bl is the bias.
",4.3 Classification Model,[0],[0]
"To learn the whole model, we train an end-toend model given the training data, and the goal of
training is to minimize the cross-entropy loss, i.e.,
L(θ) =",4.3 Classification Model,[0],[0]
"− S∑
s=1 K∑ k=1",4.3 Classification Model,[0],[0]
yks ·,4.3 Classification Model,[0],[0]
"logŷks + λ‖θ‖ 2 2 (16)
where S is the number of training data.",4.3 Classification Model,[0],[0]
ys is the true sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
ŷs is the predicted sentiment label of the s-th sample.,4.3 Classification Model,[0],[0]
K is number of all sentiment categories.,4.3 Classification Model,[0],[0]
"λ is a L2regularization term, θ is the parameter set.",4.3 Classification Model,[0],[0]
"In the above equation, the model parameters are optimized by using Adam (Kingma and Ba, 2014).",4.3 Classification Model,[0],[0]
"In this section, we evaluate the performances of the proposed approach for QA-style sentiment classification.",5 Experimentation,[0],[0]
"• Data Sets: As introduced in Section 3, the annotated QA text pairs cover three different domains.",5.1 Experimental Settings,[0],[0]
"In each domain, we randomly split the data into a training set (80% in each category) and a test set (20% in each category).",5.1 Experimental Settings,[0],[0]
"In addition, we set aside 10% from the training set as the development data for parameters tuning.",5.1 Experimental Settings,[0],[0]
"• Word Segmentation and Embeddings: FudanNLP3 (Qiu et al., 2013) is employed to segment text into Chinese words and word2vec4 (Mikolov et al., 2013) is employed to pre-train word embeddings.",5.1 Experimental Settings,[0],[0]
The vector dimensionality is set to be 100.,5.1 Experimental Settings,[0],[0]
"• Sentence Segmentation: CoreNLP5 (Manning et al., 2014) is employed to segment both the question and answer text into sentences.",5.1 Experimental Settings,[0],[0]
"• Hyper-parameters: In the experiment, all outof-vocabulary words are initialized by sampling from the uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
"All weight matrices are given their initial values by sampling from uniform distribution U(−0.01, 0.01).",5.1 Experimental Settings,[0],[0]
The LSTM hidden states are set to be 128 and all models are trained by mini-batch of 32 instances.,5.1 Experimental Settings,[0],[0]
The dropout rate is set to 0.2.,5.1 Experimental Settings,[0],[0]
The other hyper-parameters are tuned according to the development data.,5.1 Experimental Settings,[0],[0]
•,5.1 Experimental Settings,[0],[0]
"Evaluation Metric: The performance is evaluated using standard Accuracy and Macro-F1.
3https://github.com/FudanNLP/fnlp/ 4https://code.google.com/archive/p/word2vec/ 5http://stanfordnlp.github.io/CoreNLP/",5.1 Experimental Settings,[0],[0]
The following baseline approaches are employed for comparison.,5.2 Experimental Results,[0],[0]
Note that all the approaches share the same word embeddings for fair comparison.,5.2 Experimental Results,[0],[0]
• SVM:,5.2 Experimental Results,[0],[0]
This baseline employs support vector machine along with word embedding features.,5.2 Experimental Results,[0],[0]
The question and answer text in a QA text pair are chained as a sequence.,5.2 Experimental Results,[0],[0]
• LSTM:,5.2 Experimental Results,[0],[0]
A standard LSTM model utilizes word embeddings and concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bi-LSTM:,5.2 Experimental Results,[0],[0]
A bidirectional LSTM model which concatenates the question and answer text as a sequence.,5.2 Experimental Results,[0],[0]
• Bidirectional-Match:,5.2 Experimental Results,[0],[0]
"This approach employs QA bidirectional matching mechanism, without taking the sentence segmentation strategy and selfmatching attention mechanism.",5.2 Experimental Results,[0],[0]
• AtoQ-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Answer-to-Question attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
We average the Answer-to-Question matching vectors to represent the QA text pair.,5.2 Experimental Results,[0],[0]
•,5.2 Experimental Results,[0],[0]
QtoA-Match:,5.2 Experimental Results,[0],[0]
"This approach takes the sentence segmentation strategy, and employs QA unidirectional matching mechanism (i.e., only using Question-to-Answer attention), but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
"• Bidirectional-Match QA: This approach takes the sentence segmentation strategy, and employs QA bidirectional matching mechanism, but does not employ self-matching attention mechanism.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"This is our hierarchical matching network model which takes the sentence segmentation strategy and employs both QA bidirectional matching mechanism and self-matching attention mechanism.
",5.2 Experimental Results,[0],[0]
"Table 2 summarizes the experimental results of all the approaches above, and we can find that:
(1) All LSTM-based approaches are superior to SVM, indicating the effectiveness of neural network for this task.",5.2 Experimental Results,[0],[0]
"(2) The proposed approaches, with novel QA contextual representation, outperform the other baseline approaches.",5.2 Experimental Results,[0],[0]
"(3) When only employing QA bidirectional matching mechanism, Bidirectional-Match QA, which takes the sentence segmentation strategy, consistently outperforms Bidirectional-Match (without sentence segmentation) in all domains.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that sentence segmentation helps to extract the sentiment matching information between the question and answer.,5.2 Experimental Results,[0],[0]
"(4) When comparing to QA unidirectional matching mechanism, Bidirectional-Match QA, which employs QA bidirectional matching mechanism, performs better than AtoQMatch and QtoA-Match.",5.2 Experimental Results,[0],[0]
It confirms our hypothesis that both the question and answer information contribute to sentiment polarity of the QA text pair.,5.2 Experimental Results,[0],[0]
"(5) Impressively, the proposed approach HMN significantly outperforms all the other approaches in all domains (p-value<0.05 via ttest).",5.2 Experimental Results,[0],[0]
"It verifies the advantages of both QA bidirectional matching mechanism and selfmatching attention mechanism for this task.
",5.2 Experimental Results,[0],[0]
"Besides, we also implement some more recent state-of-the-art approaches for sentiment classification, which are illustrated in Table 3.",5.2 Experimental Results,[0],[0]
"This result also supports the earlier findings.
",5.2 Experimental Results,[0],[0]
"• CNN-Tensor (Lei et al., 2015):",5.2 Experimental Results,[0],[0]
"This is a stateof-the-art approach to sentence-level sentiment classification, which models n-gram interactions based on tensor product and evaluates all non-
consecutive n-gram vectors as a feature mapping operator for CNNs.",5.2 Experimental Results,[0],[0]
"• Attention-LSTM (Wang et al., 2016):",5.2 Experimental Results,[0],[0]
This is a state-of-the-art approach to aspect-level sentiment classification.,5.2 Experimental Results,[0],[0]
"In our implementation, we ignore the aspect embedding and directly use the outputs of LSTM to yield the attention.",5.2 Experimental Results,[0],[0]
• BiMPM,5.2 Experimental Results,[0],[0]
"(Wang et al., 2017):",5.2 Experimental Results,[0],[0]
"This is a state-ofthe-art approach to QA matching, which matches the question and answer from multiple perspectives.",5.2 Experimental Results,[0],[0]
"In our implementation, we use the matching representation to perform QA-style sentiment classification with a softmax classifier.",5.2 Experimental Results,[0],[0]
• HMN:,5.2 Experimental Results,[0],[0]
"The proposed hierarchical matching network which employs both QA bidirectional matching mechanism and self-matching attention mechanism, and takes the sentence segmentation strategy.
",5.2 Experimental Results,[0],[0]
Table 3 shows the comparison results of these strong baseline approaches and the proposed approach (HMN) in all domains.,5.2 Experimental Results,[0],[0]
"From this table, we can find that: (1) the approaches that take matching strategy, i.e., BiMPM and our approach (HMN), outperform other approaches.",5.2 Experimental Results,[0],[0]
"(2) The proposed approach (HMN) significantly outperforms all the other baseline approaches in terms of both Macro-F1 and Accuracy (p-value<0.05 via ttest), which confirms the initial hypotheses of this study.",5.2 Experimental Results,[0],[0]
"Table 4 shows some examples, along with the predicted categories via different approaches.",5.3 Case Study,[0],[0]
"We can find that: (1) the approaches based on matching strategy (BiMPM and HMN) are well-performed, as shown in E9, when question and answer carrying different kinds of information.",5.3 Case Study,[0],[0]
"This is a unique challenge for QA-style sentiment mining, and traditional sentiment classification approaches can hardly address this problem.",5.3 Case Study,[0],[0]
"(2) The proposed approach (HMN) performs better than other approaches when dealing with conflict instances, as shown in E10.",5.3 Case Study,[0],[0]
"To get a better understanding of our proposed hierarchical matching network for QA-style sentiment classification, we picture the attention weights obtained from Equations (5), (8) and (13).",5.4 Visualization of Attention,[0],[0]
"For
simplicity, we directly use the English translation of E11 for illustration and adopt the visualization approach presented by Yang et al. (2016), as shown in Figure 2.",5.4 Visualization of Attention,[0],[0]
"Specifically, each line is a [Qsentence, A-sentence] unit, where the red denotes the [Q-sentence, A-sentence] unit weight, the blue denotes the word weight in each [Q-sentence, Asentence], and the color depth indicates the importance of attention weights (the darker the more important).
",5.4 Visualization of Attention,[0],[0]
"From Figure 4, we can see that the QA bidirectional matching layer always assigns reasonable attention weights to words in each [Q-sentence, Asentence] unit which makes sentence from question and sentence from answer match correctly.",5.4 Visualization of Attention,[0],[0]
"In addition, the self-matching attention layer is able to select informative",5.4 Visualization of Attention,[0],[0]
"[Q-sentence, A-sentence] unit for predicting true sentiment polarity of this example.",5.4 Visualization of Attention,[0],[0]
"In this paper, we propose a novel but important sentiment analysis task, i.e., QA-style sentiment mining, and we build a large-scale highquality human annotated corpus for experiment.",6 Conclusion,[0],[0]
The dataset is shared to encourage other scholars to investigate this interesting problem.,6 Conclusion,[0],[0]
"Moreover, we propose a hierarchical matching neural network model to enable QA bidirectional matching mechanism and self-matching attention mechanism for this task.",6 Conclusion,[0],[0]
"Empirical studies show that the proposed approach significantly outperforms other strong baseline approaches in all the test domains for QA-style sentiment classification.
",6 Conclusion,[0],[0]
"In the future, we would like to investigate some other network structures to explore deeper information in each QA text pair.",6 Conclusion,[0],[0]
"Besides, we would like to test the effectiveness of the proposed approach to QA-style sentiment classification in some other languages.",6 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
"This work is partially supported by the National Key R&D Program of China under Grant No.2017YFB1002101 and two NSFC grants No.61331011, No.61672366.",Acknowledgments,[0],[0]
This work is also supported by the joint research project of Alibaba Group and Soochow University.,Acknowledgments,[0],[0]
"In an e-commerce environment, user-oriented question-answering (QA) text pair could carry rich sentiment information.",abstractText,[0],[0]
"In this study, we propose a novel task/method to address QA sentiment analysis.",abstractText,[0],[0]
"In particular, we create a high-quality annotated corpus with speciallydesigned annotation guidelines for QA-style sentiment classification.",abstractText,[0],[0]
"On the basis, we propose a three-stage hierarchical matching network to explore deep sentiment information in a QA text pair.",abstractText,[0],[0]
"First, we segment both the question and answer text into sentences and construct a number of [Q-sentence, Asentence] units in each QA text pair.",abstractText,[0],[0]
"Then, by leveraging a QA bidirectional matching layer, the proposed approach can learn the matching vectors of each [Q-sentence, A-sentence] unit.",abstractText,[0],[0]
"Finally, we characterize the importance of the generated matching vectors via a selfmatching attention layer.",abstractText,[0],[0]
"Experimental results, comparing with a number of state-ofthe-art baselines, demonstrate the impressive effectiveness of the proposed approach for QA-style sentiment classification.",abstractText,[0],[0]
Sentiment Classification towards Question-Answering with Hierarchical Matching Network,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2860–2865 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
NLP research relies heavily on annotated datasets for training and evaluation.,1 Introduction,[0],[0]
"The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.
",1 Introduction,[0],[0]
"When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators (Hovy et al., 2006), methods for aggregating annotations (Passonneau and Carpenter, 2014), and intuitive user interfaces (Stenetorp et al., 2012).
",1 Introduction,[0],[0]
"Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al.,
2008; Callison-Burch and Dredze, 2010; Graham et al., 2017), although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (CallisonBurch and Dredze, 2010; Hoßfeld et al., 2014).",1 Introduction,[0],[0]
"In most annotation exercises, the order of presentation of instances is randomised to remove bias due to similarities in topic, style and vocabulary (Koehn and Monz, 2006; Bojar et al., 2016).
",1 Introduction,[0],[0]
"When crowdsourcing judgements, the normal practise (as used in the datasets we analyse) is for the item ordering to be randomised in creating a “HIT” (i.e. a single collection of items presented to a crowdworker for judgement), and then to have each HIT annotated by multiple workers, for quality control purposes.",1 Introduction,[0],[0]
"The order of items is generally fixed across all annotators of an individual HIT (Snow et al., 2008; Graham et al., 2017).
",1 Introduction,[0],[0]
"In this paper, we show that worker scores are affected by sequence bias, whereby the order of presentation can affect individuals’ assessment of an item.",1 Introduction,[0],[0]
"Since all workers see the instances in the same order, this affects any other inferences made from the data, including aggregated assessment or inferences about individual annotators (such as their overall quality or individual thresholds).
",1 Introduction,[0],[0]
"Possible explanations for sequence effects include:
Gambler’s fallacy: Once annotators have developed an idea of the distribution of scores/labels, they can come to expect even small sequences to follow the distribution.",1 Introduction,[0],[0]
"In particular, in binary annotation tasks, if they expect that True (1) and False (0) items are equally likely, then they believe the sequence 00000 (100% False and 0% True) is less likely than the sequence 01010 (50% False and 50% True).",1 Introduction,[0],[0]
"So if they assign 0 to an item,
2860
they may approach the next item with a prior belief that it is more likely to be a 1 than a 0.",1 Introduction,[0],[0]
"Chen et al. (2016) showed evidence for the gambler’s fallacy in decisions of loan officers, asylum judges, and baseball umpires.
",1 Introduction,[0],[0]
Sequential contrast effects: A high quality item may raise the bar for the next item.,1 Introduction,[0],[0]
"On the other hand, a bad item may make the next item seem better in comparison (Kenrick and Gutierres, 1980; Hartzmark and Shue, to appear)
",1 Introduction,[0],[0]
"Assimilation and anchoring: The annotator uses their score of the previous item as an anchor, and adjusts the score of the current item from this anchor, based on perceived similarities and differences with the previous item.",1 Introduction,[0],[0]
"If they focus on similarities between the previous and current instance, the annotations show an assimilation effect (Geiselman et al., 1984; Damisch et al., 2006).",1 Introduction,[0],[0]
"Anchoring effects may decrease as people gain experience and expertise in the task (Wilson et al., 1996).",1 Introduction,[0],[0]
"We test whether the annotation of an instance is correlated with the annotation on previous instances, conditioned on control variables such as the gold standard (i.e. expert annotations1), based on the following linear model:
Yi,t = β0 + β1Yi,t−1 + β2Gold + η (1)
where Yi,t is the annotation given by an annotator i to an instance t, and η is white Gaussian noise with zero mean.",2 Methodology,[0],[0]
We use linear regression for continuous data and logistic regression for binary data.2,2 Methodology,[0],[0]
"If there is no dependence between consecutive instances, and annotators assign labels/scores based only on the aspects of the current instance, then the data can be explained from the gold score (learning a positive β2 value) and bias term (β0), with β1 set to zero.",2 Methodology,[0],[0]
"When we use the ground truth as a control, if β1 is non-zero, it is evidence of mistakes being made by annotators due to sequential bias.",2 Methodology,[0],[0]
"A positive value of β1 can be explained by priming or anchoring, and a negative value with sequential contrast effects or the gambler’s fallacy.",2 Methodology,[0],[0]
"Accordingly, we test the statistical significance of
1For the Machine Translation dataset described in Section 3.3, we use the mean of at least fifteen crowd workers as a proxy for expert annotations.
",2 Methodology,[0.9583979208553386],"['Secondly, we introduce SSBVARS as the first class of models for multivariate inference within BOCPD.']"
"2η is not included in the case of logistic regression
the β1 6= 0",2 Methodology,[0],[0]
to determine whether sequencing effects are present in crowdsourced text corpora.,2 Methodology,[0],[0]
"We analyse several influential datasets that have been constructed through crowdsourcing, including both binary and continuous annotation tasks: recognising textual entailment, event ordering, affective text analysis, and machine translation evaluation.",3 Experiments,[0],[0]
"First, we examine the recognising textual entailment (“RTE”) and event temporal ordering (“TEMPORAL”) datasets from Snow et al. (2008).",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the RTE task, annotators are presented with two sentences, and are asked to judge whether the second text can be inferred from the first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"With the TEMPORAL dataset, they are shown two sentences describing events, and asked to indicate which of the two events occurred first.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
Both datasets include both expert annotations and crowdsourced annotations constructed using Amazon Mechanical Turk (“MTurk”).,3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"On MTurk, each RTE HIT contains 20 instances, and each TEMPORAL HIT contains 10 instances, which the workers see in sequential order.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"For both tasks, each HIT was annotated by 10 workers.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"Results We use logistic regression on worker labels against labels on the previous instance in the current HIT, with the expert judgements as a control variable.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also add an additional control, namely the percentage of True labels assigned by the worker overall, which accounts for the overall annotator bias.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"To calculate this, we use scores by the worker excluding the current score, to avoid giving the model any information about the current instance.
",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"As shown in Table 1, over all workers (“All”), we find a small negative autocorrelation for both the RTE and TEMPORAL tasks.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"One possibility
is that this is biased by opportunistic workers who assign the same label to all instances in the HIT, for which we would not expect any sequential bias effects.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"When we exclude these workers (“Moderate”), the autocorrelation increases, and is highly statistically significant.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"We also show results for workers with at least 60% accuracy when compared to expert annotations (“Good”), and observe a similar effect.",3.1 Recognising Textual Entailment (RTE) and Event Temporal Ordering,[0],[0]
"In the affective text analysis task (“AFFECTIVE”), annotators are asked to rate news headlines for anger, disgust, fear, joy, sadness, and surprise on a continuous scale of 0–100.",3.2 Affective text analysis,[0],[0]
"Besides these emotions, they are asked to rate sentences for (emotive) valence, i.e., how strongly negative or positive they are (−100 to +100).",3.2 Affective text analysis,[0],[0]
"In this dataset, there are 100 headlines divided into 10 HITs, with 10 workers annotating each HIT (Snow et al., 2008).",3.2 Affective text analysis,[0],[0]
"We test for autocorrelation of scores of each aspect individually, controlling for the expert scores and worker correlation with the expert scores.",3.2 Affective text analysis,[0],[0]
"We also look separately at datasets of good and bad workers, based on whether the correlation with the expert annotations is greater than 0.5.
",3.2 Affective text analysis,[0],[0]
"Results For individual emotions, we do not observe any significant autocorrelation (p ≥ 0.05).",3.2 Affective text analysis,[0],[0]
"As there are only 1000 annotations per emotion, we also look at results when combining data for all aspects.",3.2 Affective text analysis,[0],[0]
"Though we find a statistically significant negative autocorrelation for scores of the full dataset, this disappears when we filter out bad workers (Table 2).",3.2 Affective text analysis,[0],[0]
"Given the difficulty of this very subjective task, it is likely that many of workers considered ‘bad’ might have simply found this task too difficult or arbitrary, and thus become more prone to sequence effects.",3.2 Affective text analysis,[0],[0]
"When evaluating machine translation (“MT”), we tend to focus on adequacy: the extent to which the meaning of the reference translation is captured in the MT output.",3.3 Machine Translation Adequacy,[0],[0]
"In the method of Graham et al. (2015) — the current best-practise, as adopted by WMT (Bojar et al., 2016) — annotators are asked to judge the adequacy of translations using a 100- point sliding scale which is initialised at the mid point.",3.3 Machine Translation Adequacy,[0],[0]
There are 3 marks on the scale dividing it into 4 quarters to aid workers with internal calibration.,3.3 Machine Translation Adequacy,[0],[0]
"They are given no other instructions or
guidelines.",3.3 Machine Translation Adequacy,[0],[0]
"In this paper, we base our analysis on the adequacy dataset of Graham et al. (2015), on SpanishEnglish newswire data from WMT 2013 (Bojar et al., 2013).",3.3 Machine Translation Adequacy,[0],[0]
"The dataset consists of 12 HITS of 100 sentence pairs each; each HIT is annotated by at least 15 workers.
",3.3 Machine Translation Adequacy,[0],[0]
HITs are designed to include quality control items to filter out poor quality scores.,3.3 Machine Translation Adequacy,[0],[0]
"In addition to 70 MT system translations, each HIT contains degraded versions of 10 of these translations, 10 reference translations by a human expert corresponding to 10 of these translations, and repeats of another 10 translations.",3.3 Machine Translation Adequacy,[0],[0]
"Good workers are assumed to give high scores to the references, similar scores to the pair of repeats, and high scores to the MT system translations when compared to corresponding degraded translations.",3.3 Machine Translation Adequacy,[0],[0]
Workers who submitted scores of clearly bad quality were rejected.,3.3 Machine Translation Adequacy,[0],[0]
"For the remaining workers, the Wilcoxon rank-sum test is used to test whether the score difference between the repeat judgements is less than the score difference between translations and the corresponding degraded versions.",3.3 Machine Translation Adequacy,[0],[0]
"We divide these workers into “good” and “moderate” based on the threshold of p < 0.05.
",3.3 Machine Translation Adequacy,[0],[0]
"To eliminate differences due to different internal scales, every individual worker’s scores are standardised by subtracting the mean and dividing by the standard deviation of their scores.",3.3 Machine Translation Adequacy,[0],[0]
"Following Graham et al. (2015), we use the average of standardised scores of at least 15 good workers as the ground truth.
",3.3 Machine Translation Adequacy,[0],[0]
"We refer to the final dataset as “MTadeq”.
Results As this is a (practically) continuous output, we use a linear regression model, whereby the current score is predicted based on the previous score, with the mean of all worker scores as control.",3.3 Machine Translation Adequacy,[0],[0]
"We also controlled for worker correlation with mean score, and position of the sentence in the HIT, but these were not significant and did not affect the autocorrelation.",3.3 Machine Translation Adequacy,[0],[0]
"As seen in Table 3, we see a small but significant positive autocorrelation for good workers.",3.3 Machine Translation Adequacy,[0],[0]
"The bias is much stronger with
bad (rejected) workers.",3.3 Machine Translation Adequacy,[0],[0]
"An interesting question is whether the bias changes as workers annotate more data, which could be ascribed to learning through the task, calibrating their internal scales, or becoming fatigued on a monotonous task.",3.3 Machine Translation Adequacy,[0],[0]
"Each HIT consists of 100 sentences, and we divide the dataset into 3 equal groups based on the position of sentence in the HIT.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 4, for good and moderate workers, the bias is stronger in the first group of sentences annotated, decreases in the second, and is much smaller in the last.",3.3 Machine Translation Adequacy,[0],[0]
"This could be because workers are familiarising themselves with the task earlier on, and calibrating their scale.",3.3 Machine Translation Adequacy,[0],[0]
"There is no such trend with bad quality scores, possibly because the workers are not putting in sufficient effort to produce accurate scores.
",3.3 Machine Translation Adequacy,[0],[0]
Next we assess the impact of the bias in the worst case situation.,3.3 Machine Translation Adequacy,[0],[0]
"We discretize scores into low, middle and high based on equal-frequency binning, and divide the dataset into 3 groups based on the score assigned to the previous sentence.",3.3 Machine Translation Adequacy,[0],[0]
"As shown in Table 5 we can see that the sentences in the “low” partition and the “high” partition have a difference of 0.18, which is highly significant;3 moreover, this difference is likely to be sufficiently large to alter the rankings of systems in an evaluation.",3.3 Machine Translation Adequacy,[0],[0]
"The bias remains even when we increase the number of workers and use the average score, as all workers scored the translations in the same order.",3.3 Machine Translation Adequacy,[0],[0]
"This shows that the mean is also affected by
3p < 0.001 using Welch’s two-sample t-test
sequence bias.",3.3 Machine Translation Adequacy,[0],[0]
"Thus, it is theoretically possible to exploit sequence bias to artificially deflate (or inflate) a specific system’s computed score by ordering a HIT such that the system’s output is seen consistently immediately after a bad (or good) output.",3.3 Machine Translation Adequacy,[0],[0]
"We have shown significant sequence effects across several independent crowdsourced datasets: a negative autocorrelation in the RTE and TEMPORAL datasets, and a positive autocorrelation in the MTadeq dataset.",4 Discussion and Conclusions,[0],[0]
The negative autocorrelation can be attributed either to sequential contrast effects or the gambler’s fallacy.,4 Discussion and Conclusions,[0],[0]
"These effects were not significant for the AFFECTIVE dataset, perhaps due to the nature of the annotation task, whereby annotations of one emotion are separated by six other annotations, thus limiting the potential for sequencing effects.",4 Discussion and Conclusions,[0],[0]
"It is also possible that the dataset is too small to obtain statistical significance.
",4 Discussion and Conclusions,[0],[0]
"MT judgements are subjective, and when people are asked to rate them on a continuous scale, they need time to calibrate their scale.",4 Discussion and Conclusions,[0],[0]
"We show that the sequential bias decreases for better workers as they annotate more sentences in the HIT, indicating a learning effect.",4 Discussion and Conclusions,[0],[0]
"Since the ordering of the systems is random, system scores obtained by averaging scores of all sentences translated by the system would be unbiased, assuming a sufficiently large sample of sentences.",4 Discussion and Conclusions,[0],[0]
Thus we do not expect sequential bias to have a marked effect on system rankings or other macro-level conclusions on the basis of this data.,4 Discussion and Conclusions,[0],[0]
"However, the scores of in-
dividual translations remain biased, which augurs poorly for the use of these annotations at the sentence level, such as when used in error analysis or for training automatic metrics.
",4 Discussion and Conclusions,[0],[0]
"Sequence problems can be easily addressed by adequate randomisation — providing each individual worker with a separate dataset that has been randomised, such that no two workers see the same ordered data.",4 Discussion and Conclusions,[0],[0]
"In this way sequence bias effects can be considered as independent noise sources, rather than a systematic bias, and consequently the aggregate results over several workers will remain unbiased.
",4 Discussion and Conclusions,[0],[0]
"This study has shown that sequence bias is real, and can distort evaluation and annotation exercises with crowd-workers.",4 Discussion and Conclusions,[0],[0]
"We limited our scope to binary and continuous responses, however it is likely that sequence effects are prevalent for multinomial and structured outputs, e.g., in discourse and parsing, where priming is known to have a significant effect (Reitter et al., 2006).",4 Discussion and Conclusions,[0],[0]
"Another important question for future work is whether sequence bias is detectable in expert annotators, not just crowd workers.",4 Discussion and Conclusions,[0],[0]
We thank the anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by the Australian Research Council.,Acknowledgments,[0],[0]
Manual data annotation is a vital component of NLP research.,abstractText,[0],[0]
"When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation.",abstractText,[0],[0]
"In this paper, we explore sequence effects where annotations of an item are affected by the preceding items.",abstractText,[0],[0]
"Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next.",abstractText,[0],[0]
"During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively.",abstractText,[0],[0]
We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets.,abstractText,[0],[0]
We then recommend a simple way to minimise sequence effects.,abstractText,[0],[0]
Sequence Effects in Crowdsourced Annotations,title,[0],[0]
"The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",Introduction,[0],[0]
"Comprised of elements s
t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",Introduction,[0],[0]
", sT q P X has length T which can vary between different instances.",Introduction,[0],[0]
"Sentences are a popular example of such data, where each s
j is a word from the language.",Introduction,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.
1MIT Computer Science & Artificial Intelligence Laboratory.",Introduction,[0],[0]
"Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.
",Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",Introduction,[0],[0]
"Copyright 2017 by the author(s).
",Introduction,[0],[0]
those which appear realistic),Introduction,[0],[0]
.,Introduction,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.
",Introduction,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",Introduction,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D
n",Introduction,[0],[0]
"“ tpx i , y i qun i“1 iid„ p XY
of sequence-outcome pairs.",Introduction,[0],[0]
"The marginal distribution p X
is assumed as a generative model of the natural sequences, and may be concentrated in a small subspace of X .",Introduction,[0],[0]
"Throughout this paper, p denotes both density and distribution functions depending on the referenced variable.
",Introduction,[0],[0]
"After fitting models to D n , we are presented a new sequence x0 P X (with unknown outcome), and our goal is to quickly identify a revised version that is expected to have superior outcome.",Introduction,[0],[0]
"Formally, we seek the revised sequence:
x˚ “ argmax xPC
x0
ErY | X “ xs (1)
Here, we want the set C x0 of feasible revisions to ensure that x˚ remains natural and is merely a minor revision of x0.",Introduction,[0],[0]
"Under a generative modeling perspective, these two goals are formalized as the following desiderata: p
X px˚q is not too small, and x˚ and x0 share similar underlying latent characteristics.",Introduction,[0],[0]
"When revising a sentence for example, it is imperative that the revision reads naturally (has reasonable likelihood under the distribution of realistic sentences) and retains the semantics of the original.
",Introduction,[0],[0]
This optimization is difficult because the constraint-set and objective may be highly complex and are both unknown (must be learned from data).,Introduction,[0],[0]
"For many types of sequence such as sentences, standard distance measures applied directly in the space of X or S (eg. Levenshtein distance or TF-IDF similarity) are inadequate to capture meaningful similarities, even though these can be faithfully reflected by a simple metric over an appropriately learned space of continuous latent factors (Mueller & Thyagarajan, 2016).",Introduction,[0],[0]
"In this work, we introduce a generative-modeling framework which transforms (1) into a simpler differentiable optimiza-
tion by leveraging continuous-valued latent representations learned using neural networks.",Introduction,[0],[0]
"After the generative model has been fit, our proposed procedure can efficiently revise any new sequence in a manner that satisfies the aforementioned desiderata (with high probability).",Introduction,[0],[0]
"Unlike imitation learning, our setting does not require availability of improved versions of a particular sequence.",Related Work,[0],[0]
"This prevents direct application of a sequence-to-sequence model (Sutskever et al., 2014).",Related Work,[0],[0]
"Similar to our approach, Gómez-Bombarelli et al. (2016) also utilize latent autoencoder representations in order to propose novel chemical structures via Bayesian optimization.",Related Work,[0],[0]
"However, unlike sequential bandit/reinforcement-learning settings, our learner sees no outcomes outside of the training data, neither for the new sequence it is asked to revise, nor for any of its proposed revisions of said sequence (Mueller et al., 2017).",Related Work,[0],[0]
"Our methods only require an easily-assembled dataset of sequence-outcome pairs and are thus widely applicable.
",Related Work,[0],[0]
"Combinatorial structures are often optimized via complex search heuristics such as genetic programming (Zaefferer et al., 2014).",Related Work,[0],[0]
"However, search relies on evaluating isolated changes in each iteration, whereas good revisions of a sequence are often made over a larger context (ie. altering a phrase in a sentence).",Related Work,[0],[0]
"From the vast number of possibilities, such revisions are unlikely to be found by search-procedures, and it is generally observed that such methods are outperformed by gradient-based optimization in high-dimensional continuous settings.",Related Work,[0],[0]
"Unlike combinatorial search, our framework leverages gradients in order to efficiently find good revisions at test time.",Related Work,[0],[0]
"Simonyan et al. (2014) and Nguyen et al. (2015) also proposed gradientbased optimization of inputs with respect to neural predictions, but work in this vein has been focused on conditional generation (rather than revision) and is primarily restricted to the continuous image domain (Nguyen et al., 2016).",Related Work,[0],[0]
"To identify good revisions, we first map our stochastic combinatorial optimization problem into a continuous space where the objective and constraints exhibit a simpler form.",Methods,[0],[0]
"We assume the data are generated by the probabilistic graphical model in Figure 1A. Here, latent factors Z P Rd specify a (continuous) configuration of the generative process for X,Y (both sequences and outcomes), and we adopt the prior p
Z “ Np0, Iq.",Methods,[0],[0]
"Relationships between these variables are summarized by the maps F,E,D which we parameterize using three neural networks F ,E ,D trained to enable efficient approximate inference under this model.
",Methods,[0],[0]
"The first step of our framework is to fit this model to D n
by learning the parameters of these inference networks: the encoder E , the decoder D , and the outcome-predictor F .",Methods,[0],[0]
"A good model that facilitates high-quality revision under our framework will possess the following properties: (1) Y can efficiently be inferred from Z and this relationship obeys a smooth functional form, (2) the map D produces a realistic sequence x given any z with reasonable prior probability, (3) the distribution of natural sequences is geometrically simple in the latent Z-space.",Methods,[0],[0]
"We explicitly encourage (1) by choosing F as a fairly simple feedforward network, (2) by defining D as the most-likely x given z, and (3) by endowing Z with our simple Np0, Iq prior.",Methods,[0],[0]
Another characteristic desired of our Z-representations is that they encode meaningful sequence-features such that two fundamentally similar sequences are likely to have been generated from neighboring z-values.,Methods,[0],[0]
"Applied to image data, VAE models similar to ours have been found to learn latent representations that disentangle salient characteristics such as scale, rotation, and other independent visual concepts (Higgins et al., 2016).",Methods,[0],[0]
"The latent representations of recurrent architectures trained on text (similar to the models used here) have also been shown to encode meaningful semantics, with a strong correlation between distances in the latent space and human-judged similarity between texts (Mueller & Thyagarajan, 2016).",Methods,[0],[0]
"By exploiting such simplified geometry, a basic shift in the latent vector space may be able to produce higher-quality revisions than attempts to directly manipulate the combinatorial space of sequence elements.
",Methods,[0],[0]
"After fitting a model with these desirable qualities, our strategy to revise a given sequence x0 P X is outlined in Figure 1B. First, we compute its latent representation z0 “ Epx0q using a trained encoding map.",Methods,[0],[0]
"As the latent representations z are continuous, we can employ efficient gradient-based optimization to find a nearby local optimum z˚ of F pzq (within a simple constraint-set around z0 defined later on).",Methods,[0],[0]
"To z˚, we subsequently apply a simple decoding map D (defined with respect to our learned model) in order to obtain our revised sequence x˚. Under our
assumed model, the optimization in latent representationspace attempts to identify a generative configuration which produces large values of Y (as inferred via F ).",Methods,[0],[0]
The subsequent decoding step seeks the most likely sequence produced by the optimized setting of the latent factors.,Methods,[0],[0]
"For approximate inference in the X,Z relationship, we leverage the variational autoencoder (VAE) model of Kingma & Welling (2014).",Variational Autoencoder,[0],[0]
"In our VAE, a generative model of sequences is specified by our prior over the latent values z combined with a likelihood function p
D px | zq which our decoder network D outputs in order to evaluate the likelihood of any sequence x given z P Rd.",Variational Autoencoder,[0],[0]
"Given any sequence x, our encoder network E outputs a variational approximation q
E pz | xq of the true posterior over the latent-values ppz | xq9 p
D px | zqp Z pzq.",Variational Autoencoder,[0],[0]
"As advocated by Kingma & Welling (2014) and Bowman et al. (2016), we employ the variational family q
E pz",Variational Autoencoder,[0],[0]
| xq,Variational Autoencoder,[0],[0]
"“ Npµ z|x,⌃z|x)",Variational Autoencoder,[0],[0]
"with diag-
onal covariance",Variational Autoencoder,[0],[0]
.,Variational Autoencoder,[0],[0]
"Our revision methodology employs the encoding procedure Epxq “ µ
z|x which maps a sequence to the maximum a posteriori (MAP) configuration of the latent values z (as estimated by the encoder network E ).",Variational Autoencoder,[0],[0]
"The parameters of E ,D are learned using stochastic variational inference to maximize a lower bound for the marginal likelihood of each observation in the training data:
log p X pxq • ´ “ Lrecpxq ` Lpripxq ‰ (2)",Variational Autoencoder,[0],[0]
"Lrecpxq “ ´E q
E pz|xq rlog",Variational Autoencoder,[0],[0]
"pDpx | zqs Lpripxq “ KLpqEpz | xq|| pZq
Defining z|x",Variational Autoencoder,[0],[0]
“,Variational Autoencoder,[0],[0]
"diagp⌃z|xq, the prior-enforcing KullbackLeibler divergence has a differentiable closed form expression when q
E , p Z are diagonal Gaussian distributions.",Variational Autoencoder,[0],[0]
"The reconstruction term Lrec (ie. negative log-likelihood under the decoder model) is efficiently approximated using just one Monte-Carlo sample z „ q
E pz | xq.",Variational Autoencoder,[0],[0]
"To optimize the variational lower bound over our data D
n with respect to the parameters of neural networks E ,D , we use stochastic gradients of (2) obtained via backpropagation and the reparameterization trick of Kingma & Welling (2014).
",Variational Autoencoder,[0],[0]
"Throughout, our encoder/decoder models E ,D are recurrent neural networks (RNN).",Variational Autoencoder,[0],[0]
"RNNs adapt standard feedforward neural networks for sequence data x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q, where at each time-step t P t1, . . .",Variational Autoencoder,[0],[0]
", T u, a fixed size hiddenstate vector h
t P Rd is updated based on the next element in the input sequence.",Variational Autoencoder,[0],[0]
"To produce the approximate posterior for a given x, our encoder network E appends the following additional layers to the final RNN hidden-state (parameterized by W
µ
,W ,W v , b µ , b , b v ):
µ z|x “ WµhT ` bµ P Rd
z|x",Variational Autoencoder,[0],[0]
"“ expp´|W v ` b |q, v “ ReLUpWvhT ` bvq (3)
",Variational Autoencoder,[0],[0]
The (squared) elements of z|x P Rd form the diagonal of our approximate-posterior covariance ⌃ z|x.,Variational Autoencoder,[0],[0]
"Since Lpri is minimized at z|x “ ~1 and Lrec is likely to worsen with additional variance in encodings (as our posterior approximation is unimodal), we simply do not consider
z|x values that exceed 1 in our variational family.",Variational Autoencoder,[0],[0]
This restriction results in more stable training and also encourages the encoder and decoder to co-evolve such that the true posterior is likely closer to unimodal with variance § 1.,Variational Autoencoder,[0],[0]
"To evaluate the likelihood of a sequence, RNN D computes not only its hidden state h
t
, but also the additional output:
⇡ t “ softmaxpW ⇡ h t ` b ⇡ q (4)
",Variational Autoencoder,[0],[0]
"At each position t, ⇡ t estimates pps t | s1, . . .",Variational Autoencoder,[0],[0]
", st´1q by relying on h
t to summarize the sequence history.",Variational Autoencoder,[0],[0]
"By the factorization pps1, . . .",Variational Autoencoder,[0],[0]
", sT q “ ± T
t“1 ppst | st´1, . . .",Variational Autoencoder,[0],[0]
", s1q, we have p
D px",Variational Autoencoder,[0],[0]
"| zq “ ±T t“1 ⇡trsts, which is calculated by
specifying an initial hidden-state h0 “ z and feeding x “ ps1, . . .",Variational Autoencoder,[0],[0]
", sT q into D .",Variational Autoencoder,[0],[0]
"From a given latent configuration z, our revisions are produced by decoding a sequence via the most-likely observation, which we denote as the map:
Dpzq “ argmax",Variational Autoencoder,[0],[0]
xPX,Variational Autoencoder,[0],[0]
"p D px | zq (5)
",Variational Autoencoder,[0],[0]
"While the most-likely decoding in (5) is itself a combinatorial problem, beam search can exploit the sequentialfactorization of ppx | zq to efficiently find a good approximate solution (Wiseman & Rush, 2016; Sutskever et al., 2014).",Variational Autoencoder,[0],[0]
"For x˚ “ Dpzq P X , this decoding strategy seeks to ensure neither p
X
px˚q nor ppz | x˚q is too small.",Variational Autoencoder,[0],[0]
"In addition to the VAE component, we fit a compositional outcome-prediction model which uses a standard feed forward neural network F to implement the map F : Rd Ñ R. It is assumed that F pzq “ ErY | Z “ zs under our generative model.",Compositional Prediction of Outcomes,[0],[0]
"Rather than integrating over Z to compute ErY | X “ xs “ ≥ F pzqq
E pz | xqdz, we employ the first-order Taylor approximation F pEpxqq, where the approximation-error shrinks the more closely F resembles an affine transformation.",Compositional Prediction of Outcomes,[0],[0]
"To ensure this approximateinference step accurately estimates the conditional expectation, we jointly train E and F with the loss:
Lmsepx, yq “ ry ´ F pEpxqqs2 (6)
",Compositional Prediction of Outcomes,[0],[0]
"If the architecture of networks E ,F is specified with sufficient capacity to capture the underlying conditional relationship, then we should have F pEpxqq « ErY | X “ xs after properly learning the network parameters from a sufficiently large dataset (even F is a nonlinear map).
",Compositional Prediction of Outcomes,[0],[0]
"Enforcing Invariance
In theory, it is possible that some dimensions of z pertain solely to the outcome y and do not have any effect on the decoded sequence Dpzq.",Compositional Prediction of Outcomes,[0],[0]
"Happening to learn this sort of latent representation would be troubling, since subsequent optimization of the inferred y with respect to z might not actually lead to a superior revised sequence.",Compositional Prediction of Outcomes,[0],[0]
"To mitigate this issue, we carefully ensure the dimensionality d of our latent Z does not significantly exceed the bottleneck capacity needed to produce accurate outcome-predictions and VAE reconstructions (Gupta et al., 2016).",Compositional Prediction of Outcomes,[0],[0]
"We explicitly suppress this undesirable scenario by adding the following loss to guide training of our neural networks:
Linv “ Ez„p Z
“ F pzq ´ F pEpDpzqqq ‰2",Compositional Prediction of Outcomes,[0],[0]
"(7)
When optimizing neural network parameters with respect to this loss, we treat the parameters of D and the lefthand F pzq term as fixed, solely backpropagating Monte-Carlo estimated gradients into E ,F .",Compositional Prediction of Outcomes,[0],[0]
Driving Linv toward 0 ensures our outcome-predictions remain invariant to variation introduced by the encoding-decoding process (and this term also serves as a practical regularizer to enforce additional smoothness in our learned functions).,Compositional Prediction of Outcomes,[0],[0]
"The parameters of all components of this model (q E , p D , and F ) are learned jointly in an end-to-end fashion.",Joint Training,[0],[0]
"Training is done via stochastic gradient descent applied to minimize the following objective over the examples in D
n
:
Lpx, yq",Joint Training,[0],[0]
"“ Lrec ` priLpri ` mse 2 Y Lmse ` inv 2 Y Linv (8)
where 2 Y denotes the (empirical) variance of the outcomes, and the • 0 are constants chosen to balance the relative weight of each goal so that the overall framework produces maximally useful revisions.",Joint Training,[0],[0]
"By setting mse “ inv “ 0 at first, we can optionally leverage a separate large corpus of unlabeled examples to initially train only the VAE component of our architecture, as in the unsupervised pretraining strategy used successfully by Kiros et al. (2015); Erhan et al. (2010).
",Joint Training,[0],[0]
"In practice, we found the following training strategy to work well, in which numerous mini-batch stochastic gradient updates (typically 10-30 epochs) are applied within every one of these steps:
Step 1: Begin with inv “ pri “ 0, so Lrec and Lmse are the only training objectives.",Joint Training,[0],[0]
"We found that regardless of the precise value specified for mse, both Lrec and Lmse were often driven to their lowest possible values during this joint optimization (verified by training individually against each objective).
",Joint Training,[0],[0]
"Step 2: Grow pri from 0 to 1 following the sigmoid annealing schedule proposed by Bowman et al. (2016), which is needed to ensure the variational sequence to sequence model does not simply ignore the encodings z",Joint Training,[0],[0]
(note that the formal variational lower bound is attained at pri “ 1).,Joint Training,[0],[0]
"Step 3: Gradually increase inv linearly until Linv becomes small on average across our Monte-Carlo samples z „ p
Z .",Joint Training,[0],[0]
"Here, p
D is treated as constant with respect to Linv, and each mini-batch used in stochastic gradient descent is chosen to contain the same number of Monte-Carlo samples for estimating Linv as (sequence, outcome) pairs.",Joint Training,[0],[0]
"While the aforementioned training procedure is computationally intensive, once learned, our neural networks can be leveraged for efficient inference.",Proposing Revisions,[0],[0]
"Given user-specified constant ↵ ° 0 and a to-be-revised sequence x0, we propose the revision x˚ output by the following procedure.
",Proposing Revisions,[0],[0]
"REVISE Algorithm Input: sequence x0 P X , constant ↵ P p0, |2⇡⌃ z|x0 |´ 1 2 q Output: revised sequence x˚ P X 1)",Proposing Revisions,[0],[0]
"Use E to compute q
E pz | x0q 2)",Proposing Revisions,[0],[0]
"Define C
x0 “ z P Rd :",Proposing Revisions,[0],[0]
"q E pz | x0q • ↵ (
3) Find z˚ “ argmax zPC
x0
F pzq (gradient ascent)
4) Return x˚ “ Dpz˚q (beam search)
",Proposing Revisions,[0],[0]
"Intuitively, the level-set constraint C x0 Ñ Rd ensures that z˚, the latent configuration from which we decode x˚, is likely similar to the latent characteristics responsible for the generation of x0.",Proposing Revisions,[0],[0]
Assuming x0 and x˚ share similar latent factors implies these sequences are fundamentally similar according to the generative model.,Proposing Revisions,[0],[0]
"Note that z˚ “ Epx0q is always a feasible solution of the latent-factor optimization over z P C
x0 (for any allowed value of ↵).",Proposing Revisions,[0],[0]
"Furthermore, this constrained optimization is easy under our Gaussian approximate-posterior, since C
x0 forms a simple ellipsoid centered around Epx0q.",Proposing Revisions,[0],[0]
"To find z˚ in Step 3 of the REVISE procedure, we use gradient ascent initialized at z “ Epx0q, which can quickly reach a local maximum if F is parameterized by a simple feedforward network.",Proposing Revisions,[0],[0]
"Starting the search at Epx0q makes most sense for unimodal posterior approximations like our Gaussian q
E .",Proposing Revisions,[0],[0]
"To ensure all iterates remain in the feasible region C
x0 , we instead take gradient steps with respect to a penalized objective F pzq ` µ ¨ Jpzq where:
Jpzq “ log ” K ´ pz ´ Epx0qqT ⌃´1 z|x0pz ´ Epx0qq ı
K “ ´2 logrp2⇡qd{2|⌃ z|x|1{2↵s (9)
and 0 † µ !",Proposing Revisions,[0],[0]
"1 is gradually decreased toward 0 to en-
sure the optimization can approach the boundary of C x0 .",Proposing Revisions,[0],[0]
"In terms of resulting revision quality, we found this log barrier method outperformed other standard first-order techniques for constrained optimization such as the projected gradient and Franke-Wolfe algorithms.
",Proposing Revisions,[0],[0]
"In principle, our revision method can operate on the latent representations of a traditional deterministic autoencoder for sequences, such as the seq2seq models of Sutskever et al. (2014) and Cho et al. (2014).",Proposing Revisions,[0],[0]
"However, the VAE offers numerous practical advantages, some of which are highlighted by Bowman et al. (2016) in the context of generating more-coherent sentences.",Proposing Revisions,[0],[0]
The posterior uncertainty of the VAE encourages the network to smoothly spread the training examples across the support of the latent distribution.,Proposing Revisions,[0],[0]
"In contrast, central regions of the latent space under a traditional autoencoder can contain holes (to which no examples are mapped), and it is not straightforward to avoid these in our optimization of z˚.",Proposing Revisions,[0],[0]
"Furthermore, we introduce an adaptive variant of our decoder in §S1 which is designed to avoid poor revisions in cases where the initial sequence is already not reconstructed properly: DpEpx0qq ‰ x0.",Proposing Revisions,[0],[0]
"Here, we theoretically characterize properties of revisions obtained via our REVISE procedure (all proofs are relegated to §S3 in the Supplementary Material).",Theoretical Properties of Revision,[0],[0]
"Our results imply that in an ideal setting where our neural network inference approximations are exact, the revisions proposed by our method are guaranteed to satisfy our previously stated desiderata: x˚ is associated with an expected outcome-increase, x˚ appears natural (has nontrivial probability under p
X whenever x0 is a natural sequence), and x˚ is likely to share similar latent characteristics as x0 (since x˚ is the most likely observation generated from z˚ and q
E pz˚ | x0q • ↵ by design).",Theoretical Properties of Revision,[0],[0]
"Although exact approximations are unrealistic in practice, our theory precisely quantifies the expected degradation in the quality of proposed revisions that accompanies a decline in either the accuracy of our approximate inference techniques or the marginal likelihood of the original sequence to revise.
",Theoretical Properties of Revision,[0],[0]
"Theorems 1 and 2 below ensure that for an initial sequence x0 drawn from the natural distribution, the likelihood of the revised sequence x˚ output by our REVISE procedure under p
X has lower bound determined by the user-parameter ↵ and the probability of the original sequence p
X px0q.",Theoretical Properties of Revision,[0],[0]
"Thus, when revising a sequence x0 which looks natural (has substantial probability under p
X ), our procedure is highly likely to produce a revised sequence x˚ which also looks natural.",Theoretical Properties of Revision,[0],[0]
"The strength of this guarantee can be precisely controlled by choosing ↵ appropriately large in applications where this property is critical.
",Theoretical Properties of Revision,[0.9517998007719878],['This is attractive when structural changes in the data happen slowly and are not captured well by CPS.']
"In each high probability statement, our bounds assume the
initial to-be-revised sequence x0 stems from the natural distribution p
X , and each result holds for any fixed constant ° 0.",Theoretical Properties of Revision,[0],[0]
"We first introduce the following assumptions: (A1) For ° 0,↵ ° 0, there exists 0 † § 1 such that:
i. With probability • 1 ´ {2 (over x „ p X ):
ppz | xq • ¨ q E pz | xq",Theoretical Properties of Revision,[0],[0]
"whenever q E pz | xq • ↵
ii.",Theoretical Properties of Revision,[0],[0]
"PrpZ R B R{2p0qq • ¨ Prp rZ R BR{2p0qq
where Z „ Np0, Iq, and rZ „ q Z
, the average encoding distribution defined by Hoffman & Johnson (2016) as:
q Z pzq",Theoretical Properties of Revision,[0],[0]
"“ E x„p
X
rq E pz | xqs (10) B
R p0q “ tz P Rd : ||z|| § Ru denotes the Euclidean ball centered around 0 with radius R defined here as: R “ maxtR1, R2u (11) with R1 “ a ´8 logr↵ ¨ p2⇡qd{2s
R2 “ maxt rR2, 2u, rR2 “ c 8 ´ 14d log ´ 8 ¯
(A2)",Theoretical Properties of Revision,[0],[0]
There exists ⌘ ° 0 (depends on ) such that with probability • 1 ´ {2 (over x0 „ pX ): ppz˚ | x˚q,Theoretical Properties of Revision,[0],[0]
§ ⌘,Theoretical Properties of Revision,[0],[0]
"This means the latent posterior is bounded at x˚, z˚ (as defined in REVISE), where both depend upon the initial tobe-revised sequence x0.",Theoretical Properties of Revision,[0],[0]
Theorem 1.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, (A1) and (A2) imply:
p X px˚q",Theoretical Properties of Revision,[0],[0]
"• ↵ ⌘ ¨ p X
px0q with probability • 1 ´ (over x0 „ pX ).
",Theoretical Properties of Revision,[0],[0]
"Condition (A1) forms a generalization of absolute continuity, and is required since little can be guaranteed about our inference procedures if the variational posterior is too inaccurate.",Theoretical Properties of Revision,[0],[0]
"Equality holds in (A1) with probability 1 if the variational distributions q
E exactly represent the true posterior ( Ñ 1 as the variational approximations become more accurate over the measure p
X ).",Theoretical Properties of Revision,[0],[0]
"In practice, minimization of the reverse KL divergence (Lpri) used in our VAE formulation ensures that q
E pz",Theoretical Properties of Revision,[0],[0]
"| xq is small wherever the true posterior ppz | xq takes small values (Blei et al., 2017).",Theoretical Properties of Revision,[0],[0]
"While the bound in Theorem 1 has particularly simple form, this result hinges on assumption (A2).",Theoretical Properties of Revision,[0],[0]
One can show for example that the inequality in (A2) is satisfied if the posteriors ppz,Theoretical Properties of Revision,[0],[0]
| x˚q are Lipschitz continuous functions of z at z˚ (sharing one Lipschitz constant over all possible x˚).,Theoretical Properties of Revision,[0],[0]
"In general however, (A2) heavily depends on both the data distribution p
X and decoder model p D .",Theoretical Properties of Revision,[0],[0]
"Therefore, we provide a similar lower bound guarantee on the likelihood of our revision x˚ under p
X , which instead only relies on weaker assumption (A3) below.
",Theoretical Properties of Revision,[0],[0]
"(A3) There exists L ° 0 such that for each x P X : p D px | zq is a L-Lipschitz function of z over B R`1p0q.
",Theoretical Properties of Revision,[0],[0]
"Here, L depends on (through R), and we assume L • 1 without loss of generality.",Theoretical Properties of Revision,[0],[0]
(A3) is guaranteed to hold in the setting where we only consider sequences of finite length § T .,Theoretical Properties of Revision,[0],[0]
"This is because the probability output by our decoder model, p
D px | zq, is differentiable with bounded gradients over all z P B
R p0q under any sequence-to-sequence RNN architecture which can be properly trained using gradient methods.",Theoretical Properties of Revision,[0],[0]
"Since B
R`1p0q Ä Rd is a closed interval, p D
px | zq must be Lipschitz continuous over this set, for a given value of x.",Theoretical Properties of Revision,[0],[0]
We can simply define L to be the largest Lipschitz constant over the |S|T possible choices of x P X (|S| “ size of the vocabulary).,Theoretical Properties of Revision,[0],[0]
"In the next theorem below, user-specified constant ↵ ° 0 is defined in REVISE, and L, , R all depend on .",Theoretical Properties of Revision,[0],[0]
Theorem 2.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if (A1) and (A3) hold, then with probability • 1 ´ (over x0 „ pX ):
p X
px˚q •",Theoretical Properties of Revision,[0],[0]
"Ce ´R
Ld ¨
“ ¨ ↵ ¨ p
X px0q ‰ d`1
where constant C “ ⇡ d{2 pd2 ` 1q ¨ pd ` 1q d pd ` 2qd`1
Our final result, Theorem 3, ensures that our optimization of z˚ with respect to F is tied to the expected outcomes at x˚ “ Dpz˚q, so that large improvements in the optimization objective: F pz˚q ´ F pEpx0qq imply that our revision procedure likely produces large expected improvements in the outcome: ErY | X “ x˚s ´ ErY | X “ x0s.",Theoretical Properties of Revision,[0],[0]
"For this result, we make the following assumptions:
(A4) For any ° 0, there exists  ° 0",Theoretical Properties of Revision,[0],[0]
"such that PrpX P Kq • 1 ´ {2, where we define:
K “ tx P X : x0 “ x ùñ pXpx˚q • u (12)
as the subset of sequences whose improved versions produced by our REVISE procedure remain natural with likelihood • .",Theoretical Properties of Revision,[0],[0]
"Note that either Theorem 1 or 2 (with the corresponding assumptions) ensures that one can suitably define  such that (A4) is satisfied (by considering a sufficiently large finite subset of X ).
",Theoretical Properties of Revision,[0],[0]
(A5),Theoretical Properties of Revision,[0],[0]
"For any  ° 0, there exists ✏mse ° 0 such that PrpX P Emseq",Theoretical Properties of Revision,[0],[0]
"° 1 ´ , where we define:
Emse“ tx P X : |F",Theoretical Properties of Revision,[0],[0]
"pEpxqq ´ ErY |X “ xs| § ✏mseu (13)
(A6) For any ° 0, there exists ✏inv ° 0 such that:
|F pzq ´ F pEpDpzqqq| § ✏inv for all z P BRp0q",Theoretical Properties of Revision,[0],[0]
"Ä Rd
where R is defined in (11) and depends on .
",Theoretical Properties of Revision,[0],[0]
"Here, ✏mse and ✏inv quantify the approximation error of our neural networks for predicting expected outcomes and ensuring encoding-decoding invariance with respect to F .
",Theoretical Properties of Revision,[0],[0]
"Standard learning theory implies both ✏mse, ✏inv will be driven toward 0 if we use neural networks with sufficient capacity to substantially reduce Lmse and Linv over a large training set.",Theoretical Properties of Revision,[0],[0]
Theorem 3.,Theoretical Properties of Revision,[0],[0]
"For any ° 0, if conditions (A1), (A4), (A5), and (A6) hold, then with probability • 1 ´ ´ :
z ˚ ´ ✏ § F pz˚q ´ F pEpx0qq § z ˚ ` ✏ (14)
where
z ˚ “ ErY | X “ x˚s ´ ErY | X “ x0s ✏ “ ✏
inv ` 2✏ mse
Here, , ✏inv are defined in terms of as specified in (A4), (A6), and ✏mse is defined in terms of  as specified in (A5).",Theoretical Properties of Revision,[0],[0]
"All of our RNNs employ the Gated Recurrent Unit (GRU) of Cho et al. (2014), which contains a simple gating mechanism to effectively learn long-range dependencies across a sequence.",Experiments,[0],[0]
"Throughout, F is a simple feedforward network with 1 hidden layer and tanh activations (note that the popular ReLU activation is inappropriate for F since it has zero gradient over half its domain).",Experiments,[0],[0]
"Decoding with respect to p
D is simply done entirely greedily (ie.",Experiments,[0],[0]
a beam-search of size 1) to demonstrate our approach is not reliant on search heuristics.,Experiments,[0],[0]
§S2 contains additional details for each analysis.,Experiments,[0],[0]
"To study our methods in a setting where all aspects of performance can be quantified, we construct a natural distribution p
X over sequences of lengths 10-20 whose elements stem from the vocabulary S “ tA,B, . . .",Simulation Study,[0],[0]
", I, Ju.",Simulation Study,[0],[0]
Each sequence is generated via the probabilistic grammar of Table S1.,Simulation Study,[0],[0]
"For each sequence, the associated outcome y is simply the number of times A appears in the sequence (a completely deterministic relationship).",Simulation Study,[0],[0]
"Since A often follows C and is almost always followed by B under p
X , a procedure to generate natural revisions cannot simply insert/substitute A symbols at random positions.
",Simulation Study,[0],[0]
Table 1 compares various methods for proposing revisions.,Simulation Study,[0],[0]
"Letting
Y denote the standard deviation of outcomes in D
n , we evaluate each proposed x˚ using a rescaled version of the actual underlying outcome-improvement:
Y px˚q “ ´1 Y pErY",Simulation Study,[0],[0]
| X “ x˚s ´ ErY | X “ x0sq.,Simulation Study,[0],[0]
"Except where sample size is explicitly listed, all models were trained using n “ 10, 000 (sequence, outcome) pairs sampled from the generative grammar.",Simulation Study,[0],[0]
"Wherever appropriate, the different methods all make use of the same neural network components with latent dimension d “ 128.",Simulation Study,[0],[0]
"Other than ↵, all hyperparameters of each revision method described below were chosen so that over 1000 revisions, the Levenshtein (edit) distance dpx˚, x0q « 3.3 on average.
",Simulation Study,[0],[0]
"All three results above the line in Table 1 are based on the full model described in our joint training procedure, with new sequences proposed via our REVISE algorithm (using the setting log↵ “ ´10000).",Simulation Study,[0],[0]
"In the latter two results, this model was only trained on a smaller subset of the data.",Simulation Study,[0],[0]
We also generated revisions via this same procedure with the more conservative choice log↵ “ ´1.,Simulation Study,[0],[0]
"ADAPTIVE denotes the same approach (with log↵ “ ´10000), this time using the adaptive decoding D
x0 introduced in §S1, which is intended to slightly bias revisions toward x0.",Simulation Study,[0],[0]
The model with inv “ pri “ 0 is a similar method using a deterministic sequence-to-sequence autoencoder rather than our probabilistic VAE formulation (no variational posterior approximation or invariance-enforcing) where the latent encodings are still jointly trained to predict outcomes via F .,Simulation Study,[0],[0]
"Under this model, a revision is proposed by starting at Epx0q in the latent space, taking 1000 (unconstrained) gradient steps with respect to F , and finally applying D to the resulting z.
The above methods form an ablation study of the various components in our framework.",Simulation Study,[0],[0]
"SEARCH is a different combinatorial approach where we randomly generate 100 revisions by performing 4 random edits in x0 (each individual edit is randomly selected as one of: substitution, insertion, deletion, or no change).",Simulation Study,[0],[0]
"In this approach, we separately learn a language-model RNN L on our training sequences (Mikolov et al., 2010).",Simulation Study,[0],[0]
"Sharing the same GRU architecture as our decoder model, L directly estimates the likelihood of any given sequence under p
X .",Simulation Study,[0],[0]
"Of the randomly generated revisions, we only retain those sequences x for which Lpxq • 1|S|Lpx0q (in this case, those which are not estimated to be † 10 times less likely than the original sequence x0 under pX ).",Simulation Study,[0],[0]
"Finally, we score each remaining candidate (including x0) using the outcome-prediction model F pEpxqq, and the best is chosen as x˚. Table 1 shows that our probabilistic VAE formulation outperforms the alternative approaches, both in terms of outcome-improvement achieved as well as ensuring revi-
sions follow p X .",Simulation Study,[0],[0]
"For comparison, ´ log p X px0q had an average value of 26.8 (over these 1000 starting sequences), and changing one randomly-selected symbol in each sequence to A results in an average negative log-probability of 32.8.",Simulation Study,[0],[0]
"Thus, all of our revision methods clearly account for p
X to some degree.",Simulation Study,[0],[0]
We find that all components used in our REVISION procedure are useful in achieving superior revisions.,Simulation Study,[0],[0]
"While individual standard deviations seem large, nearly all average differences in
Y or ´ log p X values produced by different methods are statistically significant considering they are over 1000 revisions.
",Simulation Study,[0],[0]
"From Supplementary Figure S1, it is clear that ↵ controls how conservative the changes proposed by our REVISE procedure tend to be, in terms of both ´ log p
X px˚q and the edit distance dpx0, x˚q.",Simulation Study,[0],[0]
"The red curve in Figure S1A suggests that our theoretical lower bounds for p
X px˚q are overly stringent in practice (although only the averagecase is depicted in the figure).",Simulation Study,[0],[0]
"The relationship between log p
X px0q and log pXpx˚q (see Figure S1B) is best-fit by a line of slope 1.2, indicating that the linear dependence on p
X px0q in the Theorem 1 bound for pXpx˚q is reasonably accurate.",Simulation Study,[0],[0]
Figure S1C shows that the magnitude of changes in the latent space (arising from z-optimization during our REVISE procedure) only exhibits a weak correlation with the edit distance between the resulting revision and the original sequence.,Simulation Study,[0],[0]
This implies that a fixed shift in different directions in the latent space can produce drastically different degrees of change in the sequence space.,Simulation Study,[0],[0]
"To ensure a high-quality revision, it is thus crucial to carefully treat the (variational) posterior landscape when performing manipulations of Z.",Simulation Study,[0],[0]
"Next, we apply our model to „1M reviews from BeerAdvocate (McAuley et al., 2012).",Improving Sentence Positivity,[0],[0]
"Each beer review is parsed into separate sentences, and each sentence is treated as an individual sequence of words.",Improving Sentence Positivity,[0],[0]
"In order to evaluate methods using an outcome that can be obtained for any proposed revision, we choose y P r0, 1s as the VADER sentiment compound score of a given sentence (Hutto & Gilbert,
2014).",Improving Sentence Positivity,[0],[0]
"VADER is a complex rule-based sentiment analysis tool which jointly estimates polarity and intensity of English text, and larger VADER scores correspond to text that humans find more positive with high fidelity.
",Improving Sentence Positivity,[0],[0]
We applied all aforementioned approaches to produce revisions for a held-out set of 1000 test sentences.,Improving Sentence Positivity,[0],[0]
"As p
X
underlying these sentences is unknown, we report estimates thereof obtained from a RNN language-model L learned on the sentences in D
n .",Improving Sentence Positivity,[0],[0]
Table 2 demonstrates that our VAE approach achieves the greatest outcome-improvement.,Improving Sentence Positivity,[0],[0]
"Moreover, Tables 3 and S2 show that our probabilisticallyconstrained VAE revision approach produces much more coherent sentences than the other strategies.",Improving Sentence Positivity,[0],[0]
"For our final application, we assemble a dataset of „100K short sentences which are either from Shakespeare or a more contemporary source (details in §S2.3).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"In this training data, each sentence is labeled with outcome y “ 0.9
if it was authored by Shakespeare and y “ 0.1 otherwise (these values are chosen to avoid the flat region of the sigmoid output layer used in network F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"When applied in this domain, our REVISE procedure thus attempts to alter a sentence so that the author is increasingly expected to be Shakespeare rather than a more contemporary source.
",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Tables 4 and S3 show revisions (of held-out sentences) proposed by our REVISE procedure with adaptive decoding (see §S1), together with sentences generated by applying the adaptive decoder at various points along an unconstrained gradient-ascent path in latent Z space (following gradients of F ).",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Since the data lack similar versions of a sentence written in both contemporary and Shakespearean language, this revision task is an ambitious application of our ideas.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Without observing a continuous spectrum of outcomes or leveraging specially-designed style transfer features (Gatys et al., 2016), our REVISE procedure has to alter the underlying semantics in order to nontrivially increase the expected outcome of the revised sentence under F .",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Nevertheless, we find that many of the revised sentences look realistic and resemble text written by Shakespeare.",Revising Modern Text in the Language of Shakespeare,[0],[0]
"Furthermore, these examples demonstrate how the probabilistic constraint in our REVISE optimization prevents the revision-generating latent Z configurations from straying into regions where decodings begin to look very unnatural.",Revising Modern Text in the Language of Shakespeare,[0],[0]
This paper presents an efficient method for optimizing discrete sequences when both the objective and constraints are stochastically estimated.,Discussion,[0],[0]
"Leveraging a latent-variable generative model, our procedure does not require any examples of revisions in order to propose natural-looking sequences with improved outcomes.",Discussion,[0],[0]
These characteristics are proven to hold with high probability in a theoretical analysis of VAE behavior under our controlled latent-variable manipulations.,Discussion,[0],[0]
"However, ensuring semantic similarity in textrevisions remains difficult for this approach, and might be improved via superior VAE models or utilizing additional similarity labels to shape the latent geometry.",Discussion,[0],[0]
"D., Bougares, F., Schwenk, H., and Bengio, Y. Learning phrase representations using rnn encoder-decoder for statistical machine translation.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Empirical Methods on Natural Language Processing, 2014.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Eck, D. and Schmidhuber, J. A first look at music composition using lstm recurrent neural networks.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"IDSIA Technical Report, 2002.
","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Erhan, D., Bengio, Y., Courville, A., Manzagol, P., Vincent, P., and Bengio, S. Why does unsupervised pretraining help deep learning?","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
"Journal of Machine Learning Research, 11:625–660, 2010.","Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau,",[0],[0]
transfer using convolutional neural networks.,"Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"Computer Vision and Pattern Recognition, 2016.","Gatys, L. A., Ecker, A. S., and Bethge, M. Image style",[0],[0]
"J. M., Aguilera-Iparraguirre, J., , Hirzel, T., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1610.02415, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Graves, A. Generating sequences with recurrent neural networks.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1308.0850, 2013.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Gupta, P., Banchs, R. E., and Rosso, P. Squeezing bottlenecks: Exploring the limits of autoencoder semantic representation capabilities.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Neurocomputing, 175:1001– 1008, 2016.
","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"Higgins, I., Matthey, L., Glorot, X., Pal, A., Uria, B., Blundell, C., Mohamed, S., and Lerchner, A. Early visual concept learning with unsupervised deep learning.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
"arXiv:1606.05579, 2016.","Gómez-Bombarelli, R., Duvenaud, D., Hernández-Lobato,",[0],[0]
another way to carve up the variational evidence lower bound.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"NIPS Workshop on Advances in Approximate Bayesian Inference, 2016.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Hutto, C.J. and Gilbert, E. Vader: A parsimonious rulebased model for sentiment analysis of social media text.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Eighth International Conference on Weblogs and Social
Media, 2014.
","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Karpathy, A.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
The unreasonable effectiveness of recurrent neural networks.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
"Andrej Karpathy blog, 2015.","Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
URL karpathy.github.io.,"Hoffman, M. D. and Johnson, M. J. Elbo surgery: yet",[0],[0]
bayes.,"Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"International Conference on Learning Representations, 2014.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Kiros, R., Zhu, Y., Salakhutdinov, R., Zemel, R. S., Torralba, A., Urtasun, R., and Fidler, S. Skip-thought vectors.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Advances in Neural Information Processing Systems, 2015.
","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"McAuley, J., Leskovec, J., and Jurafsky, D. Learning attitudes and attributes from multi-aspect reviews.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"IEEE International Conference on Data Mining, 2012.","Kingma, D. P. and Welling, M. Auto-encoding variational",[0],[0]
"Khudanpur, S. Recurrent neural network based language model.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Interspeech, 2010.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J. and Thyagarajan, A. Siamese recurrent architectures for learning sentence similarity.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
Proc.,"Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"AAAI Conference on Artificial Intelligence, 2016.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Mueller, J., Reshef, D. N., Du, G., and Jaakkola, T. Learning optimal interventions.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Artificial Intelligence and Statistics, 2017.
","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Nguyen, A., Yosinski, J., and Clune, J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Computer Vision and Pattern Recognition, 2015.","Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and",[0],[0]
"Clune, J. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
"Advances in Neural Information Processing Systems, 2016.","Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and",[0],[0]
convolutional networks: Visualising image classification models and saliency maps.,"Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"ICLR Workshop Proceedings, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Sutskever, I., Vinyals, O., and Le, Q.V. Sequence to sequence learning with neural networks.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Advances in Neural Information Processing Systems, 2014.
","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Wiseman, S. and Rush, A. M. Sequence-to-sequence learning as beam-search optimization.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"Empirical Methods in Natural Language Processing, 2016.","Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside",[0],[0]
"B., and Bartz-Beielstein, T. Efficient global optimization for combinatorial problems.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"Genetic and Evolutionary Computation Conference, 2014.","Zaefferer, M., Stork, J., Friese, M., Fischbach, A., Naujoks,",[0],[0]
"We present a model that, after learning on observations of (sequence, outcome) pairs, can be efficiently used to revise a new sequence in order to improve its associated outcome.",abstractText,[0],[0]
"Our framework requires neither example improvements, nor additional evaluation of outcomes for proposed revisions.",abstractText,[0],[0]
"To avoid combinatorial-search over sequence elements, we specify a generative model with continuous latent factors, which is learned via joint approximate inference using a recurrent variational autoencoder (VAE) and an outcome-predicting neural network module.",abstractText,[0],[0]
"Under this model, gradient methods can be used to efficiently optimize the continuous latent factors with respect to inferred outcomes.",abstractText,[0],[0]
"By appropriately constraining this optimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural.",abstractText,[0],[0]
"These desiderata are proven to hold with high probability under our approach, which is empirically demonstrated for revising natural language sentences.",abstractText,[0],[0]
"Introduction The success of recurrent neural network (RNN) models in complex tasks like machine translation and audio synthesis has inspired immense interest in learning from sequence data (Eck & Schmidhuber, 2002; Graves, 2013; Sutskever et al., 2014; Karpathy, 2015).",abstractText,[0],[0]
"Comprised of elements s t P S , which are typically symbols from a discrete vocabulary, a sequence x “ ps1, . . .",abstractText,[0],[0]
", sT q P X has length T which can vary between different instances.",abstractText,[0],[0]
"Sentences are a popular example of such data, where each s j is a word from the language.",abstractText,[0],[0]
"In many domains, only a tiny fraction of X (the set of possible sequences over a given vocabulary) represents sequences likely to be found in nature (ie.",abstractText,[0],[0]
MIT Computer Science & Artificial Intelligence Laboratory.,abstractText,[0],[0]
Correspondence to: J. Mueller <jonasmueller@csail.mit.edu>.,abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
those which appear realistic),abstractText,[0],[0]
.,abstractText,[0],[0]
"For example: a random sequence of words will almost never form a coherent sentence that reads naturally, and a random amino-acid sequence is highly unlikely to specify a biologically active protein.",abstractText,[0],[0]
"In this work, we consider applications where each sequence x is associated with a corresponding outcome y P R. For example: a news article title or Twitter post can be associated with the number of shares it subsequently received online, or the amino-acid sequence of a synthetic protein can be associated with its clinical efficacy.",abstractText,[0],[0]
"We operate under the standard supervised learning setting, assuming availability of a dataset D",abstractText,[0],[0]
Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,title,[0],[0]
"The approach of training sequence generation models using likelihood maximization suffers from known failure modes, and it is notoriously difficult to ensure multi-step generated sequences have coherent global structure.",1. Introduction,[0],[0]
"For example, long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) networks trained to predict the next character in sequences of text may produce text that has correct
1Google Brain, Mountain View, USA 2Massachusetts Institute of Technology, Cambridge, USA 3University of Cambridge, Cambridge, UK 4Max Planck Institute for Intelligent Systems, Stuttgart, Germany 5Université de Montréal, Montréal, Canada.",1. Introduction,[0],[0]
"Correspondence to: Natasha Jaques <jaquesn@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"spelling, punctuation, and even a semblance of grammar, but the generated text shifts so rapidly from topic to topic, that it is almost completely nonsensical (see (Graves, 2013) for an example).",1. Introduction,[0],[0]
"Similar networks trained to predict the next note in a melody suffer from the same problem; the generated music has no consistent theme or structure, and appears wandering and random.",1. Introduction,[0],[0]
"In addition, these models are prone to excessively repeating the same output token, a problem that has also been noted in the context of recurrent dialog generation models (Li et al., 2016).
",1. Introduction,[0],[0]
"To ameliorate these problems we propose Sequence Tutor, a novel approach which uses RL to impose structure on a sequence generation RNN via task-specific rewards, while simultaneously ensuring that information learned from data is retained.",1. Introduction,[0],[0]
"This is accomplished by maintaining a fixed copy of a sequence generation RNN pre-trained on data, which is termed the Reward RNN.",1. Introduction,[0],[0]
"Rather than simply using the Reward RNN to supply part of the rewards to our model, we derive novel off-policy RL methods for sequence generation from KL-control that allow us to directly penalize Kullback Leibler (KL) divergence from the policy defined by the Reward RNN.",1. Introduction,[0],[0]
As a byproduct of minimizing KL our objective includes an entropy regularization term that encourages high entropy in the distribution of the RL model.,1. Introduction,[0],[0]
"This is ideal for sequence generation tasks such as text, music, or molecule generation, in which maintaining diversity in the samples generated by the model is critical.
",1. Introduction,[0],[0]
"Sequence Tutor effectively combines both data and taskrelated goals, without relying on either as a perfect metric of task success.",1. Introduction,[0],[0]
This is an important novel direction of research.,1. Introduction,[0],[0]
"Much previous work on combining RL and MLE has used MLE training simply as a way to bootstrap the training of an RL model (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016), since training with RL from scratch is difficult.",1. Introduction,[0],[0]
"However, this approach does not encourage diversity of the generated samples, and can be problematic when task-specific rewards are incomplete or imperfect.",1. Introduction,[0],[0]
"Designing an appropriate reward definition is highly non-trivial, and often the hand-crafted rewards cannot be fully trusted (Vedantam et al., 2015; Liu et al., 2016).",1. Introduction,[0],[0]
"And yet, relying on data alone can be insufficient when the data itself contains biases, as has been shown for text data
ar X
iv :1
61 1.
02 79
6v 9
[ cs
.L G
] 1
6 O
ct 2
01 7
(Caliskan-Islam et al., 2016), or when domain-specific constraints cannot be encoded directly into MLE training.",1. Introduction,[0],[0]
"By learning a policy that trades off staying close to the data distribution while improving performance on specific metrics, Sequence Tutor reduces both of these problems.
",1. Introduction,[0],[0]
"This paper contributes to the sequence training and RL literature by a) proposing a novel method for combining MLE and RL training; b) showing the connection between KL control and sequence generation; c) deriving the explicit relationships among a generalized variant of Ψ-learning (Rawlik et al., 2012), G-learning (Fox et al., 2015), and Q-learning with log prior augmentation, and being the first to empirically compare these methods and use them with deep neural networks.
",1. Introduction,[0],[0]
We explore the usefulness of our approach for two sequence generation applications.,1. Introduction,[0],[0]
"The first, music generation, is a difficult problem in which the aesthetic beauty of generated sequences cannot be fully captured in a known reward function, but in which models trained purely on data cannot produce well-structured sequences.",1. Introduction,[0],[0]
"Through an empirical study, we show that by imposing rules of music theory on a melody generation model, Sequence Tutor is able to produce melodies which are varied, yet more harmonious, interesting, and rated as significantly more subjectively pleasing than those of the MLE model.",1. Introduction,[0],[0]
"Further, Sequence Tutor is able to significantly reduce unwanted behaviors and failure modes of the original RNN.",1. Introduction,[0],[0]
"The effectiveness of Sequence Tutor is also demonstrated for computational molecular generation, a task in which the goal is to generate novel drug-like molecules with desirable properties by outputting a string representation of the molecule encoding.",1. Introduction,[0],[0]
"However, generating valid molecules can prove difficult, as it is hard for probabilistic models to learn all the constraints that define physically realizable molecules directly from data (Gómez-Bombarelli et al., 2016).",1. Introduction,[0],[0]
"We show that Sequence Tutor is able to yield a higher percentage of valid molecules than the baseline MLE RNN, and the generated molecules score higher on metrics of druglikeness and ease of synthesis.",1. Introduction,[0],[0]
Recent work has attempted to use both MLE and RL in the context of structured prediction.,2. Related Work,[0],[0]
"While the attempts were successful, the problems of maintaining information about the data distribution and diversity in the generated samples were not addressed.",2. Related Work,[0],[0]
"MIXER (Mixed Incremental Cross-Entropy Reinforce) (Ranzato et al., 2015) uses BLEU score as a reward signal to gradually introduce a RL loss to a text translation model.",2. Related Work,[0],[0]
"Bahdanau et al. (2016) applies an actor-critic method and uses BLEU score directly to train a critic network to output the value of each word, where the actor is again initialized with the policy of an
RNN trained with next-step prediction.",2. Related Work,[0],[0]
Li et al. (2016) use RL to improve a pre-trained dialog model with heuristic rewards.,2. Related Work,[0],[0]
These approaches assume that the complete task reward specification is available.,2. Related Work,[0],[0]
"They pre-train a good policy with supervised learning so that RL can be used to learn the true task objective, since it can be difficult to reach convergence when training with pure RL.",2. Related Work,[0],[0]
"However, the original MLE policy of these models is overwritten by the RL training process.",2. Related Work,[0],[0]
"In contrast, Sequence Tutor uses rewards to correct certain properties of the generated data, while learning most information from data and maintaining this information; an important ability when the true reward function is not available or imperfect.
",2. Related Work,[0],[0]
"Reward augmented maximum likelihood (RAML) (Norouzi et al., 2016) is an approach designed to improve MLE training of a translation model by augmenting the ground truth targets with additional outputs that are within a small edit distance, and performing MLE training against those as well.",2. Related Work,[0],[0]
"The authors show that their approach is equivalent to minimizing KL-divergence between an RL exponentiated payoff distribution based on edit distance, and the MLE distribution.",2. Related Work,[0],[0]
"In contrast, our goal is generation rather than prediction, and we train an RL rather than MLE model.",2. Related Work,[0],[0]
"The RAML approach, while an important contribution, is only viable if it is possible to generate additional MLE training samples that are similar in terms of the reward function to the ground truth (i.e. samples within a small edit distance).",2. Related Work,[0],[0]
"However in some domains, including the two explored in this paper, generating similar samples with high reward is not only not possible, but in fact constitutes the entire problem under investigation.
",2. Related Work,[0],[0]
"Finally, our approach is related to KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012), a branch of stochastic optimal control (SOC) (Stengel, 1986).",2. Related Work,[0],[0]
"There is also a connection between this work and Maximum Entropy Inverse RL (Ziebart et al., 2008), which can be seen as KL control with a flat, improper prior.",2. Related Work,[0],[0]
"From KL control, we take inspiration from two off-policy, model-free methods, Ψ-learning (Rawlik et al., 2012) and G-learning (Fox et al., 2015).",2. Related Work,[0],[0]
"Both approaches are derived from a KLregularized RL objective, where an agent maximizes the reward while incurring additional penalty for divergence from some prior policy.",2. Related Work,[0],[0]
"While our methods rely on similar derivations presented in these papers, our methods have different motivations and forms from the original papers.",2. Related Work,[0],[0]
"The original Ψ-learning (Rawlik et al., 2012) restricts the prior policy to be the policy at the previous iteration and solves the original RL objective with conservative, KLregularized policy updates, similar to conservative policy gradient methods (?Peters et al., 2010; Schulman et al., 2015).",2. Related Work,[0],[0]
"The original G-learning (Fox et al., 2015) penalizes divergence from a simple uniform prior policy in order to cope with over-estimation of target Q values.",2. Related Work,[0],[0]
"These tech-
niques have not been applied to deep learning techniques or with RNNs, or as a way to improve a pre-trained MLE model.",2. Related Work,[0],[0]
"Our work is the first to explore these methods in such a context, and includes a Q-learning model with additional cross-entropy reward as a comparable alternative.",2. Related Work,[0],[0]
"To the best of our knowledge, our work is the first to provide comparisons among these three approaches.
",2. Related Work,[0],[0]
There has also been prior work in the domain of generative modeling of music.,2. Related Work,[0],[0]
"Using RNNs for this purpose has been explored in a variety of contexts, including generating Celtic folk music (Sturm et al., 2016), or improvising the blues (Eck & Schmidhuber, 2002).",2. Related Work,[0],[0]
"Often, this involves training the RNN to predict the next note in a monophonic melody; however, as mentioned above, the melodies generated by this model tend to wander and lack musical structure.",2. Related Work,[0],[0]
"Some authors have experimented with encoding musical structure into a hierarchical RNN with layers dedicated to generated the melody, drums, and chords (Chu et al., 2016).",2. Related Work,[0],[0]
"Other approaches have examined RNNs with richer expressivity, latent-variables for notes, or raw audio synthesis (Boulanger-Lewandowski et al., 2012; Gu et al., 2015; Chung et al., 2015).",2. Related Work,[0],[0]
"Recently, Wavenet produced impressive performance in generating music from raw audio using convolutional neural networks with receptive fields at various time scales (van den Oord et al., 2016).",2. Related Work,[0],[0]
"However, the authors themselves note that “even with a receptive field of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, and sound quality” (p. 8).
",2. Related Work,[0],[0]
"Finally, prior work has successfully performed computational molecular generation with deep neural networks.",2. Related Work,[0],[0]
Segler et al. (2017) demonstrated that an LSTM trained on sets of biologically active molecules can be used to generate novel molecules with similar properties.,2. Related Work,[0],[0]
GómezBombarelli,2. Related Work,[0],[0]
et al. (2016) trained a variational autoencoder to learn a compact embedding of molecules encoded using the SMILES notation.,2. Related Work,[0],[0]
"By interpolating in the embedding space and optimizing for desirable metrics of drug quality, the authors were able to decode molecules with high scores on these metrics.",2. Related Work,[0],[0]
"However, producing embeddings that led to valid molecules was difficult; in some cases, as little as 1% of generated sequences proved to be a valid molecule encoding.",2. Related Work,[0],[0]
"In RL, an agent interacts with an environment.",3. Background,[0],[0]
"Given the state of the environment at time t, st, the agent takes an action at according to its policy π(at|st), receives a reward r(st, at), and the environment transitions to state, st+1.The agent’s goal is to maximize reward over a sequence of actions, with a discount factor of γ applied to future rewards.",3. Background,[0],[0]
"The optimal deterministic policy π∗ is known to satisfy the
following Bellman optimality equation,
Q(st, at;π ∗)",3. Background,[0],[0]
"= r(st, at) (1)
+ γEp(st+1|st,at)[maxat+1 Q(st+1, at+1;π
∗)]
where Qπ(st, at) = Eπ[ ∑∞ t′=t",3. Background,[0],[0]
"γ
t′−tr(st′ , at′)] is the Q function of a policy π.",3. Background,[0],[0]
"In Deep Q-learning (Mnih et al., 2013), a neural network called the deep Q-network (DQN) is trained to approximate Q(s, a; θ), using the following objective,
L(θ) =",3. Background,[0],[0]
"Eβ [(r(s, a) + γmax a′ Q(s′, a′; θ−)−Q(s, a; θ))2] (2)
where β is the exploration policy, and θ− is the parameters of the target Q-network (Mnih et al., 2013) that is held fixed during the gradient computation.",3. Background,[0],[0]
"The target Q-network is updated more slowly than the Q-network; for example the moving average of θ can be used as θ−, as proposed by Lillicrap et al. (2015).",3. Background,[0],[0]
Exploration can be performed with either the -greedy method or Boltzmann sampling.,3. Background,[0],[0]
"Additional techniques such as a replay memory (Mnih et al., 2013) are used to stabilize and improve learning.",3. Background,[0],[0]
"Given a trained sequence generation RNN, we would like to impose domain-specific rewards based on the structure and quality of generated sequences, while still maintaining information about typical sequences learned from data.",4. Sequence Tutor,[0],[0]
"Therefore, we treat the trained model as a black-box prior policy, and focus on developing a method that can tune some properties of the model without interfering with the original probability distribution learned from data.",4. Sequence Tutor,[0],[0]
"The separation between the trained sequence model and the tuning method is important, as it prevents RL training from overwriting the original policy.",4. Sequence Tutor,[0],[0]
"To accomplish this task, we propose Sequence Tutor.",4. Sequence Tutor,[0],[0]
"An LSTM trained on data supplies the initial weights for three networks in the model: a recurrent Q-network and target Q-network, and a Reward RNN.",4. Sequence Tutor,[0],[0]
"The Reward RNN is held fixed during training, and treated as a prior policy which can supply the probability of a given token in a sequence as originally learned from data.
",4. Sequence Tutor,[0],[0]
"To apply RL to sequence generation, generating the next token in the sequence is treated as an action a.",4. Sequence Tutor,[0],[0]
"The state of the environment consists of all of the tokens generated so far, i.e. st = {a1, a2, ...at−1}.",4. Sequence Tutor,[0],[0]
"Given action at, we would like the reward rt to combine information about the prior policy p(at|st) as output by the Reward RNN, as well as some domain- or task-specific rewards rT .",4. Sequence Tutor,[0],[0]
Figure 1 illustrates these ideas.,4. Sequence Tutor,[0],[0]
The simplest and most naı̈ve way to incorporate information about the prior policy is to directly augment the taskspecific rewards with the output of the Reward RNN.,4.1. Q-learning with log prior augmentation,[0],[0]
"In this case, the total reward given at time t becomes:
r(s, a) = log p(a|s) + rT (a, s)/c (3)
where c is a constant controlling the emphasis placed on the task-specific rewards.",4.1. Q-learning with log prior augmentation,[0],[0]
"Given the DQN objective in Eq. 2 and modified reward function in Eq. 3, the objective and learned policy are:
L(θ) =",4.1. Q-learning with log prior augmentation,[0],[0]
Eβ,4.1. Q-learning with log prior augmentation,[0],[0]
"[(log p(a|s) + rMT (a, s)/c (4) +",4.1. Q-learning with log prior augmentation,[0],[0]
"γmax
a′ Q(s′, a′; θ−)−Q(s, a; θ))2]
πθ(a|s) = δ(a = arg max a Q(s, a; θ)).",4.1. Q-learning with log prior augmentation,[0],[0]
"(5)
",4.1. Q-learning with log prior augmentation,[0],[0]
"This modified objective forces the model to learn that the most valuable actions are those that conform to the music theory rules, but still have high probability in the original data.",4.1. Q-learning with log prior augmentation,[0],[0]
"However, the DQN learns a deterministic policy (as shown in Eq. 5), which is not ideal for sequence generation.",4.1. Q-learning with log prior augmentation,[0],[0]
"Therefore, after the model is trained, we generate sequences by sampling from the softmax function applied to the predicted Q-values.",4.1. Q-learning with log prior augmentation,[0],[0]
"If we cast sequence generation as a sequential decisionmaking problem and the desired sequence properties in terms of target rewards, the problem can be expressed as a KL control problem for a non-Markovian system.",4.2. KL Control for Sequence Generation,[0],[0]
"KL control (Todorov, 2007; Kappen et al., 2012; Rawlik et al., 2012) is a branch of stochastic optimal control (SOC) (Stengel, 1986), which studies an RL, or control, problem in which the agent tries maximizing its task reward while minimizing deviation from a prior policy.",4.2. KL Control for Sequence Generation,[0],[0]
"For our purposes, we treat a trained MLE sequence model as the prior policy, and thus the objective is to train a new policy, or sequence model, to maximize some rewards while keeping close to the original MLE model.",4.2. KL Control for Sequence Generation,[0],[0]
"We show that such KL control formulation allows us to derive additional
variants of Q-learning with minimal modifications, which give rise to different properties.",4.2. KL Control for Sequence Generation,[0],[0]
Let τ,4.2. KL Control for Sequence Generation,[0],[0]
=,4.2. KL Control for Sequence Generation,[0],[0]
{,4.2. KL Control for Sequence Generation,[0],[0]
"a1, a2, ..., at−1} represent the sequence, r(τ) the reward of the sequence, p(τ) be the prior distribution over τ given by the trained sequence model, and q(τ) be the policy of the Sequence Tutor model.",4.2. KL Control for Sequence Generation,[0],[0]
"The objective is then to maximize the following expression with respect to q(τ), where DKL represents the KL divergence of distributions:
L(q) = Eq(τ)[r(τ)]/c−DKL[q(τ)||p(τ)].",4.2. KL Control for Sequence Generation,[0],[0]
"(6)
We express q(τ) in terms of a parametrized recurrent policy πθ(at|st), i.e. q(τ) = ∏T t=1 πθ(at|st) where st = {a1, a2, ..., at−1}, indicates that the system is nonMarkovian.",4.2. KL Control for Sequence Generation,[0],[0]
The prior policy is expressed similarly p(τ) =∏T t=1 p(at|st).,4.2. KL Control for Sequence Generation,[0],[0]
"The reinforcement learning objective is the following, where Eπ[·] below indicates expectation with respect to sequences sampled from π, L(θ) =",4.2. KL Control for Sequence Generation,[0],[0]
"Eπ[ ∑
t
r(st, at)/c+ log p(at|st)− log πθ(at|st)]
",4.2. KL Control for Sequence Generation,[0],[0]
"The difference between this equation and Eq. 4 is that an entropy regularizer is now included, and thus the optimal policy is no longer deterministic.",4.2. KL Control for Sequence Generation,[0],[0]
"Below, we derive general temporal-difference based methods for the KL-control problem for sequence generation.",4.2. KL Control for Sequence Generation,[0],[0]
"Let V π(st) define the recurrent value function of the policy πθ, given by,
V π(st) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eπ[ ∞∑
t′=t
r(st′ , at′)/c+ log p(at′ |st′) (7)
− log π(at′ |st′)]
We define the generalized Ψ function, analogous toQ function for KL control, as below.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"We call this generalized Ψ function, as it was introduced in deriving Ψ-learning (Rawlik et al., 2012), and the following derivation is a generalization to the Ψ-learning algorithm.
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"r(st, at)/c+ log p(at|st) + V π(st+1) (8)
Note that the state st+1 is given deterministically by st = {a1, a2, ..., at−1} and at for sequence modeling, and thus the expressions do not contain the usual stochastic dynamics p(st+1|st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
The value function V π(st),4.3. Recurrent Generalized Ψ-learning,[0],[0]
"can be recursively expressed in terms of Ψπ ,
V π(st) = Eπ[Ψπ(st, at)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"+ H[π(.|st)] (9) = Eπ[Ψπ(st, at)− log π(at|st)]",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(10)
Fixing Ψ(st, at) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψπ(st, at) and constraining π to be a probability distribution, the optimal greedy policy update
π∗ can be derived, along with the corresponding optimal value function,
π∗(at|st) ∝ eΨ(st,at) (11) V ∗(st) = log ∑
at
eΨ(st,at) (12)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Given Eq. 8 and 12, the following Bellman optimality equation for generalized Ψ function is derived.
Ψ∗(st, at) = r(st, at)/c+ log p(at|st) + log ∑
at+1
exp(Ψ∗(st+1, at+1))",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"(13)
The Ψ-learning loss directly follows:
LΨ(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Eβ [(Ψθ(st, at)− yt)2] where (14) yt = log p(at|st) + r(st, at)/c+ γ log ∑
a′
eΨ −(st+1,a ′)
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"β corresponds to sampling sequence trajectories from an arbitrary distribution; in practice, the experience replay (Mnih et al., 2013).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Ψ− indicates that it uses the target network.,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Ψθ, i.e. πθ, is parametrized with recurrent neural networks, and for discrete actions, πθ is effectively a softmax layer on top of Ψθ.
4.4.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Recurrent G-learning
We can derive another algorithm by parametrizing Ψθ indirectly by Ψθ(st, at) = log p(at|st) +",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Gθ(st, at).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Substituting into above equations, we get a different temporaldifference method:
LG(θ) =",4.3. Recurrent Generalized Ψ-learning,[0],[0]
Eβ,4.3. Recurrent Generalized Ψ-learning,[0],[0]
"[(Gθ(st, at)− yt)2] where (15) yt = r(st, at)/c+ γ log ∑
a′
p(a′|st+1)eG −(st+1,a ′) and
πθ(at|st) ∝",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"p(at|st) exp(Gθ(st, at))
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"This formulation corresponds to G-learning (Fox et al., 2015), which can thus be seen as a special case of generalized Ψ-learning.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Unlike Ψ learning, which directly builds knowledge about the prior policy into the Ψ function, theG-function does not give the policy directly but instead needs to be dynamically mixed with the prior policy probabilities.",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"While this computation is straight-forward for discrete action domains as here, extensions to continuous action domains require additional considerations such as normalizability of Ψ-function parametrization (Gu et al., 2016).
",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"The KL control-based derivation also has another benefit in that the stochastic policies can be directly used as an exploration strategy, instead of heuristics such as -greedy or additive noise (Mnih et al., 2013; Lillicrap et al., 2015).",4.3. Recurrent Generalized Ψ-learning,[0],[0]
"Following from the above derivations, we compare three methods for implementing Sequence Tutor: Q-learning with log prior augmentation (based on Eq. 4), generalized Ψ-learning (based on Eq. 14), and G-learning (based on Eq. 15).",4.5. Sequence Tutor implementation,[0],[0]
"A pre-trained sequence generation LSTM is used as the Reward RNN, to supply the cross entropy reward in Q-learning and the prior policy in G- and generalized Ψlearning.",4.5. Sequence Tutor implementation,[0],[0]
"These approaches are compared to both the original performance of the MLE RNN, and a model trained using only RL and no prior policy.",4.5. Sequence Tutor implementation,[0],[0]
"Model evaluation is performed every 100,000 training epochs, by generating 100 sequences and assessing the average rT and log p(a|s).",4.5. Sequence Tutor implementation,[0],[0]
"The code for Sequence Tutor, including a checkpointed version of the trained melody RNN is available at https://github.com/tensorflow/magenta/ tree/master/magenta/models/rl_tuner.",4.5. Sequence Tutor implementation,[0],[0]
"Music compositions adhere to relatively well-defined structural rules, making music an interesting sequence generation challenge.",5. Experiment I: Melody Generation,[0],[0]
"For example, music theory tells that groups of notes belong to keys, chords follow progressions, and songs have consistent structures made up of musical phrases.",5. Experiment I: Melody Generation,[0],[0]
"Our research question is therefore whether such constraints can be learned by an RNN, while still allowing it to maintain note probabilities learned from data.
",5. Experiment I: Melody Generation,[0],[0]
"To test this hypothesis, we developed several rules that we believe describe pleasant-sounding melodies, taking inspiration from a text on melodic composition (Gauldin, 1995).",5. Experiment I: Melody Generation,[0],[0]
"We do not claim these characteristics are exhaustive or strictly necessary for good composition; rather, they are an incomplete measure of task success that can simply guide the model towards traditional composition structure.",5. Experiment I: Melody Generation,[0],[0]
It is therefore crucial that the Sequence Tutor approach allows the model to retain knowledge learned from real songs in the training data.,5. Experiment I: Melody Generation,[0],[0]
"The rules comprising the music-specific reward function rT (a, s) encourage melodies to: stay in key, start with the tonic note, resolve melodic leaps, have a unique maximum and minimum note, prefer harmonious intervals, play motifs and repeat them, have a low autocorrelation at a lag of 1, 2, and 3 beats, and avoid excessively repeating notes.",5. Experiment I: Melody Generation,[0],[0]
"Interestingly, while excessively repeating tokens is a common problem in RNN sequence generation models, avoiding this behavior is also Gauldin’s first rule of melodic composition (p. 42).
",5. Experiment I: Melody Generation,[0],[0]
"To train the model, we begin by extracting monophonic melodies from a corpus of 30,000 MIDI songs and encoding them as one-hot sequences of notes1.",5. Experiment I: Melody Generation,[0],[0]
"These melodies
1More information about both the note encoding and the reward metrics is available in the supplementary material.
",5. Experiment I: Melody Generation,[0],[0]
are then used to train an LSTM with one layer of 100 cells.,5. Experiment I: Melody Generation,[0],[0]
"Optimization was performed with Adam (Kingma & Ba, 2014), a batch size of 128, initial learning rate of .5, and a stepwise learning rate decay of 0.85 every 1000 steps.",5. Experiment I: Melody Generation,[0],[0]
"Gradients were clipped to ensure the L2 norm was less than 5, and weight regularization was applied with β = 2.5×10−5.",5. Experiment I: Melody Generation,[0],[0]
"Finally, the losses for the first 8 notes of each sequence were not used to train the model, since it cannot reasonably be expected to accurately predict them with no context.",5. Experiment I: Melody Generation,[0],[0]
The trained RNN eventually obtained a validation accuracy of 92% and a log perplexity score of .2536.,5. Experiment I: Melody Generation,[0],[0]
"This model was used as described above to initialize the three sub-networks in the Sequence Tutor model.
",5. Experiment I: Melody Generation,[0],[0]
"The Sequence Tutor model was trained using a similar configuration to the one above, except with a batch size of 32, and a reward discount factor of γ=.5.",5. Experiment I: Melody Generation,[0],[0]
"The TargetQ-network’s weights θ− were gradually updated towards those of the Q-network (θ) according to the formula (1 − η)θ− + ηθ, where η = .01 is the Target-Q-network update rate.",5. Experiment I: Melody Generation,[0],[0]
"A strength of our model is that the influence of data and task-specific rewards can be explicitly controlled by adjusting the temperature parameter c. We replicated our results for a number of settings for c; we present results for c=.5 below because we believe them to be most musically pleasing, however additional results are available at https://goo.gl/cTZy8r.",5. Experiment I: Melody Generation,[0],[0]
"Similarly, we replicated the results using both -greedy and Boltzmann exploration, and present the results using -greedy exploration below.",5. Experiment I: Melody Generation,[0],[0]
"Table 1 provides quantitative results in the form of performance on the music theory rules to which we trained the model to adhere; for example, we can assess the fraction of notes played by the model which belonged to the correct key, or the fraction of melodic leaps that were resolved.",5.1. Results,[0],[0]
"The statistics were computed by randomly generating 100,000 melodies from each model.
",5.1. Results,[0],[0]
"The results above demonstrate that the application of RL is able to correct almost all of the targeted “bad behaviors” of the MLE RNN, while improving performance on the desired metrics.",5.1. Results,[0],[0]
"For example, the original LSTM model was extremely prone to repeating the same note; after applying RL, we see that the number of notes belonging to some excessively repeated segment has dropped from 63% to nearly 0% in all of the Sequence Tutor models.",5.1. Results,[0],[0]
"While the metrics for the G model did not improve as consistently, the Q and Ψ models successfully learned to adhere to most of the imposed rules.",5.1. Results,[0],[0]
The degree of improvement on these metrics is related to the magnitude of the reward given for the behavior.,5.1. Results,[0],[0]
"For example, a strong penalty of -100 was applied each time a note was excessively repeated, while a reward of only 3 was applied at the end of a melody
for unique extrema notes (which most likely explains the lack of improvement on this metric).",5.1. Results,[0],[0]
"The reward values could be adjusted to improve the metrics further, however we found that these values produced pleasant melodies.
",5.1. Results,[0],[0]
"While the metrics indicate that the targeted behaviors of the RNN have improved, it is not clear whether the models have retained information about the training data.",5.1. Results,[0],[0]
"Figure 2a plots the average log p(a|s) as produced by the Reward RNN for melodies generated by the models every 100,000 training epochs; Figure 2b plots the average rT .",5.1. Results,[0],[0]
"Included in the plot is an RL only model trained using only the music theory rewards, with no information about log p(a|s).",5.1. Results,[0],[0]
"Since each model is initialized with the weights of the trained MLE RNN, we see that as the models quickly learn to adhere to the music theory constraints, log p(a|s) falls from its initial point.",5.1. Results,[0],[0]
"For the RL only model, log p(a|s) reaches an average of -3.65, which is equivalent to an average p(a|s) of approximately 0.026, or essentially a random policy over the 38 actions with respect to the distribution defined by the Reward RNN.",5.1. Results,[0],[0]
"Figure 2a shows that each of our models (Q, Ψ, and G) attain higher log p(a|s) values than this baseline, indicating they have maintained information about the data distribution, even over 3,000,000 training steps.",5.1. Results,[0],[0]
"TheG-learning implementation scores highest on this metric, at the cost of slightly lower average rT .",5.1. Results,[0],[0]
This compromise between data probability and adherence to music theory could explain the difference in the G model’s performance on the music theory metrics in Table 1.,5.1. Results,[0],[0]
"Finally, we have verified that by increasing the c parameter it is possible to train all the models to have even higher average log p(a|s), but found that c = 0.5 produced melodies that sounded better subjectively.
",5.1. Results,[0],[0]
The question remains whether the RL-tutored models actually produce more pleasing melodies.,5.1. Results,[0],[0]
"The sample melodies used for the study are available here: goo.gl/XIYt9m;
we encourage readers to judge their quality for themselves.",5.1. Results,[0],[0]
"To more formally answer this question, we conducted a user study via Amazon Mechanical Turk in which participants were asked to rate which of two randomly selected melodies they preferred on a Likert scale.",5.1. Results,[0],[0]
A total of 192 ratings were collected; each model was involved in 92 of these comparisons.,5.1. Results,[0],[0]
Figure 3 plots the number of comparisons in which a melody from each model was selected as the most musically pleasing.,5.1. Results,[0],[0]
"A Kruskal-Wallis H test of the ratings showed that there was a statistically significant difference between the models, χ2(3) = 109.480, p < 0.001.",5.1. Results,[0],[0]
"Mann-Whitney U post-hoc tests revealed that the melodies from all three Sequence Tuner models (Q, Ψ, and G) had significantly higher ratings than the melodies of the MLE RNN, p < .001.",5.1. Results,[0],[0]
"The Q and Ψ melodies were also rated as significantly more pleasing than those of the G model, but did not differ significantly from each other.",5.1. Results,[0],[0]
"Listening to the samples produced by the MLE RNN reveals that they are sometimes dischordant and usually dull; the model tends to place rests frequently, repeat the same
0 10 20 30 40 50 60 70 80 90
Number of times preferred
Ψ
Q
G
Note RNN
M o d e l
Figure 3: The number of times a melody from each model was selected as most musically pleasing.",5.2. Discussion,[0],[0]
Error bars reflect the std. dev.,5.2. Discussion,[0],[0]
"of a binomial distribution fit to the binary win/loss data from each model.
note, and produce melodies with little variation.",5.2. Discussion,[0],[0]
"In contrast, the melodies produced by the Sequence Tutor models are more varied and interesting.",5.2. Discussion,[0],[0]
"The G model tends to produce energetic and chaotic melodies, which include sequences of repeated notes.",5.2. Discussion,[0],[0]
"This repetition is likely because theG policy as defined in Eq. 15 directly mixes p(a|s) with the output of the G network, and the MLE RNN strongly favours repeating notes.",5.2. Discussion,[0],[0]
The most pleasant melodies are generated by the Q and Ψ models.,5.2. Discussion,[0],[0]
"These melodies stay firmly in key and frequently choose more harmonious interval steps, leading to melodic and pleasant melodies.",5.2. Discussion,[0],[0]
"However, it is clear they have retained information about the training data; for example, the sample q2.wav in the sample directory ends with a seemingly familiar riff.
",5.2. Discussion,[0],[0]
"While we acknowledge that the monophonic melodies generated by these models — which are based on highly simplistic rules of melodic composition — do not approach the level of artistic merit of human composers, we believe this study provides a proof-of-concept that encoding even incomplete and partially specified domain knowledge using our method can help the outputs of an LSTM adhere to a more consistent structure.",5.2. Discussion,[0],[0]
"The musical complexity of the songs is limited not just by the heuristic rules, but also by the simple monophonic encoding, which cannot represent the dynamics and expressivity of a musical performance.",5.2. Discussion,[0],[0]
"Although these melodies cannot surpass those of human musicians, attempting to train a model to generate aesthetically pleasing outputs in the absence of a better metric of human taste than log-likelihood is a problem of broader interest to the artificial intelligence community.",5.2. Discussion,[0],[0]
"As a follow-on experiment, we tested the effectiveness of Sequence Tutor for generating a higher yield of synthet-
ically accessible drug-like molecules.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Organic molecules can be encoded using the commonly used SMILES representation (Weininger, 1970).",6. Experiment II: Computational Molecular Generation,[0],[0]
"For example, amphetamine can be encoded as ‘CC(N)Cc1ccccc1’, while creatine is ‘CN(CC(=O)O)C(=N)N’.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Using this character encoding, it is straightforward to train an MLE RNN to generate sequences of SMILES characters; we trained such a model using the same settings as described above for the melody MLE RNN.",6. Experiment II: Computational Molecular Generation,[0],[0]
"However, only about a third of the molecules generated using this simple approach are actually valid SMILES encodings.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Further, this approach does not directly optimize for metrics of molecule or drug quality.",6. Experiment II: Computational Molecular Generation,[0],[0]
"These metrics include: a) the water-octanol partition coefficient (logP), which is important in assessing the druglikeness of a molecule; b) synthetic accessibility (SA) (Ertl & Schuffenhauer, 2009), a score from 1-10 that is lower if the molecule is easier to synthesize; and c) Quantitative Estimation of Drug-likeness (QED) (Bickerton et al., 2012), a more subjective measure of drug-likeness based on abstract ideas of medicinal aesthetics.
",6. Experiment II: Computational Molecular Generation,[0],[0]
"To optimize for these metrics, while simultaneously improving the percent yield of valid molecules from the RNN, we constructed a reward function that incentivizes validity, logP, SA, and QED using an open-source library called RDkit (http://www.rdkit.org/).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Included in the reward function was a penalty for molecules with unrealistically large carbon rings (size larger than 6), as per previous work (Gómez-Bombarelli et al., 2016).",6. Experiment II: Computational Molecular Generation,[0],[0]
"Finally, after observing that the model could exploit the reward function by generating the simple molecule ‘N’ repeatedly, or ‘CCCCC...’ (which produces an unrealistically high logP value), we added penalties for sequences shorter than, or with more consecutive carbon atoms than, any sequence in the training data.",6. Experiment II: Computational Molecular Generation,[0],[0]
"Sequence Tutor was then trained using these rewards, the pre-trained MLE RNN, and similar settings to the first experiment, except with -greedy exploration with = .01, a batch size of 512, and discount factor γ = .95.",6. Experiment II: Computational Molecular Generation,[0],[0]
"For this experiment, we also made use of prioritized experience replay (Schaul et al., 2015) to allow the model to more frequently learn from relatively rare valid samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"A value of c = 2.85 led to a higher yield of valid molecules with high metrics, but still encouraged the diversity of generated samples.",6. Experiment II: Computational Molecular Generation,[0],[0]
"As the Ψ algorithm produced the best results for the music generation task, we focused on using this technique for generating molecules.",6.1. Results and discussion,[0],[0]
"Table 2 shows the performance of this model against the original MLE model according to metrics of validity, drug-likeness, and synthetic accessibility.",6.1. Results and discussion,[0],[0]
"Once again, Sequence Tutor is able to significantly improve almost all of the targeted metrics.",6.1. Results and discussion,[0],[0]
"However, it should be noted that the Sequence Tutor model
tends to produce simplistic molecules involving more carbon atoms than the MLE baseline; e.g. Sequence Tutor may produce ‘SNCc1ccccc1’, while the MLE produces ‘C(=O)c1ccc(S(=O)(=O)N(C)C)c(Cl)c1’, which is the reason for the Sequence Tutor model’s lower QED scores.",6.1. Results and discussion,[0],[0]
"This effect is due to the fact that simple sequences are more likely to be valid, have high logP and SA scores, and carbon is highly likely under the distribution learned by the MLE model.",6.1. Results and discussion,[0],[0]
A higher reward for QED and further improvement of the task-specific rewards based on domain knowledge could help to alleviate these problems.,6.1. Results and discussion,[0],[0]
"Overall, the fact that Sequence Tutor can improve the percentage of valid molecules produced as well as the logP and synthetic accessibility scores serves as a proof-of-concept that Sequence Tutor may be valuable in a number of domains for imparting domain knowledge onto a sequence predictor.",6.1. Results and discussion,[0],[0]
"We have derived a novel sequence learning framework which uses RL to correct properties of sequences generated by an RNN, while maintaining information learned from MLE training on data, and ensuring the diversity of generated samples.",7. Conclusion and Future Work,[0],[0]
"By demonstrating a connection between our sequence generation approach and KL-control, we have derived three novel RL-based methods for optimizing sequence generation models.",7. Conclusion and Future Work,[0],[0]
"These methods were empirically compared in the context of a music generation task, and further demonstrated on a computational molecular generation task.",7. Conclusion and Future Work,[0],[0]
"Sequence Tutor showed promising results in terms of both adherence to task-specific rules, and subjective quality of the generated sequences.
",7. Conclusion and Future Work,[0],[0]
"We believe the Sequence Tutor approach of using RL to refine RNN models could be promising for a number of applications, including the reduction of bias in deep learning models.",7. Conclusion and Future Work,[0],[0]
"While manually writing a domain-specific reward function may seem unappealing, that approach is limited by the quality of the data that can be collected, and besides, even state-of-the-art sequence models often fail to learn all the aspects of high-level structure (van den Oord et al., 2016; Graves, 2013).",7. Conclusion and Future Work,[0],[0]
"Further, the data may contain hidden biases, as has been demonstrated for popular language models (Caliskan-Islam et al., 2016).",7. Conclusion and Future Work,[0],[0]
"In contrast to relying solely on possibly biased data, our approach allows
for encoding high-level domain knowledge into the RNN, providing a general, alternative tool for training sequence models.",7. Conclusion and Future Work,[0],[0]
"This work was supported by Google Brain, the MIT Media Lab Consortium, and Canada’s Natural Sciences and Engineering Research Council (NSERC).",ACKNOWLEDGMENTS,[0],[0]
"We thank Greg Wayne, Sergey Levine, and Timothy Lillicrap for helpful discussions on RL and stochastic optimal control and Kyle Kastner and Tim Cooijmans for valuable insight into training RNNs.",ACKNOWLEDGMENTS,[0],[0]
"This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity.",abstractText,[0],[0]
"An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy.",abstractText,[0],[0]
Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN.,abstractText,[0],[0]
"To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control.",abstractText,[0],[0]
"The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation.",abstractText,[0],[0]
"For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.",abstractText,[0],[0]
Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control,title,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) is a deep learningbased method for translation that has recently shown promising results as an alternative to statistical ap-
proaches.",1 Introduction,[0],[0]
"NMT systems directly model the probability of the next word in the target sentence simply by conditioning a recurrent neural network on the source sentence and previously generated target words.
",1 Introduction,[0],[0]
"While both simple and surprisingly accurate, NMT systems typically need to have very high capacity in order to perform well: Sutskever et al. (2014) used a 4-layer LSTM with 1000 hidden units per layer (herein 4×1000) and Zhou et al.",1 Introduction,[0],[0]
(2016) obtained state-of-the-art results on English → French with a 16-layer LSTM with 512 units per layer.,1 Introduction,[0],[0]
"The sheer size of the models requires cutting-edge hardware for training and makes using the models on standard setups very challenging.
",1 Introduction,[0],[0]
"This issue of excessively large networks has been observed in several other domains, with much focus on fully-connected and convolutional networks for multi-class classification.",1 Introduction,[0],[0]
"Researchers have particularly noted that large networks seem to be necessary for training, but learn redundant representations in the process (Denil et al., 2013).",1 Introduction,[0],[0]
Therefore compressing deep models into smaller networks has been an active area of research.,1 Introduction,[0],[0]
"As deep learning systems obtain better results on NLP tasks, compression also becomes an important practical issue with applications such as running deep learning models for speech and translation locally on cell phones.
",1 Introduction,[0],[0]
Existing compression methods generally fall into two categories: (1) pruning and (2) knowledge distillation.,1 Introduction,[0],[0]
"Pruning methods (LeCun et al., 1990; He et al., 2014; Han et al., 2016), zero-out weights or entire neurons based on an importance criterion: LeCun et al. (1990) use (a diagonal approximation to)
",1 Introduction,[0],[0]
"ar X
iv :1
60 6.
07 94
7v 4
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
2 Se
p 20
the Hessian to identify weights whose removal minimally impacts the objective function, while Han et al. (2016) remove weights based on thresholding their absolute values.",1 Introduction,[0],[0]
"Knowledge distillation approaches (Bucila et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) learn a smaller student network to mimic the original teacher network by minimizing the loss (typically L2 or cross-entropy) between the student and teacher output.
",1 Introduction,[0],[0]
"In this work, we investigate knowledge distillation in the context of neural machine translation.",1 Introduction,[0],[0]
We note that NMT differs from previous work which has mainly explored non-recurrent models in the multiclass prediction setting.,1 Introduction,[0],[0]
"For NMT, while the model is trained on multi-class prediction at the word-level, it is tasked with predicting complete sequence outputs conditioned on previous decisions.",1 Introduction,[0],[0]
"With this difference in mind, we experiment with standard knowledge distillation for NMT and also propose two new versions of the approach that attempt to approximately match the sequence-level (as opposed to word-level) distribution of the teacher network.",1 Introduction,[0],[0]
"This sequence-level approximation leads to a simple training procedure wherein the student network is trained on a newly generated dataset that is the result of running beam search with the teacher network.
",1 Introduction,[0],[0]
"We run experiments to compress a large state-ofthe-art 4 × 1000 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a 2× 500 LSTM that roughly matches the performance of the full system.",1 Introduction,[0],[0]
We see similar results compressing a 2 × 500 model down to 2 × 100 on a smaller data set.,1 Introduction,[0],[0]
"Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time.",1 Introduction,[0],[0]
As a result we are able to perform greedy decoding on the 2 × 500 model 10 times faster than beam search on the 4 × 1000 model with comparable performance.,1 Introduction,[0],[0]
"Our student models can even be run efficiently on a standard smartphone.1 Finally, we apply weight pruning on top of the student network to obtain a model that has 13× fewer parameters than the original teacher model.",1 Introduction,[0],[0]
"We have released all the code for the models described in this paper.2
1https://github.com/harvardnlp/nmt-android 2https://github.com/harvardnlp/seq2seq-attn",1 Introduction,[0],[0]
"Let s = [s1, . . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
", sI ] and t =",2.1 Sequence-to-Sequence with Attention,[0],[0]
"[t1, . .",2.1 Sequence-to-Sequence with Attention,[0],[0]
.,2.1 Sequence-to-Sequence with Attention,[0],[0]
", tJ ] be (random variable sequences representing)",2.1 Sequence-to-Sequence with Attention,[0],[0]
"the source/target sentence, with I and J respectively being the source/target lengths.",2.1 Sequence-to-Sequence with Attention,[0],[0]
"Machine translation involves finding the most probable target sentence given the source:
argmax t∈T
p(t | s)
where T is the set of all possible sequences.",2.1 Sequence-to-Sequence with Attention,[0],[0]
NMT models parameterize p(t | s) with an encoder neural network which reads the source sentence and a decoder neural network which produces a distribution over the target sentence (one word at a time) given the source.,2.1 Sequence-to-Sequence with Attention,[0],[0]
"We employ the attentional architecture from Luong et al. (2015), which achieved state-ofthe-art results on English→ German translation.3",2.1 Sequence-to-Sequence with Attention,[0],[0]
Knowledge distillation describes a class of methods for training a smaller student network to perform better by learning from a larger teacher network (in addition to learning from the training data set).,2.2 Knowledge Distillation,[0],[0]
"We generally assume that the teacher has previously been trained, and that we are estimating parameters for the student.",2.2 Knowledge Distillation,[0],[0]
Knowledge distillation suggests training by matching the student’s predictions to the teacher’s predictions.,2.2 Knowledge Distillation,[0],[0]
"For classification this usually means matching the probabilities either via L2 on the log scale (Ba and Caruana, 2014) or by crossentropy (Li et al., 2014; Hinton et al., 2015).
",2.2 Knowledge Distillation,[0],[0]
"Concretely, assume we are learning a multi-class classifier over a data set of examples of the form (x, y) with possible classes V .",2.2 Knowledge Distillation,[0],[0]
"The usual training criteria is to minimize NLL for each example from the training data,
LNLL(θ) =",2.2 Knowledge Distillation,[0],[0]
"− |V|∑ k=1 1{y = k} log p(y = k |x; θ)
where 1{·} is the indicator function and p the distribution from our model (parameterized by θ).
",2.2 Knowledge Distillation,[0],[0]
"3Specifically, we use the global-general attention model with the input-feeding approach.",2.2 Knowledge Distillation,[0],[0]
"We refer the reader to the original paper for further details.
",2.2 Knowledge Distillation,[0],[0]
"This objective can be seen as minimizing the crossentropy between the degenerate data distribution (which has all of its probability mass on one class) and the model distribution p(y |x; θ).
",2.2 Knowledge Distillation,[0],[0]
"In knowledge distillation, we assume access to a learned teacher distribution q(y |x; θT ), possibly trained over the same data set.",2.2 Knowledge Distillation,[0],[0]
"Instead of minimizing cross-entropy with the observed data, we instead minimize the cross-entropy with the teacher’s probability distribution,
LKD(θ; θT )",2.2 Knowledge Distillation,[0],[0]
"=− |V|∑ k=1 q(y = k |x; θT )×
log p(y = k |x; θ)
where θT parameterizes the teacher distribution and remains fixed.",2.2 Knowledge Distillation,[0],[0]
"Note the cross-entropy setup is identical, but the target distribution is no longer a sparse distribution.4 Training on q(y |x; θT ) is attractive since it gives more information about other classes for a given data point (e.g. similarity between classes) and has less variance in gradients (Hinton et al., 2015).
",2.2 Knowledge Distillation,[0],[0]
4 In some cases the entropy of the teacher/student distribution is increased by annealing it with a temperature term τ,2.2 Knowledge Distillation,[0],[0]
"> 1
p̃(y |x) ∝",2.2 Knowledge Distillation,[0],[0]
"p(y |x) 1 τ
After testing τ ∈ {1, 1.5, 2} we found that τ = 1 worked best.
",2.2 Knowledge Distillation,[0],[0]
"Since this new objective has no direct term for the training data, it is common practice to interpolate between the two losses,
L(θ; θT )",2.2 Knowledge Distillation,[0],[0]
= (1− α)LNLL(θ),2.2 Knowledge Distillation,[0],[0]
"+ αLKD(θ; θT )
where α is mixture parameter combining the one-hot distribution and the teacher distribution.",2.2 Knowledge Distillation,[0],[0]
The large sizes of neural machine translation systems make them an ideal candidate for knowledge distillation approaches.,3 Knowledge Distillation for NMT,[0],[0]
In this section we explore three different ways this technique can be applied to NMT.,3 Knowledge Distillation for NMT,[0],[0]
"NMT systems are trained directly to minimize word NLL, LWORD-NLL, at each position.",3.1 Word-Level Knowledge Distillation,[0],[0]
"Therefore if we have a teacher model, standard knowledge distillation for multi-class cross-entropy can be applied.",3.1 Word-Level Knowledge Distillation,[0],[0]
"We define this distillation for a sentence as,
LWORD-KD = − J∑
j=1 |V|∑ k=1 q(tj = k | s, t<j)×
log p(tj = k",3.1 Word-Level Knowledge Distillation,[0],[0]
"| s, t<j)
where V is the target vocabulary set.",3.1 Word-Level Knowledge Distillation,[0],[0]
"The student can further be trained to optimize the mixture of
LWORD-KD and LWORD-NLL.",3.1 Word-Level Knowledge Distillation,[0],[0]
"In the context of NMT, we refer to this approach as word-level knowledge distillation and illustrate this in Figure 1 (left).",3.1 Word-Level Knowledge Distillation,[0],[0]
Word-level knowledge distillation allows transfer of these local word distributions.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Ideally however, we would like the student model to mimic the teacher’s actions at the sequence-level.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence distribution is particularly important for NMT, because wrong predictions can propagate forward at testtime.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"First, consider the sequence-level distribution specified by the model over all possible sequences t ∈ T ,
p(t | s) = J∏
j=1
p(tj | s, t<j)
for any length J .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The sequence-level negative loglikelihood for NMT then involves matching the onehot distribution over all complete sequences,
LSEQ-NLL = − ∑ t∈T 1{t = y} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"− J∑
j=1 |V|∑ k=1 1{yj = k} log p(tj = k",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s, t<j)
= LWORD-NLL
where y =",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"[y1, . . .",3.2 Sequence-Level Knowledge Distillation,[0],[0]
", yJ ] is the observed sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Of course, this just shows that from a negative log likelihood perspective, minimizing word-level NLL and sequence-level NLL are equivalent in this model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
But now consider the case of sequence-level knowledge distillation.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"As before, we can simply replace the distribution from the data with a probability distribution derived from our teacher model.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"However, instead of using a single word prediction, we use q(t | s) to represent the teacher’s sequence distribution over the sample space of all possible sequences,
LSEQ-KD = − ∑ t∈T q(t | s) log p(t | s)
Note that LSEQ-KD is inherently different from LWORD-KD, as the sum is over an exponential number of terms.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Despite its intractability, we posit
that this sequence-level objective is worthwhile.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
It gives the teacher the chance to assign probabilities to complete sequences and therefore transfer a broader range of knowledge.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"We thus consider an approximation of this objective.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Our simplest approximation is to replace the teacher distribution q with its mode,
q(t | s) ∼",3.2 Sequence-Level Knowledge Distillation,[0],[0]
1{t,3.2 Sequence-Level Knowledge Distillation,[0],[0]
=,3.2 Sequence-Level Knowledge Distillation,[0],[0]
argmax,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"t∈T q(t | s)}
Observing that finding the mode is itself intractable, we use beam search to find an approximation.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"The loss is then
LSEQ-KD",3.2 Sequence-Level Knowledge Distillation,[0],[0]
≈,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"− ∑ t∈T 1{t = ŷ} log p(t | s)
=",3.2 Sequence-Level Knowledge Distillation,[0],[0]
− log p(t = ŷ,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"| s)
where ŷ is now the output from running beam search with the teacher model.
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Using the mode seems like a poor approximation for the teacher distribution q(t | s), as we are approximating an exponentially-sized distribution with a single sample.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"However, previous results showing the effectiveness of beam search decoding for NMT lead us to belief that a large portion of q’s mass lies in a single output sequence.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"In fact, in experiments we find that with beam of size 1, q(ŷ | s) (on average) accounts for 1.3% of the distribution for German→ English, and 2.3% for Thai→ English (Table 1: p(t = ŷ)).5
To summarize, sequence-level knowledge distillation suggests to: (1) train a teacher model, (2) run beam search over the training set with this model, (3) train the student network with cross-entropy on this new dataset.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Step (3) is identical to the word-level NLL process except now on the newly-generated data set.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This is shown in Figure 1 (center).
",3.2 Sequence-Level Knowledge Distillation,[0],[0]
5Additionally there are simple ways to better approximate q(t | s).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"One way would be to consider a K-best list from beam search and renormalizing the probabilities,
q(t | s) ∼ q(t | s)∑ t∈TK q(t | s)
where TK is the K-best list from beam search.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
"This would increase the training set by a factor of K. A beam of size 5 captures 2.8% of the distribution for German → English, and 3.8% for Thai → English.",3.2 Sequence-Level Knowledge Distillation,[0],[0]
Another alternative is to use a Monte Carlo estimate and sample from the teacher model (since LSEQ-KD = Et∼q(t | s)[− log p(t | s) ]).,3.2 Sequence-Level Knowledge Distillation,[0],[0]
However in practice we found the (approximate) mode to work well.,3.2 Sequence-Level Knowledge Distillation,[0],[0]
"Next we consider integrating the training data back into the process, such that we train the student model as a mixture of our sequence-level teachergenerated data (LSEQ-KD) with the original training data (LSEQ-NLL),
L = (1− α)LSEQ-NLL + αLSEQ-KD = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0],[0]
s)−,3.3 Sequence-Level Interpolation,[0],[0]
α ∑,3.3 Sequence-Level Interpolation,[0],[0]
"t∈T q(t | s) log p(t | s)
where y is the gold target sequence.",3.3 Sequence-Level Interpolation,[0],[0]
"Since the second term is intractable, we could again apply the mode approximation from the previous section,
L = −(1− α) log p(y |",3.3 Sequence-Level Interpolation,[0],[0]
"s)− α log p(ŷ | s)
and train on both observed (y) and teachergenerated (ŷ) data.",3.3 Sequence-Level Interpolation,[0],[0]
"However, this process is nonideal for two reasons: (1) unlike for standard knowledge distribution, it doubles the size of the training data, and (2) it requires training on both the teachergenerated sequence and the true sequence, conditioned on the same source input.",3.3 Sequence-Level Interpolation,[0],[0]
"The latter concern is particularly problematic since we observe that y and ŷ are often quite different.
",3.3 Sequence-Level Interpolation,[0],[0]
"As an alternative, we propose a single-sequence approximation that is more attractive in this setting.",3.3 Sequence-Level Interpolation,[0],[0]
"This approach is inspired by local updating (Liang et al., 2006), a method for discriminative training in statistical machine translation (although to our knowledge not for knowledge distillation).",3.3 Sequence-Level Interpolation,[0],[0]
"Local updating suggests selecting a training sequence which is close to y and has high probability under the teacher model,
ỹ = argmax t∈T
sim(t,y)q(t | s)
where sim is a function measuring closeness (e.g. Jaccard similarity or BLEU (Papineni et al., 2002)).",3.3 Sequence-Level Interpolation,[0],[0]
"Following local updating, we can approximate this sequence by running beam search and choosing
ỹ",3.3 Sequence-Level Interpolation,[0],[0]
≈ argmax t∈TK,3.3 Sequence-Level Interpolation,[0],[0]
"sim(t,y)
where TK is the K-best list from beam search.",3.3 Sequence-Level Interpolation,[0],[0]
"We take sim to be smoothed sentence-level BLEU (Chen and Cherry, 2014).
",3.3 Sequence-Level Interpolation,[0],[0]
"We justify training on ỹ from a knowledge distillation perspective with the following generative process: suppose that there is a true target sequence (which we do not observe) that is first generated from the underlying data distributionD. And further suppose that the target sequence that we observe (y) is a noisy version of the unobserved true sequence: i.e. (i) t ∼ D, (ii) y ∼ (t), where (t) is, for example, a noise function that independently replaces each element in t with a random element in V with some small probability.6 In such a case, ideally the student’s distribution should match the mixture distribution,
DSEQ-Inter ∼ (1− α)D + αq(t | s)
",3.3 Sequence-Level Interpolation,[0],[0]
"In this setting, due to the noise assumption,D now has significant probability mass around a neighborhood of y (not just at y), and therefore the argmax of the mixture distribution is likely something other than y (the observed sequence) or ŷ",3.3 Sequence-Level Interpolation,[0],[0]
(the output from beam search).,3.3 Sequence-Level Interpolation,[0],[0]
We can see that ỹ is a natural approximation to the argmax of this mixture distribution between D and q(t | s) for some α.,3.3 Sequence-Level Interpolation,[0],[0]
We illustrate this framework in Figure 1 (right) and visualize the distribution over a real example in Figure 2.,3.3 Sequence-Level Interpolation,[0],[0]
"To test out these approaches, we conduct two sets of NMT experiments: high resource (English → German) and low resource (Thai→ English).
",4 Experimental Setup,[0],[0]
The English-German data comes from WMT 2014.7 The training set has 4m sentences and we take newstest2012/newstest2013 as the dev set and newstest2014 as the test set.,4 Experimental Setup,[0],[0]
"We keep the top 50k most frequent words, and replace the rest with UNK.",4 Experimental Setup,[0],[0]
The teacher model is a 4 × 1000 LSTM (as in Luong et al. (2015)) and we train two student models: 2× 300 and 2× 500.,4 Experimental Setup,[0],[0]
"The Thai-English data comes from IWSLT 2015.8 There are 90k sentences in the
6While we employ a simple (unrealistic) noise function for illustrative purposes, the generative story is quite plausible if we consider a more elaborate noise function which includes additional sources of noise such as phrase reordering, replacement of words with synonyms, etc.",4 Experimental Setup,[0],[0]
"One could view translation having two sources of variance that should be modeled separately: variance due to the source sentence (t ∼ D), and variance due to the individual translator (y ∼ (t)).
",4 Experimental Setup,[0],[0]
"7http://statmt.org/wmt14 8https://sites.google.com/site/iwsltevaluation2015/mt-track
training set",4 Experimental Setup,[0],[0]
"and we take 2010/2011/2012 data as the dev set and 2012/2013 as the test set, with a vocabulary size is 25k.",4 Experimental Setup,[0],[0]
"Size of the teacher model is 2×500 (which performed better than 4×1000, 2×750 models), and the student model is 2×100.",4 Experimental Setup,[0],[0]
"Other training details mirror Luong et al. (2015).
",4 Experimental Setup,[0],[0]
"We evaluate on tokenized BLEU with multi-bleu.perl, and experiment with the following variations:
Word-Level Knowledge Distillation (Word-KD) Student is trained on the original data and additionally trained to minimize the cross-entropy of the teacher distribution at the word-level.",4 Experimental Setup,[0],[0]
"We tested α ∈ {0.5, 0.9} and found α = 0.5 to work better.
",4 Experimental Setup,[0],[0]
"Sequence-Level Knowledge Distillation (Seq-KD) Student is trained on the teacher-generated data, which is the result of running beam search and taking the highest-scoring sequence with the teacher model.",4 Experimental Setup,[0],[0]
"We use beam size K = 5 (we did not see improvements with a larger beam).
",4 Experimental Setup,[0],[0]
Sequence-Level Interpolation (Seq-Inter) Student is trained on the sequence on the teacher’s beam that had the highest BLEU (beam size K = 35).,4 Experimental Setup,[0],[0]
"We
adopt a fine-tuning approach where we begin training from a pretrained model (either on original data or Seq-KD data) and train with a smaller learning rate (0.1).",4 Experimental Setup,[0],[0]
"For English-German we generate SeqInter data on a smaller portion of the training set (∼ 50%) for efficiency.
",4 Experimental Setup,[0],[0]
The above methods are complementary and can be combined with each other.,4 Experimental Setup,[0],[0]
"For example, we can train on teacher-generated data but still include a word-level cross-entropy term between the teacher/student (Seq-KD + Word-KD in Table 1), or fine-tune towards Seq-Inter data starting from the baseline model trained on original data (Baseline + Seq-Inter in Table 1).9",4 Experimental Setup,[0],[0]
Results of our experiments are shown in Table 1.,5 Results and Discussion,[0],[0]
"We find that while word-level knowledge distillation (Word-KD) does improve upon the baseline, sequence-level knowledge distillation (SeqKD) does better on English → German and performs similarly on Thai → English.",5 Results and Discussion,[0],[0]
"Combining them (Seq-KD + Word-KD) results in further gains for the 2 × 300 and 2 × 100 models (although not for the 2 × 500 model), indicating that these methods provide orthogonal means of transferring knowledge from the teacher to the student: Word-KD is transferring knowledge at the the local (i.e. word) level while Seq-KD is transferring knowledge at the global (i.e. sequence) level.
",5 Results and Discussion,[0],[0]
"Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but finetuned towards Seq-Inter data (Baseline + Seq-Inter).",5 Results and Discussion,[0],[0]
"In fact, greedy decoding with this fine-tuned model has similar performance (19.6) as beam search with the original model (19.5), allowing for faster decoding even with an identically-sized model.
",5 Results and Discussion,[0],[0]
"We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher’s mode) instead of ‘wasting’ parameters on trying to model the entire
9For instance, ‘Seq-KD + Seq-Inter + Word-KD’ in Table 1 means that the model was trained on Seq-KD data and finetuned towards Seq-Inter data with the mixture cross-entropy loss at the word-level.
space of translations.",5 Results and Discussion,[0],[0]
Our results suggest that this is indeed the case: the probability mass that SeqKD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: p(t = ŷ)).,5 Results and Discussion,[0],[0]
"For example, on English → German the (approximate) argmax for the 2 × 500 Seq-KD model (on average) accounts for 16.9% of the total probability mass, while the corresponding number is 0.9% for the baseline.",5 Results and Discussion,[0],[0]
"This also explains the success of greedy decoding for Seq-KD models—since we are only modeling around the teacher’s mode, the student’s distribution is more peaked and therefore the argmax is much easier to find.",5 Results and Discussion,[0],[0]
"Seq-Inter offers a compromise between the two, with the greedily-decoded sequence accounting for 7.6% of the distribution.
",5 Results and Discussion,[0],[0]
"Finally, although past work has shown that models with lower perplexity generally tend to have
higher BLEU, our results indicate that this is not necessarily the case.",5 Results and Discussion,[0],[0]
"The perplexity of the baseline 2 × 500 English→ German model is 8.2 while the perplexity of the corresponding Seq-KD model is 22.7, despite the fact that Seq-KD model does significantly better for both greedy (+4.2 BLEU) and beam search (+1.4 BLEU) decoding.",5 Results and Discussion,[0],[0]
Run-time complexity for beam search grows linearly with beam size.,5.1 Decoding Speed,[0],[0]
"Therefore, the fact that sequencelevel knowledge distillation allows for greedy decoding is significant, with practical implications for running NMT systems across various devices.",5.1 Decoding Speed,[0],[0]
"To test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the average number of source words translated per second (Table 2).",5.1 Decoding Speed,[0],[0]
We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone.,5.1 Decoding Speed,[0],[0]
"We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU (1051.3 vs 101.9 words/sec), with similar performance.",5.1 Decoding Speed,[0],[0]
"Although knowledge distillation enables training faster models, the number of parameters for the student models is still somewhat large (Table 1: Params), due to the word embeddings which dominate most of the parameters.10 For example, on the
10Word embeddings scale linearly while RNN parameters scale quadratically with the dimension size.
2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters.",5.2 Weight Pruning,[0],[0]
"The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model.
",5.2 Weight Pruning,[0],[0]
We therefore focus next on reducing the memory footprint of the student models further through weight pruning.,5.2 Weight Pruning,[0],[0]
"Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance.",5.2 Weight Pruning,[0],[0]
We take our best English→ German student model (2× 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values.,5.2 Weight Pruning,[0],[0]
We then retrain the pruned model on Seq-KD data with a learning rate of 0.2 and fine-tune towards Seq-Inter data with a learning rate of 0.1.,5.2 Weight Pruning,[0],[0]
"As observed by See et al. (2016), retraining proved to be crucial.",5.2 Weight Pruning,[0],[0]
"The results are shown in Table 3.
",5.2 Weight Pruning,[0],[0]
Our findings suggest that compression benefits achieved through weight pruning and knowledge distillation are orthogonal.11 Pruning 80% of the weight in the 2 × 500 student model results in a model with 13× fewer parameters than the original teacher model with only a decrease of 0.4 BLEU.,5.2 Weight Pruning,[0],[0]
"While pruning 90% of the weights results in a more appreciable decrease of 1.0 BLEU, the model is
11To our knowledge combining pruning and knowledge distillation has not been investigated before.
drastically smaller with 8m parameters, which is 26× fewer than the original teacher model.",5.2 Weight Pruning,[0],[0]
"• For models trained with word-level knowledge
distillation, we also tried regressing the student network’s top-most hidden layer at each time step to the teacher network’s top-most hidden layer as a pretraining step, noting that Romero et al. (2015) obtained improvements with a similar technique on feed-forward models.",5.3 Further Observations,[0],[0]
"We found this to give comparable results to standard knowledge distillation and hence did not pursue this further.
",5.3 Further Observations,[0],[0]
"• There have been promising recent results on eliminating word embeddings completely and obtaining word representations directly from characters with character composition models, which have many fewer parameters than word embedding lookup tables (Ling et al., 2015a; Kim et al., 2016; Ling et al., 2015b; Jozefowicz et al., 2016; Costa-Jussa and Fonollosa, 2016).",5.3 Further Observations,[0],[0]
Combining such methods with knowledge distillation/pruning to further reduce the memory footprint of NMT systems remains an avenue for future work.,5.3 Further Observations,[0],[0]
Compressing deep learning models is an active area of current research.,6 Related Work,[0],[0]
Pruning methods involve pruning weights or entire neurons/nodes based on some criterion.,6 Related Work,[0],[0]
"LeCun et al. (1990) prune weights based on an approximation of the Hessian, while Han et al. (2016) show that a simple magnitude-based pruning works well.",6 Related Work,[0],[0]
Prior work on removing neurons/nodes include Srinivas and Babu (2015) and Mariet and Sra (2016).,6 Related Work,[0],[0]
"See et al. (2016) were the first to apply pruning to Neural Machine Translation, observing that that different parts of the architecture (input word embeddings, LSTM matrices, etc.) admit different levels of pruning.",6 Related Work,[0],[0]
"Knowledge distillation approaches train a smaller student model to mimic a larger teacher model, by minimizing the loss between the teacher/student predictions (Bucila et al., 2006; Ba and Caruana, 2014; Li et al., 2014; Hinton et al., 2015).",6 Related Work,[0],[0]
"Romero et al. (2015) additionally regress on the intermediate hidden layers of the
student/teacher network as a pretraining step, while Mou et al. (2015) obtain smaller word embeddings from a teacher model via regression.",6 Related Work,[0],[0]
There has also been work on transferring knowledge across different network architectures: Chan et al. (2015b) show that a deep non-recurrent neural network can learn from an RNN; Geras et al. (2016) train a CNN to mimic an LSTM for speech recognition.,6 Related Work,[0],[0]
"Kuncoro et al. (2016) recently investigated knowledge distillation for structured prediction by having a single parser learn from an ensemble of parsers.
",6 Related Work,[0],[0]
"Other approaches for compression involve low rank factorizations of weight matrices (Denton et al., 2014; Jaderberg et al., 2014; Lu et al., 2016; Prabhavalkar et al., 2016), sparsity-inducing regularizers (Murray and Chiang, 2015), binarization of weights (Courbariaux et al., 2016; Lin et al., 2016), and weight sharing (Chen et al., 2015; Han et al., 2016).",6 Related Work,[0],[0]
"Finally, although we have motivated sequence-level knowledge distillation in the context of training a smaller model, there are other techniques that train on a mixture of the model’s predictions and the data, such as local updating (Liang et al., 2006), hope/fear training (Chiang, 2012), SEARN (Daumé III et al., 2009), DAgger (Ross et al., 2011), and minimum risk training (Och, 2003; Shen et al., 2016).",6 Related Work,[0],[0]
"In this work we have investigated existing knowledge distillation methods for NMT (which work at the word-level) and introduced two sequence-level variants of knowledge distillation, which provide improvements over standard word-level knowledge distillation.
",7 Conclusion,[0],[0]
"We have chosen to focus on translation as this domain has generally required the largest capacity deep learning models, but the sequence-to-sequence framework has been successfully applied to a wide range of tasks including parsing (Vinyals et al., 2015a), summarization (Rush et al., 2015), dialogue (Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016), NER/POS-tagging (Gillick et al., 2016), image captioning (Vinyals et al., 2015b; Xu et al., 2015), video generation (Srivastava et al., 2015), and speech recognition (Chan et al., 2015a).",7 Conclusion,[0],[0]
We anticipate that methods described in this paper can be used to similarly train smaller models in other domains.,7 Conclusion,[0],[0]
Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches.,abstractText,[0],[0]
"However to reach competitive performance, NMT models need to be exceedingly large.",abstractText,[0],[0]
"In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT.",abstractText,[0],[0]
"We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model).",abstractText,[0],[0]
Our best student model runs 10 times faster than its state-of-the-art teacher with little loss in performance.,abstractText,[0],[0]
It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.,abstractText,[0],[0]
"Applying weight pruning on top of knowledge distillation results in a student model that has 13× fewer parameters than the original teacher model, with a decrease of 0.4 BLEU.",abstractText,[0],[0]
Sequence-Level Knowledge Distillation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 766–777 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
766",text,[0],[0]
"Semantic parsing aims to map natural language sentences to logical forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Lu et al., 2008; Kwiatkowski et al., 2013).",1 Introduction,[0],[0]
"For example, the sentence “Which states border Texas?” will be mapped to answer (A, (state (A), next to (A, stateid ( texas )))).
",1 Introduction,[0],[0]
"A semantic parser needs two functions, one for structure prediction and the other for semantic grounding.",1 Introduction,[0],[0]
"Traditional semantic parsers are usually based on compositional grammar, such as CCG (Zettlemoyer and Collins, 2005, 2007), DCS (Liang et al., 2011), etc.",1 Introduction,[0],[0]
"These parsers compose structure using manually designed grammars, use lexicons for semantic grounding, and exploit fea-
tures for candidate logical forms ranking.",1 Introduction,[0],[0]
"Unfortunately, it is challenging to design grammars and learn accurate lexicons, especially in wideopen domains.",1 Introduction,[0],[0]
"Moreover, it is often hard to design effective features, and its learning process is not end-to-end.",1 Introduction,[0],[0]
"To resolve the above problems, two promising lines of work have been proposed: Semantic graph-based methods and Seq2Seq methods.
",1 Introduction,[0],[0]
"Semantic graph-based methods (Reddy et al., 2014, 2016; Bast and Haussmann, 2015; Yih et al., 2015) represent the meaning of a sentence as a semantic graph (i.e., a sub-graph of a knowledge base, see example in Figure 1) and treat semantic parsing as a semantic graph matching/generation process.",1 Introduction,[0],[0]
"Compared with logical forms, semantic graphs have a tight-coupling with knowledge bases (Yih et al., 2015), and share many commonalities with syntactic structures (Reddy et al., 2014).",1 Introduction,[0],[0]
"Therefore both the structure and semantic constraints from knowledge bases can be easily exploited during parsing (Yih et al., 2015).",1 Introduction,[0],[0]
The main challenge of semantic graph-based parsing is how to effectively construct the semantic graph of a sentence.,1 Introduction,[0],[0]
"Currently, semantic graphs
are either constructed by matching with patterns (Bast and Haussmann, 2015), transforming from dependency tree (Reddy et al., 2014, 2016), or via a staged heuristic search algorithm (Yih et al., 2015).",1 Introduction,[0],[0]
"These methods are all based on manuallydesigned, heuristic construction processes, making them hard to handle open/complex situations.
",1 Introduction,[0],[0]
"In recent years, RNN models have achieved success in sequence-to-sequence problems due to its strong ability on both representation learning and prediction, e.g., in machine translation (Cho et al., 2014).",1 Introduction,[0],[0]
"A lot of Seq2Seq models have also been employed for semantic parsing (Xiao et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2016), where a sentence is parsed by translating it to linearized logical form using RNN models.",1 Introduction,[0],[0]
"There is no need for high-quality lexicons, manually-built grammars, and hand-crafted features.",1 Introduction,[0],[0]
"These models are trained end-to-end, and can leverage attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) to learn soft alignments between sentences and logical forms.
",1 Introduction,[0],[0]
"In this paper, we propose a new neural semantic parsing framework – Sequence-to-Action, which can simultaneously leverage the advantages of semantic graph representation and the strong prediction ability of Seq2Seq models.",1 Introduction,[0],[0]
"Specifically, we model semantic parsing as an end-to-end semantic graph generation process.",1 Introduction,[0],[0]
"For example in Figure 1, our model will parse the sentence “Which states border Texas” by generating a sequence of actions [add variable:A, add type:state, ...].",1 Introduction,[0],[0]
"To achieve the above goal, we first design an action set which can encode the generation process of semantic graph (including node actions such as add variable, add entity, add type, edge actions such as add edge, and operation actions such as argmin, argmax, count, sum, etc.).",1 Introduction,[0],[0]
And then we design a RNN model which can generate the action sequence for constructing the semantic graph of a sentence.,1 Introduction,[0],[0]
"Finally we further enhance parsing by incorporating both structure and semantic constraints during decoding.
",1 Introduction,[0],[0]
"Compared with the manually-designed, heuristic generation algorithms used in traditional semantic graph-based methods, our sequence-toaction method generates semantic graphs using a RNN model, which is learned end-to-end from training data.",1 Introduction,[0],[0]
"Such a learnable, end-to-end generation makes our approach more effective and can fit to different situations.
",1 Introduction,[0],[0]
"Compared with the previous Seq2Seq semantic parsing methods, our sequence-to-action model predicts a sequence of semantic graph generation actions, rather than linearized logical forms.",1 Introduction,[0],[0]
"We find that the action sequence encoding can better capture structure and semantic information, and is more compact.",1 Introduction,[0],[0]
And the parsing can be enhanced by exploiting structure and semantic constraints.,1 Introduction,[0],[0]
"For example, in GEO dataset, the action add edge:next to must subject to the semantic constraint that its arguments must be of type state and state, and the structure constraint that the edge next to must connect two nodes to form a valid graph.
",1 Introduction,[0],[0]
"We evaluate our approach on three standard datasets: GEO (Zelle and Mooney, 1996), ATIS (He and Young, 2005) and OVERNIGHT (Wang et al., 2015b).",1 Introduction,[0],[0]
"The results show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.
",1 Introduction,[0],[0]
"The main contributions of this paper are summarized as follows:
• We propose a new semantic parsing framework – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process.",1 Introduction,[0],[0]
"This new framework can synthesize the advantages of semantic graph representation and the prediction ability of Seq2Seq models.
",1 Introduction,[0],[0]
"• We design a sequence-to-action model, including an action set encoding for semantic graph generation and a Seq2Seq RNN model for action sequence prediction.",1 Introduction,[0],[0]
We further enhance the parsing by exploiting structure and semantic constraints during decoding.,1 Introduction,[0],[0]
Experiments validate the effectiveness of our method.,1 Introduction,[0],[0]
"Given a sentence X = x1, ..., x|X|, our sequenceto-action model generates a sequence of actions Y = y1, ..., y|Y | for constructing the correct semantic graph.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
Figure 2 shows an example.,2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"The conditional probability P (Y |X) used in our
model is decomposed as follows:
P (Y |X) =",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"|Y |∏ t=1 P (yt|y<t, X) (1)
where y<t = y1, ..., yt−1.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"To achieve the above goal, we need: 1) an action set which can encode semantic graph generation process; 2) an encoder which encodes natural language input X into a vector representation, and a decoder which generates y1, ..., y|Y | conditioned on the encoding vector.",2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
In following we describe them in detail.,2 Sequence-to-Action Model for End-to-End Semantic Graph Generation,[0],[0]
"Generally, a semantic graph consists of nodes (including variables, entities, types) and edges (semantic relations), with some universal operations (e.g., argmax, argmin, count, sum, and not).",2.1 Actions for Semantic Graph Generation,[0],[0]
"To generate a semantic graph, we define six types of actions as follows:
Add Variable Node: This kind of actions denotes adding a variable node to semantic graph.",2.1 Actions for Semantic Graph Generation,[0],[0]
"In most cases a variable node is a return node (e.g., which, what), but can also be an intermediate variable node.",2.1 Actions for Semantic Graph Generation,[0],[0]
"We represent this kind of action as add variable:A, where A is the identifier of the variable node.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"Add Entity Node: This kind of actions denotes adding an entity node (e.g., Texas, New York) and is represented as add entity node:texas.",2.1 Actions for Semantic Graph Generation,[0],[0]
"An entity node corresponds to an entity in knowledge bases.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"Add Type Node: This kind of actions denotes adding a type node (e.g., state, city).",2.1 Actions for Semantic Graph Generation,[0],[0]
"We represent them as add type node:state.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Add Edge: This kind of actions denotes adding an edge between two nodes.,2.1 Actions for Semantic Graph Generation,[0],[0]
An edge is a binary relation in knowledge bases.,2.1 Actions for Semantic Graph Generation,[0],[0]
"This kind of actions is represented as add edge:next to.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Operation Action:,2.1 Actions for Semantic Graph Generation,[0],[0]
This kind of actions denotes adding an operation.,2.1 Actions for Semantic Graph Generation,[0],[0]
"An operation can be argmax, argmin, count, sum, not, et al. Because each operation has a scope, we define two actions for an operation, one is operation start action, represented as start operation:most, and the other is operation end action, represented as end operation:most.",2.1 Actions for Semantic Graph Generation,[0],[0]
"The subgraph within the start and end operation actions is its scope.
",2.1 Actions for Semantic Graph Generation,[0],[0]
Argument Action:,2.1 Actions for Semantic Graph Generation,[0],[0]
Some above actions need argument information.,2.1 Actions for Semantic Graph Generation,[0],[0]
"For example, which nodes the add edge:next to action should connect to.",2.1 Actions for Semantic Graph Generation,[0],[0]
"In this paper, we design argument actions for add type, add edge and operation actions, and the argument actions should be put directly after its main action.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"For add type actions, we put an argument action to indicate which node this type node should constrain.",2.1 Actions for Semantic Graph Generation,[0],[0]
The argument can be a variable node or an entity node.,2.1 Actions for Semantic Graph Generation,[0],[0]
"An argument action for a type node is represented as arg:A.
For add edge action, we use two argument actions: arg1 node and arg2 node, and they are represented as arg1 node:A and arg2 node:B.
We design argument actions for different operations.",2.1 Actions for Semantic Graph Generation,[0],[0]
"For operation:sum, there are three arguments: arg-for, arg-in and arg-return.",2.1 Actions for Semantic Graph Generation,[0],[0]
"For operation:count, they are arg-for and arg-return.",2.1 Actions for Semantic Graph Generation,[0],[0]
"There are two arg-for arguments for operation:most.
",2.1 Actions for Semantic Graph Generation,[0],[0]
"We can see that each action encodes both structure and semantic information, which makes it easy to capture more information for parsing and can be tightly coupled with knowledge base.",2.1 Actions for Semantic Graph Generation,[0],[0]
"Furthermore, we find that action sequence encoding is more compact than linearized logical form (See Section 4.4 for more details).",2.1 Actions for Semantic Graph Generation,[0],[0]
"Based on the above action encoding mechanism, this section describes our encoder-decoder model for mapping sentence to action sequence.",2.2 Neural Sequence-to-Action Model,[0],[0]
"Specifically, similar to the RNN model in Jia and Liang (2016), this paper employs the attentionbased sequence-to-sequence RNN model.",2.2 Neural Sequence-to-Action Model,[0],[0]
Figure 3 presents the overall structure.,2.2 Neural Sequence-to-Action Model,[0],[0]
Encoder:,2.2 Neural Sequence-to-Action Model,[0],[0]
"The encoder converts the input sequence x1, ..., xm to a sequence of contextsensitive vectors b1, ..., bm using a bidirectional RNN (Bahdanau et al., 2014).",2.2 Neural Sequence-to-Action Model,[0],[0]
"Firstly each word xi is mapped to its embedding vector, then these vectors are fed into a forward RNN and a backward RNN.",2.2 Neural Sequence-to-Action Model,[0],[0]
"The sequence of hidden states h1, ..., hm are generated by recurrently applying the recurrence:
hi = LSTM(φ (x)(xi), hi−1).",2.2 Neural Sequence-to-Action Model,[0],[0]
"(2)
The recurrence takes the form of LSTM (Hochreiter and Schmidhuber, 1997).",2.2 Neural Sequence-to-Action Model,[0],[0]
"Finally, for each input position i, we define its context-sensitive embedding as bi =",2.2 Neural Sequence-to-Action Model,[0],[0]
"[hFi , h B i ].",2.2 Neural Sequence-to-Action Model,[0],[0]
"Decoder: This paper uses the classical attentionbased decoder (Bahdanau et al., 2014), which generates action sequence y1, ..., yn, one action at a time.",2.2 Neural Sequence-to-Action Model,[0],[0]
"At each time step j, it writes yj based on the current hidden state sj , then updates the hidden state to sj+1 based on sj and yj .",2.2 Neural Sequence-to-Action Model,[0],[0]
"The decoder is formally defined by the following equations:
s1 = tanh(W (s)[hFm, h B 1 ]) (3)
eji = s T j W (a)bi (4) aji = exp(eji)∑m
i′=1 exp(eji′ ) (5)
cj =",2.2 Neural Sequence-to-Action Model,[0],[0]
m∑ i=1,2.2 Neural Sequence-to-Action Model,[0],[0]
"ajibi (6) P (yj = w|x, y1:j−1) ∝",2.2 Neural Sequence-to-Action Model,[0],[0]
"exp(Uw[sj , cj ]) (7) sj+1 = LSTM([φ (y)(yj), cj ], sj) (8)
where the normalized attention scores aji defines the probability distribution over input words, indicating the attention probability on input word i at time j; eji is un-normalized attention score.",2.2 Neural Sequence-to-Action Model,[0],[0]
"To incorporate constraints during decoding, an extra controller component is added and its details will be described in Section 3.3.",2.2 Neural Sequence-to-Action Model,[0],[0]
Action Embedding.,2.2 Neural Sequence-to-Action Model,[0],[0]
The above decoder needs the embedding of each action.,2.2 Neural Sequence-to-Action Model,[0],[0]
"As described above, each action has two parts, one for structure (e.g., add edge), and the other for semantic (e.g., next to).",2.2 Neural Sequence-to-Action Model,[0],[0]
"As a result, actions may share the same structure or semantic part, e.g., add edge:next to and add edge:loc have the same structure part, and add node:A and arg node:A have the same semantic part.",2.2 Neural Sequence-to-Action Model,[0],[0]
"To make parameters more compact, we first embed the structure part and the semantic part independently, then concatenate them to get the final embedding.",2.2 Neural Sequence-to-Action Model,[0],[0]
"For instance, φ(y)(add edge:next to ) =",2.2 Neural Sequence-to-Action Model,[0],[0]
"[ φ(y)strut( add edge ), φ (y) sem( next to )].",2.2 Neural Sequence-to-Action Model,[0],[0]
The action embeddings φ(y) are learned during training.,2.2 Neural Sequence-to-Action Model,[0],[0]
"In this section, we describe how to build a neural semantic parser using sequence-to-action model.",3 Constrained Semantic Parsing using Sequence-to-Action Model,[0],[0]
"We first describe the training and the inference of our model, and then introduce how to incorporate structure and semantic constraints during decoding.",3 Constrained Semantic Parsing using Sequence-to-Action Model,[0],[0]
Parameter Estimation.,3.1 Training,[0],[0]
"The parameters of our model include RNN parameters W (s), W (a), Uw, word embeddings φ(x), and action embeddings φ(y).",3.1 Training,[0],[0]
We estimate these parameters from training data.,3.1 Training,[0],[0]
"Given a training example with a sentence X and its action sequence Y , we maximize the likelihood of the generated sequence of actions given X .",3.1 Training,[0],[0]
"The objective function is:
n∑ i=1",3.1 Training,[0],[0]
"logP (Yi|Xi) (9)
Standard stochastic gradient descent algorithm is employed to update parameters.",3.1 Training,[0],[0]
Logical Form to Action Sequence.,3.1 Training,[0],[0]
"Currently, most datasets of semantic parsing are labeled with logical forms.",3.1 Training,[0],[0]
"In order to train our model, we
convert logical forms to action sequences using semantic graph as an intermediate representation (See Figure 4 for an overview).",3.1 Training,[0],[0]
"Concretely, we transform logical forms into semantic graphs using a depth-first-search algorithm from root, and then generate the action sequence using the same order.",3.1 Training,[0],[0]
"Specifically, entities, variables and types are nodes; relations are edges.",3.1 Training,[0],[0]
Conversely we can convert action sequence to logical form similarly.,3.1 Training,[0],[0]
"Based on the above algorithm, action sequences can be transformed into logical forms in a deterministic way, and the same for logical forms to action sequences.",3.1 Training,[0],[0]
Mechanisms for Handling Entities.,3.1 Training,[0],[0]
"Entities play an important role in semantic parsing (Yih et al., 2015).",3.1 Training,[0],[0]
"In Dong and Lapata (2016), entities are replaced with their types and unique IDs.",3.1 Training,[0],[0]
"In Jia and Liang (2016), entities are generated via attention-based copying mechanism helped with a lexicon.",3.1 Training,[0],[0]
This paper implements both mechanisms and compares them in experiments.,3.1 Training,[0],[0]
"Given a new sentence X , we predict action sequence by:
Y ∗ = argmax Y P (Y |X) (10)
where Y represents action sequence, and P (Y |X) is computed using Formula (1).",3.2 Inference,[0],[0]
Beam search is used for best action sequence decoding.,3.2 Inference,[0],[0]
Semantic graph and logical form can be derived from Y ∗ as described in above.,3.2 Inference,[0],[0]
"For decoding, we generate action sequentially.",3.3 Incorporating Constraints in Decoding,[0],[0]
"It is obviously that the next action has a strong correlation with the partial semantic graph generated to current, and illegal actions can be filtered using structure and semantic constraints.",3.3 Incorporating Constraints in Decoding,[0],[0]
"Specifically, we incorporate constraints in decoding using a controller.",3.3 Incorporating Constraints in Decoding,[0],[0]
"This procedure has two steps: 1) the controller constructs partial semantic graph using the actions generated to current; 2) the controller checks whether a new generated action can meet
all structure/semantic constraints using the partial semantic graph.
",3.3 Incorporating Constraints in Decoding,[0],[0]
Structure Constraints.,3.3 Incorporating Constraints in Decoding,[0],[0]
The structure constraints ensure action sequence will form a connected acyclic graph.,3.3 Incorporating Constraints in Decoding,[0],[0]
"For example, there must be two argument nodes for an edge, and the two argument nodes should be different (The third candidate next action in Figure 5 violates this constraint).",3.3 Incorporating Constraints in Decoding,[0],[0]
This kind of constraints are domain-independent.,3.3 Incorporating Constraints in Decoding,[0],[0]
"The controller encodes structure constraints as a set of rules.
",3.3 Incorporating Constraints in Decoding,[0],[0]
Semantic Constraints.,3.3 Incorporating Constraints in Decoding,[0],[0]
The semantic constraints ensure the constructed graph must follow the schema of knowledge bases.,3.3 Incorporating Constraints in Decoding,[0],[0]
"Specifically, we model two types of semantic constraints.",3.3 Incorporating Constraints in Decoding,[0],[0]
One is selectional preference constraints where the argument types of a relation should follow knowledge base schemas.,3.3 Incorporating Constraints in Decoding,[0],[0]
"For example, in GEO dataset, relation next to’s arg1 and arg2 should both be a state.",3.3 Incorporating Constraints in Decoding,[0],[0]
"The second is type conflict constraints, i.e., an entity/variable node’s type must be consistent, i.e., a node cannot be both of type city and state.",3.3 Incorporating Constraints in Decoding,[0],[0]
Semantic constraints are domain-specific and are automatically extracted from knowledge base schemas.,3.3 Incorporating Constraints in Decoding,[0],[0]
The controller encodes semantic constraints as a set of rules.,3.3 Incorporating Constraints in Decoding,[0],[0]
"In this section, we assess the performance of our method and compare it with previous methods.",4 Experiments,[0],[0]
"We conduct experiments on three standard datasets: GEO, ATIS and OVERNIGHT.",4.1 Datasets,[0],[0]
GEO contains natural language questions about US geography paired with corresponding Prolog database queries.,4.1 Datasets,[0],[0]
"Following Zettlemoyer and Collins (2005), we use the standard 600/280 instance splits for training/test.",4.1 Datasets,[0],[0]
"ATIS contains natural language questions of a flight database, with each question is annotated with a lambda calculus query.",4.1 Datasets,[0],[0]
"Following Zettlemoyer and Collins (2007), we use the standard 4473/448 instance splits for training/test.",4.1 Datasets,[0],[0]
OVERNIGHT contains natural language paraphrases paired with logical forms across eight domains.,4.1 Datasets,[0],[0]
We evaluate on the standard train/test splits as Wang et al. (2015b).,4.1 Datasets,[0],[0]
Following the experimental setup of Jia and Liang (2016): we use 200 hidden units and 100- dimensional word vectors for sentence encoding.,4.2 Experimental Settings,[0],[0]
The dimensions of action embedding are tuned on validation datasets for each corpus.,4.2 Experimental Settings,[0],[0]
We initialize all parameters by uniformly sampling within the interval,4.2 Experimental Settings,[0],[0]
"[-0.1, 0.1].",4.2 Experimental Settings,[0],[0]
"We train our model for a total of 30 epochs with an initial learning rate of 0.1, and halve the learning rate every 5 epochs after epoch 15.",4.2 Experimental Settings,[0],[0]
We replace word vectors for words occurring only once with an universal word vector.,4.2 Experimental Settings,[0],[0]
The beam size is set as 5.,4.2 Experimental Settings,[0],[0]
"Our model is implemented in Theano (Bergstra et al., 2010), and the codes and settings are released on Github: https://github.com/dongpobeyond/Seq2Act.
",4.2 Experimental Settings,[0],[0]
"We evaluate different systems using the standard accuracy metric, and the accuracies on different datasets are obtained as same as Jia and Liang (2016).",4.2 Experimental Settings,[0],[0]
We compare our method with state-of-the-art systems on all three datasets.,4.3 Overall Results,[0],[0]
"Because all systems using the same training/test splits, we directly use the reported best performances from their original papers for fair comparison.
",4.3 Overall Results,[0],[0]
"For our method, we train our model with three settings: the first one is the basic sequence-toaction model without constraints – Seq2Act; the second one adds structure constraints in decoding – Seq2Act (+C1); the third one is the full model which adds both structure and semantic
constraints – Seq2Act (+C1+C2).",4.3 Overall Results,[0],[0]
Semantic constraints (C2) are stricter than structure constraints (C1).,4.3 Overall Results,[0],[0]
Therefore we set that C1 should be first met for C2 to be met.,4.3 Overall Results,[0],[0]
So in our experiments we add constraints incrementally.,4.3 Overall Results,[0],[0]
The overall results are shown in Table 1-2.,4.3 Overall Results,[0],[0]
"From the overall results, we can see that:
1) By synthetizing the advantages of semantic graph representation and the prediction ability of Seq2Seq model, our method achieves stateof-the-art performance on OVERNIGHT dataset, and gets competitive performance on GEO and ATIS dataset.",4.3 Overall Results,[0],[0]
"In fact, on GEO our full model (Seq2Act+C1+C2) also gets the best test accuracy of 88.9 if under the same settings, which only falls behind Liang et al. (2011)* which uses extra handcrafted lexicons and Jia and Liang (2016)* which uses extra augmented training data.",4.3 Overall Results,[0],[0]
"On ATIS our full model gets the second best test accuracy of 85.5, which only falls behind Rabinovich et al. (2017) which uses a supervised attention strategy.",4.3 Overall Results,[0],[0]
"On OVERNIGHT, our full model gets state-of-theart accuracy of 79.0, which even outperforms Jia and Liang (2016)* with extra augmented training data.
",4.3 Overall Results,[0],[0]
2),4.3 Overall Results,[0],[0]
"Compared with the linearized logical form representation used in previous Seq2Seq baselines, our action sequence encoding is more effective for semantic parsing.",4.3 Overall Results,[0],[0]
"On all three datasets,
our basic Seq2Act model gets better results than all Seq2Seq baselines.",4.3 Overall Results,[0],[0]
"On GEO, the Seq2Act model achieve test accuracy of 87.5, better than the best accuracy 87.1 of Seq2Seq baseline.",4.3 Overall Results,[0],[0]
"On ATIS, the Seq2Act model obtains a test accuracy of 84.6, the same as the best Seq2Seq baseline.",4.3 Overall Results,[0],[0]
"On OVERNGIHT, the Seq2Act model gets a test accuracy of 78.0, better than the best Seq2Seq baseline gets 77.5.",4.3 Overall Results,[0],[0]
"We argue that this is because our action sequence encoding is more compact and can capture more information.
3) Structure constraints can enhance semantic parsing by ensuring the validity of graph using the generated action sequence.",4.3 Overall Results,[0],[0]
"In all three datasets, Seq2Act (+C1) outperforms the basic Seq2Act model.",4.3 Overall Results,[0],[0]
"This is because a part of illegal actions will be filtered during decoding.
4) By leveraging knowledge base schemas during decoding, semantic constraints are effective for semantic parsing.",4.3 Overall Results,[0],[0]
"Compared to Seq2Act and Seq2Act (+C1), the Seq2Act (+C1+C2) gets the best performance on all three datasets.",4.3 Overall Results,[0],[0]
This is because semantic constraints can further filter semantic illegal actions using selectional preference and consistency between types.,4.3 Overall Results,[0],[0]
Effect of Entity Handling Mechanisms.,4.4 Detailed Analysis,[0],[0]
"This paper implements two entity handling mechanisms – Replacing (Dong and Lapata, 2016) which identifies entities and then replaces them with their types and IDs, and attention-based Copying (Jia and Liang, 2016).",4.4 Detailed Analysis,[0],[0]
"To compare the above two mechanisms, we train and test with our full model and the results are shown in Table 3.",4.4 Detailed Analysis,[0],[0]
"We can see that, Replacing mechanism outperforms Copying in all three datasets.",4.4 Detailed Analysis,[0],[0]
"This is because Replacing is done
in preprocessing, while attention-based Copying is done during parsing and needs additional copy mechanism.",4.4 Detailed Analysis,[0],[0]
Linearized Logical Form vs. Action Sequence.,4.4 Detailed Analysis,[0],[0]
Table 4 shows the average length of linearized logical forms used in previous Seq2Seq models and the action sequences of our model on all three datasets.,4.4 Detailed Analysis,[0],[0]
"As we can see, action sequence encoding is more compact than linearized logical form encoding: action sequence is shorter on all three datasets, 35.5%, 9.2% and 28.5% reduction in length respectively.",4.4 Detailed Analysis,[0],[0]
The main advantage of a shorter/compact encoding is that it will reduce the influence of long distance dependency problem.,4.4 Detailed Analysis,[0],[0]
We perform error analysis on results and find there are mainly two types of errors.,4.5 Error Analysis,[0],[0]
Unseen/Informal Sentence Structure.,4.5 Error Analysis,[0],[0]
Some test sentences have unseen syntactic structures.,4.5 Error Analysis,[0],[0]
"For example, the first case in Table 5 has an unseen
and informal structure, where entity word “Iowa” and relation word “borders” appear ahead of the question words “how many”.",4.5 Error Analysis,[0],[0]
"For this problem, we can employ sentence rewriting or paraphrasing techniques (Chen et al., 2016; Dong et al., 2017) to transform unseen sentence structures into normal ones.",4.5 Error Analysis,[0],[0]
Under-Mapping.,4.5 Error Analysis,[0],[0]
"As Dong and Lapata (2016) discussed, the attention model does not take the alignment history into consideration, makes some words are ignored during parsing.",4.5 Error Analysis,[0],[0]
"For example in the second case in Table 5, “first class” is ignored during the decoding process.",4.5 Error Analysis,[0],[0]
"This problem can be further solved using explicit word coverage models used in neural machine translation (Tu et al., 2016; Cohn et al., 2016)",4.5 Error Analysis,[0],[0]
"Semantic parsing has received significant attention for a long time (Kate and Mooney, 2006; Clarke et al., 2010; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Quirk et al., 2015; Artzi et al., 2015; Reddy et al., 2017).",5 Related Work,[0],[0]
"Traditional methods are mostly based on the principle of compositional semantics, which first trigger predicates using lexicons and then compose them using grammars.",5 Related Work,[0],[0]
"The prominent grammars include SCFG (Wong and Mooney, 2007; Li et al., 2015), CCG (Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2011; Cai and Yates, 2013), DCS (Liang et al., 2011; Berant et al., 2013), etc.",5 Related Work,[0],[0]
"As discussed above, the main drawback of grammar-based methods is that they rely on high-quality lexicons, manually-built grammars, and hand-crafted features.
",5 Related Work,[0],[0]
"In recent years, one promising direction of semantic parsing is to use semantic graph as representation.",5 Related Work,[0],[0]
Thus semantic parsing is modeled as a semantic graph generation process.,5 Related Work,[0],[0]
"Ge and Mooney (2009) build semantic graph by trans-
forming syntactic tree.",5 Related Work,[0],[0]
Bast and Haussmann (2015) identify the structure of a semantic query using three pre-defined patterns.,5 Related Work,[0],[0]
"Reddy et al. (2014, 2016) use Freebase-based semantic graph representation, and convert sentences to semantic graphs using CCG or dependency tree.",5 Related Work,[0],[0]
Yih et al. (2015) generate semantic graphs using a staged heuristic search algorithm.,5 Related Work,[0],[0]
"These methods are all based on manually-designed, heuristic generation process, which may suffer from syntactic parse errors (Ge and Mooney, 2009; Reddy et al., 2014, 2016), structure mismatch (Chen et al., 2016), and are hard to deal with complex sentences (Yih et al., 2015).
",5 Related Work,[0],[0]
"One other direction is to employ neural Seq2Seq models, which models semantic parsing as an end-to-end, sentence to logical form machine translation problem.",5 Related Work,[0],[0]
"Dong and Lapata (2016), Jia and Liang (2016) and Xiao et al. (2016) transform word sequence to linearized logical forms.",5 Related Work,[0],[0]
One main drawback of these methods is that it is hard to capture and exploit structure and semantic constraints using linearized logical forms.,5 Related Work,[0],[0]
"Dong and Lapata (2016) propose a Seq2Tree model to capture the hierarchical structure of logical forms.
",5 Related Work,[0],[0]
It has been shown that structure and semantic constraints are effective for enhancing semantic parsing.,5 Related Work,[0],[0]
Krishnamurthy et al. (2017) use type constraints to filter illegal tokens.,5 Related Work,[0],[0]
Liang et al. (2017) adopt a Lisp interpreter with pre-defined functions to produce valid tokens.,5 Related Work,[0],[0]
Iyyer et al. (2017) adopt type constraints to generate valid actions.,5 Related Work,[0],[0]
"Inspired by these approaches, we also incorporate both structure and semantic constraints in our neural sequence-to-action model.
",5 Related Work,[0],[0]
"Transition-based approaches are important in both dependency parsing (Nivre, 2008; Henderson et al., 2013) and AMR parsing (Wang et al., 2015a).",5 Related Work,[0],[0]
"In semantic parsing, our method has a tight-coupling with knowledge bases, and con-
straints can be exploited for more accurate decoding.",5 Related Work,[0],[0]
"We believe this can also be used to enhance previous transition based methods and may also be used in other parsing tasks, e.g., AMR parsing.",5 Related Work,[0],[0]
"This paper proposes Sequence-to-Action, a method which models semantic parsing as an end-to-end semantic graph generation process.",6 Conclusions,[0],[0]
"By leveraging the advantages of semantic graph representation and exploiting the representation learning and prediction ability of Seq2Seq models, our method achieved significant performance improvements on three datasets.",6 Conclusions,[0],[0]
"Furthermore, structure and semantic constraints can be easily incorporated in decoding to enhance semantic parsing.
",6 Conclusions,[0],[0]
"For future work, to solve the problem of the lack of training data, we want to design weakly supervised learning algorithm using denotations (QA pairs) as supervision.",6 Conclusions,[0],[0]
"Furthermore, we want to collect labeled data by designing an interactive UI for annotation assist like (Yih et al., 2016), which uses semantic graphs to annotate the meaning of sentences, since semantic graph is more natural and can be easily annotated without the need of expert knowledge.",6 Conclusions,[0],[0]
This research work is supported by the National Key Research and Development Program of China under Grant No.2017YFB1002104; and the National Natural Science Foundation of China under Grants no. 61572477 and 61772505.,Acknowledgments,[0],[0]
"Moreover, we sincerely thank the reviewers for their valuable comments.",Acknowledgments,[0],[0]
"This paper proposes a neural semantic parsing approach – Sequence-to-Action, which models semantic parsing as an endto-end semantic graph generation process.",abstractText,[0],[0]
Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing.,abstractText,[0],[0]
"Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases.",abstractText,[0],[0]
"Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation.",abstractText,[0],[0]
Experiments show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.,abstractText,[0],[0]
Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 698–707 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1065",text,[0],[0]
"Recently, Neural Machine Translation (NMT) with the attention-based encoder-decoder framework (Bahdanau et al., 2015) has achieved significant improvements in translation quality of many language pairs (Bahdanau et al., 2015; Luong et al., 2015a; Tu et al., 2016; Wu et al., 2016).",1 Introduction,[0],[0]
"In a conventional NMT model, an encoder reads in source sentences of various lengths, and transforms them into sequences of intermediate hidden vector representations.",1 Introduction,[0],[0]
"After weighted by attention operations, combined hidden vectors are used by the decoder to generate translations.",1 Introduction,[0],[0]
"In most of cases, both encoder and decoder are implemented as recurrent neural networks (RNNs).
",1 Introduction,[0],[0]
"∗Contribution during internship at Microsoft Research.
",1 Introduction,[0],[0]
Many methods have been proposed to further improve the sequence-to-sequence NMT model since it was first proposed by Sutskever et al. (2014) and Bahdanau et al. (2015).,1 Introduction,[0],[0]
"Previous work ranges from addressing the problem of out-ofvocabulary words (Jean et al., 2015), designing attention mechanism (Luong et al., 2015a), to more efficient parameter learning (Shen et al., 2016), using source-side syntactic trees for better encoding (Eriguchi et al., 2016) and so on.",1 Introduction,[0],[0]
All these NMT models employ a sequential recurrent neural network for target generations.,1 Introduction,[0],[0]
"Although in theory RNN is able to remember sufficiently long history, we still observe substantial incorrect translations which violate long-distance syntactic constraints.",1 Introduction,[0],[0]
This suggests that it is still very challenging for a linear RNN to learn models that effectively capture many subtle long-range word dependencies.,1 Introduction,[0],[0]
"For example, Figure 1 shows an incorrect translation related to the long-distance dependency.",1 Introduction,[0],[0]
"The translation fragment in italic is locally fluent around the word is, but from a global view the translation is ungrammatical.",1 Introduction,[0],[0]
"Actually, this part of translation should be mostly affected by the distant plural noun foreigners rather than words Venezuelan government nearby.
",1 Introduction,[0],[0]
"Fortunately, such long-distance word correspondence can be well addressed and modeled by syntactic dependency trees.",1 Introduction,[0],[0]
"In Figure 1, the head word foreigners in the partial dependency tree (top dashed box) can provide correct structural context for the next target word, with this information it is more likely to generate the correct word will rather than is.",1 Introduction,[0],[0]
"This structure has been successfully applied to significantly improve the performance of statistical machine translation (Shen et al., 2008).",1 Introduction,[0],[0]
"On the NMT side, introducing target syntactic structures could help solve the problem of ungrammatical output because it can bring two advantages over state-of-the-art NMT models:
698
a) syntactic trees can be used to model the grammatical validity of translation candidates; b) partial syntactic structures can be used as additional context to facilitate future target word prediction.
",1 Introduction,[0],[0]
"However, it is not trivial to build and leverage syntactic structures on the target side in current NMT framework.",1 Introduction,[0],[0]
"Several practical challenges arise:
(1) How to model syntactic structures such as dependency parse trees with recurrent neural network;
(2) How to efficiently perform both target word generation and syntactic structure construction tasks simultaneously in a single neural network;
(3) How to effectively leverage target syntactic context to help target word generation.
",1 Introduction,[0],[0]
"To address these issues, we propose and empirically evaluate a novel Sequence-to-Dependency Neural Machine Translation (SD-NMT) model in our paper.",1 Introduction,[0],[0]
"An SD-NMT model encodes source inputs with bi-directional RNNs and associates them with target word prediction via attention mechanism as in most NMT models, but it comes with a new decoder which is able to jointly generate target translations and construct their syntactic dependency trees.",1 Introduction,[0],[0]
"The key difference from conventional NMT decoders is that we use two RNNs, one for translation generation and the other for dependency parse tree construction, in which incremental parsing is performed with the arc-standard shift-reduce algorithm proposed by Nivre (2004).
",1 Introduction,[0],[0]
"We will describe in detail how these two RNNs work interactively in Section 3.
",1 Introduction,[0],[0]
We evaluate our method on publicly available data sets with Chinese-English and JapaneseEnglish translation tasks.,1 Introduction,[0],[0]
Experimental results show that our model significantly improves translation accuracy over the conventional NMT and SMT baseline systems.,1 Introduction,[0],[0]
"As a new paradigm to machine translation, NMT is an end-to-end framework (Sutskever et al., 2014; Bahdanau et al., 2015) which directly models the conditional probability P (Y |X) of target translation Y = y1,y2,...,yn given source sentence X = x1,x2,...,xm.",2.1 Neural Machine Translation,[0],[0]
An NMT model consists of two parts: an encoder and a decoder.,2.1 Neural Machine Translation,[0],[0]
"Both of them utilize recurrent neural networks which can be a Gated Recurrent Unit (GRU) (Cho et al., 2014) or a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) in practice.",2.1 Neural Machine Translation,[0],[0]
"The encoder bidirectionally encodes a source sentence into a sequence of hidden vectorsH = h1,h2,...,hm with a forward RNN and a backward RNN.",2.1 Neural Machine Translation,[0],[0]
"Then the decoder predicts target words one by one with probability
P (Y |X) = n∏
j=1
P (yj|y<j, H) (1)
Typically, for the jth target word, the probability P (yj |y<j , H) is computed as
P (yj|y<j, H) = g(sj, yj−1, cj) (2)
where g is a nonlinear function that outputs the probability of yj , and sj is the RNN hidden state.",2.1 Neural Machine Translation,[0],[0]
"The context cj is calculated at each timestamp j based on H by the attention network
cj = m∑
k=1
ajkhk (3)
ajk = exp(ejk)∑m i=1 exp(eji)
(4)
ejk = v T a tanh(Wasj−1 + Uahk) (5)
where va, Wa, Ua are the weight matrices.",2.1 Neural Machine Translation,[0],[0]
The attention mechanism is effective to model the correspondences between source and target.,2.1 Neural Machine Translation,[0],[0]
We use a shift-reduce transition-based dependency parser to build the syntactic structure for the target language in our work.,2.2 Dependency Tree Construction,[0],[0]
"Specially, we adopt the arcstandard algorithm (Nivre, 2004) to perform incremental parsing during the translation process.",2.2 Dependency Tree Construction,[0],[0]
"In this algorithm, a stack and a buffer are maintained to store the parsing state over which three kinds of transition actions are applied.",2.2 Dependency Tree Construction,[0],[0]
"Let w0 and w1 be two topmost words in the stack, and w̄ be the current new word in a sequence of input, three transition actions are described as below.
",2.2 Dependency Tree Construction,[0],[0]
"• Shift(SH) : Push w̄ to the stack.
",2.2 Dependency Tree Construction,[0],[0]
• Left-Reduce(LR(d)),2.2 Dependency Tree Construction,[0],[0]
": Link w0 and w1 with dependency label d as w0
d−→w1, and reduce them to the head w0.
",2.2 Dependency Tree Construction,[0],[0]
•,2.2 Dependency Tree Construction,[0],[0]
Right-Reduce(RR(d)),2.2 Dependency Tree Construction,[0],[0]
": Link w0 andw1 with dependency label d as w0
d←−w1, and reduce them to the head w1.
",2.2 Dependency Tree Construction,[0],[0]
"During parsing, an specific structure is used to record the dependency relationship between different words of input sentence.",2.2 Dependency Tree Construction,[0],[0]
The parsing finishes when the stack is empty and all input words are consumed.,2.2 Dependency Tree Construction,[0],[0]
"As each word must be pushed to the stack once and popped off once, the number of actions needed to parse a sentence is always 2n, where n is the length of the sentence (Nivre, 2004).",2.2 Dependency Tree Construction,[0],[0]
"Because each valid transition action sequence corresponds to a unique dependency tree, a dependency tree can also be equivalently represented by a sequence of transition actions.",2.2 Dependency Tree Construction,[0],[0]
An SD-NMT model is an extension to the conventional NMT model augmented with syntactic structural information of target translation.,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Given a source sentenceX = x1,x2,..,xm, its target translation Y = y1,y2,..,yn and Y ’s dependency parse tree T , the goal of the extension is to enable us to compute the joint probability P (Y, T |X).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"As in most structural learning tasks, the full prediction of Y and T is further decomposed into a chain of smaller predictions.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For translation Y , it is generated in the left-to-right order as y1, y2, .., yn following the way in a normal sequence-to-sequence model.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For Y ’s parse tree T , instead of directly modeling the tree itself, we predict a parsing action sequence A which can map Y to T .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Thus at
top level our SD-NMT model can be formulated as
P (Y, T |X) = P (Y,A|X) = P (y1y2..yn, a1, a2..al|X)(6)
where A = a1,a2,..,aj ,..,al 1 with length l (l = 2n), aj ∈ {SH,RR(d),LR(d)}2.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Two recurrent neural networks, Word-RNN and Action-RNN, are used to model generation processes of translation sequence Y and parsing action sequence A respectively.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Figure 2 shows an example how translation Y and its parsing actions are predicted step by step.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Because the lengths of Word-RNN and ActionRNN are different, they are designed to work in a mutually dependent way: a target word is only allowed to be generated when the SH action is predicted in the action sequence.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"In this way, we can perform incremental dependency parsing for translation Y and at the same time track the partial parsing status through the translation generation process.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For notational clarity, we introduce a virtual translation sequence Ŷ =ŷ1,ŷ2,..,ŷj ,..,ŷl for WordRNN which has the same length l with transition action sequence.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"ŷj is defined as
ŷj",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= { yvj δ(SH, aj) = 1 yvj−1 δ(SH, aj) = 0
where δ(SH, aj) is 1 when aj = SH, otherwise 0.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"vj is the index of Y , computed by vj =∑j
i=1",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"δ(SH, ai).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Apparently the mapping from Ŷ 1In the rest of this paper, aj represents the transition action, rather than the attention weight in Equation 4. 2RR(d) refers to a set of RR actions augmented with dependency labels so as to LR(d).
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
𝑇𝑖𝑚𝑒𝑠𝑡𝑎𝑚𝑝 1 2 3 4 𝑗,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"− 1
𝒋
to Y is deterministic, and Y can be easily derived given Ŷ and A.
With the notation of Ŷ , the sequence probability of Y and A can be written as
P (A|X, Ŷ<l)",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= l∏
j=1
P (aj|a<j, X, Ŷ<j) (7)
P (Ŷ |X,A≤l)",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"= l∏
j=1
P (ŷj|ŷ<j, X,A≤j)δ(SH,aj)
(8)
where Ŷ<j refers to the subsequence ŷ1, ŷ2, .., ŷj−1, and A≤j to a1, a2, .., aj .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Based on Equation 7 and 8, the overall joint model can be computed as
P (Y, T |X) = P (A|X, Ŷ<l)× P (Ŷ |X,A≤l) (9)
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"As we have two RNNs in our model, the termination condition is also different from a conventional NMT model.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"In decoding, we maintain a stack to track the parsing configuration, and our model terminates once the Word-RNN predicts a special ending symbol EOS and all the words in the stack have been reduced.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
Figure 3 (a) gives an overview of our SD-NMT model.,3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Due to space limitation, the detailed interconnections between two RNNs are only illustrated at timestamp j. The encoder of our model
follows standard bidirectional RNN configuration.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"At timestamp j during decoding, our model first predicts an action aj by Action-RNN, then WordRNN checks the condition gate δ according to aj .",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"If aj = SH, the Word-RNN will generate a new state (solid arrow) and predict a new target word yvj , otherwise it just copies previous state (dashed arrow) to the current state.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"For example, at timestamp 3, a3 6=",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"SH, the state of Word-RNN is copied from its previous one.",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Meanwhile, ŷ3 = y2 is used as the immediate proceeding word in translation history.
",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"When computing attention scores, we extend Equation 5 by replacing the decoder hidden state with the concatenation of Word-RNN hidden state s and Action-RNN hidden state s′ (gray boxes in Figure 3).",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"The new attention score is then updated as
ejk = v T a tanh(Wa[sj−1; s ′",3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
j−1] + Uahk) (10),3 Sequence-to-Dependency Neural Machine Translation,[0],[0]
"Syntax has been proven useful for sentence generation task (Dyer et al., 2016).",3.1 Syntactic Context for Target Word Prediction,[0],[0]
We propose to leverage target syntax to help translation generation.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"In our model, the syntactic context Kj at timestamp j is defined as a vector which is computed by a feed-forward network based on current
parsing configuration of Action-RNN.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Denote that w0 and w1 are two topmost words in the stack, w0l and w1l are their leftmost modifiers in the partial tree,w0r andw1r their rightmost modifiers respectively.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
We define two unigram features and four bigram features.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
The unigram features are w0 and w1 which are represented by the word embedding vectors.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"The bigram features are w0w0l, w0w0r, w1w1l and w1w1r.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Each of them is computed by bhc = tanh(WbEwh + UbEwhc), h ∈ {0, 1}, c ∈ {l, r}.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"These kinds of feature template have beeb proven effective in dependency parsing task (Zhang and Clark, 2008).",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Based on these features, the syntactic context vector Kj is computed as
Kj = tanh(Wk[Ew0;Ew1] + Uk[b0l; b0r; b1l; b1r]) (11)
where Wk, Uk, Wb, Ub are the weight matrices, E stands for the embedding matrix.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
Figure 2 (b) gives an overview of the construction of Kj .,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Note that zero vector is used for padding the words which are not available in the partial tree, so that all the K vectors have the same input size in computation.
",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"Adding Kj to Equation 2, the probabilities of transition action and word in Equation 7 and 8 are then updated as
P (aj|a<j, X, Ŷ<j) = g(s′j, aj−1, cj,Kj) (12) P (ŷj|ŷ<j, X,A≤j) = g(sj, ŷj−1, cj,Kj) (13)
",3.1 Syntactic Context for Target Word Prediction,[0],[0]
"After each prediction step in Word-RNN and Action-RNN, the syntax context vector K will be updated accordingly.",3.1 Syntactic Context for Target Word Prediction,[0],[0]
Note that K is not used to calculate the recurrent states s in this work.,3.1 Syntactic Context for Target Word Prediction,[0],[0]
"For SD-NMT model, we use the sum of loglikelihoods of word sequence and action sequence as objective function for training algorithm, so that the joint probability of target translations and their parsing trees can be maximized:
J(θ) = ∑
(X,Y,A)∈D log P (A|X, Ŷ<l)+
log P (Ŷ |X,A≤l) (14)
We also use mini-batch for model training.",3.2 Model Training and Decoding,[0],[0]
"As the target dependency trees are known in the bilingual corpus during training, we pre-compute the partial tree state and syntactic context at each time
stamp for each training instance.",3.2 Model Training and Decoding,[0],[0]
"Thus it is easy for the model to process multiple trees in one batch.
",3.2 Model Training and Decoding,[0],[0]
"In the decoding process of an SD-NMT model, the score of each search path is the sum of log probabilities of target word sequence and transition action sequence normalized by the sequence length:
score",3.2 Model Training and Decoding,[0],[0]
"= 1
l
l∑
j=1
log P (aj |a<j , X, Ŷ<j)+
1
n
l∑
j=1
δ(SH, aj) log P (ŷj |ŷ<j , X,A≤j) (15)
where n is word sequence length and l is action sequence length.",3.2 Model Training and Decoding,[0],[0]
"The experiments are conducted on the ChineseEnglish task as well as the Japanese-English translation tasks where the same data set from WAT 2016 ASPEC corpus (Nakazawa et al., 2016) 3 is used for a fair comparison with other work.",4 Experiments,[0],[0]
"In addition to evaluate translation performance, we also investigate the quality of dependency parsing as a by-product and the effect of parsing quality against translation quality.",4 Experiments,[0],[0]
"In the Chinese-English task, the bilingual training data consists of a set of LDC datasets, 4 which has around 2M sentence pairs.",4.1 Setup,[0],[0]
"We use NIST2003 as the development set, and the testsets contain NIST2005, NIST2006, NIST2008 and NIST2012.",4.1 Setup,[0],[0]
"All English words are lowercased.
",4.1 Setup,[0],[0]
"In the Japanese-English task, we use top 1M sentence pairs from ASPEC Japanese-English corpus.",4.1 Setup,[0],[0]
"The development data contains 1,790 sentences, and the test data contains 1,812 sentences with single reference per source sentence.
",4.1 Setup,[0],[0]
"To train SD-NMT model, the target dependency tree references are needed.",4.1 Setup,[0],[0]
"As there is no golden annotation of parse trees over the target training data, we use pseudo parsing results as the target dependency references, which are got from an in-house developed arc-eager dependency parser based on work in (Zhang and Nivre, 2011).
3http://orchid.kuee.kyoto-u.ac.jp/ASPEC/ 4LDC2003E14, LDC2005T10, LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85, LDC2006E92, LDC2003E07, LDC2002E18, LDC2005T06, LDC2003E07, LDC2004T07, LDC2004T08, LDC2005T06
In the neural network training, the vocabulary size is limited to 30K high frequent words for both source and target languages.",4.1 Setup,[0],[0]
"All low frequent words are normalized into a special token unk and post-processed by following the work in (Luong et al., 2015b).",4.1 Setup,[0],[0]
The size of word embedding and transition action embedding is set to 512.,4.1 Setup,[0],[0]
The dimensions of the hidden states for all RNNs are set to 1024.,4.1 Setup,[0],[0]
"All model parameters are initialized randomly with Gaussian distribution (Glorot and Bengio, 2010) and trained on a NVIDIA Tesla K40 GPU.",4.1 Setup,[0],[0]
The stochastic gradient descent (SGD) algorithm is used to tune parameters with a learning rate of 1.0.,4.1 Setup,[0],[0]
The batch size is set to 96.,4.1 Setup,[0],[0]
"In the update procedure, Adadelta (Zeiler, 2012) algorithm is used to automatically adapt the learning rate.",4.1 Setup,[0],[0]
"The beam sizes for both word prediction and transition action prediction are set to 12 in decoding.
",4.1 Setup,[0],[0]
"The baselines in our experiments are a phrasal system and a neural translation system, denoted by HPSMT and RNNsearch respectively.",4.1 Setup,[0],[0]
"HPSMT is an in-house implementation of the hierarchical phrase-based model (Chiang, 2005), where a 4- gram language model is trained using the modified Kneser-Ney smoothing (Kneser and Ney, 1995) algorism over the English Gigaword corpus (LDC2009T13) plus the target data from the bilingual corpus.",4.1 Setup,[0],[0]
"RNNsearch is an in-house implementation of the attention-based neural machine translation model (Bahdanau et al., 2015) using the same parameter settings as our SD-NMT model including word embedding size, hidden vector dimension, beam size, as well as the same mechanism for OOV word processing.
",4.1 Setup,[0],[0]
"The evaluation results are reported with the case-insensitive IBM BLEU-4 (Papineni et al., 2002).",4.1 Setup,[0],[0]
A statistical significance test is performed using the bootstrap resampling method proposed by Koehn (2004) with a 95% confidence level.,4.1 Setup,[0],[0]
"For Japanese-English task, we use the official eval-
uation procedure provided by WAT 2016.5, where both BLEU and RIBES (Isozaki et al., 2010) are used for evaluation.",4.1 Setup,[0],[0]
We evaluate our method on the Chinese-English translation task.,4.2 Evaluation on Chinese-English Translation,[0],[0]
The evaluation results over all NIST test sets against baselines are listed in Table 1.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"Generally, RNNsearch outperforms HPSMT by 3.78 BLEU points on average while SD-NMT surpasses RNNsearch 2.03 BLUE point gains on average, which shows that NMT models usually achieve better results than SMT models, and our proposed sequence-to-dependency NMT model performs much better than traditional sequence-tosequence NMT model.
",4.2 Evaluation on Chinese-English Translation,[0],[0]
We also investigate the effect of syntactic knowledge context by excluding its computation in Equation 12 and 13.,4.2 Evaluation on Chinese-English Translation,[0],[0]
The alternative model is denoted by SD-NMT\K.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"According to Table 1, SD-NMT\K outperforms RNNsearch by 0.54 BLEU points but degrades SD-NMT by 1.49 BLEU points on average, which demonstrates that the long distance dependencies captured by the target syntactic knowledge context, such as leftmost/rightmost children together with their dependency relationships, really bring strong positive effects on the prediction of target words.
",4.2 Evaluation on Chinese-English Translation,[0],[0]
"In addition to translation quality, we compare the perplexity (PPL) changes on the development set in terms of numbers of training mini-batches for RNNsearch and SD-NMT in Figure 4.",4.2 Evaluation on Chinese-English Translation,[0],[0]
"We can see that the PPL of SD-NMT is initially higher than that of RNNsearch, but decreased to be lower over time.",4.2 Evaluation on Chinese-English Translation,[0],[0]
This is mainly because the quality of parse tree is too poor at the beginning which degrades translation quality and leads to higher PPL.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"After some training iterations, the SD-NMT
5http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/index .html
model learns reasonable inferences of parse trees which begins to help target word generation and leads to lower PPL.
25 17.26 15.74
26 18.76 16.58
27 17.62 15.88
In our experiments, the time cost of SD-NMT is two times of that for RNNsearch due to a more complicated model structure.",4.2 Evaluation on Chinese-English Translation,[0],[0]
But we think it is a worthy trade to pursue high quality translations.,4.2 Evaluation on Chinese-English Translation,[0],[0]
"In this section, we report results on the JapaneseEnglish translation task.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"To ensure fair comparisons, we use the same training data and follow the pre-processing steps recommended in WAT 20166.",4.3 Evaluation on Japanese-English Translation,[0],[0]
Table 2 shows the comparison results from 8 systems with the evaluation metrics of BLEU and RIBES.,4.3 Evaluation on Japanese-English Translation,[0],[0]
The results in the first 3 rows are produced by SMT systems taken from the official WAT 2016.,4.3 Evaluation on Japanese-English Translation,[0],[0]
"The remaining results are produced by NMT systems, among which the bottom two row results are taken from our in-house NMT systems and others refer to the work in (Cromieres, 2016;
6http://lotus.kuee.kyoto-u.ac.jp/WAT/baseline/data PreparationJE.html
Cromieres et al., 2016) that are the competitive NMT results on WAT 2016.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"According to Table 2, NMT results still outperform SMT results similar to our Chinese-English evaluation results.",4.3 Evaluation on Japanese-English Translation,[0],[0]
"The SD-NMT model significantly outperforms most other NMT models, which shows that our proposed approach to modeling target dependency tree benefit NMT systems since our RNNsearch baseline achieves comparable performance with the single layer attention-based NMT system in (Cromieres, 2016).",4.3 Evaluation on Japanese-English Translation,[0],[0]
"Note that our SD-NMT gets comparable results with the 4 single-layer ensemble model in (Cromieres, 2016; Cromieres et al., 2016).",4.3 Evaluation on Japanese-English Translation,[0],[0]
We believe SD-NMT can get more improvements with an ensemble of multiple models in future experiments.,4.3 Evaluation on Japanese-English Translation,[0],[0]
The interaction effect between dependency tree conduction and target word generation is investigated in this section.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
The experiments are conducted on the Chinese-English task over multiple test sets.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
We evaluate how the quality of dependency trees affect the performance of translation.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"In the decoding phase of SD-NMT, beam search is applied to the generations of both transition and actions as illustrated in Equation 15.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"Intuitively, the larger the beam size of action prediction is, the better the dependency tree quality is.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"We fix the beam size for generating target words to 12, and change the beam size for action prediction to see the difference.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
Figure 5 shows the evaluation results of all test sets.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
There is a tendency for BLEU scores to increase with the growth of action prediction beam size.,4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"The reason is that the translation quality increases as the quality of dependency tree improves, which shows the construction of dependency trees can boost the generation of target
4 38.77 40.64 32.06 30.63
6 38.93 41.32 32.63 31.07
8 39.34 41.52 32.88 31.32
10 39.32 41.65 32.82 31.41 12 39.38 41.81 33.06 31.43
words, and vice versa we believe.",4.4 Effect of the Parsing Accuracy upon Translation Quality,[0],[0]
"As a by-product, the quality of dependency trees not only affects the performance of target word generation, but also influences the possible downstream processors or tasks such as text analyses.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
The direct evaluation of tree quality is not feasible due to the unavailable golden references.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
So we resort to estimating the consistency between the by-products and the parsing results of our standalone dependency parser with state-of-the-art performance.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"The higher the consistency is, the closer the performance of by-product is to the standalone parser.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"To reduce the influence of ill-formed data as much as possible, we build the evaluation data set by heuristically selecting 360 SD-NMT translation results together with their dependency trees from NIST test sets where both source- and target-side do not contain unk and have a length of 20-30.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
We then take the parsing results of the stand-alone parser for these translations as references to indirectly estimate the quality of byproducts.,4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"We get a UAS (unlabeled attachment score) of 94.96% and a LAS (labeled attachment score) of 93.92%, which demonstrates that the dependency trees produced by SD-NMT are much similar with the parsing results from the standalone parser.",4.5 Quality Estimation of Dependency Tree Construction,[0],[0]
"In this section, we give a case study to explain how our method works.",4.6 Translation Example,[0],[0]
Figure 6 shows a translation example from the NIST testsets.,4.6 Translation Example,[0],[0]
"SMT and RNNsearch refer to the translation results from the
baselines HPSMT and NMT.",4.6 Translation Example,[0],[0]
"For our SD-NMT model, we list both the generated translation and its corresponding dependency tree.",4.6 Translation Example,[0],[0]
"We find that the translation of SMT is disfluent and ungrammatical, whereas RNNsearch is better than SMT.",4.6 Translation Example,[0],[0]
"Although the translation of RNNsearch is locally fluent around word “have” in the rectangle, both its grammar is incorrect and its meaning is inaccurate from a global view.",4.6 Translation Example,[0],[0]
The word “have” should be in a singular form as its subject is “safety” rather than “workers”.,4.6 Translation Example,[0],[0]
"For our SD-NMT model, we can see that the translation is much better than baselines and the dependency tree is reasonable.",4.6 Translation Example,[0],[0]
"The reason is that after generating the word “workers”, the previous subtree in the gray region is transformed to the syntactic context which can guide the generation of the next word as illustrated by the dashed arrow.",4.6 Translation Example,[0],[0]
Thus our model is more likely to generate the correct verb “is” with singular form.,4.6 Translation Example,[0],[0]
"In addition, the global structure helps the model correctly identify the inverted sentence pattern of the former translated part and make better choices for the future translation (“only when .. can ..” in our translation, “only when .. will ..”",4.6 Translation Example,[0],[0]
"in the reference), which remains a challenge for conventional NMT model.",4.6 Translation Example,[0],[0]
"Incorporating linguistic knowledge into machine translation has been extensively studied in Statistic Machine Translation (SMT) (Galley et al., 2006; Shen et al., 2008; Liu et al., 2006).",5 Related Work,[0],[0]
Liu et al. (2006) proposed a tree-to-string alignment template for SMT to leverage source side syntactic information.,5 Related Work,[0],[0]
Shen et al. (2008) proposed a target dependency language model for SMT to employ target-side structured information.,5 Related Work,[0],[0]
"These methods show promising improvement for SMT.
",5 Related Work,[0],[0]
"Recently, neural machine translation (NMT) has achieved better performance than SMT in many language pairs (Luong et al., 2015a; Zhang et al., 2016; Shen et al., 2016; Wu et al., 2016; Neubig, 2016).",5 Related Work,[0],[0]
"In a vanilla NMT model, source and target sentences are treated as sequences where the syntactic knowledge of both sides is neglected.",5 Related Work,[0],[0]
Some effort has been done to incorporate source syntax into NMT.,5 Related Work,[0],[0]
Eriguchi et al. (2016) proposed a tree-to-sequence attentional NMT model where source-side parse tree was used and achieved promising improvement.,5 Related Work,[0],[0]
"Intuitively, adding source syntactic information to
[Source] 只有施工人员的安全得到了保证 , 才能继续施工 .",5 Related Work,[0],[0]
[Reference] only when the safety of the workers is guaranteed will they continue with the project .,5 Related Work,[0],[0]
"[HPSMT] only safety is assured of construction personnel , to continue construction .",5 Related Work,[0],[0]
[RNNsearch] only when the safety of construction workers have been guaranteed to continue construction .,5 Related Work,[0],[0]
"[SD-NMT] only when the safety of the workers is guaranteed can we continue to work .
",5 Related Work,[0],[0]
"NMT is straightforward, because the source sentence is definitive and easy to attach extra information.",5 Related Work,[0],[0]
"However, it is non-trivial to add target syntax as target words are uncertain in decoding process.",5 Related Work,[0],[0]
"Up to now, there is few work that attempts to build and leverage target syntactic information for NMT.
",5 Related Work,[0],[0]
There has been work that incorporates syntactic information into NLP tasks with neural networks.,5 Related Work,[0],[0]
Dyer et al. (2016) presented a RNN grammar for parsing and language modeling.,5 Related Work,[0],[0]
"They replaced SH with a set of generative actions to generate words under a Stack LSTM framework (Dyer et al., 2015), which achieves promising results for language modeling on the Penn Treebank data.",5 Related Work,[0],[0]
"In our work, we propose to involve target syntactic trees into NMT model to jointly learn target translation and dependency parsing where target syntactic context over the parse tree is used to improve the translation quality.",5 Related Work,[0],[0]
"In this paper, we propose a novel string-todependency translation model over NMT.",6 Conclusion and Future Work,[0],[0]
Our model jointly performs target word generation and arc-standard dependency parsing.,6 Conclusion and Future Work,[0],[0]
"Experimental results show that our method can boost the two procedures and achieve significant improvements on the translation quality of NMT systems.
",6 Conclusion and Future Work,[0],[0]
"In future work, along this research direction, we will try to integrate other prior knowledge, such as
semantic information, into NMT systems.",6 Conclusion and Future Work,[0],[0]
"In addition, we will apply our method to other sequenceto-sequence tasks, such as text summarization, to verify the effectiveness.",6 Conclusion and Future Work,[0],[0]
We are grateful to the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
We also thank Shujie Liu and Zhirui Zhang for the helpful discussions.,Acknowledgments,[0],[0]
"Nowadays a typical Neural Machine Translation (NMT) model generates translations from left to right as a linear sequence, during which latent syntactic structures of the target sentences are not explicitly concerned.",abstractText,[0],[0]
"Inspired by the success of using syntactic knowledge of target language for improving statistical machine translation, in this paper we propose a novel Sequence-to-Dependency",abstractText,[0],[0]
"Neural Machine Translation (SD-NMT) method, in which the target word sequence and its corresponding dependency structure are jointly constructed and modeled, and this structure is used as context to facilitate word generations.",abstractText,[0],[0]
Experimental results show that the proposed method significantly outperforms state-of-the-art baselines on Chinese-English and JapaneseEnglish translation tasks.,abstractText,[0],[0]
Sequence-to-Dependency Neural Machine Translation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1296–1306, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Sequence-to-Sequence learning with deep neural networks (herein, seq2seq) (Sutskever et al., 2011; Sutskever et al., 2014) has rapidly become a very useful and surprisingly general-purpose tool for natural language processing.",1 Introduction,[0],[0]
"In addition to demonstrating impressive results for machine translation (Bahdanau et al., 2015), roughly the same model and training have also proven to be useful for sentence compression (Filippova et al., 2015), parsing (Vinyals et al., 2015), and dialogue systems (Serban et al., 2016), and they additionally underlie other
text generation applications, such as image or video captioning (Venugopalan et al., 2015; Xu et al., 2015).
",1 Introduction,[0],[0]
"The dominant approach to training a seq2seq system is as a conditional language model, with training maximizing the likelihood of each successive target word conditioned on the input sequence and the gold history of target words.",1 Introduction,[0],[0]
"Thus, training uses a strictly word-level loss, usually cross-entropy over the target vocabulary.",1 Introduction,[0],[0]
"This approach has proven to be very effective and efficient for training neural language models, and seq2seq models similarly obtain impressive perplexities for word-generation tasks.
",1 Introduction,[0],[0]
"Notably, however, seq2seq models are not used as conditional language models at test-time; they must instead generate fully-formed word sequences.",1 Introduction,[0],[0]
"In practice, generation is accomplished by searching over output sequences greedily or with beam search.",1 Introduction,[0],[0]
"In this context, Ranzato et al. (2016) note that the combination of the training and generation scheme just described leads to at least two major issues:
1.",1 Introduction,[0],[0]
"Exposure Bias: the model is never exposed to its own errors during training, and so the inferred histories at test-time do not resemble the gold training histories.
",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU (Papineni et al., 2002).
",1 Introduction,[0],[0]
"We might additionally add the concern of label bias (Lafferty et al., 2001) to the list, since wordprobabilities at each time-step are locally normalized, guaranteeing that successors of incorrect his-
1296
tories receive the same mass as do the successors of the true history.
",1 Introduction,[0],[0]
"In this work we develop a non-probabilistic variant of the seq2seq model that can assign a score to any possible target sequence, and we propose a training procedure, inspired by the learning as search optimization (LaSO) framework of Daumé III and Marcu (2005), that defines a loss function in terms of errors made during beam search.",1 Introduction,[0],[0]
"Furthermore, we provide an efficient algorithm to backpropagate through the beam-search procedure during seq2seq training.
",1 Introduction,[0],[0]
"This approach offers a possible solution to each of the three aforementioned issues, while largely maintaining the model architecture and training efficiency of standard seq2seq learning.",1 Introduction,[0],[0]
"Moreover, by scoring sequences rather than words, our approach also allows for enforcing hard-constraints on sequence generation at training time.",1 Introduction,[0],[0]
"To test out the effectiveness of the proposed approach, we develop a general-purpose seq2seq system with beam search optimization.",1 Introduction,[0],[0]
"We run experiments on three very different problems: word ordering, syntactic parsing, and machine translation, and compare to a highlytuned seq2seq system with attention (Luong et al., 2015).",1 Introduction,[0],[0]
"The version with beam search optimization shows significant improvements on all three tasks, and particular improvements on tasks that require difficult search.",1 Introduction,[0],[0]
"The issues of exposure bias and label bias have received much attention from authors in the structured prediction community, and we briefly review some of this work here.",2 Related Work,[0],[0]
"One prominent approach to combating exposure bias is that of SEARN (Daumé III et al., 2009), a meta-training algorithm that learns a search policy in the form of a cost-sensitive classifier trained on examples generated from an interpolation of an oracle policy and the model’s current (learned) policy.",2 Related Work,[0],[0]
"Thus, SEARN explicitly targets the mismatch between oracular training and non-oracular (often greedy) test-time inference by training on the output of the model’s own policy.",2 Related Work,[0],[0]
"DAgger (Ross et al., 2011) is a similar approach, which differs in terms of how training examples are generated and aggregated, and there have additionally been impor-
tant refinements to this style of training over the past several years (Chang et al., 2015).",2 Related Work,[0],[0]
"When it comes to training RNNs, SEARN/DAgger has been applied under the name “scheduled sampling” (Bengio et al., 2015), which involves training an RNN to generate the t+ 1’st token in a target sequence after consuming either the true t’th token, or, with probability that increases throughout training, the predicted t’th token.
",2 Related Work,[0],[0]
"Though technically possible, it is uncommon to use beam search when training with SEARN/DAgger.",2 Related Work,[0],[0]
"The early-update (Collins and Roark, 2004) and LaSO (Daumé III and Marcu, 2005) training strategies, however, explicitly account for beam search, and describe strategies for updating parameters when the gold structure becomes unreachable during search.",2 Related Work,[0],[0]
"Early update and LaSO differ primarily in that the former discards a training example after the first search error, whereas LaSO resumes searching after an error from a state that includes the gold partial structure.",2 Related Work,[0],[0]
"In the context of feed-forward neural network training, early update training has been recently explored in a feedforward setting by Zhou et al. (2015) and Andor et al. (2016).",2 Related Work,[0],[0]
"Our work differs in that we adopt a LaSO-like paradigm (with some minor modifications), and apply it to the training of seq2seq RNNs (rather than feed-forward networks).",2 Related Work,[0],[0]
"We also note that Watanabe and Sumita (2015) apply maximumviolation training (Huang et al., 2012), which is similar to early-update, to a parsing model with recurrent components, and that Yazdani and Henderson (2015) use beam-search in training a discriminative, locally normalized dependency parser with recurrent components.
",2 Related Work,[0],[0]
Recently authors have also proposed alleviating exposure bias using techniques from reinforcement learning.,2 Related Work,[0],[0]
"Ranzato et al. (2016) follow this approach to train RNN decoders in a seq2seq model, and they obtain consistent improvements in performance, even over models trained with scheduled sampling.",2 Related Work,[0],[0]
"As Daumé III and Marcu (2005) note, LaSO is similar to reinforcement learning, except it does not require “exploration” in the same way.",2 Related Work,[0],[0]
"Such exploration may be unnecessary in supervised text-generation, since we typically know the gold partial sequences at each time-step.",2 Related Work,[0],[0]
"Shen et al. (2016) use minimum risk training (approximated by
sampling) to address the issues of exposure bias and loss-evaluation mismatch for seq2seq MT, and show impressive performance gains.
",2 Related Work,[0],[0]
"Whereas exposure bias results from training in a certain way, label bias results from properties of the model itself.",2 Related Work,[0],[0]
"In particular, label bias is likely to affect structured models that make sub-structure predictions using locally-normalized scores.",2 Related Work,[0],[0]
"Because the neural and non-neural literature on this point has recently been reviewed by Andor et al. (2016), we simply note here that RNN models are typically locally normalized, and we are unaware of any specifically seq2seq work with RNNs that does not use locally-normalized scores.",2 Related Work,[0],[0]
"The model we introduce here, however, is not locally normalized, and so should not suffer from label bias.",2 Related Work,[0],[0]
"We also note that there are some (non-seq2seq) exceptions to the trend of locally normalized RNNs, such as the work of Sak et al. (2014) and Voigtlaender et al. (2015), who train LSTMs in the context of HMMs for speech recognition using sequence-level objectives; their work does not consider search, however.",2 Related Work,[0],[0]
"In the simplest seq2seq scenario, we are given a collection of source-target sequence pairs and tasked with learning to generate target sequences from source sequences.",3 Background and Notation,[0],[0]
"For instance, we might view machine translation in this way, where in particular we attempt to generate English sentences from (corresponding) French sentences.",3 Background and Notation,[0],[0]
"Seq2seq models are part of the broader class of “encoder-decoder” models (Cho et al., 2014), which first use an encoding model to transform a source object into an encoded representation x. Many different sequential (and non-sequential) encoders have proven to be effective for different source domains.",3 Background and Notation,[0],[0]
"In this work we are agnostic to the form of the encoding model, and simply assume an abstract source representation x.
Once the input sequence is encoded, seq2seq models generate a target sequence using a decoder.",3 Background and Notation,[0],[0]
The decoder is tasked with generating a target sequence of words from a target vocabulary V .,3 Background and Notation,[0],[0]
"In particular, words are generated sequentially by conditioning on the input representation x and on the previously generated words or history.",3 Background and Notation,[0],[0]
"We use the notation w1:T to refer to an arbitrary word sequence
of length T , and the notation y1:T to refer to the gold (i.e., correct) target word sequence for an input x.
Most seq2seq systems utilize a recurrent neural network (RNN) for the decoder model.",3 Background and Notation,[0],[0]
"Formally, a recurrent neural network is a parameterized nonlinear function RNN that recursively maps a sequence of vectors to a sequence of hidden states.",3 Background and Notation,[0],[0]
"Let m1, . . .",3 Background and Notation,[0],[0]
",mT be a sequence of T vectors, and let h0 be some initial state vector.",3 Background and Notation,[0],[0]
"Applying an RNN to any such sequence yields hidden states ht at each time-step t, as follows:
ht ← RNN(mt,ht−1;θ),
where θ is the set of model parameters, which are shared over time.",3 Background and Notation,[0],[0]
"In this work, the vectors mt will always correspond to the embeddings of a target word sequence w1:T , and so we will also write ht ← RNN(wt,ht−1;θ), with wt standing in for its embedding.
RNN decoders are typically trained to act as conditional language models.",3 Background and Notation,[0],[0]
"That is, one attempts to model the probability of the t’th target word conditioned on x and the target history by stipulating that p(wt|w1:t−1,x) = g(wt,ht−1,x), for some parameterized function g typically computed with an affine layer followed by a softmax.",3 Background and Notation,[0],[0]
"In computing these probabilities, the state ht−1 represents the target history, and h0 is typically set to be some function of x.",3 Background and Notation,[0],[0]
"The complete model (including encoder) is trained, analogously to a neural language model, to minimize the cross-entropy loss at each time-step while conditioning on the gold history in the training data.",3 Background and Notation,[0],[0]
"That is, the model is trained to minimize − ln∏Tt=1 p(yt|y1:t−1,x).
",3 Background and Notation,[0],[0]
"Once the decoder is trained, discrete sequence generation can be performed by approximately maximizing the probability of the target sequence under the conditional distribution, ŷ1:T = argbeamw1:T ∏T t=1",3 Background and Notation,[0],[0]
"p(wt|w1:t−1,x), where we use the notation argbeam to emphasize that the decoding process requires heuristic search, since the RNN model is non-Markovian.",3 Background and Notation,[0],[0]
"In practice, a simple beam search procedure that explores K prospective histories at each time-step has proven to be an effective decoding approach.",3 Background and Notation,[0],[0]
"However, as noted above, decoding in this manner after conditional languagemodel style training potentially suffers from the is-
sues of exposure bias and label bias, which motivates the work of this paper.",3 Background and Notation,[0],[0]
We begin by making one small change to the seq2seq modeling framework.,4 Beam Search Optimization,[0],[0]
"Instead of predicting the probability of the next word, we instead learn to produce (non-probabilistic) scores for ranking sequences.",4 Beam Search Optimization,[0],[0]
"Define the score of a sequence consisting of history w1:t−1 followed by a single word wt as f(wt,ht−1,x), where f is a parameterized function examining the current hidden-state of the relevant RNN at time t− 1 as well as the input representation x.",4 Beam Search Optimization,[0],[0]
"In experiments, our f will have an identical form to g but without the final softmax transformation (which transforms unnormalized scores into probabilities), thereby allowing the model to avoid issues associated with the label bias problem.
",4 Beam Search Optimization,[0],[0]
"More importantly, we also modify how this model is trained.",4 Beam Search Optimization,[0],[0]
Ideally we would train by comparing the gold sequence to the highest-scoring complete sequence.,4 Beam Search Optimization,[0],[0]
"However, because finding the argmax sequence according to this model is intractable, we propose to adopt a LaSO-like (Daumé III and Marcu, 2005) scheme to train, which we will refer to as beam search optimization (BSO).",4 Beam Search Optimization,[0],[0]
"In particular, we define a loss that penalizes the gold sequence falling off the beam during training.1 The proposed training approach is a simple way to expose the model to incorrect histories and to match the training procedure to test generation.",4 Beam Search Optimization,[0],[0]
"Furthermore we show that it can be implemented efficiently without changing the asymptotic run-time of training, beyond a factor of the beam size K.",4 Beam Search Optimization,[0],[0]
We now formalize this notion of a search-based loss for RNN training.,4.1 Search-Based Loss,[0],[0]
"Assume we have a set St of K candidate sequences of length t. We can calculate a score for each sequence in St using a scoring function f parameterized with an RNN, as above, and we define the sequence ŷ(K)1:t ∈St to be the K’th ranked
1Using a non-probabilistic model further allows us to incur no loss (and thus require no update to parameters) when the gold sequence is on the beam; this contrasts with models based on a CRF loss, such as those of Andor et al. (2016) and Zhou et al. (2015), though in training those models are simply not updated when the gold sequence remains on the beam.
",4.1 Search-Based Loss,[0],[0]
sequence in St according to f .,4.1 Search-Based Loss,[0],[0]
"That is, assuming distinct scores,
|{ŷ(k)1:t ∈St |",4.1 Search-Based Loss,[0],[0]
"f(ŷ (k) t , ĥ
(k) t−1) >",4.1 Search-Based Loss,[0],[0]
"f(ŷ (K) t , ĥ (K) t−1)}| =",4.1 Search-Based Loss,[0],[0]
"K − 1,
where ŷ(k)t is the t’th token in ŷ (k) 1:t , ĥ
(k)
t−1 is the RNN state corresponding to its t− 1’st step, and where we have omitted the x argument to f for brevity.
",4.1 Search-Based Loss,[0],[0]
We now define a loss function that gives loss each time the score of the gold prefix y1:t does not exceed that of ŷ(K)1:,4.1 Search-Based Loss,[0],[0]
"t by a margin:
L(f) = T∑
t=1
∆(ŷ (K) 1:t ) [ 1− f(yt,ht−1) + f(ŷ(K)t , ĥ (K) t−1) ] .
",4.1 Search-Based Loss,[0],[0]
"Above, the ∆(ŷ(K)1:t ) term denotes a mistake-specific cost-function, which allows us to scale the loss depending on the severity of erroneously predicting ŷ",4.1 Search-Based Loss,[0],[0]
"(K) 1:t ; it is assumed to return 0 when the margin requirement is satisfied, and a positive number otherwise.",4.1 Search-Based Loss,[0],[0]
It is this term that allows us to use sequencerather than word-level costs in training (addressing the 2nd issue in the introduction).,4.1 Search-Based Loss,[0],[0]
"For instance, when training a seq2seq model for machine translation, it may be desirable to have ∆(ŷ(K)1:t ) be inversely related to the partial sentence-level BLEU score of ŷ(K)1:",4.1 Search-Based Loss,[0],[0]
"t with y1:t; we experiment along these lines in Section 5.3.
",4.1 Search-Based Loss,[0],[0]
"Finally, because we want the full gold sequence to be at the top of the beam at the end of search, when t=T we modify the loss to require the score of y1:T to exceed the score of the highest ranked incorrect prediction by a margin.
",4.1 Search-Based Loss,[0],[0]
"We can optimize the loss L using a two-step process: (1) in a forward pass, we compute candidate sets St and record margin violations (sequences with non-zero loss); (2) in a backward pass, we backpropagate the errors through the seq2seq RNNs.",4.1 Search-Based Loss,[0],[0]
"Unlike standard seq2seq training, the first-step requires running search (in our case beam search) to find margin violations.",4.1 Search-Based Loss,[0],[0]
The second step can be done by adapting back-propagation through time (BPTT).,4.1 Search-Based Loss,[0],[0]
We next discuss the details of this process.,4.1 Search-Based Loss,[0],[0]
"In order to minimize this loss, we need to specify a procedure for constructing candidate sequences ŷ(k)1:t
at each time step t so that we find margin violations.",4.2 Forward: Find Violations,[0],[0]
We follow LaSO (rather than early-update 2; see Section 2) and build candidates in a recursive manner.,4.2 Forward: Find Violations,[0],[0]
"If there was no margin violation at t−1, then St is constructed using a standard beam search update.",4.2 Forward: Find Violations,[0],[0]
"If there was a margin violation, St is constructed as the K best sequences assuming the gold history y1:t−1 through time-step t−1.
",4.2 Forward: Find Violations,[0],[0]
"Formally, assume the function succ maps a sequence w1:t−1 ∈Vt−1 to the set of all valid sequences of length t that can be formed by appending to it a valid word w∈V .",4.2 Forward: Find Violations,[0],[0]
"In the simplest, unconstrained case, we will have
succ(w1:t−1) = {w1:t−1, w | w ∈ V}.
",4.2 Forward: Find Violations,[0],[0]
"As an important aside, note that for some problems it may be preferable to define a succ function which imposes hard constraints on successor sequences.",4.2 Forward: Find Violations,[0],[0]
"For instance, if we would like to use seq2seq models for parsing (by emitting a constituency or dependency structure encoded into a sequence in some way), we will have hard constraints on the sequences the model can output, namely, that they represent valid parses.",4.2 Forward: Find Violations,[0],[0]
"While hard constraints such as these would be difficult to add to standard seq2seq at training time, in our framework they can naturally be added to the succ function, allowing us to train with hard constraints; we experiment along these lines in Section 5.3, where we refer to a model trained with constrained beam search as ConBSO.
",4.2 Forward: Find Violations,[0],[0]
"Having defined an appropriate succ function, we specify the candidate set as:
St = topK { succ(y1:t−1) violation at t−1⋃K
k=1",4.2 Forward: Find Violations,[0],[0]
succ(ŷ,4.2 Forward: Find Violations,[0],[0]
"(k) 1:t−1) otherwise,
where we have a margin violation at t−1 iff f(yt−1,ht−2)",4.2 Forward: Find Violations,[0],[0]
<,4.2 Forward: Find Violations,[0],[0]
"f(ŷ (K) t−1 , ĥ (K)
t−2) + 1, and where topK considers the scores given by f .",4.2 Forward: Find Violations,[0],[0]
"This search procedure is illustrated in the top portion of Figure 1.
",4.2 Forward: Find Violations,[0],[0]
"In the forward pass of our training algorithm, shown as the first part of Algorithm 1, we run this version of beam search and collect all sequences and their hidden states that lead to losses.
",4.2 Forward: Find Violations,[0],[0]
"2We found that training with early-update rather than (delayed) LaSO did not work well, even after pre-training.",4.2 Forward: Find Violations,[0],[0]
Given the success of early-update in many NLP tasks this was somewhat surprising.,4.2 Forward: Find Violations,[0],[0]
We leave this question to future work.,4.2 Forward: Find Violations,[0],[0]
Once we have collected margin violations we can run backpropagation to compute parameter updates.,4.3 Backward: Merge Sequences,[0],[0]
Assume a margin violation occurs at time-step t between the predicted history ŷ(K)1:,4.3 Backward: Merge Sequences,[0],[0]
t and the gold history y1:t.,4.3 Backward: Merge Sequences,[0],[0]
"As in standard seq2seq training we must back-propagate this error through the gold history; however, unlike seq2seq we also have a gradient for the wrongly predicted history.
",4.3 Backward: Merge Sequences,[0],[0]
"Recall that to back-propagate errors through an RNN we run a recursive backward procedure — denoted below by BRNN — at each time-step t, which accumulates the gradients of next-step and future losses with respect to ht.",4.3 Backward: Merge Sequences,[0],[0]
"We have:
∇htL ← BRNN(∇htLt+1,∇ht+1L),
where Lt+1 is the loss at step t+ 1, deriving, for instance, from the score f(yt+1,ht).",4.3 Backward: Merge Sequences,[0],[0]
"Running this BRNN procedure from t=T − 1 to t= 0 is known as back-propagation through time (BPTT).
",4.3 Backward: Merge Sequences,[0],[0]
"In determining the total computational cost of back-propagation here, first note that in the worst case there is one violation at each time-step, which leads to T independent, incorrect sequences.",4.3 Backward: Merge Sequences,[0],[0]
Since we need to call BRNN O(T ),4.3 Backward: Merge Sequences,[0],[0]
"times for each sequence, a naive strategy of running BPTT for each incorrect sequence would lead to an O(T 2) backward pass, rather than the O(T ) time required for the standard seq2seq approach.
",4.3 Backward: Merge Sequences,[0],[0]
"Fortunately, our combination of search-strategy and loss make it possible to efficiently share BRNN operations.",4.3 Backward: Merge Sequences,[0],[0]
"This shared structure comes
naturally from the LaSO update, which resets the beam in a convenient way.
",4.3 Backward: Merge Sequences,[0],[0]
We informally illustrate the process in Figure 1.,4.3 Backward: Merge Sequences,[0],[0]
The top of the diagram shows a possible sequence of ŷ(k)1:t formed during search with a beam of size 3 for the target sequence y = “a red dog runs quickly today.”,4.3 Backward: Merge Sequences,[0],[0]
"When the gold sequence falls off the beam at t= 4, search resumes with S5 = succ(y1:4), and so all subsequent predicted sequences have y1:4 as a prefix and are thus functions of h4.",4.3 Backward: Merge Sequences,[0],[0]
"Moreover, because our loss function only involves the scores of the gold prefix and the violating prefix, we end up with the relatively simple computation tree shown at the bottom of Figure 1.",4.3 Backward: Merge Sequences,[0],[0]
"It is evident that we can backpropagate in a single pass, accumulating gradients from sequences that diverge from the gold at the time-step that precedes their divergence.",4.3 Backward: Merge Sequences,[0],[0]
"The second half of Algorithm 1 shows this explicitly for a single sequence, though it is straightforward to extend the algorithm to operate in batch.3",4.3 Backward: Merge Sequences,[0],[0]
"We run experiments on three different tasks, comparing our approach to the seq2seq baseline, and to other relevant baselines.",5 Data and Methods,[0],[0]
"While the method we describe applies to seq2seq RNNs in general, for all experiments we use the global attention model of Luong et al. (2015) — which consists of an LSTM (Hochreiter and Schmidhuber, 1997) encoder and an LSTM decoder with a global attention model — as both the baseline seq2seq model (i.e., as the model that computes the g in Section 3) and as the model that computes our sequence-scores f(wt,ht−1,x).",5.1 Model,[0],[0]
"As in Luong et al. (2015), we also use “input feeding,” which involves feeding the attention distribution from the previous time-step into the decoder at the current step.",5.1 Model,[0],[0]
"This model architecture has been found to be highly performant for neural machine translation and other seq2seq tasks.
",5.1 Model,[0],[0]
"3We also note that because we do not update the parameters until after the T ’th search step, our training procedure differs slightly from LaSO (which is online), and in this aspect is essentially equivalent to the “delayed LaSO update” of Björkelund and Kuhn (2014).
",5.1 Model,[0],[0]
"Algorithm 1 Seq2seq Beam-Search Optimization 1: procedure BSO(x,Ktr, succ) 2: /*FORWARD*/",5.1 Model,[0],[0]
3:,5.1 Model,[0],[0]
"Init empty storage ŷ1:T and ĥ1:T ; init S1 4: r ← 0; violations← {0} 5: for t = 1, . . .",5.1 Model,[0],[0]
", T do 6: K =Ktr if t 6=T else argmax
k:ŷ (k) 1:t 6=y1:t
f(ŷ (k) t , ĥ
(k) t−1)
7: if f(yt,ht−1) <",5.1 Model,[0],[0]
"f(ŷ (K) t , ĥ
(K) t−1) + 1 then
8: ĥr:t−1 ← ĥ (K)
",5.1 Model,[0],[0]
"r:t−1 9: ŷr+1:t ← ŷ(K)r+1:t
10: Add t to violations 11:",5.1 Model,[0],[0]
r ← t 12: St+1,5.1 Model,[0],[0]
← topK(succ(y1:t)) 13: else 14: St+1 ← topK( ⋃K k=1,5.1 Model,[0],[0]
succ(ŷ,5.1 Model,[0],[0]
(k) 1:t )),5.1 Model,[0],[0]
"16: grad hT ← 0; grad ĥT ← 0 17: for t = T − 1, . . .",15: /*BACKWARD*/,[0],[0]
", 1 do 18: grad ht←BRNN(∇htLt+1, grad ht+1) 19: grad ĥt←BRNN(∇ĥtLt+1, grad ĥt+1) 20: if t− 1 ∈ violations then 21: grad ht ← grad ht + grad ĥt 22: grad ĥt ← 0
To distinguish the models we refer to our system as BSO (beam search optimization) and to the baseline as seq2seq.",15: /*BACKWARD*/,[0],[0]
"When we apply constrained training (as discussed in Section 4.2), we refer to the model as ConBSO.",15: /*BACKWARD*/,[0],[0]
"In providing results we also distinguish between the beam size Ktr with which the model is trained, and the beam size Kte which is used at test-time.",15: /*BACKWARD*/,[0],[0]
"In general, if we plan on evaluating with a beam of size Kte it makes sense to train with a beam of size Ktr = Kte + 1, since our objective requires the gold sequence to be scored higher than the last sequence on the beam.",15: /*BACKWARD*/,[0],[0]
Here we detail additional techniques we found necessary to ensure the model learned effectively.,5.2 Methodology,[0],[0]
"First, we found that the model failed to learn when trained from a random initialization.4 We therefore found it necessary to pre-train the model using a standard, word-level cross-entropy loss as described in Sec-
4This may be because there is relatively little signal in the sparse, sequence-level gradient, but this point requires further investigation.
tion 3.",5.2 Methodology,[0],[0]
"The necessity of pre-training in this instance is consistent with the findings of other authors who train non-local neural models (Kingsbury, 2009; Sak et al., 2014; Andor et al., 2016; Ranzato et al., 2016).5
Similarly, it is clear that the smaller the beam used in training is, the less room the model has to make erroneous predictions without running afoul of the margin loss.",5.2 Methodology,[0],[0]
"Accordingly, we also found it useful to use a “curriculum beam” strategy in training, whereby the size of the beam is increased gradually during training.",5.2 Methodology,[0],[0]
"In particular, given a desired training beam size Ktr, we began training with a beam of size 2, and increased it by 1 every 2 epochs until reaching Ktr.
",5.2 Methodology,[0],[0]
"Finally, it has been established that dropout (Srivastava et al., 2014) regularization improves the performance of LSTMs (Pham et al., 2014; Zaremba et al., 2014), and in our experiments we run beam search under dropout.6
For all experiments, we trained both seq2seq and BSO models with mini-batch Adagrad (Duchi et al., 2011) (using batches of size 64), and we renormalized all gradients so they did not exceed 5 before updating parameters.",5.2 Methodology,[0],[0]
"We did not extensively tune learning-rates, but we found initial rates of 0.02 for the encoder and decoder LSTMs, and a rate of 0.1 or 0.2 for the final linear layer (i.e., the layer tasked with making word-predictions at each timestep) to work well across all the tasks we considered.",5.2 Methodology,[0],[0]
Code implementing the experiments described below can be found at https://github.com/ harvardnlp/BSO.7,5.2 Methodology,[0],[0]
Our experiments are primarily intended to evaluate the effectiveness of beam search optimization over standard seq2seq training.,5.3 Tasks and Results,[0],[0]
"As such, we run experiments with the same model across three very dif-
5Andor et al. (2016) found, however, that pre-training only increased convergence-speed, but was not necessary for obtaining good results.
",5.3 Tasks and Results,[0],[0]
"6However, it is important to ensure that the same mask applied at each time-step of the forward search is also applied at the corresponding step of the backward pass.",5.3 Tasks and Results,[0],[0]
"We accomplish this by pre-computing masks for each time-step, and sharing them between the partial sequence LSTMs.
",5.3 Tasks and Results,[0],[0]
"7Our code is based on Yoon Kim’s seq2seq code, https: //github.com/harvardnlp/seq2seq-attn.
ferent problems: word ordering, dependency parsing, and machine translation.",5.3 Tasks and Results,[0],[0]
"While we do not include all the features and extensions necessary to reach state-of-the-art performance, even the baseline seq2seq model is generally quite performant.
",5.3 Tasks and Results,[0],[0]
"Word Ordering The task of correctly ordering the words in a shuffled sentence has recently gained some attention as a way to test the (syntactic) capabilities of text-generation systems (Zhang and Clark, 2011; Zhang and Clark, 2015; Liu et al., 2015; Schmaltz et al., 2016).",5.3 Tasks and Results,[0],[0]
"We cast this task as seq2seq problem by viewing a shuffled sentence as a source sentence, and the correctly ordered sentence as the target.",5.3 Tasks and Results,[0],[0]
"While word ordering is a somewhat synthetic task, it has two interesting properties for our purposes.",5.3 Tasks and Results,[0],[0]
"First, it is a task which plausibly requires search (due to the exponentially many possible orderings), and, second, there is a clear hard constraint on output sequences, namely, that they be a permutation of the source sequence.",5.3 Tasks and Results,[0],[0]
For both the baseline and BSO models we enforce this constraint at testtime.,5.3 Tasks and Results,[0],[0]
"However, we also experiment with constraining the BSO model during training, as described in Section 4.2, by defining the succ function to only allow successor sequences containing un-used words in the source sentence.
",5.3 Tasks and Results,[0],[0]
"For experiments, we use the same PTB dataset (with the standard training, development, and test splits) and evaluation procedure as in Zhang and Clark (2015) and later work, with performance reported in terms of BLEU score with the correctly ordered sentences.",5.3 Tasks and Results,[0],[0]
"For all word-ordering experiments we use 2-layer encoder and decoder LSTMs, each with 256 hidden units, and dropout with a rate of 0.2 between LSTM layers.",5.3 Tasks and Results,[0],[0]
"We use simple 0/1 costs in defining the ∆ function.
",5.3 Tasks and Results,[0],[0]
We show our test-set results in Table 1.,5.3 Tasks and Results,[0],[0]
"We see that on this task there is a large improvement at each beam size from switching to BSO, and a further improvement from using the constrained model.
",5.3 Tasks and Results,[0],[0]
"Inspired by a similar analysis in Daumé III and Marcu (2005), we further examine the relationship between Ktr and Kte when training with ConBSO in Table 2.",5.3 Tasks and Results,[0],[0]
"We see that larger Ktr hurt greedy inference, but that results continue to improve, at least initially, when using a Kte that is (somewhat) bigger than Ktr − 1.
",5.3 Tasks and Results,[0],[0]
Dependency Parsing,5.3 Tasks and Results,[0],[0]
"We next apply our model to dependency parsing, which also has hard constraints and plausibly benefits from search.",5.3 Tasks and Results,[0],[0]
"We treat dependency parsing with arc-standard transitions as a seq2seq task by attempting to map from a source sentence to a target sequence of source sentence words interleaved with the arc-standard, reduce-actions in its parse.",5.3 Tasks and Results,[0],[0]
"For example, we attempt to map the source sentence
But it was the Quotron problems that ...
to the target sequence
",5.3 Tasks and Results,[0],[0]
"But it was @L SBJ @L DEP the Quotron problems @L NMOD @L NMOD that ...
We use the standard Penn Treebank dataset splits with Stanford dependency labels, and the standard UAS/LAS evaluation metric (excluding punctuation) following Chen and Manning (2014).",5.3 Tasks and Results,[0],[0]
"All models thus see only the words in the source and, when decoding, the actions it has emitted so far; no other features are used.",5.3 Tasks and Results,[0],[0]
"We use 2-layer encoder and decoder LSTMs with 300 hidden units per layer
and dropout with a rate of 0.3 between LSTM layers.",5.3 Tasks and Results,[0],[0]
"We replace singleton words in the training set with an UNK token, normalize digits to a single symbol, and initialize word embeddings for both source and target words from the publicly available word2vec (Mikolov et al., 2013) embeddings.",5.3 Tasks and Results,[0],[0]
"We use simple 0/1 costs in defining the ∆ function.
",5.3 Tasks and Results,[0],[0]
"As in the word-ordering case, we also experiment with modifying the succ function in order to train under hard constraints, namely, that the emitted target sequence be a valid parse.",5.3 Tasks and Results,[0],[0]
"In particular, we constrain the output at each time-step to obey the stack constraint, and we ensure words in the source are emitted in order.
",5.3 Tasks and Results,[0],[0]
We show results on the test-set in Table 3.,5.3 Tasks and Results,[0],[0]
"BSO and ConBSO both show significant improvements over seq2seq, with ConBSO improving most on UAS, and BSO improving most on LAS.",5.3 Tasks and Results,[0],[0]
"We achieve a reasonable final score of 91.57 UAS, which lags behind the state-of-the-art, but is promising for a general-purpose, word-only model.
",5.3 Tasks and Results,[0],[0]
"Translation We finally evaluate our model on a small machine translation dataset, which allows us to experiment with a cost function that is not 0/1, and to consider other baselines that attempt to mitigate exposure bias in the seq2seq setting.",5.3 Tasks and Results,[0],[0]
"We use the dataset from the work of Ranzato et al. (2016), which uses data from the German-to-English portion of the IWSLT 2014 machine translation evaluation campaign (Cettolo et al., 2014).",5.3 Tasks and Results,[0],[0]
"The data comes from translated TED talks, and the dataset contains roughly 153K training sentences, 7K development sentences, and 7K test sentences.",5.3 Tasks and Results,[0],[0]
"We use the same preprocessing and dataset splits as Ranzato et
al. (2016), and like them we also use a single-layer LSTM decoder with 256 units.",5.3 Tasks and Results,[0],[0]
We also use dropout with a rate of 0.2 between each LSTM layer.,5.3 Tasks and Results,[0],[0]
"We emphasize, however, that while our decoder LSTM is of the same size as that of Ranzato et al. (2016), our results are not directly comparable, because we use an LSTM encoder (rather than a convolutional encoder as they do), a slightly different attention mechanism, and input feeding (Luong et al., 2015).
",5.3 Tasks and Results,[0],[0]
"For our main MT results, we set ∆(ŷ(k)1:t ) to 1−SB(ŷ(K)r+1:t, yr+1:t), where r is the last margin violation and SB denotes smoothed, sentence-level BLEU (Chen and Cherry, 2014).",5.3 Tasks and Results,[0],[0]
This setting of ∆ should act to penalize erroneous predictions with a relatively low sentence-level BLEU score more than those with a relatively high sentence-level BLEU score.,5.3 Tasks and Results,[0],[0]
"In Table 4 we show our final results and those from Ranzato et al. (2016).8 While we start with an improved baseline, we see similarly large increases in accuracy as those obtained by DAD and MIXER, in particular when Kte > 1.
",5.3 Tasks and Results,[0],[0]
"We further investigate the utility of these sequence-level costs in Table 5, which compares using sentence-level BLEU costs in defining ∆ with using 0/1 costs.",5.3 Tasks and Results,[0],[0]
"We see that the more sophisticated sequence-level costs have a moderate effect on BLEU score.
",5.3 Tasks and Results,[0],[0]
"8Some results from personal communication.
",5.3 Tasks and Results,[0],[0]
"r+1:t, yr+1:t) (bottom), and
Ktr = 6.
",5.3 Tasks and Results,[0],[0]
"Timing Given Algorithm 1, we would expect training time to increase linearly with the size of the beam.",5.3 Tasks and Results,[0],[0]
"On the above MT task, our highly tuned seq2seq baseline processes an average of 13,038 tokens/second (including both source and target tokens) on a GTX 970 GPU.",5.3 Tasks and Results,[0],[0]
"For beams of size Ktr = 2, 3, 4, 5, and 6, our implementation processes on average 1,985, 1,768, 1,709, 1,521, and 1,458 tokens/second, respectively.",5.3 Tasks and Results,[0],[0]
"Thus, we appear to pay an initial constant factor of ≈ 3.3 due to the more complicated forward and backward passes, and then training scales with the size of the beam.",5.3 Tasks and Results,[0],[0]
"Because we batch beam predictions on a GPU, however, we find that in practice training time scales sub-linearly with the beam-size.",5.3 Tasks and Results,[0],[0]
"We have introduced a variant of seq2seq and an associated beam search training scheme, which addresses exposure bias as well as label bias, and moreover allows for both training with sequencelevel cost functions as well as with hard constraints.",6 Conclusion,[0],[0]
Future work will examine scaling this approach to much larger datasets.,6 Conclusion,[0],[0]
We thank Yoon Kim for helpful discussions and for providing the initial seq2seq code on which our implementations are based.,Acknowledgments,[0],[0]
We thank Allen Schmaltz for help with the word ordering experiments.,Acknowledgments,[0],[0]
We also gratefully acknowledge the support of a Google Research Award.,Acknowledgments,[0],[0]
Sequence-to-Sequence (seq2seq) modeling has rapidly become an important generalpurpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks.,abstractText,[0],[0]
"Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions.",abstractText,[0],[0]
"In this work, we introduce a model and beamsearch training scheme, based on the work of Daumé III and Marcu (2005), that extends seq2seq to learn global sequence scores.",abstractText,[0],[0]
"This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach.",abstractText,[0],[0]
"We show that our system outperforms a highlyoptimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.",abstractText,[0],[0]
Sequence-to-Sequence Learning as Beam-Search Optimization,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1842–1852 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1842
Sequence-to-sequence Models for Cache Transition Systems
Xiaochang Peng1, Linfeng Song1, Daniel Gildea1, Giorgio Satta2 1University of Rochester 2University of Padua {xpeng,lsong10,gildea}@cs.rochester.edu,
satta@dei.unipd.it
Abstract
In this paper, we present a sequenceto-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1",text,[0],[0]
"Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph.",1 Introduction,[0],[0]
Figure 1 shows an example of an AMR in which the nodes represent the AMR concepts and the edges represent the relations between the concepts.,1 Introduction,[0],[0]
"AMR has been used in various applications such as text summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016).
",1 Introduction,[0],[0]
"1The implementation of our parser is available at https://github.com/xiaochang13/CacheTransition-Seq2seq
want-01
person
go-01
ARG0
ARG0
ARG1
name
“John”
name
op1
Figure 1: An example of AMR graph representing the meaning of: “John wants to go”
The task of AMR graph parsing is to map natural language strings to AMR semantic graphs.",1 Introduction,[0],[0]
"Different parsers have been developed to tackle this problem (Flanigan et al., 2014; Wang et al., 2015b,a; Peng et al., 2015; Artzi et al., 2015; Pust et al., 2015; van Noord and Bos, 2017).",1 Introduction,[0],[0]
"On the other hand, due to the limited amount of labeled data and the large output vocabulary, the sequence-to-sequence model has not been very successful on AMR parsing.",1 Introduction,[0],[0]
Peng et al. (2017) propose a linearization approach that encodes labeled graphs as sequences.,1 Introduction,[0],[0]
"To address the data sparsity issue, low-frequency entities and tokens are mapped to special categories to reduce the vocabulary size for the neural models.",1 Introduction,[0],[0]
Konstas et al. (2017) use self-training on a huge amount of unlabeled text to lower the out-of-vocabulary rate.,1 Introduction,[0],[0]
"However, the final performance still falls behind the best-performing models.
",1 Introduction,[0],[0]
The best performing AMR parsers model graph structures directly.,1 Introduction,[0],[0]
"One approach to modeling graph structures is to use a transition system to build graphs step by step, as shown by the system
of Wang and Xue (2017), which is currently the top performing system.",1 Introduction,[0],[0]
"This raises the question of whether the advantages of neural and transitionbased system can be combined, as for example with the syntactic parser of Dyer et al. (2015), who use stack LSTMs to capture action history information in the transition state of the transition system.",1 Introduction,[0],[0]
"Ballesteros and Al-Onaizan (2017) apply stack-LSTM to transition-based AMR parsing and achieve competitive results, which shows that local transition state information is important for predicting transition actions.
",1 Introduction,[0],[0]
"Instead of linearizing the target AMR graph to a sequence structure, Buys and Blunsom (2017) propose a sequence-to-action-sequence approach where the reference AMR graph is replaced with an action derivation sequence by running a deterministic oracle algorithm on the training sentence, AMR graph pairs.",1 Introduction,[0],[0]
"They use a separate alignment probability to explicitly model the hard alignment from graph nodes to sentence tokens in the buffer.
",1 Introduction,[0],[0]
Gildea et al. (2018) propose a special transition framework called a cache transition system to generate the set of semantic graphs.,1 Introduction,[0],[0]
"They adapt the stack-based parsing system by adding a working set, which they refer to as a cache, to the traditional stack and buffer.",1 Introduction,[0],[0]
"Peng et al. (2018) apply the cache transition system to AMR parsing and design refined action phases, each modeled with a separate feedforward neural network, to deal with some practical implementation issues.
",1 Introduction,[0],[0]
"In this paper, we propose a sequence-to-actionsequence approach for AMR parsing with cache transition systems.",1 Introduction,[0],[0]
"We want to take advantage of the sequence-to-sequence model to encode wholesentence context information and the history action sequence, while using the transition system to constrain the possible output.",1 Introduction,[0],[0]
"The transition system can also provide better local context information than the linearized graph representation, which is important for neural AMR parsing given the limited amount of data.
",1 Introduction,[0],[0]
"More specifically, we use bi-LSTM to encode two levels of input information for AMR parsing: word level and concept level, each refined with more general category information such as lemmatization, POS tags, and concept categories.
",1 Introduction,[0],[0]
We also want to make better use of the complex transition system to address the data sparsity issue for neural AMR parsing.,1 Introduction,[0],[0]
"We extend the hard attention model of Aharoni and Goldberg (2017),
which deals with the nearly-monotonic alignment in the morphological inflection task, to the more general scenario of transition systems where the input buffer is processed from left-to-right.",1 Introduction,[0],[0]
"When we process the buffer in this ordered manner, the sequence of target transition actions are also strictly aligned left-to-right according to the input order.",1 Introduction,[0],[0]
"On the decoder side, we augment the prediction of output action with embedding features from the current transition state.",1 Introduction,[0],[0]
Our experiments show that encoding information from the transition state significantly improves sequenceto-sequence models for AMR parsing.,1 Introduction,[0],[0]
"We adopt the transition system of Gildea et al. (2018), which has been shown to have good coverage of the graphs found in AMR.
",2 Cache Transition Parser,[0],[0]
"A cache transition parser consists of a stack, a cache, and an input buffer.",2 Cache Transition Parser,[0],[0]
"The stack is a sequence σ of (integer, concept) pairs, as explained below, with the topmost element always at the rightmost position.",2 Cache Transition Parser,[0],[0]
"The buffer is a sequence of ordered concepts β containing a suffix of the input concept sequence, with the first element to be read as a newly introduced concept/vertex of the graph.",2 Cache Transition Parser,[0],[0]
(We use the terms concept and vertex interchangeably in this paper.),2 Cache Transition Parser,[0],[0]
"Finally, the cache is a sequence of concepts η =",2 Cache Transition Parser,[0],[0]
"[v1, . . .",2 Cache Transition Parser,[0],[0]
", vm].",2 Cache Transition Parser,[0],[0]
"The element at the leftmost position is called the first element of the cache, and the element at the rightmost position is called the last element.
",2 Cache Transition Parser,[0],[0]
"Operationally, the functioning of the parser can be described in terms of configurations and transitions.",2 Cache Transition Parser,[0],[0]
"A configuration of our parser has the form:
C = (σ, η, β,Gp)
where σ, η and β are as described above, and Gp is the partial graph that has been built so far.",2 Cache Transition Parser,[0],[0]
"The initial configuration of the parser is ([], [$, . . .",2 Cache Transition Parser,[0],[0]
", $], [c1, . . .",2 Cache Transition Parser,[0],[0]
", cn], ∅), meaning that the stack and the partial graph are initially empty, and the cache is filled with m occurrences of the special symbol $.",2 Cache Transition Parser,[0],[0]
The buffer is initialized with all the graph vertices constrained by the order of the input sentence.,2 Cache Transition Parser,[0],[0]
"The final configuration is ([], [$, . . .",2 Cache Transition Parser,[0],[0]
", $], [], G), where the stack and the cache are as in the initial configuration and the buffer is empty.",2 Cache Transition Parser,[0],[0]
"The constructed graph is the target AMR graph.
",2 Cache Transition Parser,[0],[0]
"In the first step, which is called concept identification, we map the input sentence w1:n′ = w1, . . .",2 Cache Transition Parser,[0],[0]
", wn′ to a sequence of concepts c1:n = c1, . . .",2 Cache Transition Parser,[0],[0]
", cn.",2 Cache Transition Parser,[0],[0]
"We decouple the problem of concept identification from the transition system and initialize the buffer with a recognized concept sequence from another classifier, which we will introduce later.",2 Cache Transition Parser,[0],[0]
"As the sequence-to-sequence model uses all possible output actions as the target vocabulary, this can significantly reduce the target vocabulary size.",2 Cache Transition Parser,[0],[0]
"The transitions of the parser are specified as follows.
1.",2 Cache Transition Parser,[0],[0]
"Pop pops a pair (i, v) from the stack, where the integer i records the position in the cache that it originally came from.",2 Cache Transition Parser,[0],[0]
"We place concept v in position i in the cache, shifting the remainder of the cache one position to the right, and discarding the last element in the cache.
2.",2 Cache Transition Parser,[0],[0]
"Shift signals that we will start processing the next input concept, which will become a new vertex in the output graph.
3. PushIndex(i) shifts the next input concept out of the buffer and moves it into the last position of the cache.",2 Cache Transition Parser,[0],[0]
"We also take out the concept vi appearing at position i in the cache and push it onto the stack σ, along with the integer i recording its original position in the cache.2
2Our transition design is different from Peng et al. (2018) in two ways: the PushIndex phase is initiated before making all the arc decisions; the newly introduced concept is placed at the last cache position instead of the leftmost buffer position, which essentially increases the cache size by 1.
4.",2 Cache Transition Parser,[0],[0]
"Arc(i, d, l) builds an arc with direction d and label l between the rightmost concept and the i-th concept in the cache.",2 Cache Transition Parser,[0],[0]
The label l is NULL if no arc is made and we use the action NOARC in this case.,2 Cache Transition Parser,[0],[0]
Otherwise we decompose the arc decision into two actions ARC and d-l. We consider all arc decisions between the rightmost cache concept and each of the other concepts in the cache.,2 Cache Transition Parser,[0],[0]
"We can consider this phase as first making a binary decision whether there is an arc, and then predicting the label in case there is one, between each concept pair.
",2 Cache Transition Parser,[0],[0]
"Given the sentence “John wants to go” and the recognized concept sequence “Per want-01 go-01” (person name category Per for “John”), our cache transition parser can construct the AMR graph shown in Figure 1 using the run shown in Figure 2 with cache size of 3.",2 Cache Transition Parser,[0],[0]
"We use the following oracle algorithm (Nivre, 2008) to derive the sequence of actions that leads to the gold AMR graph for a cache transition parser with cache size m. The correctness of the oracle is shown by Gildea et al. (2018).
",2.1 Oracle Extraction Algorithm,[0],[0]
"Let EG be the set of edges of the gold graph G. We maintain the set of vertices that is not yet shifted into the cache as S, which is initialized with all vertices in G. The vertices are ordered according to their aligned position in the word sequence and the unaligned vertices are listed according to their order in the depth-first traversal of the graph.",2.1 Oracle Extraction Algorithm,[0],[0]
"The oracle algorithm can look into
EG to decide which transition to take next, or else to decide that it should fail.",2.1 Oracle Extraction Algorithm,[0],[0]
"This decision is based on the mutually exclusive rules listed below.
1.",2.1 Oracle Extraction Algorithm,[0],[0]
"ShiftOrPop phase: the oracle chooses transition Pop, in case there is no edge (vm, v) in EG such that vertex v is in S, or chooses transition Shift and proceeds to the next phase.
2.",2.1 Oracle Extraction Algorithm,[0],[0]
"PushIndex phase: in this phase, the oracle first chooses a position i (as explained below) in the cache to place the candidate concept and removes the vertex at this position and places its index, vertex pair onto the stack.",2.1 Oracle Extraction Algorithm,[0],[0]
"The oracle chooses transition PushIndex(i) and proceeds to the next phase.
3. ArcBinary, ArcLabel phases: between the rightmost cache concept and each concept in the cache, we make a binary decision about whether there is an arc between them.",2.1 Oracle Extraction Algorithm,[0],[0]
"If there is an arc, the oracle chooses its direction and label.",2.1 Oracle Extraction Algorithm,[0],[0]
"After arc decisions to m−1 cache concepts are made, we jump to the next step.
4.",2.1 Oracle Extraction Algorithm,[0],[0]
"If the stack and buffer are both empty, and the cache is in the initial state, the oracle finishes with success, otherwise we proceed to the first step.
",2.1 Oracle Extraction Algorithm,[0],[0]
We use the equation below to choose the cache concept to take out in the step PushIndex(i).,2.1 Oracle Extraction Algorithm,[0],[0]
For j ∈,2.1 Oracle Extraction Algorithm,[0],[0]
"[|β|], we write βj to denote the j-th vertex in β.",2.1 Oracle Extraction Algorithm,[0],[0]
"We choose a vertex vi∗ in η such that:
i∗ = argmax i∈[m] min {j | (vi, βj) ∈ EG}
In words, vi∗ is the concept from the cache whose closest neighbor in the buffer β is furthest forward in β.",2.1 Oracle Extraction Algorithm,[0],[0]
"We move out of the cache vertex vi∗ and push it onto the stack, for later processing.
",2.1 Oracle Extraction Algorithm,[0],[0]
"For each training example (x1:n, g), the transition system generates the output AMR graph g from the input sequence x1:n through an oracle sequence a1:q ∈ Σ∗a, where Σa is the union of all possible actions.",2.1 Oracle Extraction Algorithm,[0],[0]
"We model the probability of the output with the action sequence:
P (a1:q|x1:n) = q∏
t=1
P (at|a1, . . .",2.1 Oracle Extraction Algorithm,[0],[0]
", at−1, x1:n; θ)
which we estimate using a sequence-to-sequence model, as we will describe in the next section.",2.1 Oracle Extraction Algorithm,[0],[0]
"Shown in Figure 3, our sequence-to-sequence model takes a word sequence w1:n′ and its mapped concept sequence c1:n as the input, and the action sequence a1:q as the output.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"It uses two BiLSTM encoders, each encoding an input sequence.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"As the two encoders have the same structure, we only introduce the encoder for the word sequence in detail below.",3 Soft vs Hard Attention for Sequence-to-action-sequence,[0],[0]
"Given an input word sequence w1:n′ , we use a bidirectional LSTM to encode it.",3.1 BiLSTM Encoder,[0],[0]
"At each step j, the current hidden states ←− h wj",3.1 BiLSTM Encoder,[0],[0]
and −→ h wj are generated from the previous hidden states ←− h wj+1,3.1 BiLSTM Encoder,[0],[0]
"and −→ h wj−1,
and the representation vector xj of the current input word wj :
←− h wj = LSTM( ←− h wj+1, xj) −→ h wj = LSTM( −→ h wj−1, xj)
",3.1 BiLSTM Encoder,[0],[0]
"The representation vector xj is the concatenation of the embeddings of its word, lemma, and POS tag, respectively.",3.1 BiLSTM Encoder,[0],[0]
"Then the hidden states of both directions are concatenated as the final hidden state for word wj :
hwj =",3.1 BiLSTM Encoder,[0],[0]
"[ ←− h wj ; −→ h wj ]
Similarly, for the concept sequence, the final hidden state for concept cj is:
hcj =",3.1 BiLSTM Encoder,[0],[0]
[ ←− h cj ;,3.1 BiLSTM Encoder,[0],[0]
−→ h cj ],3.1 BiLSTM Encoder,[0],[0]
"We use an attention-based LSTM decoder (Bahdanau et al., 2014) with two attention memories Hw and Hc, where Hw is the concatenation of the state vectors of all input words, and Hc for input concepts correspondingly:
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Hw =,3.2 LSTM Decoder with Soft Attention,[0],[0]
[h w 1 ;h w 2 ; . . .,3.2 LSTM Decoder with Soft Attention,[0],[0]
;h w n′,3.2 LSTM Decoder with Soft Attention,[0],[0]
],3.2 LSTM Decoder with Soft Attention,[0],[0]
"(1)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Hc =,3.2 LSTM Decoder with Soft Attention,[0],[0]
[h c 1;h c 2; . . .,3.2 LSTM Decoder with Soft Attention,[0],[0]
";h c n] (2)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The decoder yields an action sequence a1, a2, . . .",3.2 LSTM Decoder with Soft Attention,[0],[0]
", aq as the output by calculating a sequence of hidden states s1, s2 . . .",3.2 LSTM Decoder with Soft Attention,[0],[0]
", sq recurrently.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"While generating the t-th output action, the decoder considers three factors: (1) the previous hidden state of the LSTM model st−1; (2) the embedding of the previous generated action et−1; and (3) the previous context vectors for words µwt−1 and concepts µ c t−1, which are calculated using Hw and Hc, respectively.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"When t = 1, we initialize µ0 as a zero vector, and set e0 to the embedding of the start token “〈s〉”.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The hidden state s0 is initialized as:
s0 = Wd[ ←− h w1 ; −→ h wn ; ←− h c1; −→ h cn] + bd,
where Wd and bd are model parameters.",3.2 LSTM Decoder with Soft Attention,[0],[0]
"For each time-step t, the decoder feeds the concatenation of the embedding of previous action et−1 and the previous context vectors for words µwt−1 and concepts µ c t−1 into the LSTM model to update its hidden state.
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"st = LSTM(st−1, [et−1;µwt−1;µ c t−1]) (3)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
Then the attention probabilities for the word sequence and the concept sequence are calculated similarly.,3.2 LSTM Decoder with Soft Attention,[0],[0]
"Take the word sequence as an example, αwt,i on h w",3.2 LSTM Decoder with Soft Attention,[0],[0]
i ∈,3.2 LSTM Decoder with Soft Attention,[0],[0]
"Hw for time-step t is calculated as:
t,i = v T c tanh(Whh w i +Wsst + bc)
αwt,i = exp( t,i)∑N j=1 exp( t,j)
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"Wh,Ws, vc and bc are model parameters.",3.2 LSTM Decoder with Soft Attention,[0],[0]
The new context vector µwt =,3.2 LSTM Decoder with Soft Attention,[0],[0]
∑n i=1,3.2 LSTM Decoder with Soft Attention,[0],[0]
"α w t,ih w",3.2 LSTM Decoder with Soft Attention,[0],[0]
i .,3.2 LSTM Decoder with Soft Attention,[0],[0]
"The calculation of µct follows the same procedure, but with a different set of model parameters.
",3.2 LSTM Decoder with Soft Attention,[0],[0]
"The output probability distribution over all actions at the current state is calculated by:
PΣa = softmax(Va[st;µ w t ;µ c t ] + ba), (4)
where Va and ba are learnable parameters, and the number of rows in Va represents the number of all actions.",3.2 LSTM Decoder with Soft Attention,[0],[0]
The symbol Σa is the set of all actions.,3.2 LSTM Decoder with Soft Attention,[0],[0]
"When we process each buffer input, the next few transition actions are closely related to this input position.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"The buffer maintains the order information of the input sequence and is processed strictly left-to-right, which essentially encodes a monotone alignment between the transition action sequence and the input sequence.
",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As we have generated a concept sequence from the input word sequence, we maintain two hard attention pointers, lw and lc, to model monotonic attention to word and concept sequences respectively.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"The update to the decoder state now relies on a single position of each input sequence in contrast to Equation 3:
st = LSTM(st−1, [et−1;hwlw ;h c lc ]) (5)
Control Mechanism.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
Both pointers are initialized as 0 and advanced to the next position deterministically.,3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
We move the concept attention focus lc to the next position after arc decisions to all the other m − 1 cache concepts are made.,3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"We move the word attention focus lw to its aligned position in case the new concept is aligned, otherwise we don’t move the word focus.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As shown in Figure 4, after we have made arc decisions from concept want-01 to the other cache concepts, we move the concept focus to the next concept go-01.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"As this concept is aligned, we move the word focus to its aligned position go in the word sequence and skip the unaligned word to.",3.3 Monotonic Hard Attention for Transition Systems,[0],[0]
"Another difference of our model with Buys and Blunsom (2017) is that we extract features from the current transition state configuration Ct:
ef (Ct) =",3.4 Transition State Features for Decoder,[0],[0]
"[ef1(Ct); ef2(Ct); · · · ; efl(Ct)]
where l is the number of features extracted from Ct and efk(Ct) (k = 1, . . .",3.4 Transition State Features for Decoder,[0],[0]
", l) represents the embedding for the k-th feature, which is learned during training.",3.4 Transition State Features for Decoder,[0],[0]
"These feature embeddings are concatenated as ef (Ct), and fed as additional input to the decoder.",3.4 Transition State Features for Decoder,[0],[0]
"For the soft attention decoder:
st = LSTM(st−1, [et−1;µwt−1;µ c t−1; ef (Ct)])
and for the hard attention decoder:
st = LSTM(st−1, [et−1;hwlw ;h c lc ; ef (Ct)])
We use the following features in our experiments:
1.",3.4 Transition State Features for Decoder,[0],[0]
"Phase type: indicator features showing which phase the next transition is.
2.",3.4 Transition State Features for Decoder,[0],[0]
ShiftOrPop features: token features3 for the rightmost cache concept and the leftmost buffer concept.,3.4 Transition State Features for Decoder,[0],[0]
"Number of dependencies to words on the right, and the top three dependency labels for them.
3.",3.4 Transition State Features for Decoder,[0],[0]
ArcBinary or ArcLabel features: token features for the rightmost concept and the current cache concept it makes arc decisions to.,3.4 Transition State Features for Decoder,[0],[0]
"Word, concept and dependency distance between the two concepts.",3.4 Transition State Features for Decoder,[0],[0]
The labels for the two most recent outgoing arcs for these two concepts and their first incoming arc and the number of incoming arcs.,3.4 Transition State Features for Decoder,[0],[0]
"Dependency label between the two positions if there is a dependency arc between them.
",3.4 Transition State Features for Decoder,[0],[0]
4.,3.4 Transition State Features for Decoder,[0],[0]
"PushIndex features: token features for the leftmost buffer concept and all the concepts in the cache.
",3.4 Transition State Features for Decoder,[0],[0]
The phase type features are deterministic from the last action output.,3.4 Transition State Features for Decoder,[0],[0]
"For example, if the last action output is Shift, the current phase type would be PushIndex.",3.4 Transition State Features for Decoder,[0],[0]
We only extract corresponding features for this phase and fill all the other feature types with -NULL- as placeholders.,3.4 Transition State Features for Decoder,[0],[0]
"The features for other phases are similar.
",3.4 Transition State Features for Decoder,[0],[0]
"3Concept, concept category at the specified position in concept sequence.",3.4 Transition State Features for Decoder,[0],[0]
"And the word, lemma, POS tag at the aligned input position.",3.4 Transition State Features for Decoder,[0],[0]
"We train our models using the cross-entropy loss, over each oracle action sequence a∗1, . . .",4.1 Training and Decoding,[0],[0]
", a ∗ q :
",4.1 Training and Decoding,[0],[0]
"L = − q∑
t=1
logP (a∗t |a∗1, . . .",4.1 Training and Decoding,[0],[0]
", a∗t−1, X; θ), (6)
where X represents the input word and concept sequences, and θ is the model parameters.",4.1 Training and Decoding,[0],[0]
"Adam (Kingma and Ba, 2014) with a learning rate of 0.001 is used as the optimizer, and the model that yields the best performance on the dev set is selected to evaluate on the test set.",4.1 Training and Decoding,[0],[0]
Dropout with rate 0.3 is used during training.,4.1 Training and Decoding,[0],[0]
Beam search with a beam size of 10 is used for decoding.,4.1 Training and Decoding,[0],[0]
"Both training and decoding use a Tesla K20X GPU.
",4.1 Training and Decoding,[0],[0]
Hidden state sizes for both encoder and decoder are set to 100.,4.1 Training and Decoding,[0],[0]
"The word embeddings are initialized from Glove pretrained word embeddings (Pennington et al., 2014) on Common Crawl, and are not updated during training.",4.1 Training and Decoding,[0],[0]
"The embeddings for POS tags and features are randomly initialized, with the sizes of 20 and 50, respectively.",4.1 Training and Decoding,[0],[0]
"As the AMR data is very sparse, we collapse some subgraphs or spans into categories based on the alignment.",4.2 Preprocessing and Postprocessing,[0],[0]
"We define some special categories such as named entities (NE), dates (DATE), single rooted subgraphs involving multiple concepts (MULT)4, numbers (NUMBER) and phrases (PHRASE).",4.2 Preprocessing and Postprocessing,[0],[0]
The phrases are extracted based on the multiple-to-one alignment in the training data.,4.2 Preprocessing and Postprocessing,[0],[0]
One example phrase is more than which aligns to a single concept more-than.,4.2 Preprocessing and Postprocessing,[0],[0]
"We first collapse spans and subgraphs into these categories based on the alignment from the JAMR aligner (Flanigan et al., 2014), which greedily aligns a span of words to AMR subgraphs using a set of heuristics.",4.2 Preprocessing and Postprocessing,[0],[0]
"This categorization procedure enables the parser to capture mappings from continuous spans on the sentence side to connected subgraphs on the AMR side.
",4.2 Preprocessing and Postprocessing,[0],[0]
"We use the semi-Markov model from Flanigan et al. (2016) as the concept identifier, which jointly segments the sentence into a sequence of spans and maps each span to a subgraph.",4.2 Preprocessing and Postprocessing,[0],[0]
"During decoding, our output has categories, and we need to map
4For example, verbalization of “teacher” as “(person :ARG0-of teach-01)”, or “minister” as “(person :ARG0-of (have-org-role-91 :ARG2 minister))”.
each category to the corresponding AMR concept or subgraph.",4.2 Preprocessing and Postprocessing,[0],[0]
"We save a table Q which shows the original subgraph each category is collapsed from, and map each category to its original subgraph representation.",4.2 Preprocessing and Postprocessing,[0],[0]
"We also use heuristic rules to generate the target-side AMR subgraph representation for NE, DATE, and NUMBER based on the source side tokens.",4.2 Preprocessing and Postprocessing,[0],[0]
"We evaluate our system on the released dataset (LDC2015E86) for SemEval 2016 task 8 on meaning representation parsing (May, 2016).",5 Experiments,[0],[0]
"The dataset contains 16,833 training, 1,368 development, and 1,371 test sentences which mainly cover domains like newswire, discussion forum, etc.",5 Experiments,[0],[0]
"All parsing results are measured by Smatch (version 2.0.2) (Cai and Knight, 2013).",5 Experiments,[0],[0]
We categorize the training data using the automatic alignment and dump a template for date entities and frequent phrases from the multiple to one alignment.,5.1 Experiment Settings,[0],[0]
We also generate an alignment table from tokens or phrases to their candidate targetside subgraphs.,5.1 Experiment Settings,[0],[0]
"For the dev and test data, we first extract the named entities using the Illinois Named Entity Tagger (Ratinov and Roth, 2009) and extract date entities by matching spans with the date template.",5.1 Experiment Settings,[0],[0]
We further categorize the dataset with the categories we have defined.,5.1 Experiment Settings,[0],[0]
"After categorization, we use Stanford CoreNLP",5.1 Experiment Settings,[0],[0]
"(Manning et al., 2014) to get the POS tags and dependencies of the categorized dataset.",5.1 Experiment Settings,[0],[0]
We run the oracle algorithm separately for training and dev data (with alignment) to get the statistics of individual phases.,5.1 Experiment Settings,[0],[0]
We use a cache size of 5 in our experiments.,5.1 Experiment Settings,[0],[0]
Individual Phase Accuracy We first evaluate the prediction accuracy of individual phases on the dev oracle data assuming gold prediction history.,5.2 Results,[0],[0]
"The four transition phases ShiftOrPop, PushIndex, ArcBinary, and ArcLabel account for 25%, 12.5%,
50.1%, and 12.4% of the total transition actions respectively.",5.2 Results,[0],[0]
Table 1 shows the phase-wise accuracy of our sequence-to-sequence model.,5.2 Results,[0],[0]
Peng et al. (2018) use a separate feedforward network to predict each phase independently.,5.2 Results,[0],[0]
We use the same alignment from the SemEval dataset as in Peng et al. (2018) to avoid differences resulting from the aligner.,5.2 Results,[0],[0]
"Soft+feats shows the performance of our sequence-to-sequence model with soft attention and transition state features, while Hard+feats is using hard attention.",5.2 Results,[0],[0]
"We can see that the hard attention model outperforms the soft attention model in all phases, which shows that the single-pointer attention finds more relevant information than the soft attention on the relatively small dataset.",5.2 Results,[0],[0]
"The sequence-to-sequence models perform better than the feedforward model of Peng et al. (2018) on ShiftOrPop and ArcBinary, which shows that the whole-sentence context information is important for the prediction of these two phases.",5.2 Results,[0],[0]
"On the other hand, the sequence-tosequence models perform worse than the feedforward models on PushIndex and ArcLabel.",5.2 Results,[0],[0]
"One possible reason is that the model tries to optimize the overall accuracy, while these two phases account for fewer than 25% of the total transition actions and might be less attended to during the update.
",5.2 Results,[0],[0]
Impact of Different Components Table 2 shows the impact of different components for the sequence-to-sequence model.,5.2 Results,[0],[0]
We can see that the transition state features play a very important role for predicting the correct transition action.,5.2 Results,[0],[0]
This is because different transition phases have very different prediction behaviors and need different types of local information for the prediction.,5.2 Results,[0],[0]
"Relying on the sequence-to-sequence model alone does not perform well in disambiguating these choices, while the transition state can enforce direct constraints.",5.2 Results,[0],[0]
"We can also see that while the hard attention only attends to one position of the input, it performs slightly better than the soft attention model, while the time complexity is lower.
",5.2 Results,[0],[0]
Impact of Different Cache Sizes The cache size of the transition system can be optimized as a trade-off between coverage of AMR graphs and the prediction accuracy.,5.2 Results,[0],[0]
"While larger cache size increases the coverage of AMR graphs, it complicates the prediction procedure with more cache decisions to make.",5.2 Results,[0],[0]
From Table 3 we can see that,5.2 Results,[0],[0]
the hard attention model performs best with cache size 5.,6 0.69 0.64 0.66,[0],[0]
"The soft attention model also achieves best performance with the same cache size.
",6 0.69 0.64 0.66,[0],[0]
Comparison with other Parsers Table 4 shows the comparison with other AMR parsers.,6 0.69 0.64 0.66,[0],[0]
The first three systems are some competitive neural models.,6 0.69 0.64 0.66,[0],[0]
We can see that our parser significantly outperforms the sequence-to-action-sequence model of Buys and Blunsom (2017).,6 0.69 0.64 0.66,[0],[0]
Konstas et al. (2017) use a linearization approach that linearizes the AMR graph to a sequence structure and use selftraining on 20M unlabeled Gigaword sentences.,6 0.69 0.64 0.66,[0],[0]
"Our model achieves better results without using additional unlabeled data, which shows that relevant information from the transition system is very useful for the prediction.",6 0.69 0.64 0.66,[0],[0]
"Our model also outperforms the stack-LSTM model by Ballesteros and Al-Onaizan (2017), while their model is evaluated on the previous release of LDC2014T12.
",6 0.69 0.64 0.66,[0],[0]
We also show the performance of some of the best-performing models.,6 0.69 0.64 0.66,[0],[0]
"While our hard attention achieves slightly lower performance in comparison with Wang et al. (2015a) and Wang and Xue (2017), it is worth noting that their approaches of using WordNet, semantic role labels and word cluster features are complimentary to ours.",6 0.69 0.64 0.66,[0],[0]
The alignment from the aligner and the concept identification identifier also play an important role for improving the performance.,6 0.69 0.64 0.66,[0],[0]
"Wang and Xue (2017) propose to improve AMR parsing by improving the alignment and concept identification, which can also be combined with our system to improve the performance of a sequence-to-sequence model.
",6 0.69 0.64 0.66,[0],[0]
"Dealing with Reentrancy Reentrancy is an important characteristic of AMR, and we evaluate the Smatch score only on the reentrant edges following Damonte et al. (2017).",6 0.69 0.64 0.66,[0],[0]
From Table 5 we can see that our hard attention model significantly outperforms the feedforward model of Peng et al. (2018) in predicting reentrancies.,6 0.69 0.64 0.66,[0],[0]
"This is because predicting reentrancy is directly related to the ArcBinary phase of the cache transition system since it decides to make multiple arc decisions to the same vertex, and we can see from Table 1 that the hard attention model has significantly better prediction accuracy in this phase.",6 0.69 0.64 0.66,[0],[0]
"We also compare the reentrancy results of our transition system with two other systems, Damonte et al. (2017) and JAMR, where these statistics are available.",6 0.69 0.64 0.66,[0],[0]
"From Table 5, we can see that our cache transition system slightly outperforms these two systems in predicting reentrancies.
",6 0.69 0.64 0.66,[0],[0]
"Figure 5 shows a reentrancy example where JAMR and the feedforward network of Peng et al. (2018) do not predict well, while our system predicts the correct output.",6 0.69 0.64 0.66,[0],[0]
"JAMR fails to predict the reentrancy arc from desire-01 to i, and connects the wrong arc from “live-01” to “-” instead of from “desire-01”.",6 0.69 0.64 0.66,[0],[0]
"The feedforward model of Peng et al. (2018) fails to predict the two arcs from desire-01
Sentence: I have no desire to live in any city .
and live-01 to i. This error is because their feedforward ArcBinary classifier does not model longterm dependency and usually prefers making arcs between words that are close and not if they are distant.",6 0.69 0.64 0.66,[0],[0]
"Our classifier, which encodes both word and concept sequence information, can accurately predict the reentrancy through the two arc decisions shown in Figure 5.",6 0.69 0.64 0.66,[0],[0]
"When desire-01 and live01 are shifted into the cache respectively, the transition system makes a left-going arc from each of them to the same concept i, thus creating the reentrancy as desired.",6 0.69 0.64 0.66,[0],[0]
"In this paper, we have presented a sequence-toaction-sequence approach for cache transition systems and applied it to AMR parsing.",6 Conclusion,[0],[0]
"To address the data sparsity issue for neural AMR parsing, we show that the transition state features are very helpful in constraining the possible output and improving the performance of sequence-to-sequence models.",6 Conclusion,[0],[0]
We also show that the monotonic hard attention model can be generalized to the transitionbased framework and outperforms the soft attention model when limited data is available.,6 Conclusion,[0],[0]
"While
we are focused on AMR parsing in this paper, in future work our cache transition system and the presented sequence-to-sequence models can be potentially applied to other semantic graph parsing tasks (Oepen et al., 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017).",6 Conclusion,[0],[0]
"We gratefully acknowledge the assistance of Hao Zhang from Google, New York for the monotonic hard attention idea and the helpful comments and suggestions.",Acknowledgments,[0],[0]
"In this paper, we present a sequenceto-sequence based approach for mapping natural language sentences to AMR semantic graphs.",abstractText,[0],[0]
We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system.,abstractText,[0],[0]
"To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state.",abstractText,[0],[0]
We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus.,abstractText,[0],[0]
"We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.1",abstractText,[0],[0]
Sequence-to-sequence Models for Cache Transition Systems,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 103–114, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Goal oriented dialogue systems help users with accomplishing tasks, like making restaurant reservations or booking flights, by interacting with them in natural language.",1 Introduction,[0],[0]
The capability to understand user utterances and break them down into task specific semantics is a key requirement for these systems.,1 Introduction,[0],[0]
"This is accomplished in the spoken language understanding module, which typically parses user utterances into semantic frames, composed of domains, intents and slots (Tur and De Mori, 2011), that can then be processed by downstream dia-
logue system components.",1 Introduction,[0],[0]
An example semantic frame is shown for a restaurant reservation related query in Figure 1.,1 Introduction,[0],[0]
"As the complexity of the task supported by a dialogue system increases, there is a need for an increased back and forth interaction between the user and the agent.",1 Introduction,[0],[0]
"For example, a restaurant reservation task might require the user to specify a restaurant name, date, time and number of people required for the reservation.",1 Introduction,[0],[0]
"Additionally, based on reservation availability, the user might need to negotiate on date, time, or any other attribute with the agent.",1 Introduction,[0],[0]
This puts the burden of parsing in-dialogue contextual user utterances on the language understanding module.,1 Introduction,[0],[0]
The complexity increases further when the system supports more than one task and the user is allowed to have goals spanning multiple domains within the same dialogue.,1 Introduction,[0],[0]
"Natural language utterances are often ambiguous, and the context from previous user and system turns could help resolve the errors arising from these ambiguities.
",1 Introduction,[0],[0]
"In this paper, we explore approaches to improve dialogue context modeling within a Recurrent Neural Network (RNN) based spoken language understanding system.",1 Introduction,[0],[0]
"We propose a novel model architecture to improve dialogue context modeling for spoken language understanding on a
103
multi-domain dialogue dataset.",1 Introduction,[0],[0]
The proposed architecture is an extension of Hierarchical Recurrent Encoder Decoders (HRED),1 Introduction,[0],[0]
"(Sordoni et al., 2015), where we combine the query level encodings with a representation of the current utterance, before feeding it into the session level encoder.",1 Introduction,[0],[0]
"We compare the performance of this model to a RNN tagger injected with just the previous turn context and a single hop memory network that uses an attention weighted combination of the dialogue context (Chen et al., 2016; Weston et al., 2014).",1 Introduction,[0],[0]
"Furthermore, we describe a dialogue recombination technique to enhance the complexity of the training dataset by injecting synthetic domain switches, to create a better match with the mixed domain dialogues in the test dataset.",1 Introduction,[0],[0]
"This is, in principle, a multi-turn extension of (Jia and Liang, 2016).",1 Introduction,[0],[0]
"Instead of inducing and composing grammars to synthetically enhance single turn text, we combine single domain dialogue sessions into multi-domain dialogues to provide richer context during training.",1 Introduction,[0],[0]
"The task of understanding a user utterance is typically broken down into 3 tasks: domain classification, intent classification and slot-filling (Tur and De Mori, 2011).",2 Related Work,[0],[0]
"Most modern approaches to Spoken language understanding involve training machine learning models on labeled training data (Young, 2002; Hahn et al., 2011; Wang et al., 2005, among others).",2 Related Work,[0],[0]
"More recently, recurrent neural network (RNN) based approaches have been shown to perform exceedingly well on spoken language understanding tasks (Mesnil et al., 2015; Hakkani-Tür et al., 2016; Kurata et al., 2016, among others).",2 Related Work,[0],[0]
"RNN based approaches have also been applied successfully to other tasks for di-
alogue systems, like dialogue state tracking (Henderson, 2015; Henderson et al., 2014; Perez and Liu, 2016, among others), policy learning (Su et al., 2015) and system response generation (Wen et al., 2015, 2016, among others).",2 Related Work,[0],[0]
"In parallel, joint modeling of tasks and addition of contextual signals has been shown to result in performance gains for several applications.",2 Related Work,[0],[0]
"Modeling domain, intent and slots in a joint RNN model was shown to result in reduction of overall frame error rates (Hakkani-Tür et al., 2016).",2 Related Work,[0],[0]
"Joint modeling of intent classification and language modeling showed promising improvements in intent recognition, especially in the presence of noisy speech recognition (Liu and Lane, 2016).",2 Related Work,[0],[0]
"Similarly, models incorporating more context from dialogue history (Chen et al., 2016) or semantic context from the frame (Dauphin et al., 2014; Bapna et al., 2017) tend to outperform models without context and have shown potential for greater generalization on spoken language understanding and related tasks.",2 Related Work,[0],[0]
"(Dhingra et al., 2016) show improved performance on an informational dialogue agent by incorporating knowledge base context into their dialogue system.",2 Related Work,[0],[0]
"Using dialogue context was shown to boost performance for end to end dialogue (Bordes and Weston, 2016) and next utterance prediction (Serban et al., 2015).",2 Related Work,[0],[0]
"In the next few sections, we describe the proposed model architecture, the dataset and our dialogue recombination approach.",2 Related Work,[0],[0]
This is followed by experimental results and analysis.,2 Related Work,[0],[0]
We compare the performance of 3 model architectures for encoding dialogue context on a multidomain dialogue dataset.,3 Model Architecture,[0],[0]
"Let the dialogue be a sequence of system and user utterances Dt =
{u1, u2...ut} and at time step t we are trying to output the parse of a user utterance ut, given Dt.",3 Model Architecture,[0],[0]
"Let any utterance uk be a sequence of tokens given by {xk1, xk2...xknk}.",3 Model Architecture,[0],[0]
"We divide the model into 2 components, the context encoder that acts on Dt to produce a vector representation of the dialogue context denoted by ht = H(Dt), and the tagger, which takes the dialogue context encoding ht, and the current utterance ut as input and produces the domain, intent and slot annotations as output.",3 Model Architecture,[0],[0]
In this section we describe the architectures of the context encoders used for our experiments.,3.1 Context Encoder Architectures,[0],[0]
We compare the performance of 3 different architectures that encode varying levels of dialogue context.,3.1 Context Encoder Architectures,[0],[0]
This is the baseline context encoder architecture.,3.1.1 Previous Utterance Encoder,[0],[0]
"We feed the embeddings corresponding to tokens in the previous system utterance, ut−1 = {xt−11 , xt−12 ...",3.1.1 Previous Utterance Encoder,[0],[0]
"xt−1nt−1}, into a single Bidirectional RNN (BiRNN) layer with Gated Recurrent Unit (GRU) (Chung et al., 2014) cells and 128 dimensions (64 in each direction).",3.1.1 Previous Utterance Encoder,[0],[0]
The embeddings are shared with the tagger.,3.1.1 Previous Utterance Encoder,[0],[0]
"The final state of the context encoder GRU is used as the dialogue context.
",3.1.1 Previous Utterance Encoder,[0],[0]
ht = BiGRUc(ut−1) (1),3.1.1 Previous Utterance Encoder,[0],[0]
"This architecture is identical to the approach described in (Chen et al., 2016).",3.1.2 Memory Network,[0],[0]
"We encode all dialogue context utterances, {u1, u2...ut−1}, into memory vectors denoted by {m1, m2, ...mt−1} using a Bidirectional GRU (BiGRU) encoder with 128 dimensions (64 in each direction).",3.1.2 Memory Network,[0],[0]
"To add temporal context to the dialogue history utter-
ances, we append special positional tokens to each utterance.
",3.1.2 Memory Network,[0],[0]
"mk = BiGRUm(uk) for 0 ≤ k ≤ t−1 (2)
We also encode the current utterance with another BiGRU encoder with 128 dimensions (64 in each direction), into a context vector denoted by c, as in equation 3.",3.1.2 Memory Network,[0],[0]
"This is conceptually depicted in Figure 2
c = BiGRUc(ut) (3)
Let M be a matrix with the ith row given by mi.",3.1.2 Memory Network,[0],[0]
"We obtain the cosine similarity between each memory vector, mi, and the context vector c. The softmax of this similarity is used as an attention distribution over the memory M , and an attention weighted sum of M is used to produce the dialogue context vector",3.1.2 Memory Network,[0],[0]
ht (Equation 4).,3.1.2 Memory Network,[0],[0]
"This is conceptually depicted in Figure 3.
",3.1.2 Memory Network,[0],[0]
"a = softmax(Mc)
ht = aT M (4)",3.1.2 Memory Network,[0],[0]
"We enhance the memory network architecture described above by adding a session encoder (Sordoni et al., 2015) that temporally combines a joint representation of the current utterance encoding, c, (Eq. 3) and the memory vectors, {m1, m2...mt−1}, (Eq. 2).",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"We combine the context vector c with each memory vector mk, for 1 ≤ k ≤",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"nk, by concatenating and passing them through a feed forward layer (FF) to produce 128 dimensional context encodings, denoted by {g1, g2...",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"gt−1} (Eq. 5).
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"gk = sigmoid(FF (mk, c))",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"for 0 ≤ k ≤ t−1 (5) These context encodings are fed as token level inputs into the session encoder, which is a 128 di-
Figure 4: Architecture of the Sequential Dialogue Encoder Network.",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"The feed-forward networks share weights across all memories.
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
mensional BiGRU layer.,3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"The final state of the session encoder represents the dialogue context encoding ht (Eq. 6).
",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"ht = BiGRUs({g1, g2, ...gt−1}) (6)",3.1.3 Sequential Dialogue Encoder Network,[0],[0]
The architecture is depicted in Figure 4.,3.1.3 Sequential Dialogue Encoder Network,[0],[0]
"For all our experiments we use a stacked BiRNN tagger to jointly model domain classification, intent classification and slot-filling, similar to the approach described in (Hakkani-Tür et al., 2016).",3.2 Tagger Architecture,[0],[0]
We feed learned 256 dimensional embeddings corresponding to the current utterance tokens into the tagger.,3.2 Tagger Architecture,[0],[0]
The first RNN layer uses GRU cells with 256 dimensions (128 in each direction) as in equation 7.,3.2 Tagger Architecture,[0],[0]
"The token embeddings are fed into the token level inputs of the first RNN layer to produce the token level outputs o1 = {o11, o12...o1nt}.
o1 = BiGRU1(ut) (7)
",3.2 Tagger Architecture,[0],[0]
"The second layer uses Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) cells with 256 dimensions (128 in both dimensions).",3.2 Tagger Architecture,[0],[0]
We use a LSTM based second layer since that improved slot-filling performance on the validation set for all architectures.,3.2 Tagger Architecture,[0],[0]
We apply dropout to the outputs of both layers.,3.2 Tagger Architecture,[0],[0]
The initial states of both forward and backward LSTMs of the second tagger layer are initialized with the dialogue encoding ht as in equation 8.,3.2 Tagger Architecture,[0],[0]
"The token level outputs of the first RNN layer, o1, are fed as input
into the second RNN layer to produce token level outputs o2 = {o21, o22...o2nt} and the final state s2.
",3.2 Tagger Architecture,[0],[0]
"o2, s2 = BiLSTM2(o1, ht) (8)
The final state of the second layer, s2, is used as input to classification layers for domain and intent classification.
",3.2 Tagger Architecture,[0],[0]
"pdomain = softmax(Us2)
pintent = sigmoid(V s2) (9)
",3.2 Tagger Architecture,[0],[0]
"The token level outputs of the second layer, o2, are used as input to a softmax layer that outputs the IOB slot labels.",3.2 Tagger Architecture,[0],[0]
"This results in a softmax layer with 2N+1 dimensions for a domain with N slots.
",3.2 Tagger Architecture,[0],[0]
psloti = softmax(So 2 i ) for 0 ≤,3.2 Tagger Architecture,[0],[0]
"i ≤ nt
(10) The architecture is depicted in Figure 5.",3.2 Tagger Architecture,[0],[0]
"We crowd sourced multi-turn dialogue sessions for 3 tasks: buying movie tickets, searching for a restaurant and reserving tables at a restaurant.",4 Dataset,[0],[0]
Our data collection process comprises of two steps: (i) Generating user-agent interactions comprising of dialog acts and slots based on the interplay of a simulated user and a rule based dialogue policy.,4 Dataset,[0],[0]
(ii) Using a crowd sourcing platform to elicit natural language utterances that align with the semantics of the generated interactions.,4 Dataset,[0],[0]
The goal of the spoken language understanding module of our dialogue system is to map each user utterance into frame based semantics that can be processed by the downstream components.,4 Dataset,[0],[0]
Tables describing the intents and slots present in the dataset can be found in the appendix.,4 Dataset,[0],[0]
"We use a stochastic agenda-based user simulator (Schatzmann et al., 2007; Shah et al., 2016) for interplay with our rule based system policy.",4 Dataset,[0],[0]
"The user goal is specified in terms of a tuple of slots, which denote the user constraints.",4 Dataset,[0],[0]
"Some constraints might be unspecified, in which case the user is indifferent to the value of those slots.",4 Dataset,[0],[0]
"At any given turn, the simulator samples a user dialogue act from a set of acceptable actions based on (i) the user goal and agenda that includes slots that still need to be specified, (ii) a randomly chosen user profile (co-operative/aggressive, verbose/succinct etc.)",4 Dataset,[0],[0]
"and (iii) the previous user and
system actions.",4 Dataset,[0],[0]
"Based on the chosen user dialogue act, the rule based policy might make a backend call to inquire for restaurant or movie availability.",4 Dataset,[0],[0]
"Based on the user act and the backend response the system responds back with a dialogue act or a combination of dialogue acts, based on a hand designed rule based policy.",4 Dataset,[0],[0]
These generated interactions were then translated to their natural language counterparts and sent out to crowdworkers for paraphrasing into natural language human-machine dialogues.,4 Dataset,[0],[0]
The simulator and policy were also extended to handle multiple goals spanning different domains.,4 Dataset,[0],[0]
"In this set-up, the user goal for the simulator would include multiple tasks and slot values could be conditioned on the previous task, for example, the simulator would ask for booking a table ”after the movie”, or search for a restaurant ”near the theater”.",4 Dataset,[0],[0]
The set of slots supported by the simulator is enumerated in Table 1.,4 Dataset,[0],[0]
"We collected 1319 dialogues for restaurant reservation, 976 dialogues for finding restaurants and 1048 dialogues for buying movie tickets.",4 Dataset,[0],[0]
"All single domain datasets were
used for training.",4 Dataset,[0],[0]
"The multi-domain simulator was used to collect 467 dialogues for training, 50 for validation and 273 for the test set.",4 Dataset,[0],[0]
"Since the natural language dialogues were paraphrased versions of known dialogue- act and slot combinations, they were automatically labeled.",4 Dataset,[0],[0]
"These labels were verified by an expert annotator, and turns with missing annotations were manually annotated by the expert.",4 Dataset,[0],[0]
"As described in the previous section, we train our models on a large set of single domain dialogue datasets and a small set of multi-domain dialogues.",5 Dialogue Recombination,[0],[0]
"These models are then evaluated on a test set composed of multi-domain dialogues, where the user attempts to fulfill multiple goals spanning several domains.",5 Dialogue Recombination,[0],[0]
This results in a distribution drift that might result in performance degradation.,5 Dialogue Recombination,[0],[0]
"To counter this drift in the training-test data distributions we device a dialogue recombination scheme to generate multi-domain dialogues from single domain training datasets.
",5 Dialogue Recombination,[0],[0]
"The key idea behind the recombination approach is the conditional independence of sub-dialogues aimed at performing distinct tasks (Grosz and Sidner, 1986).",5 Dialogue Recombination,[0],[0]
"We exploit the presence of task intents, or intents that denote a switch in the primary task the user is trying to perform, since they are a strong indicator of a switch in the focus of the dialogue.",5 Dialogue Recombination,[0],[0]
"We exploit the independence of the sub-dialogue following these intents from the previous dialogue context, to generate synthetic dialogues with multi-domain context.",5 Dialogue Recombination,[0],[0]
"The recombination process is described as follows: Let a dialogue d be defined as a sequence of turns and corresponding semantic labels (domain, intent and slot annotations) {(td1, fd1), (td2, fd2), ...(tdnd , fdnd}.",5 Dialogue Recombination,[0],[0]
"To obtain a re-combined dataset composed of dialogues from dataset dataset1 and dataset2, we repeat the following steps 10000 times, for each combination of (dataset1, dataset2) from the three single domain datasets.
",5 Dialogue Recombination,[0],[0]
"• Sample dialogues x and y from dataset1 and dataset2 respectively.
• Find the first user utterance labeled with a task intent in y. Let this be turn l.
•",5 Dialogue Recombination,[0],[0]
Randomly sample an insertion point in dialogue x.,5 Dialogue Recombination,[0],[0]
"Let this be turn k.
•",5 Dialogue Recombination,[0],[0]
"The new recombined dialogue is {(tx1, fx1), ...(txk, fxk), (tyl, fyl), ...(tyny , fyny)}.
",5 Dialogue Recombination,[0],[0]
A sample dialogue generated using the above procedure is described in table 2.,5 Dialogue Recombination,[0],[0]
We drop the utterances from dialogue x following the insertion point (turn k) in the recombined dialogue since these turns become ambiguous or confusing in the absence of preceding context.,5 Dialogue Recombination,[0],[0]
In a sense our approach is one of partial dialogue recombination.,5 Dialogue Recombination,[0],[0]
"We compare the domain classification, intent classification and slot-filling performances, and the overall frame error rates of the encoder-decoder, memory network and sequential dialogue encoder network on the dataset described above.",6 Experiments,[0],[0]
"The frame error rate of a SLU system is the percentage of utterances where it makes a wrong prediction i.e. any of domain, intent or slot is predicted incorrectly.",6 Experiments,[0],[0]
We trained all 3 models with RMSProp for 100000 training steps with a batch size of 100.,6 Experiments,[0],[0]
We started with a learning rate of 0.0003 which was decayed by a factor of 0.95 every 3000 steps.,6 Experiments,[0],[0]
Gradient norms were clipped if they exceed a magnitude of 2.5.,6 Experiments,[0],[0]
"All model and optimization hyper-parameters were chosen based on a grid search, to minimize validation set frame error rates.
",6 Experiments,[0],[0]
"We restrict the model vocabularies to contain only tokens occurring more than 10 times in the training set, to prevent over-fitting to training set entities.",6 Experiments,[0],[0]
Digits were replaced with a special ”#” token to allow better generalization to unseen numbers.,6 Experiments,[0],[0]
The dialogue history was padded to 40 utterances for batch processing.,6 Experiments,[0],[0]
We report results with and without the recombined dataset in Table 3.,6 Experiments,[0],[0]
"The encoder decoder model trained on just the previous turn context performs worst on almost all metrics, irrespective of the presence of recombined data.",7 Results,[0],[0]
"This can be explained by worse performance on in-dialogue utterances, where just the previous turn context isn’t sufficient to accurately identify the domain, and in several cases, the intents and slots of the utterance.",7 Results,[0],[0]
"The memory network is the best performing model in the absence of recombined data, indicating that
the model is able to encode additional context effectively to improve performance on all tasks, even when only a small amount of multi-domain data is available.",7 Results,[0],[0]
The Sequential dialogue encoder network performs slightly worse than the memory network in the absence of recombined data.,7 Results,[0],[0]
This could be explained by the model over-fitting to the single domain context seen during training and failure to utilize context effectively in a multi-domain setting.,7 Results,[0],[0]
In the presence of recombined dialogues it outperforms all other implementations.,7 Results,[0],[0]
"Apart from increasing the noise in the dialogue context, adding recombined dialogues to the training set increases the average turn length of the training data, bringing it closer to that of the test dialogues.",7 Results,[0],[0]
"Our augmentation approach is, in spirit, an extension of the data recombination described in (Jia and Liang, 2016) to conversations.",7 Results,[0],[0]
"We hypothesize that the presence of synthetic con-
text has a regularization-like effect on the models.",7 Results,[0],[0]
"Similar effects were observed by (Jia and Liang, 2016), where training with longer, syntheticallyaugmented utterances resulted in improved semantic parsing performance on a simpler test set.",7 Results,[0],[0]
This is also supported by the observation that performance improvements obtained by addition of recombined data increase as the complexity of the model increases.,7 Results,[0],[0]
"Table 4 demonstrates an example dialogue from the test set, along with the gold and model annotations from all 3 models.",8 Discussion and Conclusions,[0],[0]
"We observe that Encoder Decoder (ED) and Sequential Dialogue Encoder Network (SDEN) are able to successfully identify the domain, intent and slots, while the Memory Network (MN) fails to identify the movie name.
",8 Discussion and Conclusions,[0],[0]
"Looking at the attention distributions, we notice that the MN attention is very diffused, whereas SDEN is focusing on the most recent last 2 utterances, which directly identify the domain and the presence of the movie slot in the final user utterance.",8 Discussion and Conclusions,[0],[0]
ED is also able to identify the presence of a movie in the final user utterance from the previous utterance context.,8 Discussion and Conclusions,[0],[0]
Table 5 displays another example where the SDEN model outperforms both MN and ED.,8 Discussion and Conclusions,[0],[0]
Constrained to just the previous utterance ED is unable to correctly identify the domain of the user utterance.,8 Discussion and Conclusions,[0],[0]
"The MN model correctly identifies the domain, using its strong focus on the task-intent bearing utterance, but it is unable to identify the presence of a restaurant in the user utterance.",8 Discussion and Conclusions,[0],[0]
This highlights its failure to combine context from multiple history utterances.,8 Discussion and Conclusions,[0],[0]
"On the other hand, as indicated by its attention distribution on the final
two utterances, SDEN is able to successfully combine context from the dialogue to correctly identify the domain and the restaurant name from the user utterance, despite the presence of several outof-vocabulary tokens.",8 Discussion and Conclusions,[0],[0]
The above two examples hint that SDEN performs better in scenarios where multiple history utterances encode complementary information that could be useful to interpret user utterances.,8 Discussion and Conclusions,[0],[0]
"This is usually the case in more natural goal oriented dialogues, where several tasks and sub tasks go in and out of the focus of the conversation (Grosz, 1979).",8 Discussion and Conclusions,[0],[0]
"On the other hand, we also observed that SDEN performs significantly worse in the absence of recombined data.",8 Discussion and Conclusions,[0],[0]
Due to its complex architecture and a much larger set of parameters SDEN is prone to over-fitting in low data scenarios.,8 Discussion and Conclusions,[0],[0]
"In this paper, we collect a multi-domain dataset of goal oriented human-machine conversations and analyze and compare the SLU performance of multiple neural network based model architectures that can encode varying amounts of context.",8 Discussion and Conclusions,[0],[0]
"Our experiments suggest that encoding more context from the dialogue, and enabling the model to combine contextual information in a sequential order results in a reduction in overall frame error rate.",8 Discussion and Conclusions,[0],[0]
"We also introduce a data augmentation scheme to generate longer dialogues with richer context, and empirically demonstrate that it results in performance improvement for multiple model architectures.",8 Discussion and Conclusions,[0],[0]
"We would like to thank Pararth Shah, Abhinav Rastogi, Anna Khasin and Georgi Nikolov for their help with the user-machine conversation data collection and labeling.",9 Acknowledgements,[0],[0]
We would also like to thank the anonymous reviewers for their insightful comments.,9 Acknowledgements,[0],[0]
Spoken Language Understanding (SLU) is a key component of goal oriented dialogue systems that would parse user utterances into semantic frame representations.,abstractText,[0],[0]
Traditionally SLU does not utilize the dialogue history beyond the previous system turn and contextual ambiguities are resolved by the downstream components.,abstractText,[0],[0]
"In this paper, we explore novel approaches for modeling dialogue context in a recurrent neural network (RNN) based language understanding system.",abstractText,[0],[0]
"We propose the Sequential Dialogue Encoder Network, that allows encoding context from the dialogue history in chronological order.",abstractText,[0],[0]
"We compare the performance of our proposed architecture with two context models, one that uses just the previous turn context and another that encodes dialogue context in a memory network, but loses the order of utterances in the dialogue history.",abstractText,[0],[0]
Experiments with a multi-domain dialogue dataset demonstrate that the proposed architecture results in reduced semantic frame error rates.,abstractText,[0],[0]
Sequential Dialogue Context Modeling for Spoken Language Understanding,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 496–505 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1046",text,[0],[0]
Conversational agents include task-oriented dialog systems and non-task-oriented chatbots.,1 Introduction,[0],[0]
"Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011).",1 Introduction,[0],[0]
Existing work on building chatbots includes generation -based methods and retrieval-based methods.,1 Introduction,[0],[0]
"Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for
∗ Corresponding Author
the current conversation from a repository with response selection algorithms.",1 Introduction,[0],[0]
"While most existing work on retrieval-based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario.",1 Introduction,[0],[0]
"In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.
",1 Introduction,[0],[0]
The key to response selection lies in inputresponse matching.,1 Introduction,[0],[0]
"Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also matching between responses and utterances in previous turns.",1 Introduction,[0],[0]
"The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in context, which is crucial to selecting a proper response and leveraging relevant information in matching; and (2) how to model relationships among the utterances in the context.",1 Introduction,[0],[0]
Table 1 illustrates the challenges with an example.,1 Introduction,[0],[0]
"First, “hold a drum class” and “drum” in context are very important.",1 Introduction,[0],[0]
"Without them, one may find responses relevant to the message (i.e., the fifth utterance of the context) but nonsense in the context (e.g., “what lessons do you want?”).",1 Introduction,[0],[0]
"Second, the message highly depends on the second utterance in the context, and
496
the order of the utterances matters in response selection: exchanging the third utterance and the fifth utterance may lead to different responses.",1 Introduction,[0],[0]
"Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).
",1 Introduction,[0],[0]
"We propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way.",1 Introduction,[0],[0]
The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector.,1 Introduction,[0],[0]
"Thus, responses in these models connect with the context until the final step in matching.",1 Introduction,[0],[0]
"To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector.",1 Introduction,[0],[0]
The matching vectors are then accumulated in the utterances’ temporal order to model their relationships.,1 Introduction,[0],[0]
The final matching degree is computed with the accumulation of the matching vectors.,1 Introduction,[0],[0]
"Specifically, for each utterance-response pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the word embeddings and the hidden states of a recurrent neural network with gated recurrent units (GRU) (Chung et al., 2014) respectively.",1 Introduction,[0],[0]
"The two matrices capture important matching information in the pair on a word level and a segment (word subsequence) level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices.",1 Introduction,[0],[0]
"By this means, important information from multiple levels of granularity in context is recognized under sufficient supervision from the response and carried into matching with minimal loss.",1 Introduction,[0],[0]
The matching vectors are then uploaded to another GRU to form a matching score for the context and the response.,1 Introduction,[0],[0]
The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in context.,1 Introduction,[0],[0]
It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching.,1 Introduction,[0],[0]
"The matching degree of the context and the response is computed by a logit
model with the hidden states of the GRU. SMN extends the powerful “2D” matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage of both important information in utterance-response pairs and relationships among utterances being sufficiently preserved and leveraged in matching.
",1 Introduction,[0],[0]
"We test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale publicly available English data set for research in multi-turn conversation.",1 Introduction,[0],[0]
"The results show that our model can significantly outperform state-ofthe-art methods, and improvement to the best baseline model on R10@1 is over 6%.",1 Introduction,[0],[0]
"In addition to the Ubuntu corpus, we create a human-labeled Chinese data set, namely the Douban Conversation Corpus, and test our model on it.",1 Introduction,[0],[0]
"In contrast to the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from the open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges.",1 Introduction,[0],[0]
"On this data, our model improves the best baseline model by 3% on R10@1 and 4% on P@1.",1 Introduction,[0],[0]
"As far as we know, Douban Conversation Corpus is the first human-labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus.",1 Introduction,[0],[0]
"We have released Douban Conversation Corups and our source code at https://github.com/MarkWuNLP/ MultiTurnResponseSelection
Our contributions in this paper are three-folds: (1) the proposal of a new context based matching model for multi-turn response selection in retrieval-based chatbots; (2) the publication of a large human-labeled data set to research communities; (3) empirical verification of the effectiveness of the model on public data sets.",1 Introduction,[0],[0]
"Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Ji et al., 2014) has drawn significant attention.",2 Related Work,[0],[0]
"Existing work along this line includes retrieval-based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation-based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing et al.,
2016; Serban et al., 2016).",2 Related Work,[0],[0]
"Our work is a retrievalbased method, in which we study context-based response selection.
",2 Related Work,[0],[0]
"Early studies of retrieval-based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",2 Related Work,[0],[0]
"Recently, researchers have begun to pay attention to multi-turn conversation.",2 Related Work,[0],[0]
"For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances.",2 Related Work,[0],[0]
Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture.,2 Related Work,[0],[0]
Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view.,2 Related Work,[0],[0]
"Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained.",2 Related Work,[0],[0]
"Suppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . .",3.1 Problem Formalization,[0],[0]
", ui,ni} represents a conversation context with {ui,k}nik=1 as utterances.",3.1 Problem Formalization,[0],[0]
"ri is a response candidate and yi ∈ {0, 1} denotes a label.",3.1 Problem Formalization,[0],[0]
"yi = 1 means ri is a proper response for si, otherwise yi = 0.",3.1 Problem Formalization,[0],[0]
"Our goal is to learn a matching model g(·, ·) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r.",3.1 Problem Formalization,[0],[0]
"We propose a sequential matching network (SMN) to model g(·, ·).",3.2 Model Overview,[0],[0]
"Figure 1 gives the architecture.
",3.2 Model Overview,[0],[0]
SMN first decomposes context-response matching into several utterance-response pair matching and then all pairs matching are accumulated as a context based matching through a recurrent neural network.,3.2 Model Overview,[0],[0]
SMN consists of three layers.,3.2 Model Overview,[0],[0]
"The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution, pooling and encoded in a matching vector.",3.2 Model Overview,[0],[0]
The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context.,3.2 Model Overview,[0],[0]
"The third layer calculates the final matching score with the hidden states of the second layer.
",3.2 Model Overview,[0],[0]
SMN enjoys several advantages over existing models.,3.2 Model Overview,[0],[0]
"First, a response candidate can match each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss.",3.2 Model Overview,[0],[0]
"Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful for response selection in each utterance can be well identified and extracted.",3.2 Model Overview,[0],[0]
"Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.
",3.2 Model Overview,[0],[0]
"By taking utterance relationships into account, SMN extends the “2D” matching that has proven effective in text pair matching for single-turn response selection to sequential “2D” matching for
context based matching in response selection for multi-turn conversation.",3.2 Model Overview,[0],[0]
"In the following sections, we will describe details of the three layers.",3.2 Model Overview,[0],[0]
"Given an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U =",3.3 Utterance-Response Matching,[0],[0]
"[eu,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", eu,nu ] and R =",3.3 Utterance-Response Matching,[0],[0]
"[er,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", er,nr ] respectively, where eu,i, er,i ∈ Rd are the embeddings of the i-th word of u and r respectively.",3.3 Utterance-Response Matching,[0],[0]
U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input channels of a convolutional neural network (CNN).,3.3 Utterance-Response Matching,[0],[0]
"The CNN distills important matching information from the matrices and encodes the information into a matching vector v.
Specifically, ∀i, j, the (i, j)-th element of M1 is defined by
e1,i,j = e > u,i · er,j .",3.3 Utterance-Response Matching,[0],[0]
"(1)
M1 models the matching between u and r on a word level.
",3.3 Utterance-Response Matching,[0],[0]
"To construct M2, we first employ a GRU to transform U and R to hidden vectors.",3.3 Utterance-Response Matching,[0],[0]
Suppose that Hu =,3.3 Utterance-Response Matching,[0],[0]
"[hu,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", hu,nu ] are the hidden vectors of U, then ∀i, hu,i ∈",3.3 Utterance-Response Matching,[0],[0]
"Rm is defined by
zi = σ(Wzeu,i +Uzhu,i−1) ri = σ(Wreu,",3.3 Utterance-Response Matching,[0],[0]
"i +Urhu,i−1)
h̃u,i = tanh(Wheu,i +Uh(ri hu,i−1)) hu,i = zi h̃u,i + (1− zi) hu,i−1, (2)
",3.3 Utterance-Response Matching,[0],[0]
"where hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters.",3.3 Utterance-Response Matching,[0],[0]
"Similarly, we have Hr =",3.3 Utterance-Response Matching,[0],[0]
"[hr,1, . . .",3.3 Utterance-Response Matching,[0],[0]
", hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th element of M2 is defined by
e2,i,j = h > u,iAhr,j , (3)
where A ∈ Rm×m is a linear transformation.",3.3 Utterance-Response Matching,[0],[0]
"∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector.",3.3 Utterance-Response Matching,[0],[0]
"Therefore, M2 models the matching between u and r on a segment level.",3.3 Utterance-Response Matching,[0],[0]
M1 and M2 are then processed by a CNN to form v. ∀f,3.3 Utterance-Response Matching,[0],[0]
"= 1, 2, CNN regards Mf as
an input channel, and alternates convolution and max-pooling operations.",3.3 Utterance-Response Matching,[0],[0]
"Suppose that z(l,f) =[ z",3.3 Utterance-Response Matching,[0],[0]
"(l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) =",3.3 Utterance-Response Matching,[0],[0]
"Mf , ∀f = 1, 2.",3.3 Utterance-Response Matching,[0],[0]
"On the convolution layer, we employ a 2D convolution operation with a window size r (l,f)",3.3 Utterance-Response Matching,[0],[0]
"w × r(l,f)h , and define z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j as
z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j = σ(
Fl−1∑
f ′=0
r (l,f) w∑
s=0
r (l,f) h∑
t=0
W (l,f) s,t · z(l−1,f ′) i+s,j+t + b",3.3 Utterance-Response Matching,[0],[0]
"l,k), (4)
where σ(·) is a ReLU, W(l,",3.3 Utterance-Response Matching,[0],[0]
"f) ∈ Rr(l,f)w ×r(l,f)h and bl,k are parameters, and Fl−1 is the number of feature maps on the (l − 1)-th layer.",3.3 Utterance-Response Matching,[0],[0]
"A max pooling operation follows a convolution operation and can be formulated as
z (l,f)",3.3 Utterance-Response Matching,[0],[0]
"i,j = max
p (l,f)",3.3 Utterance-Response Matching,[0],[0]
w >,3.3 Utterance-Response Matching,[0],[0]
"s≥0 max p (l,f) h >t≥0 zi+s,j+t, (5)
where p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively.",3.3 Utterance-Response Matching,[0],[0]
The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v ∈,3.3 Utterance-Response Matching,[0],[0]
"Rq.
",3.3 Utterance-Response Matching,[0],[0]
"According to Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful for recognizing the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices.",3.3 Utterance-Response Matching,[0],[0]
These areas will be transformed and selected by convolution and pooling operations and carry important information in the utterance to the matching vector.,3.3 Utterance-Response Matching,[0],[0]
This is how our model identifies important information in context and leverage it in matching under the supervision of the response.,3.3 Utterance-Response Matching,[0],[0]
We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text.,3.3 Utterance-Response Matching,[0],[0]
"Suppose that [v1, . . .",3.4 Matching Accumulation,[0],[0]
", vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . .",3.4 Matching Accumulation,[0],[0]
", vn] as an input and encodes the matching sequence into its hidden states Hm =",3.4 Matching Accumulation,[0],[0]
"[h ′ 1, . . .",3.4 Matching Accumulation,[0],[0]
", h ′ n] ∈",3.4 Matching Accumulation,[0],[0]
Rq×n with a detailed parameterization similar to Equation (2).,3.4 Matching Accumulation,[0],[0]
"This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the
context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching.",3.4 Matching Accumulation,[0],[0]
"Moreover, from Equation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out.",3.4 Matching Accumulation,[0],[0]
"With [h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
"], we define g(s, r) as
g(s, r) =",3.5 Matching Prediction and Learning,[0],[0]
softmax(W2L[h ′,3.5 Matching Prediction and Learning,[0],[0]
"1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] + b2), (6)
where W2 and b2 are parameters.",3.5 Matching Prediction and Learning,[0],[0]
"We consider three parameterizations for L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
]: (1) only the last hidden state is used.,3.5 Matching Prediction and Learning,[0],[0]
"Then L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
] = h ′,3.5 Matching Prediction and Learning,[0],[0]
n. (2) the hidden states are linearly combined.,3.5 Matching Prediction and Learning,[0],[0]
"Then, L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n",3.5 Matching Prediction and Learning,[0],[0]
"] =∑n
i=1wih ′",3.5 Matching Prediction and Learning,[0],[0]
"i, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states.",3.5 Matching Prediction and Learning,[0],[0]
"Then, L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] is defined as
ti = tanh(W1,1hui,nu +W1,2h ′",3.5 Matching Prediction and Learning,[0],[0]
"i + b1), αi =",3.5 Matching Prediction and Learning,[0],[0]
exp(t>i ts)∑,3.5 Matching Prediction and Learning,[0],[0]
"i(exp(t > i ts)) ,
L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n] =
n∑
i=1
αih ′",3.5 Matching Prediction and Learning,[0],[0]
"i, (7)
where W1,1 ∈ Rq×m,W1,2 ∈ Rq×q and b1 ∈",3.5 Matching Prediction and Learning,[0],[0]
Rq are parameters.,3.5 Matching Prediction and Learning,[0],[0]
"h′i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively.",3.5 Matching Prediction and Learning,[0],[0]
ts ∈,3.5 Matching Prediction and Learning,[0],[0]
"Rq is a virtual context vector which is randomly initialized and jointly learned in training.
",3.5 Matching Prediction and Learning,[0],[0]
"Both (2) and (3) aim to learn weights for {h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h′n} from training data and highlight the effect of important matching vectors in the final matching.",3.5 Matching Prediction and Learning,[0],[0]
"The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors.",3.5 Matching Prediction and Learning,[0],[0]
"We denote our model with the three parameterizations of L[h′1, . . .",3.5 Matching Prediction and Learning,[0],[0]
", h ′ n]",3.5 Matching Prediction and Learning,[0],[0]
"as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.
",3.5 Matching Prediction and Learning,[0],[0]
"We learn g(·, ·) by minimizing cross entropy withD. Let Θ denote the parameters of SMN, then the objective function L(D,Θ) of learning can be
formulated as
− N∑
i=1
",3.5 Matching Prediction and Learning,[0],[0]
"[yilog(g(si, ri))",3.5 Matching Prediction and Learning,[0],[0]
"+ (1− yi)log(1− g(si, ri))] .",3.5 Matching Prediction and Learning,[0],[0]
(8),3.5 Matching Prediction and Learning,[0],[0]
"In practice, a retrieval-based chatbot, to apply the matching approach to the response selection, one needs to retrieve a number of response candidates from an index beforehand.",4 Response Candidate Retrieval,[0],[0]
"While candidate retrieval is not the focus of the paper, it is an important step in a real system.",4 Response Candidate Retrieval,[0],[0]
"In this work, we exploit a heuristic method to obtain response candidates from the index.",4 Response Candidate Retrieval,[0],[0]
"Given a message un with {u1, . . .",4 Response Candidate Retrieval,[0],[0]
", un−1} utterances in its previous turns, we extract the top 5 keywords from {u1, . . .",4 Response Candidate Retrieval,[0],[0]
", un−1} based on their tf-idf scores1 and expand un with the keywords.",4 Response Candidate Retrieval,[0],[0]
Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index.,4 Response Candidate Retrieval,[0],[0]
"Finally, we use g(s, r) to rerank the candidates and return the top one as a response to the context.",4 Response Candidate Retrieval,[0],[0]
We tested our model on a publicly available English data set and a Chinese data set published with this paper.,5 Experiments,[0],[0]
"The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of the Ubuntu Forum.",5.1 Ubuntu Corpus,[0],[0]
"The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for testing.",5.1 Ubuntu Corpus,[0],[0]
"Positive responses are true responses from humans, and negative ones are randomly sampled.",5.1 Ubuntu Corpus,[0],[0]
"The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and testing.",5.1 Ubuntu Corpus,[0],[0]
"We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders.",5.1 Ubuntu Corpus,[0],[0]
"We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.
",5.1 Ubuntu Corpus,[0],[0]
"1Tf is word frequency in the context, while idf is calculated using the entire index.
",5.1 Ubuntu Corpus,[0],[0]
2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0,5.1 Ubuntu Corpus,[0],[0]
"The Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment.",5.2 Douban Conversation Corpus,[0],[0]
"To further verify the efficacy of our model, we created a new data set with open domain conversations, called the Douban Conversation Corpus.",5.2 Douban Conversation Corpus,[0],[0]
Response candidates in the test set of the Douban Conversation Corpus are collected following the procedure of a retrieval-based chatbot and are labeled by human judges.,5.2 Douban Conversation Corpus,[0],[0]
It simulates the real scenario of a retrievalbased chatbot.,5.2 Douban Conversation Corpus,[0],[0]
"We publish it to research communities to facilitate the research of multi-turn response selection.
",5.2 Douban Conversation Corpus,[0],[0]
"Specifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China.",5.2 Douban Conversation Corpus,[0],[0]
"We randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap between the three sets.",5.2 Douban Conversation Corpus,[0],[0]
"For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response.",5.2 Douban Conversation Corpus,[0],[0]
"There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.
",5.2 Douban Conversation Corpus,[0],[0]
"To create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5.",5.2 Douban Conversation Corpus,[0],[0]
"We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs.",5.2 Douban Conversation Corpus,[0],[0]
We recruited three labelers to judge if a candidate is a proper response to the context.,5.2 Douban Conversation Corpus,[0],[0]
A proper response means the response can naturally reply to the message given the whole context.,5.2 Douban Conversation Corpus,[0],[0]
Each pair received three labels and the majority of the labels were taken as the final decision.,5.2 Douban Conversation Corpus,[0],[0]
Table 2 gives the statistics of the three sets.,5.2 Douban Conversation Corpus,[0],[0]
"Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.
",5.2 Douban Conversation Corpus,[0],[0]
"Besides Rn@ks, we also followed the conven-
3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/
tion of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics.",5.2 Douban Conversation Corpus,[0.9552499452260704],"['(2) To make the evaluation of this integral efficient, one can use conjugate models (Xuan & Murphy, 2007) or approximations (Turner et al., 2013; Niekum et al., 2014), which make the following recursion efficient, too: p(y1:t, rt,mt) =∑ mt−1 ∑ rt−1 { fmt(yt|y1:(t−1), rt)q(mt|y1:(t−1), rt,mt−1) p(rt|rt−1)p(y1:(t−1), rt−1,mt−1) } .']"
"We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation.",5.2 Douban Conversation Corpus,[0],[0]
"When using the labeled set, we removed conversations with all negative responses or all positive responses, as models make no difference with them.",5.2 Douban Conversation Corpus,[0],[0]
"There are 6, 670 contextresponse pairs left in the test set.",5.2 Douban Conversation Corpus,[0],[0]
"We considered the following baselines:
Basic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.
Multi-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.
",5.3 Baseline,[0],[0]
"Deep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.
",5.3 Baseline,[0],[0]
"Advanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016)",5.3 Baseline,[0],[0]
"(2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3.",5.3 Baseline,[0],[0]
Multi-Channel is a simple version of our model without considering utterance relationships.,5.3 Baseline,[0],[0]
"We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp.",5.3 Baseline,[0],[0]
"For baseline models, if their results are available in existing literature (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures.",5.4 Parameter Tuning,[0],[0]
"All models were implemented using Theano (Theano Development Team, 2016).",5.4 Parameter Tuning,[0],[0]
"Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200.",5.4 Parameter Tuning,[0],[0]
"For Multi-Channel and layer one of our model, we set the dimensionality of the hidden states of GRU as 200.",5.4 Parameter Tuning,[0],[0]
"We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally.",5.4 Parameter Tuning,[0],[0]
The number of feature maps is 8.,5.4 Parameter Tuning,[0],[0]
"In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50.",5.4 Parameter Tuning,[0],[0]
"The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU.",5.4 Parameter Tuning,[0],[0]
"The initial learning rate is 0.001, and the parameters of Adam, β1 and β2 are 0.9 and 0.999 respectively.",5.4 Parameter Tuning,[0],[0]
We employed early-stopping as a regularization strategy.,5.4 Parameter Tuning,[0],[0]
"Models were trained in minibatches with a batch size of 200, and the maximum utterance length is 50.",5.4 Parameter Tuning,[0],[0]
"We set the maximum context length (i.e., number of utterances) as 10, because the performance of models does not improve on contexts longer than 10 (details are shown in the Section 5.6).",5.4 Parameter Tuning,[0],[0]
"We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances.",5.4 Parameter Tuning,[0],[0]
Table 3 shows the evaluation results on the two data sets.,5.5 Evaluation Results,[0],[0]
"Our models outperform baselines
greatly in terms of all metrics on both data sets, with the improvements being statistically significant (t-test with p-value ≤ 0.01, except R10@5 on Douban Corpus).",5.5 Evaluation Results,[0],[0]
Even the state-of-the-art singleturn matching models perform much worse than our models.,5.5 Evaluation Results,[0],[0]
The results demonstrate that one cannot neglect utterance relationships and simply perform multi-turn response selection by concatenating utterances together.,5.5 Evaluation Results,[0],[0]
"Our models achieve significant improvements over Multi-View, which justified our “matching first” strategy.",5.5 Evaluation Results,[0],[0]
"DL2R is worse than our models, indicating that utterance reformulation with heuristic rules is not a good method for utilizing context information.",5.5 Evaluation Results,[0],[0]
"Rn@ks are low on the Douban Corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33).",5.5 Evaluation Results,[0],[0]
SMNdynamic is only slightly better than SMNstatic and SMNlast.,5.5 Evaluation Results,[0],[0]
"The reason might be that the GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of an attention mechanism is not obvious for the task at hand.",5.5 Evaluation Results,[0],[0]
Visualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4.,5.6 Further Analysis,[0],[0]
The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how?,5.6 Further Analysis,[0],[0]
"u4: are the files all
in the same directory?",5.6 Further Analysis,[0],[0]
u5:,5.6 Further Analysis,[0],[0]
yes they all are; r: then the command glebihan should extract them all from/to that directory}.,5.6 Further Analysis,[0],[0]
It is from the test set and our model successfully ranked the correct response to the top position.,5.6 Further Analysis,[0],[0]
"Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2.",5.6 Further Analysis,[0],[0]
"We can see that in u1 important words including “unzip”, “rar”, “files” are recognized and carried to matching by “command”, “extract”, and “directory” in r, while u3 is almost useless and thus little information is extracted from it.",5.6 Further Analysis,[0],[0]
"u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the final state.
",5.6 Further Analysis,[0],[0]
"Model ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4.",5.6 Further Analysis,[0],[0]
"First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically.",5.6 Further Analysis,[0],[0]
This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair.,5.6 Further Analysis,[0],[0]
"Together with the visualization, we can conclude that “2D” matching plays a key role in the “matching first” strategy as it captures the important matching information in each pair with minimal loss.",5.6 Further Analysis,[0],[0]
"Second, the performance drops slightly when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA).",5.6 Further Analysis,[0],[0]
This indicates that utterance relationships are useful.,5.6 Further Analysis,[0],[0]
"Finally, we left only one channel in matching
and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on the Douban Corpus).
",5.6 Further Analysis,[0],[0]
Performance across context length: we study how our model (SMNlast) performs across the length of contexts.,5.6 Further Analysis,[0],[0]
Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus.,5.6 Further Analysis,[0],[0]
"Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger.",5.6 Further Analysis,[0],[0]
"The results demonstrate that our model can well capture the dependencies, especially long dependencies, among utterances in contexts.
",5.6 Further Analysis,[0],[0]
Maximum context length: we investigate the influence of maximum context length for SMN.,5.6 Further Analysis,[0],[0]
Figure 4 shows the performance of SMN on Ubuntu Corpus and Douban Corpus with respect to maximum context length.,5.6 Further Analysis,[0],[0]
"From Figure 4, we find that performance improves significantly when the maximum context length is lower than 5, and becomes stable after the context length reaches 10.",5.6 Further Analysis,[0],[0]
"This indicates that context information is important for multi-turn response selection, and we can set the maximum context length as 10 to balance effectiveness and efficiency.
",5.6 Further Analysis,[0],[0]
"Error analysis: although SMN outperforms baseline methods on the two data sets, there are
still several problems that cannot be handled perfectly.
(1) Logical consistency.",5.6 Further Analysis,[0],[0]
"SMN models the context and response on the semantic level, but pays little attention to logical consistency.",5.6 Further Analysis,[0],[0]
This leads to several DSATs in the Douban Corpus.,5.6 Further Analysis,[0],[0]
"For example, given a context {a: Does anyone know Newton jogging shoes?",5.6 Further Analysis,[0],[0]
b: 100 RMB on Taobao.,5.6 Further Analysis,[0],[0]
a: I know that.,5.6 Further Analysis,[0],[0]
"I do not want to buy it because that is a fake which is made in Qingdao ,b: Is it the only reason you do not want to buy it? }, SMN gives a large score to the response { It is not a fake.",5.6 Further Analysis,[0],[0]
I just worry about the date of manufacture}.,5.6 Further Analysis,[0],[0]
"The response is inconsistent with the context on logic, as it claims that the jogging shoes are not fake.",5.6 Further Analysis,[0],[0]
"In the future, we shall explore the logic consistency problem in retrieval-based chatbots.
",5.6 Further Analysis,[0],[0]
(2) No correct candidates after retrieval.,5.6 Further Analysis,[0],[0]
"In the experiment, we prepared 1000 contexts for testing, but only 667 contexts have correct candidates after candidate response retrieval.",5.6 Further Analysis,[0],[0]
"This indicates that there is still room for candidate retrieval components to improve, and only expanding the input message with several keywords in context may not be a perfect approach for candidate retrieval.",5.6 Further Analysis,[0],[0]
"In the future, we will consider advanced methods for retrieving candidates.",5.6 Further Analysis,[0],[0]
We present a new context based model for multiturn response selection in retrieval-based chatbots.,6 Conclusion and Future Work,[0],[0]
Experiment results on open data sets show that the model can significantly outperform the stateof-the-art methods.,6 Conclusion and Future Work,[0],[0]
"Besides, we publish the first human-labeled multi-turn response selection data set to research communities.",6 Conclusion and Future Work,[0],[0]
"In the future, we shall study how to model logical consistency of responses and improve candidate retrieval.",6 Conclusion and Future Work,[0],[0]
We appreciate valuable comments provided by anonymous reviewers and our discussions with Zhao Yan.,7 Acknowledgment,[0],[0]
"This work was supported by the National Natural Science Foundation of China (Grand Nos. 61672081, U1636211, 61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001), National High Technology Research and Development Program of China (No.2015AA016004), and the Fund of the State Key Laboratory of Software Development Environment (No.SKLSDE-2015ZX-16).",7 Acknowledgment,[0],[0]
We study response selection for multiturn conversation in retrieval-based chatbots.,abstractText,[0],[0]
"Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among utterances or important contextual information.",abstractText,[0],[0]
We propose a sequential matching network (SMN) to address both problems.,abstractText,[0],[0]
"SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations.",abstractText,[0],[0]
The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among utterances.,abstractText,[0],[0]
The final matching score is calculated with the hidden states of the RNN.,abstractText,[0],[0]
An empirical study on two public data sets shows that SMN can significantly outperform stateof-the-art methods for response selection in multi-turn conversation.,abstractText,[0],[0]
Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2764–2768 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2764",text,[0],[0]
"Over the past decade the state of the art in language modeling has shifted from N-gram models to feed-forward networks (Bengio et al., 2006), and then to recurrent neural networks (RNNs) that read a list of words sequentially and predict the next word at each position.",1 Introduction,[0],[0]
"Starting with standard recurrent networks (Mikolov et al., 2010) the sequential modeling approach was later improved using the long-short-term memory (LSTM) architecture of (Hochreiter and Schmidhuber, 1997) for further gains (Sundermeyer et al., 2012; Medennikov et al., 2016; Xiong et al., 2017).",1 Introduction,[0],[0]
RNN models give two fundamental advantages over the old N-gram framework.,1 Introduction,[0],[0]
"First, the continuous-space embedding of word identities allows word similarities to be exploited for generalization (Bengio et al., 2006; Mikolov et al., 2013).",1 Introduction,[0],[0]
"Second, the recurrent architecture allows, in principle at least,
an unlimited history to condition the prediction of next words.
",1 Introduction,[0],[0]
"The potential advantage of unlimited history, however, is not commonly used to its full benefit, since the language model (LM) is typically “reset” at the start of each utterance in current stateof-the-art recognition systems (Saon et al., 2017; Xiong et al., 2018).",1 Introduction,[0],[0]
"This presumes that each utterance is independent of the others, and clearly violates what we know about how language and conversation works, as discussed in the next section.",1 Introduction,[0],[0]
"Consequently, there have been many proposals to inject information from a longer context into standard LM architectures, going back to Ngram models (Bellegarda, 2004), or to generalize N-grams LMs to operate across utterance boundaries and speakers (Ji and Bilmes, 2004).",1 Introduction,[0],[0]
"Based on the RNN framework, (Mikolov and Zweig, 2012) proposed augmenting network inputs with a more slowly varying context vector that would encode longer-range properties of the history, such as a latent semantic indexing vector.",1 Introduction,[0],[0]
The problem with these approaches is that the modeler has to make design decisions about how to encapsulate contextual information as network inputs.,1 Introduction,[0],[0]
"Therefore, our approach here is to simply provide the entire conversation history as input to a standard LSTM-LM, and let the network learn the information that is relevant to next-word prediction.
",1 Introduction,[0],[0]
"We start by discussing linguistic phenomena that could potentially help in conversational LM (Section 2), followed by a description of the LSTM model we propose to capture them (Section 3).",1 Introduction,[0],[0]
"Section 4 describes the data and recognition system we used to test our models, with results reported in Section 5.",1 Introduction,[0],[0]
We end with conclusions and future directions.,1 Introduction,[0],[0]
Here we review a few of the conversation-level phenomena that could be used for predicting words from longer context.,2 Conversation-level Phenomena,[0],[0]
"Perhaps the most widely studied effect is topical coherence, or the tendency of words that are semantically related to one or more underlying topics to appear together in the conversation.",2 Conversation-level Phenomena,[0],[0]
"Consequently, topic-related words are bound to re-occur across utterances, or certain related words appear to trigger one another (such as “children” and “school”).",2 Conversation-level Phenomena,[0],[0]
"This should be especially true for conversations in the Switchboard (and Fischer) corpora, which were collected by pairing up strangers to talk about a mutually agreeable topic.
",2 Conversation-level Phenomena,[0],[0]
"Another phenomenon that could lead to words reoccurring is lexical entrainment (Brennan and Clark, 1996), or the tendency of conversants to adopt the same words and phrases.",2 Conversation-level Phenomena,[0],[0]
"Entrainment can also apply to speaking style, so the use of common discourse particles, syntactic patterns (like question tags), or even disfluencies could be triggered across speakers.
",2 Conversation-level Phenomena,[0],[0]
"Other phenomena operate more locally, but across speaker turn boundaries.",2 Conversation-level Phenomena,[0],[0]
"Linguistic conversation analysis has long noted that utterance types come in adjacency pairs (Schegloff, 1968), with preferences for certain pairs over others (like a statement is preferentially followed by agreement rather than disagreement).",2 Conversation-level Phenomena,[0],[0]
"Therefore, words in an utterance should be more predicable based on the previous utterance.",2 Conversation-level Phenomena,[0],[0]
"In the past, this has been modeled by conditioning utterance words on an underlying dialog act label, which in turn is conditioned on adjacent dialog act labels via a dialog act grammar (Stolcke et al., 2000).
",2 Conversation-level Phenomena,[0],[0]
"A good part of conversational behavior has to do with how turn-taking is negotiated (Sacks et al., 1974).",2 Conversation-level Phenomena,[0],[0]
"Speakers use special discourse devices, such as backchannel words and pause fillers, to signal when they want to take the floor, or to signal that the other party should keep the floor.",2 Conversation-level Phenomena,[0],[0]
"Conversants also anticipate the ends of turns and jump in before the other speaker is completely done, making for very efficient use of time.",2 Conversation-level Phenomena,[0],[0]
"As a result of all of these mechanisms, a good portion of conversations consists of overlapping (simultaneous) speaking.",2 Conversation-level Phenomena,[0],[0]
"It was shown (Shriberg et al., 2001) that such overlap locations can be partly predicted by word-based language models.",2 Conversation-level Phenomena,[0],[0]
"This suggests reversing the modeling and using overlap (the tim-
ing of utterances) to help predict the words.",2 Conversation-level Phenomena,[0],[0]
"Our baseline language model is a standard LSTM that models utterances independently from one another, i.e., the history at the onset of each utterance is the start-of-sentence token.",3 Models,[0],[0]
"In fact, we used two version of this basic LSTM-LM:
• Word inputs encoded with one-hot vectors, combined with a jointly trained embedding layer
• Words encoded by multiple-hot vectors corresponding to the letter trigrams making up the words.
",3 Models,[0],[0]
Both types of LSTM-LMs use three 1000- dimensional hidden layers with recurrence.,3 Models,[0],[0]
"The word embedding layer is also of size 1000, and the letter-trigram encoding has size 7190 (the number of unique trigrams in our vocabulary).
",3 Models,[0],[0]
"The main addition for session-level modeling is that the LSTM history consists of all the utterances preceding the current utterance, followed by all words in the current utterance preceding the word to be predicted.",3 Models,[0],[0]
"The preceding utterances are serialized in the order of their onset times, so that the flow of words within an utterance is not disrupted.",3 Models,[0],[0]
The resulting total word history and nextword prediction is depicted in Figure 1.,3 Models,[0],[0]
"Information about utterance boundaries is encoded using a boundary tag, similar to the start-of-sentence token that is commonly used in LMs.
",3 Models,[0],[0]
Several of the conversational phenomena described in Section 2 refer to turn-taking between speakers; to capture this in the model we augment the word input encoding with an extra bit that indicates whether a speaker change occurred.,3 Models,[0],[0]
"This bit is turned on only for the start-of-utterance token.
",3 Models,[0],[0]
"We also want to capture some information about utterance overlap, since, as described earlier,
speech overlap interacts with word choice.",3 Models,[0],[0]
"Possible events to model would be overlap (exceedings a time threshold) at the starts and ends of utterances, or maybe a continuous measure of such overlaps.",3 Models,[0],[0]
"As a first proof of concept we chose to encode only one type of overlap, i.e., when the utterance in question is completely overlapped temporally by the other speaker’s turn.",3 Models,[0],[0]
"This is typical of backchannel acknowledgments (“uh-huh”) and short utterances that attempt to grab the floor (“um”, “but”).",3 Models,[0],[0]
Complete utterance overlap is also encoded by an additional input bit that is turned on for the start-of-utterance token.,3 Models,[0],[0]
"We used a single bidirectional LSTM acoustic model in experiments reported here, trained on the commonly used conversational telephone speech corpora (Switchboard, Fisher, CallHome English), estimating frame-level posterior probabilities for 9000 context-dependent phone units.",4.1 Recognition system,[0],[0]
"The system decodes speech utterances using a 4-gram language model, generating lattices.",4.1 Recognition system,[0],[0]
"These are then expanded to 500-best lists, which in turn are rescored using the various LMs.
",4.1 Recognition system,[0],[0]
"The recognition system and the N-gram LM used in decoding have a vocabulary of 165k words, but the LSTM-LMs are trained on only the 38k words occurring at least twice in the indomain conversational training data.",4.1 Recognition system,[0],[0]
Words outside of the LSTM-LM vocabulary are penalized in rescoring with a constant weight that is empirically optimized on the development set.,4.1 Recognition system,[0],[0]
"Language model training uses the Switchboard1, BBN Switchboard-2, Fisher, and English CallHome transcripts (about 23 million words in total) as well as the UW conversational Web corpus (Bulyko et al., 2003) for pre-training (see below).",4.2 Data,[0],[0]
The N-gram LM used for N-best generation also includes the LDC Hub4 (Broadcast News) corpus.,4.2 Data,[0],[0]
The Switchboard-1 and Switchboard-2 portions of the NIST 2002 CTS test set were used for tuning and development.,4.2 Data,[0],[0]
"Evaluation is carried out on the NIST 2000 CTS test set, consisting of Switchboard (SWB) and CallHome (CH) subsets.
",4.2 Data,[0],[0]
"As an expedient, we refrained from resegmenting utterances based on forced alignments of words, and instead use utterance boundaries as
given in the available transcripts (corresponding to the audio segments used in acoustic training).",4.2 Data,[0],[0]
"Similarly, in testing, we use the presegmented utterances provided by NIST.",4.2 Data,[0],[0]
"No doubt there are inconsistencies in how the different corpora define utterance units, and a consistent, alignment-based resegmentation of all training and test data based on the durations nonspeech regions and/or lexical tagging might give improved results.",4.2 Data,[0],[0]
"All LSTM-LMs are trained using the Microsoft Cognitive Toolkit, or CNTK (Yu et al., 2014; Microsoft Research, 2016) on a Linux-based multiGPU server farm.",4.3 Model training,[0],[0]
"Training is parallelized using CNTK’s distributed stochastic gradient descent (SGD) with 1-bit gradient quantization (Seide et al., 2014).",4.3 Model training,[0],[0]
"We use the CNTK “FsAdaGrad” learning algorithm, which is an implementation of Adam (Kingma and Ba, 2015).
",4.3 Model training,[0],[0]
"All LSTM-LMs are pretrained for one or two epochs on a large corpus of “conversational Web” data (Bulyko et al., 2003), followed by normal training to convergence on the in-domain data.",4.3 Model training,[0],[0]
"Each utterance in the Web data is treated as a single session for purposes of session-based LM, i.e., the extra bits for speaker change and overlap are never turned on.",4.3 Model training,[0],[0]
"When evaluating the session-based LMs on speech test data, the true utterance contexts are not known, and we must use hypothesized words for word histories preceding the current utterance.",5 Results,[0],[0]
"In our case, the histories were obtained using the output of our best recognition system, which uses a combination of acoustic models (Xiong et al., 2018), but excluding the session-based LM.1",5 Results,[0],[0]
"Per-
1We also omitted the final confusion network rescoring stage described in (Xiong et al., 2018).
",5 Results,[0],[0]
"plexity was evaluated on reference transcripts, as is customary.
",5 Results,[0],[0]
"Table 1 shows the effect of session-level modeling and of optional model elements on perplexity, based on LSTMs using letter-trigram encoding.",5 Results,[0],[0]
Baseline is the standard utterance-scope LSTMLM.,5 Results,[0],[0]
"We see a large perplexity reduction of 17- 21% by conditioning on session history words, with smaller incremental reductions from adding speaker change and overlap information.
",5 Results,[0],[0]
The last two table rows show that some of the perplexity gain over the baseline is negated by the use of errorful recognition output for the conversation history.,5 Results,[0],[0]
"It does not make much difference whether the recognized word history is generated by just the subsystem being rescored (“single system”, with 6% word error on SWB) or the full recognition system using multiple acoustic models (“full system”, with about 5% word error rate on SWB and 10% on CH).",5 Results,[0],[0]
"Using recognition output as history, the perplexity degrades about 6% relative for SWB, and 11% on CH, relative to using the true word histories.",5 Results,[0],[0]
"Even with the more errorful recognition on CH, the session-based LM still gives a perplexity reduction of 14% relative to the baseline.
",5 Results,[0],[0]
"Table 2 presents recognition results, comparing baseline LSTM-LMs to the full session-based LSTM-LMs.",5 Results,[0],[0]
Both the letter-trigram and one-word word encoding versions are reported.,5 Results,[0],[0]
"The different models may also be used jointly, using loglinear score combination in rescoring, shown in the third section of the table.",5 Results,[0],[0]
"We also tried iterating the session LM rescoring, after the recognized word histories were updated from the first rescoring pass (shown as “2nd iteration” in the table).
",5 Results,[0],[0]
"Results show that the session-based LM yields between 1% and 4% relative word error reduction for the two word encodings, and test sets.",5 Results,[0],[0]
"When the two word encoding types are combined by log-
linear combination of model scores, the gain from session-based modeling is preserved.",5 Results,[0],[0]
"Iterating the session LM rescoring to improve the word histories did not give consistent gains.
",5 Results,[0],[0]
"Even though the session-based LSTM subsumes all the information used in the standard LSTM, there is an additional gain to be had from combining those two model types (last row in the table).",5 Results,[0],[0]
"Thus, the overall gain from adding the session-based models to the two baseline models is 3-5% relative word error reduction.",5 Results,[0],[0]
"We have proposed a simple generalization of utterance-level LSTM language models aimed at capturing conversational phenomena that operate across utterances and speakers, such as lexical entrainment, adjacency pairs, speech overlap, and topical coherence.",6 Conclusion and Future Work,[0],[0]
"To capture non-local conditioning information, the LSTM-LM is trained to read the entire sequence of utterances making up a conversation, along with side information encoding speaker changes and overlap of utterances.",6 Conclusion and Future Work,[0],[0]
"This is found to reduce perplexity by about 25%, most of which is retained when errorful recognition output is used to represent the word history in previous utterances.",6 Conclusion and Future Work,[0],[0]
"The session-based LM yields up to 5% relative reduction in word error when the utterance- and session-based LMs are combined.
",6 Conclusion and Future Work,[0],[0]
It would be worthwhile to investigate which conversational phenomena are actually being exploited by the session LSTM model.,6 Conclusion and Future Work,[0],[0]
"The ease with which additional information can be input to the LSTM-LM also suggests encoding other conditioning information, such a more details about utterance timing, as well as semantic features that capture topical coherence.",6 Conclusion and Future Work,[0],[0]
"We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence.",abstractText,[0],[0]
"The model consists of a long-shortterm memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word.",abstractText,[0],[0]
"The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results.",abstractText,[0],[0]
"In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.",abstractText,[0],[0]
Session-level Language Modeling for Conversational Speech,title,[0],[0]
"Deep learning techniques have been very successful in several domains, like object recognition in images (e.g Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016), machine translation (e.g. Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016;",1 Introduction,[0],[0]
"Gehring et al., 2016) and speech recognition (e.g. Graves et al., 2013; Hannun et al., 2014; Chorowski et al., 2015; Chan et al., 2016; Collobert et al., 2016).",1 Introduction,[0],[0]
Several arguments have been brought forward to justify these empirical results.,1 Introduction,[0],[0]
"From a representational point of view, it has been argued that deep networks can efficiently
1Université of Montréal, Montréal, Canada 2DeepMind, London, United Kingdom 3Google Brain, Mountain View, United States 4CIFAR Senior Fellow.",1 Introduction,[0],[0]
"Correspondence to: Laurent Dinh <laurent.dinh@umontreal.ca>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1 Introduction,[0],[0]
"approximate certain functions (e.g. Montufar et al., 2014; Raghu et al., 2016).",1 Introduction,[0],[0]
"Other works (e.g Dauphin et al., 2014; Sagun et al., 2014; Choromanska et al., 2015) have looked at the structure of the error surface to analyze how trainable these models are.",1 Introduction,[0],[0]
"Finally, another point of discussion is how well these models can generalize (Nesterov & Vial, 2008; Keskar et al., 2017; Zhang et al., 2017).",1 Introduction,[0],[0]
"These correspond, respectively, to low approximation, optimization and estimation error as described by Bottou (2010).
",1 Introduction,[0],[0]
Our work focuses on the analysis of the estimation error.,1 Introduction,[0],[0]
"In particular, different approaches had been used to look at the question of why stochastic gradient descent results in solutions that generalize well (Bottou & LeCun, 2005; Bottou & Bousquet, 2008).",1 Introduction,[0],[0]
"For example, Duchi et al. (2011); Nesterov & Vial (2008); Hardt et al. (2016); Bottou et al. (2016); Gonen & Shalev-Shwartz (2017) rely on the concept of stochastic approximation or uniform stability (Bousquet & Elisseeff, 2002).",1 Introduction,[0],[0]
"Another conjecture that was recently (Keskar et al., 2017) explored, but that could be traced back to Hochreiter & Schmidhuber (1997), relies on the geometry of the loss function around a given solution.",1 Introduction,[0],[0]
"It argues that flat minima, for some definition of flatness, lead to better generalization.",1 Introduction,[0],[0]
"Our work focuses on this particular conjecture, arguing that there are critical issues when applying the concept of flat minima to deep neural networks, which require rethinking what flatness actually means.
",1 Introduction,[0],[0]
"While the concept of flat minima is not well defined, having slightly different meanings in different works, the intuition is relatively simple.",1 Introduction,[0],[0]
"If one imagines the error as a onedimensional curve, a minimum is flat if there is a wide region around it with roughly the same error, otherwise the minimum is sharp.",1 Introduction,[0],[0]
"When moving to higher dimensional spaces, defining flatness becomes more complicated.",1 Introduction,[0],[0]
In Hochreiter & Schmidhuber (1997) it is defined as the size of the connected region around the minimum where the training loss is relatively similar.,1 Introduction,[0],[0]
"Chaudhari et al. (2017) relies, in contrast, on the curvature of the second order structure around the minimum, while Keskar et al. (2017) looks at the maximum loss in a bounded neighbourhood of the minimum.",1 Introduction,[0],[0]
"All these works rely on the fact that flatness results in robustness to low precision arithmetic or noise in the parameter space, which, using an minimum description length-based argument, suggests a low expected overfitting.
",1 Introduction,[0],[0]
"However, several common architectures and parametrizations in deep learning are already at odds with this conjecture, requiring at least some degree of refinement in the statements made.",1 Introduction,[0],[0]
"In particular, we show how the geometry of the associated parameter space can alter the ranking between prediction functions when considering several measures of flatness/sharpness.",1 Introduction,[0],[0]
"We believe the reason for this contradiction stems from the Bayesian arguments about KLdivergence made to justify the generalization ability of flat minima (Hinton & Van Camp, 1993).",1 Introduction,[0],[0]
"Indeed, KullbackLiebler divergence is invariant to change of parameters whereas the notion of ”flatness” is not.",1 Introduction,[0],[0]
"The demonstrations of Hochreiter & Schmidhuber (1997) are approximately based on a Gibbs formalism and rely on strong assumptions and approximations that can compromise the applicability of the argument, including the assumption of a discrete function space.",1 Introduction,[0],[0]
"For conciseness, we will restrict ourselves to supervised scalar output problems, but several conclusions in this paper can apply to other problems as well.",2 Definitions of flatness/sharpness,[0],[0]
We will consider a function f that takes as input an element x from an input space X and outputs a scalar y.,2 Definitions of flatness/sharpness,[0],[0]
We will denote by fθ the prediction function.,2 Definitions of flatness/sharpness,[0],[0]
"This prediction function will be parametrized by a parameter vector θ in a parameter space Θ. Often, this prediction function will be over-parametrized and two parameters (θ, θ′) ∈ Θ2 that yield the same prediction function everywhere, ∀x ∈ X , fθ(x) = fθ′(x), are called observationally equivalent.",2 Definitions of flatness/sharpness,[0],[0]
The model is trained to minimize a continuous loss function L which takes as argument the prediction function fθ.,2 Definitions of flatness/sharpness,[0],[0]
"We will often think of the loss L as a function of θ and adopt the notation L(θ).
",2 Definitions of flatness/sharpness,[0],[0]
"The notion of flatness/sharpness of a minimum is relative, therefore we will discuss metrics that can be used to compare the relative flatness between two minima.",2 Definitions of flatness/sharpness,[0],[0]
"In this section we will formalize three used definitions of flatness in
the literature.
",2 Definitions of flatness/sharpness,[0],[0]
Hochreiter & Schmidhuber (1997) defines a flat minimum as ”a large connected region in weight space where the error remains approximately constant”.,2 Definitions of flatness/sharpness,[0],[0]
"We interpret this formulation as follows:
Definition 1.",2 Definitions of flatness/sharpness,[0],[0]
"Given > 0, a minimum θ, and a loss L, we define C(L, θ, ) as the largest (using inclusion as the partial order over the subsets of Θ) connected set containing θ such that ∀θ′ ∈ C(L, θ, ), L(θ′) < L(θ) + .",2 Definitions of flatness/sharpness,[0],[0]
"The - flatness will be defined as the volume of C(L, θ, ).",2 Definitions of flatness/sharpness,[0],[0]
"We will call this measure the volume -flatness.
",2 Definitions of flatness/sharpness,[0],[0]
"In Figure 1, C(L, θ, ) will be the purple line at the top of the red area if the height is and its volume will simply be the length of the purple line.
",2 Definitions of flatness/sharpness,[0],[0]
Flatness can also be defined using the local curvature of the loss function around the minimum if it is a critical point 1.,2 Definitions of flatness/sharpness,[0],[0]
Chaudhari et al. (2017); Keskar et al. (2017) suggest that this information is encoded in the eigenvalues of the Hessian.,2 Definitions of flatness/sharpness,[0],[0]
"However, in order to compare how flat one minimum versus another, the eigenvalues need to be reduced to a single number.",2 Definitions of flatness/sharpness,[0],[0]
"Here we consider the spectral norm and trace of the Hessian, two typical measurements of the eigenvalues of a matrix.
",2 Definitions of flatness/sharpness,[0],[0]
Additionally Keskar et al. (2017) defines the notion of - sharpness.,2 Definitions of flatness/sharpness,[0],[0]
"In order to make proofs more readable, we will slightly modify their definition.",2 Definitions of flatness/sharpness,[0],[0]
"However, because of norm equivalence in finite dimensional space, our results will transfer to the original definition in full space as well.",2 Definitions of flatness/sharpness,[0],[0]
"Our modified definition is the following:
Definition 2.",2 Definitions of flatness/sharpness,[0],[0]
"Let B2( , θ) be an Euclidean ball centered on a minimum θ with radius .",2 Definitions of flatness/sharpness,[0],[0]
"Then, for a non-negative valued loss function L, the -sharpness will be defined as proportional to
maxθ′∈B2( ,θ) ( L(θ′)− L(θ) ) 1 + L(θ) .
",2 Definitions of flatness/sharpness,[0],[0]
"In Figure 1, if the width of the red area is 2 then the height of the red area is maxθ′∈B2( ,θ) ( L(θ′)− L(θ) ) .
",2 Definitions of flatness/sharpness,[0],[0]
-sharpness can be related to the spectral norm of the Hessian.,2 Definitions of flatness/sharpness,[0],[0]
"Indeed, a second-order Taylor expansion of L around a critical point minimum is written
L(θ′) = L(θ) + 1
2 (θ′ − θ) (∇2L)(θ)(θ′ − θ)T
+ o(‖θ′",2 Definitions of flatness/sharpness,[0],[0]
"− θ‖22).
",2 Definitions of flatness/sharpness,[0],[0]
"In this second order approximation, the -sharpness at θ
1In this paper, we will often assume that is the case when dealing with Hessian-based measures in order to have them welldefined.
would be ∣∣∣∣∣∣(∇2L)(θ)∣∣∣∣∣∣ 2 2
2 ( 1 + L(θ) ) .",2 Definitions of flatness/sharpness,[0],[0]
"Before moving forward to our results, in this section we first introduce the notation used in the rest of paper.",3 Properties of Deep Rectified Networks,[0],[0]
"Most of our results, for clarity, will be on the deep rectified feedforward networks with a linear output layer that we describe below, though they can easily be extended to other architectures (e.g. convolutional, etc.).",3 Properties of Deep Rectified Networks,[0],[0]
Definition 3.,3 Properties of Deep Rectified Networks,[0],[0]
"GivenK weight matrices (θk)k≤K with nk = dim ( vec(θk) ) and n = ∑K k=1 nk, the output y of a deep rectified feedforward networks with a linear output layer is:
y = φrect ( φrect ( · · ·φrect(x · θ1) · · · ) · θK−1 ) · θK ,
where
• x is the input to the model, a high-dimensional vector
• φrect is the rectified elementwise activation function (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011), which is the positive part (zi)i 7→ (max(zi, 0))i.
",3 Properties of Deep Rectified Networks,[0],[0]
"• vec reshapes a matrix into a vector.
",3 Properties of Deep Rectified Networks,[0],[0]
"Note that in our definition we excluded the bias terms, usually found in any neural architecture.",3 Properties of Deep Rectified Networks,[0],[0]
"This is done mainly for convenience, to simplify the rendition of our arguments.",3 Properties of Deep Rectified Networks,[0],[0]
"However, the arguments can be extended to the case that includes biases (see Appendix) .",3 Properties of Deep Rectified Networks,[0],[0]
Another choice is that of the linear output layer.,3 Properties of Deep Rectified Networks,[0],[0]
"Having an output activation function does not affect our argument either: since the loss is a function of the output activation, it can be rephrased as a function of linear pre-activation.
",3 Properties of Deep Rectified Networks,[0],[0]
"Deep rectifier models have certain properties that allows us in section 4 to arbitrary manipulate the flatness of a minimum.
",3 Properties of Deep Rectified Networks,[0],[0]
"An important topic for optimization of neural networks is understanding the non-Euclidean geometry of the parameter space as imposed by the neural architecture (see, for example Amari, 1998).",3 Properties of Deep Rectified Networks,[0],[0]
"In principle, when we take a step in parameter space what we expect to control is the change in the behavior of the model (i.e. the mapping of the input x to the output y).",3 Properties of Deep Rectified Networks,[0],[0]
"In principle we are not interested in the parameters per se, but rather only in the mapping they represent.
",3 Properties of Deep Rectified Networks,[0],[0]
"If one defines a measure for the change in the behavior of the model, which can be done under some assumptions, then, it can be used to define, at any point in the parameter space, a metric that says what is the equivalent change in the parameters for a unit of change in the behavior of the model.",3 Properties of Deep Rectified Networks,[0],[0]
"As it turns out, for neural networks, this metric is not constant over Θ. Intuitively, the metric is related to the curvature, and since neural networks can be highly nonlinear, the curvature will not be constant.",3 Properties of Deep Rectified Networks,[0],[0]
See Amari (1998); Pascanu & Bengio (2014) for more details.,3 Properties of Deep Rectified Networks,[0],[0]
"Coming back to the concept of flatness or sharpness of a minimum, this metric should define the flatness.
",3 Properties of Deep Rectified Networks,[0],[0]
"However, the geometry of the parameter space is more complicated.",3 Properties of Deep Rectified Networks,[0],[0]
"Regardless of the measure chosen to compare two instantiations of a neural network, because of the structure of the model, it also exhibits a large number of symmetric configurations that result in exactly the same behavior.",3 Properties of Deep Rectified Networks,[0],[0]
"Because the rectifier activation has the non-negative homogeneity property, as we will see shortly, one can construct a continuum of points that lead to the same behavior, hence the metric is singular.",3 Properties of Deep Rectified Networks,[0],[0]
"Which means that one can exploit these directions in which the model stays unchanged to shape the neighbourhood around a minimum in such a way that, by most definitions of flatness, this property can be controlled.",3 Properties of Deep Rectified Networks,[0],[0]
"See Figure 2 for a visual depiction, where the flatness (given here as the distance between the different level curves) can be changed by moving along the curve.
",3 Properties of Deep Rectified Networks,[0],[0]
"Let us redefine, for convenience, the non-negative homogeneity property (Neyshabur et al., 2015; Lafond et al., 2016) below.",3 Properties of Deep Rectified Networks,[0],[0]
"Note that beside this property, the reason for studying the rectified linear activation is for its widespread adoption (Krizhevsky et al., 2012; Simonyan & Zisserman, 2015; Szegedy et al., 2015; He et al., 2016).
",3 Properties of Deep Rectified Networks,[0],[0]
Definition 4.,3 Properties of Deep Rectified Networks,[0],[0]
"A given a function φ is non-negative homogeneous if
∀(z, α) ∈",3 Properties of Deep Rectified Networks,[0],[0]
"R× R+, φ(αz) = αφ(z)
.
",3 Properties of Deep Rectified Networks,[0],[0]
Theorem 1.,3 Properties of Deep Rectified Networks,[0],[0]
"The rectified function φrect(x) = max(x, 0) is non-negative homogeneous.
",3 Properties of Deep Rectified Networks,[0],[0]
Proof.,3 Properties of Deep Rectified Networks,[0],[0]
"Follows trivially from the constraint that α > 0, given that x >",3 Properties of Deep Rectified Networks,[0],[0]
"0⇒ αx > 0, iff α > 0.
",3 Properties of Deep Rectified Networks,[0],[0]
"For a deep rectified neural network it means that:
φrect ( x · (αθ1) ) · θ2 = φrect(x · θ1) · (αθ2),
meaning that for this one (hidden) layer neural network, the parameters (αθ1, θ2) is observationally equivalent to (θ1, αθ2).",3 Properties of Deep Rectified Networks,[0],[0]
"This observational equivalence similarly holds for convolutional layers.
",3 Properties of Deep Rectified Networks,[0],[0]
"Given this non-negative homogeneity, if (θ1, θ2) 6= (0, 0) then { (αθ1, α −1θ2), α > 0 }
is an infinite set of observationally equivalent parameters, inducing a strong nonidentifiability in this learning scenario.",3 Properties of Deep Rectified Networks,[0],[0]
"Other models like deep linear networks (Saxe et al., 2013), leaky rectifiers (He et al., 2015) or maxout networks (Goodfellow et al., 2013) also have this non-negative homogeneity property.
",3 Properties of Deep Rectified Networks,[0],[0]
"In what follows we will rely on such transformations, in particular we will rely on the following definition:
Definition 5.",3 Properties of Deep Rectified Networks,[0],[0]
"For a single hidden layer rectifier feedforward network we define the family of transformations
Tα : (θ1, θ2) 7→ (αθ1, α−1θ2)
which we refer to as a α-scale transformation.
",3 Properties of Deep Rectified Networks,[0],[0]
"Note that a α-scale transformation will not affect the generalization, as the behavior of the function is identical.",3 Properties of Deep Rectified Networks,[0],[0]
"Also while the transformation is only defined for a single layer rectified feedforward network, it can trivially be extended to any architecture having a single rectified network as a submodule, e.g. a deep rectified feedforward network.",3 Properties of Deep Rectified Networks,[0],[0]
For simplicity and readability we will rely on this definition.,3 Properties of Deep Rectified Networks,[0],[0]
In this section we exploit the resulting strong nonidentifiability to showcase a few shortcomings of some definitions of flatness.,4 Deep Rectified networks and flat minima,[0],[0]
"Although α-scale transformation does not affect the function represented, it allows us to significantly decrease several measures of flatness.",4 Deep Rectified networks and flat minima,[0],[0]
"For another definition of flatness, α-scale transformation show that all minima are equally flat.",4 Deep Rectified networks and flat minima,[0],[0]
Theorem 2.,4.1 Volume -flatness,[0],[0]
"For a one-hidden layer rectified neural network of the form
y = φrect(x · θ1) ·",4.1 Volume -flatness,[0],[0]
"θ2,
and a minimum θ = (θ1, θ2), such that θ1 6= 0 and θ2 6= 0, ∀ > 0",4.1 Volume -flatness,[0],[0]
"C(L, θ, ) has an infinite volume.
",4.1 Volume -flatness,[0],[0]
"We will not consider the solution θ where any of the weight matrices θ1, θ2 is zero, θ1 = 0 or θ2 = 0, as it results in a constant function which we will assume to give poor training performance.",4.1 Volume -flatness,[0],[0]
"For α > 0, the α-scale transformation Tα : (θ1, θ2) 7→ (αθ1, α−1θ2) has Jacobian determinant αn1−n2 , where once again n1 = dim ( vec(θ1) ) and n2 =
dim ( vec(θ2) ) .",4.1 Volume -flatness,[0],[0]
Note that the Jacobian determinant of this linear transformation is the change in the volume induced by Tα and Tα ◦ Tβ = Tαβ .,4.1 Volume -flatness,[0],[0]
"We show below that there is a connected region containing θ with infinite volume and where the error remains approximately constant.
",4.1 Volume -flatness,[0],[0]
Proof.,4.1 Volume -flatness,[0],[0]
We will first introduce a small region with approximately constant error around θ with non-zero volume.,4.1 Volume -flatness,[0],[0]
Given > 0,4.1 Volume -flatness,[0],[0]
"and if we consider the loss function continuous with respect to the parameter, C(L, θ, ) is an open set containing θ.",4.1 Volume -flatness,[0],[0]
"Since we also have θ1 6= 0 and θ2 6= 0, let",4.1 Volume -flatness,[0],[0]
r > 0,4.1 Volume -flatness,[0],[0]
"such that the L∞ ball B∞(r, θ) is in C(L, θ, ) and has empty intersection with {θ′, θ′1 = 0}.",4.1 Volume -flatness,[0],[0]
Let v = (2r)n1+n2 > 0,4.1 Volume -flatness,[0],[0]
"the volume of B∞(r, θ).
",4.1 Volume -flatness,[0],[0]
"Since the Jacobian determinant of Tα is the multiplicative change of induced by Tα, the volume of Tα ( B∞(r, θ) ) is vαn1−n2 .",4.1 Volume -flatness,[0],[0]
"If n1 6= n2, we can arbitrarily grow the volume of Tα ( B∞(r, θ) ) , with error within an -interval of L(θ), by having α tends to +∞ if n1 > n2 or to 0 otherwise.
",4.1 Volume -flatness,[0],[0]
"If n1 = n2, ∀α′ > 0, Tα′ ( B∞(r, θ) ) has volume v. Let
C ′ =",4.1 Volume -flatness,[0],[0]
"⋃ α′>0 Tα′ ( B∞(r, θ) ) .",4.1 Volume -flatness,[0],[0]
"C ′ is a connected region where the error remains approximately constant, i.e. within an -interval of L(θ).
",4.1 Volume -flatness,[0],[0]
Let α = 2 ‖θ1‖∞+r‖θ1‖∞−r .,4.1 Volume -flatness,[0],[0]
"Since
B∞(r, θ) = B∞(r, θ1)×B∞(r, θ2),
where × is the Cartesian set product, we have
Tα ( B∞(r, θ) )",4.1 Volume -flatness,[0],[0]
"= B∞(αr, αθ1)×B∞(α−1r, α−1θ2).
",4.1 Volume -flatness,[0],[0]
"Therefore, Tα ( B∞(r, θ) ) ∩B∞(r, θ) = ∅ (see Figure 3).
",4.1 Volume -flatness,[0],[0]
"Similarly, B∞(r, θ), Tα ( B∞(r, θ) ) , T 2α ( B∞(r, θ) ) , . . .
are disjoint and have volume v.",4.1 Volume -flatness,[0],[0]
"We have also T kα ( B∞(r ′, θ) )",4.1 Volume -flatness,[0],[0]
"= Tαk ( B∞(r ′, θ) ) ∈ C ′. The volume of C ′ is then lower bounded by 0 <",4.1 Volume -flatness,[0],[0]
v + v + v + · · · and is therefore infinite.,4.1 Volume -flatness,[0],[0]
"C(L, θ, ) has then infinite volume too, making the volume -flatness of θ infinite.
",4.1 Volume -flatness,[0],[0]
This theorem can generalize to rectified neural networks in general with a similar proof.,4.1 Volume -flatness,[0],[0]
"Given that every minimum has an infinitely large region (volume-wise) in which the error remains approximately constant, that means that every minimum would be infinitely flat according to the volume -flatness.",4.1 Volume -flatness,[0],[0]
"Since all minima are equally flat, it is not possible to use volume -flatness to gauge the generalization property of a minimum.",4.1 Volume -flatness,[0],[0]
"The non-Euclidean geometry of the parameter space, coupled with the manifolds of observationally equal behavior of the model, allows one to move from one region of the parameter space to another, changing the curvature of the model without actually changing the function.",4.2 Hessian-based measures,[0],[0]
"This approach has been used with success to improve optimization, by moving from a region of high curvature to a region of well behaved
curvature (e.g. Desjardins et al., 2015; Salimans & Kingma, 2016).",4.2 Hessian-based measures,[0],[0]
"In this section we look at two widely used measures of the Hessian, the spectral radius and trace, showing that either of these values can be manipulated without actually changing the behavior of the function.",4.2 Hessian-based measures,[0],[0]
"If the flatness of a minimum is defined by any of these quantities, then it could also be easily manipulated.
",4.2 Hessian-based measures,[0],[0]
Theorem 3.,4.2 Hessian-based measures,[0],[0]
"The gradient and Hessian of the loss L with respect to θ can be modified by Tα.
",4.2 Hessian-based measures,[0],[0]
"Proof.
",4.2 Hessian-based measures,[0],[0]
"L(θ1, θ2) = L(αθ1, α −1θ2),
we have then by differentiation (∇L)(θ1, θ2) =",4.2 Hessian-based measures,[0],[0]
"(∇L)(αθ1, α−1θ2) [ αIn1 0
0 α−1In2 ] ⇔ (∇L)(αθ1, α−1θ2) =",4.2 Hessian-based measures,[0],[0]
"(∇L)(θ1, θ2) [ α−1In1 0
0 αIn2 ] and
(∇2L)(αθ1, α−1θ2)
=
[ α−1In1 0
0 αIn2
] (∇2L)(θ1, θ2) [ α−1In1 0
0 αIn2
] .
",4.2 Hessian-based measures,[0],[0]
"Sharpest direction Through these transformations we can easily find, for any critical point which is a minimum with non-zero Hessian, an observationally equivalent parameter whose Hessian has an arbitrarily large spectral norm.
",4.2 Hessian-based measures,[0],[0]
Theorem 4.,4.2 Hessian-based measures,[0],[0]
"For a one-hidden layer rectified neural network of the form
y = φrect(x · θ1) ·",4.2 Hessian-based measures,[0],[0]
"θ2,
and critical point θ = (θ1, θ2) being a minimum for L, such that (∇2L)(θ) 6= 0, ∀M > 0,∃α > 0, ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 ≥ M where ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 is
the spectral norm of (∇2L) ( Tα(θ) ) .
",4.2 Hessian-based measures,[0],[0]
Proof.,4.2 Hessian-based measures,[0],[0]
"The trace of a symmetric matrix is the sum of its eigenvalues and a real symmetric matrix can be diagonalized in R, therefore if the Hessian is non-zero, there is one nonzero positive diagonal element.",4.2 Hessian-based measures,[0],[0]
"Without loss of generality, we will assume that this non-zero element of value γ > 0 corresponds to an element in θ1.",4.2 Hessian-based measures,[0],[0]
"Therefore the Frobenius norm
∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣F of (∇2L)(αθ1, α−1θ2)
=
[ α−1In1 0
0 αIn2
] (∇2L)(θ1, θ2) [ α−1In1 0
0 αIn2
] .
is lower bounded by α−2γ.
",4.2 Hessian-based measures,[0],[0]
"Since all norms are equivalent in finite dimension, there exists a constant r > 0",4.2 Hessian-based measures,[0],[0]
such that r|||A|||F ≤,4.2 Hessian-based measures,[0],[0]
|||A|||2 for all symmetric matrices A.,4.2 Hessian-based measures,[0],[0]
So by picking α <,4.2 Hessian-based measures,[0],[0]
"√ rγ M , we are
guaranteed that ∣∣∣∣∣∣(∇2L)(Tα(θ))∣∣∣∣∣∣2 ≥M .
",4.2 Hessian-based measures,[0],[0]
Any minimum with non-zero Hessian will be observationally equivalent to a minimum whose Hessian has an arbitrarily large spectral norm.,4.2 Hessian-based measures,[0],[0]
"Therefore for any minimum in the loss function, if there exists another minimum that generalizes better then there exists another minimum that generalizes better and is also sharper according the spectral norm of the Hessian.",4.2 Hessian-based measures,[0],[0]
The spectral norm of critical points’ Hessian becomes as a result less relevant as a measure of potential generalization error.,4.2 Hessian-based measures,[0],[0]
"Moreover, since the spectral norm lower bounds the trace for a positive semi-definite symmetric matrix, the same conclusion can be drawn for the trace.
",4.2 Hessian-based measures,[0],[0]
Further properties of the Hessian are analyzed in Appendix.,4.2 Hessian-based measures,[0],[0]
We have redefined for > 0,4.3 -sharpness,[0],[0]
"the -sharpness of Keskar et al. (2017) as follow
maxθ′∈B2( ,θ) ( L(θ′)− L(θ) )",4.3 -sharpness,[0],[0]
"1 + L(θ)
where B2( , θ) is the Euclidean ball of radius centered on θ.",4.3 -sharpness,[0],[0]
This modification will demonstrate more clearly the issues of that metric as a measure of probable generalization.,4.3 -sharpness,[0],[0]
"If we use K = 2 and (θ1, θ2) corresponding to a non-constant function, i.e. θ1 6= 0 and θ2 6= 0, then we can define α = ‖θ1‖2 .",4.3 -sharpness,[0],[0]
"We will now consider the observationally equivalent parameter Tα(θ1, θ2) =",4.3 -sharpness,[0],[0]
"( θ1‖θ1‖2 , α −1θ2).
Given that ‖θ1‖2 ≤",4.3 -sharpness,[0],[0]
"‖θ‖2, we have that (0, α−1θ2) ∈ B2 ( , Tα(θ) ) , making the maximum loss in this neighborhood at least as high as the best constant-valued function,
incurring relatively high sharpness.",4.3 -sharpness,[0],[0]
"Figure 4 provides a visualization of the proof.
",4.3 -sharpness,[0],[0]
For rectified neural network every minimum is observationally equivalent to a minimum that generalizes as well but with high -sharpness.,4.3 -sharpness,[0],[0]
This also applies when using the full-space -sharpness used by Keskar et al. (2017).,4.3 -sharpness,[0],[0]
"We can prove this similarly using the equivalence of norms in finite dimensional vector spaces and the fact that for c > 0, > 0, ≤ (c + 1) (see Keskar et al. (2017)).",4.3 -sharpness,[0],[0]
"We have not been able to show a similar problem with random subspace -sharpness used by Keskar et al. (2017), i.e. a restriction of the maximization to a random subspace, which could relate to the notion of wide valleys described by Chaudhari et al. (2017).
",4.3 -sharpness,[0],[0]
"By exploiting the non-Euclidean geometry and nonidentifiability of rectified neural networks, we were able to demonstrate some of the limits of using typical definitions of minimum’s flatness as core explanation for generalization.",4.3 -sharpness,[0],[0]
"In the previous section 4 we explored the case of a fixed parametrization, that of deep rectifier models.",5 Allowing reparametrizations,[0],[0]
In this section we demonstrate a simple observation.,5 Allowing reparametrizations,[0],[0]
"If we are allowed to change the parametrization of some function f , we can obtain arbitrarily different geometries without affecting how the function evaluates on unseen data.",5 Allowing reparametrizations,[0],[0]
The same holds for reparametrization of the input space.,5 Allowing reparametrizations,[0],[0]
The implication is that the correlation between the geometry of the parameter space (and hence the error surface) and the behavior of a given function is meaningless if not preconditioned on the specific parametrization of the model.,5 Allowing reparametrizations,[0],[0]
One thing that needs to be considered when relating flatness of minima to their probable generalization is that the choice of parametrization and its associated geometry are arbitrary.,5.1 Model reparametrization,[0],[0]
"Since we are interested in finding a prediction function in a given family of functions, no reparametrization of this family should influence generalization of any of these functions.",5.1 Model reparametrization,[0],[0]
"Given a bijection g onto θ, we can define new transformed parameter η = g−1(θ).",5.1 Model reparametrization,[0],[0]
"Since θ and η represent in different space the same prediction function, they should generalize as well.
",5.1 Model reparametrization,[0],[0]
Let’s call Lη = L ◦ g the loss function with respect to the new parameter η.,5.1 Model reparametrization,[0],[0]
"We generalize the derivation of Subsec-
tion 4.2:
Lη(η) = L ( g(η) ) ⇒",5.1 Model reparametrization,[0],[0]
"(∇Lη)(η) = (∇L) ( g(η) ) (∇g)(η)
⇒ (∇2Lη)(η) =",5.1 Model reparametrization,[0],[0]
"(∇g)(η)T (∇2L) ( g(η) ) (∇g)(η)
+ (∇L) ( g(η) ) (∇2g)(η).
",5.1 Model reparametrization,[0],[0]
"At a differentiable critical point, we have by definition (∇L) ( g(η) )",5.1 Model reparametrization,[0],[0]
"= 0, therefore the transformed Hessian at a
critical point becomes
(∇2Lη)(η) =",5.1 Model reparametrization,[0],[0]
"(∇g)(η)T (∇2L) ( g(η) ) (∇g)(η).
",5.1 Model reparametrization,[0],[0]
This means that by reparametrizing the problem we can modify to a large extent the geometry of the loss function so as to have sharp minima of L in θ correspond to flat minima ofLη in η = g−1(θ) and conversely.,5.1 Model reparametrization,[0],[0]
Figure 5 illustrates that point in one dimension.,5.1 Model reparametrization,[0],[0]
"Several practical (Dinh et al., 2014; Rezende & Mohamed, 2015; Kingma et al., 2016; Dinh et al., 2016) and theoretical works (Hyvärinen & Pajunen, 1999) show how powerful bijections can be.",5.1 Model reparametrization,[0],[0]
"We can also note that the formula for the transformed Hessian at a critical point also applies if g is not invertible, g would just need to be surjective over Θ in order to cover exactly the same family of prediction functions
{fθ, θ ∈ Θ} = {fg(η), η ∈ g−1(Θ)}.
",5.1 Model reparametrization,[0],[0]
"We show in Appendix, bijections that allow us to perturb the relative flatness between a finite number of minima.
Instances of commonly used reparametrization are batch normalization (Ioffe & Szegedy, 2015), or the virtual batch normalization variant (Salimans et al., 2016), and weight normalization (Badrinarayanan et al., 2015; Salimans & Kingma, 2016; Arpit et al., 2016).",5.1 Model reparametrization,[0],[0]
Im et al. (2016) have plotted how the loss function landscape was affected by batch normalization.,5.1 Model reparametrization,[0],[0]
"However, we will focus on weight normalization reparametrization as the analysis will be simpler, but the intuition with batch normalization will be similar.",5.1 Model reparametrization,[0],[0]
"Weight normalization reparametrizes a nonzero weight w as w = s v‖v‖2 with the new parameter being the scale s and the unnormalized weight v 6= 0.
",5.1 Model reparametrization,[0],[0]
"Since we can observe that w is invariant to scaling of v, reasoning similar to Section 3 can be applied with the simpler transformations T ′α : v 7→ αv for α 6= 0.",5.1 Model reparametrization,[0],[0]
"Moreover, since this transformation is a simpler isotropic scaling, the conclusion that we can draw can be actually more powerful with respect to v:
• every minimum has infinite volume -sharpness; • every minimum is observationally equivalent to an
infinitely sharp minimum and to an infinitely flat minimum when considering nonzero eigenvalues of the Hessian;
• every minimum is observationally equivalent to a minimum with arbitrarily low full-space and random subspace -sharpness and a minimum with high full-space -sharpness.
",5.1 Model reparametrization,[0],[0]
This further weakens the link between the flatness of a minimum and the generalization property of the associated prediction function when a specific parameter space has not been specified and explained beforehand.,5.1 Model reparametrization,[0],[0]
"As we conclude that the notion of flatness for a minimum in the loss function by itself is not sufficient to determine its generalization ability in the general case, we can choose to focus instead on properties of the prediction function instead.",5.2 Input representation,[0],[0]
"Motivated by some work in adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) for deep neural networks, one could decide on its generalization property by analyzing the gradient of the prediction function on examples.",5.2 Input representation,[0],[0]
"Intuitively, if the gradient is small on typical points from the distribution or has a small Lipschitz constant, then a small change in the input should not incur a large change in the prediction.
",5.2 Input representation,[0],[0]
But this infinitesimal reasoning is once again very dependent of the local geometry of the input space.,5.2 Input representation,[0],[0]
"For an invertible preprocessing ξ−1, e.g. feature standardization, whitening or gaussianization (Chen & Gopinath, 2001), we will call fξ = f ◦ ξ",5.2 Input representation,[0],[0]
the prediction function on the preprocessed input u = ξ−1(x).,5.2 Input representation,[0],[0]
"We can reproduce the derivation in Section 5 to obtain
∂fξ ∂uT
( ξ(u) )",5.2 Input representation,[0],[0]
= ∂f ∂xT ( ξ(u) ),5.2 Input representation,[0],[0]
"∂ξ ∂uT (u).
",5.2 Input representation,[0],[0]
"As we can alter significantly the relative magnitude of the gradient at each point, analyzing the amplitude of the gradient of the prediction function might prove problematic if the choice of the input space have not been explained beforehand.",5.2 Input representation,[0],[0]
"This remark applies in applications involving images, sound or other signals with invariances (Larsen et al., 2015).",5.2 Input representation,[0],[0]
"For example, Theis et al. (2016) show for images how a small drift of one to four pixels can incur a large difference in terms of L2 norm.",5.2 Input representation,[0],[0]
"It has been observed empirically that minima found by standard deep learning algorithms that generalize well tend to be flatter than found minima that did not generalize well (Chaudhari et al., 2017; Keskar et al., 2017).",6 Discussion,[0],[0]
"However, when following several definitions of flatness, we have shown that the conclusion that flat minima should generalize better than sharp ones cannot be applied as is without further context.",6 Discussion,[0],[0]
Previously used definitions fail to account for the complex geometry of some commonly used deep architectures.,6 Discussion,[0],[0]
"In particular, the non-identifiability of the model induced by symmetries, allows one to alter the flatness of a minimum without affecting the function it represents.",6 Discussion,[0],[0]
Additionally the whole geometry of the error surface with respect to the parameters can be changed arbitrarily under different parametrizations.,6 Discussion,[0],[0]
"In the spirit of (Swirszcz et al., 2016), our work indicates that more care is needed to define flatness to avoid degeneracies of the geometry of the model under study.",6 Discussion,[0],[0]
"Also such a concept can not be divorced from the
particular parametrization of the model or input space.",6 Discussion,[0],[0]
"The authors would like to thank Grzegorz Świrszcz for an insightful discussion on the paper, Harm De Vries, Yann Dauphin, Jascha Sohl-Dickstein and César Laurent for useful discussions about optimization, Danilo Rezende for explaining universal approximation using normalizing flows and Kyle Kastner, Adriana Romero, Junyoung Chung, Nicolas Ballas, Aaron Courville, George Dahl, Yaroslav Ganin, Prajit Ramachandran, Çağlar Gülçehre, Ahmed Touati and the ICML reviewers for useful feedback.",Acknowledgements,[0],[0]
"Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice.",abstractText,[0],[0]
"However, explaining why this is the case is still an open area of research.",abstractText,[0],[0]
"One standing hypothesis that is gaining popularity, e.g. Hochreiter & Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization.",abstractText,[0],[0]
This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization.,abstractText,[0],[0]
"Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima.",abstractText,[0],[0]
"Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.",abstractText,[0],[0]
Sharp Minima Can Generalize For Deep Nets,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2820–2825 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output. Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep. This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost. By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT EnglishFrench NewsTest2014, while decoding at 100 words/sec on single-threaded CPU. We believe this is the best published accuracy/speed trade-off of an NMT system.",text,[0],[0]
"Attentional sequence-to-sequence models have become the new standard for machine translation over the last two years, and with the unprecedented improvements in translation accuracy
comes a new set of technical challenges.",1 Introduction,[0],[0]
"One of the biggest challenges is the high training and decoding costs of these neural machine translation (NMT) system, which is often at least an order of magnitude higher than a phrase-based system trained on the same data.",1 Introduction,[0],[0]
"For instance, phrasal MT systems were able achieve single-threaded decoding speeds of 100-500 words/sec on decade-old CPUs (Quirk and Moore, 2007), while Jean et al. (2015) reported single-threaded decoding speeds of 8-10 words/sec on a shallow NMT system.",1 Introduction,[0],[0]
"Wu et al. (2016) was able to reach CPU decoding speeds of 100 words/sec for a deep model, but used 44 CPU cores to do so.",1 Introduction,[0],[0]
"There has been recent work in speeding up decoding by reducing the search space (Kim and Rush, 2016), but little in computational improvements.
",1 Introduction,[0],[0]
"In this work, we consider a production scenario which requires low-latency, high-throughput NMT decoding.",1 Introduction,[0],[0]
"We focus on CPU-based decoders, since GPU/FPGA/ASIC-based decoders require specialized hardware deployment and logistical constraints such as batch processing.",1 Introduction,[0],[0]
Efficient CPU decoders can also be used for ondevice mobile translation.,1 Introduction,[0],[0]
"We focus on singlethreaded decoding and single-sentence processing, since multiple threads can be used to reduce latency but not total throughput.
",1 Introduction,[0],[0]
"We approach this problem from two angles: In Section 4, we describe a number of techniques for improving the speed of the decoder, and obtain a 4.4x speedup over a highly efficient baseline.",1 Introduction,[0],[0]
"These speedups do not affect decoding results, so they can be applied universally.",1 Introduction,[0],[0]
"In Section 5, we describe a simple but powerful network architecture which uses a single RNN (GRU/LSTM) layer at the bottom with a large number of fully-connected (FC) layers on top, and obtains improvements similar to a deep RNN model at a fraction of the training and decoding cost.
",1 Introduction,[0],[0]
2820,1 Introduction,[0],[0]
"The data set we evaluate on in this work is WMT English-French NewsTest2014, which has 380M words of parallel training data and a 3003 sentence test set.",2 Data Set,[0],[0]
The NewsTest2013 set is used for validation.,2 Data Set,[0],[0]
"In order to compare our architecture to past work, we train a word-based system without any data augmentation techniques.",2 Data Set,[0],[0]
"The network architecture is very similar to Bahdanau et al. (2014), and specific details of layer size/depth are provided in subsequent sections.",2 Data Set,[0],[0]
"We use an 80k source/target vocab and perform standard unk-replacement (Jean et al., 2015) on out-of-vocabulary words.",2 Data Set,[0],[0]
Training is performed using an in-house toolkit.,2 Data Set,[0],[0]
"Our baseline decoder is a standard beam search decoder (Sutskever et al., 2014) with several straightforward performance optimizations:
• It is written in pure C++, with no heap allocation done during the core search.",3 Baseline Decoder,[0],[0]
• A candidate list is used to reduce the output softmax from 80k to ~500.,3 Baseline Decoder,[0],[0]
"We run word alignment (Brown et al., 1993) on the training and keep the top 20 context-free translations for each source word in the test sentence.",3 Baseline Decoder,[0],[0]
•,3 Baseline Decoder,[0],[0]
"The Intel MKL library is used for matrix multiplication, as it is the fastest floating point matrix multiplication library for CPUs.",3 Baseline Decoder,[0],[0]
• Early stopping is performed when the top partial hypothesis has a log-score of δ = 3.0 worse than the best completed hypothesis.,3 Baseline Decoder,[0],[0]
• Batching of matrix multiplication is applied when possible.,3 Baseline Decoder,[0],[0]
"Since each sentence is decoded separately, we can only batch over the hypotheses in the beam as well as the input vectors on the source side.",3 Baseline Decoder,[0],[0]
This section describes a number of speedups that can be made to a CPU-based attentional sequenceto-sequence beam decoder.,4 Decoder Speed Improvements,[0],[0]
"Crucially, none of these speedups affect the actual mathematical computation of the decoder, so they can be applied to any network architecture with a guarantee that they will not affect the results.1
1Some speedups apply quantization which leads to small random perturbations, but these change the BLEU score by less than 0.02.
",4 Decoder Speed Improvements,[0],[0]
The model used here is similar to the original implementation of Bahdanau et al. (2014).,4 Decoder Speed Improvements,[0],[0]
"The exact target GRU equation is:
dij = tanh(Wahi−1 +",4 Decoder Speed Improvements,[0],[0]
"Vaxi)·tanh(Uasj) αij = edij∑ j′ e dij′
ci = ∑
j
αijsj
ui = σ(Wuhi−1 + Vuxi +",4 Decoder Speed Improvements,[0],[0]
"Uuci + bu) ri = σ(Wrhi−1 + Vrxi + Urci + br)
ĥi = σ(ri (Whhi−1) +",4 Decoder Speed Improvements,[0],[0]
"Vhxi + Uhci + bh) hi = uihi−1 + (1− ui)ĥi
Where W∗, V∗, U∗, b∗ are learned parameters, sj is the hidden vector of the jth source word, hi−1 is the previous target recurrent vector, xi is the target input (e.g., embedding of previous word).
",4 Decoder Speed Improvements,[0],[0]
"We also denote the various hyperparameters: b for the beam size, r for the recurrent hidden size, e is the embedding size, |S| for the source sentence length, and |T | for the target sentence length, |E| is the vocab size.",4 Decoder Speed Improvements,[0],[0]
"Although CPU-based matrix multiplication libraries are highly optimized, they typically only operate on 32/64-bit floats, even though DNNs can almost always operate on much lower precision without degredation of accuracy (Han et al., 2016).",4.1 16-Bit Matrix Multiplication,[0],[0]
"However, low-precision math (1-bit to 7-bit) is difficult to implement efficiently on the CPU, and even 8-bit math has limited support in terms of vectorized (SIMD) instruction sets.",4.1 16-Bit Matrix Multiplication,[0],[0]
"Here, we use 16-bit fixed-point integer math, since it has firstclass SIMD support and requires minimal changes to training.",4.1 16-Bit Matrix Multiplication,[0],[0]
"Training is still performed with 32-bit floats, but we clip the weights to the range [-1.0, 1.0] the relu activation to [0.0, 10.0] to ensure that all values fit into 16-bits with high precision.",4.1 16-Bit Matrix Multiplication,[0],[0]
"A reference implementation of 16-bit multiplication in C++/SSE2 is provided in the supplementary material, with a thorough description of lowlevel details.2
A comparison between our 16-bit integer implementation and Intel MKL’s 32-bit floating point multiplication is given in Figure 1.",4.1 16-Bit Matrix Multiplication,[0],[0]
"We can see that 16-bit multiplication is 2x-3x faster than 32- bit multiplication for batch sizes between 2 and 8, which is the typical range of the beam size",4.1 16-Bit Matrix Multiplication,[0],[0]
"b. We
2Included as ancillary file in Arxiv submission, on right side of submission page.
",4.1 16-Bit Matrix Multiplication,[0],[0]
"are able to achieve greater than a 2x speedup in certain cases because we pre-process the weight matrix offline to have optimal memory layout, which is a capability BLAS libraries do not have.",4.1 16-Bit Matrix Multiplication,[0],[0]
"In the first hidden layer on the source and target sides, xi corresponds to word embeddings.",4.2 Pre-Compute Embeddings,[0],[0]
"Since this is a closed set of values that are fixed after training, the vectors V xi can be pre-computed (Devlin et al., 2014) for each word in the vocabulary and stored in a lookup table.",4.2 Pre-Compute Embeddings,[0],[0]
"This can only be applied to the first hidden layer.
",4.2 Pre-Compute Embeddings,[0],[0]
"Pre-computation does increase the memory cost of the model, since we must store r × 3 floats per word instead of e.",4.2 Pre-Compute Embeddings,[0],[0]
"However, if we only compute the k most frequently words (e.g., k = 8, 000), this reduces the pre-computation memory by 90% but still results in 95%+ token coverage due to the Zipfian distribution of language.",4.2 Pre-Compute Embeddings,[0],[0]
"The attention context computation in the GRU can be re-factored as follows:
Uci = U( ∑ j αijsj) = ∑ j αij(Usj)
Crucially, the hidden vector representation sj is only dependent on the source sentence, while aij is dependent on the target hypothesis.",4.3 Pre-Compute Attention,[0],[0]
"Therefore, the original computation Uci requires total |T |× b multiplications per sentence, but the re-factored versionUsj only requires total |S|multiplications.",4.3 Pre-Compute Attention,[0],[0]
"The expectation over α must still be computed at each target timestep, but this is much less expensive than the multiplication by U .",4.3 Pre-Compute Attention,[0],[0]
"For the element-wise vector functions use in the GRU, we can use vectorized instructions (SSE/AVX) for the add and multiply functions, and lookup tables for sigmoid and tanh.
Reference implementations in C++ are provided in the supplementary material.",4.4 SSE & Lookup Tables,[0],[0]
"In the GRU equation, for the first target hidden layer, xi represents the previously generated word, and hi−1 encodes the hypothesis up to two words before the current word.",4.5 Merge Recurrent States,[0],[0]
"Therefore, if two partial hypotheses in the beam only differ by the last emitted word, their hi−1 vectors will be identical.",4.5 Merge Recurrent States,[0],[0]
"Thus, we can perform matrix multiplication Whi−1 only on the unique hi−1 vectors in the beam at each target timestep.",4.5 Merge Recurrent States,[0],[0]
"For a beam size of b = 6, we measured that the ratio of unique hi−1 compared to total hi−1 is approximately 70%, averaged over several language pairs.",4.5 Merge Recurrent States,[0],[0]
This can only be applied to the first target hidden layer.,4.5 Merge Recurrent States,[0],[0]
"Cumulative results from each of the preceding speedups are presented in Table 1, measured on WMT English-French NewsTest2014.",4.6 Speedup Results,[0],[0]
"The NMT architecture evaluated here uses 3-layer 512- dimensional bidirectional GRU for the source, and a 1-layer 1024-dimensional attentional GRU for the target.",4.6 Speedup Results,[0],[0]
Each sentence is decoded independently with a beam of 6.,4.6 Speedup Results,[0],[0]
"Since these speedups are all mathematical identities excluding quantization noise, all outputs achieve 36.2 BLEU and are 99.9%+ identical.
",4.6 Speedup Results,[0],[0]
"The largest improvement is from 16-bit matrix multiplication, but all speedups contribute a significant amount.",4.6 Speedup Results,[0],[0]
"Overall, we are able to achieve a 4.4x speedup over a fast baseline decoder.",4.6 Speedup Results,[0],[0]
"Although the absolute speed is impressive, the model only uses one target layer and is several BLEU behind the SOTA, so the next goal is to maximize model accuracy while still achieving speeds greater than some target, such as 100 words/sec.",4.6 Speedup Results,[0],[0]
"In NMT, like in many other deep learning tasks, accuracy can be greatly improved by adding more hidden layers, but training and decoding time increase significantly (Luong et al., 2014; Zhou et al., 2016; Wu et al., 2016).",5 Model Improvements,[0],[0]
"Several past works have noted that convolutional neural networks (CNNs) are significantly less expensive than RNNs, and replaced the source and/or target side with a CNN-based architecture (Gehring et al., 2016; Kalchbrenner et al., 2016).",5 Model Improvements,[0],[0]
"However, these works have found it is difficult to replace the target side of the model with CNN layers while maintaining high accuracy.",5 Model Improvements,[0],[0]
"The use of a recurrent target is especially important to track attentional coverage and ensure fluency.
",5 Model Improvements,[0],[0]
"Here, we propose a mixed model which uses an RNN layer at the bottom to both capture fullsentence context and perform attention, followed by a series of fully-connected (FC) layers applied on top at each timestep.",5 Model Improvements,[0],[0]
The FC layers can be interpreted as a CNN without overlapping stride.,5 Model Improvements,[0],[0]
"Since each FC layer consists of a single matrix multiplication, it is 1/6th the cost of a GRU (or 1/8th an LSTM).",5 Model Improvements,[0],[0]
"Additionally, several of the speedups from Section 4 can only be applied to the first layer, so there is strong incentive to only use a single target RNN.
",5 Model Improvements,[0],[0]
"To avoid vanishing gradients, we use ResNet-
style skip connections (He et al., 2016).",5 Model Improvements,[0],[0]
"These allow very deep models to be trained from scratch and do not require any additional matrix multiplications, unlike highway networks (Srivastava et al., 2015).",5 Model Improvements,[0],[0]
"With 5 intermediate FC layers, target timestep i is computed as:
hBi = AttGRU(h B i−1, xi, S)
h1i = relu(W 1hBi )",5 Model Improvements,[0],[0]
h2i = relu(W 2h1i ) h3i = relu(W 3h2i,5 Model Improvements,[0],[0]
+,5 Model Improvements,[0],[0]
h 1 i ),5 Model Improvements,[0],[0]
h4i = relu(W 4h3i ),5 Model Improvements,[0],[0]
"h5i = relu(W 5h4i + h 3 i ) hTi = tanh(W Th5i ) or GRU(h T i−1, h 5 i )
yi = softmax(V hTi )
",5 Model Improvements,[0],[0]
"Where xi is the target input embedding, S is the set of source hidden vectors used for attention, and V is the target output vocabulary matrix.",5 Model Improvements,[0],[0]
"The superscripts hB and hT simply denote the “bottom” and “top” hidden layers, while the numbered layers hn represent the intermediate fully-connected layers.
",5 Model Improvements,[0],[0]
"We follow He et al. (2016) and only use skip connections on every other FC layer, but do not use batch normalization.",5 Model Improvements,[0],[0]
"The same pattern can be used for more FC layers, and the FC layers can be a different size than the bottom or top hidden layers.",5 Model Improvements,[0],[0]
The top hidden layer can be an RNN or an FC layer.,5 Model Improvements,[0],[0]
"It is important to use relu activations
(opposed to tanh) for ResNet-style skip connections.",5 Model Improvements,[0],[0]
The GRUs still use tanh.,5 Model Improvements,[0],[0]
"Results using the mixed RNN+FC architecture are shown in Table 2, using all speedups.",5.1 Model Results,[0],[0]
"We have found that the benefit of using RNN+FC layers on the source is minimal, so we only perform ablation on the target.",5.1 Model Results,[0],[0]
"For the source, we use a 3-layer 512- dim bidi GRU in all models (S1)-(S6).
",5.1 Model Results,[0],[0]
Model (S1) and (S2) are one and two layer baselines.,5.1 Model Results,[0],[0]
"Model (S4), which uses 7 intermediate FC layers, has similar decoding cost to (S2) while doubling the improvement over (S1) to 1.2 BLEU.",5.1 Model Results,[0],[0]
We see minimal benefit from using a GRU on the top layer (S5) or using more FC layers (S6).,5.1 Model Results,[0],[0]
"In (E1) and (E2) we present 2 and 3 model ensembles of (S4), trained from scratch with different random seeds.",5.1 Model Results,[0],[0]
"We can see that the 2-model ensemble improves results by 0.9 BLEU, but the 3-model ensemble has little additional improvment.",5.1 Model Results,[0],[0]
"Although not presented here, we have found these improvement from decoder speedups and RNN+FC to be consistent across many language pairs.
",5.1 Model Results,[0],[0]
"All together, we were able to achieve a BLEU score of 38.3 while decoding at 100 words/sec on a single CPU core.",5.1 Model Results,[0],[0]
"As a point of comparison, Wu et al. (2016) achieves similar BLEU scores on this test set (37.9 to 38.9) and reports a CPU decoding speed of ~100 words/sec (0.2226 sents/sec), but parallelizes this decoding across 44 CPU cores.",5.1 Model Results,[0],[0]
"System (S7), which is our re-implementation of Wu et al. (2016), decodes at 28 words/sec on one CPU core, using all of the speedups described in Section 4.",5.1 Model Results,[0],[0]
"Zhou et al. (2016) has a similar computational cost to (S7), but we were not able to replicate those results in terms of accuracy.
",5.1 Model Results,[0],[0]
"Although we are comparing an ensemble to a single model, we can see ensemble (E1) is over 3x faster to decode than the single model (S7).",5.1 Model Results,[0],[0]
"Additionally, we have found that model (S4) is roughly 3x faster to train than (S7) using the same GPU resources, so (E1) is also 1.5x faster to train than a single model (S7).",5.1 Model Results,[0],[0]
"Attentional sequence-to-sequence models have become the new standard for machine translation, but one challenge of such models is a significant increase in training and decoding cost compared to phrase-based systems.",abstractText,[0],[0]
"Here, we focus on efficient decoding, with a goal of achieving accuracy close the state-of-the-art in neural machine translation (NMT), while achieving CPU decoding speed/throughput close to that of a phrasal decoder.",abstractText,[0],[0]
"We approach this problem from two angles: First, we describe several techniques for speeding up an NMT beam search decoder, which obtain a 4.4x speedup over a very efficient baseline decoder without changing the decoder output.",abstractText,[0],[0]
"Second, we propose a simple but powerful network architecture which uses an RNN (GRU/LSTM) layer at bottom, followed by a series of stacked fully-connected layers applied at every timestep.",abstractText,[0],[0]
"This architecture achieves similar accuracy to a deep recurrent model, at a small fraction of the training and decoding cost.",abstractText,[0],[0]
"By combining these techniques, our best system achieves a very competitive accuracy of 38.3 BLEU on WMT EnglishFrench NewsTest2014, while decoding at 100 words/sec on single-threaded CPU.",abstractText,[0],[0]
We believe this is the best published accuracy/speed trade-off of an NMT system.,abstractText,[0],[0]
Sharp Models on Dull Hardware: Fast and Accurate Neural Machine Translation Decoding on the CPU,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1030–1035, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
We present the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars. To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence. Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers.",text,[0],[0]
"Incremental parsing has gained popularity in both dependency (Nivre, 2004; Zhang and Clark, 2008) and constituency parsing (Zhu et al., 2013; Wang and Xue, 2014).",1 Introduction,[0],[0]
"However, the greedy or beam search algorithms used in these parsers can only explore a tiny fraction of trees among exponentially many candidates.",1 Introduction,[0],[0]
"To alleviate this problem, Huang and Sagae (2010) propose a dynamic programming (DP) algorithm, reducing the search space to a polynomial size by merging equivalent states.",1 Introduction,[0],[0]
"This idea has been extended by Kuhlmann et al. (2011) and Cohen et al. (2011) to other dependency parsing paradigms.
",1 Introduction,[0],[0]
"In constituency parsing, however, DP has not yet been applied to incremental parsing, and the bigger search space in constituency parsing suggests a potentially even bigger advantage by DP.",1 Introduction,[0],[0]
"However, with unary rules and more-than-binary branchings, constituency parsing presents challenges not found in dependency parsing that must be addressed before applying DP.",1 Introduction,[0],[0]
"Thus, we first present an odd-even
shift-reduce constituency parser which always finishes in same number of steps, eliminating the complicated asynchronicity issue in previous work (Zhu et al., 2013; Wang and Xue, 2014), and then develop dynamic programming on top of that.",1 Introduction,[0],[0]
"Secondly, to alleviate the error propagation from POS tagging, we also extends the algorithm to take a tagging sausage lattice as input, which is a compromise between pipeline and joint approaches (Hatori et al., 2011; Li et al., 2011; Wang and Xue, 2014).
",1 Introduction,[0],[0]
"Our DP parser achieves state-of-the-art performances on both Chinese and English treebanks (at 90.8% on PTB and 83.9% on CTB, the latter being the highest in literature).",1 Introduction,[0],[0]
One major challenge in constituency parsing is unary rules.,2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Unlike dependency parsing where shiftreduce always finishes in 2n−1 steps, existing incremental constituency parsers (Zhu et al., 2013; Wang and Xue, 2014) reach the goal state (full parse tree) in different steps due to different number of unary rules.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"So we propose a new, synchronized, “oddeven” system to reach the goal in the same 4n − 2 steps.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"A state is notated p = 〈S,Q〉, where S is a stack of trees ..., s1, s0, and Q is a queue of wordtag pairs.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"At even steps (when step index is even) we can choose one of the three standard actions
• sh: shift the head of Q, a word-tag pair (t, w), onto S as a singleton tree t(w);
• rexx: combine the top two trees on the stack and replace them with a new tree x(s1, s0), x being the root nonterminal, headed on s0;
• rexy: similar to rexx but headed on s1; and at odd steps we can choose two new actions:
1030
• unx : replace s0 with a new tree x(s0) with x being the root nonterminal;
• st: no action.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
Figure 1 shows the deductive system.,2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Note that we alternate between standard shift-reduce actions in even steps and unary actions (unx or st) in odd steps, and the first action must be sh, followed by a unx or st, and followed by another sh.",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"Continuing this procedure, we can always achieve the goal in 2(2n− 1) steps.
",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"In practice, we have larger than two-way rules and multi-level unary rules, so we binarize them and collapse multi-level unary rules into one level, for example,
NP
S
VP
PPNPV
=⇒
NP+S
VP
PPVP′
NPV
Following Huang and Sagae (2010), we represent feature templates as functions",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"f(·, ·) on stack S and queue Q. Table 1 shows the 43 feature templates we use in this paper, all adopted from Zhu et al. (2013).",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"They are combinations of the 32 atomic features f̃(S,Q) (e.g. s0.t and s0.c denote the head tag and
syntactic category of tree s0, resp., and s0.lc.w is the head word of its leftmost child).",2 Odd-Even Shift-Reduce CFG Parser,[0],[0]
"The key idea towards DP is the merging of equivalent states, after which the stacks are organized in a “graph-structured stack” (GSS)(Tomita, 1988).",3 Dynamic Programming,[0],[0]
"Following Huang and Sagae (2010), “equivalent states” ∼ in a same beam are defined by the atomic features f̃(S,Q) and the span of s0:
〈S,Q〉 ∼ 〈S′, Q′〉 ⇔ f̃(S,Q) = f̃(S′, Q′) and s0.span = s′0.span.
Similarly, for each state p, π(p) is a set of predictor states, each of which can be combined with p in a rexx or re x y action.",3 Dynamic Programming,[0],[0]
"For each action, we have different operations on π(p).",3 Dynamic Programming,[0],[0]
"If a state pmakes a sh action and generates a state p′, then π(p′) = {p}.",3 Dynamic Programming,[0],[0]
"If two shifted states p′ and p′′ are equivalent, p′ ∼ p′′, we merge π(p′) and π(p′′).",3 Dynamic Programming,[0],[0]
"If a state p makes a reduce (rexx or re x y) action, p tries to combine with every p′ ∈ π(p), and each combination generates a state r with π(r) = π(p′).",3 Dynamic Programming,[0],[0]
"If two reduced states are equivalent, we only keep one predictor states, as their predictor states are identical.",3 Dynamic Programming,[0],[0]
"If a state p fires an unx or a st action resulting in a state u, we copy the predictor states π(u) = π(p).",3 Dynamic Programming,[0],[0]
"Similar to reduce actions, if two resulting states after applying an unx or a st action are equivalent, we only keep the best one with highest score (the recombined ones are only useful for searching k-best trees).
",3 Dynamic Programming,[0],[0]
"input (T1, w1)...(Tn, wn)
axioms 0 : 〈 , (t, w1)...(Tn, wn)({</s>}, </s>)〉 : 0,∀ t ∈ T1
sh",3 Dynamic Programming,[0],[0]
"l : 〈S, (t, w)|(T ′, w′)|Q〉 : (c, v) l+1 : 〈S|t(w), (t′, w′)|Q〉 : (c+csh, 0) t′ ∈ T ′, l is even
Figure 3: Extended shift-reduce deductive system with tagging sausage lattice, only showing sh.
",3 Dynamic Programming,[0],[0]
"In order to compute all the scores in GSS, for each state p, we calculate the prefix score, c, which is the total cost of the best action sequence from the initial state to the end of state p, and the inside score v, which is the score since the last shift (Figure 2).
",3 Dynamic Programming,[0],[0]
"The new mechanism beyond Huang and Sagae (2010) is the non-trivial dynamic programming treatment of unary actions (unx and st), which is not found in dependency parsing.",3 Dynamic Programming,[0],[0]
Note that the score calculation is quite different from shift in the sense that unary actions are more like reduces.,3 Dynamic Programming,[0],[0]
It is easy to extend our deductive system to take tagging sausage lattices as input.,4 Incorporating Tag Lattices,[0],[0]
The key difference is that the tag t associated with each word in the input sequence becomes a set of tags T .,4 Incorporating Tag Lattices,[0],[0]
"Thus, in the sh action, we split the state with all the possible tags t′ in the tagset T ′ for the second word on the queue.",4 Incorporating Tag Lattices,[0],[0]
"Figure 3 shows the deductive system, where we only change the sh action, input and axiom.",4 Incorporating Tag Lattices,[0],[0]
"For simplicity reasons we only present one word look
87.5
88
88.5
89
89.5
90
2 4 6 8 10 12 14 16 18
F 1
o n
t h
e d
e v s
e t
iteration
11th
15th
DP non-DP
Figure 4: The learning curves of non-DP and DP parsers on the development set.",4 Incorporating Tag Lattices,[0],[0]
"DP achieves the best performance at 11th iteration with 89.8%, while non-DP gets its optimal iteration at 15th with a lower F1 89.5%.
ahead (we just need to know the tag of the first word on the queue), but in practice, we use a look ahead of 4 words (q0..q3, see Table 1), so each shift actually splits the tagset of the 5th word on the queue (q4).",4 Incorporating Tag Lattices,[0],[0]
We evaluate our parsers on both Penn English Treebank (PTB) and Chinese Treebank (CTB).,5 Experiments,[0],[0]
"For PTB, we use sections 02-21 as the training, section 24 as the dev set, and section 23 as the test.",5 Experiments,[0],[0]
"For CTB, we use the version of 5.1, articles 001-270 and 440- 1151 as the training data, articles 301-325 as the dev set, and articles 271-300 as the test set.
",5 Experiments,[0],[0]
"Besides training with gold POS tags, we add k-best automatic tagging results to the training set using a MaxEnt model with ten-way jackknifing (Collins, 2000).",5 Experiments,[0],[0]
"And we automatically tag the dev and test sets with k-best tagging sequences us-
ing the MaxEnt POS tagger (at 97.1% accuracy on English, and 94.5% on Chinese) trained on the training set.",5 Experiments,[0],[0]
We set k to 20 for English.,5 Experiments,[0],[0]
"And we run two sets of experiments, 1-best vs. 20-best, for Chinese to address the tagging issue.",5 Experiments,[0],[0]
"We train our parsers using “max-violation perceptron” (Huang et al., 2012) (which has been shown to converge much faster than “early-update” of Collins and Roark (2004)) with minibatch parallelization (Zhao and Huang, 2013) on the head-out binarized and unary-collapsed training set.",5 Experiments,[0],[0]
"We finally debinarize the trees to recover the collapsed unary rules.
",5 Experiments,[0],[0]
"We evaluate parser performance with EVALB including labeled precision (LP), labeled recall (LR), and bracketing F1.",5 Experiments,[0],[0]
"We use a beam size of 32, and pick the optimal iteration number based on the performances on the dev set.
",5 Experiments,[0],[0]
"Our baseline is the shift-reduce parser without state recombination (henceforth “non-DP”), and our dynamic programming parser (henceforth “DP”) is the extension of the baseline.",5 Experiments,[0],[0]
Figure 4 shows the learning curves on the PTB dev set.,5.1 Learning Curves and Search Quality,[0],[0]
"With a same beam width, DP parser achieves a better performance (89.8%, peaking at the 11th iteration) and converges faster than non-DP.",5.1 Learning Curves and Search Quality,[0],[0]
"Picking the optimal iterations for DP and non-DP models, we test each with various beam size, and plot the F1 curves in Figure 5.",5.1 Learning Curves and Search Quality,[0],[0]
"Again, DP is always better than non-DP, with 0.5% difference at beam of 64.",5.1 Learning Curves and Search Quality,[0],[0]
Table 2 shows the final results on the PTB test set.,5.2 Final Results on English,[0],[0]
The last column shows the empirical time complexity.,5.2 Final Results on English,[0],[0]
"Our baseline parser achieves a competitive score, which is higher than Berkeley even with a linear time complexity, and is comparable to Zhu et al. (2013).",5.2 Final Results on English,[0],[0]
"Our DP parser improves the F1 score by 0.5 points over the non-DP, and achieves the best F1 score among empirical linear-time parsers.",5.2 Final Results on English,[0],[0]
"To alleviate the propagation of errors from POS tagging, we run sausage lattice parsing on both Chinese and English, where Chinese tagging accuracy significantly lag behind English.
",5.3 Sausage Lattice Parsing,[0],[0]
Table 3 shows the F1 score and POS tagging accuracy of all parsing models on the Chinese 5.1 test set.,5.3 Sausage Lattice Parsing,[0],[0]
"Our MaxEnt POS tagger achieves an accuracy of 94.5% on 1-best outputs, and an oracle score of 97.1% on 20-best results.",5.3 Sausage Lattice Parsing,[0],[0]
"The average number of
tags for each word in the 20-best list is 1.1.",5.3 Sausage Lattice Parsing,[0],[0]
The joint tagging and parsing approach of Wang and Xue (2014) improves the F1 score from 80.1% to 83.6% (see lines 4 and 5).,5.3 Sausage Lattice Parsing,[0],[0]
"We instead use sausage lattices, a much cheaper way.",5.3 Sausage Lattice Parsing,[0],[0]
The non-DP (1-best POS) and non-DP (20-best POS) lines show the effectiveness of using sausage lattices (+1.1 for tagging and +2.6 for parsing).,5.3 Sausage Lattice Parsing,[0],[0]
"As Wang and Xue (2014) is a non-DP model, it is comparable to our non-DP results.",5.3 Sausage Lattice Parsing,[0],[0]
"With the help of 20-best tagging lattices, we achieve the same tagging accuracy at 95.5%, but still 0.4 worse on the F1 score than the joint model.",5.3 Sausage Lattice Parsing,[0],[0]
It suggests that we need a larger k to catch up the gap.,5.3 Sausage Lattice Parsing,[0],[0]
"But our DP model boosts the performance further to the best score at 83.9% with a similar set of features.
",5.3 Sausage Lattice Parsing,[0],[0]
The last two lines (non-DP and DP) in Table 2 show our English lattice parsing results.,5.3 Sausage Lattice Parsing,[0],[0]
"So we run another baseline with the non-DP English parser on 1-best POS tags, and the baseline achieves a tagging accuracy at 97.11 and an F1 score at 90.1.",5.3 Sausage Lattice Parsing,[0],[0]
"Comparing to the tagging accuracy (97.15) and F1 score (90.3) of our non-DP lattice parser, sausage lattice parsing doesn’t help the tagging accuracy, but helps parsing a little by 0.2 points.",5.3 Sausage Lattice Parsing,[0],[0]
"The statistics show that 2 percent of POS tags in the lattice parsing result are different from the baseline, and those differences lead to a slight improvement on parsing.",5.3 Sausage Lattice Parsing,[0],[0]
"In this paper, we present a dynamic programming algorithm based on graph-structured stack (GSS) for shift-reduce constituency parsing, and extend the algorithm to take tagging sausage lattices as input.",6 Conclusions,[0],[0]
"Experiments on both English and Chinese treebanks show that our DP parser outperforms almost all other parsers except of Carreras et al. (2008), which runs in a much higher time complexity.",6 Conclusions,[0],[0]
We thank the anonymous reviewers for comments.,Acknowledgment,[0],[0]
"Haitao Mi is supported by DARPA HR0011-12C-0015 (BOLT), and Liang Huang is supported by DARPA FA8750-13-2-0041 (DEFT), NSF IIS1449278, and a Google Faculty Research Award.",Acknowledgment,[0],[0]
The views and findings in this paper are those of the authors and are not endorsed by the DARPA.,Acknowledgment,[0],[0]
"We present the first dynamic programming (DP) algorithm for shift-reduce constituency parsing, which extends the DP idea of Huang and Sagae (2010) to context-free grammars.",abstractText,[0],[0]
"To alleviate the propagation of errors from part-of-speech tagging, we also extend the parser to take a tag lattice instead of a fixed tag sequence.",abstractText,[0],[0]
"Experiments on both English and Chinese treebanks show that our DP parser significantly improves parsing quality over non-DP baselines, and achieves the best accuracies among empirical linear-time parsers.",abstractText,[0],[0]
Shift-Reduce Constituency Parsing with Dynamic Programming and POS Tag Lattice,title,[0],[0]
This article deals with the estimation of the regression vector β ∈,1. Introduction,[0],[0]
"Rp in the linear regression model y = Xβ + w, where X ∈ Rn×p is a known design matrix with unit Euclidean norm columns, w is the noise vector and y is the observation vector.",1. Introduction,[0],[0]
"Throughout this article, we assume that the entries of the noise w are independent, zero mean and Gaussian distributed with variance σ2.",1. Introduction,[0],[0]
We consider the high dimensional and sample starved scenario of n < p or n p where classical techniques like ordinary least squares (OLS) are no longer applicable.,1. Introduction,[0],[0]
This problem of estimating high dimensional vectors in sample starved scenarios is ill-posed even in the absence of noise unless strong structural assumptions are made on X and β.,1. Introduction,[0],[0]
A widely used and practically valid assumption is sparsity.,1. Introduction,[0],[0]
The vector β ∈,1. Introduction,[0],[0]
Rp is sparse if the support of β given by S = supp(β) = {k : βk 6= 0} has cardinality k0 = card(S),1. Introduction,[0],[0]
"p.
*Equal contribution 1Department of Electrical Engineering, IIT Madras, India 2Department of Electrical Engineering, IIT Madras, India.",1. Introduction,[0],[0]
"Correspondence to: Sreejith Kallummil <sreejith.k.venugopal@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"A number of algorithms like least absolute shrinkage and selection operator (LASSO)(Tropp, 2006; Tibshirani, 1996), Dantzig selector (DS)(Candes & Tao, 2007), subspace pursuit (SP)(Dai & Milenkovic, 2009), OMP (Pati et al., 1993; Mallat & Zhang, 1993; Tropp, 2004; Cai & Wang, 2011), elastic net (Zou & Hastie, 2005) etc. are proposed to efficiently estimate β.",1. Introduction,[0],[0]
Tuning the hyper parameters of aforementioned algorithms to achieve optimal performance require a priori knowledge of signal parameters like sparsity k0 or noise statistics like σ2 etc.,1. Introduction,[0],[0]
"Unfortunately, these parameters are rarely known a priori.",1. Introduction,[0],[0]
"To the best of our knowledge, no computationally efficient technique to estimate k0 is reported in open literature.",1. Introduction,[0],[0]
"However, limited success on the estimation of σ2 has been reported in literature (Dicker, 2014; Fan et al., 2012; Dicker & Erdogdu, 2016; Bayati et al., 2013).",1. Introduction,[0],[0]
"However, the performance of these σ2 estimates when used for tuning hyper parameters in LASSO, DS, OMP etc. are largely unknown.",1. Introduction,[0],[0]
"Generalised techniques for hyper parameter selection like cross validation (CV)(Arlot et al., 2010), re-sampling (Meinshausen & Bühlmann, 2010) etc. are computationally challenging.",1. Introduction,[0],[0]
"Further, CV is reported to have poor variable selection behaviour(Chichignoud et al., 2016; Arlot et al., 2010).",1. Introduction,[0],[0]
"Indeed, algorithms that are oblivious to signal and noise statistics are also proposed in literature.",1. Introduction,[0],[0]
"This include algorithms inspired or related to LASSO like square root LASSO(Belloni et al., 2011), AV∞ (Chichignoud et al., 2016), approximate message passing (Mousavi et al., 2013; Bayati et al., 2013) etc. and ridge regression inspired techniques like least squares adaptive thresholding (LAT), ridge adaptive thresholding (RAT)(Wang et al., 2016) etc.",1. Introduction,[0],[0]
"However, most of existing signal and noise statistics oblivious sparse recovery techniques have only large sample performance guarantees.",1. Introduction,[0],[0]
"Further, many of these techniques assume that design matrix X is sampled from a random ensemble, a condition which is rarely satisfied in practice.",1. Introduction,[0],[0]
This article present a novel technique called residual ratio thresholding (RRT) for finding a “good” estimate of support S from the data dependent/adaptive sequence of supports generated by OMP.,1.1. Contributions of this paper,[0],[0]
"RRT is analytically shown to accomplish exact support recovery, (i.e., identifying S) under the same finite sample and deterministic constraints on X like
restricted isometry constants (RIC) or mutual coherence required by OMP with a priori knowledge of k0 or σ2.",1.1. Contributions of this paper,[0],[0]
"However, the signal to noise ratio (SNR=‖Xβ‖22/nσ2) required for support recovery using RRT is slightly higher than that of OMP with a priori knowledge of k0 or σ2.",1.1. Contributions of this paper,[0],[0]
This extra SNR requirement is shown to decrease with the increase in sample size n. RRT and OMP with a priori knowledge of k0 or σ2 are shown to be equivalent as n → ∞ in terms of the SNR required for support recovery.,1.1. Contributions of this paper,[0],[0]
RRT involves a tuning parameter α that can be set independent of ambient SNR or noise statistics.,1.1. Contributions of this paper,[0],[0]
The hyper parameter α in RRT have an interesting semantic interpretation of being the high SNR upper bound on support recovery error.,1.1. Contributions of this paper,[0],[0]
"Also RRT is asymptotically tuning free in the sense that a very wide range of α deliver similar performances as n → ∞. Numerical simulations indicate that RRT can deliver a highly competitive performance when compared to OMP having a priori knowledge of k0 or σ2, OMP with k0 estimated using CV and the recently proposed LAT algorithm.",1.1. Contributions of this paper,[0],[0]
"Further, RRT also delivered a highly competitive performance when applied to identify outliers in real data sets, an increasingly popular application of sparse estimation algorithms(Mitra et al., 2010; 2013).
",1.1. Contributions of this paper,[0],[0]
The remainder of this article is organised as follows.,1.1. Contributions of this paper,[0],[0]
In section 2 we discuss OMP algorithm.,1.1. Contributions of this paper,[0],[0]
RRT algorithm is presented in Section 3.,1.1. Contributions of this paper,[0],[0]
Section 4 presents theoretical performance guarantees for RRT.,1.1. Contributions of this paper,[0],[0]
Section 5 presents numerical simulation results.,1.1. Contributions of this paper,[0],[0]
"All the proofs are provided in the supplementary material.
1.2.",1.1. Contributions of this paper,[0],[0]
Notations used ‖x‖q =,1.1. Contributions of this paper,[0],[0]
( p∑ k=1 |xk|q ),1.1. Contributions of this paper,[0],[0]
1 q is the lq norm of x ∈,1.1. Contributions of this paper,[0],[0]
Rp. 0n is the n × 1 zero vector and In is the n × n identity matrix.,1.1. Contributions of this paper,[0],[0]
span(X) is the column space of X. X† =,1.1. Contributions of this paper,[0],[0]
(XTX)−1XT is the Moore-Penrose pseudo inverse of X. XJ denotes the sub-matrix of X formed using the columns indexed by J .,1.1. Contributions of this paper,[0],[0]
"N (u,C) represents a Gaussian random vector (R.V) with mean u and covariance matrix C. B(a, b) denotes a Beta R.V with parameters a and b. a ∼ b implies that a and b are identically distributed.",1.1. Contributions of this paper,[0],[0]
[p] represents the floor operator.,1.1. Contributions of this paper,[0],[0]
φ represents the null set.,1.1. Contributions of this paper,[0],[0]
"For any two sets J1 and J2, J1/J2 denotes the set difference.",1.1. Contributions of this paper,[0],[0]
"a
P→ b represents the convergence of R.V a to R.V b in probability.",1.1. Contributions of this paper,[0],[0]
"OMP (Algorithm 1) starts with a null support estimate and in each iteration it adds that column index to the current support which is the most correlated with the previous residual rk−1, i.e., tk = arg max
j |XTj rk−1|.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Then a
LS estimate of β restricted to the current support Skomp is
Algorithm 1 Orthogonal matching pursuit Input: Observation y, matrix X Initialize Somp0 = φ.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"k = 1 and residual r0 = y repeat
Identify the next column tk = arg max j |XTj rk−1|",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Expand current support Skomp = Sk−1omp ∪ tk,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Restricted LS estimate: β̂Skomp = X † Skomp y.
β̂{1,...,p}/Skomp = 0p−k.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Update residual: rk = y −Xβ̂ =,2. Orthogonal Matching Pursuit (OMP),[0],[0]
(In −Pk)y.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Increment k ← k + 1.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
until stopping condition (SC) is true Output: Support estimate Ŝ = Skomp.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Vector estimate β̂
computed as an intermediate estimate of β and this estimate is used to update the residual.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Note that Pk in Algorithm 1 refers to XSkompX † Skomp , the projection matrix onto span(XSkomp).",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Since the residual r k is orthogonal to span(XSkomp), X T j r
k",2. Orthogonal Matching Pursuit (OMP),[0],[0]
= 0,2. Orthogonal Matching Pursuit (OMP),[0],[0]
for all j ∈ Skomp.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, tk+1 /∈",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Skomp, i.e., the same index will not be selected in two different iterations.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Hence, Sk+1omp ⊃ Skomp, i.e. the support sequence is monotonically increasing.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"The monotonicity of Skomp in turn implies that the residual norm ‖rk‖2 is a non increasing function of k, i.e, ‖rk+1‖2 ≤ ‖rk‖2.
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Most of the theoretical properties of OMP are derived assuming a priori knowledge of true sparsity level k0 in which case OMP stops after exactly k0 iterations(Tropp, 2004; Wang, 2015).",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"When k0 is not known, one has to rely on stopping conditions (SC) based on the properties of the residual rk as k varies.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"For example, one can stop OMP iterations once the residual power is too low compared to the expected noise power.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Mathematically, when the noise w is l2 bounded, i.e., ‖w‖2 ≤ 2 for some a priori known 2, then OMP can be stopped if ‖rk‖2 ≤ 2.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"For a Gaussian noise vector w ∼ N (0n, σ2In), σ = σ √ n+ 2 √ n log(n) satisfies(Cai & Wang, 2011)
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
P(‖w‖2 ≤ σ),2. Orthogonal Matching Pursuit (OMP),[0],[0]
"≥ 1− 1
n , (1)
i.e., Gaussian noise is l2 bounded with a very high probability.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, one can stop OMP iterations in Gaussian noise once ‖rk‖2 ≤ σ .
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
A number of deterministic recovery guarantees are proposed for OMP.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
Among these guarantees the conditions based on RIC are the most popular.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"RIC of order j denoted by δj is defined as the smallest value of δ such that
(1− δ)‖b‖22 ≤ ‖Xb‖22 ≤ (1 + δ)‖b‖22",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"(2)
hold true for all b ∈",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Rp with ‖b‖0 = card(supp(b)),2. Orthogonal Matching Pursuit (OMP),[0],[0]
≤,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"j. A smaller value of δj implies that X act as a near orthogonal
matrix for all j sparse vectors b.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Such a situation is ideal for the recovery of a j-sparse vector b using any sparse recovery technique.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"The latest RIC based support recovery guarantee using OMP is given in Lemma 1(Liu et al., 2017).
",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Lemma 1.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
OMP with k0 iterations or SC ‖rk‖2 ≤,2. Orthogonal Matching Pursuit (OMP),[0],[0]
‖w‖2 can recover any k0 sparse vector β provided that δk0+1 < 1/,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"√ k0 + 1 and ‖w‖2 ≤ omp =
βmin √ 1− δk0+1  1−√k0 + 1δk0+1 1 + √ 1− δ2k0+1 − √ k0 + 1δk0+1 .",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Since P(‖w‖2 < σ) ≥ 1− 1/n when w ∼ N (0n, σ2In), it follows from Lemma 1 that OMP with k0 iterations or SC ‖rk‖2 ≤ σ can recover any k0-sparse vector β with probability greater than 1 − 1/n provided that δk0+1 < 1/ √ k0 + 1 and σ ≤ omp.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Lemma 1 implies that OMP with a priori knowledge of k0 or σ2 can recover support S once the matrix satisfies the regularity condition δk0+1 < 1/ √ k0 + 1 and the SNR is high.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
It is also known that this RIC condition is worst case necessary.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Consequently, Lemma 1 is one of the best deterministic guarantee for OMP available in literature.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Note that the mutual incoherence condition given by µX = max
j 6=k |XTj Xk|",2. Orthogonal Matching Pursuit (OMP),[0],[0]
< 1/(2k0,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"− 1)
also ensures exact support recovery at high SNR.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
Note that the a priori knowledge of k0 or σ2 required to materialise the recovery guarantees in Lemma 1 are not available in practical problems.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
"Further, k0 and σ2 are very difficult to estimate.",2. Orthogonal Matching Pursuit (OMP),[0],[0]
This motivates the proposed RRT algorithm which does not require a priori knowledge of k0 or σ2.,2. Orthogonal Matching Pursuit (OMP),[0],[0]
RRT is a novel signal and noise statistics oblivious technique to estimate the support S based on the behaviour of the residual ratio statistic RR(k) =,3. Residual Ratio Thresholding (RRT),[0],[0]
‖rk‖2/‖rk−1‖2 as k increases from k = 1 to a predefined value k = kmax > k0.,3. Residual Ratio Thresholding (RRT),[0],[0]
"As aforementioned, identifying the support using the behaviour of ‖rk‖2 requires a priori knowledge of σ2.",3. Residual Ratio Thresholding (RRT),[0],[0]
"However, as we will show in this section, support detection using RR(k) does not require a priori knowledge of σ2.",3. Residual Ratio Thresholding (RRT),[0],[0]
"Since the residual norms are non negative and non increasing, RR(k) always satisfy 0 ≤",3. Residual Ratio Thresholding (RRT),[0],[0]
RR(k) ≤ 1.,3. Residual Ratio Thresholding (RRT),[0],[0]
Consider running kmax > k0 iterations of OMP and let {Skomp}kmaxk=1 be the support sequence generated by OMP.,3.1. Minimal superset and implications,[0],[0]
"Recall that Skomp is monotonically increasing.
",3.1. Minimal superset and implications,[0],[0]
Definition 1:-,3.1. Minimal superset and implications,[0],[0]
"The minimal superset in the OMP support sequence {Skomp}kmaxk=1 is given by Skminomp , where kmin = min({k : S ⊆ Skomp}).",3.1. Minimal superset and implications,[0],[0]
"When the set {k : S ⊆ Skomp} = φ, we set kmin =∞ and Skminomp = φ.
",3.1. Minimal superset and implications,[0],[0]
"In words, minimal superset is the smallest superset of support S present in a particular realization of the support estimate sequence {Skomp}kmaxk=1 .",3.1. Minimal superset and implications,[0],[0]
Note that both kmin and Skminomp are unobservable random variables.,3.1. Minimal superset and implications,[0],[0]
"Since card(Skomp) = k, Skomp for k < k0 cannot satisfy S ⊆ Skomp and hence kmin ≥ k0.",3.1. Minimal superset and implications,[0],[0]
"Further, the monotonicity of Skomp implies that S ⊂ Skomp for all k ≥ kmin.",3.1. Minimal superset and implications,[0],[0]
"Case 1:- When kmin = k0, then Sk0omp = S and Skomp ⊃ S for k ≥ k0, i.e., S is present in the solution path.",3.1. Minimal superset and implications,[0],[0]
"Further, when kmin = k0, it is true that Skomp ⊆ S for k ≤ k0.",3.1. Minimal superset and implications,[0],[0]
Case 2:-,3.1. Minimal superset and implications,[0],[0]
"When k0 < kmin ≤ kmax, then Skomp 6= S for all k and Sompk ⊃ S for k ≥ kmin, i.e., S is not present in the solution path.",3.1. Minimal superset and implications,[0],[0]
"However, a superset of S is present.",3.1. Minimal superset and implications,[0],[0]
Case 3:-,3.1. Minimal superset and implications,[0],[0]
"When kmin = ∞, then Skomp 6⊇ S for all k, i.e., neither S nor a superset of S is present in {Skomp}kmaxk=1 .",3.1. Minimal superset and implications,[0],[0]
"To summarize, exact support recovery using any OMP based scheme including the signal and noise statistics aware schemes is possible only if kmin = k0.",3.1. Minimal superset and implications,[0],[0]
"Whenever kmin > k0, it is possible to estimate true support S without having any false negatives.",3.1. Minimal superset and implications,[0],[0]
"However, one then has to suffer from false positives.",3.1. Minimal superset and implications,[0],[0]
"When kmin =∞, any support in {Skomp}kmaxk=1 has to suffer from false negatives and all supports Skomp for k > k0 − 1 has to suffer from false positives also.",3.1. Minimal superset and implications,[0],[0]
Note that the matrix and SNR conditions required for exact support recovery in Lemma 1 automatically implies that kmin = k0.,3.1. Minimal superset and implications,[0],[0]
"We formulate the proposed RRT scheme assuming that kmin = k0.
3.2.",3.1. Minimal superset and implications,[0],[0]
"Behaviour of RR(k0)
",3.1. Minimal superset and implications,[0],[0]
"Next we consider the behaviour of residual ratio statistic at the k0 iteration, i.e., RR(k0) =",3.1. Minimal superset and implications,[0],[0]
‖rk0‖2/‖rk0−1‖2 under the assumption that ‖w‖2 ≤ omp and δk0+1 < 1/ √ k0 + 1 which ensures kmin = k0 and Skomp ⊆ S for all k ≤ k0.,3.1. Minimal superset and implications,[0],[0]
"Since Xβ = XSβS ∈ span(XS), (In − Pk)Xβ 6=",3.1. Minimal superset and implications,[0],[0]
0n if S 6⊆ Skomp and (In − Pk)Xβ = 0n if S ⊆ Skomp.,3.1. Minimal superset and implications,[0],[0]
This along with the monotonicity of Skomp implies the following.,3.1. Minimal superset and implications,[0],[0]
(In − Pk)Xβ 6= 0n for k < kmin = k0 and (In − Pk)Xβ = 0n for k ≥ kmin = k0.,3.1. Minimal superset and implications,[0],[0]
Thus rk = (In − Pk)y =,3.1. Minimal superset and implications,[0],[0]
(In − Pk)XSβS +,3.1. Minimal superset and implications,[0],[0]
"(In − Pk)w for k < kmin = k0, whereas, rk = (In − Pk)w for k ≥ kmin = k0.",3.1. Minimal superset and implications,[0],[0]
"Consequently, at k = k0, the numerator ‖rk0‖2 ofRR(k0) contains contribution only from the noise term ‖(In−Pk0)w‖2, whereas, the denominator ‖rk0−1‖2 in RR(k0) contain contributions from both the signal term i.e., (In−Pk)XSβS and the noise term (In−Pk)w.",3.1. Minimal superset and implications,[0],[0]
"This behaviour of RR(k0) along with the fact that ‖w‖2
P→ 0 as σ2 → 0 implies the following theorem.
",3.1. Minimal superset and implications,[0],[0]
Theorem 1.,3.1. Minimal superset and implications,[0],[0]
Assume that the matrix X satisfies the RIC constraint δk0+1 < 1/ √ k0 + 1 and kmax > k0.,3.1. Minimal superset and implications,[0],[0]
Then a).,3.1. Minimal superset and implications,[0],[0]
RR(kmin) P→ 0 as σ2 → 0. b).,3.1. Minimal superset and implications,[0],[0]
"lim σ2→0 P(kmin = k0) = 1.
",3.1. Minimal superset and implications,[0],[0]
"Algorithm 2 Residual ratio thresholding Input: Observation y, matrix X Step 1: Run kmax iterations of OMP.",3.1. Minimal superset and implications,[0],[0]
"Step 2: Compute RR(k) for k = 1, . . .",3.1. Minimal superset and implications,[0],[0]
", kmax.",3.1. Minimal superset and implications,[0],[0]
Step 3: Estimate kRRT = max{k : RR(k) ≤ ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k)} Output: Support estimate Ŝ = SkRRTomp .,3.1. Minimal superset and implications,[0],[0]
Vector estimate β̂(SkRRTomp ),3.1. Minimal superset and implications,[0],[0]
"= X
† SkRRTomp y, β̂({1, . . .",3.1. Minimal superset and implications,[0],[0]
", p}/SkRRTomp ) = 0p−kRRT .
",3.1. Minimal superset and implications,[0],[0]
3.3.,3.1. Minimal superset and implications,[0],[0]
Behaviour of RR(k) for k >,3.1. Minimal superset and implications,[0],[0]
"kmin
",3.1. Minimal superset and implications,[0],[0]
Next we discuss the behaviour of RR(k) for k > kmin.,3.1. Minimal superset and implications,[0],[0]
By the definition of kmin we have S ⊆ Skomp which implies that rk = (In − Pk)w for k ≥ kmin.,3.1. Minimal superset and implications,[0],[0]
"The absence of signal terms in numerator and the denominator of RR(k) = ‖(In−Pk)w‖2‖(In−Pk−1)w‖2 for k > kmin implies that even when ‖w‖2 → 0 or σ2 → 0, RR(k) for k > kmin does not converge to zero.",3.1. Minimal superset and implications,[0],[0]
"This behaviour of RR(k) for k > kmin is captured in Theorem 2 where we provide explicit σ2 or SNR independent lower bounds on RR(k) for k > kmin.
Theorem 2.",3.1. Minimal superset and implications,[0],[0]
"Let Fa,b(x) denotes the cumulative distribution function of a B(a, b) random variable.",3.1. Minimal superset and implications,[0],[0]
"Then ∀σ2 > 0,
ΓαRRT (k) =",3.1. Minimal superset and implications,[0],[0]
"√ F−1n−k
2 ,0.5
( α
kmax(p− k + 1)
) satisfies
P(RR(k) > ΓαRRT",3.1. Minimal superset and implications,[0],[0]
"(k),∀k > kmin)",3.1. Minimal superset and implications,[0],[0]
≥ 1− α.,3.1. Minimal superset and implications,[0],[0]
"(3)
Theorem 2 states that the residual ratio statistic RR(k) for k > kmin is lower bounded by the deterministic sequence {ΓαRRT (k)} kmax k=kmin+1
with a high probability (for small values of α).",3.1. Minimal superset and implications,[0],[0]
Please note that kmin is itself a R.V. Note that the sequence ΓαRRT (k) is dependent only on the matrix dimensions n,3.1. Minimal superset and implications,[0],[0]
and,3.1. Minimal superset and implications,[0],[0]
p.,3.1. Minimal superset and implications,[0],[0]
"Further, Theorem 2 does not make any assumptions on the noise variance σ2 or the design matrix X. Theorem 2 is extremely non trivial considering the fact that the support estimate sequence {Skomp} kmax k=1 produced by OMP is adaptive and data dependent.
",3.1. Minimal superset and implications,[0],[0]
Lemma 2.,3.1. Minimal superset and implications,[0],[0]
"The following important properties of ΓαRRT (k) are direct consequences of the monotonicity of CDF and the fact that a Beta R.V take values only in [0, 1]. 1). ΓαRRT",3.1. Minimal superset and implications,[0],[0]
(k) is defined only in the interval α ∈,3.1. Minimal superset and implications,[0],[0]
"[0, kmax(p− k + 1)].",3.1. Minimal superset and implications,[0],[0]
2). 0,3.1. Minimal superset and implications,[0],[0]
≤ ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) ≤ 1. 3). ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) is a monotonically increasing function of α. 4). ΓαRRT,3.1. Minimal superset and implications,[0],[0]
(k) = 0,3.1. Minimal superset and implications,[0],[0]
when α = 0 and Γ α RRT (k) = 1 when α = kmax(p− k + 1).,3.1. Minimal superset and implications,[0],[0]
"From Theorem 1, it is clear that P(kmin = k0) and
P(Sompk0 = S) increases with increasing SNR (or decreasing σ2), whereas, RR(kmin) decreases to zero with increasing SNR.",3.4. Residual ratio thresholding framework,[0],[0]
"At the same time, for small values of α like α = 0.01, RR(k) for k > kmin is lower bounded by ΓαRRT (k) with a very high probability at all SNR.",3.4. Residual ratio thresholding framework,[0],[0]
"Hence, finding the last index k such that RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k), i.e., kRRT = max{k : RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
(k)} gives k0 and equivalently Sk0omp = S with a probability increasing with increasing SNR.,3.4. Residual ratio thresholding framework,[0],[0]
This motivates the proposed signal and noise statistics oblivious RRT algorithm presented in Algorithm 2.,3.4. Residual ratio thresholding framework,[0],[0]
Remark 1.,3.4. Residual ratio thresholding framework,[0],[0]
An important aspect regarding the RRT in Algorithm 2 is the choice of kRRT when the set {k : RR(k) ≤ ΓαRRT,3.4. Residual ratio thresholding framework,[0],[0]
(k)} = φ.,3.4. Residual ratio thresholding framework,[0],[0]
This situation happens only at very low SNR.,3.4. Residual ratio thresholding framework,[0],[0]
When {k : RR(k) ≤ ΓαRRT,3.4. Residual ratio thresholding framework,[0],[0]
"(k)} = φ for a given value of α, we increase the value of α to the smallest value αnew > α such that {k : RR(k) ≤ ΓαnewRRT (k)} 6= φ.",3.4. Residual ratio thresholding framework,[0],[0]
"Mathematically, we set kRRT = max{k : RR(k) < ΓαnewRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k)}, where αnew = min
a>α {a : {k : RR(k) ≤ ΓαRRT",3.4. Residual ratio thresholding framework,[0],[0]
"(k)} 6= φ}.
",3.4. Residual ratio thresholding framework,[0],[0]
"Since α = p kmax gives ΓαRRT (1) = 1 and RR(1) ≤ 1, a value of αnew ≤ pkmax always exists.",3.4. Residual ratio thresholding framework,[0],[0]
"αnew can be easily computed by first pre-computing {ΓaRRT (k)} kmax k=1 for say 100 prefixed values of a in the interval (α, pkmax].",3.4. Residual ratio thresholding framework,[0],[0]
Remark 2.,3.4. Residual ratio thresholding framework,[0],[0]
RRT requires performing kmax iterations of OMP.,3.4. Residual ratio thresholding framework,[0],[0]
All the quantities required for RRT including RR(k) and the final estimates can be computed while performing these kmax iterations itself.,3.4. Residual ratio thresholding framework,[0],[0]
"Consequently, RRT has complexity O(kmaxnp).",3.4. Residual ratio thresholding framework,[0],[0]
"As we will see later, a good choice of kmax is kmax =",3.4. Residual ratio thresholding framework,[0],[0]
[0.5(n + 1)] which results in a complexity order O(n2p).,3.4. Residual ratio thresholding framework,[0],[0]
This complexity is approximately n/k0 times higher than the O(npk0) complexity of OMP when k0 or σ2 are known a priori.,3.4. Residual ratio thresholding framework,[0],[0]
This is the computational cost being paid for not knowing k0 or σ2 a priori.,3.4. Residual ratio thresholding framework,[0],[0]
"In contrast, L fold CV requires running (1− 1/L)n iterations of OMP L times resulting in a O(L(1 − 1/L)n2p) = O(Ln2p) complexity, i.e., RRT is L times computationally less complex than CV.",3.4. Residual ratio thresholding framework,[0],[0]
Remark 3.,3.4. Residual ratio thresholding framework,[0],[0]
RRT algorithm is developed only assuming that the support sequence generated by the sparse recovery algorithm is monotonically increasing.,3.4. Residual ratio thresholding framework,[0],[0]
"Apart from OMP, algorithms such as orthogonal least squares(Wen et al., 2017) and OMP with thresholding(Yang & de Hoog, 2015) also produce monotonic support sequences.",3.4. Residual ratio thresholding framework,[0],[0]
RRT principle can be directly applied to operate these algorithms in a signal and noise statistics oblivious fashion.,3.4. Residual ratio thresholding framework,[0],[0]
In this section we present support recovery guarantees for RRT and compare it with the results available for OMP with a priori knowledge of k0 or σ2.,4. Analytical Results for RRT,[0],[0]
"The first result in this section deals with the finite sample and finite SNR performance for RRT.
Theorem 3.",4. Analytical Results for RRT,[0],[0]
"Let kmax ≥ k0 and suppose that the matrix X satisfies δk0+1 <
1√ k0+1 .",4. Analytical Results for RRT,[0],[0]
Then RRT can recover the true support S with probability greater than 1− 1/n− α provided that σ,4. Analytical Results for RRT,[0],[0]
"< min( omp, rrt), where
rrt = ΓαRRT (k0)
√ 1− δk0βmin
1 + ΓαRRT (k0) .",4. Analytical Results for RRT,[0],[0]
"(4)
Theorem 3 implies that RRT can identify the support S at a higher SNR or lower noise level than that required by OMP with a priori knowledge of k0 and σ2.",4. Analytical Results for RRT,[0],[0]
"For small values of α like α = 0.01, the probability of exact support recovery, i.e., 1− α− 1/n is similar to that of the 1− 1/n probability of exact support recovery in Lemma 1.",4. Analytical Results for RRT,[0],[0]
"Also please note that the RRT framework does not impose any extra conditions on the design matrix X. Consequently, the only appreciable difference between RRT and OMP with a priori knowledge of k0 and σ2 is in the extra SNR required by RRT which is quantified next using the metric extra = omp/ rrt.",4. Analytical Results for RRT,[0],[0]
"Note that the larger the value of extra, larger should be the SNR or equivalently smaller should be the noise level required for RRT to accomplish exact support recovery.",4. Analytical Results for RRT,[0],[0]
Substituting the values of omp and rrt and using the bound δk0 ≤,4. Analytical Results for RRT,[0],[0]
δk0,4. Analytical Results for RRT,[0],[0]
"+1 gives
extra ≤ 1 + 1ΓαRRT (k0)
1 +
√ 1−δ2k0+1
1− √ k0+1δk0+1
.",4. Analytical Results for RRT,[0],[0]
"(5)
Note that
√ 1−δ2k0+1
1− √ k0+1δk0+1
= (
1−δk0+1 1− √ k0+1δk0+1 )√ 1+δk0+1 1−δk0+1
≥ 1.",4. Analytical Results for RRT,[0],[0]
"Consequently,
extra ≤ 0.5 ( 1 + 1
ΓαRRT (k0)
) .",4. Analytical Results for RRT,[0],[0]
"(6)
Since 0 ≤ ΓαRRT (k0) ≤ 1, it follows that 0.5 (
1 + 1ΓαRRT (k0)
) is always greater than or equal to one.
",4. Analytical Results for RRT,[0],[0]
"However, extra decreases with the increase in ΓαRRT (k0).",4. Analytical Results for RRT,[0],[0]
"In particular, when ΓαRRT (k0) = 1, there is no extra SNR requirement.",4. Analytical Results for RRT,[0],[0]
Remark 4.,4. Analytical Results for RRT,[0],[0]
RRT algorithm involves two hyper parameters viz.,4. Analytical Results for RRT,[0],[0]
kmax and α.,4. Analytical Results for RRT,[0],[0]
Exact support recovery using RRT requires only that kmax ≥ k0.,4. Analytical Results for RRT,[0],[0]
"However, k0 is an unknown quantity.",4. Analytical Results for RRT,[0],[0]
"In our numerical simulations, we set kmax = min(p, [0.5(rank(X) + 1)]).",4. Analytical Results for RRT,[0],[0]
This choice is motivated by the facts that k0 <,4. Analytical Results for RRT,[0],[0]
"[0.5(rank(X)+1)] is a necessary condition for exact support recovery using any sparse estimation algorithm(Elad, 2010) when n < p and min(n, p) is the maximum possible number of iterations in OMP.",4. Analytical Results for RRT,[0],[0]
"Since evaluating rank(X) requires extra computations, one can always use rank(X) ≤ n to set kmax = min(p, [0.5(n+1)]).",4. Analytical Results for RRT,[0],[0]
"Please note that this choice of kmax is independent of the operating SNR, design matrix and the vector to be estimated and the user is not required to tune this parameter.",4. Analytical Results for RRT,[0],[0]
"Hence, α is the only user specified hyper parameter in RRT algorithm.",4. Analytical Results for RRT,[0],[0]
"Next we discuss the behaviour of RRT as n→∞. From (6), it is clear that the extra SNR required for support recovery using RRT decreases with increasing ΓαRRT (k0).",4.1. Large sample behaviour of RRT,[0],[0]
"However, by Lemma 2 increasing ΓαRRT (k0) requires an increase in the value of α.",4.1. Large sample behaviour of RRT,[0],[0]
"However, increasing α decreases the probability of support recovery given by 1 − α − 1/n.",4.1. Large sample behaviour of RRT,[0],[0]
"In other words, one cannot have exact support recovery using RRT at lower SNR without increasing the probability of error in the process.",4.1. Large sample behaviour of RRT,[0],[0]
An answer to this conundrum is available in the large sample regime where it is possible to achieve both α,4.1. Large sample behaviour of RRT,[0],[0]
≈ 0 and ΓαRRT (k0),4.1. Large sample behaviour of RRT,[0],[0]
"≈ 1, i.e., no extra SNR requirement and no decrease in probability of support recovery.",4.1. Large sample behaviour of RRT,[0],[0]
The following theorem states the conditions required for ΓαRRT (k0),4.1. Large sample behaviour of RRT,[0],[0]
≈ 1 for large values of n. Theorem 4.,4.1. Large sample behaviour of RRT,[0],[0]
"Define klim = lim
n→∞ k0/n, plim =
lim n→∞ log(p)/n and αlim = lim n→∞ log(α)/n.",4.1. Large sample behaviour of RRT,[0],[0]
"Let
kmax = min(p, [0.5(n + 1)]).",4.1. Large sample behaviour of RRT,[0],[0]
"Then ΓαRRT (k0) =√ F−1n−k0
2 ,0.5
( α
kmax(p− k0 + 1)
) satisfies the following
asymptotic limits.",4.1. Large sample behaviour of RRT,[0],[0]
Case 1:-).,4.1. Large sample behaviour of RRT,[0],[0]
"lim
n→∞ ΓαRRT (k0) = 1, whenever klim < 0.5,
plim = 0 and αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
Case 2:-).,4.1. Large sample behaviour of RRT,[0],[0]
0,4.1. Large sample behaviour of RRT,[0],[0]
"< lim
n→∞ ΓαRRT (k0) < 1 if klim < 0.5,
αlim = 0 and plim > 0.",4.1. Large sample behaviour of RRT,[0],[0]
"In particular, lim n→∞ ΓαRRT (k0) = exp( −plim1−klim ).",4.1. Large sample behaviour of RRT,[0],[0]
"Case 3:- lim
n→∞ ΓαRRT (k0) = 0",4.1. Large sample behaviour of RRT,[0],[0]
"if klim < 0.5, αlim = 0 and
plim =∞.
Theorem 4 states that all choices of (n, p, k0) satisfying plim = 0 and klim < 0.5 can result in lim
n→∞ ΓαRRT (k0) = 1
provided that the parameter α satisfies αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Note that αlim = 0 for a wide variety of α including α = constant, α = 1/nδ for some δ > 0, α = 1/ log(n) etc.",4.1. Large sample behaviour of RRT,[0],[0]
"It is interesting to see which (n, p, k0) scenario gives plim = 0 and klim < 0.5.",4.1. Large sample behaviour of RRT,[0],[0]
Note that exact recovery in n < p scenario is possible only if k0 ≤,4.1. Large sample behaviour of RRT,[0],[0]
[0.5(n + 1)].,4.1. Large sample behaviour of RRT,[0],[0]
"Thus, the assumption klim < 0.5 will be satisfied in all interesting problem scenarios.
",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 1:- lim n→∞ ΓαRRT (k0) = 1 in low dimensional regression problems with p fixed and n → ∞ or all (n, p) → (∞,∞) with lim
n→∞ p/n ≤ 1.
",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 2:- lim n→∞ ΓαRRT (k0) = 1 in high dimensional case with p increases sub exponentially with n as exp(nδ) for some δ < 1 or p increases polynomially w.r.t n, i.e., p = nδ for some δ > 1.",4.1. Large sample behaviour of RRT,[0],[0]
"In both cases, plim = lim n→∞ log(nδ)/n = 0 and plim = lim n→∞ log(exp(nδ))/n = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 3:- lim n→∞ ΓαRRT (k0) = 1 in the extreme high dimensional case where (n, p, k0) → (∞,∞,∞) satisfy-
",4.1. Large sample behaviour of RRT,[0],[0]
ing n,4.1. Large sample behaviour of RRT,[0],[0]
≥ ck0 log(p) for some constant c > 0.,4.1. Large sample behaviour of RRT,[0],[0]
"Here plim = lim
n→∞ log(p)/n ≤",4.1. Large sample behaviour of RRT,[0],[0]
"lim n→∞
1
ck0 = 0 and klim =
lim n→∞ 1/c log(p) = 0.",4.1. Large sample behaviour of RRT,[0],[0]
Note that the sampling regime n,4.1. Large sample behaviour of RRT,[0],[0]
"≈ 2k0 log(p) is the best known asymptotic guarantee available for OMP(Fletcher & Rangan, 2012).",4.1. Large sample behaviour of RRT,[0],[0]
"Regime 4:- Consider a sampling regime where (n, p) → (∞,∞) such that k0 is fixed and n = ck0 log(p), i.e., p is exponentially increasing with n. Here plim = 1/(ck0) and klim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Consequently, lim
n→∞ ΓαRRT (k0) = exp ( −1 ck0 ) <
1.",4.1. Large sample behaviour of RRT,[0],[0]
"A good example of this sampling regime is (Tropp & Gilbert, 2007) where it was shown that OMP can recover a (not every) particular k0 dimensional signal from n random measurements (in noiseless case) when n = ck0 log(p).",4.1. Large sample behaviour of RRT,[0],[0]
Note that c ≤ 20 for all k0 and c,4.1. Large sample behaviour of RRT,[0],[0]
≈ 4 for large k0.,4.1. Large sample behaviour of RRT,[0],[0]
"Even if we assume that only n = 4k0 log(p) measurements are sufficient for recovering a k0 sparse signal, we have lim n→∞
ΓαRRT",4.1. Large sample behaviour of RRT,[0],[0]
(k0) = exp(−0.125),4.1. Large sample behaviour of RRT,[0],[0]
"= 0.9512 for k0 = 5 (i.e., extra ≤ 1.0257) and lim
n→∞ ΓαRRT (k0) = exp(−0.125)",4.1. Large sample behaviour of RRT,[0],[0]
"=
0.9753 for k0 = 10 (i.e., extra ≤ 1.0127).
",4.1. Large sample behaviour of RRT,[0],[0]
Note that ΓαRRT,4.1. Large sample behaviour of RRT,[0],[0]
"(k0)→ 1 as n→∞ implies that extra → 1 and min( omp, rrt)→ 1.",4.1. Large sample behaviour of RRT,[0],[0]
This asymptotic behaviour of ΓαRRT,4.1. Large sample behaviour of RRT,[0],[0]
"(k0) and extra imply the large sample consistency of RRT as stated in the following theorem.
",4.1. Large sample behaviour of RRT,[0],[0]
Theorem 5.,4.1. Large sample behaviour of RRT,[0],[0]
"Suppose that the sample size n → ∞ such that the matrix X satisfies δk0+1 <
1√ k0+1 , σ ≤ omp and plim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Then, a).",4.1. Large sample behaviour of RRT,[0],[0]
OMP running k0 iterations and OMP with SC ‖rk‖2,4.1. Large sample behaviour of RRT,[0],[0]
"≤ σ are large sample consistent, i.e.. lim
n→∞ P(Ŝ = S) = 1.
b).",4.1. Large sample behaviour of RRT,[0],[0]
"RRT with hyper parameter α satisfying lim n→∞ α = 0 and αlim = 0 is also large sample consistent.
",4.1. Large sample behaviour of RRT,[0],[0]
"Theorem 5 implies that at large sample sizes, RRT can accomplish exact support recovery under the same SNR and matrix conditions required by OMP with a priori knowledge of k0 or σ2.",4.1. Large sample behaviour of RRT,[0],[0]
Theorem 5 has a very important corollary.,4.1. Large sample behaviour of RRT,[0],[0]
Remark 5.,4.1. Large sample behaviour of RRT,[0],[0]
"Theorem 1 implies that all choices of α satisfying α→ 0 and αlim = 0 deliver similar performances as n→ ∞. Note that the range of adaptations satisfying α → 0 and αlim = 0 include α = 1/ log(n), α = 1/nδ for δ > 0 etc.",4.1. Large sample behaviour of RRT,[0],[0]
"Since a very wide range of tuning parameters deliver similar results as n → ∞, RRT is in fact asymptotically tuning free.",4.1. Large sample behaviour of RRT,[0],[0]
Remark 6.,4.1. Large sample behaviour of RRT,[0],[0]
"Based on the large sample analysis of RRT, one can make the following guidelines on the choice of α.",4.1. Large sample behaviour of RRT,[0],[0]
"When the sample size n is large, one can choose α as a function of n that satisfies both lim
n→∞ α = 0 and αlim = 0.",4.1. Large sample behaviour of RRT,[0],[0]
"Also since
the support recovery guarantees are of the form 1−1/n−α, it does not make sense to choose a value of α that decays to zero faster than 1/",4.1. Large sample behaviour of RRT,[0],[0]
"n. Hence, it is preferable to choose values of α that decreases to zero slower than 1/n",4.1. Large sample behaviour of RRT,[0],[0]
"like
α = 1/ log(n), α = 1/ √ n etc.",4.1. Large sample behaviour of RRT,[0],[0]
"Having discussed the large sample behaviour of RRT, we next discuss the finite sample and high SNR behaviour of RRT.",4.2. A high SNR operational interpretation of α,[0],[0]
Define the events support recovery error E = {Ŝ 6= S} and false positive F = card(Ŝ/S) > 0,4.2. A high SNR operational interpretation of α,[0],[0]
and missed discovery or false negativeM = card(S/Ŝ),4.2. A high SNR operational interpretation of α,[0],[0]
> 0.,4.2. A high SNR operational interpretation of α,[0],[0]
"The following theorem characterizes the likelihood of these events as SNR increases to infinity or σ2 → 0.
",4.2. A high SNR operational interpretation of α,[0],[0]
Theorem 6.,4.2. A high SNR operational interpretation of α,[0],[0]
Let kmax > k0 and the matrix X satisfies δk0+1 < 1/,4.2. A high SNR operational interpretation of α,[0],[0]
√ k0 + 1.,4.2. A high SNR operational interpretation of α,[0],[0]
"Then, a).",4.2. A high SNR operational interpretation of α,[0],[0]
"lim σ2→0
P(M) = 0. b).",4.2. A high SNR operational interpretation of α,[0],[0]
"lim
σ2→0 P(E) = lim σ2→0 P(F) ≤ α.
",4.2. A high SNR operational interpretation of α,[0],[0]
"Theorem 6 states that when the matrix X allows for exact support recovery in the noiseless or low noise situation, RRT will not suffer from missed discoveries.",4.2. A high SNR operational interpretation of α,[0],[0]
"Under such favourable conditions, α is a high SNR upper bound on both the probability of error and the probability of false positives.",4.2. A high SNR operational interpretation of α,[0],[0]
"Please note that such explicit characterization of hyper parameters are not available for hyper parameters in Square root LASSO, RAT, LAT etc.",4.2. A high SNR operational interpretation of α,[0],[0]
"In this section, we provide extensive numerical simulations comparing the performance of RRT with state of art sparse recovery techniques.",5. Numerical Simulations,[0],[0]
"In particular, we compare the performance of RRT with OMP with k0 estimated using five fold CV and the least squares adaptive thresholding (LAT) proposed in (Wang et al., 2016).",5. Numerical Simulations,[0],[0]
"In synthetic data sets, we also compare RRT with OMP running exactly k0 itera-
tions and OMP with SC ‖rk‖2 ≤ σ",5. Numerical Simulations,[0],[0]
"√ n+ 2 √ n log(n)(Cai & Wang, 2011).",5. Numerical Simulations,[0],[0]
"These algorithms are denoted in Figures 1-4 by “CV”, “LAT”, “OMP1” and “OMP2” respectively.",5. Numerical Simulations,[0],[0]
RRT1 and RRT2 represent RRT with parameter α set to α = 1/ log(n) and α = 1/ √ n respectively.,5. Numerical Simulations,[0],[0]
"By Theorem 5, RRT1 and RRT2 are large sample consistent.",5. Numerical Simulations,[0],[0]
The synthetic data sets are generated as follows.,5.1. Synthetic data sets,[0],[0]
"We consider two models for the matrix X. Model 1 sample each entry of the design matrix X ∈ Rn×p independently according toN (0, 1).",5.1. Synthetic data sets,[0],[0]
Matrix X in Model 2 is formed by concatenating In with a n× n Hadamard matrix,5.1. Synthetic data sets,[0],[0]
"Hn, i.e., X =",5.1. Synthetic data sets,[0],[0]
"[In,Hn].",5.1. Synthetic data sets,[0],[0]
"This matrix guarantee exact support recovery using OMP at high SNR once k0 < 1+ √ n
2 (Elad, 2010).",5.1. Synthetic data sets,[0],[0]
The columns of X in both models are normalised to have unit l2-norm.,5.1. Synthetic data sets,[0],[0]
"Based on the choice of X and support S , we conduct 4 experiments.",5.1. Synthetic data sets,[0],[0]
"Experiments 1-2 involve matrix of model 1 with (n, p) given
by (200, 300) and (200, 900) respectively with support S sampled randomly from the set {1, . . .",5.1. Synthetic data sets,[0],[0]
", p}.",5.1. Synthetic data sets,[0],[0]
"Experiment 3 and 4 involve matrix of model 2 with (n = 128, p = 256).",5.1. Synthetic data sets,[0],[0]
"For experiment 3, support S is sampled randomly from the set {1, . . .",5.1. Synthetic data sets,[0],[0]
", p}, whereas, in experiment 4, support S is fixed at {1, 2, . . .",5.1. Synthetic data sets,[0],[0]
", k0}.",5.1. Synthetic data sets,[0],[0]
"The noise w is sampled according toN (0n, σ2In) with σ2 = 1.",5.1. Synthetic data sets,[0],[0]
The non zero entries of β are randomly assigned βj = ±1.,5.1. Synthetic data sets,[0],[0]
"Subsequently, these entries are scaled to achieve SNR = ‖Xβ‖22/n = 3.",5.1. Synthetic data sets,[0],[0]
The number of non zero entries k0 in all experiments are fixed at six.,5.1. Synthetic data sets,[0],[0]
"We compare the algorithms in terms of the l2 error, the number of false positives and the number of false negatives produced in 100 runs of each experiment.
",5.1. Synthetic data sets,[0],[0]
"From the box plots given in Figures 1-4, it is clear that RRT with both values of α perform very similar to OMP1.",5.1. Synthetic data sets,[0],[0]
They differ only in one run of experiment 3 where RRT1 and RRT2 suffer from a false negative.,5.1. Synthetic data sets,[0],[0]
"Further, RRT1 and RRT2 outperform CV and LAT in all the four experiments in terms of all the three metrics considered for evaluation.",5.1. Synthetic data sets,[0],[0]
"This is primarily because LAT and CV are more prone to make false positives, whereas RRT1 and RRT2 does not report any false positives.",5.1. Synthetic data sets,[0],[0]
OMP2 consistently made false negatives which explains its poor performance in terms of l2 error.,5.1. Synthetic data sets,[0],[0]
"We have observed that once the SNR is made slightly higher, OMP2 delivers a performance similar to OMP1.",5.1. Synthetic data sets,[0],[0]
Also note that RRT with two significantly different choices of α viz.,5.1. Synthetic data sets,[0],[0]
α = 1/ √ n and α = 1/ log(n) delivered similar performances.,5.1. Synthetic data sets,[0],[0]
This observation is in agreement with the claim of asymptotic tuning freeness made in Remark 5.,5.1. Synthetic data sets,[0],[0]
Similar trends are also visible in the simulation results presented in supplementary materials.,5.1. Synthetic data sets,[0],[0]
"We next consider the application of sparse estimation techniques including RRT to identify outliers in low dimensional or full column rank (i.e., n > p) real life data sets, an approach first considered in (Mitra et al., 2010; 2013).",5.2. Outlier detection in real data sets,[0],[0]
"Consider a robust regression model of the form y = Xβ + w + gout with usual interpretations for X, β and w.",5.2. Outlier detection in real data sets,[0],[0]
The extra term gout ∈,5.2. Outlier detection in real data sets,[0],[0]
Rn represents the gross errors in the regression model that cannot be modelled using the distributional assumptions on w. Outlier detection problem in linear regression refers to the identification of the support Sg = supp(gout).,5.2. Outlier detection in real data sets,[0],[0]
"Since X has full rank, one can always annihilate the signal component Xβ by projecting onto a subspace orthogonal to span(X).",5.2. Outlier detection in real data sets,[0],[0]
"This will result in a simple linear regression model of the form given by
ỹ =",5.2. Outlier detection in real data sets,[0],[0]
(In −XX†)y =,5.2. Outlier detection in real data sets,[0],[0]
"(In −XX†)gout + (In −XX†)w, (7) i.e., identifying Sg in robust regression is equivalent to a sparse support identification problem in linear regression.",5.2. Outlier detection in real data sets,[0],[0]
"Even though this is a regression problem with n observa-
tions and n variables, the design matrix (In−XX†) in (7) is rank deficient (i.e., rank(In−XX†) = n−rank(X) < n).",5.2. Outlier detection in real data sets,[0],[0]
"Hence, classical techniques based on LS are not useful for identifying Sg.",5.2. Outlier detection in real data sets,[0],[0]
"Since card(Sg) and variance of w are unknown, we only consider the application of RRT, OMP with CV and LAT in detecting Sg .",5.2. Outlier detection in real data sets,[0],[0]
We consider four widely studied real life data sets and compare the outliers identified by these algorithms with the existing and widely replicated studies on these data sets.,5.2. Outlier detection in real data sets,[0],[0]
More details on these data sets are given in the supplementary materials.,5.2. Outlier detection in real data sets,[0],[0]
"The outliers detected by the aforementioned algorithms and outliers reported in existing literature are tabulated in TABLE 1.
",5.2. Outlier detection in real data sets,[0],[0]
"Among the four data sets considered, outliers detected by RRT and existing results are in consensus in two data sets viz.",5.2. Outlier detection in real data sets,[0],[0]
Stack loss and Stars data sets.,5.2. Outlier detection in real data sets,[0],[0]
"In AR2000 data set, RRT identifies all the outliers.",5.2. Outlier detection in real data sets,[0],[0]
"However, RRT also include observations 14 and 50 as outliers.",5.2. Outlier detection in real data sets,[0],[0]
These identifications can be potential false positives.,5.2. Outlier detection in real data sets,[0],[0]
"In Brain and Body Weight data set, RRT agrees with the existing results in 4 observations.",5.2. Outlier detection in real data sets,[0],[0]
"However, RRT misses two observations viz.",5.2. Outlier detection in real data sets,[0],[0]
14 and 17 which are claimed to be outliers by existing results.,5.2. Outlier detection in real data sets,[0],[0]
LAT agrees with RRT in all data sets except the stack loss data set where it missed outlier indices 1 and 3.,5.2. Outlier detection in real data sets,[0],[0]
CV correctly identified all the outliers identified by other algorithms in all four data sets.,5.2. Outlier detection in real data sets,[0],[0]
"However, it made lot of false positives in three data sets.",5.2. Outlier detection in real data sets,[0],[0]
"To summarize, among all the three algorithms considered, RRT delivered an outlier detection performance which is the most similar to the results reported in literature.",5.2. Outlier detection in real data sets,[0],[0]
This article proposed a novel signal and noise statistics independent sparse recovery technique based on OMP called residual ratio thresholding and derived finite and large sample guarantees for the same.,6. Conclusions,[0],[0]
Numerical simulations in real and synthetic data sets demonstrates a highly competitive performance of RRT when compared to OMP with a priori knowledge of signal and noise statistics.,6. Conclusions,[0],[0]
The RRT technique developed in this article can be used to operate sparse recovery techniques that produce a monotonic sequence of support estimates in a signal and noise statistics oblivious fashion.,6. Conclusions,[0],[0]
"However, the support estimate sequence generated by algorithms like LASSO, DS, SP etc. are not monotonic in nature.",6. Conclusions,[0],[0]
"Hence, extending the concept of RRT to operate sparse estimation techniques that produce non monotonic support sequence in a signal and noise statistics oblivious fashion is an interesting direction of future research.",6. Conclusions,[0],[0]
Orthogonal matching pursuit (OMP) is a widely used algorithm for recovering sparse high dimensional vectors in linear regression models.,abstractText,[0],[0]
The optimal performance of OMP requires a priori knowledge of either the sparsity of regression vector or noise statistics.,abstractText,[0],[0]
Both these statistics are rarely known a priori and are very difficult to estimate.,abstractText,[0],[0]
"In this paper, we present a novel technique called residual ratio thresholding (RRT) to operate OMP without any a priori knowledge of sparsity and noise statistics and establish finite sample and large sample support recovery guarantees for the same.",abstractText,[0],[0]
Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a performance comparable to OMP with a priori knowledge of sparsity and noise statistics.,abstractText,[0],[0]
Signal and Noise Statistics Oblivious Orthogonal Matching Pursuit ,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4822–4828 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4822",text,[0],[0]
An important aspect of natural language processing involves understanding events mentioned in text.,1 Introduction,[0],[0]
"Towards this end, event detection (ED) is the task of locating event triggers (usually verbs or nouns) within a given text, and classifying them among a given set of event types.",1 Introduction,[0],[0]
This task remains challenging due to the inherent ambiguity and flexibility of natural languages.,1 Introduction,[0],[0]
"The current state-of-the-art methods for ED have involved applying deep learning (DL) models to automatically extract feature representations of the text, and then treating the task as a classification problem (Chen et al., 2015; Nguyen and Grishman, 2015b).
",1 Introduction,[0],[0]
The major intuition in this paper is that the task of ED is closely related to the task of word sense disambiguation (WSD) whose datasets can help to improve the performance of the DL models for ED.,1 Introduction,[0],[0]
"This is due to the goal of WSD to determine the sense of a word within a particular context, given a set of possible senses that the word can take on.",1 Introduction,[0],[0]
"Our intuition is based on the two following aspects:
(i) Similar Context Modeling:",1 Introduction,[0],[0]
"Given a word in a context/sentence, both ED and WSD models need
to select/predict a correct label in a list of candidate labels for the word.",1 Introduction,[0],[0]
"For WSD, the candidate labels are the possible senses (e.g, sense ids in WordNet) that the word of interest can have, while for ED, they are the set of predetermined event types (e.g, the event subtypes in the ACE 2005 dataset1).",1 Introduction,[0],[0]
"Consider the word “fired” in the following sentence as an example:
The boss fired his secretary today.",1 Introduction,[0],[0]
"For WSD, there are 12 possible senses for the verb “fire” in WordNet in which the correct label for the word “fired” in this case is the sense id “fire%2:41:00::” (i.e, “terminate the employment of ”).",1 Introduction,[0],[0]
"The ED task in the ACE 2005 dataset, on the other hand, involves 33 possible event subtypes with “End-Position” as the correct event subtype/label for the word “fired” in our example.
",1 Introduction,[0],[0]
"In order to make such label predictions, both ED and WSD need to model the word itself and its context (i.e, the words “fired”, “boss”, and “secretary” in the example).",1 Introduction,[0],[0]
"This similar modeling allows the same DL model to be adopted for both ED and WSD, facilitating the use of WSD data to improve the feature representations for ED via parameter/representation tying.
",1 Introduction,[0],[0]
"(ii) Close Semantic Consideration: As there are some overlaps between the semantic differentiation in WSD and ED, the knowledge/information from WSD about a particular word in a context can help to make a better prediction for that word in ED.",1 Introduction,[0],[0]
"For instance, in the example above, the knowledge from WSD that the word “fired” is referring to a termination of employment would clearly help ED to identify “End-Position” as the correct event type (rather than the incorrect event type “Attack”) for “fired” in this case.
",1 Introduction,[0],[0]
"How can we exploit this intuition to improve the performance of the DL models for ED with WSD
1 https://www.ldc.upenn.edu/collaborations/past-projects/
ace
data?",1 Introduction,[0],[0]
"In this work, we propose a novel method based on representation matching to transfer the knowledge learned from the WSD data to the DL models for ED.",1 Introduction,[0],[0]
"In particular, two separate deep learning models are employed to model the context for WSD and ED.",1 Introduction,[0],[0]
"The two models share the network architecture, but involve different parameters that are specific to the tasks.",1 Introduction,[0],[0]
"We then transfer the knowledge from the WSD network to the ED network by ensuring that the feature representations learned by the two networks on the same contexts are similar to each other.
",1 Introduction,[0],[0]
We demonstrate the effectiveness of the proposed method on two widely used datasets for ED.,1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to study the transfer learning/multi-task learning methods for WSD and ED with DL.",1 Introduction,[0],[0]
"We consider the typical setting where we have two separate datasets Dwsd = {Wwsdi , pwsdi , ywsdi } for WSD and Ded = {W edi , pedi , yedi } for ED.",2 Model,[0],[0]
"Here, W edi is the i-the sentence of D
ed, pedi is the index of the word of interest for event type prediction in W edi , and y ed i is the corresponding event type label.",2 Model,[0],[0]
"The same conventions apply for Wwsdi , p wsd",2 Model,[0],[0]
"i , y wsd i .",2 Model,[0],[0]
"Also, let Y
wsd and Y ed be the label sets for WSD and ED respectively (i.e, ywsdi ∈ Y wsd and yedi ∈",2 Model,[0],[0]
Y ed).,2 Model,[0],[0]
"Our goal is to transfer the knowledge learned from the Dwsd dataset to improve the performance of the ED models trained on the Ded dataset (multi-task learning).
",2 Model,[0],[0]
"In the following, we will first describe the deep learning architectures to transform the sentences W in the datasets Dwsd and Ded into representation vectors.",2 Model,[0],[0]
We only focus on the deep learning architectures proposed for ED in the literature to achieve compatible comparisons for ED.,2 Model,[0],[0]
The proposed multi-task learning method for ED with the WSD dataset will follow.,2 Model,[0],[0]
Consider a sentence W in the datasets Dwsd or Ded that is represented as a sequence of tokens W =,2.1 Computing the Feature Representations,[0],[0]
"[w0, w1, . .",2.1 Computing the Feature Representations,[0],[0]
.,2.1 Computing the Feature Representations,[0],[0]
", wt].",2.1 Computing the Feature Representations,[0],[0]
Let p be the index of the word of interest in this sentence.,2.1 Computing the Feature Representations,[0],[0]
"The context for wp in W is constructed by taking the word itself, the n preceding words, and the n following words (padding or truncating when necessary).",2.1 Computing the Feature Representations,[0],[0]
The tokens in the context are re-indexed to form an instance V =,2.1 Computing the Feature Representations,[0],[0]
"[v0, v1, . . .",2.1 Computing the Feature Representations,[0],[0]
", vn, . . .",2.1 Computing the Feature Representations,[0],[0]
", v2n−1, v2n],
where vn corresponds to wp in W .",2.1 Computing the Feature Representations,[0],[0]
"Encoding The first step to prepare the instance V for the deep learning models is to map each token vj in V into two real-valued vectors, which are then concatenated to form a vector representation xj for vj (Nguyen and Grishman, 2015b; Chen et al., 2015):
1.",2.1 Computing the Feature Representations,[0],[0]
"The word embedding of vj obtained by looking up the token vj in the pre-trained word embedding table (Mikolov et al., 2013a).
2.",2.1 Computing the Feature Representations,[0],[0]
The position embedding vector for vj : obtained by looking up the relative distance,2.1 Computing the Feature Representations,[0],[0]
j − n of vj with respect to the token of interest vn in a position embedding table (randomly initialized),2.1 Computing the Feature Representations,[0],[0]
"(Chen et al., 2015; Nguyen and Grishman, 2015a).
",2.1 Computing the Feature Representations,[0],[0]
"It is important to note that, different from the prior works (Nguyen and Grishman, 2015b; Liu et al., 2017), we do not include the entity type label of each token into its representation.",2.1 Computing the Feature Representations,[0],[0]
"This is a more realistic setting for our work as the golden entity mentions do not always exist in practice, especially for the datasets in WSD.
",2.1 Computing the Feature Representations,[0],[0]
"Once each token vj is converted into the representation vector xj , the instance V becomes a sequence of vectors X =",2.1 Computing the Feature Representations,[0],[0]
"[x0, x1, . . .",2.1 Computing the Feature Representations,[0],[0]
", xn, . . .",2.1 Computing the Feature Representations,[0],[0]
", x2n−1, x2n] that would be fed into the one of the following deep learning models to learn a feature representation R for V .
",2.1 Computing the Feature Representations,[0],[0]
"Typical Deep Learning Models for ED
1.",2.1 Computing the Feature Representations,[0],[0]
CNN:,2.1 Computing the Feature Representations,[0],[0]
"This is the convolutional neural networks in(Nguyen and Grishman, 2015b; Chen et al., 2015).",2.1 Computing the Feature Representations,[0],[0]
It features convolution operations that are performed over the k consecutive vectors (k-grams) inX and followed by a max-pooling layer to generate the representation vector R for V .,2.1 Computing the Feature Representations,[0],[0]
"Multiple window values k are used to enhance the coverage of the model over the hidden k-grams in the context.
",2.1 Computing the Feature Representations,[0],[0]
"2. NCNN (Nguyen and Grishman, 2016d):",2.1 Computing the Feature Representations,[0],[0]
This model is similar to CNN.,2.1 Computing the Feature Representations,[0],[0]
"The only difference is instead of running the convolution over the k consecutive vectors, NCNN convolutes over the k arbitrarily non-consecutive k vectors in V .",2.1 Computing the Feature Representations,[0],[0]
"This helps NCNN to explicitly model the non-consecutive words in the context to improve ED.
3.",2.1 Computing the Feature Representations,[0],[0]
BiRNN:,2.1 Computing the Feature Representations,[0],[0]
"This is the bidirectional recurrent neural network (RNN) for event extraction in (Nguyen et al., 2016a).",2.1 Computing the Feature Representations,[0],[0]
"The model is
composed of two recurrent neural networks (RNN), where one runs forward and the other runs backward through the input sequence V .",2.1 Computing the Feature Representations,[0],[0]
The hidden vectors produced by the two networks are then concatenated at each position in the context.,2.1 Computing the Feature Representations,[0],[0]
The vector at the position of n for the word of interest is used as the representation vectorR for V .,2.1 Computing the Feature Representations,[0],[0]
"Due to the property of RNN, R encodes the information over the whole input V with a greater focus on vn.
4.",2.1 Computing the Feature Representations,[0],[0]
CNN+BiRNN:,2.1 Computing the Feature Representations,[0],[0]
"In this model (Feng et al., 2016), X is passed through both a CNN and a BiRNN whose results are concatenated to produce the hidden representation R for ED.",2.1 Computing the Feature Representations,[0],[0]
"The expectation is to take advantage of the modeling abilities from both the CNN and BiRNN architectures for ED.
",2.1 Computing the Feature Representations,[0],[0]
"In practice, the representation vector R (obtained from one of the deep learning models above) is also concatenated with the word embeddings of the tokens surrounding the token of interest wn to improve its expressiveness (Chen et al., 2015; Nguyen and Grishman, 2016d).",2.1 Computing the Feature Representations,[0],[0]
"We would use this extended version when we refer to R in the following.
",2.1 Computing the Feature Representations,[0],[0]
"In the final step, the representation vector R is fed into a feed-forward neural network followed by a softmax layer to perform predictions for ED and WSD.
",2.1 Computing the Feature Representations,[0],[0]
"For convenience, we denote the whole process that a DL model M is used to compute the representation vector R for the input sentence W with the token index p of interest as: R =M(W,p).",2.1 Computing the Feature Representations,[0],[0]
The previous section has described the deep learning methods that can be employed to train the models for ED and WSD separately.,2.2 Multi-task Learning Models,[0],[0]
"This section presents our proposed method to transfer the knowledge from the WSD dataset to improve the performance for ED.
",2.2 Multi-task Learning Models,[0],[0]
"A typical method for transfer learning/multitask learning in NLP is to alternate the training process for the parameter-shared models of the related tasks (possibly with different datasets) (Guo et al., 2016; Li et al., 2015; Liu et al., 2016).",2.2 Multi-task Learning Models,[0],[0]
"For instance, in (Guo et al., 2016), the authors use the same deep learning model to learn the feature representations for the text inputs of two related tasks.",2.2 Multi-task Learning Models,[0],[0]
This is then followed by task-specific output layers to perform the corresponding tasks.,2.2 Multi-task Learning Models,[0],[0]
"Note that
the two tasks in (Guo et al., 2016) are provided with two different datasets of different text inputs, thereby being similar to the setting we consider in this work.",2.2 Multi-task Learning Models,[0],[0]
"In order to learn the parameters for this model, in each iteration, (Guo et al., 2016) select one of the tasks with some probabilities, sample a mini-batch of examples in the dataset of the chosen task, and update the model parameters using the objective function specific to the chosen task.",2.2 Multi-task Learning Models,[0],[0]
"Consequently, the model parameters for feature representation learning are updated at every iteration while only the model parameters in the output layer for the chosen task are updated at the current iteration.
",2.2 Multi-task Learning Models,[0],[0]
"It has been demonstrated in (Guo et al., 2016) that the alternating method (called ALT) is more effective than pre-training the network on a related task and fine-tuning it on the expected task.",2.2 Multi-task Learning Models,[0],[0]
We thereby consider ALT as the baseline for multitask learning in our work.,2.2 Multi-task Learning Models,[0],[0]
"However, we argue that this baseline is not effective enough to transfer the knowledge from the WSD dataset to ED in our case.",2.2 Multi-task Learning Models,[0],[0]
This stems from its employment of a single DL model to induce the representations for the text inputs in both tasks.,2.2 Multi-task Learning Models,[0],[0]
"In our case of WSD and ED, although there are some overlap between the semantic differentiation of the two tasks, the labels in the WSD datasets (i.e, the sense ids) tend to be more fine-grained and exhaustive than those in ED.",2.2 Multi-task Learning Models,[0],[0]
"For instance, for the word “fire”, there might be 12 WSD labels for it in WordNet while the number of possible event types for “fire” in the ACE 2005 dataset is only 2 (i.e, “End-Position” and “Attack”).",2.2 Multi-task Learning Models,[0],[0]
"Eventually, if a single DL model is used to compute the representations for the text inputs in both WSD and ED, the model would suffer from a confusion to distinguish such subtlety in the semantic differentiation.
",2.2 Multi-task Learning Models,[0],[0]
"In order to overcome this issue, we propose to employ two versions Mwsd and M ed of the same DL model (with different model parameters) to compute the feature representations for WSD and ED respectively.",2.2 Multi-task Learning Models,[0],[0]
We then transfer the knowledge from Mwsd to M ed by encouraging the representations generated by the two versions Mwsd and M ed on the same text inputs to be similar.,2.2 Multi-task Learning Models,[0],[0]
"Formally, let (W t, pt, yt) be an example in the Dwsd or Ded dataset (t ∈ {wsd, ed}).",2.2 Multi-task Learning Models,[0],[0]
"Also, let Rwsd and Red be the representations for (W t, pt) induced by Mwsd and M ed respectively:
Rwsd =Mwsd(W t, pt), Red =M ed(W t, pt)
",2.2 Multi-task Learning Models,[0],[0]
"Such representation vectors are then followed by a task-specific output layer F t (i.e, feed-forward neural networks followed by a softmax layer) to compute the probability distribution over the possible labels for (W t, pt): P t(Y t|Rt)",2.2 Multi-task Learning Models,[0],[0]
"= F t(Rt) where Y t is the label set for the t task.
",2.2 Multi-task Learning Models,[0],[0]
"If the two models Mwsd and M ed were trained separately, the objective function for the t task for the current example would be the negative loglikelihood: Ct(W t, pt, yt) =",2.2 Multi-task Learning Models,[0],[0]
− logP t(yt|Rt).,2.2 Multi-task Learning Models,[0],[0]
"In this work, instead of just optimizing this objective, we optimize the joint function:
Ct(W t, pt, yt) =",2.2 Multi-task Learning Models,[0],[0]
"− logP t(yt|Rt)
+ λ 1
dR dR∑ i=0 ( Rwsdi −Redi )2 where λ is a trade-off parameter and dR is the dimension of the representation vectors.
",2.2 Multi-task Learning Models,[0],[0]
"The second term in the joint objective function enforces that the feature representations learned by Mwsd and M ed on the same input context (W t, pt) are close to each other (t ∈ {wsd, ed}).",2.2 Multi-task Learning Models,[0],[0]
"One the one hand, this representation matching schema helps the two models to communicate to each other so the knowledge from one model can be passed to the other one.",2.2 Multi-task Learning Models,[0],[0]
"On the other hand, the use of two separate models leaves a flexibility for the models to induce the task-specific structures.
",2.2 Multi-task Learning Models,[0],[0]
"Presumably, the objective function (2.2) can simultaneously improve the performance for both tasks of consideration.",2.2 Multi-task Learning Models,[0],[0]
"However, in our case of ED and WSD, it turns out this mechanism actually worsen the performance of the WSD models that were trained separately.",2.2 Multi-task Learning Models,[0],[0]
"We attribute this to the fact that the semantic differentiation in ED is more coarse-grained that that of WSD, causing the ineffectiveness of the datasets for ED to improve WSD performance.",2.2 Multi-task Learning Models,[0],[0]
"Eventually, we will just focus on the ED performance in the experiments.",2.2 Multi-task Learning Models,[0],[0]
"We use the Semcor dataset (Miller et al., 1994) as the dataset for WSD in this work.",3.1 Parameters and Datasets,[0],[0]
"This dataset was extracted from the Brown Corpus, and manually annotated with WordNet senses.",3.1 Parameters and Datasets,[0],[0]
"We evaluate the models on two different datasets for ED:
1.",3.1 Parameters and Datasets,[0],[0]
ACE 2005:,3.1 Parameters and Datasets,[0],[0]
This dataset has 33 event subtypes.,3.1 Parameters and Datasets,[0],[0]
"We use the same data split with
the prior work (Chen et al., 2015; Nguyen and Grishman, 2015b).",3.1 Parameters and Datasets,[0],[0]
"In particular, 40 newswire documents are used for testing, 30 other documents are reserved for validation, and the 529 remaining documents form the training data.
",3.1 Parameters and Datasets,[0],[0]
2.,3.1 Parameters and Datasets,[0],[0]
TAC 2015:,3.1 Parameters and Datasets,[0],[0]
"This dataset was released in the Event Nugget Detection Evaluation of the 2015 Text Analysis Conference (TAC) (Mitamura et al., 2015).",3.1 Parameters and Datasets,[0],[0]
It comes with 38 event subtypes.,3.1 Parameters and Datasets,[0],[0]
We follow the data split in the official evaluation to achieve compatible comparison.,3.1 Parameters and Datasets,[0],[0]
"As TAC 2015 does not have a development set, we use the best parameters tuned on ACE 2005 for the experiments with TAC 2015.
",3.1 Parameters and Datasets,[0],[0]
"We use the pre-trained word embeddings provided by (Nguyen and Grishman, 2016d).",3.1 Parameters and Datasets,[0],[0]
"For CNN, NCNN and CNN+BiRNN, we employ filter sizes of {2, 3, 4, 5} with 300 filters for each size as in (Nguyen and Grishman, 2015b), while Gated Recurrent Units (Cho et al., 2014) with 300 hidden units are applied in BiRNN and CNN+BiRNN (as do (Nguyen and Grishman, 2016d)).",3.1 Parameters and Datasets,[0],[0]
"For the other parameters, the best values suggested by the development data include: a dropout rate of 0.5, a feed-forward neural network with one hidden layer of 1200 hidden units for the output layers, and the penalty rate λ of 0.01 for both CNN and BiRNN, 0.6 for NCNN, and 0.7 for CNN+BiRNN in the proposed transfer learning method (called MATCHING).",3.1 Parameters and Datasets,[0],[0]
"For simplicity, the same hyper-parameters are used for the two versions of the same network architecture in the MATCHING method.",3.1 Parameters and Datasets,[0],[0]
"We utilize Adadelta (Zeiler, 2012) with back-propagation to train the models in this work.",3.1 Parameters and Datasets,[0],[0]
"In this section, we compare the proposed MATCHING method with the transfer learning baseline ALT in (Guo et al., 2016) and the separate training mechanism for ED (called SEPARATE) employed in the previous work for ED (Chen et al., 2015; Nguyen and Grishman, 2015b).",3.2 Experiments,[0],[0]
"Note that in the SEPARATE method, the models are only trained on the datasets for ED without utilizing any transfer learning techniques with external datasets.",3.2 Experiments,[0],[0]
"We report the performance when each of the DL methods in Section 2.1 is used as the network to learn the feature representations for ED and WSD.
",3.2 Experiments,[0],[0]
"Tables 1 and 2 present the performance (i.e, F1 scores) of the models on the ACE 2005 and TAC 2015 datasets respectively.",3.2 Experiments,[0],[0]
The first observation is that the proposed transfer learning method MATCHING is consistently better than the baseline method ALT across different deep learning models and datasets with large performance gap.,3.2 Experiments,[0],[0]
This is significantly with p < 0.05 and confirms our hypothesis in Section 2.2 about the advantage of the proposed MATCHING over the alternating training method ALT for ED and WSD.,3.2 Experiments,[0],[0]
"In fact, the performance of the ALT method is even worse than the traditional SEPARATE method also over different network architectures and datasets.",3.2 Experiments,[0],[0]
"Consequently, training a single deep learning model on a combination of ED and WSD data (as in ALT) does not automatically enable the model to learn to exploit the similar structures of the two tasks.",3.2 Experiments,[0],[0]
"In contrast, it hinders the model’s ability to effectively extract hidden representations for ED.
",3.2 Experiments,[0],[0]
"Comparing MATCHING and SEPARATE, we see that MATCHING helps to improve SEPARATE with respect to difference choices of the DL models.",3.2 Experiments,[0],[0]
The performance improvement is significant for CNN and BiRNN on ACE 2005 and for all the models on TAC 2015.,3.2 Experiments,[0],[0]
"Such results demonstrate the effectiveness of the WSD dataset for ED and the ability of the proposed method MATCHING to promote knowledge transferring between WSD and ED to improve ED performance.
",3.2 Experiments,[0],[0]
"Regarding the best reported performance, our best performance on ACE (i.e, 71.2% with CNN) is comparable with the recent state-of-the-art performance (i.e, Table 1).",3.2 Experiments,[0],[0]
"However, we note that such work heavily relies on the manual annotation of the entity mentions in the documents.",3.2 Experiments,[0],[0]
Our current work do not employ such information to better reflect the realistic setting.,3.2 Experiments,[0],[0]
"For the TAC 2015 dataset, our best performance is 60.7% with CNN+BiRNN although the performance of the other models is also very close.",3.2 Experiments,[0],[0]
"This performance is better than the best performance that has been reported on the TAC 2015 (i.e, Table 2).",3.2 Experiments,[0],[0]
"Prior works on ED include statistical models with manual feature engineering(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; Li et al., 2013; Venugopal et al., 2014; Li et al., 2015), followed by neural network models, such as CNNs (Nguyen and Grishman, 2015b; Chen et al., 2015; Nguyen
et al., 2016b,e; Chen et al., 2017), RNNs (Nguyen et al., 2016a; Jagannatha and Yu, 2016), and attention-based methods (Liu et al., 2017; Nguyen and Nguyen, 2018b).
",4 Related Work,[0],[0]
"A similar trend exists in methods proposed for WSD, with feature based methods (Miller et al., 1994; Zhong and Ng, 2010; Taghipour and Ng, 2015) succeeded recently by deep learning methods (Yuan et al., 2016; Raganato et al., 2017).
",4 Related Work,[0],[0]
"For multi-task learning in NLP, methods have been proposed for jointly modeling structured prediction tasks (Hatori et al., 2012; Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Lluı́s et al., 2013; Duong et al., 2015), and for sequence-to-sequence problems (Dong et al., 2015; Luong et al., 2015; Liu et al., 2016; Klerke et al., 2016).",4 Related Work,[0],[0]
"The prior work to solve multiple NLP tasks using an unified architecture includes (Collobert and Weston, 2008; Guo et al., 2016).",4 Related Work,[0],[0]
"We present a method that improves the performance of deep learning models for ED by training two different versions of the same network architecture for ED and WSD, while encouraging the knowledge transfer between the two versions via representation matching.",5 Conclusion,[0],[0]
The proposed method produces better results across a variety of deep learning models.,5 Conclusion,[0],[0]
Event detection (ED) and word sense disambiguation (WSD) are two similar tasks in that they both involve identifying the classes (i.e. event types or word senses) of some word in a given sentence.,abstractText,[0],[0]
"It is thus possible to extract the knowledge hidden in the data for WSD, and utilize it to improve the performance on ED.",abstractText,[0],[0]
"In this work, we propose a method to transfer the knowledge learned on WSD to ED by matching the neural representations learned for the two tasks.",abstractText,[0],[0]
Our experiments on two widely used datasets for ED demonstrate the effectiveness of the proposed method.,abstractText,[0],[0]
Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 137–149 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
137",text,[0],[0]
Image captioning attracts considerable attention in both natural language processing and computer vision.,1 Introduction,[0],[0]
The task aims to generate a description in natural language grounded on the input image.,1 Introduction,[0],[0]
It is a very challenging yet interesting task.,1 Introduction,[0],[0]
"On the one hand, it has to identify the objects in the image, associate the objects, and express them in a fluent sentence, each of which is a difficult subtask.",1 Introduction,[0],[0]
"On the other hand, it combines two important fields in artificial intelligence, namely, natural language processing and computer vision.",1 Introduction,[0],[0]
"More importantly, it has a wide range of applications, including text-based image retrieval, helping visually impaired people see (Wu et al., 2017), humanrobot interaction (Das et al., 2017), etc.
Models based on the encoder-decoder framework have shown success in image captioning.",1 Introduction,[0],[0]
"According to the pivot representation, they can be
∗Equal Contributions 1",1 Introduction,[0],[0]
"The code is available at https://github.com/
lancopku/simNet
roughly categorized into models based on visual information (Vinyals et al., 2015; Chen and Zitnick, 2015; Mao et al., 2014; Karpathy and Li, 2015, 2017), and models based on conceptual information (Fang et al., 2015; You et al., 2016; Wu et al., 2016).",1 Introduction,[0],[0]
"The later explicitly provides the visual words (e.g. dog, sit, red) to the decoder instead of the image features, and is more effective in image captioning according to the evaluation on benchmark datasets.",1 Introduction,[0],[0]
"However, the models based on conceptual information have a major drawback that it is hard for the model to associate the details with the specific objects in the image, because the visual words are inherently unordered in semantics.",1 Introduction,[0],[0]
Figure 1 shows an example.,1 Introduction,[0],[0]
"For semantic attention, although open is provided as a visual word, due to the insufficient use of visual information, the model gets confused about what objects open should be associated with and thus discards open in the caption.",1 Introduction,[0],[0]
"The model may even associate the details incorrectly, which is the case
for the position of the dog.",1 Introduction,[0],[0]
"In contrast, models based on the visual information often are accurate in details but have difficulty in describing the image comprehensively and tend to only describe a subregion.
",1 Introduction,[0],[0]
"In this work, we get the best of both worlds and integrate visual attention and semantic attention for generating captions that are both detailed and comprehensive.",1 Introduction,[0],[0]
We propose a Stepwise ImageTopic Merging Network as the decoder to guide the information flow between the image and the extracted topics.,1 Introduction,[0],[0]
"At each time step, the decoder first extracts focal information from the image.",1 Introduction,[0],[0]
"Then, it decides which topics are most probable for the time step.",1 Introduction,[0],[0]
"Finally, it attends differently to the visual information and the conceptual information to generate the output word.",1 Introduction,[0],[0]
"Hence, the model can efficiently merge the two kinds of information, leading to outstanding results in image captioning.
",1 Introduction,[0],[0]
"Overall, the main contributions of this work are:
• We propose a novel approach that can effectively merge the information in the image and the topics to generate cohesive captions that are both detailed and comprehensive.",1 Introduction,[0],[0]
"We refine and combine two previous competing attention mechanisms, namely visual attention and semantic attention, with an importancebased merging gate that effectively combines
and balances the two kinds of information.
",1 Introduction,[0],[0]
"• The proposed approach outperforms the state-of-the-art methods substantially on two benchmark datasets, Flickr30k and COCO, in terms of SPICE, which correlates the best with human judgments.",1 Introduction,[0],[0]
Systematic analysis shows that the merging gate contributes the most to the overall improvement.,1 Introduction,[0],[0]
A large number of systems have been proposed for image captioning.,2 Related Work,[0],[0]
"Neural models based on the encoder-decoder framework have been attracting increased attention in the last few years in several multi-discipline tasks, such as neural image/video captioning (NIC) and visual question answering (VQA) (Vinyals et al., 2015; Karpathy and Li, 2015; Venugopalan et al., 2015; Zhao et al., 2016; Zhang et al., 2017).",2 Related Work,[0],[0]
"State-of-theart neural approaches (Anderson et al., 2018; Liu et al., 2018; Lu et al., 2018) incorporate the attention mechanism in machine translation (Bahdanau et al., 2014) to generate grounded image captions.",2 Related Work,[0],[0]
"Based on what they attend to, the models can be categorized into visual attention models and semantic attention models.
",2 Related Work,[0],[0]
Visual attention models pay attention to the image features generated by CNNs.,2 Related Work,[0],[0]
"CNNs are typically pre-trained on the image recognition task to extract general visual signals (Xu et al., 2015; Chen et al., 2017; Lu et al., 2017).",2 Related Work,[0],[0]
The visual attention is expected to find the most relevant image regions in generating the caption.,2 Related Work,[0],[0]
"Most recently, image features based on predicted bounding boxes are used (Anderson et al., 2018; Lu et al., 2018).",2 Related Work,[0],[0]
The advantages are that the attention no longer needs to find the relevant generic regions by itself but instead find relevant bounding boxes that are object orientated and can serve as semantic guides.,2 Related Work,[0],[0]
"However, the drawback is that predicting bounding boxes is difficult, which requires large datasets (Krishna et al., 2017) and complex models (Ren et al., 2015, 2017a).
",2 Related Work,[0],[0]
"Semantic attention models pay attention to a predicted set of semantic concepts (Fang et al., 2015; You et al., 2016; Wu et al., 2016).",2 Related Work,[0],[0]
"The semantic concepts are the most frequent words in the captions, and the extractor can be trained using various methods but typically is only trained on the given image captioning dataset.",2 Related Work,[0],[0]
"This kind
of approach can be seen as the extension of the earlier template-based slotting-filling approaches (Farhadi et al., 2010; Kulkarni et al., 2013).
",2 Related Work,[0],[0]
"However, few work studies how to combine the two kinds of attention models to take advantage of both of them.",2 Related Work,[0],[0]
"On the one hand, due to the limited number of visual features, it is hard to provide comprehensive information to the decoder.",2 Related Work,[0],[0]
"On the other hand, the extracted semantic concepts are unordered, making it hard for the decoder to portray the details of the objects correctly.
",2 Related Work,[0],[0]
This work focuses on combining the visual attention and the semantic attention efficiently to address their drawbacks and make use of their merits.,2 Related Work,[0],[0]
"The visual attention is designed to focus on the attributes and the relationships of the objects, while the semantic attention only includes words that are objects so that the extracted topics could be more accurate.",2 Related Work,[0],[0]
The combination is controlled by the importance-based merging mechanism that decides at each time step which kind of information should be relied on.,2 Related Work,[0],[0]
The goal is to generate image captions that are both detailed and comprehensive.,2 Related Work,[0],[0]
"Our proposed model consists of an image encoder, a topic extractor, and a stepwise merging decoder.",3 Approach,[0],[0]
Figure 3 shows a sketch.,3 Approach,[0],[0]
We first briefly introduce the image encoder and the topic extractor.,3 Approach,[0],[0]
"Then, we introduce the proposed stepwise image-topic merging decoder in detail.",3 Approach,[0],[0]
"For an input image, the image encoder expresses the image as a series of visual feature vectors V = {v1,v2, . . .",3.1 Image Encoder,[0],[0]
",vk},vi ∈",3.1 Image Encoder,[0],[0]
Rg.,3.1 Image Encoder,[0],[0]
Each feature corresponds to a different perspective of the image.,3.1 Image Encoder,[0],[0]
The visual features serve as descriptive guides of the objects in the image for the decoder.,3.1 Image Encoder,[0],[0]
"We use a
ResNet152 (He et al., 2016), which is commonly used in image captioning, to generate the visual features.",3.1 Image Encoder,[0],[0]
"The output of the last convolutional layer is used as the visual information:
V =W V,ICNN(I) (1)
where I is the input image, and W V,I shrinks the last dimension of the output.2",3.1 Image Encoder,[0],[0]
"Typically, identifying an object requires a combination of visual features, and considering the limited capacity of the visual features, it is hard for the conventional decoder to describe the objects in the image comprehensively.",3.2 Topic Extractor,[0],[0]
An advance in image captioning is to provide the decoder with the semantic concepts in the image directly so that the decoder is equipped with an overall perspective of the image.,3.2 Topic Extractor,[0],[0]
"The semantic concepts can be objects (e.g. person, car), attributes (e.g. off, electric), and relationships (e.g. using, sitting).",3.2 Topic Extractor,[0],[0]
"We only use the words that are objects in this work, the reason of which is explained later.",3.2 Topic Extractor,[0],[0]
We call such words topics.,3.2 Topic Extractor,[0],[0]
"The topic extractor concludes a list of candidate topic embeddings T = {w1,w2, . . .",3.2 Topic Extractor,[0],[0]
",wm},wi ∈",3.2 Topic Extractor,[0],[0]
"Re from the image, where e is the dimension of the topic word embeddings.",3.2 Topic Extractor,[0],[0]
"Following common practice (Fang et al., 2015; You et al., 2016), we adopt the weakly-supervised approach of Multiple Instance Learning (Zhang et al., 2006) to build a topic extractor.",3.2 Topic Extractor,[0],[0]
"Due to limited space, please refer to Fang et al. (2015) for detailed explanation.
",3.2 Topic Extractor,[0],[0]
"Different from existing work that uses all the most frequent words in the captions as valid semantic concepts or visual words, we only include the object words (nouns) in the topic word list.",3.2 Topic Extractor,[0],[0]
"Existing work relies on attribute words and rela-
2For conciseness, all the bias terms of linear transformations in this paper are omitted.
",3.2 Topic Extractor,[0],[0]
tionship words to provide visual information to the decoder.,3.2 Topic Extractor,[0],[0]
"However, it not only complicates the extracting procedure but also contributes little to the generation.",3.2 Topic Extractor,[0],[0]
"For an image containing many objects, the decoder is likely to combine the attributes with the objects arbitrarily, as such words are specific to certain objects but are provided to the decoder unordered.",3.2 Topic Extractor,[0],[0]
"In contrast, our model has visual information as additional input and we expect that the decoder should refer to the image for such kind of information instead of the extracted concepts.",3.2 Topic Extractor,[0],[0]
The essential component of the decoder is the proposed stepwise image-topic merging network.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The decoder is based on an LSTM (Hochreiter and Schmidhuber, 1997).",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"At each time step, it combines the textual caption, the attentive visual information, and the attentive conceptual information as the context for generating an output word.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The goal is achieved by three modules, the visual attention, the topic attention, and the merging gate.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Visual Attention as Output The visual attention attends to attracting parts of the image based on the state of the LSTM decoder.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In existing work (Xu et al., 2015), only the previous hidden state ht−1 ∈ Rd of the LSTM is used in computation of the visual attention:
Zt = tanh(W Z,V V ⊕W Z,hht−1) (2) αt = softmax(Ztwα,Z) (3)
where W Z,V ∈ Rk×g,W Z,h ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rk×d,wα,Z ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Rk are the learnable parameters.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We denote the matrix-vector addition as ⊕, which is calculated by adding the vector to each column of the matrix.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
αt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Rk is the attentive weights of V and the attentive visual input,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
zt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rg is calculated as
zt = V αt (4)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The visual input zt and the embedding of the previous output word yt−1 are the input of the LSTM.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"ht = LSTM( [ zt yt−1 ] ,ht−1) (5)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, there is a noticeable drawback that the previous output word yt−1, which is a much stronger indicator than the previous hidden state ht−1, is not used in the attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"As zt is used as the input, we call it input attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"To overcome that drawback, we add another attention that incorporates the current hidden state ht, which is
based on the last generated word yt−1:
Z̃t = tanh(W̃ Z,V V ⊕ W̃ Z,hht) (6)
α̃t = softmax(Z̃tw̃α,Z) (7)
z̃t = V α̃t (8)
The procedure resembles the input attention, and we call it output attention.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It is worth mentioning that the output attention is essentially the same with the spatial visual attention proposed by Lu et al. (2017).,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, they did not see it from the input-output point of view nor combine it with the input attention.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The attentive visual output is further transformed to rt = tanh(W s,zz̃t),W s,z ∈ Re×g, which is of the same dimension as the topic word embedding to simplify the following procedure.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Topic Attention,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In an image caption, different parts concern different topics.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In the existing work (You et al., 2016), the conceptual information is attended based on the previous output word:
βt = softmax(T TUyt−1) (9)
whereU ∈ Re×e,βt ∈ Rm.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The profound issue is that this approach neglects the visual information.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It should be beneficial to provide the attentive visual information when selecting topics.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The hidden state of the LSTM contains both the information of previous words and the attentive input visual information.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Therefore, the model attends to the topics based on the hidden state of the LSTM:
Qt = tanh(WQ,TT ⊕WQ,hht) (10) βt = softmax(Qtwβ,Q) (11)
where WQ,T ∈ Rm×e,WQ,h ∈ Rm×d,wβ,",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Q ∈ Rm are the parameters to be learned.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
βt ∈,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rm is the weight of the topics, from which the attentive conceptual output qt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Re is calculated:
qt = Tβt (12)
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The topic attention qt and the hidden state ht are combined as the contextual information,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"st:
st = tanh(W s,qqt +W s,hht) (13)
where W s,q ∈ Re×e,W s,h ∈ Re×d are learnable parameters.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Merging Gate,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
We have prepared both the visual information rt and the contextual information st.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
It is not reasonable to treat the two kinds of information equally when the decoder generates different types of words.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"For example, when generating descriptive words (e.g., behind, red), rt should matter more than st.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"However, when generating
object words (e.g., people, table), st is more important.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We introduce a novel score-based merging mechanism to make the model adaptively learn to adjust the balance:
γt = σ(S(st)− S(rt))",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(14) ct = γtst + (1− γt)rt (15)
where σ is the sigmoid function, γt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"[0, 1] indicates how important the topic attention is compared to the visual attention, and S is the scoring function.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The scoring function needs to evaluate the importance of the topic attention.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
Noticing that Eq. (10) and Eq.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(11) have a similar purpose, we define S similarly:
S(st) = tanh(W S,hht +W S,sst) ·wS (16) S(rt) =",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"tanh(W S,hht +W S,rrt) ·wS (17) where · denotes dot product of vectors, W S,s ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Rm×e,W S,r ∈ Rm×e are the parameters to be learned, and W S,h,ws share the weights of WQ,h,wβ,Q from Eq.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"(10) and Eq. (11), respectively.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"Finally, the output word is generated by:
yt ∼ pt = softmax(W p,cct) (18) where each value of pt ∈",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
R|D| is a probability indicating how likely the corresponding word in vocabulary D is the current output word.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"The whole model is trained using maximum log likelihood and the loss function is the cross entropy loss.
",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"In all, our proposed approach encourages the model to take advantage of all the available information.",3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
The adaptive merging mechanism makes the model weigh the information elaborately.,3.3 Stepwise Image-Topic Merging Decoder,[0],[0]
"We describe the datasets and the metrics used for evaluation, followed by the training details and the evaluation of the proposed approach.",4 Experiment,[0],[0]
There are several datasets containing images and their captions.,4.1 Datasets and Metrics,[0],[0]
"We report results on the popular Microsoft COCO (Chen et al., 2015) dataset and the Flickr30k (Young et al., 2014) dataset.",4.1 Datasets and Metrics,[0],[0]
"They contain 123,287 images and 31,000 images, respectively, and each image is annotated with 5 sentences.",4.1 Datasets and Metrics,[0],[0]
We report results using the widely-used publicly-available splits in the work of Karpathy and Li (2015).,4.1 Datasets and Metrics,[0],[0]
"There are 5,000 images each in the validation set and the test set for COCO, 1,000 images for Flickr30k.
",4.1 Datasets and Metrics,[0],[0]
"We report results using the COCO captioning evaluation toolkit (Chen et al., 2015) that reports the widely-used automatic evaluation metrics SPICE, CIDEr, BLEU, METEOR, and ROUGE.",4.1 Datasets and Metrics,[0],[0]
"SPICE (Anderson et al., 2016), which is based on scene graph matching, and CIDEr (Vedantam et al., 2015), which is based on n-gram matching, are specifically proposed for evaluating image captioning systems.",4.1 Datasets and Metrics,[0],[0]
They both incorporate the consensus of a set of references for an example.,4.1 Datasets and Metrics,[0],[0]
"BLEU (Papineni et al., 2002) and METOR (Banerjee and Lavie, 2005) are originally proposed for machine translation evaluation.",4.1 Datasets and Metrics,[0],[0]
"ROUGE (Lin and Hovy, 2003; Lin, 2004) is designed for automatic evaluation of extractive text summarization.",4.1 Datasets and Metrics,[0],[0]
"In the related studies, it is concluded that SPICE correlates the best with human judgments with a remarkable margin over the other metrics, and is expert in judging detailedness, where the other metrics show negative correlations, surprisingly; CIDEr and METEOR follows with no particular precedence, followed by ROUGE-L, and BLEU4, in that order (Anderson et al., 2016; Vedantam et al., 2015).",4.1 Datasets and Metrics,[0],[0]
"Following common practice, the CNN used is the ResNet152 model (He et al., 2016) pre-trained on ImageNet.3 There are 2048 7 × 7 feature maps, and we project them into 512 feature maps, i.e. g is 512.",4.2 Settings,[0],[0]
The word embedding size e is 256 and the hidden size d of the LSTM is 512.,4.2 Settings,[0],[0]
"We only keep caption words that occur at least 5 times in the training set, resulting in 10,132 words for COCO and 7,544 for Flickr30k.",4.2 Settings,[0],[0]
"We use the topic extractor pre-trained by Fang et al. (2015) for 1,000 concepts on COCO.",4.2 Settings,[0],[0]
We only use 568 manuallyannotated object words as topics.,4.2 Settings,[0],[0]
"For an image, only the top 5 topics are selected, which means m is 5.",4.2 Settings,[0],[0]
"The same topic extractor is used for Flickr30k, as COCO provides adequate generality.",4.2 Settings,[0],[0]
The caption words and the topic words share the same embeddings.,4.2 Settings,[0],[0]
"In training, we first train the model without visual attention (freezing the CNN parameters) for 20 epochs with the batch size of 80.",4.2 Settings,[0],[0]
The learning rate for the LSTM is 0.0004.,4.2 Settings,[0],[0]
"Then, we switch to jointly train the full model with a learning rate of 0.00001, which exponentially decays with the number of epochs so that it is halved every 50 epochs.",4.2 Settings,[0],[0]
"We also use momen-
3We use the pre-trained model from torchvision.
",4.2 Settings,[0],[0]
tum of 0.8 and weight decay of 0.999.,4.2 Settings,[0],[0]
"We use Adam (Kingma and Ba, 2014) for parameter optimization.",4.2 Settings,[0],[0]
"For fair comparison, we adopt early stop based on CIDEr within maximum 50 epochs.",4.2 Settings,[0],[0]
"We compare our approach with various representative systems on Flickr30k and COCO, including the recently proposed NBT that is the state-of-theart on the two datasets in comparable settings.",4.3 Results,[0],[0]
Table 1 shows the result on Flickr30k.,4.3 Results,[0],[0]
"As we can see, our model outperforms the comparable systems in terms of all of the metrics except BLEU-4.",4.3 Results,[0],[0]
"Moreover, our model overpasses the state-of-theart with a comfortable margin in terms of SPICE, which is shown to correlate the best with human judgments (Anderson et al., 2016).
",4.3 Results,[0],[0]
Table 2 shows the results on COCO.,4.3 Results,[0],[0]
"Among the directly comparable models, our model is arguably the best and outperforms the existing models except in terms of BLEU-4.",4.3 Results,[0],[0]
"Most encouragingly, our model is also competitive with Up-Down (Ander-
son et al., 2018), which uses much larger dataset, Visual Genome (Krishna et al., 2017), with dense annotations to train the object detector, and directly optimizes CIDEr.",4.3 Results,[0],[0]
"Especially, our model outperforms the state-of-the-art substantially in SPICE and METEOR.",4.3 Results,[0],[0]
Breakdown of SPICE Fscores over various subcategories (see Table 3) shows that our model is in dominant lead in almost all subcategories.,4.3 Results,[0],[0]
"It proves the effectiveness of our approach and indicates that our model is quite data efficient.
",4.3 Results,[0],[0]
"For the methods that directly optimize CIDEr, it is intuitive that CIDEr can improve significantly.",4.3 Results,[0],[0]
The similar improvement of BLEU-4 is evidence that optimizing CIDEr leads to more ngram matching.,4.3 Results,[0],[0]
"However, it comes to our notice that the improvements of SPICE, METEOR, and ROUGE-L are far less significant, which suggests there may be a gaming situation where the n-gram matching is wrongfully exploited by the model in reinforcement learning.",4.3 Results,[0],[0]
"As shown by Liu et al. (2017), it is most reasonable to jointly optimize
all the metrics at the same time.",4.3 Results,[0],[0]
"We also evaluate the proposed model on the COCO evaluation server, the results of which are shown in Appendix A.1, due to limited space.",4.3 Results,[0],[0]
"In this section, we analyze the contribution of each component in the proposed approach, and give examples to show the strength and the potential improvements of the model.",5 Analysis,[0],[0]
"The analysis is conducted on the test set of COCO.
",5 Analysis,[0],[0]
Topic Extraction,5 Analysis,[0],[0]
The motivation of using objects as topics is that they are easier to identify so that the generation suffers less from erroneous predictions.,5 Analysis,[0],[0]
"This can be proved by the F-score of the identified topics in the test set, which is shown in Table 4.",5 Analysis,[0],[0]
Using top-5 object words is at least as good as using top-10 all words.,5 Analysis,[0],[0]
"However, using top-10 all words introduces more erroneous visual words to the generation.",5 Analysis,[0],[0]
"As shown in Ta-
ble 5, when extracting all words, providing more words to the model indeed increases the captioning performance.",5 Analysis,[0],[0]
"However, even when top-20 all words are used, the performance is still far behind using only top-5 object words and seems to reach the performance ceiling.",5 Analysis,[0],[0]
"It proves that for semantic attention, it is also important to limit the absolute number of incorrect visual words instead of merely the precision or the recall.",5 Analysis,[0],[0]
It is also interesting to check whether using other kind of words can reach the same effect.,5 Analysis,[0],[0]
"Unfortunately, in our experiments, only using verbs or adjectives as semantic concepts works poorly.
",5 Analysis,[0],[0]
"To examine the contributions of the submodules in our model, we conduct a series of experiments.",5 Analysis,[0],[0]
The results are summarized in Table 3.,5 Analysis,[0],[0]
"To help with the understanding of the differences, we also report the breakdown of SPICE F-scores.
",5 Analysis,[0],[0]
Visual Attention,5 Analysis,[0],[0]
"Our input attention achieves similar results to previous work (Xu et al., 2015),
if not better.",5 Analysis,[0],[0]
"Using only the output attention is much more effective than using only the input attention, with substantial improvements in all metrics, showing the impact of information gap caused by delayed input in attention.",5 Analysis,[0],[0]
"Combining the input attention and the output attention can further improve the results, especially in color and size descriptions.
",5 Analysis,[0],[0]
Topic Attention,5 Analysis,[0],[0]
"As expected, compared with visual attention, the topic attention is better at identifying objects but worse at identifying attributes.",5 Analysis,[0],[0]
"We also apply the merging gate to the topic attention, but it now merges qt and ht instead of st and rt.",5 Analysis,[0],[0]
"With the merging gate, the model can balance the information in caption text and extracted topics, resulting in better overall scores.",5 Analysis,[0],[0]
"While it overpasses the conventional visual attention, it lags behind the output attention.
",5 Analysis,[0],[0]
"Merging Gate Combing the visual attention and the topic attention directly indeed results in a huge boost in performance, which confirms our motivation.",5 Analysis,[0],[0]
"However, directly combining them also causes lower scores in attributes, color, count, and size, showing that the advantages are not fully made use of.",5 Analysis,[0],[0]
"The most dramatic improvements come from applying the merging gate to the combined attention, showing that the proposed balance mechanism can adaptively combine the two kinds of information and is essential to the overall performance.",5 Analysis,[0],[0]
"The average merging gate value summarized in Figure 4 suggests the same.
",5 Analysis,[0],[0]
We give some examples in the left plot of Figure 5 to illustrate the differences between the models more intuitively.,5 Analysis,[0],[0]
"From the examples, it is clear that the proposed simNet generates the best captions in that more objects are described and many informative and detailed attributes are included, such as the quantity and the color.
",5 Analysis,[0],[0]
Visualization Figure 6 shows the visualization of the topic attention and the visual attention with running examples.,5 Analysis,[0],[0]
"As we can see, the topic attention is active when generating a phrase containing the related topic.",5 Analysis,[0],[0]
"For example, bathroom is always most attended when generating a bathroom.",5 Analysis,[0],[0]
The merging gate learns to direct the information flow efficiently.,5 Analysis,[0],[0]
"When generating words such as on and a, it gives lower weight to the topic attention and prefers the visual attention.",5 Analysis,[0],[0]
"As to the visual attention, the output attention is much more focused than the input attention.",5 Analysis,[0],[0]
"As we hypothesized, the conventional input attention lacks the information of the last generated word and does not know what to look for exactly.",5 Analysis,[0],[0]
"For example, when generating bathroom, the input attention does not know the previous generated word is a, and it loses its focus, while the output attention is relatively more concentrated.",5 Analysis,[0],[0]
"Moreover, the merging gate learns to overcome the erroneous topics, as shown in the second example.",5 Analysis,[0],[0]
"When generating chair, the topic attention is focused on a wrong object bed, while the visual attention attends correctly to the chair, and especially the output attention attends to the armrest.",5 Analysis,[0],[0]
"The merging gate effectively remedies
the misleading information from the topic attention and outputs a lower weight, resulting in the model correctly generating the word chair.
",5 Analysis,[0],[0]
Error Analysis,5 Analysis,[0],[0]
We conduct error analysis using the proposed (full) model on the test set to provide insights on how the model may be improved.,5 Analysis,[0],[0]
We find 123 out of 1000 generated captions that are not satisfactory.,5 Analysis,[0],[0]
"There are mainly three types of errors, i.e. distance (32, 26%), movement (22, 18%), and object (60, 49%), with 9 (7%) other errors.",5 Analysis,[0],[0]
Distance error takes place when there is a lot of objects and the model cannot grasp the foreground and the background relationship.,5 Analysis,[0],[0]
Movement error means that the model fails to describe whether the objects are moving.,5 Analysis,[0],[0]
"Those two kinds of errors are hard to eliminate, as they are fundamental problems of computer vision waiting to be resolved.",5 Analysis,[0],[0]
"Object error happens when there are incorrect extracted topics, and the merging gate regards the topic as grounded in the image.",5 Analysis,[0],[0]
"In the given example, the incorrect topic is garden.",5 Analysis,[0],[0]
The tricky part is that the topic is seemingly correct according to the image features or otherwise the proposed model will choose other topics.,5 Analysis,[0],[0]
A more powerful topic extractor may help with the problem but it is unlikely to be completely avoided.,5 Analysis,[0],[0]
We propose the stepwise image-topic merging network to sequentially and adaptively merge the visual and the conceptual information for improved image captioning.,6 Conclusions,[0],[0]
"To our knowledge, we are the first to combine the visual and the semantic attention to achieve substantial improvements.",6 Conclusions,[0],[0]
We introduce the stepwise merging mechanism to efficiently guide the two kinds of information when generating the caption.,6 Conclusions,[0],[0]
"The experimental results demonstrate the effectiveness of the proposed approach, which substantially outperforms the stateof-the-art image captioning methods in terms of SPICE on COCO and Flickr30k datasets.",6 Conclusions,[0],[0]
Quantitative and qualitative analysis show that the generated captions are both detailed and comprehensive in comparison with the existing methods.,6 Conclusions,[0],[0]
This work was supported in part by National Natural Science Foundation of China (No. 61673028).,Acknowledgments,[0],[0]
We thank all the anonymous reviewers for their constructive comments and suggestions.,Acknowledgments,[0],[0]
Xu Sun is the corresponding author of this paper.,Acknowledgments,[0],[0]
A.1 Results on COCO Evaluation Server Table 6 shows the performance on the online COCO evaluation server4.,A Supplementary Material,[0],[0]
"We put it in the appendix because the results are incomplete and the SPICE metric is not available for our submission, which correlates the best with human evaluation.",A Supplementary Material,[0],[0]
"The SPICE metrics are only available at the leaderboard on the COCO dataset website5, which, unfortunately, has not been updated for more than a year.",A Supplementary Material,[0],[0]
"Our submission does not directly optimize CIDEr, use model ensemble, or use extra training data.",A Supplementary Material,[0],[0]
"The three techniques typically result in orthogonal improvements (Lu et al., 2017; Rennie et al., 2017; Anderson et al., 2018).",A Supplementary Material,[0],[0]
"Moreover, the SPICE results are missing, in which the proposed model has the most advantage.",A Supplementary Material,[0],[0]
"Nonetheless, our model is second only to Up-Down (Anderson et al., 2018) and surpasses almost all the other models in published work, especially when 40 references are considered.
",A Supplementary Material,[0],[0]
"4https://competitions.codalab.org/ competitions/3221
5http://cocodataset.org/ #captions-leaderboard",A Supplementary Material,[0],[0]
The encode-decoder framework has shown recent success in image captioning.,abstractText,[0],[0]
"Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image.",abstractText,[0],[0]
"In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time.",abstractText,[0],[0]
"At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined.",abstractText,[0],[0]
The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.1,abstractText,[0],[0]
simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 845–855 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
845",text,[0],[0]
Teaching machines to answer arbitrary usergenerated questions is a long-term goal of natural language processing.,1 Introduction,[0],[0]
"For a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer.",1 Introduction,[0],[0]
"However, automatically extracting the answer from those texts remains an open challenge.",1 Introduction,[0],[0]
"The recent success of neural models at answering questions given a related paragraph (Wang et al., 2017c; Tan et al., 2017) suggests they have the potential to be a key part of
∗Work completed while interning at the Allen Institute for Artificial Intelligence
a solution to this problem.",1 Introduction,[0],[0]
"Most neural models are unable to scale beyond short paragraphs, so typically this requires adapting a paragraph-level model to process document-level input.
",1 Introduction,[0],[0]
There are two basic approaches to this task.,1 Introduction,[0],[0]
"Pipelined approaches select a single paragraph from the input documents, which is then passed to the paragraph model to extract an answer (Joshi et al., 2017; Wang et al., 2017a).",1 Introduction,[0],[0]
"Confidence based methods apply the model to multiple paragraphs and return the answer with the highest confidence (Chen et al., 2017a).",1 Introduction,[0],[0]
"Confidence methods have the advantage of being robust to errors in the (usually less sophisticated) paragraph selection step, however they require a model that can produce accurate confidence scores for each paragraph.",1 Introduction,[0],[0]
"As we shall show, naively trained models often struggle to meet this requirement.
",1 Introduction,[0],[0]
In this paper we start by proposing an improved pipelined method which achieves state-of-the-art results.,1 Introduction,[0],[0]
"Then we introduce a method for training models to produce accurate per-paragraph confidence scores, and we show how combining this method with multiple paragraph selection further increases performance.
",1 Introduction,[0],[0]
Our pipelined method focuses on addressing the challenges that come with training on documentlevel data.,1 Introduction,[0],[0]
We use a linear classifier to select which paragraphs to train and test on.,1 Introduction,[0],[0]
"Since annotating entire documents is expensive, data of this sort is typically distantly supervised, meaning only the answer text, not the answer spans, are known.",1 Introduction,[0],[0]
"To handle the noise this creates, we use a summed objective function that marginalizes the model’s output over all locations the answer text occurs.",1 Introduction,[0],[0]
"We apply this approach with a model design that integrates some recent ideas in reading comprehension models, including selfattention (Cheng et al., 2016) and bi-directional attention (Seo et al., 2016).
",1 Introduction,[0],[0]
Our confidence method extends this approach to better handle the multi-paragraph setting.,1 Introduction,[0],[0]
Previous approaches trained the model on questions paired with paragraphs that are known a priori to contain the answer.,1 Introduction,[0],[0]
"This has several downsides: the model is not trained to produce low confidence scores for paragraphs that do not contain an answer, and the training objective does not require confidence scores to be comparable between paragraphs.",1 Introduction,[0],[0]
"We resolve these problems by sampling paragraphs from the context documents, including paragraphs that do not contain an answer, to train on.",1 Introduction,[0],[0]
"We then use a shared-normalization objective where paragraphs are processed independently, but the probability of an answer candidate is marginalized over all paragraphs sampled from the same document.",1 Introduction,[0],[0]
"This requires the model to produce globally correct output even though each paragraph is processed independently.
",1 Introduction,[0],[0]
"We evaluate our work on TriviaQA (Joshi et al., 2017) in the wiki, web, and unfiltered setting.",1 Introduction,[0],[0]
Our model achieves a nearly 10 point lead over published prior work.,1 Introduction,[0],[0]
"We additionally perform an ablation study on our pipelined method, and we show the effectiveness of our multi-paragraph methods on a modified version of SQuAD (Rajpurkar et al., 2016) where only the correct document, not the correct paragraph, is known.",1 Introduction,[0],[0]
"Finally, we combine our model with a web search backend to build a demonstration end-to-end QA system1, and show it performs well on questions from the TREC question answering task (Voorhees et al., 1999).",1 Introduction,[0],[0]
We release our code2 to facilitate future work.,1 Introduction,[0],[0]
"In this section we propose a pipelined QA system, where a single paragraph is selected and passed to a paragraph-level question answering model.",2 Pipelined Method,[0],[0]
"If there is a single source document, we select the paragraph with the smallest TF-IDF cosine distance with the question.",2.1 Paragraph Selection,[0],[0]
Document frequencies are computed using the individual paragraphs within the document.,2.1 Paragraph Selection,[0],[0]
"If there are multiple input documents, we found it beneficial to use a linear classifier that uses the same TF-IDF score, whether the paragraph was the first in its document, how
1https://documentqa.allenai.org 2https://github.com/allenai/document-qa
many tokens preceded it, and the number of question words it includes as features.",2.1 Paragraph Selection,[0],[0]
The classifier is trained on the distantly supervised objective of selecting paragraphs that contain at least one answer span.,2.1 Paragraph Selection,[0],[0]
"On TriviaQA web, relative to truncating the document as done by prior work, this improves the chance of the selected text containing the correct answer from 83.1% to 85.1%.",2.1 Paragraph Selection,[0],[0]
In a distantly supervised setup we label all text spans that match the answer text as being correct.,2.2 Handling Noisy Labels,[0],[0]
This can lead to training the model to select unwanted answer spans.,2.2 Handling Noisy Labels,[0],[0]
Figure 1 contains an example.,2.2 Handling Noisy Labels,[0],[0]
"To handle this difficulty, we use a summed objective function similar to the one from Kadlec et al. (2016), that optimizes the negative loglikelihood of selecting any correct answer span.",2.2 Handling Noisy Labels,[0],[0]
"The models we consider here work by independently predicting the start and end token of the answer span, so we take this approach for both predictions.",2.2 Handling Noisy Labels,[0],[0]
"For example, the objective for predicting the answer start token becomes − log (∑ a∈A pa
) where A is the set of tokens that start an answer and pi is the answer-start probability predicted by the model for token i.",2.2 Handling Noisy Labels,[0],[0]
"This objective has the advantage of being agnostic to how the model distributes probability mass across the possible answer spans, allowing the model to focus on only the most relevant spans.",2.2 Handling Noisy Labels,[0],[0]
"We use a model with the following layers (shown in Figure 2):
Embedding: We embed words using pretrained word vectors.",2.3 Model,[0],[0]
"We concatenate these with character-derived word embeddings, which are
produced by embedding characters using a learned embedding matrix and then applying a convolutional neural network and max-pooling.
",2.3 Model,[0],[0]
"Pre-Process: A shared bi-directional GRU (Cho et al., 2014) is used to process the question and passage embeddings.
",2.3 Model,[0],[0]
"Attention: The attention mechanism from the Bi-Directional Attention Flow (BiDAF) model (Seo et al., 2016) is used to build a queryaware context representation.",2.3 Model,[0],[0]
"Let hi and qj be the vector for context word i and question word j, and nq and nc be the lengths of the question and context respectively.",2.3 Model,[0],[0]
"We compute attention between context word i and question word j as:
aij = w1 · hi +w2 · qj +w3 · (hi qj)
where w1, w2, and w3 are learned vectors and is element-wise multiplication.",2.3 Model,[0],[0]
"We then compute an attended vector ci for each context token as:
pij = eaij∑nq j=1 e aij ci = nq∑ j=1 qjpij
We also compute a query-to-context vector qc:
mi = max 1≤j≤nq",2.3 Model,[0],[0]
"aij
pi = emi∑nc i=1",2.3 Model,[0],[0]
e mi qc = nc∑ i=1,2.3 Model,[0],[0]
"hipi
The final vector for each token is built by concatenating hi, ci, hi ci, and qc ci.",2.3 Model,[0],[0]
"In our model we subsequently pass the result through a linear layer with ReLU activations.
",2.3 Model,[0],[0]
Self-Attention:,2.3 Model,[0],[0]
Next we use a layer of residual self-attention.,2.3 Model,[0],[0]
The input is passed through another bi-directional GRU.,2.3 Model,[0],[0]
"Then we apply the same attention mechanism, only now between the passage and itself.",2.3 Model,[0],[0]
In this case we do not use query-tocontext attention and we set aij = −inf,2.3 Model,[0],[0]
"if i = j.
As before, we pass the concatenated output through a linear layer with ReLU activations.",2.3 Model,[0],[0]
"The result is then summed with the original input.
",2.3 Model,[0],[0]
Prediction:,2.3 Model,[0],[0]
"In the last layer of our model a bidirectional GRU is applied, followed by a linear layer to compute answer start scores for each token.",2.3 Model,[0],[0]
The hidden states are concatenated with the input and fed into a second bi-directional GRU and linear layer to predict answer end scores.,2.3 Model,[0],[0]
"The softmax function is applied to the start and end scores to produce answer start and end probabilities.
",2.3 Model,[0],[0]
"Dropout: We apply variational dropout (Gal and Ghahramani, 2016) to the input to all the GRUs and the input to the attention mechanisms at a rate of 0.2.",2.3 Model,[0],[0]
"We adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated (i.e., before the softmax operator is applied) score given to each span as a measure of the model’s confidence.",3 Confidence Method,[0],[0]
"For the boundary-based models we use here, a span’s score is the sum of the start and end score given to its start and end token.",3 Confidence Method,[0],[0]
At test time we run the model on each paragraph and select the answer span with the highest confidence.,3 Confidence Method,[0],[0]
"This is the approach taken by Chen et al. (2017a).
",3 Confidence Method,[0],[0]
"Our experiments in Section 5 show that these confidence scores can be very poor if the model is only trained on answer-containing paragraphs, as done by prior work.",3 Confidence Method,[0],[0]
"Table 1 contains some qualitative examples of the errors that occur.
",3 Confidence Method,[0],[0]
We hypothesize that there are two key sources of error.,3 Confidence Method,[0],[0]
"First, for models trained with the softmax objective, the pre-softmax scores for all spans can be arbitrarily increased or decreased by a constant value without changing the resulting softmax probability distribution.",3 Confidence Method,[0],[0]
"As a result, nothing prevents models from producing scores that are arbitrarily all larger or all smaller for one paragraph
than another.",3 Confidence Method,[0],[0]
"Second, if the model only sees paragraphs that contain answers, it might become too confident in heuristics or patterns that are only effective when it is known a priori that an answer exists.",3 Confidence Method,[0],[0]
"For example, the model might become too reliant on selecting answers that match semantic type the question is asking about, causing it be easily distracted by other entities of that type when they appear in irrelevant text.",3 Confidence Method,[0],[0]
"This kind of error has also been observed when distractor sentences are added to the context (Jia and Liang, 2017)
",3 Confidence Method,[0],[0]
"We experiment with four approaches to training models to produce comparable confidence scores, shown in the following subsections.",3 Confidence Method,[0],[0]
In all cases we will sample paragraphs that do not contain an answer as additional training points.,3 Confidence Method,[0],[0]
In this approach a modified objective function is used where span start and end scores are normalized across all paragraphs sampled from the same context.,3.1 Shared-Normalization,[0],[0]
This means that paragraphs from the same context use a shared normalization factor in the final softmax operations.,3.1 Shared-Normalization,[0],[0]
We train on this objective by including multiple paragraphs from the same context in each mini-batch.,3.1 Shared-Normalization,[0],[0]
"The key idea is that this will force the model to produce scores that are comparable between paragraphs, even though it does not have access to information about what other paragraphs are being considered.",3.1 Shared-Normalization,[0],[0]
"As an alternative to the previous method, we experiment with concatenating all paragraphs sam-
pled from the same context together during training.",3.2 Merge,[0],[0]
A paragraph separator token with a learned embedding is added before each paragraph.,3.2 Merge,[0],[0]
We also experiment with allowing the model to select a special “no-answer” option for each paragraph.,3.3 No-Answer Option,[0],[0]
"First we re-write our objective as:
− log (
esa∑n i=1",3.3 No-Answer Option,[0],[0]
"e si
)",3.3 No-Answer Option,[0],[0]
"− log ( egb∑n j=1 e gj ) =
− log ( esa+gb∑n
i=1",3.3 No-Answer Option,[0],[0]
"∑n j=1 e si+gj ) where sj and gj are the scores for the start and end bounds produced by the model for token j, and a and b are the correct start and end tokens.",3.3 No-Answer Option,[0],[0]
"We have the model compute another score, z, to represent the weight given to a “no-answer” possibility.",3.3 No-Answer Option,[0],[0]
"Our revised objective function becomes:
− log
( (1− δ)ez + δesa+gb
ez + ∑n
i=1",3.3 No-Answer Option,[0],[0]
∑n j=1 e si+gj ) where δ is 1 if an answer exists and 0 otherwise.,3.3 No-Answer Option,[0],[0]
"If there are multiple answer spans we use the same objective, except the numerator includes the summation over all answer start and end tokens.
",3.3 No-Answer Option,[0],[0]
We compute z by adding an extra layer at the end of our model.,3.3 No-Answer Option,[0],[0]
"We build input vectors by taking the summed hidden states of the RNNs used to predict the start/end token scores weighed by the start/end probabilities, and using a learned attention vector on the output of the self-attention layer.
",3.3 No-Answer Option,[0],[0]
These vectors are fed into a two layer network with an 80 dimensional hidden layer and ReLU activations that produces z as its only output.,3.3 No-Answer Option,[0],[0]
"As a final baseline, we consider training models with the sigmoid loss objective function.",3.4 Sigmoid,[0],[0]
"That is, we compute a start/end probability for each token by applying the sigmoid function to the start/end scores of each token.",3.4 Sigmoid,[0],[0]
A cross entropy loss is used on each individual probability.,3.4 Sigmoid,[0],[0]
"The intuition is that, since the scores are being evaluated independently of one another, they are more likely to be comparable between different paragraphs.",3.4 Sigmoid,[0],[0]
"We evaluate our approach on four datasets: TriviaQA unfiltered (Joshi et al., 2017), a dataset of questions from trivia databases paired with documents found by completing a web search of the questions; TriviaQA wiki, the same dataset but only including Wikipedia articles; TriviaQA web, a dataset derived from TriviaQA unfiltered by treating each question-document pair where the document contains the question answer as an individual training point; and SQuAD (Rajpurkar et al., 2016), a collection of Wikipedia articles and crowdsourced questions.",4.1 Datasets,[0],[0]
"We note that for TriviaQA web we do not subsample as was done by Joshi et al. (2017), instead training on the all 530k training examples.",4.2 Preprocessing,[0],[0]
"We also observe that TriviaQA documents often contain many small paragraphs, so we restructure the documents by merging consecutive paragraphs together up to a target size.",4.2 Preprocessing,[0],[0]
We use a maximum paragraph size of 400 unless stated otherwise.,4.2 Preprocessing,[0],[0]
Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information.,4.2 Preprocessing,[0],[0]
"We are also careful to mark all spans of text that would be considered an exact match by the official evaluation script, which includes some minor text pre-processing, as answer spans, not just spans that are an exact string match with the answer text.",4.2 Preprocessing,[0],[0]
Our confidence-based approaches are trained by sampling paragraphs from the context during training.,4.3 Sampling,[0],[0]
"For SQuAD and TriviaQA web we take
the top four paragraphs as judged by our paragraph ranking function (see Section 2.1).",4.3 Sampling,[0],[0]
We sample two different paragraphs from those four each epoch to train on.,4.3 Sampling,[0],[0]
"Since we observe that the higherranked paragraphs are more likely to contain the context needed to answer the question, we sample the highest ranked paragraph that contains an answer twice as often as the others.",4.3 Sampling,[0],[0]
"For the merge and shared-norm approaches, we additionally require that at least one of the paragraphs contains an answer span, and both of those paragraphs are included in the same mini-batch.",4.3 Sampling,[0],[0]
"For TriviaQA wiki we repeat the process but use the top 8 paragraphs, and for TriviaQA unfiltered we use the top 16, because much more context is given in these settings.",4.3 Sampling,[0],[0]
"We train the model with the Adadelta optimizer (Zeiler, 2012) with a batch size 60 for TriviaQA and 45 for SQuAD.",4.4 Implementation,[0],[0]
At test time we select the most probable answer span of length less than or equal to 8 for TriviaQA and 17 for SQuAD.,4.4 Implementation,[0],[0]
The GloVe 300 dimensional word vectors released by Pennington et al. (2014) are used for word embeddings.,4.4 Implementation,[0],[0]
"On SQuAD, we use a dimensionality of size 100 for the GRUs and of size 200 for the linear layers employed after each attention mechanism.",4.4 Implementation,[0],[0]
"We found for TriviaQA, likely because there is more data, using a larger dimensionality of 140 for each GRU and 280 for the linear layers is beneficial.",4.4 Implementation,[0],[0]
"During training, we maintain an exponential moving average of the weights with a decay rate of 0.999.",4.4 Implementation,[0],[0]
We use the weight averages at test time.,4.4 Implementation,[0],[0]
We do not update the word vectors during training.,4.4 Implementation,[0],[0]
"First, we do an ablation study on TriviaQA web to show the effects of our proposed methods for our pipeline model.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We start with a baseline following the one used by Joshi et al. (2017).,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"This
system uses BiDAF (Seo et al., 2016) as the paragraph model, and selects a random answer span from each paragraph each epoch to train on.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The first 400 tokens of each document are used during training, and the first 800 during testing.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"When using the TF-IDF paragraph selection approach, we instead break the documents into paragraphs of size 400 when training and 800 when testing, and select the top-ranked paragraph to feed into the model.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"As shown in Table 2, our baseline outperforms the results reported by Joshi et al. (2017) significantly, likely because we are not subsampling the data.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We find both TF-IDF ranking and the sum objective to be effective.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Using our refined model increases the gain by another 4 points.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
Next we show the results of our confidencebased approaches.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"For this comparison we split documents into paragraphs of at most 400 tokens, and rank them using TF-IDF cosine distance.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Then we measure the performance of our proposed approaches as the model is used to independently process an increasing number of these paragraphs, and the highest confidence answer is selected as the final output.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The results are shown in Figure 3.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"On this dataset even the model trained without any of the proposed training methods (“none”) im-
proves as more paragraphs are used, showing it does a passable job at focusing on the correct paragraph.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"The no-answer option training approach lead to a significant improvement, and the sharednorm and merge approaches are even better.
",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
We use the shared-norm approach for evaluation on the TriviaQA test sets.,5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"We found that increasing the paragraph size to 800 at test time, and to 600 during training, was slightly beneficial, allowing our model to reach 66.04 EM and 70.98 F1 on the dev set.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"As shown in Table 3, our model is firmly ahead of prior work on both the TriviaQA web and TriviaQA wiki test sets.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
"Since our submission, a few additional entries have been added to the public leader for this dataset5, although to the best of our knowledge these results have not yet been published.",5.1 TriviaQA Web and TriviaQA Wiki,[0],[0]
Next we apply our confidence methods to TriviaQA unfiltered.,5.2 TriviaQA Unfiltered,[0],[0]
"This dataset is of particular interest because the system is not told which document contains the answer, so it provides a plausible simulation of answering a question using a document
4Comparison made of 5/01/2018.",5.2 TriviaQA Unfiltered,[0],[0]
"5https://competitions.codalab.org/competitions/17208
retrieval system.",5.2 TriviaQA Unfiltered,[0],[0]
We show the same graph as before for this dataset in Figure 4.,5.2 TriviaQA Unfiltered,[0],[0]
"Our methods have an even larger impact on this dataset, probably because there are many more relevant and irrelevant paragraphs for each question, making paragraph selection more important.
",5.2 TriviaQA Unfiltered,[0],[0]
"Note the naively trained model starts to lose performance as more paragraphs are used, showing that errors are being caused by the model being overly confident in incorrect extractions.",5.2 TriviaQA Unfiltered,[0],[0]
We achieve a score of 61.55 EM and 67.61 F1 on the dev set.,5.2 TriviaQA Unfiltered,[0],[0]
"This advances the only prior result reported for this dataset, 50.6 EM and 57.3 F1 from Wang et al. (2017b), by 10 points.",5.2 TriviaQA Unfiltered,[0],[0]
"We additionally evaluate our model on SQuAD. SQuAD questions were not built to be answered independently of their context paragraph, which makes it unclear how effective of an evaluation tool they can be for document-level question answering.",5.3 SQuAD,[0],[0]
"To assess this we manually label 500 random questions from the training set.
",5.3 SQuAD,[0],[0]
"We categorize questions as:
1.",5.3 SQuAD,[0],[0]
"Context-independent, meaning it can be understood independently of the paragraph.
2.",5.3 SQuAD,[0],[0]
"Document-dependent, meaning it can be understood given the article’s title.",5.3 SQuAD,[0],[0]
"For example, “What individual is the school named after?”",5.3 SQuAD,[0],[0]
"for the document “Harvard University”.
3.",5.3 SQuAD,[0],[0]
"Paragraph-dependent, meaning it can only be understood given its paragraph.",5.3 SQuAD,[0],[0]
"For example, “What was the first step in the reforms?”.
",5.3 SQuAD,[0],[0]
"We find 67.4% of the questions to be contextindependent, 22.6% to be document-dependent,
and the remaining 10% to be paragraphdependent.",5.3 SQuAD,[0],[0]
There are many document-dependent questions because questions are frequently about the subject of the document.,5.3 SQuAD,[0],[0]
"Since a reasonably high fraction of the questions can be understood given the document they are from, and to isolate our analysis from the retrieval mechanism used, we choose to evaluate on the document-level.",5.3 SQuAD,[0],[0]
"We build documents by concatenating all the paragraphs in SQuAD from the same article together into a single document.
",5.3 SQuAD,[0],[0]
"Given the correct paragraph (i.e., in the standard SQuAD setting)",5.3 SQuAD,[0],[0]
our model reaches 72.14 EM and 81.05 F1 and can complete 26 epochs of training in less than five hours.,5.3 SQuAD,[0],[0]
"Most of our variations to handle the multi-paragraph setting caused a minor (up to half a point) drop in performance, while the sigmoid version fell behind by a point and a half.
",5.3 SQuAD,[0],[0]
We graph the document-level performance in Figure 5.,5.3 SQuAD,[0],[0]
"For SQuAD, we find it crucial to employ one of the suggested confidence training techniques.",5.3 SQuAD,[0],[0]
The base model starts to drop in performance once more than two paragraphs are used.,5.3 SQuAD,[0],[0]
"However, the shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs.",5.3 SQuAD,[0],[0]
"Given our estimate that 10% of the questions are ambiguous if the paragraph is unknown, our approach appears to have adapted to the document-level task very well.
",5.3 SQuAD,[0],[0]
"Finally, we compare the shared-norm model with the document-level result reported by Chen et al. (2017a).",5.3 SQuAD,[0],[0]
"We re-evaluate our model using the documents used by Chen et al. (2017a), which consist of the same Wikipedia articles SQuAD was built from, but downloaded at different dates.",5.3 SQuAD,[0],[0]
The advantage of this dataset is that it does not allow the model to know a priori which paragraphs were filtered out during the construction of SQuAD.,5.3 SQuAD,[0],[0]
"The disadvantage is that some of the articles have been edited since the questions were written, so some questions may no longer be answerable.",5.3 SQuAD,[0],[0]
"Our model achieves 59.14 EM and 67.34 F1 on this dataset, which significantly outperforms the 49.7 EM reported by Chen et al. (2017a).",5.3 SQuAD,[0],[0]
We perform one final experiment that tests our model as part of an end-to-end question answering system.,5.4 Curated TREC,[0],[0]
"For document retrieval, we re-implement the pipeline from Joshi et al. (2017).",5.4 Curated TREC,[0],[0]
"Given a question, we retrieve up to 10 web documents us-
7https://github.com/brmson/yodaqa/wiki/Benchmarks
ing a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME (Ferragina and Scaiella, 2010) identifies in the question.",5.4 Curated TREC,[0],[0]
"We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span.",5.4 Curated TREC,[0],[0]
We choose to use the shared-norm model trained on the TriviaQA unfiltered dataset since it is trained using multiple web documents as input.,5.4 Curated TREC,[0],[0]
We use the same heuristics as Joshi et al. (2017) to filter out trivia or QA websites to ensure questions cannot be trivially answered using webpages that directly address the question.,5.4 Curated TREC,[0],[0]
"A demo of the system is publicly available8.
",5.4 Curated TREC,[0],[0]
"We find accuracy on the TriviaQA unfiltered questions remains almost unchanged (within half a percent exact match score) when using our document retrieval method instead of the given documents, showing our pipeline does a good job of producing evidence documents that are similar to the ones in the training data.
",5.4 Curated TREC,[0],[0]
"We test the system on questions from the TREC QA tasks (Voorhees et al., 1999), in particular a curated set of questions from Baudiš (2015), the same dataset used in Chen et al. (2017a).",5.4 Curated TREC,[0],[0]
"We apply our system to the 694 test questions without retraining on the train questions.
",5.4 Curated TREC,[0],[0]
"We compare against DrQA (Chen et al., 2017a) and YodaQA (Baudiš, 2015).",5.4 Curated TREC,[0],[0]
"It is important to note that these systems use different document corpora (Wikipedia for DrQA, and Wikipedia, several knowledge bases, and optionally Bing web search for YodaQA) and different training data (SQuAD and the TREC training questions for DrQA, and TREC only for YodaQA), so we cannot make assertions about the relative performance of individual components.",5.4 Curated TREC,[0],[0]
"Nevertheless, it is instructive to show how the methods we experiment with in this work can advance an end-to-end QA system.
",5.4 Curated TREC,[0],[0]
The results are listed in Table 4.,5.4 Curated TREC,[0],[0]
"Our method outperforms prior work, breaking the 50% accu-
8https://documentqa.allenai.org/
racy mark.",5.4 Curated TREC,[0],[0]
This is a strong proof-of-concept that neural paragraph reading combined with existing document retrieval methods can advance the stateof-the-art on general question answering.,5.4 Curated TREC,[0],[0]
"It also shows that, despite the noise, the data from TriviaQA is sufficient to train models that can be effective on out-of-domain QA tasks.",5.4 Curated TREC,[0],[0]
We found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting.,5.5 Discussion,[0],[0]
"The results were particularly bad for SQuAD; we think this is partly because the paragraphs are shorter, so the model had less exposure to irrelevant text.
",5.5 Discussion,[0],[0]
"The shared-norm approach consistently outperformed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance.",5.5 Discussion,[0],[0]
"Figures 3, 4, and 5 show this technique has a minimal effect on the performance when only one paragraph is used, suggesting the model’s per-paragraph performance is preserved.",5.5 Discussion,[0],[0]
"Meanwhile, it can be seen the accuracy of the shared-norm model never drops as more paragraphs are added, showing it successfully resolves the problem of being distracted by irrelevant text.
",5.5 Discussion,[0],[0]
"The no-answer and merge approaches were moderately effective, we suspect because they at least expose the model to more irrelevant text.",5.5 Discussion,[0],[0]
"However, these methods do not address the fundamental issue of requiring confidence scores to be comparable between independent applications of the model to different paragraphs, which is why we think they lagged behind.",5.5 Discussion,[0],[0]
"The sigmoid objective function reduces the paragraph-level performance considerably, especially on the TriviaQA datasets.",5.5 Discussion,[0],[0]
"We suspect this is because it is vulnerable to label noise, as discussed in Section 2.2.",5.5 Discussion,[0],[0]
We perform an error analysis by labeling 200 random TriviaQA web dev-set errors made by the shared-norm model.,5.6 Error Analysis,[0],[0]
"We found 40.5% of the er-
rors were caused because the document did not contain sufficient evidence to answer the question, and 17% were caused by the correct answer not being contained in the answer key.",5.6 Error Analysis,[0],[0]
"The distribution of the remaining errors is shown in Table 5.
",5.6 Error Analysis,[0],[0]
"We found quite a few cases where a sentence contained the answer, but the model was unable to extract it due to complex syntactic structure or paraphrasing.",5.6 Error Analysis,[0],[0]
"Two kinds of multi-sentence reading errors were also common: cases that required connecting multiple statements made in a single paragraph, and long-range coreference cases where a sentence’s subject was named in a previous paragraph.",5.6 Error Analysis,[0],[0]
"Finally, some questions required background knowledge, or required the model to extract answers that were only stated indirectly (e.g., examining a list to extract the nth element).",5.6 Error Analysis,[0],[0]
"Overall, these results suggest good avenues for improvement are to continue advancing the sentence and paragraph level reading comprehension abilities of the model, and adding a mechanism to handle document-level coreferences.",5.6 Error Analysis,[0],[0]
Reading Comprehension Datasets.,6 Related Work,[0],[0]
"The state of the art in reading comprehension has been rapidly advanced by neural models, in no small part due to the introduction of many large datasets.",6 Related Work,[0],[0]
"The first large scale datasets for training neural reading comprehension models used a Cloze-style task, where systems must predict a held out word from a piece of text (Hermann et al., 2015; Hill et al., 2015).",6 Related Work,[0],[0]
"Additional datasets including SQuAD (Rajpurkar et al., 2016), WikiReading (Hewlett et al., 2016), MS Marco (Nguyen et al., 2016) and TriviaQA (Joshi et al., 2017) provided more realistic questions.",6 Related Work,[0],[0]
"Another dataset of trivia questions, Quasar-T (Dhingra et al., 2017), was introduced recently that uses ClueWeb09 (Callan et al., 2009) as its source for documents.",6 Related Work,[0],[0]
"In this work we choose to focus on SQuAD because it is well studied, and TriviaQA because it is more challenging and features documents and multi-document contexts (Quasar T is similar, but was released after we started work on this project).
",6 Related Work,[0],[0]
Neural Reading Comprehension.,6 Related Work,[0],[0]
"Neural reading comprehension systems typically use some form of attention (Wang and Jiang, 2016), although alternative architectures exist (Chen et al., 2017a; Weissenborn et al., 2017b).",6 Related Work,[0],[0]
"Our model follows this approach, but includes some recent advances such as variational dropout (Gal
and Ghahramani, 2016) and bi-directional attention (Seo et al., 2016).",6 Related Work,[0],[0]
"Self-attention has been used in several prior works (Cheng et al., 2016; Wang et al., 2017c; Pan et al., 2017).",6 Related Work,[0],[0]
Our approach to allowing a reading comprehension model to produce a per-paragraph no-answer score is related to the approach used in the BiDAFT,6 Related Work,[0],[0]
"(Min et al., 2017) model to produce per-sentence classification scores, although we use an attentionbased method instead of max-pooling.
",6 Related Work,[0],[0]
"Open QA. Open question answering has been the subject of much research, especially spurred by the TREC question answering track (Voorhees et al., 1999).",6 Related Work,[0],[0]
"Knowledge bases can be used, such as in (Berant et al., 2013), although the resulting systems are limited by the quality of the knowledge base.",6 Related Work,[0],[0]
"Systems that try to answer questions using natural language resources such as YodaQA (Baudiš, 2015) typically use pipelined methods to retrieve related text, build answer candidates, and pick a final output.
",6 Related Work,[0],[0]
"Neural Open QA. Open question answering with neural models was considered by Chen et al. (2017a), where researchers trained a model on SQuAD and combined it with a retrieval engine for Wikipedia articles.",6 Related Work,[0],[0]
Our work differs because we focus on explicitly addressing the problem of applying the model to multiple paragraphs.,6 Related Work,[0],[0]
"A pipelined approach to QA was recently proposed by Wang et al. (2017a), where a ranker model is used to select a paragraph for the reading comprehension model to process.",6 Related Work,[0],[0]
"More recent work has considered evidence aggregation techniques (Wang et al., 2017b; Swayamdipta et al., 2017).",6 Related Work,[0],[0]
"Our work shows paragraph-level models that produce well-calibrated confidence scores can effectively exploit large amounts of text without aggregation, although integrating aggregation techniques could further improve our results.",6 Related Work,[0],[0]
"We have shown that, when using a paragraph-level QA model across multiple paragraphs, our training method of sampling non-answer-containing paragraphs while using a shared-norm objective function can be very beneficial.",7 Conclusion,[0],[0]
"Combining this with our suggestions for paragraph selection, using the summed training objective, and our model design allows us to advance the state of the art on TriviaQA.",7 Conclusion,[0],[0]
"As shown by our demo, this work can be directly applied to building deep-learningpowered open question answering systems.",7 Conclusion,[0],[0]
We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input.,abstractText,[0],[0]
"Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text.",abstractText,[0],[0]
We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs.,abstractText,[0],[0]
"Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output.",abstractText,[0],[0]
We additionally identify and improve upon a number of other design decisions that arise when working with document-level data.,abstractText,[0],[0]
"Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",abstractText,[0],[0]
Simple and Effective Multi-Paragraph Reading Comprehension,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 554–558 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
554",text,[0],[0]
"We present new evidence that the SimpleQuestions benchmark (Bordes et al., 2015) can be nearly solved by standard methods.",1 Introduction,[0],[0]
"First, we show that ambiguity in the data bounds performance; there are often questions have more than one equally plausible interpretation.",1 Introduction,[0],[0]
"Second, we introduce a baseline that sets a new state-of-the-art performance level, despite using standard methods.",1 Introduction,[0],[0]
"Finally, we report an empirical analysis showing that the upperbound is loose.
",1 Introduction,[0],[0]
"The simple questions task involves mapping an English question (e.g. “Who wrote Gulliver’s travels?”) to an analogous Freebase (Bollacker et al., 2008) query, used to answer the question.",1 Introduction,[0],[0]
The query consists of a Freebase relation (e.g. /film/film/story by) and subject (e.g. 090s 0,1 Introduction,[0],[0]
[gulliver’s travels]).,1 Introduction,[0],[0]
"To understand how we might bound performance on the SimpleQuestions dataset, our first contribution in this paper, consider the following examples:
a. who wrote gulliver’s travels?",1 Introduction,[0],[0]
"(film/film/story by, 090s 0",1 Introduction,[0],[0]
"[gulliver’s travels,
TV miniseries])
b. Name a character from gullivers travels.",1 Introduction,[0],[0]
"(book/book/characters, 0btc7 [ gulliver’s travels])
",1 Introduction,[0],[0]
"In example (a) the phrase “Gulliver’s travels” is mapped to a TV miniseries, while in (b) it is mapped to a book.",1 Introduction,[0],[0]
"This introduces an unintended ambiguity, since either mapping is equally plausible for both examples (i.e. both books and TV miniseries have authors and characters).",1 Introduction,[0],[0]
"We introduce a method for automatically identifying many such ambiguities in the data, for both the entities and relations, and show that performance is upperbounded at 83.4%.
",1 Introduction,[0],[0]
"Our second main contribution is a baseline that sets a new state-of-the-art performance level, despite using standard methods.",1 Introduction,[0],[0]
Our approach includes (1) a CRF used to tag the mention of the subject in a question and (2) a BiLSTM used to classify the Freebase relation.,1 Introduction,[0],[0]
"Despite its simplicity, this approach achieves 78.1% accuracy for predicting Freebase subject-relation queries, surpassing all previous models.
",1 Introduction,[0],[0]
"Finally, we present an empirical error analysis of this model which shows the upperbound is loose and that there is likely not much more than 4% of performance to be gained with future work on the data.",1 Introduction,[0],[0]
"Together, these results suggest that the SimpleQuestions dataset is nearly solved.",1 Introduction,[0],[0]
Our code and pretrained models are available at github.com/PetrochukM/ Simple-Question-Answering.,1 Introduction,[0],[0]
"Single-relation factoid questions (simple questions) are common in many settings (e.g. Microsoft’s search query logs (Yih et al., 2014) and WikiAnswers web questions (Fader et al., 2013)).",2 Background,[0],[0]
"The SimpleQuestions dataset is one of the most
commonly used benchmarks for studying such questions.
",2 Background,[0],[0]
The Freebase knowledge graph (KG) provides the facts for answering the questions in the SimpleQuestions dataset.,2 Background,[0],[0]
"It includes 3 billion triples of the form (subject, relation, object) (e.g. [04b5zb (Fires Creek), location/location/containedby, 0f80hy (Nantahala National Forest)]).",2 Background,[0],[0]
"We denote such triples as (s, r, o).
",2 Background,[0],[0]
"The SimpleQuestions task is to rewrite questions into subject-relation pairs of the form (subject, relation), denoted in this paper as (s, r).",2 Background,[0],[0]
Each pair defines a graph query that can be used to answer the corresponding natural language question.,2 Background,[0],[0]
The subject is a Freebase object with a identifier called an MID (e.g. 04b5zb ).,2 Background,[0],[0]
"Freebase objects also typically include one or more string aliases (e.g. MID 04b5zb is named “Fires Creek”), which we will use later when computing our upper bounds.",2 Background,[0],[0]
The relation is an object property (e.g. location/location/containedby) defined by the Freebase ontology.,2 Background,[0],[0]
"For example, the question “which forest is fires creek in” corresponds with the subject-relation pair (04b5zb",2 Background,[0],[0]
"[Fires Creek], location/location/containedby).",2 Background,[0],[0]
"Finally, the SimpleQuestions task is evaluated on subject-relation pair accuracy.
",2 Background,[0],[0]
"The SimpleQuestions dataset provides a set of 108,442 simple questions; each question is accompanied by a ground truth triple (s, r, o).",2 Background,[0],[0]
This dataset also provides two subsets of Freebase: FB2M and FB5M.1,2 Background,[0],[0]
The ambiguity in the SimpleQuestions dataset likely comes from the way the data was created.,3 Dataset Ambiguity and Upperbound,[0],[0]
Annotators were shown a single Freebase triple and asked to write a question.,3 Dataset Ambiguity and Upperbound,[0],[0]
"For example, given any of the following triples:
• (0btc7 [Gulliver’s Travels, Book], book/written work/author, o3 dj",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Dean Swift])
",3 Dataset Ambiguity and Upperbound,[0],[0]
• (06znpjr,3 Dataset Ambiguity and Upperbound,[0],[0]
"[Gulliver’s Travels, American film], film/film/written by, 03whnyn",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Nicholas Stroller])
1The FB2M and FB5M subsets of Freebase KG can complete 7,188,636 and 7,688,234 graph queries respectively; therefore, the FB5M subset is 6.9% larger than the FB2M subset.",3 Dataset Ambiguity and Upperbound,[0],[0]
"More previous research has cited FB2M numbers than FB5M; therefore, we report our numbers on FB2M.
• (06znpjr",3 Dataset Ambiguity and Upperbound,[0],[0]
"[Gulliver’s Travels, American film], film/film/story by, o3 dj [Dean Swift])
",3 Dataset Ambiguity and Upperbound,[0],[0]
The annotator might reasonably contribute the question “who wrote gulliver’s travels?”,3 Dataset Ambiguity and Upperbound,[0],[0]
"However, adding all of these pairs to the data is problematic.",3 Dataset Ambiguity and Upperbound,[0],[0]
"Systems are evaluated on producing the correct subject-relation pair, and cannot learn a deterministic mapping that would get these three examples correct.",3 Dataset Ambiguity and Upperbound,[0],[0]
"In this section, we present a simple heuristic method for finding many such instances of ambiguity, and use it to upper bound performance on this benchmark.",3 Dataset Ambiguity and Upperbound,[0],[0]
"Given an example question q with the ground truth (s, r, o), our goal is to determine the set of all other subject-relation pairs that are equally supported by the text in q.
We first determine a string alias a for the subject by matching a phrase in q with a Freebase alias for s, in our example yielding “gulliver’s travels”.",3.1 Approach,[0],[0]
"For 97% of questions q, some string alias a exactly matched a question q phrase.",3.1 Approach,[0],[0]
"We then find all other Freebase entities that share this alias a and add them to a set S, in our example S is the subject column of Table 1.
",3.1 Approach,[0],[0]
We define an abstract predicate p (e.g. “who wrote e?”) as q with alias a abstracted.,3.1 Approach,[0],[0]
"We determine the set of potential relations R as the relations p co-occurs with in the SimpleQuestions dataset, in our example R is the relation column of Table 2.
",3.1 Approach,[0],[0]
"Finally, if there exists a subject-relation pair (s, r) ∈ KG such that r ∈ R ∧ s ∈ S",3.1 Approach,[0],[0]
"we de-
fine that as an accurate semantic interpretation of q. q is unanswerable if there exists multiple valid subject-relation pairs (s, r).",3.1 Approach,[0],[0]
"In our example above, the question is unanswerable because of the many different subject, relation pairs that co-occur with “gulliver’s travels” and “who wrote e?”",3.1 Approach,[0],[0]
We find that 33.9% of examples in the SimpleQuestions dataset are unanswerable.,3.2 Results,[0],[0]
"In these cases, we can predict a majority baseline (i.e. always guess the most commonly seen Freebase entity or relation), yielding an upperbound of 85.2%.
",3.2 Results,[0],[0]
"Finally, we also found that 1.8% of example questions were noisy.",3.2 Results,[0],[0]
"For example, “Which book is written about?” does not reference the corresponding ground truth subject 01n7q (california).",3.2 Results,[0],[0]
"We also consider these examples unanswerable, yielding a final upperbound of 83.4%.",3.2 Results,[0],[0]
"Our second main contribution is a baseline that sets a new state-of-the-art performance level, despite using standard methods.",4 Baseline Model,[0],[0]
"Our approach includes (1) a CRF tagger to determine the subject alias, and (2) a BiLSTM to classify the relation.",4 Baseline Model,[0],[0]
Given a question q (e.g. “who wrote gulliver’s travels?”),4.1 Approach,[0],[0]
"our model must predict the corresponding subject-relation pair (s, r).",4.1 Approach,[0],[0]
"We predict (s, r) with a pipeline that first runs top-k subject recognition and then relation classification.
",4.1 Approach,[0],[0]
We make use of two learned distributions.,4.1 Approach,[0],[0]
"The subject recognition model P (a|q) ranges over text spans A within the question q, in our example A includes the correct subject “gulliver’s travels”.",4.1 Approach,[0],[0]
"This distribution is modeled with a CRF, as defined in more detail below.",4.1 Approach,[0],[0]
"The relation classification model P (r|q, a) will be used to select a Freebase relation r that matches q.",4.1 Approach,[0],[0]
"The distribution ranges over all relations in Freebase that co-occur with a subject that is named a. It is modeled with an LSTM, that encodes q, again as defined in more detail below.
",4.1 Approach,[0],[0]
"Given these distributions, we predict the final subject-relation pair (s, r) as follows.",4.1 Approach,[0],[0]
"First, we determine the most likely subject alias a according to P (a|q) that also matches a subject alias in the KG.",4.1 Approach,[0],[0]
"We define set S as all Freebase entities named a, in our example S is the subject column of Table
1.",4.1 Approach,[0],[0]
"Second, we define all potential relations R such that ∀(s, r) ∈",4.1 Approach,[0],[0]
KG{r ∈ R ∧ s ∈ S}.,4.1 Approach,[0],[0]
"Using the relation classification model p(r|q, a), we predict the most likely relation rmax ∈ R.
Now, the answer candidates are subject-relation pairs such that (s, rmax) ∈",4.1 Approach,[0],[0]
KG{r ∈ R ∧ s ∈ S}.,4.1 Approach,[0],[0]
"In our example question, if rmax is film/film/story by then S includes both subjects 06znpjr",4.1 Approach,[0],[0]
"(Gullivers Travels, American film) and 02py9bj (Gullivers Travels, French film).",4.1 Approach,[0],[0]
"Because there is no explicit linguistic signal to disambiguate this choice, we pick the subject that cooccurs most often with rmax in Freebase.",4.1 Approach,[0],[0]
"Our approach requires two models, in this section we cover training and configuring these models.
",4.2 Model Details,[0],[0]
Top-K Subject Recognition,4.2 Model Details,[0],[0]
We model top-k subject recognition P (a|q) using a linear-chain conditional random field tagger (CRF) with a conditional log likelihood loss objective.,4.2 Model Details,[0],[0]
"k candidates are inferred with the top-k Viterbi algorithm.
",4.2 Model Details,[0],[0]
Our model is trained on a dataset of questions each with their corresponding subject alias span delimited with IO tagging.,4.2 Model Details,[0],[0]
"The gold standard subject alias spans are determined by heuristically matching a phrase in the question with a Freebase alias for the subject.
",4.2 Model Details,[0],[0]
All hyperparameters are hand tuned and then a limited set are further tuned with grid search to increase validation accuracy.,4.2 Model Details,[0],[0]
In total we evaluated at most 100 hyperparameter configurations.,4.2 Model Details,[0],[0]
"The word embeddings are initialized with GloVe (Pennington et al., 2014) and frozen.",4.2 Model Details,[0],[0]
"Adam (Kingma and Ba, 2014), initialized with a learning rate of 0.001, is employed to optimize the model weights.",4.2 Model Details,[0],[0]
"Finally, we halve the learning rate if the validation accuracy has not improved in 3 epochs.
",4.2 Model Details,[0],[0]
Relation Classification,4.2 Model Details,[0],[0]
"The relation classification distribution P (r|q, a) is modeled with a one layer BiLSTM batchnorm softmax classifier.",4.2 Model Details,[0],[0]
"The BiLSTM encodes an abstract predicate string (e.g. “who wrote e?”), as described in Section 4.1.",4.2 Model Details,[0],[0]
"The last LSTM output vector is provided as input to an output block consisting of batch normalization, ReLU, and softmax.
",4.2 Model Details,[0],[0]
"All hyperparameters are hand tuned and then a limited set are further tuned with Hyperband (Li et al., 2017) to increase validation accuracy.",4.2 Model Details,[0],[0]
"Hyperband is allowed at most 30 epochs per model
and a total of 1000 epochs.",4.2 Model Details,[0],[0]
In total we evaluated at most 500 hyperparameter configurations.,4.2 Model Details,[0],[0]
"The word embeddings are initialized with FastText (Bojanowski et al., 2017) and frozen.",4.2 Model Details,[0],[0]
"We use the AMSGrad variant of Adam (Reddi et al., 2018), initialized with an learning rate of 0.001.",4.2 Model Details,[0],[0]
"Finally, we double the batch size (Smith et al., 2017) if the validation accuracy has not improved in 3 epochs.",4.2 Model Details,[0],[0]
"Finally, we present our results on the SimpleQuestions test set.
",4.3 Results,[0],[0]
SimpleQuestions Task,4.3 Results,[0],[0]
"Our model achieves 78.1% accuracy on the SimpleQuestions test set, a new state-of-the-art without ensembling or data augmentation (Table 3).",4.3 Results,[0],[0]
"These results suggest that relatively standard architectures work well when carefully tuned, and approach the level set by our upper bound earlier in the paper.",4.3 Results,[0],[0]
"This further confirms the results of Mohammed et al. 2017.
",4.3 Results,[0],[0]
"Further Qualitative Analysis We also analyze the remaining errors, to point toward directions for future work.
",4.3 Results,[0],[0]
"In Section 3, we showed that questions can provide equal evidence for multiple subject-relation
1Türe and Jojic 2017 reported a 86.8% accuracy but we and Mohammed et al. 2017 have not been able to replicate their results.",4.3 Results,[0],[0]
"Wang et al. 2017 scored 77.5% but removed 0.5% of the test examples.
pairs.",4.3 Results,[0],[0]
"To remove this ambiguity, we count any of these options as correct, and our performance jumps to 91.5%.
",4.3 Results,[0],[0]
The remaining 8.5% error comes from a number of sources.,4.3 Results,[0],[0]
"First, we find that 1.9% of examples were incorrect due to noise, as described in Section 3.",4.3 Results,[0],[0]
"To better understand the remaining 6.5% gap, we do an empirical error analysis on a sample of 50 negative examples.
",4.3 Results,[0],[0]
"First we found that for 14 of 50 cases the question provided equal linguistic evidence for both the ground truth options and the predicted subject-relation pair, similar to the dataset ambiguity found in Section 3, suggesting that our upper bound is loose.",4.3 Results,[0],[0]
We note that Section 3 did not cover all possible question-subject-relation pair ambiguities.,4.3 Results,[0],[0]
"The approach relied on exact string matching to discover ambiguity; therefore, missing other paraphrases.",4.3 Results,[0],[0]
"For example, the abstract predicate “what classification is e” had more examples than “what classification is the e” allowing our approach to programmatically define more subject-relation pair ambiguities for the former predicate than the latter.
",4.3 Results,[0],[0]
The remaining 36 of 50 cases were linguistic mistakes by our model.,4.3 Results,[0],[0]
"Among the 36 cases, we identified these error cases:
• Low Shot (16 of 36)",4.3 Results,[0],[0]
"The relation label was seen in the training data less than 10 times.
",4.3 Results,[0],[0]
• Span Identification (14 of 36),4.3 Results,[0],[0]
"The subject span was incorrectly labeled.
",4.3 Results,[0],[0]
• Noise (2 of 36),4.3 Results,[0],[0]
"The question did not make grammatical sense.
",4.3 Results,[0],[0]
"Together, this error analysis shows that the upperbound is loose.",4.3 Results,[0],[0]
There is likely not much more than 4% of performance to be gained with future work on the data.,4.3 Results,[0],[0]
The SimpleQuestions dataset is one of the most commonly used benchmarks for studying singlerelation factoid questions.,5 Conclusions and Future Work,[0],[0]
"In this paper, we presented new evidence to suggest that this benchmark can be nearly solved by standard methods.",5 Conclusions and Future Work,[0],[0]
"These results suggest there is likely not much more than 4% to be gained with future work on the data.
",5 Conclusions and Future Work,[0],[0]
"Finally, other KG (e.g. Freebase) query datasets should consider providing a set of correct subjectrelation pairs when there is ambiguity in the linguistic input.",5 Conclusions and Future Work,[0],[0]
The SimpleQuestions dataset is one of the most commonly used benchmarks for studying single-relation factoid questions.,abstractText,[0],[0]
"In this paper, we present new evidence that this benchmark can be nearly solved by standard methods.",abstractText,[0],[0]
"First, we show that ambiguity in the data bounds performance at 83.4%; many questions have more than one equally plausible interpretation.",abstractText,[0],[0]
"Second, we introduce a baseline that sets a new state-of-the-art performance level at 78.1% accuracy, despite using standard methods.",abstractText,[0],[0]
"Finally, we report an empirical analysis showing that the upperbound is loose; roughly a quarter of the remaining errors are also not resolvable from the linguistic signal.",abstractText,[0],[0]
"Together, these results suggest that the SimpleQuestions dataset is nearly solved.",abstractText,[0],[0]
SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4273–4283 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4273",text,[0],[0]
"Neural machine translation (NMT), typically with an attention-based encoder-decoder framework (Bahdanau et al., 2015), has recently become the dominant approach to machine translation and already been deployed for online translation services (Wu et al., 2016).",1 Introduction,[0],[0]
"Recurrent neural networks (RNN), e.g., LSTMs (Hochreiter and Schmidhuber, 1997) or GRUs (Chung et al., 2014), are widely used as the encoder and decoder for NMT.",1 Introduction,[0],[0]
"In order to alleviate the gradient
∗Corresponding author.
vanishing issue found in simple recurrent neural networks (SRNN) (Elman, 1990), recurrent units in LSTMs or GRUs normally introduce different gates to create shotcuts for gradient information to pass through.
",1 Introduction,[0],[0]
"Notwithstanding the capability of these gated recurrent networks in learning long-distance dependencies, they use remarkably more matrix transformations (i.e., more parameters) than SRNN.",1 Introduction,[0],[0]
"And with many non-linear functions modeling inputs, hidden states and outputs, they are also less transparent than SRNN.",1 Introduction,[0],[0]
"These make NMT which is based on these gated RNNs suffer from not only inefficiency in training and inference due to recurrency and heavy computation in recurrent units (Vaswani et al., 2017) but also difficulty in producing interpretable models (Lee et al., 2017).",1 Introduction,[0],[0]
"These also hinder the deployment of NMT models particularly on memory- and computationlimited devices.
",1 Introduction,[0],[0]
"In this paper, our key interest is to simplify recurrent units in RNN-based NMT.",1 Introduction,[0],[0]
"In doing so, we want to investigate how further we can advance RNN-based NMT in terms of the number of parameters (i.e., memory consumption), running speed and interpretability.",1 Introduction,[0],[0]
This simplification shall preserve the capability of modeling longdistance dependencies in LSTMs/GRUs and the expressive power of recurrent non-linearities in SRNN.,1 Introduction,[0],[0]
"The simplification shall also reduce computation load and physical memory consumption in recurrent units on the one hand and allow us to take a good look into the inner workings of RNNs on the other hand.
",1 Introduction,[0],[0]
"In order to achieve this goal, we propose an addition-subtraction twin-gated recurrent network (ATR) for NMT.",1 Introduction,[0],[0]
"In the recurrent units of ATR, we only keep the very essential weight matrices: one over the input and the other over the history (similar to SRNN).",1 Introduction,[0],[0]
"Comparing with previous
RNN variants (e.g., LSTM or GRU), we have the smallest number of weight matrices.",1 Introduction,[0],[0]
This will reduce the computation load of matrix multiplication.,1 Introduction,[0],[0]
ATR also uses gates to bypass the vanishing gradient problem so as to capture long-range dependencies.,1 Introduction,[0],[0]
"Specifically, we use the addition and subtraction operations between the weighted history and input to estimate an input and forget gate respectively.",1 Introduction,[0],[0]
"These add-sub operations not only distinguish the two gates so that we do not need to have different weight matrices for them, but also make the two gates dynamically correlate to each other.",1 Introduction,[0],[0]
"Finally, we remove some non-linearities in recurrent units.
",1 Introduction,[0],[0]
"Due to these simplifications, we can easily show that each new state in ATR is an unnormalized weighted sum of previous inputs, similar to recurrent additive networks (Lee et al., 2017).",1 Introduction,[0],[0]
This property not only allows us to trace each state back to those inputs which contribute more but also establishes unnormalized forward self-attention between the current state and all its previous inputs.,1 Introduction,[0],[0]
"The self-attention mechanism has already proved very useful in non-recurrent NMT (Vaswani et al., 2017).
",1 Introduction,[0],[0]
We build our NMT systems on the proposed ATR with a single-layer encoder and decoder.,1 Introduction,[0],[0]
Experiments on WMT14 English-German and English-French translation tasks show that our model yields competitive results compared with GRU/LSTM-based NMT.,1 Introduction,[0],[0]
"When we integrate an orthogonal context-aware encoder (still single layer) into ATR-based NMT, our model (yielding 24.97 and 39.06 BLEU on English-German and English-French translation respectively) is even comparable to deep RNN and non-RNN NMT models which are all with multiple encoder/decoder layers.",1 Introduction,[0],[0]
"In-depth analyses demonstrate that ATR is more efficient than LSTM/GRU in terms of NMT training and decoding speed.
",1 Introduction,[0],[0]
"We adapt our model to other language translation and natural language processing tasks, including NIST Chinese-English translation, natural language inference and Chinese word segmentation.",1 Introduction,[0],[0]
Our conclusions still hold on all these tasks.,1 Introduction,[0],[0]
"The most widely used RNN models are LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014), both of which are good at handling gradient vanishing problem, a
notorious bottleneck of the simple RNN (Elman, 1990).",2 Related Work,[0],[0]
"The design of gates in our model follows the gate philosophy in LSTM/GRU.
",2 Related Work,[0],[0]
Our work is closely related to the recurrent additive network (RAN) proposed by Lee et al. (2017).,2 Related Work,[0],[0]
"They empirically demonstrate that many non-linearities commonly used in RNN transition dynamics can be removed, and that recurrent hidden states computed as purely the weighted sum of input vectors can be quite efficient in language modeling.",2 Related Work,[0],[0]
Our work follows the same spirit of simplifying recurrent units as they do.,2 Related Work,[0],[0]
But our proposed ATR is significantly different from RAN in three aspects.,2 Related Work,[0],[0]
"First, ATR is simpler than RAN with even fewer parameters.",2 Related Work,[0],[0]
There are only two weight matrices in ATR while four different weight matrices in the simplest version of RAN (two for each gate in RAN).,2 Related Work,[0],[0]
"Second, since the only difference between the input and forget gate in ATR is the addition/subtraction operation between the history and input, the two gates can be learned to be highly correlated as shown in our analysis.",2 Related Work,[0],[0]
"Finally, although RAN is verified effective in language modeling, our experiments show that ATR is better than RAN in machine translation in terms of both speed and translation quality.
",2 Related Work,[0],[0]
"To speed up RNN models, a line of work has attempted to remove recurrent connections.",2 Related Work,[0],[0]
"For example, Bradbury et al. (2016) propose the quasirecurrent neural network (QRNN) which uses convolutional layers and a minimalist recurrent pooling function to improve parallelism.",2 Related Work,[0],[0]
"Very recently, Lei and Zhang (2017) propose a simple recurrent unit (SRU).",2 Related Work,[0],[0]
"With the cuDNN optimization, their RNN model can be trained as fast as CNNs.",2 Related Work,[0],[0]
"However, to obtain promising results, QRNN and SRU have to use deep architectures.",2 Related Work,[0],[0]
"In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT.",2 Related Work,[0],[0]
"In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system.
",2 Related Work,[0],[0]
"Finally, our work is also related to the efforts in developing alternative architectures for NMT models.",2 Related Work,[0],[0]
Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation.,2 Related Work,[0],[0]
Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT.,2 Related Work,[0],[0]
"Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi-
×",2 Related Work,[0],[0]
"+
× × tanh
σ tanhσ",2 Related Work,[0],[0]
"σ
ct
htht−1
ct−1
xt
×
tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
"1-
× +
ht
xt
ht−1",2 Related Work,[0],[0]
"× +
σ×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(a) LSTM
× + ×",2 Related Work,[0],[0]
"× tanh σ tanhσ σ
ct
htht−1
ct−1
xt
× tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
"1-
× +
ht
xt
ht−1 × +
σ-
×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(b) GRU
× + ×",2 Related Work,[0],[0]
"× tanh σ tanhσ σ
ct htht−1 ct−1 xt
× tanhσ
ht
xt
ht−1
σ
×",2 Related Work,[0],[0]
1-,2 Related Work,[0],[0]
×,2 Related Work,[0],[0]
"+
ht
xt
ht−1 × +
σ×σ+
Concatenate CopyNeural Network Layer Pointwise Operation Vector Transfer
(c) ATR
Figure 1: Architecture for LSTM, GRU and ATR.",2 Related Work,[0],[0]
c∗ indicates the memory cell specific to the LSTM network.,2 Related Work,[0],[0]
"x∗ and h∗ denote the input and output hidden states respectively.
tectures as alternatives to RNNs for neural translation.",2 Related Work,[0],[0]
"With careful configurations, their deep models achieve state-of-the-art performance on various datasets.",2 Related Work,[0],[0]
"Given a sequence x = {x1,x2,. . .",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
",xT }, RNN updates the hidden state ht recurrently as follows:
ht = φ(ht−1,xt) (1)
where ht−1 is the previous hidden state, which is considered to store information from all previous inputs, and xt is the current input.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"The function φ(·) is a non-linear recurrent function, abstracting away from details in recurrent units.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
GRU can be considered as a simplified version of LSTM.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In this paper, theoretically, we use GRU as our benchmark and propose a new recurrent unit to further simplify it.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"The GRU function is defined as follows (see Figure 1b):
zt = σ(Wzxt +Uzht−1) (2)
rt = σ(Wrxt +Urht−1) (3)
h̃t = tanh(Whxt +Uh(rt ht−1))",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"(4) ht = zt ht−1 + (1− zt) h̃t (5)
where denotes an element-wise multiplication.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The reset gate rt and update gate zt enable manageable information flow from the history and the current input to the new state respectively.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Despite the success of these two gates in handling gradient flow, they consume extensive matrix transformations and weight parameters.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
We argue that many of these matrix transformations are not essential.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"We therefore propose an addition-subtraction twin-gated recurrent unit
(ATR), formulated as follows (see Figure 1c):
pt = Whht−1, qt = Wxxt (6)
it = σ(pt + qt) (7)
ft = σ(pt − qt) (8) ht = it qt + ft ht−1 (9)
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The hidden state ht in ATR is a weighted mixture of both the current input qt and the history ht−1 controlled by an input gate it and a forget gate ft respectively.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Notice that we use the transformed representation qt for the current input rather than the raw vector xt due to the potential mismatch in dimensions between ht and xt.
Similar to GRU, we use gates, especially the forget gate, to control the back-propagated gradient flow to make sure gradients will neither vanish nor explode.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"We also preserve the non-linearities of SRNN in ATR but only in the two gates.
",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
There are three significant differences of ATR from GRU.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
Some of these differences are due to the simplifications introduced in ATR.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"First, we squeeze the number of weight matrices in gate calculation from four to two (see Equation (2&3) and (7&8)).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In all existing gated RNNs, the inputs to gates are weighted sum of the previous hidden state and input.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In order to distinguish these gates, the weight matrices over the previous hidden state and the current input should be different for different gates.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
The number of different weight matrices in gates is therefore 2|#gates| in previous gated RNNs.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Different from them, ATR introduces different operations (i.e., addition and subtraction) between the weighted history and input to distinguish the input and forget gate.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Therefore, the weight matrices over the previous state/input in the two gates can be the same in ATR.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Second, we keep the very essential non-linearities, only in the two gates.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"In ATR, the role of qt is similar to that of h̃t in GRU (see Equation (4)).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"However, we completely wipe out the recurrent non-linearity
1
of h̃t in qt (i.e., qt = Wxxt).",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
Lee et al. (2017) show that this non-linearity is not necessary in language modeling.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
We further empirically demonstrate that it is neither essential in machine translation.,3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Third, in GRU the gates for h̃t and ht−1 are coupled and normalized to 1 while we do not explicitly associate the two gates in ATR.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Instead, they can be learned to be correlated in an implicit way, as shown in the next subsection and our empirical analyis in Section 5.1.",3 Addition-Subtraction Twin-Gated Recurrent Network,[0],[0]
"Unlike GRU, we use an addition and subtraction operation over the transformed current input qt and history pt to differentiate the two gates in ATR.",3.1 Twin-Gated Mechanism,[0],[0]
"As the two gates have the same weights for their input components with only a single difference in the operation between the input components, they act like twins.",3.1 Twin-Gated Mechanism,[0],[0]
"We term the two gates in ATR as twin gates and the procedure, shown in Equation (7&8), as the twin-gated mechanism.",3.1 Twin-Gated Mechanism,[0],[0]
This mechanism endows our model with the following two advantages: 1) Both addition and subtraction operations are completely linear so that fast computation can be expected; and 2),3.1 Twin-Gated Mechanism,[0],[0]
"No other weight parameters are introduced for gates so that our model is more memory-compact.
",3.1 Twin-Gated Mechanism,[0],[0]
A practical question for the twin-gated mechanism is whether twin gates are really capable of dynamically weighting the input and history information.,3.1 Twin-Gated Mechanism,[0],[0]
"To this end, we plot the surface of onedimensional σ(x + y)",3.1 Twin-Gated Mechanism,[0],[0]
− σ(x − y) in Figure 2.,3.1 Twin-Gated Mechanism,[0],[0]
"It is clear that both gates are highly non-linearly correlated, and that there are regions where σ(x+ y) is equal to, greater or smaller than σ(x − y).",3.1 Twin-Gated Mechanism,[0],[0]
"In other words, by adapting the distribution of input
and forget gates, the twin-gated mechanism has the potential to automatically seek suitable regions in Figure 2 to control its preference between the new and past information.",3.1 Twin-Gated Mechanism,[0],[0]
"We argue that the input and forget gates are negatively correlated after training, and empirically show their actual correlation in Section 5.1.",3.1 Twin-Gated Mechanism,[0],[0]
"Here we provide a systematical comparison of computations in LSTM, GRU, RAN and our ATR with respect to the number of weight matrices and matrix transformations.",3.2 Computation Analysis,[0],[0]
"Notice that all these units are building blocks of RNNs so that the total computational complexity and the minimum number of sequential operations required are unchanged, i.e. O(n · d2) and O(n) respectively where n is the sequence length and d is the dimensionality of hidden states.",3.2 Computation Analysis,[0],[0]
"However, the actual number of matrix transformations in the unit indeed significantly affects the running speed of RNN in practice.
",3.2 Computation Analysis,[0],[0]
We summarize the results in Table 1.,3.2 Computation Analysis,[0],[0]
"LSTM contains three different gates and a cell state, including 4 different neural layers with 8 weight matrices and transformations.",3.2 Computation Analysis,[0],[0]
"GRU simplifies LSTM by removing a gate, but still involves two gates and a candidate hidden state.",3.2 Computation Analysis,[0],[0]
It includes 3 different neural layers with 6 weight matrices and transformations.,3.2 Computation Analysis,[0],[0]
RAN further simplifies GRU by removing the non-linearity in the state transition and therefore contains 4 weight matrices in its simplest version.,3.2 Computation Analysis,[0],[0]
"Although our ATR also has two gates, however, there are only 2 weight matrices and transformations, accounting for only a third and a quarter of those in GRU and LSTM respectively.",3.2 Computation Analysis,[0],[0]
"To the best of our knowledge, ATR has the smallest number of weight transformations in existing gated RNN units.",3.2 Computation Analysis,[0],[0]
We provide a detailed and empirical analysis on the speed in Section 5.2.,3.2 Computation Analysis,[0],[0]
An appealing property of the proposed ATR is its interpretability.,3.3 Interpretability Analysis of Hidden States,[0],[0]
"This can be demonstrated by rolling out Equation (9) as follows:
ht = it qt + ft ht−1
= it Wtxt + t−1∑ k=1
ik (
t−k∏ l=1 fk+l
)",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Wxxk
≈ t∑
k=1
gk Wxxk
(10)
where gk can be considered as an approximate weight assigned to the k-th input.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Similar to the RAN model (Lee et al., 2017), the hidden state in ATR is a component-wise weighted sum of the inputs.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"This not only enables ATR to build up essential dependencies between preceding inputs and the current hidden state, but also allows us to easily detect which previous words have the promising impacts on the current state.",3.3 Interpretability Analysis of Hidden States,[0],[0]
"This desirable property obviously makes ATR highly interpretable.
",3.3 Interpretability Analysis of Hidden States,[0],[0]
"Additionally, this form of weighted sum is also related to self-attention (Vaswani et al., 2017).",3.3 Interpretability Analysis of Hidden States,[0],[0]
It can be considered as a forward unnormalized selfattention where each hidden state attends to all its previous positions.,3.3 Interpretability Analysis of Hidden States,[0],[0]
"As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments.",3.3 Interpretability Analysis of Hidden States,[0],[0]
We visualize the dependencies captured by Equation (10) in Section 5.3.,3.3 Interpretability Analysis of Hidden States,[0],[0]
We conducted our main experiments on WMT14 English-German and English-French translation tasks.,4.1 Setup,[0],[0]
"Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002).",4.1 Setup,[0],[0]
"Details about each dataset are as follows:
English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs.",4.1 Setup,[0],[0]
"We used the newstest2013 as our dev set, and the newstest2014 as our test set.
",4.1 Setup,[0],[0]
English-French We used the WMT 2014 training data.,4.1 Setup,[0],[0]
This corpora contain 12M selected sentence pairs.,4.1 Setup,[0],[0]
"We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set.
",4.1 Setup,[0],[0]
"The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent network as its decoder, companied with an additive attention mechanism (Bahdanau et al., 2015).",4.1 Setup,[0],[0]
We replaced the recurrent unit with our proposed ATR model.,4.1 Setup,[0],[0]
"More details are given in Appendix A.1.
",4.1 Setup,[0],[0]
"We also conducted experiments on ChineseEnglish translation, natural language inference and Chinese word segmentation.",4.1 Setup,[0],[0]
Details and experiment results are provided in Appendix A.2.,4.1 Setup,[0],[0]
We set the maximum length of training instances to 80 words for both English-German and EnglishFrench task.,4.2 Training,[0],[0]
"We used the byte pair encoding compression algorithm (Sennrich et al., 2016) to reduce the vocabulary size as well as to deal with the issue of rich morphology.",4.2 Training,[0],[0]
We set the vocabulary size of both source and target languages to 40K for all translation tasks.,4.2 Training,[0],[0]
"All out-of-vocabulary words were replaced with a token “unk”.
",4.2 Training,[0],[0]
We used 1000 hidden units for both encoder and decoder.,4.2 Training,[0],[0]
All word embeddings had dimensionality 620.,4.2 Training,[0],[0]
We initialized all model parameters randomly according to a uniform distribution ranging from -0.08 to 0.08.,4.2 Training,[0],[0]
"These tunable parameters were then optimized using Adam algorithm (Kingma and Ba, 2015) with the two momentum parameters set to 0.9 and 0.999 respectively.",4.2 Training,[0],[0]
Gradient clipping 5.0 was applied to avoid the gradient explosion problem.,4.2 Training,[0],[0]
We trained all models with a learning rate 5e−4 and batch size 80.,4.2 Training,[0],[0]
We decayed the learning rate with a factor of 0.5 between each training epoch.,4.2 Training,[0],[0]
Translations were generated by a beam search algorithm that was based on loglikelihood scores normalized by sentence length.,4.2 Training,[0],[0]
We used a beam size of 10 in all the experiments.,4.2 Training,[0],[0]
"We also applied dropout for English-German and English-French tasks on the output layer to avoid over-fitting, and the dropout rate was set to 0.2.
",4.2 Training,[0],[0]
"To train deep NMT models, we adopted the GNMT architecture (Wu et al., 2016).",4.2 Training,[0],[0]
"We kept all the above settings, except the dimensionality
of word embedding and hidden state which we set to be 512.",4.2 Training,[0],[0]
The translation results are shown in Table 2.,4.3 Results on English-German Translation,[0],[0]
We also provide results of several existing systems that are trained with comparable experimental settings to ours.,4.3 Results on English-German Translation,[0],[0]
"In particular, our single model yields a detokenized BLEU score of 21.99.",4.3 Results on English-German Translation,[0],[0]
"In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a singlelayer context-aware (CA) encoder (Zhang et al., 2017b) into our system.",4.3 Results on English-German Translation,[0],[0]
"The ATR+CA system further reaches 22.7 BLEU, outperforming the winner system (Buck et al., 2014) by a substantial improvement of 2 BLEU points.",4.3 Results on English-German Translation,[0],[0]
"Enhanced with the deep GNMT architecture, the GNMT+ATR system yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR.",4.3 Results on English-German Translation,[0],[0]
"Notice that different from our system which was trained on the parallel corpus alone, the winner system used a huge mono-
lingual text to enhance its language model.
",4.3 Results on English-German Translation,[0],[0]
"Compared with the existing LSTM-based (Luong et al., 2015a) deep NMT system, our shallow/deep model achieves a gain of 2.41/3.26 tokenized BLEU points respectively.",4.3 Results on English-German Translation,[0],[0]
"Under the same training condition, our ATR outperforms RAN by a margin of 0.34 tokenized BLEU points, and achieves competitive results against its GRU/LSTM counterpart.",4.3 Results on English-German Translation,[0],[0]
"This suggests that although our ATR is much simpler than GRU, LSTM and RAN, it still possesses strong modeling capacity.
",4.3 Results on English-German Translation,[0],[0]
"In comparison to several advanced deep NMT models, such as the Google NMT (8 layers, 24.61 tokenized BLEU)",4.3 Results on English-German Translation,[0],[0]
"(Wu et al., 2016) and the LAU-connected NMT (4 layers, 23.80 tokenized BLEU)",4.3 Results on English-German Translation,[0],[0]
"(Wang et al., 2017a), the performance of our shallow model (23.31) is competitive.",4.3 Results on English-German Translation,[0],[0]
"Particularly, when replacing LSTM in the Google NMT with our ATR model, the GNMT+ATR system achieves a BLEU score of 24.16, merely 0.45 BLEU points lower.",4.3 Results on English-German Translation,[0],[0]
"Notice that although all systems use the same training data of WMT14, the
tokenization of these work might be different from ours.",4.3 Results on English-German Translation,[0],[0]
"However, the overall results can indicate the competitive strength of our model.",4.3 Results on English-German Translation,[0],[0]
"In addition, SRU (Lei and Zhang, 2017), a recent proposed efficient recurrent unit, obtains a BLEU score of 20.70 with 10 layers, far more behind ATR’s.
",4.3 Results on English-German Translation,[0],[0]
We further ensemble eight likelihood-trained models with different random initializations for the ATR+CA system.,4.3 Results on English-German Translation,[0],[0]
The variance in the tokenized BLEU scores of these models is 0.07.,4.3 Results on English-German Translation,[0],[0]
"As can be seen from Table 2, the ensemble system achieves a tokenized and detokenized BLEU score of 24.97 and 24.33 respectively, obtaining a gain of 1.66 and 1.63 BLEU points over the single model.",4.3 Results on English-German Translation,[0],[0]
"The final result of the ensemble system, to the best of our knowledge, is a very promising result that can be reached by single-layer NMT systems on WMT14 English-German translation.",4.3 Results on English-German Translation,[0],[0]
"Unlike the above translation task, the WMT14 English-French translation task provides a significant larger dataset.",4.4 Results on English-French Translation,[0],[0]
"The full training data have approximately 36M sentence pairs, from which we only used 12M instances for experiments following previous work (Jean et al., 2015; Gehring et al., 2017a; Luong et al., 2015b; Wang et al., 2017a).",4.4 Results on English-French Translation,[0],[0]
"We show the results in Table 3.
",4.4 Results on English-French Translation,[0],[0]
"Our shallow model achieves a tokenized BLEU score of 36.89 and 37.88 when it is equipped
with the CA encoder, outperforming almost all the listed systems, except the Google NMT (Wu et al., 2016), the ConvS2S (Gehring et al., 2017b) and the Transformer (Vaswani et al., 2017).",4.4 Results on English-French Translation,[0],[0]
"Enhanced with the deep GNMT architecture, the GNMT+ATR system reaches a BLEU score of 38.59, which beats the base model version of the Transformer by a margin of 0.49 BLEU points.",4.4 Results on English-French Translation,[0],[0]
"When we use four ensemble models (the variance in the tokenized BLEU scores of these ensemble models is 0.16), the ATR+CA system obtains another gain of 0.47 BLEU points, reaching a tokenized BLEU score of 39.06, which is comparable with several state-of-the-art systems.",4.4 Results on English-French Translation,[0],[0]
We provide an illustration of the actual relation between the learned input and forget gate in Figure 3.,5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"Clearly, these two gates show strong negative correlation.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"When the input gate opens with high values, the forget gate prefer to be close.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"Quantitatively, on the whole test set, the Pearson’s r of the input and forget gate is -0.9819, indicating a high correlation.",5.1 Analysis on Twin-Gated Mechanism,[0],[0]
"As mentioned in Section 3.2, ATR has much fewer model parameters and matrix transformations.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We
provide more details in this section by comparing against the following two NMT systems:
• DeepRNNSearch (GRU): a deep GRUequipped RNNSearch model (Wu et al., 2016) with 5 layers.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We set the dimension of word embedding and hidden state to 620 and 1000 respectively.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"• Transformer: a purely attentional translator (Vaswani et al., 2017).",5.2 Analysis on Speed and Model Parameters,[0],[0]
We set the dimension of word embedding and filter size to 512 and 2048 respectively.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"The model was trained with a minibatch size of 256.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
We also compare with the GRU and LSTM-based RNNSearch.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"Without specific mention, all other experimental settings for all these models are the same as for our model.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We implement all these models using the Theano library, and test the speed on one GeForce GTX TITAN X GPU card.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We show the results on Table 4.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"We observe that the Transformer achieves the best training speed, processing 4961 words per second.",5.2 Analysis on Speed and Model Parameters,[0],[0]
This is reasonable since the Transformer can be trained in full parallelization.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"On the contrary, DeepRNNSearch is the slowest system.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"As RNN performs sequentially, stacking more layers of RNNs inevitably reduces the training efficiency.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"However, this situation becomes the reverse when it comes to the decoding procedure.",5.2 Analysis on Speed and Model Parameters,[0],[0]
The Transformer merely generates 44 words per second while DeepRNNSearch reaches 70.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"This is because during decoding, all these beam search-
based systems must generate translation one word after another.",5.2 Analysis on Speed and Model Parameters,[0],[0]
Therefore the parallelization advantage of the Transformer disappears.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"In comparison to DeepRNNSearch, the Transformer spends extra time on performing self-attention over all previous hidden states.
",5.2 Analysis on Speed and Model Parameters,[0],[0]
"Our model with the CA structure, using only 63.1M parameters, processes 3993 words per second during training and generates 186 words per second during decoding, which yields substantial speed improvements over the GRU- and LSTMequipped RNNSearch.",5.2 Analysis on Speed and Model Parameters,[0],[0]
This is due to the light matrix computation in recurrent units of ATR.,5.2 Analysis on Speed and Model Parameters,[0],[0]
Notice that the speed increase of ATR over GRU and LSTM does not reach 3x.,5.2 Analysis on Speed and Model Parameters,[0],[0]
"This is because at each decoding step, there are mainly two types of computation: recurrent unit and softmax layer.",5.2 Analysis on Speed and Model Parameters,[0],[0]
"The latter consumes the most calculation, which, however, is the same for different models (LSTM/GRU/ATR).",5.2 Analysis on Speed and Model Parameters,[0],[0]
"As shown in Section 3.3, a hidden state in our ATR can be formulated as a weighted sum of the previous inputs.",5.3 Analysis on Dependency Modeling,[0],[0]
"In this section, we quantitatively analyze the weights gk in Equation (10) induced from Equation (13).",5.3 Analysis on Dependency Modeling,[0],[0]
"Inspired by Lee et al. (2017), we visualize the captured dependencies of an example in Figure 4 where we connect each word to the corresponding previous word with the highest weight gk.
",5.3 Analysis on Dependency Modeling,[0],[0]
"Obviously, our model can discover strong local dependencies.",5.3 Analysis on Dependency Modeling,[0],[0]
"For example, the token “unglück@@” and “lichen” should be a
single word.",5.3 Analysis on Dependency Modeling,[0],[0]
Our model successfully associates “unglück@@” closely to the generation of “lichen” during decoding.,5.3 Analysis on Dependency Modeling,[0],[0]
"In addition, our model can also detect non-consecutive longdistance dependencies.",5.3 Analysis on Dependency Modeling,[0],[0]
"Particularly, the prediction of “Parteien” relies heavily on the token “unglücklichen”, which actually entails an amod linguistic dependency relationship.",5.3 Analysis on Dependency Modeling,[0],[0]
These captured dependencies make our model more interpretable than LSTM/GRU.,5.3 Analysis on Dependency Modeling,[0],[0]
This paper has presented a twin-gated recurrent network (ATR) to simplify neural machine translation.,6 Conclusion and Future Work,[0],[0]
"There are only two weight matrices and matrix transformations in recurrent units of ATR, making it efficient in physical memory usage and running speed.",6 Conclusion and Future Work,[0],[0]
"To avoid the gradient vanishing problem, ATR introduces a twin-gated mechanism to generate an input gate and forget gate through linear addition and subtraction operation respectively, without introducing any additional parameters.",6 Conclusion and Future Work,[0],[0]
"The simplifications allow ATR to produce interpretable results.
",6 Conclusion and Future Work,[0],[0]
Experiments on English-German and EnglishFrench translation tasks demonstrate the effectiveness of our model.,6 Conclusion and Future Work,[0],[0]
"They also show that ATR can be orthogonal to and applied with methods that improve LSTM/GRU-based NMT, indicated by the promising performance of the ATR+CA system.",6 Conclusion and Future Work,[0],[0]
Further analyses reveal that ATR can be trained more efficiently than GRU.,6 Conclusion and Future Work,[0],[0]
"It is also able to transparently model long-distance dependencies.
",6 Conclusion and Future Work,[0],[0]
We also adapt our ATR to other natural language processing tasks.,6 Conclusion and Future Work,[0],[0]
"Experiments show encouraging performance of our model on ChineseEnglish translation, natural language inference and Chinese word segmentation, demonstrating its generality and applicability on various NLP tasks.
",6 Conclusion and Future Work,[0],[0]
"In the future, we will continue to examine the effectiveness of ATR on different neural models for NMT, such as the hierarchical NMT model (Su et al., 2018b) as well as the generative NMT
model (Su et al., 2018a).",6 Conclusion and Future Work,[0],[0]
"We are also interested in adapting our ATR to summarization, semantic parsing etc.",6 Conclusion and Future Work,[0],[0]
"The authors were supported by National Natural Science Foundation of China (Grants No. 61672440, 61622209 and 61861130364), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49).",Acknowledgments,[0],[0]
Biao Zhang greatly acknowledges the support of the Baidu Scholarship.,Acknowledgments,[0],[0]
We also thank the reviewers for their insightful comments.,Acknowledgments,[0],[0]
"In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation.",abstractText,[0],[0]
The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs.,abstractText,[0],[0]
"With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated.",abstractText,[0],[0]
"Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved.",abstractText,[0],[0]
"Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification.",abstractText,[0],[0]
"Forward self-attention can be easily established in ATR, which makes the proposed network interpretable.",abstractText,[0],[0]
Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed.,abstractText,[0],[0]
"Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.",abstractText,[0],[0]
Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks,title,[0],[0]
Several machine learning settings are concerned with performing predictions in a very large discrete label space.,1. Introduction,[0],[0]
"From extreme multi-class classification to language modeling, one commonly used approach to this problem reduces it to a series of choices in a tree-structured model, where the leaves typically correspond to labels.",1. Introduction,[0],[0]
"While this allows for faster prediction, and is in many cases necessary to make the models tractable, the performance of the system can depend significantly on the structure of the tree used, e.g. (Mnih & Hinton, 2009).
",1. Introduction,[0],[0]
"Instead of relying on possibly costly heuristics (Mnih &
1New York University, New York, New York, USA 2Massachussets Institute of Technology, Cambridge, Massachussets, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yacine Jernite <jernite@cs.nyu.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Hinton, 2009), extrinsic hierarchies (Morin & Bengio, 2005) which can badly generalize across different data sets, or purely random trees, we provide an efficient datadependent algorithm for tree construction and training.",1. Introduction,[0],[0]
"Inspired by the LOM tree algorithm (Choromanska & Langford, 2015) for binary trees, we present an objective function which favors high-quality node splits, i.e. balanced and easily separable.",1. Introduction,[0],[0]
"In contrast to previous work, our objective applies to trees of arbitrary width and leads to guarantees on model accuracy.",1. Introduction,[0],[0]
"Furthermore, we show how to successfully optimize it in the setting when the data representation needs to be learned simultaneously with the classification tree.
",1. Introduction,[0],[0]
"Finally, the multi-class classification problem is closely related to that of conditional density estimation (Ram & Gray, 2011; Bishop, 2006) since both need to consider all labels (at least implicitly) during learning and at prediction time.",1. Introduction,[0],[0]
"Both problems present similar difficulties when dealing with very large label spaces, and the techniques that we present in this work can be applied indiscriminately to either.",1. Introduction,[0],[0]
"Indeed, we show how to adapt our algorithm to efficiently solve the conditional density estimation problem of learning a language model which uses a tree structured objective.
",1. Introduction,[0],[0]
"This paper is organized as follows: Section 2 discusses related work, Section 3 outlines the necessary background and defines the flat and tree-structured objectives for multi-class classification and density estimation, Section 4 presents the objective and the optimization algorithm, Section ??",1. Introduction,[0],[0]
"contains theoretical results, Section 5 adapts the algorithm to the problem of language modeling, Section 6 reports empirical results on the Flickr tag prediction dataset and Gutenberg text corpus, and finally Section 7 concludes the paper.",1. Introduction,[0],[0]
Supplementary material contains additional material and proofs of theoretical statements of the paper.,1. Introduction,[0],[0]
We also release the C++ implementation of our algorithm1.,1. Introduction,[0],[0]
The multi-class classification problem has been addressed in the literature in a variety of ways.,2. Related Work,[0],[0]
"Some examples include i) clustering methods (Bengio et al., 2010; Madzarov et al., 2009; Weston et al., 2013) ((Bengio et al., 2010)
1https://github.com/yjernite/fastTextLearnTree
was later improved in (Deng et al., 2011)), ii) sparse output coding (Zhao & Xing, 2013), iii) variants of error correcting output codes (Hsu et al., 2009), iv) variants of iterative least-squares (Agarwal et al., 2014), v) a method based on guess-averse loss functions (Beijbom et al., 2014), and vi) classification trees (Beygelzimer et al., 2009b; Choromanska & Langford, 2015; Daume et al., 2016) (that includes the Conditional Probability Trees (Beygelzimer et al., 2009a) when extended to the classification setting).
",2. Related Work,[0],[0]
"The recently proposed LOM tree algorithm (Choromanska & Langford, 2015) differs significantly from other similar hierarchical approaches, like for example Filter Trees (Beygelzimer et al., 2009b) or random trees (Breiman, 2001), in that it addresses the problem of learning good-quality binary node partitions.",2. Related Work,[0],[0]
"The method results in low-entropy trees and instead of using an inefficient enumerate-and-test approach, see e.g: (Breiman et al., 1984), to find a good partition or expensive brute-force optimization (Agarwal et al., 2013), it searches the space of all possible partitions with SGD (Bottou, 1998).",2. Related Work,[0],[0]
"Another work (Daume et al., 2016) uses a binary tree to map an example to a small subset of candidate labels and makes a final prediction via a more tractable one-against-all classifier, where this subset is identified with the proposed Recall Tree.",2. Related Work,[0],[0]
"A notable approach based on decision trees also include FastXML (Prabhu & Varma, 2014) (and its slower and less accurate at prediction predecessor (Agarwal et al., 2013)).",2. Related Work,[0],[0]
It is based on optimizing the rank-sensitive loss function and shows an advantage over some other ranking and NLP-based techniques in the context of multi-label classification.,2. Related Work,[0],[0]
"Other related approaches include the SLEEC classifier (Bhatia et al., 2015) for extreme multi-label classification that learns embeddings which preserve pairwise distances between only the nearest label vectors and ranking approaches based on negative sampling (Weston et al., 2011).",2. Related Work,[0],[0]
"Another tree approach (Kontschieder et al., 2015) shows no computational speed up but leads to significant improvements in prediction accuracy.
",2. Related Work,[0],[0]
Conditional density estimation can also be challenging in settings where the label space is large.,2. Related Work,[0],[0]
The underlying problem here consists in learning a probability distribution over a set of random variables given some context.,2. Related Work,[0],[0]
"For example, in the language modeling setting one can learn the probability of a word given the previous text, either by making a Markov assumption and approximating the left context by the last few words seen (n-grams e.g. (Jelinek & Mercer, 1980; Katz, 1987), feed-forward neural language models (Mnih & Teh, 2012; Mikolov et al., 2011; Schwenk & Gauvain, 2002)), or by attempting to learn a low-dimensional representation of the full history (RNNs (Mikolov et al., 2010; Mirowski & Vlachos, 2015; Tai et al., 2015; Kumar et al., 2015)).",2. Related Work,[0],[0]
"Both the recurrent and feed-forward Neural Probabilistic Language Models (NPLM) (Bengio et al., 2003) simultaneously learn a distributed representation for words and the probability function for word sequences, expressed in terms of these repre-
sentations.",2. Related Work,[0],[0]
"The major drawback of these models is that they can be slow to train, as they grow linearly with the vocabulary size (anywhere between 10,000 and 1M words), which can make them difficult to apply (Mnih & Teh, 2012).",2. Related Work,[0],[0]
A number of methods have been proposed to overcome this difficulty.,2. Related Work,[0],[0]
"Works such as LBL (Mnih & Hinton, 2007) or Word2Vec (Mikolov et al., 2013) reduce the model to its barest bones, with only one hidden layer and no nonlinearities.",2. Related Work,[0],[0]
"Another proposed approach has been to only compute the NPLM probabilities for a reduced vocabulary size, and use hybrid neural-n-gram model (Schwenk & Gauvain, 2005) at prediction time.",2. Related Work,[0],[0]
"Other avenues to reduce the cost of computing gradients for large vocabularies include using different sampling techniques to approximate it (Bengio & Sénécal, 2003; Bengio & Senecal, 2008; Mnih & Teh, 2012), replacing the likelihood objective by a contrastive one (Gutmann & Hyvärinen, 2012) or spherical loss (de Brébisson & Vincent, 2016), relying on self-normalizing models (Andreas & Klein, 2015), taking advantage of data sparsity (Vincent et al., 2015), or using clustering-based methods (Grave et al., 2016).",2. Related Work,[0],[0]
"It should be noted however that most of these techniques (to the exception of (Grave et al., 2016)) do not provide any speed up at test time.
",2. Related Work,[0],[0]
"Similarly to the classification case, there have also been a significant number of works that use tree structured models to accelerate computation of the likelihood and gradients (Morin & Bengio, 2005; Mnih & Hinton, 2009; Djuric et al., 2015; Mikolov et al., 2013).",2. Related Work,[0],[0]
"These use various heuristics to build a hierarchy, from using ontologies (Morin & Bengio, 2005) to Huffman coding (Mikolov et al., 2013).",2. Related Work,[0],[0]
"One algorithm which endeavors to learn a binary tree structure along with the representation is presented in (Mnih & Hinton, 2009).",2. Related Work,[0],[0]
"They iteratively learn word representations given a fixed tree structure, and use a criterion that trades off between making a balanced tree and clustering the words based on their current embedding.",2. Related Work,[0],[0]
"The application we present in the second part of our paper is most closely related to the latter work, and uses a similar embedding of the context.",2. Related Work,[0],[0]
"However, where their setting is limited to binary trees, we work with arbitrary width, and provide a tree building objective which is both less computationally costly and comes with theoretical guarantees.",2. Related Work,[0],[0]
"In this section, we define the classification and loglikelihood objectives we wish to maximize.",3. Background,[0],[0]
"Let X be an input space, and V a label space.",3. Background,[0],[0]
"Let P be a joint distribution over samples in (X ,V), and let fΘ : X → Rdr be a function mapping every input x ∈ X to a representation r ∈ Rdr , and parametrized by Θ (e.g. as a neural network).
",3. Background,[0],[0]
We consider two objectives.,3. Background,[0],[0]
"Let g be a function that takes an input representation r ∈ Rdr , and predicts for it a label g(r) ∈ V .",3. Background,[0],[0]
"The classification objective is defined as the expected proportion of correctly classified examples:
Oclass(Θ, g) = E(x,y)∼P [ 1[g ◦ fΘ(x) = y] ] (1)
Now, let pθ(·|r) define a conditional probability distribution (parametrized by θ) over V for any r ∈ Rdr .",3. Background,[0],[0]
"The density estimation task consists in maximizing the expected log-likelihood of samples from (X ,V):
Oll(Θ, θ) = E(x,y)∼P [ log pθ(y|fΘ(x))",3. Background,[0],[0]
"]
(2)
Tree-Structured Classification and Density Estimation Let us now show how to express the objectives in Equations 1 and 2 when using tree-structured prediction functions (with fixed structure) as illustrated in Figure 1.
Consider a tree T of depth D and arity M with K = |V| leaf nodes and N internal nodes.",3. Background,[0],[0]
"Each leaf l corresponds to a label, and can be identified with the path cl from the root to the leaf.",3. Background,[0],[0]
"In the rest of the paper, we will use the following notations:
cl = ((cl1,1, c l 1,2), . . .",3. Background,[0],[0]
", (c l d,1, c l d,2), . . .",3. Background,[0],[0]
", (c l D,1, c l D,2)), (3)
where cld,1 ∈",3. Background,[0],[0]
"[1, N ] correspond to the node index at depth d, and cld,2 ∈",3. Background,[0],[0]
"[1,M ] indicates which child of cld,1 is next in the path.",3. Background,[0],[0]
"In that case, our classification and density estimation problems are reduced to choosing the right child of a node or defining a probability distribution over children given x ∈ X respectively.
",3. Background,[0],[0]
We then need to replace g and pθ with node decision functions (gn)Nn=1 and conditional probability distributions (pθn) N n=1 respectively.,3. Background,[0],[0]
"Given such a tree and representation function, our objective functions then become:
Oclass(Θ, g) = E(x,y)∼P [ D∏ d=1 1[gcld,1 ◦ fΘ(x) =",3. Background,[0],[0]
"c l d,2] ] (4)
",3. Background,[0],[0]
"Oll(Θ, θ) = E(x,y)∼P",3. Background,[0],[0]
[,3. Background,[0],[0]
"D∑ d=1 log pθ cl d,1 (cld,2|fΘ(x))",3. Background,[0],[0]
"]
(5)
",3. Background,[0],[0]
"The tree objectives defined in Equations 4 and 5 can be optimized in the space of parameters of the representation
and node functions using standard gradient ascent methods.",3. Background,[0],[0]
"However, they also implicitly depend on the tree structure T .",3. Background,[0],[0]
"In the rest of the paper, we provide a surrogate objective function which determines the structure of the tree and, as we show theoretically (Section ??)",3. Background,[0],[0]
", maximizes the criterion in Equation 4 and, as we show empirically (Sections 5 and 6), maximizes the criterion in Equation 5.",3. Background,[0],[0]
"In this section, we introduce a per-node objective Jn which leads to good quality trees when maximized, and provide an algorithm to optimize it.",4. Learning Tree-Structured Objectives,[0],[0]
"We define the node objective Jn for node n as:
Jn = 2
M K∑ i=1 q",4.1. Objective function,[0],[0]
(n),4.1. Objective function,[0],[0]
i M∑ j=1 |p(n)j − p (n) j|i,4.1. Objective function,[0],[0]
"|, (6)
where q(n)i denotes the proportion of nodes reaching node n that are of class i, p(n)j|i is the probability that an example of class i reaching n will be sent to its jth child, and p(n)j is the probability that an example of any class reaching n will be sent to its jth child.",4.1. Objective function,[0],[0]
"Note that we have:
∀j ∈",4.1. Objective function,[0],[0]
"[1,M ], p(n)j = K∑ i=1",4.1. Objective function,[0],[0]
q,4.1. Objective function,[0],[0]
(n),4.1. Objective function,[0],[0]
i p (n) j|i .,4.1. Objective function,[0],[0]
"(7)
The objective in Equation 6 reduces to the LOM tree objective in the case of M = 2.
",4.1. Objective function,[0],[0]
"At a high level, maximizing the objective encourages the conditional distribution for each class to be as different as possible from the global one; so the node decision function needs to be able to discriminate between examples of the different classes.",4.1. Objective function,[0],[0]
The objective thus favors balanced and pure node splits.,4.1. Objective function,[0],[0]
"To wit, we call a split at node n perfectly balanced when the global distribution p(n)· is uniform, and perfectly pure when each p(n)·|i takes value either 0 or 1, as all data points from the same class reaching node n are sent to the same child.
",4.1. Objective function,[0],[0]
In Section ??,4.1. Objective function,[0],[0]
we discuss the theoretical properties of this objective in details.,4.1. Objective function,[0],[0]
We show that maximizing it leads to perfectly balanced and perfectly pure splits.,4.1. Objective function,[0],[0]
"We also derive the boosting theorem that shows the number of internal nodes that the tree needs to have to reduce the classification error below any arbitrary threshold, under the assumption that the objective is “weakly” optimized in each node of the tree.
",4.1. Objective function,[0],[0]
Remark 1.,4.1. Objective function,[0],[0]
"In the rest of the paper, we use node functions gn which take as input a data representation r ∈ Rdr and output a distribution over children of n (for example using a soft-max function).",4.1. Objective function,[0],[0]
"When used in the classification setting, gn sends the data point to the child with the highest predicted probability.",4.1. Objective function,[0],[0]
"With this notation, and representa-
Algorithm 1 Tree Learning Algorithm Input Input representation function: f with parameters
Θf .",4.1. Objective function,[0],[0]
Node decisions functions (gn)Kn=1 with parameters (Θn)Kn=1.,4.1. Objective function,[0],[0]
"Gradient step size .
",4.1. Objective function,[0],[0]
"Ouput Learned M -ary tree, parameters Θf and (Θn)Kn=1.
procedure InitializeNodeStats () for n = 1 to N do
for i = 1 to K do SumProbasn,i ← 0",4.1. Objective function,[0],[0]
"Countsn,i ← 0
procedure NodeCompute (w, n, i, target) p← gn(w) SumProbasn,i ← SumProbasn,i + p Countsn,i ← Countsn,i + 1 //",4.1. Objective function,[0],[0]
"Gradient step in the node parameters Θn ← Θn + ∂ptarget∂Θn return ∂ptarget∂w
InitializeNodeStats () for Each batch b do
// AssignLabels () re-builds the tree based on the // current node statistics AssignLabels ({1, . .",4.1. Objective function,[0],[0]
.,4.1. Objective function,[0],[0]
",K}, root) for each example (x, i) in b do
Compute input representation w = f(x) ∆w← 0 for d = 1 to D do
// ci1,...,D is the current path from the root to i Set node id and target: (n, j)← cid ∆w← ∆w + NodeCompute (w, n, i, j)
//",4.1. Objective function,[0],[0]
Gradient step in the parameters of f Θf ← Θf + ∂f∂Θf,4.1. Objective function,[0],[0]
"∆w
tion function fΘ, we can write:
p (n) j := E(x,y)∼P",4.1. Objective function,[0],[0]
[gn ◦ fΘ(x)],4.1. Objective function,[0],[0]
"(8)
and p
(n) j|i",4.1. Objective function,[0],[0]
":= E(x,y)∼P",4.1. Objective function,[0],[0]
[gn ◦ fΘ(x)|y = i].,4.1. Objective function,[0],[0]
"(9)
An intuitive geometric interpretation of probabilities p(n)j and p(n)j|i can be found in the Supplementary material.",4.1. Objective function,[0],[0]
In this section we present an algorithm for simultaneously building the classification tree and learning the data representation.,4.2. Algorithm,[0],[0]
We aim at maximizing the accuracy of the tree as defined in Equation 4 by maximizing the objective Jn of Equation 6 at each node of the tree (the boosting theorem that will be presented in Section ??,4.2. Algorithm,[0],[0]
"shows the connection between the two).
",4.2. Algorithm,[0],[0]
"Algorithm 2 Label Assignment Algorithm Input labels currently reaching the node
node ID n Ouput Lists of labels now assigned to the node’s children
procedure CheckFull (full, assigned, count, j) if |assignedj | ≡ 2 mod (M − 1) then
count← count− (M − 1) if count = 0 then
full← full ∪ {j} if count = 1 then
count← 0 for j′ s.t.",4.2. Algorithm,[0],[0]
"|assignedj′ | ≡ 1 mod (M − 1) do
full← full ∪ {j′}
procedure AssignLabels (labels, n) //",4.2. Algorithm,[0],[0]
"first, compute p(n)j and p (n) j|i .
",4.2. Algorithm,[0],[0]
"pavg0 ← 0 count← 0 for i in labels do
pavg0 ← p avg 0 + SumProbasn,i count← count +",4.2. Algorithm,[0],[0]
"Countsn,i pavgi ← SumProbasn,i/Countsn,i
pavg0 ← p avg 0 /count //",4.2. Algorithm,[0],[0]
"then, assign each label to a child of n unassigned← labels full← ∅ count← (|unassigned| − (M − 1)) for j = 1 to M do
assignedj ← ∅ while unassigned 6= ∅",4.2. Algorithm,[0],[0]
"do//
∂Jn ∂p (n) j|i is given in Equation 10
(i∗, j∗)← argmax i∈unassigned,j 6∈full ( ∂Jn ∂p (n) j|i )",4.2. Algorithm,[0],[0]
"if n = root then
ci ∗",4.2. Algorithm,[0],[0]
"← (n, j∗)
else ci ∗",4.2. Algorithm,[0],[0]
"← (ci∗ , (n, j∗))",4.2. Algorithm,[0],[0]
assignedj∗ ← assignedj∗ ∪ {i∗} unassigned← unassigned \ {i∗} CheckFull,4.2. Algorithm,[0],[0]
"(full, assigned, count, j∗)
for j = 1 to M do AssignLabels (assignedj , childn,j , d+ 1) return assigned
Let us now show how we can efficiently optimize Jn.",4.2. Algorithm,[0],[0]
"The gradient of Jn with respect to the conditional probability distributions is (see proof of Lemma 1 in the Supplement):
∂Jn
∂p (n) j|i
= 2
M q
(n)",4.2. Algorithm,[0],[0]
i (1− q (n) i ),4.2. Algorithm,[0],[0]
sign(p (n) j|i,4.2. Algorithm,[0],[0]
− p (n) j ).,4.2. Algorithm,[0],[0]
"(10)
Then, according to Equation 10, increasing the likelihood of sending label i to any child j of n such that p(n)j|i > p (n) j increases the objective Jn.",4.2. Algorithm,[0],[0]
"Note that we only need to con-
sider the labels i for which q(n)i > 0, that is, labels i which reach node n in the current tree.
",4.2. Algorithm,[0],[0]
"We also want to make sure that we have a well-formed M - ary tree at each step, which means that the number of labels assigned to any node is always congruent to 1 modulo (M − 1).",4.2. Algorithm,[0],[0]
"Algorithm 2 provides such an assignment by greedily choosing the label-child pair (i, j) such that j still has room for labels with the highest value of ∂Jn
∂p (n) j|i .
",4.2. Algorithm,[0],[0]
"The global procedure, described in Algorithm 1, is then the following.
",4.2. Algorithm,[0],[0]
"• At the start of each batch, re-assign targets for each node prediction function, starting from the root and going down the tree.",4.2. Algorithm,[0],[0]
"At each node, each label is more likely to be re-assigned to the child it has had most affinity with in the past (Algorithm 2).",4.2. Algorithm,[0],[0]
"This can be seen as a form of hierarchical on-line clustering.
",4.2. Algorithm,[0],[0]
•,4.2. Algorithm,[0],[0]
Every example now has a unique path depending on its label.,4.2. Algorithm,[0],[0]
"For each sample, we then take a gradient step at each node along the assigned path (see Algorithm 1).
",4.2. Algorithm,[0],[0]
Lemma 1.,4.2. Algorithm,[0],[0]
"Algorithm 2 finds the assignment of nodes to children for a fixed depth tree which most increases Jn under well-formedness constraints.
",4.2. Algorithm,[0],[0]
Remark 2.,4.2. Algorithm,[0],[0]
"An interesting feature of the algorithm, is that since the representation of examples from different classes are learned together, there is intuitively less of a risk of getting stuck in a specific tree configuration.",4.2. Algorithm,[0],[0]
"More specifically, if two similar classes are initially assigned to different children of a node, the algorithm is less likely to keep this initial decision since the representations for examples of both classes will be pulled together in other nodes.
",4.2. Algorithm,[0],[0]
"Next, we provide a theoretical analysis of the objective introduced in Equation 6.",4.2. Algorithm,[0],[0]
Proofs are deferred to the Supplementary material.,4.2. Algorithm,[0],[0]
"In this section, we first analyze theoretical properties of the objective Jn as regards node quality, then prove a boosting statement for the global tree accuracy.",5. Theoretical Results,[0],[0]
"We start by showing that maximizing Jn in every node of the tree leads to high-quality nodes, i.e. perfectly balanced and perfectly pure node splits.",5.1. Properties of the objective function,[0],[0]
"Let us first introduce some formal definitions.
",5.1. Properties of the objective function,[0],[0]
Definition 1 (Balancedness factor).,5.1. Properties of the objective function,[0],[0]
"The split in node n of the tree is β(n)-balanced if
β(n)",5.1. Properties of the objective function,[0],[0]
"≤ min j={1,2,...,M} p (n) j ,
where β(n) ∈ (0, 1M ] is a balancedness factor.
",5.1. Properties of the objective function,[0],[0]
A split is perfectly balanced if and only if β(n),5.1. Properties of the objective function,[0],[0]
= 1M .,5.1. Properties of the objective function,[0],[0]
Definition 2 (Purity factor).,5.1. Properties of the objective function,[0],[0]
"The split in node n of the tree is α(n)-pure if
1
M M∑ j=1 K∑ i=1",5.1. Properties of the objective function,[0],[0]
q,5.1. Properties of the objective function,[0],[0]
(n),5.1. Properties of the objective function,[0],[0]
"i min ( p (n) j|i , 1− p (n) j|i )",5.1. Properties of the objective function,[0],[0]
"≤ α(n),
where α(n) ∈",5.1. Properties of the objective function,[0],[0]
"[0, 1M ) is a purity factor.
",5.1. Properties of the objective function,[0],[0]
"A split is perfectly pure if and only if α(n) = 0.
",5.1. Properties of the objective function,[0],[0]
The following lemmas characterize the range of the objective Jn and link it to the notions of balancedness and purity of the split.,5.1. Properties of the objective function,[0],[0]
Lemma 2.,5.1. Properties of the objective function,[0],[0]
"The objective function Jn lies in the interval[ 0, 4M ( 1− 1M )] .
",5.1. Properties of the objective function,[0],[0]
"Let J∗ denotes the highest possible value of Jn, i.e. J∗ = 4 M ( 1− 1M ) .",5.1. Properties of the objective function,[0],[0]
Lemma 3.,5.1. Properties of the objective function,[0],[0]
"The objective function Jn admits the highest value, i.e. Jn = J∗, if and only if the split in node n is perfectly balanced, i.e. β(n)",5.1. Properties of the objective function,[0],[0]
"= 1M , and perfectly pure, i.e. α(n) = 0.
",5.1. Properties of the objective function,[0],[0]
We next show Lemmas ??,5.1. Properties of the objective function,[0],[0]
and ??,5.1. Properties of the objective function,[0],[0]
"which analyze balancedness and purity of a node split in isolation, i.e. we analyze resp. balancedness and purity of a node split when resp.",5.1. Properties of the objective function,[0],[0]
purity and balancedness is fixed and perfect.,5.1. Properties of the objective function,[0],[0]
"We show that in such isolated setting increasing Jn leads to a more balanced and more pure split.
",5.1. Properties of the objective function,[0],[0]
Lemma 4.,5.1. Properties of the objective function,[0],[0]
"If a split in node n is perfectly pure, then
β(n) ∈
[ 1 M − √ M(J∗ − Jn) 2 , 1 M ] .
",5.1. Properties of the objective function,[0],[0]
Lemma 5.,5.1. Properties of the objective function,[0],[0]
"If a split in node n is perfectly balanced, then α(n) ≤ (J∗ − Jn)/2.
",5.1. Properties of the objective function,[0],[0]
Next we provide a bound on the classification error for the tree.,5.1. Properties of the objective function,[0],[0]
"In particular, we show that if the objective is “weakly” optimized in each node of the tree, where this weak advantage is captured in a form of the Weak Hypothesis Assumption, then our algorithm will amplify this weak advantage to build a tree achieving any desired level of accuracy.",5.1. Properties of the objective function,[0],[0]
"Denote y(x) to be a fixed target function with domain X , which assigns the data point x to its label, and let P be a fixed target distribution over X .",5.2. Error bound,[0],[0]
"Together y and P induce a distribution on labeled pairs (x, y(x)).",5.2. Error bound,[0],[0]
Let t(x) be the label assigned to data point x by the tree.,5.2. Error bound,[0],[0]
"We denote as (T ) the error of tree T , i.e. (T ) := Ex∼P",5.2. Error bound,[0],[0]
[∑K i=1,5.2. Error bound,[0],[0]
1[t(x),5.2. Error bound,[0],[0]
"= i, y(x) 6=",5.2. Error bound,[0],[0]
"i]
] (1− (T ) refers to the accuracy as given by Equation 4).",5.2. Error bound,[0],[0]
"Then the following theorem holds
Theorem 1.",5.2. Error bound,[0],[0]
"The Weak Hypothesis Assumption says that for any distribution P over the data, at each node n of the tree T there exists a partition",5.2. Error bound,[0],[0]
"such that Jn ≥ γ, where
γ ∈",5.2. Error bound,[0],[0]
"[ M 2 minj=1,2,...,M pj , 1− M2 minj=1,2,...,M pj ] .
",5.2. Error bound,[0],[0]
"Under the Weak Hypothesis Assumption, for any κ ∈",5.2. Error bound,[0],[0]
"[0, 1], to obtain (T ) ≤ κ",5.2. Error bound,[0],[0]
"it suffices to have a tree with
N ≥ ( 1
κ
) 16[M(1−2γ)+2γ](M−1)",5.2. Error bound,[0],[0]
"log2 eM 2γ2 lnK
internal nodes.
",5.2. Error bound,[0],[0]
The above theorem shows the number of splits that suffice to reduce the multi-class classification error of the tree below an arbitrary threshold κ.,5.2. Error bound,[0],[0]
"As shown in the proof of the above theorem, the Weak Hypothesis Assumption implies that all pjs satisfy: pj ∈",5.2. Error bound,[0],[0]
"[ 2γM , M(1−2γ)+2γ M ].",5.2. Error bound,[0],[0]
Below we show a tighter version of this bound when assuming that each node induces balanced split.,5.2. Error bound,[0],[0]
Corollary 1.,5.2. Error bound,[0],[0]
"The Weak Hypothesis Assumption says that for any distribution P over the data, at each node n of the tree T there exists a partition such that Jn ≥ γ, where γ ∈ R+.
",5.2. Error bound,[0],[0]
"Under the Weak Hypothesis Assumption and when all nodes make perfectly balanced splits, for any κ ∈",5.2. Error bound,[0],[0]
"[0, 1], to obtain (T ) ≤ κ",5.2. Error bound,[0],[0]
"it suffices to have a tree with
N ≥ ( 1
κ
) 16(M−1)",5.2. Error bound,[0],[0]
"log2 eM 2γ2 lnK
internal nodes.",5.2. Error bound,[0],[0]
"We now show how to adapt the algorithm presented in Section 4 for conditional density estimation, using the example of language modeling.
",6. Extension to Density Estimation,[0],[0]
Hierarchical Log Bi-Linear Language Model (HLBL),6. Extension to Density Estimation,[0],[0]
"We take the same approach to language modeling as (Mnih & Hinton, 2009).",6. Extension to Density Estimation,[0],[0]
"First, using the chain rule and an order T Markov assumption we model the probability of a sentence w = (w1, w2, . . .",6. Extension to Density Estimation,[0],[0]
", wn) as:
p(w1, w2, . . .",6. Extension to Density Estimation,[0],[0]
", wn) = n∏ t=1 p(wt|wt−T,...,t−1)
Similarly to their work, we also use a low dimensional representation of the context (wt−T,...,t−1).",6. Extension to Density Estimation,[0],[0]
"In this setting, each word w in the vocabulary V has an embedding Uw ∈ Rdr .",6. Extension to Density Estimation,[0],[0]
"A given context x = (wt−T , . . .",6. Extension to Density Estimation,[0],[0]
", wt−1) corresponding to position t is then represented by a context embedding vector rx such that
rx = T∑ k=1 RkUwt−k ,
where U ∈ R|V|×dr is the embedding matrix, and Rk ∈ Rdr×dr is the transition matrix associated with the kth context word.
",6. Extension to Density Estimation,[0],[0]
"The most straight-forward way to define a probability function is then to define the distribution over the next word given the context representation as a soft-max, as done in (Mnih & Hinton, 2007).",6. Extension to Density Estimation,[0],[0]
"That is:
p(wt = i|x) = σi(r>x U + b)
= exp(r>x Ui + bi)∑
w∈V exp(r > x Uw + bw)
,
where bw is the bias for word w.",6. Extension to Density Estimation,[0],[0]
"However, the complexity of computing this probability distribution in this setting is O(|V |×dr), which can be prohibitive for large corpora and vocabularies.
",6. Extension to Density Estimation,[0],[0]
"Instead, (Mnih & Hinton, 2009) takes a hierarchical approach to the problem.",6. Extension to Density Estimation,[0],[0]
"They construct a binary tree, where each word w ∈ V corresponds to some leaf of the tree, and can thus be identified with the path from the root to the corresponding leaf by making a sequence of choices of going left versus right.",6. Extension to Density Estimation,[0],[0]
"This corresponds to the treestructured log-likelihood objective presented in Equation 5 for the case where M = 2, and fΘ(x) = rx.",6. Extension to Density Estimation,[0],[0]
"Thus, if ci is the path to word i as defined in Expression 3, then:
log p(wt = i|x) = D∑ d=1 log σcid,2((r > x U cid,1 + bc i d,1) (11)
",6. Extension to Density Estimation,[0],[0]
"In this binary case, σ is the sigmoid function, and for all non-leaf nodes n ∈",6. Extension to Density Estimation,[0],[0]
"{1, 2, . . .",6. Extension to Density Estimation,[0],[0]
", N}, we have Un ∈ Rdr and bn ∈ Rdr .",6. Extension to Density Estimation,[0],[0]
The cost of computing the likelihood of word w is then reduced to O(log(|V|) × dr).,6. Extension to Density Estimation,[0],[0]
"In their work, the authors start the training procedure by using a random tree, then alternate parameter learning with using a clusteringbased heuristic to rebuild their hierarchy.",6. Extension to Density Estimation,[0],[0]
"We expand upon their method by providing an algorithm which allows for using hierarchies of arbitrary width, and jointly learns the tree structure and the model parameters.
",6. Extension to Density Estimation,[0],[0]
"Using our Algorithm We may use Algorithm 1 as is to learn a good tree structure for classification: that is, a model that often predictswt to be the most likely word after seeing the context (wt−T , . . .",6. Extension to Density Estimation,[0],[0]
", wt−1).",6. Extension to Density Estimation,[0],[0]
"However, while this could certainly learn interesting representations and tree structure, there is no guarantee that such a model would achieve a good average log-likelihood.",6. Extension to Density Estimation,[0],[0]
"Intuitively, there are often several valid possibilities for a word given its immediate left context, which a classification objective does not necessarily take into account.",6. Extension to Density Estimation,[0],[0]
"Yet another option would be to learn a tree structure that maximizes the classification objective, then fine-tune the model parameters using the log-likelihood objective.",6. Extension to Density Estimation,[0],[0]
"We tried this method, but initial tests of this approach did not do much better than the use of random trees.",6. Extension to Density Estimation,[0],[0]
"Instead, we present here a small modification of Algorithm 1 which is equivalent to log-likelihood training when restricted to the fixed tree setting, and can be shown to increase the value of the node objectives Jn: by replacing the gradients with respect to ptarget by those with respect to log ptarget.",6. Extension to Density Estimation,[0],[0]
"Then, for a given tree structure, the algorithm takes a gradient step with respect to the
log-likelihood of the samples:
∂Jn
∂ log p (n) j|i
= 2
M q
(n)",6. Extension to Density Estimation,[0],[0]
i (1−q (n) i ),6. Extension to Density Estimation,[0],[0]
sign(p (n) j|i −p (n) j )p (n) j|i .,6. Extension to Density Estimation,[0],[0]
"(12)
Lemma 1 extends to the new version of the algorithm.",6. Extension to Density Estimation,[0],[0]
We ran experiments to evaluate both the classification and density estimation version of our algorithm.,7. Experiments,[0],[0]
"For classification, we used the YFCC100M dataset (Thomee et al., 2016), which consists of a set of a hundred million Flickr pictures along with captions and tag sets split into 91M training, 930K validation and 543K test examples.",7. Experiments,[0],[0]
We focus here on the problem of predicting a picture’s tags given its caption.,7. Experiments,[0],[0]
"For density estimation, we learned a logbilinear language model on the Gutenberg novels corpus, and compared the perplexity to that obtained with other flat and hierarchical losses.",7. Experiments,[0],[0]
Experimental settings are described in greater detail in the Supplementary material.,7. Experiments,[0],[0]
"We follow the setting of (Joulin et al., 2016) for the YFCC100M tag prediction task: we only keep the tags which appear at least a hundred times, which leaves us with a label space of size 312K. We compare our results to those obtained with the FastText software (Joulin et al., 2016), which uses a binary hierarchical softmax objective based on Huffman coding (Huffman trees are designed to minimize the expected depth of their leaves weighed by frequencies and have been shown to work well with word embedding systems (Mikolov et al., 2013)), and to the Tagspace system (Weston et al., 2014), which uses a sampling-based margin loss (this allows for training in tractable time, but does not help at test time, hence the long times reported).",7.1. Classification,[0],[0]
We also extend the FastText software to use Huffman trees of arbitrary width.,7.1. Classification,[0],[0]
All models use a bagof-word embedding representation of the caption text; the parameters of the input representation function fΘ which we learn are the word embeddings Uw ∈ Rd (as in Section 5) and a caption representation is obtained by summing the embeddings of its words.,7.1. Classification,[0],[0]
We experimented with embeddings of dimension d = 50 and d = 200.,7.1. Classification,[0],[0]
"We predict one tag for each caption, and report the precision as well as the training and test times in Table 1.
",7.1. Classification,[0],[0]
"Our implementation is based on the FastText open source version2, to which we added M -ary Huffman and learned tree objectives.",7.1. Classification,[0],[0]
"Table 1 reports the best accuracy we obtained with a hyper-parameter search using this version on our system so as to provide the most meaningful comparison, even though the accuracy is less than that reported in (Joulin et al., 2016).
",7.1. Classification,[0],[0]
We gain a few different insights from Table 1.,7.1. Classification,[0],[0]
"First, al-
2https://github.com/facebookresearch/fastText
though wider trees are theoretically slower (remember that the theoretical complexity isO(M logM (N)) for anM -ary tree with N labels), they run incomparable time in practice and always perform better.",7.1. Classification,[0],[0]
"Using our algorithm to learn the structure of the tree also always leads to more accurate models, with a gain of up to 3.3 precision points in the smaller 5-ary setting.",7.1. Classification,[0],[0]
"Further, both the importance of having wider trees and learning the structure seems to be less when the node prediction functions become more expressive.",7.1. Classification,[0],[0]
"At a high level, one could imagine that in that setting, the model can learn to use different dimensions of the input representation for different nodes, which would minimize the negative impact of having to learn a representation which is suited to more nodes.
",7.1. Classification,[0],[0]
"Another thing to notice is that since prediction time only depends on the expected depth of a label, our models which learned balanced trees are nearly as fast as Huffman coding which is optimal in that respect (except for the dimension 200, 20-ary tree, but the tree structure had not stabilized yet in that setting).",7.1. Classification,[0],[0]
"Given all of the above remarks, our algorithm especially shines in settings where computational complexity and prediction time are highly constrained at test time, such as mobile devices or embedded systems.",7.1. Classification,[0],[0]
"We also ran language modeling experiments on the Gutenberg novel corpus3, which has about 50M tokens and a vocabulary of 250,000 words.
",7.2. Density Estimation,[0],[0]
"One notable difference from the previous task is that the language modeling setting can drastically benefit from the use of GPU computing, which can make using a flat softmax tractable (if not fast).",7.2. Density Estimation,[0],[0]
"While our algorithm requires
3http://www.gutenberg.org/
more flexibility and thus does not benefit as much from the use of GPUs, a small modification of Algorithm 2 (described in the Supplementary material) allows it to run under a maximum depth constraint and remain competitive.",7.2. Density Estimation,[0],[0]
"The results presented in this section are obtained using this modified version, which learns 65-ary trees of depth 3.
",7.2. Density Estimation,[0],[0]
"Table 2 presents perplexity results for different loss functions, along with the time spent on computing and learning the objective (softmax parameters for the flat version, hierarchical softmax node parameters for the fixed tree, and hierarchical softmax structure and parameters for our algorithm).",7.2. Density Estimation,[0],[0]
"The learned tree model is nearly three and seven times as fast at train and test time respectively as the flat objective without losing any points of perplexity.
",7.2. Density Estimation,[0],[0]
Huffman coding does not apply to trees where all of the leaves are at the same depth.,7.2. Density Estimation,[0],[0]
"Instead, we use the following heuristic as a baseline, inspired by (Mnih & Hinton, 2009): we learn word embeddings using FastText, perform a hierarchical clustering of the vocabulary based on these, then use the resulting tree to learn a new language model.",7.2. Density Estimation,[0],[0]
We call this approach “Clustering Tree”.,7.2. Density Estimation,[0],[0]
"However, for all hyper-parameter settings, this tree structure did worse
than a random one.",7.2. Density Estimation,[0],[0]
We conjecture that its poor performance is because such a tree structure means that the deepest node decisions can be quite difficult.,7.2. Density Estimation,[0],[0]
"Finally, we also ran density estimation experiments on the Penn TreeBank data set, which consists of 1M tokens with a vocabulary size of 10,000, with sensibly similar performance results and a speedup factor of two (see supplementary material).",7.2. Density Estimation,[0],[0]
"It should be noted that running a softmax on a label set of this size (only 10K) fits comfortably on most modern GPUs (hence the comparatively smaller speed gain).
",7.2. Density Estimation,[0],[0]
Figure 2 shows the evolution of the test perplexity for a few epochs.,7.2. Density Estimation,[0],[0]
"It appears that most of the relevant tree structure can be learned in one epoch: from the second epoch on, the learned hierarchical soft-max performs similarly to the flat one.",7.2. Density Estimation,[0],[0]
"Figure 3 shows a part of the tree learned on the Gutenberg dataset, which appears to make semantic and syntactic sense.",7.2. Density Estimation,[0],[0]
"In this paper, we introduced a provably accurate algorithm for jointly learning tree structure and data representation for hierarchical prediction.",8. Conclusion,[0],[0]
"We applied it to a multiclass classification and a density estimation problem, and showed our models’ ability to achieve favorable accuracy in competitive times in both settings.",8. Conclusion,[0],[0]
We consider multi-class classification where the predictor has a hierarchical structure that allows for a very large number of labels both at train and test time.,abstractText,[0],[0]
"The predictive power of such models can heavily depend on the structure of the tree, and although past work showed how to learn the tree structure, it expected that the feature vectors remained static.",abstractText,[0],[0]
We provide a novel algorithm to simultaneously perform representation learning for the input data and learning of the hierarchical predictor.,abstractText,[0],[0]
Our approach optimizes an objective function which favors balanced and easilyseparable multi-way node partitions.,abstractText,[0],[0]
"We theoretically analyze this objective, showing that it gives rise to a boosting style property and a bound on classification error.",abstractText,[0],[0]
We next show how to extend the algorithm to conditional density estimation.,abstractText,[0],[0]
"We empirically validate both variants of the algorithm on text classification and language modeling, respectively, and show that they compare favorably to common baselines in terms of accuracy and running time.",abstractText,[0],[0]
Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2173–2182, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Verbs are famously both complex and variable.,1 Introduction,[0],[0]
"They express the semantics of an event as well the relational information among participants in that event, and they display a rich range of syntactic and semantic behaviour (Jackendoff, 1972; Gruber, 1976; Levin, 1993).",1 Introduction,[0],[0]
Verbs play a key role at almost every level of linguistic analysis.,1 Introduction,[0],[0]
"Information related to their predicate argument structure can benefit many NLP tasks (e.g. parsing, semantic role labelling, information extraction) and applications (e.g. machine translation,
text mining) as well as research on human language acquisition and processing (Korhonen, 2010).",1 Introduction,[0],[0]
"Precise methods for representing and understanding verb semantics will undoubtedly be necessary for machines to interpret the meaning of sentences with similar accuracy to humans.
",1 Introduction,[0],[0]
"Numerous algorithms for acquiring word representations from text and/or more structured knowledge bases have been developed in recent years (Mikolov et al., 2013; Pennington et al., 2014; Faruqui et al., 2015).",1 Introduction,[0],[0]
"These representations (or embeddings) typically contain powerful features that are applicable to many language applications (Collobert and Weston, 2008; Turian et al., 2010).",1 Introduction,[0],[0]
"Nevertheless, the predominant approaches to distributed representation learning apply a single learning algorithm and representational form for all words in a vocabulary.",1 Introduction,[0],[0]
"This is despite evidence that applying different learning algorithms to word types such as nouns, adjectives and verbs can significantly increase the ultimate usefulness of representations (Schwartz et al., 2015).
",1 Introduction,[0],[0]
One factor behind the lack of more nuanced word representation learning methods is the scarcity of satisfactory ways to evaluate or analyse representations of particular word types.,1 Introduction,[0],[0]
"Resources such as MEN (Bruni et al., 2014), Rare Words (Luong et al., 2013) and SimLex-999 (Hill et al., 2015) focus either on words from a single class or small samples of different word types, with automatic approaches already reaching or surpassing the inter-annotator agreement ceiling.",1 Introduction,[0],[0]
"Consequently, for word classes such as verbs, whose semantics is critical for language understanding, it is practically impossible to achieve statistically robust analyses and comparisons between different
2173
representation learning architectures.",1 Introduction,[0],[0]
"To overcome this barrier to verb semantics research, we introduce SimVerb-3500 – an extensive intrinsic evaluation resource that is unprecedented in both size and coverage.",1 Introduction,[0],[0]
"SimVerb-3500 includes 827 verb types from the University of South Florida Free Association Norms (USF) (Nelson et al., 2004), and at least 3 member verbs from each of the 101 top-level VerbNet classes (Kipper et al., 2008).",1 Introduction,[0],[0]
"This coverage enables researchers to better understand the complex diversity of syntactic-semantic verb behaviours, and provides direct links to other established semantic resources such as WordNet (Miller, 1995) and PropBank (Palmer et al., 2005).",1 Introduction,[0],[0]
"Moreover, the large standardised development and test sets in SimVerb-3500 allow for principled tuning of hyperparameters, a critical aspect of achieving strong performance with the latest representation learning architectures.
",1 Introduction,[0],[0]
"In § 2, we discuss previous evaluation resources targeting verb similarity.",1 Introduction,[0],[0]
"We present the new SimVerb-3500 data set along with our design choices and the pair selection process in § 3, while the annotation process is detailed in § 4.",1 Introduction,[0],[0]
"In § 5 we report the performance of a diverse range of popular representation learning architectures, together with benchmark performance on existing evaluation sets.",1 Introduction,[0],[0]
"In § 6, we show how SimVerb-3500 enables a variety of new linguistic analyses, which were previously impossible due to the lack of coverage and scale in existing resources.",1 Introduction,[0],[0]
A natural way to evaluate representation quality is by judging the similarity of representations assigned to similar words.,2 Related Work,[0],[0]
The most popular evaluation sets at present consist of word pairs with similarity ratings produced by human annotators.1,2 Related Work,[0],[0]
"Nevertheless, we find that all available datasets of this kind are insufficient for judging verb similarity due to their small size or narrow coverage of verbs.
",2 Related Work,[0],[0]
"In particular, a number of word pair evaluation sets are prominent in the distributional semantics
1In some existing evaluation sets pairs are scored for relatedness which has some overlap with similarity.",2 Related Work,[0],[0]
SimVerb-3500 focuses on similarity as this is a more focused semantic relation that seems to yield a higher agreement between human annotators.,2 Related Work,[0],[0]
"For a broader discussion see (Hill et al., 2015).
literature.",2 Related Work,[0],[0]
"Representative examples include RG-65 (Rubenstein and Goodenough, 1965) and WordSim-353 (Finkelstein et al., 2002; Agirre et al., 2009) which are small (65 and 353 word pairs, respectively).",2 Related Work,[0],[0]
"Larger evaluation sets such as the Rare Words evaluation set (Luong et al., 2013) (2034 word pairs) and the evaluations sets from Silberer and Lapata (2014) are dominated by noun pairs and the former also focuses on low-frequency phenomena.",2 Related Work,[0],[0]
"Therefore, these datasets do not provide a representative sample of verbs (Hill et al., 2015).
",2 Related Work,[0],[0]
"Two datasets that do focus on verb pairs to some extent are the data set of Baker et al. (2014) and Simlex-999 (Hill et al., 2015).",2 Related Work,[0],[0]
"These datasets, however, still contain a limited number of verb pairs (134 and 222, respectively), making them unrepresentative of the rich variety of verb semantic phenomena.
",2 Related Work,[0],[0]
In this paper we provide a remedy for this problem by presenting a more comprehensive and representative verb pair evaluation resource.,2 Related Work,[0],[0]
"In this section, we discuss the design principles behind SimVerb-3500.",3 The SimVerb-3500 Data Set,[0],[0]
We first demonstrate that a new evaluation resource for verb similarity is a necessity.,3 The SimVerb-3500 Data Set,[0],[0]
"We then describe how the final verb pairs were selected with the goal to be representative, that is, to guarantee a wide coverage of two standard semantic resources: USF and VerbNet.",3 The SimVerb-3500 Data Set,[0],[0]
"Hill et al. (2015) argue that comprehensive highquality evaluation resources have to satisfy the following three criteria: (C1) Representative (the resource covers the full range of concepts occurring in natural language); (C2) Clearly defined (it clearly defines the annotated relation, e.g., similarity); (C3) Consistent and reliable (untrained native speakers must be able to quantify the target relation consistently relying on simple instructions).
",3.1 Design Motivation,[0],[0]
"Building on the same annotation guidelines as Simlex-999 that explicitly targets similarity, we ensure that criteria C2 and C3 are satisfied.",3.1 Design Motivation,[0],[0]
"However, even SimLex, as the most extensive evaluation resource for verb similarity available at present, is still of limited size, spanning only 222 verb pairs and 170
distinct verb lemmas in total.",3.1 Design Motivation,[0],[0]
"Given that 39 out of the 101 top-level VerbNet classes are not represented at all in SimLex, while 20 classes have only one member verb,2 one may conclude that the criterion C1 is not at all satisfied with current resources.
",3.1 Design Motivation,[0],[0]
There is another fundamental limitation of all current verb similarity evaluation resources: automatic approaches have reached or surpassed the interannotator agreement ceiling.,3.1 Design Motivation,[0],[0]
"For instance, while the average pairwise correlation between annotators on SL-222 is Spearman’s ρ correlation of 0.717, the best performing automatic system reaches ρ = 0.727 (Mrkšić et al., 2016).",3.1 Design Motivation,[0],[0]
"SimVerb-3500 does not inherit this anomaly (see Tab. 2) and demonstrates that there still exists an evident gap between the human and system performance.
",3.1 Design Motivation,[0],[0]
"In order to satisfy C1-C3, the new SimVerb-3500 evaluation set contains similarity ratings for 3,500 verb pairs, containing 827 verb types in total and 3 member verbs for each top-level VerbNet class.",3.1 Design Motivation,[0],[0]
The rating scale goes from 0 (not similar at all) to 10 (synonymous).,3.1 Design Motivation,[0],[0]
We employed the SimLex-999 annotation guidelines.,3.1 Design Motivation,[0],[0]
"In particular, we instructed annotators to give low ratings to antonyms, and to distinguish between similarity and relatedness.",3.1 Design Motivation,[0],[0]
"Pairs that are related but not similar (e.g., to snore / to snooze, to walk / to crawl) thus have a fairly low rating.",3.1 Design Motivation,[0],[0]
Several example pairs are provided in Tab. 1.,3.1 Design Motivation,[0],[0]
"To ensure a wide coverage of a variety of syntacticosemantic phenomena (C1), the choice of verb pairs is steered by two standard semantic resources available online: (1) the USF norms data set3 (Nelson et al., 2004), and (2) the VerbNet verb lexicon4 (Kipper et al., 2004; Kipper et al., 2008).
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The USF norms data set (further USF) is the largest database of free association collected for English.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"It was generated by presenting human subjects with one of 5, 000 cue concepts and asking them to write the first word coming to mind that is associated with that concept.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Each cue concept c was normed in
2Note that verbs in VerbNet are soft clustered, and one verb type may be associated with more than one class.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"When computing coverage, we assume that such verbs attribute to counts of all their associated classes.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"3http://w3.usf.edu/FreeAssociation/ 4http://verbs.colorado.edu/verb-index/
this way by over 10 participants, resulting in a set of associates a for each cue, for a total of over 72, 000 (c, a) pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For each such pair, the proportion of participants who produced associate a when presented with cue c can be used as a proxy for the strength of association between the two words.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The norming process guarantees that two words in a pair have a degree of semantic association which correlates well with semantic relatedness and similarity.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Sampling from the USF set ensures that both related but non-similar pairs (e.g., to run / to sweat) as well as similar pairs (e.g., to reply / to respond) are represented in the final list of pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Further, the rich annotations of the output USF data (e.g., concreteness scores, association strength) can be directly combined with the SimVerb-3500 similarity scores to yield additional analyses and insight.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
VerbNet (VN) is the largest online verb lexicon currently available for English.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"It is hierarchical, domain-independent, and broad-coverage.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
VN is organised into verb classes extending the classes from Levin (1993) through further refinement to achieve syntactic and semantic coherence among class members.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"According to the official VerbNet guidelines,5 “Verb Classes are numbered according to shared semantics and syntax, and classes which share a toplevel number (9-109) have corresponding semantic relationships.”",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For instance, all verbs from the toplevel Class 9 are labelled “Verbs of Putting”, all verbs from Class 30 are labelled “Verbs of Perception”, while Class 39 contains “Verbs of Ingesting”.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Among others, three basic types of information are covered in VN: (1) verb subcategorization frames (SCFs), which describe the syntactic realization of the predicate-argument structure (e.g. The window broke), (2) selectional preferences (SPs), which capture the semantic preferences verbs have for their
5http://verbs.colorado.edu/verb-index/VerbNet_Guidelines.pdf
arguments (e.g. a breakable physical object broke) and (3) lexical-semantic verb classes (VCs) which provide a shared level of abstraction for verbs similar in their (morpho-)syntactic and semantic properties (e.g. BREAK verbs, sharing the VN class 45.1, and the top-level VN class 45).6",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The basic overview of the VerbNet structure already suggests that measuring verb similarity is far from trivial as it revolves around a complex interplay between various semantic and syntactic properties.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
The wide coverage of VN in SimVerb-3500 assures the wide coverage of distinct verb groups/classes and their related linguistic phenomena.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Finally, VerbNet enables further connections of SimVerb-3500 to other important lexical resources such as FrameNet (Baker et al., 1998), WordNet (Miller, 1995), and PropBank (Palmer et al., 2005) through the sets of mappings created by the SemLink project initiative (Loper et al., 2007).7
Sampling Procedure We next sketch the complete sampling procedure which resulted in the final set of 3500 distinct verb pairs finally annotated in a crowdsourcing study (§ 4).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Step 1),3.2 Choice of Verb Pairs and Coverage,[0],[0]
We extracted all possible verb pairs from USF based on the associated POS tags available as part of USF annotations.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"To ensure that semantic association between verbs in a pair is not accidental, we then discarded all such USF pairs that had been associated by 2 or less participants in USF.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Step 2),3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We then manually cleaned and simplified the list of pairs by removing all pairs with multi-word verbs (e.g., quit / give up), all pairs that contained the non-infinitive form of a verb (e.g., accomplished / finished, hidden / find), removing all pairs containing at least one auxiliary verb (e.g., must / to see, must / to be).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The first two steps resulted in 3,072 USF-based verb pairs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"(Step 3) After this stage, we noticed that several toplevel VN classes are not part of the extracted set.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"For instance, 5 VN classes did not have any member verbs included, 22 VN classes had only 1 verb, and 6 VN classes had 2 verbs included in the current set.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
We resolved the VerbNet coverage issue by sampling from such ’under-represented’ VN classes directly.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Note that this step is not related to USF at
6https://verbs.colorado.edu/verb-index/vn/break-45.1.php 7https://verbs.colorado.edu/semlink/
all.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
For each such class we sampled additional verb types until the class was represented by 3 or 4 member verbs (chosen randomly).8,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Following that, we sampled at least 2 verb pairs for each previously ’under-represented’ VN class by pairing 2 member verbs from each such class.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"This procedure resulted in 81 additional pairs, now 3,153 in total.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"(Step 4) Finally, to complement this set with a sample of entirely unassociated pairs, we followed the SimLex-999 setup.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We paired up the verbs from the 3,153 associated pairs at random.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"From these random parings, we excluded those that coincidentally occurred elsewhere in USF (and therefore had a degree of association).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
We sampled the remaining 347 pairs from this resulting set of unassociated pairs.,3.2 Choice of Verb Pairs and Coverage,[0],[0]
(Output),3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The final SimVerb-3500 data set contains 3,500 verb pairs in total, covering all associated verb pairs from USF, and (almost) all top-level VerbNet classes.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"All pairs were manually checked post-hoc by the authors plus 2 additional native English speakers to verify that the final data set does not contain unknown or invalid verb types.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
Frequency Statistics,3.2 Choice of Verb Pairs and Coverage,[0],[0]
"The 3,500 pairs consist of 827 distinct verbs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"29 top-level VN classes are represented by 3 member verbs, while the three most represented classes cover 79, 85, and 93 member verbs.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"40 verbs are not members of any VN class.
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We performed an initial frequency analysis of SimVerb-3500 relying on the BNC counts available online (Kilgarriff, 1997).9 After ranking all BNC verbs according to their frequency, we divided the list into quartiles: Q1 (most frequent verbs in BNC) - Q4 (least frequent verbs in BNC).",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Out of the 827 SimVerb-3500 verb types, 677 are contained in Q1, 122 in Q2, 18 in Q3, 4 in Q4 (to enroll, to hitchhike, to implode, to whelp), while 6 verbs are not covered in the BNC list.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"2,818 verb pairs contain Q1 verbs, while there are 43 verb pairs with both verbs not in Q1.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"Further empirical analyses are provided in § 6.10
8The following three VN classes are exceptions: (1) Class 56, consisting of words that are dominantly tagged as nouns, but can be used as verbs exceptionally (e.g., holiday, summer, honeymoon); (2) Class 91, consisting of 2 verbs (count, matter); (3) Class 93, consisting of 2 single word verbs (adopt, assume).
",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"9https://www.kilgarriff.co.uk/bnc-readme.html 10Annotations such as VerbNet class membership, relations between WordNet synsets of each verb, and frequency statistics are available as supplementary material.",3.2 Choice of Verb Pairs and Coverage,[0],[0]
"We employ the Prolific Academic (PA) crowdsourcing platform,11 an online marketplace very similar to Amazon Mechanical Turk and to CrowdFlower.",4 Word Pair Scoring,[0],[0]
"Following the SimLex-999 annotation guidelines, we had each of the 3500 verb pairs rated by at least 10 annotators.",4.1 Survey Structure,[0],[0]
"To distribute the workload, we divided the 3500 pairs into 70 tranches, with 79 pairs each.",4.1 Survey Structure,[0],[0]
"Out of the 79 pairs, 50 are unique to one tranche, while 20 manually chosen pairs are in all tranches to ensure consistency.",4.1 Survey Structure,[0],[0]
"The remaining 9 are duplicate pairs displayed to the same participant multiple times to detect inconsistent annotations.
",4.1 Survey Structure,[0],[0]
Participants see 7-8 pairs per page.,4.1 Survey Structure,[0],[0]
Pairs are rated on a scale of 0-6 by moving a slider.,4.1 Survey Structure,[0],[0]
"The first page shows 7 pairs, 5 unique ones and 2 from the consistency set.",4.1 Survey Structure,[0],[0]
The following pages are structured the same but display one extra pair from the previous page.,4.1 Survey Structure,[0],[0]
Participants are explicitly asked to give these duplicate pairs the same rating.,4.1 Survey Structure,[0],[0]
"We use them as quality control so that we can identify and exclude participants giving several inconsistent answers.
",4.1 Survey Structure,[0],[0]
Checkpoint Questions,4.1 Survey Structure,[0],[0]
The survey contains three control questions in which participants are asked to select the most similar pair out of three choices.,4.1 Survey Structure,[0],[0]
"For instance, the first checkpoint is: Which of these pairs of words is the *most* similar?",4.1 Survey Structure,[0],[0]
1. to run / to jog 2.,4.1 Survey Structure,[0],[0]
to run / to walk 3.,4.1 Survey Structure,[0],[0]
to jog / to sweat.,4.1 Survey Structure,[0],[0]
One checkpoint occurs right after the instructions and the other two later in the survey.,4.1 Survey Structure,[0],[0]
The purpose is to check that annotators have understood the guidelines and to have another quality control measure for ensuring that they are paying attention throughout the survey.,4.1 Survey Structure,[0],[0]
"If just one of the checkpoint questions is answered incorrectly, the survey ends immediately and all scores from the annotator in question are discarded.
",4.1 Survey Structure,[0],[0]
"Participants 843 raters participated in the study, producing over 65,000 ratings.",4.1 Survey Structure,[0],[0]
"Unlike other crowdsourcing platforms, PA collects and stores detailed demographic information from the participants upfront.",4.1 Survey Structure,[0],[0]
This information was used to carefully select the pool of eligible participants.,4.1 Survey Structure,[0],[0]
"We restricted the pool to native English speakers with a 90% approval
11https://prolific.ac/ (We chose PA for logistic reasons.)
rate (maximum rate on PA), of age 18-50, born and currently residing in the US (45% out of 843 raters), UK (53%), or Ireland (2%).",4.1 Survey Structure,[0],[0]
"54% of the raters were female and 46% male, with the average age of 30.",4.1 Survey Structure,[0],[0]
Participants took 8 minutes on average to complete the survey containing 79 questions.,4.1 Survey Structure,[0],[0]
"We excluded ratings of annotators who (a) answered one of the checkpoint questions incorrectly (75% of exclusions); (b) did not give equal ratings to duplicate pairs; (c) showed suspicious rating patterns (e.g., randomly alternating between two ratings or using one single rating throughout).",4.2 Post-Processing,[0],[0]
The final acceptance rate was 84%.,4.2 Post-Processing,[0],[0]
We then calculated the average of all ratings from the accepted raters ( ≥ 10 ) for each pair.,4.2 Post-Processing,[0],[0]
"The score was finally scaled linearly from the 0-6 to the 0-10 interval as in (Hill et al., 2015).",4.2 Post-Processing,[0],[0]
Inter-Annotator Agreement We employ two measures.,5 Analysis,[0],[0]
"IAA-1 (pairwise) computes the average pairwise Spearman’s ρ correlation between any two raters – a common choice in previous data collection in distributional semantics (Padó et al., 2007; Reisinger and Mooney, 2010a; Silberer and Lapata, 2014; Hill et al., 2015).
",5 Analysis,[0],[0]
A complementary measure would smooth individual annotator effects.,5 Analysis,[0],[0]
"For this aim, our IAA-2 (mean) measure compares the average correlation of a human rater with the average of all the other raters.",5 Analysis,[0],[0]
"SimVerb-3500 obtains ρ = 0.84 (IAA-1) and ρ = 0.86 (IAA-2), a very good agreement compared to other benchmarks (see Tab. 2).
",5 Analysis,[0],[0]
Vector Space Models We compare the performance of prominent representation models on SimVerb-3500.,5 Analysis,[0],[0]
"We include: (1) unsupervised models that learn from distributional information in text, including the skip-gram negative-sampling model (SGNS) with various contexts (BOW = bag of words; DEPS = dependency contexts) as in Levy and Goldberg (2014), the symmetric-pattern based vectors by Schwartz et al. (2015), and count-based PMIweighted vectors (Baroni et al., 2014); (2) Models that rely on linguistic hand-crafted resources or curated knowledge bases.",5 Analysis,[0],[0]
"Here, we use sparse binary vectors built from linguistic resources (Non-
Distributional, (Faruqui and Dyer, 2015)), and vectors fine-tuned to a paraphrase database (Paragram, (Wieting et al., 2015)) further refined using linguistic constraints (Paragram+CF, (Mrkšić et al., 2016)).",5 Analysis,[0],[0]
"Descriptions of these models are in the supplementary material.
",5 Analysis,[0],[0]
Comparison to SimLex-999 (SL-222) 170 pairs from SL-222 also appear in SimVerb-3500.,5 Analysis,[0],[0]
The correlation between the two data sets calculated on the shared pairs is ρ = 0.91.,5 Analysis,[0],[0]
"This proves, as expected, that the ratings are consistent across the two data sets.
",5 Analysis,[0],[0]
Tab. 3 shows a comparison of models’ performance on SimVerb-3500 against SL-222.,5 Analysis,[0],[0]
"Since the number of evaluation pairs may influence the results, we ideally want to compare sets of equal size for a fair comparison.",5 Analysis,[0],[0]
"Picking one random subset of 222 pairs would bias the results towards the selected pairs, and even using 10-fold cross-validation we found variations up to 0.05 depending on which subsets were used.",5 Analysis,[0],[0]
"Therefore, we employ a 2-level 10-fold crossvalidation where new random subsets are picked in each iteration of each model.",5 Analysis,[0],[0]
The numbers reported as CV-222 are averages of these ten 10-fold crossvalidation runs.,5 Analysis,[0],[0]
"The reported results come very close to the correlation on the full data set for all models.
",5 Analysis,[0],[0]
"Most models perform much better on SL-222, especially those employing additional databases or linguistic resources.",5 Analysis,[0],[0]
The performance of the best scoring Paragram+CF model is even on par with the IAA-1 of 0.72.,5 Analysis,[0],[0]
"The same model obtains the highest score on SV-3500 (ρ = 0.628), with a clear gap to IAA-1 of 0.84.",5 Analysis,[0],[0]
"We attribute these differences in
performance largely to SimVerb-3500 being a more extensive and diverse resource in terms of verb pairs.
",5 Analysis,[0],[0]
Development Set A common problem in scored word pair datasets is the lack of a standard split to development and test sets.,5 Analysis,[0],[0]
"Previous works often optimise models on the entire dataset, which leads to overfitting (Faruqui et al., 2016) or use custom splits, e.g., 10-fold cross-validation (Schwartz et al., 2015), which make results incomparable with others.",5 Analysis,[0],[0]
"The lack of standard splits stems mostly from small size and poor coverage – issues which we have solved with SimVerb-3500.
",5 Analysis,[0],[0]
"Our development set contains 500 pairs, selected to ensure a broad coverage in terms of similarity ranges (i.e., non-similar and highly similar pairs, as well as pairs of medium similarity are represented) and top-level VN classes (each class is represented by at least 1 member verb).",5 Analysis,[0],[0]
"The test set includes the remaining 3,000 verb pairs.",5 Analysis,[0],[0]
The performances of representation learning architectures on the dev and test sets are reported in Tab. 3.,5 Analysis,[0],[0]
"The ranking of models is identical on the test and the full SV-3500 set, with slight differences in ranking on the development set.",5 Analysis,[0],[0]
The large coverage and scale of SimVerb-3500 enables model evaluation based on selected criteria.,6 Evaluating Subsets,[0],[0]
"In this section, we showcase a few example analyses.
",6 Evaluating Subsets,[0],[0]
"Frequency In the first analysis, we select pairs based on their lemma frequency in the BNC corpus and form three groups, with 390-490 pairs in each group (Fig. 1).",6 Evaluating Subsets,[0],[0]
"The results from Fig. 1 suggest that the performance of all models improves as the frequency of the verbs in the pair increases, with much steeper curves for the purely distributional models (e.g., SGNS and SymPat).",6 Evaluating Subsets,[0],[0]
"The non-distributional non data-driven model of Faruqui and Dyer (2015) is only slightly affected by frequency.
",6 Evaluating Subsets,[0],[0]
"WordNet Synsets Intuitively, representations for verbs with more diverse usage patterns are more difficult to learn with statistical models.",6 Evaluating Subsets,[0],[0]
"To examine this hypothesis, we resort to WordNet (Miller, 1995), where different semantic usages of words are listed as so-called synsets.",6 Evaluating Subsets,[0],[0]
"Fig. 2 shows a clear downward trend for all models, confirming that polysemous
verbs are more difficult for current verb representation models.",6 Evaluating Subsets,[0],[0]
"Nevertheless, approaches which use additional information beyond corpus co-occurrence are again more robust.",6 Evaluating Subsets,[0],[0]
"Their performance only drops substantially for verbs with more than 10 synsets, while the performance of other models deteriorates already when tackling verbs with more than 5 synsets.
",6 Evaluating Subsets,[0],[0]
VerbNet Classes Another analysis enabled by SimVerb-3500 is investigating the connection between VerbNet classes and human similarity judgments.,6 Evaluating Subsets,[0],[0]
We find that verbs in the same top-level VerbNet class are often not assigned high similarity score.,6 Evaluating Subsets,[0],[0]
"Out of 1378 pairs where verbs share the top-level VerbNet class, 603 have a score lower than 5.",6 Evaluating Subsets,[0],[0]
Tab. 4 reports scores per VerbNet class.,6 Evaluating Subsets,[0],[0]
"When a verb be-
longs to multiple classes, we count it for each class (see Footnote 2).",6 Evaluating Subsets,[0],[0]
"We run the analysis on the five largest VN classes, each with more than 100 pairs with paired verbs belonging to the same class.
",6 Evaluating Subsets,[0],[0]
"The results indicate clear differences between classes (e.g., Class 31 vs Class 51), and suggest that further developments in verb representation learning should also focus on constructing specialised representations at the finer-grained level of VN classes.
",6 Evaluating Subsets,[0],[0]
"Lexical Relations SimVerb-3500 contains relation annotations (e.g., antonyms, synonyms, hyper/hyponyms, no relation) for all pairs extracted automatically from WordNet.",6 Evaluating Subsets,[0],[0]
"Evaluating per-relation subsets, we observe that some models draw their strength from good performance across different re-
lations.",6 Evaluating Subsets,[0],[0]
"Others have low performance on these pairs, but do very well on synonyms and hyper-/hyponyms.",6 Evaluating Subsets,[0],[0]
"Selected results of this analysis are in Tab. 5.12
Human Agreement Motivated by the varying performance of computational models regarding frequency and ambiguous words with many synsets, we analyse what disagreement effects may be captured in human ratings.",6 Evaluating Subsets,[0],[0]
"We therefore compute the average standard deviation of ratings per subset: avgstdd(S) = 1n ∑ p∈S σ(rp), where S is one subset of pairs, n is the number of pairs in this subset, p is one pair, and rp are all human ratings for this pair.
",6 Evaluating Subsets,[0],[0]
"12 Evaluation based on Spearman’s ρ may be problematic with certain categories, e.g., with antonyms.",6 Evaluating Subsets,[0],[0]
"It evaluates pairs according to their ranking; for antonyms the ranking is arbitrary - every antonym pair should have a very low rating, hence they are not included in Tab. 5.",6 Evaluating Subsets,[0],[0]
"A similar effect occurs with highly ranked synonyms, but to a much lesser degree than with antonyms.
",6 Evaluating Subsets,[0],[0]
"While the standard deviation of ratings is diverse for individual pairs, overall the average standard deviations per subset are almost identical.",6 Evaluating Subsets,[0],[0]
"For both the frequency and the WordNet synset analyses it is around ≈1.3 across all subsets, and with only little difference for the subsets based on VerbNet.",6 Evaluating Subsets,[0],[0]
"The only subsets where we found significant variations is the grouping by relations, where ratings tend to be more similar especially on antonyms (0.86) and pairs with no relation (0.92), much less similar on synonyms (1.34) and all other relations (≈1.4).",6 Evaluating Subsets,[0],[0]
These findings suggest that humans are much less influenced by frequency or polysemy in their understanding of verb semantics compared to computational models.,6 Evaluating Subsets,[0],[0]
"SimVerb-3500 is a verb similarity resource for analysis and evaluation that will be of use to researchers involved in understanding how humans or machines represent the meaning of verbs, and, by extension, scenes, events and full sentences.",7 Conclusions,[0],[0]
"The size and coverage of syntactico-semantic phenomena in SimVerb3500 makes it possible to compare the strengths and weaknesses of various representation models via statistically robust analyses on specific word classes.
",7 Conclusions,[0],[0]
"To demonstrate the utility of SimVerb-3500, we conducted a selection of analyses with existing representation-learning models.",7 Conclusions,[0],[0]
One clear conclusion is that distributional models trained on raw text (e.g. SGNS) perform very poorly on low frequency and highly polysemous verbs.,7 Conclusions,[0],[0]
"This degradation in performance can be partially mitigated by focusing models on more principled distributional contexts, such as those defined by symmetric patterns.",7 Conclusions,[0],[0]
"More generally, the finding suggests that, in order to model the diverse spectrum of verb semantics, we may require algorithms that are better suited to fast learning from few examples (Lake et al., 2011), and have some flexibility with respect to sense-level distinctions (Reisinger and Mooney, 2010b; Vilnis and McCallum, 2015).",7 Conclusions,[0],[0]
"In future work we aim to apply such methods to the task of verb acquisition.
",7 Conclusions,[0],[0]
"Beyond the preliminary conclusions from these initial analyses, the benefit of SimLex-3500 will become clear as researchers use it to probe the relationship between architectures, algorithms and representation quality for a wide range of verb classes.",7 Conclusions,[0],[0]
"Better under-
standing of how to represent the full diversity of verbs should in turn yield improved methods for encoding and interpreting the facts, propositions, relations and events that constitute much of the important information in language.",7 Conclusions,[0],[0]
This work is supported by the ERC Consolidator Grant LEXICAL (648909).,Acknowledgments,[0],[0]
"Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research.",abstractText,[0],[0]
"We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs.",abstractText,[0],[0]
"SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class.",abstractText,[0],[0]
This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning.,abstractText,[0],[0]
"Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs.",abstractText,[0],[0]
We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.,abstractText,[0],[0]
SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2072–2082 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2072",text,[0],[0]
"An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals.",1 Introduction,[0],[0]
"For example, consider the environment and instructions in Figure 1, in which a user describes moving chemicals between beakers and mixing chemicals together.",1 Introduction,[0],[0]
"To execute the second instruction, the agent needs to resolve sixth beaker and last one to objects in the environment.",1 Introduction,[0],[0]
"The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown.",1 Introduction,[0],[0]
"In this paper, we describe a model and learning approach to map sequences of instructions to actions.",1 Introduction,[0],[0]
"Our model considers previous utterances and the world state to select actions, learns to combine simple actions to achieve complex goals, and can be trained using
goal states without access to demonstrations.",1 Introduction,[0],[0]
"The majority of work on executing sequences of instructions focuses on mapping instructions to high-level formal representations, which are then evaluated to generate actions (e.g., Chen and Mooney, 2011; Long et al., 2016).",1 Introduction,[0],[0]
"For example, the third instruction in Figure 1 will be mapped to mix(prev_arg1), indicating that the mix action should be applied to first argument of the previous action (Long et al., 2016; Guu et al., 2017).",1 Introduction,[0],[0]
"In contrast, we focus on directly generating the sequence of actions.",1 Introduction,[0],[0]
"This requires resolving references without explicitly modeling them, and learning the sequences of actions required to complete high-level actions; for example, that mixing requires removing everything in the beaker and replacing with the same number of brown items.
",1 Introduction,[0],[0]
A key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world.,1 Introduction,[0],[0]
"Instructions often refer to previously
mentioned objects (e.g., it in Figure 1) or actions (e.g., do it again).",1 Introduction,[0],[0]
"The world state provides the set of objects the instruction may refer to, and implicitly determines the available actions.",1 Introduction,[0],[0]
"For example, liquid can not be removed from an empty beaker.",1 Introduction,[0],[0]
Both types of contexts continuously change during an interaction.,1 Introduction,[0],[0]
"As new instructions are given, the instruction history expands, and as the agent acts the world state changes.",1 Introduction,[0],[0]
"We propose an attentionbased model that takes as input the current instruction, previous instructions, the initial world state, and the current state.",1 Introduction,[0],[0]
"At each step, the model computes attention encodings of the different inputs, and predicts the next action to execute.
",1 Introduction,[0],[0]
We train the model given instructions paired with start and goal states without access to the correct sequence of actions.,1 Introduction,[0],[0]
"During training, the agent learns from rewards received through exploring the environment with the learned policy by mapping instructions to sequences of actions.",1 Introduction,[0],[0]
"In practice, the agent learns to execute instructions gradually, slowly correctly predicting prefixes of the correct sequences of increasing length as learning progress.",1 Introduction,[0],[0]
A key challenge is learning to correctly select actions that are only required later in execution sequences.,1 Introduction,[0],[0]
"Early during learning, these actions receive negative updates, and the agent learns to assign them low probabilities.",1 Introduction,[0],[0]
"This results in an exploration problem in later stages, where actions that are only required later are not sampled during exploration.",1 Introduction,[0],[0]
"For example, in the ALCHEMY domain shown in Figure 1, the agent behavior early during execution of instructions can be accomplished by only using POP actions.",1 Introduction,[0],[0]
"As a result, the agent quickly learns a strong bias against PUSH actions, which in practice prevents the policy from exploring them again.",1 Introduction,[0],[0]
"We address this with a learning algorithm that observes the reward for all possible actions for each visited state, and maximizes the immediate expected reward.
",1 Introduction,[0],[0]
"We evaluate our approach on SCONE (Long et al., 2016), which includes three domains, and is used to study recovering predicate logic meaning representations for sequential instructions.",1 Introduction,[0],[0]
"We study the problem of generating a sequence of low-level actions, and re-define the set of actions for each domain.",1 Introduction,[0],[0]
"For example, we treat the beakers in the ALCHEMY domain as stacks and use only POP and PUSH actions.",1 Introduction,[0],[0]
"Our approach robustly learns to execute sequential instructions with up to 89.1% task-completion
accuracy for single instruction, and 62.7% for complete sequences.",1 Introduction,[0],[0]
Our code is available at https://github.com/clic-lab/scone.,1 Introduction,[0],[0]
"Task and Notation Let S be the set of all possible world states, X be the set of all natural language instructions, and A be the set of all actions.",2 Technical Overview,[0],[0]
"An instruction x̄ ∈ X of length |x̄| is a sequence of tokens 〈x1, ...x|x̄|〉.",2 Technical Overview,[0],[0]
Executing an action modifies the world state following a transition function T : S ×A → S.,2 Technical Overview,[0],[0]
"For example, the ALCHEMY domain includes seven beakers that contain colored liquids.",2 Technical Overview,[0],[0]
The world state defines the content of each beaker.,2 Technical Overview,[0],[0]
We treat each beaker as a stack.,2 Technical Overview,[0],[0]
"The actions are POP N and PUSH N C, where 1 ≤ N ≤ 7 is the beaker number and C is one of six colors.",2 Technical Overview,[0],[0]
"There are a total of 50 actions, including the STOP action.",2 Technical Overview,[0],[0]
"Section 6 describes the domains in detail.
",2 Technical Overview,[0],[0]
"Given a start state s1 and a sequence of instructions 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄n〉, our goal is to generate the sequence of actions specified by the instructions starting from s1.",2 Technical Overview,[0],[0]
We treat the execution of a sequence of instructions as executing each instruction in turn.,2 Technical Overview,[0],[0]
"The execution ē of an instruction x̄i starting at a state s1 and given the history of the instruction sequence 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄i−1〉 is a sequence of state-action pairs ē = 〈(s1, a1), ..., (sm, am)〉, where",2 Technical Overview,[0],[0]
"ak ∈ A, sk+1 = T (sk, ak).",2 Technical Overview,[0],[0]
"The final action am is the special action STOP, which indicates the execution has terminated.",2 Technical Overview,[0],[0]
"The final state is then sm, as T (sk, STOP) = sk.",2 Technical Overview,[0],[0]
"Executing a sequence of instructions in order generates a sequence 〈ē1, ..., ēn〉, where ēi is the execution of instruction x̄i.",2 Technical Overview,[0],[0]
"When referring to states and actions in an indexed execution ēi, the k-th state and action are si,k and ai,",2 Technical Overview,[0],[0]
"k. We execute instructions one after the other: ē1 starts at the interaction initial state s1 and si+1,1 = si,|ēi|, where si+1,1 is the start state of ēi+1 and si,|ēi| is the final state of ēi.
",2 Technical Overview,[0],[0]
Model We model the agent with a neural network policy (Section 4).,2 Technical Overview,[0],[0]
"At step k of executing the i-th instruction, the model input is the current instruction x̄i, the previous instructions 〈x̄1, . . .",2 Technical Overview,[0],[0]
", x̄i−1〉, the world state s1 at the beginning of executing x̄i, and the current state sk.",2 Technical Overview,[0],[0]
The model predicts the next action ak to execute.,2 Technical Overview,[0],[0]
"If ak = STOP, we switch to the next instruction, or if at the end of the instruction sequence, terminate.",2 Technical Overview,[0],[0]
"Otherwise, we update the state to sk+1 = T (sk, ak).",2 Technical Overview,[0],[0]
"The model uses attention to
process the different inputs and a recurrent neural network (RNN) decoder to generate actions (Bahdanau et al., 2015).
",2 Technical Overview,[0],[0]
"Learning We assume access to a set of N instruction sequences, where each instruction in each sequence is paired with its start and goal states.",2 Technical Overview,[0],[0]
"During training, we create an example for each instruction.",2 Technical Overview,[0],[0]
"Formally, the training set is {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) i−1〉, g (j) i )} N,n(j)",2 Technical Overview,[0],[0]
"j=1,i=1, where x̄(j)i is an instruction, s (j) i,1 is a start state, 〈x̄(j)1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) i−1〉 is the instruction history, g (j) i is the goal state, and n(j) is the length of the j-th instruction sequence.",2 Technical Overview,[0],[0]
This training data contains no evidence about the actions and intermediate states required to execute each instruction.1,2 Technical Overview,[0],[0]
We use a learning method that maximizes the expected immediate reward for a given state (Section 5).,2 Technical Overview,[0],[0]
"The reward accounts for task-completion and distance to the goal via potential-based reward shaping.
",2 Technical Overview,[0],[0]
"Evaluation We evaluate exact task completion for sequences of instructions on a test set {(s(j)1 , 〈x̄ (j) 1 , . . .",2 Technical Overview,[0],[0]
", x̄ (j) nj 〉, g(j))}Nj=1, where g(j) is the oracle goal state of executing instructions x̄
(j) 1 , . . .",2 Technical Overview,[0],[0]
",x̄ (j) nj in order starting from s (j) 1 .",2 Technical Overview,[0],[0]
We also evaluate single-instruction task completion using per-instruction annotated start and goal states.,2 Technical Overview,[0],[0]
"Executing instructions has been studied using the SAIL corpus (MacMahon et al., 2006) with focus on navigation using high-level logical representations (Chen and Mooney, 2011; Chen, 2012; Artzi and Zettlemoyer, 2013; Artzi et al., 2014) and lowlevel actions (Mei et al., 2016).",3 Related Work,[0],[0]
"While SAIL includes sequences of instructions, the data demonstrates limited discourse phenomena, and instructions are often processed in isolation.",3 Related Work,[0],[0]
"Approaches that consider as input the entire sequence focused on segmentation (Andreas and Klein, 2015).",3 Related Work,[0],[0]
"Recently, other navigation tasks were proposed with focus on single instructions (Anderson et al., 2018; Janner et al., 2018).",3 Related Work,[0],[0]
We focus on sequences of environment manipulation instructions and modeling contextual cues from both the changing environment and instruction history.,3 Related Work,[0],[0]
"Manipulation using single-sentence instructions has been stud-
1This training set is a subset of the data used in previous work (Section 6, Guu et al., 2015), in which training uses all instruction sequences of length 1 and 2.
ied using the Blocks domain (Bisk et al., 2016, 2018; Misra et al., 2017; Tan and Bansal, 2018).",3 Related Work,[0],[0]
Our work is related to the work of Branavan et al. (2009) and Vogel and Jurafsky (2010).,3 Related Work,[0],[0]
"While both study executing sequences of instructions, similar to SAIL, the data includes limited discourse dependencies.",3 Related Work,[0],[0]
"In addition, both learn with rewards computed from surface-form similarity between text in the environment and the instruction.",3 Related Work,[0],[0]
"We do not rely on such similarities, but instead use a state distance metric.
",3 Related Work,[0],[0]
"Language understanding in interactive scenarios that include multiple turns has been studied with focus on dialogue for querying database systems using the ATIS corpus (Hemphill et al., 1990; Dahl et al., 1994).",3 Related Work,[0],[0]
Tür et al. (2010) surveys work on ATIS.,3 Related Work,[0],[0]
"Miller et al. (1996), Zettlemoyer and Collins (2009), and Suhr et al. (2018) modeled context dependence in ATIS for generating formal representations.",3 Related Work,[0],[0]
"In contrast, we focus on environments that change during execution and directly generating environment actions, a scenario that is more related to robotic agents than database query.
",3 Related Work,[0],[0]
"The SCONE corpus (Long et al., 2016) was designed to reflect a broad set of discourse context-dependence phenomena.",3 Related Work,[0],[0]
"It was studied extensively using logical meaning representations (Long et al., 2016; Guu et al., 2017; Fried et al., 2018).",3 Related Work,[0],[0]
"In contrast, we are interested in directly generating actions that modify the environment.",3 Related Work,[0],[0]
"This requires generating lower-level actions and learning procedures that are otherwise hardcoded in the logic (e.g., mixing action in Figure 1).",3 Related Work,[0],[0]
"Except for Fried et al. (2018), previous work on SCONE assumes access only to the initial and final states during training.",3 Related Work,[0],[0]
"This form of supervision does not require operating the agent manually to acquire the correct sequence of actions, a difficult task in robotic agents with complex control.",3 Related Work,[0],[0]
"Goal state supervision has been studied for instructional language (e.g., Branavan et al., 2009; Artzi and Zettlemoyer, 2013; Bisk et al., 2016), and more extensively in question answering when learning with answer annotations only (e.g., Clarke et al., 2010; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Berant and Liang, 2014, 2015; Liang et al., 2017).",3 Related Work,[0],[0]
"We map sequences of instructions 〈x̄1, . . .",4 Model,[0],[0]
", x̄n〉 to actions by executing the instructions in or-
der.",4 Model,[0],[0]
"The model generates an execution ē = 〈(s1, a1), . . .",4 Model,[0],[0]
", (smi , ami)〉 for each instruction x̄i.",4 Model,[0],[0]
"The agent context, the information available to the agent at step k, is s̃k = (x̄i, 〈x̄1, . . .",4 Model,[0],[0]
", x̄i−1〉, sk, ē[: k]), where ē[: k] is the execution up until but not including step k.",4 Model,[0],[0]
"In contrast to the world state, the agent context also includes instructions and the execution so far.",4 Model,[0],[0]
"The agent policy πθ(s̃k, a) is modeled as a probabilistic neural network parametrized by θ, where s̃k is the agent context at step k and a is an action.",4 Model,[0],[0]
"To generate executions, we generate one action at a time, execute the action, and observe the new world state.",4 Model,[0],[0]
"In step k of executing the i-th instruction, the network inputs are the current utterance x̄i, the previous instructions 〈x̄1, . . .",4 Model,[0],[0]
", x̄i−1〉, the initial state s1 at beginning of executing x̄i, and the current state sk.",4 Model,[0],[0]
"When executing a sequence of instructions, the initial state s1 is either the state at the beginning of executing the sequence or the final state of the execution of the previous instruction.",4 Model,[0],[0]
"Figure 2 illustrates our architecture.
",4 Model,[0],[0]
We generate continuous vector representations for all inputs.,4 Model,[0],[0]
"Each input is represented as a set of vectors that are then processed with an attention function to generate a single vector representation (Luong et al., 2015).",4 Model,[0],[0]
"We assume access to a domain-specific encoding function ENC(s) that, given a state s, generates a set of vectors S representing the objects in the state.",4 Model,[0],[0]
"For example, in the ALCHEMY domain, a vector is generated for each beaker using an RNN.",4 Model,[0],[0]
"Section 6 describes the different domains and their encoding functions.
",4 Model,[0],[0]
"We use a single bidirectional RNN with a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) recurrence to encode the instructions.",4 Model,[0],[0]
"All instructions x̄1,. . .",4 Model,[0],[0]
",x̄i are encoded
with a single RNN by concatenating them to x̄′.",4 Model,[0],[0]
"We use two delimiter tokens: one separates previous instructions, and the other separates the previous instructions from the current one.",4 Model,[0],[0]
"The forward LSTM RNN hidden states are computed as:2
−−→ hj+1 = −−−−−→ LSTME ( φI(x′j+1); −→ hj ) ,
where φI is a learned word embedding function and −−−−−→ LSTME is the forward LSTM recurrence function.",4 Model,[0],[0]
We use a similar computation to compute the backward hidden states ←− hj .,4 Model,[0],[0]
"For each token x′j in x̄ ′, a vector representation h′j =[−→
hj ; ←− hj ] is computed.",4 Model,[0],[0]
"We then create two sets of vectors, one for all the vectors of the current instruction and one for the previous instructions:
Xc = {h′j} J+|x̄i| j=J Xp = {h′j}j<Jj=0
where J is the index in x̄′ where the current instruction x̄i begins.",4 Model,[0],[0]
"Separating the vectors to two sets will allows computing separate attention on the current instruction and previous ones.
",4 Model,[0],[0]
"To compute each input representation during decoding, we use a bi-linear attention function (Luong et al., 2015).",4 Model,[0],[0]
"Given a set of vectors H , a query vector hq, and a weight matrix W, the attention function ATTEND(H,hq,W) computes a context vector z:
αi ∝ exp(hᵀiWh q) : i = 0, . . .",4 Model,[0],[0]
", |H|
z = |H|∑ i=1 αihi .
",4 Model,[0],[0]
"2To simplify the notation, we omit the memory cell (often denoted as cj) from all LSTM descriptions.",4 Model,[0],[0]
"We use only the hidden state hj to compute the intended representations (e.g., for the input text tokens).",4 Model,[0],[0]
"All LSTMs in this paper use zero vectors as initial hidden state h0 and initial cell memory c0.
",4 Model,[0],[0]
We use a decoder to generate actions.,4 Model,[0],[0]
"At each time step k, we compute an input representation using the attention function, update the decoder state, and compute the next action to execute.",4 Model,[0],[0]
"Attention is first computed over the vectors of the current instruction, which is then used to attend over the other inputs.",4 Model,[0],[0]
"We compute the context vectors zck and z p k for the current instruction and previous instructions:
zck = ATTEND(X c,hdk−1,W c) zpk = ATTEND(X p,",4 Model,[0],[0]
"[hdk−1, z c k],W p) ,
where hdk−1 is the decoder hidden state for step k− 1, and Xc and Xp are the sets of vector representations for the current instruction and previous instructions.",4 Model,[0],[0]
Two attention heads are used over both the initial and current states.,4 Model,[0],[0]
"This allows the model to attend to more than one location in a state at once, for example when transferring items from one beaker to another in ALCHEMY.",4 Model,[0],[0]
"The current state is computed by the transition function sk = T (sk−1, ak−1), where sk−1 and ak−1 are the state and action at step k − 1.",4 Model,[0],[0]
"The context vectors for the initial state s1 and the current state sk are:
zs1,k =",4 Model,[0],[0]
"[ATTEND(ENC(s1), [h d k−1, z c k],W sb,1);
",4 Model,[0],[0]
"ATTEND(ENC(s1),",4 Model,[0],[0]
"[hdk−1, z c k],W sb,2)]
zsk,k =",4 Model,[0],[0]
"[ATTEND(ENC(sk), [h d k−1, z c k],W sc,1);
ATTEND(ENC(sk),",4 Model,[0],[0]
"[hdk−1, z c k],W sc,2)] ,
where all W∗,∗ are learned weight matrices.
",4 Model,[0],[0]
"We concatenate all computed context vectors with an embedding of the previous action ak−1 to create the input for the decoder:
hk = tanh([z c k; z p k; z s 1,k; z s k,k;φ O(ak−1)]W d + bd) hdk = LSTM D ( hk;h d k−1 ) ,
where φO is a learned action embedding function and LSTMD is the LSTM decoder recurrence.
",4 Model,[0],[0]
"Given the decoder state hdk, the next action ak is predicted with a multi-layer perceptron (MLP).",4 Model,[0],[0]
"The actions in our domains decompose to an action type and at most two arguments.3 For example, the action PUSH 1 B in ALCHEMY has the type PUSH and two arguments: a beaker number and a color.",4 Model,[0],[0]
Section 6 describes the actions of each domain.,4 Model,[0],[0]
"The probability of an action is:
3We use a NULL argument for unused arguments.
",4 Model,[0],[0]
"hak = tanh(h d kW a)
sk,aT = h a kbaT sk,a1 = h a kba1 sk,a2 = h a kba2
p(ak = aT (a1, a2) | s̃k; θ) ∝ exp(sk,aT + sk,a1 + sk,a2) ,
where aT , a1, and a2 are an action type, first argument, and second argument.",4 Model,[0],[0]
"If the predicted action is STOP, the execution is complete.",4 Model,[0],[0]
"Otherwise, we execute the action ak to generate the next state sk+1, and update the agent context s̃k to s̃k+1 by appending the pair (sk, ak) to the execution ē and replacing the current state with sk+1.
",4 Model,[0],[0]
"The model parameters θ include: the embedding functions φI and φO; the recurrence parameters for −−−−−→ LSTME , ←−−−−− LSTME , and LSTMD; WC , WP , Wsb,1, Wsb,2, Wsc,1, Wsc,2, Wd, Wa, and bd; and the domain dependent parameters, including the parameters of the encoding function ENC and the action type, first argument, and second argument weights baT , ba1 , and ba2 .",4 Model,[0],[0]
We estimate the policy parameters θ using an exploration-based learning algorithm that maximizes the immediate expected reward.,5 Learning,[0],[0]
"Broadly speaking, during learning, we observe the agent behavior given the current policy, and for each visited state compute the expected immediate reward by observing rewards for all actions.",5 Learning,[0],[0]
"We assume access to a set of training examples {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",5 Learning,[0],[0]
", x̄ (j) i−1〉, g (j) i )} N,n(j)",5 Learning,[0],[0]
"j=1,i=1, where each instruction x̄(j)i is paired with a start state s
(j) i,1 , the previous instructions in the sequence
〈x̄(j)1 , .",5 Learning,[0],[0]
. .,5 Learning,[0],[0]
", x̄ (j) i−1〉, and a goal state g (j) i .
",5 Learning,[0],[0]
Reward The reward R(j)i : S × S ×,5 Learning,[0],[0]
"A → R is defined for each example j and instruction i:
R (j) i (s, a, s ′) = P (j) i (s, a, s ′)",5 Learning,[0],[0]
+ φ,5 Learning,[0],[0]
(j) i (s ′)− φ(j)i,5 Learning,[0],[0]
"(s) ,
where s is a source state, a is an action, and s′ is a target state.4 P (j)i (s, a, s
′) is a problem reward and φ
(j)",5 Learning,[0],[0]
i (s ′)− φ(j)i,5 Learning,[0],[0]
(s) is a shaping term.,5 Learning,[0],[0]
"The problem reward P (j)i (s, a, s
′) is positive for stopping at the goal g(j)i and negative for stopping in an incorrect
4While the reward function is defined for any state-actionstate tuple, in practice, it is used during learning with tuples that follow the system dynamics, s′ = T (s, a).
",5 Learning,[0],[0]
"Algorithm 1 SESTRA: Single-step Reward Observation.
",5 Learning,[0],[0]
"Input: Training data {(x̄(j)i , s (j) i,1 , 〈x̄ (j) 1 , . . .",5 Learning,[0],[0]
", x̄ (j) i−1〉,
g (j) i )}
N,n(j)
j=1,i=1, learning rate µ, entropy regularization coefficient λ, episode limit horizon M .",5 Learning,[0],[0]
"Definitions: πθ is a policy parameterized by θ, BEG is a special action to use for the first decoder step, and STOP indicates end of an execution.",5 Learning,[0],[0]
"T (s, a) is the state transition function, H is an entropy function, R(j)i (s, a, s
′) is the reward function for example j and instruction i, and RMSPROP divides each weight by a running average of its squared gradient (Tieleman and Hinton, 2012).
",5 Learning,[0],[0]
Output: Parameters θ defining a learned policy πθ .,5 Learning,[0],[0]
"1: for t = 1, . . .",5 Learning,[0],[0]
", T, j = 1, . . .",5 Learning,[0],[0]
", N do 2: for i = 1, . . .",5 Learning,[0],[0]
", n(j) do 3: ē← 〈 〉, k ← 0, a0 ← BEG 4: » Rollout up to STOP or episode limit.",5 Learning,[0],[0]
5:,5 Learning,[0],[0]
"while ak 6= STOP ∧ k < M do 6: k ← k + 1 7: s̃k ← (x̄i, 〈x̄1, . . .",5 Learning,[0],[0]
", x̄i−1〉, sk, ē[: k]) 8",5 Learning,[0],[0]
: » Sample an action from policy.,5 Learning,[0],[0]
"9: ak ∼ πθ(s̃k, ·) 10: sk+1 ← T (sk, ak) 11:",5 Learning,[0],[0]
ē←,5 Learning,[0],[0]
"[ē; 〈(sk, ak)〉] 12: ∆← 0̄ 13: for k′ = 1, . . .",5 Learning,[0],[0]
", k do 14: » Compute the entropy of πθ(s̃k′ , ·).",5 Learning,[0],[0]
"15: ∆← ∆ + λ∇θH(πθ(s̃k′ , ·)) 16: for a ∈",5 Learning,[0],[0]
"A do 17: s′ ← T (sk′ , a) 18: » Compute gradient for action a. 19: ∆← ∆ +R(j)i (sk′ , a, s ′)∇θπθ(s̃k′ , a)
20: θ ← θ + µRMSPROP",5 Learning,[0],[0]
"( ∆
k ) 21: return θ
state or taking an invalid action:
P (j) i (s, a, s ′) =  1.0 a = STOP ∧ s′ = g(j)i −1.0",5 Learning,[0],[0]
a = STOP ∧ s′ 6= g(j)i,5 Learning,[0],[0]
"−1.0− δ s = s′
−δ otherwise
,
where δ is a verbosity penalty.",5 Learning,[0],[0]
"The case s = s′ indicates that a was invalid in state s, as in this domain, all valid actions except STOP modify the state.",5 Learning,[0],[0]
We use a potential-based shaping term φ(j)i,5 Learning,[0],[0]
"(s
′)− φ(j)i",5 Learning,[0],[0]
"(s) (Ng et al., 1999), where φ
(j) i (s) =",5 Learning,[0],[0]
"−||s− g (j) i || computes the edit distance between the state s and the goal, measured over the objects in each state.",5 Learning,[0],[0]
"The shaping term densifies the reward, providing a meaningful signal for learning in nonterminal states.
",5 Learning,[0],[0]
Objective We maximize the immediate expected reward over all actions and use entropy regularization.,5 Learning,[0],[0]
"The gradient is approximated by sampling an execution ē = 〈(s1, a1), . . .",5 Learning,[0],[0]
", (sk, ak)〉 using our current policy:
∇θJ = 1
k k∑ k′=1 (∑ a∈A",5 Learning,[0],[0]
"R (sk, a, T (sk, a))∇θπ(s̃k, a)
+λ∇θH(π(s̃k, ·)) ) ,
where H(π(s̃k, ·) is the entropy term.
",5 Learning,[0],[0]
Algorithm Algorithm 1 shows the Single-step Reward Observation (SESTRA) learning algorithm.,5 Learning,[0],[0]
We iterate over the training data T times (line 1).,5 Learning,[0],[0]
"For each example j and turn i, we first perform a rollout by sampling an execution ē from πθ with at most M actions (lines 5-11).",5 Learning,[0],[0]
"If the rollout reaches the horizon without predicting STOP, we set the problem reward P (j)i to−1.0 for the last step.",5 Learning,[0],[0]
"Given the sampled states visited, we compute the entropy (line 15) and observe the immediate reward for all actions (line 19) for each step.",5 Learning,[0],[0]
"Entropy and rewards are used to accumulate the gradient, which is applied to the parameters using RMSPROP (Dauphin et al., 2015) (line 20).
",5 Learning,[0],[0]
Discussion Observing the rewards for all actions for each visited state addresses an on-policy learning exploration problem.,5 Learning,[0],[0]
"Actions that consistently receive negative reward early during learning will be visited with very low probability later on, and in practice, often not explored at all.",5 Learning,[0],[0]
"Because the network is randomly initialized, these early negative rewards are translated into strong general biases that are not grounded well in the observed context.",5 Learning,[0],[0]
Our algorithm exposes the agent to such actions later on when they receive positive rewards even though the agent does not explore them during rollout.,5 Learning,[0],[0]
"For example, in ALCHEMY, POP actions are sufficient to complete the first steps of good executions.",5 Learning,[0],[0]
"As a result, early during learning, the agent learns a strong bias against PUSH actions.",5 Learning,[0],[0]
"In practice, the agent then will not explore PUSH actions again.",5 Learning,[0],[0]
"In our algorithm, as the agent learns to roll out the correct POP prefix, it is then exposed to the reward for the first PUSH even though it likely sampled another POP.",5 Learning,[0],[0]
"It then unlearns its bias towards predicting POP.
",5 Learning,[0],[0]
"Our learning algorithm can be viewed as a costsensitive variant of the oracle in DAGGER (Ross et al., 2011), where it provides the rewards for all actions instead of an oracle action.",5 Learning,[0],[0]
"It is also related to Locally Optimal Learning to Search (LOLS; Chang et al., 2015) with two key distinctions: (a) instead of using different roll-in and roll-out policies, we use the model policy; and (b) we branch at each step, instead of once, but do not rollout
from branched actions since we only optimize the immediate reward.",5 Learning,[0],[0]
Figure 3 illustrates the comparison.,5 Learning,[0],[0]
"Our summation over immediate rewards for all actions is related the summation of estimated Q-values for all actions in the Mean Actor-Critic algorithm (Asadi et al., 2017).",5 Learning,[0],[0]
"Finally, our approach is related to Misra et al. (2017), who also maximize the immediate reward, but do not observe rewards for all actions for each state.",5 Learning,[0],[0]
"SCONE has three domains: ALCHEMY, SCENE, and TANGRAMS.",6 SCONE Domains and Data,[0],[0]
Each interaction contains five instructions.,6 SCONE Domains and Data,[0],[0]
Table 1 shows data statistics.,6 SCONE Domains and Data,[0],[0]
Table 2 shows discourse reference analysis.,6 SCONE Domains and Data,[0],[0]
"State encodings are detailed in the Supplementary Material.
",6 SCONE Domains and Data,[0],[0]
"ALCHEMY Each environment in ALCHEMY contains seven numbered beakers, each containing up to four colored chemicals in order.",6 SCONE Domains and Data,[0],[0]
Figure 1 shows an example.,6 SCONE Domains and Data,[0],[0]
"Instructions describe pouring chemicals between and out of beakers, and mixing beakers.",6 SCONE Domains and Data,[0],[0]
We treat all beakers as stacks.,6 SCONE Domains and Data,[0],[0]
"There
are two action types: PUSH and POP.",6 SCONE Domains and Data,[0],[0]
"POP takes a beaker index, and removes the top color.",6 SCONE Domains and Data,[0],[0]
"PUSH takes a beaker index and a color, and adds the color at the top of the beaker.",6 SCONE Domains and Data,[0],[0]
"To encode a state, we encode each beaker with an RNN, and concatenate the last output with the beaker index embedding.",6 SCONE Domains and Data,[0],[0]
"The set of vectors is the state embedding.
",6 SCONE Domains and Data,[0],[0]
"SCENE Each environment in SCENE contains ten positions, each containing at most one person defined by a shirt color and an optional hat color.",6 SCONE Domains and Data,[0],[0]
"Instructions describe adding or removing people, moving a person to another position, and moving a person’s hat to another person.",6 SCONE Domains and Data,[0],[0]
"There are four action types: ADD_PERSON, ADD_HAT, REMOVE_PERSON, and REMOVE_HAT.",6 SCONE Domains and Data,[0],[0]
ADD_PERSON and ADD_HAT take a position to place the person or hat and the color of the person’s shirt or hat.,6 SCONE Domains and Data,[0],[0]
REMOVE_PERSON,6 SCONE Domains and Data,[0],[0]
and REMOVE_HAT take the position to remove a person or hat from.,6 SCONE Domains and Data,[0],[0]
"To encode a state, we use a bidirectional RNN over the ordered positions.",6 SCONE Domains and Data,[0],[0]
The input for each position is a concatenation of the color embeddings for the person and hat.,6 SCONE Domains and Data,[0],[0]
"The set of RNN hidden states is the state embedding.
",6 SCONE Domains and Data,[0],[0]
TANGRAMS Each environment in TANGRAMS is a list containing at most five unique objects.,6 SCONE Domains and Data,[0],[0]
"Instructions describe removing or inserting an object into a position in the list, or swapping the positions of two items.",6 SCONE Domains and Data,[0],[0]
There are two action types: INSERT and REMOVE.,6 SCONE Domains and Data,[0],[0]
"INSERT takes the position to insert an object, and the object identifier.",6 SCONE Domains and Data,[0],[0]
REMOVE takes an object position.,6 SCONE Domains and Data,[0],[0]
We embed each object by concatenating embeddings for its type and position.,6 SCONE Domains and Data,[0],[0]
The resulting set is the state embedding.,6 SCONE Domains and Data,[0],[0]
"Evaluation Following Long et al. (2016), we evaluate task completion accuracy using exact match between the final state and the annotated goal state.",7 Experimental Setup,[0],[0]
"We report accuracy for complete interactions (5utts), the first three utterances of each interaction (3utts), and single instructions (Inst).",7 Experimental Setup,[0],[0]
"For single instructions, execution starts from the annotated start state of the instruction.
",7 Experimental Setup,[0],[0]
Systems We report performance of ablations and two baseline systems:,7 Experimental Setup,[0],[0]
"POLICYGRADIENT: policy gradient with cumulative episodic reward without a baseline, and CONTEXTUALBANDIT: the contextual bandit approach of Misra et al. (2017).",7 Experimental Setup,[0],[0]
"Both systems use the reward with the
shaping term and our model.",7 Experimental Setup,[0],[0]
We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using contextaction demonstration pairs.,7 Experimental Setup,[0],[0]
Only the supervised approach uses the heuristically generated labels.,7 Experimental Setup,[0],[0]
"Although the results are not comparable, we also report the performance of previous approaches to SCONE.",7 Experimental Setup,[0],[0]
All three approaches generate logical representations based on lambda calculus.,7 Experimental Setup,[0],[0]
"In contrast to our approach, this requires an ontology of hand built symbols and rules to evaluate the logical forms.",7 Experimental Setup,[0],[0]
"Fried et al. (2018) uses supervised learning with annotated logical forms.
",7 Experimental Setup,[0],[0]
"Training Details For test results, we run each experiment five times and report results for the model with best validation interaction accuracy.",7 Experimental Setup,[0],[0]
"For ablations, we do the same with three experiments.",7 Experimental Setup,[0],[0]
We use a batch size of 20.,7 Experimental Setup,[0],[0]
We stop training using a validation set sampled from the training data.,7 Experimental Setup,[0],[0]
We hold the validation set constant for each domain for all experiments.,7 Experimental Setup,[0],[0]
"We use patience over the average reward, and select the best model using interaction-level (5utts) validation accuracy.",7 Experimental Setup,[0],[0]
"We tune λ, δ, and M on the development set.",7 Experimental Setup,[0],[0]
The selected values and other implementation details are described in the Supplementary Material.,7 Experimental Setup,[0],[0]
Table 3 shows test results.,8 Results,[0],[0]
"Our approach significantly outperforms POLICYGRADIENT and CONTEXTUALBANDIT, both of which suffer due to biases learned early during learning, hindering later exploration.",8 Results,[0],[0]
"This problem does not appear in TANGRAMS, where no action type is dominant at the beginning of executions, and all methods perform well.",8 Results,[0],[0]
"POLICYGRADIENT completely fails to learn ALCHEMY and SCENE due to observing only negative total rewards early during learning.
",8 Results,[0],[0]
"Using a baseline, for example with an actor-critic method, will potentially close the gap to CONTEXTUALBANDIT.",8 Results,[0],[0]
"However, it is unlikely to address the on-policy exploration problem.
",8 Results,[0],[0]
"Table 4 shows development results, including model ablation studies.",8 Results,[0],[0]
Removing previous instructions (– previous instructions) or both states (– current and initial state) reduces performance across all domains.,8 Results,[0],[0]
Removing only the initial state (– initial state) or the current state (– current state) shows mixed results across the domains.,8 Results,[0],[0]
"Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains.",8 Results,[0],[0]
We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains.,8 Results,[0],[0]
"In our development and test results we use a single architecture across the three domains, the full approach, which has the highest interactive-level accuracy when averaged across the three domains (62.7 5utts).",8 Results,[0],[0]
We also report mean and standard deviation for our approach over five trials.,8 Results,[0],[0]
"We observe exceptionally high variance in performance on SCENE, where some experiments fail to learn and training performance remains exceptionally low (Figure 4).",8 Results,[0],[0]
"This highlights the sensitivity of the model to the random effects of initialization, dropout, and ordering of training examples.
",8 Results,[0],[0]
We analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction.,8 Results,[0],[0]
We study fifty examples in each domain to identify the type of failures.,8 Results,[0],[0]
Table 5 shows the counts of major error categories.,8 Results,[0],[0]
We consider multiple reference resolution errors.,8 Results,[0],[0]
State reference errors indicate a failure to resolve a reference to the world state.,8 Results,[0],[0]
"For example, in ALCHEMY, the phrase leftmost red beaker specifies a beaker in the environment.",8 Results,[0],[0]
"If the model picked the correct action, but the wrong beaker, we count it as a state reference.",8 Results,[0],[0]
"We distinguish between multi-turn reference errors that should be feasible, and these that that are impossible to solve without access to states before executing previous utterances, which are not provided to our model.",8 Results,[0],[0]
"For example, in TANGRAMS, the instruction put it back in the same place refers to a previouslyremoved item.",8 Results,[0],[0]
"Because the agent only has access to the world state after following this instruction, it does not observe what kind of item was previously removed, and cannot identify the item to add.",8 Results,[0],[0]
"We
also find a significant number of errors due to ambiguous or incorrect instructions.",8 Results,[0],[0]
"For example, the SCENE instruction person in green appears on the right end is ambiguous.",8 Results,[0],[0]
"In the annotated goal, it is interpreted as referring to a person already in the environment, who moves to the 10th position.",8 Results,[0],[0]
"However, it can also be interpreted as a new person in green appearing in the 10th position.
",8 Results,[0],[0]
We also study performance with respect to multi-turn coreference by observing whether the model was able to identify the correct referent for each occurrence included in the analysis in Table 2.,8 Results,[0],[0]
"The models were able to correctly resolve 92.3%, 88.7%, and 76.0% of references in ALCHEMY, SCENE, and TANGRAMS respectively.
",8 Results,[0],[0]
"Finally, we include attention visualization for examples from the three domains in the Supplementary Material.",8 Results,[0],[0]
"We propose a model to reason about contextdependent instructional language that display strong dependencies both on the history of the
interaction and the state of the world.",9 Discussion,[0],[0]
"Future modeling work may include using intermediate world states from previous turns in the interaction, which is required for some of the most complex references in the data.",9 Discussion,[0],[0]
"We propose to train our model using SESTRA, a learning algorithm that takes advantage of single-step reward observations to overcome learned biases in on-policy learning.",9 Discussion,[0],[0]
Our learning approach requires additional reward observations in comparison to conventional reinforcement learning.,9 Discussion,[0],[0]
"However, it is particularly suitable to recovering from biases acquired early during learning, for example due to biased action spaces, which is likely to lead to incorrect blame assignment in neural network policies.",9 Discussion,[0],[0]
"When the domain and model are less susceptible to such biases, the benefit of the additional reward observations is less pronounced.",9 Discussion,[0],[0]
"One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them.",9 Discussion,[0],[0]
"This research was supported by the NSF (CRII1656998), Schmidt Sciences, and cloud computing credits from Amazon.",Acknowledgements,[0],[0]
We thank John Langford and Dipendra Misra for helpful and insightful discussions with regards to our learning algorithm.,Acknowledgements,[0],[0]
We also thank the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
We propose a learning approach for mapping context-dependent sequential instructions to actions.,abstractText,[0],[0]
We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world.,abstractText,[0],[0]
"To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of singlestep reward observations and immediate expected reward maximization.",abstractText,[0],[0]
"We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%25.3% across the domains over approaches that use high-level logical representations.",abstractText,[0],[0]
Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation,title,[0],[0]
"1International Computer Science Institute and Department of Statistics, University of California at Berkeley, USA 2Department of Computer Science, Rensselaer Polytechnic Institute, USA. Correspondence to: Shusen Wang <shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).
lem while greatly mitigating the statistical risks incurred by sketching.",text,[0],[0]
Regression is one of the most fundamental problems in machine learning.,1. Introduction,[0],[0]
The simplest and most thoroughly studied regression model is least squares regression (LSR).,1. Introduction,[0],[0]
Given features X =,1. Introduction,[0],[0]
[xT1 ; . . .,1. Introduction,[0],[0]
",x T n ] ∈ Rn×d and responses y =",1. Introduction,[0],[0]
"[y1, . . .",1. Introduction,[0],[0]
", yn] T ∈ Rn, the LSR problem minw ‖Xw",1. Introduction,[0],[0]
− y‖22 can be solved in O(nd2) time using the QR decomposition or in O(ndt) time using accelerated gradient descent algorithms.,1. Introduction,[0],[0]
"Here, t is the number of iterations, which depends on the initialization, the condition number of X, and the stopping criterion.
",1. Introduction,[0],[0]
"This paper considers the n d problem, where there is much redundancy in X. Matrix sketching, as used within Randomized Linear Algebra (RLA) (Mahoney, 2011; Woodruff, 2014), works by reducing the size of X without losing too much information; this operation can be modeled as taking actual rows or linear combinations of the rows of X with a sketching matrix S to form the sketch STX.",1. Introduction,[0],[0]
Here S ∈,1. Introduction,[0],[0]
"Rn×s satisfies d < s n so that STX generically has the same rank but much fewer rows as X. Sketching has been used to speed up LSR (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) by solving the sketched LSR problem minw ‖STXw",1. Introduction,[0],[0]
− STy‖22 instead of the original LSR problem.,1. Introduction,[0],[0]
"Solving sketched LSR costs either O(sd2 + Ts) time using the QR decomposition or O(sdt + Ts) time using accelerated gradient descent algorithms, where t is as defined previously1 and Ts is the time cost of sketching.",1. Introduction,[0],[0]
"For example, Ts = O(nd log s) when S is the subsampled randomized Hadamard transform (Drineas et al., 2011), and Ts = O(nd) when S is a CountSketch matrix (Clarkson & Woodruff, 2013).
",1. Introduction,[0],[0]
"There has been much work in RLA on analyzing the quality of sketched LSR with different sketching methods and different objectives; see the reviews (Mahoney, 2011;
1The condition number of XTSSTX is very close to that of XTX, and thus the number of iterations t is almost unchanged.
",1. Introduction,[0],[0]
"Woodruff, 2014) and the references therein.
",1. Introduction,[0],[0]
"The concept of sketched LSR originated in the theoretical computer science literature, e.g., (Drineas et al., 2006; 2011), where the behavior of sketched LSR was studied from an optimization perspective.",1. Introduction,[0],[0]
Let w? be the optimal LSR solution and w̃ be the solution to sketched LSR.,1. Introduction,[0],[0]
"This line of work established that if s = O(d/ + poly(d)), then the objective function value ‖Xw̃ − y ∥∥2 2 is at most
times worse than ‖Xw?−y ∥∥2 2 .",1. Introduction,[0],[0]
"These works also bounded ‖w̃−w?‖22 in terms of the difference in the objective function values and the condition number of XTX.
",1. Introduction,[0],[0]
"A more recent line of work has studied sketched LSR from a statistical perspective: (Ma et al., 2015; Raskutti & Mahoney, 2016; Pilanci & Wainwright, 2015; Wang et al., 2016b) considered statistical properties of sketched LSR like the bias and variance.",1. Introduction,[0],[0]
"In particular, Pilanci & Wainwright (2015) showed that sketched LSR has much higher variance than the optimal solution.
",1. Introduction,[0],[0]
Both of these perspectives are important and of practical interest.,1. Introduction,[0],[0]
The optimization perspective is relevant when the data can be taken as deterministic values.,1. Introduction,[0],[0]
"The statistical perspective is relevant in machine learning and statistics applications where the data are random, and the regression coefficients are therefore themselves random variables.
",1. Introduction,[0],[0]
"In practice, regularized regression, e.g., ridge regression and LASSO, exhibit more attractive bias-variance tradeoffs and generalization errors than vanilla LSR.",1. Introduction,[0],[0]
"Furthermore, the matrix generalization of LSR, where multiple responses are to be predicted, is often more useful than LSR.",1. Introduction,[0],[0]
"However, the properties of sketched regularized matrix regression are largely unknown.",1. Introduction,[0],[0]
"Hence, the question: How, if at all, does our understanding of the optimization and statistical properties of sketched LSR generalize to sketched regularized regression?",1. Introduction,[0],[0]
"We answer this question for sketched matrix ridge regression (MRR).
",1. Introduction,[0],[0]
Recall that X is n×,1. Introduction,[0],[0]
d. Let Y ∈ Rn×m denote the matrix of corresponding responses.,1. Introduction,[0],[0]
"We study the MRR problem
min W
{ f(W) , 1
n",1. Introduction,[0],[0]
"∥∥XW −Y∥∥2 F + γ‖W‖2F } , (1)
which has optimal solution
W? =",1. Introduction,[0],[0]
(XTX + nγId) †XTY.,1. Introduction,[0],[0]
"(2)
Here, (·)† denotes the Moore-Penrose inversion operation.
",1. Introduction,[0],[0]
"LSR is a special case of MRR, with m = 1 and γ = 0.",1. Introduction,[0],[0]
"The optimal solution W? can be obtained in O(nd2 + nmd) time using a QR decomposition of X. Sketching can be applied to MRR in two ways:
Wc = (XTSSTX + nγId) †(XTSSTY), (3) Wh = (XTSSTX + nγId) †XTY.",1. Introduction,[0],[0]
"(4)
Following the convention of Pilanci & Wainwright (2015); Wang et al. (2016a), we call Wc classical sketch and Wh Hessian sketch, which approximate the optimal solution W?.",1. Introduction,[0],[0]
Table 1 lists the time costs of the three solutions to MRR.,1. Introduction,[0],[0]
We first study classical and Hessian sketches from the optimization perspective.,1.1. Main Results and Contributions,[0],[0]
"Theorems 1 and 2 show that
• Classical sketch achieves relative error in the objective value.",1.1. Main Results and Contributions,[0],[0]
"With sketch size s = Õ(d/ ), the objective satisfies f(Wc)",1.1. Main Results and Contributions,[0],[0]
"≤ (1 + )f(W?).
",1.1. Main Results and Contributions,[0],[0]
• Hessian sketch does not achieve relative error in the objective value.,1.1. Main Results and Contributions,[0],[0]
"In particular, if 1n‖Y‖ 2 F is much
larger than f(W?), then f(Wh) can be far larger than f(W?).
",1.1. Main Results and Contributions,[0],[0]
"• For both classical and Hessian sketch, the relative quality of approximation improves as the regularization parameter γ increases.
",1.1. Main Results and Contributions,[0],[0]
"We then study classical and Hessian sketches from the statistical perspective, by modeling Y = XW0 + Ξ as the sum of a true linear model and random noise, decomposing the risk R(W) = E‖XW",1.1. Main Results and Contributions,[0],[0]
"− XW0‖2F into bias and variance terms, and bounding these terms.",1.1. Main Results and Contributions,[0],[0]
"We draw the following conclusions (see Theorems 4, 5, 6):
•",1.1. Main Results and Contributions,[0],[0]
The bias of the classical sketch can be nearly as small as that of the optimal solution.,1.1. Main Results and Contributions,[0],[0]
"The variance is Θ ( n s ) times that of the optimal solution; this bound
is optimal.",1.1. Main Results and Contributions,[0],[0]
"Therefore over-regularization, i.e., large γ, should be used to supress the variance.",1.1. Main Results and Contributions,[0],[0]
"(As γ increases, the bias increases, and the variance decreases.)
",1.1. Main Results and Contributions,[0],[0]
"• Since Y is not sketched with Hessian sketch, the variance of Hessian sketch can be close to the optimal solution.",1.1. Main Results and Contributions,[0],[0]
"However, Hessian sketch has high bias, especially when nγ is small compared to ‖X‖22.",1.1. Main Results and Contributions,[0],[0]
"This indicates that over-regularization is necessary for Hessian sketch to have low bias.
",1.1. Main Results and Contributions,[0],[0]
Our empirical evaluations bear out these theoretical results.,1.1. Main Results and Contributions,[0],[0]
"In particular, in Section 4, we show in Figure 2 that even when the regularization parameter γ is fine-tuned, the risks of classical sketch and Hessian sketch are worse than that
of the optimal solution by an order of magnitude.",1.1. Main Results and Contributions,[0],[0]
"This is an empirical demonstration of the fact that the near-optimal properties of sketching from the optimization perspective are much less relevant in a statistical setting than its suboptimal statistical properties.
",1.1. Main Results and Contributions,[0],[0]
"We propose to use model averaging, which averages the solutions of g sketched MRR problems, to attain lower optimization and statistical errors.",1.1. Main Results and Contributions,[0],[0]
"Without ambiguity, we denote classical and Hessian sketches with model averaging by Wc and Wh, respectively.",1.1. Main Results and Contributions,[0],[0]
"Theorems 7, 8, 10, 11 give the following results:
• Classical Sketch.",1.1. Main Results and Contributions,[0],[0]
"Assume the sketch size s = Õ(d ) and ≤ 1g ; then the bound on f(W
c)",1.1. Main Results and Contributions,[0],[0]
− f(W?) is proportional to g .,1.1. Main Results and Contributions,[0],[0]
Assume that s = Õ,1.1. Main Results and Contributions,[0],[0]
"( d 2 ) and
2 ≤ 1 g ; the bias does not increase; the variance bound is proportional to 1g .
",1.1. Main Results and Contributions,[0],[0]
•,1.1. Main Results and Contributions,[0],[0]
Hessian Sketch.,1.1. Main Results and Contributions,[0],[0]
"Assume that s = Õ(d ) and ≤ 1 g2 ;
then the bound on f(Wh)",1.1. Main Results and Contributions,[0],[0]
− f(W?) is proportional to g2 .,1.1. Main Results and Contributions,[0],[0]
Assume that s = Õ,1.1. Main Results and Contributions,[0],[0]
"( d 2 ); the variance does not increase; if, additionally, ≤ 1g and nγ is much smaller than the squared spectral norm of X, then the bias bound is proportional to g .
",1.1. Main Results and Contributions,[0],[0]
"Note that classical sketch with uniform sampling and model averaging is very well known as bagging (Breiman, 1996) (or pasting (Breiman, 1999) or bootstrap aggregating).",1.1. Main Results and Contributions,[0],[0]
"Different from bagging, our model averaging approach is not limited to uniform sampling.
",1.1. Main Results and Contributions,[0],[0]
Classical sketch with model averaging has three immediate applications.,1.1. Main Results and Contributions,[0],[0]
"In the single-machine setting,
• Classical sketch with model averaging offers a way to improve the statistical performance in the presence of heavy noise.",1.1. Main Results and Contributions,[0],[0]
"Assume the sketch size is s = Õ( √ nd).
",1.1. Main Results and Contributions,[0],[0]
"As g grows larger than ns , the variance of the averaged solution can be even lower than the optimal solution.",1.1. Main Results and Contributions,[0],[0]
See Remark 1 for further discussion.,1.1. Main Results and Contributions,[0],[0]
"This observation is in accordance with the observation that bagging reduces variance.
",1.1. Main Results and Contributions,[0],[0]
"In the distributed setting, the feature-response pairs (x1,y1), · · · , (xn,yn) ∈",1.1. Main Results and Contributions,[0],[0]
Rd × Rm are divided among g machines.,1.1. Main Results and Contributions,[0],[0]
"Assuming that the data have been shuffled randomly, each machine contains a sketch constructed by uniformly sampled rows from the dataset without replacement.",1.1. Main Results and Contributions,[0],[0]
"In this setting, the model averaging procedure will communicate the g local models only once to return the final estimate; this process has very low communication complexity and latency, and it suggests two further applications of classical sketch with model averaging:
• Model Averaging for Machine Learning.",1.1. Main Results and Contributions,[0],[0]
"If a lowprecision solution is acceptable, the averaged solution can be used in lieu of distributed numerical optimization algorithms requiring multiple rounds of communication.",1.1. Main Results and Contributions,[0],[0]
"If ng is big enough compared to d and the row coherence of X is small, then “one-shot” model averaging has bias and variance comparable to the optimal solution.
",1.1. Main Results and Contributions,[0],[0]
• Model Averaging for Optimization.,1.1. Main Results and Contributions,[0],[0]
"If a highprecision solution to MRR is required, then an iterative numerical optimization algorithm must be used.",1.1. Main Results and Contributions,[0],[0]
"The cost of such numerical optimization algorithms heavily depends on the quality of the initialization.2
A good initialization saves lots of iterations.",1.1. Main Results and Contributions,[0],[0]
"The averaged model is provably close to the optimal solution, so model averaging provides a high-quality initialization for more expensive algorithms.",1.1. Main Results and Contributions,[0],[0]
"The body of work on sketched LSR mentioned earlier (Drineas et al., 2006; 2011; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013) shares many similarities with our results.",1.2. Prior Work,[0],[0]
"However, the theories of sketched LSR developed from the optimization perspective do not obviously extend to MRR, and the statistical analysis of LSR and MRR differ: among other differences, LSR is unbiased while MRR has a nontrivial bias and therefore has a bias-variance tradeoff that must be considered.
",1.2. Prior Work,[0],[0]
"Lu et al. (2013) has considered a different application of sketching to ridge regression: they assume d n, reduce the number of features in X using sketching, and conduct statistical analysis.",1.2. Prior Work,[0],[0]
"Our setting differs in that we consider n d, reduce the number of samples by sketching, and allow for multiple responses.
",1.2. Prior Work,[0],[0]
"The model averaging analyzed in this paper is similar in spirit to the AVGM algorithm of (Zhang et al., 2013).",1.2. Prior Work,[0],[0]
"When classical sketch is used with uniform row sampling without replacement, our model averaging procedure is a special case of AVGM.",1.2. Prior Work,[0],[0]
"However, our results do not follow from those of (Zhang et al., 2013): first, we make no assumption on the data, whereas they assumed x1, · · · ,xn are i.i.d. from an unknown distribution; second, our results apply to many other sketching ensembles than uniform sampling without replacement; and third, we provide both optimization and statistical perspectives, whereas they provide only a statistical perspective.",1.2. Prior Work,[0],[0]
"Our results clearly indicate that the
2For example, the conjugate gradient method satisfies ‖W(t)−W?‖2F ‖W(0)−W?‖2
F
≤ θt1; the stochastic block coordinate descent (Tu
et al., 2016) satisfies Ef(W (t))−f(W?)
f(W(0))−f(W?)",1.2. Prior Work,[0],[0]
≤ θ t 2.,1.2. Prior Work,[0],[0]
"Here W(t) is the
output of the t-th iteration; θ1, θ2 ∈ (0, 1) depend on the condition number of XTX + nγId and some other factors.
",1.2. Prior Work,[0],[0]
"performance critically depends on the row coherence of X; this dependence is not captured in (Zhang et al., 2013).",1.2. Prior Work,[0],[0]
"For similar reasons, our work is different from the divide-andconquer kernel ridge regression algorithm of (Zhang et al., 2015).
",1.2. Prior Work,[0],[0]
Iterative Hessian sketch has been studied by Pilanci & Wainwright (2015); Wang et al. (2016a).,1.2. Prior Work,[0],[0]
"By way of comparison, all the algorithms in this paper are “one-shot” rather than iterative.",1.2. Prior Work,[0],[0]
"Upon completion of this paper, we noticed the contemporary works (Avron et al., 2016; Thanei et al., 2017).",1.2. Prior Work,[0],[0]
"Avron et al. (2016) studied classical sketch from the optimization perspective, and Thanei et al. (2017) studied LSR with model averaging.",1.2. Prior Work,[0],[0]
Section 2 defines our notation and introduces the sketching schemes we consider.,1.3. Paper Organization,[0],[0]
Section 3 presents our theoretical results.,1.3. Paper Organization,[0],[0]
Section 4 conducts experiments to verify our theories and demonstrates the usefulness of model averaging.,1.3. Paper Organization,[0],[0]
"Proofs of our claims and more empirical evaluations can be found in the technical report version (Wang et al., 2017).",1.3. Paper Organization,[0],[0]
"Throughout, we take In to be the n×n identity matrix and 0 to be a vector or matrix of all zeroes of the appropriate size.",2. Preliminaries,[0],[0]
Given a matrix A =,2. Preliminaries,[0],[0]
"[aij ], the i-th row is denoted by ai:, and a:j denotes the j-th column.",2. Preliminaries,[0],[0]
"The Frobenius and spectral norms of A are written as, respectively, ‖A‖F and ‖A‖2.",2. Preliminaries,[0],[0]
"The set {1, 2, · · · , n} is written [n].",2. Preliminaries,[0],[0]
"Let O, Ω, and Θ be the standard asymptotic notation.",2. Preliminaries,[0],[0]
"Let Õ conceal logarithm factors.
",2. Preliminaries,[0],[0]
"Throughout, we fix X ∈ Rn×d as our matrix of features.",2. Preliminaries,[0],[0]
"We set ρ = rank(X) and write the SVD of X as X = UΣVT , where U, Σ, V are respectively n × ρ, ρ × ρ, and d × ρ matrices.",2. Preliminaries,[0],[0]
We let σ1 ≥ · · · ≥ σρ > 0 be the singular values of X. The Moore-Penrose inverse of X is defined by X† = VΣ−1UT .,2. Preliminaries,[0],[0]
The row leverage scores of X are li = ‖u:i‖22 for i ∈,2. Preliminaries,[0],[0]
[n].,2. Preliminaries,[0],[0]
The row coherence of X is µ(X) =,2. Preliminaries,[0],[0]
nρ maxi ‖u:,2. Preliminaries,[0],[0]
i‖ 2 2.,2. Preliminaries,[0],[0]
"Throughout, we let µ be shorthand for µ(X).
",2. Preliminaries,[0],[0]
Matrix sketching turns big matrices into smaller ones without losing too much information useful in tasks like linear regression.,2. Preliminaries,[0],[0]
We denote the process of sketching a matrix X ∈ Rn×d by X′ = STX.,2. Preliminaries,[0],[0]
"Here, S ∈ Rn×s is called a sketching matrix and X′ ∈ Rs×d is called a sketch of X. In practice, except for Gaussian projection (where the entries of S are i.i.d. sampled fromN (0, 1/s)), the sketching matrix S is not formed explicitly.",2. Preliminaries,[0],[0]
"Matrix sketching can be accomplished by random sampling or random projection.
",2. Preliminaries,[0],[0]
"Random sampling corresponds to sampling rows of X
i.i.d.",2. Preliminaries,[0],[0]
"with replacement according to given row sampling probabilities p1, · · · , pm ∈ (0, 1).",2. Preliminaries,[0],[0]
"The corresponding (random) sketching matrix S ∈ Rn×s has exactly one non-zero entry per column, whose position indicates the index of the selected row; in practice, this S is not explicitly formed.",2. Preliminaries,[0],[0]
Uniform sampling fixes p1 = · · · = pn = 1n .,2. Preliminaries,[0],[0]
"Leverage score sampling sets pi proportional to the (exact or approximate (Drineas et al., 2012)) row leverage scores li of X.",2. Preliminaries,[0],[0]
"In practice shrinked leverage score sampling can be a better choice than leverage score sampling (Ma et al., 2015).",2. Preliminaries,[0],[0]
The sampling probabilities of shrinked leverage score sampling are defined by pi = 12 ( li∑n j=1,2. Preliminaries,[0],[0]
"lj + 1n ) .3
Gaussian projection is also well-known as the prototypical Johnson-Lindenstrauss transform (Johnson & Lindenstrauss, 1984).",2. Preliminaries,[0],[0]
"Let G ∈ Rm×s be a standard Gaussian matrix, i.e., each entry is sampled independently from N (0, 1).",2. Preliminaries,[0],[0]
"The matrix S = 1√
s G is a Gaussian projection
matrix.",2. Preliminaries,[0],[0]
"It takes O(nds) time to apply S ∈ Rn×s to any n × d dense matrix, which makes Gaussian projection inefficient relative to other forms of sketching.
",2. Preliminaries,[0],[0]
"Subsampled randomized Hadamard transform (SRHT) (Drineas et al., 2011; Lu et al., 2013; Tropp, 2011) is a more efficient alternative to Gaussian projection.",2. Preliminaries,[0],[0]
"Let Hn ∈ Rn×n be the Walsh-Hadamard matrix with +1 and −1 entries, D ∈ Rn×n be a diagonal matrix with diagonal entries sampled uniformly from {+1,−1}, and P ∈ Rn×s be the uniform row sampling matrix defined above.",2. Preliminaries,[0],[0]
"The matrix S = 1√
n DHnP ∈",2. Preliminaries,[0],[0]
"Rn×s is an SRHT matrix, and
can be applied to any n × d matrix in O(nd log s) time.",2. Preliminaries,[0],[0]
"In practice, the subsampled randomized Fourier transform (SRFT) (Woolfe et al., 2008) is often used in lieu of the SRHT, because the SRFT exists for all values of n, whereas Hn exists only for some values of n. Their performance and theoretical analyses are very similar.
CountSketch can be applied to any X ∈ Rn×d in O(nd) time (Charikar et al., 2004; Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguyên, 2013; Pham & Pagh, 2013; Weinberger et al., 2009).",2. Preliminaries,[0],[0]
"Though more efficient to apply, CountSketch requires a bigger sketch size than Gaussian projections, SRHT, and leverage score sampling to attain the same theoretical guarantees.",2. Preliminaries,[0],[0]
"The readers can refer to (Woodruff, 2014) for a detailed description of CountSketch.",2. Preliminaries,[0],[0]
"Sections 3.1 and 3.2 analyze sketched MRR from, respectively, optimization and statistical perspectives.",3. Main Results,[0],[0]
"Sec-
3In fact, pi can be any convex combination of li∑n j=1",3. Main Results,[0],[0]
"lj and 1 n
(Ma et al., 2015).",3. Main Results,[0],[0]
"We use the weight 1 2 for simplicity; our conclusions extend in a straightforward manner to other weightings.
tions 3.3 and 3.4 capture the impacts of model averaging on, respectively, the optimization and statistical properties of sketched MRR.
",3. Main Results,[0],[0]
We described six sketching methods in Section 2.,3. Main Results,[0],[0]
"For simplicity, in this section, we refer to leverage score sampling, shrinked leverage score sampling, Gaussian projection, and SRHT as the four sketching methods; and we will mention explicitly uniform sampling and CountSketch.",3. Main Results,[0],[0]
The notation defined in Table 2 are used throughout.,3. Main Results,[0],[0]
"Theorem 1 shows that f(Wc), the objective value of classical sketch, is very close to the optimal objective value f(W?).",3.1. Sketched MRR: Optimization Perspective,[0],[0]
The approximation quality improves as γ increases.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
Theorem 1 (Classical Sketch).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For the four sketching methods with s = Õ ( βd ) , uniform sampling with s =
O ( µβd log d ) , and CountSketch with s = O ( βd2 ) , the inequality
f(Wc)− f(W?)",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"≤ f(W?)
holds with probability at least 0.9.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
The corresponding guarantee for the performance of Hessian sketch is given in Theorem 2.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"It is weaker than the guarantee for classical sketch, especially when 1n‖Y‖ 2 F is far larger than f(W?).",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"If Y is nearly noiseless—Y is wellexplained by a linear combination of the columns of X— and γ is small, then f(W?) is close to zero, and consequently f(W?) can be far smaller than 1n‖Y‖ 2 F .",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Therefore, in this case which is ideal for MRR, f(Wh) is not close to f(W?) and our theory suggests Hessian sketch does not perform as well as classical sketch.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"This is verified by our experiments, which show that unless γ is big or a large portion of Y is outside the column space of X, the ratio f(W
h) f(W?) can be large.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Theorem 2 (Hessian Sketch).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For the four sketching methods with s = Õ
( β2d ) , uniform sampling with s =
O ( µβ2d log d ) , and CountSketch with s = O(β 2d2
), the inequality
f(Wh)− f(W?) ≤",3.1. Sketched MRR: Optimization Perspective,[0],[0]
( ‖Y‖2F n,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"− f(W?) ) .
holds with probability at least 0.9.
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
These two results imply that f(Wc) and f(Wh) can be close to f(W?).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"When this is the case, curvature of the objective function ensures that the sketched solutions Wc and Wh are close to the optimal solution W?.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Lemma 3 studies the Mahalanobis distance ‖M(W−W?)‖2F .,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Here M is any non-singular matrix; in particular, it can be the identity matrix or (XTX)1/2.",3.1. Sketched MRR: Optimization Perspective,[0],[0]
Lemma 3.,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"Let f be the objective function of MRR defined in (1), W ∈ Rd×m be arbitrary, and W?",3.1. Sketched MRR: Optimization Perspective,[0],[0]
be the optimal solution defined in (2).,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"For any non-singular matrix M, the Mahalanobis distance satisfies 1
n
∥∥M(W −W?)∥∥2 F ≤ f(W)− f(W ?)
σ2min",3.1. Sketched MRR: Optimization Perspective,[0],[0]
[ (XTSSTX + nγId)1/2M−1 ] .,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"By choosing M = (XTX)1/2, we can bound 1n‖XW − XW?‖2F in terms of the difference in the objective values:
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
1 n ∥∥XW −XW?∥∥2 F ≤,3.1. Sketched MRR: Optimization Perspective,[0],[0]
β,3.1. Sketched MRR: Optimization Perspective,[0],[0]
"[ f(W)− f(W?) ] .
",3.1. Sketched MRR: Optimization Perspective,[0],[0]
"With Lemma 3, we can directly apply Theorems 1 or 2 to bound 1n‖XW c −XW?‖2F or 1n‖XW h −XW?‖2F .",3.1. Sketched MRR: Optimization Perspective,[0],[0]
We consider the following fixed design model.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Let X ∈ Rn×d be the observed feature matrix, W0 ∈ Rd×m be the true and unknown model, Ξ ∈ Rn×m contain unknown random noise, and
Y = XW0 + Ξ (5)
be the observed responses.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We make the following standard weak assumptions on the noise:
E[Ξ] = 0 and E[ΞΞT ] = ξ2In.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We observe X and Y and seek to estimate W0.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"We can evaluate the quality of the estimate by the risk: R(W) = 1 n E ∥∥XW −XW0∥∥2F , (6)
where the expectation is taken w.r.t.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"the noise Ξ. We study the risk functions R(W?), R(Wc), and R(Wh) in the following.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 4 (Bias-Variance Decomposition).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
We consider the data model described in this subsection.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Let W be W?, Wc, or Wh, as defined in (2), (3), (4), respectively; then the risk function can be decomposed as
R(W) = bias2(W) + var(W).
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Recall the SVD of X: X = UΣVT .,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"The bias and variance terms can be written as
bias ( W? )",3.2. Sketched MRR: Statistical Perspective,[0],[0]
=,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(Σ2 + nγIρ)−1ΣVTW0∥∥∥
F ,
var ( W? )",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"= ξ2
n ∥∥∥(Iρ + nγΣ−2)−1∥∥∥2 F ,
bias ( Wc ) =",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(ΣUTSSTUΣ + nγIρ)†ΣVTW0∥∥∥
F ,
var ( Wc ) = ξ2
n ∥∥∥(UTSSTU + nγΣ−2)†UTSST∥∥∥2 F ,
bias ( Wh ) =",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"γ √ n ∥∥∥(Σ−2 + UTSSTU−Iρnγ )
· ( UTSSTU + nγΣ−2 )† ΣVTW0 ∥∥∥ F ,
var ( Wh ) = ξ2
n ∥∥∥(UTSSTU + nγΣ−2)†∥∥∥2 F .
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 5 provides upper and lower bounds on the bias and variance of the classical sketch.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In particular, we see that that bias(Wc) is within a factor of (1± ) of bias(W?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"However, var(Wc) is Θ(ns ) times worse than var(W ?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 5 (Classical Sketch).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For Gaussian projection and SRHT sketching with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), or CountSketch with s = O( d2
2 ), the inequalities
1− ≤",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"bias(W c)
bias(W?) ≤ 1",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"+ ,
(1− )n s ≤ var(W
c)
var(W?)",3.2. Sketched MRR: Statistical Perspective,[0],[0]
≤ (1 + )n,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"s
hold with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For shrinked leverage score sampling with s = O(d log d 2 ), these inequalities, except for the lower bound on the variance,4 hold with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 6 establishes similar upper and lower bounds on the bias and variance of Hessian sketch.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
The situation is the reverse of that with classical sketch: the variance of Wh is close to that of W?,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"if s is large enough, but as the regularization parameter γ goes to zero, bias(Wh) becomes much larger than bias(W?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Theorem 6 (Hessian Sketch).,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"For the four sketching methods with s = Õ( d 2 ), uniform sampling with s = O(µd log d 2 ), and CountSketch with s = O( d2
2 ), the inequalities
bias(Wh) bias(W?) ≤",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"(1 + )
( 1 +
‖X‖22 nγ
) ,
1− ≤ var(W h)
var(W?) ≤ 1",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"+
hold with probability at least 0.9.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
Further assume that σ2ρ ≥ nγ .,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Then
bias(Wh)
bias(W?)",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"≥ 1 1 + ( σ2ρ nγ − 1 )
holds with probability at least 0.9.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
The lower bound on the bias shows that Hessian sketch can suffer from a much higher bias than the optimal solution.,3.2. Sketched MRR: Statistical Perspective,[0],[0]
"The gap between bias(Wh) and bias(W?) can be
4For shrinked leverage score sampling, ‖S‖22 does not enjoy nontrivial lower bound.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"This is why we do not have a lower bound on the variance.
lessened by increasing the regularization parameter γ, but such over-regularization increases the baseline bias(W?) itself.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"It is also worth mentioning that unlike bias(W?) and bias(Wc), bias(Wh) is not monotonically increasing with γ, as is empirically verified in Figure 2.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In sum, our theories show that classical and Hessian sketches are not statistically comparable to the optimal solutions: classical sketch has too high a variance, and Hessian sketch has too high a bias for reasonable amounts of regularization.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"In practice, the regularization parameter γ should be tuned to optimize the prediction accuracy.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Our experiments in Figure 2 show that even with fine-tuned γ, the risks of classical and Hessian sketches can be higher than the risk of the optimal solution by an order of magnitude.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Formally speaking, minγ R(Wc) minγ R(W?) and minγ R(Wh) minγ R(W?) hold in practice.
",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Our empirical study in Figure 2 suggests classical and Hessian sketches both require over-regularization, i.e., setting γ larger than what is best for the optimal solution W?.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Formally speaking, argminγ R(W c) > argminγ R(W ?) and argminγ R(W h) > argminγ R(W
?).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Although this is the case for both types of sketches, the underlying explanations are different.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Classical sketch has a high variance, so a large γ is required to supress the variance (its variance is non-increasing with γ).",3.2. Sketched MRR: Statistical Perspective,[0],[0]
"Hessian sketch has very high bias when γ is small, so a reasonably large γ is necessary to lower its bias.",3.2. Sketched MRR: Statistical Perspective,[0],[0]
We consider model averaging as an approach to increasing the accuracy of sketched MRR solutions.,3.3. Model Averaging: Optimization Perspective,[0],[0]
"The model averaging procedure is straightforward: one independently draws g sketching matrices S1, · · · ,Sg ∈ Rn×s, uses these to form g sketched MRR solutions, denoted by {Wci} g i=1 or {Whi} g i=1, and averages these solutions to obtain the fi-
nal estimate Wc = 1g ∑g i=1",3.3. Model Averaging: Optimization Perspective,[0],[0]
W c,3.3. Model Averaging: Optimization Perspective,[0],[0]
i or W h = 1g ∑g i=1,3.3. Model Averaging: Optimization Perspective,[0],[0]
W h,3.3. Model Averaging: Optimization Perspective,[0],[0]
i .,3.3. Model Averaging: Optimization Perspective,[0],[0]
"Practical applications of model averaging are enumerated in Section 1.1.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorems 7 and 8 present guarantees on the optimization accuracy of using model averaging to combine the classical or Hessian sketch solutions.,3.3. Model Averaging: Optimization Perspective,[0],[0]
We can contrast these with the guarantees provided for sketched MRR in Theorems 1 and 2.,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For classical sketch with model averaging, we see that when ≤ 1g , the bound on f(W
h)−f(W?) is proportional to /g.",3.3. Model Averaging: Optimization Perspective,[0],[0]
"From Lemma 3 we can see that the distances from Wc to W? also decreases accordingly.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorem 7 (Classical Sketch with Model Averaging).,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For the four methods, let s = Õ
( βd ) ; for uniform sampling, let
s = O ( µβd log d ) .",3.3. Model Averaging: Optimization Perspective,[0],[0]
"Then the inequality
f(Wc)− f(W?)",3.3. Model Averaging: Optimization Perspective,[0],[0]
≤,3.3. Model Averaging: Optimization Perspective,[0],[0]
"( g + β2 2 ) f(W?)
holds with probability at least 0.8.
",3.3. Model Averaging: Optimization Perspective,[0],[0]
"For Hessian sketch with model averaging, if β2 ≤ 1 g2 , then the bound on f(Wh)− f(W?) is proportional to g2 .
",3.3. Model Averaging: Optimization Perspective,[0],[0]
Theorem 8 (Hessian Sketch with Model Averaging).,3.3. Model Averaging: Optimization Perspective,[0],[0]
"For the four methods let s = Õ
( β2d ) , and for uniform sam-
pling let s = O ( µβ2d log d ) , then the inequality
f(Wh)− f(W?) ≤",3.3. Model Averaging: Optimization Perspective,[0],[0]
( g2 + 2 β2 ) ( ‖Y‖2F n,3.3. Model Averaging: Optimization Perspective,[0],[0]
"− f(W?) ) .
holds with probability at least 0.8.",3.3. Model Averaging: Optimization Perspective,[0],[0]
Model averaging also has the salutatory property of reducing the risks of the classical and Hessian sketch solutions.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"Our first result conducts a bias-variance decomposition for the averaged solution of sketched MRR.
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 9 (Bias-Variance Decomposition).,3.4. Model Averaging: Statistical Perspective,[0],[0]
We consider the fixed design model (5).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"The risk function defined in (6) can be decomposed as
R(W) = bias2(W) + var(W).
",3.4. Model Averaging: Statistical Perspective,[0],[0]
The bias and variance terms are bias ( Wc ) =,3.4. Model Averaging: Statistical Perspective,[0],[0]
γ √ n ∥∥∥1,3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( ΣUTSiS T i UΣ + nγIρ,3.4. Model Averaging: Statistical Perspective,[0],[0]
),3.4. Model Averaging: Statistical Perspective,[0],[0]
"† ΣVTW0 ∥∥∥ F , var ( Wc ) = ξ2
n ∥∥∥1",3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( UTSiS T,3.4. Model Averaging: Statistical Perspective,[0],[0]
i U + nγΣ −2)†UTSiSTi,3.4. Model Averaging: Statistical Perspective,[0],[0]
∥∥∥2,3.4. Model Averaging: Statistical Perspective,[0],[0]
"F ,
bias ( Wh ) =",3.4. Model Averaging: Statistical Perspective,[0],[0]
γ √ n ∥∥∥1,3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( Σ−2 + UTSiS T i U− Iρ nγ ) · ( UTSiS T i U + nγΣ −2)†ΣVTW0∥∥∥,3.4. Model Averaging: Statistical Perspective,[0],[0]
"F ,
var ( Wh ) = ξ2
n ∥∥∥1",3.4. Model Averaging: Statistical Perspective,[0],[0]
g g∑ i=1,3.4. Model Averaging: Statistical Perspective,[0],[0]
( UTSiS T,3.4. Model Averaging: Statistical Perspective,[0],[0]
"i U + nγΣ −2)†∥∥∥2 F .
",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Theorems 10 and 11 provide upper bounds on the bias and variance of model-averaged sketched MRR for, respectively, classical sketch and Hessian sketch.",3.4. Model Averaging: Statistical Perspective,[0],[0]
We can contrast them with Theorems 5 and 6 to see the statistical benefits of model averaging.,3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 10 (Classical Sketch with Model Averaging).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For shrinked leverage score sampling, Gaussian projection, SRHT with s = Õ",3.4. Model Averaging: Statistical Perspective,[0],[0]
"( d 2 ) , or uniform sampling with
s = O ( µd log d 2 ) , the inequalities
bias(Wc) bias(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+ , var(Wc)
",3.4. Model Averaging: Statistical Perspective,[0],[0]
var(W?),3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ n s (√ 1+ /g,3.4. Model Averaging: Statistical Perspective,[0],[0]
"g + )2
hold with probability at least 0.8.",3.4. Model Averaging: Statistical Perspective,[0],[0]
Remark 1.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"From this result, we see that if ≤ 1√g , then the variance is proportional to 1g .",3.4. Model Averaging: Statistical Perspective,[0],[0]
"If g and s are at least
g = O (n s ) and s",3.4. Model Averaging: Statistical Perspective,[0],[0]
"= Õ (√ nd ) ,
then the risk R(Wc) is close to R(W?).",3.4. Model Averaging: Statistical Perspective,[0],[0]
"If g and s are larger, then the variance var(Wc) can even be even lower than var(W?).
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 11 shows that model averaging decreases the bias of Hessian sketch without increasing the variance.,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For Hessian sketch without model averaging, recall that bias(Wh)
is larger than bias(W?) by a factor of O(‖X‖22/(nγ)).",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Theorem 11 shows that model averaging reduces this ratio by a factor of g when ≤ 1 g .
",3.4. Model Averaging: Statistical Perspective,[0],[0]
Theorem 11 (Hessian Sketch with Model Averaging).,3.4. Model Averaging: Statistical Perspective,[0],[0]
"For the four methods with s = Õ ( d 2 ) , or uniform sampling
with s = O ( µd log d 2 ) , the inequalities
bias(Wh) bias(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
+,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+ ( g + 2 )‖X‖22 nγ ,
var(Wh) var(W?)",3.4. Model Averaging: Statistical Perspective,[0],[0]
≤ 1,3.4. Model Averaging: Statistical Perspective,[0],[0]
"+
hold with probability at least 0.8.",3.4. Model Averaging: Statistical Perspective,[0],[0]
"Following (Ma et al., 2015; Yang et al., 2016), we constructed X ∈ Rn×d to have condition number κ(XTX) = 1012 and high row coherence, fixed w0 =",4. Sketched Ridge Regression Experiments,[0],[0]
"[10.2d; 0.110.6d; 10.2d], and set y = Xw0 +ε ∈ Rn, where the entries of ε ∈",4. Sketched Ridge Regression Experiments,[0],[0]
Rn were i.i.d.,4. Sketched Ridge Regression Experiments,[0],[0]
"sampled from N (0, ξ2).",4. Sketched Ridge Regression Experiments,[0],[0]
"The details of this data model are given in the technical report version (Wang et al., 2017).",4. Sketched Ridge Regression Experiments,[0],[0]
Let S ∈,4. Sketched Ridge Regression Experiments,[0],[0]
Rn×s be any of the six sketching methods considered in this paper.,4. Sketched Ridge Regression Experiments,[0],[0]
"We fix n = 105, d = 500, and s = 5, 000.",4. Sketched Ridge Regression Experiments,[0],[0]
"Because the analytical expressions involve the random sketching matrix S, we randomly generate S, repeat this procedure 10 times, and report the averaged results.
",4. Sketched Ridge Regression Experiments,[0],[0]
"In Figure 1, we plot the objective function value f(w) = 1 n‖Xw",4. Sketched Ridge Regression Experiments,[0],[0]
"− y‖ 2 2 + γ‖w‖22 against γ, under different settings of noise intensity ξ.",4. Sketched Ridge Regression Experiments,[0],[0]
"The results verify our theory: classical sketch wc is always close to optimal; Hessian sketch wh is much worse than the optimal when γ is small and y is mostly in the column space of X.
We conducted experiments on synthetic data to verify Theorems 5 and 6 and to show the effects of classical and Hessian sketching on the bias and variance.",4. Sketched Ridge Regression Experiments,[0],[0]
We set the noise intensity to be ξ = 0.1.,4. Sketched Ridge Regression Experiments,[0],[0]
"In Figure 2, we plot the analytical expressions for the squared bias, variance, and risk stated in Theorem 4 against the regularization parameter γ.",4. Sketched Ridge Regression Experiments,[0],[0]
"The results of this experiment match our theory: classical sketch magnified the variance, and Hessian sketch increased the bias.",4. Sketched Ridge Regression Experiments,[0],[0]
"Even if γ is fine-tuned, the risks of classical and Hessian sketches can be much higher than those of the optimal solution.5",4. Sketched Ridge Regression Experiments,[0],[0]
Our experiments also indicate that classical and Hessian sketches require setting γ larger than the best regularization parameter for the optimal solution W?.,4. Sketched Ridge Regression Experiments,[0],[0]
We studied sketched matrix ridge regression (MRR) from optimization and statistical perspectives.,5. Conclusions,[0],[0]
"Using classical sketch, by taking a large enough sketch, one can obtain an -accurate approximate solution.",5. Conclusions,[0],[0]
"Counterintuitively and in contrast to classical sketch, the relative error of Hessian sketch increases as the responses Y are better approximated by linear combinations of the columns of X. Both classical and Hessian sketches can have statistical risks that are worse than the risk of the optimal solution by an order of magnitude.",5. Conclusions,[0],[0]
We proposed the use of model averaging to attain better optimization and statistical properties.,5. Conclusions,[0],[0]
"We have shown that model averaging leads to substantial improvements in the theoretical error bounds, suggesting applications in distributed optimization and machine learning.
",5. Conclusions,[0],[0]
"5In the experiment yielding Figure 2, Hessian sketch had lower risk than classical sketch.",5. Conclusions,[0],[0]
"This is not generally true: if we used a smaller ξ, so that the variance is dominated by bias, then classical sketch results in lower risks than Hessian sketch.",5. Conclusions,[0],[0]
We thank the anonymous reviewers for their helpful suggestions.,Acknowledgements,[0],[0]
We thank the Army Research Office and the Defense Advanced Research Projects Agency for partial support of this work.,Acknowledgements,[0],[0]
We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem.,abstractText,[0],[0]
"Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem.",abstractText,[0],[0]
"We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR—namely, it recovers nearly optimal solutions.",abstractText,[0],[0]
"In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the “mass” in the responses and the optimal objective value.",abstractText,[0],[0]
"For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem.",abstractText,[0],[0]
"In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR.",abstractText,[0],[0]
"We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches.",abstractText,[0],[0]
"Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions.",abstractText,[0],[0]
We establish theoretically and empirically that model averaging greatly decreases this gap.,abstractText,[0],[0]
"Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR probInternational Computer Science Institute and Department of Statistics, University of California at Berkeley, USA Department of Computer Science, Rensselaer Polytechnic Institute, USA.",abstractText,[0],[0]
Correspondence to: Shusen Wang,abstractText,[0],[0]
"<shusen@berkeley.edu>, Alex Gittens <gittea@rpi.edu>, Michael W. Mahoney <mmahoney@stat.berkeley.edu>.",abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
lem while greatly mitigating the statistical risks incurred by sketching.,abstractText,[0],[0]
"Sketched Ridge Regression: Optimization Perspective,  Statistical Perspective, and Model Averaging",title,[0],[0]
"Topological Data Analysis (TDA) is an emerging trend in data science, grounded on topological methods to design descriptors for complex data—see e.g. (Carlsson, 2009) for an introduction to the subject.",1. Introduction,[0],[0]
"The descriptors of TDA can be used in various contexts, in particular statistical learning and geometric inference, where they provide useful insight into the structure of data.",1. Introduction,[0],[0]
"Applications of TDA can be found in a number of scientific areas, including computer vision (Li et al., 2014), materials science (Hiraoka et al., 2016), and brain science (Singh et al., 2008), to name
1INRIA Saclay 2CREST, ENSAE, Université Paris Saclay.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Mathieu Carrière <mathieu.carriere@inria.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
a few.,1. Introduction,[0],[0]
"The tools developed in TDA are built upon persistent homology theory (Edelsbrunner & Harer, 2010; Oudot, 2015), and their main output is a descriptor called persistence diagram (PD), which encodes the topology of a space at all scales in the form of a point cloud with multiplicities in the plane R2—see Section 2.1 for more details.
PDs as features.",1. Introduction,[0],[0]
"The main strength of PDs is their stability with respect to perturbations of the data (Chazal et al., 2009b; 2013).",1. Introduction,[0],[0]
"On the downside, their use in learning tasks is not straightforward.",1. Introduction,[0],[0]
"Indeed, a large class of learning methods, such as SVM or PCA, requires a Hilbert structure on the descriptors space, which is not the case for the space of PDs.",1. Introduction,[0],[0]
"Actually, many simple operators of Rn, such as addition, average or scalar product, have no analogues in that space.",1. Introduction,[0],[0]
"Mapping PDs to vectors in Rn or in some infinite-dimensional Hilbert space is one possible approach to facilitate their use in discriminative settings.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"A series of recent contributions have proposed kernels for PDs, falling into two classes.",1. Introduction,[0],[0]
"The first class of methods builds explicit feature maps: One can, for instance, compute and sample functions extracted from PDs (Bubenik, 2015; Adams et al., 2017; Robins & Turner, 2016); sort the entries of the distance matrices of the PDs (Carrière et al., 2015); treat the PD points as roots of a complex polynomial, whose coefficients are concatenated (Fabio & Ferri, 2015).",1. Introduction,[0],[0]
"The second class of methods, which is more relevant to our work, defines implicitly feature maps by focusing instead on building kernels for PDs.",1. Introduction,[0],[0]
"For instance, Reininghaus et al. (2015) use solutions of the heat differential equation in the plane and compare them with the usual L2(R2) dot product.",1. Introduction,[0],[0]
"Kusano et al. (2016) handle a PD as a discrete measure on the plane, and follow by using kernel mean embeddings with Gaussian kernels—see Section 4 for precise definitions.",1. Introduction,[0],[0]
"Both kernels are provably stable, in the sense that the metric they induce in their respective reproducing kernel Hilbert space (RKHS) is bounded above by the distance between PDs.",1. Introduction,[0],[0]
"Although these kernels are injective, there is no evidence that their induced RKHS distances are discriminative and therefore follow the geometry of the bottleneck distances, which are more widely accepted distances to compare PDs.
Contributions.",1. Introduction,[0],[0]
"In this article, we use the sliced Wasserstein (SW) distance (Rabin et al., 2011) to define a new ker-
nel for PDs, which we prove to be both stable and discriminative.",1. Introduction,[0],[0]
"Specifically, we provide distortion bounds on the SW distance that quantify its ability to mimic bottleneck distances between PDs.",1. Introduction,[0],[0]
"This is in contrast to other kernels for PDs, which only focus on stability.",1. Introduction,[0],[0]
"We also propose a simple approximation algorithm to speed up the computation of that kernel, confirm experimentally its discriminative power and show that it outperforms experimentally both proposals of (Kusano et al., 2016) and (Reininghaus et al., 2015) in several supervised classification problems.",1. Introduction,[0],[0]
"We briefly review in this section relevant material on TDA, notably persistence diagrams, and technical properties of positive and negative definite kernel functions.",2. Background on TDA and Kernels,[0],[0]
"Persistent homology (Zomorodian & Carlsson, 2005; Edelsbrunner & Harer, 2008; Oudot, 2015) is a technique inherited from algebraic topology for computing stable signatures on real-valued functions.",2.1. Persistent Homology,[0],[0]
"Given f : X → R as input, persistent homology outputs a planar point set with multiplicities, called the persistence diagram of f and denoted by Dg f .",2.1. Persistent Homology,[0],[0]
See Figure 1 for an example.,2.1. Persistent Homology,[0],[0]
"To understand the meaning of each point in this diagram, it suffices to know that, to compute Dg f , persistent homology considers the family of sublevel sets of f , i.e. the sets of the form f−1((−∞, t]) for t ∈ R, and it records the topological events (e.g. creation or merge of a connected component, creation or filling of a loop, void, etc.) that occur in f−1((−∞, t]) as t ranges from −∞ to +∞. Then, each point p ∈",2.1. Persistent Homology,[0],[0]
"Dg f represents the lifespan of a particular topological feature (connected component, loop, void, etc.), with its creation and destruction times as coordinates.",2.1. Persistent Homology,[0],[0]
"See again Figure 1 for an illustration.
",2.1. Persistent Homology,[0],[0]
"For the interested reader, we point out that the mathematical tool used by persistent homology to track the topological events in the family of sublevel sets is homological algebra, which turns the parametrized family of sublevel sets into a parametrized family of vector spaces and linear maps.",2.1. Persistent Homology,[0],[0]
"Computing persistent homology then boils down to computing a family of bases for the vector spaces, which are compatible with the linear maps.
Distance between PDs.",2.1. Persistent Homology,[0],[0]
We now define the pth diagram distance between PDs.,2.1. Persistent Homology,[0],[0]
"Let p ∈ N and Dg1,Dg2 be two PDs.",2.1. Persistent Homology,[0],[0]
Let Γ : Dg1 ⊇ A → B ⊆ Dg2 be a partial bijection between Dg1 and Dg2.,2.1. Persistent Homology,[0],[0]
"Then, for any point x ∈ A, the cost of x is defined as c(x) := ‖x − Γ(x)‖p∞, and for any point y ∈ (Dg1 t Dg2) \",2.1. Persistent Homology,[0],[0]
"(A t B), the cost of y is defined as c′(y) := ‖y",2.1. Persistent Homology,[0],[0]
"− π∆(y)‖p∞, where π∆ is the projection onto the diagonal ∆ = {(x, x) | x ∈ R}.",2.1. Persistent Homology,[0],[0]
"The cost c(Γ)
is defined as: c(Γ) := ( ∑ x c(x) + ∑ y c ′(y))1/p.",2.1. Persistent Homology,[0],[0]
"We then define the pth diagram distance dp as the cost of the best partial bijection between the PDs:
dp(Dg1,Dg2) = inf Γ c(Γ).
",2.1. Persistent Homology,[0],[0]
"In the particular case p = +∞, the cost of Γ is defined as c(Γ)",2.1. Persistent Homology,[0],[0]
:= max{maxx δ(x) + maxy δ′(y)}.,2.1. Persistent Homology,[0],[0]
The corresponding distance d∞ is often called the bottleneck distance.,2.1. Persistent Homology,[0],[0]
One can show that dp → d∞ when p → +∞. A fundamental property of PDs is their stability with respect to (small) perturbations of their originating functions.,2.1. Persistent Homology,[0],[0]
"Indeed, the stability theorem (Bauer & Lesnick, 2015; Chazal et al., 2009a; 2016; Cohen-Steiner et al., 2007) asserts that for any f, g : X → R, we have
d∞(Dg f, Dg g) ≤ ‖f",2.1. Persistent Homology,[0],[0]
"− g‖∞, (1)
See again Figure 1 for an illustration.
",2.1. Persistent Homology,[0],[0]
"In practice, PDs can be used as descriptors for data via the choice of appropriate filtering functions f , e.g. distance to the data in the ambient space, eccentricity, curvature, etc.",2.1. Persistent Homology,[0],[0]
"The main strengths of the obtained descriptors are: (a) to be provably stable as mentioned previously; (b) to be invariant under reparametrization of the data; and (c) to encode information about the topology of the data, which is complementary and of an essentially different nature compared to geometric or statistical quantities.",2.1. Persistent Homology,[0],[0]
"These properties have made persistence diagrams useful in a variety of contexts, including the ones mentioned in the introduction of the paper.",2.1. Persistent Homology,[0],[0]
"For further details on persistent homology and on applications of PDs, the interested reader can refer e.g. to (Oudot, 2015) and the references therein.",2.1. Persistent Homology,[0],[0]
Positive Definite Kernels.,2.2. Kernel Methods,[0],[0]
"Given a set X , a function k : X × X → R is called a positive definite kernel if for all integers n, for all families x1, ..., xn of points in X , the matrix [k(xi, xj)]i,j is itself positive semi-definite.",2.2. Kernel Methods,[0],[0]
For brevity we will refer to positive definite kernels as kernels in the rest of the paper.,2.2. Kernel Methods,[0],[0]
"It is known that kernels generalize scalar products, in the sense that, given a kernel k, there exists a Reproducing Kernel Hilbert Space (RKHS) Hk and a feature map φ : X → Hk such that k(x1, x2) = 〈φ(x1), φ(x2)〉Hk .",2.2. Kernel Methods,[0],[0]
"A kernel k also induces a distance dk on X that can be computed as the Hilbert norm of the difference between two embeddings:
d2k(x1, x2) def. = k(x1, x1) +",2.2. Kernel Methods,[0],[0]
"k(x2, x2)− 2 k(x1, x2).
",2.2. Kernel Methods,[0],[0]
"We will be particularly interested in this distance, since one of the goals we will aim for will be that of designing a kernel k for persistence diagrams such that dk has low distortion with respect to d1.
",2.2. Kernel Methods,[0],[0]
Negative Definite and RBF Kernels.,2.2. Kernel Methods,[0],[0]
A standard way to construct a kernel is to exponentiate the negative of a Euclidean distance.,2.2. Kernel Methods,[0],[0]
"Indeed, the Gaussian kernel for vectors with parameter σ",2.2. Kernel Methods,[0],[0]
"> 0 does follow that template approach: kσ(x, y) = exp ( −‖x−y‖ 2
2σ2
) .",2.2. Kernel Methods,[0],[0]
"An important theo-
rem of Berg et al. (1984) (Theorem 3.2.2, p.74) states that such an approach to build kernels, namely setting
kσ(x, y) def. =",2.2. Kernel Methods,[0],[0]
"exp ( −f(x, y)
2σ2
) ,
for an arbitrary function f can only yield a valid positive definite kernel for all σ > 0",2.2. Kernel Methods,[0],[0]
"if and only if f is a negative semi-definite function, namely that, for all integers n, ∀x1, ..., xn ∈ X , ∀a1, ..., an ∈ Rn such that ∑ i ai = 0,∑
i,j aiajf(xi, xj) ≤ 0.",2.2. Kernel Methods,[0],[0]
"Unfortunately, as observed in Appendix A of Reininghaus et al. (2014), d1 is not negative semi-definite (it only suffices to sample a family of point clouds to observe experimentally that more often than not the inequality above will be violated for a particular weight vector a).",2.2. Kernel Methods,[0],[0]
"In this article, we use an approximation of d1 with the Sliced Wasserstein distance, which is provably negative semi-definite, and we use it to define a RBF kernel that can be easily tuned thanks to its bandwidth parameter σ.
2.3.",2.2. Kernel Methods,[0],[0]
"Wasserstein distance for unnormalized measures on R
The Wasserstein distance (Villani, 2009, §6) is a distance between probability measures.",2.2. Kernel Methods,[0],[0]
"For reasons that will become clear in the next section, we will focus on a variant of that distance: the 1-Wasserstein distance for nonnegative, not necessarily normalized, measures on the real line (Santambrogio, 2015, §2).",2.2. Kernel Methods,[0],[0]
"Let µ and ν be two nonnegative mea-
sures on the real line such that |µ| = µ(R) and |ν| = ν(R) are equal to the same number r.",2.2. Kernel Methods,[0],[0]
"We define the three following objects:
W(µ, ν) = inf P∈Π(µ,ν)",2.2. Kernel Methods,[0],[0]
"∫∫ R×R |x− y|P (dx,dy) (2)
Qr(µ, ν) =",2.2. Kernel Methods,[0],[0]
"r ∫ R |M−1(x)−N−1(x)|dx (3)
L(µ, ν) = inf f∈1−Lipschitz ∫ R f(x)[µ(dx)− ν(dx)]",2.2. Kernel Methods,[0],[0]
"(4)
where Π(µ, ν) is the set of measures on R2 with marginals µ and ν, and M−1 and N−1 the generalized quantile functions of the probability measures µ/r and ν/r respectively.
",2.2. Kernel Methods,[0],[0]
Proposition 2.1.,2.2. Kernel Methods,[0],[0]
We haveW = Qr = L. Additionally (i),2.2. Kernel Methods,[0],[0]
"Qr is negative definite on the space of measures of mass r; (ii) for any three positive measures µ, ν, γ such that |µ| = |ν|, we have L(µ+ γ, ν + γ) = L(µ, ν).
",2.2. Kernel Methods,[0],[0]
"Equation (2) is the generic Kantorovich formulation of optimal transport, which is easily generalized to other cost functions and spaces, the variant being that we consider an unnormalized mass by reflecting it directly in the set",2.2. Kernel Methods,[0],[0]
Π.,2.2. Kernel Methods,[0],[0]
The equality between (2) and (3) is only valid for probability measures on the real line.,2.2. Kernel Methods,[0],[0]
"Because the cost function | · | is homogeneous, we see that the scaling factor r can be removed when considering the quantile function and multiplied back.",2.2. Kernel Methods,[0],[0]
"The equality between (2) and (4) is due to the well known Kantorovich duality for a distance cost (Villani, 2009, Particular case 5.4) which can also be trivially generalized to unnormalized measures, proving therefore the main statement of the proposition.",2.2. Kernel Methods,[0],[0]
"The definition of Qr shows that the Wasserstein distance is the l1 norm of
rM−1 − rN−1, and is therefore a negative definite kernel (as the l1 distance between two direct representations of µ and ν as functions rM−1 and rN−1), proving point (i).",2.2. Kernel Methods,[0],[0]
"The second statement is immediate.
",2.2. Kernel Methods,[0],[0]
We conclude with an important practical remark: for two unnormalized uniform empirical measures µ = ∑n i=1,2.2. Kernel Methods,[0],[0]
"δxi
and ν",2.2. Kernel Methods,[0],[0]
= ∑n i=1,2.2. Kernel Methods,[0],[0]
"δyi of the same size, with ordered x1 ≤
· · · ≤ xn and y1 ≤ · · · ≤",2.2. Kernel Methods,[0],[0]
"yn, one has: L(µ, ν) =∑n i=1 |xi−yi| = ‖X−Y ‖1, whereX = (x1, ..., xn) ∈",2.2. Kernel Methods,[0],[0]
"Rn and Y = (y1, ..., yn) ∈ Rn.",2.2. Kernel Methods,[0],[0]
"In this section we define a new kernel between PDs, called the Sliced Wasserstein (SW) kernel, based on the Sliced Wasserstein metric of Rabin et al. (2011).",3. The Sliced Wasserstein Kernel,[0],[0]
"The idea underlying this metric is to slice the plane with lines passing through the origin, to project the measures onto these lines whereW is computed, and to integrate those distances over all possible lines.",3. The Sliced Wasserstein Kernel,[0],[0]
Formally: Definition 3.1.,3. The Sliced Wasserstein Kernel,[0],[0]
"Given θ ∈ R2 with ‖θ‖2 = 1, let L(θ) denote the line {λ θ | λ ∈ R}, and let πθ : R2 → L(θ) be the orthogonal projection onto L(θ).",3. The Sliced Wasserstein Kernel,[0],[0]
"Let Dg1,Dg2 be two PDs, and let µθ1 := ∑ p∈Dg1 δπθ(p) and µ θ 1∆ :=∑
p∈Dg1 δπθ◦π∆(p), and similarly for µ θ 2, where π∆ is the orthogonal projection onto the diagonal.",3. The Sliced Wasserstein Kernel,[0],[0]
"Then, the Sliced Wasserstein distance is defined as:
SW(Dg1,Dg2) def. =
1
2π ∫ S1 W(µθ1 + µθ2∆, µθ2 + µθ1∆)dθ.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that, by symmetry, one can restrict on the half-circle [−π2 , π2 ] and normalize by π instead of 2π.",3. The Sliced Wasserstein Kernel,[0],[0]
"SinceQr is negative semi-definite, we can deduce that SW itself is negative semi-definite: Lemma 3.2.",3. The Sliced Wasserstein Kernel,[0],[0]
Let X be the set of bounded and finite PDs.,3. The Sliced Wasserstein Kernel,[0],[0]
"Then, SW is negative semi-definite on X .",3. The Sliced Wasserstein Kernel,[0],[0]
Proof.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let n ∈ N∗, a1, ..., an ∈ R such that ∑ i ai = 0 and
Dg1, ...,Dgn ∈ X .",3. The Sliced Wasserstein Kernel,[0],[0]
"Given 1 ≤ i ≤ n, we let µ̃θi := µθi +∑ q∈Dgk,k 6=i δπθ◦π∆(q), µ̃ θ ij∆ := ∑ p∈Dgk,k 6=i,j
δπθ◦π∆(p) and d = ∑ i |Dgi|.",3. The Sliced Wasserstein Kernel,[0],[0]
"Then:∑
i,j
aiajW(µθi + µθj∆, µθj + µθi∆)
",3. The Sliced Wasserstein Kernel,[0],[0]
"= ∑ i,j aiajL(µθi + µθj∆, µθj + µθi∆)
",3. The Sliced Wasserstein Kernel,[0],[0]
"= ∑ i,j aiajL(µθi + µθj∆",3. The Sliced Wasserstein Kernel,[0],[0]
"+ µθij∆, µθj + µθi∆ + µθij∆)
= ∑",3. The Sliced Wasserstein Kernel,[0],[0]
"i,j aiajL(µ̃θi , µ̃θj ) = ∑ i,j aiajQd(µ̃θi , µ̃θj ) ≤ 0
",3. The Sliced Wasserstein Kernel,[0],[0]
"The result follows by linearity of integration.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Hence, the theorem of Berg et al. (1984) allows us to define a valid kernel with:
kSW(Dg1,Dg2) def. = exp
( −SW(Dg1,Dg2)
2σ2
) .",3. The Sliced Wasserstein Kernel,[0],[0]
"(5)
Metric equivalence.",3. The Sliced Wasserstein Kernel,[0],[0]
"We now give the main theoretical result of this article, which states that SW is equivalent to d1.",3. The Sliced Wasserstein Kernel,[0],[0]
"This has to be compared with (Reininghaus et al., 2015) and (Kusano et al., 2016), which only prove stability and injectivity.",3. The Sliced Wasserstein Kernel,[0],[0]
"Our equivalence result states that the kSW, in addition to be stable and injective, also preserves the metric between PDs, which should intuitively lead to an improvement of the classification power.",3. The Sliced Wasserstein Kernel,[0],[0]
"This intuition is illustrated in Section 4 and Figure 4, where we show an improvement of classification accuracies on several benchmark applications.
",3. The Sliced Wasserstein Kernel,[0],[0]
Theorem 3.3.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let X be the set of bounded PDs with cardinalities bounded by N ∈ N∗. Let Dg1,Dg2 ∈ X .",3. The Sliced Wasserstein Kernel,[0],[0]
"Then, one has:
d1(Dg1,Dg2)
2M ≤ SW(Dg1,Dg2) ≤ 2
√ 2d1(Dg1,Dg2),
where M = 1 + 2N(2N − 1).
",3. The Sliced Wasserstein Kernel,[0],[0]
Proof.,3. The Sliced Wasserstein Kernel,[0],[0]
"Let sθ : Dg1 ∪π∆(Dg2)→ Dg2 ∪π∆(Dg1) be the one-to-one bijection between Dg1 ∪ π∆(Dg2) and Dg2 ∪ π∆(Dg1) induced by W(µθ1 + µθ2∆, µθ2 + µθ1∆), and let s be the one-to-one bijection between Dg1 ∪ π∆(Dg2) and Dg2 ∪ π∆(Dg1) induced by the partial bijection achieving d1(Dg1,Dg2).
",3. The Sliced Wasserstein Kernel,[0],[0]
Upper bound.,3. The Sliced Wasserstein Kernel,[0],[0]
Recall that ‖θ‖2 = 1.,3. The Sliced Wasserstein Kernel,[0],[0]
"We have: W(µθ1 + µθ2∆, µθ2 + µθ1∆) = ∑ |〈p− sθ(p), θ〉|
≤ ∑ |〈p− s(p), θ〉| ≤ √ 2 ∑ ‖p− s(p)‖∞ ≤ 2 √
2d1(Dg1,Dg2),
where the sum is taken over all p ∈ Dg1 ∪ π∆(Dg2).",3. The Sliced Wasserstein Kernel,[0],[0]
"The upper bound follows by linearity.
",3. The Sliced Wasserstein Kernel,[0],[0]
Lower bound.,3. The Sliced Wasserstein Kernel,[0],[0]
"The idea is to use the fact that sθ is a piecewise-constant function of θ, and that it has at most 2+2N(2N −1) critical values Θ0, ...,ΘM in [−π2 , π2 ].",3. The Sliced Wasserstein Kernel,[0],[0]
"Indeed, it suffices to look at all θ such that 〈p1−p2, θ〉 = 0 for some p1, p2 in Dg1 ∪ π∆(Dg2) or Dg2 ∪ π∆(Dg1).",3. The Sliced Wasserstein Kernel,[0],[0]
"Then:∫ Θi+1
Θi
∑ |〈p− sθ(p), θ〉|dθ
= ∑ ‖p− sΘi(p)‖2 ∫ Θi+1",3. The Sliced Wasserstein Kernel,[0],[0]
"Θi |cos(∠(p− sΘi(p), θ))|dθ
≥ ∑ ‖p− sΘi(p)‖2(Θi+1 −Θi)2/2π
≥ (Θi+1 −Θi)2d1(Dg1,Dg2)/2π,
where the sum is again taken over all p ∈ Dg1 ∪ π∆(Dg2), and where the inequality used to lower bound the integral of the cosine is obtained by concavity.",3. The Sliced Wasserstein Kernel,[0],[0]
"The lower bound follows then from the Cauchy-Schwarz inequality.
",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that the lower bound depends on the cardinalities of the PDs, and it becomes close to 0 if the PDs have a large number of points.",3. The Sliced Wasserstein Kernel,[0],[0]
"On the other hand, the upper bound is oblivious to the cardinality.",3. The Sliced Wasserstein Kernel,[0],[0]
"A corollary of Theorem 3.3 is that dkSW , the distance induced by kSW in its RKHS, is also equivalent to d1 in a broader sense: there exist continuous, positive and monotone functions g, h such that g(0) = h(0) = 0 and g ◦",3. The Sliced Wasserstein Kernel,[0],[0]
d1 ≤ dkSW ≤ h ◦,3. The Sliced Wasserstein Kernel,[0],[0]
d1.,3. The Sliced Wasserstein Kernel,[0],[0]
"When the condition on the cardinalities of PDs is relaxed, e.g. when we only assume the PDs to be finite and bounded, with no uniform bound, the feature map φSW associated to kSW remains continuous and injective w.r.t.",3. The Sliced Wasserstein Kernel,[0],[0]
d1.,3. The Sliced Wasserstein Kernel,[0],[0]
"This means that kSW can be turned into a universal kernel by considering exp(kSW) (cf Theorem 1 in (Kwitt et al., 2015)).",3. The Sliced Wasserstein Kernel,[0],[0]
"This can be useful in a variety of tasks, including tests on distributions of PDs.
",3. The Sliced Wasserstein Kernel,[0],[0]
Computation.,3. The Sliced Wasserstein Kernel,[0],[0]
"In practice, we propose to approximate kSW in O(N log(N)) time using Algorithm 1.",3. The Sliced Wasserstein Kernel,[0],[0]
"This algorithm first samples M directions in the half-circle S+1 ; it then computes, for each sample θi and for each PD Dg, the scalar products between the points of Dg and θi, to sort them next in a vector Vθi(Dg).",3. The Sliced Wasserstein Kernel,[0],[0]
"Finally, the `1-norm between the vectors is averaged over the sampled directions: SWM (Dg1,Dg2) = 1 M ∑M i=1 ‖Vθi(Dg1) − Vθi(Dg2)‖1.",3. The Sliced Wasserstein Kernel,[0],[0]
Note that one can easily adapt the proof of Lemma 3.2 to show that SWM is negative semi-definite by using the linearity of the sum.,3. The Sliced Wasserstein Kernel,[0],[0]
"Hence, this approximation remains a kernel.",3. The Sliced Wasserstein Kernel,[0],[0]
"If the two PDs have cardinalities bounded by N , then the running time of this procedure is O(MN log(N)).",3. The Sliced Wasserstein Kernel,[0],[0]
"This approximation of kSW is useful since, as shown in Section 4, we have observed empirically that just a few directions are sufficient to get good classification accuracies.",3. The Sliced Wasserstein Kernel,[0],[0]
"Note that the exact computation of kSW is also possible in O(N2log(N)) time using the algorithm described in (Carrière et al., 2017).",3. The Sliced Wasserstein Kernel,[0],[0]
"In this section, we compare kSW to kPSS and kPWG on several benchmark applications for which PDs have been proven useful.",4. Experiments,[0],[0]
We compare these kernels in terms of classification accuracies and computational cost.,4. Experiments,[0],[0]
"We review first our experimental setting, and then all our tasks.
",4. Experiments,[0],[0]
"Experimental setting All kernels are handled with the LIBSVM (Chang & Lin, 2011) implementation of C-SVM, and results are averaged over 10 runs on a 2.4GHz Intel Xeon E5530 Quad Core.",4. Experiments,[0],[0]
"The
Algorithm 1 Computation of SWM Input: Dg1 = {p11 ... p1N1}, Dg2 = {p21 ...",4. Experiments,[0],[0]
"p2N2},M .",4. Experiments,[0],[0]
Add π∆(Dg1) to Dg2 and vice-versa.,4. Experiments,[0],[0]
Let SWM = 0; θ = −π/2; s = π/M ; for i = 1 ...,4. Experiments,[0],[0]
"M do
Store the products 〈p1k, θ〉 in an array V1; Store the products 〈p2k, θ〉 in an array V2; Sort V1 and V2 in ascending order; SWM = SWM + s‖V1 − V2‖1; θ = θ + s;
end for Output: (1/π)SWM ;
",4. Experiments,[0],[0]
"cost factor C is cross-validated in the following grid: {0.001, 0.01, 0.1, 1, 10, 100, 1000}.",4. Experiments,[0],[0]
"Table 1 summarizes the number of labels, and the number of training and test instances for each task.",4. Experiments,[0],[0]
Figure 2 illustrate how we use PDs to represent complex data.,4. Experiments,[0],[0]
"We first describe the two baselines we considered, along with their parameterization, followed by our proposal.
PSS.",4. Experiments,[0],[0]
"The Persistence Scale Space kernel kPSS (Reininghaus et al., 2015) is defined as the scalar product of the two solutions of the heat diffusion equation with initial Dirac sources located at the PD points.",4. Experiments,[0],[0]
"It has the following closed form expression: kPSS(Dg1,Dg2) =
1 8πt ∑ p∈Dg1 ∑ q∈Dg2 exp ( −‖p−q‖ 2 8t )",4. Experiments,[0],[0]
"− exp ( −‖p−q̄‖ 2 8t ) , where q̄ = (y, x) is the symmetric of q = (x, y) along the diagonal.",4. Experiments,[0],[0]
"Since there is no clear heuristic on how to tune t, this parameter is chosen in the applications by ten-fold cross-validation with random 50%-50% training-test splits and with the following set of NPSS = 13 values: 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500 and 1000.
PWG.",4. Experiments,[0],[0]
"Let K, p, ρ be positive parameters.",4. Experiments,[0],[0]
Let kρ be the Gaussian kernel with parameter ρ and associated RKHS Hρ.,4. Experiments,[0],[0]
"Let Dg1,Dg2 be two PDs, and let µ1 := ∑ x∈Dg1 arctan(Kd∞(x,∆) p)kρ(·, x) ∈",4. Experiments,[0],[0]
Hρ be the kernel mean embedding of Dg1 weigthed by the diagonal distances.,4. Experiments,[0],[0]
"Let µ2 be defined similarly.
",4. Experiments,[0],[0]
Let τ > 0.,4. Experiments,[0],[0]
The Persistence Weighted Gaussian kernel kPWG,4. Experiments,[0],[0]
"(Kusano et al., 2016; 2017) is defined as kPWG(Dg1,Dg2) = exp ( −‖µ1−µ2‖Hρ2τ2 ) , i.e. the Gaussian kernel with parameter τ on Hρ.",4. Experiments,[0],[0]
"The authors in (Kusano et al., 2016) provide heuristics to compute K, ρ and τ and give a rule of thumb to tune p.",4. Experiments,[0],[0]
"Hence, in the applications we select p according to the rule of thumb, and we use ten-fold cross-validation with random 50%-50% training-test splits to chose K, ρ and τ .",4. Experiments,[0],[0]
"The ranges of possible values is obtained by multiplying the values computed with the heuristics with the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NPWG = 5× 5× 5 = 125 different sets of parameters.
",4. Experiments,[0],[0]
Parameters for kSW.,4. Experiments,[0],[0]
"The kernel we propose has only one parameter, the bandwidth σ in Eq.",4. Experiments,[0],[0]
"(5), which we choose using ten-fold cross-validation with random 50%- 50% training-test splits.",4. Experiments,[0],[0]
"The range of possible values is obtained by computing the squareroot of the median, the first and the last deciles of all SW(Dgi,Dgj) in the training set, then by multiplying these values by the following range of 5 factors: 0.01, 0.1, 1, 10 and 100, leading to NSW = 5× 3 = 15 possible values.
",4. Experiments,[0],[0]
Parameter Tuning.,4. Experiments,[0],[0]
"The bandwidth of kSW is, in practice, easier to tune than the parameters of its two competitors when using grid search.",4. Experiments,[0],[0]
"Indeed, as is the case for all infinitely divisible kernels, the Gram matrix does not need to be recomputed for each choice of σ, since it only suffices to compute all the Sliced Wasserstein distances between PDs in the training set once.",4. Experiments,[0],[0]
"On the contrary, neither kPSS nor kPWG share this property, and require recomputations for each hyperparameter choice.",4. Experiments,[0],[0]
Note however that this improvement may no longer hold if one uses other methods to tune parameters.,4. Experiments,[0],[0]
"For instance, using kPWG without cross-validation is possible with the heuristics given by the authors in (Kusano et al., 2016), and leads to smaller training times, but also to worse accuracies.",4. Experiments,[0],[0]
"Our first task, whose goal is to produce point classifiers for 3D shapes, follows that presented in (Carrière et al., 2015).
",4.1. 3D shape segmentation,[0],[0]
Data.,4.1. 3D shape segmentation,[0],[0]
"We use some categories of the mesh segmentation benchmark of Chen et al. (Chen et al., 2009), which contains 3D shapes classified in several categories (“airplane”, “human”, “ant”...).",4.1. 3D shape segmentation,[0],[0]
"For each category, our goal is to design a classifier that can assign, to each point in the shape, a
label that describes the relative location of that point in the shape.",4.1. 3D shape segmentation,[0],[0]
"For instance, possible labels are, for the human category, “head”, “torso”, “arm”...",4.1. 3D shape segmentation,[0],[0]
"To train classifiers, we compute a PD per point using the geodesic distance function to this point—see (Carrière et al., 2015) for details.",4.1. 3D shape segmentation,[0],[0]
"We use 1-dimensional persistent homology (0-dimensional would not be informative since the shapes are connected, leading to solely one point with coordinates (0,+∞) per PD).",4.1. 3D shape segmentation,[0],[0]
"For each category, the training set contains one hundredth of the points of the first five 3D shapes, and the test set contains one hundredth of the points of the remaining shapes in that category.",4.1. 3D shape segmentation,[0],[0]
Points in training and test sets are evenly sampled.,4.1. 3D shape segmentation,[0],[0]
See Figure 2.,4.1. 3D shape segmentation,[0],[0]
"Here, we focus on comparison between PDs, and not on achieving state-of-the-art results.",4.1. 3D shape segmentation,[0],[0]
"It has been proven that PDs bring complementary information to classical descriptors in this task—see (Carrière et al., 2015), hence reinforcing their discriminative power with appropriate kernels is of great interest.",4.1. 3D shape segmentation,[0],[0]
"Finally, since data points are in R3, we set the p parameter of kPWG to 5.
Results.",4.1. 3D shape segmentation,[0],[0]
Classification accuracies are given in Table 2.,4.1. 3D shape segmentation,[0],[0]
"For most categories, kSW outperforms competing kernels by a significant margin.",4.1. 3D shape segmentation,[0],[0]
The variance of the results over the run is also less than that of its competitors.,4.1. 3D shape segmentation,[0],[0]
"However, training times are not better in general.",4.1. 3D shape segmentation,[0],[0]
"Hence, we also provide the results for an approximation of kSW with 10 directions.",4.1. 3D shape segmentation,[0],[0]
"As one can see from Table 2 and from Figure 3, this approximation leaves the accuracies almost unchanged, while the training times become comparable with the ones of the
other competitors.",4.1. 3D shape segmentation,[0],[0]
"Moreover, according to Figure 3, using even less directions would slightly decrease the accuracies, but still outperform the competitors performances, while decreasing even more the training times.",4.1. 3D shape segmentation,[0],[0]
"In our second experiment, we use synthetized data.",4.2. Orbit recognition,[0],[0]
"The goal is to retrieve parameters of dynamical system orbits, following an experiment proposed in (Adams et al., 2017).
",4.2. Orbit recognition,[0],[0]
Data.,4.2. Orbit recognition,[0],[0]
"We study the linked twist map, a discrete dynamical system modeling fluid flow.",4.2. Orbit recognition,[0],[0]
"It was used in (Hertzsch et al., 2007) to model flows in DNA microarrays.",4.2. Orbit recognition,[0],[0]
"Its orbits can be computed given a parameter r > 0 and initial positions (x0, y0) ∈",4.2. Orbit recognition,[0],[0]
"[0, 1]× [0, 1] as follows:
{ xn+1 =",4.2. Orbit recognition,[0],[0]
xn,4.2. Orbit recognition,[0],[0]
"+ ryn(1− yn) mod 1 yn+1 = yn + rxn+1(1− xn+1) mod 1
",4.2. Orbit recognition,[0],[0]
"Depending on the values of r, the orbits may exhibit very different behaviors.",4.2. Orbit recognition,[0],[0]
"For instance, as one can see in Figure 2, when r is 2, there seems to be no interesting topological features in the orbit, while voids form when r is 1.",4.2. Orbit recognition,[0],[0]
"Following (Adams et al., 2017), we use 5 different parameters r = 2.5, 3.5, 4, 4.1, 4.3, that act as labels.",4.2. Orbit recognition,[0],[0]
"For each parameter, we generate 100 orbits with 1000 points and random initial positions.",4.2. Orbit recognition,[0],[0]
"We then compute the PDs of the distance functions to the point clouds with the GUDHI
library (The GUDHI Project, 2015) and we use them (in all homological dimensions) to produce an orbit classifier that predicts the parameter values, by training over a 70%-30% training-test split of the data.",4.2. Orbit recognition,[0],[0]
"Since data points are in R2, we set the p parameter of kPWG to 4.
Results.",4.2. Orbit recognition,[0],[0]
"Since the PDs contain thousands of points, we use kernel approximations to speed up the computation of the Gram matrices.",4.2. Orbit recognition,[0],[0]
"In order for the approximation error to be bounded by 10−3, we use an approximation of kSW with 6 directions (as one can see from Figure 3, this has a small impact on the accuracy), we approximate kPWG with 1000 random Fourier features (Rahimi & Recht, 2008), and we approximate kPSS using Fast Gauss Transform (Morariu et al., 2009) with a normalized error of 10−10.",4.2. Orbit recognition,[0],[0]
One can see from Table 2 that the accuracy is increased a lot with kSW.,4.2. Orbit recognition,[0],[0]
"Concerning training times, there is also a large improvement since we tune the parameters with grid search.",4.2. Orbit recognition,[0],[0]
"Indeed, each Gram matrix needs not be recomputed for each parameter when using kSW.",4.2. Orbit recognition,[0],[0]
"Our last experiment is inspired from (Reininghaus et al., 2015) and (Li et al., 2014).",4.3. Texture classification,[0],[0]
"We use the OUTEX00000 data base (Ojala et al., 2002) for texture classification.
Data.",4.3. Texture classification,[0],[0]
"PDs are obtained for each texture image by computing first the sign component of CLBP descriptors (Guo et al., 2010) with radius R = 1 and P = 8 neighbors for each image, and then compute the persistent homology of this descriptor using the GUDHI library (The GUDHI Project, 2015).",4.3. Texture classification,[0],[0]
See Figure 2.,4.3. Texture classification,[0],[0]
"Note that, contrary to the experiment of (Reininghaus et al., 2015), we do not downsample the images to 32× 32 images, but keep the original 128 × 128 images.",4.3. Texture classification,[0],[0]
"Following (Reininghaus et al., 2015), we restrict the focus to 0-dimensional persistent homology.",4.3. Texture classification,[0],[0]
We also use the first 50%-50% training-test split given in the database to produce classifiers.,4.3. Texture classification,[0],[0]
"Since data points are in R2, we set the p parameter of kPWG to 4.
",4.3. Texture classification,[0],[0]
Results We use the same approximation procedure as in Section 4.2.,4.3. Texture classification,[0],[0]
"According to Figure 3, even though the approximation of SW is rough, this has again a small impact on the accuracy, while reducing the training time by a significant margin.",4.3. Texture classification,[0],[0]
"As one can see from Table 2, using kPSS leads to almost state-of-the-art results (Ojala et al., 2002; Guo et al., 2010), closely followed by the accuracies of kSW and kPWG.",4.3. Texture classification,[0],[0]
"The best timing is given by kSW, again because we use grid search.",4.3. Texture classification,[0],[0]
"Hence, kSW almost achieves the best result, and its training time is better than the ones of its competitors, due to the grid search parameter tuning.
",4.3. Texture classification,[0],[0]
Metric Distortion.,4.3. Texture classification,[0],[0]
"To illustrate the equivalence theorem, we also show in Figure 4 a scatter plot where each point
represents the comparison of two PDs taken from the Airplane segmentation data set.",4.3. Texture classification,[0],[0]
Similar plots can be obtained with the other datasets considered here.,4.3. Texture classification,[0],[0]
"For all points, the x-axis quantifies the first diagram distance d1 for that pair, while the y-axis is the logarithm of the RKHS distance induced by either kSW, kPSS, kPWG or a Gaussian kernel directly applied to d1, to obtain comparable quantities.",4.3. Texture classification,[0],[0]
We use the parameters given by the cross-validation procedure described above.,4.3. Texture classification,[0],[0]
"One can see that the distances induced by kSW are less spread than the others, suggesting that the metric induced by kSW is more discriminative.",4.3. Texture classification,[0],[0]
"Moreover the distances given by kSW and the Gaussian kernel on d1 exhibit the same behavior, suggesting that kSW is the best natural equivalent of a Gaussian kernel for PDs.",4.3. Texture classification,[0],[0]
"In this article, we introduce the Sliced Wasserstein kernel, a new kernel for PDs that is provably equivalent to the first diagram distance between PDs.",5. Conclusion,[0],[0]
"We provide fast algorithms to approximate it, and show on several datasets substantial improvements in accuracy and training times (when tuning parameters is done with grid search) over competing kernels.",5. Conclusion,[0],[0]
"A particularly appealing property of that kernel is that it is infinitely divisible, substantially facilitating the tuning of parameters through cross validation.
",5. Conclusion,[0],[0]
Acknowledgements.,5. Conclusion,[0],[0]
We thank the anonymous referees for their insightful comments.,5. Conclusion,[0],[0]
SO was supported by ERC grant Gudhi and by ANR project TopData.,5. Conclusion,[0],[0]
MC was supported by a chaire de l’IDEX Paris Saclay.,5. Conclusion,[0],[0]
"Persistence diagrams (PDs) play a key role in topological data analysis (TDA), in which they are routinely used to describe topological properties of complicated shapes.",abstractText,[0],[0]
PDs enjoy strong stability properties and have proven their utility in various learning contexts.,abstractText,[0],[0]
"They do not, however, live in a space naturally endowed with a Hilbert structure and are usually compared with non-Hilbertian distances, such as the bottleneck distance.",abstractText,[0],[0]
"To incorporate PDs in a convex learning pipeline, several kernels have been proposed with a strong emphasis on the stability of the resulting RKHS distance w.r.t.",abstractText,[0],[0]
perturbations of the PDs.,abstractText,[0],[0]
"In this article, we use the Sliced Wasserstein approximation of the Wasserstein distance to define a new kernel for PDs, which is not only provably stable but also discriminative (with a bound depending on the number of points in the PDs)",abstractText,[0],[0]
w.r.t.,abstractText,[0],[0]
the first diagram distance between PDs.,abstractText,[0],[0]
"We also demonstrate its practicality, by developing an approximation technique to reduce kernel computation time, and show that our proposal compares favorably to existing kernels for PDs on several benchmarks.",abstractText,[0],[0]
Sliced Wasserstein Kernel for Persistence Diagrams,title,[0],[0]
Establishing maps (e.g. pointwise correspondences) across object collections is a fundamental problem spanning many scientific domains.,1. Introduction,[0],[0]
"High-quality maps facilitating information propagation and transformation are key to applications ranging from 3D reconstruction with partial scans (Huber & Hebert, 2001), data-driven geometry completion and reconstruction (Pauly et al., 2005), texture transfer (Schreiner et al., 2004; Kraevoy & Sheffer, 2004), to comparative biology (Boyer et al., 2011; Gao et al., 2017), joint dataanalysis (Huang et al., 2011; Kim et al., 2012; Wang et al., 2013; 2014; Huang et al., 2014), and data exploration and organization (Kim et al., 2012; Huang et al., 2014).
",1. Introduction,[0],[0]
High quality object maps are generally difficult to compute.,1. Introduction,[0],[0]
"Prior work on map computation focused on optimizing maps between pairs of objects; see (van Kaick et al., 2010) for
1Department of Computer Science, The University of Texas at Austin 2Department of Statistics, The University of Chicago 3Institute for Interdisciplinary Information Sciences, Tsinghua Univesity.",1. Introduction,[0],[0]
"Correspondence to: Chandrajit Bajaj <bajaj@cs.utexas.edu>, Qixing Huang",1. Introduction,[0],[0]
"<huangqx@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"a standard survey and (Kim et al., 2011; Mandad et al., 2017) for some recent advances.",1. Introduction,[0],[0]
"Despite the significant progress, state-of-the-art techniques tends to hit a barrier on the quality of maps that are computed in a pairwise manner.",1. Introduction,[0],[0]
"Building upon the availability of big-data, a recent line of research (Kim et al., 2012; Nguyen et al., 2011; Ovsjanikov et al., 2012; Huang et al., 2012; Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Zhou et al., 2015a;b; Shen et al., 2016; Leonardos et al., 2017; Huang et al., 2017) considered computing many pairwise maps jointly among a collection of objects.",1. Introduction,[0],[0]
"The promise of these approaches hinges upon the observation that one way to obtain a high quality pairwise map between dissimilar objects is to choose a path connecting these objects but consisting of consecutive similar shapes: maps between similar objects are typically of higher quality, and so is the resulted composed map.",1. Introduction,[0],[0]
"From a regularization perspective, joint map computation leverages the generic consistency of a network of maps among multiple objects, in which composition of maps along cycles are expected to be close to the identity map.
",1. Introduction,[0],[0]
"However, the performance of these data-driven approaches relies predominantly on the homogeneity of the object collection, i.e. the input objects fall into the same category or sub-category (e.g. Chairs, Cars, and Human models).",1. Introduction,[0],[0]
"In the presence of heterogeneous data, where the input objects fall into multiple underlying categories, applying existing data-driven approaches without the category label information tend to produce unsatisfactory results.",1. Introduction,[0],[0]
"In this setting, even though existing methods are able to suppress the noise in intra-cluster maps within a single cluster, jointly computed maps for the entire object collection leads are often significantly worse.",1. Introduction,[0],[0]
One explanation is that high fraction of incorrect inter-cluster maps tends to “contaminate” the regularization effect of intra-cluster maps.,1. Introduction,[0],[0]
A natural resolution is to employ a two-stage cascadic strategy that identifies the underlying clusters before computing the intra- and inter-cluster maps.,1. Introduction,[0],[0]
"Unfortunately, such clustering requires accurate quantification of the object similarities, which is a difficult problem in its own right.",1. Introduction,[0],[0]
"Meanwhile, the error produced in the clustering stage is unlikely remedied by the consistency-based regularization.
",1. Introduction,[0],[0]
"In this paper, we propose to solve the mapping and clustering problems simultaneously.",1. Introduction,[0],[0]
"Instead of explicitly relying on certain pairwise similarity and/or map distortion
scores, we identify the underlying clusters based on the consistency of intra- and inter-cluster maps, inspired by the observation that maps tend to be more consistent along cycles within a cluster than across clusters.",1. Introduction,[0],[0]
"This discrepancy has been observed in many different contexts, and appears to be a consequence of the energy landscape of almost all optimization-based pairwise matching algorithms.",1. Introduction,[0],[0]
"The matching energy functional between objects in the same underlying cluster tends to have simple energy landscapes with easily identifiable global optimums, resulting in fairly cycle-consistent intra-cluster maps; in contrast, the highly non-convex energy landscape for dissimilar objects from different clusters leads to more “random” maps due to random initialization and/or multiple strong local minimums, for which cycle-consistency is much less often observed.",1. Introduction,[0],[0]
This map consistency argument is the foundation of our simultaneous mapping and clustering (SMAC) algorithm.,1. Introduction,[0],[0]
We validate the map consistency argument through a motivating example (see Figure 1) on a real dataset from SHREC07,1.1. Motivating Example,[0],[0]
"Watertight benchmark (Giorgi et al., 2007).",1.1. Motivating Example,[0],[0]
"This dataset consists of 38 shapes: the first 18 are Human models and the remaining 20 are Fourleg models (e.g., Horses and Dogs).",1.1. Motivating Example,[0],[0]
"Each shape is represented as a discrete metric space with 1024 sample points generated from farthest-point sampling (Eldar et al., 1997).",1.1. Motivating Example,[0],[0]
"We compute pairwise blended intrinsic maps (Kim et al., 2011) for all objects in this collection and use these maps to compute two similarity scores for each object pair: a map distortion score that measures the squared sum of geodesic distortions across all pointpairs (c.f. (Bronstein et al., 2006)), and a cycle-consistency score that is the median value (among other options) of the distortion scores of all 3-cycles to which the pair belongs, where the distortion of a 3-cycle is defined as the squared sum of the geodesic distances between each point and its image propagated along the 3-cycle.
",1.1. Motivating Example,[0],[0]
Figure 1 illustrates the distributions of the map distortion scores (Left) and the cycle-consistency scores (Right) on the 38 models.,1.1. Motivating Example,[0],[0]
"The cycle-consistency scores clearly reveal the
underlying cluster structure and in fact better separates the two clusters of models (Human vs. Fourleg) than the mapdistortion scores (intra-cluster blocks in the right figure are darker in blue).",1.1. Motivating Example,[0],[0]
"The superior cluster separation is verified by comparing the results of spectral clustering (Ng et al., 2002; Lei & Rinaldo, 2015) using the two similarity scores: spectral clustering based on the cycle-consistency scores recovers the two underlying clusters perfectly, whereas the same procedure using the map distortion scores incorrectly puts two Fourleg models in the cluster of Human models.",1.1. Motivating Example,[0],[0]
This motivating example illustrates the effectiveness and superiority of the map consistency score as a quantification of object similarity.,1.1. Motivating Example,[0],[0]
"Motivated by the example above, we propose an algorithm for simultaneous mapping and clustering (SMAC).",1.2. Approach Overview,[0],[0]
"Our SMAC algorithm takes as input (i) an object collection that falls into multiple clusters, and (ii) some noisy maps precomputed between object pairs, and outputs the underlying clusters together with improved maps between all pairs of objects.",1.2. Approach Overview,[0],[0]
"Our SMAC algorithm builds upon the equivalence between map consistency and the low-rank property of a data matrix with consistent maps in its blocks (c.f. (Huang & Guibas, 2013)).",1.2. Approach Overview,[0],[0]
"We show that this low-rank property still holds in the setting of multiple disjoint collections of consistent intra-cluster maps, though the rank is expected to be higher due to multiple clusters.",1.2. Approach Overview,[0],[0]
"Based on this observation, the first step of our approach simultaneously recovers the underlying clusters and intra-cluster maps by spectral decomposition.",1.2. Approach Overview,[0],[0]
We show that properly “rounding off” the leading eigenvectors recovers the ground-truth clusters and intracluster maps in a single pass.,1.2. Approach Overview,[0],[0]
We then construct inter-cluster maps from the recovered intra-cluster maps.,1.2. Approach Overview,[0],[0]
"Our theoretical analysis establishes sharp exact recovery conditions for both steps under a fairly general noise model, using some novel tight L∞-stability bounds for eigen-decompositions.",1.2. Approach Overview,[0],[0]
"Joint object matching, i.e., simultaneously estimating maps among a collection of objects, is an emerging field across many scientific problems.",1.3. Related Works,[0],[0]
"Earlier works use combinatorial optimizations (Nguyen et al., 2011; Kim et al., 2012; Huang et al., 2012).",1.3. Related Works,[0],[0]
"More recent works (Huang & Guibas, 2013; Huang et al., 2014; Chen et al., 2014; Wang & Singer, 2013) rely on convex or non-convex optimization techniques.",1.3. Related Works,[0],[0]
"However, all these methods assume that the underlying object collection is homogeneous (all objects belong to a single category).",1.3. Related Works,[0],[0]
"For a heterogeneous object collection where the objects fall into multiple distinctive clusters, existing methods usually rarely succeed in producing both high-quality intra- and inter-cluster maps.
",1.3. Related Works,[0],[0]
Clustering and in particular spectral clustering is another well-studied topic.,1.3. Related Works,[0],[0]
"We refer to (Lei & Rinaldo, 2015; Rohe et al., 2011) for some recent advances and to (Filippone et al., 2008; Luxburg, 2007; Fortunato, 2010) for surveys.",1.3. Related Works,[0],[0]
"Our approach falls into the general category of graph-based clustering, but the pairwise information we utilize is of “functional” rather than “scalar” nature.",1.3. Related Works,[0],[0]
"Instead of the more common approach that derives affinity scores from the pairwise maps for clustering, our SMAC algorithm discovers the cluster structure based purely on the consistency of pairwise maps and demonstrates improved empirical performance.",1.3. Related Works,[0],[0]
"This strategy is reminiscent of heterogeneous multireference alignment (Boumal et al., 2017) and simultaneous alignment and classification (Lederman & Singer, 2016) for synchronization problems in Cryo-Electron Microscopy; in this context, our residue-based clustering strategy is closest in nature to learning group actions (Gao et al., 2016).
",1.3. Related Works,[0],[0]
Our approach relies on tight L∞-type bounds on leading eigenvectors of perturbed matrices.,1.3. Related Works,[0],[0]
"Though the stability of eigenvalues and eigenspaces are well-studied, element-wise eigenvector stability appears to be a much harder problem; see recent survey (O’Rourke et al., 2016).",1.3. Related Works,[0],[0]
We introduce new stability bounds to tackle this technical difficulty.,1.3. Related Works,[0],[0]
"We use lower bold letters a, b, c,u,v,w, · · · to denote vectors, and upper letters A,B,C, · · · for matrices.",1.4. Mathematical Notation,[0],[0]
"For a block matrix X ∈ Rn1m1×n2m2 , we use Xij ∈ Rm1×m2 to denote its ij-th block; the ij-th element of a matrix A is denoted as aij .",1.4. Mathematical Notation,[0],[0]
With ⊗ we denote the Kronecker product.,1.4. Mathematical Notation,[0],[0]
"For a symmetric matrix A ∈ Rn×n, we always sort the eigenvalues in non-increasing order, i.e., λ1(A) ≥ · · · ≥ λn(A).",1.4. Mathematical Notation,[0],[0]
"Matrix norms ‖ · ‖F , ‖ · ‖1, ‖ · ‖2, and ‖ · ‖∞ will be used for a matrix A ∈ Rn1×n2 , of which ‖ · ‖2 = σmax(A) is the maximum singular value of A, and the spectral norm ‖ · ‖2 is sometimes simplified as ‖ · ‖.",1.4. Mathematical Notation,[0],[0]
We denote Pm for the set of permutation matrices of dimension m×m.,1.4. Mathematical Notation,[0],[0]
"For simplicity, we focus on describing and analyzing our algorithm under the setting where pair-wise maps are given by permutations.",2. Algorithm,[0],[0]
"In Section 4, we show how to modify the algorithm for partially similar objects.
",2. Algorithm,[0],[0]
We first describe the problem setup.,2. Algorithm,[0],[0]
"Consider n objects S = {S1, · · · , Sn} each represented by m points.",2. Algorithm,[0],[0]
"With G = (S, E) we denote an observation graph among S. An initial map X inij ∈",2. Algorithm,[0],[0]
Pm is pre-computed on each edge (,2. Algorithm,[0],[0]
"i, j) ∈ E using an off-the-shelf pairwise object matching algorithm from Si to Sj .",2. Algorithm,[0],[0]
"We also assume the objects in S are partitioned into k ≥ 2 clusters, but k is unknown.",2. Algorithm,[0],[0]
"Our goal is to identify the underlying clusters, and in the mean-
Algorithm 1 PermSMAC: Simultaneously mapping and clustering Input: Observation graph G = (S, E) and initial pairwise
maps X inij , (i, j) ∈ E Output:",2. Algorithm,[0],[0]
"Underlying clusters S = c1 ∪ · · · ∪ ck and opti-
mized pairwise maps Xij , 1 ≤",2. Algorithm,[0],[0]
"i, j ≤ n 1: {Step 1} Simultaneously compute the intra-cluster
maps and extract the underlying clusters: 2: {Step 1.1} Form data matrix based on (1).",2. Algorithm,[0],[0]
"3: {Step 1.2} Compute the critical value r =
argmax 2≤i≤nm
λi−λi+1",2. Algorithm,[0],[0]
"λi+λi+1 .
",2. Algorithm,[0],[0]
4: {Step 1.3} LetU ∈ Rnm×r store the leading r eigenvectors of X .,2. Algorithm,[0],[0]
"Compute pair-wise maps X?ij by solving (2) 5: {Step 1.4} Use fij(X?ij) as the affinity score and apply single-linkage clustering to obtain the underlying clusters 6: {Step 2} compute the inter-cluster maps by solving (6)
while improve all pairwise maps between objects in S. As a basis for identifying the underlying clusters, we assume that the intra-cluster maps are more accurate (in terms of cycle-consistency) than inter-cluster maps.
",2. Algorithm,[0],[0]
Our algorithm proceeds in two major steps.,2. Algorithm,[0],[0]
The first step simultaneously extracts the underlying clusters and computes intra-cluster maps.,2. Algorithm,[0],[0]
The second step then computes inter-cluster maps.,2. Algorithm,[0],[0]
Now we introduce these two steps in details.,2. Algorithm,[0],[0]
Algorithm 1 provides the pseudo code.,2. Algorithm,[0],[0]
"Motivated from prior works for map synchronization (Pachauri et al., 2013; Shen et al., 2016), we use a block data matrix X ∈ Rnm×nm to encode the input maps:
Xij =
{ (X inij − 1m11
T ) (i, j) ∈ E 0",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"otherwise
(1)
Our approach is motivated by the empirical observation that the leading eigen-vectors of X reveal the underlying clusters and simultaneously denoise intra-cluster maps if intra-cluster maps are more accurate than inter-cluster maps.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"We provide the algorithmic details below; an analysis is provided in Section 3.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Given the data matrix, we first estimates the number of stable eigen-vectors as
r =",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
argmax m≤i≤nm λi,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− λi+1 |λi|+ |λi+1| .
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Here we search within the range of [m,nm], as we expect multiple underlying clusters.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Let U ∈ Rnm×r store the
top r eigen-vectors of X , and divide U into n matrices U1, . . .",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
", Un of shape m×r such that U = (UT1 , · · · , UTn )T .",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"We then compute the estimated map X?ij along each edge (i, j) ∈ E by solving a linear assignment problem, i.e.,
X?ij = min X∈Pm fij(X), fij(X) := ‖X ·",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Ui − Uj‖2F (2)
for all 1 ≤",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"i, j ≤",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
n. Note that (2) admits an exact solution via linear assignment.,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"In fact,
X?ij = argmin X∈Pm ‖XUi",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− Uj‖2F (3)
= argmin X∈Pm (‖Ui‖2F + ‖Uj‖2F",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"− 2〈XUi, Uj〉) (4)
= argmax X∈Pm",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"〈X,UjUTi 〉.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"(5)
Intuitively, UjUTi provides an approximation of the underlying map Xij , and the linear assignment procedure projects this approximation onto the space of permutations.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"For clustering, we treat the residual score fij(X?ij), 1 ≤ i, j ≤ n",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"as the distance measure between Si and Sj , and apply single-linkage clustering (Gower & Ross, 1969) to obtain the underlying clusters.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
We set k =,2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"[ rm−1 ] as the number of desired clusters.
",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"Empirically, we found that when the input inter-cluster maps are inaccurate, the quality of estimated inter-cluster maps appear to be much more noisy than estimated intra-cluster maps.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
"This motivates us to re-estimate inter-cluster maps as a second step, described in Section 2.2 below.",2.1. Step I: Extract underlying clusters and compute intra-cluster maps.,[0],[0]
We estimate the inter-cluster maps between each pair of clusters independently.,2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"Specifically, consider two clusters cs and ct .",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
Let Sis ∈ cs and Sit ∈,2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"ct be a pair of objects selected from each cluster, respectively.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"We optimize the inter-cluster map X interst , represented as a pairwise object map Xisit , by solving the following linear assignment:
X interst = argmin X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E ‖X −XjitX inij Xiis‖1
= argmax X∈Pm ∑ i∈cs,j∈ct,(i,j)∈E 〈X,XjitX inij Xiis〉.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"(6)
Note that it is possible to jointly optimize X interst among all pairs of clusters.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
"However, since the number of clusters is usually significantly smaller than the size of each cluster, we found the gain of doing so is insignificant.",2.2. Step II: Estimate inter-cluster maps.,[0],[0]
We first describe our noise model in Section 3.1.,3. Analysis,[0],[0]
"We then analyze our method under this model and present the exact
recovery conditions for both underlying clusters and the pairwise maps in Section 3.2.",3. Analysis,[0],[0]
Our analysis is based on a set of new stability bounds of eigen-decompositions.,3. Analysis,[0],[0]
The proofs of all Lemma’s and Theorem’s with technical details can be referred to in the supplemental material.,3. Analysis,[0],[0]
"We consider two models, one for the pairwise map and the other one for the observation graph and the underlying cluster structure.
",3.1. Model for Analysis,[0],[0]
Map model.,3.1. Model for Analysis,[0],[0]
"We generalize the map model described in (Shen et al., 2016) to multiple clusters: Suppose there are k underlying clusters.",3.1. Model for Analysis,[0],[0]
"With cs, 1 ≤ s ≤ k",3.1. Model for Analysis,[0],[0]
we denote the vertex indices of the s-th cluster.,3.1. Model for Analysis,[0],[0]
"In other words, we have {1, · · · , n} = c1 ∪ c2 ∪ · · · ∪ ck.",3.1. Model for Analysis,[0],[0]
"Given an observation graph, the input pairwise maps are independent, and they follow
X inij =",3.1. Model for Analysis,[0],[0]
"{ Im with probability ηij UPm with probability 1− ηij
where UPm denotes a random permutation matrix satisfying
E[UPm ] = 1
m 11T .",3.1. Model for Analysis,[0],[0]
"(7)
ηij depends on the edge type: ηij = p if (i, j) is an intra-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s≤kcs × cs), and ηij = q if (i, j) is an inter-cluster edge, i.e., (i, j) ∈ E ∩ (∪1≤s6=t≤kcs × ct).",3.1. Model for Analysis,[0],[0]
We assume p > q. Remark 1.,3.1. Model for Analysis,[0],[0]
"Note that one can also generalize the model by assuming there exist underlying permutations Pi, 1 ≤ i ≤ n, so that the ground-truth map Xij = PjPTi .",3.1. Model for Analysis,[0],[0]
"Nevertheless, it turns out that the two models are identical (Shen et al., 2016) .",3.1. Model for Analysis,[0],[0]
"Hence we adopt the simpler form for convenience.
",3.1. Model for Analysis,[0],[0]
Model for the observation graph and clusters.,3.1. Model for Analysis,[0],[0]
"To obtain concise and interpretable exact recovery conditions, we are particularly interested in analyzing our algorithm when the observation graph and underlying clusters are generated from the following established model: assume n = n0k, and the size of each underlying cluster |cs| = n0, 1 ≤ s ≤ k; the observation graph is generated from the Erdős-Rényi G(n, t) with edge selection probability t.",3.1. Model for Analysis,[0],[0]
"However, the stability bounds we introduce in the supplemental material can be used for analyzing more general noisy models, e.g., sizes of the underlying clusters are different.",3.1. Model for Analysis,[0],[0]
We begin by analyzing the leading eigenvalues and eigenvectors of E[X] to gain insights on the necessary conditions for map and cluster recovery.,3.2. Map and Cluster Recovery,[0],[0]
"To make our discussion more general, we first assume the underlying cluster and the observation graph are fixed.",3.2. Map and Cluster Recovery,[0],[0]
"Consider then the following (p, q)-
reweighted normalized adjacency matrix A(p, q) ∈",3.2. Map and Cluster Recovery,[0],[0]
"Rn×n:
(A(p, q))ij =  ",3.2. Map and Cluster Recovery,[0],[0]
"p (i, j) ∈ E ∩ ∪1≤s≤m(cs × cs),q",3.2. Map and Cluster Recovery,[0],[0]
"(i, j) ∈ E ∩ ∪1≤s 6=t≤m(cs × ct). 0",3.2. Map and Cluster Recovery,[0],[0]
"otherwise
(8) It is clear that
E[X] = A(p, q)⊗",3.2. Map and Cluster Recovery,[0],[0]
"(Im − 1
m 11T )
and thus the non-zero eigenvalues of E[X] are non-zero eigenvalues ofA(p, q) with multiplicitym−1.",3.2. Map and Cluster Recovery,[0],[0]
"Furthermore, let ( 1√
m 1, Hm) be an orthonormal basis for Rm.",3.2. Map and Cluster Recovery,[0],[0]
"Then
the leading k(m − 1) eigenvectors of E[X] are given by Sk ⊗Hm.",3.2. Map and Cluster Recovery,[0],[0]
"This leads two conditions on the eigenvalues and eigenvectors of A(p, q) for map and cluster recovery:
• Eigenvalue separation.",3.2. Map and Cluster Recovery,[0],[0]
"Since our method leverages the largest eigengap, we assume that λk(A(p, q))",3.2. Map and Cluster Recovery,[0],[0]
"− λk+1(A(p, q)) has the largest eigengap.",3.2. Map and Cluster Recovery,[0],[0]
"Define
γ =
max 1≤i6=k≤n−1
λi(A(p, q))− λi+1(A(p, q))
λk(A(p, q))− λk+1(A(p, q)) ,
Then a necessary condition for map recovery is γ < 1.
",3.2. Map and Cluster Recovery,[0],[0]
• Eigenvector separation.,3.2. Map and Cluster Recovery,[0],[0]
We further assume that the underlying clusters can be recovered by reading off the rows of Sk.,3.2. Map and Cluster Recovery,[0],[0]
"Formally, consider rows of Sk as coordinates of the corresponding objects, and define
dintra = max 1≤s≤k max i,j∈cs
‖(ei − ej)TSk‖, (9)
dinter = min 1≤s<t≤k min i∈cs,j∈ct
‖(ei − ej)TSk‖. (10)
dintra and dinter essentially measure the maximum distance within each cluster and the minimum distance between different clusters, respectively.",3.2. Map and Cluster Recovery,[0],[0]
"Thus, a necessary condition for cluster recovery is that dintra < µ · dinter for some small constant µ.
Under these two conditions, it is easy to see that when X ≈ E[X], we have Ui ≈ (eTi Sk) ⊗ Hm and UjUTi",3.2. Map and Cluster Recovery,[0],[0]
"≈ (eTj SkS T k ei)(Im− 1m11
T ).",3.2. Map and Cluster Recovery,[0],[0]
"It follows that both the underlying clusters and intra-cluster maps can be exactly recovered.
",3.2. Map and Cluster Recovery,[0],[0]
These two separation conditions are quite general.,3.2. Map and Cluster Recovery,[0],[0]
"In fact, they hold for the noisy model described in Section 3.1.",3.2. Map and Cluster Recovery,[0],[0]
"To gain some further insight, one can show that (c.f. (Le et al., 2017)) with high probability
λ1(A(p, q))",3.2. Map and Cluster Recovery,[0],[0]
=  q + p−qk +O( 1√ nt ),3.2. Map and Cluster Recovery,[0],[0]
i = 1 p−q k +O( 1√ nt ) 2 ≤,3.2. Map and Cluster Recovery,[0],[0]
i ≤ k,3.2. Map and Cluster Recovery,[0],[0]
"O( 1√
nt ) k + 1 ≤",3.2. Map and Cluster Recovery,[0],[0]
"i ≤ n
indicating that γ = o(1).",3.2. Map and Cluster Recovery,[0],[0]
"Moreover, under this model
A(p, q)",3.2. Map and Cluster Recovery,[0],[0]
"≈ (p− q)Ik ⊗ (11T ) + q(11T ).
",3.2. Map and Cluster Recovery,[0],[0]
"If p and q are well-separated, the top k eigenvectors of A(p, q) approximate Ik⊗1, meaning dintra ≈ 0 and dinter ≈ 1.",3.2. Map and Cluster Recovery,[0],[0]
"Now we formally state the exact recovery conditions:
Theorem 3.1.",3.2. Map and Cluster Recovery,[0],[0]
(Intra-Cluster and Map Recovery),3.2. Map and Cluster Recovery,[0],[0]
Assume t = Ω( log(n)n ).,3.2. Map and Cluster Recovery,[0],[0]
Consider the noise model described in Section 3.1.,3.2. Map and Cluster Recovery,[0],[0]
"There exists an absolute constant cintra such that PermSMAC recovers the underlying intra-cluster maps and the underlying clusters with high probability if
p− q ≥ cintrak √ log(n)
nt .",3.2. Map and Cluster Recovery,[0],[0]
"(11)
Remark 2.",3.2. Map and Cluster Recovery,[0],[0]
Note that the gap between p and q is used to ensure the recovery of the underlying clusters.,3.2. Map and Cluster Recovery,[0],[0]
"Moreover, the recovery rate matches the information theoretic lower bound established in (Chen et al., 2016) up to O( √ log(n), indicating the tightness of our condition for PermSMAC.
",3.2. Map and Cluster Recovery,[0],[0]
The following theorem provides an exact recovery condition for the inter-cluster maps.,3.2. Map and Cluster Recovery,[0],[0]
"Compared with the previous lower bound, the lower bound on q for inter-cluster map recovery is significantly cruder.",3.2. Map and Cluster Recovery,[0],[0]
"This shows the advantage of recovering inter-cluster maps as a separate step.
",3.2. Map and Cluster Recovery,[0],[0]
Theorem 3.2.,3.2. Map and Cluster Recovery,[0],[0]
"There exists an absolute constant cinter > 0, so that when q ≥ cinterk √ log(n)
n2t ,
PermSMAC recovers the underlying inter-cluster maps with high probability.",3.2. Map and Cluster Recovery,[0],[0]
In this section we extend the algorithm to handle partially (as opposed to fully) similar objects.,4. Partial Matching,[0],[0]
Each input object Si (1 ≤ i ≤ n) in this setting has mi ∈ N+,4. Partial Matching,[0],[0]
"points, where the mi’s vary across the collection.",4. Partial Matching,[0],[0]
"Consequently, the pairwise maps X inij ∈ {0, 1}mj×mi are no longer permutation matrices since some rows and columns may only contain zero elements.",4. Partial Matching,[0],[0]
"We propose the following modified algorithm for SMAC in this partial matching setting.
",4. Partial Matching,[0],[0]
Step I: Extract underlying clusters and compute intracluster maps.,4. Partial Matching,[0],[0]
"Forming the data matrix and leading eigenvector computation stay the same as in the full matching setting, except we replace X inij − 1m11
T by X inij in (1).",4. Partial Matching,[0],[0]
The first difference occurring in the partial matching setting is that we cannot apply (2) to obtain pair-wise maps and affinity scores for clustering.,4. Partial Matching,[0],[0]
Our strategy is to apply single linkage clustering on the rows of U .,4. Partial Matching,[0],[0]
"Specifically, the distance measure between two points p, p′ is given by
‖up − up′‖2, where up is the corresponding row of p in U .",4. Partial Matching,[0],[0]
The number of output clusters is set as r. Each output cluster of this single-linkage procedure collects a set of matched points among the input objects.,4. Partial Matching,[0],[0]
We merge two clusters if the objects they belong to overlap.,4. Partial Matching,[0],[0]
"Suppose we finally obtain k clusters c1, · · · , ck.",4. Partial Matching,[0],[0]
"For each cluster ci, we introduce a binary matrix Yi ∈ {0, 1}ni×ri , whose columns encode the enclosed point clusters.",4. Partial Matching,[0],[0]
Then it is easy to see that the blocks of YiY Ti describe intra-cluster maps.,4. Partial Matching,[0],[0]
"Note that ri = m in the full setting, but in the partial setting ri is usually larger than max1≤i≤nmi, due to partial similarity.
",4. Partial Matching,[0],[0]
Step II: Compute inter-cluster maps.,4. Partial Matching,[0],[0]
"In the partial setting, we encode the inter-cluster map from cluster cs to cluster ct as a matrix Xst ∈ {0, 1}rt×rs .",4. Partial Matching,[0],[0]
"Consider a object pair (i, j) ∈ E , where i ∈ cs and j ∈ ct.",4. Partial Matching,[0],[0]
"With Ei,s and Ej,t we denote the index matrices that extract the corresponding blocks in Ys and Yt.",4. Partial Matching,[0],[0]
"It is easy to see that the entries Y Tt Ej,tX in ij E T",4. Partial Matching,[0],[0]
"i,sYs provide cues for the inter-cluster map Xst.",4. Partial Matching,[0],[0]
"Similar to the full map setting, we compute
Cst = ∑
i∈cs,j∈ct,(i,j)∈E
Y Tt Ej,tX in ij E T",4. Partial Matching,[0],[0]
"i,sYs.
",4. Partial Matching,[0],[0]
"Since the inter-cluster maps may not be permutation matrices either, we apply a simple thresholding to obtain the inter-cluster maps:
Xst =",4. Partial Matching,[0],[0]
"Cst > βst,
where βst is set as 0.9 times the maximum element of Cst in our experiments.",4. Partial Matching,[0],[0]
"In this section, we evaluate our approach on both synthetic (Section 5.1) and real datasets (Section 5.2 and Section 5.3).",5. Experimental Results,[0],[0]
"For baseline comparison, we consider state-of-the-art approaches for clustering and joint mapping in each domain.",5. Experimental Results,[0],[0]
We apply the model described in Section 3 to generate the synthetic data sets for our experiments.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Below we summarize how the procedure depends on the model parameters:
• The observation graph G. We employ a standard twocommunity stochastic block model (Abbe et al., 2016) which enables us to fully control the vertex degree and the spectral gap.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We use this model to generate three observation graphs G1,G2,G3.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"All of them have n = 300 vertices, but the vertex degrees and spectral gaps vary.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Specifically, G1 is the clique graph.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"G2 is a sparse graph, whose average vertex degree is 50 and the spectral gap is 0.1.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The average vertex degree and spectral gap of G3 are 50 and 0.5, respectively.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"In
our experiment, we treat G1 as the default observation graph.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We also study the influence of the observation graphs on the performance of our algorithm.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
• Number of clusters k.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Without loss of generality, we allocate each object into a underlying cluster with probability 1k .",5.1. Experimental Results on Synthetic Instances,[0],[0]
"For each observation graph, we generate and fix one underlying cluster throughout our experiments.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
"• Other parameters m, γ, p, q.",5.1. Experimental Results on Synthetic Instances,[0],[0]
We fix the number of points on each object as m = 30.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"In the partial matching setting, we follow the protocol (Chen et al., 2014) to generate the input objects so that the expected size of each object is mγ.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We sample the ratio of correct inter-cluster maps q = exp(− i10 ), 15 ≤ i ≤ 50.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Since p > q, we sample the ratio of correct intra-cluster maps so that p− q = i100 , 1 ≤ i ≤ 30.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
"We now study the empirical phase transition curves when varying p and q under different k, γ and observation graphs.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying k.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"The first two rows of Figure 2 show the phase transition curves of map and cluster recovery for k = 2, 4, 6.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Across all configurations, our approach tolerates a significant portion of noise in the input maps.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The fraction of noise that our approach can handle reduces as k increases, which is consistent with the exact recovery conditions in Section 3.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"In addition, phase transitions with respect to mapping recovery and cluster recovery roughly align.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"The subtle differences are two-fold: when p and q are close, cluster recovery breaks as there is no cue for clustering; likewise, map recovery breaks when q approaches 0.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Versus mapping only.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Figure 2 compares our approach to state-of-the-art map recovery technique (Huang & Guibas, 2013).",5.1. Experimental Results on Synthetic Instances,[0],[0]
"SMAC clearly exhibits a clear advantage in the regime, where q is small and p is significantly larger than q.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"This is expected through our exact recovery condition.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying observation graph.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Figure 3 shows phase transition curves of map and cluster recovery when varying the observation graphs (k = 2, γ = 1).",5.1. Experimental Results on Synthetic Instances,[0],[0]
Our approach tolerates a larger fraction of incorrect maps for larger vertex degrees.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"Moreover, when vertex degrees are comparable, a small spectral gap means higher recovery rate.
",5.1. Experimental Results on Synthetic Instances,[0],[0]
Varying γ.,5.1. Experimental Results on Synthetic Instances,[0],[0]
Figure 4 shows the phase transition curves when varying the overlapping ratio γ.,5.1. Experimental Results on Synthetic Instances,[0],[0]
"We again show three configurations, i.e., γ = 1, γ = 0.8 and γ = 0.6.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Still, our approach can tolerate a large rate of noise in the input maps.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"Moreover, the rate reduces as γ becomes smaller.",5.1. Experimental Results on Synthetic Instances,[0],[0]
"This is expected, as low overlapping ratio introduces weak signal for mapping and cluster recovery.",5.1. Experimental Results on Synthetic Instances,[0],[0]
We proceed to evaluate SMAC on 3D shapes.,5.2. Experimental Results on 3D Shapes,[0],[0]
We consider two datasets for this task.,5.2. Experimental Results on 3D Shapes,[0],[0]
"The first dataset collects four categories of 3D shapes from SHREC07-Watertight (Giorgi et al., 2007), namely, Human, Fourleg, Armadillo and Teddy.",5.2. Experimental Results on 3D Shapes,[0],[0]
These categories appear to have both similar global structures and local geometric details.,5.2. Experimental Results on 3D Shapes,[0],[0]
"However, the intercategory variability is salient.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second dataset is more fine-grained.,5.2. Experimental Results on 3D Shapes,[0],[0]
"It has 10 underlying shape collections from FAUST training dataset (Bogo et al., 2014), where each collection consists of different poses of the same human subject (10 poses per collection).",5.2. Experimental Results on 3D Shapes,[0],[0]
"For evaluating shape maps, we follow the protocol of (Kim et al., 2011) by collecting statistics on the geodesic distortion of predicted correspondences with respect to human annotated feature correspondences.
",5.2. Experimental Results on 3D Shapes,[0],[0]
Mapping performance.,5.2. Experimental Results on 3D Shapes,[0],[0]
Figure 5(c) plots the accuracy of predicted correspondences of our approach versus the input.,5.2. Experimental Results on 3D Shapes,[0],[0]
"For a detailed assessment, we separate the statistics of intra-cluster maps and inter-cluster maps.",5.2. Experimental Results on 3D Shapes,[0],[0]
"We consider two approaches for baseline comparison: the first applies (Huang & Guibas, 2013) to the entire dataset, and the second applies (Huang & Guibas, 2013) to each category in isolation then applies the third step of our approach to compute intercluster maps.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second baseline may be considered as a performance upper bound.,5.2. Experimental Results on 3D Shapes,[0],[0]
"Our proposed SMAC is significantly better than mapping without clustering (which is seriously affected by the noise in inter-cluster maps) and competitive against the second baseline.
",5.2. Experimental Results on 3D Shapes,[0],[0]
Clustering performance.,5.2. Experimental Results on 3D Shapes,[0],[0]
"As shown in Table 1, our approach correctly identifies all underlying clusters in SHREC07 and FAUST.",5.2. Experimental Results on 3D Shapes,[0],[0]
We also applied two baseline clustering approaches on the same dataset.,5.2. Experimental Results on 3D Shapes,[0],[0]
"The first approach performs k-means on the spectral shape descriptors (Rustamov, 2007).",5.2. Experimental Results on 3D Shapes,[0],[0]
"This approach only yields 84.6% and 72.0%, respectively.",5.2. Experimental Results on 3D Shapes,[0],[0]
The second approach utilizes the mapping distortion as an affinity measure and applies spectral clustering.,5.2. Experimental Results on 3D Shapes,[0],[0]
"This approach yields 94.9% and 74.0%, respectively, which are better than the first baseline.",5.2. Experimental Results on 3D Shapes,[0],[0]
"However, our approach is still better, which shows the advantage of using the cycle-consistency constraint for clustering.",5.2. Experimental Results on 3D Shapes,[0],[0]
"Finally, we evaluate our approach on two datasets of 2D images.",5.3. Experimental Results on 2D Images,[0],[0]
The first dataset (Figure 6(Left)) consists of 600 internet images of Notre Dame.,5.3. Experimental Results on 2D Images,[0],[0]
"These images naturally fall into 3 categories, each of which collects images from a similar camera pose (Snavely et al., 2006).",5.3. Experimental Results on 2D Images,[0],[0]
"The second dataset (Figure 6(Right)) collects 600 internet images of 4 landmark churches in Europe (Amiens Cathedral (200 im-
ages), York Minster (200 images), Duomo (100 images) and Westminster Abbey (100 images)).",5.3. Experimental Results on 2D Images,[0],[0]
"As inter-cluster maps do not make much sense here, we only evaluate clustering results and intra-cluster maps in this experiment.",5.3. Experimental Results on 2D Images,[0],[0]
"We sample 400 SIFT features (Lowe, 2004) for each image and apply SIFT flow (Liu et al., 2011) to establish pairwise correspondences between the features.",5.3. Experimental Results on 2D Images,[0],[0]
"We manually mark feature correspondences for evaluation.
",5.3. Experimental Results on 2D Images,[0],[0]
Mapping performance.,5.3. Experimental Results on 2D Images,[0],[0]
Figure 6 compares our approach with the two baseline approaches introduced in Section 5.2.,5.3. Experimental Results on 2D Images,[0],[0]
The relative performance is consistent.,5.3. Experimental Results on 2D Images,[0],[0]
"Specifically, due to the small-overlapping region across different clusters, intercluster maps are rather noisy, so applying joint-mapping directly leads to sub-optimal results.",5.3. Experimental Results on 2D Images,[0],[0]
"In addition, our approach is competitive against the approach of computing intra-cluster and inter-cluster maps in a sequential manner.
",5.3. Experimental Results on 2D Images,[0],[0]
Clustering performance.,5.3. Experimental Results on 2D Images,[0],[0]
"Finally, we evaluate our approach in terms of clustering accuracy.",5.3. Experimental Results on 2D Images,[0],[0]
"We choose two base-
line approaches, where the first baseline approach performs k-means on image descriptors.",5.3. Experimental Results on 2D Images,[0],[0]
"In this case, we employ GIST (Oliva & Torralba, 2001).",5.3. Experimental Results on 2D Images,[0],[0]
The second baseline uses the residual of SIFT flow as the affinity score for clustering.,5.3. Experimental Results on 2D Images,[0],[0]
"As shown in Table 1, our approach leads to a clustering accuracy of 99.3% and 96.1% on Notre Dame and Church, respectively.",5.3. Experimental Results on 2D Images,[0],[0]
"They are higher than those of the top performing baselines, i.e., 94.5% and 92.1%, respectively.",5.3. Experimental Results on 2D Images,[0],[0]
We have introduced SMAC for simultaneously computing consistent maps across a heterogeneous data collection and identifying the underlying clusters.,6. Conclusions,[0],[0]
The key idea is to leverage the higher self-consistency within intra-cluster maps than inter-cluster maps.,6. Conclusions,[0],[0]
Enforcing this variation of consistency allows us to denoise the input maps in a sequential manner while simultaneously identifying the underlying cluster structures.,6. Conclusions,[0],[0]
"The approach is based on spectral decomposition, for which we provided tight exact recovery conditions for both the input maps and the underling clusters.",6. Conclusions,[0],[0]
"Experimental results on synthetic data sets justify our exact recovery conditions, and experimental results on real data sets demonstrate the efficacy of our approach.
",6. Conclusions,[0],[0]
Acknowledgement.,6. Conclusions,[0],[0]
"This research by Chandrajit Bajaj was supported in part by a grant from the NIH, R01 GM117594.",6. Conclusions,[0],[0]
"Tingran Gao acknowledges support from Simons Math+X Investigators Award 400837, DARPA D15AP00109, and NSF IIS 1546413.",6. Conclusions,[0],[0]
"Qixing Huang would like to acknowledge support for this research from NSF DMS-1700234, NSF CIP-1729486, and NSF IIS-1618648.",6. Conclusions,[0],[0]
"We introduce a principled approach for simultaneous mapping and clustering (SMAC) for establishing consistent maps across heterogeneous object collections (e.g., 2D images or 3D shapes).",abstractText,[0],[0]
"Our approach takes as input a heterogeneous object collection and a set of maps computed between some pairs of objects, and outputs a homogeneous object clustering together with a new set of maps possessing optimal intraand inter-cluster consistency.",abstractText,[0],[0]
Our approach is based on the spectral decomposition of a data matrix storing all pairwise maps in its blocks.,abstractText,[0],[0]
We additionally provide tight theoretical guarantees for the accuracy of SMAC under established noise models.,abstractText,[0],[0]
We also demonstrate the usefulness of our approach on synthetic and real datasets.,abstractText,[0],[0]
SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2347–2356, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Applications using social media data, such as reviews, discussion posts, and (micro) blogs are becoming increasingly popular.",1 Introduction,[0],[0]
"We observed from our collaborations with social science and health science researchers that in a typical application, the researcher first need to obtain a set of posts of a particular topic that he/she wants to study, e.g., a political issue.",1 Introduction,[0],[0]
Keyword search is often used as the first step.,1 Introduction,[0],[0]
"However, that is not sufficient due to low precision and low recall.",1 Introduction,[0],[0]
A post containing the keyword “politics” may not be a political post while a post that does not contain the keyword may be a political post.,1 Introduction,[0],[0]
"Thus,
text classification is needed to make more sophisticated decisions to improve accuracy.
",1 Introduction,[0],[0]
"For classification, the user first manually labels a set of relevant posts (positive data) about the political issue and irrelevant posts (negative data) not about the political issue and then builds a classifier by running a learning algorithm, e.g. SVM or naïve Bayes.",1 Introduction,[0],[0]
"However, the resulting classifier may not be satisfactory.",1 Introduction,[0],[0]
There may be many reasons.,1 Introduction,[0],[0]
"One key reason we observed is that the labeled negative training data is not fully representative of the negative test data.
",1 Introduction,[0],[0]
"Let the user-interested topic be P (positive), and the set of all other irrelevant topics discussed in a social media source be T = {T1, T2, …, Tn}, which forms the negative data.",1 Introduction,[0],[0]
n is usually large.,1 Introduction,[0],[0]
"However, due to the labor-intensive effort of manual labeling, the user can label only a certain number of training posts.",1 Introduction,[0],[0]
Then the labeled negative training posts may cover only a small number of irrelevant topics S of T (S ⊆ T) as negative.,1 Introduction,[0],[0]
"Further, due to the highly dynamic nature of social media, it is probably impossible to label all possible negative topics.",1 Introduction,[0],[0]
"In testing, when posts of other negative topics in T−S show up, their classification can be unpredictable.",1 Introduction,[0],[0]
"For example, in an application, the training data has no negative examples about sports.",1 Introduction,[0],[0]
"However, in testing, some sports posts show up.",1 Introduction,[0],[0]
"These unexpected sports posts may be classified arbitrarily, which results in low classification accuracy.",1 Introduction,[0],[0]
"In this paper, we aim to solve this problem.
",1 Introduction,[0],[0]
"In machine learning, this problem is called covariate shift, a type of sample selection bias.",1 Introduction,[0],[0]
"In classic machine learning, it is assumed that the training and testing data are drawn from the same distribution.",1 Introduction,[0],[0]
"However, this assumption may not hold in practice such as in our case above, i.e., the training and the test distributions are different (Heckman 1979; Shimodaira 2000; Zadrozny 2004; Huang et al. 2007; Sugiyama et al. 2008; Bickel et al. 2009).",1 Introduction,[0],[0]
"In general, the sample selection bias problem is not solvable because the two
2347
distributions can be arbitrarily far apart from each other.",1 Introduction,[0],[0]
Various assumptions were made to solve special cases of the problem.,1 Introduction,[0],[0]
One main assumption was that the conditional distribution of the class given a data instance is the same in the training and test data sets (Shimodaira 2000; Huang et al. 2007; Bickel et al. 2009).,1 Introduction,[0],[0]
"This gives the covariate shift problem.
",1 Introduction,[0],[0]
"In this paper, we focus on a special case of the covariate shift problem.",1 Introduction,[0],[0]
"We assume that the covariate shift problem occurs mainly in the negative training and test data, and no or minimum covariate shift exists in the positive training and test data.",1 Introduction,[0],[0]
"This assumption is reasonable because the user knows the type of posts/documents that s/he is looking for and can label many of them.
",1 Introduction,[0],[0]
"Following the notations in (Bickel et al. 2009), our special case of the covariate shift problem can be stated formally as follows: let the set of training examples be {(x1, y1), (x2, y2), …, (xk, yk)}, where xi is the data/feature vector and yi is the class label of xi.",1 Introduction,[0],[0]
"Let the set of test cases be {xk+1, xk+2, …, xn}, which have no class labels.",1 Introduction,[0],[0]
"Since we are interested in binary classification, yi is either 1 (positive class) or -1",1 Introduction,[0],[0]
(negative class).,1 Introduction,[0],[0]
The labeled training data and the unseen test data have the same target conditional distribution p(y|x) and the marginal distributions of the positive data in both the training and testing are also the same.,1 Introduction,[0],[0]
"But the marginal distributions of the negative data in the training and testing are different, i.e., 𝑝!(𝐱!)",1 Introduction,[0],[0]
"≠ 𝑝!(𝐱!), where L, T, and – represent the labeled training data, test data, and the negative class respectively.
",1 Introduction,[0],[0]
Existing methods for addressing the covariate shift problem basically work as follows (see the Related Work section).,1 Introduction,[0],[0]
"First, they estimate the bias of the training data based on the given test data using some statistical techniques.",1 Introduction,[0],[0]
"Then, a classifier is trained on a weighted version of the original training set based on the estimated bias.",1 Introduction,[0],[0]
"Requiring the test data to be available in training is, however, a major weakness.",1 Introduction,[0],[0]
"In the social media post classification setting, the system needs to constantly classify the incoming data.",1 Introduction,[0],[0]
"It is infeasible to perform training constantly.
",1 Introduction,[0],[0]
"In this paper, we propose a novel learning technique that does not need the test data to be available during training due to the specific nature of our problem, i.e., the positive training data does not have the covariate shift issue.
",1 Introduction,[0],[0]
"One obvious solution to this problem is oneclass classification (Schölkopf et al. 1999; Tax and Duin, 1999a), i.e., one-class SVM.",1 Introduction,[0],[0]
"We simply discard the negative training posts/documents
completely because they have the covariate shift problem.",1 Introduction,[0],[0]
"Although this is a valid solution, as we will see in the evaluation section, the models built based on one-class SVM perform poorly.",1 Introduction,[0],[0]
"Although it is conceivable to use an unsupervised method such clustering, SVD (Alter et al., 2000) or LDA (Blei et al., 2003), supervised learning usually give much higher accuracy.
",1 Introduction,[0],[0]
"In our proposed method, instead of performing supervised learning in the original document space based on n-grams, we perform learning in a similarity space.",1 Introduction,[0],[0]
"Thus, the key novelty of the method is the transformation from the original document space (DS) to a center-based similarity space (CBS).",1 Introduction,[0],[0]
"In the new space, the covariate shift problem is significantly mitigated, which enables us to build more accurate classifiers.",1 Introduction,[0],[0]
"The reason for this is that in CBS based learning the vectors in the similarity space enable SVM (which is the learning algorithm that we use) to find a good boundary of the positive class data based on similarity and to separate it from all possible negative class data, including those negative data that is not represented in training.",1 Introduction,[0],[0]
"We will explain this in greater detail in Section 3.5 after we present the proposed algorithm, which we call CBS-L (for CBS Learning).
",1 Introduction,[0],[0]
"This paper makes three contributions: First, it formulates a special case of the covariate shift problem.",1 Introduction,[0],[0]
This case occurs frequently in social media data classification as we discussed above.,1 Introduction,[0],[0]
"Second, it proposes a novel CBS space based learning method, CBS-L, which avoids the covariate shift problem to a large extent because it is able to find a good similarity boundary of the positive data.",1 Introduction,[0],[0]
"Third, it experimentally demonstrates the effectiveness of the proposed method.",1 Introduction,[0],[0]
Traditional supervised learning assumes that the training and test examples are drawn from the same distribution.,2 Related Work,[0],[0]
"However, this assumption can be violated in many applications.",2 Related Work,[0],[0]
This is especially the case for social media data because of the high topic diversity and constant changes of topics.,2 Related Work,[0],[0]
"This problem is known as covariate shift, which is a form of sample selection bias.
",2 Related Work,[0],[0]
Sample selection bias was first introduced in econometrics by Heckman (1979).,2 Related Work,[0],[0]
It came into the field of machine learning through the work of Zadrozny (2004).,2 Related Work,[0],[0]
"The main approach in machine learning is to first estimate the distribution bias of the training data based on the test data, and then learn using weighted training examples to
compensate for the bias (Bickel et al. 2009).",2 Related Work,[0],[0]
"For example, Shimodaira (2000) and Sugiyama and Muller (2005) proposed to estimate the training and test data distributions using kernel density estimation.",2 Related Work,[0],[0]
The estimated density ratio is then used to generate weighted training examples.,2 Related Work,[0],[0]
"Dudik et al. (2005) and Bickel and Scheffer (2007) used maximum entropy density estimation, while Huang et al. (2007) proposed kernel mean matching.",2 Related Work,[0],[0]
Sugiyama et al. (2008) and Tsuboi et al. (2008) estimated the weights for the training instances by minimizing the KullbackLeibler divergence between the test and the weighted training distributions.,2 Related Work,[0],[0]
Bickel et al. (2009) proposed an integrated model.,2 Related Work,[0],[0]
"As we discussed in the introduction, the need for the test data at the training time is a major weakness for social media data classification.",2 Related Work,[0],[0]
"The proposed technique CBS-L doesn’t have this restriction.
",2 Related Work,[0],[0]
"As mentioned in the introduction, one-class classification is a suitable approach to solve the problem.",2 Related Work,[0],[0]
Tax and Duin (1999a and 1999b) proposed a model for one-class classification called Support Vector Data Description (SVDD) to seek a hyper-sphere around the positive data that encompasses points in the data with the minimum radius.,2 Related Work,[0],[0]
"In order to balance between model over-fitting and under-fitting, Tax and Duin (2001) proposed a method that tries to use artificially generated outliers to optimize the model parameters.",2 Related Work,[0],[0]
"However, their experiments suggest that the procedure to generate artificial outliers in a hyper-sphere is only feasible for up to 30 dimensions.",2 Related Work,[0],[0]
"Also, as pointed out by (Khan and Madden, 2010; 2014), one drawback of their methods is that they often require a large dataset and the methods become very inefficient in high dimensional feature spaces.",2 Related Work,[0],[0]
"Since text documents are usually represented in a much higher dimensional space, these methods are less suitable for text applications.",2 Related Work,[0],[0]
Manevitz and Yousef (2001) performed one-class text classification using one-class SVM as proposed by Schölkopf et al. (1999).,2 Related Work,[0],[0]
The method is based on identifying outlier data that are representative of the second class.,2 Related Work,[0],[0]
"Instead of assuming the origin is the only member of the outlier class, it assumes those data points with few non-zero entries are also outliers.",2 Related Work,[0],[0]
"However, as reported in the paper, their methods produce quite weak results (Schölkopf et al., 1999; 2000).",2 Related Work,[0],[0]
Li et al. (2003) presented an improved version of one-class SVM for detecting anomalies.,2 Related Work,[0],[0]
Their idea is to consider all data points that are close to the origin as outliers.,2 Related Work,[0],[0]
"Both (Yang and Madden, 2007) and (Tian and
Gu, 2010) tried to refine Schölkopf’s models by searching optimal parameters.",2 Related Work,[0],[0]
"Luo et al., (2007) proposed a cost-sensitive one-class SVM algorithm for intrusion detection.",2 Related Work,[0],[0]
"We will see in the experiment section that one-class classification is far inferior to our proposed CBS-L method.
",2 Related Work,[0],[0]
"In this work, we propose to represent documents in the similarity space and thus it is related to works on document representation.",2 Related Work,[0],[0]
"Alternative document representations have been proposed in the past and have been shown to perform well in many applications (Radev et al., 2000; He et al., 2004; Lebanon 2006; Ranzato and Szummer, 2008, Wang and Domeniconi, 2008).",2 Related Work,[0],[0]
"In (Radev et al., 2000), although the centroid sentence/document vector was computed, it was not transformed to a similarity space vector representation.",2 Related Work,[0],[0]
Wang and Domeniconi (2008) proposed to use external knowledge to build semantic kernels for documents in order to improve text classification.,2 Related Work,[0],[0]
"In our problem, the main difficulty is that testing negative documents cannot be well covered in training.",2 Related Work,[0],[0]
"It is not clear how the enriched document representations could help solve our problem.
",2 Related Work,[0],[0]
"Our work is also related to learning from positive and unlabeled examples, also known as PU learning (Denis, 1998; Yu et al. 2002; Liu et al. 2003; Lee and Liu, 2003; Elkan and Noto, 2008; Li et al. 2010).",2 Related Work,[0],[0]
"In this learning model, there is a set of labeled positive training data and a set of unlabeled data, but there is no labeled negative training data.",2 Related Work,[0],[0]
"Clearly, their setting is different from ours too.",2 Related Work,[0],[0]
"There is also no guarantee that the unlabeled data has the same distribution as the future test data.
",2 Related Work,[0],[0]
Our problem is also very different from domain adaption as we work in the same domain.,2 Related Work,[0],[0]
"Due to the use of document similarity, our method has some resemblance to learning to rank (Li, 2011; Liu, 2011).",2 Related Work,[0],[0]
"However, CBS-L is very different because we perform supervised classification.",2 Related Work,[0],[0]
"Our similarity is also center-based rather than pair-wise document similarity, which is also used in (Qian and Liu 2013) for spam detection.",2 Related Work,[0],[0]
"We now formulate the proposed supervised learning in the CBS space, called CSB-L. The key difference between CBS learning and the classic document space (DS) learning is in the document representation, which applies to both training and testing documents or posts.",3 The Proposed CBS Learning,[0],[0]
"In the next subsection, we first give the intuitive idea
and a simple example.",3 The Proposed CBS Learning,[0],[0]
The detailed algorithm follows.,3 The Proposed CBS Learning,[0],[0]
"In Section 3.5, we explain why CBS-L is better than DS-based learning when unexpected negative data appear in the test set.",3 The Proposed CBS Learning,[0],[0]
"In the proposed CBS-L formulation, each document d is still represented as a feature vector, but the vector no longer represents the document d itself based on n-grams.",3.1 Basic Idea,[0],[0]
"Instead, it represents a set of similarity values between document d and the center of the positive documents.",3.1 Basic Idea,[0],[0]
"Specifically, the learning consists of the following steps:
1.",3.1 Basic Idea,[0],[0]
"Each document d (in the positive or negative class) is first represented with a set of document representations, i.e., document space vectors (ds-vectors) based on the document itself as in traditional text classification.",3.1 Basic Idea,[0],[0]
Each vector denotes one representation of the document.,3.1 Basic Idea,[0],[0]
"For example, one representation may be based on only unigrams, and another representation may be based on only bigrams.",3.1 Basic Idea,[0],[0]
"For simplicity, we use only one representation/vector x (e.g., unigrams) here to represent d. Note that we use bold lower case letters to represent vectors.",3.1 Basic Idea,[0],[0]
Each feature in a ds-vector is called a ds-feature.,3.1 Basic Idea,[0],[0]
2.,3.1 Basic Idea,[0],[0]
A center vector c is then computed for each document representation for the positive class documents using the ds-vectors of all positive and negative documents of that representation.,3.1 Basic Idea,[0],[0]
c is thus also a ds-vector.,3.1 Basic Idea,[0],[0]
3.,3.1 Basic Idea,[0],[0]
Each document d in the positive and negative class is then transformed to a center-based similarity space vector sd (called a cbs-vector).,3.1 Basic Idea,[0],[0]
"sd consists of a set of similarity values between document d’s set of ds-vectors, i.e., {x} in our case here (since we use only one representation), and the set of corresponding positive class center vectors, i.e., {c} in our case:
sd =Sim({x}, {c}),
where Sim is a similarity function consisting of a set of similarity measures.",3.1 Basic Idea,[0],[0]
Each feature in sd is called an cbs-feature.,3.1 Basic Idea,[0],[0]
sd still has the same original class label as d. Let us see an actual example.,3.1 Basic Idea,[0],[0]
"We assume that our single center vector for the positive class has been computed (see Section 3.2) based on the unigram representation of documents: c: 1:1 2:1 6:2 where y:z represents a ds-feature y (e.g., a word) and its feature value (e.g., term frequency, tf).",3.1 Basic Idea,[0],[0]
"We want to transform the follow-
ing positive document d1 and negative document d2 (ds-vectors) to their cbs-vectors (the first number is the class):
d1: 1 1:2 2:1 3:1 d2: -1 2:2 3:1 5:2 If we use cosine as the first similarity measure in Sim, we can generate a cbs-feature 1:0.50 for d1 (as cosine(c, d1) = 0.50) and a cbsfeature 1:0.27 for d2 (as cosine(c, d2) = 0.27).",3.1 Basic Idea,[0],[0]
"If we have more similarity measures, more cbs-features will be produced.",3.1 Basic Idea,[0],[0]
"The resulting cbs-vectors for d1 and d2 with their class labels, 1 and -1, are:
d1: 1 1:0.50 … d2: -1 1:0.27 … 4.",3.1 Basic Idea,[0],[0]
"We now have a binary classification problem
in the CBS space.",3.1 Basic Idea,[0],[0]
"This step simply runs a classification algorithm, e.g., SVM, to build a classifier.",3.1 Basic Idea,[0],[0]
We use SVM in our work.,3.1 Basic Idea,[0],[0]
We are given a binary text classification problem.,3.2 CBS Based Learning,[0],[0]
"Let D = {(d1, y1), (d2, y2), …, (dn, yn)} be the set of training examples, where di is a document and yi ∈ {1, -1} is its class label.",3.2 CBS Based Learning,[0],[0]
Traditional classification directly uses D to build a binary classifier.,3.2 CBS Based Learning,[0],[0]
"However, in the CBS space, we learn a classifier that returns 1 for documents that are “close enough” to the center of the training positive documents and -1 for documents elsewhere.
",3.2 CBS Based Learning,[0],[0]
We now detail the proposed technique.,3.2 CBS Based Learning,[0],[0]
"As we mentioned above, instead of using one single dsvector to represent a document di ∈D, we use a set Rd of p ds-vectors Rd = {𝐱!!",3.2 CBS Based Learning,[0],[0]
", 𝐱!!",3.2 CBS Based Learning,[0],[0]
", …, 𝐱!!}.",3.2 CBS Based Learning,[0],[0]
Each vector 𝐱!!,3.2 CBS Based Learning,[0],[0]
"denotes one document space representation of the document, e.g., unigram representation.",3.2 CBS Based Learning,[0],[0]
"We then compute the center of positive training documents, which is represented as a set of 𝑝 centroids C = {c1, c2, …, cp}, each of which corresponds to one document space representation in Rd.",3.2 CBS Based Learning,[0],[0]
"The way to compute each center ci is similar to that in the Rocchio relevance feedback method in information retrieval (Rocchio, 1971; Manning et al. 2008), which uses the corresponding ds-vectors of all training positive and negative documents.",3.2 CBS Based Learning,[0],[0]
The detail will be given below.,3.2 CBS Based Learning,[0],[0]
"Based on Rd for document d and the center C, we can transform a document d from its document space representations Rd to one center-based similarity vector cbs-v by applying a similarity function 𝑆𝑖𝑚 on each element 𝐱!!",3.2 CBS Based Learning,[0],[0]
of Rd and its corresponding center ci,3.2 CBS Based Learning,[0],[0]
.,3.2 CBS Based Learning,[0],[0]
"We now detail document transformation.
",3.2 CBS Based Learning,[0],[0]
"Training document transformation: The train-
ing data transformation from ds-vectors to cbsvectors performs the following two steps:
Step 1: Compute the set C of centroids for the positive class.",3.2 CBS Based Learning,[0],[0]
Each centroid vector ci∈C is for one document representation 𝐱!!.,3.2 CBS Based Learning,[0],[0]
"And it is computed by applying the Rocchio method to the corresponding ds-vectors of all documents in both positive and negative training data.
",3.2 CBS Based Learning,[0],[0]
"𝐜! = 𝛼 𝐷!
𝐱!!
",3.2 CBS Based Learning,[0],[0]
"𝐱!!𝐝𝐬!!∈!!                         
          ",3.2 CBS Based Learning,[0],[0]
− 𝛽 |𝐷,3.2 CBS Based Learning,[0],[0]
"− 𝐷!| 𝐱!!
𝐱!!𝐱!!∈!!!!
",3.2 CBS Based Learning,[0],[0]
where 𝐷! is the set of documents in the positive class and |.| is the size function.,3.2 CBS Based Learning,[0],[0]
"𝛼 and 𝛽 are parameters, which are usually set empirically.",3.2 CBS Based Learning,[0],[0]
"It is reported that using tf-idf representation, 𝛼 = 16 and 𝛽 = 4 usually work quite well (Buckley et al. 1994).",3.2 CBS Based Learning,[0],[0]
"The subtraction is used to reduce the influence of those terms that are not discriminative (i.e., terms appearing in both positive and negative documents).",3.2 CBS Based Learning,[0],[0]
"Step 2: Compute the similarity vector cbs-vd (center-based similarity space vector) for each document d ∈D based on its set of document space vectors Rd and the corresponding centroids C of the positive documents.
",3.2 CBS Based Learning,[0],[0]
"cbs-vd = Sim(Rd, C)
",3.2 CBS Based Learning,[0],[0]
"Sim has a set of similarity measures, and each measure mj is applied to p document representations 𝐱!!",3.2 CBS Based Learning,[0],[0]
in Rd and their corresponding centers 𝐜!,3.2 CBS Based Learning,[0],[0]
in C to generate p similarity features (cbs-features) in cbs-vd.,3.2 CBS Based Learning,[0],[0]
"We discuss the dsfeatures and similarity measures for computing cbs-features in the next two subsections.
",3.2 CBS Based Learning,[0],[0]
"Complexity: The data transformation step is clearly linear in the number of examples, i.e., n. Test document transformation: For each test document d, we can use step 2 above to produce a cbs-vector for d.",3.2 CBS Based Learning,[0],[0]
"In order to compute cbs-features (center-based similarity space features) for each document, we need to have the ds-features of a document and the center of the positive class.",3.3 DS-Features,[0],[0]
"We discuss dsfeatures first, which are extracted from each document itself.
",3.3 DS-Features,[0],[0]
"Since our task is document classification, we use the popular unigram, bigram and trigram
with tf-idf weighting as the ds-features for a document.",3.3 DS-Features,[0],[0]
These three types of ds-features also give us three different document representations.,3.3 DS-Features,[0],[0]
Ds-vectors are transformed into cbs-vectors by applying a set of similarity measures on each document space vector and the corresponding center vector.,3.4 CBS-Features,[0],[0]
"In this work, we employed five similarity measures from (Cha, 2007) to gauge the similarity of two vectors.",3.4 CBS-Features,[0],[0]
"Based on these measures, we produce 15 CBS features using the unigram, bigram, and trigrams representations of each document.",3.4 CBS-Features,[0],[0]
"The similarity measures we used are listed in Table 1, where P and Q are two vectors and d represents the dimension of P and Q.",3.4 CBS-Features,[0],[0]
"We now try to explain why CBS learning (CBSL) can deal with the covariate shift problem, and thus can perform better than document space learning.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"The reason is that due to the use of similarity features, CBS-L is essentially trying to generate a boundary for the positive training data because similarity is not directional and thus covers all directions in a spherical shape in the space.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"In classification, the negative data from anywhere or direction outside the spherical shape can be detected.",3.5 Why Does CBS Space Learning Work?,[0],[0]
The covariate shift problem will not affect the classification much.,3.5 Why Does CBS Space Learning Work?,[0],[0]
Many types of documents that are not represented in the negative training data will still be detected due to their low similarity.,3.5 Why Does CBS Space Learning Work?,[0],[0]
"For example, in Figure 1, we want to build a SVM classifier to separate positive data represented as black squares and negative data represented as empty circles.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"The constructed CBS-L classifier would look like a circle (in dashed line) in the original document space
covering the positive data.",3.5 Why Does CBS Space Learning Work?,[0],[0]
The size of this (boundary) circle depends on the separation margin between the two classes.,3.5 Why Does CBS Space Learning Work?,[0],[0]
"Although data points represented by empty triangles are not represented in the negative training data (which has only empty circles) in building the classifier, our classifier is able to identify them as not positive at the test time because they are outside the boundary circle.
",3.5 Why Does CBS Space Learning Work?,[0],[0]
"If we had used the document space (DS) features to build a SVM classifier, the classifier would be a line (see Figure 1) between the positive data (black squares) and the negative data (empty circles).",3.5 Why Does CBS Space Learning Work?,[0],[0]
"This line unfortunately will not be able to identify data points represented as empty triangles as not positive because the triangles actually lie on the positive side and would be classified as positive, which is clearly wrong.",3.5 Why Does CBS Space Learning Work?,[0],[0]
"In this section, we evaluate the proposed learning in the center-based similarity space (CBS-L) and compare it with baselines.",4 Experiments,[0],[0]
"As stated at the beginning of the paper, this work was motivated by the real-life problem of identifying the right social media posts or documents for specific applications.",4.1 Experimental Dataset,[0],[0]
"For an effective evaluation, we need a large number of classes in the data to reflect the topic richness and diversity of the social media.",4.1 Experimental Dataset,[0],[0]
The whole data also has to be labeled for evaluation.,4.1 Experimental Dataset,[0],[0]
"Using online reviews of a large number of products is a natural choice because there are many types of products and services and there is no need to do manual labeling, which is very labor intensive, time consuming, and error prone.",4.1 Experimental Dataset,[0],[0]
"We obtained the Amazon review database from the authors of (Jindal and Liu 2008), and constructed a dataset with reviews of 50 types of products, which we also call 50 topics.",4.1 Experimental Dataset,[0],[0]
Each topic (a type of products) have 1000 reviews.,4.1 Experimental Dataset,[0],[0]
"For each topic, we randomly sampled 700 reviews/documents for training and the remaining 300 reviews for testing.",4.1 Experimental Dataset,[0],[0]
"Note that although we use this product review collection, we
do not perform sentiment classification.",4.1 Experimental Dataset,[0],[0]
"Instead, we still perform the traditional topic based classification.",4.1 Experimental Dataset,[0],[0]
"That is, given a review, the system decides what type of product the review is about.",4.1 Experimental Dataset,[0],[0]
"In our experiments, we use every topic as the positive class.",4.1 Experimental Dataset,[0],[0]
This gives us 50 classification results.,4.1 Experimental Dataset,[0],[0]
We use three baselines in our evaluation.,4.2 Baselines,[0],[0]
"Document space one-class SVM (ds-osvm): As we discussed earlier, due to the covariate shift problem in the negative training data, one solution is to drop the negative training data completely to build a one-class classifier.",4.2 Baselines,[0],[0]
One-class SVM is the state-of-the-art one-class classification algorithm.,4.2 Baselines,[0],[0]
We apply one-class SVM to the documents in the document space as one of the baselines.,4.2 Baselines,[0],[0]
"One-class SVM was first introduced by Schölkopf et al. (1999; 2000), which is based on the assumption that the origin is the only member of the second class.",4.2 Baselines,[0],[0]
The data is first mapped into a transformed feature space via a kernel and then standard two-class SVM is employed to construct a hyper-plane that separates the data and the original with maximum margin.,4.2 Baselines,[0],[0]
"As mentioned earlier, there is also the support vector data description (SVDD) formulation for one-class classification proposed by Tax and Duin (1999a; 1999b).",4.2 Baselines,[0],[0]
SVDD seeks to distinguish the positive class from all other possible data in space.,4.2 Baselines,[0],[0]
It basically finds a hyper-sphere around the positive class data that contains almost all points in the data set with the minimum radius.,4.2 Baselines,[0],[0]
"It has been shown that the use of Gaussian kernel makes SVDD and One-class SVM equivalent, and the results reported in (Khan and Madden, 2014) demonstrate that SVDD and One-class SVM are comparable when the Gaussian kernel is applied.",4.2 Baselines,[0],[0]
"Thus in this paper, we just use oneclass SVM, which is one of the SVM-based classification tools in the LIBSVM1 library (version 3.20) (Chang and Lin, 2011).
",4.2 Baselines,[0],[0]
"Center-based similarity space one-class SVM (cbs-osvm): Instead of applying one-class SVM to documents in the original document space, this baseline applies it to the CBS space after the documents are transformed to CBS vectors.
",4.2 Baselines,[0],[0]
SVM:,4.2 Baselines,[0],[0]
This baseline is the SVM applied in the original document space.,4.2 Baselines,[0],[0]
"Although in this case, there is covariate shift problem, we want to see how serious the problem might be, and how the proposed CBS-L technique can deal with the 1 http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/
problem.",4.2 Baselines,[0],[0]
We use the SVM tool in LIBSVM.,4.2 Baselines,[0],[0]
"As Khan and Madden (2014) pointed out that one-class SVM performs the best when Gaussian kernel is used, we use Gaussian kernel as well.",4.3 Kernels and Parameters,[0],[0]
"Manevitz and Yousef (2001) applied one-class SVM to text classification, and the authors reported that one-class SVM works the best with binary feature weighting scheme compared to tf or tf-idf weighting schemes.",4.3 Kernels and Parameters,[0],[0]
"Also, they reported that a small number of features (10) with highest document frequency performed the best with Gaussian kernel.",4.3 Kernels and Parameters,[0],[0]
"We also use binary representation, but found that 10 features are already too many in our case.",4.3 Kernels and Parameters,[0],[0]
"In fact, 5 features give the best results.",4.3 Kernels and Parameters,[0],[0]
Using a small number of features is intuitive because to find the boundary of a very high dimensional space is very difficult.,4.3 Kernels and Parameters,[0],[0]
"We also tried more features but they were poorer.
",4.3 Kernels and Parameters,[0],[0]
"For SVM classification in the document space, we use the linear kernel as it has been shown by many researchers that the linear kernel performs the best (e.g., Joachims, 1998; Colas and Brazdil, 2006).",4.3 Kernels and Parameters,[0],[0]
"We experimented with RBF kernels extensively, but they did not perform well with the traditional document representation.",4.3 Kernels and Parameters,[0],[0]
"The term weighting scheme is tf-idf (Colas and Brazdil, 2006) with no feature selection.
",4.3 Kernels and Parameters,[0],[0]
"For our proposed method CBS-L, we use tf-idf
values of unigram, bigram and trigram to represent a document in three ways in the document space.",4.3 Kernels and Parameters,[0],[0]
"As mentioned earlier, five document similarity functions are used to transform document space vectors to CBS space vectors.",4.3 Kernels and Parameters,[0],[0]
"And in order to filter out less useful features for the center vector of the positive class, we performed feature selection in the document space using the classic information gain method (Yang and Pedersen, 1997) to empirically choose the most effective 100 features for the positive class.
",4.3 Kernels and Parameters,[0],[0]
"For all the kernels, we use the default parameter settings in the LIBSVM systems.",4.3 Kernels and Parameters,[0],[0]
"We tried to tune the parameters, but did not get better results.",4.3 Kernels and Parameters,[0],[0]
We now present the experiment results.,4.4 Results,[0],[0]
"As mentioned above, we treat each topic as the positive class.",4.4 Results,[0],[0]
This gives 50 tests.,4.4 Results,[0],[0]
"To test the effect of covariate shift, we also vary the number of topics in the negative class.",4.4 Results,[0],[0]
"We used 10, 20, 30, and 40 topics in the training negative class.",4.4 Results,[0],[0]
"The test set always has 49 topics of negative data.
",4.4 Results,[0],[0]
"For each setting, we give three sets of results for the positive class, which is the target topic data that we are interested in obtaining through classification.",4.4 Results,[0],[0]
"Each set of results includes the standard measures of precision, recall, and F1score for the positive class.",4.4 Results,[0],[0]
The three sets are: 1.,4.4 Results,[0],[0]
"In-training: In this case, the test negative data
contains only data from those topics used in training.",4.4 Results,[0],[0]
This is the classical supervised learning setting where the training and test data are randomly drawn from the same distribution.,4.4 Results,[0],[0]
2.,4.4 Results,[0],[0]
"Not-in-training: In this case, the test negative set contains only data from the other topics not used in training.",4.4 Results,[0],[0]
The classical setting of supervised learning does not deal with this problem.,4.4 Results,[0],[0]
This represents covariate shift.,4.4 Results,[0],[0]
3.,4.4 Results,[0],[0]
"Combined: In this case, the test data contains both in-training and not-in-training negative topics.",4.4 Results,[0],[0]
"Due to the use of not-in-training test data, this is also not the classical setting.",4.4 Results,[0],[0]
"Due to a large number of experiment results, we cannot report all the details.",4.4 Results,[0],[0]
Table 2 summarizes the results.,4.4 Results,[0],[0]
"Notice that for ds-osvm, it does not make sense to have in-training and not-intraining results because it does not use any training negative data.",4.4 Results,[0],[0]
"Thus, there is only one set of results for “Combined,” which is duplicated in the table for easy comparison.",4.4 Results,[0],[0]
"However, note that cbs-osvm uses negative data for training in order to compute the center for the positive class.
",4.4 Results,[0],[0]
"From the table, we can make the following observations (since there are many numbers, we only focus on F1-scores).",4.4 Results,[0],[0]
1.,4.4 Results,[0],[0]
"The proposed CBS-L method performs mark-
edly better than all baselines.",4.4 Results,[0],[0]
"For the results of in-training, not-in-training, and combined, CBS-L is consistently better in all cases than all baselines.",4.4 Results,[0],[0]
"Even for in-training, CBS-L perform better than SVM.",4.4 Results,[0],[0]
This clearly shows the superiority of the proposed CBS-L method.,4.4 Results,[0],[0]
2.,4.4 Results,[0],[0]
ds-osvm performs poorly.,4.4 Results,[0],[0]
cbs-osvm is much better because it uses the negative data in feature selection and center computation.,4.4 Results,[0],[0]
3.,4.4 Results,[0],[0]
SVM in the document space performed poorly (Combined) when only a small number of negative topics are used in training.,4.4 Results,[0],[0]
It gets better than both one-class SVM baselines when more negative topics are used in training (see the reason in the next point).,4.4 Results,[0],[0]
4.,4.4 Results,[0],[0]
"Finally, we can also see that with the number of training negative topics increases, the results of the combined case of both SVM and CBS-L improve.",4.4 Results,[0],[0]
"This is expected because with the increased number of negative topics for training, the number of not-in-training negative topics for testing decreases and the covariate shift problem gets smaller.",4.4 Results,[0],[0]
"We can also see that cbs-osvm, SVM and CBS-L’s F1-scores for not-in-training improve with the increased training negative topics due to the same reason.",4.4 Results,[0],[0]
"However, their F1-scores drop for in-training because with more negative
topics, the data becomes more skewed, which hurts in-training classification.
",4.4 Results,[0],[0]
"To give a flavor of the detailed results for each topic (product), we give the full results for one setting with 30 randomly selected topics as the training negative data (Table 3).",4.4 Results,[0],[0]
The results in the table are F1-scores of the combined case.,4.4 Results,[0],[0]
The ability to get relevant posts accurately about a topic from social media is a challenging problem.,5 Conclusion,[0],[0]
This paper attempted to solve this problem by identifying and dealing with the technical issue of covariate shift.,5 Conclusion,[0],[0]
The key idea of our technique is to transform document representation from the traditional n-gram feature space to a similarity based space.,5 Conclusion,[0],[0]
Our experimental results show that the proposed method CBS-L outperformed strong baselines by large margins.,5 Conclusion,[0],[0]
"This research was partially supported by the NSF grants IIS-1111092 and IIS-1407927, and a Google faculty award.
",Acknowledgments,[0],[0]
"Reference
Alter, O., Brown, P.O. and Bostein, D. 2000.",Acknowledgments,[0],[0]
Singular Value Decomposition for GenomeWide Expression Data Processing and Modeling.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"Nat',l Academy of Science, vol. 97, no. 18, pp. 10101-10106, Aug.
Blei, D. Ng, A. and Jordan, M., 2003.",Acknowledgments,[0],[0]
"Latent dirichlet allocation, The Journal of Machine Learning Research, 3, p.993-1022, 3/1/2003
Buckley, C., Salton, G., Allan, J. 1994.",Acknowledgments,[0],[0]
"The Effect of Adding Relevance Information in a Relevance Feedback Environment, Proceedings of SIGIR Conference.
",Acknowledgments,[0],[0]
"Bickel, S., Bruckner, M., and Scheffer. 2009.",Acknowledgments,[0],[0]
T. Discriminative learning under covariate shift.,Acknowledgments,[0],[0]
"Journal of Machine Learning Research.
",Acknowledgments,[0],[0]
Bickel S. and Scheffer T. 2007.,Acknowledgments,[0],[0]
Dirichletenhanced spam filtering based on biased samples.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Cha, S.-H. 2007.",Acknowledgments,[0],[0]
Comprehensive Survey on Distance/Similarity Measures between Probability Density Functions.,Acknowledgments,[0],[0]
"International Journal of Mathematical Models and Methods in Applied Sciences, 1(4):300--307.
",Acknowledgments,[0],[0]
"Chang, C-C. and Lin, C-J. 2011.",Acknowledgments,[0],[0]
"LIBSVM: a
library for support vector machines.",Acknowledgments,[0],[0]
"ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, http://www.csie.",Acknowledgments,[0],[0]
ntu.edu.,Acknowledgments,[0],[0]
"tw/~cjlin/libsvm
Colas, F. and Brazdil.",Acknowledgments,[0],[0]
P. 2006.,Acknowledgments,[0],[0]
Comparison of SVM and some older classification algorithms in text classification tasks.,Acknowledgments,[0],[0]
Artificial Intelligence in Theory and Practice.,Acknowledgments,[0],[0]
"IFIP International Federation for Information Processing, pp. 169-178.
",Acknowledgments,[0],[0]
"Denis, F., PAC learning from positive statistical queries.",Acknowledgments,[0],[0]
"ALT, 1998.
",Acknowledgments,[0],[0]
"Radev, D., Jing, H. and Budzikowska, M. 2000.",Acknowledgments,[0],[0]
"Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies.",Acknowledgments,[0],[0]
"In ANLP/NAACL Workshop on Summarization, Seattle, April.
Dudik, M., Schapire, R., and Phillips, S. 2005.",Acknowledgments,[0],[0]
Correcting sample selection bias in maximum entropy density estimation.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
",Acknowledgments,[0],[0]
"Elkan, C. and Noto, K. 2008.",Acknowledgments,[0],[0]
Learning classifiers from only positive and unlabeled data.,Acknowledgments,[0],[0]
"KDD, 213-220.
",Acknowledgments,[0],[0]
"He, X., Cai, D., Liu, H. and Ma, W.-Y. 2004.",Acknowledgments,[0],[0]
Locality Preserving Indexing for Document Representation.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"Of SIGIR.
",Acknowledgments,[0],[0]
"Heckman, J. 1979.",Acknowledgments,[0],[0]
Sample selection bias as a specification error.,Acknowledgments,[0],[0]
"Econometrica, 47:153– 161.
Huang, J., Smola, A. and Gretton, A., Borgwardt K., and Scholkopf B. 2007.",Acknowledgments,[0],[0]
Correcting sample selection bias by unlabeled data.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Joachims, T. 1998.",Acknowledgments,[0],[0]
Text categorization with support vector machines:,Acknowledgments,[0],[0]
Learning with many relevant features.,Acknowledgments,[0],[0]
"ECML.
",Acknowledgments,[0],[0]
"Jindal, N. and Liu, B. 2008.",Acknowledgments,[0],[0]
Opinion Spam and Analysis.,Acknowledgments,[0],[0]
"Proceedings of the ACM Conference on Web Search and Data Mining.
Khan, S., and Madden, M. 2010.",Acknowledgments,[0],[0]
A survey of recent trends in one class classification.,Acknowledgments,[0],[0]
"Artificial Intelligence and Cognitive Science, volume 6206 of Lecture Notes in Computer Science.",Acknowledgments,[0],[0]
"188–197.
",Acknowledgments,[0],[0]
"Khan, S. and Madden, M. 2014.",Acknowledgments,[0],[0]
One-Class Classification: Taxonomy of Study and Review of Techniques.,Acknowledgments,[0],[0]
"The Knowledge Engineering Review, 1-30.
Lebanon, G. 2006.",Acknowledgments,[0],[0]
"Sequential document repre-
sentations and simplicial curves.",Acknowledgments,[0],[0]
UAI.,Acknowledgments,[0],[0]
"Lee, W. S. and Liu, B. 2003.",Acknowledgments,[0],[0]
"Learning with Posi-
tive and Unlabeled Examples Using Weighted Logistic Regression.",Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Li, H. 2011.",Acknowledgments,[0],[0]
Learning to Rank for Information Retrieval and Natural Language Processing.,Acknowledgments,[0],[0]
"Morgan & Claypool publishers.
",Acknowledgments,[0],[0]
"Li, K., Huang, H., Tian, S. and Xu, W. 2003.",Acknowledgments,[0],[0]
Improving One-class SVM for anomaly detection.,Acknowledgments,[0],[0]
Proc.,Acknowledgments,[0],[0]
"of the Second International conference on Machine Learning and Cybernetics, volume 5, pages 3077–3081.
",Acknowledgments,[0],[0]
"Li, X., Liu, B. and Ng.",Acknowledgments,[0],[0]
S.-K. 2010.,Acknowledgments,[0],[0]
Negative Training Data can be Harmful to Text Classification.,Acknowledgments,[0],[0]
"EMNLP.
",Acknowledgments,[0],[0]
"Liu, B, Dai, Y., Li, X., Lee, W-S. and Yu.",Acknowledgments,[0],[0]
P. 2003.,Acknowledgments,[0],[0]
Building text classifiers using positive and unlabeled examples.,Acknowledgments,[0],[0]
"ICDM.
Liu.",Acknowledgments,[0],[0]
T. 2011.,Acknowledgments,[0],[0]
Learning to Rank for Information Retrieval.,Acknowledgments,[0],[0]
"Springer.
",Acknowledgments,[0],[0]
"Luo, J., Ding, L., Pan, Z., Ni, G. and Hu, G. 2007.",Acknowledgments,[0],[0]
Research on cost-sensitive learning in one-class anomaly detection algorithms.,Acknowledgments,[0],[0]
"Autonomic and Trusted Computing, volume 4610 of Lecture Notes in Computer Science.
",Acknowledgments,[0],[0]
"Manevitz, L. and Yousef.",Acknowledgments,[0],[0]
M. 2001.,Acknowledgments,[0],[0]
One-class SVMs for document classification.,Acknowledgments,[0],[0]
"Journal of Machine Learning research.
",Acknowledgments,[0],[0]
"Manning, C. D., Prabhakar R., and Hinrich, S. 2008.",Acknowledgments,[0],[0]
Introduction to Information Retrieval.,Acknowledgments,[0],[0]
"Cambridge University Press.
",Acknowledgments,[0],[0]
"Qian, T. and Liu, B. 2013.",Acknowledgments,[0],[0]
Identifying Multiple Userids of the Same Author.,Acknowledgments,[0],[0]
"EMNLP.
",Acknowledgments,[0],[0]
"Ranzato, M. and Szummer, M. 2008.",Acknowledgments,[0],[0]
Semisupervised learning of compact document representations with deep networks.,Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Rocchio, J. 1971.",Acknowledgments,[0],[0]
Relevant feedback in information retrieval.,Acknowledgments,[0],[0]
In G. Salton (ed.).,Acknowledgments,[0],[0]
"The smart retrieval system: experiments in automatic document processing.
",Acknowledgments,[0],[0]
"Schölkopf, B., Williamson, R., Smola, A., Taylor, J. and Platt, J. 2000.",Acknowledgments,[0],[0]
Support vector method for novelty detection.,Acknowledgments,[0],[0]
"Neural Information Processing Systems, pages 582–588.
",Acknowledgments,[0],[0]
"Schölkopf, B., Platt, J., Shawe-Taylor, J., Smola, A. and Williamson, R. 1999.",Acknowledgments,[0],[0]
Estimating the support of a high-dimensional distribution.,Acknowledgments,[0],[0]
"Technical Report, Microsoft Research, MSRTR-99-87.
",Acknowledgments,[0],[0]
"Shimodaira, H. 2000.",Acknowledgments,[0],[0]
Improving predictive inference under covariate shift by weighting the log-likelihood function.,Acknowledgments,[0],[0]
"Journal of Statistical Planning and Inference, 90:227– 244.
Sugiyama, M. and Muller, K.-R. 2005.",Acknowledgments,[0],[0]
Inputdependent estimation of generalization error under covariate shift.,Acknowledgments,[0],[0]
"Statistics and Decision, 23(4):249–279.
Sugiyama, M., Nakajima, S., Kashima, H., von Bunau P., and Kawanabe M. 2008.",Acknowledgments,[0],[0]
Direct importance estimation with model selection and its application to covariate shift adaptation.,Acknowledgments,[0],[0]
"Advances in Neural Information Processing Systems.
Tax, D. and Duin, R. 1999a.",Acknowledgments,[0],[0]
Data domain description using support vectors.,Acknowledgments,[0],[0]
"Proceedings ESAN99, Brussels. 251-256
Tax, D. and Duin, R. 1999b.",Acknowledgments,[0],[0]
Support vector domain description.,Acknowledgments,[0],[0]
"Pattern Recognition Letters 20. 1191-1199
Tax, D. and Duin, R. 2001.",Acknowledgments,[0],[0]
Uniform object generation for optimizing one-class classifiers.,Acknowledgments,[0],[0]
"J. of Machine Learning Research, 2:155–173.
Tian, J. and Gu, H. 2010.",Acknowledgments,[0],[0]
Anomaly detection combining one-class SVMs and particle swarm optimization algorithms.,Acknowledgments,[0],[0]
"Nonlinear Dynamics, 61(1-2): 303–310.
",Acknowledgments,[0],[0]
"Tsuboi, J., Kashima, H., Hido, S., Bickel, S., and Sugiyama, M. 2008.",Acknowledgments,[0],[0]
Direct density ratio estimation for large-scale covariate shift adaptation.,Acknowledgments,[0],[0]
"Proceedings of the SIAM International Conference on Data Mining (SDM).
",Acknowledgments,[0],[0]
"Wang, P. and Domeniconi, C. 2008.",Acknowledgments,[0],[0]
"Building semantic kernels for text classification using Wikipedia, KDD.
Yang, Y. and Pedersen, J. O. 1997.",Acknowledgments,[0],[0]
A comparative study on feature selection in text categorization.,Acknowledgments,[0],[0]
"ICML.
",Acknowledgments,[0],[0]
"Yang, L., and Madden, M. 2007.",Acknowledgments,[0],[0]
One-class support vector machine calibration using particle swarm optimization.,Acknowledgments,[0],[0]
"AICS, Dublin.
",Acknowledgments,[0],[0]
"Yu, H., Han, J. and Chang, K. 2002.",Acknowledgments,[0],[0]
PEBL:,Acknowledgments,[0],[0]
"Positive example based learning for Web page classification using SVM. KDD, 239-248.
",Acknowledgments,[0],[0]
"Zadrozny, B. 2004.",Acknowledgments,[0],[0]
"Learning and evaluating classifiers under s ample selection bias, ICML.",Acknowledgments,[0],[0]
"In a typical social media content analysis task, the user is interested in analyzing posts of a particular topic.",abstractText,[0],[0]
Identifying such posts is often formulated as a classification problem.,abstractText,[0],[0]
"However, this problem is challenging.",abstractText,[0],[0]
One key issue is covariate shift.,abstractText,[0],[0]
"That is, the training data is not fully representative of the test data.",abstractText,[0],[0]
"We observed that the covariate shift mainly occurs in the negative data because topics discussed in social media are highly diverse and numerous, but the user-labeled negative training data may cover only a small number of topics.",abstractText,[0],[0]
This paper proposes a novel technique to solve the problem.,abstractText,[0],[0]
The key novelty of the technique is the transformation of document representation from the traditional ngram feature space to a center-based similarity (CBS) space.,abstractText,[0],[0]
"In the CBS space, the covariate shift problem is significantly mitigated, which enables us to build much better classifiers.",abstractText,[0],[0]
Experiment results show that the proposed approach markedly improves classification.,abstractText,[0],[0]
Social Media Text Classification under Negative Covariate Shift,title,[0],[0]
"Social media sites on the Internet provide increasingly more, and increasingly popular, means for people to voice their opinions on trending events.",1 Introduction,[0],[0]
"Traditional news media — the New York Times and CNN, for example — now provide online mechanisms that allow and encourage readers to share reactions, opinions, and personal experiences relevant to a news story.",1 Introduction,[0],[0]
"For complex emerging events, in particular, user comments can provide relevant, interesting and insightful information beyond the facts reported in the news.",1 Introduction,[0],[0]
"But their large volume and tremendous variation in quality make it impossible
for readers to efficiently digest the user-generated content, much less integrate it with reported facts from the dozens or hundreds of news reports produced on the event each day.
",1 Introduction,[0],[0]
"In this work, we present a socially-informed timeline generation system that jointly generates a news article summary and a user comment summary for each day of an ongoing complex event.",1 Introduction,[0],[0]
A sample (gold standard) timeline snippet for Ukraine Crisis is shown in Figure 1.,1 Introduction,[0],[0]
"The event timeline is on the left; the comment summary for March 17th is on the right.
",1 Introduction,[0],[0]
"ar X
iv :1
60 6.
05 69
9v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
7 Ju
n 20
While generating timelines from news articles and summarizing user comments have been studied as separate problems (Yan et al., 2011; Ma et al., 2012), their joint summarization for timeline generation raises new challenges.",1 Introduction,[0],[0]
"Firstly, there should be a tight connection between the article and comment portion of the timeline.",1 Introduction,[0],[0]
"By definition, users comment on socially relevant events.",1 Introduction,[0],[0]
So the important part of articles and insightful comments should both cover these events.,1 Introduction,[0],[0]
"Moreover, good reading experience requires that the article summary and comment summary demonstrate evident connectivity.",1 Introduction,[0],[0]
"For example, Comment C in Figure 1 (“Sanctions are effective and if done in unison with the EU”) is obscure without knowing the context that “sanctions are imposed by U.S”.",1 Introduction,[0],[0]
Simply combining the outputs from a timeline generation system and a comment summarization system may lead to timelines that lack cohesion.,1 Introduction,[0],[0]
"On the other hand, articles and comments are from intrinsically different genres of text: articles emphasize facts and are written in a professional style; comments reflect opinions in a less formal way.",1 Introduction,[0],[0]
"Thus, it could be difficult to recognize the connections between articles and comments.",1 Introduction,[0],[0]
"Finally, it is also challenging to enforce continuity in timelines with many entities and events.
",1 Introduction,[0],[0]
"To address the challenges mentioned above, we formulate the timeline generation task as an optimization problem, where we maximize topic cohesion between the article and comment summaries while preserving their ability to reflect important concepts and subevents, adequate coverage of mentioned topics, and continuity of the timeline as it is updated with new material each day.",1 Introduction,[0],[0]
We design a novel alternating optimizing algorithm that allows the generation of a high quality article summary and comment summary via mutual reinforcement.,1 Introduction,[0],[0]
"We demonstrate the effectiveness of our algorithm on four disparate complex event datasets collected over months from the New York Times, CNN, and BBC.",1 Introduction,[0],[0]
"Automatic evaluation using ROUGE (Lin and Hovy, 2003) and gold standard timelines indicates that our system can effectively leverage user comments to outperform state-of-the-art approaches on timeline generation.",1 Introduction,[0],[0]
"In a human evaluation via Amazon Mechanical Turk, the comment summaries generated by our method were selected as the best in terms of informativeness and insightfulness in 66.7% and
51.7% of the evaluations (vs. 26.7% and 30.0% for randomly selected editor’s-picks).
",1 Introduction,[0],[0]
"Especially, our optimization framework relies on two scoring functions that estimate the importance of including individual article sentences and user comments in the timeline.",1 Introduction,[0],[0]
"Based on the observation that entities or events frequently discussed in the user comments can help with identify summaryworthy content, we show that the scoring functions can be learned jointly by utilizing graph-based regularization.",1 Introduction,[0],[0]
Experiments show that our joint learning model outperforms state-of-the-art ranking algorithms and other joint learning based methods when evaluated on sentence ranking and comment ranking.,1 Introduction,[0],[0]
"For example, we achieve an NDCG@3 of 0.88 on the Ukraine crisis dataset, compared to 0.77 from Yang et al. (2011) which also conducts joint learning between articles and social context using factor graphs.
",1 Introduction,[0],[0]
"Finally, to encourage continuity in the generated timeline, we propose an entity-centered event threading algorithm.",1 Introduction,[0],[0]
Human evaluation demonstrates that users who read timelines with event threads write more informative answers than users who do not see the threads while answering the same questions.,1 Introduction,[0],[0]
"This implies that our system constructed threads can help users better navigate the timelines and collect relevant information in a short time.
",1 Introduction,[0],[0]
"For the rest of the paper, we first describe data collection (Section 2).",1 Introduction,[0],[0]
We then introduce the joint learning model for importance prediction (Section 3).,1 Introduction,[0],[0]
"The full timeline generation system is presented in Section 4, which is followed by evaluations (Section 5).",1 Introduction,[0],[0]
Related work and conclusion are in Sections 6 and 7.,1 Introduction,[0],[0]
"We crawled news articles from New York Times (NYT), CNN, and BBC on four trending events: the missing Malaysia Airlines Flight MH370 (MH370), the political unrest in Ukraine (Ukraine), the IsraelGaza conflict (Israel-Gaza), and the NSA surveillance leaks (NSA).",2 Data Collection and Preprocessing,[0],[0]
"For each event, we select a set of key words (usually entities’ name), which are used to filter out irrelevant articles.",2 Data Collection and Preprocessing,[0],[0]
"We collect comments for NYT articles through NYT community API, and comments for CNN articles via Disqus
API.",2 Data Collection and Preprocessing,[0],[0]
1 NYT comments come with information on whether a comment is an editor’s-pick.,2 Data Collection and Preprocessing,[0],[0]
"The statistics on the four datasets are displayed in Table 1.2
We extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP",2 Data Collection and Preprocessing,[0],[0]
"(Manning et al., 2014).",2 Data Collection and Preprocessing,[0],[0]
"Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012).
",2 Data Collection and Preprocessing,[0],[0]
We also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3).,2 Data Collection and Preprocessing,[0],[0]
"NYT2013 contains 3, 863 articles and 833, 032 comments.",2 Data Collection and Preprocessing,[0],[0]
"We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions — a SENTENCE scorer and a COMMENT scorer — that predict the importance of including an individual news article sentence or a particular user comment in the timeline.
",3 Joint Learning for Importance Scoring,[0],[0]
"We train the model on the aforementioned NYT2013 dataset, where 20% of the articles and their comments are reserved for parameter tuning.",3 Joint Learning for Importance Scoring,[0],[0]
"Formally, the training data consists of a set of articles D = {di}|D|−1i=0 .",3 Joint Learning for Importance Scoring,[0],[0]
"Each article di contains a set of sentences xsdi = {xsdi ,j} |sdi |−1 j=0 and a set of associated comments xcdi = {xcdi ,k} |cdi |−1 k=0 , where |sdi | and |cdi | are the numbers of sentences and comments for di.",3 Joint Learning for Importance Scoring,[0],[0]
"For simplicity, we use xs or xc to denote a sentence or a comment wherever there is no ambiguity.
",3 Joint Learning for Importance Scoring,[0],[0]
"In addition, each article has a human-written abstract.",3 Joint Learning for Importance Scoring,[0],[0]
"We use the ROUGE-2 (Lin and Hovy, 2003) score of each sentence computed against the associated abstract as its gold-standard importance score.
",3 Joint Learning for Importance Scoring,[0],[0]
"1BBC comment volume is low, so we do not collect it.",3 Joint Learning for Importance Scoring,[0],[0]
"2The datasets are available at http://www.cs.
cornell.edu/˜luwang/data.html.
",3 Joint Learning for Importance Scoring,[0],[0]
"Each comment is assigned a gold-standard value of 1.0 if it is an editor’s pick, or 0.0 otherwise.
",3 Joint Learning for Importance Scoring,[0],[0]
"The SENTENCE and COMMENT scorers rely on two classifiers, each designed to handle the special characteristics of news and user comments, respectively; and a graph-based regularizing constraint that encourages similarity between selected sentences and comments.",3 Joint Learning for Importance Scoring,[0],[0]
"We describe each component below.
",3 Joint Learning for Importance Scoring,[0],[0]
Article SENTENCE Importance.,3 Joint Learning for Importance Scoring,[0],[0]
Each sentence xs in a news article is represented as a k-dimensional feature vector xs ∈,3 Joint Learning for Importance Scoring,[0],[0]
"Rk, with a gold-standard label ys.",3 Joint Learning for Importance Scoring,[0],[0]
"We denote the training set as a feature matrix X̃s, with a label vector Ỹs.",3 Joint Learning for Importance Scoring,[0],[0]
To produce the SENTENCE scoring function fs(xs),3 Joint Learning for Importance Scoring,[0],[0]
"= xs · ws, we use ridge regression to learn a vector ws that minimizes ||X̃sws",3 Joint Learning for Importance Scoring,[0],[0]
− Ỹs||22 + βs · ||ws||22.,3 Joint Learning for Importance Scoring,[0],[0]
Features used in the model are listed in Table 2.,3 Joint Learning for Importance Scoring,[0],[0]
"We also impose the following position-based regularizing constraint to encode the fact that the first sentence in a news article usually conveys the most essential information: λs · ∑ di ∑ xsdi ,j
,j 6=0 ||(xsdi ,0",3 Joint Learning for Importance Scoring,[0],[0]
"− xsdi ,j) ·ws − (ysdi ,0",3 Joint Learning for Importance Scoring,[0],[0]
"− ysdi ,j)|| 2 2 , where xsdi ,j is the j-th sentence in document di.",3 Joint Learning for Importance Scoring,[0],[0]
"Term (xsdi ,0",3 Joint Learning for Importance Scoring,[0],[0]
"− xsdi ,j) · ws measures the difference in predicted scores between the first sentence and any other sentence.",3 Joint Learning for Importance Scoring,[0],[0]
This value is expected be close to the true difference.,3 Joint Learning for Importance Scoring,[0],[0]
"We further construct X̃′s to contain all difference vectors (xsdi ,0",3 Joint Learning for Importance Scoring,[0],[0]
"− xsdi ,j), with Ỹ′s as label difference vector.",3 Joint Learning for Importance Scoring,[0],[0]
"The objective function to minimize becomes
Js(ws) = ||X̃sws",3 Joint Learning for Importance Scoring,[0],[0]
"− Ỹs||22 + λs · ||X̃′sws − Ỹ′s||22 + βs · ||ws||22 (1)
User COMMENT Importance.",3 Joint Learning for Importance Scoring,[0],[0]
"Similarly, each comment xc is represented as an l−dimensional feature vector xc ∈",3 Joint Learning for Importance Scoring,[0],[0]
"Rl, with label yc.",3 Joint Learning for Importance Scoring,[0],[0]
Comments in the training data are denoted with a feature matrix X̃c with a label vector Ỹc.,3 Joint Learning for Importance Scoring,[0],[0]
"Likewise, we learn fc(xc) =",3 Joint Learning for Importance Scoring,[0],[0]
xc,3 Joint Learning for Importance Scoring,[0],[0]
·wc by minimizing ||X̃cwc,3 Joint Learning for Importance Scoring,[0],[0]
− Ỹc||22,3 Joint Learning for Importance Scoring,[0],[0]
+ βc · ||wc||22.,3 Joint Learning for Importance Scoring,[0],[0]
Features are listed in Table 3.,3 Joint Learning for Importance Scoring,[0],[0]
"We apply a pairwise preference-based regularizing constraint (Joachims, 2002) to incorporate a bias toward editor’s picks: λc · ∑ di ∑ xcdi ,j ∈Edi ,xcdi ,k /∈Edi ||(xcdi ,j−xcdi ,k) ·wc− 1||22 , where Edi are the editor’s picks for di.",3 Joint Learning for Importance Scoring,[0],[0]
"Term (xcdi ,j − xcdi ,k) ·",3 Joint Learning for Importance Scoring,[0],[0]
wc enforces the separation of editor’s picks from regular comments.,3 Joint Learning for Importance Scoring,[0],[0]
"We further construct X̃′c to contain all the pairwise differences
(xcdi ,j − xcdi ,k).",3 Joint Learning for Importance Scoring,[0],[0]
Ỹ ′,3 Joint Learning for Importance Scoring,[0],[0]
c is a vector of same size as X̃′c with each element as 1.,3 Joint Learning for Importance Scoring,[0],[0]
"Thus, the objective function to minimize is:
Jc(wc) = ||X̃cwc",3 Joint Learning for Importance Scoring,[0],[0]
− Ỹc||22 + λc · ||X̃′cwc − Ỹ′c||22,3 Joint Learning for Importance Scoring,[0],[0]
"+ βc · ||wc||22 (2)
Graph-Based Regularization.",3 Joint Learning for Importance Scoring,[0],[0]
The regularizing constraint is based on two mutually reinforcing hypotheses: (1) the importance of a sentence depends partially on the availability of sufficient insightful comments that touch on topics in the sentence; (2) the importance of a comment depends partially on whether it addresses notable events reported in the sentences.,3 Joint Learning for Importance Scoring,[0],[0]
"For example, we want our model to bias ws to predict a high score for a sentence with high similarity to numerous insightful comments.
",3 Joint Learning for Importance Scoring,[0],[0]
"We first create a bipartite graph from sentences and comments on the same articles, where edge weights are based on the content similarity between a sentence and a comment (TF-IDF similarity is used).",3 Joint Learning for Importance Scoring,[0],[0]
"Let R̃ be an N ×M adjacency matrix, where N and M are the numbers of sentences and comments.",3 Joint Learning for Importance Scoring,[0],[0]
Rsc is the similarity between sentence xs and comment xc.,3 Joint Learning for Importance Scoring,[0],[0]
"We normalize R̃ by Q̃ = D̃− 1 2 R̃D̃′
− 12 , where D̃ and D̃′ are diagonal matrices:",3 Joint Learning for Importance Scoring,[0],[0]
D̃ ∈,3 Joint Learning for Importance Scoring,[0],[0]
"RN×N , Di,i = ∑M j=1Ri,j ; D̃ ′ ∈ RM×M , D′j,j = ∑N i=1Ri,j .",3 Joint Learning for Importance Scoring,[0],[0]
"The interplay between the two types of data is encoded in the following regularizing constraint:
Js,c(ws,wc) = λsc · ∑ di ∑ xs∈xsdi ,xc∈xcdi",3 Joint Learning for Importance Scoring,[0],[0]
"Qxs,xc · (xs ·ws − xc ·wc)2
(3)
Full Objective Function.",3 Joint Learning for Importance Scoring,[0],[0]
"Thus, the full objective function consists of the three parts discussed above:
J(ws,wc) = Js(ws) + Jc(wc) +",3 Joint Learning for Importance Scoring,[0],[0]
"Js,c(ws,wc) (4)
",3 Joint Learning for Importance Scoring,[0],[0]
"Furthermore, using the following notation,
X̃ =
[ X̃s 0
0 X̃c
]",3 Joint Learning for Importance Scoring,[0],[0]
Ỹ =,3 Joint Learning for Importance Scoring,[0],[0]
[ Ỹs Ỹc ] X̃′ =,3 Joint Learning for Importance Scoring,[0],[0]
[ X̃′s 0 0,3 Joint Learning for Importance Scoring,[0],[0]
X̃′c ],3 Joint Learning for Importance Scoring,[0],[0]
"Ỹ′ = [ Ỹ′s Ỹ′c ]
β̃ =",3 Joint Learning for Importance Scoring,[0],[0]
[ βsIk 0 0 βcIl ],3 Joint Learning for Importance Scoring,[0],[0]
"λ̃ = [ λsI|X′s| 0
0 λcI|X′c|
]
L̃ =",3 Joint Learning for Importance Scoring,[0],[0]
"[ λscI|Xs| −λscQ̃ −λscQ̃T λscI|Xc| ] w = [ ws wc ]
we can show a closed form solution to Equation 4 as follows:
ŵ",3 Joint Learning for Importance Scoring,[0],[0]
"=
(X̃TL̃X̃ + X̃TX̃ + X̃′Tλ̃X̃′ + β̃)−1(X̃TỸ + X̃′Tλ̃Ỹ′)
(5)",3 Joint Learning for Importance Scoring,[0],[0]
Now we present an optimization framework for timeline generation.,4 Timeline Generation,[0],[0]
"Formally, for each day, our system takes as input a set of sentences Vs and a set of comments Vc to be summarized, and the (automatically generated) timeline T (represented as threads) for days prior to the current day.",4 Timeline Generation,[0],[0]
"It then identifies a subset S ⊆ Vs as the article summary and a subset C ⊆ Vc as the comment summary by maximizing the following function:
Z(S,C; T ) = Squal(S; T )+Cqual(C)+δX (S,C) (6)
where Squal(S; T ) measures the quality of the article summary S in the context of the historical timeline represented as event threads T ; Cqual(C) computes the quality of the comment summary C; and X (S,C) estimates the connectivity between S and C.
We solve this maximization problem using an alternating optimization algorithm which is outlined
in Section 4.4.",4 Timeline Generation,[0],[0]
"In general, we alternately search for a better article summary S with hill climbing search and a better comment summary C with FordFulkerson algorithm until convergence.
",4 Timeline Generation,[0],[0]
"In the rest of this section, we first describe an entity-centered event threading algorithm to construct event threads T which are used to boost article timeline continuity.",4 Timeline Generation,[0],[0]
"Then we explain how to compute Squal(S; T ) and Cqual(C) in Section 4.2, followed by X (S,C) in Section 4.3.",4 Timeline Generation,[0],[0]
We present an event threading process where each thread connects sequential events centered on a set of relevant entities.,4.1 Entity-Centered Event Threading,[0],[0]
"For instance, the following thread connects events about Obama’s action towards the annexation of Crimea by Russia: Day 1: Obama declared sanctions on Russian officials.",4.1 Entity-Centered Event Threading,[0],[0]
Day 2:,4.1 Entity-Centered Event Threading,[0],[0]
President Obama warned Russian.,4.1 Entity-Centered Event Threading,[0],[0]
Day 3: Obama urges Russian to move back its troops.,4.1 Entity-Centered Event Threading,[0],[0]
"Day 4: Obama condemns Russian aggression in Ukraine.
",4.1 Entity-Centered Event Threading,[0],[0]
"We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system.",4.1 Entity-Centered Event Threading,[0],[0]
We retain extractions with confidence scores higher than 0.5.,4.1 Entity-Centered Event Threading,[0],[0]
We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns.,4.1 Entity-Centered Event Threading,[0],[0]
"Each relation contains at least one event-related word (Ritter et al., 2012).
",4.1 Entity-Centered Event Threading,[0],[0]
"The entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day’s article summary either gets attached to an existing thread or starts a new thread.",4.1 Entity-Centered Event Threading,[0],[0]
The updated threads then become the input to next day’s summary generation process.,4.1 Entity-Centered Event Threading,[0],[0]
"On day n, we have a set of threads T = {τ : s1, s2, · · · , sn−1} constructed from previous n",4.1 Entity-Centered Event Threading,[0],[0]
"− 1 days, where si represents the set of sentences attached to thread τ from day i.",4.1 Entity-Centered Event Threading,[0],[0]
"The cohesion between a new sentence s ∈ S and a thread τ is denoted as cohn(s, τ).",4.1 Entity-Centered Event Threading,[0],[0]
"s is attached to τ̂ if there exists τ̂ = maxτ∈T cohn(s, τ) and cohn(s, τ̂)",4.1 Entity-Centered Event Threading,[0],[0]
> 0.0.,4.1 Entity-Centered Event Threading,[0],[0]
"Otherwise, s becomes a new thread.",4.1 Entity-Centered Event Threading,[0],[0]
"We define cohn(s, τ) = minsi∈τ,si 6=∅ tfsimi(si, s), where tfsimi(si, s) measures the TF similarity between si and s.",4.1 Entity-Centered Event Threading,[0],[0]
"We con-
sider unigrams/bigrams/trigrams generated from the entities of our event extractions.",4.1 Entity-Centered Event Threading,[0],[0]
"Recall that we learned two separate importance scoring functions for sentences and comments, which will be denoted here as imps(s) and impc(c).",4.2 Summary Quality Measurement,[0],[0]
"With an article summary S and threads T = {τi}, the article summary quality function Squal(S; T ) has the following form:
Squal(S; T ) =",4.2 Summary Quality Measurement,[0],[0]
"∑ s∈S imp(s)
+θcov ∑ s′∈Vs min( ∑ s∈S tfidf(s, s ′), α ∑ ŝ∈Vs tfidf(ŝ, s ′))
+ θcont ∑ τ∈T maxsk∈S cohn(sk, τ)
tfidf(·, ·) is the TF-IDF similarity function.",4.2 Summary Quality Measurement,[0],[0]
"Squal(S; T ) captures three desired qualities of an article summary: importance (first item), coverage (second item), and the continuity of the current summary to previously generated summaries.",4.2 Summary Quality Measurement,[0],[0]
"The coverage function has been used to encourage summary diversity and reduce redundancy (Lin and Bilmes, 2011; Wang et al., 2014).",4.2 Summary Quality Measurement,[0],[0]
"The continuity function considers how well article summary S can be attached to each event thread, thus favors summaries that can be connected to multiple threads.
",4.2 Summary Quality Measurement,[0],[0]
"Parameters θcov and α are tuned on multidocument summarization dataset DUC 2003 (Over and Yen, 2003).",4.2 Summary Quality Measurement,[0],[0]
Experiments show that system performance peaks and is stable for θcont ∈,4.2 Summary Quality Measurement,[0],[0]
"[1.0, 5.0].",4.2 Summary Quality Measurement,[0],[0]
We thus fix θcont to 1.0.,4.2 Summary Quality Measurement,[0],[0]
We discard sentences with more than 80% of content words covered by historical summaries.,4.2 Summary Quality Measurement,[0],[0]
We use BASIC to denote a system that only optimizes on importance and coverage (i.e. first two items in Squal(S; T )).,4.2 Summary Quality Measurement,[0],[0]
"The system optimizing Squal(S; T ) is henceforth called THREAD.
",4.2 Summary Quality Measurement,[0],[0]
The comment summary quality function simply takes the form Cqual(C) = ∑ c∈C impc(c).,4.2 Summary Quality Measurement,[0],[0]
"We encode two objectives in the connectivity function X (S,C): (1) encouraging topical cohesion (i.e. connectivity) between article summary and comment summary; and (2) favoring comments that cover diversified events.
",4.3 Connectivity Measurement,[0],[0]
"Let conn(s, c) measure content similarity between a sentence s ∈ S and a comment c ∈ C. Connectivity between article summary S and comment summary C is computed as follows.",4.3 Connectivity Measurement,[0],[0]
"We build a bipartite graph G between S and C with edge weight as conn(s, c).
",4.3 Connectivity Measurement,[0],[0]
"We then find an edge setM, the best matching of G. X (S,C) is defined as the sum over edge weights in M, i.e. X (S,C) = ∑ e∈M weight(e).",4.3 Connectivity Measurement,[0],[0]
"An example is illustrated in Figure 2.
",4.3 Connectivity Measurement,[0],[0]
"We consider two options for conn(s, c).",4.3 Connectivity Measurement,[0],[0]
One is lexical similarity which is based on TF-IDF vectors.,4.3 Connectivity Measurement,[0],[0]
Another is semantic similarity.,4.3 Connectivity Measurement,[0],[0]
"Let Rs = {(as, rs, bs)} and Rc = {(ac, rc, bc)} be the sets of dependency relations in s and c. conn(s, c) is calculated as:∑
(as,rs,bs)∈Rs max(ac,rc,bc)∈Rc rs=rc
simi(as, ac)× simi(bs, bc)
where simi(·, ·) is a word similarity function.",4.3 Connectivity Measurement,[0],[0]
"We experiment with shortest path based similarity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013).",4.3 Connectivity Measurement,[0],[0]
"Systems using the three metrics that optimize Z(S,C; T ) are henceforth called THREAD+OPTTFIDF, THREAD+OPTWordNet and THREAD+OPTWordVec.",4.3 Connectivity Measurement,[0],[0]
"To maximize the full objective function Z(S,C; T ), we design a novel alternating optimization algorithm (Alg. 1) where we alternately find better S and C.
We initialize S0 by a greedy algorithm (Lin and Bilmes, 2011) with respect to Squal(S; T ).",4.4 An Alternating Optimization Algorithm,[0],[0]
"Notice that Squal(S; T ) is a submodular function, so that the greedy solution is a 1 − 1/e approximation to the optimal solution of Squal(S; T ).",4.4 An Alternating Optimization Algorithm,[0],[0]
"Fixing S0, we model the problem of finding C0 that maximizes Cqual(C) + δX (S0, C) as a maximum-weight bipar-
tite graph matching problem.",4.4 An Alternating Optimization Algorithm,[0],[0]
"This problem can be reduced to a maximum network flow problem, and then be solved by Ford-Fulkerson algorithm (details are discussed in (Kleinberg and Tardos, 2005)).",4.4 An Alternating Optimization Algorithm,[0],[0]
"Thereafter, for each iteration, we alternately find a better St with regard to Squal(S; T ) + δX (S,Ct−1) using hill climbing, and an exact solution Ct to Cqual(C)+δX (St, C) with Ford-Fulkerson algorithm.",4.4 An Alternating Optimization Algorithm,[0],[0]
"Iteration stops when the increase of Z(S,C) is below threshold (set to 0.01).",4.4 An Alternating Optimization Algorithm,[0],[0]
System performance is stable when we vary δ ∈,4.4 An Alternating Optimization Algorithm,[0],[0]
"[1.0, 10.0], so we set δ = 1.0.
",4.4 An Alternating Optimization Algorithm,[0],[0]
"Input : sentences Vs, comments Vc, threads T , δ, threshold , functions Z(S,C; T ), Squal(S; T ), Cqual(C),",4.4 An Alternating Optimization Algorithm,[0],[0]
"X (S,C) Output: article summary S, comment summary C /* Initialize S and C by greedy algorithm and Ford-Fulkerson algorithm */ S0 ←maxS Squal(S; T ); C0 ← maxC Cqual(C) + δX (S0, C); t← 1; ∆Z ←∞; while ∆Z > do
/*",4.4 An Alternating Optimization Algorithm,[0],[0]
"Step 1: Hill climbing algorithm */ St ← maxS Squal(S; T ) + δX (S,Ct−1); /*",4.4 An Alternating Optimization Algorithm,[0],[0]
Step 2: Ford-Fulkerson algorithm */ Ct,4.4 An Alternating Optimization Algorithm,[0],[0]
"← maxC Cqual(C) + δX (St, C); ∆Z = Z(St, Ct; T )−Z(St−1, Ct−1; T ); t←",4.4 An Alternating Optimization Algorithm,[0],[0]
"t+ 1;
end Algorithm 1:",4.4 An Alternating Optimization Algorithm,[0],[0]
"Generate article summary and comment summary for a given day via alternating optimization .
",4.4 An Alternating Optimization Algorithm,[0],[0]
Algorithm 1 is guaranteed to find a solution at least as good as S0 and C0.,4.4 An Alternating Optimization Algorithm,[0],[0]
"It progresses only if Step 1 finds St that improves upon Z(St−1, Ct−1; T ), and Step 2 finds Ct where Z(St, Ct; T )",4.4 An Alternating Optimization Algorithm,[0],[0]
"≥ Z(St, Ct−1; T ).",4.4 An Alternating Optimization Algorithm,[0],[0]
"5.1 Evaluation of SENTENCE and COMMENT Importance Scorers
We test importance scorers (Section 3) on single document sentence ranking and comment ranking.
",5 Experimental Results,[0],[0]
"For both tasks, we compare with two previous systems on joint ranking and summarization of news articles and tweets.",5 Experimental Results,[0],[0]
Yang et al. (2011) employ supervised learning based on factor graphs to model content similarity between the two types of data.,5 Experimental Results,[0],[0]
We use the same features for this model.,5 Experimental Results,[0],[0]
"Gao et al. (2012) summarize by including the complementary information between articles and
tweets, which is estimated by an unsupervised topic model.3",5 Experimental Results,[0],[0]
"We also consider two state-of-the-art rankers: RankBoost (Freund et al., 2003) and LambdaMART (Burges, 2010).",5 Experimental Results,[0],[0]
"Finally, we use a position baseline that ranks sentences based on their position in the article, and a rating baseline that ranks comments based on positive user ratings.
",5 Experimental Results,[0],[0]
We evaluate using normalized discounted cumulative gain at top 3 returned results (NDCG@3).,5 Experimental Results,[0],[0]
"Sentences are considered relevant if they have ROUGE-2 scores larger than 0.0 (computed against human abstracts), and comments are considered relevant if they are editor’s picks.4 Figure 3 demonstrates that our joint learning model uniformly outperforms all the other comparisons for both ranking tasks.",5 Experimental Results,[0],[0]
"In general, supervised learning based approaches (e.g. our method, Yang et al. (2011), RankBoost, and LambdaMART) produce better results than unsupervised method (e.g. Gao et al. (2012)).5",5 Experimental Results,[0],[0]
"In this section, we test if our system can leverage comments to produce better article-based summaries for event timelines.",5.2 Leveraging User Comments,[0],[0]
"We collect gold-standard timelines for each of the four events from the corresponding Wikipedia page(s), NYT topic page, or BBC news page.
",5.2 Leveraging User Comments,[0],[0]
"We consider two existing timeline creation systems that only utilize news articles, and a timeline generated from single-article human abstracts: (1) CHIEU AND LEE (2004) select sentences with high
3We thank Zi Yang and Peng Li for providing the code.",5.2 Leveraging User Comments,[0],[0]
"4We experiment with all articles for sentence ranking, and
NYT comments (with editor’s picks) for comment ranking.",5.2 Leveraging User Comments,[0],[0]
"5Similar results are obtained with mean reciprocal rank.
",5.2 Leveraging User Comments,[0],[0]
“interestingness” and “burstiness” using a likelihood ratio test to compare word distributions of sentences with articles in neighboring days.,5.2 Leveraging User Comments,[0],[0]
(2) YAN ET AL.,5.2 Leveraging User Comments,[0],[0]
"(2011) design an evolutionary summarization system that selects sentences based on on coverage, coherence, and diversity.",5.2 Leveraging User Comments,[0],[0]
"(3) We construct a timeline from the human ABSTRACTs provided with each article: we sort them chronologically according to article timestamps and add abstract sentences into each daily summary until reaching the word limit.
",5.2 Leveraging User Comments,[0],[0]
We test on five variations of our system.,5.2 Leveraging User Comments,[0],[0]
The first two systems generate article summaries with no comment information by optimizing Squal(S; T ) using a greedy algorithm: BASIC ignores event threading; THREAD considers the threads.,5.2 Leveraging User Comments,[0],[0]
"THREAD+OPTTFIDF, THREAD+OPTWordNet and THREAD+OPTWordVec (see Section 4.3) leverage user comments to generate article summaries as well as comment summaries based on alternating optimization of Equation 3.",5.2 Leveraging User Comments,[0],[0]
"Although comment summaries are generated, they are not used in the evaluation.
",5.2 Leveraging User Comments,[0],[0]
"For all systems, we generate daily article summaries of at most 100 words, and select 5 comments for the corresponding comment summary.",5.2 Leveraging User Comments,[0],[0]
"We employ ROUGE (Lin and Hovy, 2003) to automatically evaluate the content coverage (in terms of ngrams) of the article-based timelines vs. goldstandard timelines.",5.2 Leveraging User Comments,[0],[0]
ROUGE-2 (measures bigram overlap) and ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) scores are reported in Table 4.,5.2 Leveraging User Comments,[0],[0]
"As can be seen, under the alternating optimization framework, our systems, employing both articles and comments, consistently yield better ROUGE scores than the three baseline systems and our systems that do not leverage comments.",5.2 Leveraging User Comments,[0],[0]
"Though constructed from single-article abstracts, baseline ABSTRACT is found to contain redundant information and thus limited in content coverage.",5.2 Leveraging User Comments,[0],[0]
This is due to the fact that different media tend to report on the same important events.,5.2 Leveraging User Comments,[0],[0]
We evaluate the full article+comment-based timelines on Amazon Mechanical Turk.,5.3 Evaluating Socially-Informed Timelines,[0],[0]
"Turkers are presented with a timeline consisting of five consecutive days’ article summaries and four variations of the accompanying comment summary:
RANDOMly selected comments, USER’S-PICKS (ranked by positive user ratings), randomly selected EDITOR’S-PICKS and timelines produced by the THREAD+OPTWordVec version of OUR SYSTEM.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
We also include one noisy comment summary (i.e. irrelevant to the question) to avoid spam.,5.3 Evaluating Socially-Informed Timelines,[0],[0]
"We display two comments per day for each system.6
Turkers are asked to rank the comment summary variations according to informativeness and insightfulness.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"For informativeness, we ask the Turkers to judge based only on knowledge displayed in the timeline, and to rate each comment summary based on how much relevant information they learn from it.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"For insightfulness, Turkers are required to focus on insights and valuable opinions.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"They are requested to leave a short explanation of their ranking.
15 five-day periods are randomly selected.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
We solicit four distinct Turkers located in the U.S. to evaluate each set of timelines.,5.3 Evaluating Socially-Informed Timelines,[0],[0]
"An inter-rater agreement of Krippendorff’s α of 0.63 is achieved for informativeness ranking and α is 0.50 for insightfulness ranking.
",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"Table 5 shows the percentage of times a particular method is selected as producing the best comment portion of the timeline, as well as the microaverage rank of each method, for both informativeness and insightfulness.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
Our system is selected as the best in 66.7% of the evaluations for informativeness and 51.7% for insightfulness.,5.3 Evaluating Socially-Informed Timelines,[0],[0]
"In both cases, we statistically significantly outperform (p < 0.05 using a Wilcoxon signed-rank test) the editor’s-picks
6For our system, we select the two comments with highest importance scores from the comment summary.
and user’s-picks.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"Turkers’ explanations indicate that they prefer our comment summaries mainly because they are “very informative and insightful to what was happening”, and “show the sharpness of the commenter”.",5.3 Evaluating Socially-Informed Timelines,[0],[0]
"Turkers sometimes think the summaries randomly selected from editor’s-picks “lack connection”, and characterize user’s-picks as “the information was somewhat limited”.
",5.3 Evaluating Socially-Informed Timelines,[0],[0]
Figure 4 shows part of the timeline generated by our system for the Ukraine crisis.,5.3 Evaluating Socially-Informed Timelines,[0],[0]
"Here we evaluate on the utility of event threads for high-level information access guidance: can event threads allow users to easily locate and absorb information with a specific interest in mind?
",5.4 Human Evaluation of Event Threading,[0],[0]
"We first sample a 10-day timeline for each dataset from those produced by the THREAD+OPTWordVec
variation of our system.",5.4 Human Evaluation of Event Threading,[0],[0]
We designed one question for each timeline.,5.4 Human Evaluation of Event Threading,[0],[0]
"Sample questions are: “describe the activities for searching for the missing flight MH370”, and “describe the attitude and action of Russian Government on Eastern Ukraine”.",5.4 Human Evaluation of Event Threading,[0],[0]
We recruited 10 undergraduate and graduate students who are native speakers of English.,5.4 Human Evaluation of Event Threading,[0],[0]
Each student first read one question and its corresponding timeline for 5 minutes.,5.4 Human Evaluation of Event Threading,[0],[0]
"The timeline was then removed, and the student wrote down an answer for the question.",5.4 Human Evaluation of Event Threading,[0],[0]
We asked each student to answer the question for each of four timelines (one for each event dataset).,5.4 Human Evaluation of Event Threading,[0],[0]
"Two timelines are displayed with threads, and two without threads.",5.4 Human Evaluation of Event Threading,[0],[0]
"We presented threads by adding a thread number in front of each sentence.
",5.4 Human Evaluation of Event Threading,[0],[0]
We then used Amazon Mechanical Turk to evaluate the informativeness of students’ answers.,5.4 Human Evaluation of Event Threading,[0],[0]
"Turkers were asked to read all 10 answers for the same question, with five answers based on timelines with threads and five others based on timelines without threads.",5.4 Human Evaluation of Event Threading,[0],[0]
"After that, they rated each answer with an informativeness score on a 1-to-5 rating scale (1 as “not relevant to the query”, and 5 as “very informative”).",5.4 Human Evaluation of Event Threading,[0],[0]
We also added two quality control questions.,5.4 Human Evaluation of Event Threading,[0],[0]
"Table 6 shows that the average rating for answers written after reading timelines with threads is 3.29 (43% are rated≥ 4), higher than the 2.58 for the timelines with no thread exhibited (30% are rated ≥ 4).",5.4 Human Evaluation of Event Threading,[0],[0]
There is a growing interest in generating article summaries informed by social context.,6 Related Work,[0],[0]
"Existing work focuses on learning users’ interests from comments and incorporates the learned information into a news article summarization system (Hu et al., 2008).",6 Related Work,[0],[0]
"Zhao et al. (2013) instead estimate word distributions from tweets, and bias a Page Rank algorithm to give higher restart probability to sentences with similar distributions.",6 Related Work,[0],[0]
"Generating tweet+article summaries has been recently investigated in Yang et
al. (2011).",6 Related Work,[0],[0]
They propose a factor graph to allow sentences and tweets to mutually reinforce each other.,6 Related Work,[0],[0]
Gao et al. (2012) exploit a co-ranking model to identify sentence-tweet pairs with complementary information estimated from a topic model.,6 Related Work,[0],[0]
"These efforts handle a small number of documents and tweets, while we target a larger scale of data.
",6 Related Work,[0],[0]
"In terms of timeline summarization, the Chieu and Lee (2004) system ranks sentences according to “burstiness” and “interestingness” estimated by a likelihood ratio test.",6 Related Work,[0],[0]
"Yan et al. (2011) explore an optimization framework that maximizes the relevance, coverage, diversity, and coherence of the timeline.",6 Related Work,[0],[0]
Neither system has leveraged the social context.,6 Related Work,[0],[0]
"Our event threading algorithm is also inspired by work on topic detection and tracking (TDT) (Allan et al., 1998), where efforts are made for document-level link detection and topic tracking.",6 Related Work,[0],[0]
"Similarly, Nallapati et al. (2004) investigate event threading for articles, where they predict linkage based on causal and temporal dependencies.",6 Related Work,[0],[0]
Shahaf et al. (2012) instead seek for connecting articles into one coherent graph.,6 Related Work,[0],[0]
"To the best of our knowledge, we are the first to study sentence-level event threading.",6 Related Work,[0],[0]
"We presented a socially-informed timeline generation system, which constructs timelines consisting of article summaries and comment summaries.",7 Conclusion,[0],[0]
An alternating optimization algorithm is designed to maximize the connectivity between the two sets of summaries as well as their importance and information coverage.,7 Conclusion,[0],[0]
Automatic and human evaluations showed that our system produced more informative timelines than state-of-the-art systems.,7 Conclusion,[0],[0]
Our comment summaries were also rated as very insightful.,7 Conclusion,[0],[0]
"We thank John Hessel, Lillian Lee, Moontae Lee, David Lutz, Karthik Raman, Vikram Rao, Yiye Ruan, Xanda Schofield, Adith Swaminathan, Chenhao Tan, Bishan Yang, other members of Cornell NLP group, and the NAACL reviewers for valuable suggestions and advice on various aspects of this work.",Acknowledgments,[0],[0]
This work was supported in part by DARPA DEFT Grant FA8750-13-2-0015.,Acknowledgments,[0],[0]
"Existing timeline generation systems for complex events consider only information from traditional media, ignoring the rich social context provided by user-generated content that reveals representative public interests or insightful opinions.",abstractText,[0],[0]
We instead aim to generate socially-informed timelines that contain both news article summaries and selected user comments.,abstractText,[0],[0]
We present an optimization framework designed to balance topical cohesion between the article and comment summaries along with their informativeness and coverage of the event.,abstractText,[0],[0]
Automatic evaluations on real-world datasets that cover four complex events show that our system produces more informative timelines than state-of-theart systems.,abstractText,[0],[0]
"In human evaluation, the associated comment summaries are furthermore rated more insightful than editor’s picks and comments ranked highly by users.",abstractText,[0],[0]
Socially-Informed Timeline Generation for Complex Events,title,[0],[0]
"Model-free deep reinforcement learning (RL) algorithms have been applied in a range of challenging domains, from games (Mnih et al., 2013; Silver et al., 2016) to robotic control (Schulman et al., 2015).",1. Introduction,[0],[0]
"The combination of RL and high-capacity function approximators such as neural
1Berkeley Artificial Intelligence Research, University of California, Berkeley, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Tuomas Haarnoja <haarnoja@berkeley.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
networks holds the promise of automating a wide range of decision making and control tasks, but widespread adoption of these methods in real-world domains has been hampered by two major challenges.",1. Introduction,[0],[0]
"First, model-free deep RL methods are notoriously expensive in terms of their sample complexity.",1. Introduction,[0],[0]
"Even relatively simple tasks can require millions of steps of data collection, and complex behaviors with highdimensional observations might need substantially more.",1. Introduction,[0],[0]
"Second, these methods are often brittle with respect to their hyperparameters: learning rates, exploration constants, and other settings must be set carefully for different problem settings to achieve good results.",1. Introduction,[0],[0]
"Both of these challenges severely limit the applicability of model-free deep RL to real-world tasks.
",1. Introduction,[0],[0]
"One cause for the poor sample efficiency of deep RL methods is on-policy learning: some of the most commonly used deep RL algorithms, such as TRPO (Schulman et al., 2015), PPO (Schulman et al., 2017b) or A3C (Mnih et al., 2016), require new samples to be collected for each gradient step.",1. Introduction,[0],[0]
"This quickly becomes extravagantly expensive, as the number of gradient steps and samples per step needed to learn an effective policy increases with task complexity.",1. Introduction,[0],[0]
Offpolicy algorithms aim to reuse past experience.,1. Introduction,[0],[0]
"This is not directly feasible with conventional policy gradient formulations, but is relatively straightforward for Q-learning based methods (Mnih et al., 2015).",1. Introduction,[0],[0]
"Unfortunately, the combination of off-policy learning and high-dimensional, nonlinear function approximation with neural networks presents a major challenge for stability and convergence (Bhatnagar et al., 2009).",1. Introduction,[0],[0]
"This challenge is further exacerbated in continuous state and action spaces, where a separate actor network is often used to perform the maximization in Q-learning.",1. Introduction,[0],[0]
"A commonly used algorithm in such settings, deep deterministic policy gradient (DDPG) (Lillicrap et al., 2015), provides for sample-efficient learning but is notoriously challenging to use due to its extreme brittleness and hyperparameter sensitivity (Duan et al., 2016; Henderson et al., 2017).
",1. Introduction,[0],[0]
We explore how to design an efficient and stable modelfree deep RL algorithm for continuous state and action spaces.,1. Introduction,[0],[0]
"To that end, we draw on the maximum entropy framework, which augments the standard maximum reward
reinforcement learning objective with an entropy maximization term (Ziebart et al., 2008; Toussaint, 2009; Rawlik et al., 2012; Fox et al., 2016; Haarnoja et al., 2017).",1. Introduction,[0],[0]
"Maximum entropy reinforcement learning alters the RL objective, though the original objective can be recovered using a temperature parameter (Haarnoja et al., 2017).",1. Introduction,[0],[0]
"More importantly, the maximum entropy formulation provides a substantial improvement in exploration and robustness: as discussed by Ziebart (2010), maximum entropy policies are robust in the face of model and estimation errors, and as demonstrated by (Haarnoja et al., 2017), they improve exploration by acquiring diverse behaviors.",1. Introduction,[0],[0]
"Prior work has proposed model-free deep RL algorithms that perform on-policy learning with entropy maximization (O’Donoghue et al., 2016), as well as off-policy methods based on soft Q-learning and its variants (Schulman et al., 2017a; Nachum et al., 2017a; Haarnoja et al., 2017).",1. Introduction,[0],[0]
"However, the on-policy variants suffer from poor sample complexity for the reasons discussed above, while the off-policy variants require complex approximate inference procedures in continuous action spaces.
",1. Introduction,[0],[0]
"In this paper, we demonstrate that we can devise an offpolicy maximum entropy actor-critic algorithm, which we call soft actor-critic (SAC), which provides for both sampleefficient learning and stability.",1. Introduction,[0],[0]
"This algorithm extends readily to very complex, high-dimensional tasks, such as the Humanoid benchmark (Duan et al., 2016) with 21 action dimensions, where off-policy methods such as DDPG typically struggle to obtain good results (Gu et al., 2016).",1. Introduction,[0],[0]
"SAC also avoids the complexity and potential instability associated with approximate inference in prior off-policy maximum entropy algorithms based on soft Q-learning (Haarnoja et al., 2017).",1. Introduction,[0],[0]
"We present a convergence proof for policy iteration in the maximum entropy framework, and then introduce a new algorithm based on an approximation to this procedure that can be practically implemented with deep neural networks, which we call soft actor-critic.",1. Introduction,[0],[0]
We present empirical results that show that soft actor-critic attains a substantial improvement in both performance and sample efficiency over both off-policy and on-policy prior methods.,1. Introduction,[0],[0]
"We also compare to twin delayed deep deterministic (TD3) policy gradient algorithm (Fujimoto et al., 2018), which is a concurrent work that proposes a deterministic algorithm that substantially improves on DDPG.",1. Introduction,[0],[0]
"Our soft actor-critic algorithm incorporates three key ingredients: an actor-critic architecture with separate policy and value function networks, an off-policy formulation that enables reuse of previously collected data for efficiency, and entropy maximization to enable stability and exploration.",2. Related Work,[0],[0]
We review prior works that draw on some of these ideas in this section.,2. Related Work,[0],[0]
"Actor-critic algorithms are typically derived
starting from policy iteration, which alternates between policy evaluation—computing the value function for a policy— and policy improvement—using the value function to obtain a better policy (Barto et al., 1983; Sutton & Barto, 1998).",2. Related Work,[0],[0]
"In large-scale reinforcement learning problems, it is typically impractical to run either of these steps to convergence, and instead the value function and policy are optimized jointly.",2. Related Work,[0],[0]
"In this case, the policy is referred to as the actor, and the value function as the critic.",2. Related Work,[0],[0]
"Many actor-critic algorithms build on the standard, on-policy policy gradient formulation to update the actor (Peters & Schaal, 2008), and many of them also consider the entropy of the policy, but instead of maximizing the entropy, they use it as an regularizer (Schulman et al., 2017b; 2015; Mnih et al., 2016; Gruslys et al., 2017).",2. Related Work,[0],[0]
"On-policy training tends to improve stability but results in poor sample complexity.
",2. Related Work,[0],[0]
"There have been efforts to increase the sample efficiency while retaining robustness by incorporating off-policy samples and by using higher order variance reduction techniques (O’Donoghue et al., 2016; Gu et al., 2016).",2. Related Work,[0],[0]
"However, fully off-policy algorithms still attain better efficiency.",2. Related Work,[0],[0]
"A particularly popular off-policy actor-critic method, DDPG (Lillicrap et al., 2015), which is a deep variant of the deterministic policy gradient (Silver et al., 2014) algorithm, uses a Q-function estimator to enable off-policy learning, and a deterministic actor that maximizes this Q-function.",2. Related Work,[0],[0]
"As such, this method can be viewed both as a deterministic actor-critic algorithm and an approximate Q-learning algorithm.",2. Related Work,[0],[0]
"Unfortunately, the interplay between the deterministic actor network and the Q-function typically makes DDPG extremely difficult to stabilize and brittle to hyperparameter settings (Duan et al., 2016; Henderson et al., 2017).",2. Related Work,[0],[0]
"As a consequence, it is difficult to extend DDPG to complex, high-dimensional tasks, and on-policy policy gradient methods still tend to produce the best results in such settings (Gu et al., 2016).",2. Related Work,[0],[0]
"Our method instead combines off-policy actorcritic training with a stochastic actor, and further aims to maximize the entropy of this actor with an entropy maximization objective.",2. Related Work,[0],[0]
"We find that this actually results in a considerably more stable and scalable algorithm that, in practice, exceeds both the efficiency and final performance of DDPG.",2. Related Work,[0],[0]
A similar method can be derived as a zero-step special case of stochastic value gradients (SVG(0)),2. Related Work,[0],[0]
"(Heess et al., 2015).",2. Related Work,[0],[0]
"However, SVG(0) differs from our method in that it optimizes the standard maximum expected return objective, and it does not make use of a separate value network, which we found to make training more stable.
",2. Related Work,[0],[0]
Maximum entropy reinforcement learning optimizes policies to maximize both the expected return and the expected entropy of the policy.,2. Related Work,[0],[0]
"This framework has been used in many contexts, from inverse reinforcement learning (Ziebart et al., 2008) to optimal control (Todorov, 2008; Toussaint, 2009; Rawlik et al., 2012).",2. Related Work,[0],[0]
"In guided policy
search (Levine & Koltun, 2013; Levine et al., 2016), the maximum entropy distribution is used to guide policy learning towards high-reward regions.",2. Related Work,[0],[0]
"More recently, several papers have noted the connection between Q-learning and policy gradient methods in the framework of maximum entropy learning (O’Donoghue et al., 2016; Haarnoja et al., 2017; Nachum et al., 2017a; Schulman et al., 2017a).",2. Related Work,[0],[0]
"While most of the prior model-free works assume a discrete action space, Nachum et al. (2017b) approximate the maximum entropy distribution with a Gaussian and Haarnoja et al. (2017) with a sampling network trained to draw samples from the optimal policy.",2. Related Work,[0],[0]
"Although the soft Q-learning algorithm proposed by Haarnoja et al. (2017) has a value function and actor network, it is not a true actor-critic algorithm: the Q-function is estimating the optimal Q-function, and the actor does not directly affect the Q-function except through the data distribution.",2. Related Work,[0],[0]
"Hence, Haarnoja et al. (2017) motivates the actor network as an approximate sampler, rather than the actor in an actor-critic algorithm.",2. Related Work,[0],[0]
"Crucially, the convergence of this method hinges on how well this sampler approximates the true posterior.",2. Related Work,[0],[0]
"In contrast, we prove that our method converges to the optimal policy from a given policy class, regardless of the policy parameterization.",2. Related Work,[0],[0]
"Furthermore, these prior maximum entropy methods generally do not exceed the performance of state-of-the-art off-policy algorithms, such as DDPG, when learning from scratch, though they may have other benefits, such as improved exploration and ease of fine-tuning.",2. Related Work,[0],[0]
"In our experiments, we demonstrate that our soft actor-critic algorithm does in fact exceed the performance of prior state-of-the-art off-policy deep RL methods by a wide margin.",2. Related Work,[0],[0]
We first introduce notation and summarize the standard and maximum entropy reinforcement learning frameworks.,3. Preliminaries,[0],[0]
We address policy learning in continuous action spaces.,3.1. Notation,[0],[0]
"We consider an infinite-horizon Markov decision process (MDP), defined by the tuple (S,A, p, r), where the state space S and the action space A are continuous, and the unknown state transition probability p : S × S × A → [0, ∞) represents the probability density of the next state st+1 ∈ S given the current state st ∈ S and action at ∈ A.",3.1. Notation,[0],[0]
The environment emits a bounded reward r : S ×,3.1. Notation,[0],[0]
"A → [rmin, rmax] on each transition.",3.1. Notation,[0],[0]
"We will use ρπ(st) and ρπ(st,at) to denote the state and state-action marginals of the trajectory distribution induced by a policy π(at|st).",3.1. Notation,[0],[0]
"Standard RL maximizes the expected sum of rewards∑ t E(st,at)∼ρπ",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"[r(st,at)].",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"We will consider a more gen-
eral maximum entropy objective (see e.g. Ziebart (2010)), which favors stochastic policies by augmenting the objective with the expected entropy of the policy over ρπ(st):
J(π) = T∑ t=0 E(st,at)∼ρπ",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"[r(st,at) + αH(π( · |st))] .",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"(1)
The temperature parameter α determines the relative importance of the entropy term against the reward, and thus controls the stochasticity of the optimal policy.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"The maximum entropy objective differs from the standard maximum expected reward objective used in conventional reinforcement learning, though the conventional objective can be recovered in the limit as α→ 0.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"For the rest of this paper, we will omit writing the temperature explicitly, as it can always be subsumed into the reward by scaling it by α−1.
",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
This objective has a number of conceptual and practical advantages.,3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"First, the policy is incentivized to explore more widely, while giving up on clearly unpromising avenues.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"Second, the policy can capture multiple modes of nearoptimal behavior.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"In problem settings where multiple actions seem equally attractive, the policy will commit equal probability mass to those actions.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"Lastly, prior work has observed improved exploration with this objective (Haarnoja et al., 2017; Schulman et al., 2017a), and in our experiments, we observe that it considerably improves learning speed over state-of-art methods that optimize the conventional RL objective function.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
We can extend the objective to infinite horizon problems by introducing a discount factor γ to ensure that the sum of expected rewards and entropies is finite.,3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"Writing down the maximum entropy objective for the infinite horizon discounted case is more involved (Thomas, 2014) and is deferred to Appendix A.
Prior methods have proposed directly solving for the optimal Q-function, from which the optimal policy can be recovered (Ziebart et al., 2008; Fox et al., 2016; Haarnoja et al., 2017).",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"We will discuss how we can devise a soft actor-critic algorithm through a policy iteration formulation, where we instead evaluate the Q-function of the current policy and update the policy through an off-policy gradient update.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
"Though such algorithms have previously been proposed for conventional reinforcement learning, our method is, to our knowledge, the first off-policy actor-critic method in the maximum entropy reinforcement learning framework.",3.2. Maximum Entropy Reinforcement Learning,[0],[0]
Our off-policy soft actor-critic algorithm can be derived starting from a maximum entropy variant of the policy iteration method.,4. From Soft Policy Iteration to Soft Actor-Critic,[0],[0]
"We will first present this derivation, verify that the corresponding algorithm converges to the optimal policy from its density class, and then present a practical
deep reinforcement learning algorithm based on this theory.",4. From Soft Policy Iteration to Soft Actor-Critic,[0],[0]
"We will begin by deriving soft policy iteration, a general algorithm for learning optimal maximum entropy policies that alternates between policy evaluation and policy improvement in the maximum entropy framework.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"Our derivation is based on a tabular setting, to enable theoretical analysis and convergence guarantees, and we extend this method into the general continuous setting in the next section.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"We will show that soft policy iteration converges to the optimal policy within a set of policies which might correspond, for instance, to a set of parameterized densities.
",4.1. Derivation of Soft Policy Iteration,[0],[0]
"In the policy evaluation step of soft policy iteration, we wish to compute the value of a policy π according to the maximum entropy objective in Equation 1.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"For a fixed policy, the soft Q-value can be computed iteratively, starting from any function Q : S ×A → R and repeatedly applying a modified Bellman backup operator T π given by T πQ(st,at) , r(st,at) +",4.1. Derivation of Soft Policy Iteration,[0],[0]
"γ Est+1∼p [V (st+1)] , (2)
where
V (st) = Eat∼π",4.1. Derivation of Soft Policy Iteration,[0],[0]
"[Q(st,at)− log π(at|st)]",4.1. Derivation of Soft Policy Iteration,[0],[0]
(3) is the soft state value function.,4.1. Derivation of Soft Policy Iteration,[0],[0]
We can obtain the soft value function for any policy π by repeatedly applying T π as formalized below.,4.1. Derivation of Soft Policy Iteration,[0],[0]
Lemma 1 (Soft Policy Evaluation).,4.1. Derivation of Soft Policy Iteration,[0],[0]
"Consider the soft Bellman backup operator T π in Equation 2 and a mapping Q0 : S×A → R with |A| <∞, and defineQk+1 = T πQk.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"Then the sequence Qk will converge to the soft Q-value of π as k →∞.
Proof.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"See Appendix B.1.
",4.1. Derivation of Soft Policy Iteration,[0],[0]
"In the policy improvement step, we update the policy towards the exponential of the new Q-function.",4.1. Derivation of Soft Policy Iteration,[0],[0]
This particular choice of update can be guaranteed to result in an improved policy in terms of its soft value.,4.1. Derivation of Soft Policy Iteration,[0],[0]
"Since in practice we prefer policies that are tractable, we will additionally restrict the policy to some set of policies Π, which can correspond, for example, to a parameterized family of distributions such as Gaussians.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"To account for the constraint that π ∈ Π, we project the improved policy into the desired set of policies.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"While in principle we could choose any projection, it will turn out to be convenient to use the information projection defined in terms of the Kullback-Leibler divergence.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"In the other words, in the policy improvement step, for each state, we update the policy according to
πnew = arg min π′∈Π DKL
( π′( · |st) ∥∥∥∥",4.1. Derivation of Soft Policy Iteration,[0],[0]
"exp (Qπold(st, · ))Zπold(st) ) .
",4.1. Derivation of Soft Policy Iteration,[0],[0]
"(4)
The partition function Zπold(st) normalizes the distribution, and while it is intractable in general, it does not contribute to the gradient with respect to the new policy and can thus be ignored, as noted in the next section.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"For this projection, we can show that the new, projected policy has a higher value than the old policy with respect to the objective in Equation 1.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"We formalize this result in Lemma 2.
",4.1. Derivation of Soft Policy Iteration,[0],[0]
Lemma 2 (Soft Policy Improvement).,4.1. Derivation of Soft Policy Iteration,[0],[0]
Let πold ∈ Π and let πnew be the optimizer of the minimization problem defined in Equation 4.,4.1. Derivation of Soft Policy Iteration,[0],[0]
"Then Qπnew(st,at) ≥ Qπold(st,at) for all (st,at) ∈ S ×A with |A| <∞.
Proof.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"See Appendix B.2.
",4.1. Derivation of Soft Policy Iteration,[0],[0]
"The full soft policy iteration algorithm alternates between the soft policy evaluation and the soft policy improvement steps, and it will provably converge to the optimal maximum entropy policy among the policies in Π (Theorem 1).",4.1. Derivation of Soft Policy Iteration,[0],[0]
"Although this algorithm will provably find the optimal solution, we can perform it in its exact form only in the tabular case.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"Therefore, we will next approximate the algorithm for continuous domains, where we need to rely on a function approximator to represent the Q-values, and running the two steps until convergence would be computationally too expensive.",4.1. Derivation of Soft Policy Iteration,[0],[0]
"The approximation gives rise to a new practical algorithm, called soft actor-critic.
Theorem 1 (Soft Policy Iteration).",4.1. Derivation of Soft Policy Iteration,[0],[0]
"Repeated application of soft policy evaluation and soft policy improvement from any π ∈ Π converges to a policy π∗ such that Qπ∗(st,at) ≥ Qπ(st,at) for all π ∈ Π and (st,at) ∈ S × A, assuming |A| <∞.
Proof.",4.1. Derivation of Soft Policy Iteration,[0],[0]
See Appendix B.3.,4.1. Derivation of Soft Policy Iteration,[0],[0]
"As discussed above, large continuous domains require us to derive a practical approximation to soft policy iteration.",4.2. Soft Actor-Critic,[0],[0]
"To that end, we will use function approximators for both the Q-function and the policy, and instead of running evaluation and improvement to convergence, alternate between optimizing both networks with stochastic gradient descent.",4.2. Soft Actor-Critic,[0],[0]
"We will consider a parameterized state value function Vψ(st), soft Q-function Qθ(st,at), and a tractable policy πφ(at|st).",4.2. Soft Actor-Critic,[0],[0]
"The parameters of these networks are ψ, θ, and φ.",4.2. Soft Actor-Critic,[0],[0]
"For example, the value functions can be modeled as expressive neural networks, and the policy as a Gaussian with mean and covariance given by neural networks.",4.2. Soft Actor-Critic,[0],[0]
"We will next derive update rules for these parameter vectors.
",4.2. Soft Actor-Critic,[0],[0]
The state value function approximates the soft value.,4.2. Soft Actor-Critic,[0],[0]
"There is no need in principle to include a separate function approximator for the state value, since it is related to the Q-function and policy according to Equation 3.",4.2. Soft Actor-Critic,[0],[0]
"This quantity can be
estimated from a single action sample from the current policy without introducing a bias, but in practice, including a separate function approximator for the soft value can stabilize training and is convenient to train simultaneously with the other networks.",4.2. Soft Actor-Critic,[0],[0]
"The soft value function is trained to minimize the squared residual error
JV (ψ) = Est∼D",4.2. Soft Actor-Critic,[0],[0]
"[ 1 2 ( Vψ(st)− Eat∼πφ [Qθ(st,at)− log πφ(at|st)] )",4.2. Soft Actor-Critic,[0],[0]
"2] (5)
where D is the distribution of previously sampled states and actions, or a replay buffer.",4.2. Soft Actor-Critic,[0],[0]
"The gradient of Equation 5 can be estimated with an unbiased estimator
∇̂ψJV (ψ) = ∇ψVψ(st) (Vψ(st)−Qθ(st,at) + log πφ(at|st)) , (6)
where the actions are sampled according to the current policy, instead of the replay buffer.",4.2. Soft Actor-Critic,[0],[0]
"The soft Q-function parameters can be trained to minimize the soft Bellman residual
JQ(θ) =",4.2. Soft Actor-Critic,[0],[0]
"E(st,at)∼D [ 1
2
( Qθ(st,at)− Q̂(st,at) )2] ,
(7)
with Q̂(st,at) = r(st,at) + γ Est+1∼p [ Vψ̄(st+1) ] , (8)
which again can be optimized with stochastic gradients
∇̂θJQ(θ) = ∇θQθ(at, st) ( Qθ(st,at)− r(st,at)− γVψ̄(st+1) ) .
",4.2. Soft Actor-Critic,[0],[0]
"(9)
The update makes use of a target value network Vψ̄, where ψ̄ can be an exponentially moving average of the value network weights, which has been shown to stabilize training (Mnih et al., 2015).",4.2. Soft Actor-Critic,[0],[0]
"Alternatively, we can update the target weights to match the current value function weights periodically (see Appendix E).",4.2. Soft Actor-Critic,[0],[0]
"Finally, the policy parameters can be learned by directly minimizing the expected KL-divergence in Equation 4:
Jπ(φ) = Est∼D",4.2. Soft Actor-Critic,[0],[0]
"[ DKL ( πφ( · |st) ∥∥∥∥ exp (Qθ(st, · ))Zθ(st) )] .
(10)
",4.2. Soft Actor-Critic,[0],[0]
There are several options for minimizing Jπ.,4.2. Soft Actor-Critic,[0],[0]
"A typical solution for policy gradient methods is to use the likelihood ratio gradient estimator (Williams, 1992), which does not require backpropagating the gradient through the policy and the target density networks.",4.2. Soft Actor-Critic,[0],[0]
"However, in our case, the target density is the Q-function, which is represented by a neural network an can be differentiated, and it is thus convenient to apply the reparameterization trick instead, resulting in a lower variance estimator.",4.2. Soft Actor-Critic,[0],[0]
"To that end, we reparameterize the policy using a neural network transformation
at = fφ",4.2. Soft Actor-Critic,[0],[0]
"( t; st), (11)
Algorithm 1 Soft Actor-Critic Initialize parameter vectors ψ, ψ̄, θ, φ.",4.2. Soft Actor-Critic,[0],[0]
"for each iteration do
for each environment step do at ∼ πφ(at|st) st+1 ∼ p(st+1|st,at)",4.2. Soft Actor-Critic,[0],[0]
"D ← D ∪ {(st,at, r(st,at), st+1)} end for for each gradient step do ψ",4.2. Soft Actor-Critic,[0],[0]
← ψ,4.2. Soft Actor-Critic,[0],[0]
− λV ∇̂ψJV (ψ) θi ← θi,4.2. Soft Actor-Critic,[0],[0]
"− λQ∇̂θiJQ(θi) for i ∈ {1, 2} φ← φ− λπ∇̂φJπ(φ) ψ̄ ← τψ",4.2. Soft Actor-Critic,[0],[0]
+,4.2. Soft Actor-Critic,[0],[0]
"(1− τ)ψ̄
end for end for
where t is an input noise vector, sampled from some fixed distribution, such as a spherical Gaussian.",4.2. Soft Actor-Critic,[0],[0]
"We can now rewrite the objective in Equation 10 as
Jπ(φ) =",4.2. Soft Actor-Critic,[0],[0]
"Est∼D, t∼N",4.2. Soft Actor-Critic,[0],[0]
"[log πφ(fφ( t; st)|st)−Qθ(st, fφ( t; st))] , (12)
where πφ is defined implicitly in terms of fφ, and we have noted that the partition function is independent of φ and can thus be omitted.",4.2. Soft Actor-Critic,[0],[0]
"We can approximate the gradient of Equation 12 with
∇̂φJπ(φ) = ∇φ log πφ(at|st) +",4.2. Soft Actor-Critic,[0],[0]
"(∇at log πφ(at|st)−∇atQ(st,at))∇φfφ( t; st),
(13)
where at is evaluated at fφ( t; st).",4.2. Soft Actor-Critic,[0],[0]
"This unbiased gradient estimator extends the DDPG style policy gradients (Lillicrap et al., 2015) to any tractable stochastic policy.
",4.2. Soft Actor-Critic,[0],[0]
"Our algorithm also makes use of two Q-functions to mitigate positive bias in the policy improvement step that is known to degrade performance of value based methods (Hasselt, 2010; Fujimoto et al., 2018).",4.2. Soft Actor-Critic,[0],[0]
"In particular, we parameterize two Qfunctions, with parameters θi, and train them independently to optimize JQ(θi).",4.2. Soft Actor-Critic,[0],[0]
"We then use the minimum of the the Q-functions for the value gradient in Equation 6 and policy gradient in Equation 13, as proposed by Fujimoto et al. (2018).",4.2. Soft Actor-Critic,[0],[0]
"Although our algorithm can learn challenging tasks, including a 21-dimensional Humanoid, using just a single Q-function, we found two Q-functions significantly speed up training, especially on harder tasks.",4.2. Soft Actor-Critic,[0],[0]
The complete algorithm is described in Algorithm 1.,4.2. Soft Actor-Critic,[0],[0]
The method alternates between collecting experience from the environment with the current policy and updating the function approximators using the stochastic gradients from batches sampled from a replay buffer.,4.2. Soft Actor-Critic,[0],[0]
"In practice, we take a single environment step followed by one or several gradient steps (see Appendix D
for all hyperparameter).",4.2. Soft Actor-Critic,[0],[0]
Using off-policy data from a replay buffer is feasible because both value estimators and the policy can be trained entirely on off-policy data.,4.2. Soft Actor-Critic,[0],[0]
"The algorithm is agnostic to the parameterization of the policy, as long as it can be evaluated for any arbitrary state-action tuple.",4.2. Soft Actor-Critic,[0],[0]
The goal of our experimental evaluation is to understand how the sample complexity and stability of our method compares with prior off-policy and on-policy deep reinforcement learning algorithms.,5. Experiments,[0],[0]
"We compare our method to prior techniques on a range of challenging continuous control tasks from the OpenAI gym benchmark suite (Brockman et al., 2016) and also on the rllab implementation of the Humanoid task (Duan et al., 2016).",5. Experiments,[0],[0]
"Although the easier tasks can be solved by a wide range of different algorithms, the more complex benchmarks, such as the 21-dimensional Humanoid (rllab), are exceptionally difficult to solve with off-policy algorithms (Duan et al., 2016).",5. Experiments,[0],[0]
"The stability of the algorithm also plays a large role in performance: easier tasks make it more practical to tune hyperparameters to achieve good results, while the already narrow basins of effective hyperparameters become prohibitively small for the more sensitive algorithms on the hardest benchmarks, leading to poor performance (Gu et al., 2016).
",5. Experiments,[0],[0]
"We compare our method to deep deterministic policy gra-
dient (DDPG) (Lillicrap et al., 2015), an algorithm that is regarded as one of the more efficient off-policy deep RL methods (Duan et al., 2016); proximal policy optimization (PPO) (Schulman et al., 2017b), a stable and effective on-policy policy gradient algorithm; and soft Q-learning (SQL) (Haarnoja et al., 2017), a recent off-policy algorithm for learning maximum entropy policies.",5. Experiments,[0],[0]
"Our SQL implementation also includes two Q-functions, which we found to improve its performance in most environments.",5. Experiments,[0],[0]
"We additionally compare to twin delayed deep deterministic policy gradient algorithm (TD3) (Fujimoto et al., 2018), using the author-provided implementation.",5. Experiments,[0],[0]
"This is an extension to DDPG, proposed concurrently to our method, that first applied the double Q-learning trick to continuous control along with other improvements.",5. Experiments,[0],[0]
"We have included trust region path consistency learning (Trust-PCL) (Nachum et al., 2017b) and two other variants of SAC in Appendix E. We turned off the exploration noise for evaluation for DDPG and PPO.",5. Experiments,[0],[0]
"For maximum entropy algorithms, which do not explicitly inject exploration noise, we either evaluated with the exploration noise (SQL) or use the mean action (SAC).",5. Experiments,[0],[0]
The source code of our SAC implementation1 and videos2 are available online.,5. Experiments,[0],[0]
1github.com/haarnoja/sac 2sites.google.com/view/soft-actor-critic,5. Experiments,[0],[0]
"Figure 1 shows the total average return of evaluation rollouts during training for DDPG, PPO, and TD3.",5.1. Comparative Evaluation,[0],[0]
"We train five different instances of each algorithm with different random seeds, with each performing one evaluation rollout every 1000 environment steps.",5.1. Comparative Evaluation,[0],[0]
"The solid curves corresponds to the mean and the shaded region to the minimum and maximum returns over the five trials.
",5.1. Comparative Evaluation,[0],[0]
"The results show that, overall, SAC performs comparably to the baseline methods on the easier tasks and outperforms them on the harder tasks with a large margin, both in terms of learning speed and the final performance.",5.1. Comparative Evaluation,[0],[0]
"For example, DDPG fails to make any progress on Ant-v1, Humanoidv1, and Humanoid (rllab), a result that is corroborated by prior work (Gu et al., 2016; Duan et al., 2016).",5.1. Comparative Evaluation,[0],[0]
SAC also learns considerably faster than PPO as a consequence of the large batch sizes PPO needs to learn stably on more high-dimensional and complex tasks.,5.1. Comparative Evaluation,[0],[0]
"Another maximum entropy RL algorithm, SQL, can also learn all tasks, but it is slower than SAC and has worse asymptotic performance.",5.1. Comparative Evaluation,[0],[0]
"The quantitative results attained by SAC in our experiments also compare very favorably to results reported by other methods in prior work (Duan et al., 2016; Gu et al., 2016; Henderson et al., 2017), indicating that both the sample efficiency and final performance of SAC on these benchmark tasks exceeds the state of the art.",5.1. Comparative Evaluation,[0],[0]
All hyperparameters used in this experiment for SAC are listed in Appendix D.,5.1. Comparative Evaluation,[0],[0]
The results in the previous section suggest that algorithms based on the maximum entropy principle can outperform conventional RL methods on challenging tasks such as the humanoid tasks.,5.2. Ablation Study,[0],[0]
"In this section, we further examine which particular components of SAC are important for good performance.",5.2. Ablation Study,[0],[0]
"We also examine how sensitive SAC is to some of the most important hyperparameters, namely reward scaling and target value update smoothing constant.
",5.2. Ablation Study,[0],[0]
Stochastic vs. deterministic policy.,5.2. Ablation Study,[0],[0]
Soft actor-critic learns stochastic policies via a maximum entropy objective.,5.2. Ablation Study,[0],[0]
The entropy appears in both the policy and value function.,5.2. Ablation Study,[0],[0]
"In the policy, it prevents premature convergence of the policy variance (Equation 10).",5.2. Ablation Study,[0],[0]
"In the value function, it encourages exploration by increasing the value of regions of state space that lead to high-entropy behavior (Equation 5).",5.2. Ablation Study,[0],[0]
"To compare how the stochasticity of the policy and entropy maximization affects the performance, we compare to a deterministic variant of SAC that does not maximize the entropy and that closely resembles DDPG, with the exception of having two Q-functions, using hard target updates, not having a separate target actor, and using fixed rather than learned exploration noise.",5.2. Ablation Study,[0],[0]
"Figure 2 compares five individual runs with both variants, initialized with different random
seeds.",5.2. Ablation Study,[0],[0]
"Soft actor-critic performs much more consistently, while the deterministic variant exhibits very high variability across seeds, indicating substantially worse stability.",5.2. Ablation Study,[0],[0]
"As evident from the figure, learning a stochastic policy with entropy maximization can drastically stabilize training.",5.2. Ablation Study,[0],[0]
"This becomes especially important with harder tasks, where tuning hyperparameters is challenging.",5.2. Ablation Study,[0],[0]
"In this comparison, we updated the target value network weights with hard updates, by periodically overwriting the target network parameters to match the current value network (see Appendix E for a comparison of average performance on all benchmark tasks).
",5.2. Ablation Study,[0],[0]
Policy evaluation.,5.2. Ablation Study,[0],[0]
"Since SAC converges to stochastic policies, it is often beneficial to make the final policy deterministic at the end for best performance.",5.2. Ablation Study,[0],[0]
"For evaluation, we approximate the maximum a posteriori action by choosing the mean of the policy distribution.",5.2. Ablation Study,[0],[0]
Figure 3(a) compares training returns to evaluation returns obtained with this strategy indicating that deterministic evaluation can yield better performance.,5.2. Ablation Study,[0],[0]
"It should be noted that all of the training curves depict the sum of rewards, which is different from the objective optimized by SAC and other maximum entropy RL algorithms, including SQL and Trust-PCL, which maximize also the entropy of the policy.
",5.2. Ablation Study,[0],[0]
Reward scale.,5.2. Ablation Study,[0],[0]
"Soft actor-critic is particularly sensitive to the scaling of the reward signal, because it serves the role of the temperature of the energy-based optimal policy and thus controls its stochasticity.",5.2. Ablation Study,[0],[0]
Larger reward magnitudes correspond to lower entries.,5.2. Ablation Study,[0],[0]
"Figure 3(b) shows how learning performance changes when the reward scale is varied: For small reward magnitudes, the policy becomes nearly uniform, and consequently fails to exploit the reward signal, resulting in substantial degradation of performance.",5.2. Ablation Study,[0],[0]
"For large reward magnitudes, the model learns quickly at first,
but the policy then becomes nearly deterministic, leading to poor local minima due to lack of adequate exploration.",5.2. Ablation Study,[0],[0]
"With the right reward scaling, the model balances exploration and exploitation, leading to faster learning and better asymptotic performance.",5.2. Ablation Study,[0],[0]
"In practice, we found reward scale to be the only hyperparameter that requires tuning, and its natural interpretation as the inverse of the temperature in the maximum entropy framework provides good intuition for how to adjust this parameter.
",5.2. Ablation Study,[0],[0]
Target network update.,5.2. Ablation Study,[0],[0]
It is common to use a separate target value network that slowly tracks the actual value function to improve stability.,5.2. Ablation Study,[0],[0]
"We use an exponentially moving average, with a smoothing constant τ , to update the target value network weights as common in the prior work (Lillicrap et al., 2015; Mnih et al., 2015).",5.2. Ablation Study,[0],[0]
A value of one corresponds to a hard update where the weights are copied directly at every iteration and zero to not updating the target at all.,5.2. Ablation Study,[0],[0]
"In Figure 3(c), we compare the performance of SAC when τ varies.",5.2. Ablation Study,[0],[0]
Large τ can lead to instabilities while small τ can make training slower.,5.2. Ablation Study,[0],[0]
"However, we found the range of suitable values of τ to be relatively wide and we used the same value (0.005) across all of the tasks.",5.2. Ablation Study,[0],[0]
"In Figure 4 (Appendix E) we also compare to another variant of SAC, where instead of using exponentially moving average, we copy over the current network weights directly into the target network every 1000 gradient steps.",5.2. Ablation Study,[0],[0]
"We found this variant to benefit from taking more than one gradient step between the environment steps, which can improve performance but also increases the computational cost.",5.2. Ablation Study,[0],[0]
"We present soft actor-critic (SAC), an off-policy maximum entropy deep reinforcement learning algorithm that provides sample-efficient learning while retaining the benefits of entropy maximization and stability.",6. Conclusion,[0],[0]
"Our theoretical results derive soft policy iteration, which we show to converge to the optimal policy.",6. Conclusion,[0],[0]
"From this result, we can formulate a soft actor-critic algorithm, and we empirically show that it outperforms state-of-the-art model-free deep RL methods, including the off-policy DDPG algorithm and the on-policy PPO algorithm.",6. Conclusion,[0],[0]
"In fact, the sample efficiency of this approach actually exceeds that of DDPG by a substantial margin.",6. Conclusion,[0],[0]
"Our results suggest that stochastic, entropy maximizing reinforcement learning algorithms can provide a promising avenue for improved robustness and stability, and further exploration of maximum entropy methods, including methods that incorporate second order information (e.g., trust regions (Schulman et al., 2015)) or more expressive policy classes is an exciting avenue for future work.",6. Conclusion,[0],[0]
We would like to thank Vitchyr Pong for insightful discussions and help in implementing our algorithm as well as providing the DDPG baseline code; Ofir Nachum for offering support in running Trust-PCL experiments; and George Tucker for his valuable feedback on an early version of this paper.,Acknowledgments,[0],[0]
This work was supported by Siemens and Berkeley DeepDrive.,Acknowledgments,[0],[0]
Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks.,abstractText,[0],[0]
"However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning.",abstractText,[0],[0]
"Both of these challenges severely limit the applicability of such methods to complex, real-world domains.",abstractText,[0],[0]
"In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework.",abstractText,[0],[0]
"In this framework, the actor aims to maximize expected reward while also maximizing entropy.",abstractText,[0],[0]
"That is, to succeed at the task while acting as randomly as possible.",abstractText,[0],[0]
Prior deep RL methods based on this framework have been formulated as Q-learning methods.,abstractText,[0],[0]
"By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods.",abstractText,[0],[0]
"Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.",abstractText,[0],[0]
Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,title,[0],[0]
"loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy. Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension. To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming. Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs. We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity). We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011). Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.",text,[0],[0]
"The goal of supervised learning is to learn a mapping that links an input to an output objects, using examples of such pairs.",1. Introduction,[0],[0]
"This task is noticeably more difficult when the output objects have a structure, i.e. when they are not vectors (Bakir et al., 2007).",1. Introduction,[0],[0]
"We study here the case where each output object is a time series, namely a family of observations indexed by time.",1. Introduction,[0],[0]
"While it is tempting to treat time as yet another feature, and handle time series of vectors as the concatenation of all these vectors, several practical
1CREST, ENSAE, Université Paris-Saclay, France 2NTT Communication Science Laboratories, Seika-cho, Kyoto, Japan.",1. Introduction,[0],[0]
"Correspondence to: Marco Cuturi <marco.cuturi@ensae.fr>, Mathieu Blondel <mathieu@mblondel.org>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Input Output
Figure 1.",1. Introduction,[0],[0]
"Given the first part of a time series, we trained two multi-layer perceptron (MLP) to predict the entire second part.",1. Introduction,[0],[0]
"Using the ShapesAll dataset, we used a Euclidean loss for the first MLP and the soft-DTW loss proposed in this paper for the second one.",1. Introduction,[0],[0]
We display above the prediction obtained for a given test instance with either of these two MLPs in addition to the ground truth.,1. Introduction,[0],[0]
"Oftentimes, we observe that the soft-DTW loss enables us to better predict sharp changes.",1. Introduction,[0],[0]
"More time series predictions are given in Appendix F.
issues arise when taking this simplistic approach: Timeindexed phenomena can often be stretched in some areas along the time axis (a word uttered in a slightly slower pace than usual) with no impact on their characteristics; varying sampling conditions may mean they have different lengths; time series may not synchronized.
",1. Introduction,[0],[0]
The DTW paradigm.,1. Introduction,[0],[0]
"Generative models for time series are usually built having the invariances above in mind: Such properties are typically handled through latent variables and/or Markovian assumptions (Lütkepohl, 2005, Part I,§18).",1. Introduction,[0],[0]
"A simpler approach, motivated by geometry, lies in the direct definition of a discrepancy between time series that encodes these invariances, such as the Dynamic Time Warping (DTW) score (Sakoe & Chiba, 1971; 1978).",1. Introduction,[0],[0]
"DTW computes the best possible alignment between two time series (the optimal alignment itself can also be of interest, see e.g. Garreau et al. 2014) of respective length n and m by computing first the n×m pairwise distance matrix between these points to solve then a dynamic program (DP) using Bellman’s recursion with a quadratic (nm) cost.
",1. Introduction,[0],[0]
The DTW geometry.,1. Introduction,[0],[0]
"Because it encodes efficiently a useful class of invariances, DTW has often been used in a discriminative framework (with a k-NN or SVM classifier) to predict a real or a class label output, and engineered to run
faster in that context (Yi et al., 1998).",1. Introduction,[0],[0]
"Recent works by Petitjean et al. (2011); Petitjean & Gançarski (2012) have, however, shown that DTW can be used for more innovative tasks, such as time series averaging using the DTW discrepancy (see Schultz & Jain 2017 for a gentle introduction to these ideas).",1. Introduction,[0],[0]
"More generally, the idea of synthetising time series centroids can be regarded as a first attempt to output entire time series using DTW as a fitting loss.",1. Introduction,[0],[0]
"From a computational perspective, these approaches are, however, hampered by the fact that DTW is not differentiable and unstable when used in an optimization pipeline.
",1. Introduction,[0],[0]
Soft-DTW.,1. Introduction,[0],[0]
"In parallel to these developments, several authors have considered smoothed modifications of Bellman’s recursion to define smoothed DP distances (Bahl & Jelinek, 1975; Ristad & Yianilos, 1998) or kernels (Saigo et al., 2004; Cuturi et al., 2007).",1. Introduction,[0],[0]
"When applied to the DTW discrepancy, that regularization results in a soft-DTW score, which considers the soft-minimum of the distribution of all costs spanned by all possible alignments between two time series.",1. Introduction,[0],[0]
"Despite considering all alignments and not just the optimal one, soft-DTW can be computed with a minor modification of Bellman’s recursion, in which all (min,+) operations are replaced with (+,×).",1. Introduction,[0],[0]
"As a result, both DTW and soft-DTW have quadratic in time & linear in space complexity with respect to the sequences’ lengths.",1. Introduction,[0],[0]
"Because soft-DTW can be used with kernel machines, one typically observes an increase in performance when using soft-DTW over DTW (Cuturi, 2011) for classification.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"We explore in this paper another important benefit of smoothing DTW: unlike the original DTW discrepancy, soft-DTW is differentiable in all of its arguments.",1. Introduction,[0],[0]
"We show that the gradients of soft-DTW w.r.t to all of its variables can be computed as a by-product of the computation of the discrepancy itself, with an added quadratic storage cost.",1. Introduction,[0],[0]
"We use this fact to propose an alternative approach to the DBA (DTW Barycenter Averaging) clustering algorithm of (Petitjean et al., 2011), and observe that our smoothed approach significantly outperforms known baselines for that task.",1. Introduction,[0],[0]
"More generally, we propose to use soft-DTW as a fitting term to compare the output of a machine synthesizing a time series segment with a ground truth observation, in the same way that, for instance, a regularized Wasserstein distance was used to compute barycenters (Cuturi & Doucet, 2014), and later to fit discriminators that output histograms (Zhang et al., 2015; Rolet et al., 2016).",1. Introduction,[0],[0]
"When paired with a flexible learning architecture such as a neural network, soft-DTW allows for a differentiable end-to-end approach to design predictive and generative models for time series, as illustrated in Figure 1.",1. Introduction,[0],[0]
"Source code is available at https: //github.com/mblondel/soft-dtw.
Structure.",1. Introduction,[0],[0]
"After providing background material, we show
in §2 how soft-DTW can be differentiated w.r.t the locations of two time series.",1. Introduction,[0],[0]
"We follow in §3 by illustrating how these results can be directly used for tasks that require to output time series: averaging, clustering and prediction of time series.",1. Introduction,[0],[0]
"We close this paper with experimental results in §4 that showcase each of these potential applications.
Notations.",1. Introduction,[0],[0]
We consider in what follows multivariate discrete time series of varying length taking values in Ω ⊂,1. Introduction,[0],[0]
Rp.,1. Introduction,[0],[0]
A time series can be thus represented as a matrix of p lines and varying number of columns.,1. Introduction,[0],[0]
We consider a differentiable substitution-cost function δ,1. Introduction,[0],[0]
:,1. Introduction,[0],[0]
"Rp × Rp → R+ which will be, in most cases, the quadratic Euclidean distance between two vectors.",1. Introduction,[0],[0]
"For an integer n we write JnK for the set {1, . . .",1. Introduction,[0],[0]
", n} of integers.",1. Introduction,[0],[0]
Given two series’ lengths n,1. Introduction,[0],[0]
"and m, we write An,m ⊂ {0, 1} n×m for the set of (binary) alignment matrices, that is paths on a n×m matrix that connect the upper-left (1, 1) matrix entry to the lower-right (n,m) one using only ↓,→,ց moves.",1. Introduction,[0],[0]
"The cardinal of An,m is known as the delannoy(n−1,m−1) number; that number grows exponentially with m and n.",1. Introduction,[0],[0]
"We propose in this section a unified formulation for the original DTW discrepancy (Sakoe & Chiba, 1978) and the Global Alignment kernel (GAK) (Cuturi et al., 2007), which can be both used to compare two time series x = (x1, . . .",2. The DTW and soft-DTW loss functions,[0],[0]
", xn) ∈ R p×n and y = (y1, . .",2. The DTW and soft-DTW loss functions,[0],[0]
.,2. The DTW and soft-DTW loss functions,[0],[0]
", ym) ∈ R p×m.",2. The DTW and soft-DTW loss functions,[0],[0]
"Given the cost matrix ∆(x,y) := [ δ(xi, yj) ]
ij ∈ Rn×m,
the inner product 〈A,∆(x,y) 〉 of that matrix with an alignment matrix A in An,m gives the score of A, as illustrated in Figure 2.",2.1. Alignment costs: optimality and sum,[0],[0]
"Both DTW and GAK consider the costs of all possible alignment matrices, yet do so differently:
DTW(x,y) := min A∈An,m 〈A,∆(x,y) 〉,
kγGA(x,y) := ∑
A∈An,m
e−〈A,∆(x,y) 〉/γ .",2.1. Alignment costs: optimality and sum,[0],[0]
"(1)
DP Recursion.",2.1. Alignment costs: optimality and sum,[0],[0]
Sakoe & Chiba (1978) showed that the Bellman equation (1952) can be used to compute DTW.,2.1. Alignment costs: optimality and sum,[0],[0]
"That recursion, which appears in line 5 of Algorithm 1 (disregarding for now the exponent γ), only involves (min,+) operations.",2.1. Alignment costs: optimality and sum,[0],[0]
"When considering kernel kγGA and, instead, its integration over all alignments (see e.g. Lasserre 2009), Cuturi et al. (2007, Theorem 2) and the highly related formulation of Saigo et al. (2004, p.1685) use an old algorithmic appraoch (Bahl & Jelinek, 1975) which consists in (i) replacing all costs by their neg-exponential; (ii) replace (min,+) operations with (+,×) operations.",2.1. Alignment costs: optimality and sum,[0],[0]
"These two recursions can be in fact unified with the use of a soft-
minimum operator, which we present below.
",2.1. Alignment costs: optimality and sum,[0],[0]
Unified algorithm Both formulas in Eq.,2.1. Alignment costs: optimality and sum,[0],[0]
(1) can be computed with a single algorithm.,2.1. Alignment costs: optimality and sum,[0],[0]
That formulation is new to our knowledge.,2.1. Alignment costs: optimality and sum,[0],[0]
"Consider the following generalized min operator, with a smoothing parameter γ ≥ 0:
minγ{a1, . . .",2.1. Alignment costs: optimality and sum,[0],[0]
", an} :=
{
mini≤n ai, γ = 0, −γ log ∑n
i=1",2.1. Alignment costs: optimality and sum,[0],[0]
"e −ai/γ , γ > 0.
",2.1. Alignment costs: optimality and sum,[0],[0]
"With that operator, we can define γ-soft-DTW:
dtwγ(x,y) := min γ{〈A,∆(x,y) 〉, A ∈ An,m}.
",2.1. Alignment costs: optimality and sum,[0],[0]
The original DTW score is recovered by setting γ to 0.,2.1. Alignment costs: optimality and sum,[0],[0]
"When γ > 0, we recover dtwγ = −γ log k γ GA.",2.1. Alignment costs: optimality and sum,[0],[0]
"Most importantly, and in either case, dtwγ can be computed using Algorithm 1, which requires (nm) operations and (nm) storage cost as well .",2.1. Alignment costs: optimality and sum,[0],[0]
"That cost can be reduced to 2n with a more careful implementation if one only seeks to compute dtwγ(x,y), but the backward pass we consider next requires the entire matrix R of intermediary alignment costs.",2.1. Alignment costs: optimality and sum,[0],[0]
"Note that, to ensure numerical stability, the operator minγ must be computed using the usual log-sum-exp stabilization trick, namely that log ∑
i e zi =
(maxj zj) + log ∑",2.1. Alignment costs: optimality and sum,[0],[0]
i e,2.1. Alignment costs: optimality and sum,[0],[0]
zi−maxj zj .,2.1. Alignment costs: optimality and sum,[0],[0]
"A small variation in the input x causes a small change in dtw0(x,y) or dtwγ(x,y).",2.2. Differentiation of soft-DTW,[0],[0]
"When considering dtw0, that change can be efficiently monitored only when the optimal alignment matrix A⋆ that arises when computing dtw0(x,y) in Eq.",2.2. Differentiation of soft-DTW,[0],[0]
(1) is unique.,2.2. Differentiation of soft-DTW,[0],[0]
"As the minimum over a finite set of linear functions of ∆, dtw0 is therefore locally differentiable w.r.t.",2.2. Differentiation of soft-DTW,[0],[0]
"the cost matrix ∆, with gradient A⋆, a fact that has been exploited in all algorithms designed to
Algorithm 1 Forward recursion to compute dtwγ(x,y) and intermediate alignment costs
1: Inputs: x,y, smoothing γ ≥ 0, distance function δ 2: r0,0 = 0; ri,0 = r0,j = ∞; i ∈ JnK, j ∈ JmK 3: for j = 1, . . .",2.2. Differentiation of soft-DTW,[0],[0]
",m do 4: for i = 1, . . .",2.2. Differentiation of soft-DTW,[0],[0]
", n",2.2. Differentiation of soft-DTW,[0],[0]
"do 5: ri,j = δ(xi, yj) +min
γ{ri−1,j−1, ri−1,j , ri,j−1} 6: end for 7: end for 8: Output: (rn,m, R)
average time series under the DTW metric (Petitjean et al., 2011; Schultz & Jain, 2017).",2.2. Differentiation of soft-DTW,[0],[0]
"To recover the gradient of dtw0(x,y) w.r.t.",2.2. Differentiation of soft-DTW,[0],[0]
"x, we only need to apply the chain rule, thanks to the differentiability of the cost function:
∇xdtw0(x,y) =
( ∂∆(x,y)
∂x
)T
A⋆, (2)
where ∂∆(x,y)/∂x is the Jacobian of ∆ w.r.t.",2.2. Differentiation of soft-DTW,[0],[0]
"x, a linear map from Rp×n to Rn×m.",2.2. Differentiation of soft-DTW,[0],[0]
"When δ is the squared Euclidean distance, the transpose of that Jacobian applied to a matrix B ∈ Rn×m is (◦ being the elementwise product):
(∂∆(x,y)/∂x)TB = 2 ( (1p1 T mB T ) ◦ x− yBT ) .
",2.2. Differentiation of soft-DTW,[0],[0]
"With continuous data, A⋆ is almost always likely to be unique, and therefore the gradient in Eq.",2.2. Differentiation of soft-DTW,[0],[0]
(2) will be defined almost everywhere.,2.2. Differentiation of soft-DTW,[0],[0]
"However, that gradient, when it exists, will be discontinuous around those values x where a small change in x causes a change in A⋆, which is likely to hamper the performance of gradient descent methods.
",2.2. Differentiation of soft-DTW,[0],[0]
The case γ > 0.,2.2. Differentiation of soft-DTW,[0],[0]
"An immediate advantage of soft-DTW is that it can be explicitly differentiated, a fact that was also noticed by Saigo et al. (2006) in the related case of edit distances.",2.2. Differentiation of soft-DTW,[0],[0]
"When γ > 0, the gradient of Eq. (1) is obtained via the chain rule,
∇x dtwγ(x,y) =
( ∂∆(x,y)
∂x
)T
",2.2. Differentiation of soft-DTW,[0],[0]
Eγ,2.2. Differentiation of soft-DTW,[0],[0]
"[A], (3)
where Eγ [A] := 1
kγGA(x,y)
∑
A∈An,m
e−〈A,∆(x,y)/γ 〉A,
is the average alignment matrix A under the Gibbs distribution pγ ∝ e −〈A,∆(x,y) 〉/γ defined on all alignments in An",2.2. Differentiation of soft-DTW,[0],[0]
",m. The kernel k γ GA(x,y) can thus be interpreted as the normalization constant of pγ .",2.2. Differentiation of soft-DTW,[0],[0]
"Of course, since An,m has exponential size in n and m, a naive summation is not tractable.",2.2. Differentiation of soft-DTW,[0],[0]
Although a Bellman recursion to compute that average alignment matrix Eγ [A] exists (see Appendix A) that computation has quartic (n2m2) complexity.,2.2. Differentiation of soft-DTW,[0],[0]
"Note that
this stands in stark contrast to the quadratic complexity obtained by Saigo et al. (2006) for edit-distances, which is due to the fact the sequences they consider can only take values in a finite alphabet.",2.2. Differentiation of soft-DTW,[0],[0]
"To compute the gradient of soft-DTW, we propose instead an algorithm that manages to remain quadratic (nm) in terms of complexity.",2.2. Differentiation of soft-DTW,[0],[0]
"The key to achieve this reduction is to apply the chain rule in reverse order of Bellman’s recursion given in Algorithm 1, namely backpropagate.",2.2. Differentiation of soft-DTW,[0],[0]
"A similar idea was recently used to compute the gradient of ANOVA kernels in (Blondel et al., 2016).",2.2. Differentiation of soft-DTW,[0],[0]
"Differentiating algorithmically dtwγ(x,y) requires doing first a forward pass of Bellman’s equation to store all intermediary computations and recover R =",2.3. Algorithmic differentiation,[0],[0]
"[ri,j ] when running Algorithm 1.",2.3. Algorithmic differentiation,[0],[0]
"The value of dtwγ(x,y)—stored in rn,m at the end of the forward recursion—is then impacted by a change in ri,j exclusively through the terms in which ri,j plays a role, namely the triplet of terms ri+1,j , ri,j+1, ri+1,j+1.",2.3. Algorithmic differentiation,[0],[0]
"A straightforward application of the chain rule then gives
∂rn,m ∂ri,j ︸ ︷︷ ︸ ei,j = ∂rn,m ∂ri+1,j ︸ ︷︷ ︸ ei+1,j ∂ri+1,j ∂ri,j + ∂rn,m ∂ri,j+1 ︸ ︷︷ ︸ ei,j+1 ∂ri,j+1",2.3. Algorithmic differentiation,[0],[0]
"∂ri,j + ∂rn,m ∂ri+1,j+1 ︸ ︷︷ ︸ ei+1,j+1 ∂ri+1,j+1 ∂ri,j ,
in which we have defined the notation of the main object of interest of the backward recursion: ei,j := ∂rn,m ∂ri,j .",2.3. Algorithmic differentiation,[0],[0]
"The Bellman recursion evaluated at (i+1, j) as shown in line 5 of Algorithm 1 (here δi+1,j is δ(xi+1, yj)) yields :
",2.3. Algorithmic differentiation,[0],[0]
"ri+1,j = δi+1,j +min γ{ri,j−1, ri,j , ri+1,j−1},
which, when differentiated w.r.t ri,j yields the ratio:
∂ri+1,j ∂ri,j = e−ri,j/γ/
( e−ri,j−1/γ + e−ri,j/γ + e−ri+1,j−1/γ ) .
",2.3. Algorithmic differentiation,[0],[0]
"The logarithm of that derivative can be conveniently cast using evaluations of minγ computed in the forward loop:
γ log ∂ri+1,j",2.3. Algorithmic differentiation,[0],[0]
"∂ri,j = minγ{ri,j−1, ri,j , ri+1,j−1} − ri,j
= ri+1,j − δi+1,j − ri,j .
",2.3. Algorithmic differentiation,[0],[0]
"Similarly, the following relationships can also be obtained:
γ log ∂ri,j+1
∂ri,j = ri,j+1 − ri,j − δi,j+1,
γ log ∂ri+1,j+1
∂ri,j = ri+1,j+1 − ri,j − δi+1,j+1.
",2.3. Algorithmic differentiation,[0],[0]
We have therefore obtained a backward recursion to compute the entire matrix E =,2.3. Algorithmic differentiation,[0],[0]
"[ei,j ], starting from en,m = ∂rn,m ∂rn,m = 1 down to e1,1.",2.3. Algorithmic differentiation,[0],[0]
"To obtain ∇x dtwγ(x,y), notice that the derivatives w.r.t.",2.3. Algorithmic differentiation,[0],[0]
"the entries of the cost matrix ∆ can be computed by
∂rn,m ∂δi,j = ∂rn,m ∂ri,j ∂ri,j ∂δi,j = ei,j · 1 = ei,j ,
and therefore we have that
∇x dtwγ(x,y) =
( ∂∆(x,y)
∂x
)T
E,
where E is exactly the average alignment",2.3. Algorithmic differentiation,[0],[0]
Eγ [A] in Eq.,2.3. Algorithmic differentiation,[0],[0]
(3).,2.3. Algorithmic differentiation,[0],[0]
"These computations are summarized in Algorithm 2, which, once ∆ has been computed, has complexity nm in time and space.",2.3. Algorithmic differentiation,[0],[0]
"Because minγ has a 1/γ-Lipschitz continuous gradient, the gradient of dtwγ is 2/γ-Lipschitz continuous when δ is the squared Euclidean distance.
",2.3. Algorithmic differentiation,[0],[0]
"Algorithm 2 Backward recursion to compute ∇x dtwγ(x,y)
1: Inputs: x,y, smoothing γ ≥ 0, distance function δ 2: (·, R) =",2.3. Algorithmic differentiation,[0],[0]
"dtwγ(x,y), ∆ =",2.3. Algorithmic differentiation,[0],[0]
"[δ(xi, yj)]i,j 3: δi,m+1 = δn+1,j = 0, i ∈ JnK, j ∈ JmK",2.3. Algorithmic differentiation,[0],[0]
"4: ei,m+1 = en+1,j = 0, i ∈ JnK, j ∈ JmK 5: ri,m+1 = rn+1,j = −∞, i ∈ JnK, j ∈ JmK 6: δn+1,m+1 = 0, en+1,m+1 = 1, rn+1,m+1 = rn,m 7: for j = m, . . .",2.3. Algorithmic differentiation,[0],[0]
", 1 do 8: for i = n, . . .",2.3. Algorithmic differentiation,[0],[0]
", 1 do 9: a = exp 1γ (ri+1,j − ri,j − δi+1,j)
10: b = exp 1γ (ri,j+1 − ri,j − δi,j+1) 11: c = exp 1γ (ri+1,j+1 − ri,j − δi+1,j+1) 12: ei,j = ei+1,j · a+ ei,j+1 · b+ ei+1,j+1 · c 13: end for 14: end for 15: Output: ∇x dtwγ(x,y) =",2.3. Algorithmic differentiation,[0],[0]
"( ∂∆(x,y) ∂x )T E",2.3. Algorithmic differentiation,[0],[0]
We study in this section a direct application of Algorithm 2 to the problem of computing Fréchet means (1948) of time series with respect to the dtwγ discrepancy.,3.1. Averaging with the soft-DTW geometry,[0],[0]
"Given a family of N times series y1, . . .",3.1. Averaging with the soft-DTW geometry,[0],[0]
",yN , namely N matrices of p lines and varying number of columns, m1, . .",3.1. Averaging with the soft-DTW geometry,[0],[0]
.,3.1. Averaging with the soft-DTW geometry,[0],[0]
",mN , we are interested in defining a single barycenter time series x for that family under a set of normalized weights λ1, . . .",3.1. Averaging with the soft-DTW geometry,[0],[0]
", λN",3.1. Averaging with the soft-DTW geometry,[0],[0]
"∈ R+ such that ∑N
i=1",3.1. Averaging with the soft-DTW geometry,[0],[0]
λi,3.1. Averaging with the soft-DTW geometry,[0],[0]
= 1,3.1. Averaging with the soft-DTW geometry,[0],[0]
.,3.1. Averaging with the soft-DTW geometry,[0],[0]
"Our goal is thus to solve approximately the following problem, in which we have assumed that x has fixed length n:
min x∈Rp×n
N∑
i=1
λi mi dtwγ(x,yi).",3.1. Averaging with the soft-DTW geometry,[0],[0]
"(4)
Note that each dtwγ(x,yi) term is divided by mi, the length of yi.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Indeed, since dtw0 is an increasing (roughly linearly) function of each of the input lengths n",3.1. Averaging with the soft-DTW geometry,[0],[0]
"and mi, we follow the convention of normalizing in practice each discrepancy by n ×mi.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Since the length n of x is here fixed across all evaluations, we do not need to divide the objective of Eq.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"(4) by n. Averaging under the soft-DTW geometry results in substantially different results than those that can be obtained with the Euclidean geometry (which can only be used in the case where all lengths n = m1 = · · · =
mN are equal), as can be seen in the intuitive interpolations we obtain between two time series shown in Figure 4.
",3.1. Averaging with the soft-DTW geometry,[0],[0]
Non-convexity of dtwγ .,3.1. Averaging with the soft-DTW geometry,[0],[0]
A natural question that arises from Eq.,3.1. Averaging with the soft-DTW geometry,[0],[0]
(4) is whether that objective is convex or not.,3.1. Averaging with the soft-DTW geometry,[0],[0]
"The answer is negative, in a way that echoes the non-convexity of the k-means objective as a function of cluster centroids locations.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Indeed, for any alignment matrix A of suitable size, each map x 7→ 〈A,∆(x,y) 〉 shares the same convexity/concavity property that δ may have.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"However, both min and minγ can only preserve the concavity of elementary functions (Boyd & Vandenberghe, 2004, pp.72-74).",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Therefore dtwγ will only be concave if δ is concave, or become instead a (non-convex) (soft) minimum of convex functions if δ is convex.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"When δ is a squared-Euclidean distance, dtw0 is a piecewise quadratic function of x, as is also the case with the k-means energy (see for instance Figure 2 in Schultz & Jain 2017).",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Since this is the setting we consider here, all of the computations involving barycenters should be taken with a grain of salt, since we have no way of ensuring optimality when approximating Eq.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"(4).
",3.1. Averaging with the soft-DTW geometry,[0],[0]
Smoothing helps optimizing dtwγ .,3.1. Averaging with the soft-DTW geometry,[0],[0]
"Smoothing can be regarded, however, as a way to “convexify” dtwγ .",3.1. Averaging with the soft-DTW geometry,[0],[0]
"Indeed, notice that dtwγ converges to the sum of all costs as γ → ∞. Therefore, if δ is convex, dtwγ will gradually become convex as γ grows.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"For smaller values of γ, one can intuitively foresee that using minγ instead of a minimum will smooth out local minima and therefore provide a better (although slightly different from dtw0) optimization landscape.",3.1. Averaging with the soft-DTW geometry,[0],[0]
"We believe this is why our approach recovers better results, even when measured in the original dtw0 discrepancy, than subgradient or alternating minimization approaches such as DBA (Petitjean et al., 2011), which can, on the contrary, get more easily stuck in local minima.",3.1. Averaging with the soft-DTW geometry,[0],[0]
Evidence for this statement is presented in the experimental section.,3.1. Averaging with the soft-DTW geometry,[0],[0]
The (approximate) computation of dtwγ barycenters can be seen as a first step towards the task of clustering time series under the dtwγ discrepancy.,3.2. Clustering with the soft-DTW geometry,[0],[0]
"Indeed, one can naturally formulate that problem as that of finding centroids x1, . . .",3.2. Clustering with the soft-DTW geometry,[0],[0]
",xk that minimize the following energy:
min x1,...,xk∈Rp×n
N∑
i=1
1
mi min j∈[[k]] dtwγ(xj ,yi).",3.2. Clustering with the soft-DTW geometry,[0],[0]
"(5)
To solve that problem one can resort to a direct generalization of Lloyd’s algorithm (1982) in which each centering step and each clustering allocation step is done according to the dtwγ discrepancy.",3.2. Clustering with the soft-DTW geometry,[0],[0]
"One of the de-facto baselines for learning to classify time series is the k nearest neighbors (k-NN) algorithm, combined with DTW as discrepancy measure between time series.",3.3. Learning prototypes for time series classification,[0],[0]
"However, k-NN has two main drawbacks.",3.3. Learning prototypes for time series classification,[0],[0]
"First, the time series used for training must be stored, leading to potentially high storage cost.",3.3. Learning prototypes for time series classification,[0],[0]
"Second, in order to com-
pute predictions on new time series, the DTW discrepancy must be computed with all training time series, leading to high computational cost.",3.3. Learning prototypes for time series classification,[0],[0]
"Both of these drawbacks can be addressed by the nearest centroid classifier (Hastie et al., 2001, p.670), (Tibshirani et al., 2002).",3.3. Learning prototypes for time series classification,[0],[0]
This method chooses the class whose barycenter (centroid) is closest to the time series to classify.,3.3. Learning prototypes for time series classification,[0],[0]
"Although very simple, this method was shown to be competitive with k-NN, while requiring much lower computational cost at prediction time (Petitjean et al., 2014).",3.3. Learning prototypes for time series classification,[0],[0]
"Soft-DTW can naturally be used in a nearest centroid classifier, in order to compute the barycenter of each class at train time, and to compute the discrepancy between barycenters and time series, at prediction time.",3.3. Learning prototypes for time series classification,[0],[0]
Soft-DTW is ideally suited as a loss function for any task that requires time series outputs.,3.4. Multistep-ahead prediction,[0],[0]
"As an example of such a task, we consider the problem of, given the first 1, . . .",3.4. Multistep-ahead prediction,[0],[0]
", t observations of a time series, predicting the remaining (t + 1), . . .",3.4. Multistep-ahead prediction,[0],[0]
", n observations.",3.4. Multistep-ahead prediction,[0],[0]
"Let xt,t ′ ∈",3.4. Multistep-ahead prediction,[0],[0]
"Rp×(t ′−t+1) be the submatrix of x ∈ Rp×n of all columns with indices between t and t′, where",3.4. Multistep-ahead prediction,[0],[0]
1 ≤ t < t′,3.4. Multistep-ahead prediction,[0],[0]
"< n. Learning to predict the segment of a time series can be cast as the problem
min θ∈Θ
N∑
i=1
dtwγ
(
fθ(x 1,t i ),x t+1,n i
)
,
where {fθ} is a set of parameterized function that take as input a time series and outputs a time series.",3.4. Multistep-ahead prediction,[0],[0]
"Natural choices would be multi-layer perceptrons or recurrent neural networks (RNN), which have been historically trained with a Euclidean loss (Parlos et al., 2000, Eq.5).",3.4. Multistep-ahead prediction,[0],[0]
"Throughout this section, we use the UCR (University of California, Riverside) time series classification archive (Chen et al., 2015).",4. Experimental results,[0],[0]
"We use a subset containing 79 datasets encompassing a wide variety of fields (astronomy, geology, medical imaging) and lengths.",4. Experimental results,[0],[0]
Datasets include class information (up to 60 classes) for each time series and are split into train and test sets.,4. Experimental results,[0],[0]
"Due to the large number of datasets in the UCR archive, we choose to report only a summary of our results in the main manuscript.",4. Experimental results,[0],[0]
Detailed results are included in the appendices for interested readers.,4. Experimental results,[0],[0]
"In this section, we compare the soft-DTW barycenter approach presented in §3.1 to DBA (Petitjean et al., 2011) and a simple batch subgradient method.
",4.1. Averaging experiments,[0],[0]
Experimental setup.,4.1. Averaging experiments,[0],[0]
"For each dataset, we choose a class at random, pick 10 time series in that class and compute
their barycenter.",4.1. Averaging experiments,[0],[0]
"For quantitative results below, we repeat this procedure 10 times and report the averaged results.",4.1. Averaging experiments,[0],[0]
"For each method, we set the maximum number of iterations to 100.",4.1. Averaging experiments,[0],[0]
"To minimize the proposed soft-DTW barycenter objective, Eq. (4), we use L-BFGS.
",4.1. Averaging experiments,[0],[0]
Qualitative results.,4.1. Averaging experiments,[0],[0]
"We first visualize the barycenters obtained by soft-DTW when γ = 1 and γ = 0.01, by DBA and by the subgradient method.",4.1. Averaging experiments,[0],[0]
Figure 5 shows barycenters obtained using random initialization on the ECG200 dataset.,4.1. Averaging experiments,[0],[0]
"More results with both random and Euclidean mean initialization are given in Appendix B and C.
We observe that both DBA or soft-DTW with low smoothing parameter γ yield barycenters that are spurious.",4.1. Averaging experiments,[0],[0]
"On the other hand, a descent on the soft-DTW loss with sufficiently high γ converges to a reasonable solution.",4.1. Averaging experiments,[0],[0]
"For example, as indicated in Figure 5 with DTW or soft-DTW (γ = 0.01), the small kink around x = 15 is not representative of any of the time series in the dataset.",4.1. Averaging experiments,[0],[0]
"However, with soft-DTW (γ = 1), the barycenter closely matches the time series.",4.1. Averaging experiments,[0],[0]
"This suggests that DTW or soft-DTW with too low γ can get stuck in bad local minima.
",4.1. Averaging experiments,[0],[0]
"When using Euclidean mean initialization (only possible if time series have the same length), DTW or soft-DTW with low γ often yield barycenters that better match the shape of the time series.",4.1. Averaging experiments,[0],[0]
"However, they tend to overfit: they absorb the idiosyncrasies of the data.",4.1. Averaging experiments,[0],[0]
"In contrast, soft-DTW is able to learn barycenters that are much smoother.
",4.1. Averaging experiments,[0],[0]
Quantitative results.,4.1. Averaging experiments,[0],[0]
Table 1 summarizes the percentage of datasets on which the proposed soft-DTW barycenter achieves lower DTW loss when varying the smoothing parameter γ.,4.1. Averaging experiments,[0],[0]
"The actual loss values achieved by different methods are indicated in Appendix G and Appendix H.
As γ decreases, soft-DTW achieves a lower DTW loss than other methods on almost all datasets.",4.1. Averaging experiments,[0],[0]
"This confirms our
claim that the smoothness of soft-DTW leads to an objective that is better behaved and more amenable to optimization by gradient-descent methods.
4.2.",4.1. Averaging experiments,[0],[0]
"k-means clustering experiments
We consider in this section the same computational tools used in §4.1 above, but use them to cluster time series.
",4.1. Averaging experiments,[0],[0]
Experimental setup.,4.1. Averaging experiments,[0],[0]
"For all datasets, the number of clusters k is equal to the number of classes available in the dataset.",4.1. Averaging experiments,[0],[0]
Lloyd’s algorithm alternates between a centering step (barycenter computation) and an assignment step.,4.1. Averaging experiments,[0],[0]
"We set the maximum number of outer iterations to 30 and the maximum number of inner (barycenter) iterations to 100, as before.",4.1. Averaging experiments,[0],[0]
"Again, for soft-DTW, we use L-BFGS.
",4.1. Averaging experiments,[0],[0]
Qualitative results.,4.1. Averaging experiments,[0],[0]
"Figure 6 shows the clusters obtained when runing Lloyd’s algorithm on the CBF dataset with soft-DTW (γ = 1) and DBA, in the case of random initialization.",4.1. Averaging experiments,[0],[0]
"More results are included in Appendix E. Clearly, DTW absorbs the tiny details in the data, while soft-DTW is able to learn much smoother barycenters.
",4.1. Averaging experiments,[0],[0]
Quantitative results.,4.1. Averaging experiments,[0],[0]
"Table 2 summarizes the percentage of datasets on which soft-DTW barycenter achieves lower k-means loss under DTW, i.e. Eq.",4.1. Averaging experiments,[0],[0]
(5) with γ = 0.,4.1. Averaging experiments,[0],[0]
The actual loss values achieved by all methods are indicated in Appendix I and Appendix J. The results confirm the same trend as for the barycenter experiments.,4.1. Averaging experiments,[0],[0]
"Namely, as γ decreases, soft-DTW is able to achieve lower loss than other methods on a large proportion of the datasets.",4.1. Averaging experiments,[0],[0]
"Note that we have not run experiments with smaller values of γ than 0.001, since dtw0.001 is very close to dtw0 in practice.",4.1. Averaging experiments,[0],[0]
"In this section, we investigate whether the smoothing in soft-DTW can act as a useful regularization and improve classification accuracy in the nearest centroid classifier.
",4.3. Time-series classification experiments,[0],[0]
Experimental setup.,4.3. Time-series classification experiments,[0],[0]
"We use 50% of the data for training, 25% for validation and 25% for testing.",4.3. Time-series classification experiments,[0],[0]
"We choose γ from 15 log-spaced values between 10−3 and 10.
",4.3. Time-series classification experiments,[0],[0]
Quantitative results.,4.3. Time-series classification experiments,[0],[0]
Each point in Figure 7 above the diagonal line represents a dataset for which using soft-DTW for barycenter computation rather than DBA improves the accuracy of the nearest centroid classifier.,4.3. Time-series classification experiments,[0],[0]
"To summarize, we found that soft-DTW is working better or at least as well as DBA in 75% of the datasets.",4.3. Time-series classification experiments,[0],[0]
"In this section, we present preliminary experiments for the task of multistep-ahead prediction, described in §3.4.
",4.4. Multistep-ahead prediction experiments,[0],[0]
Experimental setup.,4.4. Multistep-ahead prediction experiments,[0],[0]
We use the training and test sets predefined in the UCR archive.,4.4. Multistep-ahead prediction experiments,[0],[0]
"In both the training and test sets, we use the first 60% of the time series as input and the remaining 40% as output, ignoring class information.",4.4. Multistep-ahead prediction experiments,[0],[0]
We then use the training set to learn a model that predicts the outputs from inputs and the test set to evaluate results with both Euclidean and DTW losses.,4.4. Multistep-ahead prediction experiments,[0],[0]
"In this experiment, we focus on a simple multi-layer perceptron (MLP) with one
hidden layer and sigmoid activation.",4.4. Multistep-ahead prediction experiments,[0],[0]
"We also experimented with linear models and recurrent neural networks (RNNs) but they did not improve over a simple MLP.
",4.4. Multistep-ahead prediction experiments,[0],[0]
Implementation details.,4.4. Multistep-ahead prediction experiments,[0],[0]
"Deep learning frameworks such as Theano, TensorFlow and Chainer allow the user to specify a custom backward pass for their function.",4.4. Multistep-ahead prediction experiments,[0],[0]
"Implementing such a backward pass, rather than resorting to automatic differentiation (autodiff), is particularly important in the case of soft-DTW:",4.4. Multistep-ahead prediction experiments,[0],[0]
"First, the autodiff in these frameworks is designed for vectorized operations, whereas the dynamic program used by the forward pass of Algorithm 2 is inherently element-wise; Second, as we explained in §2.2, our backward pass is able to re-use log-sum-exp computations from the forward pass, leading to both lower computational cost and better numerical stability.",4.4. Multistep-ahead prediction experiments,[0],[0]
"We implemented a custom backward pass in Chainer, which can then be used to plug soft-DTW as a loss function in any network architecture.",4.4. Multistep-ahead prediction experiments,[0],[0]
"To estimate the MLP’s parameters, we used Chainer’s implementation of Adam (Kingma & Ba, 2014).
",4.4. Multistep-ahead prediction experiments,[0],[0]
Qualitative results.,4.4. Multistep-ahead prediction experiments,[0],[0]
"Visualizations of the predictions obtained under Euclidean and soft-DTW losses are given in Figure 1, as well as in Appendix F. We find that for sim-
ple one-dimensional time series, an MLP works very well, showing its ability to capture patterns in the training set.",4.4. Multistep-ahead prediction experiments,[0],[0]
"Although the predictions under Euclidean and soft-DTW losses often agree with each other, they can sometimes be visibly different.",4.4. Multistep-ahead prediction experiments,[0],[0]
"Predictions under soft-DTW loss can confidently predict abrupt and sharp changes since those have a low DTW cost as long as such a sharp change is present, under a small time shift, in the ground truth.
",4.4. Multistep-ahead prediction experiments,[0],[0]
Quantitative results.,4.4. Multistep-ahead prediction experiments,[0],[0]
A comparison summary of our MLP under Euclidean and soft-DTW losses over the UCR archive is given in Table 3.,4.4. Multistep-ahead prediction experiments,[0],[0]
Detailed results are given in the appendix.,4.4. Multistep-ahead prediction experiments,[0],[0]
"Unsurprisingly, we achieve lower DTW loss when training with the soft-DTW loss, and lower Euclidean loss when training with the Euclidean loss.",4.4. Multistep-ahead prediction experiments,[0],[0]
"Because DTW is robust to several useful invariances, a small error in the soft-DTW sense could be a more judicious choice than an error in an Euclidean sense for many applications.",4.4. Multistep-ahead prediction experiments,[0],[0]
We propose in this paper to turn the popular DTW discrepancy between time series into a full-fledged loss function between ground truth time series and outputs from a learning machine.,5. Conclusion,[0],[0]
"We have shown experimentally that, on the existing problem of computing barycenters and clusters for time series data, our computational approach is superior to existing baselines.",5. Conclusion,[0],[0]
"We have shown promising results on the problem of multistep-ahead time series prediction, which could prove extremely useful in settings where a user’s actual loss function for time series is closer to the robust perspective given by DTW, than to the local parsing of the Euclidean distance.
Acknowledgements.",5. Conclusion,[0],[0]
MC gratefully acknowledges the support of a chaire de l’IDEX Paris Saclay.,5. Conclusion,[0],[0]
"We propose in this paper a differentiable learning loss between time series, building upon the celebrated dynamic time warping (DTW) discrepancy.",abstractText,[0],[0]
"Unlike the Euclidean distance, DTW can compare time series of variable size and is robust to shifts or dilatations across the time dimension.",abstractText,[0],[0]
"To compute DTW, one typically solves a minimal-cost alignment problem between two time series using dynamic programming.",abstractText,[0],[0]
"Our work takes advantage of a smoothed formulation of DTW, called soft-DTW, that computes the soft-minimum of all alignment costs.",abstractText,[0],[0]
"We show in this paper that soft-DTW is a differentiable loss function, and that both its value and gradient can be computed with quadratic time/space complexity (DTW has quadratic time but linear space complexity).",abstractText,[0],[0]
"We show that this regularization is particularly well suited to average and cluster time series under the DTW geometry, a task for which our proposal significantly outperforms existing baselines (Petitjean et al., 2011).",abstractText,[0],[0]
"Next, we propose to tune the parameters of a machine that outputs time series by minimizing its fit with ground-truth labels in a soft-DTW sense.",abstractText,[0],[0]
Soft-DTW: a Differentiable Loss Function for Time-Series,title,[0],[0]
In recent years there is growing interest in understanding natural language text for the purpose of answering science related questions from text as well as quantitative problems of various kinds.,1 Introduction,[0],[0]
"In this context, understanding and solving arithmetic word problems is of specific interest.",1 Introduction,[0],[0]
"Word problems arise naturally when reading the financial section of a newspaper, following election coverage, or when studying elementary school arithmetic word problems.",1 Introduction,[0],[0]
"These problems pose an interesting challenge to the NLP community, due to its concise and relatively straightforward text, and seemingly simple semantics.",1 Introduction,[0],[0]
"Arithmetic word problems are usually directed towards elementary school students, and can be solved by combining the numbers mentioned in text with basic operations (addition, subtraction, multiplication, division).",1 Introduction,[0],[0]
"They are simpler than algebra word problems which require students to identify variables, and form equations with these variables to solve the problem.
",1 Introduction,[0],[0]
"Initial methods to address arithmetic word problems have mostly focussed on subsets of problems, restricting the number or the type of operations used (Roy et al., 2015; Hosseini et al., 2014) but could not deal with
multi-step arithmetic problems involving all four basic operations.",1 Introduction,[0],[0]
"The template based method of (Kushman et al., 2014), on the other hand, can deal with all types of problems, but implicitly assumes that the solution is generated from a set of predefined equation templates.
",1 Introduction,[0],[0]
"In this paper, we present a novel approach which can solve a general class of arithmetic problems without predefined equation templates.",1 Introduction,[0],[0]
"In particular, it can handle multiple step arithmetic problems as shown in Example 1.
",1 Introduction,[0],[0]
Example 1,1 Introduction,[0],[0]
Gwen was organizing her book case making sure each of the shelves had exactly 9 books on it.,1 Introduction,[0],[0]
She has 2 types of books - mystery books and picture books.,1 Introduction,[0],[0]
"If she had 3 shelves of mystery books and 5 shelves of picture books, how many books did she have in total?
",1 Introduction,[0],[0]
"The solution involves understanding that the number of shelves needs to be summed up, and that the total number of shelves needs to be multiplied by the number of books each shelf can hold.",1 Introduction,[0],[0]
"In addition, one has to understand that the number “2” is not a direct part of the solution of the problem.
",1 Introduction,[0],[0]
"While a solution to these problems eventually requires composing multi-step numeric expressions from text, we believe that directly predicting this complex expression from text is not feasible.
",1 Introduction,[0],[0]
At the heart of our technical approach is the novel notion of an Expression Tree.,1 Introduction,[0],[0]
We show that the arithmetic expressions we are interested in can always be represented using an Expression Tree that has some unique decomposition properties.,1 Introduction,[0],[0]
"This allows us to decompose the problem of mapping the text to the arithmetic expression to a collection of simple prediction problems, each determining the lowest common ancestor operation between a pair of quantities mentioned in the problem.",1 Introduction,[0],[0]
"We then formulate the decision problem of composing the final expression tree as a joint inference problem, via an objective function that consists of all these decomposed prediction problems, along with legitimacy and background knowledge constraints.
",1 Introduction,[0],[0]
Learning to generate the simpler decomposed expressions allows us to support generalization across problems types.,1 Introduction,[0],[0]
"In particular, our system could solve Example 1 even though it has never seen a problem that requires both addition and multiplication operations.
",1 Introduction,[0],[0]
"ar X
iv :1
60 8.
01 41
3v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 2
0",1 Introduction,[0],[0]
"A
ug 2
01 6
We also introduce a second concept, that of quantity schema, that allows us to focus on the information relevant to each quantity mentioned in the text.",1 Introduction,[0],[0]
We show that features extracted from quantity schemas help reasoning effectively about the solution.,1 Introduction,[0],[0]
"Moreover, quantity schemas help identify unnecessary text snippets in the problem text.",1 Introduction,[0],[0]
"For instance, in Example 2, the information that “Tom washed cars over the weekend” is irrelevant; he could have performed any activity to earn money.",1 Introduction,[0],[0]
"In order to solve the problem, we only need to know that he had $76 last week, and now he has $86.
",1 Introduction,[0],[0]
Example 2 Last week Tom had $74.,1 Introduction,[0],[0]
He washed cars over the weekend and now has $86.,1 Introduction,[0],[0]
"How much money did he make from the job?
",1 Introduction,[0],[0]
We combine the classifiers’ decisions using a constrained inference framework that allows for incorporating world knowledge as constraints.,1 Introduction,[0],[0]
"For example, we deliberatively incorporate the information that, if the problems asks about an “amount”, the answer must be positive, and if the question starts with “how many”, the answer will most likely be an integer.
",1 Introduction,[0],[0]
"Our system is evaluated on two existing datasets of arithmetic word problems, achieving state of the art performance on both.",1 Introduction,[0],[0]
"We also create a new dataset of multistep arithmetic problems, and show that our system achieves competitive performance in this challenging evaluation setting.
",1 Introduction,[0],[0]
The next section describes the related work in the area of automated math word problem solving.,1 Introduction,[0],[0]
We then present the theory of expression trees and our decomposition strategy that is based on it.,1 Introduction,[0],[0]
"Sec. 4 presents the overall computational approach, including the way we use quantity schemas to learn the mapping from text to expression tree components.",1 Introduction,[0],[0]
"Finally, we discuss our experimental study and conclude.",1 Introduction,[0],[0]
Previous work in automated arithmetic problem solvers has focussed on a restricted subset of problems.,2 Related Work,[0],[0]
"The system described in (Hosseini et al., 2014) handles only addition and subtraction problems, and requires additional annotated data for verb categories.",2 Related Work,[0],[0]
"In contrast, our system does not require any additional annotations and can handle a more general category of problems.",2 Related Work,[0],[0]
"The approach in (Roy et al., 2015) supports all four basic operations, and uses a pipeline of classifiers to predict different properties of the problem.",2 Related Work,[0],[0]
"However, it makes assumptions on the number of quantities mentioned in the problem text, as well as the number of arithmetic steps required to solve the problem.",2 Related Work,[0],[0]
"In contrast, our system does not have any such restrictions, effectively handling problems mentioning multiple quantities and requiring multiple steps.",2 Related Work,[0],[0]
"Kushman’s approach to automatically solving algebra word problems (Kushman et al., 2014) might be the most re-
lated to ours.",2 Related Work,[0],[0]
It tries to map numbers from the problem text to predefined equation templates.,2 Related Work,[0],[0]
"However, they implicitly assume that similar equation forms have been seen in the training data.",2 Related Work,[0],[0]
"In contrast, our system can perform competitively, even when it has never seen similar expressions in training.
",2 Related Work,[0],[0]
There is a recent interest in understanding text for the purpose of solving scientific and quantitative problems of various kinds.,2 Related Work,[0],[0]
"Our approach is related to work in understanding and solving elementary school standardized tests (Clark, 2015).",2 Related Work,[0],[0]
"The system described in (Berant et al., 2014) attempts to automatically answer biology questions, by extracting the structure of biological processes from text.",2 Related Work,[0],[0]
"There has also been efforts to solve geometry questions by jointly understanding diagrams and associated text (Seo et al., 2014).",2 Related Work,[0],[0]
"A recent work (Sadeghi et al., 2015) tries to answer science questions by visually verifying relations from images.
",2 Related Work,[0],[0]
"Our constrained inference module falls under the general framework of Constrained Conditional Models (CCM) (Chang et al., 2012).",2 Related Work,[0],[0]
"In particular, we use the L + I scheme of CCMs, which predicts structured output by independently learning several simple components, combining them at inference time.",2 Related Work,[0],[0]
"This has been successfully used to incorporate world knowledge at inference time, as well as getting around the need for large amounts of jointly annotated data for structured prediction (Roth and Yih, 2005; Punyakanok et al., 2005; Punyakanok et al., 2008; Clarke and Lapata, 2006; Barzilay and Lapata, 2006; Roy et al., 2015).",2 Related Work,[0],[0]
We address the problem of automatically solving arithmetic word problems.,3 Expression Tree and Problem Decomposition,[0],[0]
"The input to our system is the problem text P , which mentions n quantities q1, q2, . . .",3 Expression Tree and Problem Decomposition,[0],[0]
", qn.",3 Expression Tree and Problem Decomposition,[0],[0]
"Our goal is to map this problem to a read-once arithmetic expression E that, when evaluated, provides the problem’s solution.",3 Expression Tree and Problem Decomposition,[0],[0]
We define a read-once arithmetic expression as one that makes use of each quantity at most once.,3 Expression Tree and Problem Decomposition,[0],[0]
"We say that E is a valid expression, if it is such a Read-Once arithmetic expression, and we only consider in this work problems that can be solved using valid expressions (it’s possible that they can be solved also with invalid expressions).
",3 Expression Tree and Problem Decomposition,[0],[0]
"An expression tree T for a valid expression E is a binary tree whose leaves represent quantities, and each internal node represents one of the four basic operations.",3 Expression Tree and Problem Decomposition,[0],[0]
"For a non-leaf node n, we represent the operation associated with it as (n), and its left and right child as lc(n) and rc(n) respectively.",3 Expression Tree and Problem Decomposition,[0],[0]
The numeric value of the quantity associated with a leaf node n is denoted as Q(n).,3 Expression Tree and Problem Decomposition,[0],[0]
"Each node n also has a value associated with it, represented as VAL(n), which can be computed in a
recursive way as follows:
VAL(n) ={ Q(n) if n is a leaf VAL(lc(n))",3 Expression Tree and Problem Decomposition,[0],[0]
(n) VAL(rc(n)),3 Expression Tree and Problem Decomposition,[0],[0]
"otherwise (1)
For any expression tree T for expression E with root node nroot, the value of VAL(nroot) is exactly equal to the numeric value of the expression E. Therefore, this gives a natural representation of numeric expressions, providing a natural parenthesization of the numeric expression.",3 Expression Tree and Problem Decomposition,[0],[0]
"Fig 1 shows an example of an arithmetic problem with solution expression and an expression tree for the solution expression.
",3 Expression Tree and Problem Decomposition,[0],[0]
Definition,3 Expression Tree and Problem Decomposition,[0],[0]
"An expression tree T for a valid expression E is called monotonic if it satisfies the following conditions:
1.",3 Expression Tree and Problem Decomposition,[0],[0]
"If an addition node is connected to a subtraction node, then the subtraction node is the parent.
",3 Expression Tree and Problem Decomposition,[0],[0]
2.,3 Expression Tree and Problem Decomposition,[0],[0]
"If a multiplication node is connected to a division node, then the division node is the parent.
",3 Expression Tree and Problem Decomposition,[0],[0]
3.,3 Expression Tree and Problem Decomposition,[0],[0]
"Two subtraction nodes cannot be connected to each other.
4.",3 Expression Tree and Problem Decomposition,[0],[0]
"Two division nodes cannot be connected to each other.
",3 Expression Tree and Problem Decomposition,[0],[0]
Fig 2 shows two different expression trees for the same expression.,3 Expression Tree and Problem Decomposition,[0],[0]
"Fig 2b is monotonic whereas fig 2a is not.
",3 Expression Tree and Problem Decomposition,[0],[0]
Our decomposition relies on the idea of monotonic expression trees.,3 Expression Tree and Problem Decomposition,[0],[0]
"We try to predict for each pair of quantities qi, qj , the operation at the lowest common ancestor (LCA) node of the monotonic expression tree for the solution expression.",3 Expression Tree and Problem Decomposition,[0],[0]
"We also predict for each quantity, whether it is relevant to the solution.",3 Expression Tree and Problem Decomposition,[0],[0]
"Finally, an inference module combines all these predictions.
",3 Expression Tree and Problem Decomposition,[0],[0]
"In the rest of the section, we show that for any pair of quantities qi, qj in the solution expression, any monotonic tree for the solution expression has the same LCA
operation.",3 Expression Tree and Problem Decomposition,[0],[0]
"Therefore, predicting the LCA operation becomes a multiclass classification problem.
",3 Expression Tree and Problem Decomposition,[0],[0]
The reason that we consider the monotonic representation of the expression tree is that different trees could otherwise give different LCA operation for a given pair of quantities.,3 Expression Tree and Problem Decomposition,[0],[0]
"For example, in Fig 2, the LCA operation for quantities 5 and 8 can be + or−, depending on which tree is considered.
",3 Expression Tree and Problem Decomposition,[0],[0]
"Definition We define an addition-subtraction chain of an expression tree to be the maximal connected set of nodes labeled with addition or subtraction.
",3 Expression Tree and Problem Decomposition,[0],[0]
The nodes of an addition-subtraction (AS) chain C represent a set of terms being added or subtracted.,3 Expression Tree and Problem Decomposition,[0],[0]
These terms are sub-expressions created by subtrees rooted at neighboring nodes of the chain.,3 Expression Tree and Problem Decomposition,[0],[0]
"We call these terms the chain terms of C, and the whole expression, after node operations have been applied to the chain terms, the chain expression of C. For example, in fig 2, the shaded nodes form an addition-subtraction chain.",3 Expression Tree and Problem Decomposition,[0],[0]
"The chain expression is (3×5)+7−8−9, and the chain terms are 3×5, 7, 8 and 9.",3 Expression Tree and Problem Decomposition,[0],[0]
"We define a multiplicationdivision (MD) chain in a similar way.
",3 Expression Tree and Problem Decomposition,[0],[0]
Theorem 3.1.,3 Expression Tree and Problem Decomposition,[0],[0]
"Every valid expression can be represented by a monotonic expression tree.
",3 Expression Tree and Problem Decomposition,[0],[0]
Proof.,3 Expression Tree and Problem Decomposition,[0],[0]
"The proof is procedural, that is, we provide a method to convert any expression tree to a monotonic expression tree for the same expression.",3 Expression Tree and Problem Decomposition,[0],[0]
"Consider a non-monotonic expression tree E, and without loss of generality, assume that the first condition for monotonicity is not valid.",3 Expression Tree and Problem Decomposition,[0],[0]
"Therefore, there exists an addition node ni and a subtraction node nj , and ni is the parent of nj .",3 Expression Tree and Problem Decomposition,[0],[0]
"Consider an addition-subtraction chain C which includes ni, nj .",3 Expression Tree and Problem Decomposition,[0],[0]
We now replace the nodes of C and its subtrees in the following way.,3 Expression Tree and Problem Decomposition,[0],[0]
"We add a single subtraction node n−. The left subtree of n− has all the addition chain terms connected by addition nodes, and the right subtree of n− has all the subtraction chain terms connected by addition nodes.",3 Expression Tree and Problem Decomposition,[0],[0]
"Both subtrees of n− only require addition nodes, hence monotonicity condition is satisfied.",3 Expression Tree and Problem Decomposition,[0],[0]
We can construct the monotonic tree in Fig 2b from the non-monotonic tree of Fig 2a using this procedure.,3 Expression Tree and Problem Decomposition,[0],[0]
"The addition chain terms are 3 × 5 and 7, and the subtraction chain terms are 8 and 9.",3 Expression Tree and Problem Decomposition,[0],[0]
"As as was described above, we introduce the root subtraction node in Fig 2b and attach the addition chain terms
to the left and the subtraction chain terms to the right.",3 Expression Tree and Problem Decomposition,[0],[0]
"The same line of reasoning can be used to handle the second condition with multiplication and division replacing addition and subtraction, respectively.
Theorem 3.2.",3 Expression Tree and Problem Decomposition,[0],[0]
"Consider two valid expression trees T 1 and T 2 for the same expression E. Let C1, C2 be the chain containing the root nodes of T 1 and T2 respectively.",3 Expression Tree and Problem Decomposition,[0],[0]
"The chain type (addition-subtraction or multiplication-division) as well as the the set of chain terms of C1 and C2 are identical.
",3 Expression Tree and Problem Decomposition,[0],[0]
Proof.,3 Expression Tree and Problem Decomposition,[0],[0]
"We first prove that the chains containing the roots are both AS or both MD, and then show that the chain terms are also identical.
",3 Expression Tree and Problem Decomposition,[0],[0]
We prove by contradiction that the chain type is same.,3 Expression Tree and Problem Decomposition,[0],[0]
Let C1’s type be “addition-subtraction” and C2’s type be “multiplication-division” (without loss of generality).,3 Expression Tree and Problem Decomposition,[0],[0]
"Since both C1 and C2 generate the same expression E, we have that E can be represented as sum (or difference) of two expressions as well as product(or division) of two expressions.",3 Expression Tree and Problem Decomposition,[0],[0]
"Transforming a sum (or difference) of expressions to a product (or division) requires taking common terms from the expressions, which imply that the sum (or difference) had duplicate quantities.",3 Expression Tree and Problem Decomposition,[0],[0]
The opposite transformation adds same term to various expressions leading to multiple uses of the same quantity.,3 Expression Tree and Problem Decomposition,[0],[0]
"Therefore, this will force at least one of C1 and C2 to use the same quantity more than once, violating validity.
",3 Expression Tree and Problem Decomposition,[0],[0]
We now need to show that individual chain terms are also identical.,3 Expression Tree and Problem Decomposition,[0],[0]
"Without loss of generality, let us assume that both C1 and C2 are “addition-subtraction” chains.",3 Expression Tree and Problem Decomposition,[0],[0]
Suppose the chain terms of C1 and C2 are not identical.,3 Expression Tree and Problem Decomposition,[0],[0]
"The chain expression for both the chains will be the same (since they are root chains, the chain expressions has to be the same as E).",3 Expression Tree and Problem Decomposition,[0],[0]
Let the chain expression for C1 be ∑,3 Expression Tree and Problem Decomposition,[0],[0]
i ti,3 Expression Tree and Problem Decomposition,[0],[0]
− ∑ i t ′,3 Expression Tree and Problem Decomposition,[0],[0]
"i, where ti’s are the addition chain terms and t′i are the subtraction chain terms.",3 Expression Tree and Problem Decomposition,[0],[0]
"Similarly, let the chain expression for C2 be ∑ i si",3 Expression Tree and Problem Decomposition,[0],[0]
− ∑ i s ′,3 Expression Tree and Problem Decomposition,[0],[0]
i.,3 Expression Tree and Problem Decomposition,[0],[0]
"We
know that ∑ i ti",3 Expression Tree and Problem Decomposition,[0],[0]
− ∑ i t ′,3 Expression Tree and Problem Decomposition,[0],[0]
i = ∑ i si,3 Expression Tree and Problem Decomposition,[0],[0]
− ∑ i s ′,3 Expression Tree and Problem Decomposition,[0],[0]
"i, but the set of ti’s and t′i’s is not the same as the set of si and s ′",3 Expression Tree and Problem Decomposition,[0],[0]
i’s.,3 Expression Tree and Problem Decomposition,[0],[0]
However it should be possible to transform one form to the other using mathematical manipulations.,3 Expression Tree and Problem Decomposition,[0],[0]
"This transformation will involve taking common terms, or multiplying two terms, or both.",3 Expression Tree and Problem Decomposition,[0],[0]
"Following previous explanation, this will force one of the expressions to have duplicate quantities, violating validity.",3 Expression Tree and Problem Decomposition,[0],[0]
"Hence, the chain terms of C1 and C2 are identical.
",3 Expression Tree and Problem Decomposition,[0],[0]
"Consider an expression tree T for a valid expression E. For a distinct pair of quantities qi, qj participating in expression E, we denote by ni, nj the leaves of the expression tree T representing qi, qj , respectively.",3 Expression Tree and Problem Decomposition,[0],[0]
"Let nLCA(qi, qj ; T ) to be the lowest common ancestor node of ni and nj .",3 Expression Tree and Problem Decomposition,[0],[0]
"We also define order(qi, qj ; T ) to be true if ni appears in the left subtree of nLCA(qi, qj ; T ) and nj appears in the right
subtree of nLCA(qi, qj ; T ) and set order(qi, qj ; T ) to false otherwise.",3 Expression Tree and Problem Decomposition,[0],[0]
"Finally we define LCA(qi, qj ; T ) for a pair of quantities qi, qj as follows :
LCA(qi, qj , T ) =",3 Expression Tree and Problem Decomposition,[0],[0]
"+ if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
=,3 Expression Tree and Problem Decomposition,[0],[0]
"+ × if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
=,3 Expression Tree and Problem Decomposition,[0],[0]
×,3 Expression Tree and Problem Decomposition,[0],[0]
"− if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
=,3 Expression Tree and Problem Decomposition,[0],[0]
"− and order(qi, qj ; T ) = true −reverse",3 Expression Tree and Problem Decomposition,[0],[0]
"if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
=,3 Expression Tree and Problem Decomposition,[0],[0]
"− and order(qi, qj ; T ) = false ÷ if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
"= ÷ and order(qi, qj ; T ) = true ÷reverse if (nLCA(qi, qj ; T ))",3 Expression Tree and Problem Decomposition,[0],[0]
"= ÷ and
order(qi, qj ; T ) = false
(2)
",3 Expression Tree and Problem Decomposition,[0],[0]
Definition,3 Expression Tree and Problem Decomposition,[0],[0]
"Given two expression trees T 1 and T 2 for the same expression E, T 1 is LCA-equivalent to T 2 if for every pair quantities qi, qj in the expression E, we have LCA(qi, qj , T 1) = LCA(qi, qj , T 2).",3 Expression Tree and Problem Decomposition,[0],[0]
Theorem 3.3.,3 Expression Tree and Problem Decomposition,[0],[0]
"All monotonic expression trees for an expression are LCA-equivalent to each other.
",3 Expression Tree and Problem Decomposition,[0],[0]
Proof.,3 Expression Tree and Problem Decomposition,[0],[0]
We prove by induction on the number of quantities used in an expression.,3 Expression Tree and Problem Decomposition,[0],[0]
"For all expressions E with 2 quantities, there exists only one monotonic expression tree, and hence, the statement is trivially true.",3 Expression Tree and Problem Decomposition,[0],[0]
"This satisfies our base case.
",3 Expression Tree and Problem Decomposition,[0],[0]
"For the inductive case, we assume that for all expressions with k < n quantities, the theorem is true.",3 Expression Tree and Problem Decomposition,[0],[0]
"Now, we need to prove that any expression with n nodes will also satisfy the property.
",3 Expression Tree and Problem Decomposition,[0],[0]
"Consider a valid (as in all cases) expression E, with monotonic expression trees T 1 and T 2.",3 Expression Tree and Problem Decomposition,[0],[0]
"From theorem 3.2, we know that the chains containing the roots of T 1 and T 2 have identical type and terms.",3 Expression Tree and Problem Decomposition,[0],[0]
"Given two quantities qi, qj of E, the lowest common ancestor of both T 1 and T 2 will either both belong to the chain containing the root, or both belong to one of the chain terms.",3 Expression Tree and Problem Decomposition,[0],[0]
"If the LCA node is part of the chain for both T 1 and T 2, monotonic property ensures that the LCA operation will be identical.",3 Expression Tree and Problem Decomposition,[0],[0]
"If the LCA node is part of a chain term (which is an expression tree of size less than n), the property is satisfied by induction hypothesis.
",3 Expression Tree and Problem Decomposition,[0],[0]
The theory just presented suggests that it is possible to uniquely decompose the overall problem to simpler steps and this will be exploited in the next section.,3 Expression Tree and Problem Decomposition,[0],[0]
"Given the uniqueness properties proved in Sec. 3, it is sufficient to identify the operation between any two relevant quantities in the text, in order to determine the unique valid expression.",4 Mapping Problems to Expression Trees,[0],[0]
"In fact, identifying the operation between any pair of quantities provides much
needed redundancy given the uncertainty in identifying the operation from text, and we exploit it in our final joint inference.
",4 Mapping Problems to Expression Trees,[0],[0]
"Consequently, our overall method proceeds as follows: given the problem text P , we detect quantities q1, q2, . . .",4 Mapping Problems to Expression Trees,[0],[0]
", qn.",4 Mapping Problems to Expression Trees,[0],[0]
"We then use two classifiers, one for relevance and other to predict the LCA operations for a monotonic expression tree of the solution.",4 Mapping Problems to Expression Trees,[0],[0]
"Our training makes use of the notion of quantity schemas, which we describe in Section 4.2.",4 Mapping Problems to Expression Trees,[0],[0]
"The distributional output of these classifiers is then used in a joint inference procedure that determines the final expression tree.
",4 Mapping Problems to Expression Trees,[0],[0]
Our training data consists of problem text paired with a monotonic expression tree for the solution expression and alignment of quantities in the expression to quantity mentions in the problem text.,4 Mapping Problems to Expression Trees,[0],[0]
Both the relevance and LCA operation classifiers are trained on gold annotations.,4 Mapping Problems to Expression Trees,[0],[0]
"In this subsection, we define the scoring functions corresponding to the decomposed problems, and show how we combine these scores to perform global inference.",4.1 Global Inference for Expression Trees,[0],[0]
"For a problem P with quantities q1, q2, . . .",4.1 Global Inference for Expression Trees,[0],[0]
", qn, we define the following scoring functions:
1.",4.1 Global Inference for Expression Trees,[0],[0]
"PAIR(qi, qj , op) :",4.1 Global Inference for Expression Trees,[0],[0]
"Scores the likelihood of LCA(qi, qj , T ) = op, where T is a monotone expression tree of the solution expression of P .",4.1 Global Inference for Expression Trees,[0],[0]
"A multiclass classifier trained to predict LCA operations (Section 4.4) can provide these scores.
2. IRR(q) :",4.1 Global Inference for Expression Trees,[0],[0]
"Scores the likelihood of quantity q being an irrelevant quantity in P , that is, q is not used in creating the solution.",4.1 Global Inference for Expression Trees,[0],[0]
"A binary classifier trained to predict whether a quantity q is relevant or not (Section 4.3), can provide these scores.
",4.1 Global Inference for Expression Trees,[0],[0]
"For an expression E, let I(E) be the set of all quantities in P which are not used in expression E. Let T be a monotonic expression tree for E. We define Score(E) of an expression E in terms of the above scoring functions and a scaling parameter wIRR as follows:
Score(E) =wIRR ∑
q∈I(E)
IRR(q)+ (3)
∑ qi,qj /∈I(E)",4.1 Global Inference for Expression Trees,[0],[0]
"PAIR(qi, qj , LCA(qi, qj , T ))
",4.1 Global Inference for Expression Trees,[0],[0]
"Our final expression tree is an outcome of a constrained optimization process, following (Roth and Yih, 2004; Chang et al., 2012).",4.1 Global Inference for Expression Trees,[0],[0]
"Our objective function makes use of the scores returned by IRR(·) and PAIR(·) to determine the expression tree and is constrained by legitimacy and background knowledge constraints, detailed below.
1.",4.1 Global Inference for Expression Trees,[0],[0]
Positive Answer: Most arithmetic problems asking for amounts or number of objects usually have a positive number as an answer.,4.1 Global Inference for Expression Trees,[0],[0]
"Therefore, while
searching for the best scoring expression, we reject expressions generating negative answer.
2.",4.1 Global Inference for Expression Trees,[0],[0]
Integral Answer: Problems with questions such as ‘how many” usually expect integral solutions.,4.1 Global Inference for Expression Trees,[0],[0]
"We only consider integral solutions as legitimate outputs for such problems.
",4.1 Global Inference for Expression Trees,[0],[0]
"Let C be the set of valid expressions that can be formed using the quantities in a problem P , and which satisfy the above constraints.",4.1 Global Inference for Expression Trees,[0],[0]
"The inference algorithm now becomes the following:
argmax E∈C Score(E) (4)
The space of possible expressions is large, and we employ a beam search strategy to find the highest scoring constraint satisfying expression (Chang et al., 2012).",4.1 Global Inference for Expression Trees,[0],[0]
"We construct an expression tree using a bottom up approach, first enumerating all possible sets of irrelevant quantities, and next over all possible expressions, keeping the top k at each step.",4.1 Global Inference for Expression Trees,[0],[0]
"We give details below.
1.",4.1 Global Inference for Expression Trees,[0],[0]
"Enumerating Irrelevant Quantities: We generate a state for all possible sets of irrelevant quantities, ensuring that there is at least two relevant quantities in each state.",4.1 Global Inference for Expression Trees,[0],[0]
We refer to each of the relevant quantities in each state as a term.,4.1 Global Inference for Expression Trees,[0],[0]
"Therefore, each state can be represented as a set of terms.
",4.1 Global Inference for Expression Trees,[0],[0]
2.,4.1 Global Inference for Expression Trees,[0],[0]
"Enumerating Expressions: For generating a next state S′ from S, we choose a pair of terms ti and tj in S and one of the four basic operations, and form a new term by combining terms ti and tj with the operation.",4.1 Global Inference for Expression Trees,[0],[0]
"Since we do not know which of the possible next states will lead to the optimal goal state, we enumerate all possible next states (that is, enumerate all possible pairs of terms and all possible operations); we prune the beam to keep only the top k candidates.",4.1 Global Inference for Expression Trees,[0],[0]
"We terminate when all the states in the beam have exactly one term.
",4.1 Global Inference for Expression Trees,[0],[0]
"Once we have a top k list of candidate expression trees, we choose the highest scoring tree which satisfies the constraints.",4.1 Global Inference for Expression Trees,[0],[0]
"However, there might not be any tree in the beam which satisfies the constraints, in which case, we choose the top candidate in the beam.",4.1 Global Inference for Expression Trees,[0],[0]
"We use k = 200 in our experiments.
",4.1 Global Inference for Expression Trees,[0],[0]
"In order to choose the value for the wIRR, we search over the set {10−6, 10−4, 10−2, 1, 102, 104, 106}, and choose the parameter setting which gives the highest accuracy on the training data.",4.1 Global Inference for Expression Trees,[0],[0]
"In order to generalize across problem types as well as over simple manipulations of the text, it is necessary to train our system only with relevant information from the problem text.",4.2 Quantity Schema,[0],[0]
"E.g., for the problem in example 2, we do not want to take decisions based on how Tom earned money.",4.2 Quantity Schema,[0],[0]
"Therefore, there is a need to extract the relevant information from the problem text.
",4.2 Quantity Schema,[0],[0]
"To this end, we introduce the concept of a quantity schema which we extract for each quantity in the problem’s text.",4.2 Quantity Schema,[0],[0]
"Along with the question asked, the quantity schemas provides all the information needed to solve most arithmetic problems.
",4.2 Quantity Schema,[0],[0]
"A quantity schema for a quantity q in problem P consists of the following components.
1.",4.2 Quantity Schema,[0],[0]
Associated Verb,4.2 Quantity Schema,[0],[0]
"For each quantity q, we detect the verb associated with it.",4.2 Quantity Schema,[0],[0]
"We traverse up the dependency tree starting from the quantity mention, and choose the first verb we reach.",4.2 Quantity Schema,[0],[0]
"We used the easy first dependency parser (Goldberg and Elhadad, 2010).
2.",4.2 Quantity Schema,[0],[0]
Subject of Associated,4.2 Quantity Schema,[0],[0]
"Verb We detect the noun phrase, which acts as subject of the associated verb (if one exists).
3.",4.2 Quantity Schema,[0],[0]
Unit We use a shallow parser to detect the phrase p in which the quantity q is mentioned.,4.2 Quantity Schema,[0],[0]
All tokens of the phrase (other than the number itself) are considered as unit tokens.,4.2 Quantity Schema,[0],[0]
"Also, if p is followed by the prepositional phrase “of” and a noun phrase (according to the shallow parser annotations), we also consider tokens from this second noun phrase as unit tokens.",4.2 Quantity Schema,[0],[0]
"Finally, if no unit token can be extracted, we assign the unit of the neighboring quantities as the unit of q (following previous work (Hosseini et al., 2014)).
4.",4.2 Quantity Schema,[0],[0]
"Related Noun Phrases We consider all noun phrases which are connected to the phrase p containing quantity q, with NP-PP-NP attachment.",4.2 Quantity Schema,[0],[0]
"If only one quantity is mentioned in a sentence, we consider all noun phrases in it as related.
5.",4.2 Quantity Schema,[0],[0]
"Rate We determine whether quantity q refers to a rate in the text, as well as extract two unit components defining the rate.",4.2 Quantity Schema,[0],[0]
"For example, “7 kilometers per hour” has two components “kilometers” and “hour”.",4.2 Quantity Schema,[0],[0]
"Similarly, for sentences describing unit cost like “Each egg costs 2 dollars”, “2” is a rate, with units “dollars” and “egg”.
",4.2 Quantity Schema,[0],[0]
"In addition to extracting the quantity schemas for each quantity, we extract the surface form text which poses the question.",4.2 Quantity Schema,[0],[0]
"For example, in the question sentence, “How much will John have to pay if he wants to buy 7 oranges?”, our extractor outputs “How much will John have to pay” as the question.",4.2 Quantity Schema,[0],[0]
"We train a binary SVM classifier to determine, given problem text P and a quantity q in it, whether q is needed in the numeric expression generating the solution.",4.3 Relevance Classifier,[0],[0]
We train on gold annotations and use the score of the classifier as the scoring function IRR(·).,4.3 Relevance Classifier,[0],[0]
"The features are extracted from the quantity schemas and can be broadly categorized into three groups:
1.",4.3.1 Features,[0],[0]
"Unit features: Most questions specifically mention the object whose amount needs to be computed, and hence questions provide valuable clue as to which quantities can be irrelevant.",4.3.1 Features,[0],[0]
We add a feature for whether the unit of quantity q is present in the question tokens.,4.3.1 Features,[0],[0]
"Also, we add a feature based on whether the units of other quantities have better matches with question tokens (based on the number of tokens matched), and one based on the number of quantities which have the maximum number of matches with the question tokens.
2.",4.3.1 Features,[0],[0]
Related NP features: Often units are not enough to differentiate between relevant and irrelevant quantities.,4.3.1 Features,[0],[0]
"Consider the following:
Example 3 Problem : There are 8 apples in a pile on the desk.",4.3.1 Features,[0],[0]
Each apple comes in a package of 11.,4.3.1 Features,[0],[0]
5 apples are added to the pile.,4.3.1 Features,[0],[0]
How many apples are there in the pile?,4.3.1 Features,[0],[0]
"Solution : (8 + 5) = 13
",4.3.1 Features,[0],[0]
"The relevance decision depends on the noun phrase “the pile”, which is absent in the second sentence.",4.3.1 Features,[0],[0]
We add a feature indicating whether a related noun phrase is present in the question.,4.3.1 Features,[0],[0]
"Also, we add a feature based on whether the related noun phrases of other quantities have better match with the question.",4.3.1 Features,[0],[0]
"Extraction of related noun phrases is described in Section 4.2.
3.",4.3.1 Features,[0],[0]
Miscellaneous Features:,4.3.1 Features,[0],[0]
"When a problem mentions only two quantities, both of them are usually relevant.",4.3.1 Features,[0],[0]
"Hence, we also add a feature based on the number of quantities mentioned in text.
",4.3.1 Features,[0],[0]
We include pairwise conjunction of the above features.,4.3.1 Features,[0],[0]
"In order to predict LCA operations, we train a multiclass SVM classifier.",4.4 LCA Operation Classifier,[0],[0]
"Given problem text P and a pair of quantities pi and pj , the classifier predicts one of the six labels described in Eq. 2.",4.4 LCA Operation Classifier,[0],[0]
We consider the confidence scores for each label supplied by the classifier as the scoring function PAIR(·).,4.4 LCA Operation Classifier,[0],[0]
"We use the following categories of features:
1.",4.4.1 Features,[0],[0]
"Individual Quantity features: Dependent verbs have been shown to play significant role in solving addition and subtraction problems (Hosseini et al., 2014).",4.4.1 Features,[0],[0]
"Hence, we add the dependent verb of the quantity as a feature.",4.4.1 Features,[0],[0]
"Multiplication and
division problems are largely dependent on rates described in text.",4.4.1 Features,[0],[0]
"To capture that, we add a feature based on whether the quantity is a rate, and whether any component of rate unit is present in the question.",4.4.1 Features,[0],[0]
"In addition to these quantity schema features, we add selected tokens from the neighborhood of the quantity mention.",4.4.1 Features,[0],[0]
"Neighborhood of quantities are often highly informative of LCA operations, for example, “He got 80 more marbles”, the term “more” usually indicates addition.",4.4.1 Features,[0],[0]
"We add as features adverbs and comparative adjectives mentioned in a window of size 5 around the quantity mention.
2.",4.4.1 Features,[0],[0]
"Quantity Pair features: For a pair (qi, qj) we add features to indicate whether they have the same dependent verbs, to indicate whether both dependent verbs refer to the same verb mention, whether the units of qi and qj are the same and, if one of them is a rate, which component of the unit matches with the other quantity’s unit.",4.4.1 Features,[0],[0]
"Finally, we add a feature indicating whether the value of qi is greater than the value of qj .
3.",4.4.1 Features,[0],[0]
"Question Features: Finally, we add a few features based on the question asked.",4.4.1 Features,[0],[0]
"In particular, for arithmetic problems where only one operation is needed, the question contains signals for the required operation.",4.4.1 Features,[0],[0]
"Specifically, we add indicator features based on whether the question mentions comparison-related tokens (e.g., “more”, “less” or “than”), or whether the question asks for a rate (indicated by tokens such as “each” or “one”).
",4.4.1 Features,[0],[0]
We include pairwise conjunction of the above features.,4.4.1 Features,[0],[0]
"For both classifiers, we use the Illinois-SL package 1 under default settings.",4.4.1 Features,[0],[0]
"In this section, we evaluate the proposed method on publicly available datasets of arithmetic word problems.",5 Experimental Results,[0],[0]
"We evaluate separately the relevance and LCA operation classifiers, and show the contribution of various features.",5 Experimental Results,[0],[0]
"Lastly, we evaluate the performance of the full system, and quantify the gains achieved by the constraints.",5 Experimental Results,[0],[0]
"We evaluate our system on three datasets, each of which comprise a different category of arithmetic word problems.
1.",5.1 Datasets,[0],[0]
AI2 Dataset:,5.1 Datasets,[0],[0]
"This is a collection of 395 addition and subtraction problems, released by (Hosseini et al., 2014).",5.1 Datasets,[0],[0]
"They performed a 3-fold cross validation, with every fold containing problems from
1 http://cogcomp.cs.illinois.edu/page/software view/IllinoisSL
different sources.",5.1 Datasets,[0],[0]
This helped them evaluate robustness to domain diversity.,5.1 Datasets,[0],[0]
"We follow the same evaluation setting.
2.",5.1 Datasets,[0],[0]
IL Dataset:,5.1 Datasets,[0],[0]
"This is a collection of arithmetic problems released by (Roy et al., 2015).",5.1 Datasets,[0],[0]
Each of these problems can be solved by performing one operation.,5.1 Datasets,[0],[0]
"However, there are multiple problems having the same template.",5.1 Datasets,[0],[0]
"To counter this, we perform a few modifications to the dataset.",5.1 Datasets,[0],[0]
"First, for each problem, we replace the numbers and nouns with the part of speech tags, and then we cluster the problems based on unigrams and bigrams from this modified problem text.",5.1 Datasets,[0],[0]
"In particular, we cluster problems together whose unigram-bigram similarity is over 90%.",5.1 Datasets,[0],[0]
We next prune each cluster to keep at most 5 problems in each cluster.,5.1 Datasets,[0],[0]
"Finally we create the folds ensuring all problems in a cluster are assigned to the same fold, and each fold has similar distribution of all operations.",5.1 Datasets,[0],[0]
"We have a final set of 562 problems, and we use a 5-fold cross validation to evaluate on this dataset.
3.",5.1 Datasets,[0],[0]
Commoncore Dataset:,5.1 Datasets,[0],[0]
"In order to test our system’s ability to handle multi-step problems, we create a new dataset of multi-step arithmetic problems.",5.1 Datasets,[0],[0]
The problems were extracted from www.commoncoresheets.com.,5.1 Datasets,[0],[0]
"In total, there were 600 problems, 100 for each of the following types:
(a) Addition followed by Subtraction (b) Subtraction followed by Addition (c) Addition and Multiplication (d) Addition and Division (e) Subtraction and Multiplication (f) Subtraction and Division
This dataset had no irrelevant quantities.",5.1 Datasets,[0],[0]
"Therefore, we did not use the relevance classifier in our evaluations.
",5.1 Datasets,[0],[0]
"In order to test our system’s ability to generalize across problem types, we perform a 6-fold cross validation, with each fold containing all the problems from one of the aforementioned categories.",5.1 Datasets,[0],[0]
"This is a more challenging setting relative to the individual data sets mentioned above, since we are evaluating on multi-step problems, without ever looking at problems which require the same set of operations.",5.1 Datasets,[0],[0]
Table 2 evaluates the performance of the relevance classifier on the AI2 and IL datasets.,5.2 Relevance Classifier,[0],[0]
"We report two accuracy values: Relax - fraction of quantities which the classifier got correct, and Strict - fraction of math problems, for which all quantities were correctly classified.",5.2 Relevance Classifier,[0],[0]
"We report accuracy using all features and then removing each feature group, one at a time.
",5.2 Relevance Classifier,[0],[0]
We see that features related to units of quantities play the most significant role in determining relevance of quantities.,5.2 Relevance Classifier,[0],[0]
"Also, the related NP features are not helpful for the AI2 dataset.",5.2 Relevance Classifier,[0],[0]
"Table 1 evaluates the performance of the LCA Operation classifier on the AI2, IL and CC datasets.",5.3 LCA Operation Classifier,[0],[0]
"As before, we report two accuracies - Relax - fraction of quantity pairs for which the classifier correctly predicted the LCA operation, and Strict - fraction of math problems, for which all quantity pairs were correctly classified.",5.3 LCA Operation Classifier,[0],[0]
"We report accuracy using all features and then removing each feature group, one at a time.
",5.3 LCA Operation Classifier,[0],[0]
"The strict and relaxed accuracies for IL dataset are identical, since each problem in IL dataset only requires one operation.",5.3 LCA Operation Classifier,[0],[0]
"The features related to individual quantities are most significant; in particular, the accuracy goes to 0.0 in the CC dataset, without using individual quantity features.",5.3 LCA Operation Classifier,[0],[0]
The question features are not helpful for classification in the CC dataset.,5.3 LCA Operation Classifier,[0],[0]
"This can be attributed to the fact that all problems in CC dataset require multiple operations, and questions in multi-step problems usually do not contain information for each of the required operations.",5.3 LCA Operation Classifier,[0],[0]
Table 3 shows the performance of our system in correctly solving arithmetic word problems.,5.4 Global Inference Module,[0],[0]
"We show the impact of various contraints, and also compare against previously best known results on the AI2 and IL datasets.",5.4 Global Inference Module,[0],[0]
"We also show results using each of the two constraints separately, and using no constraints at all.
",5.4 Global Inference Module,[0],[0]
"The previously known best result in the AI2 dataset is reported in (Hosseini et al., 2014).",5.4 Global Inference Module,[0],[0]
"Since we follow the exact same evaluation settings, our results are directly comparable.",5.4 Global Inference Module,[0],[0]
"We achieve state of the art results, without having access to any additional annotated data, unlike (Hosseini et al., 2014), who use labeled data for verb categorization.",5.4 Global Inference Module,[0],[0]
"For the IL dataset, we acquired the system of (Roy et al., 2015) from the authors, and ran it with the same fold information.",5.4 Global Inference Module,[0],[0]
We outperform their system by an absolute gain of over 20%.,5.4 Global Inference Module,[0],[0]
"We believe that the improvement was mainly due to the dependence of the system of (Roy et al., 2015) on lexical and neighborhood of quantity features.",5.4 Global Inference Module,[0],[0]
"In contrast, features from quantity schemas help us generalize across problem types.",5.4 Global Inference Module,[0],[0]
"Finally, we also compare against the template based system of (Kushman et al., 2014).",5.4 Global Inference Module,[0],[0]
"(Hosseini et al., 2014) mentions the result of running the system of (Kushman et al., 2014) on AI2 dataset, and we report their result here.",5.4 Global Inference Module,[0],[0]
"For IL and CC datasets, we used the system released by (Kushman et al., 2014).
",5.4 Global Inference Module,[0],[0]
"The integrality constraint is particularly helpful when division is involved, since it can lead to fractional answers.",5.4 Global Inference Module,[0],[0]
"It does not help in case of the AI2 dataset, which involves only addition and subtraction problems.",5.4 Global Inference Module,[0],[0]
"The role of the constraints becomes more significant in case of multi-step problems and, in particular, they contribute an absolute improvement of over 15% over the system without constraints on the CC dataset.",5.4 Global Inference Module,[0],[0]
"The template based system of (Kushman et al., 2014) performs on par with our system on the IL dataset.",5.4 Global Inference Module,[0],[0]
We believe that it is due to the small number of equation templates in the IL dataset.,5.4 Global Inference Module,[0],[0]
"It performs poorly on the CC dataset, since we evaluate on unseen problem types, which do not ensure that equation templates in the test data will be seen in the training data.",5.4 Global Inference Module,[0],[0]
The leading source of errors for the classifiers are erroneous quantity schema extraction and lack of understanding of unknown or rare verbs.,5.5 Discussion,[0],[0]
"For the relevance classifier on the AI2 dataset, 25% of the errors were due to mistakes in extracting the quantity schemas and 20% could be attributed to rare verbs.",5.5 Discussion,[0],[0]
"For the LCA operation classifier on the same dataset, 16% of the errors were due to unknown verbs and 15% were due to mistakes in extracting the schemas.",5.5 Discussion,[0],[0]
"The erroneous extraction of accurate quantity schemas is very significant for the IL dataset, contributing 57% of the errors for the relevance classifier and 39% of the errors for the LCA operation classifier.",5.5 Discussion,[0],[0]
"For the operation classifier on the CC dataset, 8% of the errors were due to verbs and 16% were due to faulty quantity schema extraction.",5.5 Discussion,[0],[0]
"Quantity Schema extraction is challenging due to parsing issues as well as some non-standard rate patterns, and it will be one of the future work targets.",5.5 Discussion,[0],[0]
"For example, in the sentence, “How many 4-dollar toys can he buy?”, we fail to extract the rate component of the quantity 4.",5.5 Discussion,[0],[0]
This paper presents a novel method for understanding and solving a general class of arithmetic word problems.,6 Conclusion,[0],[0]
"Our approach can solve all problems whose solution can be expressed by a read-once arithmetic expression, where each quantity from the problem text appears at most once in the expression.",6 Conclusion,[0],[0]
"We develop a novel theoretical framework, centered around the notion of monotone expression trees, and showed how this representation can be used to get a unique decomposition of the problem.",6 Conclusion,[0],[0]
This theory naturally leads to a computational solution that we have shown to uniquely determine the solution - determine the arithmetic operation between any two quantities identified in the text.,6 Conclusion,[0],[0]
"This theory underlies our algorithmic solution - we develop classifiers and a constrained inference approach that exploits redundancy in the information, and show that this yields strong performance on several benchmark collections.",6 Conclusion,[0],[0]
"In particular, our approach achieves state of the art performance on two publicly available arithmetic problem datasets and can support natural generalizations.",6 Conclusion,[0],[0]
"Specifically, our approach performs competitively on multistep problems, even when it has never observed the particular problem type before.
",6 Conclusion,[0],[0]
"Although we develop and use the notion of expression trees in the context of numerical expressions, the concept is more general.",6 Conclusion,[0],[0]
"In particular, if we allow leaves of expression trees to represent variables, we can express algebraic expressions and equations in this framework.",6 Conclusion,[0],[0]
"Hence a similar approach can be targeted towards algebra word problems, a direction we wish to investigate in the future.
",6 Conclusion,[0],[0]
The datasets used in the paper are available for download at http://cogcomp.cs.illinois.edu/page/resource view/98.,6 Conclusion,[0],[0]
"This research was sponsored by DARPA (under agreement number FA8750-13-2-0008), and a grant from AI2.",Acknowledgments,[0],[0]
"Any opinions, findings, conclusions or recommendations are those of the authors and do not necessarily reflect the view of the agencies.",Acknowledgments,[0],[0]
This paper presents a novel approach to automatically solving arithmetic word problems.,abstractText,[0],[0]
"This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates.",abstractText,[0],[0]
"We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework.",abstractText,[0],[0]
Our classifiers gain from the use of quantity schemas that supports better extraction of features.,abstractText,[0],[0]
"Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems.",abstractText,[0],[0]
Solving General Arithmetic Word Problems,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1466–1476, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"This paper introduces the first fully-automated system for solving unaletered SAT-level geometric word problems, each of which consists of text and the corresponding diagram (Figure 1).",1 Introduction,[0],[0]
"The geometry domain has a long history in AI, but previous work has focused on geometric theorem proving (Feigenbaum and Feldman, 1963) or geometric analogies (Evans, 1964).",1 Introduction,[0],[0]
"Arithmetic and algebraic word problems have attracted several NLP researchers (Kushman et al., 2014; Hosseini et al., 2014; Roy et al., 2015), but geometric word problems were first explored only last year by Seo et al. (2014).",1 Introduction,[0],[0]
"Still, this system merely aligned diagram elements with their textual mentions (e.g., “Circle O”)—it did not attempt to fully represent geometry problems or solve them.",1 Introduction,[0],[0]
"Answering geometry questions requires a method that interpert question text and diagrams in concert.
",1 Introduction,[0],[0]
"1The source code, the dataset and the annotations are publicly available at geometry.allenai.org.
",1 Introduction,[0],[0]
The geometry genre has several distinctive characteristics.,1 Introduction,[0],[0]
"First, diagrams provide essential information absent from question text.",1 Introduction,[0],[0]
"In Figure 1 problem (a), for example, the unstated fact that lines BD and AC intersect at E is necessary to solve the problem.",1 Introduction,[0],[0]
"Second, the text often includes difficult references to diagram elements.",1 Introduction,[0],[0]
"For example, in the sentence “In the diagram, the longer line is tangent to the circle”, resolving the referent of the phrase “longer line” is challenging.",1 Introduction,[0],[0]
"Third, the text often contains implicit relations.",1 Introduction,[0],[0]
"For example, in the sentence “AB is 5”, the relations IsLine(AB) and length(AB)=5 are implicit.",1 Introduction,[0],[0]
"Fourth, geometric terms can be ambiguous as well.",1 Introduction,[0],[0]
"For instance, radius can be a type identifier in “the length of radius AO is 5”, or a predicate in “AO is the radius of circle O”.",1 Introduction,[0],[0]
"Fifth, identifying the correct arguments for each relation is challenging.",1 Introduction,[0],[0]
"For example, in sentence “Lines AB and CD are perpendicular to EF”, the parser has to determine what is perpendicular to EF—line AB? line
1466
CD?",1 Introduction,[0],[0]
Or both AB and CD?,1 Introduction,[0],[0]
"Finally, it is hard to obtain large number of SAT-level geometry questions; Learning from a few examples makes this a particularly challenging NLP problem.
",1 Introduction,[0],[0]
"This paper introduces GEOS, a system that maps geometry word problems into a logical representation that is compatible with both the problem text and the accompanying diagram (Figure 1).",1 Introduction,[0],[0]
"We cast the mapping problem as the problem of selecting the subset of relations that is most likely to correspond to each question.
",1 Introduction,[0],[0]
We compute the mapping in three main steps (Figure 2).,1 Introduction,[0],[0]
"First, GEOS uses text- and diagramparsing to overgenerate a set of relations that potentially correspond to the question text, and associates a score with each.",1 Introduction,[0],[0]
"Second, GEOS generates a set of relations (with scores) that corresponds to the diagram.",1 Introduction,[0],[0]
"Third, GEOS selects a subset of the relations that maximizes the joint text and diagram scores.",1 Introduction,[0.9509759810834543],"['CP Detection: A good demonstration of this finding is the Nile data set, where the MAP segmentation finds a single CP, corresponding to the installation of the nilometer around 715 CE, see Fig 5.']"
"We cast this maximization as a submodular optimization problem, which enables GEOS to use a close-to-optimal greedy algorithm.",1 Introduction,[0],[0]
"Finally, we feed the derived formal model of the problem to a geometric solver, which computes the answer to the question.
",1 Introduction,[0],[0]
GEOS is able to solve unseen and unaltered multiple-choice geometry questions.,1 Introduction,[0],[0]
"We report on experiments where GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions, providing the first results of this kind.",1 Introduction,[0],[0]
"Our contributions include: (1) designing and implementing the first end-to-end system that solves SAT plane geometry problems; (2) formalizing the problem of interpreting geometry questions as a submodular optimization problem; and (3) providing the first empirical results on the geometry genre, making the data and software available for future work.",1 Introduction,[0],[0]
"Semantic parsing is an important area of NLP research (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Flanigan et al., 2014; Eisenstein et al., 2009; Kate and Mooney, 2007; Goldwasser and Roth, 2011; Poon and Domingos, 2009; Berant and Liang, 2014; Kwiatkowski et al., 2013; Reddy et al., 2014).",2 Related Work,[0],[0]
"However, semantic parsers do not tackle diagrams—a critical element of the geometry genre.",2 Related Work,[0],[0]
"In addition, the overall number of available geometry questions is quite small compared to the size of typical NLP corpora, making it
challenging to learn semantic parsers directly from geometry questions.",2 Related Work,[0],[0]
"Relation extraction is another area of NLP that is related to our task (Cowie and Lehnert, 1996; Culotta and Sorensen, 2004).",2 Related Work,[0],[0]
"Again, both diagrams and small corpora are problematic for this body of work.
",2 Related Work,[0],[0]
"Our work is part of grounded language acquisition research (Branavan et al., 2012; Vogel and Jurafsky, 2010; Chen et al., 2010; Hajishirzi et al., 2011; Liang et al., 2009; Koncel-Kedziorski et al., 2014; Bordes et al., 2010; Kim and Mooney, 2013; Angeli and Manning, 2014; Hixon et al., 2015; Koncel-Kedziorski et al., 2014; Artzi and Zettlemoyer, 2013) that involves mapping text to a restricted formalism (instead of a full, domain independent representation).",2 Related Work,[0],[0]
"In the geometry domain, we recover the entities (e.g., circles) from diagrams, derive relations compatible with both text and diagram, and re-score relations derived from text parsing using diagram information.",2 Related Work,[0],[0]
"Casting the interpretation problem as selecting the most likely subset of literals can be generalized to grounded semantic parsing domains such as navigational instructions.
",2 Related Work,[0],[0]
"Coupling images and the corresponding text has attracted attention in both vision and NLP (Farhadi et al., 2010; Kulkarni et al., 2011; Gupta and Mooney, 2010; Gong et al., 2014; Fang et al., 2014).",2 Related Work,[0],[0]
"We build on this powerful paradigm, but instead of generating captions we show how processing multimodal information help improve textual or visual interpretations for solving geometry questions.
",2 Related Work,[0],[0]
"Diagram understanding has been explored since early days in AI (Lin et al., 1985; Hegarty and Just, 1989; Novak, 1995; O’Gorman and Kasturi, 1995; Bulko, 1988; Srihari, 1994; Lovett and Forbus, 2012).",2 Related Work,[0],[0]
Most previous approaches differ from our method because they address the twin problems of diagram understanding and text understanding in isolation.,2 Related Work,[0],[0]
"Often, previous work relies on manual identification of visual primitives, or on rule-based system for text analysis.",2 Related Work,[0],[0]
"The closest work to ours is the recent work of Seo et al. (2014) that aligns geometric shapes with their textual mentions, but does not identify geometric relations or solve geometry problems.",2 Related Work,[0],[0]
"A geometry question is a tuple (t, d, c) consisting of a text t in natural language, a diagram d
in raster graphics, and multiple choice answers c = {c1, . . .",3 Problem Formulation,[0],[0]
", cM} (M = 5 in SAT).",3 Problem Formulation,[0],[0]
"Answering a geometry question is to find a correct choice ci.
",3 Problem Formulation,[0],[0]
"Our method, GEOS, consists of two steps (Figure 2): (1) interpreting a geometry question by deriving a logical expression that represents the meaning of the text and the diagram, and (2) solving the geometry question by checking the satisfiablity of the derived logical expression.",3 Problem Formulation,[0],[0]
"In this paper we mainly focus on interpreting geometry questions and use a standard algebraic solver (see section 7 for a brief description of the solver).
",3 Problem Formulation,[0],[0]
"Definitions: We formally represent logical expressions in the geometry domain with the language Ω, a subset of typed first-order logic that includes: • constants, corresponding to known numbers
(e.g., 5 and 2 in Figure 1) or entities with known geometric coordinates.",3 Problem Formulation,[0],[0]
"• variables, corresponding to unknown numbers
or geometrical entities in the question (e.g., O and CE in Figure 1).",3 Problem Formulation,[0],[0]
"• predicates, corresponding to geometric or arith-
metic relations (e.g., Equals, IsDiameter, IsTangent).",3 Problem Formulation,[0],[0]
"• functions, corresponding to properties of geo-
metrical entities (e.g., LengthOf, AreaOf) or arithmetic operations (e.g., SumOf, RatioOf).
",3 Problem Formulation,[0],[0]
"Each element in the geometry language has either boolean (e.g., true), numeric (e.g., 4), or entity (e.g., line, circle) type.",3 Problem Formulation,[0],[0]
"We refer to all symbols
in the language Ω as concepts.",3 Problem Formulation,[0],[0]
"We use the term literal to refer to the application of a predicate to a sequence of arguments (e.g., IsTriangle(ABC)).",3 Problem Formulation,[0],[0]
"Literals are possibly negated atomic formulas in the language Ω. Logical formulas contain constants, variables, functions, existential quantifiers and conjunctions over literals (e.g., ∃x, IsTriangle(x)∧IsIsosceles(x)).",3 Problem Formulation,[0],[0]
"Interpretation is the task of mapping a new geometry question with each choice, (t, d, cm), into a logical formula γ in Ω. More formally, the goal is to find γ∗ = arg maxγ∈Γ score(γ; t, d, cm) where Γ is the set of all logical formulas in Ω and score measures the interpretation score of the formula according to both text and diagram.",3 Problem Formulation,[0],[0]
"The problem of deriving the best formula γ∗ can be modeled as a combinatorial search in the space of literals L (note that each logical formula γ is represented as a conjunction over literals li).
",3 Problem Formulation,[0],[0]
GEOS efficiently searches this combinatorial space taking advantage of a submodular set function that scores a subset of literals using both text and diagram.,3 Problem Formulation,[0],[0]
"The best subset of literals is the one that has a high affinity with both text and diagram and is coherent i.e., does not suffer from redundancy (see Section 6).",3 Problem Formulation,[0],[0]
"More formally,2
L∗ = arg max L′⊂L",3 Problem Formulation,[0],[0]
"λA(L′, t, d)︸ ︷︷ ︸",3 Problem Formulation,[0],[0]
"Affinity +H(L′, t, d)︸ ︷︷ ︸",3 Problem Formulation,[0],[0]
"Coherence , (1) where A(L′, t, d) measures the affinity of the literals in L′ with both the text and the diagram, H(L′, t, d) measures the coverage of the literals in L′ compared to the text and discourages redundancies, and λ is a trade-off parameter between A andH.
The affinity A is decomposed into textbased affinity, Atext, and diagram-based affinity, Adiagram.",3 Problem Formulation,[0],[0]
The text-based affinity closely mirrors the linguistic structure of the sentences as well as type matches in the geometry language,3 Problem Formulation,[0],[0]
Ω.,3 Problem Formulation,[0],[0]
"For modeling the text score for each literal, we learn a log-linear model.",3 Problem Formulation,[0],[0]
"The diagram-based affinity Adiagram grounds literals into the diagram, and scores literals according to the diagram parse.",3 Problem Formulation,[0],[0]
We describe the details on how to compute Atext in section 4 and Adiagram in section 5.,3 Problem Formulation,[0],[0]
"The text-based scoring function Atext(L, t) computes the affinity score between the set of liter-
2We omit the argument cm for the ease of notation.
",4 Text Parser,[0],[0]
als L and the question text t.,4 Text Parser,[0],[0]
"This score is the sum of the affinity scores of individual literals lj ∈ L i.e., Atext(L, t) =",4 Text Parser,[0],[0]
"∑ j Atext(lj , t)",4 Text Parser,[0],[0]
"where Atext(lj , t) 7→",4 Text Parser,[0],[0]
"[−∞, 0].3 GEOS learns a discriminative model Atext(lj , t; θ) that scores the affinity of every literal lj ∈ L and the question text t through supervised learning from training data.
",4 Text Parser,[0],[0]
"We represent literals using a hypergraph (Figure 4) (Klein and Manning, 2005; Flanigan et al., 2014).",4 Text Parser,[0],[0]
"Each node in the graph corresponds to a concept in the geometry language (i.e. constants, variables, functions, or predicates).",4 Text Parser,[0],[0]
The edges capture the relations between concepts; concept nodes are connected if one concept is the argument of the other in the geometry language.,4 Text Parser,[0],[0]
"In order to interpret the question text (Figure 3 step 1), GEOS first identifies concepts evoked by the words or phrases in the input text.",4 Text Parser,[0],[0]
"Then, it learns the affinity scores which are the weights of edges in the hypergraph.",4 Text Parser,[0],[0]
It finally completes relations so that type matches are satistfied in the formal language.,4 Text Parser,[0],[0]
Concepts are defined as symbols in the geometry language,4.1 Concept Identification,[0],[0]
"Ω. The concept identification stage maps words or phrases to their corresponding concepts
3For the ease of notation, we use Atext as a function taking sets of literals or a literal.
in the geometry language.",4.1 Concept Identification,[0],[0]
Note that a phrase can be mapped to several concepts.,4.1 Concept Identification,[0],[0]
"For instance, in the sentence “ABCD is a square with an area of 1”, the word “square” is a noun referring to some object, so it maps to a variable square.",4.1 Concept Identification,[0],[0]
"In a similar sentence “square ABCD has an area 1”, the word “square” describes the variable ABCD, so it maps to a predicate IsSquare.
GEOS builds a lexicon from training data that maps stemmed words and phrases to the concepts in the geometry language",4.1 Concept Identification,[0],[0]
Ω. The lexicon is derived from all correspondences between geometry keywords and concepts in the geometry language as well as phrases and concepts from manual annotations in the training data.,4.1 Concept Identification,[0],[0]
"For instance, the lexicon contains (“square”, {square, IsSquare}) including all possible concepts for the phrase “square”.",4.1 Concept Identification,[0],[0]
"Note that GEOS does not make any hard decision on which identification is correct in this stage, and defers it to the relation identification stage (Section 4.2).",4.1 Concept Identification,[0],[0]
"To identify numbers and explicit variables (e.g. “5”, “AB”, “O”), GEOS uses regular expressions.",4.1 Concept Identification,[0],[0]
"For an input text t, GEOS assigns one node in the graph (Figure 4) for each concept identified by the lexicon.",4.1 Concept Identification,[0],[0]
A relation is a directed hyperedge between concept nodes.,4.2 Relation Identification,[0],[0]
"A hyperedge connects two nodes (for unary relations such as the edge between RadiusOf and O in Figure 4) or three nodes (for binary relations such as the hyperedge between Equals and its two arguments RadiusOf and 5 in Figure 4).
",4.2 Relation Identification,[0],[0]
"We use a discriminative model (logistic regression) to predict the probability of a relation ri being correct in text t: Pθ(yi|ri, t) = 1 1+exp (ftext(ri,t)·θ) , where yi ∈ {0, 1} is the label
for ri being correct in t, ftext(ri, t) is a feature vector of t and ri, and θ is a vector of parameters to be learned.",4.2 Relation Identification,[0],[0]
"We define the affinity score of ri by Atext(ri, t; θ) = logPθ(yi|ri, t).",4.2 Relation Identification,[0],[0]
The weight of the corresponding hyperedge is the relation’s affinity score.,4.2 Relation Identification,[0],[0]
"We learn θ using the maximum likelihood estimation of the training data (details in Section 8), with L2 regularization.
",4.2 Relation Identification,[0],[0]
We train two separate models for learning unary and binary relations.,4.2 Relation Identification,[0],[0]
"The training data consists of sentence-relation-label tuples (t, r, y); for instance, (“A tangent line is drawn to circle O”, IsTangent(line, O), 1) is a positive training example.",4.2 Relation Identification,[0],[0]
"All incorrect relations in the sentences of the training data are negative examples (e.g. (“A tangent line is drawn to circle O”, IsCircle(line), 0)).
",4.2 Relation Identification,[0],[0]
The features for the unary and binary models are shown in Table 1 for the text t and the relation ri.,4.2 Relation Identification,[0],[0]
We use two main feature categories.,4.2 Relation Identification,[0],[0]
"Structural features: these features capture the syntactic cues of the text in the form of text distance, dependency tree labels, and part of speech tags for the words associated with the concepts in the relation.",4.2 Relation Identification,[0],[0]
"Geometry language features: these features capture the cues available in the geometry language Ω in the form of the types and the truth values of the corresponding concepts in the relation.
",4.2 Relation Identification,[0],[0]
"At inference, GEOS uses the learned models to calculate the affinity scores of all the literals derived from the text t.",4.2 Relation Identification,[0],[0]
"The affinity score of each literal lj is calculated from the edge (relation) weights in the corresponding subgraph, i.e. Atext(lj , t) = ∑ iAtext(ri, t; θ) for all ri in the literal lj .",4.2 Relation Identification,[0],[0]
"So far, we have explained how to score the affinities between explicit relations and the question text.",4.3 Relation Completion,[0],[0]
Geometry questions usually include implicit concepts.,4.3 Relation Completion,[0],[0]
"For instance, “Circle O has a radius of 5” implies the Equals relationship between “Radius of circle O” and “5”.",4.3 Relation Completion,[0],[0]
"In addition, geometry questions include coordinating conjunctions between entities.",4.3 Relation Completion,[0],[0]
"In “AM and CM bisect BAC and BCA”, “bisect” is shared by two lines and two angles (Figure 5 (b)).",4.3 Relation Completion,[0],[0]
"Also, consider two sentences: “AB and CD are perpendicular” and “AB is perpendicular to CD”.",4.3 Relation Completion,[0],[0]
"Both have the same semantic annotation but very different syntactic structures.
",4.3 Relation Completion,[0],[0]
"It is difficult to directly fit the syntactic structure of question sentences into the formal language Ω for implications and coordinating conjunctions, especially due to small training data.",4.3 Relation Completion,[0],[0]
"We, instead, adopt a two-stage learning inspired by recent work in semantic parsing (Kwiatkowski et al., 2013).",4.3 Relation Completion,[0],[0]
Our solution assumes an intermediate representation that is syntactically sound but possibly underspecified.,4.3 Relation Completion,[0],[0]
The intermediate representation closely mirrors the linguistic structure of the sentences.,4.3 Relation Completion,[0],[0]
"In addition, it can easily be transferred to the formal representation in the geometry language Ω.
Figure 5 shows how implications and coordinating conjunctions are modeled in the intermediate representation.",4.3 Relation Completion,[0],[0]
"Bridged in Figure 5 (a) indicates that there is a special relation (edge) between the two concepts (e.g., what and PerimeterOf), but the alignment to the geometry language L is not clear.",4.3 Relation Completion,[0],[0]
CC in Figure 5 (b) indicates that there is a special relation between two concepts that are connected by “and” in the sentence.,4.3 Relation Completion,[0],[0]
"GEOS completes
the under-specified relations by mapping them to the corresponding well-defined relations in the formal language.
",4.3 Relation Completion,[0],[0]
Implication: We train a log-linear classifier to identify if a Bridged relation (implied concept) exists between two concepts.,4.3 Relation Completion,[0],[0]
"Intuitively, the classification score indicates the likelihood that certain two concepts (e.g., What and PerimeterOf) are bridged.",4.3 Relation Completion,[0],[0]
"For training, positive examples are pairs of concepts whose underlying relation is underspecified, and negative examples are all other pairs of concepts that are not bridged.",4.3 Relation Completion,[0],[0]
"For instance, (what, PerimeterOf) is a positive training example for the bridged relation.",4.3 Relation Completion,[0],[0]
"We use the same features in Table 1 for the classifier.
",4.3 Relation Completion,[0],[0]
We then use a deterministic rule to map bridged relations in the intermediate representation to the correct completed relations in the final representation.,4.3 Relation Completion,[0],[0]
"In particular, we map bridged to Equals if the two children concepts are of type number, and to IsA if the concepts are of type entity (e.g. point, line, circle).
",4.3 Relation Completion,[0],[0]
Coordinating Conjunctions: CC relations model coordinating conjunctions in the intermediate representation.,4.3 Relation Completion,[0],[0]
"For example, Figure 5 (b) shows the conjunction between the two angles BAC and BCA.",4.3 Relation Completion,[0],[0]
"We train a log-linear classifier for the CC relations, where the setup of the model is identical to that of the binary relation model in Section 4.2.
",4.3 Relation Completion,[0],[0]
"After we obtain a list of CC(x,y) in the intermediate representation, we use deterministic rules to coordinate the entities x and y in each CC relation (Figure 5 (b)).",4.3 Relation Completion,[0],[0]
"First, GEOS forms a set {x, y} for every two concepts x and y that appear in CC(x,y) and transforms every x and y in other literals to {x, y}.",4.3 Relation Completion,[0],[0]
"Second, GEOS transforms the relations with expansion and distribution rules (Figure 3 Step 1 (iv)).",4.3 Relation Completion,[0],[0]
"For instance, Perpendicular({x,y}) will be transferred to Perpendicular(x, y) (expansion rule), and LengthOf{x,y}) will be transferred to LengthOf(x) ∧ LengthOf(y)",4.3 Relation Completion,[0],[0]
(distribution rule).,4.3 Relation Completion,[0],[0]
"We use the publicly available diagram parser (Seo et al., 2014) to obtain the set of all visual elements (points, lines, circles, etc.), their coordinates, their relationships in the diagram, and their alignment with entity references in the text (e.g. “line AB”, “circle O”).",5 Diagram Parser,[0],[0]
"The diagram parser serves two purposes: (a) computing the diagram score as a mea-
sure of the affinity of each literal with the diagram; (b) obtaining high-confidence visual literals which cannot be obtained from the text.
",5 Diagram Parser,[0],[0]
Diagram score:,5 Diagram Parser,[0],[0]
"For each literal lj from the text parsing, we obtain its diagram score Adiagram(lj , d) 7→",5 Diagram Parser,[0],[0]
"[−∞, 0].",5 Diagram Parser,[0],[0]
GEOS grounds each literal derived from the text by replacing every variable (entity or numerical variable) in the relation to the corresponding variable from the diagram parse.,5 Diagram Parser,[0],[0]
The score function is the relaxed indicator function of whether a literal is true according to the diagram.,5 Diagram Parser,[0],[0]
"For instance, in Figure 1 (a), consider the literal l = Perpendicular(AC, BD).",5 Diagram Parser,[0],[0]
"In order to obtain its diagram score, we compute the angle between the lines AC and BD in the diagram and compare it with π/2.",5 Diagram Parser,[0],[0]
"The closer the two values, the higher the score (closer to 0), and the farther they are, the lower the score.",5 Diagram Parser,[0],[0]
"Note that the variables AC and BD are grounded into the diagram before we obtain the score; that is, they are matched with the actual corresponding lines AC and BD in the diagram.
",5 Diagram Parser,[0],[0]
"The diagram parser is not able to evaluate the correctness of some literals, in which case their diagram scores are undefined.",5 Diagram Parser,[0],[0]
"For instance, Equals(LengthOf(AB), 5) cannot be evaluated in the diagram because the scales in the diagram (pixel) and the text are different.",5 Diagram Parser,[0],[0]
"For another example, Equals(what, RadiusOf(circle)) cannot be evaluated because it contains an ungrounded (query) variable, what.",5 Diagram Parser,[0],[0]
"When the diagram score of a literal lj is undefined, GEOS lets Adiagram(lj)",5 Diagram Parser,[0],[0]
"= Atext(lj).
",5 Diagram Parser,[0],[0]
"If the diagram score of a literal is very low, then it is highly likely that the literal is false.",5 Diagram Parser,[0],[0]
"For example, in Figure 2, Parallel(AC, DB) has a very low diagram score, 0.02, and is apparently false in the diagram.",5 Diagram Parser,[0],[0]
"Concretely, if for some literal lj , Adiagram(li)",5 Diagram Parser,[0],[0]
"< , then GEOS disregards the text score of li by replacing Atext(lj) with Adiagram(lj).",5 Diagram Parser,[0],[0]
"On the other hand, even if the diagram score of a literal is very high, it is still possible that the literal is false, because many diagrams are not drawn to scale.",5 Diagram Parser,[0],[0]
"Hence, GEOS adds both text and diagram scores in order to score literals (Section 6).
",5 Diagram Parser,[0],[0]
High-confidence visual literals: Diagrams often contain critical information that is not present in the text.,5 Diagram Parser,[0],[0]
"For instance, to solve the question in Figure 1, one has to know that the points A, E, and C are colinear.",5 Diagram Parser,[0],[0]
"In addition, diagrams include numer-
ical labels (e.g. one of the labels in Figure 1(b) indicates the measure of the angle ABC = 40 degrees).",5 Diagram Parser,[0],[0]
This kind of information is confidently parsed with the diagram parser by Seo et al. (2014).,5 Diagram Parser,[0],[0]
We denote the set of the high-confidence literals by L∆ that are passed to the solver (Section 7).,5 Diagram Parser,[0],[0]
"Here, we describe the details of the objective function (Equation 1) and how to efficiently maximize it.",6 Optimization,[0],[0]
"The integrated affinity score of a set of literals L′ (the first term in Equation 1) is defined as: A(L′, t, d) =",6 Optimization,[0],[0]
"∑ l′j∈L′
[Atext(l′j , t)",6 Optimization,[0],[0]
"+Adiagram(l′j , d)] where Atext and Adiagram are the text and diagram affinities of l′j , respectively.
",6 Optimization,[0],[0]
"To encourage GEOS to pick a subset of literals that cover the concepts in the question text and, at the same time, avoid redundancies, we define the coherence function as:
H(L′, t, d) = Ncovered(L′)−Rredundant(L′)
where Ncovered is the number of the concept nodes used by the literals inL′, andNredundant is the number of redundancies among the concept nodes of the literals.",6 Optimization,[0],[0]
"To account for the different scales between A and H, we use the trade-off parameter λ in Equation 1 learned on the validation dataset.
",6 Optimization,[0],[0]
Maximizing the objective function in Equation 1 is an NP-hard combinatorial optimization problem.,6 Optimization,[0],[0]
"However, we show that our objective function is submodular (see Appendix (Section 11) for the proof of submodularity).",6 Optimization,[0],[0]
This means that there exists a greedy method that can provide a reliable approximation.,6 Optimization,[0],[0]
GEOS greedily maximizes Equation 1 by starting from an empty set of literals and adding the next literal,6 Optimization,[0],[0]
lj that maximizes the gain of the objective function until the gain becomes negative (details of the algorithm and the gain function are explained in Figure 3 step 3).,6 Optimization,[0],[0]
"We now have the best set of literals L∗ from the optimization, and the high-confidence visual literals L∆ from the diagram parser.",7 Solver,[0],[0]
"In this step, GEOS determines if an assignment exists to the variables X in L∗ ∪ L∆ that simultaneously satisfies all of the literals.",7 Solver,[0],[0]
"This is known as the problem
of automated geometry theorem proving in computational geometry (Alvin et al., 2014).
",7 Solver,[0],[0]
We use a numerical method to check the satisfiablity of literals.,7 Solver,[0],[0]
"For each literal lj in L∗ ∪ L∆, we define a relaxed indicator function gj :",7 Solver,[0],[0]
S 7→ zj ∈,7 Solver,[0],[0]
"[−∞, 0].",7 Solver,[0],[0]
The function zj = gj(S) indicates the relaxed satisfiability of lj given an assignment S to the variables X .,7 Solver,[0],[0]
The literal lj is completely satisfied if gj(S) = 0.,7 Solver,[0],[0]
"We formulate the problem of satisfiability of literals as the task of finding the assignment S∗ to X such that sum of all indicator functions gj(S∗) is maximized, i.e. S∗ = arg maxS ∑ j gj(S).",7 Solver,[0],[0]
"We use the basing-hopping algorithm (Wales and Doye, 1997) with sequential least squares programming (Kraft, 1988) to globally maximize the sum of the indicator functions.",7 Solver,[0],[0]
"If there exists an assignment such that ∑ j gj(S) = 0, then GEOS finds an assignment to X that satisfies all literals.",7 Solver,[0],[0]
"If such assignment does not exist, then GEOS concludes that the literals are not satisfiable simultaneously.",7 Solver,[0],[0]
GEOS chooses to answer a geometry question if the literals of exactly one answer choice are simultaneously satisfiable.,7 Solver,[0],[0]
Logical Language Ω:,8 Experimental Setup,[0],[0]
Ω consists of 13 types of entities and 94 function and predicates observed in our development set of geometry questions.,8 Experimental Setup,[0],[0]
"Implementation details: Sentences in geometry questions often contain in-line mathematical expressions, such as “If AB=x+5, what is x?”.",8 Experimental Setup,[0],[0]
These mathematical expressions cause general purpose parsers to fail.,8 Experimental Setup,[0],[0]
"GEOS uses an equation analyzer and pre-processes question text by replacing “=” with “equals”, and replacing mathematical terms (e.g., “x+5”) with a dummy noun so that the dependency parser does not fail.
",8 Experimental Setup,[0],[0]
"GEOS uses Stanford dependency parser (Chen and Manning, 2014) to obtain syntactic information, which is used to compute features for relation identification (Table 1).",8 Experimental Setup,[0],[0]
"For diagram parsing, similar to Seo et al. (2014), we assume that GEOS has access to ground truth optical character recognition for labels in the diagrams.",8 Experimental Setup,[0],[0]
"For optimization, we tune the parameters λ to 0.5, based on the training examples.4
Dataset: We built a dataset of SAT plane geometry questions where every question has a tex-
4In our dataset, the number of all possible literals for each sentence is at most 1000.
",8 Experimental Setup,[0],[0]
tual description in English accompanied by a diagram and multiple choices.,8 Experimental Setup,[0],[0]
"Questions and answers are compiled from previous official SAT exams and practice exams offered by the College Board (Board, 2014).",8 Experimental Setup,[0],[0]
"In addition, we use a portion of the publicly available high-school plane geometry questions (Seo et al., 2014) as our training set.
",8 Experimental Setup,[0],[0]
We annotate ground-truth logical forms for all questions in the dataset.,8 Experimental Setup,[0],[0]
Table 2 shows details of the data and annotation statistics.,8 Experimental Setup,[0],[0]
"For evaluating dependency parsing, we annotate 50 questions with the ground truth dependency tree structures of all sentences in the questions.",8 Experimental Setup,[0],[0]
"5
Baselines: Rule-based text parsing + GEOS diagram solves geometry questions using literals extracted from a manually defined set of rules over the textual dependency parser, and scored by diagram.",8 Experimental Setup,[0],[0]
"For this baseline, we manually designed 12 high-precision rules based on the development set.",8 Experimental Setup,[0],[0]
"Each rule compares the dependency tree of each sentence to pre-defined templates, and if a template pattern is matched, the rule outputs the relation or function structure corresponding to that template.",8 Experimental Setup,[0],[0]
"For example, a rule assigns a relation parent(child-1, child-2) for a triplet of (parent, child-1, child-2) where child-1 is the subject of parent and child-2 is the object of the parent.
",8 Experimental Setup,[0],[0]
GEOS without text parsing solves geometry questions using a simple heuristic.,8 Experimental Setup,[0],[0]
"With simple textual processing, this baseline extracts numerical relations from the question text and then computes the scale between the units in the question and the pixels in the diagram.",8 Experimental Setup,[0],[0]
"This baseline rounds the number to the closest choice available in the multiple choices.
",8 Experimental Setup,[0],[0]
GEOS without diagram parsing solves geometry questions only relying on the literals interpreted from the text.,8 Experimental Setup,[0],[0]
"It outputs all literals whose text scores are higher than a tuned threshold, 0.6 on the training set.
",8 Experimental Setup,[0],[0]
"GEOS without relation completion solves ge-
5The source code, the dataset and the annotations are publicly available at geometry.allenai.org.
ometry questions when text parsing does not use the intermediate representation and does not include the relation completion step.",8 Experimental Setup,[0],[0]
"We evaluate our method on three tasks: solving geometry question, interpreting geometry questions, and dependency parsing.",9 Experiments,[0],[0]
Solving Geometry Questions: Table 3 compares the score of GEOS in solving geometry questions in practice and official SAT questions with that of baselines.,9 Experiments,[0],[0]
SAT’s grading scheme penalizes a wrong answer with a negative score of 0.25.,9 Experiments,[0],[0]
We report the SAT score as the percentage of correctly answered questions penalized by the wrong answers.,9 Experiments,[0],[0]
"For official questions, GEOS answers 27 questions correctly, 1 questions incorrectly, and leaves 27 un-answered, which gives it a score of 26.75 out of 55, or 49%.",9 Experiments,[0],[0]
"Thus, GEOS’s precision exceeds 96% on the 51% of questions that it chooses to answer.",9 Experiments,[0],[0]
"For practice SAT questions, GEOS scores 61%.6
In order to understand the effect of individual components of GEOS, we compare the full method with a few ablations.",9 Experiments,[0],[0]
"GEOS significantly outperforms the two baselines GEOS without text parsing and GEOS without diagram parsing, demonstrating that GEOS benefits from both text and diagram parsing.",9 Experiments,[0],[0]
"In order to understand the text parsing component, we compare GEOS with Rule-based text parsing + GEOS Diagram and GEOS without relation completion.",9 Experiments,[0],[0]
The results show that our method of learning to interpret literals from the text is substantially better than the rule-based baseline.,9 Experiments,[0],[0]
"In addition, the relation completion step, which relies on the intermediate representation, helps to improve text interpretation.",9 Experiments,[0],[0]
"Error Analysis: In order to understand the errors made by GEOS, we use oracle text parsing and oracle diagram parsing (Table 3).",9 Experiments,[0],[0]
"Roughly 38% of the errors are due to failures in text parsing, and about 46% of errors are due to failures in diagram parsing.",9 Experiments,[0],[0]
"Among them, about 15% of errors were due to failures in both diagram and text parsing.",9 Experiments,[0],[0]
"For an example of text parsing failure, the literals in Figure 6 (a) are not scored accurately due to missing coreference relations (Hajishirzi et al., 2013).",9 Experiments,[0],[0]
"The rest of errors are due to problems that require more complex reasoning (Figure 6 (b)).
",9 Experiments,[0],[0]
"6Typically, 50th percentile (penalized) score in SAT math section is 27 out of 54 (50%).
",9 Experiments,[0],[0]
Interpreting Question Texts: Table 4 details the precision and recall of GEOS in deriving literals for geometry question texts for official SAT questions.,9 Experiments,[0],[0]
"The rule-based text parsing baseline achieves a high precision, but at the cost of lower recall.",9 Experiments,[0],[0]
"On the other hand, the baseline GEOS without diagram achieves a high recall, but at the cost of lower precision.",9 Experiments,[0],[0]
"Nevertheless, GEOS attains substantially higher F1 score compared to both baselines, which is the key factor in solving the questions.",9 Experiments,[0],[0]
"Direct application of a generic semantic parser (Berant et al., 2013) with full supervision does not perform well in the geometry domain, mainly due to lack of enough training data.",9 Experiments,[0],[0]
"Our initial investigations show the performance of 33% F1 in the official set.
",9 Experiments,[0],[0]
Improving Dependency Parsing: Table 5 shows the results of different methods in dependency parsing.,9 Experiments,[0],[0]
"GEOS returns a dependency parse tree by selecting the dependency tree that maximizes the text score in the objective function from the top 50 trees produced by a generic dependency parser, Stanford parser (Chen and Manning, 2014).",9 Experiments,[0],[0]
Note that Stanford parser cannot handle mathematical symbols and equations.,9 Experiments,[0],[0]
"We report the results of a baseline that extends the Stanford dependency parser by adding a pre-processing step to separate the mathematical expressions from the plain sentences (Section 8).
",9 Experiments,[0],[0]
We evaluate the performance of GEOS against the best tree returned by Stanford parser by reporting the fraction of the questions whose dependency parse structures match the ground truth annotations.,9 Experiments,[0],[0]
Our results show an improvement of 16% over the Stanford dependency parser when equipped with the equation analyzer.,9 Experiments,[0],[0]
"For example, in “AB is perpendicular to CD at E”, the Stan-
ford dependency parser predicts that “E” depends on “CD”, while GEOS predicts the correct parse in which “E” depends on “perpendicular”.",9 Experiments,[0],[0]
"This paper introduced GEOS, an automated system that combines diagram and text interpretation to solve geometry problems.",10 Conclusion,[0],[0]
Solving geometry questions was inspired by two important trends in the current NLP literature.,10 Conclusion,[0],[0]
"The first is in designing methods for grounded language acquisition to map text to a restricted formalism (instead of a full, domain independent representation).",10 Conclusion,[0],[0]
We demonstrate a new algorithm for learning to map text to a geometry language with a small amount of training data.,10 Conclusion,[0],[0]
"The second is designing methods in coupling language and vision and show how processing multimodal information help improve textual or visual interpretations.
",10 Conclusion,[0],[0]
"Our experiments on unseen SAT geometry problems achieve a score of 49% of official questions and a score of 61% on practice questions, providing a baseline for future work.",10 Conclusion,[0],[0]
"Future work includes expanding the geometry language and the reasoning to address a broader set of geometry questions, reducing the amount of supervision, learning the relevant geometry knowledge, and scaling up the dataset.
Acknowledgements.",10 Conclusion,[0],[0]
"The research was supported by the Allen Institute for AI, Allen Distinguished Investigator Award, and NSF (IIS1352249).",10 Conclusion,[0],[0]
"We thank Dan Weld, Luke Zettlemoyer, Aria Haghighi, Mark Hopkins, Eunsol Choi, and the anonymous reviewers for helpful comments.",10 Conclusion,[0],[0]
"Equation 1
We prove that the objective function in equation (1), λA(L′) + H(L′) is submodular by showing that A(L′) andH(L′) are submodular functions.",11 Appendix: Proof of Submodularity of,[0],[0]
"Submodularity of A. Consider L′ ⊂ L, and a new literal to be added, li ∈ L \ L′.",11 Appendix: Proof of Submodularity of,[0],[0]
"By the definition of A, it is clear that A(L′ ∪ {lj}) = A(L′)",11 Appendix: Proof of Submodularity of,[0],[0]
+A({lj}).,11 Appendix: Proof of Submodularity of,[0],[0]
"Hence, for all L′′ ⊂ L′ ⊂",11 Appendix: Proof of Submodularity of,[0],[0]
"L,
A(L′′ ∪ {lj})−A(L′′) = A(L′ ∪ {lj})−A(L′)
.",11 Appendix: Proof of Submodularity of,[0],[0]
Thus A is submodular.,11 Appendix: Proof of Submodularity of,[0],[0]
"Submodularity of H. We prove that the coverage function, Hcov, and the negation of the redundancy function, −Hred are submodular independently, and thus derive that their sum is submodular.",11 Appendix: Proof of Submodularity of,[0],[0]
"For both, consider we are given L′′ ⊂ L′ ⊂",11 Appendix: Proof of Submodularity of,[0],[0]
"L, and a new literal lj ∈ L \ L′.",11 Appendix: Proof of Submodularity of,[0],[0]
"Also, let K ′′ and K ′ denote the the sets of concepts covered by L′′ and L′, respectively, and let Kj denote the set of concepts covered by lj .",11 Appendix: Proof of Submodularity of,[0],[0]
"Coverage: Since K ′′ ⊂ K ′, |K ′′ ∪Kj | − |K ′′| ≥ |K ′ ∪Kj | − |K ′|, which is equivalent to
Hcov(L′′ ∪ {lj})−Hcov(L′′)",11 Appendix: Proof of Submodularity of,[0],[0]
"≥ Hcov(L′ ∪ {lj})−Hcov(L′)
Redundancy:",11 Appendix: Proof of Submodularity of,[0],[0]
"Note that Hred(L′′ ∪ {lj}) − Hred(L′′) = |K ′′ ∩Kj |, and similarly, Hred(L′ ∪ {lj}) − Hred(L′) = |K ′ ∩Kj |.",11 Appendix: Proof of Submodularity of,[0],[0]
"Since K ′′ ⊂ K ′, thus |K ′′ ∩Kj | ≤ |K ′",11 Appendix: Proof of Submodularity of,[0],[0]
∩Kj |.,11 Appendix: Proof of Submodularity of,[0],[0]
"Hence,
Hred(L′′ ∪ {lj})−Hred(L′′) ≤ Hred(L′ ∪ {lj})−Hred(L′),
By negating both sides, we derive that the negation of the redundancy function is submodular.",11 Appendix: Proof of Submodularity of,[0],[0]
"This paper introduces GEOS, the first automated system to solve unaltered SAT geometry questions by combining text understanding and diagram interpretation.",abstractText,[0],[0]
"We model the problem of understanding geometry questions as submodular optimization, and identify a formal problem description likely to be compatible with both the question text and diagram.",abstractText,[0],[0]
GEOS then feeds the description to a geometric solver that attempts to determine the correct answer.,abstractText,[0],[0]
"In our experiments, GEOS achieves a 49% score on official SAT questions, and a score of 61% on practice questions.1",abstractText,[0],[0]
"Finally, we show that by integrating textual and visual information, GEOS boosts the accuracy of dependency and semantic parsing of the question text.",abstractText,[0],[0]
Solving Geometry Problems: Combining Text and Diagram Interpretation,title,[0],[0]
"The assignment problem finds an assignment, or matching, between two finite sets U and V , each of cardinality n, such that the total cost of all matched pairs is minimized.",1. Introduction,[0],[0]
The assignment problem can also be generalized to finding matchings between more than two sets.,1. Introduction,[0],[0]
"This is a fundamental problem in computer science and has been motivated by a wide gamut of research areas spanning diverse areas such as structural biology (Singer & Shkolnisky, 2011), protein structure comparisons in bioinformatics (Zaslavskiy et al., 2009), and computer vision (Conte et al., 2004).",1. Introduction,[0],[0]
"Computer vision especially boasts a broad range of applications that include object matching, image registration (Shen & Davatzikos, 2002), stereo matching (Goesele et al., 2007), shape matching (Petterson et al., 2009; Berg et al., 2005),
1Department of Computer Science & Engineering, Indian Institute of Technology Hyderabad, Hyderabad, India.",1. Introduction,[0],[0]
"Correspondence to: Charu Sharma <charusharma1991@gmail.com>, Deepak Nathani <deepakn1019@gmail.com>, Manohar Kaul <mkaul@iith.ac.in>.",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
Figure 1.,1. Introduction,[0],[0]
"Matching cliques of two houses with 1-cliques (red), 2- cliques (green), 3-cliques (blue).",1. Introduction,[0],[0]
"Violet and yellow lines show the matchings of d-cliques where d = 2, 3 respectively.
structure from motion (SfM) (Szeliski, 2010), and object detection (Jiang et al., 2011), to name a few.
",1. Introduction,[0],[0]
"Various assignment approaches can broadly be classified as those that find a bijective assignment in the form of a permutation matrix by posing the problem as a linear assignment problem (LP) versus ones that solve a quadratic assignment problem (QAP) via graph matching, where each graph’s nodes represent the objects and the edges encode their corresponding distances; the goal of QAP then is to find node-wise correspondences between the graphs so that the overall discrepancy between their corresponding edgewise counterparts is minimized and the overall relational structure is best preserved.
",1. Introduction,[0],[0]
Partial assignment implies that only subsets of U and V can actually be assigned to each other successfully.,1. Introduction,[0],[0]
"This phenomenon is of particular interest to applications where either objects are absent due to incomplete observations, undergo deformations, and/or the objects in question cannot clearly be disambiguated because the objects in question along with their related objects are embedded in clutter.",1. Introduction,[0],[0]
"This variant of the assignment problem is widely accepted as a formidable challenge.
",1. Introduction,[0],[0]
"Although graph matching methods were found to be instrumental, they too perform poorly when faced with nonsimilar geometric transformations or transformations that produce degenerate triangulations.",1. Introduction,[0],[0]
"This is attributed to assigning weights to only node and edge assignments, while ignoring the interplay of higher-order connections/relations.",1. Introduction,[0],[0]
"For example, using triplet weights can alleviate the above
ar X
iv :1
90 7.
",1. Introduction,[0],[0]
"01 73
9v 3
[ cs
.L",1. Introduction,[0],[0]
"G
] 2
9 Ju
l 2 02
0
mentioned problem to a very large extent by defining a measure invariant to scale and other transformations (Chertok & Keller, 2010).
",1. Introduction,[0],[0]
"Motivated by the aforementioned observations and inspired by Kahle (Kahle, 2006)’s work on combinatorial topological models like the random clique complex, we focus our attention to matching higher-order components between two sets of points in the setting of some points missing completely at random.",1. Introduction,[0],[0]
"We pose our assignment problem as finding a matching between two sets of points, each represented as a random clique complex, which is a higher-order analogue of random graphs.",1. Introduction,[0],[0]
"Figure 1 illustrates such a matching of cliques of corresponding dimensionality, between two different scenes (taken from different camera angles) of the same house.",1. Introduction,[0],[0]
"Given an Erdős-Rényi (ER) graph, its clique complex is the simplicial complex with all complete subgraphs (i.e., cliques) as its faces.",1. Introduction,[0],[0]
"The Erdős-Rényi graph forms the 1-skeleton of the random clique complex, where the cliques have at most a dimension of 2, i.e., edges in the graph.",1. Introduction,[0],[0]
"The clique topology of a random adjacency matrix is analogous to its eigenvalue spectrum, as it provides a set of invariants that help detect structure (Giusti et al., 2015).",1. Introduction,[0],[0]
"This probabilistic and combinatorial framework of random clique complexes allows us to further study the assignment problem under various assumptions of the underlying distribution of the matrix entry distributions, its robustness to missing values, and its asymptotic behavior for large-scale cases.
",1. Introduction,[0],[0]
"Contributions: We present the following contributions.
1.",1. Introduction,[0],[0]
"To the best of our knowledge, our proposed approach is a first attempt to formulate higher-order matching between two sets of points, given partial or incomplete information, as a matching between two random clique complexes.",1. Introduction,[0],[0]
"We also propose an efficient matching algorithm and study both its time and storage complexity.
2.",1. Introduction,[0],[0]
"(i) We provide new bounds on the concentration inequality of eigenvalues of the QAP trace formulation for random symmetric matrices, (ii) we give tighter concentration inequality bounds on the largest eigenvalue for the Lawler QAP formulation on random matrices, in the context of affinity matrices that are used by some earlier works.",1. Introduction,[0],[0]
"Furthermore, we theoretically analyze and discuss the robustness of affinity-matrix based schemes to missing points, and (iii) we perform asymptotic analysis on the worst to best case ratio of a QAP solution for our higher-dimensional clique adjacency matrices in the clique percolation regime (Bollobás & Riordan, 2009), where the entries follow a Poisson distribution.
3.",1. Introduction,[0],[0]
"Finally, we present a comprehensive empirical study that compares our method’s matching accuracy to that
of a diverse set of matching approaches (Zhou & De la Torre, 2016; Zhou & De la Torre, 2013; Cho et al., 2010; Feizi et al., 2016; Leordeanu & Hebert, 2005; Cour et al., 2007; Pachauri et al., 2013; Gold & Rangarajan, 1996; Kuhn, 1955; Leordeanu et al., 2009; Zass & Shashua, 2008; Li et al., 2013; Duchenne et al., 2011).",1. Introduction,[0],[0]
"We conducted our experiments on both synthetic and well-known hard real-world datasets that span across affine/non-affine transformations, severe occlusions, and clutter.",1. Introduction,[0],[0]
Our study reveals much better accuracy for the popular datasets against several of the state-of-the-art matching methods.,1. Introduction,[0],[0]
We consider the problem of capturing higher-order feature groups among landmark points in an image by representing them as a random clique complex (RCC) and then using these RCCs to match two sets of groupings from two different images.,2. Matching Random Clique Complexes,[0],[0]
"We begin this section by describing the construction of a random clique complex, followed by our proposed method of matching two RCCs, and we finally analyze the runtime and storage complexity of our algorithm.",2. Matching Random Clique Complexes,[0],[0]
"We begin with general definitions pertaining to the structure of simplicial complexes and then accordingly adapt these definitions to our domain of random graphs to build random clique complexes.
",2.1. Structure of a Random Clique Complex,[0],[0]
"Let G(n, p) be an Erdős-Rényi graph with a set of n vertices denoted by V , whose edges {v, v′} ∈ ( V 2 ) , are i.i.d Bernoulli(p) distributed.",2.1. Structure of a Random Clique Complex,[0],[0]
"Recall, that a k-clique in G(n, p) is a complete subgraph that comprises of k vertices and( k 2 ) edges.",2.1. Structure of a Random Clique Complex,[0],[0]
"Here onwards, for ease of notation, we will denote G(n, p) as G. Given any affinely independent set V = {vi}ki=0 of (k + 1) points in Rn, the k-simplex σ(k) is the convex hull of V , i.e., it is the set of all points of the form w0v0 + · · ·+ wkvk, where ∑k i=0 wi = 1 and wi ≥ 0 for all i. If we imagine the vertices of G embedded generically in Rn, then each (k + 1)-clique consisting of k + 1 vertices is represented by a k-dimensional simplex σ(k) in our random clique complex.",2.1. Structure of a Random Clique Complex,[0],[0]
"For example, a 2-clique (edge) and a 3-clique (triangle) inG is represented as σ(1) and σ(2), respectively.
",2.1. Structure of a Random Clique Complex,[0],[0]
"Given 0 ≤ i ≤ k, the i-th face fi of σ(k) is the subspace of points that satisfy wi = 0; it is the (k − 1)-simplex σ(k−1) whose vertices are all those of σ(k), except the i-th vertex.",2.1. Structure of a Random Clique Complex,[0],[0]
"In other words, when σ(k) is a clique of G, then all its subsets are also cliques and hence considered faces of σ(k).",2.1. Structure of a Random Clique Complex,[0],[0]
"For example, a 3-clique (triangle) has three 2-cliques (edges) in it.
",2.1. Structure of a Random Clique Complex,[0],[0]
"With the aforementioned definitions in mind, we define our
Algorithm 1 Matching Random Clique Complexes Input: X (G) = {G(k,l)}hk=0 and X (G′) = {G′(k,l)}hk=0
1: for k = h . . .",2.1. Structure of a Random Clique Complex,[0],[0]
0,2.1. Structure of a Random Clique Complex,[0],[0]
"do 2: Let M,M ′ be the total number of (k + 1)-cliques in G(k,l) and G′(k,l), respectively 3: L := {c(k)i } M−1 i=0",2.1. Structure of a Random Clique Complex,[0],[0]
#,2.1. Structure of a Random Clique Complex,[0],[0]
list of barycenters 4: for i = 0 . .,2.1. Structure of a Random Clique Complex,[0],[0]
.M,2.1. Structure of a Random Clique Complex,[0],[0]
"− 1 do 5: Ni := Ni ∪ { g (k,l) (x,:) | x = i, g (k,l) (x,y) 6= 0
} 6: N := N ∪ {Ni} # clique neighborhoods 7: end for 8: for i = 0 . . .M",2.1. Structure of a Random Clique Complex,[0],[0]
"− 1 do 9: αi := [α1, . . .",2.1. Structure of a Random Clique Complex,[0],[0]
", αM−1]T
10: α :",2.1. Structure of a Random Clique Complex,[0],[0]
"= α ∪ {αi} # affine weight vectors 11: end for 12: Repeat steps 3–11 on G′(k,l) for L′,N ′ and α′. 13:",2.1. Structure of a Random Clique Complex,[0],[0]
"Build cost matrix C(k) from weights vectors α, α′ 14:",2.1. Structure of a Random Clique Complex,[0],[0]
X∗k,2.1. Structure of a Random Clique Complex,[0],[0]
:,2.1. Structure of a Random Clique Complex,[0],[0]
"= Kuhn-Munkres (G(k,l), G′(k,l), C(k)) 15: end for
Return: {X∗0 , . . .",2.1. Structure of a Random Clique Complex,[0],[0]
", X∗h} # set of permutation matrices
random clique complex X (G) as the set of all cliques in G such that X (G) =",2.1. Structure of a Random Clique Complex,[0],[0]
{σ ∈,2.1. Structure of a Random Clique Complex,[0],[0]
[n] | σ is a clique of G}.,2.1. Structure of a Random Clique Complex,[0],[0]
We denote a set of (k + 1)-cliques as Xk(G).,2.1. Structure of a Random Clique Complex,[0],[0]
"Additionally, X (G) also satisfies the following conditions of a simplicial complex: (i) Any face in X (G) is also a simplex in X (G) and (ii) the intersection of any two simplexes σi, σj is a face (lower dimensional clique) of both σi and σj .
",2.1. Structure of a Random Clique Complex,[0],[0]
"The faces of σ(k) are copies of σ(j) for j < k, which are glued together inductively.",2.1. Structure of a Random Clique Complex,[0],[0]
"The k-skeleton of X (G), for k ∈ N, is defined as the following quotient space
X (k)(G)",2.1. Structure of a Random Clique Complex,[0],[0]
":= ( X (k−1)(G) ∪
∐ σ:dim σ=k σ(k)
)/ ∼
where ∼ is the equivalence relation that identifies faces of σ(k) to the corresponding faces of σ ∈ X (j)(G) where j < k.",2.1. Structure of a Random Clique Complex,[0],[0]
"Finally, X (G) = ∪∞k=0X (k)(G).
",2.1. Structure of a Random Clique Complex,[0],[0]
k,2.1. Structure of a Random Clique Complex,[0],[0]
-skeleton as adjacency matrix:,2.1. Structure of a Random Clique Complex,[0],[0]
"Given a random graph G and its k-skeleton X (k)(G) that contains all its (k + 1)- cliques, we follow the idea from Bollobás et.",2.1. Structure of a Random Clique Complex,[0],[0]
"al. (Bollobás & Riordan, 2009), to represent X (k)(G) as an adjacency matrix G(k,l) whose vertex set is the set of of all (k + 1)- cliques in G and in which two vertices (i.e., (k+ 1)-cliques) are adjacent when they share a common face that has a minimum of l vertices, where k ≥ 1 and 1 ≤",2.1. Structure of a Random Clique Complex,[0],[0]
l ≤ k.,2.1. Structure of a Random Clique Complex,[0],[0]
"Such an adjacency matrix is built for each k-skeleton and therefore X (G) is expressed as a set of matrices {G(k,l)}hk=0, where (k + 1) is the dimension of the cliques.",2.1. Structure of a Random Clique Complex,[0],[0]
"The problem of matching random clique complexes each of dimension h, is the estimation of a set of optimal bijective maps of the formMi : X (i)(G) → X (i)(G′), for all i ≤ h, subject to assignment constraints.",2.2. Problem Setup,[0],[0]
"This can be formulated as a constrained quadratic assignment problem, which can later be relaxed to a linear programming optimization problem.
",2.2. Problem Setup,[0],[0]
"Given two h-dimensional random clique complexes X (G) = {G(k,l)}hk=0 and X (G′) = {G′(k,l)}hk=0, let X = {X0, . . .",2.2. Problem Setup,[0],[0]
", Xh} ∈ Π be a set of permutation matrices such that Xk encodes assignments/matchings from G(k,l) to G′(k,l).",2.2. Problem Setup,[0],[0]
The combinatorial matching requires the optimal set of permutation matrices that best align X (G) and X (G′).,2.2. Problem Setup,[0],[0]
"More formally, this can be expressed as the following constrained optimization problem
argmin X0,...,Xh
h∑ k=0 ‖G(k,l)Xk −XkG′(k,l)‖2F
subject to ∀k ≤ h,1TXk = 1, XTk 1 = 1
(1)",2.2. Problem Setup,[0],[0]
"At a high level, our goal is to minimize ‖X (G)−X (G′)‖C , where C is a combinatorial distance between two random clique complexes.",2.3. Our Algorithm,[0],[0]
Traditional metrics like Hausdorff distance are not suitable here because random clique complexes are combinatorial topological spaces.,2.3. Our Algorithm,[0],[0]
"Recall that X (G) is comprised of a family of k-skeletons {X (k)(G)}hk=0, where each k-skeleton contains cliques whose dimension is at most k + 1 and X (k)(G) has a maximum dimension h.",2.3. Our Algorithm,[0],[0]
"The solution of the optimization problem outlined in Equation (1) aims to find a set of permutation matrices {X1, . . .",2.3. Our Algorithm,[0],[0]
", Xh} that minimizes the overall number of misalignments between equi-dimensional faces of X (G) and X (G′), i.e., cliques belonging to the corresponding k-skeletons, and thus producing the optimal least cost assignment between X (G) and X (G′).
",2.3. Our Algorithm,[0],[0]
Algorithm 1 presents our method to solve the combinatorial optimization problem (Equation (1)).,2.3. Our Algorithm,[0],[0]
"In decreasing order of clique dimensionality, for a fixed dimension k and given the adjacency matrices G(k,l) and G′(k,l) for k-skeletons X (k)(G) and X (k)(G′), respectively.",2.3. Our Algorithm,[0],[0]
"In every iteration, our objective is to solve argminXk‖G
(k,l)Xk−XkG′(k,l)‖2F to find the optimal permutation X∗k .",2.3. Our Algorithm,[0],[0]
We assume the barycenters of every clique is pre-computed (Step 3).,2.3. Our Algorithm,[0],[0]
"Next, the neighborhood Ni of the i-th clique is computed as the set of entries with 1s in the i-th row of G(k,l) (Step 5).",2.3. Our Algorithm,[0],[0]
We denote the collection of every clique’s neighborhood as N (Step 6).,2.3. Our Algorithm,[0],[0]
An important objective of our method is to capture the geometric properties of the neighborhood of every clique.,2.3. Our Algorithm,[0],[0]
"We achieve this by characterizing the i-th clique’s barycenter
c (k) i as an affine combination of the barycenters (in all dimensions) associated with the cliques in its corresponding neighborhood Ni.",2.3. Our Algorithm,[0],[0]
"Given an arbitrary clique’s barycenter c (k) i , let {x (k) 1 , . . .",2.3. Our Algorithm,[0],[0]
", x (k) n } denote the barycenters of its n ad-
jacent cliques.",2.3. Our Algorithm,[0],[0]
"Then, c(k)i expressed as ∑n i=1 αix (k) i is an
affine combination of the x(k)i s, if ∑n i=0 αi = 1, i.e., the weights αi sum to 1.",2.3. Our Algorithm,[0],[0]
"Among all possible affine representations of c(k)i we chose to use least squares to guarantee minimal error under L2-norm, and furthermore it assigns non-zero weights to each of its adjacent clique barycenters, thereby capturing the local geometric properties in its neighborhood.",2.3. Our Algorithm,[0],[0]
The weight vector αi is then calculated for each clique (Step 9) and α denotes a collection of such weight vectors (Step 10).,2.3. Our Algorithm,[0],[0]
"Next, a cost matrix is built by computing the L2-norm distance between weight vectors α and α′ (Step 13).",2.3. Our Algorithm,[0],[0]
"Finally, the Kuhn-Munkres (Kuhn, 1955) algorithm is invoked with both the adjacency matrices and the cost matrix, which arrives at the optimal assignment (Step 14 ).",2.3. Our Algorithm,[0],[0]
"At the end of all iterations, our method returns a set of optimal assignments for matches between each k-skeleton for every dimension below h and the algorithm terminates.",2.3. Our Algorithm,[0],[0]
We refer the reader to our supplementary section for a working example.,2.3. Our Algorithm,[0],[0]
"To begin our analysis, we must first ascertain the dimensionality of G(k,l), which is governed by the total number of (k + 1)-cliques that exist in the underlying random graph G.",2.4. Complexity Analysis,[0],[0]
"It is important to note that there doesn’t exist any closed form solution to counting the number of cliques of a given dimension in G.
We consider the distribution of a random variable Xn(k) counting the number of (k + 1)-cliques in a realization of G. We show in Appendix A of our supplementary material that this count is upper bounded by (en/k)k, where e is Euler’s number.",2.4. Complexity Analysis,[0],[0]
This can be expressed in asymptotic notation as O(nk).,2.4. Complexity Analysis,[0],[0]
"As dimensionality increases, there occurs an explosion in the number of cliques.",2.4. Complexity Analysis,[0],[0]
"Fortunately, G(k,l) is a sparse matrix and its effective dimensionality measured by the number of non-zero rows, i.e., the number of cliques with non-empty neighborhoods, is of order O(nnz(G(k,l))).",2.4. Complexity Analysis,[0],[0]
"Therefore, we set out to count the number of non-zero entries in G(k,l).
",2.4. Complexity Analysis,[0],[0]
"We use a seminal result by Bollobás (Bollobás & Riordan, 2009), where they identify a threshold probability for percolation of cliques in G for all fixed k and l, which is given by p = Θ ( n −2 k+l−1 ) .",2.4. Complexity Analysis,[0],[0]
"Moreover, they proved that for p
around this threshold, the number of cliques asymptotically converge to a Poisson distribution.",2.4. Complexity Analysis,[0],[0]
"Exceeding this threshold results in formation of giant connected clique clusters, which causes an explosion in the number of possible cliques.
",2.4. Complexity Analysis,[0],[0]
"Recall from our definition of G(k,l), that two cliques are adjacent if they share at least l vertices.",2.4. Complexity Analysis,[0],[0]
"In order to analyze this further, we imagine an entry in G(k,l) occurs when we can migrate a (k + 1)-clique from its original position to an adjacent clique by relocating exactly (k + 1",2.4. Complexity Analysis,[0],[0]
− l) vertices and leaving the remaining l vertices intact.,2.4. Complexity Analysis,[0],[0]
The expected number of such relocations is given by (( k+1 l ),2.4. Complexity Analysis,[0],[0]
− 1 ) ( n k+1−l ) p(( k+1 2 ),2.4. Complexity Analysis,[0],[0]
"−( l 2)), where the first term denotes the number of possible vertices in a (k + 1)-clique that can be chosen for relocation, the second term counts the number of new adjacent positions a clique can relocate to, and the final term decides the probability of relocations that are correct and acceptable.",2.4. Complexity Analysis,[0],[0]
"In our case, we define cliques to be adjacent to one another when they share at least l = k vertices.",2.4. Complexity Analysis,[0],[0]
This is done in order to keep the number of adjacent cliques to a manageable size during experiments.,2.4. Complexity Analysis,[0],[0]
"Setting l = k, gives knpk expected relocations, which in turn estimates nnz(G(k,l)).
",2.4. Complexity Analysis,[0],[0]
"Note that for every iteration in Algorithm 1, the dominating cost is that of running the Kuhn-Munkres matching algorithm in Step 14, which has a cubic cost in nnz(G(k,l)).",2.4. Complexity Analysis,[0],[0]
"Let Cmax denote an upper bound on all the number of nonzero entries in {G(k,l)}hi=0.",2.4. Complexity Analysis,[0],[0]
"Then, every iteration has a runtime O(C3max) and therefore after h iterations the final cost is O(hC3max).",2.4. Complexity Analysis,[0],[0]
"Observe that as the dimensionality of the cliques increases in every iteration, pk decays very sharply and hence drastically reduces nnz(G(k,l)), which in turn reduces the overall matching cost.",2.4. Complexity Analysis,[0],[0]
"Finally, the storage complexity can simply be given as O(hCmax).",2.4. Complexity Analysis,[0],[0]
"In this section, we present three related results in the context of matching random matrices, namely: (i) concentration inequality of eigenvalue bounds on the QAP trace formulation for random symmetric matrices, (ii) tighter concentration inequality of eigenvalue bounds on the Lawler QAP formulation on random symmetric matrices in the context of works that use affinity matrices, and (iii) provide an asymptotic analysis on the worst to best case ratio of a QAP for higher-dimensional clique adjacency matrices.",3. Theoretical Analysis of QAP,[0],[0]
"For ease of notation, we will refer to the random clique adjacency matrices simply as A and B.",3. Theoretical Analysis of QAP,[0],[0]
"Let A = (avv′), B = (bvv′) ∈ Rn×n be random realsymmetric matrices.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Let X = (xij) ∈ Rn×n be a permutation matrix.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Then, the trace formulation of a QAP is given by
minimize Tr(AXBXT ) s.t. X ∈ ΠX
where ΠX is the set of permutation matrices.
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Let λ1(A) ≤ λ2(A) ≤ · · · ≤ λn(A) and λ1(B) ≥ λ2(B) ≥ · · ·,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
≥,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"λn(B)1 be the eigenvalues of A and B, respectively.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Let the corresponding eigen-decompositions of matrices A and B, be given by A = QAΛAQTA and B = QBΛBQ T B , where ΛA = diag(λ1(A), . . .",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
", λn(A))",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"and ΛB = diag(λ1(B), . . .",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
", λn(B))",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
with their corresponding orthogonal eigenvector matrices QA and QB .,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Finke et.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"al. (Martello et al., 1987) gave the following eigenvalue bounds.
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Theorem 1.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Let A and B be symmetric matrices.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Then for all X ∈ XΠ,
1. Tr(AXBXT ) = λ(A)TQ(X)λ(B), where Q(X) = 〈Q(i)A , XQ (j) B 〉2 with vectors of eigen-
values given by λ(A) = (λi(A)) and λ(B) = (λi(B)).",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Q (i) A and Q (i) B denote the i-th eigenvectors of A and B, respectively;
2.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
L ≤ Tr(AXBXT ) ≤,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"U , where L = ∑n i=1 λi(A)λi(B), and
U = ∑n i=1 λn−i+1(A)λi(B)
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"It was also noticed by Finke (Martello et al., 1987) that these bounds can further be tightened by reducing the spreads of matrices A and B, where the spread of a matrix A, denoted by S(A), is given by S(A) = maxi λi(A) −mini λi(A).",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"There is no formula to compute the spread of a matrix directly, so Finke et.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"al. (Martello et al., 1987) suggested a reduction method to further sharpen the bound by replacing matrices A and B by smaller spread symmetric matrices Ã and B̃.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"The reductions are achieved as Ã = A−MA−MTA −DA and B̃ = B−MB−MTB −DB , where MA, MB are matrices with constant columns and DA, DB are diagonal matrices, whose values are chosen appropriately in order to tighten the bounds on spreads S(Ã) and S(B̃).
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Our bounds on Random Matrices: We propose new measure concentration inequalities on the spread of a random matrix, by redefining the spread in an alternate fashion that is more amenable to our analysis.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Consider our reduced random symmetric matrix Ã ∈ Rn×n, with eigenvalues λ1(Ã) ≤ · · · ≤ λn(Ã), we define the gap (spacing) between its consecutive eigenvalues as δi(Ã)",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
:= |λi+1(Ã)− λi(Ã)| for 1 ≤ i ≤ n,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
− 1.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Then, the spread S(Ã) for the reduced matrix Ã can be redefined as: S(Ã) = ∑n i=1 δi(Ã)
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"We begin by upper bounding δi(Ã) using the following lemma 1 (proof in supplementary notes).
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Lemma 1.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Let |||.||| denote an algebraic matrix norm on a space of real n× n matricesMn, then for any A ∈Mn,
δi(A) ≤ 2|||A||| 1",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"The two sets of eigenvalues differ in ordering.
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
To the best of the author’s knowledge there does not exist a known distribution of eigenvalue gaps for a symmetric random matrix.,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"We now attempt to give concentration inequalities for the tail probabilities of the sum of eigenvalue gaps, i.e., the spread.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"For our i.i.d. random matrix A ∈ Rn×n, consider the sequence of independent eigenvalue gaps δ1(A), . . .",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
", δn(A), where each δi(A) is upper bounded by 2|||A|||, as shown in Lemma 1.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
Let us denote their sum as Sn(A) := δ1(A) + · · · + δn(A).,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"As δ1(A), . . .",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
", δn(A) are independent scalar random variables with δi(A)",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
≤ |||A|||,3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"a.s, with mean µi(A) and variance σ2i (A).",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"Then, using Chernoff bounds, for any > 0, we have
P(|Sn(A)− µ| ≥ σ) ≤",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"K max ( e(−p 2), e(−p σ/2|||A|||) )
",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"for some absolute constants K, p > 0.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"The Chernoff inequality above, shows that Sn(A) is sharply concentrated in the range nµ+O(σ √ n), when is not too large.",3.1. Eigenvalue Bounds of Trace QAP Formulation on Random Matrices,[0],[0]
"In literature, many graph matching algorithms use Lawler’s QAP formulation.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Recall, A = (aij), B = (buv) ∈ Rn×n.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Let Ω(ai,j , bu,v) denote the pairwise affinity score of assigning the (i, j)-th entry in A to the (u, v)-th entry in B, implying that node ai is matched to node bu and node aj to node bv, simultaneously.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Then, the affinity matrix A ∈ Rn 2×n2 is given by A [(i− 1)n+ u, (j − 1)n+ v] = Ω(ai,j , bu,v) and the optimal assignment to Lawler’s QAP is the one that maximizes the sum total pairwise affinity scores.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
Leordeanu et.,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"al. (Leordeanu & Hebert, 2005) show via a spectral relaxation that Lawler’s WAP reduces to solving w∗ = argmaxw wTAw wTw
, w ∈ Rn2 .",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"This is solved by finding the leading eigenvalue λ1(A ).
",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"As illustrated in (Alon et al., 2002), we also use Talagrand’s concentration inequality (Talagrand, 1995).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"We provide a tighter bound in the case of our affinity matrix using Rayleigh’s quotient.
Theorem 2.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"For a random affinity matrix A ∈ Rm×m and for a positive constant t, P[|λ1(A )−M|",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"≥ t] ≤ 4e−t
2/8, whereM is the median of λ1(A ).
",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
Discussion: We further investigate the robustness of affinity-matrix based graph matching solutions when dealing with missing or incomplete data.,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"We show the sharpness of our result in Theorem 5 on the affinity matrix, similar to (Alon et al., 2002), who analyze their results using fat matrices as an example.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Consider an affinity matrix A = (aij) ∈ Rm×m, whose entries are i.i.d.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
Bernoulli distributed.,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Simulating missing affinity scores due to missing edge assignments, we set aij = 1 with probability 1/4 and aij = 0 with probability 3/4.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Notice that our random
affinity matrix represents the Erdős-Rényi graphG(m, 1/4).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"As shown in (Alon et al., 2002), the median and expectation of λ1(A) differ by a constant factor.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Let G = (V,E) denote a general undirected graph, where the degree of each vertex v ∈ V is given by dG : V → Z. Then, the average degree of G is given by d̄ = ∑ v∈V dG(v)/|V | and its maximum degree is ∆",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
= maxv∈V dG(v).,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
It is then well known that d̄ ≤,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"λ1(A) ≤ ∆, i.e., the largest eigenvalue of a graph is squeezed between its average and maximum degree.
",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Let |E| denote the total number of edges in G(m, 1/4), then the average degree of G(m, 1/4) is given by 2|E|/n, where |E| = (Bin( ( m 2 ) , 1/4) and the standard deviation
of |E| is √(
m 2
)",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
(1/4)(3/4) = Θ(m).,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"For large m, our
binomial distribution converges to a normal distribution.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Therefore, we calculate the probability for the total number of edges |E| to deviate from its expectation by t standard deviations as e−Θ(t
2).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Furthermore, we know that if |E| exceeds its expectation by Θ(tn), then the average degree d̄ must also correspondingly exceed its expectation by Θ(t).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Therefore, the probability of the average degree d̄ exceeding its expectation by t standard deviations is at least e−Θ(t
2).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"Given that d̄ ≤ λ1(A), it follows that λ1 exceeding its expectation is also lower bounded by the same e−Θ(t
2).",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
The bounds achieved are tight up to a constant factor in the exponent.,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
Our experimental results on Factorized Graph Matching (FGM) by Zhou et.,3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
"al. (Zhou & De la Torre, 2016) and Re-weighted Random Walk Matching (RRWM) (Cho et al., 2010) also support the finding that affinity matrix based matching solutions are more robust to missing edges due to occlusions in data.",3.2. Eigenvalue bounds on Lawler’s QAP on Random Affinity Matrices,[0],[0]
Following along the same lines as Finke et.,3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
"al. (Martello et al., 1987), we study the asymptotic behavior of the worst to most optimal ratio and present it as the following theorem.",3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
Theorem 3.,3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
"Given random clique adjacency matrices Ak,ln , B k,l n ∼ Pois(λ) and their associated cost matrix Cvv′ ∼ Pois(λ).",3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
We denote by λe := E(Cvv′) and λv := V(Cvv′),3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
the expectation and variance of our Poisson distributed cost function.,3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
"For > 0 and p = Θ ( n −2 (k+l−1) ) ,
we have the following bound on the ratio of the worse to the best solution as
P  max π∈Π ∑ vv′ Cvv′
min π∈Π ∑ vv′ Cvv′ ≤ 1 +  ≥ 1− 2|Π| exp ( −2|Sπ| ( ′ √ λv
′",3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
"+ 2λv
)2) =: ψ(n, )
where, |Π| = n!, |Sπ| = ( n+1
2
) , limn→∞ ψ(n, ) = 1",3.3. Asymptotic Analysis of Higher-Order Clique Assignment,[0],[0]
"Here, we study the robustness of various matching algorithms when affected by missing or incomplete information and transformations (both affine and non-affine) on synthetic and real-world datasets.",4. Experiments,[0],[0]
"For the sake of brevity, we report detailed dataset descriptions in our supplementary notes.",4. Experiments,[0],[0]
The graph matching algorithms can broadly be classified based on their use of (i) affinity-matrix:,4. Experiments,[0],[0]
"FGM (Zhou & De la Torre, 2016; Zhou & De la Torre, 2013)2, RRWM (Cho et al., 2010), (ii) Eigenvalues: EigenAlign (Feizi et al., 2016)3, SM (Leordeanu & Hebert, 2005), SMAC (Cour et al., 2007), PermSync (Pachauri et al., 2013)4, (iii) LP relaxation: GA (Gold & Rangarajan, 1996), Kuhn-Munkres (Kuhn, 1955), (iv) Integer QAP:",4. Experiments,[0],[0]
"IPFP (Leordeanu et al., 2009), (v) Probabilistic matching: PM (Zass & Shashua, 2008), (vi) Higher-order matching given complete data: Tensor (Duchenne et al., 2011)5, and (vii) Geometric and Feature matching: LAI-LP (Li et al., 2013)6.",4. Experiments,[0],[0]
Our code7 is publicly available.,4. Experiments,[0],[0]
Simulated Dataset:,4.1. Effect of Affine Transformations,[0],[0]
"We perform affine transformations on CMU House, which is a sequence of N frames extracted from a video.",4.1. Effect of Affine Transformations,[0],[0]
"More specifically, we uniformly sample frames (at 20% and 40%) and perform affine transformations on the selected frames to distort them.",4.1. Effect of Affine Transformations,[0],[0]
Figure 2 shows examples of affine transformations on house frame sequences.,4.1. Effect of Affine Transformations,[0],[0]
Table 1 shows the comparative error in matching for all the algorithms.,4.1. Effect of Affine Transformations,[0],[0]
"We now describe each affine transformation as performance metrics in our experiments.
",4.1. Effect of Affine Transformations,[0],[0]
Rotation: Figure 8(b) shows a 180◦ rotated version of the original house frame (Figure 8(a)).,4.1. Effect of Affine Transformations,[0],[0]
"Table 1 shows errors in matching when 20% of the frames are rotated by both 20◦ and 60◦, respectively and when the same transformations are applied to 40% of the frames.",4.1. Effect of Affine Transformations,[0],[0]
"As the percentage of transformed frames with greater degree increases, we note a substantial increase in error for other methods in comparison to our method’s error increase.
",4.1. Effect of Affine Transformations,[0],[0]
Reflection: The reflected version of a house frame is shown in Figure 8(c).,4.1. Effect of Affine Transformations,[0],[0]
"Table 1 shows that affinity-based approaches also performed equally well for reflection of house frame sequences.
",4.1. Effect of Affine Transformations,[0],[0]
Scaling: Resizing an image both horizontally and vertically scales the image as is shown in Figure 8(d) .,4.1. Effect of Affine Transformations,[0],[0]
"We fixed the scales to 0.5, 0.75, 1.25, and 1.5 randomly in both the directions in order to transform the images.",4.1. Effect of Affine Transformations,[0],[0]
Our method in Table 1 produces much better matchings than the other methods.,4.1. Effect of Affine Transformations,[0],[0]
2 FGM 3 EigenAlign 4 PermSync 5 Tensor 6,4.1. Effect of Affine Transformations,[0],[0]
This algorithm serves as our naive baseline as it directly uses neighborhood properties of the underlying graph (LAI-LP).,4.1. Effect of Affine Transformations,[0],[0]
7,4.1. Effect of Affine Transformations,[0],[0]
"Our Method
Shearing",4.1. Effect of Affine Transformations,[0],[0]
: We randomly apply shearing on house in one of the directions with shear factor 0.5 (shown in Figure 8(e)) and measured the performance shown in Table 1.,4.1. Effect of Affine Transformations,[0],[0]
"In addition to our method, we find that affinity-based algorithms also produce robust matchings.",4.1. Effect of Affine Transformations,[0],[0]
"To understand the effect of occlusions, we took two realworld datasets, i.e., Books and Building (Pachauri et al., 2013) with severe occlusions which are scenes of the same 3D object taken from arbitrary camera angles.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
These datasets have widely been used in Structure from Motion (SfM) problems and are known to be difficult for matching.,4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Focusing our attention to the last two columns of Table 2, it is evident that our method gives the best results.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Figure 3 shows the Books dataset where books are placed on a table in various orientations with varying levels of occlusion, along with two sample matchings between different pairs of images.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Note that in Figure 3, when a corresponding matching clique is not found in the other image, a match isn’t forced but rather there is no match reported, which doesn’t degrade the matching accuracy.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Matching as many random cliques, in order of decreasing dimensionality, as possible, manifests itself as an advantage over existing methods, especially when dealing with clutter and/or occlusions.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Simulating missing points: In order to gain a deeper insight into the behavior of all the matching algorithms, we
Books Frame 2 and 17
Books Frame 5 and 20
Figure 3.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
Two instances of matchings in Books dataset which is severely occluded.,4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Yellow/green lines show correct/incorrect matches and isolated points show no matches.
omit 2, 4, 6, 8, and 10 (6.66%, 13.33%, 20%, 26.66%, and 33.33%) points out of total House landmark points (i.e., 30 points) from 40% (Figure 4) of frame sequences randomly.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"In general, all algorithms show an increase in error as more points are removed, but our method has a less gradual increase, while eigenvalue related methods show a rather steep increase in error.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Our method is comparable to FGM and RRWM, but the gap in error increases with more missing points.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"We also observed that FGM incurs the longest run-
10 15 20 25 30
Missing Points (%) from 40 % of Images
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
E rr
o r
(% )
OurMethod EigenAlign FGM LAI-LP PermSync RRWM Tensor IPFP PM SMAC SM GA Munkres
Figure 4.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Error (%) in matching when varying the number of missing landmarks in 40% of the images in the frame sequence.
10 20 30 40 50 60 70 80 90 100
Frame Separation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
E rr
o r
(% )
OurMethod EigenAlign FGM LAI-LP PermSync RRWM Tensor IPFP PM SMAC SM GA Munkres
20 40 60 80 100 120 140 160 180 200
Frame Separation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
E rr
o r
(% )
",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"OurMethod EigenAlign FGM LAI-LP PermSync RRWM Tensor IPFP PM SMAC SM GA Munkres
Figure 5.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Error (%) in matching by various methods with different frame separation level for CMU Hotel (on left) and Horse Shear (on right).
time for matching in this scenario.",4.2. Effect of Incomplete and Occluded Landmarks,[0],[0]
"Here, we pick two frames from a video for matching and vary the separation in their frame sequence number.",4.3. Effect of Frame Separation,[0],[0]
The farther apart two frames are the more pronounced is the effect we seek between the frame images.,4.3. Effect of Frame Separation,[0],[0]
"For example, as the frame separation increases, CMU Hotel undergoes a more severe 3D rotation, while Horse-Shear (Caetano et al., 2009) undergoes a larger degree of shear.",4.3. Effect of Frame Separation,[0],[0]
We set p = 0.7 and k = 7 as nearest neighbors to get the correct matchings.,4.3. Effect of Frame Separation,[0],[0]
"In Figure 5, in both the left and right plots we notice that most methods show a very sharp rise in error, while our method is quite stable and reports a 0% error.",4.3. Effect of Frame Separation,[0],[0]
"We observe that the naive baseline, LAI-LP also does well and doesn’t exhibit steep changes in error with larger frame separation.
",4.3. Effect of Frame Separation,[0],[0]
"Experimental Summary: In general, we find that the affinity matrix based methods like FGM and RRWM are more robust to affine transformations than other competing algorithms.",4.3. Effect of Frame Separation,[0],[0]
Our method performs the best as the weight vectors in our algorithm effectively capture even the higher-order geometric properties of the neighborhood and nearly preserves them under affine transformations.,4.3. Effect of Frame Separation,[0],[0]
"The naive baseline, i.e., LAI-LP, does not perform as well because it also has a feature-based component like SIFT which is known to fail on some affine transformations.",4.3. Effect of Frame Separation,[0],[0]
"To the best of our knowledge, we have presented the first approach towards partial higher-order matching by initially capturing higher-order structure as random clique complexes and then proposing a corresponding matching algorithm.",5. Conclusion,[0],[0]
"From a theoretical point of view, we studied matching as a QAP on random clique adjacency matrices that represented the k-skeleton of our random clique complexes and gave bounds on the concentration inequality of the spread of its eigenvalues.",5. Conclusion,[0],[0]
"We also improved bounds on the largest eigenvalue of the Lawler QAP formulation, used by affinitymatrix based approaches.",5. Conclusion,[0],[0]
We discussed the robustness of such approaches to missing points and also showed the sharpness of our result.,5. Conclusion,[0],[0]
"Furthermore, inspired by Finke et.",5. Conclusion,[0],[0]
"al. (Martello et al., 1987)",5. Conclusion,[0],[0]
we studied the asymptotic behavior of our higher dimensional clique adjacency matrices.,5. Conclusion,[0],[0]
"A more detailed investigation of the distribution of eigenvalue gaps for such random matrices with Poisson distributed entries is left for future work.
",5. Conclusion,[0],[0]
"From an empirical perspective, we compared the matching accuracies of diverse algorithms on both synthetic and realworld datasets that were known to have severe occlusions and distortions, thus posing a daunting challenge to matching algorithms.",5. Conclusion,[0],[0]
We argue that our experiments show strong evidence that our approach outperforms all the state-of-theart matching methods on a diverse range of datasets.,5. Conclusion,[0],[0]
"We thank our colleagues from the Mathematics Dept. at IIT-H (Sukumar Daniel, Narasimha Kumar, and Bhakti B. Manna) for their insight and expertise.",Acknowledgements,[0],[0]
We would also like to thank all the reviewers for their feedback and suggestions.,Acknowledgements,[0],[0]
"We are grateful to the authors of (Zhou & De la Torre, 2016; Zhou & De la Torre, 2013; Cho et al., 2010; Feizi et al., 2016; Pachauri et al., 2013; Li et al., 2013; Duchenne et al., 2011) for providing their source codes and datasets.",Acknowledgements,[0],[0]
"Let G(n, p) denote the Erdős-Rényi random graph on n vertices, i.e., G(n, p) = {Gij |1 ≤",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"i < j ≤ n}, where Gij ∼ Ber(p) are i.i.d Bernoulli random variables.",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"We denote the number of k-cliques in the realization of G(n, p) as Xn(k).",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"By definition, a k-clique in a graph G is a subset A of k vertices, which induce a complete subgraph of G. Additionally, no other vertex in G can be joined by edges to all vertices of A. Therefore, we can represent Xn(k) as a sum of indicator random variables 1A, where
1A =",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"{ 1 if A is a k-clique in G(n, p) 0",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"otherwise
(2)
It is clear that Xn(k) = ∑ |A|=k",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"1A. Hence, we get
E(Xn(k))",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
= E( ∑ |A|=k 1A) = ∑ |A|=k E(1A) =,A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"( n k ) p( k 2)
",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
"Using Stirling’s formula, we upper bound Xn(k) as ( en k )k , where e is the Euler’s number.",A.1. Upper Bound to Clique Size in a Random Graph,[0],[0]
We begin by defining the general quadratic assignment problem (QAP) using the Koopman-Beckmann version.,A.2. Quadratic Assignment Problem,[0],[0]
"Let A = (avv′), B = (bvv′) ∈ Rn×n.",A.2. Quadratic Assignment Problem,[0],[0]
"Let Π denote the set of all possible bijections (permutations) π : N → N , where N = {1, 2, . . .",A.2. Quadratic Assignment Problem,[0],[0]
n,A.2. Quadratic Assignment Problem,[0],[0]
}.,A.2. Quadratic Assignment Problem,[0],[0]
"We define the QAP as:
minimize ∑ v,v′ bvv′aπ(v)π(v′)
subject to π ∈ Π
For now on, for ease of notation, we denote the cost function bvv′aπ(v)π(v′) as Cvv′ .",A.2. Quadratic Assignment Problem,[0],[0]
"Given that the QAP is a combinatorial optimization problem, in the case of random symmetric matrices, the subset of feasible solutions Sπ is of the form:
Sπ = {(π(v), π(v′) | v < v′, u, v = 1, . . .",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
", n}
where, |Sπ| = ( n+1
2
) and |Π| = n!.
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
Recall that our cost function Cvv′ has expectation λe and variance λv .,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"For notational convenience, we set ′ = λv− .",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"Then, there exists a bijection π ∈ Π, for which the following
holds by the definition of variance
P  1|Sπ| ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"∣∣∣∣∣∣ ≥ ′ 
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"To proceed further with our proof, we make use of the following lemma by Renyi et.",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"al. (Rényi, 1970).
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
Lemma 2.,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"Let X1, . . .",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
", Xn be independent random variables with |Xk − E(Xk)| ≤ 1, k = 1, . . .",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
", n. Denote
D := √√√√ n∑ k=1 V(Xk)
and let µ be a positive real number with µ ≤ D. Then
P {∣∣∣∣∣ n∑ k=1 (Xk − E(Xk))",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
∣∣∣∣∣,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"≥ µD } ≤ 2 exp ( − µ 2 2(1 + µ/2D)2 )
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"In order to apply Lemma 2, we change the form of the inequality as follows:
P  1|Sπ| ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"∣∣∣∣∣∣ ≥ ′  (3)
≤ ∑ π∈Π P  1|Sπ| ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"− λe) ∣∣∣∣∣∣ ≥ ′  (4)
≤|Π|P  1|Sπ| ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"∣∣∣∣∣∣ ≥ ′  (5)
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"Before applying Lemma 2, we compute D as,
D = √ ∑ v,v′∈π λv = √ |Sπ|λv
We can rewrite (5) as
|Π|P  ( √ λv√ |Sπ| )( 1√ |Sπ|λv )",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
− λe),A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"∣∣∣∣∣∣ ≥ ′ 
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"=|Π|P  ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe) ∣∣∣∣∣∣ ≥",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
( ′ √ |Sπ|√ λv ) ︸,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"︷︷ ︸
µ
(√ |Sπ|λv ) ︸ ︷︷ ︸
D

",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"Now, we make use of Lemma 2 and get
|Π|P  ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
∣∣∣∣∣∣ ≥,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
( ′ √ |Sπ|√ λv )(√ |Sπ|λv ),A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"≤2|Π| exp − ( ′ √ |Sπ|√ λv )2 2 ( 1 +
′",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"√ |Sπ|√ λv
1 2 √ |Sπ|λv
)2 
=2|Π| exp ( −2|Sπ| ( ′ √ λv
′ + 2λv )2)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"Equation 3 can now be written as
P  1|Sπ| ∣∣∣∣∣∣ ∑ v,v′∈π (Cvv′ − λe) ∣∣∣∣∣∣ ≤ ′  ",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"≥ 1−
2|Π| exp ( −2|Sπ| ( ′ √ λv
′ + 2λv )2)",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"It can easily be verified that the expression in the R.H.S. of the above inequality tends to 1 as n→∞.
We know that for the expression ∣∣∣∑v,v′∈π(Cvv′",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"− λe)∣∣∣ ≤
′|Sπ|, the following bounds hold.
|Sπ|(λe − ′) ≤ ∑ v,v′∈π Cvv′ ≤ |Sπ|(λe + ′)
",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"It follows that
max π∈Π ∑ vv′ Cvv′
min π∈Π ∑ vv′ Cvv′ ≤ |Sπ|(λe + ′) |Sπ|(λe",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
− ′) ≤ 1,A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"+
This completes the proof.",A.3. Asymptotic Analysis of Higher-order Clique Assignment (Proof of Theorem 3),[0],[0]
"As illustrated in (Alon et al., 2002), we will make use of Talagrand’s concentration inequality.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"We provide a tighter bound in the case of our affinity matrix using the Rayleigh’s quotient.
Theorem 4.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"(Talagrand, 1995) Let Ω = ∏m i=1",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
Ωi be a product space of probability spaces.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Let A and At be subsets of Ω and if for each y = (y1, . . .",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
", ym) ∈",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"At, there exists a real vector α = (α1, . . .",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
", αm), such that for every x = (x1, . . .",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
", xk) ∈ A, the following inequality holds
∑ i:xi 6=yi |αi| ≤ t ( m∑ i=1 α2i )1/2
Then,
P[A]P[Āt] ≤ e−t 2/4.
Here, At denotes the set with Talagrand distance at most t from A and Āt denotes the complement of set At.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
Theorem 5.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"For a real symmetric matrix A = (aij) ∈ Rm×m and for positive constant t,
P[|λ1(A)−M| ≥ t] ≤ 4e−t 2/8,
whereM is the median of λ1(A).
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
Proof. 8,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Given a real symmetric matrix A = (aij) ∈ Rm×m and a non-zero vetor x, the Rayleigh Quotient R(A, x) is defined as
R(A, x) = x TAx
xTx
Given the eigenvalues of A in decreasing order as λ1(A) ≥ · · · ≥ λm(A), we know thatR(A, x) ∈",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"[λm(A), λ1(a)].",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"It is well known that R(A, x) attains its maximum value at λ1(A)",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"when x = v, where v is the eigenvector corresponding to λ1(A)",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Therefore, we have
λ1(A) = R(A, v) = vTAv
vT v (6)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"In our proof, we omit the constant factor vT v and normalize the eigenvector v, hence ‖v‖ = 1.
Consider the product space Ω of entries aij , 1 ≤",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
i ≤ j ≤ m.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Let t,M be real numbers, where t > 0",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
and M is the median of λ1(A).,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Let A be the set of matrices A = (aij) ∈ Ω, for which λ1(A) ≤",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"M. By definition, P[A] ≥ 1/2.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Additionally, let B be the set of matrices B = (bij) ∈ Ω, for which λ1(B) ≥M+ t. Using Rayleigh’s equation (6) for λ1(A), we rewrite it as a summation of diagonal and off-diagonal terms
λ1(A)",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"= R(A, v) = vTAv = ∑
1≤i<j≤m (vTi vj + v T j vi)aij︸ ︷︷ ︸
off-diagonal
+
m∑ i=1
vTi viaii︸ ︷︷ ︸",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"diagonal ≤M
and λ1(B) = R(B, v) = vTBv = ∑
1≤i<j≤m (vTi vj + v T j vi)bij︸ ︷︷ ︸
off-diagonal
+
m∑ i=1
vTi vibii︸ ︷︷ ︸ diagonal ≥M+ t
8",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Our proof technique follows the technique outlined in (Alon et al., 2002)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"In order to apply Talagrand’s inequality (Theorem 4), we set a real vector α = (αij)1≤i≤j≤m as follows: For offdiagonal (1 ≤ i < j ≤ m) terms, we set
αij = (v T",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"i vj + v T j vi)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"For diagonal (1 ≤ i ≤ m) terms, we set
αii = v T",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"i vi
We proceed by first proving two claims that will be used in this proof.
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
Claim 1.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"∑ 1≤i≤j≤m α2ij ≤ 2
Proof.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"By definition,
∑ 1≤i≤j≤m α2ij = m∑ i=1",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"(vTi vi) 2 + ∑ 1≤i<j≤m (vTi vj + v T j vi) 2
< 2 ( m∑ i=1",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
vTi 2 )( m∑ i=1,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
vi 2 ),A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"= 2 (since ‖v‖ = 1)
This completes the proof.
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
Claim 2.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"For every A ∈ A,∑ 1≤i≤j≤m;aij 6=bij |αij",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"| ≥ t
Proof.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Recall that for matrix A ∈ A, v is the eigenvector with unit-norm corresponding to λ1(A).",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"We know that,
vTAv ≤ λ1(A) ≤M",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"(from set A)
while,
vTBv ≥ λ1(B) ≥M+ t (from set Āt)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"We observe that the entries in affinity matrices A and B, are affinity scores in interval",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"[0, 1].",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Therefore, we have |bij − aij | ≤ 1, for all 1 ≤",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"i, j ≤ m.",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"For ease of notation, let us denote by P , the set of ordered pairs ij with 1ß, j ≤ m where aij 6= bij .",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Then,
t ≤ vT (B −A)v = ∑ i,j∈P (bij − aij)vTi vj
≤ ∑",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"i,j∈P |vTi ||vj | ≤ ∑ i,j∈P |αij",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"|
This completes the proof.
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"By the above two claims, we get the following form:
∑ xi 6=yi |αi| ≥ t > ( t√ 2 ) ∑ 1≤i≤j≤m α2ij 1/2
Applying Talagrand’s inequality, we get
P[λ1(A) ≤M]P[λ1(B) ≥M+ t] ≤ e −1 4
( t√ 2 )2 ≤ e−t 2/8
SinceM is the median of λ1(A), by definition P[λ1(A) ≤",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"M] ≥ 1/2, then
P[λ1(A) ≥M+ t] ≤ 2e−t 2/8 (7)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"Accordingly, we also have that,
P[λ1(A) ≤M− t] ≤ 2e−t 2/8 (8)
Combining results (7) and (8), we have
P[|λ1(A)−M| ≥ t] ≤ 4e−t 2/8 (9)
",A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
This completes the proof.,A.4. Eigenvalue Bounds on Lawler’s QAP Formulation on Random Matrices (Proof of Theorem 2),[0],[0]
"For ease of understanding, we drop the (A) as it is obvious from context.",A.5. Proof of Lemma 1,[0],[0]
"Let λi be the i-th eigenvalue of A, and let xi 6= 0 be its corresponding eigenvector.",A.5. Proof of Lemma 1,[0],[0]
"From Axi = λixi, we have
AXi = λiXi, where Xi := [ xi . . .",A.5. Proof of Lemma 1,[0],[0]
"xi ] ∈Mn \ {0}
It follows,
|λi||||Xi||||||Xi||| = |||λiXi||| = |||AXi||| ≤ |||A||||||Xi|||.
",A.5. Proof of Lemma 1,[0],[0]
"As |||X||| is non-negative, we get |λi| ≤ |||A|||.",A.5. Proof of Lemma 1,[0],[0]
"Thus, every eigenvalue of A is upper bounded by the matrix norm |||A|||.",A.5. Proof of Lemma 1,[0],[0]
"Applying the triangle inequality, we get that δi(A) =",A.5. Proof of Lemma 1,[0],[0]
"|λi+1(A)−λi(A)| ≤ 2|||A|||, which completes the proof.",A.5. Proof of Lemma 1,[0],[0]
"We explained our method with the help of an example shown in Figure 6, Table 3 and Table 4 for a better understanding.",B. Example,[0],[0]
We consider two random graphs G1 and G2 with 6 vertices each in Figure 6(a) for which we perform higher-order matching from 3-cliques 6(b) to 1-cliques 6(d).,B. Example,[0],[0]
"For a higherorder matching, we take the neighbourhood of a barycenter of a clique as the barycenters of the other cliques it is connected to.",B. Example,[0],[0]
"Thus, we place additional nodes of different order in the neighbourhood of each clique in addition to the same order cliques.",B. Example,[0],[0]
"This information would help the cliques to
have more accurate matches.",B. Example,[0],[0]
"The neighbours and matchings of 3-cliques, 2-cliques and 1-cliques are mentioned in the Table 3 and Table 4 for both the graphs G1 and G2 respectively.",B. Example,[0],[0]
"And, the matchings shown in Figure 6(b), 6(c) and 6(d) are based on having the same labels for each barycenter in graph G1 and G2.",B. Example,[0],[0]
We compare the performance of our proposed method with various other matching algorithms on synthetic and real world datasets.,C.1. Setup,[0],[0]
The real world datasets are categorized in Table 5.,C.1. Setup,[0],[0]
"Here, N is the total number of samples with n landmark points in each image to be matched.",C.1. Setup,[0],[0]
We represent random graphs on images in Figure 7 for better understanding and visualization of random graphs for our experiments.,C.1. Setup,[0],[0]
Matchings of two images for real world datasets (Table 5) are shown in Figure 14.,C.1. Setup,[0],[0]
"We created a synthetic dataset from CMU House and Hotel dataset by uniformly sampling 20% and 40% frames from a video sequence and performing affine transformations like rotation, reflection, scaling, and shearing.",C.2. Effect of Affine Transformation,[0],[0]
"We have explained the transformations we considered for this experiment which is similar to Figure (2) and Table (1) in
main paper.",C.2. Effect of Affine Transformation,[0],[0]
Table(1) in main paper shows the results on the CMU House dataset.,C.2. Effect of Affine Transformation,[0],[0]
Affine transformations on Hotel frame are shown in Figure 8.,C.2. Effect of Affine Transformation,[0],[0]
Figure 9 shows the results of matching for the remaining House (fig. 9(a) and 9(b)) and Hotel synthetic dataset for all the algorithms.,C.2. Effect of Affine Transformation,[0],[0]
"We observe that our method produces best results in all the cases, whereas the error for other algorithms either remains stable or increases steeply with the increase in the percentage of transformed frames in the sequence.",C.2. Effect of Affine Transformation,[0],[0]
"We considered two datasets with grave occlusions, mentioned in Table 5.",C.3. Effect of Occlusion,[0],[0]
"Figures 14(i) and 14(j) show the matching of two images for both the datasets, although the matching results are shown in Table (2) in the main paper.",C.3. Effect of Occlusion,[0],[0]
"We also created a synthetic dataset by removing 2, 4, 6, 8, and 10 (6.66%, 13.33%, 20%, 26.66%, and 33.33%) points out of total house landmark points (i.e., 30 points) from 20% and 60% of frame sequences randomly.",C.3. Effect of Occlusion,[0],[0]
Figures 10(a) and 10(b) show the increase in error as we remove more points from images.,C.3. Effect of Occlusion,[0],[0]
We also note the difference in both the results.,C.3. Effect of Occlusion,[0],[0]
"Since we remove points from more percentage of frames in 10(b), there is more gradual increase in the error.",C.3. Effect of Occlusion,[0],[0]
This experimental setup is similar to Figure (4) in our main paper.,C.3. Effect of Occlusion,[0],[0]
It shows that affinity based methods like FGM and RRWM perform well but our method still consistently outperforms all the algorithms.,C.3. Effect of Occlusion,[0],[0]
Figures 11(a) and 11(b) show the frame separation level result of CMU House and Horse Rotate frame sequences.,C.4. Effect of Frame Separation,[0],[0]
We select a pair of frames at a time with increase in their frame separation (x-axis).,C.4. Effect of Frame Separation,[0],[0]
"Here, the House dataset consists of 3D rotations of House whereas Horse Rotate dataset applies rotation with more degree of rotation as the frame separation level increases.",C.4. Effect of Frame Separation,[0],[0]
We see that most of the algorithms performs well for both the datasets even with 0% error.,C.4. Effect of Frame Separation,[0],[0]
"In Figures 12(a) and 12(b), error and computation time of matching two frames of house are shown with different probability p and nearest neighbor k values.",C.5. Effect of k-Nearest Neighbour,[0],[0]
"We observe that as the value of p and k increases, the possibility of mismatching decreases which leads to correct matching.",C.5. Effect of k-Nearest Neighbour,[0],[0]
"On the other hand, the computation time increases since it increases the number of edges in the underlying graph, which in turn leads to a larger number of d-cliques.",C.5. Effect of k-Nearest Neighbour,[0],[0]
This also causes a marked increase in the matching algorithm’s runtime.,C.5. Effect of k-Nearest Neighbour,[0],[0]
"The computation time of our algorithm considers the time of the Kuhn-Munkres algorithm, which is used as a matching algorithm to match two random clique complexes, which takes O(n3) running time.
",C.5. Effect of k-Nearest Neighbour,[0],[0]
"The overall time increases as we increase the value of p and k, since it increases the probability of an edge occurrence between two landmark points.",C.5. Effect of k-Nearest Neighbour,[0],[0]
"As the number of edges increase in a random graph, the number of d-cliques also increase.",C.5. Effect of k-Nearest Neighbour,[0],[0]
"Due to this phenomenon, the runtime of the KuhnMunkres algorithm also increases.
",C.5. Effect of k-Nearest Neighbour,[0],[0]
"Figure 12(c) shows the computation time of matching two
Correlation: 0.7267
Correlation: 0.3823
images with varying k-NN for different n landmark points in the image.",C.5. Effect of k-Nearest Neighbour,[0],[0]
We can clearly see that the time increases with increasing k and a larger number of landmark points.,C.5. Effect of k-Nearest Neighbour,[0],[0]
"Here, 60 landmark points take maximum time for the highest value of k.",C.5. Effect of k-Nearest Neighbour,[0],[0]
"On the other hand, if we consider lower values of k, even 60 landmark points take a reasonable amount of time to match, which is comparable to lower values of n. Thus, we set k value as low as possible for matching, depending on the complexity of the dataset.",C.5. Effect of k-Nearest Neighbour,[0],[0]
We analyze the performance of our method over other pairwise algorithms for two different noise models.,C.6. Noise Model,[0],[0]
"We follow the noise model setup mentioned in (Feizi et al., 2016).",C.6. Noise Model,[0],[0]
"We introduce noise in one random graph G1 and generate a noisy version G̃ to be matched with G2. G1 is a random graph here which is created as G1(n, p) with n nodes and p probability.",C.6. Noise Model,[0],[0]
We describe two noise models as follows:,C.6. Noise Model,[0],[0]
"G̃ = G1 (1−A) + (1−G1) A (10)
G̃ is generated using the aforementioned equation where A is a binary random symmetric matrix, whose entries are drawn from a Bernoulli distribution asA(n, q) with n nodes and q probability and represents the element-wise multiplication of matrices.",Noise Model I:,[0],[0]
This model flips the node-node adjacency of G1 with probability q.,Noise Model I:,[0],[0]
"G̃ = G1 (1−A) + (1−G1) B (11)
",Noise Model II:,[0],[0]
"Again, A and B are binary random symmetric matrices, whose entries are drawn from the Bernoulli distribution as A(n, q) and B(n, r) with n nodes and q and r probabilities, respectively.",Noise Model II:,[0],[0]
"This model flips node-node adjacency of G1 with probability q, and in addition it also creates edges between non-connected nodes with probability r.
Results of noise model I and II on CMU Hotel and Horse Shear for frame separation level is shown in Figures 13(a), 13(c) and 13(b), 13(d) respectively.",Noise Model II:,[0],[0]
We observe that our method is robust to noise for both the models as compared to other algorithms since there is a very small increase or no increase in error (%) for all the cases.,Noise Model II:,[0],[0]
"We present an alternate formulation of the partial assignment problem as matching random clique complexes, that are higher-order analogues of random graphs, designed to provide a set of invariants that better detect higher-order structure.",abstractText,[0],[0]
"The proposed method creates random clique adjacency matrices for each k-skeleton of the random clique complexes and matches them, taking into account each point as the affine combination of its geometric neighborhood.",abstractText,[0],[0]
"We justify our solution theoretically, by analyzing the runtime and storage complexity of our algorithm along with the asymptotic behavior of the quadratic assignment problem (QAP) that is associated with the underlying random clique adjacency matrices.",abstractText,[0],[0]
"Experiments on both synthetic and real-world datasets, containing severe occlusions and distortions, provide insight into the accuracy, efficiency, and robustness of our approach.",abstractText,[0],[0]
We outperform diverse matching algorithms by a significant margin.,abstractText,[0],[0]
Solving Partial Assignment Problems using Random Clique Complexes,title,[0],[0]
Transfer learning (TL) methods show specially appealing for real-world applications where the data from the target domain is scarce but a good amount of data from another source domain is available.,1. Introduction,[0],[0]
"With research efforts largely confined to the single-source setting (Pan et al., 2011; Wei et al., 2016; Zhou et al., 2016), an increasing amount of studies are contributing to a realistic applicability of TL by addressing the multi-source scenario, mainly for classifica-
1School of Computer Science and Engineering, Nanyang Technological University, Singapore 2Rolls-Royce@Nanyang Technological University Corporate Lab 3Rolls-Royce Advanced Technology Centre, Singapore.",1. Introduction,[0],[0]
"Correspondence to: Pengfei Wei <Pwei001@e.ntu.edu.sg>, Ramon Sagarna <saramon@ntu.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"tion (Tommasi et al., 2014; Fang et al., 2015; Bhatt et al., 2016).",1. Introduction,[0],[0]
"The problem of regression, however, has been much less studied, despite of the variety of real-world domains in which it arises; for instance wifi or indoor signal location (Pan et al., 2008), biological data analysis (Lam et al., 2016), or mechanical system design (Ghosh et al., 2015).",1. Introduction,[0],[0]
"In this work, we concentrate on multi-source transfer regression (MSTR) based on Gaussian process (GP) models.
",1. Introduction,[0],[0]
"All the way through, the TL community has been paying attention to modeling the similarity between different domains so that only the source knowledge that is helpful for the target domain is transferred.",1. Introduction,[0],[0]
"This is because designing a TL method based on the assumption that domains are mutually relevant may lead to negative transfer (Pan & Yang, 2010).",1. Introduction,[0],[0]
Similarity capture is particularly crucial in multi-source TL as the transfer capacity to the target task may differ considerably across the diverse source domains.,1. Introduction,[0],[0]
"Thus, TL methods that are capable of tuning the strength of the knowledge transfer to the similarity of the domains are attracting increasing interest (Luo et al., 2008; Wang et al., 2014; Al-Stouhi & Reddy, 2011).
",1. Introduction,[0],[0]
"As regards to MSTR, a key issue is to capture the diverse Source-Target (S-T) similarities.",1. Introduction,[0],[0]
The relatively few efforts to date have focused on ensemble methods.,1. Introduction,[0],[0]
"Particularly, an amount of works rely on the boosting strategy due to its capability to capture fine-grained S-T similarities be weighting the contribution of train instances individually (Dai et al., 2007; Pardoe & Stone, 2010; Yao & Doretto, 2010).",1. Introduction,[0],[0]
"However, as outlined in (Al-Stouhi & Reddy, 2011), such an instance-based similarity strategy in boosting has shown issues with slow/premature weights convergence that have seriously penalized the computational cost or the transfer performance.",1. Introduction,[0],[0]
"Another type of ensemble strategy for multisource transfer is stacking (Wolpert, 1992).",1. Introduction,[0],[0]
"Pardoe and Stone propose a meta-model that aggregates the predictions of several base models previously learned with each source in isolation (Pardoe & Stone, 2010) .",1. Introduction,[0],[0]
The aggregation is done by assigning each base model a model importance.,1. Introduction,[0],[0]
"In this case, the S-T similarities can be captured through the model importance.",1. Introduction,[0],[0]
"However, in such stacking-based methods, the base models suffer from a lack of consideration of the dependencies between the different source domains.
",1. Introduction,[0],[0]
"Another popular idea to model the S-T similarity is to construct a transfer covariance function that relates two data points from distinct domains through the similarity coefficients (Bonilla et al., 2008; Williams et al., 2009).",1. Introduction,[0],[0]
"Such idea has been proposed in multi-task learning (Bonilla et al., 2008), where each task pair is assigned a particular similarity coefficient.",1. Introduction,[0],[0]
"Note, however, that multi-task learning differs from the TL problem in that the former aims at improving performance across all the domains while the objective of the latter focuses on the target domain only.",1. Introduction,[0],[0]
"Nevertheless, the idea of transfer covariance function is referential for the TL problem.",1. Introduction,[0],[0]
"In (Cao et al., 2010), a single source transfer covariance function (TCSS) was proposed.",1. Introduction,[0],[0]
"In the corresponding transfer covariance matrix, one similarity coefficient was assigned to the S-T block to model the inter-domain similarity.",1. Introduction,[0],[0]
"A GP with such TCSS (called GP-TCSS) was then trained for the transfer task.
",1. Introduction,[0],[0]
"When generalizing to MSTR, one may naturally consider a multi-source transfer covariance function (TCMS) with different similarity coefficients attached to distinct S-T blocks in the corresponding transfer covariance matrix.",1. Introduction,[0],[0]
"In this work, we investigate the feasibility of such covariance function.",1. Introduction,[0],[0]
We theoretically prove that a general GP with TCMS (GP-TCMS) fails to capture the similarity diversities of various S-T domain pairs.,1. Introduction,[0],[0]
"Although TCMS intends to utilize different similarity coefficients, the learnt GPTCMS would give the same similarity coefficient for all the S-T domain pairs.",1. Introduction,[0],[0]
The generalization error bounds of the learnt GP-TCMS show that this coefficient is taking effect in every source domain.,1. Introduction,[0],[0]
"Considering the diverse S-T similarities between the sources and the target, this may jeopardize the transfer performance, especially when the number of sources increases.",1. Introduction,[0],[0]
"Moreover, the learning of GPTCMS rapidly poses a computational issue with increasing amounts of source domains as usually O(m3) computations are required to evaluate a model for m data points.
",1. Introduction,[0],[0]
The unsatisfactory performance of GP-TCMS leads us to exploit the transfer covariance function in another way.,1. Introduction,[0],[0]
"Considering that both the stacking strategy and the transfer covariance function can model the S-T similarity and using the transfer covariance function at the base models would therefore add flexibility to the similarity capture capability of the stacking approach, we propose to integrate them into one unified model.",1. Introduction,[0],[0]
"Specifically, we first discuss TCSSStack, a method that simply stacks GP-TCSS base models.",1. Introduction,[0],[0]
TCSSStack alleviates the computational issue of GP since it allows to stretch the number of sources due to its O(Nn3) cost for N sources with n points each.,1. Introduction,[0],[0]
"However, TCSSStack still suffers from the aforementioned limitation of conventional stacking.",1. Introduction,[0],[0]
"Thus, we propose a more involved TCMSStack.",1. Introduction,[0],[0]
"Two salient features make TCMSStack significantly different from TCSSStack: (i) it associates the similarity coefficients in the base GP-TCSS
with the model importance during learning, and (ii) it learns the model importance and the base GP-TCSS jointly.",1. Introduction,[0],[0]
"By doing so, on the one hand, TCMSStack further reduces the computational cost by lowering the number of optimization variables.",1. Introduction,[0],[0]
"On the other hand, although the similarity coefficient in TCMSStack represents bivariate S-T similarity relations, they are elicited by pondering all the inter-domain dependencies.",1. Introduction,[0],[0]
"In the experiments, we show the superiority of TCMSStack on the transfer performance compared to TCSSStack, GP-TCMS , and other MSTR methods.",1. Introduction,[0],[0]
A main challenge in MSTR is to precisely capture the diverse S-T transfer capacities across the different sources.,2. Related Work,[0],[0]
"Ensemble approaches (Dai et al., 2007), which can provide an explicit, fine-grained similarity capture, are widely used to handle the MSTR problems.",2. Related Work,[0],[0]
"In (Pardoe & Stone, 2010), TrAdaBoost.R2 was proposed, a boosting based algorithm that weights the contribution of train instances individually, and thus delivers a model accounting for the S-T similarities for every instance.",2. Related Work,[0],[0]
"However, such boostinglike methods suffer from slow/premature convergence issues that tremendously jeopardize the transfer performance (Al-Stouhi & Reddy, 2011).",2. Related Work,[0],[0]
"Pardoe and Stone also introduced a multi-source transfer stacking in which base models are pretrained in different source domains separately, and a meta-model is trained by aggregating the outputs of the base models (Pardoe & Stone, 2010).",2. Related Work,[0],[0]
"By doing so, the S-T similarities are captured at meta-model level by the learnt model importance.",2. Related Work,[0],[0]
"Although the stacking methods show success in some MSTR problems, they have the limitation that inter-domain dependencies between sources are ignored at the base models.
",2. Related Work,[0],[0]
At the other end of the spectrum are transfer covariance function representing a multivariate similarity relation over sources and target domains.,2. Related Work,[0],[0]
"A popular representative of this family is the work by Bonilla et al. (Bonilla et al., 2008) on multi-task learning, where a free-form kernel relates each pair of tasks.",2. Related Work,[0],[0]
"Apart from the difference of the application domain (multi-task learning versus TL), this kind of models often imply fitting an increasingly large number of hyperparameters; e.g. in the free-form kernel, this number grows as (N2 − N)/2, where N is the number of sources.",2. Related Work,[0],[0]
"Motivated by (Bonilla et al., 2008), (Cao et al., 2010) develops another transfer covariance function for the single source transfer.
",2. Related Work,[0],[0]
"In this work, we first describe a family of transfer covariance functions, and investigate their feasibility for MSTR.",2. Related Work,[0],[0]
"With the theoretical analysis showing the unsatisfactory performance of such transfer covariance function, we propose to unify the S-T similarity capture of stacking and the transfer covariance function.",2. Related Work,[0],[0]
"To the best of our knowledge,
this is the first work that analyzes the feasibility and performance of such family of transfer covariance functions for MSTR, and further combines them with stacking.",2. Related Work,[0],[0]
We denote a domain set for MSTR as D = S ∪ T where S = {Si : 1 ≤ i ≤ N} is a set of source domains and T is the target domain.,3. Problem Statement,[0],[0]
All source domain data and few target domain data are labeled.,3. Problem Statement,[0],[0]
Denote the data matrix and its corresponding label vector in each source domain Si as X(Si) ∈ RnSi×d and y(Si) ∈ RnSi .,3. Problem Statement,[0],[0]
"Likewise, we represent the target data set with X(T ) =",3. Problem Statement,[0],[0]
"{X(Tl),X(Tu)} where X(Tl) ∈",3. Problem Statement,[0],[0]
RnTl×d is the labeled target data matrix and X(Tu) ∈ RnTu×d is the unlabeled one.,3. Problem Statement,[0],[0]
We further define y(Tl) ∈ RnTl as the label vector for X(Tl).,3. Problem Statement,[0],[0]
"Moreover, we assume nTl min(nS1 , ..., nSN , nTu).",3. Problem Statement,[0],[0]
"Our objective is to utilize {X(Si),y(Si)}Ni=1 and {X(Tl),y(Tl)} to predict labels for X(Tu).
",3. Problem Statement,[0],[0]
We use the GP model for this regression task.,3. Problem Statement,[0],[0]
"We denote the underlying latent function between the inputs x and the outputs y as f , and the noise variance as σ2.",3. Problem Statement,[0],[0]
"Thus, f denotes the function vector over X. A GP model defines a Gaussian distribution over the functions, f ∼ N (µ,K) in which µ is the mean vector and K is the covariance matrix which is positive semi-definite (PSD, or equivalently denoted as K 0).",3. Problem Statement,[0],[0]
"Usually µ is assumed to be 0, and thus the GP model is completely specified by K given a covariance function which is parameterized by Ω.",3. Problem Statement,[0],[0]
"In this section, we analyze the transfer performance of GP using a specific family of transfer covariance function.",4. GP with Transfer Covariance Function,[0],[0]
"Since the GP model is specified by K, one straightforward way to achieve the knowledge transfer across multiple source domains and the target domain is to design a transfer covariance function for multi-source.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Different from a classical GP which uses a fixed covariance function for the data from different domains, we focus on the covariance function of the form (TCMS):
k∗(x,x ′) =  λik(x,x ′), x ∈ X(Si) & x′ ∈ X(T ) or x ∈ X(T )& x′ ∈ X(Si),
k(x,x′), otherwise.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(1)
where k(·, ·) is any valid covariance function, and λi is the metric measuring the similarity between the source Si and the target T .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Through the learning, λi is expected to capture the different transfer strengths in different S-T domain pairs.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Those highly target-related sources will play a more
important role in transfer, while those completely targetunrelated sources will not be considered.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"However, to guarantee the GP model is always valid, any covariance matrix K∗ constructed by k∗(·, ·) should be PSD.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Theorem 1 gives the sufficient and necessary condition for a PSD K∗. Theorem 1.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Let KDiDj (Di,Dj ∈ D) denote a covariance matrix for points in Di and Dj .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"A Gram matrix
K∗ =  KS1S1 ...",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
KS1SN λ1KS1T ... ... ...,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"...
KSNS1 ...",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
KSNSN λNKSNT λ1KT S1 ...,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"λNKT SN KT T  is PSD for any covariance matrix K in the form
K =  KS1S1 ...",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
KS1SN KS1T ... ... ...,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"...
KSNS1 ...",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
KSNSN KSNT KT S1 ...,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"KT SN KT T  if and only if λ1 = ... = λN and |λi| ≤ 1.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Proof.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Necessary condition: Let K∗ be a PSD matrix.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"We use KSS to represent the sources-to-sources block matrix, and KST , KST ∗ to represent the sources-to-target block matrix in K and K∗, respectively.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Thus, we have:
K =",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[ KSS KST KTST KT T ] ,K∗ =",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[ KSS KST ∗ KTST ∗ KT T ] .
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Since K is PSD, according to the Schur complement theorem (Zhang, 2006), we have:
(I−KSSK̃SS)KST = 0, (2)
KT T −KTST K̃SSKST 0, (3)
where K̃SS is the generalized inverse of KSS .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"By rewriting K̃SS as a block matrix using K̃SiSj as the element, we further derive eq.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
(2) and eq. (3) as: KS1T,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"N∑ j=1 KS1SiK̃SiSjKSjT ...
KSNT",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"N∑ j=1 KSNSiK̃SiSjKSjT
 = 0, (4)
KT T",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
N∑ j=1 KT SiK̃SiSjKSjT 0.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(5)
Likewise, for the PSD matrix K∗, we have the following two Schur complement derivations: λ1KS1T − N∑ i=1",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"N∑ j=1 λjKS1SiK̃SiSjKSjT ...
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
λNKSNT,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"N∑ j=1 λjKSNSiK̃SiSjKSjT  = 0. (6)
KT T",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
N∑ j=1 λiλjKT SiK̃SiSjKSjT 0.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(7)
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Combining eq. (4) and eq.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(6), we get: N∑ i=1",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"N∑ j=1 (λ1 − λj)KS1SiK̃SiSjKSjT
...",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
N∑ j=1 (λN,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− λj)KSNSiK̃SiSjKSjT
 = 0. (8)
Since Eq.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(8) must hold for all PSD K, we induce λ1 = ... = λN",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
= λ.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Based on such conclusion, we combine eq.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(5) and eq. (7):
(1− λ2)KT T + λ2M 0, (9)
where M = KT T",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
− N∑ i=1,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
N∑ j=1 KT SiK̃SiSjKSjT .,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Since eq. (9) must hold for all PSD KT T and PSD M, we resolve that |λ| ≤ 1.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Sufficient condition: Let λ1 = ...,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
= λN,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"= λ, and |λ| ≤ 1.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"According to the Theorem 1 in (Cao et al., 2010), we obtain that K∗ is a PSD matrix.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"To sum up, we conclude that if K∗ is a PSD matrix, λi should satisfy λ1 = ... = λN and |λi| ≤ 1.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"From Theorem 1, we can see that |λi| ≤ 1, which means a highly target-related source results in a full transfer of KSi,T , but a completely target-unrelated source results in a zero block matrix.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
This indicates the adaptiveness of λi.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"However, Theorem 1 also shows that k∗(·, ·) can just give one similarity coefficient for all S-T domain pairs to ensure the validity of GP-TCMS .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Such single similarity coefficient compromises the diverse similarities between different S-T domain pairs.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"This violates the original intention of λi which is to distinguish the similarity diversity between different S-T domain pairs.
4.2.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Generalization Bounds of GP-TCMS
To investigate the effect of a single compromised similarity coefficient on the performance of GP-TCMS , we derive its generalization error bounds.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"In (Chai, 2009), an earlier analysis can be found for the generalization errors and learning curves in multi-task learning (specifically, two learning tasks with the same noise variance).",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Our investigation is different from that work however as we are working on a TL setting, and more importantly, on multiple sources with different noise variances.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"We denote the single compromised similarity coefficient as λ, and the noise variance for different domains as σ2d, d ∈ {S1, ...,SN , T }.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Thus, the transfer covariance matrix of
the noisy training data is C∗ = K∗ + Σ where
K∗ =",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[ KSS λKST λKTST KT T ] ,Σ =",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[ ΣS 0 0 σ2T IT T ] and ΣS is a diagonal block matrix with the diagonal block elements {σ2S1IS1S1 , ..., σ 2 SN ISNSN }.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"According to (Rasmussen, 2006), if the GP prior is correctly specified, the generalization error at a point is also the posterior variance at such point.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Specifically, for GP-TCMS , the posterior variance at the target point xt is:
δ2T (xt, λ, {X(Si), σ2Si} N i=1,XT , σ 2 T )
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"= ktt − kT∗tC−1∗ k∗t, (10)
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"where kT∗t = (λk T St,k T T t), kSt (kT t) is the vector of covariances between {X(Si)}Ni=1 (X(T )) and xt, and ktt is the prior variance at xt.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Wherever it is not misleading, we will simplify the posterior variance expression using δ2T (λ, {σ2Si} N i=1, σ 2 T ).",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
The generalization error for the target domain can be obtained by averaging eq.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(10) over xt:
T (λ, {X(Si), σ2Si} N i=1,XT , σ 2 T )
= ∫ δ2T (xt, λ, {σ2Si} N i=1, σ 2 T )p(xt)dxt.
(11)
To derive the generalization error bounds for GP-TCMS , we first rewrite
C∗ = Λ
[ λ−2(KSS + ΣS) KST
KTST KT T + σ 2 T IT T
] Λ,
where Λ = [ λISS 0
0 IT T
] .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Thus, the posterior variance
at point xt becomes:
δ2T (λ, {σ2Si} N i=1, σ 2 T ) = ktt−kTt Φ(λ, {σ2Si} N i=1, σ 2 T ) −1kt, (12) where kTt = (k T St,k T T t) and
Φ(λ, {σ2Si} N i=1, σ 2 T )
=
[ λ−2(KSS + ΣS) KST
KTST KT T + σ 2 T IT T
] .
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Note that the above derivation excludes the situation where λ = 0.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"When λ = 0, all the source domains are unrelated to the target domain, and thus no knowledge is transferred.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
This is easy to verify by plugging λ = 0 into eq.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(12).
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Further, we observe that δ2T is equal for λ and −λ, so we only investigate the case λ ∈ (0, 1].",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"For eq. (12), we further decompose it as:
Φ(λ, {σ2Si} N i=1, σ 2 T )
=",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
[ KSS KST KTST KT T ] + [ σ2T ISS 0 0,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
σ2T IT T ],4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"+
(λ−2 − 1)",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[
KSS + ΣS 0 0 0
] + [ ΣS − σ2T ISS 0
0 0 ]",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"= Φ(1, {σ2T , ..., σ2T }N , σ2T ) + E1 + E2,
(13)
where E1 = (λ−2 − 1)",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[
KSS + ΣS 0 0 0 ] and E2 =[
ΣS − σ2T ISS 0 0 0
] .",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Eq. (13) unveils that the posterior
variance of having instances from different source domains is equivalent to the posterior variance of having those instances from target domain with two additional correlated noise terms, E1 and E2.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"This shows us the two main factors that matter in the transfer performance; namely, the S-T similarity and the noise variances.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"To further analyze how the S-T similarity affects the transfer performance, we focus on one factor and fix the other.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Assuming that all the sources are totally related to the target, i.e. λ = 1, and consequently, the noise variance for each source becomes ξ2Si , we define the difference:
∆ = δ2T (1, {ξ2Si} N i=1, σ 2 T )− δ2T (λ, {σ2Si} N i=1, σ 2 T ).
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"To obtain the upper (lower) bound of δ2T (λ, {σ2Si} N i=1, σ 2 T ), we are interested in those ξ 2
Si (ξ 2 Si ) that make ∆ ≥ 0 (∆ <
0) for all the target points.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Proposition 1.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Let δ,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"and δ be the maximum and minimum eigenvalues of KSS , ξ 2 Si = λ",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"−2σ2Si − (1 − λ −2)δ and ξ2Si = λ −2σ2Si − (1 − λ
−2)δ for every source Si.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Then, for all the target data points, δ2T (1, {ξ 2
Si }Ni=1, σ2T ) ≤
δ2T (λ, {σ2Si} N i=1, σ 2 T ) ≤",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"δ2T (1, {ξ
2 Si} N i=1, σ 2 T ).
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Proof.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"By applying eq. (12), we have:
∆ = δ2T (1, {ξ2Si} N i=1, σ 2 T )",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− δ2T (λ, {σ2Si} N i=1, σ 2 T )
= kTt [Φ(λ, {σ2Si} N i=1, σ 2 T ) −1",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− Φ(1, {ξ2Si} N i=1, σ 2 T ) −1]kt
To make ∆ ≥ 0 for all the target data points, we need to prove Φ(λ, {σ2Si} N i=1, σ 2 T ) −1−Φ(1, {ξ2Si} N i=1, σ 2 T ) −1 is PSD, which means:
Φ(λ, {σ2Si} N i=1, σ 2 T ) −1",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− Φ(1, {ξ2Si} N i=1, σ 2 T ) −1 0
⇐⇒ Φ(λ, {σ2Si} N i=1, σ 2 T ) Φ(1, {ξ2Si} N i=1, σ 2 T ) ⇐⇒",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"[
(1− λ−2)KSS + (Σ′S",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− λ−2ΣS) 0 0 0
] 0
where Σ′S is a diagonal block matrix with the diagonal block elements {ξ2S1IS1S1 , ..., ξ 2 SN ISNSN }
⇐⇒ (1− λ−2)KSS + (Σ′S",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"− λ−2ΣS) 0
⇐⇒ KSS (λ−2ΣS −Σ′S)
(1− λ−2)
⇐⇒ δ ≤ (λ−2σ2Si − ξ 2 Si)
(1− λ−2) for every Si
⇐⇒ ξ2Si ≥ λ",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"−2σ2Si − (1− λ −2)δ for every Si
Note that ∆ is a monotonically increasing function of ξ2Si , thus we take the minimum λ−2σ2Si − (1− λ −2)δ as ξ 2
Si to be the smallest upper bound of σ2T (λ, {σ2Si} N i=1, σ 2 T ).",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Similarly, we have ξ2Si = λ −2σ2Si − (1 − λ
−2)δ to construct the largest lower bound of σ2T (λ, {σ2Si} N i=1, σ 2 T ).
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Proposition 1 gives the lower and upper bounds of the posterior variance δ2T (λ, {σ2Si} N i=1, σ 2 T ).",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
By applying eq.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"(11), we readily obtain the generalization error bounds.
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Corollary 1.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Let
T (λ, {σ2Si} N i=1, σ 2 T ) = T (1, {ξ
2 Si} N i=1, σ 2 T )
T (λ, {σ2Si} N i=1, σ 2 T ) = T (1, {ξ 2 Si }Ni=1, σ2T )
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Then, T (λ, {σ2Si} N i=1, σ 2 T ) ≤ T (λ, {σ2Si} N i=1, σ 2 T ) ≤
T (λ, {σ2Si} N i=1, σ 2 T ).
",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
Proposition 1 serves to demonstrate that λ takes effect in every source on the final transfer performance.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"With the assumption that different source domains have different ST similarities with the target domain, a single λ that works for every sources has a great difficulty capturing such ST similarity diversity.",4.1. Transfer Covariance Function for Multi-Source,[0],[0]
This leads us to exploit the transfer covariance function in another way.,4.1. Transfer Covariance Function for Multi-Source,[0],[0]
"Considering the effectiveness showed by the stacking strategy for MSTR (Pardoe & Stone, 2010), we propose a framework that can integrate the capability for S-T similarity capture of both the transfer covariance function and stacking.",5. Transfer Covariance Function Stacking,[0],[0]
"We first introduce TCSSStack, a conventional way of stacking the transfer covariance function.",5. Transfer Covariance Function Stacking,[0],[0]
"Then, we design a more involved stacking-inspired approach that overcomes some limitations of the conventional stacking method.
5.1.",5. Transfer Covariance Function Stacking,[0],[0]
"Conventional Transfer Stacking TCSSStack
Motivated by the fact that both the stacking strategy and the transfer covariance function can model the S-T similarity and using the transfer covariance function at the base models would therefore add flexibility to the similarity capture capability of the stacking approach, we propose a TCSSStack method.",5. Transfer Covariance Function Stacking,[0],[0]
"In TCSSStack, we first train multiple GP-TCSS models using each Si and T (denoted as {f (Si,T )(·|Ωi, λi)}Ni=1) and then apply the conventional stacking strategy to combine their predictions.",5. Transfer Covariance Function Stacking,[0],[0]
"Given a target point x, the final prediction is given by:
f(x) = ∑N
i=1",5. Transfer Covariance Function Stacking,[0],[0]
"ωif
(Si,T )(x|Ωi, λi), ∑N
i=1 ωi",5. Transfer Covariance Function Stacking,[0],[0]
"= 1 (14)
where ωi are coefficients learned by minimizing the least square error on the target labeled data.
",5. Transfer Covariance Function Stacking,[0],[0]
There are two major issues for the above model.,5. Transfer Covariance Function Stacking,[0],[0]
"(1) Since each f (Si,T ) is pretrained separately, the parameters learnt for each f (Si,T ) do not take the inter-domain dependencies between different source domains into account.",5. Transfer Covariance Function Stacking,[0],[0]
(2) Both λi and ωi reflect the S-T domain similarity.,5. Transfer Covariance Function Stacking,[0],[0]
"However, TCSSStack takes them as two different variables and learns them separately.",5. Transfer Covariance Function Stacking,[0],[0]
"Intuitively, the model importance ωi should be positively correlated with the similarity coefficient λi.",5. Transfer Covariance Function Stacking,[0],[0]
"For example, the prediction of a GP-TCSS using an unrelated source is less trustful, and should be assigned a smaller coefficient in the stacking.
",5. Transfer Covariance Function Stacking,[0],[0]
5.2.,5. Transfer Covariance Function Stacking,[0],[0]
"Improved Transfer Stacking TCMSStack
To overcome the above issues, we propose a new transfer stacking model (TCMSStack) as follows:
f∗(x) = ∑N
i=1",5. Transfer Covariance Function Stacking,[0],[0]
"(g(λi)/Z)f
(Si,T )(x,Ωi, λi).",5. Transfer Covariance Function Stacking,[0],[0]
"(15)
where λi refers to the similarity coefficient in the GP-TCSS for the i-th source, Z = ∑N i=1 g(λi) is the normalization term, and g(λi) is any function preserving the monotonicity of |λi| so that it coordinates the model importance and the similarity coefficient.",5. Transfer Covariance Function Stacking,[0],[0]
This also reduces the search efforts by lowering the number of free parameters to fit.,5. Transfer Covariance Function Stacking,[0],[0]
"Moreover, instead of pretraining f (Si,T )(·|Ωi, λi) separately, we jointly learn f (Si,T )(·,Ωi, λi) for all the source domains.",5. Transfer Covariance Function Stacking,[0],[0]
"By doing so, the multiple GP-TCSS models are learned together with the dependencies between multiple sources taken into account.
",5. Transfer Covariance Function Stacking,[0],[0]
Notice that the model in eq.,5. Transfer Covariance Function Stacking,[0],[0]
(15) allows for multiple options to characterize the relative importance of GP-TCSS models through g(·).,5. Transfer Covariance Function Stacking,[0],[0]
"In this paper, we use a simple function g(λi) = |λi|.",5. Transfer Covariance Function Stacking,[0],[0]
"However, the absolute value function is not smooth at the origin.",5. Transfer Covariance Function Stacking,[0],[0]
"Thus, we use a smooth function studied in (Yong, 2015) to approximate it as follows:
|λi|",5. Transfer Covariance Function Stacking,[0],[0]
"≈ αLn( 1 2 e λi α + 1 2 e− λi α ).
",5. Transfer Covariance Function Stacking,[0],[0]
"We set α = 0.01 which is the best approximation stated in (Yong, 2015).",5. Transfer Covariance Function Stacking,[0],[0]
"Since Theorem 1 also tells us −1 ≤ λi ≤ 1, we propose to define λi = 2(1/(1 + µi))bi − 1 (µi ≥ 0 and bi ≥ 0), as in (Cao et al., 2010).",5. Transfer Covariance Function Stacking,[0],[0]
"Then, we conduct the learning by minimizing the squared errors:
min {Ωi,µi,bi}Ni=1
∑nTl j=1",5. Transfer Covariance Function Stacking,[0],[0]
(y (Tl) j − f ∗(x (Tl) j )) 2 .,5. Transfer Covariance Function Stacking,[0],[0]
"(16)
In the optimization, we propose to use the conjugate gradient method.",5. Transfer Covariance Function Stacking,[0],[0]
Other optimization methods can also be applied to solve this objective function.,5. Transfer Covariance Function Stacking,[0],[0]
"As in usual GP model training, the computational time complexity of each f (Si,T ) is dominated by the calculation
of the inverse of its covariance matrix, i.e. O((nTl+nSi)3).",5.3. Complexity Analysis,[0],[0]
"Considering nTl nSi and assuming nS1 = ... = nSN = nS , the evaluation of a TCMSStack model takes then O(Nn3S).",5.3. Complexity Analysis,[0],[0]
Notice that by following the stacking strategy of eq.,5.3. Complexity Analysis,[0],[0]
"(14) the training involves the steps of learning each f (Si,T ) and subsequently learning the ωi coefficients.",5.3. Complexity Analysis,[0],[0]
"The latter calls for some cross-validation approach to evaluate a meta-model, as the f (Si,T ) from the previous step have been induced using the target data (Pardoe & Stone, 2010).",5.3. Complexity Analysis,[0],[0]
"In the extreme case of leave-one-out, this would take O(nTlNn3S).",5.3. Complexity Analysis,[0],[0]
Even if we also choose a leave-one-out validation to solve eq.,5.3. Complexity Analysis,[0],[0]
"(16) the cost of a TCMSStack model evaluation would be lower, since the first step of stacking is not required.",5.3. Complexity Analysis,[0],[0]
"On the other side, by following TrAdaBoost.R2 or the GP-TCMS approach, a GP model evaluation requires O((NnS)3), which even exceeds the cost for TCMSStack using leave-one-out whenever N > √ nTl .",5.3. Complexity Analysis,[0],[0]
"In the following experimental study we aim at two main goals: (i) to assess the ability of TCMSStack in capturing inter-domain similarity, and (ii) to evaluate its predictive effectiveness compared to other approaches.",6. Experimental Study,[0],[0]
All the GPs herein build upon a standard squared exponential covariance function.,6.1. Experiment Setting,[0],[0]
"The hyperparameters of each method are optimized using the conjugate gradient implementation from the gpml package (Rasmussen & Nickisch, 2010).",6.1. Experiment Setting,[0],[0]
"For each search, we allow a maximum of 200 evaluations.",6.1. Experiment Setting,[0],[0]
The reported results correspond to the model providing the best objective function value over 5 independent runs with random initial solutions each.,6.1. Experiment Setting,[0],[0]
"We use one synthetic dataset and two real-world datasets.
",6.1. Experiment Setting,[0],[0]
Synthetic dataset.,6.1. Experiment Setting,[0],[0]
We consider a linear function f(x) = wT0,6.1. Experiment Setting,[0],[0]
"x + , where w0 ∈ R100 and is a zero-mean Gaussian noise term, as the target.",6.1. Experiment Setting,[0],[0]
"We use this function to generate 100 points as target test data, and 20 points as target train data.",6.1. Experiment Setting,[0],[0]
"For the source task, we use g(x) = (wT0 +δ∆w)x+ , where ∆w is a random fluctuation vector and δ is the variable controlling the similarity between f and g (higher δ indicates lower similarity), to generate 380 points for each source with different δ.
",6.1. Experiment Setting,[0],[0]
Amazon reviews.,6.1. Experiment Setting,[0],[0]
"We extract the raw data containing 15
product reviews from (McAuley et al., 2015), and categorize the products into four top categories according to the Amazon website.",6.1. Experiment Setting,[0],[0]
Products in the same category are conceptually similar.,6.1. Experiment Setting,[0],[0]
"Each product is taken as a domain, and we select one as target from each category (see Table 1).",6.1. Experiment Setting,[0],[0]
"Reviews in each domain are represented by the count features and are labeled using stars in the set {1, 2, 3, 4, 5}.
UJIIndoorLoc.",6.1. Experiment Setting,[0],[0]
"The building location dataset covers three buildings of Universitat Jaume I with four floors each (Torres-Sospedra et al., 2014).",6.1. Experiment Setting,[0],[0]
We build 12 domains by taking the location data from each floor of each building as a domain.,6.1. Experiment Setting,[0],[0]
The first floor of each building is taken as the target.,6.1. Experiment Setting,[0],[0]
Domains from the same building are taken as similar.,6.1. Experiment Setting,[0],[0]
"The received signal strength intensity from 520 wireless access points is used as the features, and the location represented by the latitude and longitude is taken as label.",6.1. Experiment Setting,[0],[0]
We first elucidate the ability of TCMSStack in capturing the diverse S-T similarities through the λi coefficients.,6.2. Domain Similarity Capture,[0],[0]
"To rationalize the assessment, we use the synthetic dataset and consider a variety of problems covering a broad spectrum of TL settings.",6.2. Domain Similarity Capture,[0],[0]
"Precisely, we build four scenarios of N = 2, 5, 10, 15 sources.",6.2. Domain Similarity Capture,[0],[0]
"In each scenario, we specify six problems, each given by a different combination of sources.",6.2. Domain Similarity Capture,[0],[0]
"Three problems represent settings in which all the sources are equally similar to the target, with high (δ = 0), medium (δ = 15) and low (δ = 35) similarity strength.",6.2. Domain Similarity Capture,[0],[0]
The other three problems reflect diverse S-T similarities.,6.2. Domain Similarity Capture,[0],[0]
"Each source is given by a δ randomly sampled from the set {0, 4, 7, 10, 15, 20, 25, 30, 35} and with replacement.",6.2. Domain Similarity Capture,[0],[0]
We enforce the three problems to be different and avoid all the sources to be equal.,6.2. Domain Similarity Capture,[0],[0]
"We show the results in Figure 1.
",6.2. Domain Similarity Capture,[0],[0]
"In the figure, the first three problems of each scenario are the cases with equal S-T similarities.",6.2. Domain Similarity Capture,[0],[0]
"It can be observed in the bar plots on the left hand side that the λ values learnt by TCMSStack are strictly reverse-correlated with the predefined δ values, which indicates an accurate capture of the high, medium and low strengths of S-T similarity.",6.2. Domain Similarity Capture,[0],[0]
We further observe from the black dots that GP-TCMS is also able to strongly coordinate δ with a single compromised λ.,6.2. Domain Similarity Capture,[0],[0]
"This is because all the sources share the same δ with the target, and thus can be regarded as a single larger source.
",6.2. Domain Similarity Capture,[0],[0]
The remaining three problems of each scenario reflect diverse similarities across source domains.,6.2. Domain Similarity Capture,[0],[0]
"In this case, Figure 1 shows that the λ values of TCMSStack reflect the relative differences of δ across different sources fairly well in general.",6.2. Domain Similarity Capture,[0],[0]
"The learnt λ is generally reverse-correlated with the predefined δ values, but it is not strict and tight.",6.2. Domain Similarity Capture,[0],[0]
"For instance, in problem 6 of the 5-sources scenario or in the problem 5 of the 15-sources scenario, such reversecorrelated relations do not hold in all the sources.",6.2. Domain Similarity Capture,[0],[0]
This is,6.2. Domain Similarity Capture,[0],[0]
"because, although the learnt λs only represent the bivariate S-T similarities, each of them is specified during learning by considering its influence relative to the rest of similarity coefficients, i.e. the inter-domain dependencies between different sources are taken into account during the learning of the λs.",15 Sources,[0],[0]
"Thus, in some cases, the learnt λs may not strictly approximate the real S-T similarities.",15 Sources,[0],[0]
"However, λs are always learnt to guarantee the outcome of the best transfer performance.",15 Sources,[0],[0]
"That is the reason why the above two cases
still achieve satisfactory transfer performance in terms of RMSE.",15 Sources,[0],[0]
"By contrast, we find that GP-TCMS only gives a trade-off value of λ over the diverse δ values.",15 Sources,[0],[0]
The right hand side of the figure shows a consistently lower RMSE for TCMSStack than for GP-TCMS in all the problems.,15 Sources,[0],[0]
"In particular, a dramatic improvement is observed by utilizing TCMSStack for the diverse problems 4-6.",15 Sources,[0],[0]
These results indicate the superiority of TCMSStack over GP-TCMS for MSTR.,15 Sources,[0],[0]
We further verify this conclusion on the two realworld datasets in the next section.,15 Sources,[0],[0]
"We compare TCMSStack with several MSTR approaches, namely: Tradaboost.R2, GP-TCMS , TCSSStack, a variant of TCSSStack with joint learning of model importance coefficient and λ which we call TCSSStack-Joint, and a variant of TCSSStack using the learnt λ as the model importance which we call λ-Stacking.",6.3. Performance on Real-World Datasets,[0],[0]
The evaluation comprises both the Amazon and the UJIIndoorLoc datasets.,6.3. Performance on Real-World Datasets,[0],[0]
For each source domain we sample 500 points uniformly at random for training.,6.3. Performance on Real-World Datasets,[0],[0]
"Likewise, train and test data from each target domain are obtained by sampling 25 points and 1000 points, respectively.",6.3. Performance on Real-World Datasets,[0],[0]
"For the Amazon dataset, we generated a set of problems by using each target domain in Table 1, and by randomly choosing a number of source domains subsets.",6.3. Performance on Real-World Datasets,[0],[0]
"More precisely, for each scenario of 2, 3 and 5 sources, ten different source combinations were randomly constructed.",6.3. Performance on Real-World Datasets,[0],[0]
"In addition, a scenario of 11 sources with all the source domains was selected.",6.3. Performance on Real-World Datasets,[0],[0]
"Thus, we construct 40 transfer problems for each scenario of 2, 3 and 5 sources, and 4 problems for the scenario of 11 sources.",6.3. Performance on Real-World Datasets,[0],[0]
"For the UJIIndoorLoc dataset, we generate the transfer problems in a similar way to the Amazon dataset described above.
",6.3. Performance on Real-World Datasets,[0],[0]
"In Figure 2, we show the average RMSE results over all the problems in each scenario for the two datasets.",6.3. Performance on Real-World Datasets,[0],[0]
"Overall, TCMSStack is the winner among all the baselines on the two datasets, improving the transfer performance across
the different amounts of source domains.",6.3. Performance on Real-World Datasets,[0],[0]
"This showcases the capability of TCMSStack to transfer knowledge from various sources with different S-T similarities.
",6.3. Performance on Real-World Datasets,[0],[0]
"For the other baselines, we observe that TrAdaBoost.",6.3. Performance on Real-World Datasets,[0],[0]
R2 gives the poorest results due to the premature weights convergence issue.,6.3. Performance on Real-World Datasets,[0],[0]
"If, in addition, we consider the high computational cost for the models involved, TrAdaBoost.",6.3. Performance on Real-World Datasets,[0],[0]
"R2 does not seem to be a good choice for MSTR, especially when the number of source domains is large.",6.3. Performance on Real-World Datasets,[0],[0]
"As for GPTCMS , it presents a steadily inferior performance than TCMSStack.",6.3. Performance on Real-World Datasets,[0],[0]
"Overall, the outcomes are in line with those in the synthetic dataset, offering further support to the superiority of TCMSStack to GP-TCMS .",6.3. Performance on Real-World Datasets,[0],[0]
"Notice that, since the current benchmark was generated randomly, it is likely a scenario to comprise diverse problem settings.",6.3. Performance on Real-World Datasets,[0],[0]
"Therefore, capturing the diverse similarities through a single λ coefficient may compromise the performance of GP-TCMS .",6.3. Performance on Real-World Datasets,[0],[0]
"As opposed to GP-TCMS , TCMSStack offers more robust performance improvements.
",6.3. Performance on Real-World Datasets,[0],[0]
"Finally, the comparison with the other stacking-based methods exposes the benefits of the two salient features of TCMSStack.",6.3. Performance on Real-World Datasets,[0],[0]
Both TCSSStack and λ-Stacking are beaten by TCSSStack-Joint and TCMSStack.,6.3. Performance on Real-World Datasets,[0],[0]
"Since these two sets of methods only differ in the joint learning of the parameters, the outcomes point at the benefits of bringing in the inter-domain dependencies of the other sources during the learning.",6.3. Performance on Real-World Datasets,[0],[0]
"On the other side, the results for λStacking and TCMSStack are better or comparable to those by TCSSStack and TCSSStack-Joint, repectively.",6.3. Performance on Real-World Datasets,[0],[0]
"This provides support to the correlation of the model importance with the similarity coefficients, which allows to specify the model by estimating fewer hyper-parameters while preserving the similarity capture capability.",6.3. Performance on Real-World Datasets,[0],[0]
We investigate a family of transfer covariance functions that represent the pairwise similarity between each source and the target domain for the MSTR problem.,7. Conclusions,[0],[0]
"We prove that, GP-TCMS , a Gaussian process with such a transfer covariance function can only capture the same similarity coefficient for all the sources.",7. Conclusions,[0],[0]
"By further analyzing the generalization errors of GP-TCMS , we conclude the bounds depend on the single similarity coefficient, which may penalize the transfer performance.",7. Conclusions,[0],[0]
"As an alternative, we propose TCMSStack, an approach that integrates the transfer covariance function and the stacking strategy into one unified model.",7. Conclusions,[0],[0]
TCMSStack aligns the S-T similarity coefficients with the model importance and jointly learns the base models.,7. Conclusions,[0],[0]
"Extensive experiments on one synthetic and two real-world datasets, with learning settings of up to 11 sources for the latter, show the superiority of TCMSStack to other MSTR methods.",7. Conclusions,[0],[0]
This work was conducted within the Rolls-Royce@Nanyang Technological University Corporate Lab with support from the National Research Foundation (NRF) Singapore under the Corp Lab@University Scheme.,Acknowledgments,[0],[0]
It is also partially supported by the School of Computer Science and Engineering at Nanyang Technological University.,Acknowledgments,[0],[0]
A key challenge in multi-source transfer learning is to capture the diverse inter-domain similarities.,abstractText,[0],[0]
"In this paper, we study different approaches based on Gaussian process models to solve the multi-source transfer regression problem.",abstractText,[0],[0]
"Precisely, we first investigate the feasibility and performance of a family of transfer covariance functions that represent the pairwise similarity of each source and the target domain.",abstractText,[0],[0]
"We theoretically show that using such a transfer covariance function for general Gaussian process modelling can only capture the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer performance.",abstractText,[0],[0]
"This leads us to propose TCMSStack, an integrated strategy incorporating the benefits of the transfer covariance function and stacking.",abstractText,[0],[0]
"Extensive experiments on one synthetic and two realworld datasets, with learning settings of up to 11 sources for the latter, demonstrate the effectiveness of our proposed TCMSStack.",abstractText,[0],[0]
Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1–11, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Parsing is an important problem in natural language processing which has been studied extensively for decades.,1 Introduction,[0],[0]
"Between the two basic paradigms of parsing, constituency parsing, the subject of this paper, has in general proved to be the more difficult than dependency parsing, both in terms of accuracy and the run time of parsing algorithms.
",1 Introduction,[0],[0]
"There has recently been a huge surge of interest in using neural networks to make parsing decisions, and such models continue to dominate the state of the art in dependency parsing (Andor et al., 2016).",1 Introduction,[0],[0]
"In constituency parsing, however, neural approaches are still behind the state-of-the-art (Carreras et al., 2008; Shindo et al., 2012; Thang et al., 2015); see more details in Section 5.
",1 Introduction,[0],[0]
"To remedy this, we design a new parsing framework that is more suitable for constituency parsing, and that can be accurately modeled by neural networks.",1 Introduction,[0],[0]
"Observing that constituency parsing is primarily focused on sentence spans (rather than individual words, as is dependency parsing), we propose
a novel adaptation of the shift-reduce system which reflects this focus.",1 Introduction,[0],[0]
"In this system, the stack consists of sentence spans rather than partial trees.",1 Introduction,[0],[0]
"It is also factored into two types of parser actions, structural and label actions, which alternate during a parse.",1 Introduction,[0],[0]
"The structural actions are a simplified analogue of shift-reduce actions, omitting the directionality of reduce actions, while the label actions directly assign nonterminal symbols to sentence spans.
",1 Introduction,[0],[0]
Our neural model processes the sentence once for each parse with a recurrent network.,1 Introduction,[0],[0]
We represent parser configurations with a very small number of span features (4 for structural actions and 3 for label actions).,1 Introduction,[0],[0]
"Extending Wang and Chang (2016), each span is represented as the difference of recurrent output from multiple layers in each direction.",1 Introduction,[0],[0]
"No pretrained embeddings are required.
",1 Introduction,[0],[0]
We also extend the idea of dynamic oracles from dependency to constituency parsing.,1 Introduction,[0],[0]
"The latter is significantly more difficult than the former due to F1 being a combination of precision and recall (Huang, 2008), and yet we propose a simple and extremely efficient oracle (amortizedO(1) time).",1 Introduction,[0],[0]
"This oracle is proved optimal for F1 as well as both of its components, precision and recall.",1 Introduction,[0],[0]
"Trained with this oracle, our parser achieves what we believe to be the best results for any parser without reranking which was trained only on the Penn Treebank and the French Treebank, despite the fact that it is not only lineartime, but also strictly greedy.
",1 Introduction,[0],[0]
"We make the following main contributions:
• A novel factored transition parsing system where the stack elements are sentence spans rather than partial trees (Section 2).
",1 Introduction,[0],[0]
"• A neural model where sentence spans are represented as differences of output from a multilayer bi-directional LSTM (Section 3).
",1 Introduction,[0],[0]
"• The first provably optimal dynamic oracle for
1
constituency parsing which is also extremely efficient (amortized O(1) time) (Section 4).
",1 Introduction,[0],[0]
"• The best F1 scores of any single-model, closed training set, parser for English and French.
",1 Introduction,[0],[0]
We are also publicly releasing the source code for one implementation of our parser.1,1 Introduction,[0],[0]
We present a new transition-based system for constituency parsing whose fundamental unit of computation is the sentence span.,2 Parsing System,[0],[0]
"It uses a stack in a similar manner to other transition systems, except that the stack contains sentence spans with no requirement that each one correspond to a partial tree structure during a parse.
",2 Parsing System,[0],[0]
"The parser alternates between two types of actions, structural and label, where the structural actions follow a path to make the stack spans correspond to sentence phrases in a bottom-up manner, while the label actions optionally create tree brackets for the top span on the stack.",2 Parsing System,[0],[0]
"There are only two structural actions: shift is the same as other transition systems, while combine merges the top two sentence spans.",2 Parsing System,[0],[0]
"The latter is analogous to a reduce action, but it does not immediately create a tree structure and is non-directional.",2 Parsing System,[0],[0]
"Label actions do create a partial tree on top of the stack by assigning one or more non-terminals to the topmost span.
",2 Parsing System,[0],[0]
"Except for the use of spans, this factored approach is similar to the odd-even parser from Mi and Huang (2015).",2 Parsing System,[0],[0]
"The fact that stack elements do not have to be tree-structured, however, means that we can create productions with arbitrary arity, and no binarization is required either for training or parsing.",2 Parsing System,[0],[0]
"This also allows us to remove the directionality inherent in the shift-reduce system, which is at best an imperfect fit for constituency parsing.",2 Parsing System,[0],[0]
"We do follow the practice in that system of labeling unary chains of non-terminals with a single action, which means our parser uses a fixed number of steps, (4n− 2) for a sentence of n words.
",2 Parsing System,[0],[0]
Figure 1 shows the formal deductive system for this parser.,2 Parsing System,[0],[0]
"The stack σ is modeled as a list of strictly increasing integers whose first element is always
1code: https://github.com/jhcross/span-parser
zero.",2 Parsing System,[0],[0]
These numbers are word boundaries which define the spans on the stack.,2 Parsing System,[0],[0]
"In a slight abuse of notation, however, we sometimes think of it as a list of pairs (i, j), which are the actual sentence spans, i.e., every consecutive pair of indices on the stack, initially empty.",2 Parsing System,[0],[0]
"We represent stack spans by trapezoids (iSome text and the symbol or scaled
j) in th figures to emphasize that they may or not have tree stucture.
",2 Parsing System,[0],[0]
The parser alternates between structural actions and label actions according to the parity of the parser step z.,2 Parsing System,[0],[0]
"In even steps, it takes a structural action, either combining the top two stack spans, which requires at least two spans on the stack, or introducing a new span of unit length, as long as the entire sentence is not already represented on the stack
In odd steps, the parser takes a label action.",2 Parsing System,[0],[0]
"One possibility is labeling the top span on the stack, (i, j) with either a nonterminal label or an ordered unary chain (since the parser has only one opportunity to label any given span).",2 Parsing System,[0],[0]
"Taking no action, designated nolabel, is also a possibility.",2 Parsing System,[0],[0]
"This is essentially a null operation except that it returns the parser to an even step, and this action reflects the decision that (i, j) is not a (complete) labeled phrase in the tree.",2 Parsing System,[0],[0]
"In the final step, (4n − 2), nolabel is not allowed
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
since the parser must produce a tree.",2 Parsing System,[0],[0]
Figure 2 shows a complete example of applying this parsing system to a very short sentence (“I do like eating fish”) that we will use throughout this section and the next.,2 Parsing System,[0],[0]
"The action in step 2 is labelNP because “I” is a one-word noun phrase (parts of speech are taken as input to our parser, though it could easily be adapted to include POS tagging in label actions).",2 Parsing System,[0],[0]
"If a single word is not a complete phrase (e.g., “do”), then the action after a shift is nolabel.
",2 Parsing System,[0],[0]
"The ternary branch in this tree (VP→MD VBP S) is produced by our parser in a straightforward manner: after the phrase “do like” is combined in step 7, no label is assigned in step 8, successfully delaying the creation of a bracket until the verb phrase is fully formed on the stack.",2 Parsing System,[0],[0]
"Note also that the unary production in the tree is created with a single action, label-S-VP, in step 14.
",2 Parsing System,[0],[0]
"The static oracle to train this parser simply consists of taking actions to generate the gold tree with a “short-stack” heuristic, meaning combine first whenever combine and shift are both possible.",2 Parsing System,[0],[0]
Long short-term memory networks (LSTM) are a type of recurrent neural network model proposed by Hochreiter and Schmidhuber (1997) which are very effective for modeling sequences.,3 LSTM Span Features,[0],[0]
"They are able to capture and generalize from interactions among their sequential inputs even when separated by a long distance, and thus are a natural fit for analyz-
ing natural language.",3 LSTM Span Features,[0],[0]
"LSTM models have proved to be a powerful tool for many learning tasks in natural language, such as language modeling (Sundermeyer et al., 2012) and translation (Sutskever et al., 2014).
",3 LSTM Span Features,[0],[0]
"LSTMs have also been incorporated into parsing in a variety of ways, such as directly encoding an entire sentence (Vinyals et al., 2015), separately modeling the stack, buffer, and action history (Dyer et al., 2015), to encode words based on their character forms (Ballesteros et al., 2015), and as an element in a recursive structure to combine dependency subtrees with their left and right children (Kiperwasser and Goldberg, 2016a).
",3 LSTM Span Features,[0],[0]
"For our parsing system, however, we need a way to model arbitrary sentence spans in the context of the rest of the sentence.",3 LSTM Span Features,[0],[0]
"We do this by representing each sentence span as the elementwise difference of the vector outputs of the LSTM outputs at different time steps, which correspond to word boundaries.",3 LSTM Span Features,[0],[0]
"If the sequential output of the recurrent network for the sentence is f0, ..., fn in the forward direction and bn, ..., b0 in the backward direction then the span (i, j) would be represented as the concatenation of the vector differences (fj − fi) and (bi − bj).
",3 LSTM Span Features,[0],[0]
"The spans are represented using output from both backward and forward LSTM components, as can be seen in Figure 3.",3 LSTM Span Features,[0],[0]
This is essentially the LSTMMinus feature representation described by Wang and Chang (2016) extended to the bi-directional case.,3 LSTM Span Features,[0],[0]
"In initial experiments, we found that there was essentially no difference in performance between using the difference features and concatenating all end-
〈s〉 I do like eating fish 〈/s〉0
f0
b0
1
f1
b1
2
f2
b2
3
f3
b3
4
f4
b4
5
f5
b5
Figure 3: Word spans are modeled by differences in LSTM output.",3 LSTM Span Features,[0],[0]
Here the span 3 eating fish 5 is represented by the vector differences (f5 − f3) and (b3 − b5).,3 LSTM Span Features,[0],[0]
"The forward difference corresponds to LSTM-Minus (Wang and Chang, 2016).
point vectors, but our approach is almost twice as fast.
",3 LSTM Span Features,[0],[0]
"This model allows a sentence to be processed once, and then the same recurrent outputs can be used to compute span features throughout the parse.",3 LSTM Span Features,[0],[0]
"Intuitively, this allows the span differences to learn to represent the sentence spans in the context of the rest of the sentence, not in isolation (especially true for LSTM given the extra hidden recurrent connection, typically described as a “memory cell”).",3 LSTM Span Features,[0],[0]
"In practice, we use a two-layer bi-directional LSTM, where the input to the second layer combines the forward and backward outputs from the first layer at that time step.",3 LSTM Span Features,[0],[0]
"For each direction, the components from the first and second layers are concatenated to form the vectors which go into the span features.",3 LSTM Span Features,[0],[0]
"See Cross and Huang (2016) for more details on this approach.
",3 LSTM Span Features,[0],[0]
"For the particular case of our transition constituency parser, we use only four span features to determine a structural action, and three to determine a label action, in each case partitioning the sentence exactly.",3 LSTM Span Features,[0],[0]
"The reason for this is straightforward: when considering a structural action, the top two spans on the stack must be considered to determine whether they should be combined, while for a label action, only the top span on the stack is important, since that is the candidate for labeling.",3 LSTM Span Features,[0],[0]
In both cases the remaining sentence prefix and suffix are also included.,3 LSTM Span Features,[0],[0]
"These features are shown in Table 1.
",3 LSTM Span Features,[0],[0]
"The input to the recurrent network at each time step consists of vector embeddings for each word
Action Stack LSTM Span Features Structural σ",3 LSTM Span Features,[0],[0]
"| i |k |j 02iSome text and the symbol or scaled
1
kSome text and the symbol or scaled
1
j2n Label σ",3 LSTM Span Features,[0],[0]
|,3 LSTM Span Features,[0],[0]
"i |j 02iSome text and the symbol or scaled
1
j2n
Table 1: Features used for the parser.",3 LSTM Span Features,[0],[0]
"No label or tree-structure features are required.
and its part-of-speech tag.",3 LSTM Span Features,[0],[0]
"Parts of speech are predicted beforehand and taken as input to the parser, as in much recent work in parsing.",3 LSTM Span Features,[0],[0]
"In our experiments, the embeddings are randomly initialized and learned from scratch together with all other network weights, and we would expect further performance improvement from incorporating embeddings pretrained from a large external corpus.
",3 LSTM Span Features,[0],[0]
The network structure after the the span features consists of a separate multilayer perceptron for each type of action (structural and label).,3 LSTM Span Features,[0],[0]
For each action we use a single hidden layer with rectified linear (ReLU) activation.,3 LSTM Span Features,[0],[0]
"The model is trained on a peraction basis using a single correct action for each parser state, with a negative log softmax loss function, as in Chen and Manning (2014).",3 LSTM Span Features,[0],[0]
"The baseline method of training our parser is what is known as a static oracle: we simply generate the sequence of actions to correctly parse each training sentence, using a short-stack heuristic (i.e., combine first whenever there is a choice of shift and combine).",4 Dynamic Oracle,[0],[0]
"This method suffers from a well-documeted problem, however, namely that it only “prepares” the model for the situation where no mistakes have been made during parsing, an inevitably incorrect assumption in practice.",4 Dynamic Oracle,[0],[0]
"To alleviate this problem, Goldberg and Nivre (2013) define a dynamic oracle to return the best possible action(s) at any arbitrary configuration.
",4 Dynamic Oracle,[0],[0]
"In this section, we introduce an easy-to-compute optimal dynamic oracle for our constituency parser.",4 Dynamic Oracle,[0],[0]
We will first define some concepts upon which the dynamic oracle is built and then show how optimal actions can be very efficiently computed using this framework.,4 Dynamic Oracle,[0],[0]
"In broad strokes, in any arbitrary parser configuration c there is a set of brackets t∗(c) from the gold tree which it is still possible to reach.",4 Dynamic Oracle,[0],[0]
"By following dynamic oracle actions, all of those brackets and only those brackets will be predicted.
",4 Dynamic Oracle,[0],[0]
"Even though proving the optimality of our dynamic oracle (Sec. 4.3) is involved, computing the oracle actions is extremely simple (Secs. 4.2) and efficient (Sec. 4.4).",4 Dynamic Oracle,[0],[0]
"Before describing the computation of our dynamic oracle, we first need to rigorously establish the desired optimality of dynamic oracle.",4.1 Preliminaries and Notations,[0],[0]
"The structure of this framework follows Goldberg et al. (2014).
",4.1 Preliminaries and Notations,[0],[0]
Definition 1.,4.1 Preliminaries and Notations,[0],[0]
We denote c `τ c′ iff.,4.1 Preliminaries and Notations,[0],[0]
"c′ is the result of action τ on configuration c, also denoted functionally as c′ = τ(c).",4.1 Preliminaries and Notations,[0],[0]
"We denote ` to be the union of `τ for all actions τ , and `∗ to be the reflexive and transitive closure of `.",4.1 Preliminaries and Notations,[0],[0]
Definition 2 (descendant/reachable trees).,4.1 Preliminaries and Notations,[0],[0]
"We denote D(c) to be the set of final descendant trees derivable from c, i.e., D(c) = {t | c",4.1 Preliminaries and Notations,[0],[0]
"`∗ 〈z, σ, t〉}.",4.1 Preliminaries and Notations,[0],[0]
"This set is also called “reachable trees” from c.
Definition 3 (F1).",4.1 Preliminaries and Notations,[0],[0]
We define the standard F1 metric of a tree t with respect to gold tree tG as F1(t),4.1 Preliminaries and Notations,[0],[0]
"= 2rp r+p , where r = |t∩tG| |tG| , p = |t∩tG| |t| .
",4.1 Preliminaries and Notations,[0],[0]
"The following two definitions are similar to those for dependency parsing by Goldberg et al. (2014).
",4.1 Preliminaries and Notations,[0],[0]
Definition 4.,4.1 Preliminaries and Notations,[0],[0]
"We extend the F1 function to configurations to define the maximum possible F1 from a given configuration: F1(c) = maxt1∈D(c) F1(t1).
",4.1 Preliminaries and Notations,[0],[0]
Definition 5 (oracle).,4.1 Preliminaries and Notations,[0],[0]
"We can now define the desired dynamic oracle of a configuration c to be the set of actions that retrain the optimal F1:
oracle(c) = {τ | F1(τ(c))",4.1 Preliminaries and Notations,[0],[0]
"= F1(c)}.
",4.1 Preliminaries and Notations,[0],[0]
"This abstract oracle is implemented by dyna(·) in Sec. 4.2, which we prove to be correct in Sec. 4.3.
",4.1 Preliminaries and Notations,[0],[0]
Definition 6 (span encompassing).,4.1 Preliminaries and Notations,[0],[0]
"We say span (i, j) is encompassed by span (p, q), notated (i, j) (p, q), iff.",4.1 Preliminaries and Notations,[0],[0]
p ≤,4.1 Preliminaries and Notations,[0],[0]
i < j ≤ q.,4.1 Preliminaries and Notations,[0],[0]
Definition 7 (strict encompassing).,4.1 Preliminaries and Notations,[0],[0]
"We say span (i, j) is strictly encompassed by span (p, q), notated (i, j) ≺ (p, q), iff.",4.1 Preliminaries and Notations,[0],[0]
"(i, j) (p, q) and (i, j) 6=",4.1 Preliminaries and Notations,[0],[0]
"(p, q).",4.1 Preliminaries and Notations,[0],[0]
"We then extend this relation from spans to brackets, and notate iXj ≺ pYq iff.",4.1 Preliminaries and Notations,[0],[0]
"(i, j) ≺",4.1 Preliminaries and Notations,[0],[0]
"(p, q).
",4.1 Preliminaries and Notations,[0],[0]
"0S5
1VP5
3S/VP5
4NP5
0Some text and the symbol or scaled
1Some text and the symbol or scaled
1
2Some text and th sym l or scaled
1
4 5
I do like eating fish
Figure 4: Reachable brackets (w.r.t.",4.1 Preliminaries and Notations,[0],[0]
"gold tree in Fig. 1) for c = 〈10, [0, 1, 2, 4], {0NP1}〉 which mistakenly combines “like eating”.",4.1 Preliminaries and Notations,[0],[0]
"Trapezoids indicate stack spans (the top one in red), and solid triangles denote reachable brackets, with left(c) in blue and right(c) in cyan.",4.1 Preliminaries and Notations,[0],[0]
"The next reachable bracket, next(c) = 1VP5, is in bold.",4.1 Preliminaries and Notations,[0],[0]
"Brackets 3VP5 and 3S5 (in dotted triangle) cross the top span (thus unreachable), and 0NP1 is already recognized (thus not in reach(c) either).
",4.1 Preliminaries and Notations,[0],[0]
"We next define a central concept, reachable brackets, which is made up of two parts, the left ones left(c) which encompass (i, j) without crossing any stack spans, and the right ones right(c) which are completely on the queue.",4.1 Preliminaries and Notations,[0],[0]
"See Fig. 4 for examples.
",4.1 Preliminaries and Notations,[0],[0]
Definition 8 (reachable brackets).,4.1 Preliminaries and Notations,[0],[0]
"For any configuration c = 〈z, σ | i |j, t〉, we define the set of reachable gold brackets (with respect to gold tree tG) as
reach(c) = left(c) ∪ right(c)
where the left- and right-reachable brackets are
left(c)={pXq ∈ tG",4.1 Preliminaries and Notations,[0],[0]
"| (i, j) ≺",4.1 Preliminaries and Notations,[0],[0]
"(p, q), p ∈ σ",4.1 Preliminaries and Notations,[0],[0]
| i} right(c)={pXq ∈ tG,4.1 Preliminaries and Notations,[0],[0]
"| p ≥ j}
for even z, with the ≺ replaced by for odd z. Special case (initial): reach(〈0, [0], ∅〉) = tG.",4.1 Preliminaries and Notations,[0],[0]
The notation p ∈ σ,4.1 Preliminaries and Notations,[0],[0]
|,4.1 Preliminaries and Notations,[0],[0]
"i simply means (p, q) does not “cross” any bracket on the stack.",4.1 Preliminaries and Notations,[0],[0]
"Remember our stack is just a list of span boundaries, so if p coincides with one of them, (p, q)’s left boundary is not crossing and its right boundary q is not crossing either since q ≥ j due to (i, j) ≺",4.1 Preliminaries and Notations,[0],[0]
"(p, q).
",4.1 Preliminaries and Notations,[0],[0]
"Also note that reach(c) is strictly disjoint from t, i.e., reach(c) ∩ t = ∅ and reach(c) ⊆ tG",4.1 Preliminaries and Notations,[0],[0]
"− t. See Figure 6 for an illustration.
5
Definition 9 (next bracket).",4.1 Preliminaries and Notations,[0],[0]
"For any configuration c = 〈z, σ | i |j, t〉, the next reachable gold bracket (with respect to gold tree tG) is the smallest reachable bracket (strictly) encompassing (i, j):
next(c) = min≺ left(c).",4.1 Preliminaries and Notations,[0],[0]
"For an even-step configuration c = 〈z, σ | i | j, t〉, we denote the next reachable gold bracket next(c) to be pXq, and define the dynamic oracle to be:
dyna(c) =    {sh} if p = i and q >",4.2 Structural and Label Oracles,[0],[0]
j {comb} if p < i,4.2 Structural and Label Oracles,[0],[0]
"and q = j {sh, comb} if p <",4.2 Structural and Label Oracles,[0],[0]
"i and q > j (1)
",4.2 Structural and Label Oracles,[0],[0]
"As a special case dyna(〈0, [0], ∅〉) = {sh}.",4.2 Structural and Label Oracles,[0],[0]
Figure 5 shows examples of this policy.,4.2 Structural and Label Oracles,[0],[0]
"The key insight is, if you follow this policy, you will not miss the next reachable bracket, but if you do not follow it, you certainly will.",4.2 Structural and Label Oracles,[0],[0]
"We formalize this fact below (with proof omitted due to space constraints) which will be used to prove the central results later.
",4.2 Structural and Label Oracles,[0],[0]
Lemma 1.,4.2 Structural and Label Oracles,[0],[0]
"For any configuration c, for any τ ∈ dyna(c), we have reach(τ(c))",4.2 Structural and Label Oracles,[0],[0]
= reach(c); for any τ ′ /∈,4.2 Structural and Label Oracles,[0],[0]
"dyna(c), we have reach(τ(c))",4.2 Structural and Label Oracles,[0],[0]
"( reach(c).
",4.2 Structural and Label Oracles,[0],[0]
The label oracles are much easier than structural ones.,4.2 Structural and Label Oracles,[0],[0]
"For an odd-step configuration c = 〈z, σ | i | j, t〉, we simply check if (i, j) is a valid span in the gold tree tG and if so, label it accordingly, otherwise no label.",4.2 Structural and Label Oracles,[0],[0]
"More formally,
dyna(c) = { {label-X}",4.2 Structural and Label Oracles,[0],[0]
if some iXj ∈ tG {nolabel} otherwise (2),4.2 Structural and Label Oracles,[0],[0]
"To show the optimality of our dynamic oracle, we begin by defining a special tree t∗(c) and show that it is optimal among all trees reachable from configuration c. We then show that following our dynamic oracle (Eqs. 1–2) from c will lead to t∗(c).
",4.3 Correctness,[0],[0]
Definition 10 (t∗(c)).,4.3 Correctness,[0],[0]
"For any configuration c = 〈z, σ, t〉, we define the optimal tree t∗(c) to include all reachable gold brackets and nothing else.",4.3 Correctness,[0],[0]
"More formally, t∗(c) = t ∪ reach(c).
",4.3 Correctness,[0],[0]
"configuration oracle
static dynamic 0Some t xt and t e symbol or scaled
1Some text and the symbol or scaled
2Some text and the symbol or scaled
3 comb {comb, sh}
I do like 1∧52Some text and the symbol or scaled
1
3
0Some text and the symbol or scaled
1
1Some text and the sym ol or scaled
1
3
undef.
",4.3 Correctness,[0],[0]
"{sh}
I do like t={..., 1VP3} 1∧5Some text and the symbol or scaled
1
3
0Some text and the symbol or scaled
1
1Some te t and the symbol or scaled
1
2Som t xt and th sym l or scaled
1
4 {comb, sh}
I do like eating 1∧52Some text and the symbol or scaled
1
4
0Some text and the symbol or scaled
1Some text and the symbol or scaled
1
2Some text and the sym ol or scaled
1
4Some text and the symbol o scaled
1
5 {comb}
I do like eating fish 1∧54Some text and the symbol or scaled
1
Figure 5: Dynamic oracle with respect to the gold parse in Fig. 2.",4.3 Correctness,[0],[0]
The last three examples are off the gold path with strike out indicating structural or label mistakes.,4.3 Correctness,[0],[0]
"Trapezoids denote stack spans (top one in red) and the blue triangle denotes the next reachable bracket next(c) which is 1VP5 in all cases.
",4.3 Correctness,[0],[0]
"We can show by induction that t∗(c) is attainable:
Lemma 2.",4.3 Correctness,[0],[0]
"For any configuration c, the optimal tree is a descendant of c, i.e., t∗(c) ∈ D(c).
",4.3 Correctness,[0],[0]
The following Theorem shows that t∗(c) is indeed the best possible tree: Theorem 1 (optimality of t∗).,4.3 Correctness,[0],[0]
"For any configuration c, F1(t∗(c))",4.3 Correctness,[0],[0]
"= F1(c).
",4.3 Correctness,[0],[0]
Proof.,4.3 Correctness,[0],[0]
(SKETCH),4.3 Correctness,[0],[0]
"Since t∗(c) adds all possible additional gold brackets (the brackets in reach(c)), it is not possible to get higher recall.",4.3 Correctness,[0],[0]
"Since it adds no incorrect brackets, it is not possible to get higher pre-
6
cision.",4.3 Correctness,[0],[0]
"Since F1 is the harmonic mean of precision and recall, it also leads to the best possible F1.
",4.3 Correctness,[0],[0]
Corollary 1.,4.3 Correctness,[0],[0]
"For any c = 〈z, σ, t〉, for any t′ ∈ D(c) and t′ 6= t∗(c), we have F1(t′) < F1(c).
",4.3 Correctness,[0],[0]
We now need a final lemma about the policy dyna(·) (Eqs. 1–2) before proving the main result.,4.3 Correctness,[0],[0]
Lemma 3.,4.3 Correctness,[0],[0]
"From any c = 〈z, σ, t〉, for any action τ ∈ dyna(c), we have t∗(τ(c))",4.3 Correctness,[0],[0]
= t∗(c).,4.3 Correctness,[0],[0]
For any action τ ′ /∈,4.3 Correctness,[0],[0]
"dyna(c), we have t∗(τ ′(c))",4.3 Correctness,[0],[0]
"6= t∗(c).
",4.3 Correctness,[0],[0]
Proof.,4.3 Correctness,[0],[0]
(SKETCH),4.3 Correctness,[0],[0]
"By case analysis on even/odd z.
We are now able to state and prove the main theoretical result of this paper (using Lemma 3, Theorem 1 and Corollary 1):
Theorem 2.",4.3 Correctness,[0],[0]
The function dyna(·) in Eqs.,4.3 Correctness,[0],[0]
"(1–2) satisfies the requirement of a dynamic oracle (Def. 5):
dyna(c) = oracle(c) for any configuration c.",4.3 Correctness,[0],[0]
"For any configuration, our dynamic oracle can be computed in amortized constant time since there are only O(n) gold brackets and thus bounding |reach(c)| and the choice of next(c).",4.4 Implementation and Complexity,[0],[0]
"After each action, next(c) either remains unchanged, or in the case of being crossed by a structural action or mislabeled by a label action, needs to be updated.",4.4 Implementation and Complexity,[0],[0]
"This update is simply tracing the parent link to the next smallest gold bracket repeatedly until the new bracket encompasses span (i, j).",4.4 Implementation and Complexity,[0],[0]
"Since there are at most O(n) choices of next(c) and there are O(n) steps, the per-step cost is amortized constant time.",4.4 Implementation and Complexity,[0],[0]
Thus our dynamic oracle is much faster than the super-linear time oracle for arc-standard dependency parsing in Goldberg et al. (2014).,4.4 Implementation and Complexity,[0],[0]
Neural networks have been used for constituency parsing in a number of previous instances.,5 Related Work,[0],[0]
"For example, Socher et al. (2013) learn a recursive network that combines vectors representing partial trees, Vinyals et al. (2015) adapt a sequence-tosequence model to produce parse trees, Watanabe and Sumita (2015) use a recursive model applying a shift-reduce system to constituency parsing with
beam search, and Dyer et al. (2016) adapt the StackLSTM dependency parsing approach to this task.",5 Related Work,[0],[0]
Durrett and Klein (2015) combine both neural and sparse features for a CKY parsing system.,5 Related Work,[0],[0]
"Our own previous work (Cross and Huang, 2016) use a recurrent sentence representation in a head-driven transition system which allows for greedy parsing but does not achieve state-of-the-art results.
",5 Related Work,[0],[0]
The concept of “oracles” for constituency parsing (as the tree that is most similar to tG among all possible trees) was first defined and solved by Huang (2008) in bottom-up parsing.,5 Related Work,[0],[0]
"In transition-based parsing, the dynamic oracle for shift-reduce dependency parsing costs worst-case O(n3) time (Goldberg et al., 2014).",5 Related Work,[0],[0]
"On the other hand, after the submission of our paper we became aware of a parallel work (Coavoux and Crabbé, 2016) that also proposed a dynamic oracle for their own incremental constituency parser.",5 Related Work,[0],[0]
"However, it is not optimal due to dummy non-terminals from binarization.",5 Related Work,[0],[0]
"We present experiments on both the Penn English Treebank (Marcus et al., 1993) and the French Treebank (Abeillé et al., 2003).",6 Experiments,[0],[0]
"In both cases, all stateaction training pairs for a given sentence are used at the same time, greatly increasing training speed since all examples for the same sentence share the same forward and backward pass through the recurrent part of the network.",6 Experiments,[0],[0]
"Updates are performed in minibatches of 10 sentences, and we shuffle the training sentences before each epoch.",6 Experiments,[0],[0]
"The results we report are trained for 10 epochs.
",6 Experiments,[0],[0]
"The only regularization which we employ during training is dropout (Hinton et al., 2012), which is applied with probability 0.5 to the recurrent outputs.",6 Experiments,[0],[0]
"It is applied separately to the input to the second LSTM layer for each sentence, and to the input to the ReLU hidden layer (span features) for each stateaction pair.",6 Experiments,[0],[0]
"We use the ADADELTA method (Zeiler, 2012) to schedule learning rates for all weights.",6 Experiments,[0],[0]
"All of these design choices are summarized in Table 2.
",6 Experiments,[0],[0]
"In order to account for unknown words during training, we also adopt the strategy described by Kiperwasser and Goldberg (2016b), where words in the training set are replaced with the unknownword symbol UNK with probability punk = zz+f(w) where f(w) is the number of times the word appears in the training corpus.",6 Experiments,[0],[0]
We choose the parameter z so that the training and validation corpora have approximately the same proportion of unknown words.,6 Experiments,[0],[0]
"For the Penn Treebank, for example, we used z = 0.8375 so that both the validation set and the (rest of the) training set contain approximately 2.76% unknown words.",6 Experiments,[0],[0]
"This approach was helpful but not critical, improving F1 (on dev) by about 0.1 over training without any unknown words.",6 Experiments,[0],[0]
"The most straightforward use of dynamic oracles to train a neural network model, where we collect all action examples for a given sentence before updating, is “training with exploration” as proposed by Goldberg and Nivre (2013).",6.1 Training with Dynamic Oracle,[0],[0]
This involves parsing each sentence according to the current model and using the oracle to determine correct actions for training.,6.1 Training with Dynamic Oracle,[0],[0]
"We saw very little improvement on the Penn treebank validation set using this method, however.",6.1 Training with Dynamic Oracle,[0],[0]
"Based on the parsing accuracy on the training sentences, this appears to be due to the model overfitting the training data early during training, thus negating the benefit of training on erroneous paths.
",6.1 Training with Dynamic Oracle,[0],[0]
"Accordingly, we also used a method recently proposed by Ballesteros et al. (2016), which specifically addresses this problem.",6.1 Training with Dynamic Oracle,[0],[0]
This method introduces stochasticity into the training data parses by randomly taking actions according to the softmax distribution over action scores.,6.1 Training with Dynamic Oracle,[0],[0]
"This introduces realistic mistakes into the training parses, which we found was also very effective in our case, leading to higher F1 scores, though it noticeably sacrifices
recall in favor of precision.",6.1 Training with Dynamic Oracle,[0],[0]
This technique can also take a parameter α to flatten or sharpen the raw softmax distribution.,6.1 Training with Dynamic Oracle,[0],[0]
The results on the Penn treebank development set for various values of α are presented in Table 3.,6.1 Training with Dynamic Oracle,[0],[0]
"We were surprised that flattening the distribution seemed to be the least effective, as training accuracy (taking into account sampled actions) lagged somewhat behind validation accuracy.",6.1 Training with Dynamic Oracle,[0],[0]
"Ultimately, the best results were for α = 1, which we used for final testing.",6.1 Training with Dynamic Oracle,[0],[0]
"Following the literature, we used the Wall Street Journal portion of the Penn Treebank, with standard splits for training (secs 2–21), development (sec 22), and test sets (sec 23).",6.2 Penn Treebank,[0],[0]
"Because our parsing system seamlessly handles non-binary productions, minimal data preprocessing was required.",6.2 Penn Treebank,[0],[0]
"For the part-of-speech tags which are a required input to our parser, we used the Stanford tagger with 10-way jackknifing.
",6.2 Penn Treebank,[0],[0]
Table 4 compares test our results on PTB to a range of other leading constituency parsers.,6.2 Penn Treebank,[0],[0]
"Despite being a greedy parser, when trained using dynamic oracles with exploration, it achieves the best F1 score of any closed-set single-model parser.",6.2 Penn Treebank,[0],[0]
"We also report results on the French treebank, with one small change to network structure.",6.3 French Treebank,[0],[0]
"Specifically, we also included morphological features for each word as input to the recurrent network, using a small embedding for each such feature, to demonstrate that our parsing model is able to exploit such additional features.
",6.3 French Treebank,[0],[0]
"We used the predicted morphological features, part-of-speech tags, and lemmas (used in place of word surface forms) supplied with the SPMRL 2014
data set (Seddah et al., 2014).",6.3 French Treebank,[0],[0]
It is thus possible that results could be improved further using an integrated or more accurate predictor for those features.,6.3 French Treebank,[0],[0]
"Our parsing and evaluation also includes predicting POS tags for multi-word expressions as is the standard practice for the French treebank, though our results are similar whether or not this aspect is included.
",6.3 French Treebank,[0],[0]
We compare our parser with other recent work in Table 5.,6.3 French Treebank,[0],[0]
"We achieve state-of-the-art results even in comparison to Björkelund et al. (2014), which utilized both external data and reranking in achieving the best results in the SPMRL 2014 shared task.",6.3 French Treebank,[0],[0]
"For these experiments, we performed very little hyperparameter tuning, due to time and resource contraints.",6.4 Notes on Experiments,[0],[0]
"We have every reason to believe that performance could be improved still further with such techniques as random restarts, larger hidden layers, external embeddings, and hyperparameter grid search, as demonstrated by Weiss et al. (2015).
",6.4 Notes on Experiments,[0],[0]
"We also note that while our parser is very accurate even with greedy decoding, the model is easily adaptable for beam search, particularly since the parsing system already uses a fixed number of actions.",6.4 Notes on Experiments,[0],[0]
"Beam search could also be made considerably more efficient by caching post-hidden-layer feature components for sentence spans, essentially using the precomputation trick described by Chen and Manning (2014), but on a per-sentence basis.",6.4 Notes on Experiments,[0],[0]
We have developed a new transition-based constituency parser which is built around sentence spans.,7 Conclusion and Future Work,[0],[0]
It uses a factored system alternating between structural and label actions.,7 Conclusion and Future Work,[0],[0]
We also describe a fast dynamic oracle for this parser which can determine the optimal set of actions with respect to a gold training tree in an arbitrary state.,7 Conclusion and Future Work,[0],[0]
"Using an LSTM model and only a few sentence spans as features, we achieve state-of-the-art accuracy on the Penn Treebank for all parsers without reranking, despite using strictly greedy inference.
",7 Conclusion and Future Work,[0],[0]
"In the future, we hope to achieve still better results using beam search, which is relatively straightforward given that the parsing system already uses a fixed number of actions.",7 Conclusion and Future Work,[0],[0]
"Dynamic programming (Huang and Sagae, 2010) could be especially powerful in this context given the very simple feature representation used by our parser, as noted also by Kiperwasser and Goldberg (2016b).",7 Conclusion and Future Work,[0],[0]
"We thank the three anonymous reviewers for comments, Kai Zhao, Lemao Liu, Yoav Goldberg, and Slav Petrov for suggestions, Juneki Hong for proofreading, and Maximin Coavoux for sharing their manuscript.",Acknowledgments,[0],[0]
"This project was supported in part by NSF IIS-1656051, DARPA FA8750-13-2-0041 (DEFT), and a Google Faculty Research Award.",Acknowledgments,[0],[0]
Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks.,abstractText,[0],[0]
"Despite striking results in dependency parsing, however, neural models have not surpassed stateof-the-art approaches in constituency parsing.",abstractText,[0],[0]
"To remedy this, we introduce a new shiftreduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features.",abstractText,[0],[0]
"We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n) oracles for standard dependency parsing.",abstractText,[0],[0]
"Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.",abstractText,[0],[0]
Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles,title,[0],[0]
"We consider high-dimensional statistical models where the ambient dimension p is much larger than the number of observations n. Under such high-dimensional scaling, it is still possible to obtain consistent estimators by imposing low-dimensional structural constraints upon the statistical models, such as sparsity (e.g. in compressed sens-
1School of Computing, KAIST, Daejeon, South Korea 2AItrics, Seoul, South Korea 3IBM T.J. Watson Research Center, Yorktown Heights, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Eunho Yang <eunhoy@kaist.ac.kr>, Aurelie C. Lozano <aclozano@us.ibm.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"ing (Baraniuk, 2007) and Lasso (Tibshirani, 1996)), lowrank structure (Recht et al., 2007; Negahban & Wainwright, 2010), sparse graphical model structure (Friedman et al., 2007; Ravikumar et al., 2008), and sparse additive structure for non-parametric models (Ravikumar et al., 2009).",1. Introduction,[0],[0]
A widely used approach to structured learning is via specific regularization functions.,1. Introduction,[0],[0]
"For instance, `1-regularization is employed for sparse models (Tibshirani, 1996), `1/`q norms for group sparsity (Yuan & Lin, 2006), and nuclear norm for low-rank matrix-structured models (Candès & Tao, 2010).",1. Introduction,[0],[0]
"Much attention has been devoted to the study of these structured norms and their theoretical properties.
",1. Introduction,[0],[0]
"Such a “clean” regularization approach, however, might be too stringent in practice.",1. Introduction,[0],[0]
"For instance in linear regression, a blend of element-wise sparsity and group-sparsity might be more appropriate than a purely sparse or purely groupsparse solution.",1. Introduction,[0],[0]
"In multitask learning, while some parameters might be shared across tasks, others might only be relevant to a subset of tasks or a single task.",1. Introduction,[0],[0]
"To overcome this limitation, a line of work on so-called dirty models has emerged, which addresses this caveat by “mixing and matching” different structures.",1. Introduction,[0],[0]
"One basic approach consists in decomposing the model parameters as a sum of two components, each penalized separately: one component captures the common structure across tasks and the other task-specific characteristics (Jalali et al., 2010; Gong et al., 2012).",1. Introduction,[0],[0]
"For instance the dirty model in Jalali et al. (2010) employs `1,1 and `1,∞ regularizers to the two components.",1. Introduction,[0],[0]
"Chandrasekaran et al. (2011) consider the problem of recovering unknown low-rank and sparse matrices, given the sum of their sum, with application such as optical imaging systems.",1. Introduction,[0],[0]
"Robust principal component analysis and related extensions (Candès et al., 2011; Agarwal et al., 2012; Hsu et al., 2011) estimate a covariance matrix that is the sum of a low-rank matrix and a structured (e.g. sparse, column sparse) matrix.
",1. Introduction,[0],[0]
"A general framework for studying dirty models was recently proposed in Yang & Ravikumar (2013), which bridges and extends several analyses for specific pairs of superposition structures and specific statistical models (e.g., Jalali et al. (2010); Chandrasekaran et al. (2011); Candès et al. (2011); Agarwal et al. (2012); Hsu et al.
(2011))",1. Introduction,[0],[0]
"Specifically, this framework applies to a general class of M -estimators employing a so-called hybrid regularization function, which is the infimal convolution of weighted regularization functions, one for each structural component.",1. Introduction,[0],[0]
"This formulation is equivalent to an M - estimator that combines a loss function applied to the sum of multiple parameter vectors (one per structural component) and a weighted sum of regularization functions (one per parameter vector).
",1. Introduction,[0],[0]
"For the sparse + group sparse decomposition, however existing analyses are highly problematic.",1. Introduction,[0],[0]
The key weakness is that they require some form of structural incoherence condition which captures the interaction between the different structured components.,1. Introduction,[0],[0]
"While such a structural incoherence is a reasonable assumption for e.g. sparse + low rank superposition, it is what too stringent for the sparse+group sparse case because the two structures are completely coherent for this case!",1. Introduction,[0],[0]
"This yields a key motivating question for this paper: Under the sparse + group sparse setting, can we bypass structural incoherence conditions and yet obtain tight error bounds?
",1. Introduction,[0],[0]
In this paper we provide a positive answer by developing a novel proof technique.,1. Introduction,[0],[0]
Prior analyses require ‘local’ restricted strong convexity conditions (RSC): one condition for the sparse component and one for the group sparse component.,1. Introduction,[0],[0]
The use of structural incoherence between sparse and group sparse components in then needed to show ‘global’ RSC for the vector concatenating sparse and group sparse components.,1. Introduction,[0],[0]
"To avoid the need for structural incoherence, we use RSC in the summed space directly (namely for the summed sparse + group-sparse structure).",1. Introduction,[0],[0]
"However, this brings in a new issue: in this case, the dirty regularizer for the parameter vector is not decomposable.",1. Introduction,[0],[0]
"To circumvent this issue, our key ingredient is to introduce “surrogate” sparse and group sparse components depending on our estimators such that i) their sum equals the sum of the true parameter components and ii) corresponding error vectors are decomposable even though the regularizer itself is not decomposable.",1. Introduction,[0],[0]
"Using the decomposability of error vectors, we are then able to show `2 consistency for general loss functions.
",1. Introduction,[0],[0]
"As an additional key contribution of this paper, we consider the extension of sparse+group sparse dirty models to non-convex regularizers, and show their `∞ consistency.",1. Introduction,[0],[0]
"Interestingly, these models are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models.",1. Introduction,[0],[0]
"In particular, our `∞ consistency results require neither incoherence in the loss function nor structural incoherence between sparse and group sparse parameters.",1. Introduction,[0],[0]
"We illustrate the practical impact of this superior theoretical results with simulation experiments.
",1. Introduction,[0],[0]
The remainder of this paper is organized as follows.,1. Introduction,[0],[0]
In Section 2 we review sparse+group-sparse dirty models with convex penalties and introduce their non-convex counterparts.,1. Introduction,[0],[0]
In Section 3 we discuss the incoherence assumption required by prior analyses and explain why such an assumption is unreasonable.,1. Introduction,[0],[0]
Section 4 introduces the key ingredient of our novel proof technique.,1. Introduction,[0],[0]
Section 5 presents the convergence bounds for models with convex penalties.,1. Introduction,[0],[0]
Those for non-convex penalties are stated in Section 6.,1. Introduction,[0],[0]
"Finally, simulation experiments are provided in Section 7 to illustrate the remarkable practical advantage of non-convex penalties, agreeing with their superior convergence rates.",1. Introduction,[0],[0]
"Consider a data collection Z = {Z1, . . .",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
", Zn}, where each element is drawn independently from distribution P, and a loss function L(· ;Z) : Ω → R where L(θ ;Z) measures the goodness of fit of parameter θ ∈ Ω to the given data collection Z. Typically Ω =",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
Rp (parameters are vectors) or Rp×r (parameters are matrices).,2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
"Assume there are some known groups G = {G1, . . .",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
", Gq} that partition the parameter index set: Gi ∩Gj = φ and ∪qg=1Gg = {1, . . .",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
", p}.
",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
"We aim at recovering parameter θ∗ which is the unique minimizer of the population risk: θ∗ := argminθ∈Ω EZ [L(θ;Z)] in cases where
θ∗ = α∗ + β∗, (1)
where α∗ is a sparse component and β∗ is a group-sparse component obeying the group structure G.",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
"For that purpose, we focus on regularized M -estimators under a dirty learning setting that combines sparsity and group-sparsity.",2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
We consider both convex and non-convex regularizers as follows.,2. Sparse + Group-Sparse Dirty Models: Setup and Formulations,[0],[0]
"We focus on regularized M -estimators of the form
minimize α,β
L(α+ β;Z) + λ1‖α‖1",2.1. Dirty models with convex regularizers,[0],[0]
"+ λ2‖β‖1,a, (2)
where the loss function L(· ;Z) is possibly nonconvex.",2.1. Dirty models with convex regularizers,[0],[0]
"Here, given known parameter groups G = {G1, G2, . . .",2.1. Dirty models with convex regularizers,[0],[0]
", Gq}, the group regularizer is defined as ‖β‖1,a := ∑q t=1 ‖βGt‖a for a ≥ 2, where βGt denotes the parameter subset in group Gt.",2.1. Dirty models with convex regularizers,[0],[0]
"The constant a determines how the elements within each group are combined.
",2.1. Dirty models with convex regularizers,[0],[0]
"We provide examples for the popular settings of linear regression and inverse covariance estimation.
",2.1. Dirty models with convex regularizers,[0],[0]
Linear regression.,2.1. Dirty models with convex regularizers,[0],[0]
"Consider the standard linear model y = Xθ∗ +w where y ∈ Rn is the observation vector, θ∗
is the true regression parameter which is the sum of sparse α∗ and group sparse β∗, X ∈ Rn×p is the design matrix, and w ∈",2.1. Dirty models with convex regularizers,[0],[0]
Rn is the observation noise.,2.1. Dirty models with convex regularizers,[0],[0]
"The “dirty” regularized least squares solves
minimize α,β∈Rp
1
2n ‖y",2.1. Dirty models with convex regularizers,[0],[0]
"−X
( α+ β)‖22 + λ1‖α‖1 + λ2‖β‖1,a
(3)
",2.1. Dirty models with convex regularizers,[0],[0]
where groups are defined within a (single) parameter vector space via β.,2.1. Dirty models with convex regularizers,[0],[0]
"The formulation can be seamlessly extended to cover the dirty multitask learning setting of Jalali et al. (2010):
minimize α,β∈Rp×m m∑ k=1 1 2n ∥∥y(k) −X(k)([α+ β](·,k))∥∥22 (4) +λ1‖α‖1 + λ2‖β‖1,∞
where we have m related tasks in columns: α,β ∈ Rp×m, and the groups can be defined across tasks in rows.",2.1. Dirty models with convex regularizers,[0],[0]
"E.g. for predictor j, β(j,1), . . .",2.1. Dirty models with convex regularizers,[0],[0]
",β(j,m) belong to the same group.",2.1. Dirty models with convex regularizers,[0],[0]
"Here, [α + β](·,k) indicates k-th column of matrix input α+ β.
",2.1. Dirty models with convex regularizers,[0],[0]
Graphical Model Estimation.,2.1. Dirty models with convex regularizers,[0],[0]
Another key example is a modified graphical Lasso where the goal is to estimate the structure of the underlying graphs representing conditional independences across variables.,2.1. Dirty models with convex regularizers,[0],[0]
Assume that there are some known set of edge groups and that the true parameter Θ∗ has only a small number of active edge groups plus some individual edges.,2.1. Dirty models with convex regularizers,[0],[0]
"To recover Θ∗ we solve
minimize S+B 0
trace ( (S +B) Σ̂ )",2.1. Dirty models with convex regularizers,[0],[0]
"− log det(S +B) (5)
+λ1‖S‖1 + λ2‖B‖1,a
where Σ̂ is the sample covariance matrix and regularizers are applied to off-diagonal entries of S and B. As done in (4) for the linear model, the formulation (5) can be seamlessly extended to the multitask setting where we wish to estimate multiple precision matrices jointly, encouraging similar structure while allowing for some discrepancy across them.",2.1. Dirty models with convex regularizers,[0],[0]
"This estimator is discussed in Hara & Washio (2013).
",2.1. Dirty models with convex regularizers,[0],[0]
Equivalent Program.,2.1. Dirty models with convex regularizers,[0],[0]
"As shown in Yang & Ravikumar (2013), the formulation (2) can be rewritten as:
minimize θ
L(θ;Z) +",2.1. Dirty models with convex regularizers,[0],[0]
"‖θ‖λ (6)
where ‖θ‖λ is the infimal convolution of two regularizers
‖θ‖λ := inf α,β
{ λ1‖α‖1 + λ2‖β‖1,a : α+ β = θ } .",2.1. Dirty models with convex regularizers,[0],[0]
"(7)
It is known that ‖ · ‖λ is a norm and its dual is defined as ‖θ‖∗λ := max{‖θ‖∞/λ1, ‖θ‖∞,a∗/λ2} where 1/a + 1/a∗ = 1 so that ‖ · ‖∞,a∗ is the dual norm of ‖ · ‖1,a (see Yang & Ravikumar (2013) for details).",2.1. Dirty models with convex regularizers,[0],[0]
"In this paper, we introduce and study estimators of the form
minimize α,β
L(α+ β) + ρλ1 ( α ) +",2.2. Dirty models with non-convex regularizers,[0],[0]
"φλ2,a ( β).",2.2. Dirty models with non-convex regularizers,[0],[0]
"(8)
Here ρλ1(·) is any regularizer inducing sparsity beyond the `1-norm (note that the notation encapsulates the regularization parameter λ1 itself within the regularizer) satisfying the following conditions (Loh & Wainwright, 2014):
(C1) ρλ1(0)",2.2. Dirty models with non-convex regularizers,[0],[0]
= 0,2.2. Dirty models with non-convex regularizers,[0],[0]
and is symmetric.,2.2. Dirty models with non-convex regularizers,[0],[0]
"For t > 0, ρλ1(t) is non-decreasing but ρλ1(t)/t is non-increasing in t. Besides, ρλ1(t) is differentiable for t 6= 0 with limt→0+ ρ ′",2.2. Dirty models with non-convex regularizers,[0],[0]
"λ1 (t) = λ1, and is ρλ1(t) + µ 2 t
2 is convex for some µ > 0.
(C2) There exists some scalar γ ∈ (0,∞) such that ρ′λ1(t)",2.2. Dirty models with non-convex regularizers,[0],[0]
= 0,2.2. Dirty models with non-convex regularizers,[0],[0]
"when t ≥ γλ1.
",2.2. Dirty models with non-convex regularizers,[0],[0]
"Following the notation of Loh & Wainwright (2014), we call ρλ1(·) µ-amenable if it satisfies (C1) and (µ, γ)amenable if it additionally satisfies (C2).",2.2. Dirty models with non-convex regularizers,[0],[0]
"The popular non-convex regularizers SCAD (Fan & Li, 2001) and MCP (Zhang, 2010) are both (µ, γ)-amenable (Loh & Wainwright, 2014).
",2.2. Dirty models with non-convex regularizers,[0],[0]
"The regularizer φλ2,a(·) a non-convex counterpart of the group regularizer λ2‖ · ‖1,a employed in (2) where we use ρλ2(·) instead of λ2‖ · ‖1, over groups:
φλ2,a(β) := ρλ2(G(β))
where G(β) := (‖βG1‖a, . . .",2.2. Dirty models with non-convex regularizers,[0],[0]
", ‖βGq‖a)>.",2.2. Dirty models with non-convex regularizers,[0],[0]
"Example of non-convex regularizers include the Group-SCAD and Group-MCP penalties where SCAD and MCP penalties are respectively used on the norm of each group.
",2.2. Dirty models with non-convex regularizers,[0],[0]
"Remarkably, the proof techniques developed in this paper make it possible to provide not only `2-error bounds under milder conditions than prior work on convex problem (2), but also support set recovery guarantees for non-convex one (8).",2.2. Dirty models with non-convex regularizers,[0],[0]
"In fact we shall see that dirty models with nonconvex regularizers (8) enjoy strictly better statistical guarantees than their convex counterpart (2), with practical consequences.",2.2. Dirty models with non-convex regularizers,[0],[0]
"As our starting point, we focus on the case of convex dirty models in (2) or equivalently in (6).","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"A key ingredient for showing statistical guarantees of regularized Mestimators is the decomposability of regularizer (Negahban et al., 2012).","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"However, considering the form of regularizer in (6), it is not obvious to find the model space and its orthogonal complement with which we could directly derive
error bounds with optimal rates.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"To circumvent this problem, Yang & Ravikumar (2013) utilize the decomposability of each component separately, but this requires restricted strong convexity (RSC) to hold jointly for all component parameters.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"In order to have the “joint” RSC property from “local” RSC with respect to each individual component, Yang & Ravikumar (2013) assume a structural incoherence condition.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"Even if the loss function is strongly convex with respect to each component, such incoherence across components is essential for the joint RSC due to the linearity across components.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"To see this more clearly, suppose we have the function z2 for z ∈ R, which is obviously strongly convex.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"If we assume, however, that z is the sum of two components x, y ∈ R, then one can immediately see that (x + y)2 is not strongly convex jointly in x and y because x and y are completely coherent in this one dimensional example.
","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"The problem is that the structural incoherence condition for the `1+`1,a setting is way too restrictive because the sparse and the group-sparse structures essentially share the same model and its orthogonal spaces1.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"In order to see this, we consider the popular linear model setting (3) for example.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
Let s∗ (and b∗) be the support set of true sparse (groupsparse) component and sc be its complement.,"3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"Furthermore, [ 1nX
>X](sc∩b∗) denotes the projection of the sample covariance onto sc ∩ b∗-coordinate space (j-th coordinate becomes zero if j /∈ sc ∩ b∗).","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
Projections on other spaces are defined similarly.,"3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"Then, the structural incoherence condition for joint RSC can be reduced as: for all (s, b) ∈ {(sc, b∗), (s∗, bc), (sc, bc)},
σmax ( [ 1nX >X](s∩b) )","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"≤ Cκ1 (9)
where σmax(·) is the maximum singular value of a matrix, κ1 is the curvature of (restricted) eigenvalue condition, and C is some fixed constant.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"Informally, this condition requires the maximum singular value of sample covariance (modulo the projection onto the true model and its orthogonal space) to be smaller than its minimum singular value (Note that for linear models, the curvature parameter of the eigenvalue condition is related to the minimum singular value of the sample covariance).","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
This condition can be easily shown to fail in many cases.,"3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"For instance consider the popular setting where the design matrix X is a set of samples from Gaussian ensemble with covariance Σ, and the true parameter is the sum of group sparse + a single nonzero component as depicted in Figure 1.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"Then, the incoherence condition in (9) implies maxi,j |[ 1nX
>X]ij | ≤ 1/128σmin(Σ), which can be easily violated in many natural setting of Σ because the minimum eigenvalue of Σ is smaller than the maximum element of Σ
1Note that the sparse + group sparse setting is outstanding.","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"The structural incohence assumption makes sense in other dirty models settings, e.g. sparse + low rank dirty models.
by the Rayleigh quotient.
","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"This naturally leads to the following question:
Can we provide tight error bounds for the problem (2) not requiring the joint RSC across individual structures and hence bypassing the incoherence condition?","3. Structural Incoherence: essential in prior work, yet an unreasonable assumption",[0],[0]
"In order to address the above question, our key proof technique is to establish the decomposability between two components of error vectors, by making the target components dependent of our estimation.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
Consider arbitrary target parameter θ∗ such that θ∗ = α∗ + β∗.,4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"Note that we do not impose additional constraints on defining the sparse component α∗ and the group sparse component β∗, hence the possible combination of (α∗,β∗) is not unique.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"As we will see later, we provide estimation error bounds that depend on the selection of (α∗,β∗)–more precisely on the sparsity level of α∗ and the group sparsity level of β∗.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
In that sense our theorems provide sets of estimation bounds.,4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"However, it is important to note that we still do not need to worry about the identifiability between structures, because we only care about the `2 and `∞ error rates of the final or “summed” estimator (we do not recover (nor care about) the individual components).
",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
Suppose we compute θ̃ from the program (6) where α̃ and β̃ are minimizing its dirty regularizer (7).,4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"Then, rather than directly deriving error bounds of θ̃ − θ∗ from α̃−α∗ and β̃ − β∗, which are not decomposable, we introduce an additional set of vectors, ᾱ, β̄ and θ̄ from the following rules:
1.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"If α∗j = β ∗ j = 0, then ᾱj = β̄j := 0.
2.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"If α∗j 6= 0 and β∗j = 0, then β̄j := β̃j , and ᾱj := θ∗j − β̃j .
3.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"If β∗j 6= 0, then ᾱj := α̃j and β̄j := θ∗j − α̃j .
4.",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"θ̄ is defined as the sum of ᾱ and β̄.
Sparse + Group Sparse Dirty Models
3 1 1 1 2 1 0 0",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"1 0 1 0 0 0 0
Theta^*
(a) θ∗
0 0 0 0 0 0 0 0",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"1 0 1 0 0 0 0
a^*
(b) α∗
3 1 1 1 2 1 0 0 0 0 0 0 0 0 0
b^*
(c) β∗
1.9 0 0.2 0",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"1.1 0 0.1 0 1 0 0.9 0.2 0.1 0 0
a_hat
(d) α̃
0.8 0.8 0.8 1.1 1.1 1.1 0.2 -0.2 0.2 0 0 0 0 0 0
",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"b_hat
(e) β̃
1.9 0 0.2 0 1.1 0 0.1 0 0 0 -0.1 0.2 0.1 0 0
Del^*
(f) α̃−α∗
-2.2 -0.2 -0.2 0.1 -0.9 0.1 0.2 -0.2 0.2 0 0 0 0 0 0
Gam^*
(g) β̃",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"− β∗
1.9 0 0.2 0 1.1 0 0 0 0.8 0 1 0 0 0 0
a_bar
(h) ᾱ",4. Our key strategy: Constructing surrogate components that are always decomposable.,[0],[0]
"1 0.9 1 0 0 0.2 0 0 0 0 0 0
b_bar
(i) β̄
0 0 0 0 0 0 0.1 0 0.2 0 -0.1 0.2 0.1 0 0
Del^bar
(j) α̃− ᾱ
-0.3 -0.2 0 0.1 0.2 0.1 0.2 -0.2 0 0 0 0 0 0 0
Gam^bar
(k) β̃",1.1 1 0.8,[0],[0]
"− β̄
Figure 2.",1.1 1 0.8,[0],[0]
"Example of constructing surrogate target parameters given (b) α∗, (c) β∗, (d) α̃ and (e) β̃ via transformation T (·).",1.1 1 0.8,[0],[0]
"Error vectors based on surrogates are decomposable (see text for details).
",1.1 1 0.8,[0],[0]
"By construction, θ̄ is same as θ∗, but ᾱ has different sparsity pattern and values fromα∗, depending on α̃. The same holds for β̄ as well.",1.1 1 0.8,[0],[0]
"We denote the above transformation as (ᾱ, β̄)",1.1 1 0.8,[0],[0]
":= T (α∗,β∗; α̃, β̃).
",1.1 1 0.8,[0],[0]
"It turns out that the error vectors computed based on the surrogate ᾱ and β̄ are always decomposable as described in the following proposition, and the consequence of this decomposability plays a key role for showing i) `2-error bounds without incoherence condition and ii) support set recovery guarantee for non-convex `1 + `1,a dirty regularizers (with faster estimation rates than for convex dirty regularizers).
",1.1 1 0.8,[0],[0]
Proposition 1.,1.1 1 0.8,[0],[0]
"Consider any local optimum θ̃ of convex or non-convex dirty models, and corresponding θ̄ := T (α∗,β∗; α̃, β̃).",1.1 1 0.8,[0],[0]
"Then, the error vectors for individual components, ∆ := α̃ − ᾱ and Γ := β̃ − β̄, are decomposable in the sense that
∣∣[∆ + Γ]j∣∣",1.1 1 0.8,[0],[0]
"= |∆j |+ |Γj | for all j, and the overall `2 error ‖θ̃",1.1 1 0.8,[0],[0]
"− θ∗‖2 is lower bounded as follows:
‖θ̃",1.1 1 0.8,[0],[0]
− θ∗‖22 ≥ ‖∆‖22,1.1 1 0.8,[0],[0]
+ ‖Γ‖22 .,1.1 1 0.8,[0],[0]
"(10)
Moreover, let s∗ := supp(α∗) (the support set ofα∗), s̄ := supp(α∗) ∪ supp(ᾱ) and U := supp(α∗) ∪ supp(β∗).",1.1 1 0.8,[0],[0]
"Similarly, we also define b∗ := supp(β∗) and b̄ := supp(β̄).",1.1 1 0.8,[0],[0]
"Then, by construction of T , s∗ ⊆ s̄ ⊆ U and b∗ ⊆ b̄ ⊆ U .",1.1 1 0.8,[0],[0]
"However, it is always guaranteed that
∆s∗ = ∆s̄ = ∆U and Γb∗ = Γb̄",1.1 1 0.8,[0],[0]
"= ΓU (11)
where ∆s∗ represents the projection of ∆ onto the s∗coordinate space; that is, [∆s∗ ]j is ∆j if j ∈ s∗, and 0 otherwise.
",1.1 1 0.8,[0],[0]
Illustrative example.,1.1 1 0.8,[0],[0]
Figure 2 describes an example: consider a 5× 3 matrix parameter with 5 known groups in rows.,1.1 1 0.8,[0],[0]
"Suppose (i) the target parameter is given by (a), (ii) we define (b) and (c) as the sparse and group sparse components of θ∗, and (iii) the minimizer of program (6) are computed as in (d) and (e), respectively for α̃ and β̃. Then, (f) and (g) show the error vectors for sparse and group-sparse components, which are not decomposable ((10) does not hold for (f) and (g)).",1.1 1 0.8,[0],[0]
"On the other hand, for ᾱ in (h) and β̄",1.1 1 0.8,[0],[0]
"in (i) derived from T (·), we can verify that surrogate error vectors (shown in (j) and (k)) are decomposable; at every position, α̃ − ᾱ and β̃",1.1 1 0.8,[0],[0]
− β̄ are sign consistent (or at least one of them is zero).,1.1 1 0.8,[0],[0]
"Throughout our analysis, we assume that the loss function L(·) is twice differentiable and and satisfies the restricted strong convexity condition
(RSC) For any vector θ1,θ2 ∈",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Rp, the loss function L(·) satisfies 〈
∇L(θ1 + θ2)−∇L(θ1),θ2 〉
≥ { κ1‖θ2‖22 − τ1‖θ2‖2η , for all ‖θ2‖2 ≤ 1 , (12) κ2‖θ2‖2",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"− τ2‖θ2‖η , for all ‖θ2‖2 ≥ 1 .",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"(13)
RSC of the loss is also used to guarantee `2-consistency (Negahban et al., 2012; Loh & Wainwright, 2015) or `∞- consistency (Loh & Wainwright, 2014) of “clean” structurally constrained problems (i.e. problems with a single
structure).",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Note that there are slight variations in the definition of RSC conditions in the literature.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Here we adopt the form with tolerance terms in Loh & Wainwright (2014; 2015), to allow for a wide class of non quadratic and/or non-convex loss functions.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"We will show that RSC with tolerance in dirty norm holds with high probability under the popular setting of Gaussian ensembles, as an example.
",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"For the analysis, we consider a slight modification of the program (6):
minimize ‖θ‖η≤r
L(θ) + ‖θ‖λ (14)
where L is possibly non-convex, but satisfies (RSC).",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
The additional constraint ‖θ‖η ≤ r also involves the dirty norm (7) but with a different parameter vector η.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
This constraint is a safety radius commonly used for analyzing non-convex problems to ensure that the global minimum exists (see e.g. Loh & Wainwright (2014; 2015)).,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"In practice, we can disregard this additional constraint.
",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Theorem 1.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Consider the dirty model for problem (14) where L(·) is possibly non-convex but satisfies the restricted strong convexity (RSC).,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Suppose that θ∗ is feasible and the regularization parameters are set so that λ1 ≥ 4‖∇L(θ∗)‖∞ and λ2 ≥ 4‖∇L(θ∗)‖∞,a∗ .",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Suppose furthermore that r ≤ min { κ2 4τ2 , κ25 max{λ1/η1,λ2/η2} , λ1 8τ1η1 , λ28τ1η2 } .",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Then, any local optimum θ̃ of (14) is guaranteed to be `2 consistent:∥∥θ̃ − θ∗∥∥ 2 ≤ 3 κ1 max { λ1 √ s , λ2 √ sG } (15)
where s is the number of nonzero elements in α∗ and sG is the number of nonzero groups in β∗.
Remarks.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"The error bound in (15) scales with (n, p, s, sG) at the same rate as previous analysis (Yang & Ravikumar, 2013) for the sparse plus group sparse setting, which required a much stringent incoherence condition, as we already discussed.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"It is also instructive to note that Theorem 1 holds for any combination of (α∗,β∗) such that α∗ + β∗ = θ∗, but different views of (α∗,β∗) constructing θ∗ give different bounds depending on sparsity/group sparsity levels of (α∗,β∗) (i.e. s and sG).",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"In this sense, Theorem 1 provides a set of `2 estimation upper bounds.
",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Linear model and modified graphical Lasso.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"In the following corollaries, we apply Theorem 1 to the linear model (3) and the modified graphical Lasso problem (5) and derive their corresponding `2 estimation bounds.
",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Corollary 1.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Consider the linear model (3).,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Assume that (i) each row Xi of the observation matrix X is independently sampled fromN(0,Σ), (ii)X is (group) column normalized by scaling as in (Negahban et al., 2012), and (iii)
w is independent sub-Gaussian with parameter σ.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Now suppose that in (14) we set a := 2 (where a is the parameter for the group norm both for ‖θ‖η and ‖θ‖λ), r constant (r only depends on Σ and σ), λ1 = η1 := 8σ √ log p/n
and λ2 = η2 := 8σ( √ m/n+",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"√ log q/n) for q groups and maximum group size m (maxg=1,...,q |Gg|).",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
Suppose that θ∗ is feasible to program (14) with these settings.,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Then with probability at least 1− c1 exp(−c2nλ2s)− c3/q2, any local optimum θ̃ satisfies
‖θ̃",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"− θ∗‖2 ≤ 24σ
κ1 max
{√ s log p
n ,
√ sGm
n +
√ sG log q
n
} .
where κ1 is some constant depending on Σ.
Corollary 2.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Consider the modified graphical Lasso (5) to estimate inverse covariance Θ∗. Suppose we set the parameters of (14) as λ1 = η1 := 4 maxi 6=j ∣∣Σ̂ij − (Θ∗)−1ij ∣∣, λ2 = η2 := 4 maxg∈G
∥∥Σ̂g",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"− (Θ∗)−1g ∥∥a∗ and r ≤ 1
5(|||Θ∗|||2+1)2 where ||| · |||2 is the spectral norm of the matrix and a ≥ 2.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"In addition, assume that Θ∗ is feasible for this problem.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Then, any local optimum Θ̃ satisfies
‖Θ̃−Θ∗‖F ≤ 3(|||Θ∗|||2 + 1)2 max { λ1 √ s , λ2 √ sG } .
",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"(16)
Remark.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"Since ‖θ‖η scales with 1√n for the specified values of η, the constraint ‖θ‖η ≤ r gets milder as n increases.",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"It is also important to note that this constraint is no more stringent than those of non-convex analyses with a single regularizer (Loh & Wainwright, 2015; 2014): their constraints can be written as η1‖θ‖1 ≤",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"r (since η1 √
log p/n for linear models for example.)",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
"in our notation, which directly implies ‖θ‖η ≤ r",5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
since ‖θ‖η ≤ η1‖θ‖1 by the definition of ‖ · ‖η .,5. Statistical Guarantees of Models with Convex Regularizers,[0],[0]
A natural extension of (14) is to incorporate non-convex regularizers that have some advantages such as unbiasedness.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"For that purpose, in this section we consider the following formulation
minimize ‖θ‖η≤r
L(θ) +R(θ;λ)",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"(17)
whereR(θ;λ) = infα,β{ρλ1 ( α ) +",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"φλ2,a ( β )
: α+ β = θ}.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"While the `2 analysis in Theorem 1 can be extended to non-convex regularizers following proof techniques recently developed in Loh & Wainwright (2015), using nonconvex unbiased regularizers has no benefit in terms of asymptotic convergence rates of `2 estimation errors.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Instead, we here investigate the `∞-norm bound and related support set recovery guarantees where non-convex unbiased regularizers help.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"To derive `∞ bounds, we use the
primal-dual witness method described in the supplementary materials.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Theorem 2.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Consider the dirty program with nonconvex penalties in (17), under (RSC).",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Suppose 2r(τ2 + 2 max{λ1η1 , λ2 η2 }) ≤ 1.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Also suppose,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"that for some
δ ∈ [ max{ 4rτ1η1λ1 , 4rτ1η2 λ2 }, 1 ] , the strict dual feasibility of primal-dual witness holds.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Then, any stationary point θ̃ of (17) is supported by U (recall U := supp(α∗) ∪ supp(β∗))",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"if the number of samples is large enough to satisfy max{λ1 √ s, λ2 √ sG}2",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"< c for some constant c depending only on κ1, τ1 and δ.
",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"As in Theorem 1, the decomposability in (10) and (11) with respect to the surrogates ᾱ and β̄, plays a crucial role in establishing the support set recovery guarantee of any local optimum in Theorem 2.
",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Based on Theorem 2, we can derive the `∞ bounds following the standard steps in (Loh & Wainwright, 2014):",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Corollary 3.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Suppose the assumptions in Theorem 2 hold.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Then,
1.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"If κ1−µ2 ≥ τ1 ( max{η1 √ s, η2 √ sG} )2
holds for large enough sample size n, the program (17) has a unique stationary point θ̂, specified by the construction of the primal dual witness.
2.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Letting Q̂ := ∫ 1
0 ∇2L
( θ∗ + t(θ̂ − θ∗) )",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"dt, it holds
that ‖θ̂ − θ∗‖∞ ≤ ∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"+
min{λ1, λ2} ∣∣∣∣∣∣(Q̂UU)−1∣∣∣∣∣∣∞ where ||| · |||∞ denotes a matrix induced norm (maximum absolute row sum).
3.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Moreover, if ρ is (µ, γ)-amenable, and the minimum absolute value θ∗min := minj |θ∗j | is lowerbounded by θ∗min ≥ ∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞ + min{λ1, λ2}
∣∣∣∣∣∣(Q̂UU)−1∣∣∣∣∣∣∞ + 2 max{λ1, λ2}γ.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Then, the error bound in the statement 2 is reduced to tighter bound as ‖θ̂ − θ∗‖∞ ≤∥∥(Q̂UU)−1∇L(θ∗)U∥∥∞.
Multi-task high-dimensional linear regression.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"We consider the multi-task high-dimensional linear regression, as a concrete example of using non-convex dirty regularizers.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
This is the counterpart of model (4) which uses convex dirty regularizer.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"In the following corollary, we analyze the sparsistency of dirty multi-task linear regression with nonconvex regularizers:
minimize Θ∈Rp×ms.t.‖Θ‖η≤r m∑ k=1 1 2n ∥∥y(k) −X(k)Θ(·,k)∥∥22 +R(Θ;λ)",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"(18)
where R(Θ;λ) = infα,β{ρλ1 ( α ) +",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"φλ2,a ( β )
: α+ β = Θ}.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Now, we derive a corollary for this particular nonconvex dirty model.
",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Corollary 4.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Consider the multitask regression model.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Suppose that for each task, design matrix X(k) is a zeromean Gaussian ensemble and is column normalized, w(k) is independent sub-Gaussian with parameter σ.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Now suppose we set parameters of (18) as a :=∞, r constant (only depends on Σ and σ.), λ1 := c1σ √ log(pm)/n, λ2 :=
c2σ √
(log p+m log 2)/n, and Θ∗ is feasible to program (14) with these settings.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Then, for any local optimum Θ̃, with probability at least 1 − c1 exp(−c2 log(pm))",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"− c3 exp ( −c4(log p+m log 2) ) (which is approaching to 1) for some positive constants c1 − c4,
1. supp(Θ̃) ⊆ supp(Θ∗),
2.",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"if additionally the regularizer ρλ is (µ, γ)-amenable with µ < λmin(Σ) (the minimum eigenvalue of Σ) and Cmin := mink=1,...,m λmin ( Σ (k) UkUk ) > 0, then
supp(Θ̃) = supp(Θ∗) and the element-wise difference is bounded as follows:
|||Θ̃−Θ∗|||max := max i,j |[Θ̃−Θ∗]i,j | ≤ σ
√ 100 log(pm)
nCmin
provided that θ∗min ≥ σ",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"√ 100 log(pm) nCmin + min{λ1, λ2} maxk=1,...,m |||(Σ(k)UkUk) −1|||∞ + 2 max{λ1, λ2}γ.
",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
Remark.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"In order to highlight the benefit of using (µ, γ)-amenable regularizers, we briefly compare the result of Corollary 4 with that of `1 + `1,a case in (Jalali et al., 2010).",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Not only the result in (Jalali et al., 2010) requires the incoherence on X (specifically, maxj∈Uc ∑m k=1
∥∥Σ(k)j,Uk(Σ(k)j,Uk)−1∥∥1 < 1), but it also has an additional sλ1
Cmin √ n term in |||Θ̃",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
− Θ∗|||max bound.,6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"Moreover, the required λ1 and λ2 there can converge
to zero more slowly: λ1 √
log(pm) √ n− √ s log(pm)
and λ2 √ m(m+log p)
√ n− √ sm(m+log p) .",6. Statistical Guarantees of Models with Non-convex Penalties,[0],[0]
"To illustrate the practical consequences of the superior statistical guarantees of models with non-convex penalties, we perform experiments on both simulated and real-world data and compare convex and non-convex dirty models for sparse + group-sparse structures.
",7. Experiments,[0],[0]
Simulated data.,7. Experiments,[0],[0]
"We consider multitask regression problems with m = 10 tasks and p = 260 variables for settings of parameters (s, sG) ∈ {(2p/10, p/20), (p/10, p/10)} with respectively less / more support overlap across tasks (recall s and sG are the number of nonzero elements in α∗ and the number of nonzero groups in β∗, respectively).",7. Experiments,[0],[0]
"The
rows of the design matrices X are sampled i.i.d.",7. Experiments,[0],[0]
from a zero-mean Gaussian distribution with correlation of 0.2 between feature pairs.,7. Experiments,[0],[0]
"For each set of parameters (s, sG), we generate 100 instances of the problem where for each instance the non-zero entries of the true model parameter matrix are i.i.d. zero-mean Gaussian to agree with s and sG .",7. Experiments,[0],[0]
Gaussian error with standard deviation of 4 is added to each observation.,7. Experiments,[0],[0]
"For varying sample size n we measure the `∞ error of parameters estimated by (i) convex dirty model (Jalali et al., 2010), (ii) non-convex dirty model with SCAD + Group-SCAD penalty, and (iii) nonconvex dirty model with MCP + Group-MCP penalty.",7. Experiments,[0],[0]
We also evaluate the following baselines:,7. Experiments,[0],[0]
"Lasso, MCP, SCAD, Group-Lasso, Group-MCP and Group-SCAD 2.",7. Experiments,[0],[0]
The regularization parameters of each method are tuned via 5-folds cross-validation.,7. Experiments,[0],[0]
"The results are presented in Figure 3 (To avoid cluttering the graphs, we do not display standard errors as these are much lower than the gaps between the pertinent groups of methods, and we only display the best group of baselines for each setting).",7. Experiments,[0],[0]
"As can be seen from the figure, dirty models with non-convex penalties enjoy superior performance over their counterparts with convex penalties as a function of the sample size.",7. Experiments,[0],[0]
"In terms of computational cost, (Group) coordinate descent steps for (group) lasso, (group) MCP and (group) SCAD all have simple closed-form expressions (Huang et al., 2012), similarly for proximal-based approaches.",7. Experiments,[0],[0]
"We noticed that for a wide range of (λ1, λ2), non-convex procedures took less time and converged faster (See supplements).",7. Experiments,[0],[0]
"As future work it would be interesting to study their theoretical numerical convergence rates.
",7. Experiments,[0],[0]
Real data analysis.,7. Experiments,[0],[0]
We consider the problem of predicting biological activities of molecules given features extracted from their chemical structures.,7. Experiments,[0],[0]
"We analyze three biological activity datasets from the “molec-
2Our theorem on `∞ consistency requires the sample size to be larger than the maximum of two terms, which precludes from presenting graphs with curve alignment across p (by rescaling the x-axis with a control parameter as in Jalali et al. (2010)).
ular activity challenge” (http://www.kaggle.com/ c/MerckActivity).",7. Experiments,[0],[0]
"Specifically we consider multitask regression with three tasks corresponding to predicting the raw value (− log(IC50)) of three different types of biological activities : ‘binding to cannabinoid receptor 1’, ‘inhibition of dipeptidyl peptidase 4’ and ‘time dependent 3A4 inhibitions’.",7. Experiments,[0],[0]
For each task we used n = 200 observations with p = 3000 molecular features.,7. Experiments,[0],[0]
"We consider 20 random data splits into training and validation sets, using 2/3 of the data for tranining and 1/3 for validation, and report the average R2 over these random splits.",7. Experiments,[0],[0]
"As shown in table1, dirty models outperformed “clean” models suggesting the importance to strike a balanc e between task specificity and sharing for this data.",7. Experiments,[0],[0]
"Non-convex dirty models achieved the best R2, which illustrate their capability as a valuable tool for high-dimensional data analysis.",7. Experiments,[0],[0]
"This paper finally resolved the outstanding case of sparse + group-sparse dirty models with convex penalties: we provided the first satisfactory consistency results that do not require implausible assumptions, thereby fully justifying their practical success.",8. Concluding Remarks,[0],[0]
In addition we proposed and studied dirty models with non-convex penalties and showed that they enjoy superior theoretical guarantees that translate into significant practical impact.,8. Concluding Remarks,[0],[0]
An interesting direction for future work is to investigate whether our proof technique might be applicable to other dirty models and beyond.,8. Concluding Remarks,[0],[0]
"E.Y. acknowledges the support of MSIP/NRF (National Research Foundation of Korea) via NRF2016R1A5A1012966 and MSIP/IITP (Institute for Information & Communications Technology Promotion of Korea) via ICT R&D program 2016-0-00563, 2017-0-00537.",Acknowledgments,[0],[0]
Imposing sparse + group-sparse superposition structures in high-dimensional parameter estimation is known to provide flexible regularization that is more realistic for many real-world problems.,abstractText,[0],[0]
"For example, such a superposition enables partially-shared support sets in multi-task learning, thereby striking the right balance between parameter overlap across tasks and task specificity.",abstractText,[0],[0]
"Existing theoretical results on estimation consistency, however, are problematic as they require too stringent an assumption: the incoherence between sparse and group-sparse superposed components.",abstractText,[0],[0]
"In this paper, we fill the gap between the practical success and suboptimal analysis of sparse + group-sparse models, by providing the first consistency results that do not require unrealistic assumptions.",abstractText,[0],[0]
We also study non-convex counterparts of sparse + groupsparse models.,abstractText,[0],[0]
"Interestingly, we show that these are guaranteed to recover the true support set under much milder conditions and with smaller sample size than convex models, which might be critical in practical applications as illustrated by our experiments.",abstractText,[0],[0]
Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370–376 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
370",text,[0],[0]
"Neural machine translation (NMT) emerged in the last few years as a very successful paradigm (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017).",1 Introduction,[0],[0]
"While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern (Koehn and Knowles, 2017): common mistakes include dropping source words and repeating words in the generated translation.
",1 Introduction,[0],[0]
Previous work has attempted to mitigate this problem in various ways.,1 Introduction,[0],[0]
"Wu et al. (2016) incorporate coverage and length penalties during beam search—a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam.",1 Introduction,[0],[0]
"Other approaches involve architectural changes: providing coverage vectors to track the attention history (Mi et al., 2016; Tu et al., 2016), using gating architectures and adaptive attention to control the amount of source context provided (Tu et al., 2017a; Li and Zhu, 2017), or adding a reconstruction loss (Tu et al., 2017b).",1 Introduction,[0],[0]
"Feng et al. (2016) also use the notion of fertility
∗Work done during an internship at Unbabel.
implicitly in their proposed model.",1 Introduction,[0],[0]
"Their fertility conditioned decoder uses a coverage vector and an extract gate which are incorporated in the decoding recurrent unit, increasing the number of parameters.
",1 Introduction,[0],[0]
"In this paper, we propose a different solution that does not change the overall architecture, but only the attention transformation.",1 Introduction,[0],[0]
"Namely, we replace the traditional softmax by other recently proposed transformations that either promote attention sparsity (Martins and Astudillo, 2016) or upper bound the amount of attention a word can receive (Martins and Kreutzer, 2017).",1 Introduction,[0],[0]
The bounds are determined by the fertility values of the source words.,1 Introduction,[0],[0]
"While these transformations have given encouraging results in various NLP problems, they have never been applied to NMT, to the best of our knowledge.",1 Introduction,[0],[0]
"Furthermore, we combine these two ideas and propose a novel attention transformation, constrained sparsemax, which produces both sparse and bounded attention weights, yielding a compact and interpretable set of alignments.",1 Introduction,[0],[0]
"While being in-between soft and hard alignments (Figure 2), the constrained sparsemax transformation is end-to-end differentiable, hence amenable for training with gradient backpropagation.
",1 Introduction,[0],[0]
"To sum up, our contributions are as follows:1
• We formulate constrained sparsemax and derive efficient linear and sublinear-time algorithms for running forward and backward propagation.",1 Introduction,[0],[0]
"This transformation has two levels of sparsity: over time steps, and over the attended words at each step.
",1 Introduction,[0],[0]
"• We provide a detailed empirical comparison of various attention transformations, including softmax (Bahdanau et al., 2014), sparse1Our software code is available at the OpenNMT fork
www.github.com/Unbabel/OpenNMT-py/tree/dev and the running scripts at www.github.com/Unbabel/ sparse constrained attention.
",1 Introduction,[0],[0]
"max (Martins and Astudillo, 2016), constrained softmax (Martins and Kreutzer, 2017), and our newly proposed constrained sparsemax.",1 Introduction,[0],[0]
We provide error analysis including two new metrics targeted at detecting coverage problems.,1 Introduction,[0],[0]
"Our underlying model architecture is a standard attentional encoder-decoder (Bahdanau et al., 2014).",2 Preliminaries,[0],[0]
Let x,2 Preliminaries,[0],[0]
":= x1:J and y := y1:T denote the source and target sentences, respectively.",2 Preliminaries,[0],[0]
We use a Bi-LSTM encoder to represent the source words as a matrix H :=,2 Preliminaries,[0],[0]
"[h1, . . .",2 Preliminaries,[0],[0]
",hJ ] ∈ R2D×J .",2 Preliminaries,[0],[0]
"The conditional probability of the target sentence is given as
p(y | x) := ∏T t=1 p(yt | y1:(t−1), x), (1)
where p(yt | y1:(t−1), x) is computed by a softmax output layer that receives a decoder state st as input.",2 Preliminaries,[0],[0]
"This state is updated by an auto-regressive LSTM, st = RNN(embed(yt−1), st−1, ct), where ct is an input context vector.",2 Preliminaries,[0],[0]
"This vector is computed as ct := Hαt, where αt is a probability distribution that represents the attention over the source words, commonly obtained as
αt = softmax(zt), (2)
",2 Preliminaries,[0],[0]
where zt ∈ RJ is a vector of scores.,2 Preliminaries,[0],[0]
"We follow Luong et al. (2015) and define zt,j := s>t−1Whj as a bilinear transformation of encoder and decoder states, whereW is a model parameter.2",2 Preliminaries,[0],[0]
"In this work, we consider alternatives to Eq. 2.",3 Sparse and Constrained Attention,[0],[0]
"Since the softmax is strictly positive, it forces all words in the source to receive some probability mass in the resulting attention distribution, which can be wasteful.",3 Sparse and Constrained Attention,[0],[0]
"Moreover, it may happen that the decoder attends repeatedly to the same source words across time steps, causing repetitions in the generated translation, as Tu et al. (2016) observed.
",3 Sparse and Constrained Attention,[0],[0]
"With this in mind, we replace Eq. 2 by αt = ρ(zt,ut), where ρ is a transformation that may depend both on the scores zt ∈ RJ and on upper bounds ut ∈",3 Sparse and Constrained Attention,[0],[0]
RJ,3 Sparse and Constrained Attention,[0],[0]
that limit the amount of attention that each word can receive.,3 Sparse and Constrained Attention,[0],[0]
"We consider three alternatives to softmax, described next.
",3 Sparse and Constrained Attention,[0],[0]
2This is the default implementation in the OpenNMT package.,3 Sparse and Constrained Attention,[0],[0]
"In preliminary experiments, feedforward attention (Bahdanau et al., 2014) did not show improvements.
Sparsemax.",3 Sparse and Constrained Attention,[0],[0]
"The sparsemax transformation (Martins and Astudillo, 2016) is defined as:
sparsemax(z) := argmin α∈∆J
‖α− z‖2, (3)
",3 Sparse and Constrained Attention,[0],[0]
"where ∆J := {α ∈ RJ | α ≥ 0, ∑
j αj = 1}.",3 Sparse and Constrained Attention,[0],[0]
"In words, it is the Euclidean projection of the scores z onto the probability simplex.",3 Sparse and Constrained Attention,[0],[0]
"These projections tend to hit the boundary of the simplex, yielding a sparse probability distribution.",3 Sparse and Constrained Attention,[0],[0]
"This allows the decoder to attend only to a few words in the source, assigning zero probability mass to all other words.",3 Sparse and Constrained Attention,[0],[0]
"Martins and Astudillo (2016) have shown that the sparsemax can be evaluated in O(J) time (same asymptotic cost as softmax) and gradient backpropagation takes sublinear time (faster than softmax), by exploiting the sparsity of the solution.
",3 Sparse and Constrained Attention,[0],[0]
Constrained softmax.,3 Sparse and Constrained Attention,[0],[0]
"The constrained softmax transformation was recently proposed by Martins and Kreutzer (2017) in the context of easy-first sequence tagging, being defined as follows:
csoftmax(z;u) := argmin α∈∆J
KL(α‖ softmax(z))
",3 Sparse and Constrained Attention,[0],[0]
s.t.,3 Sparse and Constrained Attention,[0],[0]
α ≤,3 Sparse and Constrained Attention,[0],[0]
"u, (4)
where u is a vector of upper bounds, and KL(.‖.) is the Kullback-Leibler divergence.",3 Sparse and Constrained Attention,[0],[0]
"In other words, it returns the distribution closest to softmax(z) whose attention probabilities are bounded by u. Martins and Kreutzer (2017) have shown that this transformation can be evaluated in O(J log J) time and its gradients backpropagated in O(J) time.
",3 Sparse and Constrained Attention,[0],[0]
"To use this transformation in the attention mechanism, we make use of the idea of fertility (Brown et al., 1993).",3 Sparse and Constrained Attention,[0],[0]
"Namely, let βt−1 :=∑t−1
τ=1ατ denote the cumulative attention that each source word has received up to time step t, and let f := (fj)Jj=1 be a vector containing fertility upper bounds for each source word.",3 Sparse and Constrained Attention,[0],[0]
"The attention at step t is computed as
αt = csoftmax(zt,f − βt−1).",3 Sparse and Constrained Attention,[0],[0]
"(5)
Intuitively, each source word j gets a credit of fj units of attention, which are consumed along the decoding process.",3 Sparse and Constrained Attention,[0],[0]
"If all the credit is exhausted, it receives zero attention from then on.",3 Sparse and Constrained Attention,[0],[0]
"Unlike the sparsemax transformation, which places sparse attention over the source words, the constrained softmax leads to sparsity over time steps.
",3 Sparse and Constrained Attention,[0],[0]
Constrained sparsemax.,3 Sparse and Constrained Attention,[0],[0]
"In this work, we propose a novel transformation which shares the two properties above: it provides both sparse and bounded probabilities.",3 Sparse and Constrained Attention,[0],[0]
"It is defined as:
csparsemax(z;u) := argmin α∈∆J
‖α− z‖2
s.t.",3 Sparse and Constrained Attention,[0],[0]
"α ≤ u. (6)
",3 Sparse and Constrained Attention,[0],[0]
"The following result, whose detailed proof we include as supplementary material (Appendix A), is key for enabling the use of the constrained sparsemax transformation in neural networks.
",3 Sparse and Constrained Attention,[0],[0]
"Proposition 1 Let α? = csparsemax(z;u) be the solution of Eq. 6, and define the sets",3 Sparse and Constrained Attention,[0],[0]
A = {j ∈,3 Sparse and Constrained Attention,[0],[0]
"[J ] | 0 < α?j < uj}, AL = {j ∈",3 Sparse and Constrained Attention,[0],[0]
"[J ] | α?j = 0}, and AR = {j ∈",3 Sparse and Constrained Attention,[0],[0]
[J ] | α?j = uj}.,3 Sparse and Constrained Attention,[0],[0]
Then: • Forward propagation.,3 Sparse and Constrained Attention,[0],[0]
"α? can be com-
puted in O(J) time with the algorithm of Pardalos and Kovoor (1990) (Alg. 1 in Appendix A).",3 Sparse and Constrained Attention,[0],[0]
"The solution takes the form α?j = max{0,min{uj , zj − τ}}, where τ is a normalization constant.
",3 Sparse and Constrained Attention,[0],[0]
• Gradient backpropagation.,3 Sparse and Constrained Attention,[0],[0]
Backpropagation takes sublinear time O(|A| + |AR|).,3 Sparse and Constrained Attention,[0],[0]
"Let L(θ)
be a loss function, dα = ∇αL(θ) be the output gradient, and dz = ∇zL(θ) and du = ∇uL(θ) be the input gradients.",3 Sparse and Constrained Attention,[0],[0]
"Then, we have:
dzj = 1(j ∈ A)(dαj −m) (7) duj = 1(j ∈ AR)(dαj −m), (8)
where m = 1|A| ∑ j∈A dαj .",3 Sparse and Constrained Attention,[0],[0]
"We experiment with three ways of setting the fertility of the source words: CONSTANT, GUIDED, and PREDICTED.",4 Fertility Bounds,[0],[0]
"With CONSTANT, we set the fertilities of all source words to a fixed integer value f .",4 Fertility Bounds,[0],[0]
"With GUIDED, we train a word aligner based on IBM Model 2 (we used fast align in our experiments, Dyer et al. (2013)) and, for each word in the vocabulary, we set the fertilities to the maximal observed value in the training data (or 1 if no alignment was observed).",4 Fertility Bounds,[0],[0]
"With the PREDICTED strategy, we train a separate fertility predictor model using a bi-LSTM tagger.3 At training time, we provide as supervision the fertility estimated by fast align.",4 Fertility Bounds,[0],[0]
"Since our model works
3A similar strategy was recently used by Gu et al. (2018) as a component of their non-autoregressive NMT model.
with fertility upper bounds and the word aligner may miss some word pairs, we found it beneficial to add a constant to this number (1 in our experiments).",4 Fertility Bounds,[0],[0]
"At test time, we use the expected fertilities according to our model.
",4 Fertility Bounds,[0],[0]
Sink token.,4 Fertility Bounds,[0],[0]
"We append an additional <SINK> token to the end of the source sentence, to which we assign unbounded fertility (fJ+1 = ∞).",4 Fertility Bounds,[0],[0]
The token is akin to the null alignment in IBM models.,4 Fertility Bounds,[0],[0]
"The reason we add this token is the following: without the sink token, the length of the generated target sentence can never exceed ∑ j fj words if we use constrained softmax/sparsemax.",4 Fertility Bounds,[0],[0]
"At training time this may be problematic, since the target length is fixed and the problems in Eqs.",4 Fertility Bounds,[0],[0]
4–6 can become infeasible.,4 Fertility Bounds,[0],[0]
"By adding the sink token we guarantee ∑ j fj =∞, eliminating the problem.
Exhaustion strategies.",4 Fertility Bounds,[0],[0]
"To avoid missing source words, we implemented a simple strategy to encourage more attention to words with larger credit: we redefine the pre-attention word scores as z′t = zt + cut, where c is a constant (c = 0.2 in our experiments).",4 Fertility Bounds,[0],[0]
This increases the score of words which have not yet exhausted their fertility (we may regard it as a “soft” lower bound in Eqs. 4–6).,4 Fertility Bounds,[0],[0]
We evaluated our attention transformations on three language pairs.,5 Experiments,[0],[0]
"We focused on small datasets, as they are the most affected by coverage mistakes.",5 Experiments,[0],[0]
"We use the IWSLT 2014 corpus for DE-EN, the KFTT corpus for JA-EN (Neubig, 2011), and the WMT 2016 dataset for ROEN.",5 Experiments,[0],[0]
"The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively.",5 Experiments,[0],[0]
"Our reason to prefer smaller datasets is that this regime is what brings more adequacy issues and demands more structural biases, hence it is a good test bed for our methods.",5 Experiments,[0],[0]
"We tokenized the data using the Moses scripts and preprocessed it with subword units (Sennrich et al., 2016) with a joint vocabulary and 32k merge operations.",5 Experiments,[0],[0]
"Our implementation was done on a fork of the OpenNMT-py toolkit (Klein et al., 2017) with the default parameters 4.",5 Experiments,[0],[0]
We used a validation set to tune hyperparameters introduced by our model.,5 Experiments,[0],[0]
"Even though our attention implementations are CPU-based using NumPy (unlike the rest of the computation which is done on the GPU), we did not observe any noticeable slowdown using multiple devices.
",5 Experiments,[0],[0]
"As baselines, we use softmax attention, as well as two recently proposed coverage models: • COVPENALTY (Wu et al., 2016, §7).",5 Experiments,[0],[0]
"At test
time, the hypotheses in the beam are rescored with a global score that includes a length and a coverage penalty.5",5 Experiments,[0],[0]
"We tuned α and β with grid search on {0.2k}5k=0, as in Wu et al. (2016).
",5 Experiments,[0],[0]
"• COVVECTOR (Tu et al., 2016).",5 Experiments,[0],[0]
"At training and test time, coverage vectors β and additional parameters v are used to condition the next attention step.",5 Experiments,[0],[0]
"We adapted this to our bilinear attention by defining zt,j = s>t−1(Whj + vβt−1,j).
",5 Experiments,[0],[0]
"We also experimented combining the strategies above with the sparsemax transformation.
",5 Experiments,[0],[0]
"As evaluation metrics, we report tokenized BLEU, METEOR (Denkowski and Lavie (2014), as well as two new metrics that we describe next to account for over and under-translation.6
4We used a 2-layer LSTM, embedding and hidden size of 500, dropout 0.3, and the SGD optimizer for 13 epochs.
",5 Experiments,[0],[0]
"5Since our sparse attention can become 0 for some words, we extended the original coverage penalty by adding another parameter , set to 0.1: cp(x; y) := β ∑J j=1 logmax{ ,min{1, ∑|y|
t=1 αjt}}.",5 Experiments,[0],[0]
"6Both evaluation metrics are included in our software
package at www.github.com/Unbabel/ sparse constrained attention.
",5 Experiments,[0],[0]
REP-score: a new metric to count repetitions.,5 Experiments,[0],[0]
"Formally, given an n-gram s ∈ V n, let t(s) and r(s) be the its frequency in the model translation and reference.",5 Experiments,[0],[0]
"We first compute a sentence-level score
σ(t, r) =",5 Experiments,[0],[0]
"λ1 ∑
s∈V n, t(s)≥2",5 Experiments,[0],[0]
"max{0, t(s)−",5 Experiments,[0],[0]
r(s)},5 Experiments,[0],[0]
"+ λ2 ∑ w∈V max{0, t(ww)− r(ww)}.
",5 Experiments,[0],[0]
"The REP-score is then given by summing σ(t, r) over sentences, normalizing by the number of words on the reference corpus, and multiplying by 100.",5 Experiments,[0],[0]
"We used n = 2, λ1 = 1 and λ2 = 2.
DROP-score: a new metric that accounts for possibly dropped words.",5 Experiments,[0],[0]
"To compute it, we first compute two sets of word alignments: from source to reference translation, and from source to the predicted translation.",5 Experiments,[0],[0]
"In our experiments, the alignments were obtained with fast align (Dyer et al., 2013), trained on the training partition of the data.",5 Experiments,[0],[0]
"Then, the DROP-score computes the percentage of source words that aligned with some word from the reference translation, but not with any word from the predicted translation.
",5 Experiments,[0],[0]
Table 1 shows the results.,5 Experiments,[0],[0]
"We can see that on average, the sparse models (csparsemax as well as sparsemax combined with coverage models) have higher scores on both BLEU and METEOR.",5 Experiments,[0],[0]
"Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of coverage to some extent.
",5 Experiments,[0],[0]
"To compare different fertility strategies, we ran experiments on the DE-EN for the csparsemax transformation (Table 2).",5 Experiments,[0],[0]
"We see that the PREDICTED strategy outperforms the others both in terms of BLEU and METEOR, albeit slightly.
",5 Experiments,[0],[0]
"Figure 2 shows examples of sentences for which the csparsemax fixed repetitions, along with the corresponding attention maps.",5 Experiments,[0],[0]
"We see that in the case of softmax repetitions, the decoder attends repeatedly to the same portion of the source sentence (the expression “letzten hundert” in the first sentence and “regierung” in the second sentence).",5 Experiments,[0],[0]
"Not only did csparsemax avoid repetitions, but it also yielded a sparse set of alignments, as expected.",5 Experiments,[0],[0]
Appendix B provides more examples of translations from all models in discussion.,5 Experiments,[0],[0]
"We proposed a new approach to address the coverage problem in NMT, by replacing the softmax attentional transformation by sparse and constrained alternatives: sparsemax, constrained softmax, and the newly proposed constrained sparsemax.",6 Conclusions,[0],[0]
"For the latter, we derived efficient forward and backward propagation algorithms.",6 Conclusions,[0],[0]
"By incorporating a model for fertility prediction, our attention transformations led to sparse alignments, avoiding repeated words in the translation.",6 Conclusions,[0],[0]
"We thank the Unbabel AI Research team for numerous discussions, and the three anonymous reviewers for their insightful comments.",Acknowledgments,[0],[0]
"This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMUPERI/TIC/0046/2014 (GoLocal).",Acknowledgments,[0],[0]
"In NMT, words are sometimes dropped from the source or generated repeatedly in the translation.",abstractText,[0],[0]
We explore novel strategies to address the coverage problem that change only the attention transformation.,abstractText,[0],[0]
"Our approach allocates fertilities to source words, used to bound the attention each word can receive.",abstractText,[0],[0]
"We experiment with various sparse and constrained attention transformations and propose a new one, constrained sparsemax, shown to be differentiable and sparse.",abstractText,[0],[0]
Empirical evaluation is provided in three languages pairs.,abstractText,[0],[0]
Sparse and Constrained Attention for Neural Machine Translation,title,[0],[0]
Determining the linguistic structure of natural language texts based on rich hand-crafted features has a long-going history in natural language processing.,1 Introduction,[0],[0]
"The focus of traditional approaches has mostly been on building linguistic analyzers for a particular kind of analysis, which often leads to the incorporation of extensive linguistic and/or domain knowledge for defining the feature space.",1 Introduction,[0],[0]
"Consequently, traditional models easily become language and/or task specific resulting in improper generalization properties.
",1 Introduction,[0],[0]
"A new research direction has emerged recently, that aims at building more general models that require far less feature engineering or none at all.",1 Introduction,[0],[0]
"These advancements in natural language processing, pioneered by Bengio et al. (2003), followed by Collobert and Weston (2008), Collobert et al. (2011),
Mikolov et al. (2013a) among others, employ a different philosophy.",1 Introduction,[0],[0]
"The objective of these works is to find representations for linguistic phenomena in an unsupervised manner by relying on large amounts of text.
",1 Introduction,[0],[0]
"Natural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words.",1 Introduction,[0],[0]
"In our paper we empirically verify via rigorous experiments that turning these dense representations into a much sparser (yet denser than one-hot encoding) form can keep the most salient parts of word representations that are highly suitable for sequence models.
",1 Introduction,[0],[0]
"Furthermore, our experiments reveal that our proposed model performs substantially better than traditional feature-rich models in the absence of abundant training data.",1 Introduction,[0],[0]
"Our proposed model also has the advantage of performing well on multiple sequence labeling tasks without any modification in the applied word representations thanks to the sparse features derived from continuous word representations.
",1 Introduction,[0],[0]
Our work aims at introducing a novel sequence labeling model solely utilizing features derived from the sparse coding of continuous word embeddings.,1 Introduction,[0],[0]
"Even though sparse coding had previously been utilized in NLP prior to us (Faruqui et al., 2015; Chen et al., 2016), to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:
• We show that the proposed sparse representation is general as sequence labeling models trained on them achieve (near) state-of-the-art performances for both POS tagging and NER.
247
Transactions of the Association for Computational Linguistics, vol. 5, pp. 247–261, 2017.",1 Introduction,[0],[0]
Action Editor: Hinrich Schütze.,1 Introduction,[0],[0]
"Submission batch: 12/2015; Revision batch: 5/2016; 11/2016; Published 7/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"• We show that the representation is general in the other sense, that it produces reasonable results for more than 40 treebanks for POS tagging,
• rigorously compare different sparse coding approaches in conjunction with differently trained continuous word embeddings,
• highlight the favorable generalization properties of our model in settings when access to a very limited training corpus is assumed,
• release the sparse word representations determined for our experiments at https:// begab.github.io/sparse_embeds to ensure the replicability of our results and to foster further multilingual NLP research.",1 Introduction,[0],[0]
"The line of research introduced in this paper relies on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding (Mairal et al., 2010) and also shows close resemblance to (Faruqui et al., 2015).",2 Related work,[0],[0]
"Distributed word representations assign some relatively low-dimensional, dense vectors to each word in a corpus such that words with similar context and meaning tend to have similar representations.",2.1 Distributed word representations,[0],[0]
"From an algebraic point of view, the embedding of word i having index idxi in a vocabulary V can be thought of as the result of a matrix-vector multiplication W1i, where the ith column of matrix W ∈ Rk×|V | contains the k-dimensional (k |V |) embedding for word i and vector 1i ∈",2.1 Distributed word representations,[0],[0]
R|V| is the one-hot representation of word i.,2.1 Distributed word representations,[0],[0]
"The one-hot representation of word i is such a vector, which contains zeros for all of its entries except for index idxi where it stores a one.",2.1 Distributed word representations,[0],[0]
"Depending on how the columns of W (i.e. the word embeddings) get determined, we could distinguish a plethora of approaches (Bengio et al., 2003; Lebret and Collobert, 2014; Mnih and Kavukcuoglu, 2013; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014).
",2.1 Distributed word representations,[0],[0]
"Prediction-based distributed word embedding approaches such as word2vec (Mikolov et al.,
2013a) have been conjectured to have superior performance over count-based word representations (Baroni et al., 2014).",2.1 Distributed word representations,[0],[0]
"However, as Lebret and Collobert (2015), Levy et al. (2015) and Qu et al. (2015) point out count-based distributional models can perform on par with prediction-based distributed word embedding models.",2.1 Distributed word representations,[0],[0]
"Levy et al. (2015) illustrate that the effectiveness of neural word embeddings largely depend on the selection of model hyperparameters and other design choices.
",2.1 Distributed word representations,[0],[0]
"According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we primarily use the publicly available pre-trained polyglot word embeddings of Al-Rfou et al. (2013) instead, without any task specific modification for our experiments.",2.1 Distributed word representations,[0],[0]
A key thing to note is that polyglot word embeddings are not tailored toward any specific language analysis task such as POS tagging or NER.,2.1 Distributed word representations,[0],[0]
These word embeddings are instead trained in a manner favoring the word analogy task introduced by Mikolov et al. (2013c).,2.1 Distributed word representations,[0],[0]
The polyglot project distributes word embeddings for more than 100 languages.,2.1 Distributed word representations,[0],[0]
"AlRfou et al. (2013) also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.
",2.1 Distributed word representations,[0],[0]
There has been previous research on training neural networks for learning distributed word representations for various specific language analysis tasks.,2.1 Distributed word representations,[0],[0]
"Collobert et al. (2011) propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking.",2.1 Distributed word representations,[0],[0]
"Collobert et al. (2011) train word representations on large amounts of unannotated texts from Wikipedia, then update the pretrained word representations for the individual tasks.",2.1 Distributed word representations,[0],[0]
Our approach is different in that we do not update our word representations for the different tasks and most importantly that we use successfully the features derived from sparse coding in a log-linear model instead of a neural network architecture.,2.1 Distributed word representations,[0],[0]
"A final difference to (Collobert et al., 2011) is that we experiment with a much wider range of languages while they report results for English only.
",2.1 Distributed word representations,[0],[0]
"Qu et al. (2015) evaluate the impacts of choosing different embedding methods on four sequence labeling tasks, i.e. POS tagging, NER, syntactic
chunking and multiword expression identification.",2.1 Distributed word representations,[0],[0]
The hand-crafted features they employ for POS tagging and NER are the same as in Collobert et al. (2011) and Turian et al. (2010).,2.1 Distributed word representations,[0],[0]
"The general goal of sparse coding is to express signals in the form of sparse linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem (Mairal et al., 2010).",2.2 Sparse coding,[0],[0]
"Generally, given a data matrix X ∈ Rk×n with its ith column xi representing the ith k-dimensional signal, the task is to findD ∈ Rk×m and α ∈ Rm×n, such thatX ≈ Dα.",2.2 Sparse coding,[0],[0]
"This can be formalized into an `1-regularized linear least-squares minimization problem having the form
min D∈C,α
1
2n
n∑
i=1
( ‖xi −Dαi‖22 + λ‖αi‖1 ) , (1)
with C being the convex set of matrices of column vectors having an `2 norm at most one, matrix D acting as the shared dictionary across the signals, and the columns of the sparse matrix α containing the coefficients for the linear combinations of each of the n observed signals.
",2.2 Sparse coding,[0],[0]
"Performing sparse coding of word embeddings has recently been proposed by Faruqui et al. (2015), however, the objective function they optimize differs from (1).",2.2 Sparse coding,[0],[0]
"In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in (Faruqui et al., 2015).
",2.2 Sparse coding,[0],[0]
"In their work, Yogatama et al. (2015) proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods.",2.2 Sparse coding,[0],[0]
"Most recently, Sun et al. (2016) have proposed an online learning algorithm using regularized dual averaging to directly obtain `1 regularized continuous bag of words (CBOW) representations (Mikolov et al., 2013a) without the need to determine dense CBOW representations first.",2.2 Sparse coding,[0],[0]
This section introduces the sequence labeling framework we use for both POS tagging and NER.,3 Sequence labeling framework,[0],[0]
"Since our goal is to measure the effectiveness of sparse
word embeddings alone, we do not apply any features based on gazetters, capitalization patterns or character suffixes.
",3 Sequence labeling framework,[0],[0]
"As described previously, word embedding methods turn a high-dimensional (i.e., as many dimensions as words in the vocabulary) and extremely sparse (i.e. containing only one non-zero element at the vocabulary index of the word it represents) onehot encoded representation of words into a dense embedding of much lower dimensionality k.
In our work, instead of using the low dimensional dense word embeddings, we use a dictionary learning approach to obtain sparse codings for the embedded word representations.",3 Sequence labeling framework,[0],[0]
"Formally, given the lookup matrix W ∈ Rk×|V | which contains the embedding vectors, we learned D ∈ Rk×m being the dictionary matrix shared across all the embedding vectors and α ∈ Rm×|V | containing sparse linear combination coefficients for each of the word embeddings so that ‖W−Dα‖2F+λ‖α‖1 is minimized.
",3 Sequence labeling framework,[0],[0]
"Once the dictionary matrix D is learned, the sparse linear combination coefficients αi can easily be determined for a word embedding vector wi by solving an `1-regularized linear least-squares minimization problem (Mairal et al., 2010).",3 Sequence labeling framework,[0],[0]
"We define features based on vector αi by taking the signs and indices of its non-zero coefficients, that is
f(wi) = {sign(αi[j])j | αi[j] 6= 0}, (2) where αi[j] denotes the jth coefficient in the sparse vector αi.",3 Sequence labeling framework,[0],[0]
The intuition behind this feature is that words with similar meaning are expected to use an overlapping set of basis vectors from,3 Sequence labeling framework,[0],[0]
"dictionary D. Incorporating the signs of coefficients into the feature function can help to distinguish cases when a basis vector takes part in the reconstruction of a word representation “destructively” or “constructively”.
",3 Sequence labeling framework,[0],[0]
"When assigning features to a target word at some position within a sentence, we determine the same set of feature functions for the target word itself and its neighboring words of window size 1.",3 Sequence labeling framework,[0],[0]
Experiments with window size 2 were also performed.,3 Sequence labeling framework,[0],[0]
"However, we omit these results for brevity as they do not substantially differ from those obtained with a window size of 1.
",3 Sequence labeling framework,[0],[0]
"We then use the previously described set of features in a linear chain CRF (Lafferty et al., 2001)
using CRFsuite (Okazaki, 2007) with its default settings for hyperparameters, i.e., the coefficients of 1.0 and 0.001 for `1 and `2 regularization, respectively.",3 Sequence labeling framework,[0],[0]
We rely on the SPArse Modeling Software1 (SPAMS),4 Experiments,[0],[0]
"(Mairal et al., 2010) for performing sparse coding of distributed word representations.",4 Experiments,[0],[0]
"For dictionary learning as formulated in Equation 1, one should choose m and λ, controlling the number of the basis vectors and the regularization coefficient affecting the sparsity of α, respectively.",4 Experiments,[0],[0]
"Starting with m = 256 and doubling it at each iteration, our preliminary investigations showed a steady growth in the usefulness of sparse word representations as a function of m, plateauing at m = 1024.",4 Experiments,[0],[0]
We set m to that value for further experiments.,4 Experiments,[0],[0]
"Brown clustering Various studies have identified Brown clustering (Brown et al., 1992) as a useful source of feature generation for sequence labeling tasks (Ratinov and Roth, 2009; Turian et al., 2010; Owoputi et al., 2013; Stratos and Collins, 2015; Derczynski et al., 2015).",4.1 Baseline methods,[0],[0]
"We should note that sparse coding can also be viewed as a kind of clustering that – unlike Brown clustering – has the capability of assigning word forms to multiple clusters at a time (corresponding to the non-zero coefficients in α).
",4.1 Baseline methods,[0],[0]
We thus define a linear chain CRF relying on features from the Brown cluster identifier of words as one of our baseline approach.,4.1 Baseline methods,[0],[0]
"Since Brown clustering defines a hierarchical clustering over words, cluster supersets can easily function as features.",4.1 Baseline methods,[0],[0]
"We generate features from length-p (p ∈ {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and Turian et al. (2010).
",4.1 Baseline methods,[0],[0]
In our experiments we use the implementation by Liang (2005) for performing Brown clustering2.,4.1 Baseline methods,[0],[0]
We provide the very same Wikipedia articles as input text for determining Brown clusters that are used for training the polyglot3 word embeddings.,4.1 Baseline methods,[0],[0]
"We
1http://spams-devel.gforge.inria.fr/ 2https://github.com/percyliang/
brown-cluster 3https://sites.google.com/site/rmyeid/ projects/polyglot
also set the number of Brown clusters to be identified to 1024, which is the number of basis vectors applied during sparse coding (cf. D ∈ R64×1024).
",4.1 Baseline methods,[0],[0]
Feature-rich representation We report results relying on linear chain CRFs that assign standard state-of-the-art feature-rich representation to sequences.,4.1 Baseline methods,[0],[0]
We apply the very same features and feature templates included in the POS tagging model of CRFSuite4.,4.1 Baseline methods,[0],[0]
"We summarize these features in Table 1, where ⊕ denotes the binary operator which defines features as a combination of word forms at different (not necessarily contiguous) positions of a sentence.
",4.1 Baseline methods,[0],[0]
We use the same pool of features described in Table 1 for both POS tagging and NER.,4.1 Baseline methods,[0],[0]
"The reason why we do not adjust the feature-rich representation employed as our baseline for the different tasks is that we do not alter our representation in any way when using our sparse coding-based model either.
",4.1 Baseline methods,[0],[0]
"Note that features #1 through #5 in Table 1 operate at character-level, whereas our proposed framework solely uses features derived from the sparse coding of word forms.",4.1 Baseline methods,[0],[0]
"We thus distinguish two feature-rich baselines, i.e. FRw+c including both word and character-level features and FRw treating word forms as atomic units to derive features from.
",4.1 Baseline methods,[0],[0]
"Using dense word representations As our ultimate goal is to demonstrate the usefulness of sparse
4http://github.com/chokkan/crfsuite/ blob/master/example/pos.py
features derived from dense word representations, it is important to address the question of whether sparse word representations are more beneficial for sequence labeling tasks compared to their dense counterparts.",4.1 Baseline methods,[0],[0]
"To this end, we developed a similar model to the one proposed in Section 3, except for using the original dense word representations for inducing features.
",4.1 Baseline methods,[0],[0]
"According to this modification, we made the following change in our feature function: instead of calculating Equation (2) for some word i, the modified feature function we use for this baseline is
f(wi) = {j : wi[j] | ∀j ∈ {1, . . .",4.1 Baseline methods,[0],[0]
", k}}.
",4.1 Baseline methods,[0],[0]
"That is, instead of relying on the nonzero values in αi, each word is characterized by its k real-valued coordinates in the embedding space.",4.1 Baseline methods,[0],[0]
"In order to notationally distinguish sparse and dense representations, we add subscript SC when we refer to a sparse coded version of some word embedding (e.g. SGSC).",4.1 Baseline methods,[0],[0]
"Even though it is reasonable to assume that languages share a common coarse set of linguistic categories, linguistic resources had their own notations for part-of-speech tags.",4.2 POS tagging experiments,[0],[0]
"The first notable attempt to canonize the multiple tag sets was the Google universal part-of-speech tags introduced by Petrov et al. (2012) in which the POS tags of various tagging schemes were mapped to 12 language-independent part-of-speech tags.
",4.2 POS tagging experiments,[0],[0]
"The recent initiative of universal dependencies (UD) (Nivre, 2015) aims to provide a unified notation for multiple linguistic phenomena, including part-of-speech tags as well.",4.2 POS tagging experiments,[0],[0]
The POS tag set proposed for UD has 17 categories which partially overlap with those defined by Petrov et al. (2012).,4.2 POS tagging experiments,[0],[0]
"We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 (Buchholz and Marsi, 2006; Nivre et al., 2007) shared tasks.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"The complete list of the treebanks included in our experiments is presented in Table 2.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"We rely on the official scripts released by Petrov et al. (2012)5 for mapping the treebank specific
5https://github.com/slavpetrov/ universal-pos-tags
POS tags to the Google universal POS tags in order to obtain results comparable across languages.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"For our experiments we used the original CoNLL-X train/test splits of the treebanks.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"A key factor for the efficiency of our proposed model resides in the coverage of word embeddings, i.e. the proportion of tokens/word forms for which distributed representation is determined.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Figure 1 depicts these coverage scores calculated over the merged training and test sets for the different languages.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 1 reveals that a substantial amount of tokens has distributed representation defined for (around 90% for the majority of languages, except for Turkish where it is 5 point less).",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Token coverages of the word embeddings are most likely affected by the morphological richness of the languages and the elaborateness of the corresponding Wikipedia articles used for training word embeddings.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Comparing word embeddings Our motivation for choosing polyglot word embeddings as input to sparse coding is that they are publicly available for a variety of languages.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"However, distributed word representations trained in any other reasonable manner can serve as input to our approach.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"In order to investigate if some of the popular word embedding techniques seem favorable for our algorithm, we conduct experiments using alternatively trained embeddings, i.e. skip-gram (SG), continuous bagof-words (CBOW) and Glove.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"In order that the utility of different word embeddings not to be conflated with other factors, we train them on the same Wikipedia dumps used for training the polyglotword vectors.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"We choose further hyperparameters identically to polyglot, i.e. we
train 64 dimensional dense word representations using a symmetric context window of size 2 for both SG/CBOW6 and Glove7.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Figure 2 includes POS tagging accuracies over the 12 treebanks from the CoNLL 2006/07 shared tasks evaluated against Google Universal POS tags.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Instead of reporting results as a function of λ, we rather present accuracies as a function of the different sparsity levels induced by different λ values.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 2 demonstrates that POS tagging performance is quite insensitive to the choice of λ unless it yields some extreme sparsity level (>99.5%).
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 2 also reveals that the usage of 6https://code.google.com/archive/p/ word2vec/ 7http://nlp.stanford.edu/projects/glove/
polyglotSC word representations tend to produce superior results over all alternative representations we experiment with.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Furthermore, models using polyglotSC consistently outperform the FRw and Brown clustering-based baselines.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Models relying on SGSC and CBOWSC representations have an average tagging accuracy of 93.74 and 93.63, respectively, and they typically perform better than the baseline using Brown clustering with an average tagging performance of 93.27.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Although utilizing Glove embeddings produce the lowest scores (91.92 on average), its scores still surpass those of the FRw baseline for all languages except for Turkish.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
The average tagging performance over the 12 languages when relying on features based on polyglotSC is only 1.3 points below that of FRw+c (i.e. 94.4 versus 95.7).,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Recall that FRw+c uses a feature-rich representation, whereas our proposed model uses only O(m) features, i.e. it is tied to the number of the basis vectors employed for sparse coding.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Furthermore, our model does not employ word identity features, nor does it rely on character-level features of words.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Analyzing the effects of window size Hyperparameters for training word representations can greatly impact their quality as also concluded by Levy et al. (2015).,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their performance in our model.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"According to Figure 3 applying context window sizes of 2 for training the word embeddings tend to
produce better overall POS tagging accuracies than applying a larger window size of 10.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Differences are the most pronounced in case of skip-gram representation, confirming the findings of Lin et al. (2015), i.e. embedding models that model short-range context are more effective for POS tagging.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Comparing dense and sparse representations Unless stated otherwise, we use λ = 0.1 for the experiments below in accordance to Figure 2.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Table 3 demonstrates that performances obtained by models using dense word representations as features are consistently inferior to those models relying on sparse word representations.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"In Table 3b, we can see that polyglot embeddings perform the best for dense representations as well.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"When using dense features, the CBOW representation-based model tends to produce results better than by a 1.4 points margin on average compared to SG embeddings.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"This performance gap between the two word2vec variants vanishes, however, when dense representations are replaced by their sparse counterparts.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Table 3 also reveals that
sparse word representations improve average POS tagging accuracy by 3.3, 5.4, 6.7 and 10.4 points for polylgot, CBOW, SG and Glove word representations, respectively.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Comparing the effects of training corpus size We also investigate the generalization characteristics of the proposed representation by training models that have access to substantially different amounts of training data per language.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"We distinguish three scenarios, i.e. when using only the first 150, the first 1,500 and all the available training sentences from each corpus.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 4 illustrates the average POS tagging accuracy over the 12 CoNLL-X datasets for different amounts of training data and models.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Table 4 further reveals that the average performance of polyglotSC is 14.55 and 3.76 points better compared to the FRw and FRw+c baselines when using only 1.2% of all the available training data, i.e. 150 sentences per language.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
By discarding 98.8% of the training data polyglotSC obtains 89.8% of its average performance compared to the scenario when it has access to all the training sentences.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"However, under the same scenario the FRw+c and FRw models only manage to preserve 85% and 77% of their original performance, respectively.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Our model performs on par with FRw+c and has a
6.85 points advantage over FRw with a training corpus of 1,500 sentences.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"FRw+c has an average of 1.3 points advantage over polyglotSC when we provide access to all training data during training, nevertheless FRw still underperforms polyglotSC in that setting by 3.67 points.
Comparing sparse coding techniques Next, we compare different sparse coding approaches on the
pre-trained polyglot word representations.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
The recent work of Faruqui et al. (2015) formulated alternative approaches to determine sparse word representations.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"One of the objective functions Faruqui et al. (2015) apply is
min D,α
1
2n
n∑
i=1
‖xi−Dαi‖22+λ‖αi‖1+ τ‖D‖22.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"(3)
The main difference in Eq. 1 and 3 is that the latter does not explicitly constrain D to be a member of the convex set of matrices comprising of column vectors having a pre-defined upper bound on their norm.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"In order to implicitly control for the norms of the basis vectors Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter τ in their objective function.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Faruqui et al. (2015) also formulated a constrained objective function of the form
min D∈Rk×m≥0 α∈Rk×|V |≥0
1
2n
n∑
i=1
‖xi−Dαi‖22+λ‖αi‖1+ τ‖D‖22, (4)
for which a non-negativity constraint on the elements of α (but no constraint on D) is imposed.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"When using the objective functions introduced by Faruqui et al. (2015), we use the default τ = 10−5 value.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i ∈ {1, 3, 4}.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"We applied λ = 0.05 for SC-1 and λ = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
the left of Figure 5).,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"The right of Figure 5 further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Although Murphy et al. (2012) mentions nonnegativity as a desired property of word representations for cognitive plausibility, Figure 5 reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.7 points below that of SC-3 approach.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"The average performances when applying SC-1 and SC-3 are nearly identical with a 0.18 point difference between the two.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
It is instructive to analyze the patterns different sparse coding approaches exhibit.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Even though the objective functions used by the different approaches are similar, decompositions obtained by them convey rather different sparsity structures.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Figure 6a illustrates that there exist substantial variation in the length of the basis vectors obtained by SC-3 and SC-4 both within and across languages.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"However, SC-1 produces practically no variation in the length of the basis vectors comprising D due to the constraint present in the objective function it employs.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 6b shows similar differences about the relative frequency of basis vectors taking part in the reconstruction of word embeddings.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Figure 7 shows a strong correlation between the `2 norm of basis vectors and the relative number of times a non-zero coefficient is assigned to them in α for SC-3 and SC-4 but not for SC-1.
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"It can be further noted from Figure 7 that the norm
of the basis vectors determined by SC-3 and SC-4 are often orders of magnitude larger than those determined by SC-1.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"This effect, however, can be naturally mitigated by increasing τ .
",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"Overall, the different approaches convey comparable POS tagging accuracies but different decompositions due to the differences in the objective functions they employ.",4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
Experiments described below are conducted using the objective function in Eq. 1.,4.2.1 Experiments using CoNLL 2006/07 data,[0],[0]
"For POS tagging we also experiment with UD v1.2 (Nivre et al., 2015) treebanks.",4.2.2 Experiments using UD treebanks,[0],[0]
We used the default train-test splits of the treebanks not utilizing the development sets for fine tuning performance on any of the languages during our experiments.,4.2.2 Experiments using UD treebanks,[0],[0]
We omitted the Japanese treebank as words in it are stripped off due to licensing issues.,4.2.2 Experiments using UD treebanks,[0],[0]
Also there is no polyglot vector released for Old Church Slavonic and Gothic.,4.2.2 Experiments using UD treebanks,[0],[0]
"Even though polyglotword representations are released for Arabic, it was of no practical use as it contained unvocalized surface forms of tokens in contrast to the vocalized forms in UD v.1.2.",4.2.2 Experiments using UD treebanks,[0],[0]
"For this reason, we discarded the Arabic treebank as less than 30% of its tokens could be associated with a representation.",4.2.2 Experiments using UD treebanks,[0],[0]
By omitting these 4 languages from our experiments we are finally left with 33 treebanks for 29 languages.,4.2.2 Experiments using UD treebanks,[0],[0]
"We note that for
Ancient Greek treebanks (grc*) we use word embeddings trained on Modern Greek.
",4.2.2 Experiments using UD treebanks,[0],[0]
"We should add that there are 4 languages (related to 6 treebanks) for which polyglot word vectors are accessible, however, the Wikipedia dumps used for training them are not distributed.",4.2.2 Experiments using UD treebanks,[0],[0]
"For this reason, Brown clustering-based baselines are missing for the affected treebanks.
",4.2.2 Experiments using UD treebanks,[0],[0]
We report our results on UD v1.2 in Table 5.,4.2.2 Experiments using UD treebanks,[0],[0]
Recall that the default behavior of our sparse codingbased models (SC in Table 5) is that they do not handle word identity as an explicit feature.,4.2.2 Experiments using UD treebanks,[0],[0]
We now investigate how much contribution word identity features convey on their own and also when used in conjunction with sparse coding-derived features.,4.2.2 Experiments using UD treebanks,[0],[0]
For this end we introduce a simple linear chain CRF model generating features solely on the identity of the current word and the ones surrounding it (WI in Table 5).,4.2.2 Experiments using UD treebanks,[0],[0]
"Likewise, we define a model that relies on WI and SC features simultaneously (WI+SC).",4.2.2 Experiments using UD treebanks,[0],[0]
"Table 5 reveals that SC outperforms WI by a large margin and that combining the two feature sets together yields some further improvements over SC scores.
",4.2.2 Experiments using UD treebanks,[0],[0]
We also present in Table 5 the state-of-the-art results of the bidirectional LSTM models by Plank et al. (2016) for comparative purposes.,4.2.2 Experiments using UD treebanks,[0],[0]
"Note that the authors reported results only on a subset of UD v1.2 (i.e. treebanks with at least 60k tokens), for which reason we can include their results on 21 treebanks.",4.2.2 Experiments using UD treebanks,[0],[0]
"Out of these 21 UD v1.2 treebanks there are 15 and 20 cases, respectively, for which SC and WI+SC produces better results than bi-LSTMw.",4.2.2 Experiments using UD treebanks,[0],[0]
"Only FRw+c and bi-LSTMw+c, models which enjoy the additional benefit of employing character-level features besides word-level ones, are capable of outperforming SC and WI+SC.",4.2.2 Experiments using UD treebanks,[0],[0]
"Besides the POS tagging experiments, we investigated if the very same features as the ones applied for POS tagging can be utilized in a different sequence labeling task, namely named entity recognition.",4.3 Named entity recognition experiments,[0],[0]
"In order to evaluate our approach, we obtained the English, Spanish and Dutch datasets from the 2002 and 2003 CoNLL shared tasks on multilingual Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).
",4.3 Named entity recognition experiments,[0],[0]
"We use the train-test splits provided by the or-
ganizers and report our NER results using the F1 scores based on the official evaluation script of the CoNLL shared task.",4.3 Named entity recognition experiments,[0],[0]
Similar to Collobert et al. (2011) we also apply the 17-tag IOBES tagging scheme during training and inference.,4.3 Named entity recognition experiments,[0],[0]
The best F1 scores reported for English by Collobert et al. (2011) without employing additional unlabeled texts to enhance their language model is 81.47.,4.3 Named entity recognition experiments,[0],[0]
When pre-training their neural language model on large amounts of Wikipedia texts they report an F1 score of 87.58.,4.3 Named entity recognition experiments,[0],[0]
Figure 8 includes our NER results obtained using different word embedding representations as input for sparse coding and different levels of sparsity.,4.3 Named entity recognition experiments,[0],[0]
"Similar to our POS tagging experiments, using polyglotSC vectors tend to perform best for NER as well.",4.3 Named entity recognition experiments,[0],[0]
"However, a substantial difference compared to the POS tagging results is that NER performances
do not degrade even for extreme levels of sparsity.",4.3 Named entity recognition experiments,[0],[0]
"Also, the sparse coding-based models perform much better when compared to the FRw+c baseline.
",4.3 Named entity recognition experiments,[0],[0]
"In Table 6, we compare the effectiveness of models relying on sparse and dense word representations for NER.",4.3 Named entity recognition experiments,[0],[0]
"In order not to fine-tune hyperparameters for a particular experiment, similarly to our previous choices m and λ are set to 1024 and 0.1, respectively.",4.3 Named entity recognition experiments,[0],[0]
Results in Table 6 are in line with those reported in Table 3 for POS tagging.,4.3 Named entity recognition experiments,[0],[0]
In this paper we show that it is possible to train sequence models that perform nearly as well as best existing models on a variety of languages for both POS tagging and NER.,5 Conclusion,[0],[0]
"Our approach does not require word identity features to perform reliably, furthermore, it is capable of achieving comparable results to traditional feature-rich models.",5 Conclusion,[0],[0]
"We also il-
lustrate the advantageous generalization property of our model as it retained 89.8% of its original average POS tagging accuracy when trained on only 1.2% of the total accessible training sentences.
",5 Conclusion,[0],[0]
"As Mikolov et al. (2013b) pointed out the similarities of continuous word embeddings across languages, we think that our proposed model could be employed not in just multi-lingual, but also in crosslingual language analysis settings.",5 Conclusion,[0],[0]
"In fact, we investigate its feasibility in our future work.",5 Conclusion,[0],[0]
"Finally, we have made the sparse coded word embedding vectors publicly available in order to facilitate the reproducibility of our results and to foster multilingual and cross-lingual research.",5 Conclusion,[0],[0]
The author would like to thank the TACL editors and the anonymous reviewers for their valuable feedbacks and suggestions.,Acknowledgement,[0],[0]
In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations.,abstractText,[0],[0]
The proposed model obtains (near) state-of-the art performance for both part-ofspeech tagging and named entity recognition for a variety of languages.,abstractText,[0],[0]
"Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks.",abstractText,[0],[0]
"The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.",abstractText,[0],[0]
Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling,title,[0],[0]
"We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features. We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus. Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models. The addition of skip-gram features yields a model that is in the same league as the stateof-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark. On the Gigaword corpus further improvements are observed using features that cross sentence boundaries. The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.",text,[0],[0]
A statistical language model estimates probability values P (W ) for strings of words W in a vocabulary V whose size can be in the tens or hundreds of thousands and sometimes even millions.,1 Introduction,[0],[0]
"Typically the string W is broken into sentences, or other segments such as utterances in automatic speech recognition, which are often assumed to be conditionally independent; we will assume that W is such a segment, or sentence.
",1 Introduction,[0],[0]
"Estimating full sentence language models (Rosenfeld et al., 2001) is computationally hard if one
seeks a properly normalized probability model1 over strings of words of finite length in V∗. A simple and sufficient way to ensure proper normalization of the model is to decompose the sentence probability according to the chain rule and make sure that the end-of-sentence symbol </S> is predicted with non-zero probability in any context.",1 Introduction,[0],[0]
"With W = wN1 = w1, . . .",1 Introduction,[0],[0]
", wN we get:
P (wN1 ) = N∏
k=1
P (wk|wk−11 ) (1)
Since the parameter space of P (wk|wk−11 ) is too large, the language model is forced to put the context wk−11 into an equivalence class determined by a function Φ(wk−11 ).",1 Introduction,[0],[0]
"As a result,
P (wN1 ) ∼=
N∏
k=1
P (wk|Φ(wk−11 )) (2)
Research in language modeling consists of finding appropriate equivalence classifiers Φ and methods to estimate P (wk|Φ(wk−11 )).",1 Introduction,[0],[0]
"Arguably the most successful paradigm in language modeling uses the n-gram equivalence classification, that is, defines
Φn-gram(w k−1 1 ) .",1 Introduction,[0],[0]
"= wk−n+1, wk−n+2, . . .",1 Introduction,[0],[0]
", wk−1
Once the form Φ(wk−11 ) is specified, only the problem of estimating P (wk|Φ(wk−11 )) from training data remains.
",1 Introduction,[0],[0]
"In order to outperform the n-gram equivalence class, one must find a way to leverage long-distance context.",1 Introduction,[0],[0]
"This can be done explicitly, e.g. by combining multiple arbitrary features (Rosenfeld, 1994), or implicitly as is the case for the current state of the art
1In some practical systems the constraint on using a properly normalized language model is side-stepped at a gain in modeling power and simplicity, see e.g. Chen et al. (1998).
",1 Introduction,[0],[0]
"329
Transactions of the Association for Computational Linguistics, vol. 4, pp. 329–342, 2016.",1 Introduction,[0],[0]
Action Editor: Jason Eisner.,1 Introduction,[0],[0]
"Submission batch: 12/2014; Revision batch: 7/2015; 11/2015; 4/2016; 6/2016; Published 7/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"recurrent neural network language models (Mikolov, 2012).",1 Introduction,[0],[0]
"Unfortunately, either method comes at a large computational cost which makes training and evaluation on a large corpus impractical.
",1 Introduction,[0],[0]
"In this paper we present a novel probability estimation technique, called Sparse Non-negative Matrix (SNM) estimation.",1 Introduction,[0],[0]
"Although SNM estimation is a general approach that can be applied to many problems, its efficient combination of arbitrary features makes it particularly interesting for language modeling.",1 Introduction,[0],[0]
"We demonstrate this by training models with variable-length n-gram features and skip-gram features to incorporate long-distance context.
",1 Introduction,[0],[0]
The paper is organized as follows: Section 2 discusses work that is related to SNM which is described in Section 3.,1 Introduction,[0],[0]
We then present a complexity analysis in Section 4 and experimental results on two English corpora in Sections 5 and 6.,1 Introduction,[0],[0]
We end with conclusions and future work in Section 7.,1 Introduction,[0],[0]
"Recently, neural networks (NN) (Bengio et al., 2003; Emami, 2006; Schwenk, 2007), and in particular recurrent neural networks (RNN) (Mikolov, 2012; Sundermeyer et al., 2012) have shown excellent performance in language modeling (Chelba et al., 2014).",2.1 Neural networks,[0],[0]
"RNNLMs have two main advantages over n-gram language models: 1) they learn a lowdimensional continuous vector representation for words which allows them to discover fine-grained similarities between words; 2) they are capable of modeling dependencies that span over longer distances, i.e. they can extend the context past the ngram window.",2.1 Neural networks,[0],[0]
Their main disadvantage however is that they take a long time to train and evaluate.,2.1 Neural networks,[0],[0]
"Another popular method to leverage long-distance context is Maximum Entropy (ME) (Rosenfeld, 1994).",2.2 Feature-based models,[0],[0]
"ME is interesting because it can mix different types of features extracted from large context windows, e.g. n-gram, skip-gram, bag-of-word and syntactic features.",2.2 Feature-based models,[0],[0]
"Unfortunately it suffers from the same drawback as neural networks, as we will see in Section 2.4.
",2.2 Feature-based models,[0],[0]
"The above-mentioned features can also be used in
other ways, e.g. Chelba and Jelinek (2000) use a leftto-right syntactic parser to identify long-distance dependencies (at sentence level), whereas approaches such as Bellegarda (2000) leverage latent semantic information (at document level).",2.2 Feature-based models,[0],[0]
Tan et al. (2012) integrate both syntactic and topic-based modeling with n-grams in a unified approach.,2.2 Feature-based models,[0],[0]
"The type of long-distance features that we incorporate into our SNMLMs are skip-grams (Huang et al., 1993; Ney et al., 1994; Rosenfeld, 1994), which can effectively capture dependencies across longer contexts.",2.3 Skip-grams,[0],[0]
We are not the first to highlight this effectiveness; previous such results were reported in Singh and Klakow (2013).,2.3 Skip-grams,[0],[0]
"Recently, Pickhardt et al. (2014) also showed that a backoff generalization using single skips yields significant perplexity reductions.",2.3 Skip-grams,[0],[0]
"We note though that our SNMLMs are trained by mixing single as well as longer skips, combining both in one model.",2.3 Skip-grams,[0],[0]
"More fundamentally, the SNM model parameterization and method of estimation are completely original, as far as we know.
",2.3 Skip-grams,[0],[0]
"In our approach, a skip-gram feature extracted from the context wk−11 is characterized by the tuple (r, s, a) where:
• r denotes the number of remote context words • s denotes the number of skipped words • a denotes the number of adjacent context words
relative to the target word wk being predicted.",2.3 Skip-grams,[0],[0]
The window size of a feature extractor then corresponds to r + s + a.,2.3 Skip-grams,[0],[0]
"For example, in the sentence <S",2.3 Skip-grams,[0],[0]
>,2.3 Skip-grams,[0],[0]
The quick brown fox jumps over the lazy dog </S>,2.3 Skip-grams,[0],[0]
"a (1, 2, 3) skip-gram feature for the target word dog is:
[brown skip-2 over the lazy]
For performance reasons, it is recommended to limit s and to limit either (r + a) or both r and s.
We configure the skip-gram feature extractor to produce all features F , defined by the equivalence class Φ(wk−11 ), that meet constraints on the minimum and maximum values for:
• the number of context words r + a • the number of remote words r • the number of adjacent words a • the skip length s
We also allow the option of not including the exact value of s in the feature representation; this may help with smoothing by sharing counts for various skip features.",2.3 Skip-grams,[0],[0]
"The resulting tied skip-gram features will look like:
[curiosity skip-* the cat]
",2.3 Skip-grams,[0],[0]
"In order to build a good probability estimate for the target word wk in a context wk−11 we need a way of combining an arbitrary number of skip-gram features, which do not fall into a simple hierarchy like regular n-gram features.",2.3 Skip-grams,[0],[0]
"The standard way to combine such predictors is ME, but it is computationally hard.",2.3 Skip-grams,[0],[0]
"The proposed SNM estimation on the other hand is capable of combining such predictors in a way that is computationally easy, scales up gracefully to large amounts of data and as it turns out is also very effective from a modeling point of view.",2.3 Skip-grams,[0],[0]
"Neural networks and ME are related in the sense that for both models P (wk|Φ(wk−11 )) takes the following form:
P (wk|Φ(wk−11 ))",2.4 Log-linear models,[0],[0]
"= exp(ŷwk)∑
t′∈V exp(ŷt′)
(3)
where the ŷt′ are the unnormalized log-probabilities for each potential target word t′ and depend on the model in question.",2.4 Log-linear models,[0],[0]
"For a ME model with features F , they can be represented as follows:
ŷ =",2.4 Log-linear models,[0],[0]
"xTM (4)
where x is the word feature activation vector and M is a |F|×|V| feature weight matrix.",2.4 Log-linear models,[0],[0]
"The ŷi of neural networks on the other hand are computed as follows:
ŷ = g(xTH)W (5)
where g(·) is the activation function of the hidden layer (typically a tanh or sigmoid) and W and H are weight matrices for the output and hidden layer respectively.",2.4 Log-linear models,[0],[0]
"Feed-forward and recurrent neural networks differ only in their input vectors x: in a feedforward neural network, x is a concatenation of the input features whereas in a recurrent neural network, x is a concatenation of the input word with the previous hidden state.",2.4 Log-linear models,[0],[0]
"Because of their shared loglinearity, training and evaluating these models becomes computationally complex.
",2.4 Log-linear models,[0],[0]
"Although log-linear models have been shown to perform better than linear models (Klakow, 1998), their performance is also hampered by their complexity and we will show in the rest of the paper that a linear model can in fact compete with the state of the art when trained with variable-length n-gram and skip-gram features combined.",2.4 Log-linear models,[0],[0]
"Contrary to neural networks and ME, SNM language models do not estimate P (wk|Φ(wk−11 )) in a loglinear fashion, but are in fact linear models:
P (wk|Φ(wk−11 ))",3.1 Linear model,[0],[0]
"= ŷwk∑
t′∈V ŷt′
(6)
where ŷ is defined as in Eq.",3.1 Linear model,[0],[0]
(4).,3.1 Linear model,[0],[0]
"Like ME however, SNM uses features F that are predefined and arbitrary, e.g. n-grams, skip-grams, bags of words, syntactic features, ...",3.1 Linear model,[0],[0]
"The features are extracted from the left context of wk and stored in a feature activation vector x = Φ(wk−11 ), which is binary-valued, i.e. xf represents the presence or absence of the feature with index f .
",3.1 Linear model,[0],[0]
"In what follows, we represent the target word wk by a vector y, which is a one-hot encoding of the vocabulary V: yt = 1 for t = wk, yt = 0 otherwise.",3.1 Linear model,[0],[0]
"To further simplify notation, we will not make the distinction between a feature or target and its index, but rather denote both of them by f and t, respectively.
",3.1 Linear model,[0],[0]
"The ŷt′ in SNM are computed in the same way as ME, using Eq.",3.1 Linear model,[0],[0]
"(4), where M is a |F| × |V| feature weight matrix, which is sparse and non-negative.",3.1 Linear model,[0],[0]
Mft is indexed by feature f and target t and denotes the influence of feature f in the prediction of t. Plugging Eq.,3.1 Linear model,[0],[0]
"(4) into Eq. (6), we can derive the complete form of the conditional distribution P (y|x)",3.1 Linear model,[0],[0]
= P (wk|Φ(wk−11 )),3.1 Linear model,[0],[0]
"in SNMLMs:
P (y|x) = (x TM)wk∑
t′∈V(x TM)t′
= ∑ f ′∈F xf ′Mf ′wk∑
t′∈V ∑ f ′∈F xf ′Mf ′t′
= ∑ f ′∈F xf ′Mf ′wk∑
f ′∈F xf ′ ∑ t′∈VMf ′t′ (7)
",3.1 Linear model,[0],[0]
As required by the denominator in Eq.,3.1 Linear model,[0],[0]
"(7), this computation also involves summing over all the present features for the entire vocabulary.",3.1 Linear model,[0],[0]
"However, because of the linearity of the model, we can precompute the row sums ∑ t′∈VMf ′t′ for each f
′ and store them together with the model.",3.1 Linear model,[0],[0]
"This means that the evaluation can be done very efficiently, since the remaining summation involves a limited number of terms: even though the amount of features |F| gathered over the entire training data is potentially huge, the amount of active, non-zero features for a given x is small.",3.1 Linear model,[0],[0]
"For example, for SNM models using variable-length n-gram features, the maximum number of active features is n; in our experiments with a large variety of skip-grams, it was around 100.
",3.1 Linear model,[0],[0]
"Notice that this precomputation is not possible for the log-linear ME which is otherwise similar, because the sum over all features does not distribute outside the sum over all targets in the denominator:
P (y|x) = exp( ∑ f ′∈F xf ′Mf ′wk)∑
t′∈V exp(
∑
f ′∈F xf ′Mf ′t′)
(8)
This is a huge difference and essentially makes SNM a more efficient model at runtime.",3.1 Linear model,[0],[0]
"We let the entries of M be a slightly modified or adjusted version of the relative frequencies:
Mft = e A(f,t)Cft
Cf∗ (9)
where A(f, t) is a real-valued function, dubbed the adjustment function (to be defined below), and C is a feature-target count matrix, computed over the entire training corpus T .",3.2 Adjustment function and meta-features,[0],[0]
"Cft denotes the cooccurrence count of feature f and target t, whereas Cf∗ denotes the total occurrence count of feature f , summed over all targets t′.
An unadjusted SNM model, where A(f, t) = 0, is a linear mixture of simple feature models P (t|f) with uniform mixture weights.",3.2 Adjustment function and meta-features,[0],[0]
"The adjustment function enables the models to be weighted by the relative importance of each input feature and, because it also parameterized by t, takes into account the current target.",3.2 Adjustment function and meta-features,[0],[0]
"The function is computed by a linear model on binary meta-features (Lee et al., 2007):
A(f, t) = θ · h(f, t) (10)
where h(f, t) is the meta-feature vector extracted from the feature-target pair (f, t).
",3.2 Adjustment function and meta-features,[0],[0]
Estimating weights θk on the meta-feature level rather than the input feature level enables similar input features to share weights which improves generalization.,3.2 Adjustment function and meta-features,[0],[0]
"We illustrate this by an example.
",3.2 Adjustment function and meta-features,[0],[0]
"Given the word sequence the quick brown fox, we extract the following elementary metafeatures from the 3-gram feature the quick brown and the target fox:
• feature identity: [the quick brown] • feature type: 3-gram • feature count: Cf∗",3.2 Adjustment function and meta-features,[0],[0]
• target identity: fox • feature-target count:,3.2 Adjustment function and meta-features,[0],[0]
Cft We also allow conjunctions of (single or multiple) elementary meta-features to form more complex meta-features.,3.2 Adjustment function and meta-features,[0],[0]
"This explains the absence of the feature-target identity (and others, see Appendix A) in the above list: it is represented by the conjunction of the feature and target identities.",3.2 Adjustment function and meta-features,[0],[0]
"The resulting meta-features enable the model to share weights between, e.g. all 3-grams, all 3-grams that have target fox, etc.",3.2 Adjustment function and meta-features,[0],[0]
"Although these conjunctions may in theory override Cft/Cf∗ in Eq. (9), keeping the relative frequencies allows us to train the adjustment function on part of the data (see also Section 3.4).
",3.2 Adjustment function and meta-features,[0],[0]
"We apply smoothing to all of the count metafeatures: since count meta-features of the same order of magnitude carry similar information, we group them so they can share weights.",3.2 Adjustment function and meta-features,[0],[0]
We do this by bucketing the count meta-features according to their (floored) log2 value.,3.2 Adjustment function and meta-features,[0],[0]
"As this effectively puts the lowest count values, of which there are many, into a different bucket, we optionally introduce a second (ceilinged) bucket to assure smoother transitions.",3.2 Adjustment function and meta-features,[0],[0]
Both buckets are then weighted according to the log2 fraction lost by the corresponding rounding operation.,3.2 Adjustment function and meta-features,[0],[0]
"Pseudocode for meta-feature extraction and count bucketing is presented in Appendix A.
To control memory usage, we employ a feature hashing technique (Langford et al., 2007; Ganchev and Dredze, 2008) where we store the meta-feature weights in a flat hash table θ of predefined size.",3.2 Adjustment function and meta-features,[0],[0]
"Strings are fingerprinted (converted into a byte array, then hashed), counts are hashed, and the resulting integer is mapped to an index in θ by taking its
value modulo the pre-defined size(θ).",3.2 Adjustment function and meta-features,[0],[0]
"We do not prevent collisions, which has the potentially undesirable effect of tying together the weights of different meta-features.",3.2 Adjustment function and meta-features,[0],[0]
"However, as was previously observed by Mikolov et al. (2011), when this happens the most frequent meta-feature will dominate the final value after training, which essentially boils down to a form of pruning.",3.2 Adjustment function and meta-features,[0],[0]
"Because of this, the model performance does not strongly depend on the size of the hash table.",3.2 Adjustment function and meta-features,[0],[0]
Note that we only apply hashing to the meta-feature weights: the adjusted and raw relative frequencies are stored as SSTables (Sorted String Table).,3.2 Adjustment function and meta-features,[0],[0]
"Although it is in principle possible to use regularized maximum likelihood to estimate the parameters of the model, a gradient-based approach would end up with parameter updates involving the gradient of the log of Eq.",3.3 Model estimation,[0],[0]
"(7) which works out to:
∂ logP (y|x) ∂A(f, t) = xfMft",3.3 Model estimation,[0],[0]
"( yt ŷwk − 1∑
t′∈V ŷt′
) (11)
",3.3 Model estimation,[0],[0]
"For the complete derivation, see Appendix B. The problem with this gradient is that we need to sum over the entire vocabulary V in the denominator.",3.3 Model estimation,[0],[0]
"In Eq. (7) we could get away with this by precomputing the row sums, but here the sums change after each update.",3.3 Model estimation,[0],[0]
"Instead, we were inspired by Xu et al. (2011) and chose to use an independent binary predictor for each word in the vocabulary during estimation.",3.3 Model estimation,[0],[0]
"Our approach however differs from Xu et al. (2011) in that we do not use |V| Bernoullis, but |V| Poissons2, using the fact that for a large number of trials a Bernoulli with small p is well approximated by a Poisson with small λ.
",3.3 Model estimation,[0],[0]
"If we consider each yt′ in y to be Poisson distributed with parameter ŷt′ , the conditional probability PPois(y|x) is given by:
PPois(y|x) = ∏
t′∈V
ŷ yt′ t′",3.3 Model estimation,[0],[0]
e −ŷt′ yt′ !,3.3 Model estimation,[0],[0]
=,3.3 Model estimation,[0],[0]
∏,3.3 Model estimation,[0],[0]
t′∈V ŷ yt′ t′,3.3 Model estimation,[0],[0]
"e −ŷt′ (12)
2We originally chose Poisson so we could apply the model to tasks with outputs",3.3 Model estimation,[0],[0]
yt > 1.,3.3 Model estimation,[0],[0]
"More recent experiments using a multinomial loss can be found in Chelba and Pereira (2016).
and the gradient of the log-probability works out to:
∂ logPPois(y|x) ∂A(f, t) =",3.3 Model estimation,[0],[0]
xfMft,3.3 Model estimation,[0],[0]
"( yt ŷwk − 1 )
(13)
",3.3 Model estimation,[0],[0]
"For the complete derivation, see Appendix C. The parameters θ of the adjustment function are learned by maximizing the Poisson log-probability, using stochastic gradient ascent.",3.3 Model estimation,[0],[0]
"That is, for each feature-target pair (f, t) we compute the gradient in Eq.",3.3 Model estimation,[0],[0]
"(13) and propagate it to the meta-feature weights θk by multiplying it with ∂A(f, t)/∂θk = hk.",3.3 Model estimation,[0],[0]
"At the N th occurrence of feature-target pair (f, t), each weight θk is updated using the propagated gradient, weighted by a learning rate η:
θk,N ← θk,N−1 + η∂N",3.3 Model estimation,[0],[0]
"(f, t)",3.3 Model estimation,[0],[0]
"(14)
where ∂N",3.3 Model estimation,[0],[0]
"(f, t) is a short-hand notation for the N th gradient with respect to θk.",3.3 Model estimation,[0],[0]
"Rather than using a single fixed learning rate, we use AdaGrad (Duchi, 2011) which uses a separate adaptive learning rate ηk,N for each weight θk,N :
ηk,N = γ√
∆0 + ∑N n=1 ∂n(f, t) 2
(15)
where γ is a constant scaling factor for all learning rates and ∆0 is an initial accumulator constant.",3.3 Model estimation,[0],[0]
Basing the learning rate on historical information tempers the effect of frequently occurring features which keeps the weights small and as such acts as a form of regularization.,3.3 Model estimation,[0],[0]
"Each feature-target pair (f, t) constitutes a training example where examples with yt = 0 are called negative and others positive.",3.4 Optimization and leave-one-out training,[0],[0]
"Using the short-hand notations T = |T |, F = |F| and V = |V|, this means that the training data consists of approximately TF (V − 1) negative and only TF positive training examples.",3.4 Optimization and leave-one-out training,[0],[0]
If we examine the two terms of Eq.,3.4 Optimization and leave-one-out training,[0],[0]
"(13) separately, we see that the first term xfMft
yt ŷwk depends on yt which means it becomes zero for all the negative training examples.",3.4 Optimization and leave-one-out training,[0],[0]
The second term −xfMft however does not depend on yt and therefore never becomes zero.,3.4 Optimization and leave-one-out training,[0],[0]
"This also means that the total gradient is never zero and because of
this, the vast amount of updates required for the negative examples makes the update algorithm computationally too expensive.
To speed up the algorithm we use a heuristic that allows us to express the second term as a function of yt, essentially redistributing the updates for the numerous negative examples to the fewer positive training examples.",3.4 Optimization and leave-one-out training,[0],[0]
Appendix D shows that for batch training this has the same effect if run over the entire corpus.,3.4 Optimization and leave-one-out training,[0],[0]
"We note that for online training this is not strictly correct, sinceMft changes after each update.",3.4 Optimization and leave-one-out training,[0],[0]
"Nonetheless, we found this to yield good results as well as seriously reducing the computational cost.",3.4 Optimization and leave-one-out training,[0],[0]
"After applying the redistribution, the online gradient that is applied to each training example becomes:
∂ logPPois(y|x) ∂A(f, t) =",3.4 Optimization and leave-one-out training,[0],[0]
"xfytMft
( 1
ŷwk − Cf∗",3.4 Optimization and leave-one-out training,[0],[0]
"Cft
) (16)
which is non-zero only for positive training examples, hence making training independent of the size of the vocabulary.
",3.4 Optimization and leave-one-out training,[0],[0]
"One practical way to further prevent overfitting and adapt the model to a specific task is to use heldout data, i.e. compute the count matrix C on the training data and estimate the parameters θ on the held-out data.",3.4 Optimization and leave-one-out training,[0],[0]
"Unfortunately, since the aggregated gradients in Eq.",3.4 Optimization and leave-one-out training,[0],[0]
"(16) tie the updates to the counts Cf∗ and Cft in the training data, they can’t differentiate between held-out and training data, which means that the meta-feature weights can’t be tuned specifically to the held-out data.",3.4 Optimization and leave-one-out training,[0],[0]
"Experiments in which we tried to use the held-out counts instead did not yield good results, presumably because we are violating the redistribution heuristic.
",3.4 Optimization and leave-one-out training,[0],[0]
"Rather than adding a regularizer on the metafeature weights, we instead opted for leave-one-out training.",3.4 Optimization and leave-one-out training,[0],[0]
"With the notation A(f, t, Cf∗, Cft) reflecting the dependence of the adjustment function on feature and feature-target counts, the gradient under leave-one-out training becomes:
xfyt
( ( 1
ŷ+wk",3.4 Optimization and leave-one-out training,[0],[0]
− 1)M+ft − Cf∗,3.4 Optimization and leave-one-out training,[0],[0]
"− Cft Cft M−ft
) (17)
where M−ft, M + ft and ŷ + wk are defined as follows:
M−ft = e A(f,t,Cf∗−1,Cft) Cft
Cf∗",3.4 Optimization and leave-one-out training,[0],[0]
"− 1
M+ft = e A(f,t,Cf∗−1,Cft−1)Cft − 1
Cf∗",3.4 Optimization and leave-one-out training,[0],[0]
"− 1 ŷ+wk = (x TM+)wk
The full derivation can be found in Appendix E. We note that in practice, it often suffices to use only a subset of the training examples for leave-oneout training, which has the additional advantage of speeding up training even further.",3.4 Optimization and leave-one-out training,[0],[0]
"Besides their excellent results, RNNs have also been shown to scale well with large amounts of data with regards to memory and accuracy (Williams et al., 2015).",4 Complexity analysis,[0],[0]
"Compared to n-gram models which grow huge very quickly with only modest improvements, RNNs take up but a fraction of the memory and exhibit a near linear reduction in log perplexity with log training words.",4 Complexity analysis,[0],[0]
"Moreover, a larger hidden layer can yield more improvements, whereas ngram models quickly suffer from data sparsity.",4 Complexity analysis,[0],[0]
The problem with RNNs however is that they are computationally complex which makes training and evaluation slow.,4 Complexity analysis,[0],[0]
"A standard Elman network (Elman, 1990) with hidden layer of size H trained on a corpus of size T with vocabulary of size V has complexity
IT (H2 +HV ) (18)
where I indicates the number of iterations.",4 Complexity analysis,[0],[0]
"Several attempts have been made to reduce training time, focusing mostly on reducing the large factors T or V :
• vocabulary shortlisting (Schwenk and Gauvain, 2004)",4 Complexity analysis,[0],[0]
"• subsampling (Schwenk and Gauvain, 2005; Xu
et al., 2011) • class-based (Goodman, 2001b; Morin and Ben-
gio, 2005; Mikolov et al., 2011) • noise-contrastive estimation (Gutmann and
Hyvärinen, 2012; Chen et al., 2015)
",4 Complexity analysis,[0],[0]
"However, these techniques either come with a serious performance degradation (Le et al., 2013) or
do not sufficiently speed up training.",4 Complexity analysis,[0],[0]
"The classbased implementation for example, still has a training computational complexity of:
IT (H2 +HC + CVC) (19)
where C indicates the number of classes and VC the variable amount of words in a class.",4 Complexity analysis,[0],[0]
"Although this is a significant reduction in complexity, the dominant term ITH2 is still large.",4 Complexity analysis,[0],[0]
"The same applies to noisecontrastive estimation.
",4 Complexity analysis,[0],[0]
"As was shown in Mikolov et al. (2011), a Maximum Entropy model can be regarded as a neural network with direct connections for the features, i.e. it has no hidden layers.",4 Complexity analysis,[0],[0]
"The model uses the same softmax activation at its output and its complexity therefore also depends on the size of the vocabulary:
IT (F+V ) (20)
where F+ F denotes the number of active features.",4 Complexity analysis,[0],[0]
"To achieve state-of-the-art results this model is often combined with an RNN, which yields a total complexity of:
IT (H2 +HV + F+V ) (21)
",4 Complexity analysis,[0],[0]
"The computational complexity for training SNM models on the other hand is independent of V :
TF+ +",4 Complexity analysis,[0],[0]
"IT ′F+Θ+ (22)
where Θ+ is the number of meta-features for each of the F+ input features.",4 Complexity analysis,[0],[0]
The first term is related to counting features and feature-target pairs and the second term to training the adjustment model on a subset T ′ of the training data.,4 Complexity analysis,[0],[0]
"If we compare an SNMLM with typical values of F+ ≈ 100 and Θ+ < 40, to the RNNLM configurations with H = 1024 in Chelba et al. (2014) and Williams et al. (2015), we find that training comes at a reduced complexity of at least two orders of magnitude.
",4 Complexity analysis,[0],[0]
A even more striking difference in complexity can be seen at test time.,4 Complexity analysis,[0],[0]
"Whereas the complexity of a class-based RNN for a single test step is proportional toH2+HC+CVC , testing SNMLMs is linear in F+ because of the reasons outlined in Section 3.1.",4 Complexity analysis,[0],[0]
Our first experimental setup used the One Billion Word Benchmark3 made available by Chelba et al. (2014).,5 Experiment 1: 1B Word Benchmark,[0],[0]
"It consists of an English training and test set of about 0.8 billion and 159658 tokens, respectively.",5 Experiment 1: 1B Word Benchmark,[0],[0]
The vocabulary contains 793471 words and was constructed by discarding all words with count below 3.,5 Experiment 1: 1B Word Benchmark,[0],[0]
OOV words are mapped to an <UNK> token which is also part of the vocabulary.,5 Experiment 1: 1B Word Benchmark,[0],[0]
The OOV rate of the test set is 0.28%.,5 Experiment 1: 1B Word Benchmark,[0],[0]
"Sentence order is randomized.
",5 Experiment 1: 1B Word Benchmark,[0],[0]
All of the described SNM models are initialized with meta-feature weights θk = 0 which are updated using AdaGrad with accumulator ∆0 = 1 and scaling factor γ = 0.02 over a single epoch of 30M training examples.,5 Experiment 1: 1B Word Benchmark,[0],[0]
The hash table for the metafeatures was limited to 200M entries as increasing it yielded no significant improvements.,5 Experiment 1: 1B Word Benchmark,[0],[0]
"In the first set of experiments, we used all variablelength n-gram features that appeared at least once in the training data up to a given length.",5.1 N-gram experiments,[0],[0]
This yields at most n active features: one for each m-gram of length 0 ≤ m < n where m = 0 corresponds to an empty feature which is always present and produces the unigram distribution.,5.1 N-gram experiments,[0],[0]
"The number of features is smaller than n when the context is shorter than n−1 words (near sentence boundaries) and during evaluation where an n-gram that did not occur in the training data is discarded.
",5.1 N-gram experiments,[0],[0]
"When trained using these features, SNMLMs come very close to n-gram models with interpolated Kneser-Ney (KN) smoothing (Kneser and Ney, 1995), where no count cut-off was applied and the discount does not change with the order of the model.",5.1 N-gram experiments,[0],[0]
"Table 1 shows that Katz smoothing (Katz, 1987) performs considerably worse than both SNM and KN.",5.1 N-gram experiments,[0],[0]
KN and SNM are not very complementary as linear interpolation with weights optimized on the test data only yields an additional perplexity reduction of about 1%.,5.1 N-gram experiments,[0],[0]
"The difference between KN and SNM becomes smaller when we increase the size of the context, going from 5% for 5-grams to 3% for 8-grams, which indicates that SNMLMs might be better suited to a large number of features.
",5.1 N-gram experiments,[0],[0]
3http://www.statmt.org/lm-benchmark,5.1 N-gram experiments,[0],[0]
"To incorporate skip-gram features, we can either build a ‘pure’ skip-gram SNMLM that contains no regular n-gram features (except for unigrams) and interpolate this model with KN, or we can build a single SNMLM that has both the regular n-gram features and the skip-gram features.",5.2 Integrating skip-gram features,[0],[0]
"We compared the two approaches by choosing skip-gram features that can be considered the skip-equivalent of 5-grams, i.e. they contain at most 4 context words.",5.2 Integrating skip-gram features,[0],[0]
"In particular, we configured the following feature extractors:
• 1 ≤ r ≤ 3; 1 ≤ s ≤ 3; 1 ≤ r + a ≤ 4 • 1 ≤ r ≤ 2; s ≥ 4 (tied); 1 ≤ r + a ≤ 4
We then built a model that uses both these features and regular 5-grams (SNM5-skip), as well as one that only uses the skip-gram features (SNM5-skip (no n-grams)).",5.2 Integrating skip-gram features,[0],[0]
"In addition, both models were interpolated with a KN 5-gram model (KN5).
",5.2 Integrating skip-gram features,[0],[0]
"As can be seen from Table 2, it is better to incorporate all features into one single SNM model than to interpolate with a KN 5-gram model (KN5).",5.2 Integrating skip-gram features,[0],[0]
"This is not surprising as linear interpolation uses a fixed weight for the evaluation of every word sequence, whereas the SNM model applies a variable weight that is dependent both on the context and the target
word.",5.2 Integrating skip-gram features,[0],[0]
"Finally, interpolating the all-in-one SNM5skip with KN5 yields almost no additional gain.",5.2 Integrating skip-gram features,[0],[0]
"The best SNMLM results so far (SNM10-skip) were achieved using 10-grams, together with skip-grams defined by the following feature extractors:
• s = 1; 1 ≤ r + a ≤ 5 • r = 1; 1 ≤ s ≤ 10 (tied); 1 ≤ r + a ≤ 4",5.3 Skip-gram experiments,[0],[0]
This mixture of rich (large context) short-distance and shallow long-distance features enables the model to achieve state-of-the-art results.,5.3 Skip-gram experiments,[0],[0]
"Table 3 compares its perplexity to KN5 as well as to the following language models:
•",5.3 Skip-gram experiments,[0],[0]
"Stupid Backoff LM (SBO) (Brants et al., 2007) •",5.3 Skip-gram experiments,[0],[0]
"Hierarchical Softmax Maximum Entropy LM
(HSME) (Goodman, 2001b; Morin and Bengio, 2005) •",5.3 Skip-gram experiments,[0],[0]
"Recurrent Neural Network LM with Maximum
Entropy (RNNME) (Mikolov, 2012)
",5.3 Skip-gram experiments,[0],[0]
Describing these models however is beyond the scope of this paper.,5.3 Skip-gram experiments,[0],[0]
Instead we refer the reader to Chelba et al. (2014) for a detailed description.,5.3 Skip-gram experiments,[0],[0]
"The table also lists the number of model parameters, which in the case of SNMLMs consist of the non-zero entries and precomputed row sums of M.
When we compare the perplexity of SNM10-skip with the state-of-the-art RNNLM with 1024 hidden neurons (RNNME-1024), the difference is only 3%.",5.3 Skip-gram experiments,[0],[0]
"Moreover, this small advantage comes at the cost of increased training and evaluation complexity.",5.3 Skip-gram experiments,[0],[0]
"Interestingly, when we interpolate the two models, we have an additional gain of 20%, which shows that SNM10-skip and RNNME-1024 are also complementary.",5.3 Skip-gram experiments,[0],[0]
"As far as we know, the resulting perplexity of 41.3 is already the best ever reported on this corpus, beating the optimized combination of several models, reported in Chelba et al. (2014) by 6%.",5.3 Skip-gram experiments,[0],[0]
"Finally, interpolation over all models shows that the contribution of other models as well as the additional perplexity reduction of 0.3 is negligible.",5.3 Skip-gram experiments,[0],[0]
"In this Section we present actual runtimes to give some idea of how the theoretical complexity analysis of Section 4 translates to a practical application.
",5.4 Runtime experiments,[0],[0]
"More specifically, we compare the training runtime (in machine hours) of the best SNM model to the best RNN and n-gram models:
• KN5: 28 machine hours • SNM5: 115 machine hours • SNM10-skip: 487 machine hours •",5.4 Runtime experiments,[0],[0]
"RNNME-1024: 5760 machine hours
As these models were trained using different architectures (number of CPUs, type of distributed computing, etc.), a runtime comparison is inherently hard and we would therefore like to stress that these numbers should be taken with a grain of salt.",5.4 Runtime experiments,[0],[0]
"However, based on the order of magnitude we can clearly conclude that SNM’s reduced training complexity shown in Section 4 translates to a substantial reduction in training time compared to RNNs.",5.4 Runtime experiments,[0],[0]
"Moreover, the large difference between KN5 and SNM5 suggests that our vanilla implementation can be further improved to achieve even larger speed-ups.",5.4 Runtime experiments,[0],[0]
"In addition to the experiments on the One Billion Word Benchmark, we also conducted experiments on a small subset of the LDC English Gigaword corpus.",6 Experiment 2: 44M Word Corpus,[0],[0]
"This has the advantage that the experiments are more easily reproducible and, since this corpus preserves the original sentence order, it also allows us to investigate SNM’s capabilities of modeling phenomena that cross sentence boundaries.
",6 Experiment 2: 44M Word Corpus,[0],[0]
"The corpus is the one used in Tan et al. (2012), which we acquired with the help of the authors and is now available at http://www.esat. kuleuven.be/psi/spraak/downloads/4.",6 Experiment 2: 44M Word Corpus,[0],[0]
"It consists of a training set of 44M tokens, a check set of 1.7M tokens and a test set of 13.7M tokens.",6 Experiment 2: 44M Word Corpus,[0],[0]
"The vocabulary contains 56k words which corresponds to an OOV rate of 0.89% and 1.98% for the check and test set, respectively.",6 Experiment 2: 44M Word Corpus,[0],[0]
OOV words are mapped to an <UNK> token.,6 Experiment 2: 44M Word Corpus,[0],[0]
"The large difference in OOV rate between the check and test set is explained by the fact that the training data and check data are from the same source (Agence France-Presse), whereas the test data is drawn from CNA (Central News Agency of Taiwan) which seems to be out of domain relative to the training data.",6 Experiment 2: 44M Word Corpus,[0],[0]
"This discrepancy also shows in the perplexity results, presented in Table 4.
",6 Experiment 2: 44M Word Corpus,[0],[0]
All of the described SNM models are initialized with meta-feature weights θk = 0 which are updated using AdaGrad with accumulator ∆0 = 1 and scaling factor γ = 0.02 over a single epoch of 10M training examples.,6 Experiment 2: 44M Word Corpus,[0],[0]
"The hash table for the metafeatures was limited to 10M entries as increasing it yielded no significant improvements.
",6 Experiment 2: 44M Word Corpus,[0],[0]
"With regards to n-gram modeling, the results are analogous to the 1B word experiment: SNM5 is close to KN5; both outperform Katz5 by a large mar-
4In order to comply with the LDC license, the data was encrypted using a key derived from the original data.
gin.",6 Experiment 2: 44M Word Corpus,[0],[0]
This is the case for the check set and the test set.,6 Experiment 2: 44M Word Corpus,[0],[0]
"Tan et al. (2012) showed that by crossing sentence boundaries, perplexities can be drastically reduced.",6 Experiment 2: 44M Word Corpus,[0],[0]
"Although they did not publish any results on the check set, their mixture of n-gram, syntactic language models and topic models achieved a perplexity of 176 on the test set, a 23% relative reduction compared to KN5.",6 Experiment 2: 44M Word Corpus,[0],[0]
"A similar observation was made for the SNM models by adding a feature extractor (r, s, a) analogous to regular skip-grams, but with s now denoting the number of skipped sentence boundaries </S> instead of words.",6 Experiment 2: 44M Word Corpus,[0],[0]
Adding skip-</S> features with r +,6 Experiment 2: 44M Word Corpus,[0],[0]
"a = 4, 1 ≤ r ≤ 2 and 1 ≤ s ≤ 10, yielded an even larger reduction of 26% than the one reported by Tan et al. (2012).",6 Experiment 2: 44M Word Corpus,[0],[0]
"On the check set we observed a 25% reduction.
",6 Experiment 2: 44M Word Corpus,[0],[0]
The RNNME results are achieved with a setup similar to the one in Chelba et al. (2014).,6 Experiment 2: 44M Word Corpus,[0],[0]
The main differences are related to the ME features (3-grams only instead of 10-grams and bag-of-words features) and the number of iterations over the training data (20 epochs instead of 10).,6 Experiment 2: 44M Word Corpus,[0],[0]
These choices are related to the size of the training data.,6 Experiment 2: 44M Word Corpus,[0],[0]
It can be seen from Table 4 that the best RNNME model outperforms the best SNM model by 13% on the check set.,6 Experiment 2: 44M Word Corpus,[0],[0]
"The outof-domain test set shows that due to its compactness, RNNME is better suited for LM adaptation.",6 Experiment 2: 44M Word Corpus,[0],[0]
"We have presented SNM, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features.",7 Conclusions and Future Work,[0],[0]
A first set of empirical evaluations on two data sets shows that SNM n-gram LMs perform almost as well as the well-established KN models.,7 Conclusions and Future Work,[0],[0]
"When we add skipgram features, the models are able to match the state-of-the-art RNNLMs on the One Billion Word Benchmark (Chelba et al., 2014).",7 Conclusions and Future Work,[0],[0]
"Combining the two modeling techniques yields the best known result on the benchmark which shows that the two models are complementary.
",7 Conclusions and Future Work,[0],[0]
"On a smaller subset of the LDC English Gigaword corpus, SNMLMs are able to exploit cross-sentence dependencies and outperform a mixture of n-gram models, syntactic language models and topic models.",7 Conclusions and Future Work,[0],[0]
"Although RNNLMs still outperform SNM by 13% on this corpus, a complexity analysis and mea-
sured runtimes show that the RNN comes at an increased training and evaluation time.
",7 Conclusions and Future Work,[0],[0]
"We conclude that the computational advantages of SNMLMs over both Maximum Entropy and RNN estimation promise an approach that has large flexibility in combining arbitrary features effectively and yet scales gracefully to large amounts of data.
",7 Conclusions and Future Work,[0],[0]
"Future work includes exploring richer features similar to Goodman (2001a), as well as richer metafeatures in the adjustment model.",7 Conclusions and Future Work,[0],[0]
A comparison of SNM models with Maximum Entropy at feature parity is also planned.,7 Conclusions and Future Work,[0],[0]
One additional idea was pointed out to us by action editor Jason Eisner.,7 Conclusions and Future Work,[0],[0]
"Rather than using one-hot target vectors which emphasizes fit, it is possible to use low-dimensional word embeddings.",7 Conclusions and Future Work,[0],[0]
This would most likely yield a smaller model with improved generalization.,7 Conclusions and Future Work,[0],[0]
We would like to thank Mike Schuster for his help with training and evaluating the RNN models.,8 Acknowledgments,[0],[0]
Thanks also go to Tan et al. who provided us with the 44M word corpus and to action editor Jason Eisner and the anonymous reviewers whose helpful comments certainly improved the quality of the paper.,8 Acknowledgments,[0],[0]
"// Meta-features are represented as tuples (hash value, weight).",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
//,Appendix A Meta-feature Extraction Pseudocode,[0],[0]
New meta-features are either added (metafeatures.Add(mf new)) or // joint (metafeatures.Join(mf new)) with the existing meta-features.,Appendix A Meta-feature Extraction Pseudocode,[0],[0]
//,Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Strings are fingerprinted, counts are hashed.",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"function COMPUTE METAFEATURES(FeatureTargetPair pair)
// feature-related meta-features metafeatures = {} metafeatures.Add(Fingerprint(pair.feature identity), 1.0) metafeatures.",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Add(Fingerprint(pair.feature type), 1.0) log count = log(pair.feature count) / log(2)",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"bucket1 = floor(log count) bucket2 = ceil(log count) weight1 = bucket2 - log count weight2 = log count - bucket1 metafeatures.Add(Hash(bucket1), weight1) metafeatures.Add(Hash(bucket2), weight2)
// target-related meta-features metafeatures.",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Join(Fingerprint(pair.target identity), 1.0)
// feature-target-related meta-features log count = log(pair.feature target count) / log(2)",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"bucket1 = floor(log count) bucket2 = ceil(log count) weight1 = bucket2 - log count weight2 = log count - bucket1 metafeatures.Join(Hash(bucket1), weight1) metafeatures.Join(Hash(bucket2), weight2)
return metafeatures
Appendix B Multinomial Gradient
∂",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
logPmulti(y|x,Appendix A Meta-feature Extraction Pseudocode,[0],[0]
") ∂A(f, t) =
( ∂ log(xTM)wk
∂Mft",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"− ∂ log
∑ t′∈V(x TM)t′
∂Mft
)",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"∂Mft ∂Aft
=
( 1
(xTM)wk ∂(xTM)wk ∂Mft",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"− 1∑ t′∈V(x TM)t′
∂ ∑
t′∈V(x TM)t′
∂Mft
)",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Mft
= ( xfyt ŷwk − xf∑ t′∈V(x TM)t′ )",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Mft
= xfMft",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"( yt ŷwk − 1∑ t′∈V ŷt′ )
",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Appendix C Poisson Gradient
∂ logPPois(y|x) ∂A(f, t) =
( ∂ ∑
t′∈V yt′",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"log(x TM)t′
∂Mft",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"− ∂
∑ t′∈V(x TM)t′
∂Mft
) ∂Mft ∂A(f, t)
=
( 1
(xTM)wk ∂(xTM)wk ∂Mft
− xf )",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Mft
= xfMft",Appendix A Meta-feature Extraction Pseudocode,[0],[0]
( yt ŷwk − 1 ),Appendix A Meta-feature Extraction Pseudocode,[0],[0]
"Over the entire training set, adding Cf∗CftMft once on the target t that occurs with feature f amounts to the same as traversing all targets t′ that co-occur with f in the training set and adding the term Mft to each:
Mft ∑
(f,t′)∈T xf =
Cf∗",Appendix D Distributing Negative Updates,[0],[0]
Cft MftCft = Cf∗,Appendix D Distributing Negative Updates,[0],[0]
"Cft
Mft ∑
(f,t′)∈T xfyt′
Applying this to the second term of the Poisson gradient, we get:
∂ logPPois(y|x) ∂A(f, t) =",Appendix D Distributing Negative Updates,[0],[0]
xfMft yt ŷwk,Appendix D Distributing Negative Updates,[0],[0]
−,Appendix D Distributing Negative Updates,[0],[0]
xfMft =,Appendix D Distributing Negative Updates,[0],[0]
xfMft yt ŷwk,Appendix D Distributing Negative Updates,[0],[0]
− xfytMft Cf∗,Appendix D Distributing Negative Updates,[0],[0]
"Cft = xfytMft
( 1
ŷwk − Cf∗",Appendix D Distributing Negative Updates,[0],[0]
"Cft
)",Appendix D Distributing Negative Updates,[0],[0]
In leave-one-out training we exclude the event that generates the gradients from the counts used to compute those gradients.,Appendix E Leave-one-out Training,[0],[0]
"More specifically, for each training example (f, t) we let:
Cf∗",Appendix E Leave-one-out Training,[0],[0]
← Cf∗,Appendix E Leave-one-out Training,[0],[0]
"− 1 if xf = 1 Cft ← Cft − 1 if xf = 1, yt = 1
which means that the gradients for the positive and the negative examples are changed in a different way.",Appendix E Leave-one-out Training,[0],[0]
Since Eq.,Appendix E Leave-one-out Training,[0],[0]
"(16) expresses the general update rule for both type of examples, we first have to separate it into updates for negative and positive examples and then adapt accordingly.
",Appendix E Leave-one-out Training,[0],[0]
"In particular, the second term of Eq.",Appendix E Leave-one-out Training,[0],[0]
"(16), i.e. −xfytMft Cf∗Cft is a distribution of Cf∗",Appendix E Leave-one-out Training,[0],[0]
"− Cft negative and Cft positive updates over Cft positive examples:
−xfytMft Cf∗",Appendix E Leave-one-out Training,[0],[0]
"Cft
= −xfytMft ( Cf∗",Appendix E Leave-one-out Training,[0],[0]
"− Cft
Cft + Cft Cft
) = −xfytMft
Cf∗",Appendix E Leave-one-out Training,[0],[0]
− Cft Cft,Appendix E Leave-one-out Training,[0],[0]
"− xfytMft
Furthermore, recall that the first term of Eq.",Appendix E Leave-one-out Training,[0],[0]
"(16), i.e. xfytMftŷwk is non-zero only for positive examples, so it can be added to the positive updates.",Appendix E Leave-one-out Training,[0],[0]
"We can then apply leave-one-out to positive and negative updates separately, ending up with:
∂ logPPois(y|x) ∂A(f, t) =",Appendix E Leave-one-out Training,[0],[0]
"xfyt
( ( 1
ŷ+wk − 1)M+ft",Appendix E Leave-one-out Training,[0],[0]
− Cf∗,Appendix E Leave-one-out Training,[0],[0]
"− Cft Cft M−ft
)
where M−ft, M + ft and ŷ + wk are defined as follows:
M−ft = e A(f,t,Cf∗−1,Cft) Cft
Cf∗",Appendix E Leave-one-out Training,[0],[0]
"− 1
M+ft = e A(f,t,Cf∗−1,Cft−1)Cft − 1
Cf∗",Appendix E Leave-one-out Training,[0],[0]
− 1 ŷ+wk = (x TM+)wk,Appendix E Leave-one-out Training,[0],[0]
"We present Sparse Non-negative Matrix (SNM) estimation, a novel probability estimation technique for language modeling that can efficiently incorporate arbitrary features.",abstractText,[0],[0]
We evaluate SNM language models on two corpora: the One Billion Word Benchmark and a subset of the LDC English Gigaword corpus.,abstractText,[0],[0]
Results show that SNM language models trained with n-gram features are a close match for the well-established Kneser-Ney models.,abstractText,[0],[0]
"The addition of skip-gram features yields a model that is in the same league as the stateof-the-art recurrent neural network language models, as well as complementary: combining the two modeling techniques yields the best known result on the One Billion Word Benchmark.",abstractText,[0],[0]
On the Gigaword corpus further improvements are observed using features that cross sentence boundaries.,abstractText,[0],[0]
"The computational advantages of SNM estimation over both maximum entropy and neural network estimation are probably its main strength, promising an approach that has large flexibility in combining arbitrary features and yet scales gracefully to large amounts of data.",abstractText,[0],[0]
Sparse Non-negative Matrix Language Modeling,title,[0],[0]
"combinatorial number of structures. To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function. SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns nonzero probability to all structures, including implausible ones. SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment. Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers. Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.",text,[0],[0]
"Structured prediction involves the manipulation of discrete, combinatorial structures, e.g., trees and alignments (Bakır et al., 2007; Smith, 2011; Nowozin et al., 2014).",1. Introduction,[0],[0]
"Such structures arise naturally as machine learning outputs, and as intermediate representations in deep pipelines.",1. Introduction,[0],[0]
"However, the set of possible structures is typically prohibitively large.",1. Introduction,[0],[0]
"As such, inference is a core challenge, often sidestepped by greedy search, factorization assumptions, or continuous relaxations (Belanger & McCallum, 2016).
",1. Introduction,[0],[0]
"1Cornell University, Ithaca, NY 2Unbabel & Instituto de Telecomunicações, Lisbon, Portugal 3NTT",1. Introduction,[0],[0]
"Communication Science Laboratories, Kyoto, Japan.",1. Introduction,[0],[0]
"Correspondence to: Vlad Niculae <vlad@vene.ro>, André F. T. Martins <andre.martins@unbabel.com>, Mathieu Blondel <mathieu@mblondel.org>, Claire Cardie <cardie@cs.cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"△
argmax (1, 0, 0)
softmax (.5, .3, .2)
sparsemax (.6, .4, 0)
M
MAP
⋆
Marginal ⋆
SparseMAP ⋆
Figure 1.",1. Introduction,[0],[0]
"Left: in the unstructured case, softmax and sparsemax can be interpreted as regularized, differentiable argmax approximations; softmax returns dense solutions while sparsemax favors sparse ones.",1. Introduction,[0],[0]
"Right: in this work, we extend this view to structured inference, which consists of optimizing over a polytope M, the convex hull of all possible structures (depicted: the arborescence polytope, whose vertices are trees).",1. Introduction,[0],[0]
"We introduce SparseMAP as a structured extension of sparsemax: it is situated in between MAP inference, which yields a single structure, and marginal inference, which returns a dense combination of structures.
",1. Introduction,[0],[0]
"In this paper, we propose an appealing alternative: a new inference strategy, dubbed SparseMAP, which encourages sparsity in the structured representations.",1. Introduction,[0],[0]
"Namely, we seek solutions explicitly expressed as a combination of a small, enumerable set of global structures.",1. Introduction,[0],[0]
"Our framework departs from the two most common inference strategies in structured prediction: maximum a posteriori (MAP) inference, which returns the highest-scoring structure, and marginal inference, which yields a dense probability distribution over structures.",1. Introduction,[0],[0]
"Neither of these strategies is fully satisfactory: for latent structure models, marginal inference is appealing, since it can represent uncertainty and, unlike MAP inference, it is continuous and differentiable, hence amenable for use in structured hidden layers in neural networks (Kim et al., 2017).",1. Introduction,[0],[0]
"It has, however, several limitations.",1. Introduction,[0],[0]
"First, there are useful problems for which MAP is tractable, but marginal inference is not, e.g., linear assignment (Valiant, 1979; Taskar, 2004).",1. Introduction,[0],[0]
"Even when marginal inference is available, case-bycase derivation of the backward pass is needed, sometimes producing fairly complicated algorithms,",1. Introduction,[0],[0]
"e.g., second-order expectation semirings (Li & Eisner, 2009).",1. Introduction,[0],[0]
"Finally, marginal inference is dense: it assigns nonzero probabilities to all structures and cannot completely rule out irrelevant ones.",1. Introduction,[0],[0]
"This can be statistically and computationally wasteful, as well as qualitatively harder to interpret.
",1. Introduction,[0],[0]
"In this work, we make the following contributions:
1.",1. Introduction,[0],[0]
"We propose SparseMAP: a new framework for sparse
structured inference (§3.1).",1. Introduction,[0],[0]
The main idea is illustrated in Figure 1.,1. Introduction,[0],[0]
"SparseMAP is a twofold generalization: first, as a structured extension of the sparsemax transformation (Martins & Astudillo, 2016); second, as a continuous yet sparse relaxation of MAP inference.",1. Introduction,[0],[0]
MAP yields a single structure and marginal inference yields a dense distribution over all structures.,1. Introduction,[0],[0]
"In contrast, the SparseMAP solutions are sparse combinations of a small number of often-overlapping structures.
",1. Introduction,[0],[0]
2.,1. Introduction,[0],[0]
"We show how to compute SparseMAP effectively, re-
quiring only a MAP solver as a subroutine (§3.2), by exploiting the problem’s sparsity and quadratic curvature.",1. Introduction,[0],[0]
"Noticeably, the MAP oracle can be any arbitrary solver, e.g., the Hungarian algorithm for linear assignment, which permits tackling problems for which marginal inference is intractable.
3.",1. Introduction,[0],[0]
"We derive expressions for gradient backpropagation
through SparseMAP inference, which, unlike MAP, is differentiable almost everywhere (§3.3).",1. Introduction,[0],[0]
"The backward pass is fully general (applicable to any type of structure), and it is efficient, thanks to the sparsity of the solutions and to reusing quantities computed in the forward pass.
4.",1. Introduction,[0],[0]
"We introduce a novel SparseMAP loss for structured pre-
diction, placing it into a family of loss functions which generalizes the CRF and structured SVM losses (§4).",1. Introduction,[0],[0]
"Inheriting the desirable properties of SparseMAP inference, the SparseMAP loss and its gradients can be computed efficiently, provided access to MAP inference.
",1. Introduction,[0],[0]
"Our experiments demonstrate that SparseMAP is useful both for predicting structured outputs, as well as for learning latent structured representations.",1. Introduction,[0],[0]
"On dependency parsing (§5.1), structured output networks trained with the SparseMAP loss yield more accurate models with sparse, interpretable predictions, adapting to the ambiguity (or lack thereof) of test examples.",1. Introduction,[0],[0]
"On natural language inference (§5.2), we learn latent structured alignments, obtaining good predictive performance, as well as useful natural visualizations concentrated on a small number of structures.1
Notation.",1. Introduction,[0],[0]
"Given vectors a ∈ Rm, b ∈ Rn, [a; b] ∈ R
m+n denotes their concatenation; given matrices A ∈ R
m×k,B ∈ Rn×k, we denote their row-wise stacking as [A;B] ∈ R(m+n)×k.",1. Introduction,[0],[0]
"We denote the columns of a matrix A by aj ; by extension, a slice of columns of A is denoted AI for a set of indices I. We denote the canonical simplex by △d := {y ∈ Rd : y 0, ∑d
i=1 yi = 1}, and the indicator function of a predicate p as I[p] = {1 if p, 0 otherwise }.
1 General-purpose dynet and pytorch implementations available at https://github.com/vene/sparsemap.",1. Introduction,[0],[0]
"As a basis for the more complex structured case, we first consider the simple problem of selecting the largest value in a vector θ ∈ Rd.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"We denote the vector mapping
argmax(θ) := argmax y∈△d
θ⊤y.
","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"When there are no ties, argmax has a unique solution ei peaking at the index i of the highest value of θ.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"When there are ties, argmax is set-valued.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"Even assuming no ties, argmax is piecewise constant, and thus is ill-suited for direct use within neural networks, e.g., in an attention mechanism.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"Instead, it is common to use softmax, a continuous and differentiable approximation to argmax, which can be seen as an entropy-regularized argmax
softmax(θ) := argmax y∈△d
θ⊤y+H(y) = expθ
∑d i=1","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
exp,"2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"θi
(1)
where H(y) =","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"− ∑
i yi ln yi, i.e. the negative Shannon entropy.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"Since exp · > 0 strictly, softmax outputs are dense.
","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"By replacing the entropic penalty with a squared ℓ2 norm, Martins & Astudillo (2016) introduced a sparse alternative to softmax, called sparsemax, given by
sparsemax(θ) := argmax y∈△d
θ⊤y − 1
2 ‖y‖
2 2
= argmin y∈△d
‖y","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"− θ‖ 2 2 .
(2)
Both softmax and sparsemax are continuous and differentiable almost everywhere; however, sparsemax encourages sparsity in its outputs.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"This is because it corresponds to an Euclidean projection onto the simplex, which is likely to hit its boundary as the magnitude of θ increases.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"Both mechanisms, as well as variants with different penalties (Niculae & Blondel, 2017), have been successfully used in attention mechanisms, for mapping a score vector θ to a d-dimensional normalized discrete probability distribution over a small set of choices.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"The relationship between argmax, softmax, and sparsemax, illustrated in Figure 1, sits at the foundation of SparseMAP.","2.1. Regularized Max Operators: Softmax, Sparsemax",[0],[0]
"In structured prediction, the space of possible outputs is typically very large: for instance, all possible labelings of a length-n sequence, spanning trees over n nodes, or oneto-one alignments between two sets.",2.2. Structured Inference,[0],[0]
"We may still write optimization problems such as maxDs=1 θs, but it is impractical to enumerate all of the D possible structures and, in turn, to specify the scores for each structure in θ.
Instead, structured problems are often parametrized through structured log-potentials (scores) θ",2.2. Structured Inference,[0],[0]
":= A⊤η, where A ∈ R k×D is a matrix that specifies the structure of the problem, and η ∈",2.2. Structured Inference,[0],[0]
"Rk is lower-dimensional parameter vector, i.e., k ≪ D.",2.2. Structured Inference,[0],[0]
"For example, in a factor graph (Kschischang et al., 2001) with variables U and factors F , θ is given by
θs := ∑
i∈U
ηU,i(si) + ∑
f∈F
ηF,f (sf ),
where ηU and ηF are unary and higher-order log-potentials, and si and sf are local configurations at variable and factor nodes.",2.2. Structured Inference,[0],[0]
"This can be written in matrix notation as θ = M⊤ηU+N
⊤ηF for suitable matrices {M ,N}, fitting the assumption above with A =",2.2. Structured Inference,[0],[0]
"[M ;N ] and η = [ηU ;ηF ].
",2.2. Structured Inference,[0],[0]
"We can then rewrite the MAP inference problem, which seeks the highest-scoring structure, as a k-dimensional problem, by introducing variables [u;v] ∈",2.2. Structured Inference,[0],[0]
"Rk to denote configurations at variable and factor nodes:2
MAPA(η)",2.2. Structured Inference,[0],[0]
":= argmax u:=My
y∈△D
θ⊤y
= argmax u: [u;v]∈MA
η⊤Uu+ η",2.2. Structured Inference,[0],[0]
"⊤ F v,
(3)
whereMA",2.2. Structured Inference,[0],[0]
":= {[u;v] : u = My, v = Ny, y ∈ △",2.2. Structured Inference,[0],[0]
"D} is the marginal polytope (Wainwright & Jordan, 2008), with one vertex for each possible structure (Figure 1).",2.2. Structured Inference,[0],[0]
"However, as previously said, since it is equivalent to a D-dimensional argmax, MAP is piecewise constant and discontinuous.
",2.2. Structured Inference,[0],[0]
"Negative entropy regularization over y, on the other hand, yields marginal inference,
MarginalA(η)",2.2. Structured Inference,[0],[0]
":= argmax u:=My
y∈△D
θ⊤y +H(y)
= argmax u: [u;v]∈MA
η⊤Uu+ η",2.2. Structured Inference,[0],[0]
"⊤ F v +HA(u,v).
",2.2. Structured Inference,[0],[0]
"(4)
Marginal inference is differentiable, but may be more difficult to compute; the entropy HA(u,v) = H(y) itself lacks a closed form (Wainwright & Jordan, 2008, §4.1.2).",2.2. Structured Inference,[0],[0]
"Gradient backpropagation is available only to specialized problem instances, e.g. those solvable by dynamic programming (Li & Eisner, 2009).",2.2. Structured Inference,[0],[0]
"The entropic term regularizes y toward more uniform distributions, resulting in strictly dense solutions, just like in the case of softmax (Equation 1).
",2.2. Structured Inference,[0],[0]
"Interesting types of structures, which we use in the experiments described in Section 5, include the following.
2We use the notation argmax u:",2.2. Structured Inference,[0],[0]
"[u;v]∈M to convey that the maximization is over both u and v, but only u is returned.",2.2. Structured Inference,[0],[0]
"Separating the variables as [u;v] loses no generality and allows us to isolate the unary posteriors u as the return value of interest.
",2.2. Structured Inference,[0],[0]
Sequence tagging.,2.2. Structured Inference,[0],[0]
"Consider a sequence of n items, each assigned one out of a possible m tags.",2.2. Structured Inference,[0],[0]
"In this case, a global structure s is a joint assignment of tags (t1, · · · , tn).",2.2. Structured Inference,[0],[0]
"The matrix M is nm-by-mn–dimensional, with columns ms ∈ {0, 1}nm :=",2.2. Structured Inference,[0],[0]
"[et1 , ..., etn ] indicating which tag is assigned to each variable in the global structure s. N is nm2-bymn–dimensional, with ns encoding the transitions between consecutive tags, i.e., ns(i, a, b) := I[ti−1 = a & ti = b].",2.2. Structured Inference,[0],[0]
"The Viterbi algorithm provides MAP inference and forwardbackward provides marginal inference (Rabiner, 1989).
",2.2. Structured Inference,[0],[0]
Non-projective dependency parsing.,2.2. Structured Inference,[0],[0]
"Consider a sentence of length n. Here, a structure s is a dependency tree: a rooted spanning tree over the n2 possible arcs (for example, the arcs above the sentences in Figure 3).",2.2. Structured Inference,[0],[0]
"Each column ms ∈ {0, 1} n2 encodes a tree by assigning a 1 to its arcs.",2.2. Structured Inference,[0],[0]
"N is empty,MA is known as the arborescence polytope (Martins et al., 2009).",2.2. Structured Inference,[0],[0]
"MAP inference may be performed by maximal arborescence algorithms (Chu & Liu, 1965; Edmonds, 1967; McDonald et al., 2005), and the MatrixTree theorem (Kirchhoff, 1847) provides a way to perform marginal inference (Koo et al., 2007; Smith & Smith, 2007).
",2.2. Structured Inference,[0],[0]
Linear assignment.,2.2. Structured Inference,[0],[0]
Consider a one-to-one matching (linear assignment) between two sets of n nodes.,2.2. Structured Inference,[0],[0]
"A global structure s is a n-permutation, and a column ms ∈ {0, 1} n2 can be seen as a flattening of the corresponding permutation matrix.",2.2. Structured Inference,[0],[0]
"Again, N is empty.",2.2. Structured Inference,[0],[0]
"MA is the Birkhoff polytope (Birkhoff, 1946), and MAP inference can be performed by, e.g., the Hungarian algorithm (Kuhn, 1955) or the Jonker-Volgenant algorithm (Jonker & Volgenant, 1987).",2.2. Structured Inference,[0],[0]
"Noticeably, marginal inference is known to be #P-complete (Valiant, 1979; Taskar, 2004, Section 3.5).",2.2. Structured Inference,[0],[0]
This makes it an open problem how to use matchings as latent variables.,2.2. Structured Inference,[0],[0]
"Armed with the parallel between structured inference and regularized max operators described in §2, we are now ready to introduce SparseMAP, a novel inference optimization problem which returns sparse solutions.",3. SparseMAP,[0],[0]
"We introduce SparseMAP by regularizing the MAP inference problem in Equation 3 with a squared ℓ2 penalty on the returned posteriors, i.e., 12 ‖u‖ 2 2.",3.1. Definition,[0],[0]
"Denoting, as above, θ := A⊤η, the result is a quadratic optimization problem,
SparseMAPA(η)",3.1. Definition,[0],[0]
":= argmax u:=My
y∈△D
θ⊤y − 1
2 ‖My‖
2 2
= argmax u: [u,v]∈MA
η⊤Uu+ η ⊤ F v −
1 2 ‖u‖ 2 2 .
(5)
The quadratic penalty replaces the entropic penalty from marginal inference (Equation 4), which pushes the solutions to the strict interior of the marginal polytope.",3.1. Definition,[0],[0]
"In consequence, SparseMAP favors sparse solutions from the faces of the marginal polytope MA, as illustrated in Figure 1.",3.1. Definition,[0],[0]
"For the structured prediction problems mentioned in Section 2.2, SparseMAP would be able to return, for example, a sparse combination of sequence labelings, parse trees, or matchings.",3.1. Definition,[0],[0]
"Moreover, the strongly convex regularization on u ensures that SparseMAP has a unique solution and is differentiable almost everywhere, as we will see.
3.2.",3.1. Definition,[0],[0]
"Solving SparseMAP
We now tackle the optimization problem in Equation 5.",3.1. Definition,[0],[0]
"Although SparseMAP is a QP over a polytope, even describing it in standard form is infeasible, since enumerating the exponentially-large set of vertices is infeasible.",3.1. Definition,[0],[0]
"This prevents direct application of, e.g., the generic differentiable QP solver of Amos & Kolter (2017).",3.1. Definition,[0],[0]
"We instead focus on SparseMAP solvers that involve a sequence of MAP problems as a subroutine—this makes SparseMAP widely applicable, given the availability of MAP implementations for various structures.",3.1. Definition,[0],[0]
"We discuss two such methods, one based on the conditional gradient algorithm and another based on the active set method for quadratic programming.",3.1. Definition,[0],[0]
"We provide a full description of both methods in Appendix A.
Conditional gradient.",3.1. Definition,[0],[0]
"One family of such solvers is based on the conditional gradient (CG) algorithm (Frank & Wolfe, 1956; Lacoste-Julien & Jaggi, 2015), considered in prior work for solving approximations of the marginal inference problem (Belanger et al., 2013; Krishnan et al., 2015).",3.1. Definition,[0],[0]
Each step must solve a linearized subproblem.,3.1. Definition,[0],[0]
"Denote by f the SparseMAP objective from Equation 5,
f(u,v) := η⊤Uu+ η ⊤ F v −
1 2 ‖u‖ 2 2 .
",3.1. Definition,[0],[0]
"The gradients of f with respect to the two variables are
∇uf(u ′,v′) = ηU",3.1. Definition,[0],[0]
"− u ′, ∇vf(u ′,v′) = ηV .
",3.1. Definition,[0],[0]
"A linear approximation to f around a point [u′;v′] is
f̂(u,v) := (∇uf) ⊤u+(∇vf) ⊤v",3.1. Definition,[0],[0]
"= (ηU−u ′)⊤u+η⊤F v.
Minimizing f̂ overM is exactly MAP inference with adjusted variable scores ηU",3.1. Definition,[0],[0]
"− u ′. Intuitively, at each step we seek a high-scoring structure while penalizing sharing variables with already-selected structures Vanilla CG simply adds the new structure to the active set at every iteration.",3.1. Definition,[0],[0]
"The pairwise and away-step variants trade off between the direction toward the new structure, and away from one of the already-selected structures.",3.1. Definition,[0],[0]
"More sophisticated variants have been proposed (Garber & Meshi, 2016) which can provide sparse solutions when optimizing over a polytope.
",3.1. Definition,[0],[0]
Active set method.,3.1. Definition,[0],[0]
"Importantly, the SparseMAP problem in Equation 5 has quadratic curvature, which the general CG algorithms may not optimally leverage.",3.1. Definition,[0],[0]
"For this reason, we consider the active set method for constrained QPs: a generalization of Wolfe’s min-norm point algorithm (Wolfe, 1976), also used in structured prediction for the quadratic subproblems by Martins et al. (2015).",3.1. Definition,[0],[0]
"The active set algorithm, at each iteration, updates an estimate of the solution support by adding or removing one constraint to/from the active set; then it solves the Karush–Kuhn–Tucker (KKT) system of a relaxed QP restricted to the current support.
",3.1. Definition,[0],[0]
Comparison.,3.1. Definition,[0],[0]
"Both algorithms enjoy global linear convergence with similar rates (Lacoste-Julien & Jaggi, 2015), but the active set algorithm also exhibits exact finite convergence—this allows it, for instance, to capture the optimal sparsity pattern (Nocedal & Wright, 1999, Ch. 16.4 & 16.5).",3.1. Definition,[0],[0]
Vinyes & Obozinski (2017) provide a more in-depth discussion of the connections between the two algorithms.,3.1. Definition,[0],[0]
We perform an empirical comparison on a dependency parsing instance with random potentials.,3.1. Definition,[0],[0]
"Figure 2 shows that active set substantially outperforms all CG variants, both in terms of objective value as well as in the solution sparsity, suggesting that the quadratic curvature makes SparseMAP solvable in very few iterations to high accuracy.",3.1. Definition,[0],[0]
"We therefore use the active set solver in the remainder of the paper.
3.3.",3.1. Definition,[0],[0]
"Backpropagating Gradients through SparseMAP
In order to use SparseMAP as a neural network layer trained with backpropagation, one must compute products of the SparseMAP Jacobian with a vector p.",3.1. Definition,[0],[0]
"Computing the Jacobian of an optimization problem is an active research topic known as argmin differentiation, and is generally difficult.",3.1. Definition,[0],[0]
"Fortunately, as we show next, argmin differentiation is always easy and efficient in the case of SparseMAP.
",3.1. Definition,[0],[0]
Proposition 1,3.1. Definition,[0],[0]
Denote a SparseMAP solution by y⋆ and its support by I := {s : ys > 0}.,3.1. Definition,[0],[0]
"Then, SparseMAP is
differentiable almost everywhere with Jacobian
∂u⋆
∂η = MD(I)A⊤, where D(I) = D(I)⊤given by
d(I)s :=
{
(
I − 1 1TZ1 Z11T ) zs, s ∈",3.1. Definition,[0],[0]
"I
0 s /∈",3.1. Definition,[0],[0]
"I ,
Z := (MI ⊤MI) −1.
",3.1. Definition,[0],[0]
"The proof, given in Appendix B, relies on the KKT conditions of the SparseMAP QP.",3.1. Definition,[0],[0]
"Importantly, because D(I) is zero outside of the support of the solution, computing the Jacobian only requires the columns of M and A corresponding to the structures in the active set.",3.1. Definition,[0],[0]
"Moreover, when using the active set algorithm discussed in §3.2, the matrix Z is readily available as a byproduct of the forward pass.",3.1. Definition,[0],[0]
"The backward pass can, therefore, be computed in O(k|I|).
",3.1. Definition,[0],[0]
Our approach for gradient computation draws its efficiency from the solution sparsity and does not depend on the type of structure considered.,3.1. Definition,[0],[0]
This is contrasted with two related lines of research.,3.1. Definition,[0],[0]
"The first is “unrolling” iterative inference algorithms, for instance belief propagation (Stoyanov et al., 2011; Domke, 2013) and gradient descent (Belanger et al., 2017), where the backward pass complexity scales with the number of iterations.",3.1. Definition,[0],[0]
"In the second, employed by Kim et al. (2017), when inference can be performed via dynamic programming, backpropagation can be performed using secondorder expectation semirings (Li & Eisner, 2009) or more general smoothing (Mensch & Blondel, 2018), in the same time complexity as the forward pass.",3.1. Definition,[0],[0]
"Moreover, in our approach, neither the forward nor the backward passes involve logarithms, exponentiations or log-domain classes, avoiding the slowdown and stability issues normally incurred.
",3.1. Definition,[0],[0]
"In the unstructured case, since M = I , Z is also an identity matrix, uncovering the sparsemax Jacobian (Martins & Astudillo, 2016).",3.1. Definition,[0],[0]
"In general, structures are not necessarily orthogonal, but may have degrees of overlap.",3.1. Definition,[0],[0]
"and the SparseMAP Loss
With the efficient algorithms derived above in hand, we switch gears to defining a SparseMAP loss function.",4. Structured Fenchel-Young Losses,[0],[0]
"Structured output prediction models are typically trained by minimizing a structured loss measuring the discrepancy between the desired structure (encoded, for instance, as an indicator vector y = es) and the prediction induced by the log-potentials η.",4. Structured Fenchel-Young Losses,[0],[0]
We provide here a general family of structured prediction losses that will make the newly proposed SparseMAP loss arise as a very natural case.,4. Structured Fenchel-Young Losses,[0],[0]
"Below, we let Ω : RD → R denote a convex penalty function and denote
by Ω△",4. Structured Fenchel-Young Losses,[0],[0]
its restriction to△,4. Structured Fenchel-Young Losses,[0],[0]
"D ⊂ RD, i.e.,
Ω△(y) :=
{
Ω(y), y ∈ △D; ∞, y /∈ △",4. Structured Fenchel-Young Losses,[0],[0]
"D.
The Fenchel convex conjugate of Ω△ is
Ω⋆△(θ) := sup y∈RD",4. Structured Fenchel-Young Losses,[0],[0]
"θ⊤y − Ω△(y) = sup y∈△D θ⊤y − Ω(y).
",4. Structured Fenchel-Young Losses,[0],[0]
"We next introduce a family of structured prediction losses, named after the corresponding Fenchel-Young duality gap.
",4. Structured Fenchel-Young Losses,[0],[0]
Definition 1 (Fenchel-Young losses),4. Structured Fenchel-Young Losses,[0],[0]
"Given a convex penalty function Ω : RD → R, and a (k ×D)-dimensional matrix A =",4. Structured Fenchel-Young Losses,[0],[0]
"[M ;N ] encoding the structure of the problem, we define the following family of structured losses:
ℓΩ,A(η,y) := Ω ⋆ △(A ⊤η)",4. Structured Fenchel-Young Losses,[0],[0]
"+ Ω△(y)− η ⊤Ay. (6)
",4. Structured Fenchel-Young Losses,[0],[0]
"This family, studied in more detail in(Blondel et al., 2018), includes the commonly-used structured losses:
• Structured perceptron (Collins, 2002): Ω ≡ 0;
• Structured SVM (Taskar et al., 2003; Tsochantaridis
et al., 2004): Ω ≡ ρ",4. Structured Fenchel-Young Losses,[0],[0]
"(·, ȳ) for a cost function ρ, where ȳ is the true output;
• CRF (Lafferty et al., 2001): Ω ≡ −H;
• Margin CRF (Gimpel & Smith, 2010):
Ω ≡",4. Structured Fenchel-Young Losses,[0],[0]
−H,4. Structured Fenchel-Young Losses,[0],[0]
"+ ρ(·, ȳ).
",4. Structured Fenchel-Young Losses,[0],[0]
"This leads to a natural way of defining SparseMAP losses, by plugging the following into Equation 6:
• SparseMAP loss: Ω(y) = 12 ‖My‖ 2 2, • Margin SparseMAP: Ω(y) = 12 ‖My‖ 2 2 + ρ(y, ȳ).
",4. Structured Fenchel-Young Losses,[0],[0]
"It is well-known that the subgradients of structured perceptron and SVM losses consist of MAP inference, while the CRF loss gradient requires marginal inference.",4. Structured Fenchel-Young Losses,[0],[0]
"Similarly, the subgradients of the SparseMAP loss can be computed via SparseMAP inference, which in turn only requires MAP.",4. Structured Fenchel-Young Losses,[0],[0]
"The next proposition states properties of structured FenchelYoung losses, including a general connection between a loss and its corresponding inference method.
",4. Structured Fenchel-Young Losses,[0],[0]
Proposition 2 Consider a convex Ω and a structured model defined by the matrix A ∈ Rk×D.,4. Structured Fenchel-Young Losses,[0],[0]
Denote the inference objective fΩ(y),4. Structured Fenchel-Young Losses,[0],[0]
":= η
⊤Ay − Ω(y), and a solution y⋆ := argmax y∈△D fΩ(y).",4. Structured Fenchel-Young Losses,[0],[0]
"Then, the following properties hold:
1.",4. Structured Fenchel-Young Losses,[0],[0]
"ℓΩ,A(η,y) ≥ 0, with equality when fΩ(y) = fΩ(y ⋆);
2.",4. Structured Fenchel-Young Losses,[0],[0]
"ℓΩ,A(η,y) is convex, ∂ℓΩ,A(η,y) ∋",4. Structured Fenchel-Young Losses,[0],[0]
"A(y ⋆ − y);
3.",4. Structured Fenchel-Young Losses,[0],[0]
"ℓtΩ,A(η,y) = tℓΩ(η/t,y) for any t ∈ R, t > 0.
",4. Structured Fenchel-Young Losses,[0],[0]
"Proof is given in Appendix C. Property 1 suggests that pminimizing ℓΩ,A aligns models with the true label.",4. Structured Fenchel-Young Losses,[0],[0]
"Property 2 shows how to compute subgradients of ℓΩ,A provided access to the inference output [u⋆;v⋆] = Ay⋆ ∈ Rk.",4. Structured Fenchel-Young Losses,[0],[0]
"Combined with our efficient procedure described in Section 3.2, it makes the SparseMAP losses promising for structured prediction.",4. Structured Fenchel-Young Losses,[0],[0]
Property 3 suggests that the strength of the penalty Ω can be adjusted by simply scaling η.,4. Structured Fenchel-Young Losses,[0],[0]
"Finally, we remark that for a strongly-convex Ω, ℓΩ,A can be seen as a smoothed perceptron loss; other smoothed losses have been explored by Shalev-Shwartz & Zhang (2016).",4. Structured Fenchel-Young Losses,[0],[0]
"In this section, we experimentally validate SparseMAP on two natural language processing applications, illustrating the two main use cases presented: structured output prediction with the SparseMAP loss (§5.1) and structured hidden layers (§5.2).",5. Experimental Results,[0],[0]
"All models are implemented using the dynet library v2.0.2 (Neubig et al., 2017).
5.1.",5. Experimental Results,[0],[0]
"Dependency Parsing with the SparseMAP Loss
We evaluate the SparseMAP losses against the commonly used CRF and structured SVM losses.",5. Experimental Results,[0],[0]
"The task we focus on is non-projective dependency parsing: a structured output task consisting of predicting the directed tree of grammatical dependencies between words in a sentence (Jurafsky & Martin, 2018, Ch. 14).",5. Experimental Results,[0],[0]
"We use annotated Universal Dependency data (Nivre et al., 2016), as used in the CoNLL 2017 shared task (Zeman et al., 2017).",5. Experimental Results,[0],[0]
"To isolate the effect of the loss, we use the provided gold tokenization and part-of-speech tags.",5. Experimental Results,[0],[0]
"We follow closely the bidirectional LSTM arc-factored parser of Kiperwasser & Goldberg (2016), using the same model configuration; the only exception is not using externally pretrained embeddings.",5. Experimental Results,[0],[0]
"Parameters are trained using Adam (Kingma & Ba, 2015), tuning the learning rate on the grid {.5, 1, 2, 4, 8} × 10−3, expanded by a factor of 2 if the best model is at either end.
",5. Experimental Results,[0],[0]
"We experiment with 5 languages, diverse both in terms of
family and in terms of the amount of training data (ranging from 1,400 sentences for Vietnamese to 12,525 for English).",5. Experimental Results,[0],[0]
Test set results (Table 1) indicate that the SparseMAP losses outperform the SVM and CRF losses on 4 out of the 5 languages considered.,5. Experimental Results,[0],[0]
"This suggests that SparseMAP is a good middle ground between MAP-based and marginalbased losses in terms of smoothness and gradient sparsity.
",5. Experimental Results,[0],[0]
"Moreover, as illustrated in Figure 4, the SparseMAP loss encourages sparse predictions: models converge towards sparser solutions as they train, yielding very few ambiguous arcs.",5. Experimental Results,[0],[0]
"When confident, SparseMAP can predict a single tree.",5. Experimental Results,[0],[0]
"Otherwise, the small set of candidate parses returned can be easily visualized, often indicating genuine linguistic ambiguities (Figure 3).",5. Experimental Results,[0],[0]
"Returning a small set of parses, also sought concomittantly by Keith et al. (2018), is valuable in pipeline systems, e.g., when the parse is an input to a downstream application: error propagation is diminished in cases where the highest-scoring tree is incorrect (which is the case for the sentences in Figure 3).",5. Experimental Results,[0],[0]
"Unlike K-best heuristics, SparseMAP dynamically adjusts its output sparsity, which is desirable on realistic data where most instances are easy.",5. Experimental Results,[0],[0]
"In this section, we demonstrate SparseMAP for inferring latent structure in large-scale deep neural networks.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We focus on the task of natural language inference, defined as the classification problem of deciding, given two sentences (a premise and a hypothesis), whether the premise entails the hypothesis, contradicts it, or is neutral with respect to it.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We consider novel structured variants of the state-of-the-art ESIM model (Chen et al., 2017).",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Given a premise P of length m and a hypothesis H of length n, ESIM:
1.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Encodes P and H with an LSTM.
2.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
Computes alignment scores G ∈ Rm×n; with gij the inner product between the P word,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"i and H word j.
3.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Computes P-to-H and H-to-P alignments using row-wise,
respectively column-wise softmax on G.
4.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Augments P words with the weighted average of its
aligned H words, and vice-versa.
5.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Passes the result through another LSTM, then predicts.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We consider the following structured replacements for the independent row-wise and column-wise softmaxes (step 3):
Sequential alignment.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We model the alignment of p to h as a sequence tagging instance of length m, with n possible tags corresponding to the n words of the hypothesis.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Through transition scores, we enable the model to capture continuity and monotonicity of alignments: we parametrize transitioning from word t1 to t2 by binning the distance t2 − t1 into 5 groups, {−2 or less,−1, 0, 1, 2 or more}.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We similarly parametrize the initial alignment using bins {1, 2 or more} and the final alignment as {−2 or less,−1}, allowing the model to express whether an alignment starts at the beginning or ends on the final word of h; formally
ηF",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"(i, t1, t2) :=

 
 
wbin(t2−t1) 0",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"< i < n, wstart bin(t2) i = 0, wend bin(t1)",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"i = n.
We align p to h applying the same method in the other direction, with different transition scores w. Overall, sequential alignment requires learning 18 additional scalar parameters.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
Matching alignment.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
We now seek a symmetrical alignment in both directions simultaneously.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"To this end, we cast the alignment problem as finding a maximal weight bipartite matching.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We recall from §2.2 that a solution can be found via the Hungarian algorithm (in contrast to marginal
inference, which is #P-complete).",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"When n = m, maximal matchings can be represented as permutation matrices, and when n",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
6= m some words remain unaligned.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
SparseMAP returns a weighted average of a few maximal matchings.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"This method requires no additional learned parameters.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We evaluate the two models alongside the softmax baseline on the SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018)",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"datasets.3 All models are trained by SGD, with 0.9× learning rate decay at epochs when the validation accuracy is not the best seen.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"We tune the learning rate on the grid { 2k : k ∈ {−6,−5,−4,−3} } , extending the range if the best model is at either end.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"The results in Table 2 show that structured alignments are competitive with softmax in terms of accuracy, but are orders of magnitude sparser.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"This sparsity allows them to produce global alignment structures that are interpretable, as illustrated in Figure 5.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Interestingly, we observe computational advantages of sparsity.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Despite the overhead of GPU memory copying, both training and validation in our latent structure models take roughly the same time as with softmax and become faster as the models grow more certain.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"For the sake of comparison, Kim et al. (2017) report a 5× slow-down in their structured attention networks, where they use marginal inference.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"3We split the MultiNLI matched validation set into equal validation and test sets; for SNLI we use the provided split.
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"a po lic e of fic er
w at
ch es a
si tu
at io
n
cl os
el y .
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"(a) softmax
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"a po lic e of fic er
w at
ch es a
si tu
at io
n
cl os
el y .
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"(b) sequence
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"a
police
officer
watches
a
situation
closely
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"a po lic e of fic er
w at
ch es a
si tu
at io
n
cl os
el y .
a
gentleman
overlooking
a
neighborhood
situation
.
",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"(c) matching
Figure 5.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"Latent alignments on an example from the SNLI validation set, correctly predicted as neutral by all compared models.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"The premise is on the y-axis, the hypothesis on the x-axis.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
Top: columns sum to 1; bottom: rows sum to 1.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
"The matching alignment mechanism yields a symmetrical alignment, and is thus shown only once.",5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
Softmax yields a dense alignment (nonzero weights are marked with a border).,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
The structures selected by sequential alignment are overlayed as paths; the selected matchings are displayed in the top right.,5.2. Latent Structured Alignment for Natural Language Inference,[0],[0]
Structured attention networks.,6. Related Work,[0],[0]
Kim et al. (2017) and Liu & Lapata (2018) take advantage of the tractability of marginal inference in certain structured models and derive specialized backward passes for structured attention.,6. Related Work,[0],[0]
"In contrast, our approach is modular and general: with SparseMAP, the forward pass only requires MAP inference, and the backward pass is efficiently computed based on the forward pass results.",6. Related Work,[0],[0]
"Moreover, unlike marginal inference, SparseMAP yields sparse solutions, which is an appealing property statistically, computationally, and visually.
K-best inference.",6. Related Work,[0],[0]
"As it returns a small set of structures, SparseMAP brings to mind K-best inference, often used in pipeline NLP systems for increasing recall and handling uncertainty (Yang & Cardie, 2013).",6. Related Work,[0],[0]
"K-best inference can be approximated (or, in some cases, solved), roughly K times slower than MAP inference (Yanover & Weiss, 2004; Camerini et al., 1980; Chegireddy & Hamacher, 1987; Fromer & Globerson, 2009).",6. Related Work,[0],[0]
"The main advantages of SparseMAP are convexity, differentiablity, and modularity, as SparseMAP can be computed in terms of MAP subproblems.",6. Related Work,[0],[0]
"Moreover, it yields a distribution, unlike K-best, which does not reveal the gap between selected structures,
Learning permutations.",6. Related Work,[0],[0]
"A popular approach for differentiable permutation learning involves mean-entropic optimal transport relaxations (Adams & Zemel, 2011; Mena et al., 2018).",6. Related Work,[0],[0]
"Unlike SparseMAP, this does not apply to general
structures, and solutions are not directly expressible as combinations of a few permutations.
",6. Related Work,[0],[0]
Regularized inference.,6. Related Work,[0],[0]
"Ravikumar et al. (2010), Meshi et al. (2015), and Martins et al. (2015) proposed ℓ2 perturbations and penalties in various related ways, with the goal of solving LP-MAP approximate inference in graphical models.",6. Related Work,[0],[0]
"In contrast, the goal of our work is sparse structured prediction, which is not considered in the aforementioned work.",6. Related Work,[0],[0]
"Nevertheless, some of the formulations in their work share properties with SparseMAP; exploring the connections further is an interesting avenue for future work.",6. Related Work,[0],[0]
"We introduced a new framework for sparse structured inference, SparseMAP, along with a corresponding loss function.",7. Conclusion,[0],[0]
We proposed efficient ways to compute the forward and backward passes of SparseMAP.,7. Conclusion,[0],[0]
Experimental results illustrate two use cases where sparse inference is well-suited.,7. Conclusion,[0],[0]
"For structured prediction, the SparseMAP loss leads to strong models that make sparse, interpretable predictions, a good fit for tasks where local ambiguities are common, like many natural language processing tasks.",7. Conclusion,[0],[0]
"For structured hidden layers, we demonstrated that SparseMAP leads to strong, interpretable networks trained end-to-end.",7. Conclusion,[0],[0]
"Modular by design, SparseMAP can be applied readily to any structured problem for which MAP inference is available, including combinatorial problems such as linear assignment.",7. Conclusion,[0],[0]
"We thank Tim Vieira, David Belanger, Jack Hessel, Justine Zhang, Sydney Zink, the Unbabel AI Research team, and the three anonymous reviewers for their insightful comments.",Acknowledgements,[0],[0]
"This work was supported by the European Research Council (ERC StG DeepSPIN 758969) and by the Fundação para a Ciência e Tecnologia through contracts UID/EEA/50008/2013, PTDC/EEI-SII/7092/2014 (LearnBig), and CMUPERI/TIC/0046/2014 (GoLocal).",Acknowledgements,[0],[0]
Structured prediction requires searching over a combinatorial number of structures.,abstractText,[0],[0]
"To tackle it, we introduce SparseMAP: a new method for sparse structured inference, and its natural loss function.",abstractText,[0],[0]
"SparseMAP automatically selects only a few global structures: it is situated between MAP inference, which picks a single structure, and marginal inference, which assigns nonzero probability to all structures, including implausible ones.",abstractText,[0],[0]
"SparseMAP can be computed using only calls to a MAP oracle, making it applicable to problems with intractable marginal inference, e.g., linear assignment.",abstractText,[0],[0]
"Sparsity makes gradient backpropagation efficient regardless of the structure, enabling us to augment deep neural networks with generic and sparse structured hidden layers.",abstractText,[0],[0]
"Experiments in dependency parsing and natural language inference reveal competitive accuracy, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline systems.",abstractText,[0],[0]
SparseMAP: Differentiable Sparse Structured Inference,title,[0],[0]
Real-world spatio-temporal processes are often poorly modelled by standard inference methods that assume stationarity in time and space.,1. Introduction,[0],[0]
"A variety of techniques have been developed for modelling non-stationarity in time via changepoints (CPS), ranging from methods for Gaussian Processes (GPS) (Garnett et al., 2009), the Lasso (Lin et al., 2017) or the Ising model (Fazayeli & Banerjee, 2016) over approaches using density ratio estimation (Liu et al., 2013) and kernelbased methods exploiting M-statistics (Li et al., 2015) to framing CP detection as time series clustering (Khaleghi & Ryabko, 2014).",1. Introduction,[0],[0]
"In contrast, CP inference allowing for non-stationarity in space (Herlands et al., 2016) has received comparatively little attention.
",1. Introduction,[0],[0]
We offer the first on-line solution to this problem by modeling non-stationarity in both space and time.,1. Introduction,[0],[0]
"CPS are used to model non-stationarity in time, and the use of spatially structured Bayesian Vector Autoregressions (SSBVAR) circumvents the assumption of stationarity in space.",1. Introduction,[0],[0]
"We unify Adams & MacKay (2007) and Fearnhead & Liu (2007) into
1Department of Statistics, University of Warwick, UK 2Department of Computer Science, University of Warwick, UK 3The Alan Turing Institute for Data Science & AI, UK.",1. Introduction,[0],[0]
"Correspondence to: Jeremias Knoblauch <j.knoblauch@warwick.ac.uk>.
",1. Introduction,[0],[0]
"A version of this paper appeared in the Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.
",1. Introduction,[0],[0]
"an inference procedure for on-line prediction, model selection and CP detection, see Fig. 1.",1. Introduction,[0],[0]
"Our construction exploits that both algorithms use Product Partition Models (Barry & Hartigan, 1992), which assume independence of parameters conditional on the CPS and independence of observations conditional on these parameters.
",1. Introduction,[0],[0]
Our method can be seen as modified on-line version of Xuan & Murphy (2007).,1. Introduction,[0],[0]
"In their method, inference is off-line, the model universeM is built during execution and multivariate dependencies are restricted to decomposable graph.",1. Introduction,[0],[0]
"In contrast, our procedure specifies M before execution, but runs on-line and does not restrict dependencies.",1. Introduction,[0],[0]
The closest competing on-line procedure in the literature thus far is the work of Saatçi,1. Introduction,[0],[0]
"et al. (2010), which develops Gaussian Process (GP) CP models for Bayesian On-line Changepoint Detection (BOCPD).",1. Introduction,[0],[0]
"Though our results suggest that parametric models may be preferable to GP models, the latter can still be integrated into our method as elements of the model universeM without any further modifications.
",1. Introduction,[0],[0]
"In summary, we make three contributions:",1. Introduction,[0],[0]
"Firstly, we substantially augment the existing work on BOCPD by allowing for model uncertainty.",1. Introduction,[0],[0]
"Unlike previous extensions of the al-
gorithm (e.g. Adams & MacKay, 2007; Saatçi",1. Introduction,[0],[0]
"et al., 2010), this avoids having to guess a single best model a priori.",1. Introduction,[0],[0]
"Secondly, we introduce SSBVARS as the first class of models for multivariate inference within BOCPD.",1. Introduction,[0],[0]
"Thirdly, we demonstrate that using a collection of parametric models can outperform nonparametric GP models in terms of prediction, CP detection and computational efficiency.
",1. Introduction,[0],[0]
"The structure of this paper is as follows: Section 2 generalizes the BOCPD algorithm of Adams & MacKay (2007), henceforth AM, by integrating it with the approach of Fearnhead & Liu (2007), henceforth FL.",1. Introduction,[1.0],"['The structure of this paper is as follows: Section 2 generalizes the BOCPD algorithm of Adams & MacKay (2007), henceforth AM, by integrating it with the approach of Fearnhead & Liu (2007), henceforth FL.']"
"In so doing, we arrive at BOCPD with Model Selection, henceforth BOCPDMS.",1. Introduction,[0],[0]
Section 3 proposes VAR models for non-stationary processes within the BOCPD framework.,1. Introduction,[0],[0]
This motivates populating the model universeMwith spatially structured BVAR (SSBVAR) models.,1. Introduction,[0],[0]
Sections 4–5 address computational aspects.,1. Introduction,[0],[0]
Section 6 demonstrates the algorithm’s advantages on real world data.,1. Introduction,[0],[0]
Let {Y t}∞t=1 be a data stream with an unknown number of CPs.,2. BOCPDMS,[0],[0]
"Focusing on univariate data, FL and AM tackled inference by tracking the posterior distribution for the most recent CP.",2. BOCPDMS,[1.0],"['Focusing on univariate data, FL and AM tackled inference by tracking the posterior distribution for the most recent CP.']"
"While FL allow the data to be described by different models between CPS, AM only allow for a single model.",2. BOCPDMS,[0],[0]
"However, AM perform one-step-ahead predictions, whereas FL do not.",2. BOCPDMS,[0],[0]
"Instead, they propose a Maximum A Posteriori (MAP) segmentation for CPS and models.",2. BOCPDMS,[0],[0]
"In the remainder of this section, we unify both inference approaches.",2. BOCPDMS,[0],[0]
"We call the resulting algorithm BOCPD with model selection (BOCPDMS), as it performs prediction, MAP segmentation and model selection on-line.",2. BOCPDMS,[0],[0]
"The run-length rt at time t is defined as the time since the most recent CP at time t, so rt = 0 corresponds to a CP at time t. Suppose that data between successive CPS can be described by Bayesian models collected in the model universeM. For the process {Y t} on RS , a model m ∈ M with finite memory of length L ∈ N0 consists of an observation density fm(Y t = yt|θm,y(t−L):(t−1)) on RS and a parameter prior πm(θm) on Θm depending on hyperparameters νm.",2.1. Run-length & model universe,[0],[0]
The notion ofM is due to FL and allows for model uncertainty amongst models developed for BOCPD.,2.1. Run-length & model universe,[0],[0]
"For instance, m ∈M could be a GP (Saatçi",2.1. Run-length & model universe,[0],[0]
"et al., 2010), a time-deterministic regression (Fearnhead, 2005) or a mixture distribution (Caron et al., 2012).",2.1. Run-length & model universe,[0],[0]
"Denote by mt the model describing y(t−rt):t, i.e. the data since the last CP.",2.2. Probabilistic formulation & recursions,[0],[0]
Given hazard function H : N,2.2. Probabilistic formulation & recursions,[0],[0]
"→ [0, 1],
BOCPD with Model Selection (BOCPDMS) Input at time 0: model universeM; hazard H; prior q Input at time t: next observation yt",2.2. Probabilistic formulation & recursions,[0],[0]
"Output at time t: ŷ(t+1):(t+hmax), St, p(mt|y1:t) for next observation yt at time t do
// STEP",2.2. Probabilistic formulation & recursions,[0],[0]
"I: Compute model-specific quantities for m ∈M do
if t− 1 = lag length(m) then I.A Initialize p(y1:t, rt = 0,mt = m) with prior else if t− 1 > lag length(m) then I.B.1 Update p(y1:t, rt,mt = m) via (5a), (5b) I.B.2",2.2. Probabilistic formulation & recursions,[0.9506953400832118],"['Given hazard function H : N → [0, 1], BOCPD with Model Selection (BOCPDMS) Input at time 0: model universeM; hazard H; prior q Input at time t: next observation yt Output at time t: ŷ(t+1):(t+hmax), St, p(mt|y1:t) for next observation yt at time t do // STEP I: Compute model-specific quantities for m ∈M do if t− 1 = lag length(m) then I.A Initialize p(y1:t, rt = 0,mt = m) with prior else if t− 1 > lag length(m) then I.B.1 Update p(y1:t, rt,mt = m) via (5a), (5b) I.B.2 Prune model-specific run-length distribution I.B.3 Perform hyperparameter inference via (12) end if end for // STEP II: Aggregate over models if t >= min(lag length(m)) then II.1 Obtain joint distribution overM via (6a)–(6f) II.2 Compute (7)–(9) II.3 Output: ŷ(t+1):(t+hmax), St, p(mt|y1:t) end if end for and model prior q :M→ [0, 1], the prior beliefs are p(rt|rt−1) =  1−H(rt−1 + 1) if rt = rt−1 + 1 H(rt−1 + 1) if rt = 0 0 otherwise.']"
"Prune model-specific run-length distribution I.B.3 Perform hyperparameter inference via (12)
end if end for
//",2.2. Probabilistic formulation & recursions,[0],[0]
STEP II:,2.2. Probabilistic formulation & recursions,[0],[0]
Aggregate over models if t >= min(lag length(m)),2.2. Probabilistic formulation & recursions,[0],[0]
"then
II.1",2.2. Probabilistic formulation & recursions,[0],[0]
"Obtain joint distribution overM via (6a)–(6f) II.2 Compute (7)–(9) II.3 Output: ŷ(t+1):(t+hmax), St, p(mt|y1:t)
end if end for
and model prior q",2.2. Probabilistic formulation & recursions,[0],[0]
:,2.2. Probabilistic formulation & recursions,[0],[0]
"M→ [0, 1], the prior beliefs are
p(rt|rt−1) =  1−H(rt−1 + 1) if rt = rt−1 + 1 H(rt−1 + 1) if rt = 0 0 otherwise.",2.2. Probabilistic formulation & recursions,[0],[0]
"(1a)
q(mt|mt−1, rt) = { 1mt−1(mt)",2.2. Probabilistic formulation & recursions,[0],[0]
if rt = rt−1 + 1 q(mt) if rt = 0.,2.2. Probabilistic formulation & recursions,[0],[0]
"(1b)
Eq.",2.2. Probabilistic formulation & recursions,[0],[0]
"(1b) implies that the model at time t will be equal to the model at time t− 1 unless a CP occured at t, in which case the next model mt will be a random draw from q. At time t, the algorithm requires for all possible models m and run-lengths rt the computation of the posterior predictives
fm(yt|y1:(t−1), rt)
= ∫",2.2. Probabilistic formulation & recursions,[0],[0]
Θm fm(yt|θm)πm(θm|y(t−L−rt):(t−1))dθm.,2.2. Probabilistic formulation & recursions,[0],[0]
"(2)
To make the evaluation of this integral efficient, one can use conjugate models (Xuan & Murphy, 2007) or approximations (Turner et al., 2013; Niekum et al., 2014), which make the following recursion efficient, too:
p(y1:t, rt,mt) =∑ mt−1 ∑ rt−1 { fmt(yt|y1:(t−1), rt)q(mt|y1:(t−1), rt,mt−1)
p(rt|rt−1)p(y1:(t−1), rt−1,mt−1) } .",2.2. Probabilistic formulation & recursions,[0],[0]
"(3)
The recursion in AM is the special case for |M| = 1.",2.2. Probabilistic formulation & recursions,[0],[0]
"For |M| > 1, q(mt|mt−1, rt,y1:(t−1)) arises as a new term, which for 1a as the indicator function of a is given by{ 1mt−1(mt)q(mt−1|y1:(t−1), rt−1) if rt = rt−1 + 1 q(mt) if rt = 0.",2.2. Probabilistic formulation & recursions,[0],[0]
"(4)
Next, define the growth- and changepoint probabilities as
p(y1:t, rt = rt−1 + 1,mt)",2.2. Probabilistic formulation & recursions,[0],[0]
"=
fmt(yt|y1:(t−1), rt)p(y1:(t−1), rt−1,mt−1)× (5a) (1−H(rt))q(mt−1|y1:(t−1), rt),
",2.2. Probabilistic formulation & recursions,[0],[0]
"p(y1:t, rt = 0,mt) =
fmt(yt|y1:(t−1), rt)q(mt)× (5b)∑ mt−1 ∑ rt−1 { H(rt−1 + 1)p(y1:(t−1), rt−1,mt−1) } .
",2.2. Probabilistic formulation & recursions,[0],[0]
The evidence can then be calculated via Eq.,2.2. Probabilistic formulation & recursions,[0],[0]
"(6a), which in turn allows calculating the joint model-and-run-length distribution (6b), the model posterior (6c), as well as the model-specific (6d) and global (6e) run-length distributions:
p(y1:t) = ∑
mt ∑ rt p(y1:t,mt, rt) (6a)
",2.2. Probabilistic formulation & recursions,[0],[0]
"p(rt,mt|y1:t) = p(y1:t, rt,mt)/p(y1:t) (6b)",2.2. Probabilistic formulation & recursions,[0],[0]
p(mt|y1:t) = ∑,2.2. Probabilistic formulation & recursions,[0],[0]
"rt p(rt,mt|y1:t) (6c) p(rt|mt,y1:t) =",2.2. Probabilistic formulation & recursions,[0],[0]
"p(rt,mt|y1:t)/p(mt|y1:t) (6d) p(rt|y1:t) = ∑ mt p(rt,mt|y1:t) (6e)
q(mt−1|y1:(t−1), rt−1) = p(mt−1, rt−1|y1:(t−1))
p(rt−1|y1:(t−1)) .",2.2. Probabilistic formulation & recursions,[0],[0]
"(6f)
Eq. (6f) is the conditional model posterior from Eq.",2.2. Probabilistic formulation & recursions,[0.9558457719457416],['(6f) is the conditional model posterior from Eq.']
(4).,2.2. Probabilistic formulation & recursions,[0],[0]
Eq.,2.2. Probabilistic formulation & recursions,[0],[0]
(6e) is arrived at directly in FL and used for on-line MAP segmentation.,2.2. Probabilistic formulation & recursions,[0],[0]
"By framing our derivations in the run-length framework of AM, we additionally obtain (4)–(6d), thus enabling on-line prediction and model selection at the same computational cost.",2.2. Probabilistic formulation & recursions,[0],[0]
"Prediction: Recursive h-step-ahead forecasting uses (6b):
p(Y t+h|y1:t) = ∑ rt,mt { p(Y t+h|y1:t, ŷ",2.3. On-line algorithm outputs,[0],[0]
"h t , rt,mt)p(rt,mt|y1:t) } , (7)
",2.3. On-line algorithm outputs,[0],[0]
"where ŷht = ∅ if h = 1 and ŷ h t = ŷ(t+1):(t+h−1) otherwise, with ŷt+h = E(Y t+h|y1:t, ŷ",2.3. On-line algorithm outputs,[0],[0]
h,2.3. On-line algorithm outputs,[0],[0]
t ),2.3. On-line algorithm outputs,[0],[0]
"the recursive forecast.
",2.3. On-line algorithm outputs,[0],[0]
Tracking the model posterior/Bayes Factors: One of the novel capabilites of the algorithm is on-line monitoring of the model posterior via Eq.,2.3. On-line algorithm outputs,[0],[0]
(6c).,2.3. On-line algorithm outputs,[0],[0]
This is attractive when structural changes in the data happen slowly and are not captured well by CPS.,2.3. On-line algorithm outputs,[0],[0]
"In this case, P(mt|y1:t) can be used
to identify periods of change, see Fig. 6.",2.3. On-line algorithm outputs,[0],[0]
"For pairwise comparisons, Bayes Factors can be monitored, too:
BF(m1, m2)t = p(mt = m1|y1:t) · q(m2) p(mt = m2|y1:t) · q(m1) .",2.3. On-line algorithm outputs,[0],[0]
"(8)
Maximum A Posteriori (MAP) segmentation:",2.3. On-line algorithm outputs,[0],[0]
"For MAPt the density of the MAP-estimate of models and CPS before t and MAP0 = 1, FL’s recursive estimator is given by
MAPt = max r,m
{ p(y1:t, rt = r,mt = m)MAPt−r−1 } .",2.3. On-line algorithm outputs,[0],[0]
"(9)
For r∗t ,m ∗ t maximizers for time t, the MAP segmentation is St = St−r∗t−1",2.3. On-line algorithm outputs,[0],[0]
"∪{(t− r ∗ t ,m ∗ t )}, S0 = ∅, where",2.3. On-line algorithm outputs,[0],[0]
"(t′,mt′) ∈ St means a CP at t′ ≤ t, with mt′ ∈M the model for yt′:t.",2.3. On-line algorithm outputs,[0],[0]
The last section derived BOCPDMS for arbitrary data streams {Y t}.,3. Building a spatio-temporal model universe,[0],[0]
"Next, we propose models forM if {Y t} can be mapped into a space S. Let S with |S| = S be a set of spatial locations in S with measurements Y t = (Yt,1, Yt,2, . . .",3. Building a spatio-temporal model universe,[0],[0]
", Yt,S) T recorded at times t = 1, 2, . . .",3. Building a spatio-temporal model universe,[0],[0]
"Inference on {Y t} can be drawn using conjugate Bayesian Vector Autoregressions (BVAR) with lag length L and E additional variables Zt as elements of model universeM:
σ2 ∼ InverseGamma(a, b) (10a) εt|σ2 ∼ N (0, σ2 ·Ω) (10b) c|σ2 ∼ N",3.1. Bayesian VAR (BVAR),[0],[0]
"(0, σ2 · V c) (10c) Y t = α+BZt + ∑L l=1AlY t−l + εt. (10d)
Here, Al,B are S × S, S × E matrices, c = (α, vec(B), vec(A1), vec(A2), . . .",3.1. Bayesian VAR (BVAR),[0],[0]
vec(AL))T is a vector of S · (LS + 1 + E) model parameters.,3.1. Bayesian VAR (BVAR),[0],[0]
"Scalars a, b > 0, matrix V c, and diagonal matrix Ω are hyperparameters.",3.1. Bayesian VAR (BVAR),[0],[0]
"Modelling {Y t} as VAR is attractive, as many complex non-linear processes have VAR representations, including HMMS, time-stationary GPS as well as multivariate GARCH and fractionally integrated VARMA processes (Inoue & Kasahara, 2006; Inoue et al., 2018).",3.2. Approximating processes using VARS,[0],[0]
"Performance guarantees for VAR approximations to such processes are derived using Baxter’s Inequalitiy with multivariate versions of results in Hannan & Kavalieris (1986).
",3.2. Approximating processes using VARS,[0],[0]
Theorem 1.,3.2. Approximating processes using VARS,[0],[0]
"Let {Y t} be a time-stationary spatio-temporal process with spectral density satisfying regularity condition A in the Appendix, || · ||",3.2. Approximating processes using VARS,[0],[0]
"a matrix norm, E(Y t) = 0, E(Y tY Tt )",3.2. Approximating processes using VARS,[0],[0]
"< ∞, ∑∞",3.2. Approximating processes using VARS,[0],[0]
h=−∞(1 + |h|)3||E[Y tY ′,3.2. Approximating processes using VARS,[0],[0]
"t+h]|| < ∞. Then (1)–(3) hold.
",3.2. Approximating processes using VARS,[0],[0]
"(1) Y t = ∑∞
i=1AiY t−i + εt for matrices {Al}l∈N and E(εt) = 0, E(εtε′t) =",3.2. Approximating processes using VARS,[0],[0]
"D,D diagonal.
",3.2. Approximating processes using VARS,[0],[0]
"(2) For Y t = ∑L l=1A L l Y t−l + et with {A L l }Ll=1 the
best linear projection coefficients, ∃L0 : ∀L > L0,∑L l=1(1 + |l|)3||A",3.2. Approximating processes using VARS,[0],[0]
L l − Al|| ≤,3.2. Approximating processes using VARS,[0],[0]
C · ∑∞,3.2. Approximating processes using VARS,[0],[0]
"l=L+1(1 +
|l|)3||Al|| with C constant.",3.2. Approximating processes using VARS,[0],[0]
"(3) Using T observations with L = O([T/ ln(T )]1/6) to
estimateALl as MAP Â L l of (10a)–(10d), it holds that L(T )2 ∑L(T )
l=1 ||Â L(T )",3.2. Approximating processes using VARS,[0],[0]
l −A L(T ),3.2. Approximating processes using VARS,[0],[0]
l || P→ 0,3.2. Approximating processes using VARS,[0],[0]
"as T →∞.
Proof.",3.2. Approximating processes using VARS,[0],[0]
"Part (1) is shown in Inoue et al. (2018), part (2) in Lemma 3.1 of Meyer & Kreiss (2015).",3.2. Approximating processes using VARS,[0],[0]
"Part (3) follows by their Remark 3.3 if we can prove that the MAP estimator ĉ(L(T )) of c equals its Yule-Walker estimator (YWE) as T →∞. LetB = 0,α = 0 and note that YWE equals OLS as T →∞. WithX1:T the regressor matrix of Y t−L(T ):t, ĉ(L(T ))",3.2. Approximating processes using VARS,[0],[0]
= (X ′1:TX1:T + V −1 c ) −1(X,3.2. Approximating processes using VARS,[0],[0]
′1:TY 1:T ).,3.2. Approximating processes using VARS,[0],[0]
"Then, part (3) holds as OLS P→ E(X ′1:TX1:T )",3.2. Approximating processes using VARS,[0],[0]
"−1E(X ′ 1:TY 1:T ) and
ĉ(L(T ))",3.2. Approximating processes using VARS,[0],[0]
= (X ′1:TX1:T + V −1 c ) −1(X,3.2. Approximating processes using VARS,[0],[0]
"′1:TY 1:T )
= ( 1
T X ′1:TX1:T +
1 T V −1c ) −1",3.2. Approximating processes using VARS,[0],[0]
"1 T (X ′1:TY 1:T )
P→ E(X ′1:TX1:T )−1E(X ′ 1:TY 1:T ).
",3.2. Approximating processes using VARS,[0],[0]
"In Thm. 1, assuming E(Y t) = 0 is without loss of generality: If E(Y t) = α+BZt, define Y ∗t = Y t− (α+BZt) and apply the theorem to {Y ∗t }.",3.2. Approximating processes using VARS,[0],[0]
"Moreover, the results do not require stationarity in space.",3.2. Approximating processes using VARS,[0],[0]
"Lastly, part (3) suggests a principled way of picking lag lengths L for BVAR models based on functions L(T ) =",3.2. Approximating processes using VARS,[0],[0]
"C · (T/ ln(T ))1/6, with C a constant: If between T1 and T2 observations are expected between CPS, L = {L ∈ N",3.2. Approximating processes using VARS,[0.9696685126006825],"['Lastly, part (3) suggests a principled way of picking lag lengths L for BVAR models based on functions L(T ) = C · (T/ ln(T ))1/6, with C a constant: If between T1 and T2 observations are expected between CPS, L = {L ∈ N : L(T1) ≤ L ≤ L(T2)}.']"
:,3.2. Approximating processes using VARS,[0],[0]
L(T1) ≤,3.2. Approximating processes using VARS,[0],[0]
L ≤ L(T2)}.,3.2. Approximating processes using VARS,[0],[0]
"In our experiments, we employ this strategy using T1 = 1, T2 = T .",3.2. Approximating processes using VARS,[0],[0]
"While Thm. 1 motivates approximating spatio-temporal processes between CPS with (10a)–(10d), the matrices {ALl }Ll=1 have S(LS + 1 + E) parameters.",3.3. Modeling spatial dependence,[0],[0]
This increases model complexity and ignores spatial information.,3.3. Modeling spatial dependence,[0],[0]
We remedy both issues through neighbourhood systems on S. Definition 1 (Neighbourhood system).,3.3. Modeling spatial dependence,[0],[0]
For a set of locations S with the sets Ni(s) ⊆ S as the i-th neighbourhoods of s for 0 ≤ i ≤ n,3.3. Modeling spatial dependence,[0],[0]
"and all s ∈ S, let Ni(s) ∩ Nj(s) = ∅, s′ ∈ Ni(s) ⇐⇒ s ∈ Ni(s′) and N0(s) = {s}.",3.3. Modeling spatial dependence,[0],[0]
"Then, the corresponding neighbourhood system is N(S) =",3.3. Modeling spatial dependence,[0],[0]
"{{Ni(s)}ni=1 : s ∈ S, 0 ≤ i ≤ n}.
",3.3. Modeling spatial dependence,[0],[0]
"In the remainder of the paper, smaller indices i imply that the neighbourhoods Ni(s) are closer to s. For a BVAR model of lag length L, the decay of spatial dependence is encapsulated through Π : {1, . . .",3.3. Modeling spatial dependence,[0],[0]
", L} → {0, . . .",3.3. Modeling spatial dependence,[0],[0]
", n}.",3.3. Modeling spatial dependence,[0],[0]
"In
particular, only s′ ∈ Ni(s) with i ≤ Π(l) are modeled as affecting s after l time periods.",3.3. Modeling spatial dependence,[1.0000000534431075],"['In particular, only s′ ∈ Ni(s) with i ≤ Π(l) are modeled as affecting s after l time periods.']"
"In principle, given N(S), sparsification of the BVAR model (10a)–(10d) is possible in two ways: As restriction on the contemporaneous dependence via the covariance matrix of the error term εt, or as restriction on the conditional dependence via the coefficient matrices {Al}Ll=1.",3.4. Spatializing BVAR,[0],[0]
"We choose the latter for three reasons: Firstly, linear effects have more interesting interpretations than error covariances.",3.4. Spatializing BVAR,[0],[0]
"Secondly, using {Al}Ll=1 to encode spatial dependency allows us to work with arbitrary neighbourhoods.",3.4. Spatializing BVAR,[0],[0]
"In contrast, modelling dependent errors under conjugacy limits dependencies to decomposable graphs (Xuan & Murphy, 2007).",3.4. Spatializing BVAR,[0],[0]
"Since not even a regular grid is decomposable, this is problematic for spatial data.",3.4. Spatializing BVAR,[0],[0]
"Thirdly, modelling errors as contemporaneous is attractive for low-frequency data where the resolution of temporal effects is coarse, but the situation reverses for high-frequency data.",3.4. Spatializing BVAR,[0],[0]
"Since the algorithm runs on-line, we expect {Y t} to be observed with high frequency.",3.4. Spatializing BVAR,[0],[0]
Definition 2 (Spatially structured BVAR (SSBVAR)).,3.4. Spatializing BVAR,[0],[0]
"For process {Y t} on S and (L,N(S),Π(·)), define the matrices {Ãl}Ll=1 by imposing that [Ãl](s,s′) = 0 ⇐⇒ s′ /∈",3.4. Spatializing BVAR,[0],[0]
Ni(s),3.4. Spatializing BVAR,[0],[0]
for any i ≤ Π(l).,3.4. Spatializing BVAR,[0],[0]
"Let Ã 6=0 l be the vector of non-zero entries in Ãl and c̃ = (α, vec(B), Ã 6=0 1 , Ã 6=0 2 , . . .",3.4. Spatializing BVAR,[0],[0]
Ã,3.4. Spatializing BVAR,[0],[0]
"6=0 L )
T .",3.4. Spatializing BVAR,[0],[0]
"The SSBVAR model on {Y t} induced by (L,N(S),Π(·)) is obtained by combining (10a)–(10b) with
c̃|σ2 ∼ N",3.4. Spatializing BVAR,[0],[0]
"(0, σ2 · V c̃) (10e) Y t = α+BZt + ∑L l=1 ÃlY t−l",3.4. Spatializing BVAR,[0],[0]
"+ εt. (10f)
Fig. 2 illustrates this idea.",3.4. Spatializing BVAR,[0],[0]
"Further sparsification is possible by modelling neighbourhoods jointly, i.e.",3.4. Spatializing BVAR,[0],[0]
"[Ãl](s,s′) = ai(s),∀s′ ∈ Ni(s), reducing the number of parameters to S · ∑L l=1 Π(l).",3.4. Spatializing BVAR,[0],[0]
"If one imposes ai(s) = ai(s
′) = · · · = ai, this number drops to ∑L l=1 Π(l).",3.4. Spatializing BVAR,[0],[0]
"For the choice of lag lengths L, part (3) of Thm. 1 suggests L ∈ {L′ ∈ N : L(T1) ≤ L′ ≤","3.5. Building SSBVARS: choosing L,N(S),Π(·)",[0],[0]
L(T2)} if one expects T1 to T2 observations between CPS.,"3.5. Building SSBVARS: choosing L,N(S),Π(·)",[0],[0]
"For any data stream {Y t} on a space S, there are different ways of constructing neighbourhood structures N(S).","3.5. Building SSBVARS: choosing L,N(S),Π(·)",[1.0],"['For any data stream {Y t} on a space S, there are different ways of constructing neighbourhood structures N(S).']"
"For example, when analysing pollutants in London’s air in section 6, N(S) could be constructed from Euclidean or Road distances.","3.5. Building SSBVARS: choosing L,N(S),Π(·)",[0],[0]
"By fillingM with SSBVARS constructed using competing versions of N(S), BOCPDMS provides a way of dealing with such uncertainty about spatial relations.","3.5. Building SSBVARS: choosing L,N(S),Π(·)",[0],[0]
"In fact, it can dynamically discern changing spatial relationships on S. Lastly, Π(·) should usually be decreasing to reflect that measurements affect each other less when further apart.","3.5. Building SSBVARS: choosing L,N(S),Π(·)",[0],[0]
"Hyperparameter inference on νm can be addressed either by introducing an additional hierarchical layer (Wilson et al., 2010) or using type-II ML.",4. Hyperparameter optimization,[0],[0]
"The latter is obtained by maximizing the model-specific evidence
log p(y1:T |νm) = T∑
t=1
log p(yt|νm,y1:(t−1)).",4. Hyperparameter optimization,[0],[0]
"(11)
Computation of the righthand side requires evaluating the gradients ∇νmp(y1:t, rt|νm), which are obtained efficiently and recursively (Turner et al., 2009). Saatçi",4. Hyperparameter optimization,[0],[0]
"et al. (2010) use y1:T ′ as a test set, and run BOCPD K times to find ν̂m = arg maxνm {p(y1:T ′ |νm)}.",4. Hyperparameter optimization,[0],[0]
"Most other on-line GP approaches also require substantial recomputations for hyperparameter learning (e.g., Ranganathan et al., 2011).",4. Hyperparameter optimization,[0],[0]
"In contrast, Caron et al. (2012) propose on-line gradient descent updates via
νm,t+1 = νm,t + αt∇νm,t log p(yt+1|y1:t,νm1:t).",4. Hyperparameter optimization,[0],[0]
"(12)
The latter is preferable for two reasons:",4. Hyperparameter optimization,[0],[0]
"Firstly, inference and type-II ML are executed simultaneously (rather than sequentially) and thus enable cold-starts of BOCPDMS.",4. Hyperparameter optimization,[0],[0]
"Secondly, neither the on-line nature nor the computational complexity of BOCPDMS is affected.",4. Hyperparameter optimization,[0],[0]
"While tracking |M| models, BOCPDMS has linear time complexity.",5. Computation & Complexity,[0],[0]
"Step 1 in the pseudocode is the bottleneck, but looping overM can be parallelized: With N threads, it executes in O (d|M|/Ne ·maxM∈M CmpTime(M)).",5. Computation & Complexity,[0],[0]
"Step 2 takes O(|R(t)||M|), for R(t) all run-lengths at time t.",5. Computation & Complexity,[0],[0]
"In a naive implementation, all run-lengths are retained and R(t) = {1, 2, . . .",5.1. Pruning the run-length distribution,[0],[0]
", t}.",5.1. Pruning the run-length distribution,[0],[0]
"This implies execution time of order
O(t) for processing yt, but can be made time-constant by pruning: If one discards run-lengths whose posterior probability is ≤ 1/Rmax or only keeps the Rmax most probable ones, |R(t)| ≤",5.1. Pruning the run-length distribution,[0],[0]
"Rmax (Adams & MacKay, 2007).",5.1. Pruning the run-length distribution,[0],[0]
"A third way is Stratified Rejection Control (SRC) (Fearnhead & Liu, 2007), which Caron et al. (2012) and the current paper found to perform as well as the other approaches.",5.1. Pruning the run-length distribution,[0],[0]
"In our experiments, we prune by keeping the Rmax most probable model-specific run-lengths p(rt|mt,y1:t) for each model.",5.1. Pruning the run-length distribution,[0],[0]
"The bottleneck when updating a BVAR model in M is step I.B.1 in the pseudocode of BOCPDMS, when updating the MAP estimate c(r, t) = F (r, t)W (r, t) of the coefficient vector , where F (r, t) =",5.2. BVAR updates,[0.9616628393478991],"['The bottleneck when updating a BVAR model in M is step I.B.1 in the pseudocode of BOCPDMS, when updating the MAP estimate c(r, t) = F (r, t)W (r, t) of the coefficient vector , where F (r, t) = (X ′(t−r):tX(t−r):t +V c̃) −1 and W (r, t) = X ′(t−r):tY (t−r):t for all r ∈ R(t).']"
"(X ′(t−r):tX(t−r):t +V c̃) −1 and W (r, t) = X ′(t−r):tY (t−r):t for all r ∈ R(t).",5.2. BVAR updates,[0],[0]
"Since W (r, t) = W (r − 1, t− 1) +X ′tY t, updates are O(kS).",5.2. BVAR updates,[0],[0]
"F (r − 1, t − 1) can be updated to F (r, t) using rank-k updates to its QR-decomposition in O(k3) or using Woodbury’s formula, in O(S3), implying an overall complexity of O(|R(t)|min{k3, S3}) at time t.",5.2. BVAR updates,[0],[0]
"Define kmax as the largest number of regressors of any BVAR model inM. From the previous paragraphs, it follows that if all models in M are BVARS, the overhead C = dN/|M|e · min{k3max, S3} is time-constant.",5.3. Comparison with GP-based approaches,[0],[0]
"Thus, BOCPDMS runs in O(TRmax) on T observations.",5.3. Comparison with GP-based approaches,[0],[0]
"In contrast, the models of Saatçi",5.3. Comparison with GP-based approaches,[0],[0]
et al. (2010) run in O(TR3max).,5.3. Comparison with GP-based approaches,[0],[0]
"The experiments in section 6 confirm this: Using the software of Turner (2012) on the Nile data, fitting one ARGPCP model takes 42 seconds compared to 12 seconds when fitting three models in BOCPDMS, so a BVAR model is > 10× faster to process.",5.3. Comparison with GP-based approaches,[0],[0]
"Per inferred parameter, BOCPDMS is > 60× faster than ARGPCP; and this factor is much larger for multivariate data (e.g., > 270 for the 30 Portfolio data).",5.3. Comparison with GP-based approaches,[1.0],"['Per inferred parameter, BOCPDMS is > 60× faster than ARGPCP; and this factor is much larger for multivariate data (e.g., > 270 for the 30 Portfolio data).']"
More details on the run-times can be found in the Appendix.,5.3. Comparison with GP-based approaches,[0],[0]
We evaluate performance with code available from https://github.com/alan-turing-institute/bocpdms in two parts.,6. Experimental results,[0],[0]
"First, we compare to benchmark performances of GP-based models on real world data reported by Saatçi",6. Experimental results,[0],[0]
et al. (2010).,6. Experimental results,[0],[0]
"This shows that as implied by Thm. 1, VARS are excellent approximations for a large variety of data streams.",6. Experimental results,[0],[0]
"Next, we showcase BOCPDMS’ novelty in the multivariate setting.",6. Experimental results,[0],[0]
"We use uniform model priors q, a constant Hazard functions H and gradient descent for hyperparameter optimization as in Section 4.",6. Experimental results,[0],[0]
"The lag lengths of models inM are chosen based on Thm. 1 (3) and the rates of Hannan & Kavalieris (1986) for BVARS and Bayesian Autoregressions
(BARS), respectively.",6. Experimental results,[0],[0]
As in Saatçi,6.1. Comparison with GP-based approaches,[0],[0]
"et al. (2010), ARGPCP will refer to the non-linear GP-based AR model, GPTSCP to the timedeterministic model, and NSGP to the non-stationary GP allowing hyper-parameters to change at every CP. Saatçi",6.1. Comparison with GP-based approaches,[0.9931776251140707],"['As in Saatçi et al. (2010), ARGPCP will refer to the non-linear GP-based AR model, GPTSCP to the timedeterministic model, and NSGP to the non-stationary GP allowing hyper-parameters to change at every CP.']"
"et al. (2010) compute the mean squared error (MSE) as well as the negative log predictive likelihood (NLL) of the onestep-ahead predictions for three data sets: The water height of the Nile between 622−1284 AD, the snowfall in Whistler (Canada) over a 37 year period and the 3-dimensional time series (x-, y-coordinate and headangle) of a honey bee during a waggle dance sequence.",6.1. Comparison with GP-based approaches,[0],[0]
"In Turner (2012), all of the models except NSGP were also compared on daily returns for 30 industry portfolios from 1975 − 2008.",6.1. Comparison with GP-based approaches,[0],[0]
"In Table 1, BOCPDMS is compared to these benchmarks forM consisting of BAR and SSBVAR models.
",6.1. Comparison with GP-based approaches,[0],[0]
6.1.1.,6.1. Comparison with GP-based approaches,[0],[0]
"DESIGNINGM
Both the Nile and the snowfall data are univariate, soM consists of BARS with varying lag lengths.",6.1. Comparison with GP-based approaches,[0],[0]
"For the 3- dimensional bee data,M additionally contains unrestricted BVARS.",6.1. Comparison with GP-based approaches,[0],[0]
"Lastly, SSBVARS are used on the 30 Portfolio data.",6.1. Comparison with GP-based approaches,[0],[0]
"Two neighbourhood systems are constructed from distances in the spaces of pairwise contemporaneous correlations and autocorrelations prior to 1975, a third using the Standard Industrial Classification (SIC), with Π(·) decreasing linearly.",6.1. Comparison with GP-based approaches,[0],[0]
"Predictive performance and fit: In terms of MSE, BOCPDMS clearly outperforms all GP-models on multivariate data.",6.1.2. FINDINGS,[0],[0]
"Even on univariate data, the only exception to this is the snowfall data, where NSGP does better.",6.1.2. FINDINGS,[0],[0]
"However, NSGP requires grid search or Hamiltonian Monte Carlo sampling for hyperparameter optimization at each obser-
vation (Saatçi et al., 2010).",6.1.2. FINDINGS,[0],[0]
"Overall, there are three main reasons why BOCPDMS performs better: Firstly, being able to change lag lengths between CPS seems more important to predictive performance than being able to model non-linear dynamics.",6.1.2. FINDINGS,[0],[0]
"Secondly, unlike the GP-models, we allow the time series to communicate via {ALl }.",6.1.2. FINDINGS,[0],[0]
"Thirdly, the hyperparameters of the GP have a strong influence on inference.",6.1.2. FINDINGS,[0],[0]
"In particular, the noise variance σ is treated as a hyperparameter and optimized via type-II ML.",6.1.2. FINDINGS,[0],[0]
"Except for the NSGP, this is only done during a training period.",6.1.2. FINDINGS,[0],[0]
"Thus, the GP-models cannot adapt to the observations after training, leading to overconfident predictive distributions that are too narrow (see Turner, 2012, p. 172).",6.1.2. FINDINGS,[0],[0]
"This in turn leads them to be more sensitive to outliers, and to mislabel them as CPS.",6.1.2. FINDINGS,[0],[0]
"In contrast, (10a)–(10d) models σ as part of the inferential Bayesian hierarchy, and hyperparameter optimization is instead applied at one level higher.",6.1.2. FINDINGS,[0],[0]
"Consequently, our predictive distributions are wider, and the algorithm is less confident about the next observations, making it more robust to outliers.",6.1.2. FINDINGS,[0],[0]
"This is also responsible for the overall smaller standard errors of the GP-models in Table 1, since the GPS interpret outliers as CPS and immediately adapt to short-term highs or lows.
",6.1.2. FINDINGS,[0],[0]
"CP Detection: A good demonstration of this finding is the Nile data set, where the MAP segmentation finds a single CP, corresponding to the installation of the nilometer
around 715 CE, see Fig 5.",6.1.2. FINDINGS,[0],[0]
"In contrast, Saatçi",6.1.2. FINDINGS,[0],[0]
et al. (2010) report 18 additional CPS corresponding to outliers.,6.1.2. FINDINGS,[0],[0]
"The same phenomenon is also reflected in the run-length distribution (RLD): While the probability mass in Figs. 3, 4 and 5 are spread across the retained run-lengths, the RLD reported in Saatçi",6.1.2. FINDINGS,[0],[0]
et al. (2010) is more concentrated and even degenerate for the 30 Portfolio data set.,6.1.2. FINDINGS,[0],[0]
"On the other hand, such enhanced sensitivity to change can be advantageous.",6.1.2. FINDINGS,[0],[0]
"For instance, in the bee waggle dance, the GP-based techniques are better at identifying the true CPS.",6.1.2. FINDINGS,[0],[0]
The reason is twofold:,6.1.2. FINDINGS,[0],[0]
"Firstly, the variance for the bee waggle data is homogeneous across time, so treating it as fixed helps inference.",6.1.2. FINDINGS,[0],[0]
"Secondly, the CPS in this data set are subtle, so having narrower predictive distributions is of great help in detecting them.",6.1.2. FINDINGS,[0],[0]
"However, it adversely affects performance when changes in the error variance are essential, as for financial data: In particular, BOCPDMS finds the ground truths labelled in Saatçi",6.1.2. FINDINGS,[0],[0]
"et al. (2010), and discovers even more, see Fig. 3.",6.1.2. FINDINGS,[0],[0]
This is especially apparent in times of market turmoil where changes in the variance of returns are significant.,6.1.2. FINDINGS,[0],[0]
We show this using the example of the subprime mortgage financial crisis: While the RLD of Saatçi,6.1.2. FINDINGS,[0],[0]
"et al. (2010) identified only 2 CPS with ground truth and a third unlabelled one during the height of the crisis, BOCPDMS detects a large number of CPS corresponding to ground truths, see Fig. 4.
",6.1.2. FINDINGS,[0],[0]
"Lastly, we note that segmentations obtained off-line for both the bee waggle dance and the 30 Portfolios are reported in Xuan & Murphy (2007).",6.1.2. FINDINGS,[0],[0]
"Compared to the on-line segmentations produced by BOCPDMS, these are closer to the truth for the bee waggle data, but not for the 30 Portfolio data set.
",6.1.2. FINDINGS,[0.9508246004470421],"['For the bee waggle and the 30 Portfolio data set, BVARS are preferred to BARS.']"
"Model selection: In most of the experiments where abrupt changes model the non-stationarity well, the model posterior is fairly concentrated and periods of model uncertainty are short.",6.1.2. FINDINGS,[0],[0]
"This is different when changes are slower, see Fig. 6.",6.1.2. FINDINGS,[0.9572471365080232],"['This is different when changes are slower, see Fig.']"
"The implicit model complexity penalization Bayesian model selection performs provides BOCPDMS with an Occam’s
Razor mechanism: Simple models are typically favoured until evidence for more complex dynamics accumulates.",6.1.2. FINDINGS,[1.0000000442511354],['The implicit model complexity penalization Bayesian model selection performs provides BOCPDMS with an Occam’s Razor mechanism: Simple models are typically favoured until evidence for more complex dynamics accumulates.']
"For the bee waggle and the 30 Portfolio data set, BVARS are preferred to BARS.",6.1.2. FINDINGS,[0],[0]
"For the 30 Portfolio data, the MAP segmentation only selects SSBVARS with neighbourhoods constructed from contemporaneous correlation and autocorrelations.",6.1.2. FINDINGS,[1.0],"['For the 30 Portfolio data, the MAP segmentation only selects SSBVARS with neighbourhoods constructed from contemporaneous correlation and autocorrelations.']"
"Neighbourhoods using SIC codes are not selected, reflecting that this classification from 1937 is out of date.",6.1.2. FINDINGS,[0],[0]
European Temperature:,6.2. Performance on spatio-temporal data,[0],[0]
Monthly temperature averages 01/01/1880− 01/01/2010 for the 21 longest-running stations across Europe are taken from http://www.ecad.eu/.,6.2. Performance on spatio-temporal data,[0],[0]
We adjust for seasonality by subtracting monthly averages for each station.,6.2. Performance on spatio-temporal data,[0],[0]
"Station longitudes and latitudes are available, so N(S) is based on concentric rings around the stations using Euclidean distances.",6.2. Performance on spatio-temporal data,[0],[0]
"Two different decay functions Π(·),Π+(·) are used, with Π+(·) using larger neighbourhoods and slower decaying.",6.2. Performance on spatio-temporal data,[0],[0]
Temperature changes are poorly modeled by CPS and more likely to undergo slow transitions.,6.2. Performance on spatio-temporal data,[0],[0]
Fig. 6 shows the way in which the model posterior captures such longer periods of change in dynamics.,6.2. Performance on spatio-temporal data,[0],[0]
The values on the bottom panel are calculated by considering m̂t = arg maxmt∈M p(mt|y1:t) as |M|-dimensional multinomial random variable.,6.2. Performance on spatio-temporal data,[0],[0]
"Its Standardized Generalized Variance (SGV) (Wilks, 1960; SenGupta, 1987) is calculated as |M|-th root of the covariance matrix determinant.",6.2. Performance on spatio-temporal data,[0],[0]
We plot the log of the SGV computed using the model posteriors for the last 8 years.,6.2. Performance on spatio-temporal data,[0],[0]
"This provides an informative summary of the model posterior dispersion.
",6.2. Performance on spatio-temporal data,[0],[0]
"Air Pollution: Finally, we analyze Nitrogen Oxide (NOX) observed at 29 locations across London 17/08/2002",6.2. Performance on spatio-temporal data,[0.9866289464882005],"['Air Pollution: Finally, we analyze Nitrogen Oxide (NOX) observed at 29 locations across London 17/08/2002 − 17/08/2003.']"
− 17/08/2003.,6.2. Performance on spatio-temporal data,[0],[0]
"The quarterhourly measurements are aver-
aged over 24 hours.",6.2. Performance on spatio-temporal data,[0],[0]
Weekly seasonality is accounted for by subtracting week-day averages for each station.,6.2. Performance on spatio-temporal data,[1.0],['Weekly seasonality is accounted for by subtracting week-day averages for each station.']
M is populated with SSBVAR models whose neighbourhoods are constructed from both road- and Euclidean distances.,6.2. Performance on spatio-temporal data,[0],[0]
"As 17/02/2003 marks the introduction of London’s first ever congestion charge, we find structural changes in the dynamics around that date.",6.2. Performance on spatio-temporal data,[1.0],"['As 17/02/2003 marks the introduction of London’s first ever congestion charge, we find structural changes in the dynamics around that date.']"
A model with shorter lag length but identical neighbourhood structure is preferred after the congestion charge.,6.2. Performance on spatio-temporal data,[0],[0]
"In Fig. 7, Bayes Factors (BFS) capture the shift: Kass & Raftery (1995) classify logs of BFS as very strong evidence if their absolute value exceeds 5.",6.2. Performance on spatio-temporal data,[0],[0]
"We have extended Bayesian On-line Changepoint Detection (BOCPD) to multiple models by generalizing Fearnhead & Liu (2007) and Adams & MacKay (2007), arriving at BOCPDMS.",7. Conclusion,[0],[0]
"For inference in multivariate data streams, we propose BVARS with closed form distributions that have strong theoretical guarantees summarized in Thm. 1.",7. Conclusion,[0],[0]
"We sparsify BVARS based on neighbourhood systems, thus making BOCPDMS especially amenable to spatio-temporal inference.",7. Conclusion,[0],[0]
"To demonstrate the power of the resulting framework, we apply it to multivariate real world data, outperforming the state of the art.",7. Conclusion,[0],[0]
"In future work, we would like to add and remove models from M on-line.",7. Conclusion,[1.0],"['In future work, we would like to add and remove models from M on-line.']"
This could lower the computational cost for the case where |M| is significantly larger than the number of threads.,7. Conclusion,[0],[0]
We want to thank N. Karampatziakis for his help with making the method computationally more efficient.,Acknowledgements,[0],[0]
JK is funded by EPSRC grant EP/L016710/1.,Acknowledgements,[0],[0]
"Further, this work was supported by The Alan Turing Institute for Data Science and AI under EPSRC grant EP/N510129/1 and the Lloyds Register Foundation programme on Data Centric Engineering.",Acknowledgements,[0],[0]
Bayesian On-line Changepoint Detection is extended to on-line model selection and nonstationary spatio-temporal processes.,abstractText,[0],[0]
We propose spatially structured Vector Autoregressions (VARS) for modelling the process between changepoints (CPS) and give an upper bound on the approximation error of such models.,abstractText,[0],[0]
"The resulting algorithm performs prediction, model selection and CP detection on-line.",abstractText,[0],[0]
"Its time complexity is linear and its space complexity constant, and thus it is two orders of magnitudes faster than its closest competitor.",abstractText,[0],[0]
"In addition, it outperforms the state of the art for multivariate data.",abstractText,[0],[0]
Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection ,title,[0],[0]
