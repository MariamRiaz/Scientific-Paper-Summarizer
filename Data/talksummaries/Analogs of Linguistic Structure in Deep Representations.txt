2	51	More generally, any encoder–decoder model (Sutskever et al., 2014) can be viewed as implementing an analogous communication protocol, with the input encoding playing the role of a message in an artificial “language” shared by the encoder and decoder (Yu et al., 2016).
3	53	Earlier work has found that under suitable conditions, these protocols acquire simple interpretable lexical (Dircks and Stoness, 1999; Lazaridou et al., 2016) and sequential structure (Mordatch and Abbeel, 2017), even without natural language training data.
5	107	One of the distinguishing features of natural language is compositionality: the existence of operations like negation and coordination that can be applied to utterances with predictable effects on meaning.
6	34	RNN models trained for natural language processing tasks have been found to learn representations that encode some of this compositional structure—for example, sentence representations for machine translation encode explicit features for certain syntactic phenomena (Shi et al., 2016) and represent some semantic relationships translationally (Levy et al., 2014).
8	20	Rather than using language as a form of supervision, we propose to use it as a probe—exploiting post-hoc statistical correspondences between natural language descriptions and neural encodings to discover regular structure in representation space.
12	15	We investigate a communication game previously studied by FitzGerald et al. (2013), and make two discoveries: in a model trained without any access to language data, 1.
17	38	In this game, the speaker observes (1) a world W of 1–20 objects labeled with with attributes and (2) a designated target subset X of objects in the world.
19	28	The GENX dataset collected for this purpose contains 4170 human-generated natural-language referring expressions and corresponding logical forms for 273 instances of this game.
23	46	The encoder is a single-layer RNN with GRU cells (Cho et al., 2014) that consumes both the input world and target labeling and outputs a 64-dimensional hidden representation.
26	16	The decoder makes an independent labeling decision about every object in W (taking as input both f and a feature representation of a particular object Wi).
31	32	But how do we judge semantic equivalence between natural language and vector representations?
36	17	Instead, we sample a collection of alternative worlds {Wi} observed elsewhere in the dataset, and compute a tabular meaning representation rep(e) = {JeKWi} by evaluating e in each world Wi.
37	17	We similarly compute rep(f) = {JfKWi}, allowing the learned decoder model to play the role of logical evaluation for message vectors.
39	28	It additionally allows us to compute softer notions of equivalence by measuring agreement on individual worlds or objects.
40	13	We begin with the simplest question we can answer with this tool: how often do the messages generated by the encoder model have the same meaning as messages generated by humans for the same context?
43	43	Or does it behave in a way indistinguishable from a random classifier?
44	133	For each scene in the GENX test set, we compute the model-generated message f and its tabular representation rep(f), and measure the extent to which this agrees with representations produced by three “theories” of model behavior (Figure 2): (1) a random theory that accepts or rejects objects with uniform probability, (2) a literal theory that predicts membership only for objects that exactly match some object in the original target set, and (3) a human theory that predicts according to the most frequent logical form associated with natural language descriptions of the target set (as described in the preceding section).
45	15	We evaluate agreement at the level of individual objects, worlds, and full tabular meaning representations.
57	49	Conversely, if there is a first-class notion of negation, we should be able to select an arbitrary representation vector f with an associated referring expression e, apply some transformation N to f , and be able to predict a priori how the decoder model will interpret the representation Nf—i.e. in correspondence with ¬e.
58	29	Here we make the strong assumption that the negation operation is not only predictable but linear.
64	27	We conclude that the estimated linear operator N̂ is analogous to negation in natural language.
65	143	Indeed, the behavior of this operator is readily visible in Figure 3: predicted negated forms (in red) lie close in vector space to their true values, and negation corresponds roughly to mirroring across a central point.
66	17	In our final experiment, we explore whether the same kinds of linear maps can be learned for the binary operations of conjunction and disjunction.
67	47	As in the previous section, we collect examples from the training data of representations whose denotations are known to correspond to groups of logical forms in the desired relationship—in this case tuples (e, f, e′, f ′, e′′, f ′′), where rep(e) = rep(f), rep(e′) = rep(f ′), rep(e′′) = rep(f ′′) and either e′′ = e ∧ e′ (conjunction) or e′′ = e ∨ e′ (disjunction).
71	31	At the same time, the estimated operators are clearly capturing some structure: in the case of disjunction, for example, model interpretations are correctly modeled by the logical form 92% of the time at the object level and 19% of the time at the denotation level.
72	83	This suggests that the operations of conjunction and disjunction do have some functional counterparts in the RNN language, but that these functions are not everywhere well approximated as linear.
73	28	Building on earlier tools for identifying neural codes with natural language strings, we have presented a technique for exploring compositional structure in a space of vector-valued representations.
74	28	Our analysis of an encoder–decoder model trained on a reference game identified a number of language-like properties in the model’s representation space, including transformations corresponding to negation, disjunction, and conjunction.
75	88	One major question left open by this analysis is what happens when multiple transformations are applied hierarchically, and future work might focus on extending the techniques in this paper to explore recursive structure.
76	46	We believe our experiments so far highlight the usefulness of a denotational perspective from formal semantics when interpreting the behavior of deep models.
