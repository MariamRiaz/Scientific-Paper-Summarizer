{"sections": [{"text": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of nonconvex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projectionfree algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal O( \u221a T ) adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an O(T 2/3) stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel \u201clifting\u201d framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments.\n1Yale Institute for Network Science, Yale University, New Haven, CT, USA 2Department of Electrical Engineering, Yale University 3Department of Computer Science, Yale University 4Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA. Correspondence to: Lin Chen <lin.chen@yale.edu>."}, {"heading": "1. Introduction", "text": "As the amount of collected data becomes massive in both size and complexity, algorithm designers are faced with unprecedented challenges in statistics, machine learning, and control. In the past decade, online optimization has provided a successful computational framework for tackling a wide variety of challenging problems, ranging from non-parametric regression to portfolio management (Calandriello et al., 2017; Agarwal et al., 2006). In online optimization, a large or complex optimization problem is broken down into a sequence of smaller optimization problems, each of which must be solved with limited information. This framework captures many real-world scenarios in which standard optimization theory does not apply. For instance, a machine learning application cannot feasibly process terabytes of data at a single time; rather, subsets of data may be handled in a sequential fashion. Another example is when the true objective function is the expectation of an unknown distribution of functions, and may only be accessible via samples, as is the case for problems in online learning and control theory (Xiao, 2010; Wang & Boyd, 2008).\nOnline convex optimization, a branch of online optimization that considers sequentially minimizing convex functions, has proved particularly useful for statistical and machine learning applications. Online convex optimization has enjoyed much success in these areas because most offline machine learning techniques utilize the existing theory of convex optimization. As in the offline setting, gradient methods are a popular class of algorithms for online convex optimization due to their simplicity; however, they require projections onto the constraint set, which involve solving a quadratic program in the general case. These projections are infeasible for large scale applications with complicated constraints such as matrix completion, network routing problems, and maximum matchings. Online projection-free methods have been proposed and are much more efficient , replacing a projection onto the constraint set with a linear optimization over the constraint set at each iteration (Hazan & Kale, 2012; Garber & Hazan, 2013). However, these projection-free methods require exact gradient computations, which may be prohibitively expensive for even moderately sized data sets and intractable when a ar X iv :1 80 2.\n08 18\n3v 4\n[ st\nat .M\nL ]\n1 4\nJu n\n20 18\nclosed form does not exist. Thus, there is a huge need for online convex optimization routines that are projection-free and also robust to stochastic gradient estimates.\nWhile convex programs may be efficiently solved (at least in theory), there is a growing number of non-convex problems arising in machine learning and statistics. Notable examples include nonnegative principle component analysis, low-rank matrix recovery, sigmoid loss functions for binary classification, and the training of deep neural networks, to name a few. Understanding which types of non-convex functions may be efficiently optimized and developing techniques for doing so is a pressing research question for both theory and practice. Recently, continuous DR-submodular functions have been proposed as a broad class of non-convex functions which admit efficient approximate maximization routines, even though exact maximization is NP-Hard (Bian et al., 2017). These functions capture many real-life applications, such as optimal experiment design, non-definite quadratic programming, coverage and diversity functions, and continuous relaxation of discrete submodular functions. Recent works (Chen et al., 2018) have proposed methods for online continuous DR-submodular optimization; however, these too require either expensive projections or exact gradient computations.\nOur contributions In this paper, we present a suite of projection-free algorithms for online optimization that use stochastic estimates of the gradient and leverage the averaging technique (Mokhtari et al., 2018a;b) to reduce their variance. This includes\n\u2022 Meta-Frank-Wolfe, the first projection-free algorithm for adversarial online optimization which requires only stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves optimal O( \u221a T ) regret and (1\u2212 1/e)-regret\nbounds for convex and submodular optimization, respectively.\n\u2022 One-Shot Frank-Wolfe, a simpler projection-free algorithm for stochastic online optimization which requires only a single stochastic gradient estimate in each round. This simpler algorithm achieves O(T 2/3) regret and (1 \u2212 1/e)-regret bounds for the convex and submodular case, respectively.\n\u2022 A novel class of algorithms for online discrete submodular optimization which are based on lifting discrete functions to the continuous domain, applying our methods with an extremely efficient sampling technique, and using rounding schemes to produce a discrete solution.\nFinally, to demonstrate the effectiveness of our algorithms,\nwe tested their performance on an extensive set of experiments and measured against common baselines."}, {"heading": "2. Related Work", "text": "The Frank-Wolfe algorithm, also known as the conditional gradient descent, was originally proposed for the offline setting in (Frank & Wolfe, 1956). The framework of online convex optimization was introduced by Zinkevich (2003), in which the online projected gradient descent was proposed and proved to achieve an O( \u221a T ) regret bound. However, the projections required for such an algorithm are too expensive for many large-scale online problems. The online conditional gradient descent was the first projection-free online algorithm, originally proposed in (Hazan & Kale, 2012). An improved conditional gradient algorithm was later designed for smooth and strongly convex optimization which achieves the optimal O( \u221a T ) adversarial regret bound (Garber & Hazan, 2013). However, both of these algorithms can perform arbitrarily poorly if supplied with stochastic gradient estimates. Lafond et al. (2015) proposed an online Frank-Wolfe variant for the any-time stochastic online setting that converges to a stationary point for nonconvex expected functions . While convergence is an important property of the any-time methods, arbitrary stationary points do not yield approximation guarantees for general non-convex functions.\nJohnson & Zhang (2013) introduced the variance reduction technique for accelerating stochastic gradient descent. It was independently discovered by Mahdavi et al. (2013). Allen-Zhu & Hazan (2016) applied this technique to nonconvex optimization. Hazan & Luo (2016) devised a projection-free stochastic convex optimization algorithm based on this technique. Mokhtari et al. (2018a;b) proposed the first sample-efficient variance reduction technique for projection-free algorithms that does not require increasing batch sizes. Their method achieves the tight (1 \u2212 1/e) approximation guarantee for monotone and continuous DRsubmodular functions. Although these variance reduction techniques have enjoyed success in the offline setting, they have yet to be as extensively applied in the online setting that we consider in this paper.\nIn the discrete domain, Streeter & Golovin (2009) studied the online maximization problem of monotone submodular set functions subject to a knapsack constraint and introduced the meta-action technique. In a celebrated work, Calinescu et al. (2011) proposed an (offline) method for maximizing monotone submodular set functions subject to a matroid constraint by working in the continuous domain via the multilinear extension, then rounding the fractional solution. By combining the meta-action and lifting techniques, Golovin et al. (2014) presented an algorithm whose (1\u2212 1/e)-regret is bounded by O( \u221a T ). The lifting method therein relies\non an expensive sampling procedure that does not scale favorably to large applications.\nBach (2015) demonstrated connections between continuous submodular functions and convex functions in the context of minimization. Building upon the continuous greedy algorithm of (Calinescu et al., 2011), Bian et al. (2017) proposed an algorithm that achieves a (1\u2212 1/e)-approximation guarantee for maximizing monotone continuous DR-submodular functions subject to down-closed convex constraints. Projected gradient methods were investigated in (Hassani et al., 2017) and were shown to attain a 1/2-approximation ratio for monotone continuous DR-submodular functions. Very recently, Chen et al. (2018) borrowed the idea of metaaction (Streeter & Golovin, 2009) and proposed several online algorithms for maximizing monotone continuous DR-submodular functions. However, each of these methods either requires an expensive projection step at each iteration or cannot handle stochastic gradient estimates."}, {"heading": "3. Preliminaries", "text": "In this work, we are interested in optimizing two classes of functions, namely convex and continuous DR-submodular. To begin defining continuous submodular functions, we first recall the definition of a submodular set function. A real-valued set function f : 2\u2126 \u2192 R+ is submodular if\nf(A) + f(B) \u2265 f(A \u222aB) + f(A \u2229B)\nfor all A,B \u2282 \u2126. The notion of submodularity has been extended to continuous domains (Wolsey, 1982; Vondra\u0301k, 2007; Bach, 2015). Consider a function f : X \u2192 R+ where the domain is of the form X = \u220fn i=1 Xi and each Xi is a compact subset of R+. We say that f is continuous submodular if f is continuous and for all x,y \u2208 X , we have f(x) + f(y) \u2265 f(x \u2228 y) + f(x \u2227 y) where x \u2228 y and x \u2227 y are component-wise maximum and minimum, respectively. Note that we have defined both discrete and continuous functions to be nonnegative on their respective domains. For efficient maximization, we also require that these functions satisfy a diminishing returns condition (Bian et al., 2017). We say that f is continuous DR-submodular if f is differentiable and\n\u2207f(x) \u2265 \u2207f(y)\nfor all x \u2264 y. The main attraction of continuous DRsubmodular functions is that they are concave in positive directions; that is, for all x \u2264 y,\nf(y) \u2264 f(x) + \u3008\u2207f(x),y \u2212 x\u3009\n(Calinescu et al., 2011; Bian et al., 2017). A function f is monotone if f(x) \u2264 f(y) for all x \u2264 y. A function f is L-smooth if \u2016\u2207f(x)\u2212\u2207f(y)\u2016\u2264 L\u2016x\u2212 y\u2016 for all x,y.\nWe now provide a brief introduction to online optimization, referring the interested reader to the excellent survey of (Hazan et al., 2016). In the online setting, a player seeks to iteratively optimize a sequence of functions f1, . . . fT over T rounds. In each round, a player must first choose a point xt from the constraint set K. After playing xt, the value of ft(xt) is revealed to the player, along with access to the gradient\u2207f . Although the player does not know the function ft while choosing xt, they may use information of previously seen functions to guide their choice. The situation where an arbitrary sequence of functions f1, . . . , fT is presented is known as the adversarial online setting. In the adversarial setting, the goal of the player is to minimize adversarial regret, which is defined as\nRT , T\u2211\nt =1\nft(xt)\u2212 inf x \u2208K T\u2211 t =1 ft(x)\nfor minimization problems and analogously defined for maximization problems. Intuitively, a player\u2019s regret is low if the accumulated value of their actions over the T rounds is close to that of the single best action in hindsight. Indeed, this is a natural framework for data-intensive applications where the entire data may not fit onto a single disk and thus needs to be processed in T batches. The algorithm designer would like to devise a scheme to process the T batches separately in a way that is competitive with the best single disk solution.\nA slightly different formulation known as stochastic online setting is when the functions are chosen i.i.d. from some unknown distribution ft \u223c D. In this case, the player seeks to minimize stochastic regret, which is defined as\nSRT , T\u2211\nt =1\nf(xt)\u2212 T \u00b7 inf x \u2208K f(x)\nwhere f(x) = Eft\u223cD[ft(x)] denotes the expected function. This is a natural framework for many statistical and machine learning applications, such as empirical risk minimization, where the true objective is unknown but pairs of data points and labels are sampled. While the stochastic setting appears \u201ceasier\u201d than the adversarial setting (in the sense that any strategy for the adversarial settings applies to stochastic settings and obtains a potentially lower regret), the strategies designed for the stochastic setting may be much simpler and more computationally efficient. For both adversarial and stochastic settings, a strategy that achieves a regret that is sublinear in T is considered good andO( \u221a T ) regret bounds are optimal for convex functions in both settings. Although convex programs can be efficiently solved to high accuracy, general non-convex programs cannot be efficiently exactly optimized, thus necessitating another definition of regret.\nThe \u03b1-regret is defined as\n\u03b1-RT , \u03b1 sup x \u2208K T\u2211 t =1 ft(x)\u2212 T\u2211 t =1 ft(xt)\nfor adversarial maximization problems, and may be analogously extended to other scenarios. Intuitively, \u03b1-regret compares a player\u2019s actions with the best \u03b1-approximation to the optimal solution in hindsight. This is appropriate when the objective functions do not admit efficient optimization routines, but do admit constant-factor approximations, as is the case with continuous DR-submodular functions.\nNearly all optimization methods for both offline and online settings use first order information of the objective function; however, exact gradient computations can be costly, especially when the objective function is only readily expressed as a large sum of individual functions or is itself an expectation over an unknown distribution. In this case, stochastic estimates are usually much more computationally efficient to obtain via sampling or simulation. In this work, we assume that once a function ft is revealed, the player gains oracle access to unbiased stochastic estimates of the gradient, rather than the exact gradient. More precisely, the player may query the oracle to obtain a random linear function \u2207\u0303f(x) such that E[\u2207f(x)\u2212\u2207\u0303f(x)] = 0 for all x. This computational model captures commonly used mini-batch methods for estimating gradients, among other examples. In this work, we make a few main assumptions that allow our algorithms to be analyzed.\nAssumption 1. The constraint setK is convex and compact, with diameter D = supx,y\u2208K\u2016x \u2212 y\u2016 and radius R = supx\u2208K\u2016x\u2016.\nAssumption 2. In the adversarial setting, each function ft is L-smooth and in the stochastic setting, the expected function f is L-smooth.\nAssumption 3. In the adversarial setting, the gradient oracle is unbiased E[\u2207ft(x)\u2212\u2207\u0303ft(x)] = 0 and has a bounded variance E[\u2016\u2207ft(x)\u2212\u2207\u0303ft(x)\u20162] \u2264 \u03c32 for all points x and functions ft. In the stochastic setting, the gradient oracle is unbiased E[\u2207f(x) \u2212 \u2207\u0303ft(x)] = 0 and has a bounded variance E[\u2016\u2207f(x)\u2212 \u2207\u0303ft(x)\u20162] \u2264 \u03c32 for all points x and functions ft.\nWe remark that in the stochastic setting and under mild regularity conditions, unbiasedness of the gradients E[\u2207ft(x)\u2212 \u2207\u0303ft(x)] = 0 implies unbiasedness E[\u2207f(x)\u2212\u2207\u0303ft(x)] = 0 in Assumption 3 because f(x) = Eft\u223cD[ft(x)] Moreover, upper bounds on the variance terms E[\u2016\u2207f(x) \u2212 \u2207ft(x)\u20162] \u2264 \u03c32a and E[\u2016\u2207ft(x)\u2212 \u2207\u0303ft(x)\u20162] \u2264 \u03c32b yield a variance bound of E[\u2016\u2207f(x) \u2212 \u2207\u0303ft(x)\u20162] \u2264 \u03c32a + \u03c32b , by the triangle inequality."}, {"heading": "4. Main Results", "text": "We now present two algorithms for online optimization of convex and continuous DR-submodular functions in the adversarial and stochastic settings. Unlike previous work, these methods are projection-free and require only stochastic estimates of the gradients, rather than exact gradient computations. In both algorithms, the main computational primitive is linear optimization over a compact convex set. In addition, we remark that both algorithms can be converted into an anytime algorithm that does not require the knowledge of the horizon T via the doubling trick; see Section 2.3.1 of (Shalev-Shwartz et al., 2012)."}, {"heading": "4.1. Adversarial Online Setting", "text": "Algorithm 1 combines the recent variance reduction technique of (Mokhtari et al., 2018a) along with the use of online linear optimization oracles to minimize the regret in each round. An online linear optimization oracle is an instance of an online linear optimization (minimization/maximization in the convex/DR-submodular setting, respectively) algorithm that optimizes linear objectives in a sequential manner. Both the variance reduction in the stochastic gradient estimates and the online linear oracles are crucial in the algorithm, as just one technique is not enough to get sublinear regret bounds in the adversarial setting. At a high level, our algorithm produces iterates xt by running K steps of a Frank-Wolfe procedure, using an average of previous gradient estimates and linear online optimization oracles in place of exact optimization of the true gradient. After a point xt is played in round t, our algorithm queries the gradient oracle \u2207\u0303ft at K points. Then, the gradient estimates are averaged with those from previous rounds and fed as objective functions into K linear online optimization oracles. The K points chosen by the oracles are used as iterates in a full K-step Frank-Wolfe subroutine to obtain the next point xt+1. A formal description is provided in Algorithm 1.\nThere are only a few differences in Algorithm 1 for convex and submodular optimization. First, the online oracles should be minimizing in the case of convex optimization and maximizing in the case of submodular optimization. Second, the initial point x1 may be any point in K for convex problems but should be set to 0 for submodular problems (even if K is not down-closed). Finally, the update rule is\nx (k+1) t \u2190 (1\u2212 \u03b7k)x (k) t + \u03b7kv (k) t\nfor convex problems and\nx (k+1) t \u2190 x (k) t + \u03b7kv (k) t\nfor submodular problems. We now provide a formal regret bound.\nAlgorithm 1 Meta-Frank-Wolfe Input: convex set K, time horizon T , linear optimization\noracles E(1) . . . E(K), step sizes \u03c1k \u2208 (0, 1) and \u03b7k \u2208 (0, 1), and initial point x1 Output: {xt : 1 \u2264 t \u2264 T} 1: Initialize online linear optimization oracles E(1) . . . E(K)\n2: Initialize d(0)t = 0 and x (1) t = x1 3: for t\u2190 1, 2, 3, . . . , T do 4: v(k)t \u2190 output of oracle E(k) in round t\u2212 1 5: x(k+1)t \u2190 update(x (k) t ,v (k) t , \u03b7k) for k = 1 . . .K 6: Play xt = x (K+1) t , then obtain value ft(xt) and unbiased oracle access to\u2207ft 7: d(k)t \u2190 (1 \u2212 \u03c1k)d (k\u22121) t + \u03c1k\u2207\u0303ft(x (k) t ) for k = 1 . . .K 8: Feedback \u3008v(k)t ,d (k) t \u3009 to E(k) for k = 1 . . .K 9: end for\nTheorem 1 (Proof in Appendices B and C). Suppose Assumptions 1 - 3 hold, the online linear optimization oracles have regret at most RET , and the averaging parameters are chosen as \u03c1k = 2(k+3)2/3 . Then for convex functions f1, . . . , fT and step sizes \u03b7k = 1k+3 , the adversarial regret of Algorithm 1 is at most\n4TDQ1/2\nK1/3 +\n4T\nK\n( M + LD2\n3 log(K + 1)\n) + 4\n3 RET\nin expectation, where M = max1\u2264t\u2264T [ft(x1)\u2212 ft(x\u2217)] and Q , max{42/3 max1\u2264t\u2264T \u2016\u2207ft(x1)\u20162, 4\u03c32 + 3(LD)2/2}. For monotone continuous DR-submodular functions f1, . . . , fT and step sizes \u03b7k = 1K , the adversarial (1\u2212 1/e)-regret of Algorithm 1 is at most\n3TDQ1/2 2K1/3 + LD2T 2K +RET\nin expectation, where Q , max{max1\u2264t\u2264T \u2016\u2207ft(x1)\u2016242/3, 4\u03c32 + 6L2R2}.\nFrom Theorem 1, we observe that by setting K = T 3/2 and choosing a projection-free online linear optimization oracle with RET = O( \u221a T ), such as Follow the Perturbed Leader (Cohen & Hazan, 2015), both regrets are bounded above by O( \u221a T ). We remark that the expectation in Theorem 1 is with respect to the stochastic gradient estimates."}, {"heading": "4.2. Stochastic Online Setting", "text": "In the stochastic online setting, where functions are sampled i.i.d. ft \u223c D, we can develop much simpler algorithms that still achieve sublinear regret. Algorithm 2 works without\ninstantiating any online linear optimization oracles and requires only a single stochastic estimate of the gradient at each round. Indeed, because the functions are not arbitrarily chosen, variance reduction along with one Frank-Wolfe step suffices to achieve a sublinear regret bound.\nAlgorithm 2 One-Shot Frank-Wolfe Input: convex set K, time horizon T , step sizes \u03c1t \u2208 (0, 1)\nand \u03b7t \u2208 (0, 1), and initial point x1 Output: {xt : 1 \u2264 t \u2264 T}\n1: d0 \u2190 0 2: for t\u2190 1, 2, 3, . . . , T do 3: Play xt, then obtain value ft(xt) and unbiased oracle access to \u2207ft 4: dt \u2190 (1\u2212 \u03c1t)dt\u22121 + \u03c1t\u2207\u0303ft(xt) 5: vt \u2190 arg maxv\u2208K\u3008dt,v\u3009 6: xt+1 \u2190 update(xt,vt, \u03b7t) 7: end for\nThe differences in Algorithm 2 for convex and submodular optimization are similar to those in Algorithm 1. Namely, the update rules are the same and the initial point x1 may be arbitrarily chosen from K for convex optimization, and set to 0 for submodular optimization.\nTheorem 2 (Proof in Appendices D and E). Suppose Assumptions 1 - 3 hold and the averaging parameters are chosen as \u03c1t = 2(t+3)2/3 . Then for a convex expected function f and step sizes \u03b7t = 1t+3 , the stochastic regret of Algorithm 1 is at most\n4M log(T + 1) + 6Q1/2DT 2/3 + 4\n3 LD2 log2(T + 3)\nin expectation, where M = f(x1) \u2212 f(x\u2217) and Q , max{42/3\u2016\u2207F (x1)\u20162, 4\u03c32 + 3(LD)2/2}. For expected functions f which are monotone continuous DR-submodular and step sizes \u03b7k = 1K , the stochastic (1 \u2212 1/e)-regret of Algorithm 2 is at most\n(1\u2212 1/e)M + 3DQ 1/2\n10 (3T 2/3 + 2T\u22121) +\nLD2\n2 .\nin expectation, where M = f(x\u2217) \u2212 f(0) and Q , max{\u2016\u2207f(0)\u2016242/3, 4\u03c32 + 6L2R2}"}, {"heading": "4.3. Lifting Methods for Discrete Online Optimization", "text": "One exciting application of our online continuous DRsubmodular optimization algorithms is a new approach for online discrete submodular optimization. While previous methods could only handle knapsack constraints (Streeter & Golovin, 2009) or required expensive sampling procedures (Golovin et al., 2014), our continuous methods can\nbe applied to the discrete setting to handle general matroid constraints and computationally cheap sampling procedures.\nSuppose f1, . . . fT are nonnegative monotone submodular set functions on a ground set \u2126 with matroid constraint I and f\u03041, . . . f\u0304T are corresponding multi-linear extensions with matroid polytope K \u2282 [0, 1]n. A discrete procedure that uses our continuous algorithm is as follows: at each round t, the online continuous algorithm produces a fractional solution xt \u2208 K, which is then rounded to a set Xt \u2208 I and played as the discrete solution. The value ft(Xt) is revealed and the player is granted access to the discrete function ft. Then, the player supplies the continuous algorithm with a stochastic gradient estimate \u2207\u0303f\u0302t obtained by a single function evaluation, as\n\u2202ft(x)\n\u2202xi = E[f(R \u222a {i})\u2212 f(R)], \u2200i \u2208 [n], (1)\nwhere R is random subset of [n] \\ {i} such that for every j \u2208 [n] \\ {i}, the event j \u2208 R happens with an independent probability of xj . Because a lossless rounding scheme is used, the discrete player enjoys a regret that is no worse than that of the continuous solution. Provably lossless rounding schemes include the pipage rounding (Ageev & Sviridenko, 2004; Calinescu et al., 2011) and contention resolution (Vondra\u0301k et al., 2011).\nMost discrete submodular maximization algorithms that go through the multi-linear extension require a gradient estimate with high accuracy. In order to do this, they appeal to a concentration bound, which requires O(n2) evaluations of the discrete function for independently chosen samples. In stark contrast, our algorithms can handle stochastic gradient estimates and thus require only a single function evaluation, finally making continuous methods a reality for large-scale online discrete optimization problems. The framework of the one-sampling lifting method is illustrated in Fig. 1.\nAs an example, we present in Algorithm 3 how to use Meta-Frank-Wolfe as an online maximization algorithm of submodular set functions. According to Theorem 1, the (1\u2212 1/e)-regret of Algorithm 3 is bounded by 3TDQ 1/2\nK1/3 +\nLD2T 2K +R E T , whereRET is the regret of E(k) up to horizon T . If one sets E(k) to an online linear maximization algo-\nrithm with regret bound O( \u221a T ) and sets K = T 3/2, the (1\u2212 1/e)-regret is at most O( \u221a T ).\nAlgorithm 3 Meta-Frank-Wolfe for online discrete submodular maximization Input: matroid constraint I, time horizon T , linear opti-\nmization oracles E(1) . . . E(K), step sizes \u03c1k \u2208 (0, 1) and \u03b7k \u2208 (0, 1), and initial point x1 Output: {Xt : 1 \u2264 t \u2264 T} 1: Initialize online linear optimization oracles E(1)...E(K),\nsetting the constraint set to the matroid polytope of I 2: Initialize d(0)t = 0 and x (1) t = x1 3: for t\u2190 1, 2, 3, . . . , T do 4: v(k)t \u2190 output of oracle E(k) in round t\u2212 1 5: x(k+1)t \u2190 update(x (k) t ,v (k) t , \u03b7k) for k = 1 . . .K 6: xt \u2190 x(K+1)t 7: play Xt \u2190 round(xt), obtain value ft(Xt) and observe the function ft 8: Sample \u2207\u0303f\u0304t(x(k)t ) for k = 0, . . . ,K \u2212 1 9: d(k)t \u2190 (1 \u2212 \u03c1k)d (k\u22121) t + \u03c1k\u2207\u0303ft(x (k) t ) for k =\n1 . . .K 10: Feedback \u3008v(k)t ,d (k) t \u3009 to E(k) for k = 1 . . .K 11: end for"}, {"heading": "5. Experiment", "text": "In this section, we test our online algorithms for monotone continuous DR-submodular and convex optimization on both real-world and synthetic data sets. We find that our algorithms outperform most baselines, including projected gradient descent, when supplied with stochastic gradient estimates. All code was written in the Julia programming language and tested on a Macintosh desktop with an Intel Processor i7 with 16 GB of RAM. No parts of the code were optimized past basic Julia usage. A list of all algorithms to be compared in this section is presented below.\n\u2022 Meta-Frank-Wolfe is Algorithm 1. We compare the variance-reduced meta-Frank-Wolfe algorithm and the analogue without variance reduction, denoted MetaFW w/ VR and Meta-FW w/o VR, respectively.\n\u2022 One-shot Frank-Wolfe is Algorithm 2. We compare the One-shot online Frank-Wolfe algorithm with and without variance reduction, denoted OS-FW w/ VR OS-FW w/o NVR, respectively.\n\u2022 Regularized online Frank-Wolfe is referred to as the online conditional gradient algorithm in (Hazan et al., 2016). It has a regularizer term when computing the gradient. Thus we term it the regularized online FrankWolfe algorithm and denote it as Regularized-OFW.\n\u2022 Online projected gradient ascent (OGA) follows the direction of the projected gradient. Its 1/2-regret is at most O( \u221a T ) for online monotone continuous DR-\nsubmodular maximization if the step size is set to \u0398(1/ \u221a t) on the t-th iteration (Chen et al., 2018). Note that OGA is not a projection-free algorithm. In the setting of convex minimization, we use online projected gradient descent instead (denoted by OGD).\n\u2022 When we perform experiments on discrete submodular maximization problems using our lifting method, we also compare the above algorithms with the Online Greedy algorithm (Streeter & Golovin, 2009)."}, {"heading": "5.1. Online DR-Submodular Maximization", "text": "In order to test the performance of algorithms for online maximization of monotone continuous DR-submodular functions with stochastic gradient estimates, we conducted three sets of experiments on real-world datasets. We approximate the (1\u2212 1/e)-regret by running an offline Frank Wolfe maximization to produce a solution that is a (1\u22121/e) approximation to the optimum.\nJoke Recommendations (Continuous) The first set of experiments is to optimize a sequence of continuous facility location objectives on the Jester dataset (Goldberg et al., 2001). It contains ratings of 100 jokes from 73,421 users and the rating range is [\u221210, 10]. We re-scale the rating range into [0, 20] so that all ratings are nonnegative. Let Ruj be user u\u2019s rating of joke j. All users are splitted into disjoint batches B1, B2, . . . , BT , each containing B users. The facility location objective is defined as ft(X) = \u2211 u\u2208Bt maxj\u2208X Ruj , \u2200X \u2286 [J ], where J = 100 is the total number of jokes and [J ] = {1, 2, 3, . . . , J}. Its multilinear extension is given by f\u0304t(x) = \u2211 u\u2208Bt \u2211J l=1Rujluxjlu \u220fl\u22121 m=1(1 \u2212 xjmu ), \u2200x \u2208 [0, 1]J , where j1u, j 2 u, . . . , j J u is a permutation of 1, 2, . . . , J such that Ruj1u \u2265 Ruj2u \u2265 . . . \u2265 RujJu (Iyer et al., 2014). In this experiment, the sequence of objective functions to be optimized is {f\u03041, f\u03042, . . . , f\u0304T }. The stochastic gradient is obtained by the sampling method given in Eq. (1) with only one sample for each coordinate of the gradient. We set the constraint set to {x \u2208 [0, 1]J : 1>x \u2264 1} and choose B = 5. We present the results in Fig. 2(a). Meta-FW w/ VR attains the smallest regret. The counterpart without variance reduction Meta-FW w/o VR is inferior to Meta-FW w/ VR in terms of the regret. OS-FW w/ VR outperforms OSFW w/o NVR, which suggests that the variance reduction technique improves the performance of the algorithms.\nJoke Recommendations (Discrete) In the second set experiments, we consider online maximization of discrete submodular functions. The problem set up is the same\nas before, but instead of evaluating regret of the multilinear extensions, we round solutions using pipage rounding and evaluate the regret on the discrete submodular functions. We set the batch size B to 40 and we recommend 10 jokes for users. The results are illustrated in Fig. 2(b). We observe that Meta-FW w/ VR outperforms all other algorithms again. The projected algorithm OGA is second to Meta-FW w/ VR. Online Greedy appears only better than Regularized-OFW. The experiment result show that the continuous algorithms designed under the framework of the lifting method perform better than the discrete algorithms.\nTopic Summarization We consider the problem of selecting news documents in order to maximize the probabilistic coverage of news topics (El-Arini et al., 2009; Yue & Guestrin, 2011). We applied the latent Dirichlet allocation to the corpus of Reuters-21578, Distribution 1.0, set the number of topics to 10, and extracted the topic distribution of each news document. We sample T batches of news documents from the corpus and denote them by B1, B2, . . . , BT , where each batch contains 50 randomly sampled documents. For each batch Bi, we define the probabilistic coverage function as follows fi(X) = 1 10 \u221110 j=1[1 \u2212 \u220f a\u2208X(1 \u2212 pa(j))], \u2200X \u2286 Bi, where pa(\u00b7) is the topic distribution of news document a. Its multilinear extension is f\u0304i(x) = 110 \u221110 j=1[1\u2212 \u220f a\u2208X(1\u2212 pa(j)xa)], \u2200x \u2208 [0, 1]50, see (Iyer et al., 2014). The sequence of objective functions that the algorithms are expected to maximize is f\u03041, f\u03042, . . . , f\u0304T . As in the experiments on joke recommendations, the stochastic gradient is obtained by the sampling method given in Eq. (1) with only one sample for each coordinate of the gradient. The constraint set is {x \u2208 [0, 1]50 : 1>x \u2264 45}. We show the (1\u22121/e)-regret of the algorithms in Fig. 2(c). Again, MetaFW w/ VR exhibits the lowest regret than any other algorithm. Its non-variance-reduced counterpart Meta-FW w/o VR is second to it. OS-FW w/ VR outperforms OS-FW w/o NVR, which confirms the improvement brought by the variance reduction technique."}, {"heading": "5.2. Online Convex Minimization", "text": "The next two sets of experiments test the performance of the algorithms for online minimization of convex functions with stochastic gradient estimates. For these experiments, the regret is computed by obtaining the offline solutions with a Frank-Wolfe solver.\nStochastic Cost Network Flow The fourth set of experiments is a minimum stochastic cost flow in a directed network. A directed graph G = (V,E) with source s \u2208 V , sink v \u2208 V , and edge capacities c : E \u2192 R+ is known to the player. A flow is a function x : R|E|+ \u2192 R+ that satisfies the capacities on each edge 0 \u2264 x(e) \u2264 c(e) and obeys the\n0\n1000\n2000\n3000\n4000\n0 25 50 75 100 Iteration index\n1 \u2212\n1 e \u2212r\neg re\nt\nMeta\u2212FW w/ VR Meta\u2212FW w/o VR OGA OS\u2212FW w/ VR OS\u2212FW w/o VR Regularized\u2212OFW\nMeta-FW w/ VR\nMeta-FW w/o VR\nOGA\nRegularized-OFW\nOS-FW w/ VR OS-FW w/o VR\n(a) Continuous facility location on Jester dataset\n0\n500\n1000\n1500\n2000\n0 100 200 300 400 Iteration index\n1 \u2212\n1 e \u2212r\neg re\nt\nMeta\u2212FW w/ VR Meta\u2212FW w/o VR OGA Regularized\u2212OFW Online greedy\nMeta-FW w/ VR\nMeta-FW w/o VR OGA\nOnline greedy\nRegularized-OFW\n(b) Discrete facility location on Jester dataset\n0\n100\n200\n300\n400\n0 250 500 750 1000 Iteration index\n1 \u2212\n1 e \u2212r\neg re\nt\nMeta\u2212FW w/ VR Meta\u2212FW w/o VR OGA OS\u2212FW w/ VR OS\u2212FW w/o VR Regularized\u2212OFW\nMeta-FW w/ VR Meta-FW w/o VR\nOGA\nRegularized-OFW\nOS-FW w/ VR\nOS-FW w/o VR\n(c) News recommendation in Reuters corpus\nconservation laws for all vertices z,\n\u2211 {z,r}\u2208E x(r) =  a z = s \u2212a z = v 0 otherwise\nfor some fixed a \u2265 0. In each round t, a convex cost function on the flow ft : R|E| \u2192 R+ is drawn from a distribution, unknown to the player. The goal is to minimize the stochastic regret of the flows chosen. Linear optimizations for this problem may be implemented as combinatorial network flow algorithms. We used the directed Zachary Karate network with 34 nodes and 78 arcs (Zachary, 1977). We set all edge capacities to 1 and cost functions are of the form f(x) = \u2211 e\u2208E wex(e)\n2 where we \u223c Unif[100, 120]. The results are presented in Fig. 2(d). Meta-FW w/ VR attains the lowest regret among all baselines. Again, the regret of Meta-FW w/o VR is larger than the variance-reduced MetaFW w/ VR. Similarly, OS-FW w/ VR also outperforms OS-FW w/o NVR.\nMatrix Completion In the online convex matrix completion problem, one would like to construct a low rank matrix X \u2208 Rm\u00d7n that well-approximates a given matrix\nM \u2208 Rm\u00d7n on observed entries OB \u2286 [m]\u00d7 [n]. The convex relaxation is minTrace(X)\u2264k \u2211 (i,j)\u2208OB(Xi,j\u2212Mi,j)2. In the online setting, observed entries of the matrix arrive in T batches, OB1, OB2, . . . OBT , each of size B. In each round, we construct a low-rank matrix to minimize the total regret over the T rounds. Although projection involves a full singular value decomposition, linear optimization here is simply a calculation of the largest singular vectors of (X \u2212 M)OB , see Chapter 7 of (Hazan et al., 2016). In our experiment, M is a rank 10 matrix with m = n = 50, and B = 100. We illustrate the results in Fig. 2(e) and the computational time is shown in Fig. 2(f). Meta-FW w/ VR is only second to OGD. However, OGD is the slowest algorithm due to the computationally expensive projection operations and its computational time is five times that of Meta-FW w/ VR. The non-variance-reduced Meta-FW w/o VR is inferior to Meta-FW w/ VR in terms of regret."}, {"heading": "Acknowledgments", "text": "AK was supported by AFOSR YIP (FA9550-18-1-0160). CH was supported in part by NSF GRFP (DGE1122492) and by ONR Award N00014-16-1-2374."}, {"heading": "B. Proof of Theorem 1: Convex Case", "text": "We begin by examining the sequence of iterates x(1)t ,x (2) t , . . . ,x (K+1) t produced in Algorithm 1 for a fixed t. By definition of the update and because ft is L-smooth, we have\nft(x (k+1) t )\u2212 ft(x\u2217) = ft(x (k) t + \u03b7k(v (k) t \u2212 x (k) t ))\u2212 ft(x\u2217)\n\u2264 ft(x(k)t )\u2212 ft(x\u2217) + \u03b7k\u3008\u2207ft(x (k) t ),v (k) t \u2212 x (k) t \u3009+ \u03b72k\nL 2 \u2016v(k)t \u2212 x (k) t \u20162\n\u2264 ft(x(k)t )\u2212 ft(x\u2217) + \u03b7k\u3008\u2207ft(x (k) t ),v (k) t \u2212 x (k) t \u3009+ \u03b72k\nLD2\n2 .\nNow, observe that the dual pairing may be decomposed as\n\u3008\u2207ft(x(k)t ),v (k) t \u2212 x (k) t \u3009 = \u3008\u2207ft(x (k) t )\u2212 d (k) t ,v (k) t \u2212 x\u2217\u3009+ \u3008\u2207ft(x (k) t ),x \u2217 \u2212 x(k)t \u3009+ \u3008d (k) t ,v (k) t \u2212 x\u2217\u3009.\nWe can bound the first term using Young\u2019s Inequality to get\n\u3008\u2207ft(x(k)t )\u2212 d (k) t ,v (k) t \u2212 x\u2217\u3009 \u2264\n1\n2\u03b2k \u2016ft(x(k)t )\u2212 d (k) t \u20162 + 2\u03b2k\u2016v (k) t \u2212 x\u2217\u20162\n\u2264 1 2\u03b2k \u2016ft(x(k)t )\u2212 d (k) t \u20162 + 2\u03b2kD2\nfor any \u03b2k > 0, which will be chosen later in the proof. We may also bound the second term in the decomposition of the dual pairing using convexity of ft, i.e. \u3008\u2207ft(x(k)t ),x\u2217 \u2212 x (k) t \u3009 \u2264 ft(x\u2217)\u2212 ft(x (k) t ). Using these upper bounds, we get that\n\u3008\u2207ft(x(k)t ),v (k) t \u2212 x (k) t \u3009 \u2264\n1\n2\u03b2k \u2016ft(x(k)t )\u2212 d (k) t \u20162 + 2\u03b2kD2 + ft(x\u2217)\u2212 ft(x (k) t ) + \u3008d (k) t ,v (k) t \u2212 x\u2217\u3009.\nUsing this upper bound on the dual pairing in the first inequality, we get that\nft(x (k+1) t )\u2212 ft(x\u2217)\u2264 (1\u2212\u03b7k)(ft(x (k) t )\u2212ft(x\u2217))+\u03b7k\n[ 1\n2\u03b2k \u2016ft(x(k)t )\u2212d (k) t \u20162 +2\u03b2kD2 +\u3008d (k) t ,v (k) t \u2212x\u2217\u3009+\u03b7k\nLD2\n2\n] .\nNow we will apply the variance reduction technique. Note that\n\u2016\u2207ft(x(k+1)t \u2212\u2207ft(x (k) t )\u2016\u2264 L\u2016x (k+1) t \u2212 x (k) t \u2016\u2264 L\u03b7k\u2016x (k) t \u2212 v (k) t \u2016\u2264\nLD\nk + 3\nWhere we have used that ft is L-smooth, the convex update, and that the step size is \u03b7k = 1k+3 . Now, using Theorem 3 with G = LD and s = 3, we have that\nE[\u2016ft(x(k)t )\u2212 d (k) t \u20162] \u2264 Qt (k + 4)2/3 \u2264 Q (k + 4)2/3 .\nWhere Qt , max{\u2016\u2207ft(x1)\u2016242/3, 4\u03c32 + 3(LD)2/2} and Q , max{42/3 max1\u2264t\u2264T \u2016\u2207ft(x1)\u20162, 4\u03c32 + 3(LD)2/2} Thus, taking expectation of both sides of the optimality gap and setting \u03b2k = Q 1/2\n2D(k+4)1/3 yields\nE[ft(x(k+1)t )]\u2212 ft(x\u2217) \u2264 (1\u2212 \u03b7k)(E[ft(x (k) t )]\u2212 ft(x\u2217)) + \u03b7k\n[ 2Q1/2D\n(k + 4)1/3 + \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009+ \u03b7k\nLD2\n2\n] .\nNow we have obtained an upper bound on the expected optimality gap E[ft(x(k+1)t )] \u2212 ft(x\u2217) in terms of the expected optimality gap E[ft(x(k)t )]\u2212 ft(x\u2217) in the previous iteration. By induction on k, we get that the final iterate in the sequence, xt , x (K+1) t , satisfies the following expected optimality gap E[ft(xt)]\u2212 ft(x\u2217) \u2264 K\u220f\nk=1\n(1\u2212 \u03b7k) [ft(x1)\u2212 ft(x\u2217)] + K\u2211\nk=1\n\u03b7k K\u220f j=k+1 (1\u2212 \u03b7j) [ 2Q1/2D (k + 4)1/3 + \u3008d(k)t ,v (k) t \u2212x\u2217\u3009+ \u03b7k LD2 2 ] (2)\nRecall that the Frank Wolfe step sizes are \u03b7k = 1k+3 . We may obtain upper bounds on product of the form \u220fK k=r(1\u2212 \u03b7k) by\nK\u220f k =r (1\u2212 \u03b7k) = K\u220f k=r ( 1\u2212 1 k + 3 ) \u2264 exp ( \u2212 K\u2211 k=r 1 x+ 3 ) \u2264 exp ( \u2212 \u222b K+1 x=r 1 x+ 3 dx ) = r + 3 K + 4 \u2264 r + 3 K\nSubstituting step sizes \u03b7k = 1k+3 into Eq (2) and using this upper bound yields\n(3)E[ft(xt)]\u2212 ft(x\u2217) \u2264 4\nK [ft(x1)\u2212 ft(x\u2217)] + K\u2211 k=1 ( 1 k + 3 \u00b7 k + 4 K )[ 2Q1/2D (k + 4)1/3 + \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009+ LD2 2(k + 3) ]\nWhich may be further simplified by using (\n1 k+3 \u00b7 k+4 K ) \u2264 43K to obtain\nE[ft(xt)]\u2212 ft(x\u2217) \u2264 4\nK [ft(x1)\u2212 ft(x\u2217)] +\n4\n3K K\u2211 k=1 [ 2Q1/2D (k + 3)1/3 + \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009+ LD2 2(k + 3) ] ,\nAs before, we can obtain the following upper bounds using integral methods:\nK\u2211 k =1 1 k + 3 \u2264 log ( K + 3 3 ) \u2264 log(K + 1) and K\u2211 k =1\n1 (k + 3)1/3 \u2264 3 2\n( (K + 3)2/3 \u2212 32/3 ) \u2264 3\n2 K2/3\nSubstituting these bounds into Eq (3) yields\nE[ft(xt)]\u2212 ft(x\u2217) \u2264 4\nK [ft(x1)\u2212 ft(x\u2217)] +\n4Q1/2D\nK1/3 +\n4LD2 log(K + 1)\n3K +\n4\n3K K\u2211 k=1 \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009.\nNow, we can begin to bound regret by summing over all t = 1 . . . T to obtain\nT\u2211 t =1 E[ft(xt)]\u2212 T\u2211 t =1 ft(x \u2217) \u2264 4 K T\u2211 t=1 [ft(x1)\u2212 ft(x\u2217)] + 4TQ1/2D K1/3\n+ 4TLD2 log(K + 1)\n3K +\n4\n3K T\u2211 t=1 K\u2211 k=1 \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009\nRecall that for a fixed k, the sequence {v(k)t }Tt=1 is produced by a online linear minimization oracle with regretRET so that\nT\u2211 t =1 \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009 \u2264 T\u2211 t=1 \u3008d(k)t ,v (k) t \u3009 \u2212min x\u2208K T\u2211 t=1 \u3008d(k)t ,x\u3009 \u2264 RET .\nSubstituting this into the upper bound and using M = max1\u2264t\u2264T [ft(x1)\u2212 ft(x\u2217)] yields\nT\u2211 t =1 E[ft(xt)]\u2212 T\u2211 t =1 ft(x \u2217) \u2264 4TDQ 1/2 K1/3 + 4T K ( M + LD2 3 log(K + 1) ) + 4 3 RET\nNow, setting K = T 3/2 and using a linear oracle withRET = O( \u221a T ) yields\nT\u2211 t =1 E[ft(xt)]\u2212 T\u2211 t =1 ft(x \u2217) \u2264 4 \u221a TDQ1/2 + 4\u221a T ( M + LD2 3 (log T 3/2 + 1) ) + 4 3 RET\n= O( \u221a T )."}, {"heading": "C. Proof of Theorem 1: DR-Submodular Case", "text": "Using the smoothness of ft and recalling x (k+1) t \u2212 x (k) t = 1 Kv (k) t , we have\n(4)\nft(x (k+1) t ) \u2265 ft(x (k) t ) + \u3008\u2207ft(x (k) t ),x (k+1) t \u2212 x (k) t \u3009 \u2212\nL 2 \u2016x(k+1)t \u2212 x (k) t \u20162\n= ft(x (k) t ) + \u3008\n1\nK \u2207ft(x(k)t ),v (k) t \u3009 \u2212\nL\n2K2 \u2016v(k)t \u20162\n\u2265 ft(x(k)t ) + 1\nK \u3008\u2207ft(x(k)t ),v (k) t \u3009 \u2212\nLD2 2K2 .\nWe can re-write the term \u3008\u2207ft(x(k)t ),v (k) t \u3009 as\n(5) \u3008\u2207ft(x(k)t ),v (k) t \u3009 = \u3008\u2207ft(x (k) t )\u2212 d (k) t ,v (k) t \u3009+ \u3008d (k) t ,v (k) t \u3009\n= \u3008\u2207ft(x(k)t )\u2212 d (k) t ,v (k) t \u3009+ \u3008d (k) t ,x \u2217\u3009+ \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009 = \u3008\u2207ft(x(k)t )\u2212 d (k) t ,v (k) t \u2212 x\u2217\u3009+ \u3008\u2207ft(x (k) t ),x \u2217\u3009+ \u3008d(k)t ,v (k) t \u2212 x\u2217\u3009.\nWe claim \u3008\u2207ft(x(k)t ),x\u2217\u3009 \u2265 ft(x\u2217) \u2212 ft(x (k) t ). Indeed, using monotonicity of ft and concavity along non-negative directions, we have\n(6) ft(x\n\u2217)\u2212 ft(x(k)t ) \u2264 ft(x\u2217 \u2228 x (k) t )\u2212 ft(x (k) t )\n\u2264 \u3008\u2207ft(x(k)t ),x\u2217 \u2228 x (k) t \u2212 x (k) t \u3009 = \u3008\u2207ft(x(k)t ), (x\u2217 \u2212 x (k) t ) \u2228 0\u3009 \u2264 \u3008\u2207ft(x(k)t ),x\u2217\u3009.\nPlugging Eq. (6) into Eq. (5), we obtain\n(7)\u3008\u2207ft(x(k)t ),v (k) t \u3009 \u2265 \u3008\u2207ft(x (k) t )\u2212 d (k) t ,v (k) t \u2212 x\u2217\u3009+ \u3008d (k) t ,v (k) t \u2212 x\u2217\u3009+ (ft(x\u2217)\u2212 ft(x (k) t )).\nUsing Young\u2019s inequality, we can show that\n(8)\u3008\u2207ft(x (k) t )\u2212 d (k) t ,v (k) t \u2212 x\u2217\u3009 \u2265 \u2212\n1\n2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 \u2212\n\u03b2(k)\n2 \u2016v(k)t \u2212 x\u2217\u20162\n\u2265 \u2212 1 2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 \u2212 \u03b2(k)D2/2\nThen we plug Eqs. (7) and (8) into Eq. (4), we deduce\nft(x (k+1) t )\u2265 ft(x (k) t )+\n1\nK\n[ \u2212 1\n2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212d (k) t \u20162\u2212\u03b2(k)D2/2+\u3008d (k) t ,v (k) t \u2212x\u2217\u3009+(ft(x\u2217)\u2212ft(x (k) t ))\n] \u2212 LD 2\n2K2 .\nEquivalently, we have\n(9)ft(x \u2217)\u2212 ft(x(k+1)t ) \u2264 (1\u2212 1/K)[ft(x\u2217)\u2212 ft(x (k) t )]\n\u2212 1 K\n[ \u2212 1\n2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 \u2212 \u03b2(k)D2/2 + \u3008d (k) t ,v (k) t \u2212 x\u2217\u3009\n] + LD2\n2K2 .\nApplying Eq. (9) recursively for 1 \u2264 k \u2264 K immediately yields\nft(x \u2217)\u2212 ft(x(k+1)t ) \u2264 (1\u2212 1/K)K [ft(x\u2217)\u2212 ft(x (1) t )]\n+ 1\nK K\u2211 k=1 [ 1 2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 + \u03b2(k)D2/2 + \u3008d (k) t ,x \u2217 \u2212 v(k)t \u3009 ] + LD2 2K .\nRecall that the point played in round t is xt , x (K+1) t , the first iterate in the sequence is x (1) t = 0, and that (1\u2212 1/K)K \u2264 1/e for all K \u2265 1 so that\nft(x \u2217)\u2212 ft(xt) \u2264\n1 e [ft(x \u2217)\u2212 ft(0)] + 1 K K\u2211 k=1 [ 1 2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 + \u03b2(k)D2/2 + \u3008d (k) t ,x \u2217 \u2212 v(k)t \u3009 ] + LD2 2K .\nSince ft(0) \u2265 0, we obtain\n(10)(1\u2212 1/e)ft(x\u2217)\u2212 ft(xt) \u2264 1\nK K\u2211 k=1 [ 1 2\u03b2(k) \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 + \u03b2(k)D2/2 + \u3008d (k) t ,x \u2217 \u2212 v(k)t \u3009 ] + LD2 2K .\nIf we sum Eq. (10) over t = 1, 2, 3, . . . , T , we obtain\n(1\u2212 1/e) T\u2211\nt=1\nft(x \u2217)\n\u2212 T\u2211\nt=1\nft(xt) \u2264 1\nK K\u2211 k=1\n[ 1\n2\u03b2(k) T\u2211 t=1 \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 + \u03b2(k)D2T/2 + T\u2211 t=1 \u3008d(k)t ,x\u2217 \u2212 v (k) t \u3009\n] + LD2T\n2K .\nBy the definition of the regret, we have T\u2211\nt =1\n\u3008d(k)t ,x\u2217 \u2212 v (k) t \u3009 \u2264 RET .\nTherefore, we deduce\n(1\u2212 1/e) T\u2211\nt=1\nft(x \u2217)\u2212 T\u2211 t=1 ft(xt)\n\u2264 1 K K\u2211 k=1\n[ 1\n2\u03b2(k) T\u2211 t=1 \u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162 + \u03b2(k)D2T/2\n] + LD2T\n2K +RET .\nTaking the expectation in both sides, we obtain\n(11) (1\u2212 1/e) T\u2211 t=1 E[ft(x\u2217)]\u2212 T\u2211 t=1 E[ft(xt)]\n\u2264 1 K K\u2211 k=1\n[ 1\n2\u03b2(k) T\u2211 t=1 E[\u2016\u2207ft(x(k)t )\u2212 d (k) t \u20162] + \u03b2(k)D2T/2\n] + LD2T\n2K +RET .\nNotice that \u2016\u2207ft(x(k)t )\u2212\u2207ft(x (k\u22121) t )\u2016 \u2264 L\u2016v (k) t \u2016/T \u2264 LR/T \u2264 2LR/(k+ 3). By Theorem 3, if we set \u03c1k = 2(k+3)2/3 , we have\n(12)E[\u2016\u2207ft(x (k) t )\u2212 d (k) t \u20162] \u2264 Qt (k + 4)2/3\n\u2264 Q (k + 4)2/3 ,\nwhere Qt , max{\u2016\u2207ft(0)\u2016242/3, 4\u03c32 + 6L2R2} and Q , max{max1\u2264t\u2264T \u2016\u2207ft(x1)\u2016242/3, 4\u03c32 + 6L2R2}.\nPlugging Eq. (12) into Eq. (11) and setting \u03b2(k) = (Q1/2)/(D(k + 3)1/3), we deduce\n(1\u2212 1/e) T\u2211\nt=1\nE[ft(x\u2217)]\u2212 T\u2211\nt=1\nE[ft(xt)] \u2264 TDQ1/2\nK\nK\u2211 k=1\n1 (k + 4)1/3 + LD2T 2K +RET\nSince \u2211K\nk=1 1 (k+4)1/3 \u2264 \u222bK 0 dx (x+4)1/3 = 32 [(K + 4) 2/3 \u2212 92/3] \u2264 32K 2/3, we have\n(1\u2212 1/e) T\u2211\nt=1\nE[ft(x\u2217)]\u2212 T\u2211\nt=1\nE[ft(xt)] \u2264 3TDQ1/2 2K1/3 + LD2T 2K +RET ."}, {"heading": "D. Proof of Theorem 2: Convex Case", "text": "Let f(x) = Eft\u223cD[ft(x)] denote the expected function. Because f is L-smooth and convex, we have\nf(xt+1)\u2212 f(x\u2217) = f(xt + \u03b7t(vt \u2212 xt))\u2212 f(x\u2217)\n\u2264 f(xt)\u2212 f(x\u2217) + \u03b7t\u3008\u2207f(xt),vt \u2212 xt\u3009+ \u03b72t L\n2 \u2016vt \u2212 xt\u20162\n\u2264 f(xt)\u2212 ft(x\u2217) + \u03b7t\u3008\u2207f(xt),vt \u2212 xt\u3009+ \u03b72t LD2\n2 .\nAs before, the dual pairing may be decomposed as\n\u3008\u2207f(xt),vt \u2212 xt\u3009 = \u3008\u2207f(xt)\u2212 dt,vt \u2212 x\u2217\u3009+ \u3008\u2207f(xt),x\u2217 \u2212 xt\u3009+ \u3008dt,vt \u2212 x\u2217\u3009.\nWe can bound the first term using Young\u2019s Inequality to get\n\u3008\u2207f(xt)\u2212 dt,vt \u2212 x\u2217\u3009 \u2264 1\n2\u03b2 \u2016f(xt)\u2212 dt\u20162 + 2\u03b2\u2016vt \u2212 x\u2217\u20162\n\u2264 1 2\u03b2 \u2016f(xt)\u2212 dt\u20162 + 2\u03b2D2.\nfor any \u03b2 > 0, which will be chosen later in the proof. We may also bound the second term in the decomposition of the dual pairing using convexity of f , i.e. \u3008\u2207f(xt),x\u2217 \u2212 xt\u3009 \u2264 ft(x\u2217)\u2212 f(xt). Finally, the third term is nonpositive, by the choice of vt, namely vt = arg minv\u2208K\u3008dt,v\u3009. Using these inequalities, we now have that\nf(xt+1)\u2212 f(x\u2217) \u2264 (1\u2212 \u03b7t) (f(xt)\u2212 f(x\u2217)) + \u03b7t ( 1\n2\u03b2 \u2016f(xt)\u2212 dt\u20162 + 2\u03b2D2\n) + \u03b72t LD2\n2 .\nTaking expectation over the randomness in the iterates (i.e. the stochastic gradient estimates), we have that\n(13)E[f(xt+1)]\u2212 f(x\u2217) \u2264 (1\u2212 \u03b7t) (E[f(xt)]\u2212 f(x\u2217)) + \u03b7t ( 1\n2\u03b2 E[\u2016f(xt)\u2212 dt\u20162] + 2\u03b2D2\n) + \u03b72t LD2\n2 .\nNow we will apply the variance reduction technique. Note that\n\u2016\u2207f(xt+1)\u2212\u2207f(xt)\u2016\u2264 L\u2016xt+1 \u2212 xt\u2016\u2264 L\u03b7t\u2016xt \u2212 vt\u2016\u2264 L\u03b7tD\nwhere we have used that f is L-smooth, the convex update, and the diameter. Now, using Theorem 3 with G = LD and s = 3, we have that\nE[\u2016f(xt)\u2212 dt\u20162] \u2264 Q\n(t+ 4)2/3 ,\nwhere Q , max{42/3\u2016\u2207f(x1)\u20162, 4\u03c32 + 3(LD)2/2}. Using this bound in Eq (13) and setting \u03b2 = Q 1/2\n2D(t+4)1/3 yields\nE[f(xt+1)]\u2212 f(x\u2217) \u2264 (1\u2212 \u03b7t) (E[f(xt)]\u2212 f(x\u2217)) + \u03b7t 2Q1/2D\n(t+ 4)1/3 + \u03b72t\nLD2\n2 .\nBy induction, we have\nE[f(xt+1)]\u2212 f(x\u2217) \u2264 t\u220f\nk=1\n(1\u2212 \u03b7k)M + t\u2211\nk=1\n\u03b7k t\u220f j=k+1 (1\u2212 \u03b7j) ( 2Q1/2D (k + 4)1/3 + \u03b7k LD2 2 ) ,\nwhere M = f(x1)\u2212 f(x\u2217). Recall that the step size is set to be \u03b7t = 1t+3 . As in Appendix B, we can obtain the bounds\u220ft k=1(1 \u2212 \u03b7k) = \u220ft k=1(1 \u2212 1 k+3 ) \u2264 exp(\u2212 \u2211t k=1 1 k+3 ) \u2264 exp(\u2212 \u222b t+1 1 dx x+3 ) = 4/(t + 4) and similarly \u220ft j=k+1(1 \u2212\n1 j+3 ) \u2264 k+4 t+4 . Using these bounds as well as the choice of step size \u03b7t = 1 t+3 in the above yields\nE[f(xt+1)]\u2212 f(x\u2217) \u2264 4M\nt+ 4 + t\u2211 k=1 ( 1 k + 3 \u00b7 k + 4 t+ 4 )( 2Q1/2D (k + 4)1/3 + 1 k + 3 LD2 2 )\n= 4M\nt+ 4 +\n4\n3(t+ 4) t\u2211 k=1 ( 2Q1/2D (k + 4)1/3 + 1 k + 3 LD2 2 )\nwhere the second inequality used (\n1 k+3 \u00b7 k+4 (t+4) ) < 43(t+4) . As before in Section B, using the inequalities \u2211t k=1 1 k+3 \u2264\nlog(t+ 1) and \u2211t\nk=1 1 (k+3)1/3 \u2264 32 t 2/3 in the above yields\nE[f(xt+1)]\u2212 f(x\u2217) \u2264 4M\nt+ 4 + 4Q1/2D\nt2/3\nt+ 4 +\n4 3 LD2 log(t+ 1) t+ 4 . (14)\nTo obtain a regret bound, we sum over rounds t = 1, . . . T to obtain\nT\u2211 t=1 E[f(xt)]\u2212 Tf(x\u2217) \u2264 4M\n( T\u2211\nt=1\n1\nt+ 4\n) + 4Q1/2D ( T\u2211\nt=1\nt2/3\nt+ 4\n) + 4\n3 LD2\n( T\u2211\nt=1\nlog(t+ 1)\nt+ 4\n)\nUsing the integral trick again, we obtain the upper bounds \u2211T\nt=1 1 t+4 \u2264 log(T + 1), \u2211T t=1 t2/3 t+4 \u2264 3 2T\n2/3, and\u2211T t=1 log(t+3) t+4 \u2264 log 2(T + 3). Substituting these bounds in the regret bound above yields\nT\u2211 t=1 E[f(xt)]\u2212 Tf(x\u2217) \u2264 4M log(T + 1) + 6Q1/2DT 2/3 + 4 3 LD2 log2(T + 3) = O ( T 2/3 )"}, {"heading": "E. Proof of Theorem 2: DR-Submodular Case", "text": "Since f is L-smooth, we obtain\nf(xt+1) \u2265 f(xt) + \u3008\u2207f(xt), 1\nT vt\u3009 \u2212\nL 2 \u2016 1 T vt\u20162\n\u2265 f(xt) + 1\nT \u3008\u2207f(xt),vt\u3009 \u2212\nLD2\n2T 2\n= f(xt) + 1\nT \u3008dt,vt\u3009+\n1 T \u3008\u2207f(xt)\u2212 dt,vt\u3009 \u2212\nLD2\n2T 2\n\u2265 f(xt) + 1\nT \u3008dt,x\u2217\u3009+\n1 T \u3008\u2207f(xt)\u2212 dt,vt\u3009 \u2212\nLD2\n2T 2\n= f(xt) + 1\nT \u3008\u2207f(xt)\u2212 dt,vt \u2212 x\u2217\u3009+\n1 T \u3008f(xt),x\u2217\u3009 \u2212\nLD2 2T 2 .\nIn the last inequality, we used the fact that vt = arg maxv\u2208K\u3008dt,v\u3009. Similar to Eq. (6) in Appendix C, we have \u3008f(xt),x\u2217\u3009 \u2265 f(x\u2217)\u2212 f(xt). Again, Young\u2019s inequality gives \u3008\u2207f(xt)\u2212 dt,vt \u2212 x\u2217\u3009 \u2265 \u221212 (\u03b2t\u2016vt \u2212 x\n\u2217\u20162 + \u2016f(xt)\u2212 dt\u20162/\u03b2t). Therefore, we deduce\nf(xt+1) \u2265 f(xt)\u2212 1\n2T (\u03b2t\u2016vt \u2212 x\u2217\u20162 + \u2016f(xt)\u2212 dt\u20162/\u03b2t) +\n1 T (f(x\u2217)\u2212 f(xt))\u2212\nLD2\n2T 2\n\u2265 f(xt)\u2212 1\n2T (\u03b2tD\n2 + \u2016f(xt)\u2212 dt\u20162/\u03b2t) + 1\nT (f(x\u2217)\u2212 f(xt))\u2212\nLD2 2T 2 .\nRe-arrangement of the terms yields\nf(x\u2217)\u2212 f(xt+1) \u2264 (1\u2212 1/T )(f(x\u2217)\u2212 f(xt)) + 1\n2T (\u03b2tD\n2 + \u2016f(xt)\u2212 dt\u20162/\u03b2t) + LD2\n2T 2 .\nRecalling that (1\u2212 1/T )T \u2264 1/e and f(x1) = f(0) \u2265 0, we have\nf(x\u2217)\u2212 f(xt+1) \u2264 (1\u2212 1/T )t(f(x\u2217)\u2212 f(x1)) + 1\n2T t\u2211 i=1 (\u03b2iD 2 + \u2016f(xi)\u2212 di\u20162/\u03b2i) + LD2 2T\n\u2264 1 e f(x\u2217) + 1 2T t\u2211 i=1 (\u03b2iD 2 + \u2016f(xi)\u2212 di\u20162/\u03b2i) + LD2 2T ,\nwhich in turn yields\n(1\u2212 1/e)f(x\u2217)\u2212 f(xt+1) \u2264 1\n2T t\u2211 i=1 (\u03b2iD 2 + \u2016f(xi)\u2212 di\u20162/\u03b2i) + LD2 2T .\nTaking expectation in both sides gives\n(1\u2212 1/e)E[f(x\u2217)]\u2212 E[f(xt+1)] \u2264 1\n2T t\u2211 i=1 (\u03b2iD 2 + E[\u2016f(xi)\u2212 di\u20162]/\u03b2i) + LD2 2T .\nNotice that \u2016\u2207f(xt)\u2212\u2207f(xt\u22121)\u2016 \u2264 L\u2016vt\u2016/K \u2264 LR/K \u2264 2LR/(k + 3). By Theorem 3, if we set \u03c1i = 2(i+3)2/3 , we have\nE[\u2016f(xi)\u2212 di\u20162] \u2264 Q\n(i+ 4)2/3\nfor every i \u2264 T and Q = max{\u2016\u2207f(0)\u2016242/3, 4\u03c32 + 6L2R2}. If we set \u03b2i = Q 1/2\nD(i+4)1/3 , we have\n(1\u2212 1/e)E[f(x\u2217)]\u2212 E[f(xt+1)] \u2264 t\u2211\ni=1\nDQ1/2 (i+ 4)1/3T + LD2 2T \u2264 3DQ 1/2t2/3 2T + LD2 2T\nsince \u2211t\ni=1 1 (i+4)1/3 \u2264 \u222b t 0 1 (x+4)1/3 dx = 32 [(x+ 4) 2/3]t0 \u2264 32 [x 2/3]t0 = 3 2 t 2/3.\nTherefore we have\n(1\u2212 1/e)TE[f(x\u2217)]\u2212 T\u2211\nt=1\nE[f(xt)]\n= (1\u2212 1/e)E[f(x\u2217)]\u2212 f(0) + T\u22121\u2211 t=1 [(1\u2212 1/e)E[f(x\u2217)]\u2212 E[f(xt)]]\n\u2264 (1\u2212 1/e)E[f(x\u2217)]\u2212 f(0) + T\u22121\u2211 t=1 [ 3DQ1/2t2/3 2T + LD2 2T ] .\nSince \u2211T\u22121\nt=1 t 2/3 = 1 + \u2211T\u22121 t=2 t 2/3 \u2264 1 + \u222b T 1 t2/3dt = 35T 5/3 + 25 , we conclude\n(1\u2212 1/e)TE[f(x\u2217)]\u2212 T\u2211\nt=1\nE[f(xt)] \u2264 (1\u2212 1/e)E[f(x\u2217)]\u2212 f(0) + 3DQ1/2\n10 (3T 2/3 + 2T\u22121) +\nLD2\n2 = O(T 2/3)."}], "year": 2018, "references": [{"title": "Algorithms for portfolio management based on the newton method", "authors": ["A. Agarwal", "E. Hazan", "S. Kale", "R.E. Schapire"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "year": 2006}, {"title": "Pipage rounding: A new method of constructing algorithms with proven performance guarantee", "authors": ["A.A. Ageev", "M.I. Sviridenko"], "venue": "Journal of Combinatorial Optimization,", "year": 2004}, {"title": "Variance reduction for faster non-convex optimization", "authors": ["Z. Allen-Zhu", "E. Hazan"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Submodular functions: from discrete to continous domains", "authors": ["F. Bach"], "venue": "arXiv preprint arXiv:1511.00394,", "year": 2015}, {"title": "Guaranteed non-convex optimization: Submodular maximization over continuous domains", "authors": ["A. Bian", "B. Mirzasoleiman", "J.M. Buhmann", "A. Krause"], "year": 2017}, {"title": "Second-order kernel online convex optimization with adaptive sketching", "authors": ["D. Calandriello", "A. Lazaric", "M. Valko"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "Maximizing a monotone submodular function subject to a matroid constraint", "authors": ["G. Calinescu", "C. Chekuri", "M. P\u00e1l", "J. Vondr\u00e1k"], "venue": "SIAM Journal on Computing,", "year": 2011}, {"title": "Online continuous submodular maximization", "authors": ["L. Chen", "H. Hassani", "A. Karbasi"], "venue": "In AISTATS, pp. to appear,", "year": 2018}, {"title": "Following the perturbed leader for online structured learning", "authors": ["A. Cohen", "T. Hazan"], "venue": "In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37,", "year": 2015}, {"title": "Turning down the noise in the blogosphere", "authors": ["K. El-Arini", "G. Veda", "D. Shahaf", "C. Guestrin"], "venue": "In SIGKDD,", "year": 2009}, {"title": "An algorithm for quadratic programming", "authors": ["M. Frank", "P. Wolfe"], "venue": "Naval Research Logistics (NRL),", "year": 1956}, {"title": "A linearly convergent conditional gradient algorithm with applications to online and stochastic optimization", "authors": ["D. Garber", "E. Hazan"], "venue": "arXiv preprint arXiv:1301.4666,", "year": 2013}, {"title": "Eigentaste: A constant time collaborative filtering", "authors": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "algorithm. information retrieval,", "year": 2001}, {"title": "Online submodular maximization under a matroid constraint with application to learning assignments", "authors": ["D. Golovin", "A. Krause", "M. Streeter"], "venue": "Technical report,", "year": 2014}, {"title": "Gradient methods for submodular maximization", "authors": ["H. Hassani", "M. Soltanolkotabi", "A. Karbasi"], "venue": "arXiv preprint arXiv:1708.03949,", "year": 2017}, {"title": "Projection-free online learning", "authors": ["E. Hazan", "S. Kale"], "venue": "In ICML, pp", "year": 2012}, {"title": "Variance-reduced and projection-free stochastic optimization", "authors": ["E. Hazan", "H. Luo"], "venue": "In ICML,", "year": 2016}, {"title": "Monotone closure of relaxed constraints in submodular optimization: Connections between minimization and maximization", "authors": ["R. Iyer", "S. Jegelka", "J. Bilmes"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "authors": ["R. Johnson", "T. Zhang"], "venue": "In NIPS, pp", "year": 2013}, {"title": "On the online Frank-Wolfe algorithms for convex and non-convex optimizations", "authors": ["J. Lafond", "Wai", "H.-T", "E. Moulines"], "venue": "arXiv preprint arXiv:1510.01171,", "year": 2015}, {"title": "Mixed optimization for smooth functions", "authors": ["M. Mahdavi", "L. Zhang", "R. Jin"], "venue": "In NIPS, pp", "year": 2013}, {"title": "Conditional gradient method for stochastic submodular maximization: Closing the gap", "authors": ["A. Mokhtari", "H. Hassani", "A. Karbasi"], "venue": "In AISTATS,", "year": 2018}, {"title": "Stochastic conditional gradient methods: From convex minimization to submodular maximization", "authors": ["A. Mokhtari", "H. Hassani", "A. Karbasi"], "venue": "arXiv preprint arXiv:1804.09554,", "year": 2018}, {"title": "An online algorithm for maximizing submodular functions", "authors": ["M. Streeter", "D. Golovin"], "venue": "In NIPS, pp", "year": 2009}, {"title": "Submodularity in combinatorial optimization", "authors": ["J. Vondr\u00e1k"], "venue": "PhD thesis, Charles University,", "year": 2007}, {"title": "Submodular function maximization via the multilinear relaxation and contention resolution schemes", "authors": ["J. Vondr\u00e1k", "C. Chekuri", "R. Zenklusen"], "venue": "In STOC,", "year": 2011}, {"title": "Fast model predictive control using online optimization", "authors": ["Y. Wang", "S. Boyd"], "venue": "IFAC Proceedings Volumes,", "year": 2008}, {"title": "An analysis of the greedy algorithm for the submodular set covering problem", "authors": ["L.A. Wolsey"], "year": 1982}, {"title": "Dual averaging methods for regularized stochastic learning and online optimization", "authors": ["L. Xiao"], "venue": "J. Mach. Learn. Res.,", "year": 2010}, {"title": "Linear submodular bandits and their application to diversified retrieval", "authors": ["Y. Yue", "C. Guestrin"], "venue": "In NIPS, pp", "year": 2011}, {"title": "An information flow model for conflict and fission in small groups", "authors": ["W.W. Zachary"], "venue": "Journal of anthropological research,", "year": 1977}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "authors": ["M. Zinkevich"], "venue": "In ICML, pp", "year": 2003}], "id": "SP:388c1432b679364851080a1b8c1905f4e84eff2f", "authors": [{"name": "Lin Chen", "affiliations": []}, {"name": "Christopher Harshaw", "affiliations": []}, {"name": "Hamed Hassani", "affiliations": []}, {"name": "Amin Karbasi", "affiliations": []}], "abstractText": "Online optimization has been a successful framework for solving large-scale problems under computational constraints and partial information. Current methods for online convex optimization require either a projection or exact gradient computation at each step, both of which can be prohibitively expensive for large-scale applications. At the same time, there is a growing trend of nonconvex optimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been proposed as a broad class of non-convex functions which may be efficiently optimized. Although online methods have been introduced, they suffer from similar problems. In this work, we propose Meta-Frank-Wolfe, the first online projectionfree algorithm that uses stochastic gradient estimates. The algorithm relies on a careful sampling of gradients in each round and achieves the optimal O( \u221a T ) adversarial regret bounds for convex and continuous submodular optimization. We also propose One-Shot Frank-Wolfe, a simpler algorithm which requires only a single stochastic gradient estimate in each round and achieves an O(T ) stochastic regret bound for convex and continuous submodular optimization. We apply our methods to develop a novel \u201clifting\u201d framework for the online discrete submodular maximization and also see that they outperform current state-of-the-art techniques on various experiments. Yale Institute for Network Science, Yale University, New Haven, CT, USA Department of Electrical Engineering, Yale University Department of Computer Science, Yale University Department of Electrical and Systems Engineering, University of Pennsylvania, Philadelphia, PA, USA. Correspondence to: Lin Chen <lin.chen@yale.edu>.", "title": "Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity"}