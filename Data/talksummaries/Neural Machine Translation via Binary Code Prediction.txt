0	14	When handling broad or open domains, machine translation systems usually have to handle a large vocabulary as their inputs and outputs.
1	32	This is particularly a problem in neural machine translation (NMT) models (Sutskever et al., 2014), such as the attention-based models (Bahdanau et al., 2014; Luong et al., 2015) shown in Figure 1.
3	14	Because this is a significant problem for neural language and translation models, there are a number of methods proposed to resolve this problem, which we detail in Section 2.2.
4	26	However, none of these previous methods simultaneously satisfies the following desiderata, all of which, we argue, are desirable for practical use in NMT systems: Memory efficiency: The method should not require large memory to store the parameters and calculated vectors to maintain scalability in resource-constrained environments.
7	24	Compatibility with parallel computation: It should be easy for the method to be minibatched and optimized to run efficiently on GPUs, which are essential for training large NMT models.
15	37	Most of current NMT models use one-hot representations to represent the words in the output vocabulary – each word w is represented by a unique sparse vector eid(w) ∈ RV , in which only one element at the position corresponding to the word ID id(w) ∈ {x ∈ N | 1 ≤ x ≤ V } is 1, while others are 0.
19	43	Several previous works have proposed methods to reduce computation in the output layer.
21	19	However, this method still requires O(HV ) space for the parameters, and requires calculation much more complicated than the standard softmax, particularly at test time.
22	11	The differentiated softmax (Chen et al., 2016) divides words into clusters, and predicts words using separate part of the hidden layer for each word clusters.
26	16	However, these methods are basically not able to be applied at test time, still require heavy computation like the standard softmax.
34	12	Second, all unique words can be discriminated by b, i.e., all bit arrays satisfy that:1 id(w) 6= id(w′)⇒ b(w) 6= b(w′).
40	16	However, this calculation may generate invalid bit arrays which do not correspond to actual words according to the mapping between words and bit arrays.
46	57	For learning correct binary representations, we can use any loss functions that is (sub-)differentiable and satisfies a constraint that: LB(q, b) { = L, if q = b, ≥ L, otherwise, (9) Algorithm 1 Mapping words to bit arrays.
50	22	The computational complexity for the parameters Whq and βq is O(HB).
52	17	For example, if we chose V = 65536 = 216 and use Algorithm 1’s mapping method, then B = 16 and total amount of computation in the output layer could be suppressed to 1/4096 of its original size.
61	13	As a result, the proposed model mostly learns characteristics for frequent words and cannot obtain enough opportunities to learn for rare words.
62	15	To alleviate this problem, we introduce a hybrid model using both softmax prediction and binary code prediction as shown in Figure 2(c).
64	28	When the softmax layer predicts OTHER, then the binary code layer is used to predict the representation of rare words.
67	20	These also can be adjusted according to the training data, but in this study, we only used λH = λB = 1 for simplicity.
106	19	Algorithm 2 increases the number of bits from B intoB′ = 2(B+6), but does not restrict the actual value of B.
117	15	For evaluating the quality of each model, we calculated case-insensitive BLEU (Papineni et al., 2002) every 1000 mini-batches.
137	32	In contrast, Hybrid-N and Binary-EC models clearly improve BLEU from Binary, and they approach that of Softmax.
138	29	This demonstrates that these two methods effectively improve the robustness of binary code prediction models.
145	40	In particular on CPU, where the computation speed is directly affected by the size of the output layer, the proposed methods translate significantly faster than Softmax by x5 to x20.
150	25	In addition, we can see that the BLEU score in BTEC quickly improves, and saturates at N = 1024 in contrast to the ASPEC model, which is still improving at N = 2048.
151	20	We presume that the shape of curves in Figure 6 is also affected by the difficulty of the corpus, i.e., when we train the hybrid model for easy datasets (e.g., BTEC is easier than ASPEC), it is enough to use a small softmax layer (e.g. N ≤ 1024).
152	29	In this study, we proposed neural machine translation models which indirectly predict output words via binary codes, and two model improvements: a hybrid prediction model using both softmax and binary codes, and introducing error-correcting codes to introduce robustness of binary code prediction.
153	27	Experiments show that the proposed model can achieve comparative translation qualities to standard softmax prediction, while significantly suppressing the amount of parameters in the output layer, and improving calculation speeds while training and especially testing.
154	119	One interesting avenue of future work is to automatically learn encodings and error correcting codes that are well-suited for the type of binary code prediction we are performing here.
155	81	In Algorithms 2 and 3 we use convolutions that were determined heuristically, and it is likely that learning these along with the model could result in improved accuracy or better compression capability.
