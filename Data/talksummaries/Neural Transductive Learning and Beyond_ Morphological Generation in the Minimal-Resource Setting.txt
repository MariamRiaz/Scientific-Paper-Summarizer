0	30	Morphological generation of previously unencountered word forms is a crucial problem in many areas of natural language processing (NLP).
1	51	High performance can lead to better systems for downstream tasks, e.g., machine translation (Tamchyna et al., 2017).
2	28	Since existing lexicons have limited coverage, learning morphological inflection patterns from labeled data is an important mission and has recently been the subject of multiple shared tasks (Cotterell et al., 2016, 2017a).
3	25	In morphologically rich languages, words inflect, i.e., they change their surface form in oder to express certain properties, e.g., number or tense.
4	19	A word’s canonical form, which can be found in a dictionary, is called the lemma, and the set of all inflected forms is referred to as the lemma’s paradigm.
5	117	In this work, we address paradigm completion (PC), the morphological task of, given a partial paradigm of a lemma, generating all of its missing forms.
6	95	For the partial paradigm represented by the input subset {(“Schneemannes”, GEN;SG), (“Schneemännern”, DAT;PL)} of the German noun “Schneemann” shown in Figure 1, the goal of PC is to generate the output subset consisting of the six remaining forms.
7	15	Neural seq2seq models define the state of the art for morphological generation if training sets are large; however, they have been less successful in the low-resource setting (Cotterell et al., 2017a).
8	15	In this paper, we address an even more extreme minimalresource setting: for some of our experiments, our training sets only contain k⇡10 paradigms.
9	20	Each paradigm has multiple cells, so the number of forms (as opposed to the number of paradigms) is not necessarily minimal.
10	19	However, we will see that generalizing from paradigm to paradigm is a key challenge, making the number of paradigms a good measure of the effective training set size.
11	35	We propose two PC methods for the minimalresource setting: paradigm transduction and source selection with high precision (SHIP).
12	29	We define a learning algorithm as transductive1 if its goal is to generalize from specific training examples to specific test examples (Vapnik, 1998).
15	11	There is no such dependence in inductive inference.
16	117	Our motivation for transduction is that, in the minimal-resource setting, neural seq2seq models capture relationships between paradigm cells like affix substitution and umlauting, but are tied to the idiosyncracies of the k training paradigms.
17	55	For example, if all source forms in the training set start with “b” or “d”, a purely inductive model may then be unable to generate targets with different initials.
18	26	By transductive inference on the information available in the input subset at test time, i.e., the given partial paradigm, our model can learn idiosyncracies.
20	25	Thus, we exploit the input subset for learning idiosyncracies at test time and then generate the output subset using a modified model.
28	16	Then, it identifies the most deterministic source slot for the generation of each target inflection.
30	58	Our experiments show that, in an extreme minimal-resource setting, a combination of SHIP and a non-neural approach is most effective; for slightly more data, a combination of a neural model, paradigm transduction and SHIP obtains the best results.
31	53	(i) We introduce neural paradigm transduction, which exploits the structure of the PC task to mitigate the negative effect of limited training data.
36	18	A training set in our setup consists of complete paradigms, i.e., all inflected forms of each lemma are available.
40	18	The PC task consists of generating the output subset of the paradigm, i.e., the forms belonging to form-tag pairs which are missing from the collected subset.
42	21	In this section, we first cover required background on MED and then introduce our new approaches.
43	170	MED converts one inflected form of a paradigm into another, given the two respective tags.
44	105	Thus, the input of MED is a sequence of subtags of the source and the target form (e.g., NOM and SG are subtags of NOM;SG), as well as the characters of the source form.
45	30	All elements are represented by embeddings, which are trained together with the model.
46	39	The output of MED is the character sequence of the target inflected form.
47	58	An example from the paradigm in Figure 1 is: INPUT: DATS PLS GENT SGT S c h n e e m ä n n e r n OUTPUT: S c h n e e m a n n e s Encoder.
48	10	The model’s encoder consists of a bidirectional gated recurrent neural network (GRU) with a single hidden layer.
