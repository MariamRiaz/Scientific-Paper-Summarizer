0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.",text,[0.9528030789759568],"['The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.']"
"Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision.",1 Introduction,[0],[0]
"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence.",1 Introduction,[0],[0]
"That is, how to capture the
1https://www.github.com/ facebookresearch/InferSent
relationships among multiple words and phrases in a single vector remains an question to be solved.
",1 Introduction,[0],[0]
"In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.",1 Introduction,[0],[0]
"Two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained.",1 Introduction,[0],[0]
"Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought (Kiros et al., 2015) or FastSent (Hill et al., 2016).",1 Introduction,[0],[0]
"Here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the ImageNet (Deng et al., 2009) before being transferred.",1 Introduction,[0],[0]
"We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.",1 Introduction,[0],[0]
"We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.
",1 Introduction,[0],[0]
"Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.",1 Introduction,[0],[0]
"Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes.",1 Introduction,[0],[0]
"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings com-
670
pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.",1 Introduction,[0],[0]
We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.,1 Introduction,[0],[0]
"Transfer learning using supervised features has been successful in several computer vision applications (Razavian et al., 2014).",2 Related work,[0],[0]
"Striking examples include face recognition (Taigman et al., 2014) and visual question answering (Antol et al., 2015), where image features trained on ImageNet (Deng et al., 2009) and word embeddings trained on large unsupervised corpora are combined.
",2 Related work,[0],[0]
"In contrast, most approaches for sentence representation learning are unsupervised, arguably because the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence.",2 Related work,[0],[0]
"Another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.",2 Related work,[0],[0]
Learning models on large unsupervised task makes it harder for the model to specialize.,2 Related work,[0],[0]
"Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder.",2 Related work,[0],[0]
"They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect.
",2 Related work,[0],[0]
"Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures.",2 Related work,[0],[0]
"SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level.",2 Related work,[0],[0]
"By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks.",2 Related work,[0],[0]
"They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016).",2 Related work,[0],[0]
Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality.,2 Related work,[0],[0]
"In addition to unsupervised methods, they included supervised training in their comparison—namely, on
machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014).",2 Related work,[0],[0]
"These models obtained significantly lower results compared to the unsupervised Skip-Thought approach.
",2 Related work,[0],[0]
"Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015).",2 Related work,[0],[0]
"The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017).",2 Related work,[0],[0]
"To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders.",2 Related work,[0],[0]
"As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data.",2 Related work,[0],[0]
"This work combines two research directions, which we describe in what follows.",3 Approach,[0],[0]
"First, we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task.",3 Approach,[0],[0]
"We subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use.",3 Approach,[0],[0]
"Specifically, we examine standard recurrent models such as LSTMs and GRUs, for which we investigate mean and maxpooling over the hidden representations; a selfattentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction.",3 Approach,[0],[0]
"The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral.",3.1 The Natural Language Inference task,[0],[0]
"It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.",3.1 The Natural Language Inference task,[0],[0]
"We hypothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence
embeddings in a supervised way.",3.1 The Natural Language Inference task,[0],[0]
"That is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features.
",3.1 The Natural Language Inference task,[0],[0]
"Models can be trained on SNLI in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).
",3.1 The Natural Language Inference task,[0],[0]
"Since our goal is to train a generic sentence encoder, we adopt the first setting.",3.1 The Natural Language Inference task,[0],[0]
"As illustrated in Figure 1, a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v.",3.1 The Natural Language Inference task,[0],[0]
"Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : (i) concatenation of the two representations (u, v); (ii) element-wise product u ∗ v; and (iii) absolute element-wise difference |u− v|.",3.1 The Natural Language Inference task,[0],[0]
"The resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fullyconnected layers culminating in a softmax layer.",3.1 The Natural Language Inference task,[0],[0]
"A wide variety of neural networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information.",3.2 Sentence encoder architectures,[0],[0]
"We compare 7 different architectures: standard recurrent encoders with either Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), concatenation of last hidden states of forward and backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive network and hierarchical convolutional networks.",3.2 Sentence encoder architectures,[0],[0]
"Our first, and simplest, encoders apply recurrent neural networks using either LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) modules, as in sequence to sequence encoders (Sutskever et al., 2014).",3.2.1 LSTM and GRU,[0],[0]
"For a sequence of T words (w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ), the network computes a set of T hidden representations h1, . . .",3.2.1 LSTM and GRU,[0],[0]
", hT , with ht = −−−−→ LSTM(w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ) (or using GRU units instead).",3.2.1 LSTM and GRU,[0],[0]
"A sentence is represented by the last hidden vector, hT .
",3.2.1 LSTM and GRU,[0],[0]
"We also consider a model BiGRU-last that concatenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.",3.2.1 LSTM and GRU,[0],[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.2.2 BiLSTM with mean/max pooling,[0],[0]
For t ∈,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", T ], ht, is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions:
−→ ht = −−−−→ LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )←−
",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"ht = ←−−−− LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )",3.2.2 BiLSTM with mean/max pooling,[0],[0]
ht =,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[ −→ ht , ←− ht ]
We experiment with two ways of combining the varying number of {ht}t to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) (Collobert and Weston, 2008) or by considering the average of the representations (mean pooling).",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"The self-attentive sentence encoder (Liu et al., 2016; Lin et al., 2017) uses an attention mechanism over the hidden states of a BiLSTM to generate a representation u of an input sentence.",3.2.3 Self-attentive network,[0],[0]
"The attention mechanism is defined as :
h̄i = tanh(Whi + bw)
αi = eh̄",3.2.3 Self-attentive network,[0],[0]
T,3.2.3 Self-attentive network,[0],[0]
"i uw∑
",3.2.3 Self-attentive network,[0],[0]
"i e h̄Ti uw u = ∑
t
αihi
where {h1, . . .",3.2.3 Self-attentive network,[0],[0]
", hT } are the output hidden vectors of a BiLSTM.",3.2.3 Self-attentive network,[0],[0]
"These are fed to an affine transformation (W , bw) which outputs a set of keys (h̄1, . . .",3.2.3 Self-attentive network,[0],[0]
", h̄T ).",3.2.3 Self-attentive network,[0],[0]
The {αi} represent the score of similarity between the keys and a learned context query vector uw.,3.2.3 Self-attentive network,[0],[0]
"These weights are used to produce the final representation u, which is a weighted linear combination of the hidden vectors.
",3.2.3 Self-attentive network,[0],[0]
"Following Lin et al. (2017) we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task.",3.2.3 Self-attentive network,[0],[0]
"Concretely, we have 4 context vectors u1w, u 2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u. Figure 3 illustrates this architecture.",3.2.3 Self-attentive network,[0],[0]
"One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent (Zhao et al., 2015), which concatenates different representations of the sentences
at different level of abstractions.",3.2.4 Hierarchical ConvNet,[0],[0]
"Inspired by this architecture, we introduce a faster version consisting of 4 convolutional layers.",3.2.4 Hierarchical ConvNet,[0],[0]
"At every layer, a representation ui is computed by a max-pooling operation over the feature maps (see Figure 4).
",3.2.4 Hierarchical ConvNet,[0],[0]
The final representation u =,3.2.4 Hierarchical ConvNet,[0],[0]
"[u1, u2, u3, u4] concatenates representations at different levels of the input sentence.",3.2.4 Hierarchical ConvNet,[0],[0]
The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.,3.2.4 Hierarchical ConvNet,[0],[0]
"For all our models trained on SNLI, we use SGD with a learning rate of 0.1 and a weight decay of 0.99.",3.3 Training details,[0],[0]
"At each epoch, we divide the learning rate by 5 if the dev accuracy decreases.",3.3 Training details,[0],[0]
We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10−5.,3.3 Training details,[0],[0]
"For the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units.",3.3 Training details,[0],[0]
We use opensource GloVe vectors trained on Common Crawl 840B2 with 300 dimensions as fixed word embeddings.,3.3 Training details,[0],[0]
"Our aim is to obtain general-purpose sentence embeddings that capture generic information that is
2https://nlp.stanford.edu/projects/ glove/
useful for a broad set of tasks.",4 Evaluation of sentence representations,[0],[0]
"To evaluate the quality of these representations, we use them as features in 12 transfer tasks.",4 Evaluation of sentence representations,[0],[0]
We present our sentence-embedding evaluation procedure in this section.,4 Evaluation of sentence representations,[0],[0]
We constructed a sentence evaluation tool3 to automate evaluation on all the tasks mentioned in this paper.,4 Evaluation of sentence representations,[0],[0]
"The tool uses Adam (Kingma and Ba, 2014) to fit a logistic regression classifier, with batch size 64.
",4 Evaluation of sentence representations,[0],[0]
"Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).",4 Evaluation of sentence representations,[0.9602948044649519],"['We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called Consistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}: CORE D,S,E (m) = MIN p∈S ( AVG d∈D (RS(Ed(p)) + Vd∈D(RS(Ed(p))) ) AVG d∈D ( RS(Ed(m)) ) + Vd∈D ( RS(Ed(m)) ) (7) With RS(Ed(m)) the rank of m according to the evaluation measure E on dataset d w.r.t.']"
We generate sentence vectors and train a logistic regression on top.,4 Evaluation of sentence representations,[0],[0]
"A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited.",4 Evaluation of sentence representations,[0],[0]
"We tune the L2 penalty of the logistic regression with grid-search on the validation set.
",4 Evaluation of sentence representations,[0],[0]
Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R).,4 Evaluation of sentence representations,[0],[0]
We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint representation.,4 Evaluation of sentence representations,[0],[0]
"For semantic relatedness evaluation, we follow the approach of (Tai et al., 2015) and learn to predict the probability distribution of relatedness scores.",4 Evaluation of sentence representations,[0],[0]
"We report Pearson correlation.
",4 Evaluation of sentence representations,[0],[0]
"STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with
3https://www.github.com/ facebookresearch/SentEval
a similarity score between 0 and 5.",4 Evaluation of sentence representations,[0],[0]
"These tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations.
",4 Evaluation of sentence representations,[0],[0]
Paraphrase detection The Microsoft Research Paraphrase Corpus is composed of pairs of sentences which have been extracted from news sources on the Web.,4 Evaluation of sentence representations,[0],[0]
Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship.,4 Evaluation of sentence representations,[0],[0]
"We use the same approach as with SICK-E, except that our classifier has only 2 classes.
",4 Evaluation of sentence representations,[0],[0]
"Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Hodosh et al., 2013; Lin et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Caption Retrieval).",4 Evaluation of sentence representations,[0],[0]
"We use a pairwise rankingloss Lcir(x, y): ∑ y ∑ k
max(0, α− s(V y, Ux) + s(V y, Uxk))",4 Evaluation of sentence representations,[0],[0]
"+∑ x ∑ k′ max(0, α− s(Ux, V y) + s(Ux, V yk′))
",4 Evaluation of sentence representations,[0],[0]
"where (x, y) consists of an image y with one of its associated captions x, (yk)k and (yk′)k′ are negative examples of the ranking loss, α is the margin and s corresponds to the cosine similarity.",4 Evaluation of sentence representations,[0],[0]
U and V are learned linear transformations that project the caption x and the image y to the same embedding space.,4 Evaluation of sentence representations,[0],[0]
We use a margin α = 0.2 and 30 contrastive terms.,4 Evaluation of sentence representations,[0],[0]
"We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test.",4 Evaluation of sentence representations,[0],[0]
"For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits.",4 Evaluation of sentence representations,[0],[0]
"For fair comparison, we also report SkipThought results in our setting, using 2048-dimensional pretrained ResNet101 (He et al., 2016) with 113k training images.",4 Evaluation of sentence representations,[0],[0]
"In this section, we refer to ”micro” and ”macro” averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples.",5 Empirical results,[0],[0]
Model We observe in Table 3 that different models trained on the same NLI corpus lead to different transfer tasks results.,5.1 Architecture impact,[0],[0]
The BiLSTM-4096 with the max-pooling operation performs best on both SNLI and transfer tasks.,5.1 Architecture impact,[0],[0]
"Looking at the micro and macro averages, we see that it performs significantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet.
",5.1 Architecture impact,[0],[0]
"Table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM-Mean for instance.
",5.1 Architecture impact,[0],[0]
We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence.,5.1 Architecture impact,[0],[0]
"For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks.",5.1 Architecture impact,[0],[0]
"On the other hand, BiLSTM-Mean does not make sharp choices on which part of the sentence is more important than others.",5.1 Architecture impact,[0],[0]
"The difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand.
",5.1 Architecture impact,[0],[0]
"For a given model, the transfer quality is also sensitive to the optimization algorithm: when training with Adam instead of SGD, we observed that the BiLSTM-max converged faster on SNLI (5 epochs instead of 10), but obtained worse results on the transfer tasks, most likely because of the model and classifier’s increased capability to over-specialize on the training task.
",5.1 Architecture impact,[0],[0]
"Embedding size Figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with
Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57",5.1 Architecture impact,[0],[0]
ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0,5.1 Architecture impact,[0],[0]
90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7,5.1 Architecture impact,[0],[0]
.65/.64 fastText BOW†,5.1 Architecture impact,[0],[0]
76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62,5.1 Architecture impact,[0],[0]
GloVe BOW†,5.1 Architecture impact,[0],[0]
78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9,5.1 Architecture impact,[0],[0]
.51/.54,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48
Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35",5.1 Architecture impact,[0],[0]
"SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45
Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70",5.1 Architecture impact,[0],[0]
NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67
Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - -
Table 4: Transfer test results for various architectures trained in different ways.",5.1 Architecture impact,[0],[0]
"Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way.",5.1 Architecture impact,[0],[0]
"† indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016).",5.1 Architecture impact,[0],[0]
"For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014).",5.1 Architecture impact,[0.9540023289200468],"['We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).']"
(*),5.1 Architecture impact,[0],[0]
"Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods.
regard to the embedding size.
",5.1 Architecture impact,[0],[0]
"Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models.",5.1 Architecture impact,[0],[0]
"However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows.",5.1 Architecture impact,[0],[0]
We hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task (results on SNLI are relatively stable with regard to embedding size) but that can nevertheless be useful as features for transfer tasks.,5.1 Architecture impact,[0],[0]
We report in Table 4 transfer tasks results for different architectures trained in different ways.,5.2 Task transfer,[0],[0]
We group models by the nature of the data on which they were trained.,5.2 Task transfer,[0],[0]
The first group corresponds to models trained with unsupervised unordered sentences.,5.2 Task transfer,[0],[0]
"This includes bag-of-words models such as word2vec-SkipGram, the UnigramTFIDF model, the Paragraph Vector model (Le and Mikolov, 2014), the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016) and the SIF model (Arora et al., 2017), all trained on the Toronto book corpus (Zhu et al., 2015).",5.2 Task transfer,[0],[0]
"The second group consists of models trained with unsu-
pervised ordered sentences such as FastSent and SkipThought (also trained on the Toronto book corpus).",5.2 Task transfer,[0],[0]
We also include the FastSent variant “FastSent+AE” and the SkipThought-LN version that uses layer normalization.,5.2 Task transfer,[0],[0]
"We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches.
",5.2 Task transfer,[0],[0]
"Comparison with SkipThought The best performing sentence encoder to date is the SkipThought-LN model, which was trained on a very large corpora of ordered sentences.",5.2 Task transfer,[0],[0]
"With much less data (570k compared to 64M sentences) but with high-quality supervision from the SNLI dataset, we are able to consistently outperform the results obtained by SkipThought vectors.",5.2 Task transfer,[0],[0]
We train our model in less than a day on a single GPU compared to the best SkipThought-LN network trained for a month.,5.2 Task transfer,[0],[0]
"Our BiLSTM-max trained on SNLI performs much better than released SkipThought vectors on MR, CR, MPQA, SST, MRPC-accuracy, SICK-R, SICK-E and STS14 (see Table 4).",5.2 Task transfer,[0],[0]
"Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA.",5.2 Task transfer,[0],[0]
We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST-LN).,5.2 Task transfer,[0],[0]
"We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance (element-wise product and absolute difference) during training.
",5.2 Task transfer,[0.9515653088470141],"['Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems S. The maximum value of CORE is 1 for the best performing system according to its rank.']"
"NLI as a supervised training set Our findings indicate that our model trained on SNLI obtains much better overall results than models trained on other supervised tasks such as COCO, dictionary definitions, NMT, PPDB (Ganitkevitch et al., 2013) and SST.",5.2 Task transfer,[0],[0]
"For SST, we tried exactly the same models as for SNLI; it is worth noting that SST is smaller than NLI.",5.2 Task transfer,[0],[0]
Our representations constitute higher-quality features for both classification and similarity tasks.,5.2 Task transfer,[0],[0]
"One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform NLI is generally discriminative and informative.
",5.2 Task transfer,[0],[0]
Domain adaptation on SICK tasks Our transfer learning approach obtains better results than previous state-of-the-art on the SICK task - can be seen as an out-domain version of SNLI - for both entailment and relatedness.,5.2 Task transfer,[0],[0]
"We obtain a pearson score of 0.885 on SICK-R while (Tai et al., 2015) obtained 0.868, and we obtain 86.3% test accuracy on SICK-E while previous best handengineered models (Lai and Hockenmaier, 2014) obtained 84.5%.",5.2 Task transfer,[0],[0]
"We also significantly outperformed previous transfer learning approaches on SICK-E (Bowman et al., 2015) that used the parameters of an LSTM model trained on SNLI to fine-tune on SICK (80.8% accuracy).",5.2 Task transfer,[0],[0]
"We hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task.
",5.2 Task transfer,[0],[0]
"Image-caption retrieval results In Table 5, we report results for the COCO image-caption retrieval task.",5.2 Task transfer,[0],[0]
We report the mean recalls of 5 random splits of 1K test images.,5.2 Task transfer,[0],[0]
"When trained with
ResNet features and 30k more training data, the SkipThought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval R@1, and from 25.9 to 30.6 on image retrieval R@1.",5.2 Task transfer,[0],[0]
"Our approach pushes the results even further, from 37.9 to 42.4 on caption retrieval, and 30.6 to 33.2 on image retrieval.",5.2 Task transfer,[0],[0]
"These results are comparable to previous approach of (Ma et al., 2015) that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task.",5.2 Task transfer,[0],[0]
"This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task.
",5.2 Task transfer,[0],[0]
MultiGenre NLI,5.2 Task transfer,[0],[0]
"The MultiNLI corpus (Williams et al., 2017) was recently released as a multi-genre version of SNLI.",5.2 Task transfer,[0],[0]
"With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language.",5.2 Task transfer,[0],[0]
We augment Table 4 with our model trained on both SNLI and MultiNLI (AllNLI).,5.2 Task transfer,[0],[0]
We observe a significant boost in performance overall compared to the model trained only on SLNI.,5.2 Task transfer,[0],[0]
"Our model even reaches AdaSent performance on CR, suggesting that having a larger coverage for the training task helps learn even better general representations.",5.2 Task transfer,[0],[0]
"On semantic textual similarity STS14, we are also competitive with PPDB based paragramphrase embeddings with a pearson score of 0.70.",5.2 Task transfer,[0],[0]
"Interestingly, on caption-related transfer tasks such as the COCO image caption retrieval task, training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI (which contains mostly captions), which confirms the generalization power of our embeddings.",5.2 Task transfer,[0],[0]
This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks.,6 Conclusion,[0],[0]
We showed that models learned on NLI can perform better than models trained in unsupervised conditions or on other supervised tasks.,6 Conclusion,[0],[0]
"By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.
",6 Conclusion,[0],[0]
We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.,6 Conclusion,[0],[0]
Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level.,6 Conclusion,[0],[0]
"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features.",abstractText,[0],[0]
"Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful.",abstractText,[0],[0]
Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted.,abstractText,[0],[0]
"In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks.",abstractText,[0],[0]
"Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.",abstractText,[0],[0]
Our encoder is publicly available1.,abstractText,[0],[0]
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 242–251, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Current virtual personal assistants (PAs) require users to either formulate complex intents in one utterance (e.g., “call Peter Miller on his mobile phone”) or go through tedious sub-dialogues (e.g., “phone call” – who would you like to call? – “Peter Miller” – I have a mobile number and a work number.",1 Introduction,[0],[0]
Which one do you want?).,1 Introduction,[0],[0]
"This is not how one would interact with a human assistant, where the request would be naturally structured into smaller chunks that individually get acknowledged (e.g., “Can you make a connection for me?” – sure – “with Peter Miller” - uh huh",1 Introduction,[0],[0]
- “on his mobile” - dialling now).,1 Introduction,[0],[0]
"Current PAs signal ongoing understanding by displaying the state of
the recognised speech (ASR) to the user, but not their semantic interpretation of it.",1 Introduction,[0],[0]
Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context.,1 Introduction,[0],[0]
"GoogleNow, for example, might present traffic information to a user picking up their mobile phone at their typical commute time.",1 Introduction,[0],[0]
"These systems display their “understanding” state, but do not allow any type of interaction with it apart from dismissing the provided information.
",1 Introduction,[0],[0]
"In this work, we explore adding a graphical user interface (GUI) modality that makes it possible to see these interaction styles as extremes on a continuum, and to realise positions between these extremes and present a mixed graphical/voice enabled PA that can provide feedback of understanding to the user incrementally as the user’s utterance unfolds–allowing users to make requests in instalments instead of fully thought-out requests.",1 Introduction,[0],[0]
It does this by signalling ongoing understanding in an intuitive tree-like GUI that can be displayed on a mobile device.,1 Introduction,[0],[0]
"We evaluate our system by directing users to perform tasks using it under nonincremental (i.e., ASR endpointing) and incremental conditions and then compare the two conditions.",1 Introduction,[0],[0]
"We further compare a non-adaptive with an adaptive (i.e., infers likely events) version of our system.",1 Introduction,[0],[0]
"We report that the users found the interface intuitive and easy to use, and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system.",1 Introduction,[0],[0]
This work builds upon several threads of previous research: Chai et al. (2014),2 Related Work,[0],[0]
"addressed misalignments in understanding (i.e., common ground (Clark and Schaefer, 1989)) between robots and humans by informing the human of the internal system state via speech.",2 Related Work,[0],[0]
"We take this idea and ap-
242
ply it to a PA by displaying the internal state of the system to the user via a GUI (explained in Section 3.5), allowing the user to determine if system understanding has taken place–a way of providing feedback and backchannels to the user.",2 Related Work,[0],[0]
"Dethlefs et al. (2016) provide a good review of work that show how backchannels facilitate grounding, feedback, and clarifications in human spoken dialogue, and apply an information density approach to determine when to backchannel using speech.",2 Related Work,[0],[0]
"Because we don’t backchannel using speech here, there is no potential overlap between the user and the system; rather, our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps.
",2 Related Work,[0],[0]
"Though different in many ways, our work is similar in some regards to Larsson et al. (2011), which displays information to the user and allows the user to navigate the display itself (e.g., by saying up or down in a menu list)–functionality that we intend to apply to our GUI in future work.",2 Related Work,[0],[0]
"Our work is also comparable to SDS toolkits such as IrisTK (Skantze and Moubayed, 2012) and OpenDial (Lison, 2015) which enable SDS designers to visualise the internal state of their systems, though not for end user interpretability.
",2 Related Work,[0],[0]
"Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service (LUIS) project (Williams et al., 2015).",2 Related Work,[0],[0]
"While our system by no means achieves the scale that LUIS does, we offer here an additional contribution of an open source LUIS-like system (with the important addition of the graphical interface) that is authorable (using JSON files; we leave authoring using a web interface like that of LUIS to future work), extensible (affordances can be easily added), incremental (in that respect going beyond LUIS), trainable (i.e., can learn from examples, but can still function well without examples), and can learn through interacting (here we apply a user model that learns during interaction).",2 Related Work,[0],[0]
"This section introduces and describes our SDS, which is modularised into four main components: ASR, natural language understanding (NLU), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree.",3 System Description,[0],[0]
The overall system is represented in Figure 1.,3 System Description,[0],[0]
"For the remainder of this section, each module is explained in
turn.",3 System Description,[0],[0]
"As each module processes input incrementally (i.e., word for word), we first explain our framework for incremental processing.",3 System Description,[0],[0]
An aspect of our SDS that sets it apart from others is the requirement that it process incrementally.,3.1 Incremental Dialogue,[0],[0]
"One potential concern with incremental processing is regarding informativeness: why act early when waiting might provide additional information, resulting in better-informed decisions?",3.1 Incremental Dialogue,[0],[0]
The trade off is naturalness as perceived by the user who is interacting with the SDS.,3.1 Incremental Dialogue,[0],[0]
"Indeed, it has been shown that human users perceive incremental systems as being more natural than traditional, turn-based systems (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991; Asri et al., 2014), offer a more human-like experience (Edlund et al., 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007).",3.1 Incremental Dialogue,[0],[0]
"Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002).
",3.1 Incremental Dialogue,[0],[0]
The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired.,3.1 Incremental Dialogue,[0],[0]
"Such mechanisms are offered by the incremental unit (IU) framework for SDS (Schlangen and Skantze, 2011), which we apply here.",3.1 Incremental Dialogue,[0],[0]
"Following Kennington et al. (2014), the IU framework consists of a network of processing modules.",3.1 Incremental Dialogue,[0],[0]
"A typical module takes input, performs some kind of processing on that data, and produces output.
",3.1 Incremental Dialogue,[0],[0]
The data are packaged as the payload of incremental units (IUs) which are passed between modules.,3.1 Incremental Dialogue,[0],[0]
"The IUs themselves are interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (see Figure 2 for an example of incremental ASR).",3.1 Incremental Dialogue,[0],[0]
"Thus IUs can be added, but can be later revoked and replaced in light of new information.",3.1 Incremental Dialogue,[0],[0]
"The IU framework can take advantage of up-to-date information, but have the potential to function in such a way that users perceive as more natural.
",3.1 Incremental Dialogue,[0],[0]
The modules explained in the remainder of this section are implemented as IU-modules and process incrementally.,3.1 Incremental Dialogue,[0],[0]
Each will now be explained.,3.1 Incremental Dialogue,[0],[0]
The module that takes speech input from the user in our SDS is the ASR component.,3.2 Speech Recognition,[0],[0]
"Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible (i.e., the ASR must not wait for endpointing to produce output).",3.2 Speech Recognition,[0],[0]
"Each module that follows must also process incrementally, acting in lock-step upon input as it is received.",3.2 Speech Recognition,[0],[0]
"Incremental ASR is not new (Baumann et al., 2009) and many of the current freely-accessible ASR systems can produce output (semi-) incrementally.",3.2 Speech Recognition,[0],[0]
We opt for Google ASR for its vocabulary coverage of our evaluation language (German).,3.2 Speech Recognition,[0],[0]
"Following, Baumann et al. (2016), we package output from the Google service into IUs which are passed to the NLU module, which we now explain.",3.2 Speech Recognition,[0],[0]
We approach the task of NLU as a slot-filling task (a very common approach; see Tur et al. (2012)) where an intent is complete when all slots of a frame are filled.,3.3 Language Understanding,[0],[0]
"The main driver of the NLU in
our SDS is the SIUM model of NLU introduced in Kennington et al. (2013).",3.3 Language Understanding,[0],[0]
"SIUM has been used in several systems which have reported substantial results in various domains, languages, and tasks (Han et al., 2015; Kennington et al., 2015; Kennington and Schlangen, 2017)",3.3 Language Understanding,[0],[0]
"Though originally a model of reference resolution, it was always intended to be used for general NLU, which we do here.",3.3 Language Understanding,[0],[0]
"The model is formalised as follows:
",3.3 Language Understanding,[0],[0]
P (I|U) = 1 P (U) P (I) ∑ r∈R P (U |R = r)P,3.3 Language Understanding,[0],[0]
"(R = r|I) (1)
That is, P (I|U) is the probability of the intent",3.3 Language Understanding,[0],[0]
"I (i.e., a frame slot) behind the speaker’s (ongoing) utterance U .",3.3 Language Understanding,[0],[0]
"This is recovered using the mediating variable R, a set of properties which map between aspects of U and aspects of I .",3.3 Language Understanding,[0],[0]
"We opt for abstract properties here (e.g., the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta, mediterranean, vegetarian, etc.).",3.3 Language Understanding,[0],[0]
Properties are pre-defined by a system designer and can match words that might be uttered to describe the intent in question.,3.3 Language Understanding,[0],[0]
"For P (R|I), probability is distributed uniformly over all properties that a given intent is specified to have.",3.3 Language Understanding,[0],[0]
"(If other information is available, more informative priors could be used as well.)",3.3 Language Understanding,[0],[0]
The mapping between properties and aspects of U can be learned from data.,3.3 Language Understanding,[0],[0]
"During application, R is marginalised over, resulting in a distribution over possible intents.1",3.3 Language Understanding,[0],[0]
"This occurs at each word increment, where the distribution from the previous increment is combined via P (I), keeping track of the distribution over time.
",3.3 Language Understanding,[0],[0]
We further apply a simple rule to add in apriori knowledge: if some r ∈ R and w ∈ U are such that r,3.3 Language Understanding,[0],[0]
".= w (where .= is string equality; e.g., an intent has the property of pasta and the word pasta is uttered), then we set C(U=w|R=r)=1.",3.3 Language Understanding,[0],[0]
"To allow for possible ASR confusions, we also apply C(U=w|R=r)= 1",3.3 Language Understanding,[0],[0]
"− ld(w, r)/max(len(w), len(r)), where ld is the Levenshtein distance (but we only apply this if the calculated value is above a threshold of 0.6; i.e., the two strings are mostly similar).",3.3 Language Understanding,[0],[0]
"For all otherw, C(w|r)=0.",3.3 Language Understanding,[0],[0]
"This results in a distribution C, which we renormalise and blend with learned distribution to yield P (U |R).
",3.3 Language Understanding,[0],[0]
"1In Kennington et al. (2013) the authors apply Bayes’ Rule to allow P (U |R) to produce a distribution over properties, which we adopt here.
",3.3 Language Understanding,[0],[0]
We apply an instantiation of SIUM for each slot.,3.3 Language Understanding,[0],[0]
"The candidate slots which are processed depends on the state of the dialogue; only slots represented by visible nodes are considered, thereby reducing the possible frames that could be predicted.",3.3 Language Understanding,[0],[0]
"At each word increment, the updated slots (and their corresponding) distributions are given to the DM, which will now be explained.",3.3 Language Understanding,[0],[0]
"The DM plays a crucial role in our SDS: as well as determining how to act, the DM is called upon to decide when to act, effectively giving the DM the control over timing of actions rather than relying on ASR endpointing–further separating our SDS from other systems.",3.4 Dialogue Manager,[0],[0]
"The DM policy is based on a confidence score derived from the NLU (in this case, we used the distribution’s argmax value) using thresholds for the actions (see below), set by hand (i.e., trial and error).",3.4 Dialogue Manager,[0],[0]
"At each word and resulting distribution from NLU, the DM needs to choose one of the following:
• wait – wait for more information (i.e., for the next word)
• select – as the NLU is confident enough, fill the slot can with the argmax from NLU
• request – signal a (yes/no) clarification request on the current slot and the proposed filler
• confirm – act on the confirmation of the user; in effect, select the proposed slot value
Though the thresholds are statically set, we applied OpenDial (Lison, 2015) as an IU-module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning (which OpenDial could provide).",3.4 Dialogue Manager,[0],[0]
"The DM processes and makes a decision for each slot, with the assumption that only one slot out of all that are processed will result in an non-wait action (though this is not enforced).",3.4 Dialogue Manager,[0],[0]
The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding.,3.5 Graphical User Interface,[0],[0]
"One motivation for this is that the user can determine if the system understood the user’s intent before providing the user with a response
(e.g., a list of restaurants of a certain type); i.e., if any misunderstanding takes place, it happens before the system commits to an action and is potentially more easily repaired.
",3.5 Graphical User Interface,[0],[0]
"The display is a rightbranching tree, where the branches directly off the root node display the affordances of the system (i.e., what domains of things it can understand and do something about).",3.5 Graphical User Interface,[0],[0]
"When the first tree is displayed, it represents a state of the NLU where none of the slots are filled, as in Figure 3.
",3.5 Graphical User Interface,[0],[0]
"When a user verbally selects a domain to ask about, the tree is adjusted to make that domain the only one displayed and
the slots that are required for that domain are shown as branches.",3.5 Graphical User Interface,[0],[0]
"The user can then fill those slots (i.e., branches) by uttering the displayed name, or, alternatively, by uttering the item to fill the slot directly.",3.5 Graphical User Interface,[0],[0]
"For example, at a minimum, the user could utter the name of the domain then an item for each slot (e.g., food Thai downtown) or the speech could be more natural (e.g., I’m quite hungry, I am looking for some Thai food maybe in the downtown area).",3.5 Graphical User Interface,[0],[0]
"Crucially, the user can also hesitate within and between chunks, as advancement is not triggered by silence thresholding, but rather semantically.",3.5 Graphical User Interface,[0],[0]
"When something is uttered that falls into the request state of the DM as explained above, the display expands the subtree under question and marks the item with a question mark (see Figure 4).",3.5 Graphical User Interface,[0],[0]
"At this point, the user can utter any kind of confirmation.",3.5 Graphical User Interface,[0],[0]
A positive confirmation fills the slot with the item in question.,3.5 Graphical User Interface,[0],[0]
"A negative confirmation retracts the question, but leaves the branch expanded.",3.5 Graphical User Interface,[0],[0]
The expanded branches are displayed according to their rank as given by the NLU’s probability distribution.,3.5 Graphical User Interface,[0],[0]
"Though a branch in the display can theoretically display an unlimited number of children, we opted to only show 7 children; if a branch had more, the final child displayed as an ellipsis.
",3.5 Graphical User Interface,[0],[0]
"A completed branch is collapsed, visually marking its corresponding slot as filled.",3.5 Graphical User Interface,[0],[0]
"At any
time, a user can backtrack by saying no (or equivalent) or start the entire interaction over from the beginning with a keyword, e.g., restart.",3.5 Graphical User Interface,[0],[0]
"To aid the user’s attention, the node under question is marked in red, where completed slots are represented by outlined nodes, and filled nodes represent candidates for the current slot in question (see examples of all three in Figure 4).",3.5 Graphical User Interface,[0],[0]
"For cases where the system is in the wait state for several words (during which there is no change in the tree), the system signals activity at each word by causing the red node in question to temporarily change to white, then back to red (i.e., appearing as a blinking node to the user).",3.5 Graphical User Interface,[0.9600170134348412],"['We use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf.']"
"Figure 5 shows a filled frame, represented as tree with one branch for each filled slot.
",3.5 Graphical User Interface,[0],[0]
Figure 5: Example tree where all of the slots are filled.,3.5 Graphical User Interface,[0],[0]
"(i.e., domain:food, location:university, type:thai)
",3.5 Graphical User Interface,[0],[0]
Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far.,3.5 Graphical User Interface,[0],[0]
"It is designed to aid the user’s attention to the slot in question, and clearly indicates the affordances that the system has.",3.5 Graphical User Interface,[0],[0]
"The interface is currently a read-only display that is purely speech-driven, but it could be augmented with additional functionalities, such as tapping a node for expansion or typing input that the system might not yet display.",3.5 Graphical User Interface,[0],[0]
"It is currently implemented as a web-based interface (using the JavaScript D3 library), allowing it to be usable as a web application on any machine or mobile device.
",3.5 Graphical User Interface,[0],[0]
"Adaptive Branching The GUI as explained affords an additional straight-forward extension: in order to move our system towards adaptivity on the above-mentioned continuum, the GUI can be used to signal what the system thinks the user might say next.",3.5 Graphical User Interface,[0],[0]
"This is done by expanding a branch and displaying a confirmation on that branch, signalling that the system predicts that the user will choose that particular branch.",3.5 Graphical User Interface,[0],[0]
"Alternatively, if the system is confident that a user will fill a slot with a particular value, that particular slot can be filled without confirmation.",3.5 Graphical User Interface,[0],[0]
This is displayed as a collapsed tree branch.,3.5 Graphical User Interface,[0],[0]
"A system that perfectly predicts a user’s intent would fill an entire tree (i.e., all slots) only requiring the user to confirm once.",3.5 Graphical User Interface,[0],[0]
A more careful system would confirm at each step (such an interaction would only require the user to utter confirmations and nothing else).,3.5 Graphical User Interface,[0],[0]
We applied this adaptive variant of the tree in one of our experiments explained below.,3.5 Graphical User Interface,[0],[0]
"In this section, we describe two experiments where we evaluated our system.",4 Experiments,[0],[0]
It is our primary goal to show that our GUI is useful and signals understanding to the user.,4 Experiments,[0],[0]
We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system.,4 Experiments,[0],[0]
We further want to show that an adaptive system is more effective than a non-adaptive system (though both would process incrementally).,4 Experiments,[0],[0]
"In order to best evaluate our system, we recruited participants to interact with our system in varied settings to compare endpointed (i.e., non-incremental) and nonadaptive as well as adaptive versions.",4 Experiments,[0],[0]
"We describe how the data were collected from the participants, then explain each experiment and give results.",4 Experiments,[0],[0]
The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time.,4.1 Task & Procedure,[0],[0]
Figure 6 shows some example tasks as they would be displayed (one at a time) to the user.,4.1 Task & Procedure,[0],[0]
"A screen, tablet, and keyboard were on the desk in front of the user (see Figure",4.1 Task & Procedure,[0],[0]
7).2,4.1 Task & Procedure,[0],[0]
"The user was instructed to convey the task presented on the screen to the system such
2We used a Samsung 8.4 Pro tablet turned to its side to show a larger width for the tree to grow to the right.",4.1 Task & Procedure,[0],[0]
"The tablet only showed the GUI; the SDS ran on a separate computer.
that the GUI on the tablet would have a completed tree (e.g., as in Figure 5).",4.1 Task & Procedure,[0],[0]
"When the participant was satisfied that the system understood her intent, she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet (as in Figure 3).
",4.1 Task & Procedure,[0],[0]
"The possible task domains were call, which had a single slot for name to be filled (i.e., one out of the 22 most common German given names); message which had a slot for name and a slot for the message (which, when invoked, would simply fill in directly from the
ASR until 1 second of silence was detected); eat which had slots for type (in this case, 6 possible types) and location (in this case, 6 locations based around the city of Bielefeld); route which had slots for source city and the destination city (which shared the same list of the top 100 most populous German cities); and reminder which had a slot for message.
",4.1 Task & Procedure,[0],[0]
"For each task, the domain was first randomly chosen from the 5 possible domains, and then each slot value to be filled was randomly chosen (the message slot for the name and message domains was randomly selected from a list of 6 possible “messages”, each with 2-3 words; e.g., feed the cat, visit grandma, etc.).",4.1 Task & Procedure,[0],[0]
The system kept track of which tasks were already presented to the participant.,4.1 Task & Procedure,[0],[0]
"At any time after the first task, the system could choose a task that was previously presented and present it again to the participant (with a 50% chance) so the user would often see tasks that she had seen before (with the assumption that humans who use PAs often do perform similar, if not the same, tasks more than once).
",4.1 Task & Procedure,[0],[0]
"The participant was told that she would interact with the system in three different phases, each for 4 minutes, and to accomplish as many tasks as possible in that time allotment.",4.1 Task & Procedure,[0],[0]
The participant was not told what the different phases were.,4.1 Task & Procedure,[0],[0]
"The experiments described in Sections 4.2 and
4.3 respectively describe and report a comparison first between the Phase 1 and 2 (denoted as the endpointed and incremental variants of the system) in order to establish whether or not the incremental variant produced better results than the endpointed variant.",4.1 Task & Procedure,[0],[0]
We also report a comparison between Phase 2 and 3 (incremental and incremental-adaptive phases).,4.1 Task & Procedure,[0],[0]
Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2.,4.1 Task & Procedure,[0],[0]
"Because of this, we fixed the order of the phase presentation for all participants.",4.1 Task & Procedure,[0],[0]
Each of these phases are described below.,4.1 Task & Procedure,[0],[0]
"Before the participant began Phase 1, they were able to try it out for up to 4 minutes (in Phase 1 settings) and ask for help from the experimenter, allowing them to get used to the Phase 1 interface before the actual experiment began.",4.1 Task & Procedure,[0],[0]
"After this trial phase, the experiment began with Phase 1.
",4.1 Task & Procedure,[0],[0]
"Phase 1: Non-incremental In this phase, the system did not appear to work incrementally; i.e., the system displayed tree updates after ASR endpointing (of 1.2 seconds–a reasonable amount of time to expect a response from a commercial spoken PA).",4.1 Task & Procedure,[0],[0]
The system displayed the ongoing ASR on the tablet as it was recognised (as is often done in commercial PAs).,4.1 Task & Procedure,[0],[0]
"At the end of Phase 1, a pop up window notified the user that the phase was complete.",4.1 Task & Procedure,[0],[0]
"They then moved onto Phase 2.
",4.1 Task & Procedure,[0],[0]
"Phase 2: Incremental In this phase, the system displayed the tree information incrementally without endpointing.",4.1 Task & Procedure,[0],[0]
"The ASR was no longer displayed; only the tree provided feedback in understanding, as explained in Section 3.5.
",4.1 Task & Procedure,[0],[0]
"After Phase 2, a 10-question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2.",4.1 Task & Procedure,[0],[0]
"For each question, they had the choice of Phase 1, Phase
2, Both, and Neither.",4.1 Task & Procedure,[0],[0]
(See Appendix for full list of questions.),4.1 Task & Procedure,[0],[0]
"After completing the questionnaire, they moved onto Phase 3.
",4.1 Task & Procedure,[0],[0]
"Phase 3: Incremental-adaptive In this phase, the incremental system was again presented to the participant with an added user model that “learned” about the user.",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than once, the user model would predict that, if the user chose that task domain again (e.g., route) then the system would automatically ask a clarification using the previously filled values (except for the message slot, which the user always had to fill).",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than 3 times, the system skipped asking for clarifications and filled in the domain slots completely, requiring the user only to press the space bar to confirm it was the correct one (i.e., to complete the task).",4.1 Task & Procedure,[0],[0]
"An example progression might be as follows: a participant is presented with the task route from Bielefeld to Berlin, then the user would attempt to get the system to fill in the tree (i.e., slots) with those values.",4.1 Task & Procedure,[0],[0]
"After some interaction in other domains, the user sees the same task again, and now after indicating the intent type route, the user must only say “yes” for each slot to confirm the system’s prediction.",4.1 Task & Procedure,[0],[0]
"Later, if the task is presented a third time, when entering that domain (i.e, route), the two slots would already be filled.",4.1 Task & Procedure,[0],[0]
"If later a different route task was presented, e.g., route from Bielefeld to Hamburg, the system would already have the two slots filled, but the user could backtrack by saying “no, to Hamburg” which would trigger the system to fill the appropriate slot with the corrected value.",4.1 Task & Procedure,[0],[0]
"Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant, but continue to fill the from slot with Bielefeld.
",4.1 Task & Procedure,[0],[0]
"After Phase 3, the participants were presented with another questionnaire on the screen to fill out with the same questions (plus two additional questions), this time comparing Phase 2 and Phase 3.",4.1 Task & Procedure,[0],[0]
"For each item, they had the choice of Phase 2, Phase 3, Both, and Neither.",4.1 Task & Procedure,[0],[0]
"At the end of the three phases and questionnaires, the participants were given a final questionnaire to fill out by hand on their general impressions of the systems.
",4.1 Task & Procedure,[0],[0]
We recruited 14 participants for the evaluation.,4.1 Task & Procedure,[0],[0]
"We used the Mint tools data collection framework (Kousidis et al., 2012) to log the interactions.",4.1 Task & Procedure,[0],[0]
"Due to some technical issues, one of the participants
did not log interactions.",4.1 Task & Procedure,[0],[0]
"We collected data from 13 participants, post-Phase 2 questionnaires from 12 participants, post-Phase 3 questionnaires from all 14 participants, and general questionnaires from all 14 participants.",4.1 Task & Procedure,[0],[0]
"In the experiments that follow, we report objective and subjective measures to determine the settings that produced superior results.
",4.1 Task & Procedure,[0],[0]
Metrics We report the subjective results of the participant questionnaires.,4.1 Task & Procedure,[0],[0]
We only report those items that were statistically significant (see Appendix for a full list of the questions).,4.1 Task & Procedure,[0],[0]
"We further report objective measures for each system variant: total number of completed tasks, fully correct frames, average frame f-score, and average time elapsed (averages are taken over all participants for each variant; we only used the 10 participants who fully interacted with all three phases).",4.1 Task & Procedure,[0],[0]
Discussion is left to the end of this section.,4.1 Task & Procedure,[0],[0]
"In this section we report the results of the evaluation between the endpointed (i.e., nonincremental; Phase 1) variant vs the incremental (Phase 2) variant of our system.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Subjective Results We applied a multinomial test of significance to the results, treating all four possible answers as equally likely (with Bonferroni correction of 10).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item The interface was useful and easy to understand with the answer of Both was significant (χ2 (4, N = 12)",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"= 9.0, p < .005), as was The assistant was easy and intuitive to use also with the answer Both (χ2 (4, N = 12) = 9.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item I always understood what the system wanted from me was also answered Both significantly more times than other answers (χ2 (4, N = 14) = 9.0, p< .005), similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both (χ2 (4, N = 12) = 10.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"These responses tell us that though the participants did not report preference for either system variant, they reported a general positive impression of the GUI (in both variants).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a nice result; the GUI could be used in either system with benefit to the users.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
Objective Results The endpointed (Phase 1) and incremental (Phase 2) columns in Table 1 show the results of the objective evaluation.,4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Though the average time per task and fscore for the endpointed variant are better than those of the
incremental variant, the total number of tasks for the incremental variant was higher.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Manual inspection of logs indicate that participants took advantage of the system’s flexibility of understanding instalments (i.e., filling frames incrementally).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is evidenced in that participants often uttered words understood by the system as being negative (e.g., nein/no), either as a result of an explicit confirmation request by the system (e.g., Thai?) or after a slot was incorrectly filled (something very easily determined through the GUI).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a desired outcome of using our system; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent (i.e., frame).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"However, we cannot fully empirically measure these tendencies given our data.",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"In this section we report results for the evaluation between the incremental (Phase 2) and incremental-adaptive (henceforth just adaptive; Phase 3) systems.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Subjective Results We applied the same significance test as Experiment 1 (with Bonferroni correction of 12).,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"The item The interface was useful and easy to understand was answered with Both significantly (χ2 (4, N = 14)",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"= 10.0, p < .0042), The item I had the feeling that the assistant attempted to learn about me was answered with Neither (χ2 (4, N = 14) = 8.0, p < .0042), though Phase 3 was also marked (6 times).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
All other items were not significant.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Here again we see that there is a general positive impression of the GUI under all conditions.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"If anyone noticed that a system variant was attempting to learn a user model at all, they noticed that it was in Phase 3, as expected.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"Objective Results The incremental (Phase 2) and adaptive (Phase 3) columns in Table 1 show
the results for the objective evaluation for this experiment.",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"There is a clear difference between the two variants, with the adaptive showing more completed tasks, more fully correct frames, and a higher average fscore (all three likely due to the fact that frames were potentially pre-filled).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"While the responses don’t express any preference for a particular system variant, the overall impression of the GUI was positive.",4.4 Discussion,[0],[0]
"The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level, due to the higher number of completed tasks and locallymade repairs.",4.4 Discussion,[0],[0]
"There are further gains to be made when the system applies simple user modelling (i.e., adaptivity) by attempting to predict what the user might want to do in a chosen domain, decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks.",4.4 Discussion,[0],[0]
"Participants also didn’t just get used to the system over time, as the average time per episode was fairly similar in all three phases.
",4.4 Discussion,[0],[0]
The open-ended questionnaire sheds additional light.,4.4 Discussion,[0],[0]
"Most of the suggestions for improvement related to ASR misrecognition and speed (i.e., not about the system itself).",4.4 Discussion,[0],[0]
Two participants suggested an ability to add “free input” or select alternatives from the tree.,4.4 Discussion,[0],[0]
"Two participants suggested that the system be more responsive (i.e., in wait states), and give more feedback (i.e., backchannels) more often.",4.4 Discussion,[0],[0]
"For those participants that expressed preference to the non-incremental system (Phase 1), none of them had used a speech-based PA before, whereas those that expressed preference to the incremental versions (Phases 2 and 3) use them regularly.",4.4 Discussion,[0],[0]
"We conjecture that people without SDS experience equate understanding with ASR, whereas those that are more familiar with PAs know that perfect ASR doesn’t translate to perfect understanding–hence the need for a GUI.",4.4 Discussion,[0],[0]
"A potential remedy would be to display ASR with the tree, signalling understanding despite ASR errors.",4.4 Discussion,[0],[0]
"Given the results and analysis, we conclude that an intuitive presentation that signals a system’s ongoing understanding benefits end users who perform simple tasks which might be performed by a PA.",5 Conclusion & Future Work,[0],[0]
"The GUI that we provided, using a right-branching
tree, worked well; indeed, the participants who used it found it intuitive and easy to understand.",5 Conclusion & Future Work,[0],[0]
There are gains to be made when the system signals understanding at finer-grained levels than just at the end of a pre-formulated utterance.,5 Conclusion & Future Work,[0],[0]
There are further gains to be made when a PA attempts to learn (even a rudimentary) user model to predict what the user might want to do next.,5 Conclusion & Future Work,[0],[0]
"The adaptivity moves our system from one extreme of the continuum–simple slot filling–closer towards the extreme that is fully predictive, with the additional benefit of being able to easily correct mistakes in the predictions.
",5 Conclusion & Future Work,[0],[0]
"For future work, we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy.",5 Conclusion & Future Work,[0],[0]
We want to improve the NLU and scale to larger domains.3,5 Conclusion & Future Work,[0],[0]
"We also plan on implementing this as a standalone application that could be run on a mobile device, which could actually perform the tasks.",5 Conclusion & Future Work,[0],[0]
"It would further be beneficial to compare the GUI with a system that responds with speech (i.e., without a GUI).",5 Conclusion & Future Work,[0],[0]
"Lastly, we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system.
",5 Conclusion & Future Work,[0],[0]
Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions.,5 Conclusion & Future Work,[0],[0]
Thanks also to Julian Hough for helping with experiments.,5 Conclusion & Future Work,[0],[0]
"We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and the BMBF KogniHome project.
",5 Conclusion & Future Work,[0],[0]
"Appendix The following questions were asked on both questionnaires following Phase 2 and Phase 3 (comparing the two most latest used system versions; as translated into English):
•",5 Conclusion & Future Work,[0],[0]
The interface was useful and easy to understand.,5 Conclusion & Future Work,[0],[0]
• The assistant was easy and intuitive to use.,5 Conclusion & Future Work,[0],[0]
• The assistant understood what I wanted to say.,5 Conclusion & Future Work,[0],[0]
• I always understood what the system wanted from me.,5 Conclusion & Future Work,[0],[0]
•,5 Conclusion & Future Work,[0],[0]
The assistant made many mistakes.,5 Conclusion & Future Work,[0],[0]
• The assistant did not respond while I spoke.,5 Conclusion & Future Work,[0],[0]
"3Kennington and Schlangen (2017) showed that our chosen NLU approach can scale fairly well, but the GUI has some limits when applied to larger domains with thousands of items.",5 Conclusion & Future Work,[0],[0]
"We leave improved scaling to future work.
",5 Conclusion & Future Work,[0],[0]
"• It was sometimes unclear to me if the assistant understood me.
",5 Conclusion & Future Work,[0],[0]
• The assistant responded while I spoke.,5 Conclusion & Future Work,[0],[0]
• The assistant sometimes did things that I did not expect.,5 Conclusion & Future Work,[0],[0]
"• When the assistant made mistakes, it was easy for me
to correct them.
",5 Conclusion & Future Work,[0],[0]
"In addition to the above 10 questions, the following were also asked on the questionnaire following Phase 3: • I had the feeling that the assistant attempted to learn
about me.
",5 Conclusion & Future Work,[0],[0]
"• I had the feeling that the assistant made incorrect guesses.
",5 Conclusion & Future Work,[0],[0]
The following questions were used on the general questionnaire:,5 Conclusion & Future Work,[0],[0]
"• I regularly use personal assistants such as Siri, Cortana,
Google now or Amazon Echo:",5 Conclusion & Future Work,[0],[0]
"Yes/No
• I have never used a speech-based personal assistant: Yes/No
• What was your general impression of our personal assistants?
",5 Conclusion & Future Work,[0],[0]
• Would you use one of these assistants on a smart phone or tablet if it were available?,5 Conclusion & Future Work,[0],[0]
"If yes, which one?
• Do you have suggestions that you think would help us improve our assistants?
",5 Conclusion & Future Work,[0],[0]
"• If you have used other speech-based interfaces before, do you prefer this interface?",5 Conclusion & Future Work,[0],[0]
"Arguably, spoken dialogue systems are most often used not in hands/eyes-busy situations, but rather in settings where a graphical display is also available, such as a mobile phone.",abstractText,[0],[0]
We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system.,abstractText,[0],[0]
"By visualising the current dialogue state and possible continuations of it as a simple tree, and allowing interaction with that visualisation (e.g., for confirmations or corrections), the system provides both feedback on past user actions and guidance on possible future ones, and it can span the continuum from slot filling to full prediction of user intent (such as GoogleNow).",abstractText,[0],[0]
"We evaluate our system with real users and report that they found the system intuitive and easy to use, and that incremental and adaptive settings enable users to accomplish more tasks.",abstractText,[0],[0]
Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 640–645 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
640",text,[0],[0]
"In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output.",1 Introduction,[0],[0]
"Many attention mechanisms have been proposed (Xu et al., 2015; Bahdanau et al., 2014; Luong et al., 2015; Martins and Astudillo, 2016) but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted ’soft’ aligned input state, which finally derives the output distribution.",1 Introduction,[0],[0]
"This method is end to end differentiable and easy to implement.
",1 Introduction,[0],[0]
Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state.,1 Introduction,[0],[0]
"When successfully trained, hard attention is often found to be more accurate (Xu et al., 2015; Zaremba and Sutskever, 2015).",1 Introduction,[0],[0]
"In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks (Yu et al., 2016; Aharoni and Goldberg, 2017).",1 Introduction,[0],[0]
"For general seq2seq learning, methods like SparseMax (Martins and Astudillo, 2016) and local attention (Luong et al., 2015) were proposed to bridge the gap between soft and hard attention.
",1 Introduction,[0],[0]
"∗Both authors contributed equally to this work
In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations.",1 Introduction,[0],[0]
"The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft attention.",1 Introduction,[0],[0]
"When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint.",1 Introduction,[0],[0]
"This approximation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.
",1 Introduction,[0],[0]
"We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft attention, which in turn was better than hard and the recent Sparsemax (Martins and Astudillo, 2016) attention.",1 Introduction,[0],[0]
"More importantly, the training process was as easy as soft attention.",1 Introduction,[0],[0]
"For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.",1 Introduction,[0],[0]
For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we review it here.,2 Background and Related Work,[0],[0]
We then review related work on attention mechanisms on these models.,2 Background and Related Work,[0],[0]
"Let x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm, which we jointly denote as x1...m. Let y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn denote the output tokens in the target sequence.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"The Encoder-Decoder (ED) network factorizes Pr(y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn|x1...m) as ∏n t=1 Pr(yt|x1...m, st) where st is a decoder state summarizing y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
yt−1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"For each t, a hidden attention variable at is used to denote which part of x1...m aligns with yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Let P (at = j|x1...m, st) denote the
probability that encoder state xj is relevant for output yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Typically this is estimated using a softmax function over attention scores computed from xj and decoder state st as follows.
",2.1 Attention-based Encoder Decoder Model,[0],[0]
"P (at = j|x1...m, st) =",2.1 Attention-based Encoder Decoder Model,[0],[0]
"eAθ(xj ,st)∑m r=1",2.1 Attention-based Encoder Decoder Model,[0],[0]
"e Aθ(xr,st) (1)
where Aθ(., .) is the attention unit that scores each input state xj as per the decoder state st.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Thereafter, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each yt as
log Pr(yt|x1...m) = log Pr(yt| ∑ a Pt(a)xa) (2)
where Pt(at = j) is the short form for P (at = j|x1...m, st).",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Also, here and in the rest of the paper we drop st from P (yt) and Pt(a) for ease of notation.",2.1 Attention-based Encoder Decoder Model,[0],[0]
The weighted sum ∑ a Pt(a)xa is called an input context ct which is fed to the decoder RNN along with yt for computing the next state st+1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"We next review existing attention types.
",2.2 Related Work,[0],[0]
"Soft Attention is the attention method described in the previous section and is the current standard for seq2seq learning (Xu Chen, 2018; Koehn, 2017).",2.2 Related Work,[0],[0]
"It was proposed for translation in (Bahdanau et al., 2014) and refined further in (Luong et al., 2015).",2.2 Related Work,[0],[0]
"As shown in Eq 2, here each output is derived from an attention averaged input.",2.2 Related Work,[0],[0]
This diffuses the coupling between the input and output.,2.2 Related Work,[0],[0]
"The advantage of soft attention is end to end differentiability, and fast training and inference.
",2.2 Related Work,[0],[0]
"Hard Attention was proposed in its current form in (Xu et al., 2015) and attends to exactly one input state for an output1.",2.2 Related Work,[0],[0]
"During training, log-likelihood is an expectation over sampled attentions:
logPt(yt|x1...m) = M∑ l=1 logPt(yt|xãl) (3)
where ã1, . . .",2.2 Related Work,[0],[0]
", ãM are sampled from the multinomial Pt(a).",2.2 Related Work,[0],[0]
"Because of the sampling, the gradient has to be computed by Monte Carlo gradient/REINFORCE (Williams, 1992) and is subject to high variance.",2.2 Related Work,[0],[0]
"Many tricks are required to train
1Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.
hard attention and there is little standardization across implementations.",2.2 Related Work,[0],[0]
Xu et al (2015) use a combination of REINFORCE and soft attention.,2.2 Related Work,[0],[0]
Zaremba et al(2015) uses curriculum learning that starts as soft-attention and gradually becomes discrete.,2.2 Related Work,[0],[0]
"Ling& Rush (2017) aggregates multiple samples during training, and a single sampled attention while testing.",2.2 Related Work,[0],[0]
"However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance (Xu et al., 2015; Shankar and Sarawagi, 2018).
",2.2 Related Work,[0],[0]
Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention.,2.2 Related Work,[0],[0]
Luong et al (2015) proposes local attention that averages a window of input.,2.2 Related Work,[0],[0]
"This has been refined later to include syntax (Chen et al., 2017; Sennrich and Haddow, 2016; Chen et al., 2018).",2.2 Related Work,[0],[0]
"Another idea is to replace the softmax in soft attention with sparsity inducing operators (Martins and Astudillo, 2016; Niculae and Blondel, 2017).",2.2 Related Work,[0],[0]
"However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.",2.2 Related Work,[0],[0]
"We start from an explicit joint representation of the uncertainty of the attention and output variables.
",3 Joint Attention-Output Models,[0],[0]
logPt(yt|x1...m) = log ∑ a Pt(a)Pt(yt|xa),3 Joint Attention-Output Models,[0],[0]
"(4)
The joint model directly couples individual input states to the output, and thus is a type of hard attention.",3 Joint Attention-Output Models,[0],[0]
"Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention.",3 Joint Attention-Output Models,[0],[0]
"We call this the full-joint method.
",3 Joint Attention-Output Models,[0],[0]
"Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical.",3 Joint Attention-Output Models,[0],[0]
Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention.,3 Joint Attention-Output Models,[0],[0]
We show a surprisingly simple alternative approximation that provides hard attention without its training complexity.,3 Joint Attention-Output Models,[0],[0]
"Our method called Beam-joint deterministically selects the top-k highest attention values and approximates the full joint log probability as
logPt(yt|x1...m)",3 Joint Attention-Output Models,[0],[0]
"≈ log ∑
a∈TopK(Pt(a))
Pt(a)Pt(yt|xa)",3 Joint Attention-Output Models,[0],[0]
"(5)
Thus, in beam-joint, we first compute the multinomial attention distribution in O(m) time using
Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each position compute K output softmax, and finally compute the attention weighted output mixture distribution.",3 Joint Attention-Output Models,[0],[0]
The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20–30% for translation tasks.,3 Joint Attention-Output Models,[0],[0]
We used the default pass-through TopK operator (which is not differentiable) and optimize the beamapproximation directly.,3 Joint Attention-Output Models,[0],[0]
"We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beamapproximation directly leads to best results.
",3 Joint Attention-Output Models,[0],[0]
We show empirically that this very simple scheme is surprisingly effective compared to existing hard and soft attention over several translation tasks.,3 Joint Attention-Output Models,[0],[0]
"Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as softattention, without significant increase in training time because even K = 6 works well enough.
",3 Joint Attention-Output Models,[0],[0]
"Another reason why our ’sum of probabilities’ form performs better could be the softmax barrier effect highlighted in (Yang et al., 2018).",3 Joint Attention-Output Models,[0],[0]
The authors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix.,3 Joint Attention-Output Models,[0],[0]
They improve performance using a Mixture of Softmax model.,3 Joint Attention-Output Models,[0],[0]
Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax.,3 Joint Attention-Output Models,[0],[0]
"However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.",3 Joint Attention-Output Models,[0],[0]
We compare attention models on two NLP tasks: machine translation and morphological inflection.,4 Experiments,[0],[0]
We experiment on five language pairs from three datasets:,4.1 Machine translation,[0],[0]
"IWSLT15 English↔Vietnamese (Cettolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German↔English (Cettolo et al., 2014) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respectively ; Workshop on Asian Translation 2017 Japanese→English",4.1 Machine translation,[0],[0]
"(Nakazawa et al., 2016) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively.",4.1 Machine translation,[0],[0]
"We use a 2 layer bi-
directional encoder and a 2 layer unidirectional decoder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer.",4.1 Machine translation,[0],[0]
We base our implementation2 on the NMT code3 in Tensorflow.,4.1 Machine translation,[0],[0]
"We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.
",4.1 Machine translation,[0],[0]
Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention.,4.1 Machine translation,[0],[0]
"To further dissect the reasons behind beam-joint’s gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples.",4.1 Machine translation,[0],[0]
We train samplejoint as well as hard-attention with REINFORCE with 6-samples.,4.1 Machine translation,[0],[0]
"Also to ascertain that our gains are not explained by sparsity alone, we compare with Sparsemax (Martins and Astudillo, 2016).
",4.1 Machine translation,[0],[0]
In Table 1 we show perplexity and BLEU with three beam sizes (B).,4.1 Machine translation,[0],[0]
"Beam-joint significantly outperforms all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points.",4.1 Machine translation,[0],[0]
The perplexity shows even a more impressive drop in all five datasets.,4.1 Machine translation,[0],[0]
"Also we observe training times for beam-joint to be only 20–30% higher than softattention, establishing that beam-joint is both practical and more accurate.
",4.1 Machine translation,[0],[0]
Sample-joint is much worse than beam-joint.,4.1 Machine translation,[0],[0]
"Apart from the problem of high variance of gradients in the reinforce step, another problem is that sampling repeats states whereas TopK in beamjoint gets distinct states.",4.1 Machine translation,[0],[0]
"Hard attention too faces training issues and performs worse than soft attention, explaining why it is not commonly used in NMT.",4.1 Machine translation,[0],[0]
"Sample-joint is better than Hard attention, further highlighting the merits of the joint distribution.",4.1 Machine translation,[0],[0]
Sparsemax is competitive but marginally worse than soft attention.,4.1 Machine translation,[0],[0]
"This is concordant with the recent experiments of (Niculae and Blondel, 2017).
",4.1 Machine translation,[0],[0]
Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention.,4.1 Machine translation,[0],[0]
"Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32.",4.1 Machine translation,[0],[0]
Figure 1a shows final BLEU of these methods as well as BLEU against increasing training steps.,4.1 Machine translation,[0],[0]
"Beam-joint both converges faster and to a higher score than soft-
2https://github.com/sid7954/beam-joint-attention 3https://github.com/tensorflow/nmt
attention.",4.1 Machine translation,[0],[0]
"For example by 10000 steps ( 5 epochs), beam-joint has surpassed soft-attention by almost 2 BLEU points (20 vs 22).",4.1 Machine translation,[0],[0]
"Moreover beam-joint tracks full-joint well, and both converge finally to similar BLEUs near 27 against 26 for soft attention.",4.1 Machine translation,[0],[0]
"This shows that an attention-beam of size 6 suffices to approximate full joint almost perfectly.
",4.1 Machine translation,[0],[0]
"Next, in Figure 1b, we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets.",4.1 Machine translation,[0],[0]
"For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.
",4.1 Machine translation,[0],[0]
Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in Figure 2 on the En-Vi and De-En tasks.,4.1 Machine translation,[0],[0]
On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly.,4.1 Machine translation,[0],[0]
Similar behavior is observed in the other dataset.,4.1 Machine translation,[0],[0]
"This shows that small K values like 6 suffice for translation.
",4.1 Machine translation,[0],[0]
We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in Table 2.,4.1 Machine translation,[0],[0]
"We used our models trained with K=6, and deployed them for test-time greedy decoding with K set to 1.",4.1 Machine translation,[0],[0]
"Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention.",4.1 Machine translation,[0],[0]
"One can observe that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrating that the performance gain is not solely due to the effect of ensembling or softmax-barrier.",4.1 Machine translation,[0],[0]
"To demonstrate the use of this approach beyond translation, we next consider two morphological
inflection tasks.",4.2 Morphological Inflection,[0],[0]
"We use (Durrett and DeNero, 2013)’s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V).",4.2 Morphological Inflection,[0],[0]
The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each.,4.2 Morphological Inflection,[0],[0]
"We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam(Kingma and Ba, 2014) and measure 0/1 accuracy for soft, hard and full-joint attention models.",4.2 Morphological Inflection,[0],[0]
"Due to limited input length and vocabulary, we were able to run directly the full-joint model.",4.2 Morphological Inflection,[0],[0]
"We also ran the 100 units wide two layer LSTM with hard-monotonic attention provided by (Aharoni and Goldberg, 2017) labeled Hard-Mono4.",4.2 Morphological Inflection,[0],[0]
The table below shows that even for this task full-joint scores over existing attention models5.,4.2 Morphological Inflection,[0],[0]
"The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention.
",4.2 Morphological Inflection,[0],[0]
"Dataset Soft Hard HardMono
FullJoint
de-N 85.50 85.13 85.65 85.81 de-V 94.91 95.04 95.31 95.52
Conclusion
",4.2 Morphological Inflection,[0],[0]
In this paper we showed a simple yet effective approximation of the joint attention-output distribution in sequence to sequence learning.,4.2 Morphological Inflection,[0],[0]
Our joint model consistently provides higher accuracy without significant running time overheads in five translation and two morphological inflection tasks.,4.2 Morphological Inflection,[0],[0]
"An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (Vaswani et al., 2017; Xu Chen, 2018).
",4.2 Morphological Inflection,[0],[0]
"Acknowledgements We thank NVIDIA Corporation for supporting this research by the donation of Titan X GPU.
4https://github.com/roeeaharoni/morphologicalreinflection
5Our numbers are lower than earlier reported because ours use a single model whereas (Aharoni and Goldberg, 2017) and others report from an ensemble of five models.",4.2 Morphological Inflection,[0],[0]
"In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning.",abstractText,[0],[0]
The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention.,abstractText,[0],[0]
On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.,abstractText,[0],[0]
Surprisingly Easy Hard-Attention for Sequence to Sequence Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
93
We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",text,[0],[0]
"When we read a story, we bring to it a large body of implicit knowledge about the physical world.",1 Introduction,[0],[0]
"For instance, given the context “on stage, a woman takes a seat at the piano,” shown in Table 1, we can easily infer what the situation might look like: a woman is giving a piano performance, with a crowd watching her.",1 Introduction,[0],[0]
"We can furthermore infer her likely next action: she will most likely set her fingers on the piano keys and start playing.
",1 Introduction,[0],[0]
"This type of natural language inference requires commonsense reasoning, substantially broadening the scope of prior work that focused primarily on
linguistic entailment (Chierchia and McConnellGinet, 2000).",1 Introduction,[0],[0]
"Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed.",1 Introduction,[0],[0]
"Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998).
",1 Introduction,[0],[0]
A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset.,1 Introduction,[0],[0]
"However, recent work has shown that human-written datasets are susceptible to annotation artifacts: unintended stylistic patterns that give out clues for the gold labels (Gururangan et al., 2018; Poliak et al., 2018).",1 Introduction,[0],[0]
"As a result, models trained on such datasets with hu-
man biases run the risk of over-estimating the actual performance on the underlying task, and are vulnerable to adversarial or out-of-domain examples (Wang et al., 2018; Glockner et al., 2018).
",1 Introduction,[0],[0]
"In this paper, we introduce Adversarial Filtering (AF), a new method to automatically detect and reduce stylistic artifacts.",1 Introduction,[0.9552357994649253],"['In this paper, we use intuitions from a common representation in DNA sequence alignment to design a new standalone similarity measure called TextFlow (XF).']"
We use this method to construct Swag: an adversarial dataset with 113k multiple-choice questions.,1 Introduction,[0],[0]
"We start with pairs of temporally adjacent video captions, each with a context and a follow-up event that we know is physically possible.",1 Introduction,[0],[0]
We then use a state-of-theart language model fine-tuned on this data to massively oversample a diverse set of possible negative sentence endings (or counterfactuals).,1 Introduction,[0],[0]
"Next, we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones.",1 Introduction,[0],[0]
"Finally, these filtered counterfactuals are validated by crowd workers to further ensure data quality.
",1 Introduction,[0],[0]
"Extensive empirical results demonstrate unique contributions of our dataset, complementing existing datasets for natural langauge inference (NLI) (Bowman et al., 2015; Williams et al., 2018) and commonsense reasoning (Roemmele et al., 2011; Mostafazadeh et al., 2016; Zhang et al., 2017).",1 Introduction,[0],[0]
"First, our dataset poses a new challenge of grounded commonsense inference that is easy for humans (88%) while hard for current state-ofthe-art NLI models (<60%).",1 Introduction,[0],[0]
"Second, our proposed adversarial filtering methodology allows for cost-effective construction of a large-scale dataset while substantially reducing known annotation artifacts.",1 Introduction,[0],[0]
"The generality of adversarial filtering allows it to be applied to build future datasets, ensuring that they serve as reliable benchmarks.
",1 Introduction,[0],[0]
"2 Swag: Our new dataset
We introduce a new dataset for studying physically grounded commonsense inference, called Swag.1",1 Introduction,[0],[0]
Our task is to predict which event is most likely to occur next in a video.,1 Introduction,[0],[0]
"More formally, a model is given a context c = (s,n): a complete sentence s and a noun phrase n that begins a second sentence, as well as a list of possible verb phrase sentence endings V = {v1, . . .",1 Introduction,[0],[0]
",v4}.",1 Introduction,[0],[0]
"See Figure 1 for an example triple (s,n,vi).",1 Introduction,[0],[0]
"The model must then select the most appropriate verb phrase vî ∈ V .
1Short for Situations With Adversarial Generations.
",1 Introduction,[0],[0]
"Overview Our corpus consists of 113k multiple choice questions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017).",1 Introduction,[0],[0]
The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts).,1 Introduction,[0],[0]
"For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors.",1 Introduction,[0],[0]
"In this section, we outline the construction of Swag.",3 A solution to annotation artifacts,[0],[0]
"We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases.",3 A solution to annotation artifacts,[0],[0]
"For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance.
",3 A solution to annotation artifacts,[0],[0]
"To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyapplicable treatment involving the iterative refinement of a set of assignments to increase the entropy under a chosen model family.",3 A solution to annotation artifacts,[0],[0]
"We then discuss how we generate counterfactual endings, and
2We filter out sentences with rare tokens (≤3 occurrences), that are short (l ≤ 5), or that lack a verb phrase.
",3 A solution to annotation artifacts,[0],[0]
Algorithm 1 Adversarial filtering (AF) of negative samples.,3 A solution to annotation artifacts,[0],[0]
"During our experiments, we set Neasy = 2 for refining a population ofN− = 1023 negative examples to k = 9, and used a 80%/20% train/test split.
while convergence not reached do • Split the dataset D randomly up into training and testing portions Dtr and Dte. •",3 A solution to annotation artifacts,[0],[0]
Optimize a model fθ on Dtr. for index i in Dte do •,3 A solution to annotation artifacts,[0],[0]
Identify easy indices:,3 A solution to annotation artifacts,[0],[0]
Aeasyi = {j ∈ Ai : fθ(x,3 A solution to annotation artifacts,[0],[0]
+ i ) > fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,j)}
• Replace N easy easy indices j ∈ Aeasyi with adversarial indices k 6∈",3 A solution to annotation artifacts,[0],[0]
Ai satisfying fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,k) > fθ(x",3 A solution to annotation artifacts,[0],[0]
"− i,j).
end for end while
finally, the models used for filtering.",3 A solution to annotation artifacts,[0],[0]
"In this section, we formalize what it means for a dataset to be adversarial.",3.1 Formal definition,[0],[0]
"Intuitively, we say that an adversarial dataset for a model f is one on which f will not generalize, even if evaluated on test data from the same distribution.",3.1 Formal definition,[0],[0]
"More formally, let our input space be X and the label space be Y .",3.1 Formal definition,[0],[0]
"Our trainable classifier f , taking parameters θ is defined as fθ : X → R|Y|.",3.1 Formal definition,[0],[0]
"Let our dataset of size N be defined as D = {(xi, yi)}1≤i≤N , and let the loss function over the dataset be L(fθ,D).",3.1 Formal definition,[0],[0]
"We say that a dataset is adversarial with respect to f if we expect high empirical error I over all leave-one-out train/test splits (Vapnik, 2000):
I(D, f)",3.1 Formal definition,[0],[0]
"= 1 N N∑ i=1 L(fθ?i , {(xi, yi)}), (1)
where θ?i = argmin θ L(fθ,D \ {(xi, yi)}), (2)
",3.1 Formal definition,[0],[0]
with regularization terms omitted for simplicity.,3.1 Formal definition,[0],[0]
"In this section, we outline an approach for generating an adversarial dataset D, effectively maximizing empirical error I with respect to a family of trainable classifiers f .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"Without loss of generality, we consider the situation where we have N contexts, each associated with a single positive example (x+i , 1)∈X ×Y , and a large population of context-specific negative examples (x−i,j , 0)∈X ×Y , where 1≤j≤N− for each i. For instance, the negative examples could be incorrect relations in knowledge-base completion (Socher et al., 2013), or all words in a dictionary for a
single-word cloze task (Zweig and Burges, 2011).",3.2 Adversarial filtering (AF) algorithm,[0],[0]
Our goal will be to filter the population of negative examples for each instance i to a size of k N−.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This will be captured by returning a set of assignments A, where for each instance the assignment will be a k-subset Ai = [1 . . .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
N−]k.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"The filtered dataset will then be:
DAF = {(xi, 1), {(x−i,j , 0)}j∈Ai}1≤i≤N (3)
Unfortunately, optimizing I(DAF , f) is difficult as A is global and non-differentiable.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To address this, we present Algorithm 1.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"On each iteration, we split the data into dummy ‘train’ and ‘test’ splits.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"We train a model f on the training portion and obtain parameters θ, then use the remaining test portion to reassign the indices of A.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"For each context, we replace some number of ‘easy’ negatives in A that fθ classifies correctly with ‘adversarial’ negatives outside ofA that fθ misclassifies.
",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This process can be thought of as increasing the overall entropy of the dataset: given a strong model fθ that is compatible with a random subset of the data, we aim to ensure it cannot generalize to the held-out set.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
We repeat this for several iterations to reduce the generalization ability of the model family f over arbitrary train/test splits.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To generate counterfactuals for Swag, we use an LSTM (Hochreiter and Schmidhuber, 1997) language model (LM), conditioned on contexts from video captions.",3.3 Generating candidate endings,[0],[0]
"We first pretrain on BookCorpus (Zhu et al., 2015), then finetune on the video caption datasets.",3.3 Generating candidate endings,[0],[0]
The architecture uses standard best practices and was validated on held-out perplexity of the video caption datasets; details are in the appendix.,3.3 Generating candidate endings,[0],[0]
"We use the LM to sample N−=1023 unique endings for a partial caption.3
Importantly, we greedily sample the endings, since beam search decoding biases the generated endings to be of lower perplexity (and thus easily distinguishable from found endings).",3.3 Generating candidate endings,[0],[0]
"We find this process gives good counterfactuals: the generated endings tend to use topical words, but often make little sense physically, making them perfect for our task.",3.3 Generating candidate endings,[0],[0]
"Further, the generated endings are marked as “gibberish” by humans only 9.1% of the time (Sec 3.5); in that case the ending is filtered out.
",3.3 Generating candidate endings,[0],[0]
"3To ensure that the LM generates unique endings, we split the data into five validation folds and train five separate LMs, one for each set of training folds.",3.3 Generating candidate endings,[0],[0]
This means that each LM never sees the found endings during training.,3.3 Generating candidate endings,[0],[0]
"In creating Swag, we designed the model family f to pick up on low-level stylistic features that we posit should not be predictive of whether an event happens next in a video.",3.4 Stylistic models for adversarial filtering,[0],[0]
"These stylistic features are an obvious case of annotation artifacts (Cai et al., 2017; Schwartz et al.,",3.4 Stylistic models for adversarial filtering,[0],[0]
2017).4 Our final classifier is an ensemble of four stylistic models:,3.4 Stylistic models for adversarial filtering,[0],[0]
1.,3.4 Stylistic models for adversarial filtering,[0],[0]
A multilayer perceptron (MLP) given LM perplexity features and context/ending lengths.,3.4 Stylistic models for adversarial filtering,[0],[0]
2.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bag-of-words model that averages the word embeddings of the second sentence as features.,3.4 Stylistic models for adversarial filtering,[0],[0]
3.,3.4 Stylistic models for adversarial filtering,[0],[0]
"A one-layer CNN, with filter sizes ranging from 2-5, over the second sentence.",3.4 Stylistic models for adversarial filtering,[0],[0]
4.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bidirectional LSTM over the 100 most common words in the second sentence; uncommon words are replaced by their POS tags.,3.4 Stylistic models for adversarial filtering,[0],[0]
We ensemble the models by concatenating their final representations and passing it through an MLP.,3.4 Stylistic models for adversarial filtering,[0],[0]
"On every adversarial iteration, the ensemble is trained jointly to minimize cross-entropy.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"The accuracies of these models (at each iteration, evaluated on a 20% split of the test dataset before indices of A get remapped) are shown in Figure 2.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Performance decreases from 60% to close to random chance; moreover, confusing the perplexity-based MLP is not sufficient to lower performance of the ensemble.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Only once the other stylistic models are added does the ensemble accuracy drop substantially, suggesting that our approach is effective at reducing stylistic artifacts.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"4A broad definition of annotation artifacts might include aspects besides lexical/stylistic features: for instance, certain events are less likely semantically regardless of the context (e.g. riding a horse using a hose).",3.4 Stylistic models for adversarial filtering,[0],[0]
"For this work, we erred more conservatively and only filtered based on style.",3.4 Stylistic models for adversarial filtering,[0],[0]
The final data-collection step is to have humans verify the data.,3.5 Human verification,[0],[0]
"Workers on Amazon Mechanical Turk were given the caption context, as well as six candidate endings: one found ending and five adversarially-sampled endings.",3.5 Human verification,[0],[0]
The task was twofold:,3.5 Human verification,[0],[0]
"Turkers ranked the endings independently as likely, unlikely, or gibberish, and selected the best and second best endings (Fig 3).
",3.5 Human verification,[0],[0]
We obtained the correct answers to each context in two ways.,3.5 Human verification,[0],[0]
"If a Turker ranks the found ending as either best or second best (73.7% of the time), we add the found ending as a gold example, with negatives from the generations not labelled best or gibberish.",3.5 Human verification,[0],[0]
"Further, if a Turker ranks a generated ending as best, and the found ending as second best, then we have reason to believe that the generation is good.",3.5 Human verification,[0],[0]
"This lets us add an additional training example, consisting of the generated best ending as the gold, and remaining generations as negatives.5 Examples with ≤3 nongibberish endings were filtered out.6
We found after 1000 examples that the annotators tended to have high agreement, also generally choosing found endings over generations (see Table 2).",3.5 Human verification,[0],[0]
"Thus, we collected the remaining 112k examples with one annotator each, periodically verifying that annotators preferred the found endings.",3.5 Human verification,[0],[0]
"In this section, we evaluate the performance of various NLI models on Swag.",4 Experiments,[0],[0]
"Recall that models
5These two examples share contexts.",4 Experiments,[0],[0]
"To prevent biasing the test and validation sets, we didn’t perform this procedure on answers from the evaluation sets’ context.
",4 Experiments,[0],[0]
"6To be data-efficient, we reannotated filtered-out examples by replacing gibberish endings, as well as generations that outranked the found ending, with candidates from A.
for our dataset take the following form: given a sentence and a noun phrase as context c = (s,n), as well as a list of possible verb phrase endings V = {v1, . . .",4 Experiments,[0],[0]
",v4}, a model fθ must select a verb î that hopefully matches igold:
î =",4 Experiments,[0],[0]
"argmax i fθ(s,n,vi) (4)
To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi, or the entire second sentence (n,vi).",4 Experiments,[0],[0]
"For our learned models, we train f by minimizing multi-class cross-entropy.",4 Experiments,[0],[0]
"We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018).",4 Experiments,[0],[0]
"We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data.",4 Experiments,[0],[0]
See the appendix for more details.,4 Experiments,[0],[0]
"The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage.",4.1 Unary models,[0],[0]
a.,4.1 Unary models,[0],[0]
"fastText (Joulin et al., 2017):",4.1 Unary models,[0],[0]
"This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7",4.1 Unary models,[0],[0]
"b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent
7The fastText model is trained using binary cross-entropy; at test time we extract the prediction by selecting the ending with the highest positive likelihood under the model.
",4.1 Unary models,[0],[0]
"(Conneau et al., 2017).",4.1 Unary models,[0],[0]
"SkipThoughts was trained by predicting adjacent sentences in book data, whereas InferSent was trained on supervised NLI data.",4.1 Unary models,[0],[0]
"For each second sentence (or just the ending), we feed the encoding into an MLP.",4.1 Unary models,[0],[0]
c. LSTM sentence encoder,4.1 Unary models,[0],[0]
"Given an arbitrary span of text, we run a two-layer BiLSTM over it.",4.1 Unary models,[0],[0]
"The final hidden states are then max-pooled to obtain a fixed-size representation, which is then used to predict the potential for that ending.",4.1 Unary models,[0],[0]
The following models predict labels from two spans of text.,4.2 Binary models,[0],[0]
"We consider two possibilties for these models: using just the second sentence, where the two text spans are n,vi, or using the context and the second sentence, in which case the spans are s, (n,vi).",4.2 Binary models,[0],[0]
The latter case includes many models developed for the NLI task.,4.2 Binary models,[0],[0]
"d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c,vi).",4.2 Binary models,[0],[0]
We model the probability of picking an ending i using a bilinear model: softmaxi(cWvTi ).,4.2 Binary models,[0],[0]
"8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations.",4.2 Binary models,[0],[0]
"f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017).",4.2 Binary models,[0],[0]
"We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example.",4.2 Binary models,[0],[0]
We then train a log-linear model using these 3-way probabilities as features.,4.2 Binary models,[0],[0]
g. SNLI models (retrained),4.2 Binary models,[0],[0]
"Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi) with a softmax over i.",4.2 Binary models,[0],[0]
We also considered the following models:,4.3 Other models,[0],[0]
h. Length:,4.3 Other models,[0],[0]
"Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias.",4.3 Other models,[0],[0]
"For this baseline, we always choose the shortest ending.",4.3 Other models,[0],[0]
i. ConceptNet,4.3 Other models,[0],[0]
"As our task requires world knowledge, we tried a rule-based system on top of the
8We also tried using an MLP, but got worse results.
",4.3 Other models,[0],[0]
"ConceptNet knowledge base (Speer et al., 2017).",4.3 Other models,[0],[0]
"For an ending sentence, we use the spaCy dependency parser to extract the head verb and its dependent object.",4.3 Other models,[0],[0]
The ending score is given by the number of ConceptNet causal relations9 between synonyms of the verb and synonyms of the object.,4.3 Other models,[0],[0]
"j. Human performance To benchmark human performance, five Mechanical Turk workers were asked to answer 100 dataset questions, as did an ‘expert’ annotator (the first author of this paper).",4.3 Other models,[0],[0]
Predictions were combined using a majority vote.,4.3 Other models,[0],[0]
We present our results in Table 3.,4.4 Results,[0],[0]
"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings, which obtains 43.6%.",4.4 Results,[0],[0]
"This model, as with most models studied, greatly improves with more context: by 3.1% when given the initial noun phrase, and by an ad-
9We used the relations ‘Causes’, ‘CapableOf’, ‘ReceivesAction’, ‘UsedFor’, and ‘HasSubevent’.",4.4 Results,[0],[0]
"Though their coverage is low (30.4% of questions have an answer with≥1 causal relation), the more frequent relations in ConceptNet, such as ‘IsA’, at best only indirectly relate to our task.
",4.4 Results,[0],[0]
ditional 4% when also given the first sentence.,4.4 Results,[0],[0]
Further improvement is gained from models that compute pairwise representations of the inputs.,4.4 Results,[0],[0]
"While the simplest such model, DualBoW, obtains only 35.1% accuracy, combining InferSent sentence representations gives 40.5% accuracy (InferSent-Bilinear).",4.4 Results,[0],[0]
"The best results come from pairwise NLI models: when fully trained on Swag, ESIM+ELMo obtains 59.2% accuracy.
",4.4 Results,[0],[0]
"When comparing machine results to human results, we see there exists a lot of headroom.",4.4 Results,[0],[0]
"Though there likely is some noise in the task, our results suggest that humans (even untrained) converge to a consensus.",4.4 Results,[0],[0]
"Our in-house “expert” annotator is outperformed by an ensemble of 5 Turk workers (with 88% accuracy); thus, the effective upper bound on our dataset is likely even higher.",4.4 Results,[0],[0]
"5.1 Swag versus existing NLI datasets The past few years have yielded great advances in NLI and representation learning, due to the availability of large datasets like SNLI and MultiNLI
(Bowman et al., 2015; Williams et al., 2018).",5 Analysis,[0],[0]
"With the release of Swag, we hope to continue this trend, particularly as our dataset largely has the same input/output format as other NLI datasets.",5 Analysis,[0],[0]
"We observe three key differences between our dataset and others in this space:
First, as noted in Section 1, Swag requires a unique type of temporal reasoning.",5 Analysis,[0],[0]
"A state-of-theart NLI model such as ESIM, when bottlenecked through the SNLI notion of entailment (SNLIESIM), only obtains 36.1% accuracy.10 This implies that these datasets necessitate different (and complementary) forms of reasoning.
",5 Analysis,[0],[0]
"Second, our use of videos results in wide coverage of dynamic and temporal situations Compared with SNLI, with contexts from Flickr30K (Plummer et al., 2017) image captions, Swag has more active verbs like ‘pull’ and ‘hit,’ and fewer static verbs like ‘sit’ and ‘wear’ (Figure 4).11
Third, our dataset suffers from few lexical biases.",5 Analysis,[0],[0]
"Whereas fastText, a bag of n-gram model, obtains 67.0% accuracy on SNLI versus a 34.3% baseline (Gururangan et al., 2018), fastText obtains only 29.0% accuracy on Swag.12",5 Analysis,[0],[0]
"We sought to quantify how human judgments differ from the best studied model, ESIM+ELMo.",5.2 Error analysis,[0],[0]
"We randomly sampled 100 validation questions
10The weights of SNLI-ESIM pick up primarily on entailment probability (0.59), as with neutral (0.46), while contradiction is negatively correlated (-.42).
",5.2 Error analysis,[0],[0]
"11Video data has other language differences; notably, character names in LSMDC were replaced by ‘someone’
12The most predictive individual words on SWAG are infrequent in number: ‘dotted‘ with P(+|dotted) = 77% with 10.3 counts, and P(−|similar)",5.2 Error analysis,[0],[0]
= 81% with 16.3 counts.,5.2 Error analysis,[0],[0]
"(Counts from negative endings were discounted 3x, as there are 3 times as many negative endings as positive endings).
",5.2 Error analysis,[0],[0]
"that ESIM+ELMo answered incorrectly, for each extracting both the gold ending and the model’s preferred ending.",5.2 Error analysis,[0],[0]
"We asked 5 Amazon Mechanical Turk workers to pick the better ending (of which they preferred the gold endings 94% of the time) and to select one (or more) multiple choice reasons explaining why the chosen answer was better.
",5.2 Error analysis,[0],[0]
"The options, and the frequencies, are outlined in Table 4.",5.2 Error analysis,[0],[0]
"The most common reason for the turkers preferring the correct answer is situational (52.3% of the time), followed by weirdness (17.5%) and plausibility (14.4%).",5.2 Error analysis,[0],[0]
"This suggests that ESIM+ELMo already does a good job at filtering out weird and implausible answers, with the main bottleneck being grounded physical understanding.",5.2 Error analysis,[0],[0]
"The ambiguous percentage is also relatively low (12.0%), implying significant headroom.",5.2 Error analysis,[0],[0]
"Last, we show several qualitative examples in Table 5.",5.3 Qualitative examples,[0],[0]
"Though models can do decently well by identifying complex alignment patterns between the two sentences (e.g. being “up a tree” implies that “tree” is the end phrase), the incorrect model predictions suggest this strategy is insuffi-
cient.",5.3 Qualitative examples,[0],[0]
"For instance, answering “An old man rides a small bumper car” requires knowledge about bumper cars and how they differ from regular cars: bumper cars are tiny, don’t drive on roads, and don’t work in parking lots, eliminating the alternatives.",5.3 Qualitative examples,[0],[0]
"However, this knowledge is difficult to extract from existing corpora: for instance, the ConceptNet entry for Bumper Car has only a single relation: bumper cars are a type of vehicle.",5.3 Qualitative examples,[0],[0]
"Other questions require intuitive physical reasoning: e.g, for “he pours the raw egg batter into the pan,” about what happens next in making an omelet.",5.3 Qualitative examples,[0],[0]
Our results suggest that Swag is a challenging testbed for NLI models.,5.4 Where to go next?,[0],[0]
"However, the adversarial models used to filter the dataset are purely stylistic and focus on the second sentence; thus, subtle artifacts still likely remain in our dataset.",5.4 Where to go next?,[0],[0]
"These patterns are ostensibly picked up by the NLI models (particularly when using ELMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset.",5.4 Where to go next?,[0],[0]
"As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models.",5.4 Where to go next?,[0],[0]
"Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018).",6 Related Work,[0],[0]
"Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components
for performing better video captioning (Pasunuru and Bansal, 2017), summarization (Pasunuru and Bansal, 2018), and generation (Holtzman et al., 2018), confirming the importance of NLI research.",6 Related Work,[0],[0]
"The NLI task requires a variety of commonsense knowledge (LoBue and Yates, 2011), which our work complements.",6 Related Work,[0],[0]
"However, previous datasets for NLI have been challenged by unwanted annotation artifacts, (Gururangan et al., 2018; Poliak et al., 2018) or scale issues.",6 Related Work,[0],[0]
"Our work addresses these challenges by constructing a new NLI benchmark focused on grounded commonsense reasoning, and by introducing an adversarial filtering mechanism that substantially reduces known and easily detectable annotation artifacts.
",6 Related Work,[0],[0]
"Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)).",6 Related Work,[0],[0]
These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model.,6 Related Work,[0],[0]
"In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans.",6 Related Work,[0],[0]
"In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality.
",6 Related Work,[0],[0]
"13For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories.
",6 Related Work,[0],[0]
"Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about.
",6 Related Work,[0],[0]
Vision datasets Several resources have been introduced to study temporal inference in vision.,6 Related Work,[0],[0]
"The Visual Madlibs dataset has 20k image captions about hypothetical next/previous events (Yu et al., 2015); similar to our work, the test portion is multiple-choice, with counterfactual answers retrieved from similar images and verified by humans.",6 Related Work,[0],[0]
"The question of ‘what will happen next?’ has also been studied in photo albums (Huang et al., 2016), videos of team sports, (Felsen et al., 2017) and egocentric dog videos (Ehsani et al., 2018).",6 Related Work,[0],[0]
"Last, annotation artifacts are also a recurring problem for vision datasets such as Visual Genome (Zellers et al., 2018) and Visual QA (Jabri et al., 2016); recent work was done to create a more challenging VQA dataset by annotating complementary image pairs (Goyal et al., 2016).
",6 Related Work,[0],[0]
"Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017).",6 Related Work,[0],[0]
"Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017).",6 Related Work,[0],[0]
"However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017).",6 Related Work,[0],[0]
"Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias.
",6 Related Work,[0],[0]
"Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax parses (Forbes and Choi, 2017), word embeddings (Lucy and Gauthier, 2017), and images and dictionary definitions (Zellers and Choi, 2017).",6 Related Work,[0],[0]
"An alternate thread of work has been to learn scripts: high-level representations of event chains (Schank and Abelson, 1975; Chambers and Jurafsky, 2009).",6 Related Work,[0],[0]
"Swag evaluates both of these strands.
",6 Related Work,[0],[0]
14Prior work on sentence completion filtered negatives with heuristics based on LM perplexities.,6 Related Work,[0],[0]
"We initially tried something similar, but found the result to still be gameable.",6 Related Work,[0],[0]
We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning.,7 Conclusion,[0],[0]
"To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions.",7 Conclusion,[0],[0]
"Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that can be easily detected by a committee of strong baseline models.",7 Conclusion,[0],[0]
"Our adversarial filtering paradigm is general, allowing potential applications to other datasets that require human composition of question answer pairs.",7 Conclusion,[0],[0]
"We thank the anonymous reviewers, members of the ARK and xlab at the University of Washington, researchers at the Allen Institute for AI, and Luke Zettlemoyer for their helpful feedback.",Acknowledgements,[0],[0]
We also thank the Mechanical Turk workers for doing a fantastic job with the human validation.,Acknowledgements,[0],[0]
"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook.",Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.",Acknowledgements,[0],[0]
"Given a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (“then, she examined the engine”).",abstractText,[0],[0]
"In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.",abstractText,[0],[0]
"We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations.",abstractText,[0],[0]
"To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data.",abstractText,[0],[0]
"To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals.",abstractText,[0],[0]
"Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task.",abstractText,[0],[0]
We provide comprehensive analysis that indicates significant opportunities for future research.,abstractText,[0],[0]
Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856–861 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
856
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",text,[0],[0]
Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms.,1 Introduction and Related Work,[0],[0]
"While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016).",1 Introduction and Related Work,[0],[0]
"In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are both widely utilized and highly effective (Huang et al., 2016; Zagoruyko and Komodakis, 2016).
",1 Introduction and Related Work,[0],[0]
"However, it is nontrivial to find simple equivalences for NLP tasks like machine translation, because even slight modifications of sentences can result in significant changes in their semantics, or
*: Equal contributions.
require corresponding changes in the translations in order to keep the data consistent.",1 Introduction and Related Work,[0],[0]
"In fact, indiscriminate modifications of data in NMT can introduce noise that makes NMT systems brittle (Belinkov and Bisk, 2018).
",1 Introduction and Related Work,[0],[0]
"Due to such difficulties, the literature in data augmentation for NMT is relatively scarce.",1 Introduction and Related Work,[0],[0]
"To our knowledge, data augmentation techniques for NMT fall into two categories.",1 Introduction and Related Work,[0],[0]
"The first category is based on back-translation (Sennrich et al., 2016b; Poncelas et al., 2018), which utilizes monolingual data to augment a parallel training corpus.",1 Introduction and Related Work,[0],[0]
"While effective, back-translation is often vulnerable to errors in initial models, a common problem of self-training algorithms (Chapelle et al., 2009).",1 Introduction and Related Work,[0],[0]
The second category is based on word replacements.,1 Introduction and Related Work,[0],[0]
"For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly.",1 Introduction and Related Work,[0],[0]
"While this method generates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets.",1 Introduction and Related Work,[0],[0]
"Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.
",1 Introduction and Related Work,[0],[0]
"In this paper, we derive an extremely simple and efficient data augmentation technique for NMT.",1 Introduction and Related Work,[0],[0]
"First, we formulate the design of a data augmentation algorithm as an optimization problem, where we seek the data augmentation policy that maximizes an objective that encourages two desired properties: smoothness and diversity.",1 Introduction and Related Work,[0],[0]
"This optimization problem has a tractable analytic solution,
which describes a generic framework of which both word dropout and RAML are instances.",1 Introduction and Related Work,[0],[0]
"Second, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the target sentence by other words uniformly sampled from the source and the target vocabularies, respectively.",1 Introduction and Related Work,[0],[0]
"Experiments show that this method, which we name SwitchOut, consistently improves over strong baselines on datasets of different scales, including the large-scale WMT 15 English-German dataset, and two medium-scale datasets: IWSLT 2016 German-English and IWSLT 2015 EnglishVietnamese.",1 Introduction and Related Work,[0],[0]
"We use uppercase letters, such as X , Y , etc., to denote random variables and lowercase letters such as x, y, etc., to denote the corresponding actual values.",2.1 Notations,[0],[0]
"Additionally, since we will discuss a data augmentation algorithm, we will use a hat to denote augmented variables and their values, e.g. bX , bY , bx, by, etc.",2.1 Notations,[0],[0]
"We will also use boldfaced characters, such as p, q, etc., to denote probability distributions.",2.1 Notations,[0],[0]
We facilitate our discussion with a probabilistic framework that motivates data augmentation algorithms.,2.2 Data Augmentation,[0],[0]
"With X , Y being the sequences of words in the source and target languages (e.g. in machine translation), the canonical MLE framework maximizes the objective
JMLE(✓) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(X,Y )",2.2 Data Augmentation,[0],[0]
"[logp✓(y|x)] .
",2.2 Data Augmentation,[0],[0]
"Here bp(X,Y ) is the empirical distribution over all training data pairs (x, y) and p
✓ (y|x) is a parameterized distribution that we aim to learn, e.g. a neural network.",2.2 Data Augmentation,[0],[0]
"A potential weakness of MLE is the mismatch between bp(X,Y ) and the true data distribution p(X,Y ).",2.2 Data Augmentation,[0],[0]
"Specifically, bp(X,Y ) is usually a bootstrap distribution defined only on the observed training pairs, while p(X,Y ) has a much larger support, i.e. the entire space of valid pairs.",2.2 Data Augmentation,[0],[0]
"This issue can be dramatic when the empirical observations are insufficient to cover the data space.
",2.2 Data Augmentation,[0],[0]
"In practice, data augmentation is often used to remedy this support discrepancy by supplying additional training pairs.",2.2 Data Augmentation,[0],[0]
"Formally, let q( bX, bY ) be the augmented distribution defined on a larger support than the empirical distribution bp(X,Y ).",2.2 Data Augmentation,[0],[0]
"Then,
MLE training with data augmentation maximizes
JAUG(✓) = Ebx,by⇠q( bX,bY )",2.2 Data Augmentation,[0],[0]
"[logp✓(by|bx)] .
",2.2 Data Augmentation,[0],[0]
"In this work, we focus on a specific family of q, which depends on the empirical observations by
q( bX, bY ) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(x,y)
h q( bX, bY |x, y) i .
",2.2 Data Augmentation,[0],[0]
"This particular choice follows the intuition that an augmented pair (bx, by) that diverges too far from any observed data is more likely to be invalid and thus harmful for training.",2.2 Data Augmentation,[0],[0]
The reason will be more evident later.,2.2 Data Augmentation,[0],[0]
"Certainly, not all q are equally good, and the more similar q is to p, the more desirable q will be.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Unfortunately, we only have access to limited observations captured by bp.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Hence, in order to use q to bridge the gap between bp and p, it is necessary to utilize some assumptions about p. Here, we exploit two highly generic assumptions, namely:
• Diversity: p(X,Y ) has a wider support set, which includes samples that are more diverse than those in the empirical observation set.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"• Smoothness: p(X,Y ) is smooth, and similar (x, y) pairs will have similar probabilities.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"To formalize both assumptions, let s(bx, by;x, y) be a similarity function that measures how similar an augmented pair (bx, by) is to an observed data pair (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Then, an ideal augmentation policy q( bX, bY |x, y) should have two properties.",2.3 Diverse and Smooth Augmentation,[0],[0]
"First, based on the smoothness assumption, if an augmented pair (bx, by) is more similar to an empirical pair (x, y), it is more likely that (bx, by) is sampled under the true data distribution p(X,Y ), and thus q( bX, bY |x, y) should assign a significant amount of probability mass to (bx, by).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Second, to quantify the diversity assumption, we propose that the entropy H[q( bX, bY |x, y)] should be large, so that the support of q( bX, bY ) is larger than the support of bp and thus is closer to the support p(X,Y ).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Combining these assumptions implies that q( bX, bY |x, y) should maximize the objective
J(q;x, y) = Ebx,by⇠q( bX,bY |x,y) ⇥",2.3 Diverse and Smooth Augmentation,[0],[0]
"s(bx, by;x, y) ⇤
+ ⌧H(q( bX, bY |x, y)), (1)
where ⌧ controls the strength of the diversity objective.",2.3 Diverse and Smooth Augmentation,[0],[0]
"The first term in (1) instantiates the smoothness assumption, which encourages q to draw samples that are similar to (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Meanwhile, the second term in (1) encourages more diverse samples from q. Together, the objective J(q;x, y) extends the information in the “pivotal” empirical sample (x, y) to a diverse set of similar cases.",2.3 Diverse and Smooth Augmentation,[0],[0]
"This echoes our particular parameterization of q in Section 2.2.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"The objective J(q;x, y) in (1) is the canonical maximum entropy problem that one often encounters in deriving a max-ent model (Berger et al., 1996), which has the analytic solution:
q⇤(bx, by|x, y) = exp {s(bx, by;x, y)/⌧}P bx0,by0 exp {s(bx0, by0;x, y)/⌧}
(2) Note that (2) is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Next, we will show that some existing algorithms can be seen as specific instantiations under our framework.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Moreover, this leads us to propose a novel and effective data augmentation algorithm.",2.3 Diverse and Smooth Augmentation,[0],[0]
Word Dropout.,2.4 Existing and New Algorithms,[0],[0]
"In the context of machine translation, Sennrich et al. (2016a) propose to randomly choose some words in the source and/or target sentence, and set their embeddings to 0 vectors.",2.4 Existing and New Algorithms,[0],[0]
"Intuitively, it regards every new data pair generated by this procedure as similar enough and then includes them in the augmented training set.",2.4 Existing and New Algorithms,[0],[0]
"Formally, word dropout can be seen as an instantiation of our framework with a particular similarity function s(x̂, ŷ;x, y) (see Appendix A.1).
RAML.",2.4 Existing and New Algorithms,[0],[0]
"From the perspective of reinforcement learning, Norouzi et al. (2016) propose to train the model distribution to match a target distribution proportional to an exponentiated reward.",2.4 Existing and New Algorithms,[0],[0]
"Despite the difference in motivation, it can be shown (c.f. Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(bx, by;x, y) = r(by; y) if bx = x and 1 otherwise.",2.4 Existing and New Algorithms,[0],[0]
"Here, r is a task-specific reward function which measures the similarity between by and y. Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.
SwitchOut.",2.4 Existing and New Algorithms,[0],[0]
"After reviewing the two existing augmentation schemes, there are two immediate
insights.",2.4 Existing and New Algorithms,[0],[0]
"Firstly, augmentation should not be restricted to only the source side or the target side.",2.4 Existing and New Algorithms,[0],[0]
"Secondly, being able to incorporate prior knowledge, such as the task-specific reward function r in RAML, can lead to a better similarity measure.
",2.4 Existing and New Algorithms,[0],[0]
"Motivated by these observations, we propose to perform augmentation in both source and target domains.",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we separately measure the similarity between the pair (bx, x) and the pair (by, y) and then sum them together, i.e.
s(bx, by;x, y)/⌧ ⇡ r x (bx, x)/⌧ x + r y (by, y)/⌧ y , (3)
where r x and r y are domain specific similarity functions and ⌧
x , ⌧ y are hyper-parameters that absorb the temperature parameter ⌧ .",2.4 Existing and New Algorithms,[0],[0]
"This allows us to factor q⇤(bx, by|x, y) into:
q⇤(bx, by|x, y) = exp {rx(bx, x)/⌧x}P bx0 exp {rx(bx0, x)/⌧x}
⇥",2.4 Existing and New Algorithms,[0],[0]
"exp {ry(by, y)/⌧y}P by0 exp {ry(by0, y)/⌧y}
(4)
",2.4 Existing and New Algorithms,[0],[0]
"In addition, notice that this factored formulation allows bx and by to be sampled independently.
",2.4 Existing and New Algorithms,[0],[0]
Sampling Procedure.,2.4 Existing and New Algorithms,[0],[0]
"To complete our method, we still need to define r
x and r y , and then design a practical sampling scheme from each factor in (4).",2.4 Existing and New Algorithms,[0],[0]
"Though non-trivial, both problems have been (partially) encountered in RAML (Norouzi et al., 2016; Ma et al., 2017).",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we follow previous work to use the negative Hamming distance for both r
x and r y .",2.4 Existing and New Algorithms,[0],[0]
"For a more parallelized implementation, we sample an augmented sentence bs from a true sentence s as follows:
1.",2.4 Existing and New Algorithms,[0],[0]
"Sample bn 2 {0, 1, ..., |s|} by p(bn) /",2.4 Existing and New Algorithms,[0],[0]
"e bn/⌧ .
",2.4 Existing and New Algorithms,[0],[0]
2.,2.4 Existing and New Algorithms,[0],[0]
"For each i 2 {1, 2, ..., |s|}, with probability bn/ |s|, we can replace s
i by a uniform bs",2.4 Existing and New Algorithms,[0],[0]
"i 6= s i .
",2.4 Existing and New Algorithms,[0],[0]
"This procedure guarantees that any two sentences bs1 and bs2 with the same Hamming distance to s have the same probability, but slightly changes the relative odds of sentences with different Hamming distances to s from the true distribution by negative Hamming distance, and thus is an approximation of the actual distribution.",2.4 Existing and New Algorithms,[0],[0]
"However, this efficient sampling procedure is much easier to implement while achieving good performance.
",2.4 Existing and New Algorithms,[0],[0]
"Algorithm 1 illustrates this sampling procedure, which can be applied independently and in parallel for each batch of source sentences and target
sentences.",2.4 Existing and New Algorithms,[0],[0]
"Additionally, we open source our implementation in TensorFlow and in PyTorch (respectively in Appendix A.5 and A.6).
",2.4 Existing and New Algorithms,[0],[0]
Algorithm 1: Sampling with SwitchOut.,2.4 Existing and New Algorithms,[0],[0]
"Input : s: a sentence represented by vocab integral ids,
⌧ : the temperature, V : the vocabulary Output : bs: a sentence with words replaced
1 Function HammingDistanceSample(s, ⌧ , |V |): 2 Let Z(⌧) P|s| n=0 e
n/⌧ be the partition function.",2.4 Existing and New Algorithms,[0],[0]
"3 Let p(n) e n/⌧/Z(⌧) for n = 0, 1, ..., |s|.",2.4 Existing and New Algorithms,[0],[0]
4 Sample bn ⇠ p(n).,2.4 Existing and New Algorithms,[0],[0]
5,2.4 Existing and New Algorithms,[0],[0]
"In parallel, do: 6 Sample a
i ⇠ Bernoulli(bn/ |s|).",2.4 Existing and New Algorithms,[0],[0]
7,2.4 Existing and New Algorithms,[0],[0]
"if a
i = 1 then 8 bs
i Uniform(V \{s i })",2.4 Existing and New Algorithms,[0],[0]
.,2.4 Existing and New Algorithms,[0],[0]
"9 else
10 bs",2.4 Existing and New Algorithms,[0],[0]
i s i .,2.4 Existing and New Algorithms,[0],[0]
11 end 12 return bs,2.4 Existing and New Algorithms,[0],[0]
Datasets.,3 Experiments,[0],[0]
We benchmark SwitchOut on three translation tasks of different scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de).,3 Experiments,[0],[0]
All translations are wordbased.,3 Experiments,[0],[0]
"These tasks and pre-processing steps are standard, used in several previous works.",3 Experiments,[0],[0]
"Detailed statistics and pre-processing schemes are in Appendix A.3.
",3 Experiments,[0],[0]
Models and Experimental Procedures.,3 Experiments,[0],[0]
"Our translation model, i.e. p
✓ (y|x), is a Transformer network (Vaswani et al., 2017).",3 Experiments,[0],[0]
"For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results.",3 Experiments,[0],[0]
(w.r.t.,3 Experiments,[0],[0]
Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)),3 Experiments,[0],[0]
.,3 Experiments,[0],[0]
"Then, fixing all hyper-parameters, and fixing ⌧
y = 0, we tune the ⌧ x rate, which controls how far we are willing to let bx deviate from x.",3 Experiments,[0],[0]
"Our hyper-parameters are listed in Appendix A.4.
Baselines.",3 Experiments,[0],[0]
"While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4.",3 Experiments,[0],[0]
"Additionally, on the en-de task, we compare SwitchOut against back-translation (Sennrich et al., 2016b).
",3 Experiments,[0],[0]
SwitchOut vs. Word Dropout and RAML.,3 Experiments,[0],[0]
"We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in Table 1.",3 Experiments,[0],[0]
"To account for variance, we run each experiment multiple times and report the median BLEU.",3 Experiments,[0],[0]
"Specifically, each experiment without SwitchOut is run for 4 times, while each experiment with SwitchOut is run for 9 times due to its inherently higher variance.",3 Experiments,[0],[0]
"We also conduct pairwise statistical significance tests using paired bootstrap (Clark et al., 2011), and record the results in Table 1.",3 Experiments,[0],[0]
"For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut.",3 Experiments,[0],[0]
"For the remaining two settings, the differences are not statistically significant.",3 Experiments,[0],[0]
The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p < 0.0002).,3 Experiments,[0],[0]
"Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.
SwitchOut vs. Back Translation.",3 Experiments,[0],[0]
"Traditionally, data-augmentation is viewed as a method to enlarge the training datasets (Krizhevsky et al., 2012; Szegedy et al., 2014).",3 Experiments,[0],[0]
"In the context of neural MT, Sennrich et al. (2016b) propose to use artificial data generated from a weak back-translation model, effectively utilizing monolingual data to enlarge the bilingual training datasets.",3 Experiments,[0],[0]
"In connection, we compare SwitchOut against back translation.",3 Experiments,[0],[0]
"We only compare SwitchOut against back translation on the en-de task, where the amount of bilingual training data is already sufficiently large2.",3 Experiments,[0],[0]
"The
2We add the extra monolingual data from http://data.statmt.org/rsennrich/wmt16_ backtranslations/en-de/
BLEU scores with back-translation are reported in Table 2.",3 Experiments,[0],[0]
These results provide two insights.,3 Experiments,[0],[0]
"First, the gain delivered by back translation is less significant than the gain delivered by SwitchOut.",3 Experiments,[0],[0]
"Second, SwitchOut and back translation are not mutually exclusive, as one can additionally apply SwitchOut on the additional data obtained from back translation to further improve BLEU scores.
",3 Experiments,[0],[0]
Effects of ⌧ x and ⌧ y .,3 Experiments,[0],[0]
We empirically study the effect of these temperature parameters.,3 Experiments,[0],[0]
"During the tuning process, we translate the dev set of the tasks and report the BLEU scores in Figure 1.",3 Experiments,[0],[0]
"We observe that when fixing ⌧
y , the best performance is always achieved with a non-zero ⌧
.
",3 Experiments,[0],[0]
Where does SwitchOut Help the Most?,3 Experiments,[0],[0]
"Intuitively, because SwitchOut is expanding the support of the training distribution, we would expect that it would help the most on test sentences that are far from those in the training set and would thus benefit most from this expanded support.",3 Experiments,[0],[0]
"To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their
nearest neighbor and measure the gain in BLEU afforded by SwitchOut for each bucket.",3 Experiments,[0],[0]
"Specifically, we use (negative) word error rate (WER) as the similarity measure, and plot the bucket-by-bucket performance gain for each group in Figure 2.",3 Experiments,[0],[0]
"As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training.",3 Experiments,[0],[0]
This is the desirable effect of data augmentation techniques.,3 Experiments,[0],[0]
"In this paper, we propose a method to design data augmentation algorithms by solving an optimization problem.",4 Conclusion,[0],[0]
"These solutions subsume a few existing augmentation schemes and inspire a novel augmentation method, SwitchOut.",4 Conclusion,[0],[0]
SwitchOut delivers improvements over translation tasks at different scales.,4 Conclusion,[0],[0]
"Additionally, SwitchOut is efficient and easy to implement, and thus has the potential for wide application.",4 Conclusion,[0],[0]
"We thank Quoc Le, Minh-Thang Luong, Qizhe Xie, and the anonymous EMNLP reviewers, for their suggestions to improve the paper.
",Acknowledgements,[0],[0]
This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114.,Acknowledgements,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgements,[0],[0]
"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT).",abstractText,[0],[0]
"We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution.",abstractText,[0],[0]
"This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies.",abstractText,[0],[0]
We name this method SwitchOut.,abstractText,[0],[0]
"Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a).",abstractText,[0],[0]
Code to implement this method is included in the appendix.,abstractText,[0],[0]
SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3772–3782 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3772",text,[0],[0]
"As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited.",1 Introduction,[0],[0]
"Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014).",1 Introduction,[0],[0]
"Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017).
",1 Introduction,[0],[0]
"Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization.",1 Introduction,[0],[0]
"We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing?
",1 Introduction,[0],[0]
"We propose a multitask learning approach to incorporating syntactic information into learned
representations of neural semantics models (§2).",1 Introduction,[0],[0]
"Our approach, the syntactic scaffold, minimizes an auxiliary supervised loss function, derived from a syntactic treebank.",1 Introduction,[0],[0]
"The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling.",1 Introduction,[0],[0]
"We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications)",1 Introduction,[0],[0]
the semantic analyzer has no additional cost over a syntax-free baseline.,1 Introduction,[0],[0]
"Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task.
",1 Introduction,[0],[0]
"Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016).",1 Introduction,[0.9579040101435408],"['More generally, existing standalone (or traditional) text similarity measures rely on the intersections between token sets and/or text sizes and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein (Sankoff and Kruskal, 1983), Jaccard (Jain and Dubes, 1988) and Jaro (Jaro, 1989).']"
These spans are usually syntactic constituents (cf.,1 Introduction,[0],[0]
"PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold.",1 Introduction,[0],[0]
See Figure 1 for an example sentence with syntactic and semantic annotations.,1 Introduction,[0],[0]
"Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree.",1 Introduction,[0],[0]
"This means we never need to run a syntactic parsing algorithm.
",1 Introduction,[0],[0]
Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6).,1 Introduction,[0],[0]
"Our models use the strongest available neural network architectures for these tasks, integrating deep representation learning (He et al., 2017) and structured prediction at the level of spans (Kong et al., 2016).",1 Introduction,[0],[0]
"For SRL, the base-
line itself is a novel globally normalized structured conditional random field, which outperforms the previous state of the art.1 Syntactic scaffolds result in further improvements over prior work— 3.6 absolute F1 in FrameNet SRL, 1.1 absolute F1 in PropBank SRL, and 0.6 F1 in coreference resolution (averaged across three standard scores).",1 Introduction,[0],[0]
Our code is open source and available at https: //github.com/swabhs/scaffolding.,1 Introduction,[0],[0]
"Multitask learning (Caruana, 1997) is a collection of techniques in which two or more tasks are learned from data with at least some parameters shared.",2 Syntactic Scaffolds,[0],[0]
"We assume there is only one task about whose performance we are concerned, denoted T1 (in this paper, T1 is either SRL or coreference resolution).",2 Syntactic Scaffolds,[0],[0]
"We use the term “scaffold” to refer to a second task, T2, that can be combined with T1 during multitask learning.",2 Syntactic Scaffolds,[0],[0]
"A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T1, and after learning is completed, the scaffold is discarded.
",2 Syntactic Scaffolds,[0],[0]
"A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic
1This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours.
structure.",2 Syntactic Scaffolds,[0],[0]
It could be defined through a syntactic parser that shares some parameters with T1’s model.,2 Syntactic Scaffolds,[0],[0]
"Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees.
",2 Syntactic Scaffolds,[0],[0]
"As with multitask learning in general, we do not assume that the same data are annotated with outputs for T1 and T2.",2 Syntactic Scaffolds,[0],[0]
"In this work, T2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).",2 Syntactic Scaffolds,[0],[0]
We experiment with three settings: one where the corpus for T2 does not overlap with the training datasets for T1 (frame-SRL) and two where there is a complete overlap (PropBank SRL and coreference).,2 Syntactic Scaffolds,[0],[0]
"Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T1 and T2 output.",2 Syntactic Scaffolds,[0],[0]
"We briefly contrast the syntactic scaffold with existing alternatives.
Pipelines.",3 Related Work,[0],[0]
"In a typical pipeline, T1 and T2 are separately trained, with the output of T2 used to define the inputs to T1 (Wolpert, 1992).",3 Related Work,[0],[0]
"Using syntax as T2 in a pipeline is perhaps the most
common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T2’s mistakes affect the performance, and perhaps the training, of T1; He et al., 2013).",3 Related Work,[0],[0]
"To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006).",3 Related Work,[0],[0]
"A syntactic scaffold is quite different from a pipeline since the output of T2 is never explicitly used.
",3 Related Work,[0],[0]
Latent variables.,3 Related Work,[0],[0]
Another solution is to treat the output of T2 as a (perhaps structured) latent variable.,3 Related Work,[0],[0]
This approach obviates the need of supervision for T2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T1.,3 Related Work,[0],[0]
Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012).,3 Related Work,[0],[0]
"Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T2, and it need not overlap the T1 training data.
",3 Related Work,[0],[0]
Joint learning of syntax and semantics.,3 Related Work,[0],[0]
"The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Lluı́s and Màrquez, 2008; Lluı́s et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016).",3 Related Work,[0],[0]
"This typically requires joint prediction of the outputs of T1 and T2, which tends to be computationally expensive at both training and test time.
",3 Related Work,[0],[0]
Part of speech scaffolds.,3 Related Work,[0],[0]
"Similar to our work, there have been multitask models that use partof-speech tagging as T2, with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T1.",3 Related Work,[0],[0]
Both of the above approaches assumed parallel input data and used both tasks as supervision.,3 Related Work,[0],[0]
"Notably, we simplify our T2, throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with.",3 Related Work,[0],[0]
"While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations.",3 Related Work,[0],[0]
"In-
2",3 Related Work,[0],[0]
"There has been some recent work on SRL which completely forgoes syntactic processing (Zhou and Xu, 2015), however it has been shown that incorporating syntactic information still remains useful (He et al., 2017).
",3 Related Work,[0],[0]
"stead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks.",3 Related Work,[0],[0]
"Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture.",3 Related Work,[0],[0]
"To the best of our knowledge, such simplified syntactic scaffolds have not been tried before.
",3 Related Work,[0],[0]
Word embeddings.,3 Related Work,[0],[0]
"Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018).",3 Related Work,[0],[0]
"After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings.",3 Related Work,[0],[0]
"A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T1 through a multitask objective.
",3 Related Work,[0],[0]
Multitask learning.,3 Related Work,[0],[0]
"Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017).",3 Related Work,[0],[0]
"In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018).",3 Related Work,[0],[0]
"Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013).",3 Related Work,[0],[0]
"Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.",3 Related Work,[0],[0]
"We assume two sources of supervision: a corpusD1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebankD2 with sentences x, each with a phrase-structure tree z.",4 Syntactic Scaffold Model,[0],[0]
"Each task has an associated loss, and we seek to minimize the combination of task losses,∑
(x,y)∈D1 L1(x, y) + δ ∑ (x,z)∈D2 L2(x, z) (1)
with respect to parameters, which are partially shared, where δ is a tunable hyperparameter.",4.1 Loss,[0],[0]
"In
the rest of this section, we describe the scaffold task.",4.1 Loss,[0],[0]
"We define the primary tasks in Sections 5–6.
",4.1 Loss,[0],[0]
"Each input is a sequence of tokens, x = 〈x1, x2, . . .",4.1 Loss,[0],[0]
", xn〉, for some n. We refer to a span of contiguous tokens in the sentence as xi: j = 〈xi, xi+1, . . .",4.1 Loss,[0],[0]
", x",4.1 Loss,[0],[0]
"j〉, for any 1 6 i 6 j 6 n. In our experiments we consider only spans up to a maximum length D, resulting in O(nD) spans.
",4.1 Loss,[0],[0]
"Supervision comes from a phrase-syntactic tree z for the sentence, comprising a syntactic category zi: j ∈ C for every span xi: j in x (many spans are given a null label).",4.1 Loss,[0],[0]
"We experiment with different sets of labels C (§4.2).
",4.1 Loss,[0],[0]
"In our model, every span xi: j is represented by an embedding vector vi: j (see details in §5.3).",4.1 Loss,[0],[0]
"A distribution over the category assigned to zi: j is derived from vi: j:
p(zi: j = c | xi: j) = softmax c wc · vi: j (2)
where wc is a parameter vector associated with category c.",4.1 Loss,[0],[0]
"We sum the log loss terms for all the spans in a sentence to give its loss:
L2(x, z) =",4.1 Loss,[0],[0]
"− ∑
16i6 j6n j−i6D
log p(zi: j | xi: j).",4.1 Loss,[0],[0]
(3),4.1 Loss,[0],[0]
"Different kinds of syntactic labels can be used for learning syntactically-aware span representations: • Constituent identity: C = {0, 1}; is a span a
constituent, or not?",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal: c is the category of a span,
including a null for non-constituents.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal and parent: c is the category
of a span, concatenated with the category of its immediate ancestor.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"null is used for nonconstituents, and for empty ancestors.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Common non-terminals: Since a majority
of semantic arguments and entity mentions are labeled with a small number of syntactic categories,3 we experiment with a threeway classification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER.
",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"3In the OntoNotes corpus, which includes both syntactic and semantic annotations, 44% of semantic arguments are noun phrases and 13% are prepositional phrases.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds.,5 Semantic Role Labeling,[0],[0]
"The performance of this baseline itself is competitive with state-of-the-art methods (§7).
",5 Semantic Role Labeling,[0],[0]
FrameNet.,5 Semantic Role Labeling,[0],[0]
"In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements.",5 Semantic Role Labeling,[0],[0]
"A frame can be evoked by a word or phrase in a sentence, called a target.",5 Semantic Role Labeling,[0],[0]
"Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized).",5 Semantic Role Labeling,[0],[0]
"Arguments for a given frame do not overlap.
",5 Semantic Role Labeling,[0],[0]
PropBank.,5 Semantic Role Labeling,[0],[0]
PropBank similarly disambiguates predicates and identifies argument spans.,5 Semantic Role Labeling,[0],[0]
"Targets are disambiguated to lexically specific senses rather than shared frames, and a set of generic roles is used for all targets, reducing the argument label space by a factor of 17.",5 Semantic Role Labeling,[0],[0]
"Most importantly, the arguments were annotated on top of syntactic constituents, directly coupling syntax and semantics.",5 Semantic Role Labeling,[0],[0]
"A detailed example for both formalisms is provided in Figure 1.
",5 Semantic Role Labeling,[0],[0]
"Semantic structure prediction is the task of identifying targets, labeling their frames or senses, and labeling all their argument spans in a sentence.",5 Semantic Role Labeling,[0],[0]
"Here we assume gold targets and frames, and consider only the SRL task.
",5 Semantic Role Labeling,[0],[0]
"Formally, a single input instance for argument identification consists of: an n-word sentence x = 〈x1, x2, . . .",5 Semantic Role Labeling,[0],[0]
", xn〉, a single target span t = 〈tstart, tend〉, and its evoked frame, or sense, f .",5 Semantic Role Labeling,[0],[0]
"The argument labeling task is to produce a segmentation of the sentence: s = 〈s1, s2, . . .",5 Semantic Role Labeling,[0],[0]
", sm〉 for each input x.",5 Semantic Role Labeling,[0],[0]
A segment s = 〈,5 Semantic Role Labeling,[0],[0]
"i, j, yi: j〉 corresponds to a labeled span of the sentence, where the label yi: j ∈ Y f ∪ {null} is either a role that the span fills, or null if the span does not fill any role.",5 Semantic Role Labeling,[0],[0]
"In the case of PropBank, Y f consists of all possible roles.",5 Semantic Role Labeling,[0],[0]
The segmentation is constrained so that argument spans cover the sentence and do not overlap (ik+1 = 1 + jk for sk; i1 = 1; jm = n).,5 Semantic Role Labeling,[0],[0]
Segments of length 1 such that i = j are allowed.,5 Semantic Role Labeling,[0],[0]
A separate segmentation is predicted for each target annotation in a sentence.,5 Semantic Role Labeling,[0],[0]
"In order to model the non-overlapping arguments of a given target, we use a semi-Markov conditional random field (semi-CRF; Sarawagi et al., 2004).",5.1 Semi-Markov CRF,[0],[0]
"Semi-CRFs define a conditional distribution over labeled segmentations of an input sequence, and are globally normalized.",5.1 Semi-Markov CRF,[0],[0]
A single target’s arguments can be neatly encoded as a labeled segmentation by giving the spans in between arguments a reserved null label.,5.1 Semi-Markov CRF,[0],[0]
"Semi-Markov models are more powerful than BIO tagging schemes, which have been used successfully for PropBank SRL (Collobert et al., 2011; Zhou and Xu, 2015, inter alia), because the semi-Markov assumption allows scoring variable-length segments, rather than fixed-length label n-grams as under an (n − 1)-order Markov assumption.",5.1 Semi-Markov CRF,[0],[0]
Computing the marginal likelihood with a semi-CRF can be done using dynamic programming in O(n2) time (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"By filtering out segments longer than D tokens, this is reduced to O(nD).
",5.1 Semi-Markov CRF,[0],[0]
"Given an input x, a semi-CRF defines a conditional distribution p(s | x).",5.1 Semi-Markov CRF,[0],[0]
Every segment s = 〈,5.1 Semi-Markov CRF,[0],[0]
"i, j, yi: j〉 is given a real-valued score, ψ(〈i, j, yi: j = r〉, xi: j) = wr · vi: j, where vi: j is an embedding of the span (§5.3) and wr is a parameter vector corresponding to its label.",5.1 Semi-Markov CRF,[0],[0]
"The score of the entire segmentation s is the sum of the scores of its segments: Ψ(x, s) =",5.1 Semi-Markov CRF,[0],[0]
"∑m k=1 ψ(sk, xik: jk ).",5.1 Semi-Markov CRF,[0],[0]
These scores are exponentiated and normalized to define the probability distribution.,5.1 Semi-Markov CRF,[0],[0]
The sum-product variant of the semi-Markov dynamic programming algorithm is used to calculate the normalization term (required during learning).,5.1 Semi-Markov CRF,[0],[0]
"At test time, the maxproduct variant returns the most probable segmentation, ŝ = arg max sΨ(s, x).
",5.1 Semi-Markov CRF,[0],[0]
The parameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"The learner evaluates and adjusts segment scores ψ(sk, x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3).",5.1 Semi-Markov CRF,[0],[0]
Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective.,5.2 Softmax-Margin Objective,[0],[0]
"In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant
of log-likelihood:
L1 = − ∑
(x,s∗)∈D1 log
exp Ψ(s∗, x) Z(x, s∗) , (4)
Z(x, s∗) =",5.2 Softmax-Margin Objective,[0],[0]
"∑
s exp {Ψ(s, x) +",5.2 Softmax-Margin Objective,[0],[0]
"cost(s, s∗)}.",5.2 Softmax-Margin Objective,[0],[0]
"(5)
We design the cost function so that it factors by predicted span, in the same way Ψ does:
cost(s, s∗)",5.2 Softmax-Margin Objective,[0],[0]
"= ∑ s∈s cost(s, s∗) = ∑ s∈s I(s < s∗).",5.2 Softmax-Margin Objective,[0],[0]
"(6)
The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations.",5.2 Softmax-Margin Objective,[0],[0]
"The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function:
α j = ∑
s=〈i, j,yi: j〉 j−i6D
αi−1 exp{Ψ(s, x) + cost(s, s∗)}, (7)
where Z = αn, under the base case α0 = 1.",5.2 Softmax-Margin Objective,[0],[0]
"The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1:
γ j = max s=〈i, j,yi: j〉
j−i6D
γi−1 exp Ψ(s, x).",5.2 Softmax-Margin Objective,[0],[0]
"(8)
Our model formulation enforces that arguments do not overlap.",5.2 Softmax-Margin Objective,[0],[0]
"We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012).",5.2 Softmax-Margin Objective,[0],[0]
"This section describes the neural architecture used to obtain the span embedding, vi: j, corresponding to a span xi: j and the target in consideration, t = 〈tstart, tend〉.",5.3 Input Span Representation,[0],[0]
"For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used.",5.3 Input Span Representation,[0],[0]
"If there are no verbs, we use the first token in the sentence as a placeholder target.",5.3 Input Span Representation,[0],[0]
"The parameters used to learn v are shared between the tasks.
",5.3 Input Span Representation,[0],[0]
"We construct an embedding for the span using • hi and h j: contextualized embeddings for the
words at the span boundary (§5.3.1), • ui: j: a span summary that pools over the con-
tents of the span (§5.3.2), and
• ai: j: and a hand-engineered feature vector for the span (§5.3.3).
",5.3 Input Span Representation,[0],[0]
"This embedding is then passed to a feedforward layer to compute the span representation, vi: j.",5.3 Input Span Representation,[0],[0]
"To obtain contextualized embeddings of each token in the input sequence, we run a bidirectional LSTM (Graves, 2012) with ` layers over the full input sequence.",5.3.1 Contextualized Token Embeddings,[0],[0]
"To indicate which token is a predicate, a linearly transformed one-hot embedding v is used, following Zhou and Xu (2015) and He et al. (2017).",5.3.1 Contextualized Token Embeddings,[0],[0]
The input vector representing the token at position q in the sentence is the concatenation of a fixed pretrained embedding xq and vq.,5.3.1 Contextualized Token Embeddings,[0],[0]
"When given as input to the bidirectional LSTM, this yields a hidden state vector hq representing the qth token in the context of the sentence.",5.3.1 Contextualized Token Embeddings,[0],[0]
Tokens within a span might convey different amounts of information necessary to label the span as a semantic argument.,5.3.2 Span Summary,[0],[0]
"Following Lee et al. (2017), we use an attention mechanism (Bahdanau et al., 2014) to summarize each span.",5.3.2 Span Summary,[0],[0]
"Each contextualized token in the span is passed through a feed-forward network to obtain a weight, normalized to give σk = softmax
i6k6 j whead · hk, where whead
is a learned parameter.",5.3.2 Span Summary,[0],[0]
"The weights σ are then used to obtain a vector that summarizes the span, ui: j = ∑ i6k6 j; j−i<D σk · hk.",5.3.2 Span Summary,[0],[0]
"We use the following three features for each span: • width of the span in tokens (Das et al., 2014) • distance (in tokens) of the span from the tar-
get (Täckström et al., 2015) • position of the span with respect to the tar-
get (before, after, overlap) (Täckström et al., 2015)
",5.3.3 Span Features,[0],[0]
"Each of these features is encoded as a one-hotembedding and then linearly transformed to yield a feature vector, ai: j.",5.3.3 Span Features,[0],[0]
Coreference resolution is the task of determining clusters of mentions that refer to the same entity.,6 Coreference Resolution,[0],[0]
"Formally, the input is a document x = x1, x2, . . .",6 Coreference Resolution,[0],[0]
", xn consisting of n words.",6 Coreference Resolution,[0],[0]
"The goal is to predict a set of clusters c = {c1, c2, . . .",6 Coreference Resolution,[0],[0]
"}, where each cluster c = {s1, s2, .",6 Coreference Resolution,[0],[0]
. .,6 Coreference Resolution,[0],[0]
"} is a set of spans and
each span s = 〈i, j〉 is a pair of indices such that 1 6 i 6 j 6 n.
As a baseline, we use the model of Lee et al. (2017), which we describe briefly in this section.",6 Coreference Resolution,[0],[0]
This model decomposes the prediction of coreference clusters into a series of span classification decisions.,6 Coreference Resolution,[0],[0]
"Every span s predicts an antecedent ws ∈ Y(s) = {null, s1, s2, . . .",6 Coreference Resolution,[0],[0]
", sm}.",6 Coreference Resolution,[0],[0]
"Labels s1 to sm indicate a coreference link between s and one of the m spans that precede it, and null indicates that s does not link to anything, either because it is not a mention or it is in a singleton cluster.",6 Coreference Resolution,[0],[0]
"The predicted clustering of the spans can be recovered by aggregating the predicted links.
",6 Coreference Resolution,[0],[0]
"Analogous to the SRL model (§5), every span s is represented by an embedding vs, which is central to the model.",6 Coreference Resolution,[0],[0]
"For each span s and a potential antecedent a ∈ Y(s), pairwise coreference scores Ψ(vs, va, φ(s, a)) are computed via feedforward networks with the span embeddings as input.",6 Coreference Resolution,[0],[0]
"φ(s, a) are pairwise discrete features encoding the distance between span s and span a and metadata, such as the genre and speaker information.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for the details of the scoring function.
",6 Coreference Resolution,[0],[0]
"The scores from Ψ are normalized over the possible antecedents Y(s) of each span to induce a probability distribution for every span:
p(ws = a) = softmax a∈Y(s) Ψ(vs, va, φ(s, a))",6 Coreference Resolution,[0],[0]
"(9)
In learning, we minimize the negative loglikelihood marginalized over the possibly correct antecedents:
L1 = − ∑ s∈D log ∑ a∗∈G(s)∩Y(s) p(ws = a∗) (10)
whereD is the set of spans in the training dataset, and G(s) indicates the gold cluster of s if it belongs to one and {null} otherwise.
",6 Coreference Resolution,[0],[0]
"To operate under reasonable computational requirements, inference under this model requires a two-stage beam search, which reduces the number of span pairs considered.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for details.
",6 Coreference Resolution,[0],[0]
Input span representation.,6 Coreference Resolution,[0],[0]
"The input span embedding, vs for coreference resolution and its syntactic scaffold follow the definition used in §5.3, with the key difference of using no target features.",6 Coreference Resolution,[0],[0]
"Since there is a complete overlap of input sentences betweenDsc andDpr as the coreference annotations are also from OntoNotes (Pradhan et al.,
2012), we reuse the v for the scaffold task.",6 Coreference Resolution,[0],[0]
"Additionally, instead of the entire document, each sentence in it is independently given as input to the bidirectional LSTMs.",6 Coreference Resolution,[0],[0]
We evaluate our models on the test set of FrameNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference.,7 Results,[0],[0]
"For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material.
",7 Results,[0],[0]
Frame SRL.,7 Results,[0],[0]
Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold.,7 Results,[0],[0]
"We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007).
",7 Results,[0],[0]
"Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; Täckström",7 Results,[0],[0]
"et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015).
",7 Results,[0],[0]
The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017).,7 Results,[0],[0]
"In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer.",7 Results,[0],[0]
"In their relational model (Rel), they treat the same problem as a span classification problem.",7 Results,[0],[0]
"Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints.",7 Results,[0],[0]
"Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5
Notably, all prior systems for frame SRL listed in Table 1 use a pipeline of syntax and semantics.",7 Results,[0],[0]
"Our semi-CRF baseline outperforms all prior work, without any syntax.",7 Results,[0],[0]
"This highlights the ben-
4http://cemantix.org/data/ontonotes.html 5Yang and Mitchell (2017) also evaluated on the full frame-semantic parsing task, which includes frame-SRL as well as identifying frames.",7 Results,[0],[0]
"Since our frame SRL performance improves over theirs, we expect that incorporation into a full system (e.g., using their frame identification module) would lead to overall benefits as well; this experiment is left to future work.
",7 Results,[0],[0]
"efits of modeling spans and of global normalization.
",7 Results,[0],[0]
"Turning to scaffolds, even the most coarsegrained constituent identity scaffold improves the performance of our syntax-agnostic baseline.",7 Results,[0],[0]
"The nonterminal and nonterminal and parent scaffolds, which use more detailed syntactic representations, improve over this.",7 Results,[0],[0]
"The greatest improvements come from the scaffold model predicting common nonterminal labels (NP and PP, which are the most common syntactic categories of semantic arguments, vs. others): 3.6% absolute improvement in F1 measure over prior work.
",7 Results,[0],[0]
"Contemporaneously with this work, Peng et al. (2018) proposed a system for joint frame-semantic and semantic dependency parsing.",7 Results,[0],[0]
"They report results for joint frame and argument identification, and hence cannot be directly compared in Table 1.",7 Results,[0],[0]
"We evaluated their output for argument identification only; our semi-CRF baseline model exceeds their performance by 1 F1, and our common nonterminal scaffold by 3.1 F1.6
6This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set.
",7 Results,[0],[0]
PropBank SRL.,7 Results,[0],[0]
"We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL.",7 Results,[0],[0]
"Table 2 reports results using gold predicates.
",7 Results,[0],[0]
"Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax.",7 Results,[0],[0]
"He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance.",7 Results,[0],[0]
Tan et al. (2018) employ a similar approach but use feed-forward networks with selfattention.,7 Results,[0],[0]
"He et al. (2018a) use a span-based classification to jointly identify and label argument spans.
",7 Results,[0],[0]
"Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction.",7 Results,[0],[0]
We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures.
",7 Results,[0],[0]
"He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set.",7 Results,[0],[0]
"The improvements from ELMo are methodologically orthogonal to syntactic scaffolds.
",7 Results,[0],[0]
"Since the datasets for learning PropBank semantics and syntactic scaffolds completely overlap, the performance improvement cannot be attributed to a larger training corpus (or, by extension, a larger vocabulary), though that might be a factor for frame SRL.
",7 Results,[0],[0]
"A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017).",7 Results,[0],[0]
"This, along with other recent ap-
proaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL.
Coreference.",7 Results,[0],[0]
"We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3.",7 Results,[0],[0]
"Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax.
",7 Results,[0.9539672377481859],"['In a more general study, Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task.']"
"Our baseline is the model from Lee et al. (2017), described in §6.",7 Results,[0],[0]
"Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax.
",7 Results,[0],[0]
We experiment with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases.",7 Results,[0],[0]
The syntactic scaffold outperforms the baseline by 0.6 absolute F1.,7 Results,[0],[0]
"Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1.",7 Results,[0],[0]
"All the above are orthogonal to our approach, and could be incorporated to yield higher gains.",7 Results,[0],[0]
"To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline.
",8 Discussion,[0],[0]
"We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2.",8 Discussion,[0],[0]
"Not surpris-
7We used FrameNet syntactic phrase annotations for analysis only, and not in our models, since they are annotated only for the gold arguments.
",8 Discussion,[0],[0]
"ingly, we observe large improvements in the common nonterminals used (NP and PP).",8 Discussion,[0],[0]
"However, the phrase type annotations in FrameNet do not correspond exactly to the OntoNotes phrase categories.",8 Discussion,[0],[0]
"For instance, FrameNet annotates nonmaximal (A) and standard adjective phrases (AJP), while OntoNotes annotations for noun-phrases are flat, ignore the underlying adjective phrases.",8 Discussion,[0],[0]
"This explains why the syntax-agnostic baseline is able to recover the former while the scaffold is not.
",8 Discussion,[0],[0]
"Similarly, for frequent frame elements, scaffolding improves performance across the board, as shown in Fig. 3.",8 Discussion,[0],[0]
"The largest improvements come for Theme and Goal, which are predominantly realized as noun phrases and prepositional phrases.",8 Discussion,[0],[0]
"We introduced syntactic scaffolds, a multitask learning approach to incorporate syntactic bias into semantic processing tasks.",9 Conclusion,[0],[0]
"Unlike pipelines and approaches which jointly model syntax and semantics, no explicit syntactic processing is required at runtime.",9 Conclusion,[0],[0]
"Our method improves the performance of competitive baselines for semantic role labeling on both FrameNet and PropBank, and for coreference resolution.",9 Conclusion,[0],[0]
"While our focus was on span-based tasks, syntactic scaffolds could be applied in other settings (e.g., dependency and graph representations).",9 Conclusion,[0],[0]
"Moreover, scaffolds need not be syntactic; we can imagine, for example, semantic scaffolds being used to improve NLP applications with limited annotated data.",9 Conclusion,[0],[0]
"It remains an open empirical question to determine the relative merits of different kinds of scaffolds and multitask learners, and how they can be most produc-
tively combined.",9 Conclusion,[0],[0]
Our code is publicly available at https://github.com/swabhs/scaffolding.,9 Conclusion,[0],[0]
"We thank several members of UW-NLP, particularly Luheng He, as well as David Weiss and Emily Pitler for thoughtful discussions on prior versions of this paper.",Acknowledgments,[0],[0]
We also thank the three anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS1562364 and by the NVIDIA Corporation through the donation of a Tesla GPU.,Acknowledgments,[0],[0]
"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks.",abstractText,[0],[0]
"Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective.",abstractText,[0],[0]
"We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",abstractText,[0],[0]
Syntactic Scaffolds for Semantic Structures,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2061",text,[0],[0]
"Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.",1 Introduction,[0],[0]
"Specifically, we seek to identify arguments and label their semantic roles given a predicate.",1 Introduction,[0],[0]
"SRL is an impor-
∗ These authors made equal contribution.† Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04).
tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016).
",1 Introduction,[0],[0]
"There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.",1 Introduction,[0],[0]
"The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments.",1 Introduction,[0],[0]
"Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification.
",1 Introduction,[0],[0]
"In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.",1 Introduction,[0],[0]
"In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008).",1 Introduction,[0],[0]
"However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance.
",1 Introduction,[0],[0]
"To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input.",1 Introduction,[0],[0]
"It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002).",1 Introduction,[0],[0]
"This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL.
",1 Introduction,[0],[0]
"This paper will focus on semantic dependency parsing and formulate SRL as one or two se-
quence tagging tasks with predicate-specific encoding.",1 Introduction,[0],[0]
"With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.
",1 Introduction,[0.9575132800369615],"['4.1 Canonical Text Flow TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures.']"
"In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL2008 Shared Task1 as evaluation metric.",1 Introduction,[0],[0]
"Considering that various syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study.",1 Introduction,[0],[0]
"To fully disclose the predicate-argument structure, typical SRL systems have to step by step perform four subtasks.",2 Model,[0],[0]
"Since the predicates in CoNLL2009 (Hajič et al., 2009) corpus have been preidentified, we need to tackle three other subtasks, which are formulized into two-step pipeline in this work, predicate disambiguation and argument labeling.",2 Model,[0],[0]
"Namely, we do the work of argument identification and classification in one model.
",2 Model,[0],[0]
Argument structure for each known predicate will be disclosed by our argument labeler over a sequence including possible arguments (candidates).,2 Model,[0],[0]
"There are two ways to determine the sequence, one is to simply input the entire sentence as a syntax-agnostic SRL system does, the other is to select words according to syntactic parse tree around the predicate as most previous SRL systems did.",2 Model,[0],[0]
The latter strategy usually works through a syntactic tree based argument pruning algorithm.,2 Model,[0],[0]
"We will use the proposed k-order argument pruning algorithm (Section 2.1) to get a sequence w = (w1, . . .",2 Model,[0],[0]
", wn) for each predicate.",2 Model,[0],[0]
"Then, we represent each word wi ∈ w as xi (Section 2.2).",2 Model,[0],[0]
"Eventually, we obtain contextual features with sequence encoder (Section 2.3).",2 Model,[0],[0]
The overall role labeling model is depicted in Figure 1.,2 Model,[0],[0]
"As pointed out by Punyakanok et al. (2008), syntactic information is most relevant in identifying
1CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one.",2.1 Argument Pruning,[0],[0]
"Their main difference is that predicates have been beforehand indicated for the latter.
",2.1 Argument Pruning,[0],[0]
"the arguments, and the most crucial contribution of full parsing is in the pruning stage.",2.1 Argument Pruning,[0],[0]
"In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b).",2.1 Argument Pruning,[0],[0]
"First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd).",2.1 Argument Pruning,[0],[0]
"Then we define k-order descendants of given node satisfying D(n, nd) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order.",2.1 Argument Pruning,[0],[0]
"Note that the definition of k-order traversal is somewhat different from tree traversal in terminology.
",2.1 Argument Pruning,[0],[0]
A brief description of the proposed k-order pruning algorithm is given as follow.,2.1 Argument Pruning,[0],[0]
"Initially, we set a given predicate as the current node in a syntactic dependency tree.",2.1 Argument Pruning,[0],[0]
"Then, collect all its argument candidates by the strategy of k-order traversal.",2.1 Argument Pruning,[0],[0]
"Afterwards, reset the current node to its syntactic head and repeat the previous step till the root of the tree.",2.1 Argument Pruning,[0],[0]
"Finally, collect the root and stop.",2.1 Argument Pruning,[0],[0]
The k-order argument algorithm is presented in Algorithm 1 in detail.,2.1 Argument Pruning,[0],[0]
"An example of a syntactic dependency tree for sentence She began to trade the art for money is shown in Figure 2.
",2.1 Argument Pruning,[0],[0]
"The main reasons for applying the extended korder argument pruning algorithm are two-fold.
",2.1 Argument Pruning,[0],[0]
Algorithm 1 k-order argument pruning algorithm,2.1 Argument Pruning,[0],[0]
"Input: A predicate p, the root node r given a syn-
tactic dependency tree T , the order k Output:",2.1 Argument Pruning,[0],[0]
"The set of argument candidates S
1: initialization set p as current node c, c = p 2: for each descendant ni of c in T do 3: if D(c, ni) ≤ k",2.1 Argument Pruning,[0],[0]
"and ni /∈ S then 4: S = S + ni 5: end if 6: end for 7: find the syntactic head ch of c, and let c = ch 8: if c = r then 9: S = S + r
10: else 11: goto step 2 12: end if 13: return argument candidates set S
First, previous standard pruning algorithm may hurt the argument coverage too much, even though indeed arguments usually tend to surround their predicate in a close distance.",2.1 Argument Pruning,[0],[0]
"As a sequence tagging model has been applied, it can effectively handle the imbalanced distribution between arguments and non-arguments, which is hardly tackled by early argument classification models that commonly adopt the standard pruning algorithm.",2.1 Argument Pruning,[0],[0]
"Second, the extended pruning algorithm provides a better trade-off between computational cost and performance by carefully tuning k.",2.1 Argument Pruning,[0],[0]
"We produce a predicate-specific word representation xi for each word wi, where i stands for the word position in an input sequence, following Marcheggiani et al. (2017).",2.2 Word Representation,[0],[0]
"However, we differ by (1) leveraging a predicate-specific indicator embedding, (2) using deeper refined representation, including character and dependency relation embeddings, and (3) applying recent advances in RNNs, such as highway connections (Srivastava et al., 2015).
",2.2 Word Representation,[0],[0]
"In this work, word representation xi is the concatenation of four types of features: predicatespecific feature, character-level, word-level and linguistic features.",2.2 Word Representation,[0],[0]
"Unlike previous work, we leverage a predicate-specific indicator embedding xiei rather than directly using a binary flag either 0 or 1.",2.2 Word Representation,[0],[0]
"At character level, we exploit convolutional neural network (CNN) with bidirectional LSTM (BiLSTM) to learn character embedding
xcei .",2.2 Word Representation,[0],[0]
"As shown in Figure 1, the representation calculated by the CNN is fed as input to BiLSTM.",2.2 Word Representation,[0],[0]
"At word level, we use a randomly initialized word embedding xrei and a pre-trained word embedding xpei .",2.2 Word Representation,[0],[0]
"For linguistic features, we employ a randomly initialized lemma embedding xlei and a randomly initialized POS tag embedding xposi .",2.2 Word Representation,[0],[0]
"In order to incorporate more syntactic information, we adopt an additional feature, the dependency relation to syntactic head.",2.2 Word Representation,[0],[0]
"Likewise, it is a randomly initialized embedding xdei .",2.2 Word Representation,[0],[0]
The resulting word representation is concatenated as xi =,2.2 Word Representation,[0],[0]
"[x ie i , x ce i , x re i , x pe",2.2 Word Representation,[0],[0]
"i , x le i , x pos",2.2 Word Representation,[0],[0]
"i , x de i ].",2.2 Word Representation,[0],[0]
"As Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have shown significant representational effectiveness to NLP tasks, we thus use BiLSTM as the sentence encorder.",2.3 Sequence Encoder,[0],[0]
"Given an input sequence x = (x1, . . .",2.3 Sequence Encoder,[0],[0]
", xn), BiLSTM processes the sequence in both forward and backward direction to obtain two separated hidden states, −→ h i which handles data from x1 to xi and ←− h i which tackles data from xn to xi for each word representation.",2.3 Sequence Encoder,[0],[0]
"Finally, we get a contextual representation hi =",2.3 Sequence Encoder,[0],[0]
"[ −→ h i, ←− h",2.3 Sequence Encoder,[0],[0]
i] by concatenating the states of BiLSTM networks.,2.3 Sequence Encoder,[0],[0]
"To get the final predicted semantic roles, we exploit a multi-layer perceptron (MLP) with highway connections on the top of BiLSTM networks, which takes as input the hidden representation hi
of all time steps.",2.3 Sequence Encoder,[0],[0]
The MLP network consists of 10 layers with highway connections and we employ ReLU activations for the hidden layers.,2.3 Sequence Encoder,[0],[0]
"Finally, we use a softmax layer over the outputs to maximize the likelihood of labels.",2.3 Sequence Encoder,[0],[0]
"Although predicates have been identified given a sentence, predicate disambiguation is an indispensable task, which aims to determine the predicate-argument structure for an identified predicate in a particular context.",2.4 Predicate Disambiguation,[0],[0]
"Here, we also use the identical model (BiLSTM composed with MLP) for predicate disambiguation, in which the only difference is that we remove the syntactic dependency relation feature in corresponding word representation (Section 2.2).",2.4 Predicate Disambiguation,[0],[0]
"Exactly, given a predicate p, the resulting word representation is pi =",2.4 Predicate Disambiguation,[0],[0]
"[p ie i , p ce",2.4 Predicate Disambiguation,[0],[0]
"i , p re i , p pe",2.4 Predicate Disambiguation,[0],[0]
"i , p le i , p pos",2.4 Predicate Disambiguation,[0],[0]
i ].,2.4 Predicate Disambiguation,[0],[0]
"Our model2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits.",3 Experiments,[0],[0]
"The hyperparameters in our model were selected based on the development set, and are summarized in Table 1.",3 Experiments,[0],[0]
Note that the parameters of predicate model are the same as these in argument model.,3 Experiments,[0],[0]
"All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors (Pennington et al., 2014).",3 Experiments,[0],[0]
"For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov
2The code is available at https://github.com/ bcmi220/srl_syn_pruning.
",3 Experiments,[0],[0]
"et al., 2013).",3 Experiments,[0],[0]
"During training procedures, we use the categorical cross-entropy as objective, with Adam optimizer (Kingma and Ba, 2015).",3 Experiments,[0],[0]
We train models for a maximum of 20 epochs and obtain the nearly best model based on development results.,3 Experiments,[0],[0]
"For argument labeling, we preprocess corpus with k-order argument pruning algorithm.",3 Experiments,[0],[0]
"In addition, we use four CNN layers with singlelayer BiLSTM to induce character representations derived from sentences.",3 Experiments,[0],[0]
"For English3, to further enhance the representation, we adopt CNNBiLSTM character embedding structure from AllenNLP toolkit (Peters et al., 2018).",3 Experiments,[0],[0]
"During the pruning of argument candidates, we use the officially predicted syntactic parses provided by CoNLL-2009 shared-task organizers on both English and Chinese.",3.1 Preprocessing,[0],[0]
Figure 3 shows changing curves of coverage and reduction following k on the English train set.,3.1 Preprocessing,[0],[0]
"According to our statistics, the number of non-arguments is ten times more than that of arguments, where the data distribution is fairly unbalanced.",3.1 Preprocessing,[0],[0]
"However, a proper pruning strategy could alleviate this problem.",3.1 Preprocessing,[0],[0]
"Accordingly, the first-order pruning reduces more than 50% candidates at the cost of missing 5.5% true ones on average, and the second-order prunes about 40% candidates with nearly 2.0% loss.",3.1 Preprocessing,[0],[0]
"The coverage of third-order has achieved 99% and it reduces approximately 1/3 corpus size.
",3.1 Preprocessing,[0],[0]
"It is worth noting that as k is larger than 19,
3For Chinese, we do not use character embedding.
",3.1 Preprocessing,[0],[0]
"there will come full coverage on all argument candidates for English training set, which let our high order pruning algorithm degrade into a syntaxagnostic setting.",3.1 Preprocessing,[0],[0]
"In this work, we use the tenthorder pruning for pursuing the best performance.",3.1 Preprocessing,[0],[0]
"Our system performance is measured with the official script from CoNLL-2009 benchmarks, combining the output of our predicate disambiguation with our semantic role labeling.",3.2 Results,[0],[0]
"Our predicate disambiguation model achieves the accuracy of 95.01% and 95.58%4 on development and test sets, respectively.",3.2 Results,[0],[0]
"We compare our model performance with the state-of-the-art models for dependency SRL.5 Noteworthily, our model is local and single without reranking, which neither includes global inference nor combines multiple models.",3.2 Results,[0],[0]
"The experimental results on the English in-domain (WSJ) and out-of-domain (Brown) test sets are shown in Tables 2 and 3, respectively.
",3.2 Results,[0],[0]
"For English, our syntax-aware model outperforms previously published best single model, scoring 89.5% F1 with 1.5% absolute improvement on the in-domain (WSJ) test data.",3.2 Results,[0],[0]
"Compared
4Note that we give a slightly better predicate model than Roth and Lapata (2016), with 94.77% and 95.47% accuracy on development and test sets, respectively.
5Here, we do not compare against span-based SRL models, which annotate roles for entire argument spans instead of semantic dependencies.
with ensemble models, our single model even provides better performance (+0.4% F1) than the system (Marcheggiani and Titov, 2017), and significantly surpasses all the rest models.",3.2 Results,[0],[0]
"In the syntaxagnostic setting (without pruning and dependency relation embedding), we also reach the new stateof-the-art, achieving a performance gain of 1% F1.
",3.2 Results,[0],[0]
"On the out-of-domain (Brown) test set, we achieve the new best results of 79.3% (syntaxaware) and 78.8% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"Moreover, our syntax-aware model performs better than the syntax-agnostic one.
",3.2 Results,[0],[0]
Table 4 presents the results on Chinese test set.,3.2 Results,[0],[0]
"Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"To evaluate the contributions of key factors in our method, a series of ablation studies are performed on the English development set.
",3.3 Analysis,[0],[0]
"In order to demonstrate the effectiveness of our k-order pruning algorithm, we report the SRL performance excluding predicate senses in evaluation, eliminating the performance gain from predicate disambiguation.",3.3 Analysis,[0],[0]
Table 5 shows the results from our syntax-aware model with lower order argument pruning.,3.3 Analysis,[0],[0]
"Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score.",3.3 Analysis,[0],[0]
"It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition.
",3.3 Analysis,[0],[0]
"Table 6 presents the performance of our syntaxagnostic SRL system with a basic configuration, which removes components, including indicator and character embeddings.",3.3 Analysis,[0],[0]
"Note that the first row is the results of BiLSTM (removing MLP from basic model), whose encoding is the same as Marcheggiani et al. (2017).",3.3 Analysis,[0],[0]
"Experiments show that both enhanced representations improve over our basic model, and our adopted labeling model is superior to the simple BiLSTM.
Figure 4 shows F1 scores in different k-order pruning together with our syntax-agnostic model.",3.3 Analysis,[0],[0]
"It also indicates that the least first-order pruning fails to give satisfactory performance, the best performing setting coming from a moderate setting of k = 10, and the largest k shows that our argu-
ment pruning falls back to syntax-agnostic type.",3.3 Analysis,[0],[0]
"Meanwhile, from the best k setting to the lower order pruning, we receive a much faster performance drop, compared to the higher order pruning until the complete syntax-agnostic case.",3.3 Analysis,[0],[0]
"The proposed k-order pruning algorithm always works even it reaches the syntax-agnostic setting, which empirically explains why the current syntax-aware and syntax-agnostic SRL models hold little performance difference, as maximum k-order pruning actually removes few words just like syntaxagnostic model.",3.3 Analysis,[0],[0]
"In this work, we consider additional model that integrates predicate disambiguation and argument labeling into one sequence labeling model.",3.4 End-to-end SRL,[0],[0]
"In order to implement an end-to-end model, we introduce a virtual root (VR) for predicate disambiguation similar to Zhao et al. (2013) who handled the entire SRL task as word pair classification.",3.4 End-to-end SRL,[0],[0]
"Concretely, we add a predicate sense feature to the input sequence by concatenating a VR.",3.4 End-to-end SRL,[0],[0]
The word representation of VR is randomly initialized during training.,3.4 End-to-end SRL,[0],[0]
"In Figure 5, we give an example sequence with the labels for the given sentence.
",3.4 End-to-end SRL,[0],[0]
We also report results of our end-to-end model on CoNLL-2009 test set with syntax-aware and syntax-agnostic settings.,3.4 End-to-end SRL,[0],[0]
"As shown in Table 7, our end-to-end model yields slightly weaker performance compared with our pipeline.",3.4 End-to-end SRL,[0],[0]
"A reasonable account for performance degradation is that the training data has completely different genre distributions over predicate senses and argument roles, which may be somewhat confusing for integrative model to make classification decisions.",3.4 End-to-end SRL,[0],[0]
"For a full SRL task, the predicate identification subtask is also indispensable, which has been included in CoNLL-2008 shared task.",3.5 CoNLL-2008 SRL Setting,[0],[0]
"We thus evaluate our model in terms of data and setting of the CoNLL-2008 benchmark (WSJ).
",3.5 CoNLL-2008 SRL Setting,[0],[0]
"To identify predicates, we train the BiLSTMMLP sequence labeling model with same parameters in Section 2.4 to tackle the predicate identification and disambiguation subtasks in one shot, and the only difference is that we remove the predicate-specific indicator feature.",3.5 CoNLL-2008 SRL Setting,[0],[0]
The F1 score of our predicate labeling model is 90.53% on indomain (WSJ) data.,3.5 CoNLL-2008 SRL Setting,[0],[0]
"Compared with the best reported results, we observe absolute improvements in semantic F1 of 0.8% (in Table 8).",3.5 CoNLL-2008 SRL Setting,[0],[0]
"Note that as predicate identification is introduced, our same model shows about 6% performance loss for either syntax-agnostic or syntax-aware case, which indicates that predicate identification should be carefully handled, as it is very needed in a complete practical SRL system.",3.5 CoNLL-2008 SRL Setting,[0],[0]
Syntactic information plays an informative role in semantic role labeling.,4 Syntactic Contribution,[0],[0]
"However, few studies were done to quantitatively evaluate the syntactic contribution to SRL.",4 Syntactic Contribution,[0],[0]
"Furthermore, we observe that most of the above compared neural SRL systems took the syntactic parser of (Björkelund et al., 2010) as syntactic inputs instead of the one from CoNLL-2009 shared task, which adopted a much weaker syntactic parser.",4 Syntactic Contribution,[0],[0]
"Especially (Marcheggiani and Titov, 2017), adopted an external syntactic
parser with even higher parsing accuracy.",4 Syntactic Contribution,[0],[0]
"Contrarily, our SRL model is based on the automatically predicted parse with moderate performance provided by CoNLL-2009 shared task, but outperforms their models.
",4 Syntactic Contribution,[0],[0]
This section thus attempts to explore how much syntax contributes to dependency-based SRL in deep learning framework and how to effectively evaluate relative performance of syntax-based SRL.,4 Syntactic Contribution,[0],[0]
"To this end, we conduct experiments for empirical analysis with different syntactic inputs.
",4 Syntactic Contribution,[0],[0]
"Syntactic Input In order to obtain different syntactic inputs, we design a faulty syntactic tree generator (refer to STG hereafter), which is able to produce random errors in the output parse tree like a true parser does.",4 Syntactic Contribution,[0],[0]
"To simplify implementation, we construct a new syntactic tree based on the gold standard parse tree.",4 Syntactic Contribution,[0],[0]
"Given an input error probability distribution estimated from a true parser output, our algorithm presented in Algorithm 2 stochastically modifies the syntactic heads of nodes on the premise of a valid tree.
",4 Syntactic Contribution,[0],[0]
"Evaluation Measure For SRL task, the primary evaluation measure is the semantic labeled F1 score.",4 Syntactic Contribution,[0],[0]
"However, the score is influenced by the quality of syntactic input to some extent, leading to unfaithfully reflecting the competence of syntax-based SRL system.",4 Syntactic Contribution,[0],[0]
"Namely, this is not the outcome of a true and fair quantitative comparison for these types of SRL models.",4 Syntactic Contribution,[0],[0]
"To normalize the semantic score relative to syntactic parse, we take into account additional evaluation measure to estimate the actual overall performance of SRL.",4 Syntactic Contribution,[0],[0]
"Here, we use the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies
Algorithm 2 Faulty Syntactic Tree Generator Input: A gold standard syntactic tree GT , the
specific error probability p Output: The new generative syntactic tree NT
1: N denotes the number of nodes in GT 2: for each node n ∈ GT do 3: r = random(0, 1), a random number 4: if r < p then 5: h = random(0, N ), a random integer 6: find the syntactic head nh of n in GT 7: modify nh = h, and get a new tree NT 8: if NT is a valid tree then 9: break
10: else 11: goto step 5 12: end if 13: end if 14: end for 15: return the new generative tree NT
proposed by Surdeanu et al. (2008) as evaluation metric.6 The benefits of this measure are twofold: quantitatively evaluating syntactic contribution to SRL and impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.
",4 Syntactic Contribution,[0],[0]
Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.,4 Syntactic Contribution,[0],[0]
"Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we
6The idea of ratio score in Surdeanu et al. (2008) actually was from author of this paper, Hai Zhao, which has been indicated in the acknowledgement part of Surdeanu et al. (2008).
7Note that several SRL systems without providing syntactic information are not listed in the table.
obtain the highest results both on Sem-F1 and the Sem-F1/LAS ratio, respectively.",4 Syntactic Contribution,[0],[0]
These results show that our SRL component is relatively much stronger.,4 Syntactic Contribution,[0],[0]
"Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself.",4 Syntactic Contribution,[0],[0]
"All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a)",4 Syntactic Contribution,[0],[0]
(Joint track using self-developed parser).,4 Syntactic Contribution,[0],[0]
"We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task.
",4 Syntactic Contribution,[0],[0]
"We also perform our first and tenth order pruning models with different erroneous syntactic inputs generated from STG and evaluate their per-
formance using the Sem-F1/LAS ratio.",4 Syntactic Contribution,[0],[0]
Figure 6 shows Sem-F1 scores at different quality of syntactic parse inputs on the English test set whose LAS varies from 85% to 100%.,4 Syntactic Contribution,[0],[0]
"Compared to previous state-of-the-arts (Marcheggiani and Titov, 2017).",4 Syntactic Contribution,[0],[0]
"Our tenth-order pruning model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, while our firstorder pruning model yields overall lower results (1-5% F1 drop), owing to missing too many true arguments.",4 Syntactic Contribution,[0],[0]
These results show that high-quality syntactic parses may indeed enhance dependency SRL.,4 Syntactic Contribution,[0],[0]
"Furthermore, it indicates that our model with an accurate enough syntactic input as Marcheggiani and Titov (2017), namely, 90% LAS, will give a Sem-F1 exceeding 90% for the first time in the research timeline of semantic role labeling.",4 Syntactic Contribution,[0],[0]
Semantic role labeling was pioneered by Gildea and Jurafsky (2002).,5 Related Work,[0],[0]
"Most traditional SRL models rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Björkelund et al., 2009).",5 Related Work,[0],[0]
"Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm.",5 Related Work,[0],[0]
"Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task.
",5 Related Work,[0],[0]
"With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed.",5 Related Work,[0],[0]
"Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach.",5 Related Work,[0],[0]
"Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning.",5 Related Work,[0],[0]
Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success.,5 Related Work,[0],[0]
Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models.,5 Related Work,[0],[0]
"Differently, Marcheggiani et al. (2017) proposed a
syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models.
",5 Related Work,[0],[0]
"However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance.",5 Related Work,[0],[0]
"This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning.",5 Related Work,[0],[0]
"Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017).",5 Related Work,[0],[0]
"Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage.",5 Related Work,[0],[0]
"He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance.",5 Related Work,[0],[0]
"This paper presents a simple and effective neural model for dependency-based SRL, incorporating syntactic information with the proposed extended k-order pruning algorithm.",6 Conclusion and Future Work,[0],[0]
"With a large enough setting of k, our pruning algorithm will result in a syntax-agnostic setting for the argument labeling model, which smoothly unifies syntax-aware and syntax-agnostic SRL in a consistent way.",6 Conclusion and Future Work,[0],[0]
"Experimental results show that with the help of deep enhanced representation, our model outperforms the previous state-of-the-art models in both syntaxaware and syntax-agnostic situations.
",6 Conclusion and Future Work,[0],[0]
"In addition, we consider the Sem-F1/LAS ratio as a mean of evaluating syntactic contribution to SRL, and true performance of SRL independent of the quality of syntactic parser.",6 Conclusion and Future Work,[0],[0]
"Though we again confirm the importance of syntax to SRL with empirical experiments, we are aware that since (Pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic SRL has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work.",6 Conclusion and Future Work,[0],[0]
"However, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic SRL system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input.",6 Conclusion and Future Work,[0],[0]
Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence.,abstractText,[0],[0]
Previous studies have shown syntactic information has a remarkable contribution to SRL performance.,abstractText,[0],[0]
"However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone.",abstractText,[0],[0]
This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework.,abstractText,[0],[0]
We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information.,abstractText,[0],[0]
"Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",abstractText,[0.9522303051545512],"['When compared to state-of-the-art measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8.']"
"Syntax for Semantic Role Labeling, To Be, Or Not To Be",title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Simultaneous interpretation is challenging because it demands both quality and speed.,1 Introduction,[0],[0]
Conventional batch translation waits until the entire sentence is completed before starting to translate.,1 Introduction,[0],[0]
This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience.,1 Introduction,[0],[0]
Simultaneous interpretation instead requires a tradeoff between quality and speed.,1 Introduction,[0],[0]
A common strategy is to translate independently translatable segments as soon as possible.,1 Introduction,[0],[0]
"Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders.",1 Introduction,[0],[0]
We show an example of Japanese-English translation in Figure 1.,1 Introduction,[0],[0]
"Consider the batch translation: in English, the verb change comes immediately after the subject",1 Introduction,[0],[0]
"We, whereas in Japanese it comes at the end
of the sentence; therefore, to produce an intelligible English sentence, we must translate the object after the final verb is observed, resulting in one large and painfully delayed segment.
",1 Introduction,[0],[0]
"To reduce structural discrepancy, we can apply syntactic transformations to make the word order of one language closer to the other.",1 Introduction,[0],[0]
Consider the monotone translation in Figure 1.,1 Introduction,[0],[0]
"By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb.",1 Introduction,[0],[0]
"Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction.",1 Introduction,[0],[0]
"These transformations enable us to divide the input into shorter segments, thus reducing translation delay.
",1 Introduction,[0],[0]
"To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order.",1 Introduction,[0],[0]
Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff.,1 Introduction,[0],[0]
"However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010).",1 Introduction,[0],[0]
"In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice.
",1 Introduction,[0],[0]
We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ syntactic flexibility.,1 Introduction,[0],[0]
"We propose to rewrite the reference translation in a way that uses the original lexicon, obeys standard grammar rules of
55
the target language, preserves the original semantics, and yields more monotonic translations.",1 Introduction,[0],[0]
We then train the MT system with the rewritten references so that it learns how to produce low-latency translations from the data.,1 Introduction,[0],[0]
A data-driven approach to learning these rewriting rules is hampered by the dearth of parallel data: we have few examples of text that have been both interpreted and translated.,1 Introduction,[0],[0]
"Therefore, we design syntactic transformation rules based on linguistic analysis of the source and the target languages.",1 Introduction,[0],[0]
We apply these rules to parsed text and decide whether to accept the rewritten sentence based on the amount of delay reduction.,1 Introduction,[0],[0]
"In this work, we focus on Japanese to English translation, because (i) Japanese and English have significantly different word orders (SOV vs. SVO); and consequently, (ii) the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence.
",1 Introduction,[0],[0]
We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting.,1 Introduction,[0],[0]
Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations.,1 Introduction,[0.9526822310549969],['The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches.']
Simultaneous interpretation has two goals: producing good translations and producing them promptly.,2 The Problem of Delay Reduction,[0],[0]
"However, most existing parallel corpora and MT systems do not address the issue of delay during translation.",2 The Problem of Delay Reduction,[0],[0]
We explicitly adapt the training data by rewriting rules to reduce delay.,2 The Problem of Delay Reduction,[0],[0]
We first define translation delay and describe—in general terms— our rewriting rules.,2 The Problem of Delay Reduction,[0],[0]
"In the next section, we describe the rules in more detail.
",2 The Problem of Delay Reduction,[0],[0]
"While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input.",2 The Problem of Delay Reduction,[0],[0]
"Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation.",2 The Problem of Delay Reduction,[0],[0]
We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1,2 The Problem of Delay Reduction,[0],[0]
"For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word.",2 The Problem of Delay Reduction,[0],[0]
"At test time, when the input is segmented, the delay is the average segment length.",2 The Problem of Delay Reduction,[0],[0]
"During the data preprocessing step of rewriting, we calculate delay from word alignments (Section 4).
",2 The Problem of Delay Reduction,[0],[0]
"Given a reference batch translation x, we apply a set of rewriting rulesR to x to minimize its delay.",2 The Problem of Delay Reduction,[0],[0]
"A rewriting rule r ∈ R is a mapping that takes the constituent parse tree of x as input and outputs a modified parse tree, which specifies a rewritten sentence x′.",2 The Problem of Delay Reduction,[0],[0]
"The tree-editing operation includes node deletion, insertion, and swapping, as well as induced changes of word form and node label.",2 The Problem of Delay Reduction,[0],[0]
"A valid transformation rule should rearrange constituents in x to follow the word order of the input sentence as closely as possible, subject to grammatical constraints and preservation of the original meaning.
",2 The Problem of Delay Reduction,[0],[0]
"1Ideally, delay should be based on time lapse.",2 The Problem of Delay Reduction,[0],[0]
"However, timestamping is not applicable to typical MT corpus, therefore we approximate it by number of tokens and ignore decoding time.",2 The Problem of Delay Reduction,[0],[0]
We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences.,3 Transformation Rules,[0],[0]
"Our rules cover verb, noun, and clause reordering.",3 Transformation Rules,[0],[0]
"While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",3 Transformation Rules,[0],[0]
The most significant difference between Japanese and English is that the head of a verb phrase comes at the end of Japanese sentences.,3.1 Verb Phrases,[0],[0]
"In English, it occupies one of the initial positions.",3.1 Verb Phrases,[0],[0]
"We now introduce rules that can postpone a head verb.
",3.1 Verb Phrases,[0],[0]
Passivization and Activization,3.1 Verb Phrases,[0],[0]
"In Japanese, the standard structure of a sentence is NP1 NP2 verb, where case markers following the verb indicate the voice of the sentence.",3.1 Verb Phrases,[0],[0]
"However, in English, we have NP1 verb NP2, where the form of the verb indicates its voice.",3.1 Verb Phrases,[0],[0]
Changing the voice is particularly useful when NP2 (object in an active-voice sentence and subject in a passive-voice sentence) is long.,3.1 Verb Phrases,[0],[0]
"By reversing positions of verb and NP2, we are not held back by the upcoming verb and can start to translate NP2 immediately.",3.1 Verb Phrases,[0],[0]
"Figure 1 shows an example in which passive voice can help make the target and source word orders more compatible, but it is not the case that passivizing every sentence would be a good idea; sometimes making a passive sentence active makes the word orders more compatible if the objects are relatively short:
",3.1 Verb Phrases,[0],[0]
O: The talk was denied by the boycott group spokesman.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The boycott group spokesman denied the talk.
",3.1 Verb Phrases,[0],[0]
"Quotative Verbs Quotative verbs are verbs that, syntactically and semantically, resemble said and often start an independent clause.",3.1 Verb Phrases,[0],[0]
"Such verbs are frequent, especially in news, and can be moved to the end of a sentence:
",3.1 Verb Phrases,[0],[0]
O: They announced that the president will restructure the division.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The president will restructure the division, they announced.
",3.1 Verb Phrases,[0],[0]
"In addition to quotative verbs, candidates typically include factive (e.g., know, realize, observe), factive-like (e.g., announce, determine), belief (e.g., believe, think, suspect), and antifactive (e.g., doubt, deny) verbs.",3.1 Verb Phrases,[0],[0]
"When these verbs are followed by a
clause (S or SBAR), we move the verb and its subject to the end of the clause.
",3.1 Verb Phrases,[0],[0]
"While some exploratory work automatically extracts factive verbs, to our knowledge, an exhaustive list does not exist.",3.1 Verb Phrases,[0],[0]
"To obtain a list with reasonable coverage, we exploit the fact that Japanese has an unambiguous quotative particle, to, that precedes such verbs.2 We identify all of the verbs in the Kyoto corpus (Neubig, 2011) marked by the quotative particle and translate them into English.",3.1 Verb Phrases,[0],[0]
We then use these as our quotative verbs.3,3.1 Verb Phrases,[0],[0]
Another difference between Japanese and English lies in the order of adjectives and the nouns they modify.,3.2 Noun Phrases,[0],[0]
"We identify two situations where we can take advantage of the flexibility of English grammar to favor sentence structures consistent with positions of nouns in Japanese.
",3.2 Noun Phrases,[0],[0]
"Genitive Reordering In Japanese, genitive constructions always occur in the form of X no Y, where Y belongs to X.",3.2 Noun Phrases,[0],[0]
"In English, however, the order may be reversed through the of construction.",3.2 Noun Phrases,[0],[0]
"Therefore, we transform constructions NP1 of NP2 to possessives using the apostrophe-s, NP2’(s) NP1 (Figure 1).",3.2 Noun Phrases,[0],[0]
We use simple heuristics to decide if such a transformation is valid.,3.2 Noun Phrases,[0],[0]
"For example, when X / Y contains proper nouns (e.g., the City of New York), numbers (e.g., seven pounds of sugar), or pronouns (e.g., most of them), changing them to the possessive case is not legal.
that Clause In English, clauses are often modified through a pleonastic pronoun.",3.2 Noun Phrases,[0],[0]
"E.g., It is ADJP to/that SBAR/S.",3.2 Noun Phrases,[0],[0]
"In Japanese, however, the subject (clause) is usually put at the beginning.",3.2 Noun Phrases,[0],[0]
"To be consistent with the Japanese word order, we move the modified clause to the start of the sentence: To S/SBAR is ADJP.",3.2 Noun Phrases,[0],[0]
"The rewritten English sentence is still grammatical, although its structure is less frequent in common English usage.",3.2 Noun Phrases,[0],[0]
"For example,
O: It is important to remain watchful.",3.2 Noun Phrases,[0],[0]
"R: To remain watchful is important.
",3.2 Noun Phrases,[0],[0]
2We use a morphological analyzer to distinguish between the conjunction and quotative particles.,3.2 Noun Phrases,[0],[0]
"Examples of words marked by this particle include 見られる (expect), 言う (say), 思われる (seem), する (assume), 信じる (believe) and so on.
",3.2 Noun Phrases,[0],[0]
3We also include the phrase It looks like.,3.2 Noun Phrases,[0],[0]
"In Japanese, clausal conjunctions are often marked at the end of the initial clause of a compound sentence.",3.3 Conjunction Clause,[0],[0]
"In English, however, the order of clauses is more flexible.",3.3 Conjunction Clause,[0],[0]
We can therefore reduce delay by reordering the English clauses to mirror how they typically appear in Japanese.,3.3 Conjunction Clause,[0],[0]
"Below we describe rules reversing the order of clauses connected by these conjunctions:
• Clausal conjunctions: because (of), in order to • Contrastive conjunctions: despite, even though, although • Conditionals: (even) if, as a result (of) •",3.3 Conjunction Clause,[0],[0]
"Misc: according to
In standard Japanese, such conjunctions include no de, kara, de mo and so on.",3.3 Conjunction Clause,[0],[0]
"The sentence often appears in the form of S2 conj, S1.",3.3 Conjunction Clause,[0],[0]
"In English, however, two common constructions are
S1 conj S2: We should march because winter is coming.",3.3 Conjunction Clause,[0],[0]
"conj S2, S1: Because winter is coming, we should march.
",3.3 Conjunction Clause,[0],[0]
"To follow the Japanese clause order, we adapt the above two constructions to
S2, conj’ S1: Winter is coming, because of this, we should march.
",3.3 Conjunction Clause,[0],[0]
Here conj’ represents the original conjunction word appended with simple pronouns/phrases to refer to S2.,3.3 Conjunction Clause,[0],[0]
"For example, because → because of this, even if → even if this is the case.",3.3 Conjunction Clause,[0],[0]
We now turn our attention to the implementation of the syntactic transformation rules described above.,4 Sentence Rewriting Process,[0],[0]
"Applying a transformation consists of three steps:
1.",4 Sentence Rewriting Process,[0],[0]
Detection:,4 Sentence Rewriting Process,[0],[0]
Identify nodes in the parse tree for which the transformation is applicable; 2.,4 Sentence Rewriting Process,[0],[0]
Modification: Transform nodes and labels; 3.,4 Sentence Rewriting Process,[0],[0]
"Evaluation: Compute delay reduction, and
decide whether to accept the rewritten sentence.
",4 Sentence Rewriting Process,[0],[0]
Figure 2 illustrates the process using passivization as an example.,4 Sentence Rewriting Process,[0],[0]
"In the detection step, we find the subtree that satisfies the condition of applying a rule.",4 Sentence Rewriting Process,[0],[0]
"In this case, we look for an S node whose children include an NP (denoted by NP1), the subject, and a VP to its right, such that the VP node has a leaf VB*, the main verb,4 followed by another NP (denoted by NP2), the object.",4 Sentence Rewriting Process,[0],[0]
We allow the parent nodes (S and VP) to have additional children besides the matched ones.,4 Sentence Rewriting Process,[0],[0]
They are not affected during the transformation.,4 Sentence Rewriting Process,[0],[0]
"In the modification step, we swap the subject node and object node; we add the verb be in its correct form by checking the tense of the verb and the form of NP2;5and we add the preposition by before the subject.",4 Sentence Rewriting Process,[0],[0]
"The process is executed recursively throughout the parse tree.
",4 Sentence Rewriting Process,[0],[0]
"4The main verb excludes be and have when it indicates tense (e.g., have done).
",4 Sentence Rewriting Process,[0],[0]
"5We use the Nodebox linguistic library (https://www. nodebox.net/code) to detect and modify word forms.
",4 Sentence Rewriting Process,[0],[0]
"Although our rules are designed to minimize long range reordering, there are exceptions.6 Thus applying a rule does not always reduce delay.",4 Sentence Rewriting Process,[0],[0]
"In the evaluation step, we compare translation delay before and after applying the rule.",4 Sentence Rewriting Process,[0],[0]
"We accept a rewritten sentence if its delay is reduced; otherwise, we revert to the input sentence.",4 Sentence Rewriting Process,[0],[0]
"Since we do not segment sentences during rewriting, we must estimate the delay.
",4 Sentence Rewriting Process,[0],[0]
"To estimate the delay, we use word alignments.",4 Sentence Rewriting Process,[0],[0]
Figure 2c shows the source Japanese sentence in its word-for-word English translation and alignments from the target words to the source words.,4 Sentence Rewriting Process,[0],[0]
"The first English word, We, is aligned to the first Japanese word; it can thus be treated as an independent segment and translated immediately.",4 Sentence Rewriting Process,[0],[0]
"The second English word, love, is aligned to the last Japanese word, which means the system cannot start to translate until four more Japanese words are revealed.",4 Sentence Rewriting Process,[0],[0]
This alignment therefore forms a segment with delay of four words/seg.,4 Sentence Rewriting Process,[0],[0]
"Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay.",4 Sentence Rewriting Process,[0],[0]
"In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg.",4 Sentence Rewriting Process,[0],[0]
"Therefore, we accept the rewritten sentence.",4 Sentence Rewriting Process,[0],[0]
"However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay.
",4 Sentence Rewriting Process,[0],[0]
We can now formally define the delay.,4 Sentence Rewriting Process,[0],[0]
Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to.,4 Sentence Rewriting Process,[0],[0]
"We define the delay of ei as di = max(0, ai−maxj<i aj).",4 Sentence Rewriting Process,[0],[0]
The delay of x is then ∑N i=1,4 Sentence Rewriting Process,[0],[0]
"di/N , where the sum is over all aligned words except punctuation and stopwords.
",4 Sentence Rewriting Process,[0],[0]
"Given a set of rules, we need to decide which rules to apply and in what order.",4 Sentence Rewriting Process,[0],[0]
"Fortunately, our rules have little interaction with each other, and the order of application has a negligible effect.",4 Sentence Rewriting Process,[0],[0]
"We apply the rules, roughly, sequentially in order of complexity: if the output of current rule is not accepted, the sentence is reverted to the last accepted version.",4 Sentence Rewriting Process,[0],[0]
"We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003).",5 Experiments,[0],[0]
"For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1.",5 Experiments,[0],[0]
"The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents.
",5 Experiments,[0],[0]
"We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences.",5 Experiments,[0],[0]
"Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings.",5 Experiments,[0],[0]
We use GIZA++,5 Experiments,[0],[0]
"(Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning.",5 Experiments,[0],[0]
"The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model.",5 Experiments,[0],[0]
"After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted.",5.1 Quality of Rewritten Translations,[0],[0]
The most generalizable rules are passivization and delaying quotative verbs.,5.1 Quality of Rewritten Translations,[0],[0]
"We rewrite 32.2% of sentences, reducing the delay from 9.9 words/seg to 6.3 words/seg per segment for rewritten sentences and from 7.8 words/seg to 6.7 words/seg overall.
",5.1 Quality of Rewritten Translations,[0],[0]
"6For example, in clause transformation, the Japanese conjunction moshi, which is clause initial, may appear at the beginning of a sentence to emphasize conditionals, although its appearance is relatively rare.
",5.1 Quality of Rewritten Translations,[0],[0]
"7Available at http://eijiro.jp 8Available at http://www.atilika.org/ 9In contrast to BLEU, RIBES is an order-sensitive metric commonly used for translation between Japanese and English.
",5.1 Quality of Rewritten Translations,[0],[0]
We evaluate the quality of our rewritten sentences from two perspectives: grammaticality and preserved semantics.,5.1 Quality of Rewritten Translations,[0],[0]
"To examine how close the rewritten sentences are to standard English, we train a 5-gram language model using the English data from the Europarl corpus, consisting of 46 million words, and use it to compute perplexity.",5.1 Quality of Rewritten Translations,[0],[0]
Rewriting references increases the perplexity under the language model only slightly: from 332.0 to 335.4.,5.1 Quality of Rewritten Translations,[0],[0]
"To ensure that rewrites leave meaning unchanged, we use the SEMAFOR semantic role labeler (Das et al., 2014) on the original and modified sentence; for each role-labeled token in the reference sentence, we examine its corresponding role in the rewritten sentence and calculate the average accuracy acrosss all sentences.",5.1 Quality of Rewritten Translations,[0],[0]
"Even ignoring benign lexical changes—for example, he becoming him in a passivized sentence—95.5% of the words retain their semantic roles in the rewritten sentences.
",5.1 Quality of Rewritten Translations,[0],[0]
"Although our rules are conservative to minimize corruption, some errors are unavoidable propagation of parser errors.",5.1 Quality of Rewritten Translations,[0],[0]
"For example, the sentence the London Stock Exchange closes at 1230 GMT today is parsed as:10 (S (NP the London Stock Exchange) (VP (VBZ closes)
(PP at 1230) (NP GMT today)))
",5.1 Quality of Rewritten Translations,[0],[0]
GMT today is separated from the PP as an NP and is mistaken as the object.,5.1 Quality of Rewritten Translations,[0],[0]
The passive version is then GMT today is closed at 1230 by the London Stock Exchange.,5.1 Quality of Rewritten Translations,[0],[0]
"Such errors could be reduced by skipping nodes with low inside/outside scores given by the parser, or skipping low-frequency patterns.",5.1 Quality of Rewritten Translations,[0],[0]
"However, we leave this for future work.",5.1 Quality of Rewritten Translations,[0],[0]
"At test time, we use right probability (Fujita et al., 2013, RP) to decide when to start translating a
10For simplicity we show the shallow parse only.
sentence.",5.2 Segmentation,[0],[0]
"As we read in the source Japanese sentence, if the input segment matches an entry in the learned phrase table, we query the RP of the Japanese/English phrase pair.",5.2 Segmentation,[0],[0]
A higher RP indicates that the English translation of this Japanese phrase will likely be followed by the translation of the next Japanese phrase.,5.2 Segmentation,[0],[0]
"In other words, translation of the two consecutive Japanese phrases is monotonic, thus, we can begin translating immediately.",5.2 Segmentation,[0],[0]
"Following (Fujita et al., 2013), if the RP of the current phrase is lower than a fixed threshold, we cache the current phrase and wait for more words from the source sentence; otherwise, we translate all cached phrases.",5.2 Segmentation,[0],[0]
"Finally, translations of segments are concatenated to form a complete translation of the input sentence.",5.2 Segmentation,[0],[0]
"To show the effect of rewritten references, we compare the following MT systems:
• GD: only gold reference translations; • RW: only rewritten reference translations; • RW+GD: both gold and the rewritten refer-
ences; and • RW-LM+GD: using gold reference transla-
tions but using the rewritten references for training the LM and for tuning.
",5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD and RW-LM+GD, we interpolate the language models of GD and RW.",5.3 Speed/Accuracy Trade-off,[0],[0]
The interpolating weight is tuned with the rewritten sentences.,5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new.
Increasing the RP threshold increases interpretation delay but improves the quality of the translation.",5.3 Speed/Accuracy Trade-off,[0],[0]
"We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation).",5.3 Speed/Accuracy Trade-off,[0],[0]
Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold.,5.3 Speed/Accuracy Trade-off,[0],[0]
Rewritten sentences alone do not significantly improve over the baseline.,5.3 Speed/Accuracy Trade-off,[0],[0]
"We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing errors, which impairs learning.",5.3 Speed/Accuracy Trade-off,[0],[0]
"However, combining RW and GD results in a better speed-accuracy tradeoff: the RW+GD curve completely dominates other curves in Figure 3a, 3c.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Thus, using more monotone translations improves simultaneous machine translation, and because RW-LM+GD is about
0 5 10 15 20 25 30 35
Average # of words per segment
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(a) BLEU w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD RW-LM+GD RW GD
(b) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
10
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(c) BLEU",5.3 Speed/Accuracy Trade-off,[0],[0]
w.r.t.,5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD",5.3 Speed/Accuracy Trade-off,[0],[0]
"RW-LM+GD RW GD
(d) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
Figure 3: Speed/accuracy tradeoff curves: BLEU (left) /",5.3 Speed/Accuracy Trade-off,[0],[0]
"RIBES (right) versus translation delay (average number of words per segment).
",5.3 Speed/Accuracy Trade-off,[0],[0]
"the same as GD, the major improvement likely comes from the translation model from rewritten sentences.
",5.3 Speed/Accuracy Trade-off,[0],[0]
The right two plots recapitulate the evaluation with the RIBES metric.,5.3 Speed/Accuracy Trade-off,[0],[0]
"This result is less clear, as MT systems are optimized for BLEU and RIBES penalizes word reordering, making it difficult to compare systems that intentionally change word order.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Nevertheless, RW is comparable to GD on gold references and superior to the baseline on rewritten references.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation.",5.4 Effect on Verbs,[0],[0]
One reason is that SOV to SVO translation often drops the verb because of long range reordering.,5.4 Effect on Verbs,[0],[0]
"(We see this for Japanese here, but this is also true for German.)",5.4 Effect on Verbs,[0],[0]
"Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins
et al., 2005; Xu et al., 2009).",5.4 Effect on Verbs,[0],[0]
"Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW+GD, as well as the number in the gold reference translation.",5.4 Effect on Verbs,[0],[0]
"Both RW and RW+GD produce more verbs (a statistically significant result), although RW+GD captures the most verbs.",5.4 Effect on Verbs,[0],[0]
Table 4 compares translations by GD and RW.,5.5 Error Analysis,[0],[0]
"RW correctly puts the verb said at the end, while GD drops the final verb.",5.5 Error Analysis,[0],[0]
"However, RW still produces he at the beginning (also the first word in the Japanese source sentence).",5.5 Error Analysis,[0],[0]
This is because our current segmentation strategy do not preserve words for later translation—a note-taking strategy used by human interpreters.,5.5 Error Analysis,[0],[0]
Previous approaches to simultaneous machine translation have employed explicit interpretation strategies for coping with delay.,6 Related Work,[0],[0]
"Two major approaches are segmentation and prediction.
",6 Related Work,[0],[0]
"Most segmentation strategies are based on heuristics, such as pauses in speech (Fügen et al., 2007; Bangalore et al., 2009), comma prediction (Sridhar et al., 2013) and phrase reordering probability (Fujita et al., 2013).",6 Related Work,[0],[0]
Learning-based methods have also been proposed.,6 Related Work,[0],[0]
Oda et al. (2014) find segmentations that maximize the BLEU score of the final concatenated translation by dynamic programming.,6 Related Work,[0],[0]
Grissom II et al. (2014) formulate simultaneous translation as a sequential decision making problem and uses reinforcement learning to decide when to translate.,6 Related Work,[0],[0]
"One limitation of these methods is that when learning with standard batch MT corpus, their gain can be restricted by natural word reordering between the source and the target sentences, as explained in Section 1.
",6 Related Work,[0],[0]
"In an SOV-SVO context, methods to predict unseen words are proposed to alleviate the above restriction.",6 Related Work,[0],[0]
Matsubara et al. (1999) predict the English verb in the target sentence and integrates it syntactically.,6 Related Work,[0],[0]
"Grissom II et al. (2014) predict the final verb in the source sentence and decide when to use the predicted verb with reinforcement learning.
",6 Related Work,[0],[0]
"Nevertheless, unless the predictor considers contextual and background information, which human interpreters often rely on for prediction (Hönig, 1997; Camayd-Freixas, 2011), such a prediction task is inherently hard.
",6 Related Work,[0],[0]
"Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one.",6 Related Work,[0],[0]
"We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system.",6 Related Work,[0],[0]
"In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined.
",6 Related Work,[0],[0]
"This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders.",6 Related Work,[0],[0]
"However, our problem is different in several ways.",6 Related Work,[0],[0]
"First, while the approaches resemble each other, our motivation is to reduce translation delay.",6 Related Work,[0],[0]
"Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed.",6 Related Work,[0],[0]
"Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences.",6 Related Work,[0],[0]
Training MT systems with more monotonic (interpretation-like) sentences improves the speedaccuracy tradeoff for simultaneous machine translation.,7 Conclusion,[0],[0]
"By designing syntactic transformations and rewriting batch translations into more monotonic translations, we reduce the translation delay.",7 Conclusion,[0],[0]
"MT systems trained on the rewritten reference translations learn interpretation strategies implicitly from the data.
",7 Conclusion,[0],[0]
Our rewrites are based on linguistic knowledge and inspired by techniques used by human interpreters.,7 Conclusion,[0],[0]
"They cover a wide range of reordering phenomena between Japanese and English, and more generally, between SOV and SVO languages.",7 Conclusion,[0],[0]
A natural extension is to automatically extract such rules from parallel corpora.,7 Conclusion,[0],[0]
"While there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing.",7 Conclusion,[0],[0]
This work was supported by NSF grant IIS1320538.,Acknowledgments,[0],[0]
Boyd-Graber is also partially supported by NSF grants CCF-1409287 and NCSE-1422492.,Acknowledgments,[0],[0]
Daumé III,Acknowledgments,[0],[0]
and He are also partially supported by NSF grant IIS-0964681.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
Divergent word order between languages causes delay in simultaneous machine translation.,abstractText,[0],[0]
We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff.,abstractText,[0],[0]
We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.,abstractText,[0],[0]
We apply the rules to reference translations to make their word order closer to the source language word order.,abstractText,[0],[0]
"On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",abstractText,[0],[0]
Syntax-based Rewriting for Simultaneous Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1325–1337 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1325",text,[0],[0]
Dependency parsing is a core task in natural language processing (NLP).,1 Introduction,[0],[0]
"Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words.",1 Introduction,[0],[0]
"While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareček, 2016).",1 Introduction,[0],[0]
So it is extremely helpful to have some treebank of supervised parses for training purposes.,1 Introduction,[0],[0]
"Unfortunately, manually constructing a treebank for a new target language is expensive (Böhmová et al., 2003).",1.1 Past work: Cross-lingual transfer,[0],[0]
"As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017).",1.1 Past work: Cross-lingual transfer,[0],[0]
The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages.,1.1 Past work: Cross-lingual transfer,[0],[0]
"Although the parser cannot be expected to know the words of the target language, it can make do with parts of
speech (POS) (McDonald et al., 2011; Täckström",1.1 Past work: Cross-lingual transfer,[0],[0]
"et al., 2013; Zhang and Barzilay, 2015) or crosslingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).",1.1 Past work: Cross-lingual transfer,[0],[0]
"A more serious challenge is that the parser may not know how to handle the word order of the target language, unless the source treebank comes from a closely related language (e.g., using German to parse Luxembourgish).",1.1 Past work: Cross-lingual transfer,[0],[0]
"Training the parser on trees from multiple source languages may mitigate this issue (McDonald et al., 2011) because the parser is more likely to have seen target part-of-speech sequences somewhere in the training data.",1.1 Past work: Cross-lingual transfer,[0],[0]
"Some authors (Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016) have shown additional improvements by preferring source languages that are “close” to the target language, where the closeness is measured by distance between POS language models trained on the source and target corpora.",1.1 Past work: Cross-lingual transfer,[0],[0]
"We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree.",1.2 This paper: Tailored synthetic data,[0],[0]
"We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"This is the setup of Zeman and Resnik (2008) and Søgaard (2011a).
",1.2 This paper: Tailored synthetic data,[0],[0]
"Our novel ingredient is that rather than seek a close source language that already exists, we create one.",1.2 This paper: Tailored synthetic data,[0],[0]
How?,1.2 This paper: Tailored synthetic data,[0],[0]
"Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language.
",1.2 This paper: Tailored synthetic data,[0],[0]
And how do we find this distribution?,1.2 This paper: Tailored synthetic data,[0],[0]
We adopt the tree-permutation model of Wang and Eisner (2016).,1.2 This paper: Tailored synthetic data,[0],[0]
"We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bigrams in the permuted source treebank.",1.2 This paper: Tailored synthetic data,[0],[0]
"This allows us to evaluate p by computing the divergence between the bigram POS language model formed by these expected counts,
and the one formed by the observed counts of POS bigrams in the unparsed target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"In order to find a p that locally minimizes this divergence, we adjust the model parameters by stochastic gradient descent (SGD).",1.2 This paper: Tailored synthetic data,[0],[0]
Better measures of surface closeness between two languages might be devised.,1.3 Key limitations in this paper,[0],[0]
"However, even counting the expected POS N -grams is moderately expensive, taking time exponential in N if done exactly.",1.3 Key limitations in this paper,[0],[0]
"So we compute only these local statistics, and only for N = 2.",1.3 Key limitations in this paper,[0],[0]
We certainly need N > 1 because the 1-gram distribution is not affected by permutation at all.,1.3 Key limitations in this paper,[0],[0]
"N = 2 captures useful bigram statistics: for example, to mimic a verb-final language with prenominal modifiers, we would seek constituent permutations that result in matching its relatively high rate of VERB–PUNCT and ADJ–NOUN bigrams.",1.3 Key limitations in this paper,[0],[0]
"While N > 2 might have improved the results, it was too slow for our large-scale experimental design.",1.3 Key limitations in this paper,[0],[0]
"§7 discusses how richer measures could be used in the future.
",1.3 Key limitations in this paper,[0],[0]
"We caution that throughout this paper, we assume that our corpora are annotated with gold POS tags, even in the target language (which lacks any gold training trees).",1.3 Key limitations in this paper,[0],[0]
This is an idealized setting that has often been adopted in work on unsupervised and cross-lingual transfer.§7 discusses a possible avenue for doing without gold tags.,1.3 Key limitations in this paper,[0],[0]
We begin by motivating the idea of tree permutation.,2 Modeling Surface Realization,[0],[0]
Let us suppose that the dependency tree for a sentence starts as a labeled graph—a tree in which siblings are not yet ordered with respect to their parent or one another.,2 Modeling Surface Realization,[0],[0]
Each language has some systematic way to realize its unordered trees as surface strings:1 it imposes a particular order on the tree’s word tokens.,2 Modeling Surface Realization,[0],[0]
"More precisely, a language specifies a distribution p(string | unordered tree) over a tree’s possible realizations.
",2 Modeling Surface Realization,[0],[0]
"As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages.",2 Modeling Surface Realization,[0],[0]
"That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface.
",2 Modeling Surface Realization,[0],[0]
"1Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018).",2 Modeling Surface Realization,[0],[0]
"Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018).
",2 Modeling Surface Realization,[0],[0]
"Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees.",2 Modeling Surface Realization,[0],[0]
"To obtain samples of the latter distribution, we use the treebanks of one or more other languages.",2 Modeling Surface Realization,[0],[0]
The present paper evaluates our method when only a single source treebank is used.,2 Modeling Surface Realization,[0],[0]
"In the future, we could try tuning a mixture of all available source treebanks.",2 Modeling Surface Realization,[0],[0]
We presume that the target language applies the same stochastic realization model to all trees.,2.1 Realization is systematic,[0],[0]
All that we can optimize is the parameter vector of this model.,2.1 Realization is systematic,[0],[0]
"Thus, we deny ourselves the freedom to realize each individual tree in an ad hoc way.",2.1 Realization is systematic,[0],[0]
"To see why this is important, suppose the target language is French, whose corpus u contains many NOUN–ADJ bigrams.",2.1 Realization is systematic,[0],[0]
"We could achieve such a bigram from the unordered source tree
DET NOUN VERB PROPN ADJ
the cake made Sue sleepy
det nsubj dobj xcomp
by ordering
it to yield DET NOUN ADJ VERB PROPN the cake sleepy made Sue
det dobjxcomp nsubj
.",2.1 Realization is systematic,[0],[0]
"However, that realization is not in fact appropriate for French, so that ordered tree would not be a useful training tree for French.",2.1 Realization is systematic,[0],[0]
"Our approach should disprefer this tempting but incorrect realization, because any model with a high probability of this realization would, if applied systematically over the whole corpus, also yield sentences like He sleepy made Sue, with unwanted PRON–ADJ bigrams that would not match the surface statistics of French.",2.1 Realization is systematic,[0],[0]
"We hope our approach will instead choose the realization model that is correct for French, in which the NOUN–ADJ bigrams arise instead from source trees where the ADJ is a dependent of the NOUN, yielding (e.g.)
DET NOUN ADJ VERB PROPN the cake tasty pleased Sue
dobjdet amod nsubj
.",2.1 Realization is systematic,[0],[0]
"This has the same POS sequence as the example above (as it happens), but now assigns the correct tree to it.",2.1 Realization is systematic,[0],[0]
"As our family of realization distributions, we adopt the log-linear model used for this purpose by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
"The model assumes that the root node a of the unordered dependency tree selects an ordering π(a) of the na nodes consisting
of a and its na − 1 dependent children.",2.2 A parametric realization model,[0],[0]
The procedure is repeated recursively at the child nodes.,2.2 A parametric realization model,[0],[0]
"This method can produce only projective trees.
",2.2 A parametric realization model,[0],[0]
"Each node a draws its ordering π(a) independently according to
pθ(π | a) = 1
Z(a) exp ∑ 1≤i<j≤na θ · f(π, i, j) (1)
which is a distribution over the na! possible orderings.",2.2 A parametric realization model,[0],[0]
Z(a) is a normalizing constant.,2.2 A parametric realization model,[0],[0]
"f is a feature vector extracted from the ordered pair of nodes πi, πj , and θ is the model’s parameter vector of feature weights.",2.2 A parametric realization model,[0],[0]
"See Appendix A for the feature templates, which are a subset of those used by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
These features are able to examine the tree’s node labels (POS tags) and edge labels (dependency relations).,2.2 A parametric realization model,[0],[0]
"Thus, when a is a verb, the model can assign a positive weight to “subject precedes verb” or “subject precedes object,” thus preferring orderings with these features.
",2.2 A parametric realization model,[0],[0]
"Following Wang and Eisner (2016, §3.1), we choose new orderings for the noun and verb nodes only,2 preserving the source treebank’s order at all other nodes a.",2.2 A parametric realization model,[0],[0]
"Given a source treebank B and some parameters θ, we can use equation (1) to randomly sample realizations of the trees inB.",2.3 Generating training data,[0],[0]
The effect is to reorder dependent phrases within those trees.,2.3 Generating training data,[0],[0]
The resulting permuted treebank B′ can be used to train a parser for the target language.,2.3 Generating training data,[0],[0]
So how do we choose θ that works for the target language?,2.4 Choosing parameters θ,[0],[0]
"Suppose u is a corpus of targetlanguage POS sequences, using the same set of POS tags as B. We evaluate parameters θ according to whether POS tag sequences in B′ will be distributed like POS tag sequences in u.
To do this, first we estimate a bigram language model q̂ from the actual distribution q of POS sequences observed in u. Second, let pθ denote the distribution of POS sequences that we expect to see in B′, that is, POS sequences obtained by
2Specifically, the 93% of nodes tagged with NOUN, PROPN, PRON or VERB in Universal Dependencies format.",2.4 Choosing parameters θ,[0],[0]
"In retrospect, this restriction was unnecessary in our setting, but it skipped only 4.4% of nodes on average (from 2% to 11% depending on language).",2.4 Choosing parameters θ,[0],[0]
"The remaining nodes were nouns, verbs, or childless.
stochastically realizing observed trees in B according to θ.",2.4 Choosing parameters θ,[0],[0]
"We estimate another bigram model p̂θ from this distribution pθ.
",2.4 Choosing parameters θ,[0],[0]
"We then try to set θ, using SGD, to minimize a divergence D(p̂θ, q̂) that we will define below.",2.4 Choosing parameters θ,[0],[0]
"Estimating q̂ is straightforward: q̂(t | s) = cq(st)/cq(s), where cq(st) is the count of POS bigram st in the average3 sentence of u and cq(s) =∑
t′ cq(st ′).",2.4.1 Estimation of bigram models,[0],[0]
"We estimate p̂θ in the same way, where cp(st) denotes the expected count of st in a random POS sequence y ∼ pθ.",2.4.1 Estimation of bigram models,[0],[0]
"This is equivalent to choosing q̂, p̂θ to minimize the KL-divergences KL(q ||",2.4.1 Estimation of bigram models,[0],[0]
"q̂),KL(pθ || p̂θ).",2.4.1 Estimation of bigram models,[0],[0]
"It ensures that each model’s expected bigram counts match those in the POS sequences.
",2.4.1 Estimation of bigram models,[0],[0]
"However, these maximum-likelihood estimates might overfit on our finite data, u and B. We therefore smooth both models by first adding λ = 0.1 to all bigram counts cq(st) and cp(st).4",2.4.1 Estimation of bigram models,[0],[0]
We need a metric to evaluate θ.,2.4.2 Divergence of bigram models,[0],[0]
"If p and q are bigram language models over POS sequences y (sentences), their Kullback-Leibler divergence is
KL(p || q) def=",2.4.2 Divergence of bigram models,[0],[0]
"Ey∼p[log p(y)− log q(y)] (2) = ∑ s,t cp(st) (3)
· (log p(t | s)− log q(t | s))
where y ranges over POS sequences and st ranges over POS bigrams.",2.4.2 Divergence of bigram models,[0],[0]
"These include bigrams where s = BOS (“beginning of sequence”) or t = EOS (“end of sequence”), which are boundary tags that we take to surround y.
All quantities in equation (3) can be determined directly from the (expected) bigram counts given by cp and cq.",2.4.2 Divergence of bigram models,[0],[0]
"No other model estimation is needed.
",2.4.2 Divergence of bigram models,[0],[0]
A concern about equation (3) is that a single bigram st that is badly underrepresented in q may contribute an arbitrarily large term log p(t|s)q(t|s) .,2.4.2 Divergence of bigram models,[0],[0]
"To limit this contribution to at most log 1α , for some small α ∈ (0, 1), we define KLα(p || q) by a variant of equation (3) in which q(t | s) has been replaced by q̃(t | s) def= αp(t",2.4.2 Divergence of bigram models,[0],[0]
"| s) + (1− α)q(t | s).5
3A more familiar definition of cq would use the total count in u. Our definition, which yields the same bigram probabilities, is analogous to our definition of cp.",2.4.2 Divergence of bigram models,[0],[0]
"This cp is needed for KL(p || q) in (3), and cq symmetrically for KL(q || p).
",2.4.2 Divergence of bigram models,[0],[0]
"4Ideally one should tune λ to minimize the language model perplexity on held-out data (e.g., by cross-validation).
",2.4.2 Divergence of bigram models,[0],[0]
"5This is inspired by the α-skew divergence of Lee (1999,
Our final divergence metric D(p̂θ, q̂) defines D as a linear combination of exclusive and inclusive KLα divergences, which respectively emphasize pθ’s precision and recall at matching q’s bigrams:
D(p, q) =",2.4.2 Divergence of bigram models,[0],[0]
(1−β)·KLα1(p || q) Ey∼p[ |y| ] +β·KLα2(q ||,2.4.2 Divergence of bigram models,[0],[0]
"p) Ey∼q[ |y| ] (4) where β, α1, α2 are tuned by cross-validation to maximize the downstream parsing performance.",2.4.2 Divergence of bigram models,[0],[0]
"The division by average sentence length converts KL from nats per sentence to nats per word,6 so that the KL values have comparable scale even if B has much longer or shorter sentences than u.",2.4.2 Divergence of bigram models,[0],[0]
"We now present a polynomial-time algorithm for computing the expected bigram counts cp under pθ (or equivalently p̂θ), for use above.",3.1 Efficiently computing expected counts,[0],[0]
"This averages expected counts from each unordered tree x ∈ B. Algorithm 1 in the supplement gives pseudocode.
",3.1 Efficiently computing expected counts,[0],[0]
"The insight is that rather than sampling a single realization of x (as B′ does), we can use dynamic programming to sum efficiently over all of its exponentially many realizations.",3.1 Efficiently computing expected counts,[0],[0]
This gives an exact answer.,3.1 Efficiently computing expected counts,[0],[0]
"It algorithmically resembles tree-to-string machine translation, which likewise considers the possible reorderings of a source tree and incorporates a language model by similarly tracking their surface N -grams (Chiang, 2007, §5.3.2).
",3.1 Efficiently computing expected counts,[0],[0]
"For each node a of the tree x, let the POS string ya be the realization of the subtree rooted at a. Let ca(st) be the expected count of bigram st in ya, whose distribution is governed by equation (1).",3.1 Efficiently computing expected counts,[0],[0]
"We allow s = BOS or t = EOS as defined in §2.4.2.
",3.1 Efficiently computing expected counts,[0],[0]
The ca function can be represented as a sparse map from POS bigrams to reals.,3.1 Efficiently computing expected counts,[0],[0]
We compute ca at each node a of x in a bottom-up order.,3.1 Efficiently computing expected counts,[0],[0]
"The final step computes croot, giving the expected bigram counts in x’s realization y",3.1 Efficiently computing expected counts,[0],[0]
"(that is, cp in §2.4).
",3.1 Efficiently computing expected counts,[0],[0]
We find ca as follows.,3.1 Efficiently computing expected counts,[0],[0]
"Let n = na and recall from §2.2 that π(a) is an ordering of a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an, where a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an−1 are the child nodes of a, and an is a dummy node representing a’s head token.
2001).",3.1 Efficiently computing expected counts,[0],[0]
"Indeed, we may regard KLα(p || q) as the α-skew divergence between the unigram distributions p(· | s) and q(· | s), averaged over all s in proportion to cp(s).",3.1 Efficiently computing expected counts,[0],[0]
"In principle, we could have used the α-skew divergence between the distributions p(·) and q(·) over POS sequences y, but computing that would have required a sampling-based approximation (§7).
6Recall that the units of negated log-probability are called bits for log base 2, but nats for log base e.
Also, let a0 and an+1 be dummy nodes that always appear at the start and end of any ordering.
",3.1 Efficiently computing expected counts,[0],[0]
For all 0 ≤,3.1 Efficiently computing expected counts,[0],[0]
i ≤ n,3.1 Efficiently computing expected counts,[0],[0]
and 1 ≤ j ≤ n,3.1 Efficiently computing expected counts,[0],[0]
"+ 1, let pa(i, j) denote the expected count of the aiaj node bigram—the probability that π(a) places node ai immediately before node aj .",3.1 Efficiently computing expected counts,[0],[0]
"These node bigram probabilities can be obtained by enumerating all possible orderings π, a matter we return to below.
",3.1 Efficiently computing expected counts,[0],[0]
"It is now easy to compute ca:
ca(st) =",3.1 Efficiently computing expected counts,[0],[0]
"c within a (st) + c between a (st) (5)
",3.1 Efficiently computing expected counts,[0],[0]
"cwithina (st) =
{∑n i=1 cai(st)",3.1 Efficiently computing expected counts,[0],[0]
"if s 6= BOS, t 6=",3.1 Efficiently computing expected counts,[0],[0]
"EOS
0 otherwise
cacrossa (st) = n∑ i=0 n+1∑ j=1 pa(i, j)cai(s EOS)caj (BOS t)
",3.1 Efficiently computing expected counts,[0],[0]
"That is, ca inherits all non-boundary bigrams st that fall within its child constituents (via cwithina ).",3.1 Efficiently computing expected counts,[0],[0]
"It also counts bigrams st that cross the boundary between consecutive nodes (via cacrossa ), where nodes ai and aj are consecutive with probability pa(i, j).
",3.1 Efficiently computing expected counts,[0],[0]
"When computing ca via (5), we will have already computed ca1 , . . .",3.1 Efficiently computing expected counts,[0],[0]
", can−1 bottom-up.",3.1 Efficiently computing expected counts,[0],[0]
"As for the dummy nodes, an is realized by the length-1 string hwhere h is the head token of node a, while a0 and an+1 are each realized by the empty string.",3.1 Efficiently computing expected counts,[0],[0]
"Thus, can simply assigns count 1 to the bigrams BOS h and h EOS, and ca0 and can+1 each assign expected count 1 to BOS EOS.",3.1 Efficiently computing expected counts,[0],[0]
"(Notice that thus, cacrossa (st) counts ya’s boundary bigrams—the bigrams stwhere s = BOS or t = EOS—when i = 0 or j = n+ 1 respectively.)",3.1 Efficiently computing expected counts,[0],[0]
"The main challenge above is computing the node bigram probabilities pa(i, j).",3.2 Efficient enumeration over permutations,[0],[0]
"These are marginals of p(π | a) as defined by (1), which unfortunately is intractable to marginalize: there is no better way than enumerating all n! permutations.
",3.2 Efficient enumeration over permutations,[0],[0]
"That said, there is a particularly efficient way to enumerate the permutations.",3.2 Efficient enumeration over permutations,[0],[0]
"The SteinhausJohnson-Trotter (SJT) algorithm (Sedgewick, 1977) does so in O(1) time per permutation, obtaining each permutation by applying a single swap to the previous one.",3.2 Efficient enumeration over permutations,[0],[0]
Only the features that are affected by this swap need to be recomputed.,3.2 Efficient enumeration over permutations,[0],[0]
"For our features (Appendix A), this cuts the runtime per permutation from O(n2) to O(n).
",3.2 Efficient enumeration over permutations,[0],[0]
"Furthermore, the single swap of adjacent nodes only changes 3 bigrams (possibly including boundary bigrams).",3.2 Efficient enumeration over permutations,[0],[0]
"As a result, it is possible to
obtain the marginal probabilities with O(1) additional work per permutation.",3.2 Efficient enumeration over permutations,[0],[0]
"When a node bigram is destroyed, we increment its marginal probability by the total probability of permutations encountered since the node bigram was last created.",3.2 Efficient enumeration over permutations,[0],[0]
This can be found as a difference of partial sums.,3.2 Efficient enumeration over permutations,[0],[0]
"The final partial sum is the normalizing constant Z(a), which can be applied at the end.",3.2 Efficient enumeration over permutations,[0],[0]
"Pseudocode is given in supplementary material as Algorithm 2.
",3.2 Efficient enumeration over permutations,[0],[0]
"When we train the parameters θ (§2.4), we must back-propagate through the whole computation of equation (4), which depends on tag bigram counts ca(st), which depend via (5) on expected node bigram counts pa(i, j), which depend via Algorithm 2 on the permutation probabilities p(π | a), which depend via (1) on the feature weights θ.",3.2 Efficient enumeration over permutations,[0],[0]
"As a further speedup, we only train on trees with number of words < 40 and maxa na ≤ 5, so na!",4.1 Pruning high-degree trees,[0],[0]
≤,4.1 Pruning high-degree trees,[0],[0]
120.7 We then produce the synthetic treebank B′,4.1 Pruning high-degree trees,[0],[0]
(§2.3) by drawing a single realization of each tree in B for which maxa na ≤ 7.,4.1 Pruning high-degree trees,[0],[0]
"This requires sampling from up to 7! = 5040 candidates per node, again using SJT.8
That is, in this paper we run exact algorithms (§3), but only on a subset of B. The subset is not necessarily representative.",4.1 Pruning high-degree trees,[0],[0]
"An improvement would use importance sampling, with a proposal distribution that samples the slower trees less often during SGD but upweights them to compensate.",4.1 Pruning high-degree trees,[0],[0]
"§7 suggests a future strategy that would run on all trees in B via approximate, sampling-based algorithms.",4.1 Pruning high-degree trees,[0],[0]
The exact methods would remain useful for calibrating the approximation quality.,4.1 Pruning high-degree trees,[0],[0]
"To minimize (4), we use the Adam variant of SGD (Kingma and Ba, 2014), with learning rate 0.01 chosen by cross-validation (§5.1).
",4.2 Minibatch estimation of cp,[0],[0]
SGD requires a stochastic estimate of the gradient of the training objective.,4.2 Minibatch estimation of cp,[0],[0]
"Ordinarily this is done by replacing an expectation over the entire training set with an expectation over a minibatch.
",4.2 Minibatch estimation of cp,[0],[0]
"7We found that this threshold worked much better than ≤ 4 and about as well as the much slower ≤ 6.
8This pruning heuristic retains 36.1% of the trees (averaging over the 20 development treebanks (§5.1)) for training, and 66.6% for actual realization.",4.2 Minibatch estimation of cp,[0],[0]
"The latter restriction follows Wang and Eisner (2016, §4.2): they too discarded trees with nodes having na ≥ 8.
",4.2 Minibatch estimation of cp,[0],[0]
Equation (2) with p = p̂θ is indeed an expectation over sentences of B. It can be stochastically estimated as (3) where cp gives the expected bigram counts averaged over only the sentences in a minibatch of B. These are found using §3’s algorithms with the current θ.,4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately, the term log p(t | s) depends on bigram counts that should be derived from the entire corpus B in the same way.",4.2 Minibatch estimation of cp,[0],[0]
Our solution is to simply reuse the minibatch estimate of cp for the latter counts.,4.2 Minibatch estimation of cp,[0],[0]
"We use a large minibatch of 500 sentences from B so that this drop-in estimate does not introduce too much bias into the stochastic gradient: after all, we only need to estimate bigram statistics on 17 POS types.9
By contrast, the cq values that are used for the expectation in the second term of (4) and in log q(t | s) do not change during optimization, so we simply compute them once from all of u.",4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately the objective (4) is not convex, so the optimizer is sensitive to initialization (see §5.3 below for empirical discussion).",4.3 Informed initialization,[0],[0]
Initializing θ = 0,4.3 Informed initialization,[0],[0]
(so that p(π | a) is uniform),4.3 Informed initialization,[0],[0]
gave poor results in pilot experiments.,4.3 Informed initialization,[0],[0]
"Instead, we initially choose θ to be the realization parameters of the source language, as estimated from the source",4.3 Informed initialization,[0],[0]
treebank B.,4.3 Informed initialization,[0],[0]
"This is at least a linguistically realistic θ, although it may not be close to the target language.10
For this initial estimation, we follow Wang and Eisner (2016) and perform supervised training on B of the log-linear realization model (1), by maximizing the conditional log-likelihood of B, namely ∑ (x,t)∈B log pθ(t | x), where (x, t) are an unordered tree and its observed ordering in B. This initial objective is convex.11",4.3 Informed initialization,[0],[0]
We performed a large-scale experiment requiring hundreds of thousands of CPU-hours.,5 Experiments,[0],[0]
"To our knowledge, this is the largest study of parsing transfer yet attempted.
",5 Experiments,[0],[0]
"9We also used the minibatch to estimate the average sentence length Ey∼p[ |y| ] in (4), although here we could have simply used all of B since this value does not change.
",5 Experiments,[0],[0]
"10As an improvement, one could also try initial realization parameters for B that are estimated from treebanks of other languages.",5 Experiments,[0],[0]
"Concretely, the optimizer could start by selecting a “galactic” treebank from Wang and Eisner (2016) that is already close to the target language, according to (4), and try to make it even closer.",5 Experiments,[0],[0]
"We leave this to future work.
",5 Experiments,[0],[0]
"11Unfortunately, we did not regularize it, which probably resulted in initializing some parameters too close to ±∞ for the optimizer to change them meaningfully.",5 Experiments,[0],[0]
"As our main dataset, we use Universal Dependencies version 1.2 (Nivre et al., 2015)—a set of 37 dependency treebanks for 33 languages, with a unified POS-tag set and relation label set.
",5.1 Data and setup,[0],[0]
Our evaluation metric was unnormalized attachment score (UAS) when parsing a target treebank with a parser trained on a (possibly permuted) source treebank.,5.1 Data and setup,[0],[0]
"For both evaluation and training, we used only the training portion of each treebank.
",5.1 Data and setup,[0],[0]
"Our parser was Yara (Rasooli and Tetreault, 2015), a fast and accurate transition-based dependency parser that can be rapidly retrained.",5.1 Data and setup,[0],[0]
We modified Yara to ignore the input words and use only the input gold POS tags (see §1.3).,5.1 Data and setup,[0],[0]
"To train the Yara parser on a (possibly permuted) source treebank, we first train on 80% of the trees and use the remaining 20% to tune Yara’s hyperparameters.",5.1 Data and setup,[0],[0]
"We then retrain Yara on 100% of the source trees and evaluate it on the target treebank.
Similar to Wang and Eisner (2017), we use 20 treebanks (18 distinct languages) as development data, and hold out the remaining 17 treebanks for the final evaluation.",5.1 Data and setup,[0],[0]
"We chose the hyperparameters (α1, α2, β) of (4) to maximize the target-language UAS, averaged over all 376 transfer experiments where the source and target treebanks were development treebanks of different languages.12 (See Appendix C for details.)
",5.1 Data and setup,[0],[0]
The next few sections perform some exploratory analysis on these 376 experiments.,5.1 Data and setup,[0],[0]
"Then, for the final test in §5.4, we will evaluate UAS on all 337 transfer experiments where the source is a development treebank and the target is a test treebank of a different language.13",5.1 Data and setup,[0],[0]
We have assumed that a smaller divergence between source and target treebanks results in better transfer parsing accuracy.,5.2 Exploratory analysis,[0],[0]
"Figure 1 shows that these quantities are indeed correlated, both for the original source treebanks and for their “made to order” permuted versions.
",5.2 Exploratory analysis,[0],[0]
"12We have 19*20=380 pairs in total, minus the four excluded pairs (grc, grc proiel), (grc proiel, grc), (la proiel, la itt) and (la itt, la proiel).",5.2 Exploratory analysis,[0],[0]
"Unlike Wang and Eisner (2017), we exclude duplicated languages in development and testing.
",5.2 Exploratory analysis,[0],[0]
"13Specifically, there are 3 duplicated sets: {grc, grc proiel}, {la, la proiel, la itt}, and {fi, fi ftb}.",5.2 Exploratory analysis,[0],[0]
"Whenever one treebank is used as the target language, we exclude the other treebanks in the same set.
",5.2 Exploratory analysis,[0],[0]
"15According to the family (and sub-family) information at http://universaldependencies.org.
",5.2 Exploratory analysis,[0],[0]
"Thus, we hope that the optimizer will find a systematic permutation that reduces the divergence.",5.2 Exploratory analysis,[0],[0]
Does it?,5.2 Exploratory analysis,[0],[0]
"Yes: Figures 5 and 6 in the supplementary material show that the optimizer almost always manages to reduce the objective on training data, as expected.
",5.2 Exploratory analysis,[0],[0]
"One concern is that our divergence metric might misguide us into producing dysfunctional languages whose trees cannot be easily recovered from their surface strings, i.e., they have no good parser.",5.2 Exploratory analysis,[0],[0]
"In such a language, the word order might be extremely free (e.g., θ = 0), or common constructions might be syntactically ambiguous.",5.2 Exploratory analysis,[0],[0]
"Fortunately, Appendix D shows that our synthetic languages appear natural with respect to their their parsability.
",5.2 Exploratory analysis,[0],[0]
The above findings are promising.,5.2 Exploratory analysis,[0],[0]
So does permuting the source language in fact result in better transfer parsing of the target language?,5.2 Exploratory analysis,[0],[0]
"We experiment on the 376 development pairs.
",5.2 Exploratory analysis,[0],[0]
"The solid lines in Figure 2 show our improvements on the dev data, with a simpler scatterplot given by in Figure 7 in the supplementary material.",5.2 Exploratory analysis,[0],[0]
The upshot is that the synthetic source treebanks yield a transfer UAS of 52.92 on average.,5.2 Exploratory analysis,[0],[0]
This is not yet a result on held-out test data: recall that 52.92 was the best transfer UAS achieved by any hyperparameter setting.,5.2 Exploratory analysis,[0],[0]
"That said, it is 1.00 points better than transferring from the original source treebanks, a significant difference (paired permutation test by language pair, p < 0.01).
",5.2 Exploratory analysis,[0],[0]
"Figure 2 shows that this average improvement is mainly due to the many cases where the source and target languages come from different families.
",5.2 Exploratory analysis,[0],[0]
Permutation tends to improve source languages that were doing badly to start with.,5.2 Exploratory analysis,[0],[0]
"However, it tends to hurt a source language that is already in the target language family.
",5.2 Exploratory analysis,[0],[0]
A hypothetical experiment shows that permuting the source does have good potential to help (or at least not hurt) in both cases.,5.2 Exploratory analysis,[0],[0]
"The dashed lines in Figure 2—and the scatterplot in Figure 8— show the potential of the method, by showing the improvement we would get from permuting each source treebank using an “oracle” realization policy—the supervised realization parameters θ that are estimated from the actual target treebank.",5.2 Exploratory analysis,[0],[0]
"The usefulness of this oracle-permuted source varies depending on the source language, but it is usually much better than the automaticallypermuted version of the same source.
",5.2 Exploratory analysis,[0],[0]
This shows that large improvements would be possible if we could only find the best permutation policy allowed by our model family.,5.2 Exploratory analysis,[0],[0]
"The question for future work is whether such gains can be achieved by a more sensitive permutation model than (1), a better divergence objective than (4), or a better search algorithm than §4.2.",5.2 Exploratory analysis,[0],[0]
"Identifying the best available source treebank, or the best mixture of all source treebanks, would also help greatly.",5.2 Exploratory analysis,[0],[0]
Figure 2 makes clear that performance of the synthetic source treebanks is strongly correlated with that of their original versions.,5.3 Sensitivity to initializer,[0],[0]
Most points in Figure 7 lie near the diagonal (Kendall’s τ = 0.85).,5.3 Sensitivity to initializer,[0],[0]
"Even with oracle permutation in Figure 8, the correlation remains strong (τ = 0.59), suggesting that the choice of source treebank is important even beyond its effect on search initialization.
",5.3 Sensitivity to initializer,[0],[0]
"We suspected that when “made to order” source treebanks (more than the oracle versions) have performance close to their original versions, this is in part because the optimizer can get stuck near the initializer (§4.3).",5.3 Sensitivity to initializer,[0],[0]
"To examine this, we experimented with random restarts, as follows.",5.3 Sensitivity to initializer,[0],[0]
"In addition to informed initialization (§4.3), we optimized from 5 other starting points θ ∼ N (0, I).",5.3 Sensitivity to initializer,[0],[0]
"From these 6 runs, we selected the final parameters that achieved the best divergence (4).",5.3 Sensitivity to initializer,[0],[0]
"As shown by
Figure 9 in the supplement, greater gains appear to be possible with more aggressive search methods of this sort, which we leave to future work.",5.3 Sensitivity to initializer,[0],[0]
"We could also try non-random restarts based on the realization parameters of other languages, as suggested in footnote 10.",5.3 Sensitivity to initializer,[0],[0]
"For our final evaluation (§5.1), we use the same hyperparameters (Appendix C) and report on single-source transfer to the 17 held-out treebanks.
",5.4 Final evaluation on the test languages,[0],[0]
The development results hold up in Figure 3.,5.4 Final evaluation on the test languages,[0],[0]
"Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p < 0.01).
",5.4 Final evaluation on the test languages,[0],[0]
"In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer.",5.4 Final evaluation on the test languages,[0],[0]
"Unsupervised parsing has remained challenging for decades (Mareček, 2016).",6.1 Unsupervised parsing,[0],[0]
"Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse.",6.1 Unsupervised parsing,[0],[0]
Some such approaches try to improve the grammar model.,6.1 Unsupervised parsing,[0],[0]
"For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012).",6.1 Unsupervised parsing,[0],[0]
"Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareček and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013).
",6.1 Unsupervised parsing,[0],[0]
"The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences.",6.1 Unsupervised parsing,[0],[0]
McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s).,6.1 Unsupervised parsing,[0],[0]
"Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013) features (Naseem et al., 2012; Täckström",6.1 Unsupervised parsing,[0],[0]
"et al., 2013;
Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2).",6.1 Unsupervised parsing,[0],[0]
Our novel proposal ties into the recent interest in data augmentation in supervised machine learning.,6.2 Synthetic data generation,[0],[0]
"In unsupervised parsing, the most widely
adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation.",6.2 Synthetic data generation,[0],[0]
"Of course, this requires bilingual corpora as an additional resource.",6.2 Synthetic data generation,[0],[0]
"Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014).",6.2 Synthetic data generation,[0],[0]
"Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agić et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016).
",6.2 Synthetic data generation,[0],[0]
"On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages.",6.2 Synthetic data generation,[0],[0]
"They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages.",6.2 Synthetic data generation,[0],[0]
"Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language.",6.2 Synthetic data generation,[0],[0]
"Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016), typology
prediction (Wang and Eisner, 2017), and parsing unknown languages (Wang and Eisner, 2018).
",6.2 Synthetic data generation,[0],[0]
Our work in this paper differs in that our synthetic treebanks are “made to order.”,6.2 Synthetic data generation,[0],[0]
"Rather than combine aspects of different treebanks and hope to get at least one combination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source.
",6.2 Synthetic data generation,[0],[0]
"Beyond unsupervised parsing, synthetic data has been used for several other tasks.",6.2 Synthetic data generation,[0],[0]
"In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them.",6.2 Synthetic data generation,[0],[0]
"In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order.",6.2 Synthetic data generation,[0],[0]
We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams).,7 Conclusion & Future Work,[0],[0]
The code is available at https://github. com/wddabc/ordersynthetic.,7 Conclusion & Future Work,[0],[0]
"Our work is grounded in the notion that by trying to explain the POS bigram counts in a target corpus, we can discover a stochastic realization policy for the target language, which correctly “translates” the source trees into appropriate target trees.
",7 Conclusion & Future Work,[0],[0]
"We formulated an objective for evaluating such a policy, based on KL-divergence between bigram models.",7 Conclusion & Future Work,[0],[0]
"We showed that the objective could be computed efficiently by dynamic programming, thanks to the limitation to bigram statistics.
",7 Conclusion & Future Work,[0],[0]
"Experimenting on the Universal Dependencies treebanks v1.2, we showed that the synthetic treebanks were—on average—modestly but significantly better than the corresponding real treebanks for single-source transfer (and in Appendix E, on multi-source transfer).
",7 Conclusion & Future Work,[0],[0]
"On the downside, Figure 7 shows that with our current method, permuting the source language to be more like the target language is helpful (on average) only when the source language is from a different language family.",7 Conclusion & Future Work,[0],[0]
"This contrast would be
even more striking if we had a better optimizer:
Figure 9 shows that SGD’s initialization bias limits permutation’s benefit for cross-family training, as well as its harm for within-family training.
",7 Conclusion & Future Work,[0],[0]
Several opportunities for future work have already been mentioned throughout the paper.,7 Conclusion & Future Work,[0],[0]
"We are also interested in experimenting with richer families of permutation distributions, as well as “conservative” distributions that tend to prefer the original source order.",7 Conclusion & Future Work,[0],[0]
"We could use entropy regularization (Grandvalet and Bengio, 2005) to encourage more “deterministic” patterns of realization in the synthetic languages.
",7 Conclusion & Future Work,[0],[0]
"We would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (RNNLMs) for q̂ and p̂θ.",7 Conclusion & Future Work,[0],[0]
"This means abandoning our exact dynamic programming methods; we would also like to abandon exact exhaustive enumeration in order to drop §4.1’s bounds on n. Fortunately, there exist powerful MCMC methods (Eisner and Tromble, 2006) that can sample from interesting distributions over the space of n!",7 Conclusion & Future Work,[0],[0]
"permutations, even for large n. Thus, we could approximately sample from pθ by drawing permuted versions of each tree in B.
Given this change, a very interesting direction would be to graduate from POS language models to word language models, using cross-lingual unsupervised word embeddings (Ruder et al., 2017).",7 Conclusion & Future Work,[0],[0]
This would eliminate the need for the gold POS tags that we unrealistically assumed in this paper (which are typically unavailable for a low-resource target language).,7 Conclusion & Future Work,[0],[0]
"Furthermore, it would enable us to harness richer lexical information beyond the 17 UD POS tags.",7 Conclusion & Future Work,[0],[0]
"After all, even a (gold) POS corpus might not be sufficient to determine the word order of the target language: “NOUN VERB NOUN” could be either subject-verb-object or object-verbsubject.",7 Conclusion & Future Work,[0],[0]
"However, “water drink boy” is presumably object-verb-subject.",7 Conclusion & Future Work,[0],[0]
"Thus, using crosslingual embeddings, we would try to realize the unordered source trees so that their word strings, with few edits, can achieve high probability under a neural language model of the target.",7 Conclusion & Future Work,[0],[0]
This work was supported by National Science Foundation Grants 1423276 & 1718846.,Acknowledgements,[0],[0]
"We are grateful to the state of Maryland for the Maryland Advanced Research Computing Center, a crucial resource.",Acknowledgements,[0],[0]
"We thank Shijie Wu and Adithya Renduchintala for early discussion, Argo lab members for further discussion, and the 3 reviewers for quality comments.",Acknowledgements,[0],[0]
"To approximately parse an unfamiliar language, it helps to have a treebank of a similar language.",abstractText,[0],[0]
But what if the closest available treebank still has the wrong word order?,abstractText,[0],[0]
We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language.,abstractText,[0],[0]
The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum).,abstractText,[0],[0]
This optimization procedure yields trees for a new artificial language that resembles the target language.,abstractText,[0],[0]
We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.,abstractText,[0],[0]
Synthetic Data Made to Order: The Case of Parsing,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–757 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
752",text,[0],[0]
"Story comprehension has been one of the longestrunning ambitions in artificial intelligence (Dijk, 1980; Charniak, 1972).",1 Introduction,[0],[0]
One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested.,1 Introduction,[0],[0]
"Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address
*",1 Introduction,[0],[0]
"This work was performed at University of Rochester.
this issue.",1 Introduction,[0],[0]
"This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’",1 Introduction,[0],[0]
"Then, the system’s task is to choose the right ending.",1 Introduction,[0],[0]
"In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).",1 Introduction,[0],[0]
"Each story follows a character through a fairly simple series of events to a conclusion.
",1 Introduction,[0],[0]
"Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.",1 Introduction,[0],[0]
"A variety of new systems were proposed (Mihaylov and Frank, 2017; Schenk and Chiarcos, 2017; Schwartz et al., 2017b; Roemmele et al., 2017) as a part of the first shared task on SCT at LSDSem’17 workshop (Mostafazadeh et al., 2017).",1 Introduction,[0],[0]
"Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.",1 Introduction,[0],[0]
"Clearly, this success does not seem to reflect the intent of the original task, where the systems should leverage narrative understanding as opposed to the statistical biases in the data.",1 Introduction,[0],[0]
"In this paper, we study the effect of such biases between the ending choices and present a new scheme to reduce such stylistic artifacts.
",1 Introduction,[0],[0]
"The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety
of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).",1 Introduction,[0],[0]
"The results show that the topperforming SCT system on the the leaderboard1 (Chaturvedi et al., 2017) fails to keep up the performance on our new dataset.",1 Introduction,[0],[0]
We discuss the implications of this experiment to the greater research community in terms of data collection and benchmarking practices in Section 6.,1 Introduction,[0],[0]
All the code and datasets for this paper will be released to the public.,1 Introduction,[0],[0]
We hope that the availability of the new evaluation set can further support the continued research on story understanding.,1 Introduction,[0],[0]
"This paper mainly extends the work on creating the Story Cloze Test set (Mostafazadeh et al., 2016), hereinafter SCT-v1.0.",2 Related Work,[0],[0]
"The SCT-v1.0 dataset was created as follows: full five-sentence stories from the ROC Stories corpus were sampled, then, the initial four sentences were shown to a set of MTurk2 crowd workers who were prompted to author ‘right’ and ‘wrong’ endings.",2 Related Work,[0],[0]
"Mostafazadeh et al. (Mostafazadeh et al., 2016) give special care to make sure there were no boundary cases for ‘right’ and ‘wrong’ endings by implementing extra rounds of data filtering.",2 Related Work,[0],[0]
"The resulting SCT-v1.0 dataset had a validation (hereinafter, SCT-v1.0 Val) and a test set (SCT-v1.0 test), each with 1,871 cases.",2 Related Work,[0],[0]
Table 1 shows two example story cloze test cases from SCT-v1.0 corpus.,2 Related Work,[0],[0]
"As for positive training data, they had provided a collection of 100K five sentence stories.",2 Related Work,[0],[0]
"Human performance is reported to be 100% on SCT-v1.0.
",2 Related Work,[0],[0]
"Mostafazadeh et al. (2016) provide a variety of baseline models for SCT-v1.0, with the best model performing with an accuracy of 59%.",2 Related Work,[0],[0]
"The first
1As of 15th February 2018.",2 Related Work,[0],[0]
"2http://mturk.com
shared task on SCT-v1.0 was conducted at the LSDSem’17 workshop (Mostafazadeh et al., 2017), where most of the models performed with 60- 70% accuracy.",2 Related Work,[0],[0]
"One of the top-performing models, msap (Schwartz et al., 2017b,a), built a classifier using linguistic features that have been previously useful in authorship style detection, using only the ending sentences.",2 Related Work,[0],[0]
"They used stylistic features such as sentence length, word, and character level n-grams for each ending (fully discarding the context), achieving an accuracy of 72%.",2 Related Work,[0],[0]
"In conjunction with their work, Cai et al., (Cai et al., 2017) reported similar observations separately, exposing that features such as sentiment, negation, and length are different between the right and wrong endings.",2 Related Work,[0],[0]
"The best model on SCT-v1.0 to this date is cogcomp, which is a linear model that uses event sequences, sentiment trajectory, and topical consistency as features, and performs with an accuracy of 77.6%.
",2 Related Work,[0],[0]
This paper takes all their analysis further and introduces a model aggregating all the pinpointed features to shed more light into the stylistic biases isolated in SCT-v1.0 endings.,2 Related Work,[0],[0]
"Despite all the efforts made in the original SCT paper, there was never an extensive analysis of the features isolated in the endings of the stories.",3 Stylistic Feature Analysis,[0],[0]
"We explored the differences among stylistic features such as word-token count, sentiment, and the sentence complexity between the endings, to determine a composite score for identifying sources of bias.",3 Stylistic Feature Analysis,[0],[0]
"For determining the sentiment, we used Stanford CoreNLP",3 Stylistic Feature Analysis,[0],[0]
"(Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).",3 Stylistic Feature Analysis,[0],[0]
"For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).",3 Stylistic Feature Analysis,[0],[0]
Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.,3 Stylistic Feature Analysis,[0],[0]
"The feature distribution plots can be found in the supplementary material.
",3 Stylistic Feature Analysis,[0],[0]
"Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b))",3 Stylistic Feature Analysis,[0],[0]
as features.,3 Stylistic Feature Analysis,[0],[0]
"We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation.",3 Stylistic Feature Analysis,[0],[0]
"In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.",3 Stylistic Feature Analysis,[0],[0]
"Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one.",3 Stylistic Feature Analysis,[0],[0]
"An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material.",3 Stylistic Feature Analysis,[0],[0]
"Following the analysis above, we developed a Story Cloze model, hereinafter EndingReg, that only uses the ending features while disregarding the story context for choosing the right ending.",4 Model,[0],[0]
We expanded each Story Cloze Test case’s ending options into a set of two single sentences.,4 Model,[0],[0]
"Then, for each sentence, we created the following features:
1.",4 Model,[0],[0]
Number of tokens 2.,4 Model,[0],[0]
VADER composite sentiment score 3.,4 Model,[0],[0]
Yngve complexity score 4.,4 Model,[0],[0]
Token-POS n-grams 5.,4 Model,[0],[0]
POS n,4 Model,[0],[0]
-grams 6.,4 Model,[0],[0]
"Four length character-grams
All n-gram features needed to appear at least five times throughout the dataset.",4 Model,[0],[0]
The features were collected for each five-sentence story and then fed into a logistic regression classifier.,4 Model,[0],[0]
"As an initial experiment, we trained this model using the SCTv1.0 validation set and tested on the SCT-v1.0 test set.",4 Model,[0],[0]
"An L2 regularization penalty was used to enforce a Gaussian prior on the feature-space, where a grid search was conducted for hyper-parameter tuning.",4 Model,[0],[0]
This model achieves an accuracy of 71.5% on the SCT-v1.0 dataset which is on par with the highest score achieved by any model using only the endings.,4 Model,[0],[0]
"Table 3 shows the accuracies ob-
tained by models using only those particular features.",4 Model,[0],[0]
"We achieve minimal but sometimes important classification using token count, VADER, and Yngve in combination alone, better classification using POS or char-grams alone, and best classification using n-grams alone.",4 Model,[0],[0]
By combining all of them we achieve the overall best results.,4 Model,[0],[0]
"Based on the findings above, a new test set for the SCT was deemed necessary.",5 Data Collection,[0],[0]
"The premise of predicting an ending to a short story, as opposed to predicting say a middle sentence, enables a more systematic evaluation where human can agree on the cases 100%.",5 Data Collection,[0],[0]
"Hence, our goal was to come up with a data collection scheme that overcomes the data collection biases, while keeping the original evaluation format.",5 Data Collection,[0],[0]
"As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.",5 Data Collection,[0],[0]
We set the following goals for sourcing the new ‘right’ and ‘wrong’ endings.,5 Data Collection,[0],[0]
"They both should:
1.",5 Data Collection,[0],[0]
Contain a similar number of tokens 2.,5 Data Collection,[0],[0]
"Have similar distributions of token n-grams
and char-grams 3.",5 Data Collection,[0],[0]
"Occur as standalone events with the same
likelihood to occur, with topical, sentimental, or emotion consistencies when applicable.
",5 Data Collection,[0],[0]
"First, we crowdsourced 5,000 new five-sentence stories through Amazon Mechanical Turk.",5 Data Collection,[0],[0]
We prompted the users in the same manner described in Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"In order to source new ‘wrong’ endings, we tried two different methods.",5 Data Collection,[0],[0]
"In Method #1, we kept the original ending sourcing format of Mostafazadeh et al., but imposed some further restrictions.",5 Data Collection,[0],[0]
"This was done
by taking the first four sentences of the newly collected stories and asking an MTurker to write a ‘right’ and ‘wrong’ ending for each.",5 Data Collection,[0],[0]
"The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’",5 Data Collection,[0],[0]
"The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.
",5 Data Collection,[0],[0]
"In Method #2, we took the five sentences stories and prompted a second set of MTurk workers to modify the fifth sentence in order to make a resulting five-sentence story non-sensible.",5 Data Collection,[0],[0]
"Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.",5 Data Collection,[0],[0]
"As a result, the workers had much less flexibility for changing the underlying linguistic structures which can help tackle the authorship style differences between the ‘right’ and ‘wrong’ endings.
",5 Data Collection,[0],[0]
"The results in Table 4, which show classification accuracy when using EndingReg on the two new data sources, show that Method #2 is a slightly better data sourcing scheme in reducing the bias, since the EndingReg model’s performance is slightly worse.",5 Data Collection,[0],[0]
The set was further filtered through human verification similar to Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.",5 Data Collection,[0],[0]
Stories were only selected if all the three MTurk users verified that the story with the ‘right ending’ and the corresponding story with the ‘wrong ending’ were verified to be indeed right and wrong respectively.,5 Data Collection,[0],[0]
This ensured a higher quality of data and eliminating boundary cases.,5 Data Collection,[0],[0]
"This entire process resulted in creating the Story Cloze Test v1.5 (SCT-v1.5) dataset, consisting of 1,571 stories for each validation and test sets.",5 Data Collection,[0],[0]
"In order to test the decrease in n-gram bias, which was the most salient feature for the classification task using only the endings, we compare the variance between the n-gram counts from SCT-v1.0 to SCT-v1.5.",6 Results,[0],[0]
"The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.",6 Results,[0],[0]
Table 6 shows the classification results of various models on SCT-v1.5.,6 Results,[0],[0]
"The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.
",6 Results,[0],[0]
"Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.",6 Results,[0],[0]
"Furthermore, the CogComp model’s performance has significantly dropped on SCT-v1.5.",6 Results,[0],[0]
"Although this model seems to be using story comprehension features such as event sequencing, since the endings are included in the sequences, the biases within the endings have influenced the predictions and the weak performance of the model in SCT-v1.5 suggest that this model had picked up on the biases of SCT-v1.0 as opposed to really understanding the context.",6 Results,[0],[0]
"In particular, the posterior probabilities for each ending choice using their features are quite similar on the SCT-v1.5.",6 Results,[0],[0]
"These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.",6 Results,[0],[0]
"Al-
though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.
",6 Results,[0],[0]
It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.,6 Results,[0],[0]
"This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.",6 Results,[0],[0]
"This showcases the importance of maintaining benchmarks that evolve and improve over time, where systems should not be optimized for particular narrow test sets.",6 Results,[0],[0]
"We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.",6 Results,[0],[0]
"In this paper, we presented a comprehensive analysis of the stylistic features isolated in the endings of the original Story Cloze Test (SCT-v1.0).",7 Conclusion,[0],[0]
"Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.",7 Conclusion,[0],[0]
"Based on the results presented in this paper, we believe that our SCT-v1.5 is a better benchmark for story comprehension.",7 Conclusion,[0],[0]
"However, as shown in multiple AI tasks (Ettinger et al., 2017; Antol et al., 2015; Jabri et al., 2016; Poliak et al., 2018), no collected dataset is entirely without its inherent biases and often the biases in datasets go undiscovered.",7 Conclusion,[0],[0]
We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.,7 Conclusion,[0],[0]
"All the new versions, along with a leader-board showcasing the stateof-the-art results, will be tracked via CodaLab
https://competitions.codalab.org/ competitions/15333.
",7 Conclusion,[0],[0]
The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.,7 Conclusion,[0],[0]
"We suggest the next SCT challenges to be completely blind, where the participants cannot deliberately leverage any particular data biases.",7 Conclusion,[0],[0]
"Along with this paper, we are releasing the datasets and the developed models to the community.",7 Conclusion,[0],[0]
"All the announcements, new supplementary material, and datasets can be accessed through http://cs.",7 Conclusion,[0],[0]
rochester.edu/nlp/rocstories/.,7 Conclusion,[0],[0]
We hope that this work ignites further interest in the community for making progress on story understanding.,7 Conclusion,[0],[0]
We would like to thank Roy Schwartz for his valuable feedback regarding some of the experiments.,Acknowledgement,[0],[0]
"We also thank the amazing crowd workers, without the work of whom this work would have been impossible.",Acknowledgement,[0],[0]
This work was supported in part by grant W911NF15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA) as a part of the Communicating with Computers (CwC) program.,Acknowledgement,[0],[0]
The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning.,abstractText,[0],[0]
There have been a variety of models tackling the SCT so far.,abstractText,[0],[0]
"Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset.",abstractText,[0],[0]
"In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task.",abstractText,[0],[0]
"Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases.",abstractText,[0],[0]
We benchmark a few models on the new dataset and show that the topperforming model on the original SCT dataset fails to keep up its performance.,abstractText,[0],[0]
Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.,abstractText,[0],[0]
Tackling the Story Ending Biases in The Story Cloze Test,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2026–2031, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Online discussion forums are a popular platform for people to share their views about current events and learn about issues of concern to them.,1 Introduction,[0],[0]
"Discussion forums tend to specialize on different topics, and people participating in them form communities of interest.",1 Introduction,[0],[0]
The reaction of people within a community to comments posted provides an indication of community endorsement of opinions and value of information.,1 Introduction,[0],[0]
"In most discussions, the vast majority of comments spawn little reaction.",1 Introduction,[0],[0]
"In this paper, we look at whether (and how) language use affects the reaction, compared to the relative importance of the author and timing of the post.
",1 Introduction,[0],[0]
"Early work on factors that appear to influence crowd-based judgments of comments in the Slashdot forum (Lampe and Resnick, 2004) indicate that timing, starting score, length of the comment, and poster anonymity/reputation appear to play a role (where anonymity has a negative effect).",1 Introduction,[0],[0]
"Judging by differences in popularity of various discussion forums, topic is clearly important.",1 Introduction,[0],[0]
"Evidence that language use also matters is provided by recent work (Danescu-Niculescu-Mizil et al., 2012; Lakkaraju et al., 2013; Althoff et al., 2014;
Tan et al., 2014).",1 Introduction,[0],[0]
"Teasing these different factors apart, however, is a challenge.",1 Introduction,[0],[0]
The work presented in this paper provides additional insight into this question by controlling for these factors in a different way than previous work and by examining multiple communities of interest.,1 Introduction,[0],[0]
"Specifically, using data from Reddit discussion forums, we look at the role of author reputation as measured in terms of a karma k-index, and control for topic and timing by ranking comments in a constrained window within a discussion.
",1 Introduction,[0],[0]
"The primary contributions of this work include findings about the role of author reputation and variation across communities in terms of aspects of language use that matter, as well as the problem formulation, associated data collection, and development of a variety of features for characterizing informativeness, community response, relevance and mood.",1 Introduction,[0],[0]
"Reddit1 is the largest public online discussion forum with a wide variety of subreddits, which makes it a good data source for studying how textual content in a discussion impacts the response of the crowd.",2 Data,[0],[0]
"On Reddit, people initiate a discussion thread with a post (a question, a link to a news item, etc.), and others respond with comments.",2 Data,[0],[0]
Registered users vote on which posts and comments are important.,2 Data,[0],[0]
"The total amount of up votes minus the down votes (roughly) is called karma; it provides an indication of community endorsement and popularity of a comment, as used in (Lakkaraju et al., 2013).",2 Data,[0],[0]
"Karma is valued as it impacts the order in which the posts or comments are displayed, with the high karma content rising to the top.",2 Data,[0],[0]
"Karma points are also accumulated by members of the discussion forum as a function of the karma associated with their comments.
",2 Data,[0],[0]
"1http://www.reddit.com
2026
The Reddit data is highly skewed.",2 Data,[0],[0]
"Although there are thousands of active communities, only a handful of them are large.",2 Data,[0],[0]
"Similarly, out of the more than a million comments made per day2, most of them receive little to no attention; the distributions of positive comment karma and author karma are Zipfian.",2 Data,[0],[0]
"Slightly more than half of all comments have exactly one karma point (no votes beyond the author), and only 5% of comments have less than one karma point.
",2 Data,[0],[0]
"For this study, we downloaded all the posts and associated comments made to six subreddits over a few weeks, as summarized in Table 1, as well as karma of participants in the discussion3.",2 Data,[0],[0]
All available comments on each post were downloaded at least 48 hours after the post was made.4,2 Data,[0],[0]
"Factors other than the language use that influence whether a comment will have uptake from the community include the topic, the timing of the message, and the messenger.",3 Uptake Factors,[0],[0]
These factors are all evident in the Reddit discussions.,3 Uptake Factors,[0],[0]
"Some subreddits are more popular and thus have higher karma comments than others, reflecting the influence of topic.",3 Uptake Factors,[0],[0]
"Comments that are posted early in the discussion are more likely to have high karma, since they have more potential responses.
",3 Uptake Factors,[0],[0]
"Previous studies on Twitter show that the reputation of the author substantially increases the chances of the retweet (Suh et al., 2010; Cha et al., 2010), and reputation is also raised as a factor in Slashdot (Lampe and Resnick, 2004).",3 Uptake Factors,[0],[0]
"On Reddit most users are anonymous, but it is possible that members of a forum become familiar with particular usernames associated with high karma comments.",3 Uptake Factors,[0],[0]
"In order to see how important per-
2http://www.redditblog.com/2014/12/reddit-in-2014.html 3Our data collection is available online at https://ssli.ee.washington.edu/tial/data/reddit 4Based on our initial look at the data, we noticed that most posts receive all of their comments within 48 hours.",3 Uptake Factors,[0],[0]
"Some comments are deleted before we are able to download them.
",3 Uptake Factors,[0],[0]
"sonal reputation is, we looked at how often the top karma comments are associated with the top karma participants in the discussion.",3 Uptake Factors,[0],[0]
"Since an individual’s karma can be skewed by a few very popular posts, we measure reputation instead using a measure we call the k-index, defined to be equal to the number of comments in each user’s history that have karma ≥ k.",3 Uptake Factors,[0],[0]
"The k-index is analgous to the h-index (Hirsch, 2005) and arguably a better indicator of extended impact than total karma.
",3 Uptake Factors,[0],[0]
The results in Table 2 address the question of whether the top karma comments always come from the top karma person.,3 Uptake Factors,[0],[0]
The Top1 column shows the percentage of threads where the top karma comment in a discussion happens to be made by the highest k-index person participating in the discussion; the next column shows the percentage of threads where the comment comes from any one of the top 3 k-index people.,3 Uptake Factors,[0],[0]
"We find that, in fact, the highest karma comment in a discussion is rarely from the highest k-index people.",3 Uptake Factors,[0],[0]
"The highest percentage is in ASKSCIENCE, where expertise is more highly valued.",3 Uptake Factors,[0],[0]
"If we consider whether any one of the multiple comments that the top k-index person made is the top karma comment in the discussion, then the frequency is even lower.",3 Uptake Factors,[0],[0]
"Having shown that the reputation of the author of a post is not a dominating factor in predicting high karma comments, we propose to control for topic and timing by ranking a set of 10 comments that were made consecutively in a short window of time within one discussion thread according to the karma they finally received.",4.1 Tasks,[0],[0]
The ranking has access to the comment history about these posts.,4.1 Tasks,[0],[0]
"This simulates the view of an early reader of these posts, i.e., without influence of the ratings of oth-
ers, so that the language content of the post is more likely to have an impact.",4.1 Tasks,[0],[0]
"Very long threads are sampled, so that these do not dominate the set of lists.",4.1 Tasks,[0],[0]
"Approximately 75% of the comment lists are designated for training and the rest is for testing, with splits at the discussion thread level.",4.1 Tasks,[0],[0]
"Here, feature selection is based on mean precision of the top-ranked comment (P@1), so as to emphasize learning the rare high karma events.",4.1 Tasks,[0],[0]
(Note that P@1 is equivalent to accuracy but allows for any top-ranking comment to count as correct in the case of ties.),4.1 Tasks,[0],[0]
"The system performance is evaluated using both P@1 and normalized discounted cumulative gain (NDCG) (Burges et al., 2005), which is a standard criterion for ranking evaluation when the samples to be ranked have meaningful differences in scores, as is the case for karma of the comments.
",4.1 Tasks,[0],[0]
"In addition, for analysis purposes, we report results for three surrogate tasks that can be used in the ranking problem: i) the binary ranker trained on all comment pairs within each list, in which low karma comments dominate, ii) a positive vs. negative karma classifier, and iii) a high vs. medium karma classifier.",4.1 Tasks,[0],[0]
"All use class-balanced data; the second two are trained and tested on a biased sampling of the data, where the pairs need not be from the same discussion thread.",4.1 Tasks,[0],[0]
"We use the support vector machine (SVM) rank algorithm (Joachims, 2002) to predict a rank order for each list of comments.",4.2 Classifier,[0],[0]
The SVM is trained to predict which of a pair of comments has higher karma.,4.2 Classifier,[0],[0]
"The error term penalty parameter is tuned to maximize P@1 on a held-out validation set (20% of the training samples).
",4.2 Classifier,[0],[0]
"Since much of the data includes low-karma comments, there will be a tendancy for the learning to emphasize features that discriminate comments at the lower end of the scale.",4.2 Classifier,[0],[0]
"In order to learn features that improve P@1, and to understand the relative importance of different features, we use a greedy automatic feature selection process that incrementally adds one feature whose resulting feature set achives the highest P@1 on the validation set.",4.2 Classifier,[0],[0]
"Once all features have been used, we select the model with the subset of features that obtains the best P@1 on the validation set.",4.2 Classifier,[0],[0]
The features are designed to capture several key attributes that we hypothesize are predictive of comment karma motivated by related work.,4.3 Features,[0],[0]
"The features are categorized in groups as summarized below, with details in supplementary material.",4.3 Features,[0],[0]
"• Graph and Timing (G&T): A baseline that
captures discourse history (response structure) and comment timing, but no text content.",4.3 Features,[0],[0]
"• Authority and Reputation (A&R): K-index,
whether the commenter was the original poster, and in some subreddits “flair” (display next to a comment author’s username that is subject to a cursory verification by moderators).",4.3 Features,[0],[0]
•,4.3 Features,[0],[0]
Informativeness (Info.):,4.3 Features,[0],[0]
"Different indicators
suggestive of informative content and novelty, including various word counts, named entity counts, urls, and unseen n-grams.",4.3 Features,[0],[0]
• Lexical Unigrams (Lex.):,4.3 Features,[0],[0]
"Miscellaneous word
class indicators, puncutation, and part-ofspeech counts • Predicted Community Response (Resp.):
",4.3 Features,[0],[0]
"Probability scores from surrogate classification tasks (reply vs. no reply, positive vs. negative sentiment) to measure the community response of a comment using bag-of-words predictors.",4.3 Features,[0],[0]
• Relevance (Rel.):,4.3 Features,[0],[0]
"Comment similarity to the
parent, post and title in terms of topic, computed with three methods: i) a distributed vector representation of topic using a non-negative matrix factorization (NMF) model (Xu et al., 2003), ii)",4.3 Features,[0],[0]
"the average of skip-gram word embeddings (Mikolov et al., 2013), and iii) word set Jaccard similarity (Strehl et al., 2000).",4.3 Features,[0],[0]
•,4.3 Features,[0],[0]
Mood: Mean and std.,4.3 Features,[0],[0]
"deviation of sentence sen-
timent in the comment; word list indicators for politeness, argumentativeness and profanity.",4.3 Features,[0],[0]
• Community Style (Comm.):,4.3 Features,[0],[0]
"Posterior proba-
bility of each subreddit given the comment using a bag-of-words model.",4.3 Features,[0],[0]
The various word lists are motivated by feature exploration studies in surrogate tasks.,4.3 Features,[0],[0]
"For example, projecting words to a two dimensional space of positive vs. negative and likelihood of reply showed that self-oriented pronouns were more likely to have no response and secondperson pronouns were more likely to have a negative response.",4.3 Features,[0],[0]
"The politeness and argumentativeness/profanity lists are generated by starting with hand-specified seed lists used to train an SVM to classify word embeddings (Mikolov et al., 2013)
into these categories, and expanding the lists with 500 words farthest from the decision boundary.
",4.3 Features,[0],[0]
"Both the NMF and the skip-gram topic models use a cosine distance to determine topic similarity, with 300 as the word embedding dimension.",4.3 Features,[0],[0]
Both are trained on approximately 2 million comments in high karma posts taken across a wide variety of subreddits.,4.3 Features,[0],[0]
"We use topic models in various measures of comment relevance to the discussion, but we do not use topic of the comment on its own since topic is controlled for by ranking within a thread.",4.3 Features,[0],[0]
"We present three sets of experiments on comment karma ranking, all of which show very different behavior for the different subreddits.",5 Ranking Experiments,[0],[0]
Fig. 1 shows the relative gain in P@1 over the G&T baseline associated with using different feature groups.,5 Ranking Experiments,[0],[0]
The importance of the different features reflect the nature of the different communities.,5 Ranking Experiments,[0],[0]
"The authority/reputation features help most for ASKSCIENCE, consistent with our k-index study.",5 Ranking Experiments,[0],[0]
Informativeness and relevance help all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,[0],[0]
"Lexical, mood and community style features are useful in some cases, but hurt others.",5 Ranking Experiments,[0],[0]
"The predicted probability of a reply was least useful, possibly because of the low-karma training bias.
",5 Ranking Experiments,[0],[0]
Tables 3 and 4 summarize the results for the P@1 and NDCG criteria using the greedy selection procedure (which optimizes P@1) compared to a random baseline and the G&T baseline.,5 Ranking Experiments,[0],[0]
The random baseline for P@1 is greater than 10% because of ties.,5 Ranking Experiments,[0],[0]
"The G&T baseline results show that the graph and timing features alone obtain 21-32%
of top karma comments depending on subreddits.",5 Ranking Experiments,[0],[0]
Adding the textual features gives an improvement in P@1 performance over the G&T baseline for all subreddits except ASKMEN and WORLDNEWS.,5 Ranking Experiments,[0],[0]
"The trends for performance measured with NDCG are similar, but the benefit from textual features is smaller.",5 Ranking Experiments,[0],[0]
"The results in both tables show different ways of reporting performance of the same system, but the system has been optimized for P@1 in terms of feature selection.",5 Ranking Experiments,[0],[0]
"In initial exploratory experiments, this seems to have a small impact: when optimizing for NDCG in feature selection we obtain 0.61 vs. 0.60 with the P@1-optimized features.
",5 Ranking Experiments,[0],[0]
"A major challenge with identifying high karma comments (and negative karma comments) is that
they are so rare.",5 Ranking Experiments,[0],[0]
"Although our feature selection tunes for high rank precision, it is possible that the low-karma data dominate the learning.",5 Ranking Experiments,[0],[0]
"Alternatively, it may be that language cues are mainly useful for identifying distinguishing the negative or mid-level karma comments, and that the very high karma comments are a matter of timing.",5 Ranking Experiments,[0],[0]
"To better understand the role of language for these different types, we trained classifiers on balanced data for positive vs. negative karma and high vs. mid levels of karma.",5 Ranking Experiments,[0],[0]
"For these models, the training pairs could come from different threads, but topic is controlled for in that all topic features are relative (similarity to original post, parent, etc.).",5 Ranking Experiments,[0],[0]
"We compared the results to the binary classifier used in ranking, where all pairs are considered.",5 Ranking Experiments,[0],[0]
"In all three cases, random chance accuracy is 50%.
",5 Ranking Experiments,[0],[0]
Table 5 shows the pairwise accuracy of these classifiers.,5 Ranking Experiments,[0],[0]
"We find that distinguishing positive from negative classes is fairly easy, with the notable exception of the more information-oriented subreddit ASKSCIENCE.",5 Ranking Experiments,[0],[0]
"Averaging across the different subreddits, the high vs. mid task is slightly easier than the general ranking task, but the variation across subreddits is substantial.",5 Ranking Experiments,[0],[0]
"The high vs. mid distinction for FITNESS falls below chance (likely overtraining), whereas it seems to be an easier task for the ASKWOMEN, ASKMEN, and WORLDNEWS.",5 Ranking Experiments,[0],[0]
"Interest in social media is rapidly growing in recent years, which includes work on predicting the popularity of posts, comments and tweets.",6 Related Work,[0],[0]
Danescu-Niculescu-Mizil et al. (2012) investigate phrase memorability in the movie quotes.,6 Related Work,[0],[0]
Cheng et al. (2014) explore prediction of information cascades on Facebook.,6 Related Work,[0],[0]
"Weninger et al. (2013) analyze the hierarchy of the Reddit discussions, topic shifts, and popularity of the comment, using among the others very simple language analysis.",6 Related Work,[0],[0]
"Lampos et al. (2014) study the problem of predicting a Twitter user impact score (determined by combining the numbers of user’s followers, followees, and listings) using text-based and nontextual features, showing that performance improves when user participation in particular topics is included.
",6 Related Work,[0],[0]
Most relevant to this paper are studies of the effect of language in popularity predictions.,6 Related Work,[0],[0]
"Tan et al. (2014) study how word choice affects the pop-
ularity of Twitter messages.",6 Related Work,[0],[0]
"As in our work, they control for topic, but they also control for the popularity of the message authors.",6 Related Work,[0],[0]
"On Reddit, we find that celebrity status is less important than it is on Twitter since on Reddit almost everyone is anonymous.",6 Related Work,[0],[0]
Lakkaraju et al. (2013) study how timing and language affect the popularity of posting images on Reddit.,6 Related Work,[0],[0]
They control for content by only making comparisons between reposts of the same image.,6 Related Work,[0],[0]
"Our focus is on studying comments within a discussion instead of standalone posts, and we analyze a vast majority of language features.",6 Related Work,[0],[0]
Althoff et al. (2014) use deeper language analysis on Reddit to predict the success of receiving a pizza in the Random Acts of Pizza subreddit.,6 Related Work,[0],[0]
"To our knowledge, this is the first work on ranking comments in terms of community endorsement.",6 Related Work,[0],[0]
This paper addresses the problem of how language affects the reaction of community in Reddit comments.,7 Conclusion,[0],[0]
We collect a new dataset of six subredit discussion forums.,7 Conclusion,[0],[0]
"We introduce a new task of ranking comments based on karma in Reddit discussions, which controls for topic and timing of comments.",7 Conclusion,[0],[0]
Our results show that using language features improve the comment ranking task in most of the subreddits.,7 Conclusion,[0],[0]
"Informativeness and relevance are the most broadly useful feature categories; reputation matters for ASKSCIENCE, and other categories could either help or hurt depending on the community.",7 Conclusion,[0],[0]
Future work involves improving the classification algorithm by using new approaches to learning about rare events.,7 Conclusion,[0],[0]
"This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger.",abstractText,[0],[0]
"A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments.",abstractText,[0],[0]
Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.,abstractText,[0],[0]
Talking to the crowd: What do people react to in online discussions?,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 896–905 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1083
Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).
Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated
topics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.
The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.
The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.
For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.
896",text,[0],[0]
"The anchor algorithm computes the topic matrix A, where Av,k is the conditional probability of observing word v given topic k, e.g., the probability of seeing the word “lens” given the camera topic in a corpus of Amazon product reviews.",1 Vanilla Anchor Algorithm,[0],[0]
Arora et al. (2012a) find these probabilities by assuming that every topic contains at least one ‘anchor’ word which has a non-zero probability only in that topic.,1 Vanilla Anchor Algorithm,[0],[0]
"Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.
",1 Vanilla Anchor Algorithm,[0],[0]
"To recover the topic matrix A using anchor words, we first compute a V × V cooccurrence matrix Q, where Qi,j is the conditional probability p(wj |wi) of seeing word type wj after having seen wi in the same document.",1 Vanilla Anchor Algorithm,[0],[0]
A form of the Gram-Schmidt process on Q finds anchor words {g1 . . .,1 Vanilla Anchor Algorithm,[0],[0]
"gk} (Arora et al., 2013).
",1 Vanilla Anchor Algorithm,[0],[0]
"Once we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A).",1 Vanilla Anchor Algorithm,[0],[0]
"This coefficient matrix C is defined row-wise for each word i
C∗i,· = argmin Ci,· DKL
( Qi,· ∥∥∥∥",1 Vanilla Anchor Algorithm,[0],[0]
"K∑
k=1
Ci,kQgk,·
) ,
(1) which gives the best reconstruction (based on Kullback-Leibler divergence DKL) of non-anchor words given anchor words’ conditional probabilities.",1 Vanilla Anchor Algorithm,[0],[0]
"For example, in our product review data, a word such as “battery” is a convex combination of the anchor words’ contexts (Qgk,·) such as “camera”, “phone”, and “car”.",1 Vanilla Anchor Algorithm,[0],[0]
Solving each row of C is fast and is embarrassingly parallel.,1 Vanilla Anchor Algorithm,[0],[0]
"Finally, we apply Bayes’ rule to recover the topic matrix A from the coefficient matrix",1 Vanilla Anchor Algorithm,[0],[0]
"C.
The anchor algorithm can be orders of magnitude faster than probabilistic inference (Arora et al., 2013).",1 Vanilla Anchor Algorithm,[0],[0]
The construction of Q has a runtime of O(DN2) where D is the number of documents and N is the average number of tokens per document.,1 Vanilla Anchor Algorithm,[0],[0]
This computation requires only a single pass over the data and can be pre-computed for interactive use-cases.,1 Vanilla Anchor Algorithm,[0],[0]
"Once Q is constructed, topic recovery requires O(KV 2 +K2V I), where K is the number of topics, V is the vocabulary size, and I is the average number of iterations (typically 100-1000).",1 Vanilla Anchor Algorithm,[0],[0]
"In contrast, traditional topic
Anchor Top Words in Topics backpack backpack camera lens bag room carry fit cameras equipment comfortable camera camera lens pictures canon digital lenses batteries filter mm photos bag bag camera diaper lens bags genie smell
room diapers odor
Table 1: Three separate attempts to construct a topic concerning camera bags in Amazon product reviews with single word anchors.",1 Vanilla Anchor Algorithm,[0],[0]
This example is drawn from preliminary experiments with an author as the user.,1 Vanilla Anchor Algorithm,[0],[0]
The term “backpack” is a good anchor because it uniquely identifies the topic.,1 Vanilla Anchor Algorithm,[0],[0]
"However, both “camera” and “bag” are poor anchors for this topic.
model inference typically requires multiple passes over the entire data.",1 Vanilla Anchor Algorithm,[0],[0]
"Techniques such as Online LDA (Hoffman et al., 2010) or Stochastic Variation Inference (Hoffman et al., 2013) improves this to a single pass over the entire data.",1 Vanilla Anchor Algorithm,[0],[0]
"However, from Heaps’ law (Heaps, 1978)",1 Vanilla Anchor Algorithm,[0],[0]
"it follows that V 2 DN for large datasets, leading to much faster inference times for anchor methods compared to probabilistic topic modeling.",1 Vanilla Anchor Algorithm,[0],[0]
"Further, even if online were to be adapted to incorporate human guidance, a single pass is not tractable for interactive use.",1 Vanilla Anchor Algorithm,[0],[0]
Single word anchors can be opaque to users.,2 Tandem Anchor Extension,[0],[0]
"For an example of bewildering anchor words, consider a camera bag topic from a collection of Amazon product reviews (Table 1).",2 Tandem Anchor Extension,[0],[0]
The anchor word “backpack” may seem strange.,2 Tandem Anchor Extension,[0],[0]
"However, this dataset contains nothing about regular backpacks; thus, “backpack” is unique to camera bags.",2 Tandem Anchor Extension,[0],[0]
"Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic.
",2 Tandem Anchor Extension,[0],[0]
The anchor selection strategy can mitigate this problem to some degree.,2 Tandem Anchor Extension,[0],[0]
"For example, rather than selecting anchors using an approximate convex hull in high-dimensional space, we can find an exact convex hull in a low-dimensional embedding (Lee and Mimno, 2014).",2 Tandem Anchor Extension,[0],[0]
"This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling.
",2 Tandem Anchor Extension,[0],[0]
"If we instead ask users to give us representative
words for this topic, we would expect combinations of words like “camera” and “bag.”",2 Tandem Anchor Extension,[0],[0]
"However, with single word anchors we must choose a single word to anchor each topic.",2 Tandem Anchor Extension,[0],[0]
"Unfortunately, because these words might appear in multiple topics, individually they are not suitable as anchor words.",2 Tandem Anchor Extension,[0],[0]
"The anchor word “camera” generates a general camera topic instead of camera bags, and the topic anchored by “bag” includes bags for diaper pails (Table 1).
",2 Tandem Anchor Extension,[0],[0]
"Instead, we need to use sets of representative terms as an interpretable, parsimonious description of a topic.",2 Tandem Anchor Extension,[0],[0]
This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics.,2 Tandem Anchor Extension,[0],[0]
This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings.,2 Tandem Anchor Extension,[0],[0]
We first need to turn words into an anchor.,2.1 Anchor Facets,[0],[0]
"If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V -dimensional space.",2.1 Anchor Facets,[0],[0]
We then model each point as a convex combination of anchor words to reconstruct the topic matrix A (Equation 1).,2.1 Anchor Facets,[0],[0]
"Instead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic.",2.1 Anchor Facets,[0],[0]
"The facets for each anchor form a new pseudoword, or an invented point in V -dimensional space (described in more detail in Section 2.2).
",2.1 Anchor Facets,[0],[0]
"While these new points do not correspond to words in the vocabulary, we can express nonanchor words as convex combinations of pseudowords.",2.1 Anchor Facets,[0],[0]
"To construct these pseudowords from their facets, we combine the co-occurrence profiles of the facets.",2.1 Anchor Facets,[0],[0]
These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors.,2.1 Anchor Facets,[0],[0]
"We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified.
",2.1 Anchor Facets,[0],[0]
Our augmented matrix S is therefore a (V + K) × V matrix.,2.1 Anchor Facets,[0],[0]
"As before, V is the number of token types in the data and K is the number of topics.",2.1 Anchor Facets,[0],[0]
"The first V rows of S correspond to the V token types observed in the data, while the additionalK rows correspond to the pseudowords constructed from anchor facets.",2.1 Anchor Facets,[0],[0]
"Each entry of S en-
codes conditional probabilities so that Si,j is equal to p(wi |wj).",2.1 Anchor Facets,[0],[0]
"For the additionalK rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities.
",2.1 Anchor Facets,[0],[0]
"This modification is similar in spirit to supervised anchor words (Nguyen et al., 2015).",2.1 Anchor Facets,[0],[0]
This supervised extension of the anchor words algorithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word.,2.1 Anchor Facets,[0],[0]
"By extending the vector-space representation of each word, anchor words corresponding to metadata values can be found.",2.1 Anchor Facets,[0],[0]
"In contrast, our extension does not add dimensions to the representation, but simply places additional points corresponding to pseudoword words in the vectorspace representation.",2.1 Anchor Facets,[0],[0]
We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor.,2.2 Combining Facets into Pseudowords,[0],[0]
"In tandem anchors, we create vector representations that combine the information from anchor facets.",2.2 Combining Facets into Pseudowords,[0],[0]
Our anchor facets are G1 . .,2.2 Combining Facets into Pseudowords,[0],[0]
.GK,2.2 Combining Facets into Pseudowords,[0],[0]
", where Gk is a set of anchor facets which will form the kth pseudoword anchor.",2.2 Combining Facets into Pseudowords,[0],[0]
The pseudowords are g1 . . .,2.2 Combining Facets into Pseudowords,[0],[0]
"gK , where gk is the pseudoword from Gk.",2.2 Combining Facets into Pseudowords,[0],[0]
"These pseudowords form the new rows of S. We give several candidates for combining anchors facets into a single multiword anchor; we compare their performance in Section 3.
",2.2 Combining Facets into Pseudowords,[0],[0]
Vector Average An obvious function for computing the central tendency is the vector average.,2.2 Combining Facets into Pseudowords,[0],[0]
"For each anchor facet,
Sgk,j = ∑
i∈Gk
Si,j |Gk| , (2)
where |Gk| is the cardinality of Gk.",2.2 Combining Facets into Pseudowords,[0],[0]
"Vector average makes the pseudoword Sgk,j more central, which is intuitive but inconsistent with the interpretation from Arora et al. (2013) that anchors should be extreme points whose linear combinations explain more central words.
",2.2 Combining Facets into Pseudowords,[0],[0]
Or-operator An alternative approach is to consider a cooccurrence with any anchor facet in Gk.,2.2 Combining Facets into Pseudowords,[0],[0]
"For word j, we use De Morgan’s laws to set
Sgk,j = 1− ∏
i∈Gk (1− Si,j).",2.2 Combining Facets into Pseudowords,[0],[0]
"(3)
Unlike the average, which pulls the pseudoword inward, this or-operator pushes the word outward,
increasing each of the dimensions.",2.2 Combining Facets into Pseudowords,[0],[0]
"Increasing the volume of the simplex spanned by the anchors explains more words.
",2.2 Combining Facets into Pseudowords,[0],[0]
Element-wise Min Vector average and oroperator are both sensitive to outliers and cannot account for polysemous anchor facets.,2.2 Combining Facets into Pseudowords,[0],[0]
"Returning to our previous example, both “camera” and “bag” are bad anchors for camera bags because they appear in documents discussing other products.",2.2 Combining Facets into Pseudowords,[0],[0]
"However, if both “camera” and “bag” are anchor facets, we can look at an intersection of their contexts: words that appear with both.",2.2 Combining Facets into Pseudowords,[0],[0]
"Using the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags.
",2.2 Combining Facets into Pseudowords,[0],[0]
"Mathematically, this is an element-wise min operator,
Sgk,j = min i∈Gk Si,j .",2.2 Combining Facets into Pseudowords,[0],[0]
"(4)
This construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic.
",2.2 Combining Facets into Pseudowords,[0],[0]
"Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean.",2.2 Combining Facets into Pseudowords,[0],[0]
"Thus, for each anchor facet
Sgk,j = ∑
i∈Gk
( S−1i,j |Gk| )−1 .",2.2 Combining Facets into Pseudowords,[0],[0]
"(5)
Since the harmonic mean tends towards the lowest values in the set, it is not sensitive to large outliers, giving us robustness to polysemous words.",2.2 Combining Facets into Pseudowords,[0],[0]
"After constructing the pseudowords of S we then need to find the coefficients Ci,k which describe each word in our vocabulary as a convex combination of the multiword anchors.",2.3 Finding Topics,[0],[0]
"Like standard anchor methods, we solve the following for each token type:
C∗i,· = argmin Ci,· DKL
( Si,· ∥∥∥∥ K∑
k=1
Ci,kSgk,·
) .
",2.3 Finding Topics,[0],[0]
"(6) Finally, we appeal to Bayes’ rule, we recover the topic-word matrix A from the coefficients of C.
The correctness of the topic recovery algorithm hinges upon the assumption of separability.",2.3 Finding Topics,[0],[0]
"Separability means that the occurrence pattern across
documents of the anchor words across the data mirrors that of the topics themselves.",2.3 Finding Topics,[0],[0]
"For single word anchors, this has been observed to hold for a wide variety of data (Arora et al., 2012b).",2.3 Finding Topics,[0],[0]
"With our tandem anchor extension, we make similar assumptions as the vanilla algorithm, except with pseudowords constructed from anchor facets.",2.3 Finding Topics,[0],[0]
"So long as the occurrence pattern of our tandem anchors mirrors that of the underlying topics, we can use the same reasoning as Arora et al. (2012a) to assert that we can provably recover the topic-word matrix A with all of the same theoretical guarantees of complexity and robustness.",2.3 Finding Topics,[0],[0]
"Furthermore, we runtime analysis given by Arora et al. (2013) applies to tandem anchors.
",2.3 Finding Topics,[0],[0]
"If desired, we can also add further robustness and extensibility to tandem anchors by adding regularization to Equation 6.",2.3 Finding Topics,[0],[0]
"Regularization allows us to add something which is mathematically similar to priors, and has been shown to improve the vanilla anchor word algorithm (Nguyen et al., 2014).",2.3 Finding Topics,[0],[0]
"We leave the question of the best regularization for tandem anchors as future work, and focus our efforts on solving the problem of interactive topic modeling.",2.3 Finding Topics,[0],[0]
"Before addressing interactivity, we apply tandem anchors to real world data, but with anchors gleaned from metadata.",3 High Water Mark for Tandem Anchors,[0],[0]
Our purpose is twofold.,3 High Water Mark for Tandem Anchors,[0],[0]
"First, we determine which combiner from Section 2.2 to use in our interactive experiments in Section 4 and second, we confirm that well-chosen tandem anchors can improve topics.",3 High Water Mark for Tandem Anchors,[0],[0]
"In addition, we examine the runtime of tandem anchors and compare to traditional model-based interactive topic modeling techniques.",3 High Water Mark for Tandem Anchors,[0],[0]
"We cannot assume that we will have metadata available to build tandem anchors, but we use them here because they provide a high water mark without the variance introduced by study participants.",3 High Water Mark for Tandem Anchors,[0],[0]
"We use the well-known 20 Newsgroups dataset (20NEWS) used in previous interactive topic modeling work: 18,846 Usenet postings from 20 different newgroups in the early 1990s.1 We remove the newsgroup headers from each message, which contain the newsgroup names, but otherwise left messages intact with any footers or quotes.",3.1 Experimental Setup,[0],[0]
"We
1http://qwone.com/˜jason/20Newsgroups/
then remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.
",3.1 Experimental Setup,[0],[0]
"To seed the tandem anchors, we use the titles of newsgroups.",3.1 Experimental Setup,[0],[0]
"To build each multiword anchor facet, we split the title on word boundaries and expand any abbreviations or acronyms.",3.1 Experimental Setup,[0],[0]
"For example, the newsgroup title ‘comp.os.mswindows.misc’ becomes {“computer”, “operating”, “system”, “microsoft”, “windows”, “miscellaneous”}.",3.1 Experimental Setup,[0],[0]
"We do not fully specify the topic; the title gives some intuition, but the topic modeling algorithm must still recover the complete topic-word distributions.",3.1 Experimental Setup,[0],[0]
This is akin to knowing the names of the categories used but nothing else.,3.1 Experimental Setup,[0],[0]
"Critically, the topic modeling algorithm has no knowledge of document-label relationships.",3.1 Experimental Setup,[0],[0]
Our first evaluation is a classification task to predict documents’ newsgroup membership.,3.2 Experimental Results,[0],[0]
"Thus, we do not aim for state-of-the-art accuracy,2 but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than Gram-Schmidt anchors.",3.2 Experimental Results,[0],[0]
"After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tandem anchors and the Gram-Schmidt single word anchors.3 For multiword anchors, we use each of the combiner functions from Section 2.2.",3.2 Experimental Results,[0],[0]
"The anchor algorithm only gives the topic-word distributions and not word-level topic assignments, so we infer token-level topic assignments using LDA Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method.",3.2 Experimental Results,[0],[0]
We use our own implementation of Gibbs sampling with fixed topics and a symmetric documenttopic Dirichlet prior with concentration α = .01.,3.2 Experimental Results,[0],[0]
"Since the topics are fixed, this inference is very fast and can be parallelized on a per-document basis.",3.2 Experimental Results,[0],[0]
We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit4 with topic-word pairs as features.,3.2 Experimental Results,[0],[0]
"Finally, we infer topic assignments in the test data and evaluate the classification using those topic-word features.",3.2 Experimental Results,[0],[0]
"For both training and test, we exclude words outside
2The best system would incorporate topic features with other features, making it harder to study and understand the topical trends in isolation.
3With fixed anchors and data the anchor algorithm is deterministic, so we use random splits instead of the standard train/test splits so that we can compute variance.
",3.2 Experimental Results,[0],[0]
"4http://hunch.net/˜vw/
the LDA vocabulary.",3.2 Experimental Results,[0],[0]
The topics created from multiword anchor facets are more accurate than Gram-Schmidt topics (Figure 1).,3.2 Experimental Results,[0],[0]
This is true regardless of the combiner function.,3.2 Experimental Results,[0],[0]
"However, harmonic mean is more accurate than the other functions.5
Since 20NEWS has twenty classes, accuracy alone does not capture confusion between closely related newsgroups.",3.2 Experimental Results,[0],[0]
"For example, accuracy penalizes a classifier just as much for labeling a document from ‘rec.sport.baseball’ with ‘rec.sport.hockey’ as with ‘alt.atheism’ despite the similarity between sports newsgroups.",3.2 Experimental Results,[0],[0]
"Consequently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes.
",3.2 Experimental Results,[0],[0]
"The first clustering metric is the adjusted Rand index (Yeung and Ruzzo, 2001), which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clustering.",3.2 Experimental Results,[0],[0]
Adjusted Rand index (ARI) also accounts for chance groupings of documents.,3.2 Experimental Results,[0],[0]
"Next we use F-measure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives.",3.2 Experimental Results,[0],[0]
"Finally, we use variation of information (VI).",3.2 Experimental Results,[0],[0]
"This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003).",3.2 Experimental Results,[0],[0]
"Since we are measuring the amount of information lost, lower variation of information is better.
",3.2 Experimental Results,[0],[0]
"Based on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1).",3.2 Experimental Results,[0],[0]
"As with accuracy, this is true regardless of which combination function we use.",3.2 Experimental Results,[0],[0]
"Furthermore, harmonic mean produces the least confusion between classes.5
The final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality.",3.2 Experimental Results,[0],[0]
"Given V , the set of the n most probable words of a topic, coherence is
∑
v1,v2∈V log
D(v1, v2) +
D(v2) (7)
where D(v1, v2) is the co-document frequency of
5Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.2 Experimental Results,[0],[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
word types v1 and v2, and D(v2) is the document frequency of word type v2.",3.2 Experimental Results,[0],[0]
"A smoothing parameter prevents zero logarithms.
",3.2 Experimental Results,[0],[0]
Figure 1 also shows topic coherence.,3.2 Experimental Results,[0],[0]
"Although title-based anchor facets produce better classification features, topics from Gram-Schmidt anchors have better coherence than title-based anchors with the vector average or the or-operator.",3.2 Experimental Results,[0],[0]
"However, when using the harmonic mean combiner, title-based anchors produce the most human interpretable topics.6
Harmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet.",3.2 Experimental Results,[0],[0]
"Both the vector average and the or-operator are swayed by large outliers, making them sensitive to ambiguous terms in an anchor facet.",3.2 Experimental Results,[0],[0]
"Element-wise min also has this robustness, but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min.",3.2 Experimental Results,[0],[0]
Tandem anchors will enable users to direct topic inference to improve topic quality.,3.3 Runtime Considerations,[0],[0]
"However, for the algorithm to be interactive we must also consider runtime.",3.3 Runtime Considerations,[0],[0]
Cook and Thomas (2005) argue that for interactive applications with user-initiated actions like ours the response time should be less than ten seconds.,3.3 Runtime Considerations,[0],[0]
"Longer waits can increase the cognitive load on the user and harm the user interaction.
6Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction.",3.3 Runtime Considerations,[0],[0]
"For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.
",3.3 Runtime Considerations,[0],[0]
"Fortunately, the runtime of tandem anchors is amenable to interactive topic modeling.",3.3 Runtime Considerations,[0],[0]
"On 20NEWS, interactive updates take a median time of 2.13 seconds.",3.3 Runtime Considerations,[0],[0]
This result was obtained using a single core of an AMD Phemon II X6 1090T processor.,3.3 Runtime Considerations,[0],[0]
"Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets.
",3.3 Runtime Considerations,[0],[0]
"Compared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time.",3.3 Runtime Considerations,[0],[0]
"For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model updates with a median time of 24.8 seconds (Hu and Boyd-Graber, 2012), which is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors.
",3.3 Runtime Considerations,[0],[0]
"Another promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words.",3.3 Runtime Considerations,[0],[0]
Utopian is much slower than tandem anchors.,3.3 Runtime Considerations,[0],[0]
"Even on the small InfoVisVAST dataset which contains only 515 documents, Utopian takes 48 seconds to converge.",3.3 Runtime Considerations,[0],[0]
"While the times are not strictly comparable due to differing datasets, Utopian scales linearly with the size of the data, we can intuit that even for moderately sized datasets such as 20NEWS, Utopian is infeasible for interactive topic modeling due to run time.
",3.3 Runtime Considerations,[0],[0]
"While each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for inter-
activity.",3.3 Runtime Considerations,[0],[0]
"Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pronounced as we increase the amount of data.",3.3 Runtime Considerations,[0],[0]
"Given high quality anchor facets, the tandem anchor algorithm can produce high quality topic models (particularly when the harmonic mean combiner is used).",4 Interactive Anchor Words,[0],[0]
"Moreover, the tandem anchor algorithm is fast enough to be interactive (as opposed to model-based approaches such as the Interactive Topic Model).",4 Interactive Anchor Words,[0],[0]
We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling.,4 Interactive Anchor Words,[0],[0]
We compare both single word and tandem anchors in our study.,4 Interactive Anchor Words,[0],[0]
"We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users.",4 Interactive Anchor Words,[0],[0]
"To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to understand a dataset using the anchor word algorithm.",4.1 Interface and User Study,[0],[0]
"For this user study, we recruit twenty participants drawn from a university student body.",4.1 Interface and User Study,[0],[0]
The student median age is twenty-two.,4.1 Interface and User Study,[0],[0]
"Seven are female, and thirteen are male.",4.1 Interface and User Study,[0],[0]
"None of the students had any prior familiarity with topic modeling or the 20NEWS dataset.
",4.1 Interface and User Study,[0],[0]
Each participant sees a simple user interface (Figure 2) with topic given as a row with two columns.,4.1 Interface and User Study,[0],[0]
The left column allows users to view and edit topics’ anchor words; the right column lists the most probable words in each topic.7,4.1 Interface and User Study,[0],[0]
"The user can remove an anchor word or drag words from
7While we use topics generated using harmonic mean for our final analysis, users were shown topics generated using the min combiner.",4.1 Interface and User Study,[0],[0]
"However, this does not change our result.
",4.1 Interface and User Study,[0],[0]
the topic word lists (right column) to become an anchor word.,4.1 Interface and User Study,[0],[0]
Users can also add additional topics by clicking the “Add Anchor” to create additional anchors.,4.1 Interface and User Study,[0],[0]
"If the user wants to add a word to a tandem anchor set that does not appear in the interface, they manually type the word (restricted to the model’s vocabulary).",4.1 Interface and User Study,[0],[0]
"When the user wants to see the updated topics for their newly refined anchors, they click “Update Topics”.
",4.1 Interface and User Study,[0],[0]
We give each a participant a high level overview of topic modeling.,4.1 Interface and User Study,[0],[0]
"We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous topics.",4.1 Interface and User Study,[0],[0]
Users are instructed to use their best judgement to determine if topics are useful.,4.1 Interface and User Study,[0],[0]
The task is to edit the anchor words to improve the topics.,4.1 Interface and User Study,[0],[0]
"We asked that users spend at least twenty minutes, but no more than thirty minutes.",4.1 Interface and User Study,[0],[0]
"We repeat the task twice: once with tandem anchors, and once with single word anchors.8",4.1 Interface and User Study,[0],[0]
"We now validate our main result that for interactive topic modeling, tandem anchors yields better topics than single word anchors.",4.2 Quantitative Results,[0],[0]
"Like our titlebased experiments in Section 3, topics generated from users become features to train and test a classifier for the 20NEWS dataset.",4.2 Quantitative Results,[0],[0]
We choose this dataset for easier comparison with the Interactive Topic Modeling result of Hu et al. (2014).,4.2 Quantitative Results,[0],[0]
"Basedsie on our results with title-based anchors, we use the harmonic mean combiner in our analysis.",4.2 Quantitative Results,[0],[0]
"As before, we report not only accuracy, but also multiple clustering metrics using the confusion matrix from the classification task.",4.2 Quantitative Results,[0],[0]
"Finally, we report topic coherence.
",4.2 Quantitative Results,[0],[0]
Figure 3 summarizes the results of our quantitative evaluation.,4.2 Quantitative Results,[0],[0]
"While we only compare user generated anchors in our analysis, we include the unsupervised Gram-Schmidt anchors as a baseline.",4.2 Quantitative Results,[0],[0]
Some of the data violate assumptions of normality.,4.2 Quantitative Results,[0],[0]
"Therefore, we use Wilcoxon’s signed-rank test (Wilcoxon, 1945) to determine if the differences between multiword anchors and single word anchors are significant.
Topics from user generated multiword anchors yield higher classification accuracy (Figure 3).",4.2 Quantitative Results,[0],[0]
"Not only is our approach more scalable than the Interactive Topic Model, but we also achieve
8The order in which users complete these tasks is counterbalanced.
higher classification accuracy than Hu et al. (2014).9 Tandem anchors also improve clustering metrics.10
While user selected tandem anchors produce better classification features than single word anchors, users selected single word anchors produce topics with similar topic coherence scores.11
To understand this phenomenon, we use quality metrics (AlSumait et al., 2009) for ranking topics by their correspondence to genuine themes in the data.",4.2 Quantitative Results,[0],[0]
"Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words.",4.2 Quantitative Results,[0],[0]
"Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution.",4.2 Quantitative Results,[0],[0]
"Finally, background topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data.
",4.2 Quantitative Results,[0],[0]
Figure 4 reports our topic significance findings.,4.2 Quantitative Results,[0],[0]
"For all three significance metrics, multiword anchors produce more significant topics than single word anchors.10 Topic coherence is based solely on the top n words of a topic, while both accuracy and topic significance depend on the entire topicword distributions.",4.2 Quantitative Results,[0],[0]
"With single word anchors, topics with good coherence may still be too general.",4.2 Quantitative Results,[0],[0]
Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification.,4.2 Quantitative Results,[0],[0]
We examine the qualitative differences between how users select multiword anchor facets versus single word anchors.,4.3 Qualitative Results,[0],[0]
Table 2 gives examples of topics generated using different anchor strategies.,4.3 Qualitative Results,[0],[0]
"In a follow-up survey with our users, 75% find it easier to affect individual changes in the topics using tandem anchors compared to single word anchors.",4.3 Qualitative Results,[0],[0]
"Users who prefer editing multiword anchors over single word anchors often report that
9However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits.
10Significant at p < 0.01 when using Wilcoxon’s signedrank test.
",4.3 Qualitative Results,[0],[0]
"11The difference between coherence scores was not statistically significant using Wilcoxon’s signed-rank test.
",4.3 Qualitative Results,[0],[0]
multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors.,4.3 Qualitative Results,[0],[0]
"For example, by combining multiple words related to Christianity, users were able to create a topic which is highly specific, and differentiated from general religion themes which included terms about Atheism and Judaism.
",4.3 Qualitative Results,[0],[0]
"While users find that use tandem anchors is easier, only 55% of our users say that they prefer the final topics produced by tandem anchors compared to single word anchors.",4.3 Qualitative Results,[0],[0]
"This is in harmony with our quantitative measurements of topic coherence, and may be the result of our stopping criteria: when users judged the topics to be useful.
",4.3 Qualitative Results,[0],[0]
"However, 100% of our users feel that the topics created through interaction were better than those generated from Gram-Schmidt anchors.",4.3 Qualitative Results,[0],[0]
"This was true regardless of whether we used tandem anchors or single word anchors.
",4.3 Qualitative Results,[0],[0]
Our participants also produce fewer topics when using multiword anchors.,4.3 Qualitative Results,[0],[0]
The mean difference between topics under single word anchors and multiple word anchors is 9.35.,4.3 Qualitative Results,[0],[0]
"In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors.",4.3 Qualitative Results,[0],[0]
"In contrast, fixing an ambiguous tandem anchor is simple: users just add more terms to the anchor facet.",4.3 Qualitative Results,[0],[0]
Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets.,5 Conclusion,[0],[0]
"For interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use.",5 Conclusion,[0],[0]
"Furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large
datasets for which interactivity was previous impossible.",5 Conclusion,[0],[0]
This work was supported by the collaborative NSF Grant IIS-1409287 (UMD) and,Acknowledgements,[0],[0]
IIS- 1409739 (BYU).,Acknowledgements,[0],[0]
Boyd-Graber is also supported by NSF grants IIS-1320538 and NCSE-1422492.,Acknowledgements,[0],[0]
Interactive topic models are powerful tools for understanding large collections of text.,abstractText,[0],[0]
"However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets.",abstractText,[0],[0]
"Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for userfacing applications.",abstractText,[0],[0]
"We propose combinations of words as anchors, going beyond existing single word anchor algorithms— an approach we call “Tandem Anchors”.",abstractText,[0],[0]
We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and noninteractive approaches.,abstractText,[0],[0]
Tandem anchors are faster and more intuitive than existing interactive approaches.,abstractText,[0],[0]
"Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation.",abstractText,[0],[0]
"In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008).",abstractText,[0],[0]
"However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).",abstractText,[0],[0]
"Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition.",abstractText,[0],[0]
"Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.",abstractText,[0],[0]
"The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009).",abstractText,[0],[0]
We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.,abstractText,[0],[0]
"The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1).",abstractText,[0],[0]
"This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.",abstractText,[0],[0]
A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive.,abstractText,[0],[0]
We extend the anchor algorithm to use multiple anchor words in tandem (Section 2).,abstractText,[0],[0]
"Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.",abstractText,[0],[0]
"For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3).",abstractText,[0],[0]
Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4).,abstractText,[0],[0]
"Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.",abstractText,[0],[0]
Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling,title,[0],[0]
"Applications using machine learning techniques have exploded during the recent years, with “deep learning” techniques being applied on a wide variety of tasks that had hitherto proved challenging.",1. Introduction,[0],[0]
"Training highly accurate machine learning models requires large quantities of (high quality) data, technical expertise and computational resources.",1. Introduction,[0],[0]
"An important recent paradigm is prediction as a service, whereby a service provider with expertise and resources can make predictions for clients.",1. Introduction,[0],[0]
"However, this approach requires trust between service provider and client; there are several instances where clients may be unwilling or unable to provide data to service providers due to privacy
1University of Oxford, Oxford, UK 2The Alan Turing Institute, London, UK 3University of Warwick, Coventry, UK.",1. Introduction,[0],[0]
"Correspondence to: Amartya Sanyal <amartya.sanyal@cs.ox.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
concerns.",1. Introduction,[0],[0]
"Examples include assisting in medical diagnoses (Kononenko, 2001; Blecker et al., 2017), detecting fraud from personal finance data (Ghosh & Reilly, 1994), and detecting online communities from user data (Fortunato, 2010).",1. Introduction,[0],[0]
"The ability of a service provider to predict on encrypted data can alleviate concerns of data leakage.
",1. Introduction,[0],[0]
The framework of fully homomorphic encryption (FHE) is ideal for this paradigm.,1. Introduction,[0],[0]
Fully homomorphic encryption schemes support arbitrary computations to be performed directly on encrypted data without prior decryption.,1. Introduction,[0],[0]
"The first fully homomorphic encryption system was developed just 10 years ago by Gentry (2009), after being an open question for 30 years (Rivest et al., 1978).",1. Introduction,[0],[0]
"Since then several other schemes have been proposed (Gentry et al., 2012; 2013; Brakerski & Vaikuntanathan, 2014; Ducas & Micciancio, 2015; Chillotti et al., 2016).",1. Introduction,[0],[0]
"However, without significant changes to machine learning models and improved algorithmic tools, homomorphic encryption does not scale to real-world machine learning applications.
",1. Introduction,[0],[0]
"Indeed, already there have been several recent works trying to accelerate predictions of machine learning models on fully homomorphic encrypted data.",1. Introduction,[0],[0]
"In general, the approach has been to approximate all or parts of a machine learning model to accommodate the restrictions of an FHE framework.",1. Introduction,[0],[0]
"Often, certain kind of FHE schemes are preferred because they allow for “batched” parallel encrypted computations, called SIMD operations (Smart & Vercauteren, 2014).",1. Introduction,[0],[0]
"This technique is exemplified by the CryptoNets model (Gilad-Bachrach et al., 2016).",1. Introduction,[0],[0]
"While these models allow for high-throughput (via SIMD), they are not particularly suited for the prediction as a service framework for individual users, as single predictions are slow.",1. Introduction,[0],[0]
"Further, because they employ a leveled homomorphic encryption scheme, they are unable to perform many nested multiplications, a requirement for state-of-the-art deep learning models (He et al., 2016; Huang et al., 2017).
",1. Introduction,[0],[0]
"Our solution demonstrates that existing work on Binary Neural Networks (BNNs) (Kim & Smaragdis, 2015; Courbariaux et al., 2016) can be adapted to produce efficient and highly accurate predictions on encrypted data.",1. Introduction,[0],[0]
"We show that a recent FHE encryption scheme (Chillotti et al., 2016) which only supports operations on binary data can be leveraged to compute all of the operations of BNNs.",1. Introduction,[0],[0]
"To do so,
we develop specialized circuits for fully-connected, convolutional, and batch normalization layers (Ioffe & Szegedy, 2015).",1. Introduction,[0],[0]
"Additionally we design tricks to sparsify encrypted computation that reduce computation time even further.
",1. Introduction,[0],[0]
Most similar to our work is Bourse et al. (2017) who use neural networks with signed integer weights and binary activations to perform encrypted prediction.,1. Introduction,[0],[0]
"However, this model is only evaluated on MNIST, with modest accuracy results, and the encryption scheme parameters depend on the structure of the model, potentially requiring clients to re-encrypt their data if the service provider updates their model.",1. Introduction,[0],[0]
"Our framework allows the service provider to update their model at anytime, and allows one to use binary neural networks of Courbariaux et al. (2016) which, in particular, achieve high accuracy on MNIST (99.04%).",1. Introduction,[0],[0]
Another closely related work is Meehan et al. (2018) who design encrypted adder and multiplier circuits so that they can implement machine learning models on integers.,1. Introduction,[0],[0]
"This can be seen as complementary to our work on binary networks: while they achieve improved accuracy because of greater precision, they are less efficient than our methods (however on MNIST we achieve the same accuracy with a 29× speedup, via our sparsification and parallelization tricks).
",1. Introduction,[0],[0]
Private training.,1. Introduction,[0],[0]
"In this work, we do not address the question of training machine learning models with encrypted data.",1. Introduction,[0],[0]
"There has been some recent work in this area (Hardy et al., 2017; Aono et al., 2017).",1. Introduction,[0],[0]
"However, as of now it appears possible only to train very small models using fully homomorphic encryption.",1. Introduction,[0],[0]
We leave this for future work.,1. Introduction,[0],[0]
"In this work, our focus is on achieving speed-ups when using complex models on fully homomorphic encrypted data.",1.1. Our contributions,[0],[0]
"In order to achieve these speed-ups, we propose several methods to modify the training and design of neural networks, as well as algorithmic tricks to parallelize and accelerate computation on encrypted data:
• We propose two types of circuits for performing inner products between unencrypted and encrypted data: reduce tree circuits and sorting networks.",1.1. Our contributions,[0],[0]
"We give a runtime comparison of each method.
",1.1. Our contributions,[0],[0]
"• We introduce an easy trick, which we call the +1 trick to sparsify encrypted computations.
",1.1. Our contributions,[0],[0]
"• We demonstrate that our techniques are easily parallelizable and we report timing for a variety of computation settings on real world datasets, alongside classification accuracies.",1.1. Our contributions,[0],[0]
In this section we describe our Encrypted Prediction as a Service (EPAAS) paradigm.,2. Encrypted Prediction as a Service,[0],[0]
We then detail our privacy and computational guarantees.,2. Encrypted Prediction as a Service,[0],[0]
"Finally, we discuss how different related work is suited to this paradigm and propose a solution.
",2. Encrypted Prediction as a Service,[0],[0]
"In the EPAAS setting we have any number of clients, say C1, . . .",2. Encrypted Prediction as a Service,[0],[0]
", Cn that have data x1, . . .",2. Encrypted Prediction as a Service,[0],[0]
",xn.",2. Encrypted Prediction as a Service,[0],[0]
The clients would like to use a highly-accurate model f provided by a server S to predict some outcome.,2. Encrypted Prediction as a Service,[0],[0]
"In cases where data x is not sensitive there are already many solutions for this such as BigML, Wise.io, Google Cloud AI, Amazon Machine Learning, among others.",2. Encrypted Prediction as a Service,[0],[0]
"However, if the data is sensitive so that the clients would be uncomfortable giving the raw data to the server, none of these systems can offer the client a prediction.",2. Encrypted Prediction as a Service,[0],[0]
"If data x is sensitive (e.g., x may be the health record of client C, and f(x) may be the likelihood of heart disease), then we would like to have the following privacy guarantees:
P1.",2.1. Privacy and computational guarantees,[0],[0]
"Neither the server S, or any other party, learn anything about client data x, other than its size (privacy of the data).
P2.",2.1. Privacy and computational guarantees,[0],[0]
"Neither the client C, or any other party, learn anything about model f , other than the prediction f(x) given client data x (and whatever can be deduced from it) (privacy of the model).
",2.1. Privacy and computational guarantees,[0],[0]
"Further, the main attraction of EPAAS is that the client is involved as little as possible.",2.1. Privacy and computational guarantees,[0],[0]
"More concretely, we wish to have the following computational guarantees:
C1.",2.1. Privacy and computational guarantees,[0],[0]
"No external party is involved in the computation.
",2.1. Privacy and computational guarantees,[0],[0]
C2.,2.1. Privacy and computational guarantees,[0],[0]
"The rounds of communication between client and server should be limited to 2 (send data & receive prediction).
",2.1. Privacy and computational guarantees,[0],[0]
C3.,2.1. Privacy and computational guarantees,[0],[0]
Communication and computation at the client side should be independent of model f .,2.1. Privacy and computational guarantees,[0],[0]
"In particular, (i) the server should be able to update f without communicating with any client, and (ii) clients should not need to be online during the computation of f(x).
",2.1. Privacy and computational guarantees,[0],[0]
Note that these requirements rule out protocols with preprocessing stages or that involve third parties.,2.1. Privacy and computational guarantees,[0],[0]
"Generally speaking, a satisfactory solution based on FHE would proceed as follows: (1) a client generates encryption parameters, encrypts their data x using the private key, and sends the resulting encryption x̃, as well as the public key to the server.",2.1. Privacy and computational guarantees,[0],[0]
"(2) The server evaluates f on x̃ leveraging the homomorphic properties of the encryption, to obtain an encryption f̃(x) without learning anything whatsoever about x, and sends f̃(x) to the client.",2.1. Privacy and computational guarantees,[0],[0]
"(3) Finally, the client decrypts and recovers the prediction f(x) in the clear.",2.1. Privacy and computational guarantees,[0],[0]
A high level depiction of these steps is shown in Figure 1.,2.1. Privacy and computational guarantees,[0],[0]
Table 1 describes whether prior work satisfy the above privacy and computational guarantees.,2.2. Existing approaches,[0],[0]
"First, note that Cryptonets (Gilad-Bachrach et al., 2016) violates C3(i) and P2.",2.2. Existing approaches,[0],[0]
"This is because the clients would have to generate parameters for the encryption according to the structure of f , so we are able to make inferences about the model (violating P2) and the client is not allowed to change the model f without telling the client (violating C3(i)).",2.2. Existing approaches,[0],[0]
The same holds for the work of Chabanne et al. (2017).,2.2. Existing approaches,[0],[0]
"The approach of Bourse et al. (2017) requires the server to calibrate the parameters of the encryption scheme according to the magnitude of intermediate values, thus C3(i) is not necessarily satisfied.",2.2. Existing approaches,[0],[0]
"Closely related to our work is that of Meehan et al. (2018)
which satisfies our privacy and computational requirements.",2.2. Existing approaches,[0],[0]
"We will show that our method is significantly faster than this method, with very little sacrifice in accuracy.
",2.2. Existing approaches,[0],[0]
Multi-Party Computation (MPC).,2.2. Existing approaches,[0],[0]
"It is important to distinguish between approaches purely based on homomorphic encryption (described above), and those involving MultiParty Computation (MPC) techniques, such as (Mohassel & Zhang, 2017; Liu et al., 2017; Rouhani et al., 2017; Riazi et al., 2017; Chase et al.;",2.2. Existing approaches,[0],[0]
"Juvekar et al., 2018).",2.2. Existing approaches,[0],[0]
"While generally MPC approaches are faster, they crucially rely on all parties being involved in the whole computation, which is in conflict with requirement C3(ii).",2.2. Existing approaches,[0],[0]
"Additionally, in MPC the structure of the computation is public to both parties, which means that the server would have to communicate basic information such as the number of layers of f .",2.2. Existing approaches,[0],[0]
"This is conflict with requirements P1, C2, and C3(i).
",2.2. Existing approaches,[0],[0]
"In this work, we propose to use a very tailored homomorphic encryption technique to guarantee all privacy and computational requirements.",2.2. Existing approaches,[0],[0]
In the next section we give background on homomorphic encryption.,2.2. Existing approaches,[0],[0]
"Further, we motivate the encryption protocol and the machine learning model class we use to satisfy all guarantees.",2.2. Existing approaches,[0],[0]
All cryptosystems define two functions: 1. an encryption function E(·) that maps data (often called plaintexts) to encrypted data (ciphertexts); 2.,3. Background,[0],[0]
a decryption function D(·) that maps ciphertexts back to plaintexts.,3. Background,[0],[0]
"In public-key cryptosystems, to evaluate the encryption function E , one needs to hold a public key kPUB, so the encryption of data x is E(x, kPUB).",3. Background,[0],[0]
"Similarly, to compute the decryption function D(·) one needs to hold a secret key kSEC which allows us to recover: D(E(x, kPUB), kSEC)",3. Background,[0],[0]
"= x.
A cryptosystem is homomorphic in some operation if it is possible to perform another (possibly different) operation such that: E(x, kPUB) E(x, kPUB) = E(x y, kPUB).",3. Background,[0],[0]
"Finally, in this work we assume all data to be binary ∈ {0, 1}.",3. Background,[0],[0]
"For more detailed background on FHE beyond what is described below, see the excellent tutorial of Halevi (2017).",3. Background,[0],[0]
"In 1978, cryptographers posed the question: Does an encryption scheme exist that allows one to perform arbitrary computations on encrypted data?",3.1. Fully Homomorphic Encryption,[0],[0]
"The implications of this, called a Fully homomorphic encryption (FHE) scheme, would enable clients to send computations to the cloud while retaining control over the secrecy of their data.",3.1. Fully Homomorphic Encryption,[0],[0]
This was still an open problem however 30 years later.,3.1. Fully Homomorphic Encryption,[0],[0]
"Then, in 2009, a cryptosystem (Gentry, 2009) was devised that could, in principle, perform such computations on encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"Similar to previous approaches, in each computation, noise is introduced into the encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"And after a certain number of computations, the noise grows too large so that the encryptions can no longer be decrypted.",3.1. Fully Homomorphic Encryption,[0],[0]
"The key innovation was a technique called bootstrapping, which allows one to reduce the noise to its original level without decrypting.",3.1. Fully Homomorphic Encryption,[0],[0]
"That result constituted a massive breakthrough, as it established, for the first time, a fully homomorphic encryption scheme (Gentry, 2009).",3.1. Fully Homomorphic Encryption,[0],[0]
"Unfortunately, the original bootstrapping procedure was highly impractical.",3.1. Fully Homomorphic Encryption,[0],[0]
"Consequently, much of the research since the first FHE scheme has been devoted to reducing the growth of noise so that the scheme never has to perform bootstrapping.",3.1. Fully Homomorphic Encryption,[0],[0]
"Indeed, even in recent FHE schemes bootstrapping is slow (roughly six minutes in a highly-optimized implementation of a recent popular scheme (Halevi & Shoup, 2015)) and bootstrapping many times increases the memory requirements of encrypted data.",3.1. Fully Homomorphic Encryption,[0],[0]
"Thus, one common technique to implement encrypted prediction was to take an existing ML algorithm and approximate it with as few operations as possible, in order to never have to bootstrap.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
"This involved careful parameter tuning to ensure that the security of the encryption scheme was sufficient, that it didn’t require too much memory, and that it ran in a reasonable amount of time.",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
"One prominent example of this is Cryptonets (Gilad-Bachrach et al., 2016).",3.1.1. ENCRYPTED PREDICTION WITH LEVELED HE,[0],[0]
Recent developments in cryptography call for rethinking this approach.,3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
Ducas & Micciancio (2015) devised a scheme that that could bootstrap a single Boolean gate in under one second with reduced memory.,3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"Recently, Chillotti et al. (2016) introduced optimizations implemented in the TFHE library, which further reduced bootstrapping of to under 0.1 seconds.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"In this paper, we demonstrate that this change has a huge impact on designing encrypted machine learning algorithms.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"Specifically, encrypted computation is now modular: the cost of adding a few layers to an encrypted neural network is simply the added cost of each layer in isolation.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"This is particularly important as recent developments in deep learning such as Residual Networks (He
et al., 2016) and Dense Networks (Huang et al., 2017) have shown that networks with many layers are crucial to achieve state-of-the-art accuracy.",3.1.2. ENCRYPTED PREDICTION WITH FHE,[0],[0]
"The cryptosystem that we will use in this paper, TFHE, is however restricted to computing binary operations.",3.2. Binary Neural Networks,[0],[0]
"We note that, concurrent to the work that led to TFHE, was the development of neural network models that perform binary operations between binary weights and binary activations.",3.2. Binary Neural Networks,[0],[0]
"These models, called Binary Neural Networks (BNNs), were first devised by Kim & Smaragdis (2015); Courbariaux et al. (2016), and were motivated by the prospect of training and testing deep models on limited memory and limited compute devices, such as mobile phones.
",3.2. Binary Neural Networks,[0],[0]
Technical details.,3.2. Binary Neural Networks,[0],[0]
We now describe the technical details of binary networks that we will aim to replicate on encrypted data.,3.2. Binary Neural Networks,[0],[0]
"In a Binary Neural Network (BNN) every layer maps a binary input x ∈ {−1, 1}d to a binary output z ∈ {−1, 1}p using a set of binary weights W ∈ {−1, 1}(p,d) and a binary activation function sign(·) that is 1 if x ≥ 0 and −1 otherwise.",3.2. Binary Neural Networks,[0],[0]
"Although binary nets don’t typically use a bias term, applying batch-normalization (Ioffe & Szegedy, 2015) when evaluating the model it means that a bias term b ∈",3.2. Binary Neural Networks,[0],[0]
Zp may need to be added before applying the activation function (cf. Sec. 4.1.2).,3.2. Binary Neural Networks,[0],[0]
"Thus, when evaluating the model, a fully connected layer in a BNN implements the following transformation z",3.2. Binary Neural Networks,[0],[0]
:= sign(Wx + b).,3.2. Binary Neural Networks,[0],[0]
"From now on we will call all data represented as {−1, 1} non-standard binary and data represented as {0, 1} as binary.",3.2. Binary Neural Networks,[0],[0]
"Kim & Smaragdis (2015); Courbariaux et al. (2016) were the first to note that the above inner product nonlinearity in BNNs could be implemented using the following steps:
1.",3.2. Binary Neural Networks,[0],[0]
"Transform data and weights from non-standard binary to binary: w,x→ w,x by replacing −1 with 0. n
2.",3.2. Binary Neural Networks,[0],[0]
"Element-wise multiply by applying the logical operator XNOR(w,x) for each element of w and x.
3.",3.2. Binary Neural Networks,[0],[0]
"Sum result of previous step by using popcount operation (which counts the number of 1s), call this S.
4.",3.2. Binary Neural Networks,[0],[0]
"If the bias term is b, check if 2S ≥",3.2. Binary Neural Networks,[0],[0]
d,3.2. Binary Neural Networks,[0],[0]
"− b, if so the activation is positive and return 1, otherwise return −1.
",3.2. Binary Neural Networks,[0],[0]
"Thus we have that,
zi = sign(2 · popcount(XNOR(wi,x))− d + b)
",3.2. Binary Neural Networks,[0],[0]
Related binary models.,3.2. Binary Neural Networks,[0],[0]
"Since the initial work on BNNs there has been a wealth of work on binarizing, ternarizing, and quantizing neural networks Chen et al. (2015); Courbariaux et al. (2015); Han et al. (2016); Hubara et al. (2016);
Zhu et al. (2016); Chabanne et al. (2017); Chen et al. (2017).",3.2. Binary Neural Networks,[0],[0]
Our approach is currently tailored to methods that have binary activations and we leave the implementation of these methods on encrypted data for future work.,3.2. Binary Neural Networks,[0],[0]
"In this work, we make the observation that BNNs can be run on encrypted data by designing circuits in TFHE for computing their operations.",4. Methods,[0],[0]
In this section we consider Boolean circuits that operate on encrypted data and unencrypted weights and biases.,4. Methods,[0],[0]
"We show how these circuits allow us to efficiently implement the three main layers of binary neural networks: fully connected, convolutional, and batch-normalization.",4. Methods,[0],[0]
We then show how a simple trick allows us to sparsify our computations.,4. Methods,[0],[0]
Our techniques can be easily parallelized.,4. Methods,[0],[0]
"During the evaluation of a circuit, gates at the same level in the tree representation of the circuit can be evaluated in parallel.",4. Methods,[0],[0]
"Hence, when implementing a function, “shallow” circuits are preferred in terms of parallelization.",4. Methods,[0],[0]
While parallel computation was often used to justify employing second generation FHE techniques— where parallelization comes from ciphertext packing—we show in the following section that our techniques create dramatic speedups for a state-of-the-art FHE technique.,4. Methods,[0],[0]
We emphasize that a key challenge is that we need to use data oblivious algorithms (circuits) when dealing with encrypted data as the algorithm never discovers the actual value of any query made on the data.,4. Methods,[0],[0]
The three primary circuits we need are for the following tasks: 1. computing the inner product; 2. computing the binary activation function (described in the previous section) and; 3. dealing with the bias.,4.1. Binary OPs,[0],[0]
"As described in the previous section, BNNs can speed up an inner product by computing XNORs (for element-wise multiplication) followed by a POPCOUNT (for summing).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In our case, we compute an inner product of size d by computing XNORs element-wise between d bits of encrypted data and d bits of unencrypted data, which results in an encrypted d bit output.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"To sum this output, the POPCOUNT operation is useful when weights and data are unencrypted because POPCOUNT is implemented in the instruction set of Intel and AMD processors, but when dealing with encrypted data we simply resort to using shallow circuits.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"We consider two circuits for summation, both with sublinear depth: a reduce tree adder and a sorting network.
",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Reduce tree adder.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"We implement the sum using a binary tree of half and ripple-carry (RC) adders organized into a reduction tree, as shown in Figure 2 (Left).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"All these structures can be implemented to run on encrypted data because TFHE allows us to compute XNOR, AND, and OR on encrypted data.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The final number returned by the reduction tree S̃ is the binary representation of the number of 1s resulting from the XNOR, just like POPCOUNT.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Thus, to compute the BNN activation function sign(·) we need to check whether 2S̃ ≥ d− b, where d is the number of bits in S̃ and b is the bias.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Note that if the bias is zero we simply need to check if S̃ ≥ d/2.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
To do so we can simply return the second-to-last bit of S̃. If it is 1 then S̃ is at least d/2.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"If the bias b is non-zero (because of batch-normalization, described in Section 4.1.2), we can implement a circuit to perform the check 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The bias b (which is available in the clear) may be an integer as large as S̃. Let B[(d − b)/2], B[S̃] be the binary representations of b and S̃. Algorithm 1 describes a comparator circuit that returns an encrypted value of 1 if the above condition holds and (encrypted) 0",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"otherwise (where MUX(s, a, b) returns a if s = 1 and b otherwise).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"As encrypted operations dominate
the running time of our computation, in practice this computation essentially corresponds to evaluating d MUX gates.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"This gate has a dedicated implementation in TFHE, which results in a very efficient comparator in our setting.
",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Algorithm 1 Comparator Inputs: Encrypted B[S̃], unencrypted B[(d− b)/2], size d of B[(d− b)/2],B[S̃]",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Output: Result of 2S̃ ≥ d− b
1: o = 0 2: for i = 1, . . .",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
", d do 3: if B[(d− b)/2]i = 0",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"then 4: o = MUX(B[S̃]i, 1̃, o) 5: else 6: o = MUX(B[S̃]i, o, 0̃) 7: end if 8: end for 9: Return: o
Sorting network.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
We do not technically care about the sum of the result of the element-wise XNOR between w̄ and x̄.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In fact, all we care about is if the result of the comparison: 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
− b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Thus, another idea is to take the output of the (bitwise) XNOR and sort it.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Although this sorting needs to be performed over encrypted data, the rest of the computation does not require any homomorphic operations; after sorting we hold a sequence of encrypted 1s, followed by encrypted 0s.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"To output the correct value, we only need to select one the (encrypted) bit in the correct position and return it.",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"If b = 0 we can simply return the encryption of the central bit in the sequence; indeed, if the central bit is 1, then there are more 1s than 0s and thus 2S̃ ≥ d",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
and we return 1.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
If b 6= 0,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
we need to offset the returned index by b in the correct direction depending on the sign of b.,4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"In order to sort the initial array we implement a sorting network, shown in Figure 2 (Right).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"The sorting network is a sequence of swap gates between individuals bits, where SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"(OR(a, b), AND(a, b)).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"Note that if a ≥ b then SWAP(a, b) =",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"(a, b), and otherwise is (b, a).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
"More specifically, we implement Batcher’s sorting network (Batcher, 1968), which consists of O(n log2(n)) swap gates, and has depth O(log2(n)).",4.1.1. ENCRYPTED INNER PRODUCT,[0],[0]
Batch normalization is mainly used during training; however during evaluating a model this requires us scale and translate and scale the input (which is the output of the previous layer).,4.1.2. BATCH NORMALIZATION,[0],[0]
"In practice, when our activation function is the sign function, this only means that we need to update the bias term (the actual change to the bias term is an elementary calculation).",4.1.2. BATCH NORMALIZATION,[0],[0]
"As our circuits are designed to work with a bias term, and the scaling and translation factors are available as
plaintext (as they are part of the model), this operation is easily implemented during test time.",4.1.2. BATCH NORMALIZATION,[0],[0]
"Since we have access to W ∈ {−1, 1}p×d and the bias term b ∈ Zp in the clear (only data x and subsequent activations are encrypted), we can exploit the fact that W always has values±1 to roughly halve the cost computation.",4.2. Sparsification via “+1”-trick,[0],[0]
"We consider w ∈ {−1, 1}d which is a single row of W and observe that:
w>x = (1 + w)>(1 + x)− ∑ i wi − (1 + x)>1,
where 1 denotes the vector in which every entry is 1.",4.2. Sparsification via “+1”-trick,[0],[0]
"Further note that (1 + w) ∈ {0, 2}d which means that the product (1+w)>(1+x) is simply the quantity 4 ∑ i:wi=1
x̄i, where x̄ refers to the standard binary representation of the nonstandard binary x.",4.2. Sparsification via “+1”-trick,[0],[0]
"Assuming at most half of the wis were originally +1, if w ∈ {−1, 1}d, only d/2 encrypted values need be added.",4.2. Sparsification via “+1”-trick,[0],[0]
"We also need to compute the encrypted sum ∑ i xi; however, this latter sum need only be computed once, no matter how many output units the layer has.",4.2. Sparsification via “+1”-trick,[0],[0]
"Thus, this small bit of extra overhead roughly halves the amount of computation required.",4.2. Sparsification via “+1”-trick,[0],[0]
"We note that if w has more −1s than +1s, w>x can be computed using (1−w) and (1− x) instead.",4.2. Sparsification via “+1”-trick,[0],[0]
This guarantees that we never need to sum more than half the inputs for any output unit.,4.2. Sparsification via “+1”-trick,[0],[0]
The sums of encrypted binary values can be calculated as described in Sec. 4.1.,4.2. Sparsification via “+1”-trick,[0],[0]
"The overheads are two additions required to compute (1+x)>1 and (1 − x)>1, and then a subtraction of two log(d)-bit long encrypted numbers.",4.2. Sparsification via “+1”-trick,[0],[0]
"(The multiplication by 2 or 4 as may be sometimes required is essentially free, as bit shifts correspond to dropping bits, and hence do not require homomorphic operations).",4.2. Sparsification via “+1”-trick,[0],[0]
"As our experimental results show this simple trick roughly halves the computation time of one
layer; the actual savings appear to be even more than half as in many instances the number of elements we need to sum over is significantly smaller than half.
",4.2. Sparsification via “+1”-trick,[0],[0]
It is worth emphasizing the advantage for binarizing and then using the above approach to making the sums sparse.,4.2. Sparsification via “+1”-trick,[0],[0]
"By default, units in a neural network compute an affine function to which an activation function is subsequently applied.",4.2. Sparsification via “+1”-trick,[0],[0]
The affine map involves an inner product which involves d multiplications.,4.2. Sparsification via “+1”-trick,[0],[0]
Multiplication under fully homomorphic encryption schemes is however significantly more expensive than addition.,4.2. Sparsification via “+1”-trick,[0],[0]
"By binarizing and applying the above calculation, we’ve replaced the inner product operation by selection (which is done in the clear as W is available in plaintext) and (encrypted) addition.",4.2. Sparsification via “+1”-trick,[0],[0]
"Ternary neural networks use weights in {−1, 0, 1} rather than {−1, 1}; this can alternatively be viewed as dropping connections from a BNN.",4.3. Ternarization (Weight Dropping),[0],[0]
"Using ternary neural networks rather than binary reduces the computation time as encrypted inputs for which the corresponding wi is 0 can be safely dropped from the computation, before the method explained in section 4.2 is applied to the remaining elements.",4.3. Ternarization (Weight Dropping),[0],[0]
Our experimental results show that a binary network can be ternarized to maintain the same level of test accuracy with roughly a quarter of the weights being 0,4.3. Ternarization (Weight Dropping),[0],[0]
(cf. Sec. 5.4).,4.3. Ternarization (Weight Dropping),[0],[0]
In this section we report encrypted binary neural network prediction experiments on a number of real-world datasets.,5. Experimental Results,[0],[0]
"We begin by comparing the efficiency of the two circuits used for inner product, the reduce tree and the sorting network.",5. Experimental Results,[0],[0]
We then describe the datasets and the architecture of the BNNs used for classification.,5. Experimental Results,[0],[0]
"We report the classification timings of these BNNs for each dataset, for different computational settings.",5. Experimental Results,[0],[0]
"Finally, we give accuracies of the BNNs compared to floating point networks.",5. Experimental Results,[0],[0]
"Our code is freely available at (tap, 2018).",5. Experimental Results,[0],[0]
"We show timings of reduce tree and sorting network for different number of input bits, with and without parallelization in Figure 3 (parallelization is over 16 CPUs).",5.1. Reduce tree vs. sorting network,[0],[0]
We notice that the reduce tree is strictly better when comparing parallel or non-parallel timings of the circuits.,5.1. Reduce tree vs. sorting network,[0],[0]
"As such, from now on we use the reduce tree circuit for inner product.
",5.1. Reduce tree vs. sorting network,[0],[0]
"It should be mentioned that at the outset this result was not obvious because while sorting networks have more levels of computation, they have fewer gates.",5.1. Reduce tree vs. sorting network,[0],[0]
"Specifically, the sorting network used for encrypted sorting is the bitonic sorting network which for n bits has O(log2 n) levels of computa-
tion whereas the reduce tree only has O(log n) levels.",5.1. Reduce tree vs. sorting network,[0],[0]
"On the other hand, the reduce tree requires 2 gates for each half adder and 5k gates for each k-bit RC adder, whereas a sorting network only requires 2 gates per SWAP operation.",5.1. Reduce tree vs. sorting network,[0],[0]
"Another factor that may slow down sorting networks is that is that our implementation of sorting networks is recursive, whereas the reduce tree is iterative.",5.1. Reduce tree vs. sorting network,[0],[0]
"We evaluate on four datasets, three of which have privacy implications due to health care information (datasets Cancer and Diabetes) or applications in surveillance (dataset Faces).",5.2. Datasets,[0],[0]
"We also evaluate on the standard benchmark MNIST dataset.
Cancer.",5.2. Datasets,[0],[0]
The Cancer dataset1 contains 569 data points where each point has 30 real-valued features.,5.2. Datasets,[0],[0]
The task is to predict whether a tumor is malignant (cancerous) or benign.,5.2. Datasets,[0],[0]
Similar to Meehan et al. (2018) we divide the dataset into a training set and a test in a 70 : 30 ratio.,5.2. Datasets,[0],[0]
"For every real-valued feature, we divide the range of each feature into three equal-spaced bins and one-hot encode each feature by its bin-membership.",5.2. Datasets,[0],[0]
This creates a 90-dimensional binary vector for each example.,5.2. Datasets,[0],[0]
"We use a single fully connected layer 90→ 1 followed by a batch normalization layer, as is common practice for BNNs (Courbariaux et al., 2016).
",5.2. Datasets,[0],[0]
Diabetes.,5.2. Datasets,[0],[0]
This dataset2 contains data on 100000 patients with diabetes.,5.2. Datasets,[0],[0]
The task is to predict one of three possible labels regarding hospital readmission after release.,5.2. Datasets,[0],[0]
We divide patients into a 80/20 train/test split.,5.2. Datasets,[0],[0]
"As this dataset contains real and categorical features, we bin them as in the Cancer dataset.",5.2. Datasets,[0],[0]
We obtain a 1704 dimensional binary data point for each entry.,5.2. Datasets,[0],[0]
"Our network (selected by cross validation) consists of a fully connected layer 1704→ 10, a batch normalization layer, a SIGN activation function, followed by another fully connected layer 10→ 3, and a batch normalization layer.
",5.2. Datasets,[0],[0]
Faces.,5.2. Datasets,[0],[0]
The Labeled Faces in the Wild-a dataset contains 13233 gray-scale face images.,5.2. Datasets,[0],[0]
We use the binary classification task of gender identification from the images.,5.2. Datasets,[0],[0]
We resize the images to size 50 × 50.,5.2. Datasets,[0],[0]
"Our network architecture (selected by cross-validation) contains 5 convolutional layers, each of which is followed by a batch normalization layer and a SIGN activation function (except the last which has no activation).",5.2. Datasets,[0],[0]
All convolutional layers have unit stride and filter dimensions 10 × 10.,5.2. Datasets,[0],[0]
All layers except the last layer have 32 output channels (the last has a single output channel).,5.2. Datasets,[0],[0]
"The output is flattened and passed through a fully connected layer 25→ 1 and a batch normalization layer.
",5.2. Datasets,[0],[0]
1https://tinyurl.com/gl3yhzb 2https://tinyurl.com/m6upj7y,5.2. Datasets,[0],[0]
"in Table 2 (computed with Intel Xeon CPUs @ 2.40GHz, processor number E5-2673V3).",5.3. Timing,[0],[0]
"We notice that without parallelization over BNN outputs, the predictions on datasets which use fully connected layers: Cancer and Diabetes, finish within seconds or minutes.",5.3. Timing,[0],[0]
"While the for the datasets that use convolutional layers: Faces and MNIST, predictions require multiple days.",5.3. Timing,[0],[0]
The +1-trick cuts the time of MNIST prediction by half and reduces the time of Faces prediction by 200 hours.,5.3. Timing,[0],[0]
"With only a bit of parallelism over outputs (Out 16-Parallel) prediction on the Faces dataset now requires less than 1.5 days and MNIST can be done
3https://tinyurl.com/yc8d79oe
in 2 hours.",5.3. Timing,[0],[0]
With complete parallelism (Out N-Parallel) all methods reduce to under 2 hours.,5.3. Timing,[0],[0]
We wanted to ensure that BNNs can still achieve similar test set accuracies to floating point networks.,5.4. Accuracy,[0],[0]
"To do so, for each dataset we construct similar floating point networks.",5.4. Accuracy,[0],[0]
"For the Cancer dataset we use the same network except we use the original 30 real-valued features, so the fully connected layer is 30 → 1, as was used in Meehan et al. (2018).",5.4. Accuracy,[0],[0]
"For Diabetes and Faces, just like for our BNNs we cross validate to find the best networks (for Faces: 4 convolutional layers, with filter sizes of 5× 5 and 64 output channels; for Diabetes the best network is the same as used in the BNN).",5.4. Accuracy,[0],[0]
"For MNIST we report the accuracy of the best performing method (Wan et al., 2013) as reported4.",5.4. Accuracy,[0],[0]
"Additionally, we report the accuracy of the weight-dropping method described in Section 4.",5.4. Accuracy,[0],[0]
"The results are shown in
Table 3.",5.4. Accuracy,[0],[0]
"We notice that apart from the Faces dataset, the accuracies differ between the floating point networks and BNNs by at most 1.2% (on MNIST).",5.4. Accuracy,[0],[0]
The face dataset uses a different network in floating point which seems to be able to exploit the increased precision to increase accuracy by 5.1%.,5.4. Accuracy,[0],[0]
We also observe that weight dropping by 10% reduces the accuracy by at most 1.2% (on Faces).,5.4. Accuracy,[0],[0]
"Dropping 20% of the weights seem to have small effect on all datasets except Cancer, which has only a single layer and so likely relies more on every individual weight.",5.4. Accuracy,[0],[0]
"In this work, we devised a set of techniques that allow for practical Encrypted Prediction as a Service.",6. Conclusion,[0],[0]
"In future work, we aim to develop techniques for encrypting non-binary quantized neural networks, and well as design methods for encrypted model training.
4https://tinyurl.com/knn2434",6. Conclusion,[0],[0]
The authors would like to thank Nick Barlow and Oliver Strickson for their support in using the SHEEP platform.,Acknowledgments,[0],[0]
"AS acknowledges support from The Alan Turing Institute under the Turing Doctoral Studentship grant TU/C/000023. AG, MK, and VK were supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.",Acknowledgments,[0],[0]
Machine learning methods are widely used for a variety of prediction problems.,abstractText,[0],[0]
Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients.,abstractText,[0],[0]
"However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed.",abstractText,[0],[0]
Equally important is to minimize the amount of computation and communication required between client and server.,abstractText,[0],[0]
"Fully homomorphic encryption offers a possible way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations.",abstractText,[0],[0]
The main drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data.,abstractText,[0],[0]
"We combine ideas from the machine learning literature, particularly work on binarization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.",abstractText,[0],[0]
TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 957–967 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
957",text,[0],[0]
"Aspect sentiment classification (ASC) is a core problem of sentiment analysis (Liu, 2012).",1 Introduction,[0],[0]
"Given an aspect and a sentence containing the aspect, ASC classifies the sentiment polarity expressed in the sentence about the aspect, namely, positive, neutral, or negative.",1 Introduction,[0],[0]
"Aspects are also called opinion targets (or simply targets), which are usually product/service features in customer reviews.",1 Introduction,[0],[0]
"In this paper, we use aspect and target interchangeably.",1 Introduction,[0],[0]
"In practice, aspects can be specified by the user or extracted automatically using an aspect extraction technique (Liu, 2012).",1 Introduction,[0],[0]
"In this work, we assume the aspect terms are given and only focus on the classification task.
",1 Introduction,[0],[0]
"Due to their impressive results in many NLP tasks (Deng et al., 2014), neural networks have been applied to ASC (see the survey (Zhang et al., 2018)).",1 Introduction,[0],[0]
"Memory networks (MNs), a type of neural networks which were first proposed for question answering (Weston et al., 2015; Sukhbaatar et al., 2015), have achieved the state-of-the-art results in ASC (Tang et al., 2016).",1 Introduction,[0],[0]
A key factor for their success is the attention mechanism.,1 Introduction,[0],[0]
"However, we found that using existing MNs to deal with ASC has an important problem and simply relying on attention modeling cannot solve it.",1 Introduction,[0],[0]
"That is, their performance degrades when the sentiment of a context word is sensitive to the given target.
",1 Introduction,[0],[0]
"Let us consider the following sentences:
(1)",1 Introduction,[0],[0]
The screen resolution is excellent but the price is ridiculous.,1 Introduction,[0],[0]
(2) The screen resolution is excellent but the price is high.,1 Introduction,[0],[0]
(3) The price is high.,1 Introduction,[0],[0]
"(4) The screen resolution is high.
",1 Introduction,[0],[0]
"In sentence (1), the sentiment expressed on aspect screen resolution (or resolution for short) is positive, whereas the sentiment on aspect price is negative.",1 Introduction,[0],[0]
"For the sake of predicting correct sentiment, a crucial step is to first detect the sentiment context about the given aspect/target.",1 Introduction,[0],[0]
We call this step targeted-context detection.,1 Introduction,[0],[0]
Memory networks (MNs) can deal with this step quite well because the sentiment context of a given aspect can be captured by the internal attention mechanism in MNs.,1 Introduction,[0],[0]
"Concretely, in sentence (1) the word “excellent” can be identified as the sentiment context when resolution is specified.",1 Introduction,[0],[0]
"Likewise, the context word “ridiculous” will be placed with a high attention when price is the target.",1 Introduction,[0],[0]
"With the correct targeted-context detected, a trained MN, which recognizes “excellent” as positive sentiment and “ridiculous” as negative sentiment, will infer correct sentiment polarity for the given target.",1 Introduction,[0],[0]
"This
is relatively easy as “excellent” and “ridiculous” are both target-independent sentiment words, i.e., the words themselves already indicate clear sentiments.
",1 Introduction,[0],[0]
"As illustrated above, the attention mechanism addressing the targeted-context detection problem is very useful for ASC, and it helps classify many sentences like sentence (1) accurately.",1 Introduction,[0],[0]
This also led to existing and potential research in improving attention modeling (discussed in Section 5).,1 Introduction,[0],[0]
"However, we observed that simply focusing on tackling the target-context detection problem and learning better attention are not sufficient to solve the problem found in sentences (2), (3) and (4).
",1 Introduction,[0],[0]
Sentence (2) is similar to sentence (1) except that the (sentiment) context modifying aspect/target price is “high”.,1 Introduction,[0],[0]
"In this case, when “high” is assigned the correct attention for the aspect price, the model also needs to capture the sentiment interaction between “high” and price in order to identify the correct sentiment polarity.",1 Introduction,[0],[0]
This is not as easy as sentence (1) because “high” itself indicates no clear sentiment.,1 Introduction,[0],[0]
"Instead, its sentiment polarity is dependent on the given target.
",1 Introduction,[0],[0]
"Looking at sentences (3) and (4), we further see the importance of this problem and also why relying on attention mechanism alone is insufficient.",1 Introduction,[0],[0]
"In these two sentences, sentiment contexts are both “high” (i.e., same attention), but sentence (3) is negative and sentence (4) is positive simply because their target aspects are different.",1 Introduction,[0],[0]
"Therefore, focusing on improving attention will not help in these cases.",1 Introduction,[0],[0]
"We will give a theoretical insight about this problem with MNs in Section 3.
",1 Introduction,[0],[0]
"In this work, we aim to solve this problem.",1 Introduction,[0],[0]
"To distinguish it from the aforementioned targetedcontext detection problem as shown by sentence (1), we refer to the problem in (2), (3) and (4) as the target-sensitive sentiment (or target-dependent sentiment) problem, which means that the sentiment polarity of a detected/attended context word is conditioned on the target and cannot be directly inferred from the context word alone, unlike “excellent” and “ridiculous”.",1 Introduction,[0],[0]
"To address this problem, we propose target-sensitive memory networks (TMNs), which can capture the sentiment interaction between targets and contexts.",1 Introduction,[0],[0]
We present several approaches to implementing TMNs and experimentally evaluate their effectiveness.,1 Introduction,[0],[0]
"This section describes our basic memory network for ASC, also as a background knowledge.",2 Memory Network for ASC,[0],[0]
"It does not include the proposed target-sensitive sentiment solutions, which are introduced in Section 4.",2 Memory Network for ASC,[0],[0]
"The model design follows previous studies (Sukhbaatar et al., 2015; Tang et al., 2016) except that a different attention alignment function is used (shown in Eq. 1).",2 Memory Network for ASC,[0],[0]
Their original models will be compared in our experiments as well.,2 Memory Network for ASC,[0],[0]
"The definitions of related notations are given in Table 1.
",2 Memory Network for ASC,[0],[0]
Input Representation:,2 Memory Network for ASC,[0],[0]
"Given a target aspect t, an embedding matrix A is used to convert t into a vector representation, vt (vt = At).",2 Memory Network for ASC,[0],[0]
"Similarly, each context word (non-aspect word in a sentence) xi ∈ {x1, x2, ...xn} is also projected to the continuous space stored in memory, denoted by mi (mi = Axi) ∈ {m1,m2, ...mn}.",2 Memory Network for ASC,[0],[0]
Here n is the number of words in a sentence and i is the word position/index.,2 Memory Network for ASC,[0],[0]
Both t and xi are one-hot vectors.,2 Memory Network for ASC,[0],[0]
"For an aspect expression with multiple words, its aspect representation vt is the averaged vector of those words (Tang et al., 2016).
",2 Memory Network for ASC,[0],[0]
Attention:,2 Memory Network for ASC,[0],[0]
Attention can be obtained based on the above input representation.,2 Memory Network for ASC,[0],[0]
"Specifically, an attention weight αi for the context word xi is computed based on the alignment function:
αi = softmax(v T t Mmi) (1)
where M ∈ Rd×d is the general learning matrix suggested by Luong et al. (2015).",2 Memory Network for ASC,[0],[0]
"In this manner, attention α = {α1, α2, ..αn} is represented as a vector of probabilities, indicating the weight/importance of context words towards a given target.",2 Memory Network for ASC,[0],[0]
Note that αi ∈,2 Memory Network for ASC,[0],[0]
"(0, 1) and
∑ i αi = 1.
",2 Memory Network for ASC,[0],[0]
Output Representation: Another embedding matrixC is used for generating the individual (output) continuous vector ci (ci = Cxi) for each context word xi.,2 Memory Network for ASC,[0],[0]
"A final response/output vector o is produced by summing over these vectors weighted with the attention α, i.e., o =
∑ i αici.
",2 Memory Network for ASC,[0],[0]
"Sentiment Score (or Logit): The aspect sentiment scores (also called logits) for positive, neutral, and negative classes are then calculated, where a sentiment-specific weight matrix W ∈ RK×d is used.",2 Memory Network for ASC,[0],[0]
"The sentiment scores are represented in a vector s ∈ RK×1, whereK is the number of (sentiment) classes, which is 3 in ASC.
s =W (o+ vt) (2)
",2 Memory Network for ASC,[0],[0]
"The final sentiment probability y is produced with a softmax operation, i.e., y = softmax(s).",2 Memory Network for ASC,[0],[0]
This section analyzes the problem of targetsensitive sentiment in the above model.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
The analysis can be generalized to many existing MNs as long as their improvements are on attention α only.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We first expand the sentiment score calculation from Eq. 2 to its individual terms:
s =W (o+ vt) =W ( ∑ i αici + vt)
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"= α1Wc1 + α2Wc2 + ...αnWcn +Wvt
(3)
where “+” denotes element-wise summation.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"In Eq. 3, αiWci can be viewed as the individual sentiment logit for a context word and Wvt is the sentiment logit of an aspect.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
They are linearly combined to determine the final sentiment score s. This can be problematic in ASC.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"First, an aspect word often expresses no sentiment, for example, “screen”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"However, if the aspect term vt is simply removed from Eq. 3, it also causes the problem that the model cannot handle target-dependent sentiment.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For instance, the sentences (3) and (4) in Section 1 will then be treated as identical if their aspect words are not considered.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Second, if an aspect word is considered and it directly bears some positive or negative sentiment, then when an aspect word occurs with different context words for expressing opposite sentiments, a contradiction can be resulted from them, especially in the case that the context word is a target-sensitive sentiment word.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We explain it as follows.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Let us say we have two target words price and resolution (denoted as p and r).,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
We also have two possible context words “high” and “low” (denoted as h and l).,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"As these two sentiment words can modify both aspects, we can construct four snippets “high price”, “low price”, “high resolution” and “low resolution”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Their sentiments are negative, positive, positive, and negative respectively.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Let us set W to R1×d so that s becomes a 1-dimensional sentiment score indicator.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
s > 0 indicates a positive sentiment and s < 0 indicates a negative sentiment.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Based on the above example snippets or phrases we have four corresponding inequalities: (a) W (αhch + vp) < 0, (b) W (αlcl+ vp) > 0, (c) W (αhch+ vr) > 0",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
and (d) W (αlcl + vr) < 0.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We can drop all α terms here as they all equal to 1, i.e., they are the only context word in the snippets to attend to (the target words are not contexts).",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (a) and (b) we can infer (e),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wch < −Wvp,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
<,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wcl.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (c) and (d) we can infer (f),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
Wcl < −Wvr < Wch.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
From (e) and (f) we have (g),3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Wch < Wcl < Wch, which is a contradiction.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
This contradiction means that MNs cannot learn a set of parameters W and C to correctly classify the above four snippets/sentences at the same time.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
This contradiction also generalizes to realworld sentences.,3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"That is, although real-world review sentences are usually longer and contain more words, since the attention mechanism makes MNs focus on the most important sentiment context (the context with high αi scores), the problem is essentially the same.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, in sentences (2) and (3) in Section 1, when price is targeted, the main attention will be placed on “high”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For MNs, these situations are nearly the same as that for classifying the snippet “high price”.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"We will also show real examples in the experiment section.
",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"One may then ask whether improving attention can help address the problem, as αi can affect the final results by adjusting the sentiment effect of the context word via αiWci.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"This is unlikely, if not impossible.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"First, notice that αi is a scalar ranging in (0,1), which means it essentially assigns higher or lower weight to increase or decrease the sentiment effect of a context word.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"It cannot change the intrinsic sentiment orientation/polarity of the context, which is determined by Wci.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, if Wci assigns the context word “high” a positive sentiment (Wci > 0), αi will not make it negative (i.e., αiWci < 0 cannot be achieved by chang-
ing αi).",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"Second, other irrelevant/unimportant context words often carry no or little sentiment information, so increasing or decreasing their weights does not help.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"For example, in the sentence “the price is high”, adjusting the weights of context words “the” and “is” will neither help solve the problem nor be intuitive to do so.",3 Problem of the above Model for Target-Sensitive Sentiment,[0],[0]
"This section introduces six (6) alternative targetsensitive memory networks (TMNs), which all can deal with the target-sensitive sentiment problem.",4 The Proposed Approaches,[0],[0]
"Each of them has its characteristics.
",4 The Proposed Approaches,[0],[0]
Non-linear Projection (NP): This is the first approach that utilizes a non-linear projection to capture the interplay between an aspect and its context.,4 The Proposed Approaches,[0],[0]
"Instead of directly following the common linear combination as shown in Eq. 3, we use a non-linear projection (tanh) as the replacement to calculate the aspect-specific sentiment score.
",4 The Proposed Approaches,[0],[0]
s =W · tanh( ∑,4 The Proposed Approaches,[0],[0]
"i αici + vt) (4)
",4 The Proposed Approaches,[0],[0]
"As shown in Eq. 4, by applying a non-linear projection over attention-weighted ci and vt, the context and aspect information are coupled in a way that the final sentiment score cannot be obtained by simply summing their individual contributions (compared with Eq. 3).",4 The Proposed Approaches,[0],[0]
This technique is also intuitive in neural networks.,4 The Proposed Approaches,[0],[0]
"However, notice that by using the non-linear projection (or adding more sophisticated hidden layers) over them in this way, we sacrifice some interpretability.",4 The Proposed Approaches,[0],[0]
"For example, we may have difficulty in tracking how each individual context word (ci) affects the final sentiment score s, as all context and target representations are coupled.",4 The Proposed Approaches,[0],[0]
"To avoid this, we can use the following five alternative techniques.
",4 The Proposed Approaches,[0],[0]
"Contextual Non-linear Projection (CNP): Despite the fact that it also uses the non-linear projection, this approach incorporates the interplay between a context word and the given target into its (output) context representation.",4 The Proposed Approaches,[0],[0]
"We thus name it Contextual Non-linear Projection (CNP).
",4 The Proposed Approaches,[0],[0]
s,4 The Proposed Approaches,[0],[0]
"=W ∑ i αi · tanh(ci + vt) (5)
",4 The Proposed Approaches,[0],[0]
"From Eq. 5, we can see that this approach can keep the linearity of attention-weighted context aggregation while taking into account the aspect information with non-linear projection, which works
in a different way compared to NP.",4 The Proposed Approaches,[0],[0]
"If we define c̃i = tanh(ci + vt), c̃i can be viewed as the target-aware context representation of context xi and the final sentiment score is calculated based on the aggregation of such c̃i.",4 The Proposed Approaches,[0],[0]
"This could be a more reasonable way to carry the aspect information rather than simply summing the aspect representation (Eq. 3).
",4 The Proposed Approaches,[0],[0]
"However, one potential disadvantage is that this setting uses the same set of vector representations (learned by embeddings C) for multiple purposes, i.e., to learn output (context) representations and to capture the interplay between contexts and aspects.",4 The Proposed Approaches,[0],[0]
"This may degenerate its model performance when the computational layers in memory networks (called “hops”) are deep, because too much information is required to be encoded in such cases and a sole set of vectors may fail to capture all of it.
",4 The Proposed Approaches,[0],[0]
"To overcome this, we suggest the involvement of an additional new set of embeddings/vectors, which is exclusively designed for modeling the sentiment interaction between an aspect and its context.",4 The Proposed Approaches,[0],[0]
"The key idea is to decouple different functioning components with different representations, but still make them work jointly.",4 The Proposed Approaches,[0],[0]
"The following four techniques are based on this idea.
",4 The Proposed Approaches,[0],[0]
Interaction Term (IT): The third approach is to formulate explicit target-context sentiment interaction terms.,4 The Proposed Approaches,[0],[0]
"Different from the targeted-context detection problem which is captured by attention (discussed in Section 1), here the targetcontext sentiment (TCS) interaction measures the sentiment-oriented interaction effect between targets and contexts, which we refer to as TCS interaction (or sentiment interaction) for short in the rest of this paper.",4 The Proposed Approaches,[0],[0]
"Such sentiment interaction is captured by a new set of vectors, and we thus also call such vectors TCS vectors.
s = ∑ i αi(Wsci + wI〈di, dt〉) (6)
",4 The Proposed Approaches,[0],[0]
"In Eq. 6, Ws ∈ RK×d and wI ∈ RK×1 are used instead of W in Eq. 3.",4 The Proposed Approaches,[0],[0]
Ws models the direct sentiment effect from ci while wI works with di and dt together for learning the TCS interaction.,4 The Proposed Approaches,[0],[0]
"di and dt are TCS vector representations of context xi and aspect t, produced from a new embedding matrix D,",4 The Proposed Approaches,[0],[0]
"i.e., di = Dxi, dt = Dt (D ∈ Rd×V and di, dt ∈ Rd×1).
",4 The Proposed Approaches,[0],[0]
"Unlike input and output embeddings A and C, D is designed to capture the sentiment interac-
tion.",4 The Proposed Approaches,[0],[0]
"The vectors fromD affect the final sentiment score through wI〈di, dt〉, where wI is a sentimentspecific vector and 〈di, dt〉 ∈ R denotes the dot product of the two TCS vectors di and dt.",4 The Proposed Approaches,[0],[0]
"Compared to the basic MNs, this model can better capture target-sensitive sentiment because the interactions between a context word h and different aspect words (say, p and r) can be different, i.e., 〈dh, dp〉 6=",4 The Proposed Approaches,[0],[0]
"〈dh, dr〉.
",4 The Proposed Approaches,[0],[0]
The key advantage is that now the sentiment effect is explicitly dependent on its target and context.,4 The Proposed Approaches,[0],[0]
"For example, 〈dh, dp〉 can help shift the final sentiment to negative and 〈dh, dr〉 can help shift it to positive.",4 The Proposed Approaches,[0],[0]
Note that α is still needed to control the importance of different contexts.,4 The Proposed Approaches,[0],[0]
"In this manner, targeted-context detection (attention) and TCS interaction are jointly modeled and work together for sentiment inference.",4 The Proposed Approaches,[0],[0]
The proposed techniques introduced below also follow this core idea but with different implementations or properties.,4 The Proposed Approaches,[0],[0]
"We thus will not repeat similar discussions.
",4 The Proposed Approaches,[0],[0]
Coupled Interaction (CI): This proposed technique associates the TCS interaction with an additional set of context representation.,4 The Proposed Approaches,[0],[0]
"This representation is for capturing the global correlation between context and different sentiment classes.
",4 The Proposed Approaches,[0],[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ei) (7)
",4 The Proposed Approaches,[0],[0]
"Specifically, ei is another output representation for xi, which is coupled with the sentiment interaction factor 〈di, dt〉.",4 The Proposed Approaches,[0],[0]
"For each context word xi, ei is generated as ei = Exi whereE ∈ Rd×V is an embedding matrix.",4 The Proposed Approaches,[0],[0]
"〈di, dt〉 and ei function together as a target-sensitive context vector and are used to produce sentiment scores with WI (WI ∈ RK×d).
",4 The Proposed Approaches,[0],[0]
"Joint Coupled Interaction (JCI): A natural variant of the above model is to replace ei with ci, which means to learn a joint output representation.",4 The Proposed Approaches,[0],[0]
"This can also reduce the number of learning parameters and simplify the CI model.
",4 The Proposed Approaches,[0],[0]
"s = ∑ i αi(Wsci +WI〈di, dt〉ci) (8)
Joint Projected Interaction (JPI): This model also employs a unified output representation like JCI, but a context output vector ci will be projected to two different continuous spaces before sentiment score calculation.",4 The Proposed Approaches,[0],[0]
"To achieve the goal, two projection matrices W1, W2 and the non-linear projection function tanh are used.",4 The Proposed Approaches,[0],[0]
"The intuition is
that, when we want to reduce the (embedding) parameters and still learn a joint representation, two different sentiment effects need to be separated in different vector spaces.",4 The Proposed Approaches,[0],[0]
"The two sentiment effects are modeled as two terms:
s = ∑ i αiWJ tanh(W1ci)
+ ∑",4 The Proposed Approaches,[0],[0]
"i αiWJ〈di, dt〉 tanh(W2ci) (9)
where the first term can be viewed as learning target-independent sentiment effect while the second term captures the TCS interaction.",4 The Proposed Approaches,[0],[0]
"A joint sentiment-specific weight matrix WJ(WJ ∈ RK×d) is used to control/balance the interplay between these two effects.
",4 The Proposed Approaches,[0],[0]
"Discussions: (a) In IT, CI, JCI, and JPI, their first-order terms are still needed, because not in all cases sentiment inference needs TCS interaction.",4 The Proposed Approaches,[0],[0]
"For some simple examples like “the battery is good”, the context word “good” simply indicates clear sentiment, which can be captured by their first-order term.",4 The Proposed Approaches,[0],[0]
"However, notice that the modeling of second-order terms offers additional help in both general and target-sensitive scenarios.",4 The Proposed Approaches,[0],[0]
(b) TCS interaction can be calculated by other modeling functions.,4 The Proposed Approaches,[0],[0]
"We have tried several methods and found that using the dot product 〈di, dt〉 or dTi Wdt (with a projection matrix W ) generally produces good results.",4 The Proposed Approaches,[0],[0]
"(c) One may ask whether we can use fewer embeddings or just use one universal embedding to replace A, C and D (the definition of D can be found in the introduction of IT).",4 The Proposed Approaches,[0],[0]
We have investigated them as well.,4 The Proposed Approaches,[0],[0]
We found that merging A and C is basically workable.,4 The Proposed Approaches,[0],[0]
But merging D and A/C produces poor results because they essentially function with different purposes.,4 The Proposed Approaches,[0],[0]
"While A and C handle targeted-context detection (attention), D captures the TCS interaction.",4 The Proposed Approaches,[0],[0]
(d),4 The Proposed Approaches,[0],[0]
"Except NP, we do not apply non-linear projection to the sentiment score layer.",4 The Proposed Approaches,[0],[0]
"Although adding non-linear transformation to it may further improve model performance, the individual sentiment effect from each context will become untraceable, i.e., losing some interpretability.",4 The Proposed Approaches,[0],[0]
"In order to show the effectiveness of learning TCS interaction and for analysis purpose, we do not use it in this work.",4 The Proposed Approaches,[0],[0]
"But it can be flexibly added for specific tasks/analyses that do not require strong interpretability.
",4 The Proposed Approaches,[0],[0]
Loss function: The proposed models are all trained in an end-to-end manner by minimizing the cross entropy loss.,4 The Proposed Approaches,[0],[0]
"Let us denote a sentence and a
target aspect as x and t respectively.",4 The Proposed Approaches,[0],[0]
"They appear together in a pair format (x, t) as input and all such pairs construct the dataset H .",4 The Proposed Approaches,[0],[0]
"g(x,t) is a one-hot vector and gk(x,t) ∈ {0, 1} denotes a gold sentiment label, i.e., whether (x, t) shows sentiment k. yx,t is the model-predicted sentiment distribution for (x, t).",4 The Proposed Approaches,[0],[0]
"ykx,t denotes its probability in class k.",4 The Proposed Approaches,[0],[0]
"Based on them, the training loss is constructed as:
loss = − ∑
(x,t)∈H ∑ k∈K gk(x,t) log y k",4 The Proposed Approaches,[0],[0]
"(x,t) (10)",4 The Proposed Approaches,[0],[0]
"Aspect sentiment classification (ASC) (Hu and Liu, 2004), which is different from document or sentence level sentiment classification (Pang et al., 2002; Kim, 2014; Yang et al., 2016), has recently been tackled by neural networks with promising results (Dong et al., 2014; Nguyen and Shirai, 2015) (also see the survey (Zhang et al., 2018)).",5 Related Work,[0],[0]
"Later on, the seminal work of using attention mechanism for neural machine translation (Bahdanau et al., 2015) popularized the application of the attention mechanism in many NLP tasks (Hermann et al., 2015; Cho et al., 2015; Luong et al., 2015), including ASC.
Memory networks (MNs) (Weston et al., 2015; Sukhbaatar et al., 2015) are a type of neural models that involve such attention mechanisms (Bahdanau et al., 2015), and they can be applied to ASC.",5 Related Work,[0],[0]
Tang et al. (2016) proposed an MN variant to ASC and achieved the state-of-the-art performance.,5 Related Work,[0],[0]
"Another common neural model using attention mechanism is the RNN/LSTM (Wang et al., 2016).
",5 Related Work,[0],[0]
"As discussed in Section 1, the attention mechanism is suitable for ASC because it effectively addresses the targeted-context detection problem.",5 Related Work,[0],[0]
"Along this direction, researchers have studied more sophisticated attentions to further help the ASC task (Chen et al., 2017; Ma et al., 2017; Liu and Zhang, 2017).",5 Related Work,[0],[0]
Chen et al. (2017) proposed to use a recurrent attention mechanism.,5 Related Work,[0],[0]
"Ma et al. (2017) used multiple sets of attentions, one for modeling the attention of aspect words and one for modeling the attention of context words.",5 Related Work,[0],[0]
"Liu and Zhang (2017) also used multiple sets of attentions, one obtained from the left context and one obtained from the right context of a given target.",5 Related Work,[0],[0]
Notice that our work does not lie in this direction.,5 Related Work,[0],[0]
"Our goal is to solve the target-sensitive sen-
timent and to capture the TCS interaction, which is a different problem.",5 Related Work,[0],[0]
"This direction is also finergrained, and none of the above works addresses this problem.",5 Related Work,[0],[0]
"Certainly, both directions can improve the ASC task.",5 Related Work,[0],[0]
"We will also show in our experiments that our work can be integrated with an improved attention mechanism.
",5 Related Work,[0],[0]
"To the best of our knowledge, none of the existing studies addresses the target-sensitive sentiment problem in ASC under the purely data-driven and supervised learning setting.",5 Related Work,[0],[0]
"Other concepts like sentiment shifter (Polanyi and Zaenen, 2006) and sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2013) are also related, but they are not learned automatically and require rule/patterns or external resources (Liu, 2012).",5 Related Work,[0],[0]
"Note that our approaches do not rely on handcrafted patterns (Ding et al., 2008; Wu and Wen, 2010), manually compiled sentiment constraints and review ratings (Lu et al., 2011), or parse trees (Socher et al., 2013).",5 Related Work,[0],[0]
"We perform experiments on the datasets of SemEval Task 2014 (Pontiki et al., 2014), which contain online reviews from domain Laptop and Restaurant.",6 Experiments,[0],[0]
"In these datasets, aspect sentiment polarities are labeled.",6 Experiments,[0],[0]
The training and test sets have also been provided.,6 Experiments,[0],[0]
Full statistics of the datasets are given in Table 2.,6 Experiments,[0],[0]
MN:,6.1 Candidate Models for Comparison,[0],[0]
"The classic end-to-end memory network (Sukhbaatar et al., 2015).",6.1 Candidate Models for Comparison,[0],[0]
"AMN: A state-of-the-art memory network used for ASC (Tang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
"The main difference from MN is in its attention alignment function, which concatenates the distributed representations of the context and aspect, and uses an additional weight matrix for attention calculation, following the method introduced in (Bahdanau et al., 2015).",6.1 Candidate Models for Comparison,[0],[0]
"BL-MN: Our basic memory network presented in Section 2, which does not use the proposed techniques for capturing target-sensitive sentiments.",6.1 Candidate Models for Comparison,[0],[0]
AE-LSTM: RNN/LSTM is another popular attention based neural model.,6.1 Candidate Models for Comparison,[0],[0]
"Here we compare
with a state-of-the-art attention-based LSTM for ASC, AE-LSTM (Wang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
ATAE-LSTM:,6.1 Candidate Models for Comparison,[0],[0]
"Another attention-based LSTM for ASC reported in (Wang et al., 2016).",6.1 Candidate Models for Comparison,[0],[0]
"Target-sensitive Memory Networks (TMNs): The six proposed techniques, NP, CNP, IT, CI, JCI, and JPI give six target-sensitive memory networks.
",6.1 Candidate Models for Comparison,[0],[0]
"Note that other non-neural network based models like SVM and neural models without attention mechanism like traditional LSTMs have been compared and reported with inferior performance in the ASC task (Dong et al., 2014; Tang et al., 2016; Wang et al., 2016), so they are excluded from comparisons here.",6.1 Candidate Models for Comparison,[0],[0]
"Also, note that non-neural models like SVMs require feature engineering to manually encode aspect information, while this work aims to improve the aspect representation learning based approaches.",6.1 Candidate Models for Comparison,[0],[0]
"Since we have a three-class classification task (positive, negative and neutral) and the classes are imbalanced as shown in Table 2, we use F1-score as our evaluation measure.",6.2 Evaluation Measure,[0],[0]
We report both F1Macro over all classes and all individual classbased F1 scores.,6.2 Evaluation Measure,[0],[0]
"As our problem requires finegrained sentiment interaction, the class-based F1 provides more indicative information.",6.2 Evaluation Measure,[0],[0]
"In addition, we report the accuracy (same as F1-Micro), as it is used in previous studies.",6.2 Evaluation Measure,[0],[0]
"However, we suggest using F1-score because accuracy biases towards the majority class.",6.2 Evaluation Measure,[0],[0]
We use the open-domain word embeddings1 for the initialization of word vectors.,6.3 Training Details,[0],[0]
"We initialize other model parameters from a uniform distribution U (-0.05, 0.05).",6.3 Training Details,[0],[0]
The dimension of the word embedding and the size of the hidden layers are 300.,6.3 Training Details,[0],[0]
The learning rate is set to 0.01 and the dropout rate is set to 0.1.,6.3 Training Details,[0],[0]
Stochastic gradient descent is used as our optimizer.,6.3 Training Details,[0],[0]
"The position encoding is also used (Tang et al., 2016).",6.3 Training Details,[0],[0]
"We also compare the memory networks in their multiple computational layers version (i.e., multiple hops) and the number of hops is set to 3 as used in the mentioned previous studies.",6.3 Training Details,[0],[0]
"We implemented all models in the TensorFlow environment using same input, embedding size, dropout rate, optimizer, etc.
",6.3 Training Details,[0],[0]
"1https://github.com/mmihaltz/word2vec-GoogleNewsvectors
so as to test our hypotheses, i.e., to make sure the achieved improvements do not come from elsewhere.",6.3 Training Details,[0],[0]
"Meanwhile, we can also report all evaluation measures discussed above2.",6.3 Training Details,[0],[0]
10% of the training data is used as the development set.,6.3 Training Details,[0],[0]
We report the best results for all models based on their F-1 Macro scores.,6.3 Training Details,[0],[0]
The classification results are shown in Table 3.,6.3.1 Result Analysis,[0],[0]
"Note that the candidate models are all based on classic/standard attention mechanism, i.e., without sophisticated or multiple attentions involved.",6.3.1 Result Analysis,[0],[0]
We compare the 1-hop and 3-hop memory networks as two different settings.,6.3.1 Result Analysis,[0],[0]
The top three F1-Macro scores are marked in bold.,6.3.1 Result Analysis,[0],[0]
"Based on them, we have the following observations:
1.",6.3.1 Result Analysis,[0],[0]
"Comparing the 1-hop memory networks (first nine rows), we see significant performance gains achieved by CNP, CI, JCI, and JPI on both datasets, where each of them has p < 0.01 over the strongest baseline (BL-MN) from paired t-test using F1-Macro.",6.3.1 Result Analysis,[0],[0]
IT also outperforms the other baselines while NP has similar performance to BL-MN.,6.3.1 Result Analysis,[0],[0]
"This indicates that TCS interaction is very useful, as BL-MN and NP do not model it.",6.3.1 Result Analysis,[0],[0]
2.,6.3.1 Result Analysis,[0],[0]
"In the 3-hop setting, TMNs achieve much better results on Restaurant.",6.3.1 Result Analysis,[0],[0]
"JCI, IT, and CI achieve the best scores, outperforming the strongest baseline AMN by 2.38%, 2.18%, and 2.03%.",6.3.1 Result Analysis,[0],[0]
"On Laptop, BL-MN and most TMNs (except CNP and JPI) perform similarly.",6.3.1 Result Analysis,[0],[0]
"However, BL-MN performs poorly on Restaurant (only better than two models) while TMNs show more stable performance.",6.3.1 Result Analysis,[0],[0]
3.,6.3.1 Result Analysis,[0],[0]
"Comparing all TMNs, we see that JCI works the best as it always obtains the top-three scores on two datasets and in two settings.",6.3.1 Result Analysis,[0],[0]
CI and JPI also perform well in most cases.,6.3.1 Result Analysis,[0],[0]
"IT, NP, and CNP can achieve very good scores in some cases but are less stable.",6.3.1 Result Analysis,[0],[0]
We also analyzed their potential issues in Section 4. 4.,6.3.1 Result Analysis,[0],[0]
It is important to note that these improvements are quite large because in many cases sentiment interactions may not be necessary (like sentence (1) in Section 1).,6.3.1 Result Analysis,[0],[0]
"The overall good results obtained by TMNs demonstrate their capability of handling both general and target-sensitive sentiments, i.e., the proposed
2Most related studies report accuracy only.
",6.3.1 Result Analysis,[0],[0]
techniques do not bring harm while capturing additional target-sensitive signals.,6.3.1 Result Analysis,[0],[0]
5.,6.3.1 Result Analysis,[0],[0]
"Micro-F1/accuracy is greatly affected by the majority class, as we can see the scores from Pos. and Micro are very consistent.",6.3.1 Result Analysis,[0],[0]
"TMNs, in fact, effectively improve the minority classes, which are reflected in Neg.",6.3.1 Result Analysis,[0],[0]
"and Neu., for example, JCI improves BL-MN by 3.78% in Neg.",6.3.1 Result Analysis,[0],[0]
on Restaurant,6.3.1 Result Analysis,[0],[0]
.,6.3.1 Result Analysis,[0],[0]
This indicates their usefulness of capturing fine-grained sentiment signals.,6.3.1 Result Analysis,[0],[0]
"We will give qualitative examples in next section to show their modeling superiority for identifying target-sensitive sentiments.
Integration with Improved Attention: As discussed, the goal of this work is not for learning better attention but addressing the targetsensitive sentiment.",6.3.1 Result Analysis,[0],[0]
"In fact, solely improving attention does not solve our problem (see Sections 1 and 3).",6.3.1 Result Analysis,[0],[0]
"However, better attention can certainly help achieve an overall better performance for the ASC task, as it makes the targeted-context detection more accurate.",6.3.1 Result Analysis,[0],[0]
"Here we integrate our pro-
posed technique JCI with a state-of-the-art sophisticated attention mechanism, namely, the recurrent attention framework, which involves multiple attentions learned iteratively (Kumar et al., 2016; Chen et al., 2017).",6.3.1 Result Analysis,[0],[0]
We name our model with this integration as Target-sensitive Recurrent-attention Memory Network (TRMN) and the basic memory network with the recurrent attention as Recurrentattention Memory Network (RMN).,6.3.1 Result Analysis,[0],[0]
Their results are given in Table 4.,6.3.1 Result Analysis,[0],[0]
TRMN achieves significant performance gain with p < 0.05 in paired t-test.,6.3.1 Result Analysis,[0],[0]
"We now give some real examples to show the effectiveness of modeling TCS interaction for identifying target-sensitive sentiments, by comparing a regular MN and a TMN.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Specifically, BL-MN and JPI are used.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other MNs/TMNs have similar performances to BL-MN/JPI qualitatively, so we do not list all of them here.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"For BL-MN and JPI, their sentiment scores of a single context word i are calculated by αiWci (from Eq. 3) and αiWJ tanh(W1ci) + αiWJ〈di, dt〉tanh(W2ci) (from Eq. 9), each of which results in a 3-dimensional vector.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
Illustrative Examples: Table 5 shows two records in Laptop.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In record 1, to identify the sentiment of target price in the presented sentence, the sentiment interaction between the context word “higher” and the target word price is the key.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"The
specific sentiment scores of the word “higher” towards negative, neutral and positive classes in both models are reported.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
We can see both models accurately assign the highest sentiment scores to the negative class.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"We also observe that in MN the negative score (0.3641) in the 3-dimension vector {0.3641,−0.3275,−0.0750} calculated by αiWci is greater than neutral (−0.3275) and positive (−0.0750) scores.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Notice that αi is always positive (ranging in (0, 1)), so it can be inferred that the first value in vector Wci is greater than the other two values.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Here ci denotes the vector representation of “higher” so we use chigher to highlight it and we have {Wchigher}Negative > {Wchigher}Neutral/Positive as an inference.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In record 2, the target is resolution and its sentiment is positive in the presented sentence.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Although we have the same context word “higher”, different from record 1, it requires a positive sentiment interaction with the current target.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Looking at the results, we see TMN assigns the highest sentiment score of word “higher” to positive class correctly, whereas MN assigns it to negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
This error is expected if we consider the above inference {Wchigher}Negative > {Wchigher}Neutral/Positive in MN.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
The cause of this unavoidable error is that Wci is not conditioned on the target.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In contrast, WJ〈di, ·dt〉tanh(W2ci) can change the sentiment polarity with the aspect vector dt encoded.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other TMNs also achieve it (like WI〈di, dt〉ci in JCI).
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
One may notice that the aspect information (vt) is actually also considered in the form of αiWci+ Wvt in MNs and wonder whether Wvt may help address the problem given different vt.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Let us assume it helps, which means in the above example an MN makes Wvresolution favor the positive class and Wvprice favor the negative class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"But then we will have trouble when the context word is “lower”, where it requires Wvresolution to favor the negative class and Wvprice to favor the positive class.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"This contradiction reflects the theoretical problem discussed in Section 3.
",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"Other Examples: We also found other interesting target-sensitive sentiment expressions like “large bill” and “large portion”, “small tip” and “small portion” from Restaurant.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
Notice that TMNs can also improve the neutral sentiment (see Table 3).,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"For instance, TMN generates a sentiment score vector of the context “over” for target aspect price: {0.1373, 0.0066, -0.1433} (negative) and for target aspect dinner: {0.0496, 0.0591, - 0.1128} (neutral) accurately.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"But MN produces both negative scores {0.0069, 0.0025, -0.0090} (negative) and {0.0078, 0.0028, -0.0102} (negative) for the two different targets.",6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
The latter one in MN is incorrect.,6.4 Effect of TCS Interaction for Identifying Target-Sensitive Sentiment,[0],[0]
"In this paper, we first introduced the targetsensitive sentiment problem in ASC.",7 Conclusion and Future Work,[0],[0]
"After that, we discussed the basic memory network for ASC and analyzed the reason why it is incapable of capturing such sentiment from a theoretical perspective.",7 Conclusion and Future Work,[0],[0]
We then presented six techniques to construct target-sensitive memory networks.,7 Conclusion and Future Work,[0],[0]
"Finally, we reported the experimental results quantitatively and qualitatively to show their effectiveness.
",7 Conclusion and Future Work,[0],[0]
"Since ASC is a fine-grained and complex task, there are many other directions that can be further explored, like handling sentiment negation, better embedding for multi-word phrase, analyzing sentiment composition, and learning better attention.",7 Conclusion and Future Work,[0],[0]
We believe all these can help improve the ASC task.,7 Conclusion and Future Work,[0],[0]
"The work presented in this paper lies in the direction of addressing target-sensitive sentiment, and we have demonstrated the usefulness of capturing this signal.",7 Conclusion and Future Work,[0],[0]
We believe that there will be more effective solutions coming in the near future.,7 Conclusion and Future Work,[0],[0]
This work was partially supported by National Science Foundation (NSF) under grant nos.,Acknowledgments,[0],[0]
"IIS1407927 and IIS-1650900, and by Huawei Technologies Co. Ltd with a research gift.",Acknowledgments,[0],[0]
Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis.,abstractText,[0],[0]
"Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence.",abstractText,[0],[0]
Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results.,abstractText,[0],[0]
"In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target.",abstractText,[0],[0]
"However, we found an important problem with the current MNs in performing the ASC task.",abstractText,[0],[0]
Simply improving the attention mechanism will not solve it.,abstractText,[0],[0]
"The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone.",abstractText,[0],[0]
"To tackle this problem, we propose the targetsensitive memory networks (TMNs).",abstractText,[0],[0]
Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.,abstractText,[0],[0]
Target-Sensitive Memory Networks for Aspect Sentiment Classification,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192–1202 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1192",text,[0],[0]
A language model (LM) defines a probability distribution over sequences of words.,1 Introduction,[0],[0]
Recent technological advances have led to an explosion of neural network-based LM architectures.,1 Introduction,[0],[0]
"The most popular ones are based on recurrent neural networks (RNNs) (Elman, 1990; Mikolov et al., 2010), in particular Long Short-Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997).",1 Introduction,[0],[0]
"While a large number of alternative architectures have been proposed in the past few years, LSTMs are still highly competitive (Melis et al., 2018).
",1 Introduction,[0],[0]
Language models are typically evaluated using perplexity: it is considered desirable for an LM to assign a high probability to held-out data from the same corpus as the training data.,1 Introduction,[0],[0]
"This measure conflates multiple sources of success (or failure) in predicting the next word: common collo-
cations, semantics, pragmatics, syntax, and so on.",1 Introduction,[0],[0]
"The quality of the syntactic predictions made by the LM is arguably particularly difficult to measure using perplexity: since most sentences are grammatically simple and most words can be predicted from their local context, perplexity rewards LMs primarily for collocational and semantic predictions.
",1 Introduction,[0],[0]
We propose to supplement perplexity with a metric that assesses whether the probability distribution defined by the model conforms to the grammar of the language.,1 Introduction,[0],[0]
"Following previous work (Lau et al., 2017; Linzen et al., 2016; Gulordava et al., 2018), we suggest that given two sentences that differ minimally from each other, one of which is grammatical and the other which is not, it is desirable for the model to assign a higher probability to the grammatical one.
",1 Introduction,[0],[0]
"The value of this approach can be illustrated with a recent study by Tran et al. (2018), where a standard LSTM language model was compared to an attention-only LM without recurrence (Vaswani et al., 2017).",1 Introduction,[0],[0]
"Although the attention-only model had somewhat better perplexity on the validation set, when the models were tested specifically on challenging subject-verb agreement dependencies, the attention-only model made three times as many errors as the LSTM.",1 Introduction,[0],[0]
"In other words, the LSTM learned more robust syntactic representations, but this advantage was not reflected in its average perplexity on the corpus, since syntactically challenging sentences are relatively infrequent.
",1 Introduction,[0],[0]
"Previous work on targeted syntactic evaluation of language models has identified syntactically challenging sentences in corpora (Linzen et al., 2016; Gulordava et al., 2018).",1 Introduction,[0],[0]
"While evaluation on naturally occurring examples is appealing, this approach has its limitations (see Section 2).",1 Introduction,[0],[0]
"In particular, syntactically challenging examples are sparsely represented in a corpus, their identifica-
tion requires a clean parsed corpus, and naturally occurring sentences are difficult to control for confounds.",1 Introduction,[0],[0]
"We contrast the naturalistic approach with a constructed dataset, which allows us to examine a much larger range of specific grammatical phenomena than has been possible before.",1 Introduction,[0],[0]
"We use templates to automatically create our test sentences, making it possible to generate a large test set while maintaining experimental control over our materials as well as a balanced number of examples of each phenomenon.
",1 Introduction,[0],[0]
"We test three LMs on the data set we develop: an n-gram baseline, an RNN LM trained on an unannotated corpus, and an RNN LM trained on a multitask objective: language modeling and Combinatory Categorial Grammar (CCG) supertagging (Bangalore and Joshi, 1999).",1 Introduction,[0],[0]
We also conduct a human experiment using the same materials.,1 Introduction,[0],[0]
"The n-gram baseline largely performed at chance, suggesting that good performance on the task requires syntactic representations.",1 Introduction,[0],[0]
"The RNN LMs performed well on simple cases, but struggled on more complex ones.",1 Introduction,[0],[0]
"Multi-task training with a supervised syntactic objective improved the performance of the RNN, but it was still much weaker than humans.",1 Introduction,[0],[0]
"This suggests that our data set is challenging, especially when explicit syntactic supervision is not available, and can therefore motivate richer language modeling architectures.",1 Introduction,[0],[0]
How should grammaticality be captured in the probability distribution defined by an LM?,2.1 Grammaticality and LM probability,[0],[0]
The most extreme position would be that a language model should assign a probability of zero to ungrammatical sentences.,2.1 Grammaticality and LM probability,[0],[0]
"For most applications, some degree of error tolerance is desirable, and it is not practical to assign a sentence a probability of exactly zero.1 Following Linzen et al. (2016) and Gulordava et al. (2018), our desideratum for the language model is more modest: if two closely matched sentence differ only in their grammaticality, the probability of the grammatical sentence should be higher than the probability of the ungrammatical one.",2.1 Grammaticality and LM probability,[0],[0]
"For example, the following minimal pair illustrates the fact that third-
1Nor is it possible to have a threshold such that all grammatical sentences have probability higher than and all ungrammatical sentences have probability lower than , for the simple reason that there is an infinite number of grammatical sentences (Lau et al., 2017).
person present English verbs agree with the number of their subject:
(1) Simple agreement:",2.1 Grammaticality and LM probability,[0],[0]
a.,2.1 Grammaticality and LM probability,[0],[0]
The author laughs.,2.1 Grammaticality and LM probability,[0],[0]
b. *,2.1 Grammaticality and LM probability,[0],[0]
"The author laugh.
",2.1 Grammaticality and LM probability,[0],[0]
We expect the probability of (1a) to be higher than the probability of (1b).,2.1 Grammaticality and LM probability,[0],[0]
Previous work has simplified this setting further by comparing the probability that the LM assigns to a single word that is the locus of ungrammaticality.,2.1 Grammaticality and LM probability,[0],[0]
"In (1), for example, the LM would be fed the first two words of the sentence, and would be considered successful on the task if it predicts P (laughs) >",2.1 Grammaticality and LM probability,[0],[0]
"P (laugh).
",2.1 Grammaticality and LM probability,[0],[0]
"The prediction setting is only applicable when the locus of ungrammaticality is a single word, rather than, say, the interaction between two words; moreover, the information needed to make the grammaticality decision needs to be available in the left context of the locus of grammaticality.",2.1 Grammaticality and LM probability,[0.9557525948219309],['The sequential nature of natural language is taken into account mostly through word n-grams and skipgrams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.']
These conditions do not always hold.,2.1 Grammaticality and LM probability,[0],[0]
"Negative polarity items (NPIs), for example, are words like any and ever that can only be used in the scope of negation.2 The grammaticality of placing a particular quantifier in the beginning of the sentences in (2) depends on whether the sentence contains an NPI later on:
(2) Simple NPI:",2.1 Grammaticality and LM probability,[0],[0]
a. No students have ever lived here.,2.1 Grammaticality and LM probability,[0],[0]
b. *,2.1 Grammaticality and LM probability,[0],[0]
"Most students have ever lived here.
",2.1 Grammaticality and LM probability,[0],[0]
It would not be possible to compare these two sentences using the prediction task.,2.1 Grammaticality and LM probability,[0],[0]
"In the current paper, we use the more general setting and compare the probability of the two complete sentences.",2.1 Grammaticality and LM probability,[0],[0]
Previous work has used syntactically complex sentences identified from a parsed corpus.,2.2 Data set construction,[0],[0]
This approach has several limitations.,2.2 Data set construction,[0],[0]
"If the corpus is automatically parsed, the risk of a parse error increases with the complexity of the construction (Bender et al., 2011).",2.2 Data set construction,[0],[0]
"If the test set is restricted to sentences with gold parses, it can be difficult or impossible to find a sufficient number of examples of syntactically challenging cases.",2.2 Data set construction,[0],[0]
"Moreover, using naturally occurring sentences can introduce
2In practice, the conditions that govern the distribution of NPIs are much more complicated, but this first approximation will suffice for the present purposes.",2.2 Data set construction,[0],[0]
"For a review, see Giannakidou (2011).
",2.2 Data set construction,[0],[0]
"confounds that may complicate the interpretation of the experiments (Ettinger et al., 2018).
",2.2 Data set construction,[0],[0]
"To circumvent these issues, we use templates to automatically construct a large number of English sentence pairs (∼350,000).",2.2 Data set construction,[0],[0]
"Our data set includes three phenomena that linguists consider to be sensitive to hierarchical syntactic structure (Everaert et al., 2015; Xiang et al., 2009): subjectverb agreement (described in detail in Sections 4.1 and 4.2), reflexive anaphora (Section 4.3) and negative polarity items (Section 4.4).
",2.2 Data set construction,[0],[0]
The templates can be described using nonrecursive context-free grammars.,2.2 Data set construction,[0],[0]
We specify the preterminal symbols that make up a syntactic construction and have different terminal symbols that those preterminals could be mapped to.,2.2 Data set construction,[0],[0]
"For example, the template for the simple agreement construction illustrated in (1) consists of the following rules:
(3) a. Simple agreement→ D MS MV b. D→ the c. MS→ {author, pilot, . . .}",2.2 Data set construction,[0],[0]
"d. MV→ {laughs, smiles, . . .",2.2 Data set construction,[0],[0]
"}
We generate all possible combinations of the terminals.",2.2 Data set construction,[0],[0]
"The Supplementary Materials provide a full description of all our templates.3
While these examples are somewhat artificial, our goal is to isolate the syntactic capabilities of the model; it is in fact beneficial to minimize the semantic or collocational cues that can be used to identify the grammatical sentence.",2.2 Data set construction,[0],[0]
Gulordava et al. took this approach further and constructed “colorless green ideas” test cases by substituting random content words into sentences from a corpus.,2.2 Data set construction,[0],[0]
"We take a more moderate position and avoid combinations that are very implausible or violate selectional restrictions (e.g., the apple laughs).",2.2 Data set construction,[0],[0]
We do this by having separate templates for animate and inanimate subjects and verbs so that the resulting sentences are always reasonably plausible.,2.2 Data set construction,[0],[0]
"Targeted evaluation: LM evaluation data sets using challenging prediction tasks have been proposed in the context of semantics and discourse comprehension (Zweig and Burges, 2011; Paperno et al., 2016).",3 Related work,[0],[0]
"Evaluation sets consisting of chal-
3The code, the data set and the Supplementary Materials can be found at https://github.com/ BeckyMarvin/LM_syneval.
",3 Related work,[0],[0]
"lenging syntactic constructions have been constructed for parser evaluation (Rimell et al., 2009; Nivre et al., 2010; Bender et al., 2011), and minimal pair approaches have been proposed for evaluating image captioning (Shekhar et al., 2017) and machine translation systems (Sennrich, 2017), but no data sets exist that target a range of syntactic constructions for language model evaluation.
",3 Related work,[0],[0]
Acceptability judgments: Lau et al. (2017) compared the ability of different LMs to predict graded human acceptability judgments.,3 Related work,[0],[0]
"The forced-choice approach used in the current paper has been shown to be effective in human acceptability judgment experiments (Sprouse and Almeida, 2017).",3 Related work,[0],[0]
"In some early work, neural networks were trained explicitly to predict acceptability judgments (Lawrence et al., 1996; Allen and Seidenberg, 1999); Post (2011) likewise trained a classifier on top of a parser to predict grammaticality.",3 Related work,[0],[0]
"Warstadt et al. (2018) use a transfer learning approach, where an unsupervised model is finetuned on acceptability prediction.",3 Related work,[0],[0]
"Our work differs from those studies in that we do not advocate providing any explicit grammaticality signal to the LM at any point (“no negative evidence”).
",3 Related work,[0],[0]
"Syntax in LMs: There have been several proposals over the years to incorporate explicit syntax into LMs to overcome the inability of n-gram LMs to model long-distance dependencies (Jurafsky et al., 1995; Roark, 2001; Pauls and Klein, 2012).",3 Related work,[0],[0]
"While RNN language models can in principle model longer dependencies (Mikolov et al., 2010; Linzen et al., 2016), in practice it can still be beneficial to inject syntax into the model.",3 Related work,[0],[0]
"This can be done by combining it with a supervised parser (Dyer et al., 2016) or other multi-task learning objectives (Enguehard et al., 2017).",3 Related work,[0],[0]
"Our work is orthogonal to this area of research, but can be seen as providing a potential opportunity to underscore the advantage of such syntax-infused models.",3 Related work,[0],[0]
"This section describes all of the types of sentence pairs included in our data set, which include examples of subject-verb agreement (Sections 4.1 and 4.2), reflexive anaphoras (Section 4.3) and negative polarity items (Section 4.4).",4 Data set composition,[0],[0]
"Determining the correct number of the verb is trivial in examples such as (1) above, in which the sentence only contains a single noun.",4.1 Subject-verb agreement,[0],[0]
"By contrast, in cases where there are multiple nouns in the sentence, identifying which of them is the subject of a given verb requires understanding the structure of the sentence.",4.1 Subject-verb agreement,[0],[0]
"In particular, the relevant subject is not necessarily the first noun of the sentence:
(4) Agreement in a sentential complement:",4.1 Subject-verb agreement,[0],[0]
a. The bankers knew the officer smiles.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The bankers knew the officer smile.
",4.1 Subject-verb agreement,[0],[0]
Here the verb smiles needs to agree with the embedded subject officer rather than the main clause subject bankers.,4.1 Subject-verb agreement,[0],[0]
"The subject is also not necessarily the most recent noun before the verb: when the subject is modified by a phrase, a distracting noun (“attractor”) often intervenes in the linear order of the sentence between the head of the subject and the verb.",4.1 Subject-verb agreement,[0],[0]
"Two examples of such modifiers are prepositional phrases and relative clauses (RCs):
(5) Agreement across a prepositional phrase: a. The farmer near the parents smiles.",4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
*,4.1 Subject-verb agreement,[0],[0]
"The farmer near the parents smile.
",4.1 Subject-verb agreement,[0],[0]
(6) Agreement across a subject relative clause:,4.1 Subject-verb agreement,[0],[0]
a. The officers that love the skater smile.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The officers that love the skater smiles.
",4.1 Subject-verb agreement,[0],[0]
"We include all four possible configurations of noun number for each type of minimal pair; for (5), these would be:4
(7) a.",4.1 Subject-verb agreement,[0],[0]
The farmer near the parent smiles/*smile.,4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
The farmer near the parents smiles/*smile.,4.1 Subject-verb agreement,[0],[0]
c.,4.1 Subject-verb agreement,[0],[0]
The farmers near the parent smile/*smiles.,4.1 Subject-verb agreement,[0],[0]
d.,4.1 Subject-verb agreement,[0],[0]
"The farmers near the parents
smile/*smiles.
",4.1 Subject-verb agreement,[0],[0]
"Sentences where the two nouns conflict in number are expected to be more challenging, but interpretable errors may certainly occur even when they do not.",4.1 Subject-verb agreement,[0],[0]
"For example, the model may use the heuristic that sentences with multiple nouns are likely to have a plural verb (a heuristic that
4The slash notation indicates the word that differs between the grammatical and ungrammatical sentence; for example, in (7a), the full sentence pair would be:
(i) a.",4.1 Subject-verb agreement,[0],[0]
The farmer near the parent smiles.,4.1 Subject-verb agreement,[0],[0]
b.,4.1 Subject-verb agreement,[0],[0]
*,4.1 Subject-verb agreement,[0],[0]
"The farmer near the parent smile.
would be effective for coordination); alternatively, it might prefer singular verbs to plural ones regardless of whether the subject is singular or plural, simply because the singular form of the verb is more frequent.
",4.1 Subject-verb agreement,[0],[0]
"Next, in verb phrase (VP) coordination, both of the verbs need to agree with the subject:
(8) Short VP coordination: a.",4.1 Subject-verb agreement,[0],[0]
The senator smiles and laughs.,4.1 Subject-verb agreement,[0],[0]
b. *,4.1 Subject-verb agreement,[0],[0]
"The senator smiles and laugh.
",4.1 Subject-verb agreement,[0],[0]
We had both singular and plural subjects.,4.1 Subject-verb agreement,[0],[0]
The number of the verb immediately adjacent to the subject was always grammatical.,4.1 Subject-verb agreement,[0],[0]
"This problem can in principle be solved with a trigram model (smiles and laughs is likely to be a more frequent trigram than smiles and laugh); to address this potential concern, we also included a coordination condition with a longer dependency:
(9) Long VP coordination: The manager writes in a journal every day and likes/*like to watch television shows.",4.1 Subject-verb agreement,[0],[0]
"We go into greater depth in object relative clauses, which most clearly require a hierarchical representation.",4.2 Agreement and object relative clauses,[0],[0]
"In (10) and (11), the model needs to be able to distinguish the embedded subject (parents) from the main clause subject (farmer) when making its predictions:
(10) Agreement across an object relative clause: a. The farmer that the parents love swims.",4.2 Agreement and object relative clauses,[0],[0]
b. *,4.2 Agreement and object relative clauses,[0],[0]
"The farmer that the parents love swim.
",4.2 Agreement and object relative clauses,[0],[0]
(11) Agreement in an object relative clause:,4.2 Agreement and object relative clauses,[0],[0]
a. The farmer that the parents love swims.,4.2 Agreement and object relative clauses,[0],[0]
b. *,4.2 Agreement and object relative clauses,[0],[0]
"The farmer that the parents loves swims.
",4.2 Agreement and object relative clauses,[0],[0]
"In keeping with the minimal pair approach, we never introduce two agreement errors at the same time: either the embedded verb or the main verb is incorrectly inflected, but not both.
",4.2 Agreement and object relative clauses,[0],[0]
We include a number of variations on the pattern in (11).,4.2 Agreement and object relative clauses,[0],[0]
"First, we delete the relativizer that, with the hypothesis that the absence of an overt cue to structure will make the task more difficult:
(12) The farmer the parents love/*loves swims.
",4.2 Agreement and object relative clauses,[0],[0]
"In another condition, we replace the main subject with an inanimate noun and keep the embed-
ded subject animate.",4.2 Agreement and object relative clauses,[0],[0]
"We base this manipulation on human experimental work showing that similar nouns (for example, two animate nouns) are more likely to cause confusion during comprehension than dissimilar nouns, such as an animate and an inanimate noun (Van Dyke, 2007):
(13) The movies that the author likes are/*is good.
",4.2 Agreement and object relative clauses,[0],[0]
"For a complete list of all the types of minimal pairs we include, see the Supplementary Materials.",4.2 Agreement and object relative clauses,[0],[0]
A reflexive pronoun such as himself needs to have an antecedent from which it derives its interpretation.,4.3 Reflexive anaphora,[0],[0]
"The pronoun needs to agree in number (and gender) with its antecedent:
(14)",4.3 Reflexive anaphora,[0],[0]
Simple reflexive:,4.3 Reflexive anaphora,[0],[0]
a.,4.3 Reflexive anaphora,[0],[0]
The senators embarrassed themselves.,4.3 Reflexive anaphora,[0],[0]
b. *,4.3 Reflexive anaphora,[0],[0]
"The senators embarrassed herself.
",4.3 Reflexive anaphora,[0],[0]
There are structural conditions on the nouns to which a reflexive pronoun can be bound.,4.3 Reflexive anaphora,[0],[0]
One of these conditions requires the antecedent to be in the same clause as the reflexive pronoun.,4.3 Reflexive anaphora,[0],[0]
"For example, (15b) cannot refer to a context in which the pilot embarrassed the bankers:
(15) Reflexive in a sentential complement: a.",4.3 Reflexive anaphora,[0],[0]
"The bankers thought the pilot embar-
rassed himself.",4.3 Reflexive anaphora,[0],[0]
b. *,4.3 Reflexive anaphora,[0],[0]
"The bankers thought the pilot embar-
rassed themselves.
",4.3 Reflexive anaphora,[0],[0]
"Likewise, in the following minimal pair, sentence (16b) is ungrammatical, because the reflexive pronoun themselves, which is part of the main clause, cannot be bound to the noun phrase the architects, which is inside an embedded clause:
(16) Reflexive across an object relative clause:",4.3 Reflexive anaphora,[0],[0]
"a. The manager that the architects like
doubted himself.",4.3 Reflexive anaphora,[0],[0]
b.,4.3 Reflexive anaphora,[0],[0]
*,4.3 Reflexive anaphora,[0],[0]
"The manager that the architects like
doubted themselves.",4.3 Reflexive anaphora,[0],[0]
"Negative polarity items, introduced in example (2) above, are words that (to a first approximation) need to occur in the context of negation.",4.4 Negative polarity items,[0],[0]
"Crucially for the purposes of the present work, the scope of negation is structurally defined.",4.4 Negative polarity items,[0],[0]
"In particular
the negative noun phrase needs to c-command the NPI: the syntactic non-terminal node that dominates the negative noun phrase must also dominate the NPI.",4.4 Negative polarity items,[0],[0]
"This is the case in (17a), but not in (17b), where the negative noun phrase is too deep in the tree to c-command the NPI ever (Xiang et al., 2009; Everaert et al., 2015).
",4.4 Negative polarity items,[0],[0]
(17) NPI across a relative clause:,4.4 Negative polarity items,[0],[0]
"a. No authors that the security guards like
have ever been famous.",4.4 Negative polarity items,[0],[0]
b. *,4.4 Negative polarity items,[0],[0]
"The authors that no security guards like
have ever been famous.
",4.4 Negative polarity items,[0],[0]
All of the nouns and verbs in the NPI cases were plural.,4.4 Negative polarity items,[0],[0]
"As in some of the agreement cases, we included a variant of (17) in which the subject was inanimate.",4.4 Negative polarity items,[0],[0]
"To show how our challenge set can be used to evaluate the syntactic performance of LMs, we trained three LMs with increasing levels of syntactic sophistication.",5 Experimental setup,[0],[0]
"All of the LMs were trained on a 90 million word subset of Wikipedia (Gulordava et al., 2018).",5 Experimental setup,[0],[0]
Our n-gram LM and LSTM LM do not require annotated data.,5 Experimental setup,[0],[0]
"The third model is also an LSTM LM, but it requires syntactically annotated data (CCG supertags).
N-gram model:",5 Experimental setup,[0],[0]
"We trained a 5-gram model on the same 90M word corpus using the SRILM toolkit (Stolcke, 2002) which backs off to smaller n-grams using Kneser-Ney smoothing.
",5 Experimental setup,[0],[0]
Single-task RNN:,5 Experimental setup,[0],[0]
"The RNN LM had two layers of 650 LSTM units, a batch size of 128, a dropout rate of 0.2, and a learning rate of 20.0, and was trained for 40 epochs (following the hyperparameters of Gulordava et al. 2018).
",5 Experimental setup,[0],[0]
Multi-task RNN:,5 Experimental setup,[0],[0]
"In multi-task learning, the system is trained to optimize an objective function that combines the objective functions of several tasks.",5 Experimental setup,[0],[0]
"We combine language modeling with CCG supertagging, a task that predicts for each word in the sentence its CCG supertag (Bangalore and Joshi, 1999; Lewis et al., 2016).",5 Experimental setup,[0],[0]
"We simply sum the two objective functions with equal weights (Enguehard et al., 2017).",5 Experimental setup,[0],[0]
Early stopping in this model is based on the combined loss on language modeling and supertagging.,5 Experimental setup,[0],[0]
"Supertags provide a large amount of syntactic information
about the word; the sequence of supertags of a sentence strongly constrains the possible parses of the sentence.",5 Experimental setup,[0],[0]
"We use supertagging as a “scaffold” task (Swayamdipta et al., 2017): our goal is not to produce a competitive supertagger, but to induce better syntactic representations, which would then lead to improved language modeling.",5 Experimental setup,[0],[0]
"We used CCG-Bank (Hockenmaier and Steedman, 2007) as our CCG corpus.
Human evaluation: We designed a human experiment on Amazon Mechanical Turk that mirrored the task that was given to the LMs: both versions of a minimal pair were shown on the screen at the same time, and participants were asked to judge which one of them was more acceptable (for details, see the Supplementary Materials).",5 Experimental setup,[0],[0]
We emphasize that we do not see human performance on complex syntactic dependencies as setting an upper bound on the performance that we should expect from an LM.,5 Experimental setup,[0],[0]
"There is a rich literature showing that humans make mistakes such as subject-verb agreement errors; in fact, most of the phenomena we test were inspired by work in psycholinguistics that studies these errors (Bock and Miller, 1991; Phillips et al., 2011).",5 Experimental setup,[0],[0]
"At the same time, while we do not see a reason not to aspire for 100% accuracy, we are interested in comparing LM and human errors: if the errors are similar, the two systems may be using similar representations.",5 Experimental setup,[0],[0]
Local agreement: The overall accuracy per condition can be seen in Table 1.,6 Results,[0],[0]
"The n-gram LM’s accuracy was only 79% for simple agreement and agreement in a sentential complement, both of which can be solved entirely using local context.",6 Results,[0],[0]
"This is because not all subject and verb combinations in our materials appeared verbatim in the 90M word training corpus; for those combinations, the model fell back on unigram probabilities, which in this context amounts to selecting the more frequent form of the verb.
",6 Results,[0],[0]
"Both RNNs performed much better than the n-gram model on the simple agreement case (single-task: 94%; multi-task: 100%), reflecting these models’ ability to generalize beyond the specific bigrams that occurred in the corpus.",6 Results,[0],[0]
Accuracy on agreement in a sentential complement was also very high (single-task: 99%; multi-task: 93%).,6 Results,[0],[0]
"This indicates that the RNNs do not rely on the heuristic whereby the first noun of the sentence
is likely to be its subject.",6 Results,[0],[0]
"They did slightly worse but still very well on short VP coordination (both 90%); this dependency is also local, albeit across the word and.
",6 Results,[0],[0]
Non-local agreement: The accuracy of the n-gram model on non-local dependencies (long VP coordination and agreement across a phrase or a clause) was very close to 50%.,6 Results,[0],[0]
This suggests that local collocational information is not useful in these conditions.,6 Results,[0],[0]
"The single-task RNN also performed much more poorly on these conditions than on the local agreement conditions, though for the most part its accuracy was better than chance.",6 Results,[0],[0]
"Humans did worse on these dependencies as well, but their accuracy did not drop as sharply as the RNNs’ (human accuracies ranged from 82% to 88%).",6 Results,[0],[0]
"In most of these cases, multitask learning was very helpful; for example, accuracy in long VP coordination increased from 61% to 81%.",6 Results,[0],[0]
"Still, both RNNs performed poorly on agreement across an object RC, especially without that, whereas humans performed comparably on all non-local dependencies.
",6 Results,[0],[0]
"Agreement inside an object RC: This case is particularly interesting, because this dependency is purely local (see (11)), and the interference is from the distant sentence-initial noun.",6 Results,[0],[0]
"Although this configuration is similar to the sentential complement case, performance was worse both in RNNs and humans.",6 Results,[0],[0]
"However, RNNs performed better than humans, at least when the sentence included the overt relativizer that.",6 Results,[0],[0]
"This suggests that interference is sensitive to proximity in RNNs but to syntactic status in humans — humans appear to be confusing the main clause subject and the embedded subject (Wagers et al., 2009).
",6 Results,[0],[0]
Reflexive anaphora:,6 Results,[0],[0]
"The RNNs’ performance was significantly worse on simple reflexives (83%) than on simple agreement (94%), and did not differ between the single-task and multi-task models.",6 Results,[0],[0]
"By contrast, human performance did not differ between subject-verb agreement and reflexive anaphoras.",6 Results,[0],[0]
"The surprisingly poor performance for this adjacent dependency seems to be due to an asymmetry in accuracy between himself and themselves on the one hand (100% accuracy in the multi-task RNN) and herself on the other hand (49% accuracy).5 Accuracy was very low for all
5This may be because himself and themselves are significantly more frequent than herself, and consequently the num-
pronouns in the structurally complex case in which the dependency was across a relative clause (55% compared to 87% in humans).
",6 Results,[0],[0]
NPIs:,6 Results,[0],[0]
"The dependency in simple NPIs spans only four words, so the n-gram model could in principle capture it.",6 Results,[0],[0]
"In practice, the n-gram model systematically selected the wrong answer, suggesting that it backed off to comparing the bigrams no students and most students, the first of which is presumably less frequent.",6 Results,[0],[0]
"Surprisingly, the n-gram model’s accuracy was higher than 50% on NPIs across a relative clause, a dependency that spans more than five words.",6 Results,[0],[0]
"In this case, the bigrams that the and the chef (for example) happen to be more frequent than the that no and no chef.",6 Results,[0],[0]
"This difference was apparently strong enough to make up for the low-frequency bigram at the start of the sentence.
",6 Results,[0],[0]
The RNNs did poorly on this task.,6 Results,[0],[0]
The accuracy of the single-task model was around 40%.,6 Results,[0],[0]
The multi-task did somewhat better on the simple NPIs (48%) and much better on the NPIs across a relative clause (73%).,6 Results,[0],[0]
"At the same time, an examination of the plot of log probability of each word in a sentence (Figure A.1 in the Supplementary Materials) suggests that the single-task RNN is in
ber representation learned for herself was not robust.",6 Results,[0],[0]
"Another possibility is that gender bias reduces the probability of an anaphoric relation between herself and words such as surgeon (Rudinger et al., 2018).
",6 Results,[0],[0]
"fact able to differentiate between the grammatical and ungrammatical sentences when it reaches the NPI, but this difference does not offset the overall probability advantage of the ungrammatical sentence (which is likely due to non-grammatical collocational factors).",6 Results,[0],[0]
"In any case, the fact that the n-gram baseline did not perform at chance suggests that there are non-syntactic cues to this task, complicating the interpretation of the performance of other LMs.
",6 Results,[0],[0]
"Perplexity: The perplexity of the n-gram model on the Wikipedia test data was 157.5, much higher than the perplexity of the single-task RNN (78.65) and the multi-task RNN (61.10).",6 Results,[0],[0]
"In other words, perplexity tracked accuracy on our syntactic data set – an unsatisfying outcome given our goal of dissociating perplexity and our syntactic evaluation method, but an expected one given that each model was conditioned on richer information than the previous one.",6 Results,[0],[0]
"In previous work, perplexity and syntactic judgment accuracy have been found to be partly dissociable (Kuncoro et al., 2018; Tran et al., 2018).
",6 Results,[0],[0]
Lexical variation and frequency: There was considerable lexical variation in the results; we have mentioned the surprising asymmetry between himself and herself above.,6 Results,[0],[0]
"As another case study, we examine variation in the results of the simple agreement condition in the single-
task RNN.",6 Results,[0],[0]
"Accuracy varied by verb, ranging from is and are, which had 100% accuracy, to swims, where accuracy was only 60% (recall that average accuracy was 94%).",6 Results,[0],[0]
"This may be a frequency effect: either the LM is learning less robust number representations for infrequent verbs, or the tail of the distribution over the vocabulary is more fragile during word prediction.",6 Results,[0],[0]
Pauls and Klein (2012) propose normalizing for unigram frequency when deriving acceptability judgments from an LM.,6 Results,[0],[0]
"Our preliminary experiments with this method did not significantly improve overall performance; regardless of the effectiveness of this method, such corrections should arguably not be necessary in an LM that adequately captures grammaticality.",6 Results,[0],[0]
The overall results in Table 1 were averaged over all of the possible number configurations within each condition.,7 Case study: agreement and object relative clauses,[0],[0]
"In this section, we take a closer look at agreement in sentences with an object RC (see Table 2).",7 Case study: agreement and object relative clauses,[0],[0]
"This kind of finer-grained analysis helps explain the cases in which the LMs are failing, and might reveal some of the patterns or heuristics the LMs are using.
",7 Case study: agreement and object relative clauses,[0],[0]
Performance in agreement across an object RC was poor.,7 Case study: agreement and object relative clauses,[0],[0]
Both RNNs made attraction errors: they often preferred the verb that agreed in number with the irrelevant embedded subject to the verb that agreed with the correct main subject.,7 Case study: agreement and object relative clauses,[0],[0]
"The multitask RNN showed greater symmetry between the simpler singular/singular and plural/plural cases, whereas the single-task RNN performed poorly even in these cases, often preferring a singular
verb when both subjects were plural.",7 Case study: agreement and object relative clauses,[0],[0]
"This default preference for singular verbs matches the behavior of younger children (Franck et al., 2004).
",7 Case study: agreement and object relative clauses,[0],[0]
"Performance in agreement within an object RC was better; still, the single-task RNN made the most errors when both subjects were singular, perhaps due to a heuristic in which a sentence with multiple subjects is likely to have a plural verb (as in coordination sentences).",7 Case study: agreement and object relative clauses,[0],[0]
"By contrast, the multitask model seemed to have a general bias towards singular subjects in this condition.",7 Case study: agreement and object relative clauses,[0],[0]
"Incidentally, the human results with object RCs were also unexpected: while attraction errors when the two subjects differ in number are to be expected (Wagers et al., 2009), our participants made a sizable number of errors even when both subjects were plural.
",7 Case study: agreement and object relative clauses,[0],[0]
"Despite the generally poor performance in object RCs, Figures A.2 and A.3 in the Supplementary Materials show that the single-task RNN is typically assigning a higher probability to the grammatical word of a minimal pair than to the ungrammatical word.",7 Case study: agreement and object relative clauses,[0],[0]
We have described a template-based data set for targeted syntactic evaluation of language models.,8 Discussion,[0],[0]
"The data set consists of pairs of sentences that are matched except for their grammaticality; we consider a language model to capture the relevant aspects of the grammar of the language if it assigns a higher probability to the grammatical sentence than to the ungrammatical one.
",8 Discussion,[0],[0]
"An RNN language model performed very well on local subject-verb agreement dependencies, significantly outperforming an n-gram baseline.
",8 Discussion,[0],[0]
This suggests that the task is a viable evaluation strategy.,8 Discussion,[0],[0]
"Even on simple cases, however, the RNN’s accuracy was sensitive to the particular lexical items that occurred in the sentence; this would not be expected if its syntactic representations were fully abstract.",8 Discussion,[0],[0]
"The RNN’s performance degraded markedly on non-local dependencies, approaching chance levels on agreement across an object relative clause.",8 Discussion,[0],[0]
Multi-task training with a syntactic objective (CCG supertagging) mitigated this drop in performance for some but not all of the dependencies we tested.,8 Discussion,[0],[0]
"We conjecture that the benefits of the inductive bias conferred by multi-task learning will be amplified when the amount of training data is limited.
",8 Discussion,[0],[0]
"Our results contrast with the results of Gulordava et al. (2018), who obtained a prediction accuracy of 81% on English sentences from their test corpus and 74% on constructed sentences modeled after sentences from the corpus.",8 Discussion,[0],[0]
"It is likely that our sentences are more syntactically challenging than the ones they were able to find in the relatively small manually annotated treebank they used.
",8 Discussion,[0],[0]
One limitation of our approach is that it is not always clear what constitutes a minimal grammaticality contrast.,8 Discussion,[0],[0]
"In the subject-verb agreement case, the contrast was clear: the two present-tense forms of the verb, e.g., laugh vs. laughs.",8 Discussion,[0],[0]
"Our NPI manipulations, on the other hand, were less successful: the members of the contrasts differed not only in their syntactic structure but also in low-level n-gram probabilities, making the performance on this particular contrast harder to interpret.
",8 Discussion,[0],[0]
"We emphasize that the goal of this article was not to advocate for LSTMs in particular as an effective architecture for modeling syntax; indeed, our results show that LSTM language models are far from matching naive annotators’ performance on this task, let alone performing at 100% accuracy.",8 Discussion,[0],[0]
"We hope that our data set, and future extensions to other phenomena and languages, will make it possible to measure progress in syntactic language modeling and will lead to better understanding of the syntactic generalizations captured by language models.",8 Discussion,[0],[0]
We would like to thank Ming Xiang for sharing materials from human experiments that inspired many of our test cases.,9 Acknowledgments,[0],[0]
"We also thank Brian Roark and the JHU Computational Psycholinguistics lab
for discussion, and Brian Leonard for help conducting the human experiment.",9 Acknowledgments,[0],[0]
We present a dataset for evaluating the grammaticality of the predictions of a language model.,abstractText,[0],[0]
"We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence.",abstractText,[0],[0]
"The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.",abstractText,[0],[0]
We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one.,abstractText,[0],[0]
"In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.",abstractText,[0],[0]
"Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online.",abstractText,[0],[0]
This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.,abstractText,[0],[0]
Targeted Syntactic Evaluation of Language Models,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 574–583 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Search engines help us find what we need among the vast array of available data.,1 Introduction,[0],[0]
"When we request some information using a long or inexact description of it, these systems, however, often fail to deliver relevant items.",1 Introduction,[0],[0]
"In this case, what typically follows is an iterative process in which we try to express our need differently in the hope that the system will return what we want.",1 Introduction,[0],[0]
This is a major issue in information retrieval.,1 Introduction,[0],[0]
"For instance, Huang and Efthimiadis (2009) estimate that 28-52% of all the web queries are modifications of previous ones.
",1 Introduction,[0],[0]
"To a certain extent, this problem occurs because search engines rely on matching words in the query with words in relevant documents, to
perform retrieval.",1 Introduction,[0],[0]
"If there is a mismatch between them, a relevant document may be missed.
",1 Introduction,[0],[0]
One way to address this problem is to automatically rewrite a query so that it becomes more likely to retrieve relevant documents.,1 Introduction,[0],[0]
This technique is known as automatic query reformulation.,1 Introduction,[0],[0]
"It typically expands the original query by adding terms from, for instance, dictionaries of synonyms such as WordNet (Miller, 1995), or from the initial set of retrieved documents (Xu and Croft, 1996).",1 Introduction,[0],[0]
"This latter type of reformulation is known as pseudo (or blind) relevance feedback (PRF), in which the relevance of each term of the retrieved documents is automatically inferred.
",1 Introduction,[0],[0]
"The proposed method is built on top of PRF but differs from previous works as we frame the query
574
reformulation problem as a reinforcement learning (RL) problem.",1 Introduction,[0],[0]
"An initial query is the natural language expression of the desired goal, and an agent (i.e. reformulator) learns to reformulate an initial query to maximize the expected return (i.e. retrieval performance) through actions (i.e. selecting terms for a new query).",1 Introduction,[0],[0]
The environment is a search engine which produces a new state (i.e. retrieved documents).,1 Introduction,[0],[0]
"Our framework is illustrated in Fig. 1.
",1 Introduction,[0],[0]
The most important implication of this framework is that a search engine is treated as a black box that an agent learns to use in order to retrieve more relevant items.,1 Introduction,[0],[0]
This opens the possibility of training an agent to use a search engine for a task other than the one it was originally intended for.,1 Introduction,[0],[0]
"To support this claim, we evaluate our agent on the task of question answering (Q&A), citation recommendation, and passage/snippet retrieval.
",1 Introduction,[0],[0]
"As for training data, we use two publicly available datasets (TREC-CAR and Jeopardy) and introduce a new one (MS Academic) with hundreds of thousands of query/relevant document pairs from the academic domain.
",1 Introduction,[0],[0]
"Furthermore, we present a method to estimate the upper bound performance of our RL-based model.",1 Introduction,[0],[0]
"Based on the estimated upper bound, we claim that this framework has a strong potential for future improvements.
",1 Introduction,[0],[0]
"Here we summarize our main contributions:
•",1 Introduction,[0],[0]
"A reinforcement learning framework for automatic query reformulation.
",1 Introduction,[0],[0]
"• A simple method to estimate the upper-bound performance of an RL-based model in a given environment.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
A new large dataset with hundreds of thousands of query/relevant document pairs.1,1 Introduction,[0],[0]
"In this section we describe the proposed method, illustrated in Fig. 2.
",2.1 Model Description,[0],[0]
"The inputs are a query q0 consisting of a sequence of words (w1, ..., wn) and a candidate term ti with some context words (ti−k, ..., ti+k), where k ≥ 0 is the context window size.",2.1 Model Description,[0],[0]
"Candidate terms
1The dataset and code to run the experiments are available at https://github.com/nyu-dl/ QueryReformulator.
are from q0 ∪ D0, the union of the terms in the original query and those from the documents D0 retrieved using q0.
",2.1 Model Description,[0],[0]
"We use a dictionary of pretrained word embeddings (Mikolov et al., 2013) to convert the symbolic terms wj",2.1 Model Description,[0],[0]
"and ti to their vector representations vj and ei ∈ Rd, respectively.",2.1 Model Description,[0],[0]
"We map outof-vocabulary terms to an additional vector that is learned during training.
",2.1 Model Description,[0],[0]
"We convert the sequence {vj} to a fixed-size vector φa(v) by using either a Convolutional Neural Network (CNN) followed by a max pooling operation over the entire sequence (Kim, 2014) or by using the last hidden state of a Recurrent Neural Network (RNN).2
Similarly, we fed the candidate term vectors ei to a CNN or RNN to obtain a vector representation φb(ei) for each term ti.",2.1 Model Description,[0],[0]
"The convolutional/recurrent layers serve an important role of capturing context information, especially for outof-vocabulary and rare terms.",2.1 Model Description,[0],[0]
"CNNs can process candidate terms in parallel, and, therefore, are faster for our application than RNNs.",2.1 Model Description,[0],[0]
"RNNs, on the other hand, can encode longer contexts.
",2.1 Model Description,[0],[0]
"Finally, we compute the probability of selecting
2To deal with variable-length inputs in a mini-batch, we pad smaller ones with zeros on both ends so they end up as long as the largest sample in the mini-batch.
ti as:
P (ti|q0) = σ(UT tanh(W (φa(v)‖φb(ei))",2.1 Model Description,[0],[0]
"+ b)), (1)
where σ is the sigmoid function, ‖ is the vector concatenation operation,",2.1 Model Description,[0],[0]
"W ∈ Rd×2d and U ∈ Rd are weights, and b ∈ R is a bias.
",2.1 Model Description,[0],[0]
"At test time, we define the set of terms used in the reformulated query as T = {ti | P (ti|q0) > }, where is a hyper-parameter.",2.1 Model Description,[0],[0]
"At training time, we sample the terms according to their probability distribution, T = {ti | α = 1∧α ∼ P (ti|q0)}.",2.1 Model Description,[0],[0]
"We concatenate the terms in T to form a reformulated query q′, which will then be used to retrieve a new set of documents D′.",2.1 Model Description,[0],[0]
One problem with the method previously described is that terms are selected independently.,2.2 Sequence Generation,[0],[0]
This may result in a reformulated query that contains duplicated terms since the same term can appear multiple times in the feedback documents.,2.2 Sequence Generation,[0],[0]
"Another problem is that the reformulated query can be very long, resulting in a slow retrieval.
",2.2 Sequence Generation,[0],[0]
"To solve these problems, we extend the model to sequentially generate a reformulated query, as proposed by Buck et al. (2017).",2.2 Sequence Generation,[0],[0]
We use a Recurrent Neural Network (RNN) that selects one term at a time from the pool of candidate terms and stops when a special token is selected.,2.2 Sequence Generation,[0],[0]
The advantage of this approach is that the model can remember the terms previously selected through its hidden state.,2.2 Sequence Generation,[0],[0]
"It can, therefore, produce more concise queries.
",2.2 Sequence Generation,[0],[0]
"We define the probability of selecting ti as the k-th term of a reformulated query as:
P (tki |q0) ∝",2.2 Sequence Generation,[0],[0]
"exp(φb(ei)Thk), (2) where hk is the hidden state vector at the k-th step, computed as:
hk = tanh(Waφa(v)",2.2 Sequence Generation,[0],[0]
"+Wbφb(tk−1) +Whhk−1), (3) where tk−1 is the term selected in the previous step and Wa ∈ Rd×d, Wb ∈ Rd×d, and Wh ∈ Rd×d are weight matrices.",2.2 Sequence Generation,[0],[0]
"In practice, we use an LSTM (Hochreiter and Schmidhuber, 1997) to encode the hidden state as this variant is known to perform better than a vanilla RNN.
",2.2 Sequence Generation,[0],[0]
We avoid normalizing over a large vocabulary by using only terms from the retrieved documents.,2.2 Sequence Generation,[0],[0]
"This makes inference faster and training practical since learning to select words from the whole
vocabulary might be too slow with reinforcement learning, although we leave this experiment for a future work.",2.2 Sequence Generation,[0],[0]
"We train the proposed model using REINFORCE (Williams, 1992) algorithm.",2.3 Training,[0],[0]
"The perexample stochastic objective is defined as
Ca = (R− R̄) ∑ t∈T",2.3 Training,[0],[0]
"− logP (t|q0), (4)
where R is the reward and R̄ is the baseline, computed by the value network as:
R̄ = σ(ST tanh(V (φa(v)‖ē) + b)), (5) where ē = 1N ∑N i=1 φb(ei), N = |q0 ∪ D0|, V ∈ Rd×2d and S ∈ Rd are weights and b ∈ R is a bias.",2.3 Training,[0],[0]
"We train the value network to minimize
Cb = α||R− R̄||2, (6) where α is a small constant (e.g. 0.1) multiplied to the loss in order to stabilize learning.",2.3 Training,[0],[0]
We conjecture that the stability is due to the slowly evolving value network which directly affects the learning of the policy.,2.3 Training,[0],[0]
"This effectively prevents the value network to fit extreme cases (unexpectedly high or low reward.)
",2.3 Training,[0],[0]
"We minimize Ca and Cb using stochastic gradient descent (SGD) with the gradient computed by backpropagation (Rumelhart et al., 1988).",2.3 Training,[0],[0]
"This allows the entire model to be trained end-to-end directly to optimize the retrieval performance.
",2.3 Training,[0],[0]
Entropy Regularization We observed that the probability distribution in Eq.(1) became highly peaked in preliminary experiments.,2.3 Training,[0],[0]
This phenomenon led to the trained model not being able to explore new terms that could lead to a betterreformulated query.,2.3 Training,[0],[0]
We address this issue by regularizing the negative entropy of the probability distribution.,2.3 Training,[0],[0]
We add the following regularization term to the original cost function in Eq.,2.3 Training,[0],[0]
"(4):
CH = −λ ∑
t∈q0∪D0 P (t|q0) logP (t|q0), (7)
where λ is a regularization coefficient.",2.3 Training,[0],[0]
"Query reformulation techniques are either based on a global method, which ignores a set of documents returned by the original query, or a local
method, which adjusts a query relative to the documents that initially appear to match the query.",3 Related Work,[0],[0]
"In this work, we focus on local methods.
",3 Related Work,[0],[0]
"A popular instantiation of a local method is the relevance model, which incorporates pseudo-relevance feedback into a language model form (Lavrenko and Croft, 2001).",3 Related Work,[0],[0]
The probability of adding a term to an expanded query is proportional to its probability of being generated by the language models obtained from the original query and the document the term occurs in.,3 Related Work,[0],[0]
"This framework has the advantage of not requiring query/relevant documents pairs as training data since inference is based on word co-occurrence statistics.
",3 Related Work,[0],[0]
"Unlike the relevance model, algorithms can be trained with supervised learning, as proposed by Cao et al. (2008).",3 Related Work,[0],[0]
A training dataset is automatically created by labeling each candidate term as relevant or not based on their individual contribution to the retrieval performance.,3 Related Work,[0],[0]
Then a binary classifier is trained to select expansion terms.,3 Related Work,[0],[0]
"In Section 4, we present a neural network-based implementation of this supervised approach.
",3 Related Work,[0],[0]
A generalization of this supervised framework is to iteratively reformulate the query by selecting one candidate term at each retrieval step.,3 Related Work,[0],[0]
"This can be viewed as navigating a graph where the nodes represent queries and associated retrieved results and edges exist between nodes whose queries are simple reformulations of each other (Diaz, 2016).",3 Related Work,[0],[0]
"However, it can be slow to reformulate a query this way as the search engine must be queried for each newly added term.",3 Related Work,[0],[0]
"In our method, on the contrary, the search engine is queried with multiple new terms at once.
",3 Related Work,[0],[0]
"An alternative technique based on supervised learning is to learn a common latent representation of queries and relevant documents terms by using a click-through dataset (Sordoni et al., 2014).",3 Related Work,[0],[0]
Neighboring document terms of a query in the latent space are selected to form an expanded query.,3 Related Work,[0],[0]
"Instead of using a click-through dataset, which is often proprietary, it is possible to use an alternative dataset consisting of anchor text/title pairs.",3 Related Work,[0],[0]
"In contrast, our approach does not require a dataset of paired queries as it learns term selection strategies via reinforcement learning.
",3 Related Work,[0],[0]
"Perhaps the closest work to ours is that by Narasimhan et al. (2016), in which a reinforcement learning based approach is used to reformu-
late queries iteratively.",3 Related Work,[0],[0]
A key difference is that in their work the reformulation component uses domain-specific template queries.,3 Related Work,[0],[0]
"Our method, on the other hand, assumes open-domain queries.",3 Related Work,[0],[0]
"In this section we describe our experimental setup, including baselines against which we compare the proposed method, metrics, reward for RL-based models, datasets and implementation details.",4 Experiments,[0],[0]
Raw: The original query is given to a search engine without any modification.,4.1 Baseline Methods,[0],[0]
"We evaluate two search engines in their default configuration: Lucene3 (Raw-Lucene) and Google Search4 (Raw-Google).
",4.1 Baseline Methods,[0],[0]
Pseudo Relevance Feedback (PRF-TFIDF):,4.1 Baseline Methods,[0],[0]
A query is expanded with terms from the documents retrieved by a search engine using the original query.,4.1 Baseline Methods,[0],[0]
"In this work, the top-N TF-IDF terms from each of the top-K retrieved documents are added to the original query, where N and K are selected by a grid search on the validation data.
",4.1 Baseline Methods,[0],[0]
PRF-Relevance Model (PRF-RM):,4.1 Baseline Methods,[0],[0]
This is a popular relevance model for query expansion by Lavrenko and Croft (2001).,4.1 Baseline Methods,[0],[0]
"The probability of using a term t in an expanded query is given by:
P (t|q0) =",4.1 Baseline Methods,[0],[0]
(1− λ)P ′(t|q0),4.1 Baseline Methods,[0],[0]
"+ λ ∑ d∈D0 P (d)P (t|d)P (q0|d), (8)
where P (d) is the probability of retrieving the document d, assumed uniform over the set, P (t|d) and P (q0|d) are the probabilities assigned by the language model obtained from d to t and q0, respectively.",4.1 Baseline Methods,[0],[0]
"P ′(t|q0) = tf(t∈q)|q| , where tf(t, d) is the term frequency of t in d. We set the interpolation parameter λ to 0.5, following Zhai and Lafferty (2001).
",4.1 Baseline Methods,[0],[0]
"We use a Dirichlet smoothed language model (Zhai and Lafferty, 2001) to compute a language model from a document d ∈ D0:
P (t|d) = tf(t, d) + uP (t|C)|d|+ u , (9)
3https://lucene.apache.org/ 4https://cse.google.com/cse/
where u is a scalar constant (u = 1500 in our experiments), and P (t|C) is the probability of t occurring in the entire corpus C.
We use the N terms with the highest P (t|q0) in an expanded query, whereN is a hyper-parameter.
",4.1 Baseline Methods,[0],[0]
"Embeddings Similarity: Inspired by the methods proposed by Roy et al. (2016) and Kuzi et al. (2016), the top-N terms are selected based on the cosine similarity of their embeddings against the original query embedding.",4.1 Baseline Methods,[0],[0]
"Candidate terms come from documents retrieved using the original query (PRF-Emb), or from a fixed vocabulary (Vocab-Emb).",4.1 Baseline Methods,[0],[0]
"We use pretrained embeddings from Mikolov et al. (2013), and it contains 374,000 words.",4.1 Baseline Methods,[0],[0]
Supervised Learning (SL): Here we detail a deep learning-based variant of the method proposed by Cao et al. (2008).,4.2 Proposed Methods,[0],[0]
It assumes that query terms contribute independently to the retrieval performance.,4.2 Proposed Methods,[0],[0]
We thus train a binary classifier to select a term if the retrieval performance increases beyond a preset threshold when that term is added to the original query.,4.2 Proposed Methods,[0],[0]
"More specifically, we mark a term as relevant if (R′ −R)/R > 0.005, where R and R′ are the retrieval performances of the original query and the query expanded with the term, respectively.
",4.2 Proposed Methods,[0],[0]
"We experiment with two variants of this method: one in which we use a convolutional network for both original query and candidate terms (SL-CNN), and the other in which we replace the convolutional network with a single hidden layer feed-forward neural network (SL-FF).",4.2 Proposed Methods,[0],[0]
"In this variant, we average the output vectors of the neural network to obtain a fixed size representation of q0.
",4.2 Proposed Methods,[0],[0]
Reinforcement Learning (RL): We use multiple variants of the proposed RL method.,4.2 Proposed Methods,[0],[0]
"RL-CNN and RL-RNN are the models described in Section 2.1, in which the former uses CNNs to encode query and term features and the latter uses RNNs (more specifically, bidirectional LSTMs).",4.2 Proposed Methods,[0],[0]
RL-FF is the model in which term and query vectors are encoded by single hidden layer feed-forward neural networks.,4.2 Proposed Methods,[0],[0]
"In the RL-RNN-SEQ model, we add the sequential generator described in Section 2.2 to the RL-RNN variant.",4.2 Proposed Methods,[0],[0]
"We summarize in Table 1 the datasets.
",4.3 Datasets,[0],[0]
TREC - Complex Answer Retrieval (TRECCAR),4.3 Datasets,[0],[0]
"This is a publicly available dataset automatically created from Wikipedia whose goal is to encourage the development of methods that respond to more complex queries with longer answers (Dietz and Ben, 2017).",4.3 Datasets,[0],[0]
A query is the concatenation of an article title and one of its section titles.,4.3 Datasets,[0],[0]
The ground-truth documents are the paragraphs within that section.,4.3 Datasets,[0],[0]
"For example, a query is “Sea Turtle, Diet” and the ground truth documents are the paragraphs in the section “Diet” of the “Sea Turtle” article.",4.3 Datasets,[0],[0]
"The corpus consists of all the English Wikipedia paragraphs, except the abstracts.",4.3 Datasets,[0],[0]
"The released dataset has five predefined folds, and we use the first three as the training set and the remaining two as validation and test sets, respectively.
",4.3 Datasets,[0],[0]
Jeopardy This is a publicly available Q&A dataset introduced by Nogueira and Cho (2016).,4.3 Datasets,[0],[0]
A query is a question from the Jeopardy! TV Show and the corresponding document is a Wikipedia article whose title is the answer.,4.3 Datasets,[0],[0]
"For example, a query is “For the last eight years of his life, Galileo was under house arrest for espousing this mans theory” and the answer is the Wikipedia article titled “Nicolaus Copernicus”.",4.3 Datasets,[0],[0]
"The corpus consists of all the articles in the English Wikipedia.
",4.3 Datasets,[0],[0]
Microsoft Academic (MSA),4.3 Datasets,[0],[0]
This dataset consists of academic papers crawled from Microsoft Academic API.5,4.3 Datasets,[0],[0]
"The crawler started at the paper Silver et al. (2016) and traversed the graph of references until 500,000 papers were crawled.",4.3 Datasets,[0],[0]
We then removed papers that had no reference within or whose abstract had less than 100 characters.,4.3 Datasets,[0],[0]
"We ended up with 480,000 papers.
",4.3 Datasets,[0],[0]
"A query is the title of a paper, and the groundtruth answer consists of the papers cited within.",4.3 Datasets,[0],[0]
Each document in the corpus consists of its title and abstract.6,4.3 Datasets,[0],[0]
"Three metrics are used to evaluate performance:
Recall@K:",4.4 Metrics and Reward,[0],[0]
"Recall of the top-K retrieved documents:
R@K = |DK",4.4 Metrics and Reward,[0],[0]
"∩D∗| |D∗| , (10)
5https://www.microsoft.com/cognitive-services/enus/academic-knowledge-api
6This was done to avoid a large computational overhead for indexing full papers.
where DK are the top-K retrieved documents and D∗ are the relevant documents.",4.4 Metrics and Reward,[0],[0]
"Since one of the goals of query reformulation is to increase the proportion of relevant documents returned, recall is our main metric.
",4.4 Metrics and Reward,[0],[0]
"Precision@K: Precision of the top-K retrieved documents:
P@K = |DK",4.4 Metrics and Reward,[0],[0]
∩D∗| |DK,4.4 Metrics and Reward,[0],[0]
"| (11)
Precision captures the proportion of relevant documents among the returned ones.",4.4 Metrics and Reward,[0],[0]
"Despite not being the main goal of a reformulation method, improvements in precision are also expected with a good query reformulation method.",4.4 Metrics and Reward,[0],[0]
"Therefore, we include this metric.
",4.4 Metrics and Reward,[0],[0]
Mean Average Precision:,4.4 Metrics and Reward,[0],[0]
"The average precision of the top-K retrieved documents is defined as:
AP@K = ∑K
k=1 P@k × rel(k) |D∗| , (12)
where
rel(k) = { 1, if the k-th document is relevant; 0, otherwise.
",4.4 Metrics and Reward,[0],[0]
"(13)
The mean average precision of a set of queries Q is then:
MAP@K = 1 |Q| ∑ q∈Q AP@Kq, (14)
where AP@Kq is the average precision at K for a query q.",4.4 Metrics and Reward,[0],[0]
"This metric values the position of a relevant document in a returned list and is, therefore, complementary to precision and recall.
",4.4 Metrics and Reward,[0],[0]
"Reward We use R@K as a reward when training the proposed RL-based models as this metric has shown to be effective in improving the other metrics as well.
",4.4 Metrics and Reward,[0],[0]
"SL-Oracle In addition to the baseline methods and proposed reinforcement learning approach, we report two oracle performance bounds.",4.4 Metrics and Reward,[0],[0]
The first oracle is a supervised learning oracle (SLOracle).,4.4 Metrics and Reward,[0],[0]
It is a classifier that perfectly selects terms that will increase performance according to the procedure described in Section 4.2.,4.4 Metrics and Reward,[0],[0]
This measure serves as an upper-bound for the supervised methods.,4.4 Metrics and Reward,[0],[0]
Notice that this heuristic assumes that each term contributes independently from all the other terms to the retrieval performance.,4.4 Metrics and Reward,[0],[0]
"There may be, however, other ways to explore the dependency of terms that would lead to a higher performance.
",4.4 Metrics and Reward,[0],[0]
"RL-Oracle Second, we introduce a reinforcement learning oracle (RL-Oracle) which estimates a conservative upper-bound performance for the RL models.",4.4 Metrics and Reward,[0],[0]
"Unlike the SL-Oracle, it does not assume that each term contributes independently to the retrieval performance.",4.4 Metrics and Reward,[0],[0]
"It works as follows: first, the validation or test set is divided into N small subsets {Ai}Ni=1 (each with 100 examples, for instance).",4.4 Metrics and Reward,[0],[0]
"An RL model is trained on each subset Ai until it overfits, that is, until the reward R∗i stops increasing or an early stop mechanism ends training.7",4.4 Metrics and Reward,[0],[0]
"Finally, we compute the oracle performance R∗ as the average reward over all the subsets: R∗ = 1N ∑N i=1R ∗",4.4 Metrics and Reward,[0],[0]
"i .
",4.4 Metrics and Reward,[0],[0]
"This upper bound by the RL-Oracle is, however, conservative since there might exist better reformulation strategies that the RL model was not able to discover.",4.4 Metrics and Reward,[0],[0]
"Search engine We use Lucene and BM25 as the search engine and the ranking function, respectively, for all PRF, SL and RL methods.",4.5 Implementation Details,[0],[0]
"For RawGoogle, we restrict the search to the wikipedia.org domain when evaluating its performance on the Jeopardy dataset.",4.5 Implementation Details,[0],[0]
"We could not apply the same restriction to the two other datasets as Google does not index Wikipedia paragraphs, and as it is not trivial to match papers from MS Academic to the ones returned by Google Search.
Candidate terms We use Wikipedia articles as a source for candidate terms since it is a well curated, clean corpus, with diverse topics.
",4.5 Implementation Details,[0],[0]
"At training and test times of SL methods, and at test time of RL methods, the candidate terms are from the first M words of the top-K Wikipedia articles retrieved.",4.5 Implementation Details,[0],[0]
"We select M and K using grid search on the validation set over {50, 100, 200, 300} and {1, 3, 5, 7}, respectively.",4.5 Implementation Details,[0],[0]
The best values are M = 300 and K = 7.,4.5 Implementation Details,[0],[0]
"These correspond to the maximum number of terms we could fit in a single GPU.
7The subset should be small enough, or the model should be large enough so it can overfit.
",4.5 Implementation Details,[0],[0]
"At training time of an RL model, we use only one document uniformly sampled from the top-K retrieved ones as a source for candidate terms, as this leads to a faster learning.
",4.5 Implementation Details,[0],[0]
"For the PRF methods, the top-M terms according to a relevance metric (i.e., TF-IDF for PRF-TFIDF, cosine similarity for PRF-Emb, and conditional probability for PRF-RM) from each of the top-K retrieved documents are added to the original query.",4.5 Implementation Details,[0],[0]
"We select M and K using grid search over {10, 50, 100, 200, 300, 500} and {1, 3, 5, 9, 11}, respectively.",4.5 Implementation Details,[0],[0]
"The best values are M = 300 and K = 9.
",4.5 Implementation Details,[0],[0]
"Multiple Reformulation Rounds Although our framework supports multiple rounds of search and reformulation, we did not find any significant improvement in reformulating a query more than once.",4.5 Implementation Details,[0],[0]
"Therefore, the numbers reported in the results section were all obtained from models running two rounds of search and reformulation.
",4.5 Implementation Details,[0],[0]
"Neural Network Setup For SL-CNN and RLCNN variants, we use a 2-layer convolutional network for the original query.",4.5 Implementation Details,[0],[0]
Each layer has a window size of 3 and 256 filters.,4.5 Implementation Details,[0],[0]
"We use a 2-layer convolutional network for candidate terms with window sizes of 9 and 3, respectively, and 256 filters in each layer.",4.5 Implementation Details,[0],[0]
"We set the dimension d of the weight matrices W,S,U , and V to 256.",4.5 Implementation Details,[0],[0]
"For the optimizer, we use ADAM (Kingma and Ba, 2014) with α = 10−4, β1 = 0.9, β2 = 0.999, and = 10−8.",4.5 Implementation Details,[0],[0]
"We set the entropy regularization coefficient λ to 10−3.
",4.5 Implementation Details,[0],[0]
"For RL-RNN and RL-RNN-SEQ, we use a 2- layer bidirectional LSTM with 256 hidden units in each layer.",4.5 Implementation Details,[0],[0]
We clip the gradients to unit norm.,4.5 Implementation Details,[0],[0]
"For RL-RNN-SEQ, we set the maximum possible
number of generated terms to 50 and we use beam search of size four at test time.
",4.5 Implementation Details,[0],[0]
"We fix the dictionary of pre-trained word embeddings during training, except the vector for outof-vocabulary words.",4.5 Implementation Details,[0],[0]
We found that this led to faster convergence and observed no difference in the overall performance when compared to learning embeddings during training.,4.5 Implementation Details,[0],[0]
Table 2 shows the main result.,5 Results and Discussion,[0],[0]
"As expected, reformulation based methods work better than using the original query alone.",5 Results and Discussion,[0],[0]
"Supervised methods (SL-FF and SL-CNN) have in general a better performance than unsupervised ones (PRF-TFIDF, PRF-RM, PRF-Emb, and Emb-Vocab), but perform worse than RL-based models (RL-FF, RLCNN, RL-RNN, and RL-RNN-SEQ).
",5 Results and Discussion,[0],[0]
"RL-RNN-SEQ performs slightly worse than RL-RNN but produces queries that are three times shorter, on average (15 vs 47 words).",5 Results and Discussion,[0],[0]
"Thus, RLRNN-SEQ is faster in retrieving documents and therefore might be a better candidate for a production implementation.
",5 Results and Discussion,[0],[0]
"The performance gap between the oracle and best performing method (Table 2, RL-Oracle vs. RL-RNN) suggests that there is a large room for improvement.",5 Results and Discussion,[0],[0]
"The cause for this gap is unknown but we suspect, for instance, an inherent difficulty in learning a good selection strategy and the partial observability from using a black box search engine.",5 Results and Discussion,[0],[0]
The proportion of relevant terms selected by the SL- and RL-Oracles over the total number of candidate terms (Table 3) indicates that only a small subset of terms are useful for the reformulation.,5.1 Relevant Terms per Document,[0],[0]
"Thus, we may conclude that the proposed method was able to learn an efficient term selection strategy in an environment where relevant terms are infrequent.",5.1 Relevant Terms per Document,[0],[0]
Fig. 3 shows the improvement in recall as more candidate terms are provided to a reformulation method.,5.2 Scalability: Number of Terms vs Recall,[0],[0]
"The RL-based model benefits from more candidate terms, whereas the classical PRF method quickly saturates.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"In our experiments, the best performing RL-based model uses the maximum number of candidate terms that we could fit
on a single GPU.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"We, therefore, expect further improvements with more computational resources.",5.2 Scalability: Number of Terms vs Recall,[0],[0]
"We show two examples of queries and the probabilities of each candidate term of being selected by the RL-CNN model in Fig. 4.
Notice that terms that are more related to the query have higher probabilities, although common words such as ”the” are also selected.",5.3 Qualitative Analysis,[0],[0]
"This is a consequence of our choice of a reward that does
not penalize the selection of neutral terms.",5.3 Qualitative Analysis,[0],[0]
"In Table 4 we show an original and reformulated query examples extracted from the MS Academic and TREC-CAR datasets, and their top-3 retrieved documents.",5.3 Qualitative Analysis,[0],[0]
Notice that the reformulated query retrieves more relevant documents than the original one.,5.3 Qualitative Analysis,[0],[0]
"As we conjectured earlier, we see that a search engine tends to return a document simply with the largest overlap in the text, necessitating the reformulation of a query to retrieve semantically relevant documents.
",5.3 Qualitative Analysis,[0],[0]
"Same query, different tasks We compare in Table 5 the reformulation of a sample query made by models trained on different datasets.",5.3 Qualitative Analysis,[0],[0]
"The model trained on TREC-CAR selects terms that are similar to the ones in the original query, such as “serves” and “accreditation”.",5.3 Qualitative Analysis,[0],[0]
These selections are expected for this task since similar terms can be effective in retrieving similar paragraphs.,5.3 Qualitative Analysis,[0],[0]
"On the other hand, the model trained on Jeopardy prefers to select proper nouns, such as “Tunxis”, as these have a higher chance of being an answer to the question.",5.3 Qualitative Analysis,[0],[0]
"The model trained on MSA selects terms that cover different aspects of the entity being queried, such as “arts center” and “library”, since retrieving a diverse set of documents is necessary for the task the of citation recommendation.",5.3 Qualitative Analysis,[0],[0]
"Our best model, RL-RNN, takes 8-10 days to train on a single K80 GPU.",5.4 Training and Inference Times,[0],[0]
"At inference time, it takes
approximately one second to reformulate a batch of 64 queries.",5.4 Training and Inference Times,[0],[0]
Approximately 40% of this time is to retrieve documents from the search engine.,5.4 Training and Inference Times,[0],[0]
We introduced a reinforcement learning framework for task-oriented automatic query reformulation.,6 Conclusion,[0],[0]
An appealing aspect of this framework is that an agent can be trained to use a search engine for a specific task.,6 Conclusion,[0],[0]
The empirical evaluation has confirmed that the proposed approach outperforms strong baselines in the three separate tasks.,6 Conclusion,[0],[0]
The analysis based on two oracle approaches has revealed that there is a meaningful room for further development.,6 Conclusion,[0],[0]
"In the future, more research is necessary in the directions of (1) iterative reformulation under the proposed framework, (2) using information from modalities other than text, and (3) better reinforcement learning algorithms for a partially-observable environment.",6 Conclusion,[0],[0]
RN is funded by Coordenao de Aperfeioamento de Pessoal de Nvel Superior (CAPES).,Acknowledgements,[0],[0]
"KC thanks support by Facebook, Google and NVIDIA.",Acknowledgements,[0],[0]
This work was partly funded by the Defense Advanced Research Projects Agency (DARPA) D3M program.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.",Acknowledgements,[0],[0]
Search engines play an important role in our everyday lives by assisting us in finding the information we need.,abstractText,[0],[0]
"When we input a complex query, however, results are often far from satisfactory.",abstractText,[0],[0]
"In this work, we introduce a query reformulation system based on a neural network that rewrites a query to maximize the number of relevant documents returned.",abstractText,[0],[0]
We train this neural network with reinforcement learning.,abstractText,[0],[0]
"The actions correspond to selecting terms to build a reformulated query, and the reward is the document recall.",abstractText,[0],[0]
We evaluate our approach on three datasets against strong baselines and show a relative improvement of 5-20% in terms of recall.,abstractText,[0],[0]
"Furthermore, we present a simple method to estimate a conservative upperbound performance of a model in a particular environment and verify that there is still large room for improvements.",abstractText,[0],[0]
Task-Oriented Query Reformulation with Reinforcement Learning,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1088–1097 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1088",text,[0],[0]
"Automated essay scoring (AES) utilizes natural language processing and machine learning techniques to automatically rate essays written for a target prompt (Dikli, 2006).",1 Introduction,[0],[0]
"Currently, the AES systems have been widely used in large-scale English writing tests, e.g. Graduate Record Examination (GRE), to reduce the human efforts in the writing assessments (Attali and Burstein, 2006).
",1 Introduction,[0],[0]
"Existing AES approaches are promptdependent, where, given a target prompt, rated essays for this particular prompt are required for training (Dikli, 2006; Williamson, 2009; Foltz et al., 1999).",1 Introduction,[0],[0]
"While the established models are
effective (Chen and He, 2013; Taghipour and Ng, 2016; Alikaniotis et al., 2016; Cummins et al., 2016; Dong et al., 2017), we argue that the models for prompt-independent AES are also desirable to allow for better feasibility and flexibility of AES systems especially when the rated essays for a target prompt are difficult to obtain or even unaccessible.",1 Introduction,[0],[0]
"For example, in a writing test within a small class, students are asked to write essays for a target prompt without any rated examples, where the prompt-dependent methods are unlikely to provide effective AES due to the lack of training data.",1 Introduction,[0],[0]
"Prompt-independent AES, however, has drawn little attention in the literature, where there only exists unrated essays written for the target prompt, as well as the rated essays for several non-target prompts.
",1 Introduction,[0],[0]
"We argue that it is not straightforward, if possible, to apply the established promptdependent AES methods for the mentioned prompt-independent scenario.",1 Introduction,[0],[0]
"On one hand, essays for different prompts may differ a lot in the uses of vocabulary, the structure, and the grammatic characteristics; on the other hand, however, established prompt-dependent AES models are designed to learn from these prompt-specific features, including the on/off-topic degree, the tf - idf weights of topical terms (Attali and Burstein, 2006; Dikli, 2006), and the n-gram features extracted from word semantic embeddings (Dong and Zhang, 2016; Alikaniotis et al., 2016).",1 Introduction,[0],[0]
"Consequently, the prompt-dependent models can hardly learn generalized rules from rated essays for nontarget prompts, and are not suitable for the promptindependent AES.
Being aware of this difficulty, to this end, a twostage deep neural network, coined as TDNN, is proposed to tackle the prompt-independent AES problem.",1 Introduction,[0],[0]
"In particular, to mitigate the lack of the prompt-dependent labeled data, at the first stage,
a shallow model is trained on a number of rated essays for several non-target prompts; given a target prompt and a set of essays to rate, the trained model is employed to generate pseudo training data by selecting essays with the extreme quality.",1 Introduction,[0],[0]
"At the second stage, a novel end-to-end hybrid deep neural network learns prompt-dependent features from these selected training data, by considering semantic, part-of-speech, and syntactic features.
",1 Introduction,[0],[0]
"The contributions in this paper are threefold: 1) a two-stage learning framework is proposed to bridge the gap between the target and non-target prompts, by only consuming rated essays for nontarget prompts as training data; 2) a novel deep model is proposed to learn from pseudo labels by considering semantic, part-of-speech, and syntactic features; and most importantly, 3) to the best of our knowledge, the proposed TDNN is actually the first approach dedicated to addressing the prompt-independent AES.",1 Introduction,[0],[0]
"Evaluation on the standard ASAP dataset demonstrates the effectiveness of the proposed method.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we describe our novel TDNN model, including the two-stage framework and the proposed deep model.",1 Introduction,[0],[0]
"Following that, we describe the setup of our empirical study in Section 3, thereafter present the results and provide analyzes in Section 4.",1 Introduction,[0],[0]
"Section 5 recaps existing literature and put our work in context, before drawing final conclusions in Section 6.",1 Introduction,[0],[0]
"In this section, the proposed two-stage deep neural network (TDNN) for prompt-independent AES is described.",2 Two-stage Deep Neural Network for AES,[0],[0]
"To accurately rate an essay, on one hand, we need to consider its pertinence to the given prompt; on the other hand, the organization, the analyzes, as well as the uses of the vocabulary are all crucial for the assessment.",2 Two-stage Deep Neural Network for AES,[0],[0]
"Henceforth, both prompt-dependent and -independent factors should be considered, but the latter ones actually do not require prompt-dependent training data.",2 Two-stage Deep Neural Network for AES,[0],[0]
"Accordingly, in the proposed framework, a supervised ranking model is first trained to learn from prompt-independent data, hoping to roughly assess essays without considering the prompt; subsequently, given the test dataset, namely, a set of essays for a target prompt, a subset of essays are selected as positive and negative training
data based on the prediction of the trained model from the first stage; ultimately, a novel deep model is proposed to learn both prompt-dependent and -independent factors on this selected subset.",2 Two-stage Deep Neural Network for AES,[0],[0]
"As indicated in Figure 1, the proposed framework includes two stages.",2 Two-stage Deep Neural Network for AES,[0],[0]
Prompt-independent stage.,2.1 Overview,[0],[0]
"Only the promptindependent factors are considered to train a shallow model, aiming to recognize the essays with the extreme quality in the test dataset, where the rated essays for non-target prompts are used for training.",2.1 Overview,[0],[0]
"Intuitively, one could recognize essays with the highest and the lowest scores correctly by solely examining their quality of writing, e.g., the number of typos, without even understanding them, and the prompt-independent features such as the number of grammatic and spelling errors should be sufficient to fulfill this screening procedure.",2.1 Overview,[0],[0]
"Accordingly, a supervised model trained solely on prompt-independent features is employed to identify the essays with the highest and lowest scores in a given set of essays for the target prompt, which are used as the positive and negative training data in the follow-up prompt-dependent learning phase.
",2.1 Overview,[0],[0]
Prompt-dependent stage.,2.1 Overview,[0],[0]
"Intuitively, most essays are with a quality in between the extremes, requiring a good understanding of their meaning to make an accurate assessment, e.g., whether the examples from the essay are convincing or whether the analyzes are insightful, making the consideration of prompt-dependent features crucial.",2.1 Overview,[0],[0]
"To achieve that, a model is trained to learn from the comparison between essays with the highest and lowest scores for the target prompt according to the predictions from the first step.",2.1 Overview,[0],[0]
"Akin to the settings in transductive transfer learning (Pan and
Yang, 2010), given essays for a particular prompt, quite a few confident essays at two extremes are selected and are used to train another model for a fine-grained content-based prompt-dependent assessment.",2.1 Overview,[0],[0]
"To enable this, a powerful deep model is proposed to consider the content of the essays from different perspectives using semantic, part-of-speech (POS) and syntactic network.",2.1 Overview,[0],[0]
"After being trained with the selected essays, the deep model is expected to memorize the properties of a good essay in response to the target prompt, thereafter accurately assessing all essays for it.",2.1 Overview,[0],[0]
"In Section 2.2, building blocks for the selection of the training data and the proposed deep model are described in details.",2.1 Overview,[0],[0]
Select confident essays as training data.,2.2 Building Blocks,[0],[0]
"The identification of the extremes is relatively simple, where a RankSVM (Joachims, 2002) is trained on essays for different non-target prompts, avoiding the risks of over-fitting some particular prompts.",2.2 Building Blocks,[0],[0]
"A set of established prompt-independent features are employed, which are listed in Table 2.",2.2 Building Blocks,[0],[0]
"Given a prompt and a set of essays for evaluation, to begin with, the trained RankSVM is used to assign prediction scores to individual prompt-essay pairs, which are uniformly transformed into a 10- point scale.",2.2 Building Blocks,[0],[0]
"Thereafter, the essays with predicted scores in [0, 4] and [8, 10] are selected as negative and positive examples respectively, serving as the bad and good templates for training in the next stage.",2.2 Building Blocks,[0],[0]
"Intuitively, an essay with a score beyond eight out of a 10-point scale is considered good, while the one receiving less than or equal to four, is considered to be with a poor quality.
",2.2 Building Blocks,[0],[0]
A hybrid deep model for fine-grained assessment.,2.2 Building Blocks,[0],[0]
"To enable a prompt-dependent assessment, a model is desired to comprehensively capture the ways in which a prompt is described or discussed in an essay.",2.2 Building Blocks,[0],[0]
"In this paper, semantic meaning, part-of-speech (POS), and the syntactic taggings of the token sequence from an essay are considered, grasping the quality of an essay for a target prompt.",2.2 Building Blocks,[0],[0]
The model architecture is summarized in Figure 2.,2.2 Building Blocks,[0],[0]
"Intuitively, the model learns the semantic meaning of an essay by encoding it in terms of a sequence of word embeddings, denoted as −→e sem, hoping to understand what the essay is about; in addition, the part-of-speech information is encoded as a sequence of POS tag-
gings, coined as −→e pos; ultimately, the structural connections between different components in an essay (e.g., terms or phrases) are further captured via syntactic network, leading to −→e synt, where the model learns the organization of the essay.",2.2 Building Blocks,[0],[0]
"Akin to (Li et al., 2015) and (Zhou and Xu, 2015), biLSTM is employed as a basic component to encode a sequence.",2.2 Building Blocks,[0],[0]
"Three features are separately captured using the stacked bi-LSTM layers as building blocks to encode different embeddings, whose outputs are subsequently concatenated and fed into several dense layers, generating the ultimate rating.",2.2 Building Blocks,[0],[0]
"In the following, the architecture of the model is described in details.
- Semantic embedding.",2.2 Building Blocks,[0],[0]
"Akin to the existing works (Alikaniotis et al., 2016; Taghipour and Ng, 2016), semantic word embeddings, namely, the pre-trained 50-dimension GloVe (Pennington et al., 2014), are employed.",2.2 Building Blocks,[0],[0]
"On top of the word embeddings, two bi-LSTM layers are stacked, namely, the essay layer is constructed on top of the sentence layer, ending up with the semantic representation of the whole essay, which is denoted as −→e sem in Figure 2.",2.2 Building Blocks,[0],[0]
-,2.2 Building Blocks,[0],[0]
"Part-Of-Speech (POS) embeddings for individual terms are first generated by the Stanford Tagger (Toutanova et al., 2003), where 36 different POS tags present.",2.2 Building Blocks,[0],[0]
"Accordingly, individual words are embedded with 36-dimensional one-hot representation, and is transformed to a 50-dimensional vector through a lookup layer.",2.2 Building Blocks,[0],[0]
"After that, two biLSTM layers are stacked, leading to −→e pos.",2.2 Building Blocks,[0],[0]
"Take Figure 3 for example, given a sentence “Attention please, here is an example.”",2.2 Building Blocks,[0],[0]
", it is first converted into a POS sequence using the tagger, namely, VB, VBP, RB, VBZ, DT, NN; thereafter it is further mapped to vector space through one-hot embedding and a lookup layer.
",2.2 Building Blocks,[0],[0]
"- Syntactic embedding aims at encoding an essay in terms of the syntactic relationships among different syntactic components, by encoding an essay recursively.",2.2 Building Blocks,[0],[0]
"The Stanford Parser (Socher et al., 2013) is employed to label the syntactic structure of words and phrases in sentences, accounting for 59 different types in total.",2.2 Building Blocks,[0],[0]
"Similar to (Tai et al., 2015), we opt for three stacked bi-LSTM, aiming at encoding individual phrases, sentences, and ultimately the whole essay in sequence.",2.2 Building Blocks,[0],[0]
"In particular, according to the hierarchical structure from a parsing tree, the phrase-level bi-LSTM first encodes different phrases by consuming syntactic
embeddings ( −→ Sti in Figure 2) from a lookup table of individual syntactic units in the tree; thereafter, the encoded dense layers in individual sentences are further consumed by a sentence-level bi-LSTM, ending up with sentence-level syntactic representations, which are ultimately combined by the essay-level bi-LSTM, resulting in −→e synt.",2.2 Building Blocks,[0],[0]
"For example, the parsed tree for a sentence “Attention please, here is an example.” is displayed in Figure 3.",2.2 Building Blocks,[0],[0]
"To start with, the sentence is parsed into ((NP VP)(NP VP NP)), and the dense embeddings are fetched from a lookup table for all tokens, namely, NP and VP; thereafter, the phraselevel bi-LSTM encodes (NP VP) and (NP VP NP) separately, which are further consumed by the sentence-level bi-LSTM.",2.2 Building Blocks,[0],[0]
"Afterward, essay-level bi-LSTM further combines the representations of different sentences into −→e synt.
- Combination.",2.2 Building Blocks,[0],[0]
"A feed-forward network linearly transforms the concatenated representations of an essay from the mentioned three perspectives into a scalar, which is further normalized into [0, 1] with a sigmoid function.",2.2 Building Blocks,[0],[0]
Objective.,2.3 Objective and Training,[0],[0]
"Mean square error (MSE) is optimized, which is widely used as a loss function in regression tasks.",2.3 Objective and Training,[0],[0]
"Given N pairs of a target prompt pi and an essay ei, MSE measures the average value of square error between the normalized gold standard rating r∗(pi, ei) and the predicted rating r(pi, ei) assigned by the AES model, as summarized in Equation 1.
1
N N∑ i=1",2.3 Objective and Training,[0],[0]
"( r(pi, ei)− r∗(pi, ei) )2",2.3 Objective and Training,[0],[0]
(1) Optimization.,2.3 Objective and Training,[0],[0]
"Adam (Kingma and Ba, 2014) is employed to minimize the loss over the training data.",2.3 Objective and Training,[0],[0]
"The initial learning rate η is set to 0.01 and the gradient is clipped between [−10, 10] during training.",2.3 Objective and Training,[0],[0]
"In addition, dropout (Srivastava et al., 2014) is introduced for regularization with a dropout rate of 0.5, and 64 samples are used in each batch with batch normalization (Ioffe and Szegedy, 2015).",2.3 Objective and Training,[0],[0]
30% of the training data are reserved for validation.,2.3 Objective and Training,[0],[0]
"In addition, early stopping (Yao et al., 2007) is employed according to the validation loss, namely, the training is terminated if no decrease of the loss is observed for ten consecutive epochs.",2.3 Objective and Training,[0],[0]
"Once training is finished,
akin to (Dong et al., 2017), the model with the best quadratic weighted kappa on the validation set is selected.",2.3 Objective and Training,[0],[0]
Dataset.,3 Experimental Setup,[0],[0]
"The Automated Student Assessment Prize (ASAP) dataset has been widely used for AES (Alikaniotis et al., 2016; Chen and He, 2013; Dong et al., 2017), and is also employed as the prime evaluation instrument herein.",3 Experimental Setup,[0],[0]
"In total, ASAP consists of eight sets of essays, each of which associates to one prompt, and is originally written by students between Grade 7 and Grade 10.",3 Experimental Setup,[0],[0]
"As summarized in Table 1, essays from different sets differ in their rating criteria, length, as well as the rating distribution1.",3 Experimental Setup,[0],[0]
Cross,3 Experimental Setup,[0],[0]
-,3 Experimental Setup,[0],[0]
validation.,3 Experimental Setup,[0],[0]
"To fully employ the rated data, a prompt-wise eight-fold cross validation on the ASAP is used for evaluation.",3 Experimental Setup,[0],[0]
"In each fold, essays corresponding to a prompt is reserved for testing, and the remaining essays are used as training data.",3 Experimental Setup,[0],[0]
Evaluation metric.,3 Experimental Setup,[0],[0]
"The model outputs are first uniformly re-scaled into [0, 10], mirroring the range of ratings in practice.",3 Experimental Setup,[0],[0]
"Thereafter, akin to (Yannakoudakis et al., 2011; Chen and He, 2013; Alikaniotis et al., 2016), we report our results primarily based on the quadratic weighted Kappa (QWK), examining the agreement between the predicted ratings and the ground truth.",3 Experimental Setup,[0],[0]
Pearson correlation coefficient (PCC) and Spearman rankorder correlation coefficient (SCC) are also reported.,3 Experimental Setup,[0],[0]
"The correlations obtained from individual folds, as well as the average over all eight folds, are reported as the ultimate results.",3 Experimental Setup,[0],[0]
Competing models.,3 Experimental Setup,[0],[0]
"Since the promptindependent AES is of interests in this work, the existing AES models are adapted for prompt-independent rating prediction, serving as baselines.",3 Experimental Setup,[0],[0]
"This is due to the facts that the
1Details of this dataset can be found at https://www. kaggle.com/c/asap-aes.
prompt-dependent and -independent models differ a lot in terms of problem settings and model designs, especially in their requirements for the training data, where the latter ones release the prompt-dependent requirements and thereby are accessible to more data.",3 Experimental Setup,[0],[0]
"- RankSVM, using handcrafted features for AES (Yannakoudakis et al., 2011; Chen et al., 2014), is trained on a set of pre-defined promptindependent features as listed in Table 2, where the features are standardized beforehand to remove the mean and variance.",3 Experimental Setup,[0],[0]
The RankSVM is also used for the prompt-independent stage in our proposed TDNN model.,3 Experimental Setup,[0],[0]
"In particular, the linear kernel RankSVM2 is employed, where C is set to 5 according to our pilot experiments.",3 Experimental Setup,[0],[0]
- 2L-LSTM.,3 Experimental Setup,[0],[0]
"Two-layer bi-LSTM with GloVe for AES (Alikaniotis et al., 2016) is employed as another baseline.",3 Experimental Setup,[0],[0]
Regularized word embeddings are dropped to avoid over-fitting the prompt-specific features.,3 Experimental Setup,[0],[0]
- CNN-LSTM.,3 Experimental Setup,[0],[0]
"This model (Taghipour and Ng, 2016) employs a convolutional (CNN) layer over one-hot representations of words, followed by an LSTM layer to encode word sequences in a given essay.",3 Experimental Setup,[0],[0]
A linear layer with sigmoid activation function is then employed to predict the essay rating.,3 Experimental Setup,[0],[0]
- CNN-LSTM-ATT.,3 Experimental Setup,[0],[0]
"This model (Dong et al., 2017) employs a CNN layer to encode word sequences into sentences, followed by an LSTM layer to generate the essay representation.",3 Experimental Setup,[0],[0]
"An attention mechanism is added to model the influence of individual sentences on the final essay representation.
",3 Experimental Setup,[0],[0]
"2http://svmlight.joachims.org/
For the proposed TDNN model, as introduced in Section 2.2, different variants of TDNN are examined by using one or multiple components out of the semantic, POS and the syntactic networks.",3 Experimental Setup,[0],[0]
The combinations being considered are listed in the following.,3 Experimental Setup,[0],[0]
"In particular, the dimensions of POS tags and syntactic network are fixed to 50, whereas the sizes of the hidden units in LSTM, as well as the output units of the linear layers are tuned by grid search.",3 Experimental Setup,[0],[0]
"- TDNN(Sem) only includes the semantic building block, which is similar to the two-layer LSTM neural network from (Alikaniotis et al., 2016) but without regularizing the word embeddings; - TDNN(Sem+POS) employs the semantic and the POS building blocks; - TDNN(Sem+Synt) uses the semantic and the syntactic network building blocks; - TDNN(POS+Synt) includes the POS and the syntactic network building blocks; - TDNN(ALL) employs all three building blocks.
",3 Experimental Setup,[0],[0]
The use of POS or syntactic network alone is not presented for brevity given the facts that they perform no better than TDNN(POS+Synt) in our pilot experiments.,3 Experimental Setup,[0],[0]
Source code of the TDNN model is publicly available to enable further comparison3.,3 Experimental Setup,[0],[0]
"In this section, the evaluation results for different competing methods are compared and analyzed in terms of their agreements with the manual ratings using three correlation metrics, namely, QWK, PCC and SCC, where the best results for each prompt is highlighted in bold in Table 3.
",4 Results and Analyzes,[0],[0]
"It can be seen that, for seven out of all eight prompts, the proposed TDNN variants outperform the baselines by a margin in terms of QWK, and the TDNN variant with semantic and syntactic features, namely, TDNN(Sem+Synt), consistently performs the best among different competing methods.",4 Results and Analyzes,[0],[0]
"More precisely, as indicated in the bottom right corner in Table 3, on average, TDNN(Sem+Synt) outperforms the baselines by at least 25.52% under QWK, by 10.28% under PCC, and by 15.66% under SCC, demonstrating that the proposed model not only correlates better with the manual ratings in terms of QWK, but also linearly (PCC) and monotonically (SCC) correlates better with the manual ratings.",4 Results and Analyzes,[0],[0]
"As for the
3https://github.com/ucasir/TDNN4AES
four baselines, note that, the relatively underperformed deep models suffer from larger variances of performance under different prompts, e.g., for prompts two and eight, 2L-LSTM’s QWK is lower than 0.3.",4 Results and Analyzes,[0],[0]
"This actually confirms our choice of RankSVM for the first stage in TDNN, since a more complicated model (like 2L-LSTM) may end up with learning prompt-dependent signals, making it unsuitable for the prompt-independent rating prediction.",4 Results and Analyzes,[0],[0]
"As a comparison, RankSVM performs more stable among different prompts.
",4 Results and Analyzes,[0],[0]
"As for the different TDNN variants, it turns out that the joint uses of syntactic network with semantic or POS features can lead to better performances.",4 Results and Analyzes,[0],[0]
"This indicates that, when learning the prompt-dependent signals, apart from the widelyused semantic features, POS features and the sentence structure taggings (syntactic network) are also essential in learning the structure and the arrangement of an essay in response to a particular prompt, thereby being able to improve the results.",4 Results and Analyzes,[0],[0]
"It is also worth mentioning, however, when using all three features, the TDNN actually performs worse than when only using (any) two features.",4 Results and Analyzes,[0],[0]
"One possible explanation is that the uses of all three features result in a more complicated model, which over-fits the training data.
",4 Results and Analyzes,[0],[0]
"In addition, recall that the prompt-independent RankSVM model from the first stage enables the proposed TDNN in learning prompt-dependent information without manual ratings for the target prompt.",4 Results and Analyzes,[0],[0]
"Therefore, one would like to understand how good the trained RankSVM is in feeding training data for the model in the second stage.",4 Results and Analyzes,[0],[0]
"In particular, the precision, recall and F-score (P/R/F) of the essays selected by RanknSVM, namely, the negative ones rated between [0, 4], and the positive ones rated between [8, 10], are displayed in Figure 4.",4 Results and Analyzes,[0],[0]
It can be seen that the P/R/F scores of both positive and negative classes differ a lot among different prompts.,4 Results and Analyzes,[0],[0]
"Moreover, it turns out that the P/R/F scores do not necessarily correlate with the performance of the TDNN model.",4 Results and Analyzes,[0],[0]
"Take TDNN(Sem+Synt), the best TDNN variant, as an example: as indicated in Table 4, the performance and the P/R/F scores of the pseudo examples are only weakly correlated in most cases.
",4 Results and Analyzes,[0],[0]
"To gain a better understanding in how the quality of pseudo examples affects the performance of TDNN, the sanctity of the selected essays are examined.",4 Results and Analyzes,[0],[0]
"In Figure 5, the relative precision of
the selected positive and negative training data by RankSVM are displayed for all eight prompts in terms of their concordance with the manual ratings, by computing the number of positive (negative) essays that are better (worse) than all negative (positive) essays.",4 Results and Analyzes,[0],[0]
"It can be seen that, such relative precision is at least 80% and mostly beyond 90% on different prompts, indicating that the overlap of the selected positive and negative essays are fairly small, guaranteeing that the deep model in the second stage at least learns from correct labels, which are crucial for the success of our TDNN model.
",4 Results and Analyzes,[0],[0]
"Beyond that, we further investigate the class balance of the selected training data from the first
stage, which could also influence the ultimate results.",4 Results and Analyzes,[0],[0]
"The number of selected positive and negative essays are reported in Table 5, where for prompts three and eight the training data suffers from serious imbalanced problem, which may explain their lower performance (namely, the two lowest QWKs among different prompts).",4 Results and Analyzes,[0],[0]
"On one hand, this is actually determined by real distribution of ratings for a particular prompt, e.g., how many essays are with an extreme quality for a given prompt in the target data.",4 Results and Analyzes,[0],[0]
"On the other hand, a fine-grained tuning of the RankSVM (e.g., tuning C+ and C− for positive and negative exam-
ples separately) may partially resolve the problem, which is left for the future work.",4 Results and Analyzes,[0],[0]
"Classical regression and classification algorithms are widely used for learning the rating model based on a variety of text features including lexical, syntactic, discourse and semantic features (Larkey, 1998; Rudner, 2002; Attali and Burstein, 2006; Mcnamara et al., 2015; Phandi et al., 2015).",5 Related Work,[0],[0]
There are also approaches that see AES as a preference ranking problem by applying learning to ranking algorithms to learn the rating model.,5 Related Work,[0],[0]
"Results show improvement of learning to rank approaches over classical regression and classification algorithms (Chen et al., 2014; Yannakoudakis et al., 2011).",5 Related Work,[0],[0]
"In addition, Chen & He propose to incorporate the evaluation metric into the loss function of listwise learning to rank for AES (Chen and He, 2013).
",5 Related Work,[0],[0]
"Recently, there have been efforts in developing AES approaches based on deep neural networks (DNN), for which feature engineering is not required.",5 Related Work,[0],[0]
"Taghipour & Ng explore a variety of neural network model architectures based on recurrent neural networks which can effectively encode the information required for essay scoring and learn the complex connections in the data through the non-linear neural layers (Taghipour and Ng, 2016).",5 Related Work,[0],[0]
"Alikaniotis et al. introduce a neural network model to learn the extent to which specific words contribute to the text’s score, which
is embedded in the word representations.",5 Related Work,[0],[0]
"Then a two-layer bi-directional Long-Short Term Memory networks (bi-LSTM) is used to learn the meaning of texts, and finally the essay score is predicted through a mutli-layer feed-forward network (Alikaniotis et al., 2016).",5 Related Work,[0],[0]
"Dong & Zhang employ a hierarchical convolutional neural network (CNN) model, with a lower layer representing sentence structure and an upper layer representing essay structure based on sentence representations, to learn features automatically (Dong and Zhang, 2016).",5 Related Work,[0],[0]
This model is later improved by employing attention layers.,5 Related Work,[0],[0]
"Specifically, the model learns text representation with LSTMs which can model the coherence and co-reference among sequences of words and sentences, and uses attention pooling to capture more relevant words and sentences that contribute to the final quality of essays (Dong et al., 2017).",5 Related Work,[0],[0]
"Song et al. propose a deep model for identifying discourse modes in an essay (Song et al., 2017).
",5 Related Work,[0],[0]
"While the literature has shown satisfactory performance of prompt-dependent AES, how to achieve effective essay scoring in a promptindependent setting remains to be explored.",5 Related Work,[0],[0]
"Chen & He studied the usefulness of promptindependent text features and achieved a humanmachine rating agreement slightly lower than the use of all text features (Chen and He, 2013) for prompt-dependent essay scoring prediction.",5 Related Work,[0],[0]
"A constrained multi-task pairwise preference learning approach was proposed in (Cummins et al., 2016) to combine essays from multiple prompts for training.",5 Related Work,[0],[0]
"However, as shown by (Dong and Zhang, 2016; Zesch et al., 2015; Phandi et al., 2015), straightforward applications of existing AES methods for prompt-independent AES lead to a poor performance.",5 Related Work,[0],[0]
"This study aims at addressing the promptindependent automated essay scoring (AES), where no rated essay for the target prompt is available.",6 Conclusions & Future Work,[0],[0]
"As demonstrated in the experiments, two kinds of established prompt-dependent AES models, namely, RankSVM for AES (Yannakoudakis et al., 2011; Chen et al., 2014) and the deep models for AES (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017), fail to provide satisfactory performances, justifying our arguments in Section 1 that the application of estab-
lished prompt-dependent AES models on promptindependent AES is not straightforward.",6 Conclusions & Future Work,[0],[0]
"Therefore, a two-stage TDNN learning framework was proposed to utilize the prompt-independent features to generate pseudo training data for the target prompt, on which a hybrid deep neural network model is proposed to learn a rating model consuming semantic, part-of-speech, and syntactic signals.",6 Conclusions & Future Work,[0],[0]
"Through the experiments on the ASAP dataset, the proposed TDNN model outperforms the baselines, and leads to promising improvement in the human-machine agreement.
",6 Conclusions & Future Work,[0],[0]
"Given that our approach in this paper is similar to the methods for transductive transfer learning (Pan and Yang, 2010), we argue that the proposed TDNN could be further improved by migrating the non-target training data to the target prompt (Busto and Gall, 2017).",6 Conclusions & Future Work,[0],[0]
Further study of the uses of transfer learning algorithms on promptindependent AES needs to be undertaken.,6 Conclusions & Future Work,[0],[0]
"This work is supported in part by the National Natural Science Foundation of China (61472391), and the Project of Beijing Advanced Innovation Center for Language Resources (451122512).",Acknowledgments,[0],[0]
Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data.,abstractText,[0],[0]
"Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available.",abstractText,[0],[0]
"To close this gap, a two-stage deep neural network (TDNN) is proposed.",abstractText,[0],[0]
"In particular, in the first stage, using the rated essays for nontarget prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step.",abstractText,[0],[0]
Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.,abstractText,[0],[0]
TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1237–1246 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1237",text,[0],[0]
"The current leading perspective on temporal information extraction regards three phases: (1) a temporal entity recognition phase, extracting events (blue boxes in Fig. 1) and their attributes, and extracting temporal expressions (green boxes), and normalizing their values to dates or durations, (2) a relation extraction phase, where temporal links (TLinks) among those entities, and between events and the document-creation time (DCT) are found (arrows in Fig. 1, left).",1 Introduction,[0],[0]
"And (3), construction of a time-line (Fig. 1, right) from the extracted temporal links, if they are temporally consistent.",1 Introduction,[0],[0]
"Much research concentrated on the first two steps, but very little research looks into step 3, time-line construction, which is the focus of this work.
",1 Introduction,[0],[0]
"In this paper, we propose a new time-line construction paradigm that evades phase 2, the relation extraction phase, because in the classical
paradigm temporal relation extraction comes with many difficulties in training and prediction that arise from the fact that for a text with n temporal entities (events or temporal expressions) there are n2 possible entity pairs, which makes it likely for annotators to miss relations, and makes inference slow as n2 pairs need to be considered.",1 Introduction,[0],[0]
"Temporal relation extraction models consistently give lower performance than those in the entity recognition phase (UzZaman et al., 2013; Bethard et al., 2016, 2017), introducing errors in the time-line construction pipe-line.
",1 Introduction,[0],[0]
"The ultimate goal of our proposed paradigm is to predict from a text in which entities are already detected, for each entity: (1) a probability distribution on the entity’s starting point, and (2) another distribution on the entity’s duration.",1 Introduction,[0],[0]
The probabilistic aspect is crucial for time-line based decision making.,1 Introduction,[0],[0]
"Constructed time-lines allow for further quantitative reasoning with the temporal information, if this would be needed for certain applications.
",1 Introduction,[0],[0]
"As a first approach towards this goal, in this paper, we propose several initial time-line models in this paradigm, that directly predict - in a linear fashion - start points and durations for each entity, using text with annotated temporal entities as input (shown in Fig. 1).",1 Introduction,[0],[0]
"The predicted start points and durations constitute a relative time-line, i.e. a total order on entity start and end points.",1 Introduction,[0],[0]
"The time-line is relative, as start and duration values cannot (yet) be mapped to absolute calender dates or durations expressed in seconds.",1 Introduction,[0],[0]
It represents the relative temporal order and inclusions that temporal entities have with respect to each other by the quantitative start and end values of the entities.,1 Introduction,[0],[0]
"Relative time-lines are a first step toward our goal, building models that predict statistical absolute time-lines.",1 Introduction,[0],[0]
"To train our relative time-line models, we define novel loss functions that exploit TimeML-style an-
notations, used in most existing temporal corpora.",1 Introduction,[0],[0]
"This work leads to the following contributions:
• A new method to construct a relative time-line from a set of temporal relations (TL2RTL).
",1 Introduction,[0],[0]
"• Two new models that, for the first time, directly predict (relative) time-lines - in linear complexity - from entity-annotated texts without doing a form of temporal relation extraction (S-TLM & C-TLM).
",1 Introduction,[0],[0]
"• Three new loss functions based on the mapping between Allen’s interval algebra and the end-point algebra to train time-line models from TimeML-style annotations.
",1 Introduction,[0],[0]
In the next sections we will further discuss the related work on temporal information extraction.,1 Introduction,[0],[0]
"We will describe the models and training losses in detail, and report on conducted experiments.",1 Introduction,[0],[0]
The way temporal information is conveyed in language has been studied for a long time.,2.1 Temporal Information Extraction,[0],[0]
"It can be conveyed directly through verb tense, explicit temporal discourse markers (e.g. during or afterwards) (Derczynski, 2017) or temporal expressions such as dates, times or duration expressions (e.g. 10-05-2010 or yesterday).",2.1 Temporal Information Extraction,[0],[0]
"Temporal information is also captured in text implicitly, through background knowledge about, for example, duration of events mentioned in the text (e.g. even without context, walks are usually shorter than journeys).
",2.1 Temporal Information Extraction,[0],[0]
"Most temporal corpora are annotated with TimeML-style annotations, of which an example is shown in Fig 1, indicating temporal entities, their attributes, and the TLinks among them.
",2.1 Temporal Information Extraction,[0],[0]
The automatic extraction of TimeML-style temporal information from text using machine learning was first explored by Mani et al. (2006).,2.1 Temporal Information Extraction,[0],[0]
They proposed a multinomial logistic regression classifier to predict the TLinks between entities.,2.1 Temporal Information Extraction,[0],[0]
"They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data.
",2.1 Temporal Information Extraction,[0],[0]
"Since then, much research focused on further improving the pairwise classification models, by
exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017).",2.1 Temporal Information Extraction,[0],[0]
"Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components.
",2.1 Temporal Information Extraction,[0],[0]
"Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs.",2.1 Temporal Information Extraction,[0],[0]
"To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017).",2.1 Temporal Information Extraction,[0],[0]
"In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition.",2.1 Temporal Information Extraction,[0],[0]
"Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990).",2.2 Temporal Reasoning,[0],[0]
"Or (2), by first mapping the temporal interval expressions to expressions about interval end-points (start and endings of entities) (Vilain et al., 1990).",2.2 Temporal Reasoning,[0],[0]
"An example of such mapping is that If event X occurs before Y then the end of X should be before the start of Y. Then reasoning can be done with end-points in a point algebra, which has only three point-wise relations (=, <,>), making reasoning much more efficient compared to reasoning with Allen’s thirteen interval relations.
",2.2 Temporal Reasoning,[0],[0]
"Mapping interval relations to point-wise expressions has been exploited for model inference by Denis and Muller (2011), and for evaluation by UzZaman and Allen (2011).",2.2 Temporal Reasoning,[0],[0]
"In this work, we ex-
ploit it for the first time for model training, in our loss functions.",2.2 Temporal Reasoning,[0],[0]
"We propose two model structures for direct time-line construction: (1) a simple contextindependent model (S-TLM), and (2) a contextual model (C-TLM).",3 Models,[0],[0]
Their structures are shown in Fig. 2.,3 Models,[0],[0]
"Additionally, we propose a method to construct relative time-lines from a set of (extracted) TLinks (TL2RTL).",3 Models,[0],[0]
"In this section we first explain the first two direct models S-TLM and C-TLM, and afterwards the indirect method TL2RTL.",3 Models,[0],[0]
"In both S-TLM and C-TLM, words are represented as a concatenation of a word embedding, a POS embedding, and a Boolean feature vector containing entity attributes such as the type, class, aspect, following (Do et al., 2012).",Word representation,[0],[0]
Further details on these are given in the experiments section.,Word representation,[0],[0]
"For the simple context-independent time-line model, each entity is encoded by the word representation of the last word of the entity (generally the most important).",Simple Time-line Model (S-TLM),[0],[0]
"From this representation we have a linear projection to the duration d, and the start s. S-TLM is shown by the dotted edges in Fig 2.",Simple Time-line Model (S-TLM),[0],[0]
"An advantage of S-TLM is that it has very few parameters, and each entity can be placed on the time-line independently of the others, allowing parallelism during prediction.",Simple Time-line Model (S-TLM),[0],[0]
The downside is that S-TLM is limited in its use of contextual information.,Simple Time-line Model (S-TLM),[0],[0]
"To better exploit the entity context we also propose a contextual time-line model C-TLM (solid edges in Fig 2), that first encodes the full text using two bi-directional recurrent neural networks, one for entity starts (BiRNNs), and one for entity durations (BiRNNd).1 On top of the encoded text we learn two linear mappings, one from the BiRNNd output of the last word of the entity mention to its duration d, and similarly for the start time, from the BiRNNs output to the entity’s start s.",Contextual Time-line Model (C-TLM),[0],[0]
"Both proposed models use linear mappings2 to predict the start value si and duration di for the encoded entity i. By summing start si and duration di we can calculate the entity’s end-point ei.
ei = si +max(di, dmin) (1)
","Predicting Start, Duration, and End",[0],[0]
"Predicting durations rather than end-points makes it easy to control that the end-point lies after the start-point by constraining the duration di by a constant minimum duration value dmin above 0, as shown in Eq. 1.","Predicting Start, Duration, and End",[0],[0]
"Although the DCT is often not found explicitly in the text, it is an entity in TimeML, and has TLinks to other entities.",Modeling Document-Creation Time,[0],[0]
"We model it by assigning it a text-independent start sDCT and duration dDCT.
",Modeling Document-Creation Time,[0],[0]
Start sDCT is set as a constant (with value 0).,Modeling Document-Creation Time,[0],[0]
"This way the model always has the same reference point, and can learn to position the entities w.r.t.",Modeling Document-Creation Time,[0],[0]
"the DCT on the time-line.
",Modeling Document-Creation Time,[0],[0]
1We also experimented with sharing weights among BiRNNd and BiRNNs.,Modeling Document-Creation Time,[0],[0]
"In our experiments, this gave worse performance, so we propose to keep them separate.
",Modeling Document-Creation Time,[0],[0]
"2Adding more layers did not improve results.
",Modeling Document-Creation Time,[0],[0]
"In contrast, DCT duration dDCT is modeled as a single variable that is learned (initialized with 1).",Modeling Document-Creation Time,[0],[0]
"Since multiple entities may be included in the DCT, and entities have a minimum duration dmin, a constant dDCT could possibly prevent the model from fitting all entities in the DCT.",Modeling Document-Creation Time,[0],[0]
Modeling dDCT as a variable allows growth of dDCT and averts this issue.3,Modeling Document-Creation Time,[0],[0]
"We propose three loss functions to train time-line models from TimeML-style annotations: a regular time-line loss Lτ , and two slightly expanded discriminative time-line losses, Lτce and Lτh.",Training Losses,[0],[0]
Ground-truth TLinks can be seen as constraints on correct positions of entities on a time-line.,Regular Time-line Loss (Lτ ),[0],[0]
The regular time-line loss Lτ expresses the degree to which these constraints are met for a predicted time-line.,Regular Time-line Loss (Lτ ),[0],[0]
"If all TLinks are satisfied in the timeline for a certain text, Lτ will be 0 for that text.
",Regular Time-line Loss (Lτ ),[0],[0]
"As TLinks relate entities (intervals), we first convert the TLinks to expressions that relate the start and end points of entities.",Regular Time-line Loss (Lτ ),[0],[0]
"How each TLink is translated to its corresponding point-algebraic constraints is given in Table 1, following Allen (1990).
",Regular Time-line Loss (Lτ ),[0],[0]
"As can be seen in the last column there are only two point-wise operations in the point-algebraic constraints: an order operation (<), and an equality operation (=).",Regular Time-line Loss (Lτ ),[0],[0]
"To model to what degree each point-wise constraint is met, we employ hinge losses, with a margin mτ , as shown in Eq. 2.
",Regular Time-line Loss (Lτ ),[0],[0]
"3Other combinations of modeling sDCT and dDCT as variable or constant decreased performance.
",Regular Time-line Loss (Lτ ),[0],[0]
"4No TLink for Allen’s overlap relation is present in TimeML, also concluded by UzZaman and Allen (2011).
",Regular Time-line Loss (Lτ ),[0],[0]
To explain the intuition and notation: If we have a point-wise expression ξ of the form,Regular Time-line Loss (Lτ ),[0],[0]
x < y,Regular Time-line Loss (Lτ ),[0],[0]
"(first case of Eq. 2), then the predicted point x̂ should be at least a distance mτ smaller (or earlier on the time-line) than predicted point ŷ",Regular Time-line Loss (Lτ ),[0],[0]
in order for the loss to be 0.,Regular Time-line Loss (Lτ ),[0],[0]
"Otherwise, the loss represents the distance x̂ or ŷ still has to move to make x̂ smaller than ŷ (and satisfy the constraint).",Regular Time-line Loss (Lτ ),[0],[0]
"For the second case, if ξ is of the form x = y, then point x̂ and ŷ should lie very close to each other, i.e. at most a distance mτ away from each other.",Regular Time-line Loss (Lτ ),[0],[0]
Any distance further than the margin mτ is counted as loss.,Regular Time-line Loss (Lτ ),[0],[0]
"Notice that if we set margin mτ to 0, the second case becomes an L1 loss |x̂",Regular Time-line Loss (Lτ ),[0],[0]
− ŷ|.,Regular Time-line Loss (Lτ ),[0],[0]
"However, we use a small margin mτ to promote some distance between ordered points and prevent con-
fusion with equality.",Regular Time-line Loss (Lτ ),[0],[0]
"Fig. 3 visualizes the loss for three TLinks.
Lp(ξ|t, θ) = { max(x̂+mτ − ŷ, 0) iff x < y max(|x̂−",Regular Time-line Loss (Lτ ),[0],[0]
"ŷ| −mτ , 0) iff x = y
(2)
",Regular Time-line Loss (Lτ ),[0],[0]
"The total time-line loss Lτ (t|θ) of a model with parameters θ on text t with ground-truth TLinks R(t), is the sum of the TLink-level losses of all TLinks r ∈ R(t).",Regular Time-line Loss (Lτ ),[0],[0]
"Each TLink-level loss Lr(r|t, θ) for TLink r is the sum of the pointwise losses Lp(ξ|t, θ) of the corresponding pointalgebraic constraints ξ ∈ IPA(r) from Table 1.5
Lr(r|t, θ) = ∑
ξ∈IPA(r)
",Regular Time-line Loss (Lτ ),[0],[0]
"Lp(ξ|t, θ) (3)
",Regular Time-line Loss (Lτ ),[0],[0]
"Lτ (t, θ) = ∑ r∈R(t) Lr(r|t, θ) (4)",Regular Time-line Loss (Lτ ),[0],[0]
"To promote a more explicit difference between the relations on the time-line we introduce two discriminative loss functions, Lτce and Lτh, which build on top of Lr.",Discriminative Time-line Losses,[0],[0]
"Both discriminative loss functions use an intermediate score S(r|t, θ) for each TLink r based on the predicted time-line.",Discriminative Time-line Losses,[0],[0]
"As scoring function, we use the negativeLr loss, as shown in Eq. 5.
S(r|t, θ) = −Lr(r|t, θ) (5) 5The TLink during and its inverse are mapped to simulta-
neous, following the evaluation of TempEval-3.
",Discriminative Time-line Losses,[0],[0]
"Then, a lower time-line loss Lr(r|t, θ) results in a higher score for relation type r. Notice that the maximum score is 0, as this is the minimum Lr.",Discriminative Time-line Losses,[0],[0]
Our first discriminative loss is a cross-entropy based loss.,Probabilistic Loss (Lτce),[0],[0]
For this the predicted scores are normalized using a soft-max over the possible relation types (TL).,Probabilistic Loss (Lτce),[0],[0]
"The resulting probabilities are used to calculate a cross-entropy loss, shown in Eq. 6.",Probabilistic Loss (Lτce),[0],[0]
"This way, the loss does not just promote the correct relation type but also distantiates from the other relation types.
",Probabilistic Loss (Lτce),[0],[0]
"Lτce(t|θ) = ∑ r∈R(t) r · log ( eS(r|t,θ)∑ r′∈TL e S(r′|t,θ) )",Probabilistic Loss (Lτce),[0],[0]
(6),Probabilistic Loss (Lτce),[0],[0]
"When interested in discriminating relations on the time-line, we want the correct relation type to have the highest score from all possible relation types TL.",Ranking Loss (Lτh),[0],[0]
"To represent this perspective, we also define a ranking loss with a score margin mh in Eq. 7.
",Ranking Loss (Lτh),[0],[0]
Lτh(t|θ),Ranking Loss (Lτh),[0],[0]
"=∑ r∈R(t) ∑ r′∈TL\{r} max(S(r′|t, θ)−S(r|t, θ)+mh, 0)
(7)",Ranking Loss (Lτh),[0],[0]
"S-TLM and C-TLM are trained by by iterating through the training texts, sampling mini-batches of 32 annotated TLinks.",Training Procedure,[0],[0]
"For each batch we (1) perform a forward pass, (2) calculate the total loss (for one of the loss functions), (3) derive gradients using Adam6 (Kingma and Ba, 2014), and (4) update the model parameters θ via back-propagation.",Training Procedure,[0],[0]
After each epoch we shuffle the training texts.,Training Procedure,[0],[0]
"As stopping criteria we use early stopping (Morgan and Bourlard, 1990), with a patience of 100 epochs and a maximum number of 1000 epochs.",Training Procedure,[0],[0]
"To model the indirect route, we construct a novel method, TL2RTL, that predicts relative time lines from a subset of TLinks, shown in Fig 1.",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"One can choose any method to obtain a set of TLinks R(t) from a text t, serving as input to TL2RTL.
",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"6Using the default parameters from the paper.
",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"TL2RTL constructs a relative time-line, by assigning start and end values to each temporal entity, such that the resulting time-line satisfies the extracted TLinksR(t) by minimizing a loss function that is 0 when the extracted TLinks are satisfied.",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
TL2RTL on itself is a method and not a model.,3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"The only variables over which it optimizes the loss are the to be assigned starts and duration values.
",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"In detail, for a text t, with annotated entities E(t), we first extract a set of TLinks R(t).",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"In this work, to extract TLinks, we use the current state-of-the-art structured TLink extraction model by Ning et al. (2017).",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"Secondly, we assign a start variable si, and duration variable di to each entity i ∈ E(t).",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"Similar to S-TLM and C-TLM, for each i ∈ E(t), di is bounded by a minimum duration dmin to ensure start si always lies before end ei.",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"Also, we model the DCT start sDCT as a constant, and its duration dDCT as a variable.",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"Then we minimize one of the loss functions Lτ , Lτce, or Lτh on the extracted TLinks R(t), obtaining three TL2RTL variants, one for each loss.",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"If the initially extracted set of TLinks R(t) is consistent, and the loss is minimized sufficiently, all si and di form a relative time-line that satisfies the TLinks R(t), but from which we can now also derive consistent TLinks for any entity pair, also the pairs that were not in R(t).",3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
To minimize the loss we use Adam for 10k epochs until the loss is zero for each document.7,3.2 From TLinks to Time-lines (TL2RTL),[0],[0]
"Because prediction of relative time-lines trained on TimeML-style annotations is new, we cannot compare our model directly to relation extraction or classification models, as the latter do not provide completely temporally consistent TLinks for all possible entity pairs, like the relative timelines do.",4.1 Evaluation and Data,[0],[0]
"Neither can we compare directly to existing absolute time-line prediction models such as Reimers et al. (2018) because they are trained on different data with a very different annotation scheme.
",4.1 Evaluation and Data,[0],[0]
"To evaluate the quality of the relative time-line models in a fair way, we use TimeML-style test sets as follows: (1) We predict a time-line for each test-text, and (2) we check for all ground-truth an-
7For some documents the extracted TLinks were temporally inconsistent, resulting in a non-zero loss.",4.1 Evaluation and Data,[0],[0]
"Nevertheless, > 96% of the extracted TLinks were satisfied.
",4.1 Evaluation and Data,[0],[0]
"notated TLinks that are present in the data, what would be the derived relation type based on the predicted time-line, which is the relation type that gives the lowest time-line loss Lr.",4.1 Evaluation and Data,[0],[0]
"This results in a TLink assignment for each annotated pair in the TimeML-style reference data, and therefor we can use similar metrics.",4.1 Evaluation and Data,[0],[0]
"As evaluation metric we employ the temporal awareness metric, used in TempEval-3, which takes into account temporal closure (UzZaman et al., 2013).",4.1 Evaluation and Data,[0],[0]
"Notice that although we use the same metric, comparison against relation classification systems would be unfair, as our model assigns consistent labels to all pairs, whereas relation classification systems do not.
",4.1 Evaluation and Data,[0],[0]
"For training and evaluation we use two data splits, TE‡ and TD‡, exactly following Ning et al. (2017).",4.1 Evaluation and Data,[0],[0]
"Some statistics about the data are shown in Table 2.8 The splits are constituted from various smaller datasets: the TimeBank (TB) (Pustejovsky et al., 2002), the AQUANT dataset (AQ), and the platinum dataset (PT) all from TempEval-3 (UzZaman et al., 2013).",4.1 Evaluation and Data,[0],[0]
"And, the TimeBank Dense (Cassidy et al., 2014) , and the Verb-Clause dataset (VC) (Bethard et al., 2007).",4.1 Evaluation and Data,[0],[0]
Hyper-parameters shared in all settings can be found in Table 3.,4.2 Hyper-parameters and Preprocessing,[0],[0]
"The following hyper-parameters are tuned using grid search on a development set (union of TB and AQ): dmin is chosen from {1, 0.1, 0.01}, mτ from {0, 0.025, 0.05, 0.1}, αd from {0, 0.1, 0.2, 0.4, 0.8}, and αrnn from {10, 25, 50}.",4.2 Hyper-parameters and Preprocessing,[0],[0]
"We use LSTM (Hochreiter and Schmidhuber, 1997) as RNN units9 and employ 50-dimensional GloVe word-embeddings pre-trained10 on 6B words (Wikipedia and NewsCrawl) to initialize the models’ word embeddings.
",4.2 Hyper-parameters and Preprocessing,[0],[0]
"We use very simple tokenization and consider punctuation11 or newline tokens as individual tokens, and split on spaces.",4.2 Hyper-parameters and Preprocessing,[0],[0]
"Additionally, we lowercase the text and use the Stanford POS Tagger (Toutanova et al., 2003) to obtain POS.
8We explicitly excluded all test documents from training as some corpora annotated the same documents.
",4.2 Hyper-parameters and Preprocessing,[0],[0]
"9We also experimented with GRU as RNN type, obtaining similar results.
",4.2 Hyper-parameters and Preprocessing,[0],[0]
"10https://nlp.stanford.edu/projects/glove 11, ./\""’=+-;:()!?<>%&$*|",4.2 Hyper-parameters and Preprocessing,[0],[0]
[]{},4.2 Hyper-parameters and Preprocessing,[0],[0]
"We compared our three proposed models for the three loss functions Lτ , Lτce, and Lτh, and their linear (unweighted) combination L∗, on TE3‡ and TD‡, for which the results are shown in Table 4.
",5 Results,[0],[0]
"A trend that can be observed is that overall performance on TD‡ is higher than that of TE3‡, even though less documents are used for training.",5 Results,[0],[0]
"We inspected why this is the case, and this is caused by a difference in class balance between both test sets.",5 Results,[0],[0]
"In TE3‡ there are many more TLinks of type simultaneous (12% versus 3%), which are very
difficult to predict, resulting in lower scores for TE3‡ compared to TD‡.",5 Results,[0],[0]
"The difference in performance between the datasets is probably also be related to the dense annotation scheme of TD‡ compared to the sparser annotations of TE3‡, as dense annotations give a more complete temporal view of the training texts.",5 Results,[0],[0]
"For TL2RTL better TLink extraction12 is also propagated into the final timeline quality.
",5 Results,[0],[0]
"If we compare loss functions Lτ , Lτce, and Lτh, and combination L∗, it can be noticed that, although all loss functions seem to give fairly similar performance, Lτ gives the most robust results (never lowest), especially noticeable for the smaller dataset TD‡.",5 Results,[0],[0]
"This is convenient, because Lτ is fastest to compute during training, as it requires no score calculation for each TLink type.",5 Results,[0],[0]
Lτ is also directly interpretable on the timeline.,5 Results,[0],[0]
"The combination of losses L∗ shows mixed results, and has lower performance for S-TLM and C-TLM, but better performance for TL2RTL.",5 Results,[0],[0]
"However, it is slowest to compute, and less interpretable, as it is a combined loss.
",5 Results,[0],[0]
"Moreover, we can clearly see that on TE3‡, CTLM performs better than the indirect models, across all loss functions.",5 Results,[0],[0]
"This is a very interesting result, as C-TLM is an order of complexity faster in prediction speed compared to the indirect models (O(n) compared to O(n2) for a text with n",5 Results,[0],[0]
entities).13,5 Results,[0],[0]
"We further explore why this is the case through our error analysis in the next section.
",5 Results,[0],[0]
"On TD‡, the indirect models seem to perform slightly better.",5 Results,[0],[0]
"We suspect that the reason for this is that C-TLM has more parameters (mostly the LSTM weights), and thus requires more data (TD‡ has much fewer documents than TE3‡) compared to the indirect methods.",5 Results,[0],[0]
"Another result supporting this hypothesis is the fact that the difference between C-TLM and S-TLM is small on the smaller
12F1 of 40.3 for TE3‡ and 48.5 for TD‡ (Ning et al., 2017)",5 Results,[0],[0]
"13We do not directly compare prediction speed, as it would result in unfair evaluation because of implementation differences.",5 Results,[0],[0]
"However, currently, C-TLM predicts at∼100 w/s incl.",5 Results,[0],[0]
"POS tagging, and ∼2000 w/s without.",5 Results,[0],[0]
"When not using POS, overall performance decreases consistently with 2-4 points.
TD‡, indicating that C-TLM does not yet utilize contextual information from this dataset, whereas, in contrast, on TE3‡, the larger dataset, C-TLM clearly outperforms S-TLM across all loss functions, showing that when enough data is available C-TLM learns good LSTM weights that exploit context substantially.",5 Results,[0],[0]
"We compared predictions of TL2RTL(Lτ ) with those of C-TLM (Lτ ), the best models of each paradigm.",6 Error Analysis,[0],[0]
"In Table 4, we show the confusion matrices of both systems on TE3‡.
When looking at the overall pattern in errors, both models seem to make similar confusions on both datasets (TD‡ was excluded for space constraints).",6 Error Analysis,[0],[0]
"Overall, we find that simultaneous is the most violated TLink for both models.",6 Error Analysis,[0],[0]
This can be explained by two reasons: (1) It is the least frequent TLink in both datasets.,6 Error Analysis,[0],[0]
"And (2), simultaneous entities are often co-referring events.",6 Error Analysis,[0],[0]
"Event co-reference resolution is a very difficult task on its own.
",6 Error Analysis,[0],[0]
We also looked at the average token-distance between arguments of correctly satisfied TLinks by the time-lines of each model.,6 Error Analysis,[0],[0]
For TL2RTL (Lτ ),6 Error Analysis,[0],[0]
"this is 13 tokens, and for C-TLM (Lτ ) 15.",6 Error Analysis,[0],[0]
"When looking only at the TLinks that C-TLM (Lτ ) satisfied and TL2RTL (Lτ ) did not, the average distance is 21.",6 Error Analysis,[0],[0]
These two observations suggest that the direct C-TLM (Lτ ) model is better at positioning entities on the time-line that lie further away from each other in the text.,6 Error Analysis,[0],[0]
"An explanation for this can be error propagation of TLink extraction to the time-line construction, as the pairwise TLink extraction of the indirect paradigm extracts TLinks in a contextual window, to prune the O(n2) number of possible TLink candidates.",6 Error Analysis,[0],[0]
"This
consequently prevents TL2RTL to properly position distant events with respect to each other.
",6 Error Analysis,[0],[0]
To get more insight in what the model learns we calculated mean durations and mean starts of CTLM (Lτ ) predictions.,6 Error Analysis,[0],[0]
"Table 5 contains examples from the top-shortest, and top-longest duration assignments and earliest and latest starting points.",6 Error Analysis,[0],[0]
We observe that events that generally have more events included are assigned longer duration and vice versa.,6 Error Analysis,[0],[0]
"And, events with low start values are in the past tense and events with high start values are generally in the present (or future) tense.",6 Error Analysis,[0],[0]
"A characteristic of our model is that it assumes that all events can be placed on a single timeline, and that it does not assume that unlabeled pairs are temporally unrelated.",7 Discussion,[0],[0]
"This has big advantages: it results in fast prediction, and missed annotation do not act as noise to the training, as they do for pairwise models.",7 Discussion,[0],[0]
"Ning et al. (2018) argue that actual, negated, hypothesized, expected or opinionated events should possibly be annotated
on separate time-axis.",7 Discussion,[0],[0]
We believe such multi-axis representations can be inferred from the generated single time-lines if hedging information is recognized.,7 Discussion,[0],[0]
"This work leads to the following three main contributions14: (1) Three new loss functions that connect the interval-based TimeML-annotations to points on a time-line, (2) A new method, TL2RTL, to predict relative time-lines from a set of predicted temporal relations.",8 Conclusions,[0],[0]
"And (3), most importantly, two new models, S-TLM and C-TLM, that – to our knowledge for the first time – predict (relative) time-lines in linear complexity from text, by evading the computationally expensive (often O(n2))",8 Conclusions,[0],[0]
intermediate relation extraction phase in earlier work.,8 Conclusions,[0],[0]
"From our experiments, we conclude that the proposed loss functions can be used effectively to train direct and indirect relative time-line models, and that, when provided enough data, the – much faster – direct model C-TLM outperforms the indirect method TL2RTL.
",8 Conclusions,[0],[0]
"As a direction for future work, it would be very interesting to extend the current models, diving further into direct time-line models, and learn to predict absolute time-lines, i.e. making the time-lines directly mappable to calender dates and times, e.g. by exploiting complementary data sources such as the EventTimes Corpus (Reimers et al., 2016) and extending the current loss functions accordingly.",8 Conclusions,[0],[0]
"The proposed models also provide a good starting point for research into probabilistic time-line models, that additionally model the (un)certainty of the predicted positions and durations of the entities.",8 Conclusions,[0],[0]
The authors thank Geert Heyman and the reviewers for their constructive comments which helped us to improve the paper.,Acknowledgments,[0],[0]
"This work was funded by the KU Leuven C22/15/16 project ”MAchine Reading of patient recordS (MARS)”, and by the IWT-SBO 150056 project ”ACquiring CrUcial Medical information Using LAnguage TEchnology” (ACCUMULATE).
",Acknowledgments,[0],[0]
14Code is available at: liir.cs.kuleuven.be/software.php,Acknowledgments,[0],[0]
"The current leading paradigm for temporal information extraction from text consists of three phases: (1) recognition of events and temporal expressions, (2) recognition of temporal relations among them, and (3) time-line construction from the temporal relations.",abstractText,[0],[0]
"In contrast to the first two phases, the last phase, time-line construction, received little attention and is the focus of this work.",abstractText,[0],[0]
"In this paper, we propose a new method to construct a linear time-line from a set of (extracted) temporal relations.",abstractText,[0],[0]
"But more importantly, we propose a novel paradigm in which we directly predict start and end-points for events from the text, constituting a time-line without going through the intermediate step of prediction of temporal relations as in earlier work.",abstractText,[0],[0]
"Within this paradigm, we propose two models that predict in linear complexity, and a new training loss using TimeML-style annotations, yielding promising results.",abstractText,[0],[0]
Temporal Information Extraction by Predicting Relative Time-lines,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 162–171 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
162
is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.",text,[0],[0]
We examine the task of Natural Sentence Grounding in Video (NSGV).,1 Introduction,[0],[0]
"Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sentence, as shown in Figure 1 (a).",1 Introduction,[0],[0]
"Comparing with the other video researches, such as bidirectional video-sentence retrieval (Xu et al., 2015b), video attractiveness prediction (Chen et al., 2018, 2016), and video captioning (Pasunuru and Bansal, 2017; Wang et al., 2018a,b), NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging.
",1 Introduction,[0],[0]
"∗ Work done while Jingyuan Chen and Xinpeng Chen were Research Interns with Tencent AI Lab.
",1 Introduction,[0],[0]
1,1 Introduction,[0],[0]
"The project homepage is https:// jingyuanchen.github.io/archive/tgn.html.
",1 Introduction,[0],[0]
"Recently, several related works (Gao et al., 2017; Hendricks et al., 2017) leverage one temporal sliding window approach over video sequences to generate video segment candidates, which are then independently combined (Gao et al., 2017) or compared (Hendricks et al., 2017) with the given sentence to make the grounding prediction.",1 Introduction,[0],[0]
"Although the existing works have achieved promising performances, they are still suffering from inferior effectiveness and efficiency.",1 Introduction,[0],[0]
"First, existing methods project the video segment and sentence into one common space, as shown in Figure 1 (b),
where the two generated embedding vectors are used to perform the matching between video segment and sentence.",1 Introduction,[0],[0]
"Such a matching is only performed in the global segment and sentence level and thus not expressive enough, which ignores the fine-grained matching relations between video frames and the words in sentences.",1 Introduction,[0],[0]
"Second, in order to handle the diverse temporal scales and locations of the candidate segments, exhaustive matching between the large amount of overlapping segments and the sentence is required.",1 Introduction,[0],[0]
"As such, the sliding window methods are very computationally expensive.
",1 Introduction,[0],[0]
"In order to tackle the above two limitations, we introduce a novel Temporal GroundNet (TGN) model, the first dynamic single-stream deep architecture for the NSGV task that takes full advantage of fine-grained interactions between video frames and words in a sentence, as shown in Figure 1 (c).",1 Introduction,[0],[0]
"TGN sequentially processes video frames, where at each time step we rely on a novel multimodal interactor to exploit the evolving fine-grained frameby-word interactions.",1 Introduction,[0],[0]
"Then, TGN works on the yielded interaction status to simultaneously score a set of temporal candidates of multiple scales and finally localize the video segment that corresponds to the sentence.",1 Introduction,[0],[0]
"More importantly, our proposed TGN is able to analyze an untrimmed video frame by frame without resorting to handling overlapping temporal video segments.",1 Introduction,[0],[0]
Grounding natural language in image is also known as natural language object retrieval.,2.1 Grounding Natural Language in Image,[0],[0]
"The task is to localize an image region described by natural language, which involves comprehending and modeling different spatial contexts, such as spatial configurations (Hu et al., 2016), attributes (Yu et al., 2018), and relationships between objects (Hu et al., 2017).",2.1 Grounding Natural Language in Image,[0],[0]
"Specifically, the task is usually formulated as a ranking problem over a set of candidate regions in a given image, where candidate spatial locations come from region proposal methods (Uijlings et al., 2013; Jie et al., 2016b,a; Ren et al., 2017) such as EdgeBox (Zitnick and Dollár, 2014).",2.1 Grounding Natural Language in Image,[0],[0]
"Earlier studies (Mao et al., 2016; Rohrbach et al., 2016) score the generated candidate regions according to their appearances and spatial features along with features of the entire image.",2.1 Grounding Natural Language in Image,[0],[0]
"However, these meth-
ods fail to incorporate the interactions between objects, because the scoring process of each region proposal is isolated.",2.1 Grounding Natural Language in Image,[0],[0]
"More recent studies (Hu et al., 2017; Nagaraja et al., 2016) improve the performance with the aid of modeling relationships between objects.",2.1 Grounding Natural Language in Image,[0],[0]
"Analogous to spatial grounding in image, this work studies a similar problem—temporal natural language grounding in video.",2.2 Grounding Natural Language in Video,[0],[0]
"Earlier works (Yu and Siskind, 2013; Lin et al., 2014) learn the semantics of sentences, which are then matched to visual concepts via exploiting object appearance, motion and spatial relationships.",2.2 Grounding Natural Language in Video,[0],[0]
"However, they are limited to a small set of objects.",2.2 Grounding Natural Language in Video,[0],[0]
"Recently, larger datasets (Gao et al., 2017; Hendricks et al., 2017) are constructed to support more flexible groundings.",2.2 Grounding Natural Language in Video,[0],[0]
"The methods proposed in (Gao et al., 2017; Hendricks et al., 2017) learn a common embedding space shared by video segment features and sentence representations, in which their similarities are measured.",2.2 Grounding Natural Language in Video,[0],[0]
"Specifically, moment context network (MCN) (Hendricks et al., 2017) learns a shared embedding for video clip-level features and language features.",2.2 Grounding Natural Language in Video,[0],[0]
"The video features integrate local video features, global features, and temporal endpoint features.",2.2 Grounding Natural Language in Video,[0],[0]
"Cross-modal temporal regression localizer (CTRL) (Gao et al., 2017) contains four modules, specifically a visual encoder extracting clip-level features with context, a sentence encoder yielding its embedding through LSTM, a multimodal processing network generating the fused representations via element-wise operations, and a temporal regression network producing the alignment scores and location offsets.",2.2 Grounding Natural Language in Video,[0],[0]
"One limitation of those common space matching methods is that the video segment generation process is computationally expensive, as they carry out overlapping sliding window matching (Gao et al., 2017) or exhaustive search (Hendricks et al., 2017).",2.2 Grounding Natural Language in Video,[0],[0]
"Another weakness is that they exploit the relationships between textual and visual modalities by conducting a simple concatenation (Gao et al., 2017) or measuring a squared distance loss (Hendricks et al., 2017), which ignores the evolving fine-grained video-sentence interactions.",2.2 Grounding Natural Language in Video,[0],[0]
"In this paper, a novel model TGN is proposed to deal with the aforementioned limitations for the task of natural sentence grounding in video.",2.2 Grounding Natural Language in Video,[0],[0]
"Given a long and untrimmed video sequence V and a natural sentence S, the NSGV task is to localize a video segment Vs = {ft}tet=tb from V , beginning at tb and ending at te, which corresponds to and expresses the same semantic meaning as the given sentence S.",3 Approach,[0],[0]
"In order to perform the grounding, each video is represented as V = {ft}Tt=1, where T is the total number of frames and ft denotes the feature representation of the t-th video frame.",3 Approach,[0],[0]
"Similarly, each sentence is represented as S = {wn}Nn=1, where wn is the embedding vector of the n-th word in the sentence",3 Approach,[0],[0]
"andN denotes the total number of words.
",3 Approach,[0],[0]
"We propose a novel model, namely Temporal GroundNet (TGN), to tackle the NSGV problem.",3 Approach,[0],[0]
"As illustrated in Figure 2, TGN consists of three modules.",3 Approach,[0],[0]
"1) Encoder: visual and textual encoders are used to compose the video frame representations and word embeddings, respectively.",3 Approach,[0],[0]
2) Interactor: a multimodal interactor learns the frame-byword interactions between the video and sentence.,3 Approach,[0],[0]
3) Grounder: a grounder generates the temporal localization in one single pass.,3 Approach,[0],[0]
"Please note that these three modules are fully coupled together, which can thus be trained in an end-to-end fashion.",3 Approach,[0],[0]
"With the obtained video frame features V = {ft}Tt=1 and word embeddings of the sentence S = {wn}Nn=1, we employ two long shortterm memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to sequentially process the two different modalities, i.e., video and sentence, independently.",3.1 Encoder,[0],[0]
"Specifically, one LSTM sequentially models the video V , yielding the hidden states {hvt }Tt=1, while the other LSTM processes the sequential words in the sentence S, resulting in its corresponding hidden states {hsn}Nn=1.",3.1 Encoder,[0],[0]
"Owing to natural behaviors and characteristics of LSTMs, both {hvt }Tt=1 and {hsn}Nn=1 can encode and aggregate the contextual evidences (Wang and Jiang, 2016b) from the sequential video frame representations and word embeddings of the sentence, respectively, meanwhile casting aside the irrelevant information.",3.1 Encoder,[0],[0]
"Based on the hidden states of the video and sentence yielded from the leveraged encoders, we de-
sign a multimodal interactor to perform the frameby-word interactions between the video and sentence.",3.2 Interactor,[0],[0]
"First, the frame-specific sentence feature is generated through summarizing the sentence hidden states by considering their relationships with the specific video frame at each time step.",3.2 Interactor,[0],[0]
"Afterwards, an interaction LSTM, dubbed i-LSTM, is performed to aggregate frame-by-word interactions.",3.2 Interactor,[0],[0]
Directly operating on the clip-level and sentencelevel features generated by the encoders cannot well exploit the frame-by-word relationships between video and sentence that evolve over time.,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Inspired by (Wang and Jiang, 2016a; Feng et al., 2018), we introduce one novel frame-specific sentence feature, which adaptively summarizes the hidden states of the sentence {hsn}Nn=1 with respect to the t-th video frame:
",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Hst = N∑
n=1
αnt h s n, (1)
where Hst denotes the summarized sentence representation specified by the t-th video frame.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"At each time step t, we utilize the hidden state hvt to
selectively attend the words and summarize them accordingly.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
The attention weight αnt encodes the degree to which the n-th word in the sentence is aligned with the t-th video frame.,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"As the processing of video frames proceeds, the attention weights dynamically change regarding to the current video frame.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"As such, the generated framespecific sentence features {Hst}Tt=1 consider the frame-by-word relationships between all the video frames and all the words in the sentence.
",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"As the generation of frame-specific sentence feature is deeply coupled with the following interaction LSTM, we will explain the calculation of the attention weight αnt later.
",3.2.1 Frame-Specific Sentence Feature,[0],[0]
3.2.2 Interaction LSTM (i-LSTM),3.2.1 Frame-Specific Sentence Feature,[0],[0]
"In order to accurately ground the sentence in a video, the multimodal interation behaviors between the video and sentence need to be comprehensively modeled.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Previous approaches on multimodal interactions were limited to concatenation (Zhu et al., 2016), element-wise product or sum (Gao et al., 2017), and bilinear pooling (Fukui et al., 2016).",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"These methods are not expressive enough since they ignore the evolving fine-grained interactions across video and sentence, particularly the frame-by-word interactions.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"In this paper, we propose a novel multimodal interaction model, which is realized by LSTM.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"We term it interaction LSTM (i-LSTM), which sequentially processes the video sequence frame by frame, holding deep interactions with the words in the sentence.
",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"In order to well capture the complicated temporal interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th video hidden state hvt and the t-th frame-specific sentence feature Hst as:",3.2.1 Frame-Specific Sentence Feature,[0],[0]
rt = h v t ‖ Hst .,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"rt is then fed into the i-LSTM unit to yield the t-th intermediate interaction status between the video and sentence:
hrt = i-LSTM(rt,h r t−1), (2)
where hrt is the yielded hidden state, encoding the fine-grained interactions between the word and video frame.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
hrt will be further used to perform the grounding process.,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Due to the inherent properties and characteristics of LSTMs, important cues regarding to grounding up to the current stage will be “remembered”, while non-essential ones will be “forgotten”.
",3.2.1 Frame-Specific Sentence Feature,[0],[0]
Now we go back to the generation of attention weight αnt in Eq.,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"(1), based on the obtained vi-
sual hidden states hvt and textual hidden state h s n as well as the yielded interaction status hrt−1 in the previous step.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"The widely used soft-attention mechanism (Xu et al., 2015a; Chen et al., 2017) is used to generate the attention weights in a frameby-word manner.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"As aforementioned, the i-LSTM models the evolving frame-by-word interactions between the sentence and video.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Therefore, the attention weight between the n-th word hsn and the t-th video frame hvt is determined by not only the content of the video and sentence but also their interaction status.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
"Thus, we design one network to compute the relevance score of one video frame with respect to each word:
βnt = w ᵀ tanh(WShsn+W V hvt +W Rhrt−1+b)+c, (3)
where vector w, matrices W∗, bias vector b, and bias c are the network parameters to be learned.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
hrt−1 is the hidden state of the i-LSTM at t − 1 time step.,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"The final word-level attention weights are obtained by:
αnt = exp(βnt )",3.2.1 Frame-Specific Sentence Feature,[0],[0]
∑N j=1 exp(β j t ) .,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"(4)
The obtained attention weight αnt is thereafter to generate the frame-specific sentence feature as in Eq.",3.2.1 Frame-Specific Sentence Feature,[0],[0]
(1).,3.2.1 Frame-Specific Sentence Feature,[0],[0]
"In this section, we introduce the grounder, which works on the yielded interaction status hrt from i-LSTM, to localize the video segment that corresponds to the sentence.",3.3 Grounder,[0],[0]
"Our proposed grounder works in one single pass without introducing overlapping sliding windows, which thus results in a fast runtime.",3.3 Grounder,[0],[0]
"As shown in Figure 2, at each time step t, the grounder efficiently scores a set of K grounding candidates by considering multiple time scales (Buch et al., 2017) that end at time step t. Specifically, we use different K for different datasets, which is determined by the distribution of the lengths of all ground-truth groundings in a certain dataset.",3.3 Grounder,[0],[0]
"To simplify the following discussions, the lengths of K time scales are assumed to be an arithmetic sequence with the common difference δ",3.3 Grounder,[0],[0]
and all the temporal candidates are sorted by increasing lengths.,3.3 Grounder,[0],[0]
"In other words, the length of the k-th candidate is kδ.",3.3 Grounder,[0],[0]
"Note that all grounding candidates considered at time t have a fixed ending boundary.
",3.3 Grounder,[0],[0]
"Specifically, at each time step t, the grounder will classify each temporal candidate in consideration as a positive grounding or a negative one with respect to the given sentence.",3.3 Grounder,[0],[0]
"Considering multiple time scales, the grounder will generate the confidence scores Ct = (c1t , c 2 t , ..., c K t ) that correspond to the set of K visual grounding candidates, all ending at time step t. The hidden state hrt generated by i-LSTM at time t, representing the interaction status between the sentence and video sequence up to the current position, is naturally suited to yield the confidence scores for the different time scales ending at time step t.",3.3 Grounder,[0],[0]
"In this paper, the confidence scores, indicating the sentence grounding, are generated by a fullyconnected layer with sigmoid nonlinearity:
Ct = σ(W Khrt + b r t ), (5)
where WK and brt are the corresponding parameters, and σ denotes the nonlinear sigmoid function.",3.3 Grounder,[0],[0]
The training samples collected in X for NSGV are video-sentence pairs.,3.4 Training,[0],[0]
"Specifically, each video V is temporally associated with a set of sentence annotations: A = {(Si, tbi , tei )}Mi=1, where M is the number of annotated sentences of the video, and Si is a sentence description of a video clip, with tbi and t e",3.4 Training,[0],[0]
i indicating the beginning and ending time in the video.,3.4 Training,[0],[0]
Each training sample corresponds to a ground-truth matrix y ∈ RT×K with binary entries.,3.4 Training,[0],[0]
"We use ykt to denote the (t, k)-th entry of the ground-truth matrix.",3.4 Training,[0],[0]
ykt is interpreted as whether the k-th grounding candidate at time step t corresponds to the given natural sentence.,3.4 Training,[0],[0]
"Concretely, the entry ykt is set as 1, indicating that the corresponding video segment (ends at time step t with length kδ) has a temporal Intersection-over-Union (IoU) with (tb, te) larger than a threshold θ.",3.4 Training,[0],[0]
"Otherwise ykt is set as 0.
",3.4 Training,[0],[0]
"For a training pair (V, S) ∈ X , the objective at time step t is given by a weighted binary cross entropy loss L(t, V, S):
− K∑ k=1 wk0y",3.4 Training,[0],[0]
k,3.4 Training,[0],[0]
t log c k,3.4 Training,[0],[0]
"t +w k 1(1−ykt ) log(1− ckt ), (6)
where the weights wk0 and w k 1 are calculated according to the frequencies of positive and negative samples in the training set with length kδ.",3.4 Training,[0],[0]
"ykt is the ground-truth value and ckt denotes the prediction results by our proposed model.
",3.4 Training,[0],[0]
"Our TGN backpropagates at every time step t to learn all the parameters of the fully-coupled three modules: encoder, interactor, and grounder.",3.4 Training,[0],[0]
"The objective of all training video-sentence pairs X is defined as:
LX = ∑
(V,S)∈X T∑ t=1 L(t, V, S).",3.4 Training,[0],[0]
(7),3.4 Training,[0],[0]
"During the inference stage, given a testing video V and a sentence S, the textual and visual encoders first generate hidden states for each word and video frame, respectively.",3.5 Inference,[0],[0]
"Then, the interactor sequentially goes through the video frame by frame to yield the frame-by-word interaction status.",3.5 Inference,[0],[0]
"At each position t, a K-dimensional score vector Ct is generated by the grounder.",3.5 Inference,[0],[0]
"Therefore, after processing the last frame in the video, a T × K score matrix is obtained for the whole video, with the (t, k)-th entry in the matrix indicating the probability that the video segment ended at position t with length kδ in video V corresponds to sentence S. Eventually, the evaluation is reduced to a ranking problem over all the grounding candidates based on the generated scores.",3.5 Inference,[0],[0]
"In this section, we evaluate the effectiveness of our proposed TGN on the NSGV task.",4 Experiments,[0],[0]
"We begin by describing the datasets used for evaluation, followed by the introduction of the experimental settings including the baselines, configurations, as well as the evaluation metrics.",4 Experiments,[0],[0]
"Afterwards, we demonstrate the effectiveness of TGN by comparing with the state-of-the-art approaches and efficiency through a runtime test.",4 Experiments,[0],[0]
"We experiment on three publicly accessible datasets: DiDeMo (Hendricks et al., 2017), TACoS",4.1 Datasets,[0],[0]
"(Regneri et al., 2013), and ActivityNet Captions (Fabian Caba Heilbron and Niebles, 2015).",4.1 Datasets,[0],[0]
These datasets consist of videos as well as their associated temporally annotated sentences.,4.1 Datasets,[0],[0]
DiDeMo2 consists of 10464 25-50 second long videos.,4.1 Datasets,[0],[0]
"The same split provided by (Hendricks et al., 2017) is used for a fair comparison, with 33008, 4180, and 4022 video-sentence pairs for training, validation, and testing, respectively.
",4.1 Datasets,[0],[0]
"2https://goo.gl/JpbAhg.
",4.1 Datasets,[0],[0]
"TACoS3 consists of 127 videos selected from the MPII Cooking Composite Activities video corpus (Rohrbach et al., 2012).",4.1 Datasets,[0],[0]
"The same split as in (Gao et al., 2017) is used, consisting of 10146, 4589, and 4083 video-sentence pairs for training, validation, and testing, respectively.",4.1 Datasets,[0],[0]
"ActivityNet Captions4 consists of 19, 209 videos amounting to 849 hours.",4.1 Datasets,[0],[0]
"The public split is used for our experiments, which has 37421, 17505, and 17031 video-sentence pairs for training, validation, and testing, respectively.",4.1 Datasets,[0],[0]
"We compare our proposed TGN against the following two state-of-the-art models, specifically, the MCN (Hendricks et al., 2017), CTRL (Gao et al., 2017), visual-semantic alignment with LSTM (VSA-RNN) (Karpathy and Li, 2015), and visual-semantic alignment with skip thought vector (VSA-STV) (Kiros et al., 2015).",4.2.1 Baselines,[0],[0]
"For fair comparisons, we compare the results of MCN on DiDeMo and the results of CTRL, VSA-RNN, VSA-STV on TACoS reported in their papers.",4.2.1 Baselines,[0],[0]
A grounding of one natural sentence in a video is considered as “correct” if its temporal IoU with the ground-truth boundary is above a threshold θ.,4.2.2 Evaluation Metrics,[0],[0]
"To be consistent with the baselines, we adopt R@N , IoU=θ, and mean IoU (mIoU) as our evaluation metrics.",4.2.2 Evaluation Metrics,[0],[0]
"R@N , IoU=θ represents the percentage of testing samples which have at least one of the top-N results with IoU larger than θ. mIoU means the average IoU over all testing samples.",4.2.2 Evaluation Metrics,[0],[0]
"Generally, the video frame features are usually extracted with a time resolution.",4.2.3 Configurations,[0],[0]
"For the videos in DiDeMo and TACoS, we sample every 5 second as done by (Hendricks et al., 2017).",4.2.3 Configurations,[0],[0]
"As the videos in DiDeMo are 25-30 second long, the video feature length is reduced to 6.",4.2.3 Configurations,[0],[0]
"For videos in ActivityNet Captions, we sample every second.",4.2.3 Configurations,[0],[0]
"To extract visual features, we consider both appearance and optical flow features.",4.2.3 Configurations,[0],[0]
"Specifically, we study four widely-used visual features: VGG16 (Simonyan and Zisserman, 2014), C3D (Tran et al., 2015), Inception-V4 (Szegedy et al., 2017), and optical flow (Wang et al., 2016).",4.2.3 Configurations,[0],[0]
"Please note that when
3https://goo.gl/ajmsva.",4.2.3 Configurations,[0],[0]
"4https://goo.gl/N355bG.
comparing with specific baseline methods, we use the same features as baseline methods, specifically, VGG16 and optical flow for MCN and C3D for CTRL, VSA-RNN, and VSA-STV.
",4.2.3 Configurations,[0],[0]
"For sentences, we tokenize each sentence by Stanford CoreNLP",4.2.3 Configurations,[0],[0]
"(Manning et al., 2014) and use the 300-D word embeddings from GloVe (Pennington et al., 2014) to initialize the models.",4.2.3 Configurations,[0],[0]
The words not found in GloVe are initialized as zero vectors.,4.2.3 Configurations,[0],[0]
"The hidden state dimensions of all LSTMs (including the video, sentence, and interaction LSTMs) are set as 512.",4.2.3 Configurations,[0],[0]
"We use the Adam (Kingma and Ba, 2014) optimizer with β1 = 0.5 and β2 = 0.999.",4.2.3 Configurations,[0],[0]
The initial learning rate is set to 0.001.,4.2.3 Configurations,[0],[0]
"We train the network for 200 iterations, and the learning rate is gradually decayed over time.",4.2.3 Configurations,[0],[0]
The mini-batch size is set to 64.,4.2.3 Configurations,[0],[0]
Experiments on DiDeMo.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
Table 1 illustrates the performance comparisons on the DiDeMo dataset.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"In addition to MCN, we also compare with the baseline Moment Frequency Prior (MFP) in (Hendricks et al., 2017), which selects segments corresponding to the positions of videos in the training dataset with most annotations.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"First, TGN with different features can significantly outperforms the “prior baseline” MFP, which retrieves segments corresponding to the most common start and end points in the dataset.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Second, it can be observed that with the same visual features, specifically VGG16 and optical flow, TGN significantly outperforms MCN.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
And the performance of TGN with optical flow is better than that with VGG16.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"One possible reason is that the videos in DiDeMo are relatively short, which only contain a single event.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"In such a case, the action information plays
a more critical role.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"This finding is also consistent with (Hendricks et al., 2017).",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"By fusing the results obtained by VGG16 and optical flow together, the performance can be further boosted, as demonstrated by TGN-Fusion and MCN-Fusion.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Third, MCN introduces the temporal endpoint feature (TEF) as prior knowledge, which indicates when a segment occurs in a video.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"With TEF, the performance of MCN can be significantly improved.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"However, it is still inferior to our proposed TGN.
MCN is designed as an enumeration-based approach.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
Each video in the DiDeMo dataset is split into six five-second chunks which are considered as the time unit for localization.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Therefore, in total there are only C27 = 7 × 6/2 = 21 different ways of localization for DiDeMo videos.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Therefore, although MCN can be effectively applied to videos with several chunks due to the small search space, it is not practical for untrimmed long videos.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"In the Section 4.3.3, we will evaluate and compare the efficiencies of MCN, CTRL, and our proposed TGN.
Experiments on TACoS. Table 2 illustrates the experimental results on TACoS. First, it can be observed that CTRL performs much better than VSA-RNN and VSA-STV.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"The reasons lie in twofold (Gao et al., 2017).",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"On one hand, CTRL utilizes a multilayer alignment network to learn better alignment.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"On the other hand, VSA-RNN and VSA-STV do not encode temporal context information of video.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Second, with the same visual feature, specifically C3D, TGN-C3D significantly outperforms CTRL-C3D.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
This is due to the fact that TGN exploits not only the contextual information but also the fine-grained interaction behaviors.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"More concretely, TGN considers the frameby-word correlations by introducing an attentive combinations of the words in the sentence, where each weight encodes the degree to which the word is aligned with each specific frame.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"This mechanism is beneficial to capturing the informative se-
mantics in the sentences for alignment.
",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
Experiments on ActivityNet Captions.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Besides the two benchmarks, we also evaluate our model on the ActivityNet Captions dataset.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
Different CNNs are used to encode video visual information.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Specifically, we consider VGG16, C3D, and Inception-V4.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
The results are included in Table 3.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"First, our proposed TGN can perform effectively on long untrimmed videos.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Second, Inception-V4 performs generally better than VGG16 and C3D, which is consistent with the finding in (Canziani et al., 2016).",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"Therefore, more powerful visual representations of video features will undoubtedly improve the the performance of our proposed TGN on the NSGV task.
",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
Some qualitative results of our proposed TGN on ActiveityNet Captions dataset is illustrated in Figure 3.,4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"It can be observed that with different visual features, different grounding results are obtained.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"For the first and second examples, TGN with VGG16 and Inception-V4 generates more accurate groundings than that with C3D, while TGN with C3D yields more accurate grounding results for the third example.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
"More specifically, our proposed TGN with VGG16 and Inception-V4 can well identify the visual information related with the sentence, i.e. “A man in a red shirt claps his hands”.",4.3.1 Comparisons with State-of-the-Arts,[0],[0]
We examine the effect of the frame-by-word attention in interactor.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
We ablate TGN into two other methods.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
1) NA: There is no attention layer in this model.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"After obtaining the sequential hidden states of the sentence, mean pooling is used to generate the representation for the whole sentence.
",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"Then the generated representation is concatenated with video representation, based on which the scores for multiple grounding candidates are predicted.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
2) NM:,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
The idea of generating framespecific sentence feature is still reserved in the NM model.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
The difference between NM and TGN is that there is no interaction LSTM in NM.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"Specifically, when calculating the attention weight for each word as in Eq. (3), the hidden state hrt−1 indicating the interaction status is not incorporated.
",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
The quantitative results are displayed in Table 4.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"First, when the attention mechanism is applied (NM), the performance is improved as compared with utilizing mean pooling (NA) for sentence features.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
The better performance demonstrates that our assumption about the evolving frame-by-word correlations between two modalities is reasonable.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"This also indicates that it is necessary to discriminate the contribution of each word in a sentence
to perform the NSGV task.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"Second, utilizing the interaction LSTM module (TGN) achieves better performance than simply concatenating the video representation and the attentive sentence representation (NM).",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"This result indicates that the interaction LSTM yields better interaction status between these two modalities, which can thereby benefit the final grounding.
",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
We provide some qualitative examples in Figure 4 for a better understanding of the frame-byword attention.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"Meanwhile, the grounding results yielded by TGN-Fusion (considering both VGG16 and optical flow) are also illustrated.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
This experiment is designed to verify whether the frameby-word attention mechanism in interactor is useful to highlight the representative concepts in the sentence.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"The attention weights α for two testing samples in DiDeMo are illustrated in Figure 4, where the darker the color is, the larger
the attention weight is.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
It can be observed that some words well match the frames.,4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"For example, in Figure 4 (a), the concept “forest” appears across all the video frames presenting an evenly distributed attention weights, while the other concept “waterfall” only presents in the first two frames.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"In addition to nouns, the adjective “blue” in Figure 4 (b) also receives relatively higher attention weights in relevant frames.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"Lastly, for stop words like “a”, “the” and “in”, their attention weights, which are very small, also present an even distribution.",4.3.2 Effect of Frame-by-Word Attention,[0],[0]
"We evaluate the efficiency of our proposed TGN, by comparing its runtime with MCN and CTRL on a Tesla M40 GPU.",4.3.3 Efficiency,[0],[0]
The efficiency is measured by frames per second (FPS) as shown in Table 5.,4.3.3 Efficiency,[0],[0]
Please not that the feature extraction time is excluded.,4.3.3 Efficiency,[0],[0]
"It can be observed that our TGN model achieves much faster processing speeds, with 1,363 fps vs. 562 and 286 for CTRL and MCN, respectively.",4.3.3 Efficiency,[0],[0]
The reason mainly attributes to that the proposed TGN only process each video in one single pass without processing overlapped sliding windows.,4.3.3 Efficiency,[0],[0]
"In this paper, we focused on the task of natural sentence grounding in video that is believed to offer a comprehensive understanding of bridging computer vision and natural language processing.",5 Conclusion,[0],[0]
"Towards this task, we proposed an end-to-end Temporal GroundNet (TGN) by incorporating the evolving fine-grained frame-by-word interactions across video-sentence modalities to generate a visual grounding tailored to each given natural sentence.",5 Conclusion,[0],[0]
"Moreover, TGN performs efficiently, which only needs to process the video sequence in one single pass.",5 Conclusion,[0],[0]
Extensive experiments on three realworld datasets clearly demonstrate the effectiveness and efficiency of the proposed TGN.,5 Conclusion,[0],[0]
"We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences.",abstractText,[0],[0]
"Specifically, a novel Temporal GroundNet (TGN)1 is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence.",abstractText,[0],[0]
"TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frameby-word interactions, and finally grounds the segment corresponding to the sentence.",abstractText,[0],[0]
"Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass.",abstractText,[0],[0]
We extensively evaluate our proposed TGN on three public datasets with significant improvements over the stateof-the-arts.,abstractText,[0],[0]
We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.,abstractText,[0],[0]
Temporally Grounding Natural Sentence in Video,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2225–2229, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Understanding language requires the ability to perform basic inference– to make conclusions about what is likely true or false based on what is said.,1 Overview,[0],[0]
"For example, given the sentence She fixed the bug, we should almost certainly infer that the bug is fixed.",1 Overview,[0],[0]
"However, rather than stating plainly that She fixed the bug, one might instead say:
(1a)",1 Overview,[0],[0]
"She managed to fix the bug before midnight.
",1 Overview,[0],[0]
(1b),1 Overview,[0],[0]
"She happened to fix the bug while refactoring.
",1 Overview,[0],[0]
"In either case, the hearer should still infer that the bug is fixed.",1 Overview,[0],[0]
But it is not as easy as always inferring that embedded clauses are true.,1 Overview,[0],[0]
"By changing only one word, these sentence no longer give a clear indication as to whether or not the bug has been fixed:
(2a) She wanted to fix the bug before midnight.
",1 Overview,[0],[0]
(2b),1 Overview,[0],[0]
"She planned to fix the bug while refactoring.
",1 Overview,[0],[0]
"Implicative verbs, like those in (1), give rise to entailments, while non-implicative verbs, like those in (2), do not.",1 Overview,[0],[0]
"It is therefore vital to natural language understanding to differentiate between clauses that are embedded under implicatives, which we can often infer to be either true or false, and those which are embedded under non-implicatives, for which such inferences cannot be made.",1 Overview,[0],[0]
"In this paper, we exploit a known linguistic property of implicative verbs– that their complement clause is constrained to be in the same tense as the main clause– in order to predict the tendency of verbs to behave implicatively.",1 Overview,[0],[0]
"We show that our method almost perfectly separates non-implicatives from implicatives in a small hand-labeled dataset, and that it provides strong signal for predicting entailments in sentences involving implicative verbs.",1 Overview,[0],[0]
"Some English verbs can take infinitival complements, meaning they can appear in constructions of the form VB∗1 to VB2, where VB ∗ 1 is the “main” verb (which can be conjugated1) and VB2 is the “complement” verb (which is in infinitive form).",2 Implicative Verbs,[0],[0]
"Examples (1a)-(2b) illustrate verbs taking infinitive complements.
",2 Implicative Verbs,[0],[0]
"Implicative verbs are a special subclass2 of such verbs which give rise to entailments involving their
1Here, * indicates that VB1 can match any verb form, e.g. VB, VBD, VBP, etc.",2 Implicative Verbs,[0],[0]
"VB2 can only match the base form VB.
2We note that factive verbs represent another special class of verbs which can take infinitival complements.",2 Implicative Verbs,[0],[0]
"Unlike implica-
2225
complement clauses.",2 Implicative Verbs,[0],[0]
"Individual implicatives can differ in the entailments they generate: e.g. while manage entails the truth of its complement, fail entails the falsity of its complement (failed to solve the problem⇒ didn’t solve the problem).",2 Implicative Verbs,[0],[0]
"Despite these differences, however, implicatives represent a coherent class of verbs in that they permit some inference to be made about their complements, and this inference is sensitive to the context (positive/negated) of the main clause.",2 Implicative Verbs,[0],[0]
"This contrasts with non-implicative verbs, like want, which do not permit any inference regarding their complements, and for which the truth of the complement is unaffected by negation in the main clause (Table 1).
",2 Implicative Verbs,[0],[0]
"The method described in this paper aims to separate implicatives from non-implicatives (manage vs. want), rather than to differentiate between types implicatives (manage vs. fail).",2 Implicative Verbs,[0],[0]
"Making this implicative/non-implicative distinction is a necessary first step toward handling inferences involving embedded clauses, and one that, to date, has only been performed using manually-constructed word lists (MacCartney, 2009; Recasens et al., 2013).",2 Implicative Verbs,[0],[0]
"Karttunen (1971) observed that, in sentences involving implicatives, the tense of the main verb must necessarily match the tense of the complement clause.",2.1 Tense Constraints on Complement Clauses,[0],[0]
"For example, (3), in which the main clause and the complement are both in the past tense, is acceptable but (4), in which the complement is in the future, is clearly not.",2.1 Tense Constraints on Complement Clauses,[0],[0]
"For non-implicatives, however,
tives, factives presuppose, rather than entail, their complements.",2.1 Tense Constraints on Complement Clauses,[0],[0]
E.g. both I was/was not glad to solve the problem entail,2.1 Tense Constraints on Complement Clauses,[0],[0]
I solved the problem.,2.1 Tense Constraints on Complement Clauses,[0],[0]
"We do not address factives here, as factives rarely take infinitival complements: more often, they take “that” complements (e.g. know that, realize that).",2.1 Tense Constraints on Complement Clauses,[0],[0]
"Factives that do take infinitival complements are mostly phrasal (e.g. be glad to).
",2.1 Tense Constraints on Complement Clauses,[0],[0]
"no such constraint exists: (6) is perfectly felicitous.
",2.1 Tense Constraints on Complement Clauses,[0],[0]
"(3) I managed to solve the problem last night.
",2.1 Tense Constraints on Complement Clauses,[0],[0]
"(4) #I managed to solve the problem tomorrow.
",2.1 Tense Constraints on Complement Clauses,[0],[0]
"(5) I planned to solve the problem last night.
",2.1 Tense Constraints on Complement Clauses,[0],[0]
"(6) I planned to solve the problem tomorrow.
",2.1 Tense Constraints on Complement Clauses,[0],[0]
We exploit this property to predict implicativeness– whether the truth of a verb’s complement can be inferred– by observing the verb’s usage in practice.,2.1 Tense Constraints on Complement Clauses,[0],[0]
"We hypothesize that, given a large corpus, we should be able to distinguish implicative verbs from nonimplicative verbs by observing how often the main verb tense agrees/disagrees with the tense of the complement clause.",3 Method,[0],[0]
"Unfortunately, verbs in infinitival complement clauses are not conjugated, and so are not necessarily marked for tense.",3 Method,[0],[0]
"We therefore use the Stanford Temporal Tagger (TT) (Chang and Manning, 2012) in order to identify time-referring expressions (e.g. tomorrow or last night) and resolve them to either past, present, or future tense.
",3 Method,[0],[0]
"We find all sentences containing VB∗1 to VB2 constructions in the Annotated Gigaword corpus (Napoles et al., 2012).",3 Method,[0],[0]
We run the the TT over all of the sentences in order to identify time-referring expressions.,3 Method,[0],[0]
We only consider sentences in which a time-referring expression appears and is in a direct dependency relationship with the complement verb (VB2).,3 Method,[0],[0]
"We provide the TT with the document publication dates,3 which are used to resolve each time mention to a specific calendar date and time.",3 Method,[0],[0]
"We then map these time expressions coarsely to either past, present, or future tense by comparing the
3Provided as metadata in the Annotated Gigaword.
resolved time to the document creation time.",3 Method,[0],[0]
"Because of the fact that days were often not resolved correctly, or at all, we eventually throw away sentences in which the complement clause is labeled as present tense, as these are rarely true references to the present, and rather the result of incorrect time resolution, or implicit future references (e.g. I am going to solve the problem today implies the future as in later today, but this is not captured by the TT).",3 Method,[0],[0]
"We also assign the main clause to past, present, or future tense by using the fine-grained POS tag and a set of heuristics (for example, to check for modals).4
We assign a tense agreement score to each verb v as follows.",3 Method,[0],[0]
Let S be the set of all VB∗1 to VB2 constructions in which VB∗1 = v. Then tense agreement is simply 1|S| × |{s,3 Method,[0],[0]
∈ S,3 Method,[0],[0]
"| complement tense = main tense}|, i.e. the fraction of constructions in S in which the tenses of the main and complement clauses agree.",3 Method,[0],[0]
"We expect implicative verbs to occur mostly in agreeing constructions, and thus have high tense agreement, while nonimplicatives may occur in both agreeing and nonagreeing constructions, and thus should have lower tense agreement.",3 Method,[0],[0]
"Note that while in theory, implicatives should never appear in non-agreeing constructions, the time annotation process is very imprecise, and thus we do not expect perfect results.",3 Method,[0],[0]
Recreating list from Karttunen (1971).,4 Evaluation,[0],[0]
Karttunen (1971) provides a short illustrative list of 7 known implicatives5 and 8 non-implicatives (shown in Table 2).,4 Evaluation,[0],[0]
"As a first evaluation, we test whether tense agreement can accurately separate the verbs in this list, such that the implicatives are assigned higher agreement scores than the non-implicatives.",4 Evaluation,[0],[0]
Table 2 shows that this is indeed the case.,4 Evaluation,[0],[0]
"Tense agreement almost perfectly divides the list, with implicative verbs appearing above non-implicative verbs in all cases.",4 Evaluation,[0],[0]
"The one exception is decide (reportedly non-implicative), which appears above dare (reportedly implicative).",4 Evaluation,[0],[0]
"This error, however,
4Full set of heuristics in supplementary material.",4 Evaluation,[0],[0]
"5The original list had 8 implicatives, but we omit remember since, in our data, it occurred almost exclusively with recurring time expressions, which we were not able to map to a specific date/time and thus tense, e.g. consumers must remember to make payments every 14 days.
",4 Evaluation,[0],[0]
"seems understandable: while decide is not strictly implicative in the way manage is, it is often used as an implicative.",4 Evaluation,[0],[0]
"E.g. the sentence I decided to leave would likely be taken to mean I left.
",4 Evaluation,[0],[0]
Predicting Entailment.,4 Evaluation,[0],[0]
"Our interest is not in distinguishing implicatives from non-implicatives for its own sake, but rather to predict, based on the main verb, whether the truth of the complement can be inferred.",4 Evaluation,[0],[0]
We therefore conduct a second evaluation to assess how well tense agreement predicts this entailment property.,4 Evaluation,[0],[0]
"We design our evaluation following the recognizing textual entailment (RTE) task (Dagan et al., 2006), in which two sentences are given, a premise p and a hypothesis h, and the goal is to determine whether p reasonably entails h. To construct our p/h pairs, we take all the verbs extracted in Section 3 which appear in at least 50 tense-labeled sentences.",4 Evaluation,[0],[0]
"For each of these verbs, we choose 3 random sentences in which the verb appears as VB∗1 in a VB∗1 to VB2 construction.
6",4 Evaluation,[0],[0]
"From each sentence, we extract the complement clause by deleting VB∗1 to from the sentence, and conjugating VB2 to match the tense of VB∗1.",4 Evaluation,[0],[0]
We then use the original sentence as p and the extracted complement as h: e.g. a p/h pair might look like I get to interact with fellow professors/I interact with fellow professors.,4 Evaluation,[0],[0]
"We ask 5 independent annotators on Amazon Mechanical Turk to read each p and then determine whether h is true, false, or unclear given p.7",4 Evaluation,[0],[0]
We take the majority answer as the true label.,4 Evaluation,[0],[0]
"We expect that implicative verbs should lead to judgements which are decidedly true or false while non-implicatives should lead
6These sentences can come from anywhere in the Gigaword corpus, they are not required to contain time expressions.
",4 Evaluation,[0],[0]
"7Full annotation guidelines in supplementary material.
",4 Evaluation,[0],[0]
to mostly judgements of unclear.,4 Evaluation,[0],[0]
Figure 1 shows that these expectations hold.,4 Evaluation,[0],[0]
"When a verb with low tense agreement appeared as the main verb of a sentence, the truth of the complement could only be inferred 30% of the time.",4 Evaluation,[0],[0]
"When a verb with high tense agreement appeared as the main verb, the truth of the complement could be inferred 91% of the time.",4 Evaluation,[0],[0]
This difference is significant at p < 0.01.,4 Evaluation,[0],[0]
"That is, tense agreement provides a strong signal for identifying non-implicative verbs, and thus can help systems avoid false-positive entailment judgements, e.g. incorrectly inferring that wanting to merge⇒ merging (Table 3).
",4 Evaluation,[0],[0]
"Interestingly, tense agreement accurately models verbs that are not implicative by definition, but which nonetheless tend to behave implicatively in practice.",4 Evaluation,[0],[0]
"For example, our method finds high tense agreement for choose to and be allowed to, which are often used to communicate, albeit indirectly, that their complements did in fact happen.",4 Evaluation,[0],[0]
"To convince ourselves that treating such verbs as implicatives makes sense in practice, we manually look through
the RTE3 dataset (Giampiccolo et al., 2007) for examples containing high-scoring verbs according to our method.",4 Evaluation,[0],[0]
Table 3 shows some example inferences that hinge precisely on recognizing these types of de facto implicatives.,4 Evaluation,[0],[0]
"Language understanding tasks such as RTE (Clark et al., 2007; MacCartney, 2009) and bias detection (Recasens et al., 2013) have been shown to require knowledge of implicative verbs, but such knowledge has previously come from manually-built word lists rather than from data.",5 Discussion and Related Work,[0],[0]
"Nairn et al. (2006) and Martin et al. (2009) describe automatic systems to handle implicatives, but require hand-crafted rules for each unique verb that is handled.",5 Discussion and Related Work,[0],[0]
"The tense agreement method we present offers a starting point for acquiring such rules from data, and is well-suited for incorporating into statistical systems.",5 Discussion and Related Work,[0],[0]
"The clear next step is to explore similar data-driven means for learning the specific behaviors of individual implicative verbs, which has been well-studied from a theoretical perspective (Karttunen, 1971; Nairn et al., 2006; Amaral et al., 2012; Karttunen, 2012).",5 Discussion and Related Work,[0],[0]
Another interesting extension concerns the role of tense in word representations.,5 Discussion and Related Work,[0],[0]
"While currently, tense is rarely built directly into distributional representations of words (Mikolov et al., 2013; Pennington et al., 2014), our results suggest it may offer important insights into the semantics of individual words.",5 Discussion and Related Work,[0],[0]
We leave this question as a direction for future work.,5 Discussion and Related Work,[0],[0]
Differentiating between implicative and nonimplicative verbs is important for discriminating inferences that can and cannot be made in natural language.,6 Conclusion,[0],[0]
"We have presented a data-driven method
that captures the implicative tendencies of verbs by exploiting the tense relationship between the verb and its complement clauses.",6 Conclusion,[0],[0]
"This method effectively separates known implicatives from known non-implicatives, and, more importantly, provides good predictive signal in an entailment recognition task.",6 Conclusion,[0],[0]
We would like to thank Florian Schwartz for valuable discussions.,Acknowledgments,[0],[0]
"This research was supported by a Facebook Fellowship, and by gifts from the Alfred P. Sloan Foundation, Google, and Facebook.",Acknowledgments,[0],[0]
This material is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program).,Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.,Acknowledgments,[0],[0]
The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.,Acknowledgments,[0],[0]
"Implicative verbs (e.g. manage) entail their complement clauses, while non-implicative verbs (e.g. want) do not.",abstractText,[0],[0]
"For example, while managing to solve the problem entails solving the problem, no such inference follows from wanting to solve the problem.",abstractText,[0],[0]
"Differentiating between implicative and non-implicative verbs is therefore an essential component of natural language understanding, relevant to applications such as textual entailment and summarization.",abstractText,[0],[0]
We present a simple method for predicting implicativeness which exploits known constraints on the tense of implicative verbs and their complements.,abstractText,[0],[0]
"We show that this yields an effective, data-driven way of capturing this nuanced property in verbs.",abstractText,[0],[0]
Tense Manages to Predict Implicative Behavior in Verbs,title,[0],[0]
"Matrix balancing is the problem of rescaling a given square nonnegative matrix A ∈ Rn×n≥0 to a doubly stochastic matrix RAS, where every row and column sums to one, by multiplying two diagonal matrices R and S.",1. Introduction,[0],[0]
"This is a fundamental process for analyzing and comparing matrices in a wide range of applications, including input-output analysis in economics, called the RAS approach (Parikh, 1979; Miller & Blair, 2009; Lahr & de Mesnard, 2004), seat assignments in elections (Balinski, 2008; Akartunalı &
1National Institute of Informatics 2JST",1. Introduction,[0],[0]
"PRESTO 3RIKEN Brain Science Institute 4Graduate School of Frontier Sciences, The University of Tokyo 5RIKEN AIP 6NIMS.",1. Introduction,[0],[0]
"Correspondence to: Mahito Sugiyama <mahito@nii.ac.jp>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Every ber sums to 1
Given tensor A
Multistochastic tensor A’ Submanifold (β)
",1. Introduction,[0],[0]
"Probability distribution P
Statistical manifold (dually at Riemannian manifold)
",1. Introduction,[0],[0]
"ProjectionTensor balancing
Projected distribution Pβ
Figure 1.",1. Introduction,[0],[0]
"Overview of our approach.
",1. Introduction,[0],[0]
"Knight, 2016), Hi-C data analysis (Rao et al., 2014; Wu & Michor, 2016), the Sudoku puzzle (Moon et al., 2009), and the optimal transportation problem (Cuturi, 2013; Frogner et al., 2015; Solomon et al., 2015).",1. Introduction,[0],[0]
"An excellent review of this theory and its applications is given by Idel (2016).
",1. Introduction,[0],[0]
"The standard matrix balancing algorithm is the SinkhornKnopp algorithm (Sinkhorn, 1964; Sinkhorn & Knopp, 1967; Marshall & Olkin, 1968; Knight, 2008), a special case of Bregman’s balancing method (Lamond & Stewart, 1981) that iterates rescaling of each row and column until convergence.",1. Introduction,[0],[0]
The algorithm is widely used in the above applications due to its simple implementation and theoretically guaranteed convergence.,1. Introduction,[0],[0]
"However, the algorithm converges linearly (Soules, 1991), which is prohibitively slow for recently emerging large and sparse matrices.",1. Introduction,[0],[0]
"Although Livne & Golub (2004) and Knight & Ruiz (2013) tried to achieve faster convergence by approximating each step of Newton’s method, the exact Newton’s method with quadratic convergence has not been intensively studied yet.
",1. Introduction,[0],[0]
"Another open problem is tensor balancing, which is a generalization of balancing from matrices to higher-order multidimentional arrays, or tensors.",1. Introduction,[0],[0]
"The task is to rescale an N th order nonnegative tensor to a multistochastic tensor, in which every fiber sums to one, by multiplying (N −1)th order N tensors.",1. Introduction,[0],[0]
"There are some results about mathematical properties of multistochastic tensors (Cui et al., 2014; Chang et al., 2016; Ahmed et al., 2003).",1. Introduction,[0],[0]
"However, there is no result for tensor balancing algorithms with guaranteed convergence that transforms a given tensor to a multistochastic tensor until now.
",1. Introduction,[0],[0]
Here we show that Newton’s method with quadratic convergence can be applied to tensor balancing while avoiding solving a linear system on the full tensor.,1. Introduction,[0],[0]
"Our strategy is to realize matrix and tensor balancing as projection onto a dually flat Riemmanian submanifold (Figure 1), which is a statistical manifold and known to be the essential structure for probability distributions in information geometry (Amari, 2016).",1. Introduction,[0],[0]
"Using a partially ordered outcome space, we generalize the log-linear model (Agresti, 2012) used to model the higher-order combinations of binary variables (Amari, 2001; Ganmor et al., 2011; Nakahara & Amari, 2002; Nakahara et al., 2003), which allows us to model tensors as probability distributions in the statistical manifold.",1. Introduction,[0],[0]
"The remarkable property of our model is that the gradient of the manifold can be analytically computed using the Möbius inversion formula (Rota, 1964), the heart of combinatorial mathematics (Ito, 1993), which enables us to directly obtain the Jacobian matrix in Newton’s method.",1. Introduction,[0],[0]
"Moreover, we show that (n − 1)N entries for the size nN of a tensor are invariant with respect to one of the two coordinate systems of the statistical manifold.",1. Introduction,[0],[0]
"Thus the number of equations in Newton’s method is O(nN−1).
",1. Introduction,[0],[0]
The remainder of this paper is organized as follows: We begin with a low-level description of our matrix balancing algorithm in Section 2 and demonstrate its efficiency in numerical experiments in Section 3.,1. Introduction,[0],[0]
"To guarantee the correctness of the algorithm and extend it to tensor balancing, we provide theoretical analysis in Section 4.",1. Introduction,[0],[0]
"In Section 4.1, we introduce a generalized log-linear model associated with a partial order structured outcome space, followed by introducing the dually flat Riemannian structure in Section 4.2.",1. Introduction,[0],[0]
"In Section 4.3, we show how to use Newton’s method to compute projection of a probability distribution onto a submanifold.",1. Introduction,[0],[0]
"Finally, we formulate the matrix and tensor balancing problem in Section 5 and summarize our contributions in Section 6.",1. Introduction,[0],[0]
"Given a nonnegative square matrix A = (aij) ∈ Rn×n≥0 , the task of matrix balancing is to find r, s ∈ Rn that satisfy
(RAS)1 = 1, (RAS)T1 = 1, (1)
where R = diag(r) and S = diag(s).",2. The Matrix Balancing Algorithm,[0],[0]
The balanced matrix A′,2. The Matrix Balancing Algorithm,[0],[0]
"= RAS is called doubly stochastic, in which each entry a′ij = aijrisj and all the rows and columns sum to one.",2. The Matrix Balancing Algorithm,[0],[0]
"The most popular algorithm is the Sinkhorn-Knopp algorithm, which repeats updating r and s as r = 1/(As) and s = 1/(ATr).",2. The Matrix Balancing Algorithm,[0],[0]
"We denote by [n] = {1, 2, . . .",2. The Matrix Balancing Algorithm,[0],[0]
", n} hereafter.
",2. The Matrix Balancing Algorithm,[0],[0]
"In our algorithm, instead of directly updating r and s, we update two parameters θ and η defined as
log pij = ∑ i′≤i ∑ j′≤j θi′j′ , ηij = ∑ i′≥i ∑ j′≥j pi′j′ (2)
Matrix Constraints for balancing
for each i, j ∈",2. The Matrix Balancing Algorithm,[0],[0]
"[n], where we normalized entries as pij = aij/ ∑",2. The Matrix Balancing Algorithm,[0],[0]
ij,2. The Matrix Balancing Algorithm,[0],[0]
aij,2. The Matrix Balancing Algorithm,[0],[0]
so that ∑ ij pij = 1.,2. The Matrix Balancing Algorithm,[0],[0]
We assume for simplicity that each entry is strictly larger than zero.,2. The Matrix Balancing Algorithm,[0],[0]
"The assumption will be removed in Section 5.
",2. The Matrix Balancing Algorithm,[0],[0]
"The key to our approach is that we update θ(t)ij with i = 1 or j = 1 by Newton’s method at each iteration t = 1, 2, . . .",2. The Matrix Balancing Algorithm,[0],[0]
"while fixing θij with i, j ̸= 1 so that η(t)ij satisfies the following condition (Figure 2):
η (t) i1 = (n− i+ 1)/n, η (t) 1j =",2. The Matrix Balancing Algorithm,[0],[0]
"(n− j + 1)/n.
Note that the rows and columns sum not to 1 but to 1/n due to the normalization.",2. The Matrix Balancing Algorithm,[0],[0]
"The update formula is described as θ (t+1) 11 ... θ (t+1) 1n θ (t+1) 21
... θ (t+1) n1
 =  θ (t) 11 ...",2. The Matrix Balancing Algorithm,[0],[0]
"θ (t) 1n
θ (t) 21 ...",2. The Matrix Balancing Algorithm,[0],[0]
"θ (t) n1
 − J−1  η (t) 11",2. The Matrix Balancing Algorithm,[0],[0]
− (n− 1 + 1)/n ...,2. The Matrix Balancing Algorithm,[0],[0]
η (t),2. The Matrix Balancing Algorithm,[0],[0]
1n,2. The Matrix Balancing Algorithm,[0],[0]
− (n− n+ 1)/n η (t) 21,2. The Matrix Balancing Algorithm,[0],[0]
"− (n− 2 + 1)/n
... η (t) n1",2. The Matrix Balancing Algorithm,[0],[0]
"− (n− n+ 1)/n
 , (3)
where J is the Jacobian matrix given as
J(ij)(i′j′)= ∂η
(t) ij",2. The Matrix Balancing Algorithm,[0],[0]
"∂θ (t) i′j′ = ηmax{i,i′}max{j,j′}−n2ηijηi′j′ , (4)
which is derived from our theoretical result in Theorem 3.",2. The Matrix Balancing Algorithm,[0],[0]
"Since J is a (2n−1)×(2n−1) matrix, the time complexity of each update is O(n3), which is needed to compute the inverse of J .
",2. The Matrix Balancing Algorithm,[0],[0]
"After updating to θ(t+1)ij , we can compute p (t+1) ij and η (t+1) ij by Equation (2).",2. The Matrix Balancing Algorithm,[0],[0]
"Since this update does not ensure the condition ∑ ij p (t+1) ij = 1, we again update θ (t+1) 11 as θ (t+1) 11 = θ (t+1) 11 − log ∑ ij p (t+1) ij and recompute p (t+1) ij and η(t+1)ij for each i, j ∈",2. The Matrix Balancing Algorithm,[0],[0]
"[n].
By iterating the above update process in Equation (3) until convergence, A = (aij) with aij = npij becomes doubly stochastic.",2. The Matrix Balancing Algorithm,[0],[0]
"We evaluate the efficiency of our algorithm compared to the two prominent balancing methods, the standard SinkhornKnopp algorithm (Sinkhorn, 1964) and the state-of-the-art
algorithm BNEWT (Knight & Ruiz, 2013), which uses Newton’s method-like iterations with conjugate gradients.",3. Numerical Experiments,[0],[0]
All experiments were conducted on Amazon Linux AMI release 2016.09 with a single core of 2.3 GHz Intel Xeon CPU E5-2686 v4 and 256 GB of memory.,3. Numerical Experiments,[0],[0]
All methods were implemented in C++ with the Eigen library and compiled with gcc 4.8.31.,3. Numerical Experiments,[0],[0]
"We have carefully implemented BNEWT by directly translating the MATLAB code provided in (Knight & Ruiz, 2013) into C++ with the Eigen library for fair comparison, and used the default parameters.",3. Numerical Experiments,[0],[0]
"We measured the residual of a matrix A′ = (a′ij) by the squared norm ∥(A′1−1, A′T1−1)∥2, where each entry a′ij is obtained as npij in our algorithm, and ran each of three algorithms until the residual is below the tolerance threshold 10−6.
",3. Numerical Experiments,[0],[0]
Hessenberg Matrix.,3. Numerical Experiments,[0],[0]
"The first set of experiments used a Hessenberg matrix, which has been a standard benchmark for matrix balancing (Parlett & Landis, 1982; Knight & Ruiz, 2013).",3. Numerical Experiments,[0],[0]
Each entry of an n × n Hessenberg matrix,3. Numerical Experiments,[0],[0]
Hn = (hij) is given as hij = 0,3. Numerical Experiments,[0],[0]
if j < i,3. Numerical Experiments,[0],[0]
− 1 and hij = 1 otherwise.,3. Numerical Experiments,[0],[0]
"We varied the size n from 10 to 5, 000, and measured running time (in seconds) and the number of iterations of each method.
",3. Numerical Experiments,[0],[0]
Results are plotted in Figure 3.,3. Numerical Experiments,[0],[0]
"Our balancing algorithm with the Newton’s method (plotted in blue in the figures)
1An implementation of algorithms for matrices and third order tensors is available at: https://github.com/ mahito-sugiyama/newton-balancing
is clearly the fastest: It is three to five orders of magnitude faster than the standard Sinkhorn-Knopp algorithm (plotted in red).",3. Numerical Experiments,[0],[0]
"Although the BNEWT algorithm (plotted in green) is competitive if n is small, it suddenly fails to converge whenever n ≥ 200, which is consistent with results in the original paper (Knight & Ruiz, 2013) where there is no result for the setting n ≥ 200 on the same matrix.",3. Numerical Experiments,[0],[0]
"Moreover, our method converges around 10 to 20 steps, which is about three and seven orders of magnitude smaller than BNEWT and Sinkhorn-Knopp, respectively, at n = 100.
",3. Numerical Experiments,[0],[0]
"To see the behavior of the rate of convergence in detail, we plot the convergence graph in Figure 4 for n = 20, where we observe the slow convergence rate of the SinkhornKnopp algorithm and unstable convergence of the BNEWT algorithm, which contrasts with our quick convergence.
",3. Numerical Experiments,[0],[0]
Trefethen Matrix.,3. Numerical Experiments,[0],[0]
"Next, we collected a set of Trefethen matrices from a collection website2, which are nonnegative diagonal matrices with primes.",3. Numerical Experiments,[0],[0]
"Results are plotted in Figure 5, where we observe the same trend as before:",3. Numerical Experiments,[0],[0]
Our algorithm is the fastest and about four orders of magnitude faster than the Sinkhorn-Knopp algorithm.,3. Numerical Experiments,[0],[0]
"Note that larger matrices with n > 300 do not have total support, which is the necessary condition for matrix balancing (Knight & Ruiz, 2013), while the BNEWT algorithm fails to converge if n = 200 or n = 300.",3. Numerical Experiments,[0],[0]
"In the following, we provide theoretical support to our algorithm by formulating the problem as a projection within a statistical manifold, in which a matrix corresponds to an element, that is, a probability distribution, in the manifold.
",4. Theoretical Analysis,[0],[0]
"We show that a balanced matrix forms a submanifold and matrix balancing is projection of a given distribution onto the submanifold, where the Jacobian matrix in Equation (4) is derived from the gradient of the manifold.
2http://www.cise.ufl.edu/research/sparse/",4. Theoretical Analysis,[0],[0]
matrices/,4. Theoretical Analysis,[0],[0]
"We introduce our log-linear probabilistic model, where the outcome space is a partially ordered set, or a poset (Gierz et al., 2003).",4.1. Formulation,[0],[0]
"We prepare basic notations and the key mathematical tool for posets, the Möbius inversion formula, followed by formulating the log-linear model.",4.1. Formulation,[0],[0]
"A poset (S,≤), the set of elements S and a partial order ≤ on S, is a fundamental structured space in computer science.",4.1.1. MÖBIUS INVERSION,[0],[0]
"A partial order “≤” is a relation between elements in S that satisfies the following three properties: For all x, y, z ∈ S, (1) x ≤ x (reflexivity), (2) x ≤ y, y ≤ x ⇒",4.1.1. MÖBIUS INVERSION,[0],[0]
"x = y (antisymmetry), and (3) x",4.1.1. MÖBIUS INVERSION,[0],[0]
"≤ y, y ≤ z ⇒ x ≤ z",4.1.1. MÖBIUS INVERSION,[0],[0]
(transitivity).,4.1.1. MÖBIUS INVERSION,[0],[0]
"In what follows, S is always finite and includes the least element (bottom) ⊥ ∈ S; that is, ⊥ ≤ x for all x ∈ S. We denote S \ {⊥} by S+.
",4.1.1. MÖBIUS INVERSION,[0],[0]
Rota (1964) introduced the Möbius inversion formula on posets by generalizing the inclusion-exclusion principle.,4.1.1. MÖBIUS INVERSION,[0],[0]
"Let ζ :S × S → {0, 1} be the zeta function defined as
ζ(s, x) = { 1 if s ≤ x, 0 otherwise.
",4.1.1. MÖBIUS INVERSION,[0],[0]
"The Möbius function µ :S×S → Z satisfies ζµ = I , which is inductively defined for all x, y with x ≤ y as
µ(x, y) =  1 if x = y, − ∑
x≤s<y µ(x, s) if x < y, 0 otherwise.
",4.1.1. MÖBIUS INVERSION,[0],[0]
"From the definition, it follows that∑ s∈S ζ(s, y)µ(x, s) = ∑ x≤s≤y
µ(x, s) = δxy,∑ s∈S ζ(x, s)µ(s, y) = ∑ x≤s≤y µ(s, y) = δxy (5)
with the Kronecker delta δ such that δxy = 1 if x = y and δxy = 0 otherwise.",4.1.1. MÖBIUS INVERSION,[0],[0]
"Then for any functions f , g, and h with the domain S such that
g(x) = ∑ s∈S ζ(s, x)f(s) =",4.1.1. MÖBIUS INVERSION,[0],[0]
"∑ s≤x f(s),
h(x) = ∑ s∈S ζ(x, s)f(s) = ∑ s≥x f(s)",4.1.1. MÖBIUS INVERSION,[0],[0]
",
f is uniquely recovered with the Möbius function: f(x) = ∑ s∈S µ(s, x)g(s), f(x) = ∑ s∈S µ(x, s)h(s).
",4.1.1. MÖBIUS INVERSION,[0],[0]
"This is called the Möbius inversion formula and is at the heart of enumerative combinatorics (Ito, 1993).",4.1.1. MÖBIUS INVERSION,[0],[0]
"We consider a probability vector p on (S,≤) that gives a discrete probability distribution with the outcome space S.
A probability vector is treated as a mapping p :S → (0, 1) such that ∑ x∈S p(x) = 1, where every entry p(x) is assumed to be strictly larger than zero.
",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"Using the zeta and the Möbius functions, let us introduce two mappings θ :S → R and η :S → R as
θ(x)",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"= ∑ s∈S µ(s, x) log p(s), (6)
η(x) = ∑ s∈S ζ(x, s)p(s) = ∑ s≥x p(s).",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"(7)
From the Möbius inversion formula, we have log p(x) = ∑ s∈S ζ(s, x)θ(s) = ∑ s≤x θ(s), (8)
p(x) = ∑ s∈S µ(x, s)η(s).",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"(9)
They are generalization of the log-linear model (Agresti, 2012) that gives the probability p(x) of an n-dimensional binary vector x = (x1, . .",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
.,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
", xn) ∈ {0, 1}n as
log p(x) = ∑",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
i θixi,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"+ ∑ i<j θijxixj + ∑ i<j<k θijkxixjxk
+ · · ·+ θ1...",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
nx1x2 . . .,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
xn,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"− ψ,
where θ = (θ1, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
", θ12...n) is a parameter vector, ψ is a normalizer, and η = (η1, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
", η12...n) represents the expectation of variable combinations such that
ηi = E[xi] = Pr(xi = 1),
ηij = E[xixj ] = Pr(xi = xj = 1), i < j, . .",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
".
",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
η1...n = E[x1 . . .,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"xn] = Pr(x1 = · · · = xn = 1).
",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"They coincide with Equations (8) and (7) when we let S = 2V with V = {1, 2, . . .",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
",",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"n},",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"each x ∈ S as the set of indices of “1” of x, and the order ≤ as the inclusion relationship, that is, x ≤ y",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
if and only if x ⊆ y. Nakahara et al. (2006) have pointed out that θ can be computed from p using the inclusion-exclusion principle in the log-linear model.,4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"We exploit this combinatorial property of the loglinear model using the Möbius inversion formula on posets and extend the log-linear model from the power set 2V to any kind of posets (S,≤).",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"Sugiyama et al. (2016) studied a relevant log-linear model, but the relationship with Möbius inversion formula has not been analyzed yet.",4.1.2. LOG-LINEAR MODEL ON POSETS,[0],[0]
"We theoretically analyze our log-linear model introduced in Equations (6), (7) and show that they form dual coordinate systems on a dually flat manifold, which has been mainly studied in the area of information geometry (Amari, 2001; Nakahara & Amari, 2002; Amari, 2014; 2016).",4.2. Dually Flat Riemannian Manifold,[0],[0]
"Moreover, we show that the Riemannian metric and connection of our model can be analytically computed in closed forms.
",4.2. Dually Flat Riemannian Manifold,[0],[0]
"In the following, we denote by ξ the function θ or η and by ∇ the gradient operator with respect to S+ = S \{⊥}, i.e., (∇f(ξ))(x) = ∂f/∂ξ(x) for x ∈ S+, and denote by S the set of probability distributions specified by probability vectors, which forms a statistical manifold.",4.2. Dually Flat Riemannian Manifold,[0],[0]
"We use uppercase letters P,Q,R, . . .",4.2. Dually Flat Riemannian Manifold,[0],[0]
"for points (distributions) in S and their lowercase letters p, q, r, . . .",4.2. Dually Flat Riemannian Manifold,[0],[0]
for the corresponding probability vectors treated as mappings.,4.2. Dually Flat Riemannian Manifold,[0],[0]
"We write θP and ηP if they are connected with p by Equations (6) and (7), respectively, and abbreviate subscripts if there is no ambiguity.",4.2. Dually Flat Riemannian Manifold,[0],[0]
We show that S has the dually flat Riemannian structure induced by two functions θ and η in Equation (6) and (7).,4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"We define ψ(θ) as
ψ(θ) = −θ(⊥) =",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"− log p(⊥), (10)
which corresponds to the normalizer of p.",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"It is a convex function since we have
ψ(θ) = log ∑ x∈S exp  ∑ ⊥<s≤x θ(s)  from log p(x) = ∑ ⊥<s≤x θ(s)−ψ(θ).",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"We apply the Legendre transformation to ψ(θ) given as
φ(η) = max θ′
( θ′η − ψ(θ′) ) , θ′η = ∑ x∈S+ θ′(x)η(x).",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"(11)
Then φ(η) coincides with the negative entropy.
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"Theorem 1 (Legendre dual). φ(η) = ∑ x∈S p(x) log p(x).
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
Proof.,4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"From Equation (5), we have
θ′η = ∑ x∈S+  ∑ ⊥<s≤x µ(s, x) log p′(s) ∑ s≥x p(s)  = ∑ x∈S+ p(x) ( log p′(x)− log p′(⊥) ) .
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
Thus it holds that θ′η − ψ(θ′) = ∑ x∈S p(x) log p′(x).,4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"(12)
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"Hence it is maximized with p(x) = p′(x).
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"Since they are connected with each other by the Legendre transformation, they form a dual coordinate system ∇ψ(θ) and ∇φ(η) of S (Amari, 2016, Section 1.5), which coincides with θ and η as follows.
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"Theorem 2 (dual coordinate system).
∇ψ(θ) = η, ∇φ(η) = θ. (13)
Proof.",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"They can be directly derived from our definitions (Equations (6) and (11)) as
∂ψ(θ) ∂θ(x) =
∑ y≥x exp (∑ ⊥<s≤y θ(s) )",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"∑
y∈S exp (∑ ⊥<s≤y θ(s) )",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"=∑ s≥x p(s) = η(x),
∂φ(η) ∂η(x) = ∂ ∂η(x)
( θη − ψ(θ) )",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"= θ(x).
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"Moreover, we can confirm the orthogonality of θ and η as
E
[ ∂ log p(s)
∂θ(x)
∂ log p(s)
∂η(y) ]",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"= ∑ s∈S ζ(x, s)µ(s, y) = δxy.
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"The last equation holds from Equation (5), hence the Möbius inversion directly leads to the orthogonality.
",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"The Bregman divergence is known to be the canonical divergence (Amari, 2016, Section 6.6) to measure the difference between two distributions P and Q on a dually flat manifold, which is defined as
D",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"[P,Q] = ψ(θP )",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
+ φ(ηQ)− θP ηQ.,4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"In our case, since we have φ(ηQ) = ∑
x∈S q(x) log q(x) and θP ηQ−ψ(θP ) = ∑ x∈S q(x) log p(x) from Theorem 1 and Equation (12), it is given as
D [P,Q] = ∑ x∈S q(x) log q(x) p(x) ,
which coincides with the Kullback–Leibler divergence (KL divergence) from Q to P :",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
"D [P,Q] = DKL [Q,P ].",4.2.1. DUALLY FLAT STRUCTURE,[0],[0]
Next we analyze the Riemannian structure on S and show that the Möbius inversion formula enables us to compute the Riemannian metric of S. Theorem 3 (Riemannian metric).,4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"The manifold (S, g(ξ)) is a Riemannian manifold with the Riemannian metric g(ξ) such that for all x, y ∈ S+
gxy(ξ) =  ∑ s∈S [ ζ(x, s)ζ(y, s)p(s)− η(x)η(y) ] if ξ = θ,
∑ s∈S µ(s, x)µ(s, y)p(s)−1 if ξ = η.
Proof.",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"Since the Riemannian metric is defined as
g(θ) = ∇∇ψ(θ), g(η) = ∇∇φ(η),
when ξ = θ",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"we have
gxy(θ) = ∂2
∂θ(x)∂θ(y) ψ(θ) =
∂
∂θ(x) η(y)
= ∂
∂θ(x) ∑ s∈S ζ(y, s) exp  ∑ ⊥<u≤s θ(u)− ψ(θ) 
= ∑ s∈S ζ(x, s)ζ(y, s)p(s)− |S|η(x)η(y).
",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"When ξ = η, it follows that
gxy(η) = ∂2
∂η(x)∂η(y) φ(η) =
∂
∂η(x) θ(y)
= ∂
∂η(x) ∑ s≤y µ(s, y) log ∑ u≥s µ(s, u)η(u)  = ∑ s∈S µ(s, x)µ(s, y)p(s)−1.
",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"Since g(ξ) coincides with the Fisher information matrix,
E
[ ∂
∂θ(x) log p(s)
∂
∂θ(y) log p(s)
] = gxy(θ),
E
[ ∂
∂η(x) log p(s)
∂
∂η(y) log p(s)
] = gxy(η).
",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"Then the Riemannian (Levi–Chivita) connection Γ(ξ) with respect to ξ, which is defined as
Γxyz(ξ) = 1
2
( ∂gyz(ξ)
∂ξ(x) + ∂gxz(ξ)",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
∂ξ(y) − ∂gxy(ξ) ∂ξ(z) ),4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"for all x, y, z ∈ S+, can be analytically obtained.",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
Theorem 4 (Riemannian connection).,4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"The Riemannian connection Γ(ξ) on the manifold (S, g(ξ)) is given in the following for all x, y, z ∈ S+,
Γxyz(ξ) =  1 2 ∑ s∈S ( ζ(x, s)− η(x) )",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"( ζ(y, s)− η(y) )",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"( ζ(z, s)− η(z) )",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
p(s),4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"if ξ = θ, −1 2 ∑ s∈S µ(s, x)µ(s, y)µ(s, z)p(s)−2 if ξ = η.
Proof.",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"Connections Γxyz(θ) and Γxyz(η) can be obtained by directly computing ∂gyz(θ)/∂θ(x) and ∂gyz(η)/∂η(x), respectively.",4.2.2. RIEMANNIAN STRUCTURE,[0],[0]
"Projection of a distribution onto a submanifold is essential; several machine learning algorithms are known to be formulated as projection of a distribution empirically estimated from data onto a submanifold that is specified by the target model (Amari, 2016).",4.3. The Projection Algorithm,[0],[0]
Here we define projection of distributions on posets and show that Newton’s method can be applied to perform projection as the Jacobian matrix can be analytically computed.,4.3. The Projection Algorithm,[0],[0]
"Let S(β) be a submanifold of S such that
S(β) = {P ∈ S | θP (x) = β(x), ∀x ∈ dom(β)} (14)
specified by a function β with dom(β) ⊆ S+.",4.3.1. DEFINITION,[0],[0]
"Projection of P ∈ S onto S(β), calledm-projection, which is defined as the distribution Pβ ∈ S(β) such that{
θPβ (x) = β(x) if x ∈ dom(β), ηPβ (x) = ηP (x)",4.3.1. DEFINITION,[0],[0]
"if x ∈ S+ \ dom(β),
is the minimizer of the KL divergence from P to S(β):
Pβ = argmin Q∈S(β) DKL[P,Q].
",4.3.1. DEFINITION,[0],[0]
"The dually flat structure with the coordinate systems θ and η guarantees that the projected distribution Pβ always exists and is unique (Amari, 2009, Theorem 3).",4.3.1. DEFINITION,[0],[0]
"Moreover, the Pythagorean theorem holds in the dually flat manifold, that is, for any Q ∈ S(β)",4.3.1. DEFINITION,[0],[0]
"we have
DKL[P,Q] = DKL[P, Pβ ] +DKL[Pβ , Q].
",4.3.1. DEFINITION,[0],[0]
"We can switch η and θ in the submanifold S(β) by changing DKL[P,Q] to DKL[Q,P ], where the projected distribution Pβ of P is given as{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",4.3.1. DEFINITION,[0],[0]
"if x ∈ dom(β),
This projection is called e-projection.
",4.3.1. DEFINITION,[0],[0]
Example 1 (Boltzmann machine).,4.3.1. DEFINITION,[0],[0]
"Given a Boltzmann machine represented as an undirected graph G = (V,E) with a vertex set V and an edge set E ⊆ {{i, j} | i, j ∈ V }.",4.3.1. DEFINITION,[0],[0]
"The set of probability distributions that can be modeled by a Boltzmann machine G coincides with the submanifold
SB = {P ∈ S | θP (x) = 0",4.3.1. DEFINITION,[0],[0]
"if |x| > 2 or x ̸∈ E},
with S = 2V .",4.3.1. DEFINITION,[0],[0]
Let P̂ be an empirical distribution estimated from a given dataset.,4.3.1. DEFINITION,[0],[0]
"The learned model is the mprojection of the empirical distribution P̂ onto SB, where the resulting distribution Pβ is given as{
θPβ (x) = 0",4.3.1. DEFINITION,[0],[0]
"if |x| > 2 or x ̸∈ E, ηPβ (x) = ηP̂ (x) if |x| = 1 or x ∈ E.",4.3.1. DEFINITION,[0],[0]
Here we show how to compute projection of a given probability distribution.,4.3.2. COMPUTATION,[0],[0]
"We show that Newton’s method can be used to efficiently compute the projected distribution Pβ by iteratively updating P (0)β = P as P (0) β , P (1) β , P (2) β , . . .",4.3.2. COMPUTATION,[0],[0]
"until converging to Pβ .
Let us start with the m-projection with initializing P (0)β = P .",4.3.2. COMPUTATION,[0],[0]
"In each iteration t, we update θ(t)Pβ (x) for all x ∈ domβ while fixing η(t)Pβ (x) = ηP (x) for all x ∈ S
+ \ dom(β), which is possible from the orthogonality of θ and η.",4.3.2. COMPUTATION,[0],[0]
"Using Newton’s method, η(t+1)Pβ (x) should satisfy( θ (t)",4.3.2. COMPUTATION,[0],[0]
Pβ (x)− β(x) ),4.3.2. COMPUTATION,[0],[0]
"+ ∑
y∈dom(β)
",4.3.2. COMPUTATION,[0],[0]
Jxy ( η (t+1),4.3.2. COMPUTATION,[0],[0]
Pβ (y)− η(t)Pβ (y) ),4.3.2. COMPUTATION,[0],[0]
"= 0,
for every x ∈ dom(β), where Jxy is an entry of the |dom(β)| × |dom(β)| Jacobian matrix J and given as
Jxy = ∂θ
(t)",4.3.2. COMPUTATION,[0],[0]
"Pβ (x)
∂η",4.3.2. COMPUTATION,[0],[0]
(t),4.3.2. COMPUTATION,[0],[0]
"Pβ (y) = ∑ s∈S µ(s, x)µ(s, y)p (t) β (s) −1
from Theorem 3.",4.3.2. COMPUTATION,[0],[0]
"Therefore, we have the update formula for all x ∈ dom(β) as
η (t+1)",4.3.2. COMPUTATION,[0],[0]
Pβ (x) = η (t),4.3.2. COMPUTATION,[0],[0]
"Pβ
(x)− ∑
y∈dom(β)
J−1xy ( θ (t)",4.3.2. COMPUTATION,[0],[0]
"Pβ (y)− β(y) ) .
",4.3.2. COMPUTATION,[0],[0]
"In e-projection, update η(t)Pβ (x) for x ∈ dom(β) while fixing θ(t)Pβ (x) = θP (x) for all x ∈ S
+ \ dom(β).",4.3.2. COMPUTATION,[0],[0]
To ensure η (t),4.3.2. COMPUTATION,[0],[0]
"Pβ
(⊥) = 1, we add ⊥ to dom(β) and β(⊥) = 1.",4.3.2. COMPUTATION,[0],[0]
"We update θ(t)Pβ (x) at each step t as
θ (t+1)",4.3.2. COMPUTATION,[0],[0]
Pβ (x) = θ (t),4.3.2. COMPUTATION,[0],[0]
"Pβ
(x)− ∑
y∈dom(β)
",4.3.2. COMPUTATION,[0],[0]
J ′ −1,4.3.2. COMPUTATION,[0],[0]
xy ( η (t),4.3.2. COMPUTATION,[0],[0]
"Pβ (y)− β(y) ) ,
J ′xy = ∂η
(t)",4.3.2. COMPUTATION,[0],[0]
"Pβ (x)
∂θ (t)",4.3.2. COMPUTATION,[0],[0]
"Pβ (y) = ∑ s∈S ζ(x, s)ζ(y, s)p (t) β (s)
",4.3.2. COMPUTATION,[0],[0]
− |S|η(t)Pβ (x)η (t),4.3.2. COMPUTATION,[0],[0]
"Pβ (y).
",4.3.2. COMPUTATION,[0],[0]
"In this case, we also need to update θ(t)Pβ (⊥) as it is not guaranteed to be fixed.",4.3.2. COMPUTATION,[0],[0]
"Let us define
p ′(t+1) β",4.3.2. COMPUTATION,[0],[0]
(x) = p (t),4.3.2. COMPUTATION,[0],[0]
"β (x) ∏ s∈dom(β)
exp ( θ (t+1)",4.3.2. COMPUTATION,[0],[0]
"Pβ (s) )
exp ( θ (t) Pβ (s) ) ζ(s, x).
",4.3.2. COMPUTATION,[0],[0]
"Since we have
p (t+1) β (x) =
exp ( θ (t+1)",4.3.2. COMPUTATION,[0],[0]
"Pβ (⊥) )
exp ( θ (t)",4.3.2. COMPUTATION,[0],[0]
Pβ (⊥) ),4.3.2. COMPUTATION,[0],[0]
"p′(t+1)β (x),
it follows that
θ (t+1)",4.3.2. COMPUTATION,[0],[0]
"Pβ (⊥)− θ(t)Pβ (⊥)
=",4.3.2. COMPUTATION,[0],[0]
− log ( exp ( θ (t),4.3.2. COMPUTATION,[0],[0]
Pβ (⊥) ),4.3.2. COMPUTATION,[0],[0]
+ ∑ x∈S+ p ′(t+1) β,4.3.2. COMPUTATION,[0],[0]
"(x) ) ,
The time complexity of each iteration is O(|dom(β)|3), which is required to compute the inverse of the Jacobian matrix.
",4.3.2. COMPUTATION,[0],[0]
Global convergence of the projection algorithm is always guaranteed by the convexity of a submanifold S(β) defined in Equation (14).,4.3.2. COMPUTATION,[0],[0]
"Since S(β) is always convex with respect to the θ- and η-coordinates, it is straightforward to see that our e-projection is an instance of the Bregman algorithm onto a convex region, which is well known to always converge to the global solution (Censor & Lent, 1981).",4.3.2. COMPUTATION,[0],[0]
Now we are ready to solve the problem of matrix and tensor balancing as projection on a dually flat manifold.,5. Balancing Matrices and Tensors,[0],[0]
"Recall that the task of matrix balancing is to find r, s ∈",5.1. Matrix Balancing,[0],[0]
"Rn that satisfy (RAS)1 = 1 and (RAS)T1 = 1 with R = diag(r) and S = diag(s) for a given nonnegative square matrix A = (aij) ∈ Rn×n≥0 .
",5.1. Matrix Balancing,[0],[0]
"Let us define S as S = {(i, j) |",5.1. Matrix Balancing,[0],[0]
"i, j ∈",5.1. Matrix Balancing,[0],[0]
"[n] and aij ̸= 0}, (15)
where we remove zero entries from the outcome space S as our formulation cannot treat zero probability, and give each probability as p((i, j))",5.1. Matrix Balancing,[0],[0]
= aij/ ∑ ij aij .,5.1. Matrix Balancing,[0],[0]
"The partial order ≤ of S is naturally introduced as x = (i, j) ≤ y",5.1. Matrix Balancing,[0],[0]
=,5.1. Matrix Balancing,[0],[0]
"(k, l) ⇔ i ≤ j and k ≤",5.1. Matrix Balancing,[0],[0]
"l, (16)
resulting in ⊥ = (1, 1).",5.1. Matrix Balancing,[0],[0]
"In addition, we define ιk,m for each k ∈",5.1. Matrix Balancing,[0],[0]
"[n] and m ∈ {1, 2}",5.1. Matrix Balancing,[0],[0]
"such that
ιk,m = min{x =",5.1. Matrix Balancing,[0],[0]
"(i1, i2) ∈ S | im = k }, where the minimum is with respect to the order ≤.",5.1. Matrix Balancing,[0],[0]
"If ιk,m does not exist, we just remove the entire kth row if m = 1 or kth column if m = 2 from A.",5.1. Matrix Balancing,[0],[0]
"Then we switch rows and columns of A so that the condition
ι1,m ≤ ι2,m ≤ · · · ≤ ιn,m (17) is satisfied for each m ∈ {1, 2}, which is possible for any matrices.",5.1. Matrix Balancing,[0],[0]
"Since we have
η(ιk,m)− η(ιk+1,m) = { ∑n j=1 p((k, j)) if m = 1,∑n i=1",5.1. Matrix Balancing,[0],[0]
"p((i, k))",5.1. Matrix Balancing,[0],[0]
"if m = 2
if the condition (17) is satisfied, the probability distribution is balanced if for all",5.1. Matrix Balancing,[0],[0]
k ∈,5.1. Matrix Balancing,[0],[0]
[n],5.1. Matrix Balancing,[0],[0]
"and m ∈ {1, 2}
η(ιk,m) = n−k+1
n .
",5.1. Matrix Balancing,[0],[0]
"Therefore, we obtain the following result.
",5.1. Matrix Balancing,[0],[0]
Matrix balancing as e-projection:,5.1. Matrix Balancing,[0],[0]
"Given a matrix A ∈ Rn×n with its normalized probability distribution P ∈ S such that p((i, j))",5.1. Matrix Balancing,[0],[0]
= aij/ ∑ ij aij .,5.1. Matrix Balancing,[0],[0]
"Define the poset (S,≤) by Equations (15) and (16) and let S(β) be the submanifold of S such that S(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},
where the function β is given as dom(β)",5.1. Matrix Balancing,[0],[0]
"= {ιk,m ∈ S | k ∈",5.1. Matrix Balancing,[0],[0]
"[n],m ∈ {1, 2}},
β(ιk,m) = n−k+1
n .
",5.1. Matrix Balancing,[0],[0]
"Matrix balancing is the e-projection of P onto the submanifold S(β), that is, the balanced matrix (RAS)/n is the distribution Pβ such that{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",5.1. Matrix Balancing,[0],[0]
"if x ∈ dom(β),
which is unique and always exists in S, thanks to its dually flat structure.",5.1. Matrix Balancing,[0],[0]
"Moreover, two balancing vectors r and s are
exp
( i∑
k=1
θPβ (ιk,m)− θP (ιk,m)
)",5.1. Matrix Balancing,[0],[0]
"= { ri if m = 1, ai if m = 2,
for every i ∈",5.1. Matrix Balancing,[0],[0]
"[n] and r = rn/ ∑
ij aij .",5.1. Matrix Balancing,[0],[0]
■,5.1. Matrix Balancing,[0],[0]
"Next, we generalize our approach from matrices to tensors.",5.2. Tensor Balancing,[0],[0]
"For anN th order tensorA = (ai1i2...iN ) ∈ Rn1×n2×···×nN and a vector b ∈ Rnm , the m-mode product of A and b is defined as
(A×m b)i1...im−1im+1...iN = nm∑
im=1
ai1i2...iN bim .
",5.2. Tensor Balancing,[0],[0]
We define tensor balancing as follows:,5.2. Tensor Balancing,[0],[0]
"Given a tensor A ∈ Rn1×n2×···×nN with n1 = · · · = nN = n, find (N − 1) order tensors R1, R2, . . .",5.2. Tensor Balancing,[0],[0]
", RN such that
A′ ×m 1 = 1 (∈ Rn1×···×nm−1×nm+1×···×nN ) (18) for all m ∈",5.2. Tensor Balancing,[0],[0]
"[N ], i.e., ∑n
im=1",5.2. Tensor Balancing,[0],[0]
"a′i1i2...iN = 1, where each
entry a′i1i2...iN of the balanced tensor A ′ is given as a′i1i2...iN = ai1i2...iN ∏
m∈[N ]
Rmi1...im−1im+1...iN .
",5.2. Tensor Balancing,[0],[0]
"A tensor A′ that satisfies Equation (18) is called multistochastic (Cui et al., 2014).",5.2. Tensor Balancing,[0],[0]
"Note that this is exactly the same as the matrix balancing problem if N = 2.
",5.2. Tensor Balancing,[0],[0]
It is straightforward to extend matrix balancing to tensor balancing as e-projection onto a submanifold.,5.2. Tensor Balancing,[0],[0]
"Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P such that
p(x) = ai1i2...iN / ∑ j1j2...jN aj1j2...jN (19)
for all x = (i1, i2, . . .",5.2. Tensor Balancing,[0],[0]
", iN ).",5.2. Tensor Balancing,[0],[0]
"The objective is to obtain Pβ such that ∑n im=1 pβ((i1, . . .",5.2. Tensor Balancing,[0],[0]
", iN ))",5.2. Tensor Balancing,[0],[0]
=,5.2. Tensor Balancing,[0],[0]
1/(n N−1) for all m ∈,5.2. Tensor Balancing,[0],[0]
"[N ] and i1, . . .",5.2. Tensor Balancing,[0],[0]
", iN ∈",5.2. Tensor Balancing,[0],[0]
[n].,5.2. Tensor Balancing,[0],[0]
"In the same way as matrix balancing, we define S as
S = { (i1, i2, . . .",5.2. Tensor Balancing,[0],[0]
", iN ) ∈",5.2. Tensor Balancing,[0],[0]
"[n]N ∣∣ ai1i2...iN ̸= 0 } with removing zero entries and the partial order ≤ as
x = (i1 . . .",5.2. Tensor Balancing,[0],[0]
iN ),5.2. Tensor Balancing,[0],[0]
≤ y = (j1 . . .,5.2. Tensor Balancing,[0],[0]
jN ) ⇔ ∀m ∈,5.2. Tensor Balancing,[0],[0]
"[N ], im ≤ jm.
",5.2. Tensor Balancing,[0],[0]
"In addition, we introduce ιk,m as
ιk,m = min{x =",5.2. Tensor Balancing,[0],[0]
"(i1, i2, . . .",5.2. Tensor Balancing,[0],[0]
", iN ) ∈ S",5.2. Tensor Balancing,[0],[0]
"| im = k }.
and require the condition in Equation (17).
",5.2. Tensor Balancing,[0],[0]
Tensor balancing as e-projection:,5.2. Tensor Balancing,[0],[0]
Given a tensor A ∈ Rn1×n2×···×nN with its normalized probability distribution P ∈ S given in Equation (19).,5.2. Tensor Balancing,[0],[0]
"The submanifold S(β) of multistochastic tensors is given as
S(β) = {P ∈ S | ηP (x) = β(x) for all x ∈ dom(β)},
where the domain of the function β is given as
dom(β)",5.2. Tensor Balancing,[0],[0]
"= { ιk,m | k ∈",5.2. Tensor Balancing,[0],[0]
"[n],m ∈",5.2. Tensor Balancing,[0],[0]
"[N ] }
and each value is described using the zeta function as β(ιk,m) = ∑ l∈[n] ζ(ιk,m, ιl,m) 1 nN−1 .
",5.2. Tensor Balancing,[0],[0]
"Tensor balancing is the e-projection of P onto the submanifold S(β), that is, the multistochastic tensor is the distribution",5.2. Tensor Balancing,[0],[0]
"Pβ such that{
θPβ (x) = θP (x) if x ∈ S+ \ dom(β), ηPβ (x) = β(x)",5.2. Tensor Balancing,[0],[0]
"if x ∈ dom(β),
which is unique and always exists in S, thanks to its dually flat structure.",5.2. Tensor Balancing,[0],[0]
"Moreover, each balancing tensor Rm is
Rmi1...",5.2. Tensor Balancing,[0],[0]
"im−1im+1...iN
= exp  ∑ m′",5.2. Tensor Balancing,[0],[0]
̸=m im′∑,5.2. Tensor Balancing,[0],[0]
"k=1 θPβ (ιk,m′)− θP (ιk,m′)  for every m ∈",5.2. Tensor Balancing,[0],[0]
"[N ] and R1 = R1nN−1/ ∑ j1...jN
aj1...jN to recover a multistochastic tensor.",5.2. Tensor Balancing,[0],[0]
■ Our result means that the e-projection algorithm based on Newton’s method proposed in Section 4.3 converges to the unique balanced tensor whenever S(β) ̸= ∅ holds.,5.2. Tensor Balancing,[0],[0]
"In this paper, we have solved the open problem of tensor balancing and presented an efficient balancing algorithm using Newton’s method.",6. Conclusion,[0],[0]
"Our algorithm quadratically converges, while the popular Sinkhorn-Knopp algorithm linearly converges.",6. Conclusion,[0],[0]
"We have examined the efficiency of our algorithm in numerical experiments on matrix balancing and showed that the proposed algorithm is several orders of magnitude faster than the existing approaches.
",6. Conclusion,[0],[0]
"We have analyzed theories behind the algorithm, and proved that balancing is e-projection in a special type of a statistical manifold, in particular, a dually flat Riemannian manifold studied in information geometry.",6. Conclusion,[0],[0]
"Our key finding is that the gradient of the manifold, equivalent to Riemannian metric or the Fisher information matrix, can be analytically obtained using the Möbius inversion formula.
",6. Conclusion,[0],[0]
Our information geometric formulation can model several machine learning applications such as statistical analysis on a DAG structure.,6. Conclusion,[0],[0]
"Thus, we can perform efficient learning as projection using information of the gradient of manifolds by reformulating such models, which we will study in future work.",6. Conclusion,[0],[0]
The authors sincerely thank Marco Cuturi for his valuable comments.,Acknowledgements,[0],[0]
"This work was supported by JSPS KAKENHI Grant Numbers JP16K16115, JP16H02870 (MS), JP26120732 and JP16H06570 (HN).",Acknowledgements,[0],[0]
"The research of K.T. was supported by JST CREST JPMJCR1502, RIKEN PostK, KAKENHI Nanostructure and KAKENHI JP15H05711.",Acknowledgements,[0],[0]
"We solve tensor balancing, rescaling an N th order nonnegative tensor by multiplying N tensors of order N −1",abstractText,[0],[0]
so that every fiber sums to one.,abstractText,[0],[0]
This generalizes a fundamental process of matrix balancing used to compare matrices in a wide range of applications from biology to economics.,abstractText,[0],[0]
We present an efficient balancing algorithm with quadratic convergence using Newton’s method and show in numerical experiments that the proposed algorithm is several orders of magnitude faster than existing ones.,abstractText,[0],[0]
"To theoretically prove the correctness of the algorithm, we model tensors as probability distributions in a statistical manifold and realize tensor balancing as projection onto a submanifold.",abstractText,[0],[0]
"The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton’s method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics.",abstractText,[0],[0]
"Our model is not limited to tensor balancing, but has a wide applicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.",abstractText,[0],[0]
Tensor Balancing on Statistical Manifold,title,[0],[0]
Probabilistic graphical models provide a general framework to conveniently build probabilistic models in a modular and compact way.,1. Introduction,[0],[0]
"They are commonly used in statistics, computer vision, natural language processing, machine learning and many related fields (Wainwright & Jordan, 2008).",1. Introduction,[0],[0]
The success of graphical models depends largely on the availability of efficient inference algorithms.,1. Introduction,[0],[0]
"Unfortunately, exact inference is intractable in general, making approximate inference an important research topic.
",1. Introduction,[0],[0]
Approximate inference algorithms generally adopt a variational approach or a sampling approach.,1. Introduction,[0],[0]
"The variational approach formulates the inference problem as an optimi-
1Australian National University, Canberra, Australia.",1. Introduction,[0],[0]
"2National University of Singapore, Singapore.",1. Introduction,[0],[0]
"3Queensland University of Technology, Brisbane, Australia.",1. Introduction,[0],[0]
"Correspondence to: Andrew Wrigley <andrew.wrigley@anu.edu.au>, Wee Sun Lee <leews@comp.nus.edu.sg>, Nan Ye <n.ye@qut.edu.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
sation problem and constructs approximations by solving relaxations of the optimisation problem.,1. Introduction,[0],[0]
"A number of wellknown inference algorithms can be seen as variational algorithms, such as loopy belief propagation, mean-field variational inference, and generalized belief propagation (Wainwright & Jordan, 2008).",1. Introduction,[0],[0]
The sampling approach uses sampling to approximate either the underlying distribution or key quantities of interest.,1. Introduction,[0],[0]
"Commonly used sampling methods include particle filters and Markov-chain Monte Carlo (MCMC) algorithms (Andrieu et al., 2003).
",1. Introduction,[0],[0]
"Our proposed algorithm, tensor belief propagation (TBP), can be seen as a sampling-based algorithm.",1. Introduction,[0],[0]
"Unlike particle filters or MCMC methods, which sample states (also known as particles), TBP samples functions in the form of rank-1 tensors.",1. Introduction,[0],[0]
"Specifically, we use a data structure commonly used in exact inference, the junction tree, and perform approximate message passing on the junction tree using messages represented as mixtures of rank-1 tensors.
",1. Introduction,[0],[0]
We assume that each factor in the graphical model is originally represented as a tensor decomposition (mixture of rank-1 tensors).,1. Introduction,[0],[0]
"Under this assumption, all messages and intermediate factors also have the same representation.",1. Introduction,[0],[0]
"Our key observation is that marginalisation can be performed efficiently for mixtures of rank-1 tensors, and multiplication can be approximated by sampling.",1. Introduction,[0],[0]
"This leads to an approximate message passing algorithm where messages and intermediate factors are approximated by low-rank tensors.
",1. Introduction,[0],[0]
"We provide analysis, giving conditions under which the method performs well.",1. Introduction,[0],[0]
"We compare TBP experimentally with several existing approximate inference methods using Ising models, random MRFs and two real-world datasets, demonstrating promising results.",1. Introduction,[0],[0]
"Exact inference on tree-structured graphical models can be performed efficiently using belief propagation (Pearl, 1982), a dynamic programming algorithm that involves passing messages between nodes containing the results of intermediate computations.",2. Related Work,[0],[0]
"For arbitrary graphical models, the well-known junction tree algorithm (Shafer & Shenoy, 1990; Lauritzen & Spiegelhalter, 1988; Jensen et al., 1990) is commonly used.",2. Related Work,[0],[0]
"The model is first compiled into a junc-
tion tree data structure and a similar message passing algorithm is then run over the junction tree.",2. Related Work,[0],[0]
"Unfortunately, for non-tree models the time and space complexity of the junction tree algorithm grows exponentially with a property of the graph called its treewidth.",2. Related Work,[0],[0]
"For high-treewidth graphical models, exact inference is intractable in general.
",2. Related Work,[0],[0]
Our work approximates the messages passed in the junction tree algorithm to avoid the exponential runtime caused by exact computations in high-treewidth models.,2. Related Work,[0],[0]
Various previous work has taken the same approach.,2. Related Work,[0],[0]
"Expectation propagation (EP) (Minka, 2001) approximates messages by minimizing the Kullback-Leiber (KL) divergence between the actual message and its approximation.",2. Related Work,[0],[0]
"Structured message passing (Gogate & Domingos, 2013) can be considered as a special case of EP where structured representations, in particular algebraic decision diagrams (ADDs) and sparse hash tables, are used so that EP can be performed efficiently.",2. Related Work,[0],[0]
"In contrast to ADDs, the tensor decompositions used for TBP may provide a more compact representation for some problems.",2. Related Work,[0],[0]
An ADD partitions a tensor into axis-aligned hyper-rectangles – it is possible to represent a hyper-rectangle using a rank-1 tensor but a rank-1 tensor is generally not representable as an axis-aligned rectangle.,2. Related Work,[0],[0]
"Furthermore, the supports of the rank-1 tensors in the mixture may overlap.",2. Related Work,[0],[0]
"However, ADD compresses hyperrectangles that share sub-structures and this may result in the methods having different strengths.",2. Related Work,[0],[0]
"Sparse tables, on the other hand, work well for cases with extreme sparsity.
",2. Related Work,[0],[0]
"Several methods use sampled particles to approximate messages (Koller et al., 1999; Ihler & McAllester, 2009; Sudderth et al., 2010).",2. Related Work,[0],[0]
"To allow their algorithms to work well on problems with less sparsity, Koller et al. (1999) and Sudderth et al. (2010) use non-parametric methods to smooth the particle representation of messages.",2. Related Work,[0],[0]
"In contrast, we decompose each tensor into a mixture of rank-1 tensors and sample the rank-1 tensors directly, instead of through the intermediate step of sampling particles.",2. Related Work,[0],[0]
"Another approach, which pre-samples the particles at each node and passes messages between these pre-sampled particles was taken by Ihler & McAllester (2009).",2. Related Work,[0],[0]
"The methods of Ihler & McAllester (2009) and Sudderth et al. (2010) were also applied on graphs with loops using loopy belief propagation.
",2. Related Work,[0],[0]
Xue et al. (2016) use discrete Fourier representations for inference via the elimination algorithm.,2. Related Work,[0],[0]
The discrete Fourier representation is a special type of tensor decomposition.,2. Related Work,[0],[0]
"Instead of sampling, the authors perform approximations by truncating the Fourier coefficients, giving different approximation properties.",2. Related Work,[0],[0]
"Other related works include (Darwiche, 2000; Park & Darwiche, 2002; Chavira & Darwiche, 2005), where belief networks are compiled into compact arithmetic circuits (ACs).",2. Related Work,[0],[0]
"On the related problem of MAP inference, McAuley & Caetano (2011) show that junction tree
clusters that factor over subcliques or consist only of latent variables yield improved complexity properties.",2. Related Work,[0],[0]
"For simplicity we limit our discussion to Markov random fields (MRFs), but our results apply equally to Bayesian networks and general factor graphs.",3. Preliminaries,[0],[0]
We focus only on discrete models.,3. Preliminaries,[0],[0]
"A Markov random field G is an undirected graph representing a probability distribution P (X1, . . .",3. Preliminaries,[0],[0]
", XN ), such that P factorises over the maxcliques in G, i.e.
P (X1, . . .",3. Preliminaries,[0],[0]
", XN ) = 1
Z ∏ c∈cl(G) φc(Xc) (1)
where cl(G) is the set of max-cliques in G and Z =∑ X ∏ c∈cl(G) φc(Xc) ensures normalisation.",3. Preliminaries,[0],[0]
"We call the factors φc clique potentials or potentials.
",3. Preliminaries,[0],[0]
"TBP is based on the junction tree algorithm (see e.g. (Koller & Friedman, 2009)).",3. Preliminaries,[0],[0]
"A junction tree is a special type of cluster graph, i.e. an undirected graph with nodes called clusters that are associated with sets of variables rather than single variables.",3. Preliminaries,[0],[0]
"Specifically, a junction tree is a cluster graph that is a tree and which also satisfies the running intersection property.",3. Preliminaries,[0],[0]
"The running intersection property states that if a variable is in two clusters, it must also be in every cluster on the path that connects the two clusters.
",3. Preliminaries,[0],[0]
The junction tree algorithm is essentially the well-known belief propagation algorithm applied to the junction tree after the cluster potentials have been initialized.,3. Preliminaries,[0],[0]
"At initialisation, each clique potential is first associated with a cluster.",3. Preliminaries,[0],[0]
Each cluster potential Φt(Xt) is computed by multiplying all the clique potentials φc(Xc) associated with the cluster Xt.,3. Preliminaries,[0],[0]
"Thereafter, the algorithm is defined recursively in terms of messages passed between neighbouring clusters.",3. Preliminaries,[0],[0]
"A message is always a function of the variables in the receiving cluster, and represents an intermediate marginalisation over a partial set of factors.",3. Preliminaries,[0],[0]
"The messagemt→s(Xs) sent from a cluster t to a neighbouring cluster s is defined recursively by
mt→s(Xs) = ∑
Xt\Xs
Φt(Xt)",3. Preliminaries,[0],[0]
"∏
u∈N(t)\{s}
mu→t(Xt), (2)
where N(t) is the set of neighbours of t. Since the junction tree is singly connected, this recursion is well-defined.",3. Preliminaries,[0],[0]
"After all messages have been computed, the marginal distribution on a cluster of variables Xs is computed using
Ps(Xs) ∝ Φs(Xs)",3. Preliminaries,[0],[0]
"∏
t∈N(s)
mt→s(Xs), (3)
and univariate marginals can be computed by summation over cluster marginals.",3. Preliminaries,[0],[0]
"The space and time complexity of
the junction tree inference algorithm is exponential in the induced width of the graph, i.e. the number of variables in the largest tree cluster minus 1 (Koller & Friedman, 2009).",3. Preliminaries,[0],[0]
The lowest possible induced width (over all possible junction trees for the graph) is defined as the treewidth of the graph1.,3. Preliminaries,[0],[0]
"The TBP algorithm we propose (Algorithm 1) is the same as the junction tree algorithm except for the approximations at line 1 and line 4, which we describe below.
",4. Tensor Belief Propagation,[0],[0]
"Algorithm 1 Tensor Belief Propagation input Clique potentials {φc(Xc)}, junction tree J .",4. Tensor Belief Propagation,[0],[0]
"output Approximate cluster marginals {P̃s(Xs)} for all s.
1: For each cluster Xt, compute Φ̃t(Xt)",4. Tensor Belief Propagation,[0],[0]
"≈ ∏ c φc(Xc),
where the product is over all cliques c associated with t. 2: while there is any unsent message do 3: Pick an unsent message mt→s(Xs) with all messages to t from neighbours other than s sent.",4. Tensor Belief Propagation,[0],[0]
4: Send m̃t→s(Xs),4. Tensor Belief Propagation,[0],[0]
"≈
∑ Xt\Xs Φ̃t(Xt) ∏",4. Tensor Belief Propagation,[0],[0]
"u∈N(t)\{s} m̃u→t(Xt).
",4. Tensor Belief Propagation,[0],[0]
"5: end while 6: return P̃s(Xs) ∝ Φ̃s(Xs) ∏ t∈N(s) m̃t→s(Xs).
",4. Tensor Belief Propagation,[0],[0]
"There are two challenges in applying the junction tree algorithm to high-treewidth graphical models: representing the intermediate potential functions, and computing them.",4. Tensor Belief Propagation,[0],[0]
"For representation, using tables to represent the cluster potentials and messages requires space exponential in the induced width.",4. Tensor Belief Propagation,[0],[0]
"For computation, the two operations relevant to the complexity of the algorithm are marginalisation over a subset of variables in a cluster and multiplication of multiple factors.",4. Tensor Belief Propagation,[0],[0]
"When clusters become large, these operations become intractable unless additional structure can be exploited.
",4. Tensor Belief Propagation,[0],[0]
TBP alleviates these difficulties by representing all potential functions as mixtures of rank-1 tensors.,4. Tensor Belief Propagation,[0],[0]
"We show how to perform exact marginalisation of a mixture (required in line 4) efficiently, and how to perform approximate multiplication of mixtures using sampling (used for approximation in lines 1 and 4).",4. Tensor Belief Propagation,[0],[0]
"As we are concerned with discrete distributions, each potential φc can be represented by a multidimensional array, i.e. a tensor.",4.1. Mixture of Rank-1 Tensors,[0],[0]
"Furthermore, a d-dimensional tensor T ∈ RN1×···×Nd can be decomposed into a weighted sum
1Finding the treewidth of a graph is NP-hard; in practice we use various heuristics to construct the junction tree.
of outer products of vectors as
T = r∑ k=1 wk a 1 k ⊗ a2k ⊗ · · · ⊗ adk, (4)
where wk ∈ R, aik ∈ RNi and ⊗ is the outer product, i.e.( a1k ⊗ a2k ⊗ · · · ⊗ adk )",4.1. Mixture of Rank-1 Tensors,[0],[0]
"i1,...,id = ( a1k ) i1 · · · · · ( adk ) id
.",4.1. Mixture of Rank-1 Tensors,[0],[0]
We denote the vector of weights {wk} as w. The smallest r for which an exact r-term decomposition exists is called the rank of T and a decomposition (4) using this r is a tensor rank decomposition.,4.1. Mixture of Rank-1 Tensors,[0],[0]
"This decomposition is known by several names including CANDECOMP/PARAFAC (CP) decomposition and Hitchcock decomposition (Kolda & Bader, 2009).",4.1. Mixture of Rank-1 Tensors,[0],[0]
"In this paper, we assume without loss of generality that the weights are non-negative and sum to 1, giving a mixture of rank-1 tensors.",4.1. Mixture of Rank-1 Tensors,[0],[0]
"Such a mixture forms a probability distribution over rank-1 tensors, which we refer to as a tensor belief.",4.1. Mixture of Rank-1 Tensors,[0],[0]
"We also assume that the decomposition is non-negative, i.e. the rank-1 tensors are nonnegative, although the method can be extended to allow negative values.
",4.1. Mixture of Rank-1 Tensors,[0],[0]
"For a (clique or cluster) potential function φs(Xs) over |Xs| = Ns variables, (4) is equivalent to decomposing φs into a sum of fully-factorised terms, i.e.
φs(Xs) =",4.1. Mixture of Rank-1 Tensors,[0],[0]
"r∑ k=1 wsk ψ s k(Xs)
=",4.1. Mixture of Rank-1 Tensors,[0],[0]
"r∑ k=1 wsk ψ s k,1(Xs1) · · ·ψsk,Ns(XsNs ).",4.1. Mixture of Rank-1 Tensors,[0],[0]
"(5)
",4.1. Mixture of Rank-1 Tensors,[0],[0]
This observation allows us to perform marginalisation and multiplication operations efficiently.,4.1. Mixture of Rank-1 Tensors,[0],[0]
"Marginalising out a variable Xi from a cluster Xs simplifies in the same manner as if the distribution was fullyfactorised, namely∑ Xi φs(Xs) = r∑ k=1 wsk (∑ Xi ψsk,j(Xi) ) ·",4.2. Marginalisation,[0],[0]
"(6)
ψsk,1(Xs1)ψ s k,2(Xs2) · · ·ψsk,Ns(XsNs )︸",4.2. Marginalisation,[0],[0]
"︷︷ ︸
excluding ψsk,j(Xi) where we simply push the sum ∑ Xi
inside and only evaluate it over the univariate factor ψsk,j(Xi).",4.2. Marginalisation,[0],[0]
We can then absorb this sum into the weights {wsk} and the result stays in decomposed form (5).,4.2. Marginalisation,[0],[0]
"To marginalise over multiple variables, we evaluate (6) for each variable in turn.",4.2. Marginalisation,[0],[0]
"The key observation for multiplication is that a mixture of rank-1 tensors can be treated as a probability distribution
over the rank-1 tensors with expectation equal to the true function, by considering the weight of each rank-1 term wk as its probability.
",4.3. Multiplication,[0],[0]
"To multiply two potentials φi and φj , we repeatedly sample rank-1 terms from each multiplicand to build the product.",4.3. Multiplication,[0],[0]
"We draw a sample of K pairs of indices {(kr, lr)}Kr=1 independently from wi and wj respectively, and use the approximation
φi(Xi) · φj(Xj) ≈ 1
K K∑ r=1 ψikr (Xi) ·",4.3. Multiplication,[0],[0]
ψ,4.3. Multiplication,[0],[0]
j lr (Xj).,4.3. Multiplication,[0],[0]
"(7)
The approximation is also a mixture of rank-1 tensors, with the rank-1 tensors being the ψikr (Xi) ·",4.3. Multiplication,[0],[0]
ψ,4.3. Multiplication,[0],[0]
"j lr
(Xj), and their weights being the frequencies of the sampled (kr, lr) pairs.",4.3. Multiplication,[0],[0]
This process is equivalent to drawing a sample of the same size from the distribution representing φi(Xi) ·φj(Xj) and hence provides an unbiased estimate of the product function.,4.3. Multiplication,[0],[0]
Multiplication of each pair ψikr (Xi) · ψ,4.3. Multiplication,[0],[0]
"j lr
(Xj) can be performed efficiently due to their factored forms.",4.3. Multiplication,[0],[0]
"It is possible to extend the method to allow multiplication of more potential functions simultaneously but we only use pairwise multiplication in this paper, repeating the process as necessary to multiply more functions.",4.3. Multiplication,[0],[0]
"For simplicity, we give results for binary MRFs.",4.4. Theoretical Analysis,[0],[0]
Extension to non-binary variables is straightforward.,4.4. Theoretical Analysis,[0],[0]
"We assume that exact tensor decompositions are given for all initial clique potentials.
",4.4. Theoretical Analysis,[0],[0]
Theorem 1.,4.4. Theoretical Analysis,[0],[0]
Consider a binary MRF with C max-cliques with potential functions represented as non-negative tensor decompositions.,4.4. Theoretical Analysis,[0],[0]
Consider the unnormalised distribution D(X) = ∏ c∈cl(G) φc(Xc).,4.4. Theoretical Analysis,[0],[0]
Let Di(Xi) = ∑ X\Xi D(X) be the unnormalised marginal for variableXi.,4.4. Theoretical Analysis,[0],[0]
"Assume that the values of the rank-1 tensors in any mixture resulting from pairwise multiplication is upper bounded by M , and that the approximation target for any cell in any multiplication operation is lower bounded by B. Let D̃i(Xi) be the estimates produced by TBP using a junction tree with an induced width T .",4.4. Theoretical Analysis,[0],[0]
"With probability at least 1 − δ, for all i and xi,
(1− )Di(xi) ≤",4.4. Theoretical Analysis,[0],[0]
"D̃i(xi) ≤ (1 + )Di(xi)
if the sample size used for all multiplication operations is at least
Kmin( , δ) ∈",4.4. Theoretical Analysis,[0],[0]
"O ( C2 2 M
B
( logC + T + log 2
δ
)) .
",4.4. Theoretical Analysis,[0],[0]
"Furthermore, D̃i(Xi) remains a consistent estimator for Di(Xi) if B = 0.
",4.4. Theoretical Analysis,[0],[0]
Proof.,4.4. Theoretical Analysis,[0],[0]
We give the proof for the case B 6= 0 here.,4.4. Theoretical Analysis,[0],[0]
"The consistency proof for the case B = 0 can be found in the supplementary material.
",4.4. Theoretical Analysis,[0],[0]
The errors in the unnormalised marginals are due to the errors in approximate pairwise multiplication defined by Equation (7).,4.4. Theoretical Analysis,[0],[0]
"We first give bounds for the errors in all the pairwise multiplication by sampling operations, then derive the bounds for the errors in the unnormalised marginals.
",4.4. Theoretical Analysis,[0],[0]
"Recall that by the multiplicative Chernoff bound (see e.g. (Koller & Friedman, 2009)), for K i.i.d.",4.4. Theoretical Analysis,[0],[0]
"random variables Y1, . . .",4.4. Theoretical Analysis,[0],[0]
", YK in range [0,M ] with expected value µ, we have
P
( 1
K K∑ i=1",4.4. Theoretical Analysis,[0],[0]
Yi /∈,4.4. Theoretical Analysis,[0],[0]
"[(1− ζ)µ, (1 + ζ)µ]
) ≤ 2 exp ( −ζ 2µK
3M
) .
",4.4. Theoretical Analysis,[0],[0]
The number of pairwise multiplications required to initialize the cluster potentials {Φ̃t(Xt)} is at most C (line 1).,4.4. Theoretical Analysis,[0],[0]
"The junction tree has at most C clusters, and thus at most 2(C − 1) messages need to be computed.",4.4. Theoretical Analysis,[0],[0]
Each message requires at most C pairwise multiplications (line 4).,4.4. Theoretical Analysis,[0],[0]
Hence the total number of pairwise multiplications needed is at most 2C2.,4.4. Theoretical Analysis,[0],[0]
"Each pairwise multiplication involves functions defined on at most T binary variables, and thus it simultaneously estimates at most 2T values.",4.4. Theoretical Analysis,[0],[0]
"Hence at most 2T+1C2 values are estimated by sampling in the algorithm.
",4.4. Theoretical Analysis,[0],[0]
"Since each estimate satisfies the Chernoff bound, we can apply a union bound to bound the probability that the estimate for any µj is outside [(1− ζ)µj , (1 + ζ)µj ], giving
Pζ
( 1
K K∑ i=1",4.4. Theoretical Analysis,[0],[0]
Xji /∈,4.4. Theoretical Analysis,[0],[0]
"[(1− ζ)µj , (1 + ζ)µj ] for
any estimate j ) ≤ 2T+2C2 exp ( −ζ 2BK
3M
) ,
whereB is a lower bound on the minimum value of all cells estimated during the algorithm.",4.4. Theoretical Analysis,[0],[0]
"If we set an upper-bound on this error probability of δ and rearrange for K, we have that with probability at least δ, all estimates are within a factor of (1± ζ) of their true values when
K ≥ 3 ζ2 M B ln
2T+2C2
δ .",4.4. Theoretical Analysis,[0],[0]
"(8)
We now seek an expression for the sample size required for the unnormalised marginals to have small errors.",4.4. Theoretical Analysis,[0],[0]
First we argue that at most C + Q,4.4. Theoretical Analysis,[0],[0]
"− 1 pairwise multiplications are used in the process of constructing the marginals at any node u, where Q is the number of clusters.",4.4. Theoretical Analysis,[0],[0]
This is true for the base case of a tree of size 1 as no messages need to be passed.,4.4. Theoretical Analysis,[0],[0]
"As the inductive hypothesis, assume that the statement is true for trees of size less than n. The node u is
considered as the root of the tree and by the inductive hypothesis the number of multiplications used in each subtree i is at most Ci + Qi",4.4. Theoretical Analysis,[0],[0]
"− 1 where Ci and Qi are the number of clique potentials and clusters associated with subtree i. Summing them up and noting that we need to perform at most one additional multiplication for each clique potential associated with u for initialisation, and one additional multiplication for each subtree, gives us the required result.",4.4. Theoretical Analysis,[0],[0]
"To simplify analysis, we bound C + Q",4.4. Theoretical Analysis,[0],[0]
"− 1 by 2C from here onwards.
",4.4. Theoretical Analysis,[0],[0]
"At worst, each pairwise multiplication results in an extra (1 ± ζ) factor in the bound.",4.4. Theoretical Analysis,[0],[0]
"Since we are using a multiplicative bound, marginalisation operations have no effect on the bound.",4.4. Theoretical Analysis,[0],[0]
"As we do no more than 2C multiplications, the final marginal estimates are all within a factor (1±ζ)2C of their true value.
",4.4. Theoretical Analysis,[0],[0]
"To bound the marginal so that it is within a factor (1 ± ) of its true value for a chosen > 0, we note that choosing ζ = ln(1+ )",4.4. Theoretical Analysis,[0],[0]
"2C implies (1 − ζ)
2C ≥ 1 − and (1 + ζ)2C ≤",4.4. Theoretical Analysis,[0],[0]
"(1 + ):
(1− ζ)2C = (
1− ln(1 + )",4.4. Theoretical Analysis,[0],[0]
"2C
)2C (9)
≥ (
1− 2C
)",4.4. Theoretical Analysis,[0],[0]
"2C ≥ 1−
(1 + ζ)2C =",4.4. Theoretical Analysis,[0],[0]
"( 1 + ln(1 + )
2C
)2C (10)
≤ exp (ln(1 + ))",4.4. Theoretical Analysis,[0],[0]
"= 1 + .
",4.4. Theoretical Analysis,[0],[0]
"In (9) we use Bernoulli’s inequality together with ln(1 + ) ≤ , and in (10) we use (1 + x)r ≤ exp(rx).
",4.4. Theoretical Analysis,[0],[0]
"Substituting this ζ into (8), we have that all marginal estimates are accurate within factor (1± ) with probability at least 1− δ, when
K ≥ 12C 2 (ln(1 + ))",4.4. Theoretical Analysis,[0],[0]
"2 M B ln 2T+2C2 δ (11)
",4.4. Theoretical Analysis,[0],[0]
Using the fact that ln(1 + ),4.4. Theoretical Analysis,[0],[0]
"≥ · ln 2 for 0 ≤ ≤ 1, and 24 ln 2 < 35, we can relax this bound to
K ≥ 35C 2 2 M
B
( logC + T + log 2
δ
) .
",4.4. Theoretical Analysis,[0],[0]
Corollary 1.,4.4. Theoretical Analysis,[0],[0]
"Under the same conditions as Theorem 1, with probability at least 1− δ, the normalised marginal estimates p̃i(xi) satisfy
(1− γ)pi(xi) ≤ p̃i(xi) ≤ (1 + γ)pi(xi)
for all i and xi, if the sample size used for all multiplication operations is at least
K ′min(γ, δ) ∈",4.4. Theoretical Analysis,[0],[0]
"O ( C2
γ2 M B
( logC + T + log 1
δ
)) .
",4.4. Theoretical Analysis,[0],[0]
Proof.,4.4. Theoretical Analysis,[0],[0]
"Suppose the unnormalised marginals have relative error bounded by (1± ), i.e.
(1− )Di(xi) ≤",4.4. Theoretical Analysis,[0],[0]
"D̃i(xi) ≤ (1 + )Di(xi)
for all i and xi.",4.4. Theoretical Analysis,[0],[0]
"Then we have
p̃i(xi) = D̃i(xi)∑ xi D̃i(xi) ≤ (1 + )Di(xi)∑",4.4. Theoretical Analysis,[0],[0]
xi (1− )Di(xi),4.4. Theoretical Analysis,[0],[0]
= 1,4.4. Theoretical Analysis,[0],[0]
"+ 1− pi(xi).
",4.4. Theoretical Analysis,[0],[0]
"To bound the relative error on p̃i(xi) to (1 + γ), we set
1 + 1− = 1 + γ =⇒ =",4.4. Theoretical Analysis,[0],[0]
"γ γ + 2 .
",4.4. Theoretical Analysis,[0],[0]
Since 1 2 = ( γ+2 γ )2 <,4.4. Theoretical Analysis,[0],[0]
"( 3 γ )2 = O ( 1 γ2 ) for γ < 1, the increase in Kmin required to bound the normalised estimates rather than the unnormalised estimates is at most a constant.",4.4. Theoretical Analysis,[0],[0]
"Thus,
K ′min(γ, δ) ∈ O ( C2
γ2 M B
( logC + T + log 1
δ
)) .
",4.4. Theoretical Analysis,[0],[0]
as required.,4.4. Theoretical Analysis,[0],[0]
"The negative side of the bound is similar.
",4.4. Theoretical Analysis,[0],[0]
"Interestingly, the sample size does not grow exponentially with the induced width, and hence the treewidth of the graph.",4.4. Theoretical Analysis,[0],[0]
"As the inference problem is NP-hard, we expect the ratio M/B to be large in difficult problems.",4.4. Theoretical Analysis,[0],[0]
The M/B ratio comes from bounding the relative error when sampling a mixture φ(x) =,4.4. Theoretical Analysis,[0],[0]
∑r k=1 wkψk(x).,4.4. Theoretical Analysis,[0],[0]
A more refined bound can be obtained by bounding maxk maxx ψk(x)/φ(x) instead; this bound would not grow as quickly if ψk(x) is always small whenever φ(x) is small.,4.4. Theoretical Analysis,[0],[0]
"Understanding the properties of function classes where these bounds are small may help us understand when TBP works well.
",4.4. Theoretical Analysis,[0],[0]
Theorem 1 suggests that that it may be useful to reweight the rank-1 tensors in a multiplicand to give a smaller M/B ratio.,4.4. Theoretical Analysis,[0],[0]
"The following proposition gives a reweighting scheme that minimises the maximum value of the rank-1 tensors in a multiplicand, which leads to a smaller M with B fixed.",4.4. Theoretical Analysis,[0],[0]
"Theorem 1 still holds with this reweighting.
Proposition 1.",4.4. Theoretical Analysis,[0],[0]
Let φ(x) =,4.4. Theoretical Analysis,[0],[0]
"∑r k=1 wkψk(x) where wk ≥
0, ∑r k=1 wk = 1 and ψk(x) ≥ 0 for all x. Con-
sider a reweighted representation ∑r",4.4. Theoretical Analysis,[0],[0]
k=1 w ′,4.4. Theoretical Analysis,[0],[0]
"kψ ′ k(x), where ψ′k(x) = wk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k",4.4. Theoretical Analysis,[0],[0]
= 1.,4.4. Theoretical Analysis,[0],[0]
Then maxk maxx ψ′k(x) is minimized when w ′,4.4. Theoretical Analysis,[0],[0]
"k ∝ wk maxx ψk(x).
",4.4. Theoretical Analysis,[0],[0]
Proof.,4.4. Theoretical Analysis,[0],[0]
"Let ak = wk maxx ψk(x), and let v =",4.4. Theoretical Analysis,[0],[0]
maxk,4.4. Theoretical Analysis,[0],[0]
"maxx ψ ′ k(x) = maxk
ak w′k .",4.4. Theoretical Analysis,[0],[0]
"For any choice of w′, we have v ≥ ∑
k",4.4. Theoretical Analysis,[0],[0]
ak∑ k w ′,4.4. Theoretical Analysis,[0],[0]
k = ∑ k ak.,4.4. Theoretical Analysis,[0],[0]
"The first inequality holds be-
cause vw′k ≥ ak for any k by the definition of v. Since
Tensor Belief Propagation∑ k ak is a constant lower bound for v, and this is clearly achieved by setting w′k ∝",4.4. Theoretical Analysis,[0],[0]
"ak, we have the claimed result.
",4.4. Theoretical Analysis,[0],[0]
"Note that with ψk(x) represented as a rank-1 tensor, the maximum value over x can be computed quickly with the help of the factored structure.
",4.4. Theoretical Analysis,[0],[0]
Reweighting the rank-1 tensors as described in Proposition 1 and then sampling gives a form of importance sampling.,4.4. Theoretical Analysis,[0],[0]
Importance sampling is often formulated instead with the objective of minimizing the variance of the estimates.,4.4. Theoretical Analysis,[0],[0]
"Since we expect lowering the variance of intermediate estimates to lead to lower variance on the final marginal estimates, we also examine the following alternative reweighting scheme.",4.4. Theoretical Analysis,[0],[0]
Proposition 2.,4.4. Theoretical Analysis,[0],[0]
Let φ(x) =,4.4. Theoretical Analysis,[0],[0]
"∑r k=1 wkψk(x) where wk ≥
0, ∑r k=1 wk = 1.",4.4. Theoretical Analysis,[0],[0]
"Consider a reweighted representa-
tion ∑r k=1",4.4. Theoretical Analysis,[0],[0]
w ′,4.4. Theoretical Analysis,[0],[0]
"kψ ′ k(x), where ψ ′ k(x) =
wk w′k ψk(x), each w′k ≥ 0, and ∑ k w ′ k",4.4. Theoretical Analysis,[0],[0]
= 1.,4.4. Theoretical Analysis,[0],[0]
Let φ̃(x) = 1 K ∑K i=1 ψ ′,4.4. Theoretical Analysis,[0],[0]
"Ki(x) be an estimator for φ(x), where each Ki is drawn independently from the categorical distribution with parameters {w′1, . . .",4.4. Theoretical Analysis,[0],[0]
", w′r}.",4.4. Theoretical Analysis,[0],[0]
"Then φ̃(x) is unbiased, and the total variance ∑ x Var[φ̃
′(x)] is minimized when w′k ∝",4.4. Theoretical Analysis,[0],[0]
"wk √∑ x ψk(x) 2.
",4.4. Theoretical Analysis,[0],[0]
Proof.,4.4. Theoretical Analysis,[0],[0]
Clearly φ̃(x) is an unbiased estimator for φ(x).,4.4. Theoretical Analysis,[0],[0]
"The variance of this estimator is
Var[φ̃(x)]",4.4. Theoretical Analysis,[0],[0]
"= 1
K Var[ψ′K′i(x)]
= 1
K
( E[ψ′Ki(x) 2]− E[ψ′Ki(x)] 2 )
",4.4. Theoretical Analysis,[0],[0]
"= 1
K (∑ k w′kψ ′ k(x) 2 − φ(x)2 )
",4.4. Theoretical Analysis,[0],[0]
"= 1
K (∑ k w′k",4.4. Theoretical Analysis,[0],[0]
w2k,4.4. Theoretical Analysis,[0],[0]
"w′2k ψk(x) 2 − φ(x)2 )
",4.4. Theoretical Analysis,[0],[0]
"= 1
K (∑ k w2k w′k ψk(x) 2",4.4. Theoretical Analysis,[0],[0]
"− φ(x)2 ) .
",4.4. Theoretical Analysis,[0],[0]
"Since K and φ(x) are constant, we have
{w′k}∗ = argmin {w′k} ∑ x Var[φ̃′(x)]
= argmin {w′k} ∑ x ∑ k 1 w′k (wkψk(x)) 2,
where each w′k ≥ 0, ∑ k w ′ k = 1.",4.4. Theoretical Analysis,[0],[0]
"A straightforward application of the method of Lagrange multipliers yields
w′k = wk √∑ x ψk(x)",4.4. Theoretical Analysis,[0],[0]
"2∑
l wl √∑ x ψl(x) 2 .
",4.4. Theoretical Analysis,[0],[0]
"The factored forms of rank-1 tensors again allows the importance reweighting to be computed efficiently.
",4.4. Theoretical Analysis,[0],[0]
Propositions 1 and 2 could be applied directly to the algorithm by multiplying two potential functions fully before sampling.,4.4. Theoretical Analysis,[0],[0]
"If each potential function has K terms, the multiplication would result in K2 terms which is computationally expensive.",4.4. Theoretical Analysis,[0],[0]
We only partially exploit the results of Propositions 1 and 2 in our algorithm.,4.4. Theoretical Analysis,[0],[0]
"When multiplying two potential functions, we draw K pairs of rank-1 tensors from their respective distributions and multiply them as described earlier but re-weight the resulting mixture using either Proposition 1 or 2.",4.4. Theoretical Analysis,[0],[0]
"We call the former max-norm reweighting, and the latter min-variance reweighting.",4.4. Theoretical Analysis,[0],[0]
"We present experiments on grid-structured Ising models, random graphs with pairwise Ising potentials, and two real-world datasets from the UAI 2014 Inference Competition (Gogate, 2014).",5. Experiments,[0],[0]
"We test our algorithm against commonly used approximate inference methods, namely loopy belief propagation (labelled BP), mean-field (MF), treereweighted BP (TRW) and Gibbs sampling using existing implementations from the libDAI package (Mooij, 2010).",5. Experiments,[0],[0]
"TBP was implemented in C++ inside the libDAI framework using Eigen (Guennebaud et al., 2010) and all tests were executed on a single core of a 1.4 GHz Intel Core i5 processor.",5. Experiments,[0],[0]
"In each experiment, we time the execution of TBP and allow Gibbs sampling the same wall-clock time.",5. Experiments,[0],[0]
"We run the other algorithms until convergence, which occurs quickly in each case (hence only the final performance is shown).",5. Experiments,[0],[0]
"Parameters used for BP, MF, TRW and Gibbs are given in the supplementary material.",5. Experiments,[0],[0]
"To build the junction tree, we use the min-fill heuristic2 implemented in libDAI.",5. Experiments,[0],[0]
Figure 1 gives results for 10×10 Ising models.,5.1. Grid-structured Ising Models,[0],[0]
TheN×N,5.1. Grid-structured Ising Models,[0],[0]
"Ising model is a planar grid-structured MRF described by the joint distribution
p(x1, . . .",5.1. Grid-structured Ising Models,[0],[0]
", xN2) = 1
Z exp ∑ (i,j) wijxixj + ∑ i bixi  where (i, j) runs over all edges in the grid.",5.1. Grid-structured Ising Models,[0],[0]
Each variable Xi takes value either 1 or −1.,5.1. Grid-structured Ising Models,[0],[0]
"In our experiments, we choose the wij uniformly from [−2, 2] (mixed interactions) or [0, 2] (attractive interactions), and the b uniformly from [−1, 1].",5.1. Grid-structured Ising Models,[0],[0]
We use a symmetric rank-2 tensor decomposition for the pairwise potentials.,5.1. Grid-structured Ising Models,[0],[0]
"We measure performance
2Min-fill repeatedly eliminates variables that result in the lowest number of additional edges in the graph (Koller & Friedman, 2009).
",5.1. Grid-structured Ising Models,[0],[0]
"by the mean L1 error of marginal estimates over the N2 variables,
1
N2 N2∑ i=1 |Pexact(Xi",5.1. Grid-structured Ising Models,[0],[0]
"= 1)− Papprox(Xi = 1)|.
",5.1. Grid-structured Ising Models,[0],[0]
Exact marginals were obtained using the exact junction tree algorithm (possible because the grid size is small).,5.1. Grid-structured Ising Models,[0],[0]
Results are all averages over 100 model instances generated randomly with the specified parameters; error bars show standard error.,5.1. Grid-structured Ising Models,[0],[0]
"We give a description of the tensor decomposition and additional results for a range of grid sizes N and interaction strengths wij in the supplementary material.
",5.1. Grid-structured Ising Models,[0],[0]
"As we increase the number of samples used for multiplication K, and hence running time, the performance of TBP improves as expected.",5.1. Grid-structured Ising Models,[0],[0]
"On both attractive and mixed interaction models, loopy BP, mean-field and tree-reweighted BP perform poorly.",5.1. Grid-structured Ising Models,[0],[0]
Gibbs sampling performs poorly on models with attractive interactions but performs better on models with mixed interactions.,5.1. Grid-structured Ising Models,[0],[0]
This is expected since models with mixed interactions mix faster.,5.1. Grid-structured Ising Models,[0],[0]
"Reweighting gives a noticeable improvement over not using reweighting; both max-norm reweighting and min-variance reweighting give very similar results (in Figure 1, they not easily differentiated).",5.1. Grid-structured Ising Models,[0],[0]
"For the remainder of our experiments, we show results for max-norm reweighting only.",5.1. Grid-structured Ising Models,[0],[0]
We also test TBP on random binary N -node MRFs with pairwise Ising potentials.,5.2. Random Pairwise MRFs,[0],[0]
"We construct the graphs by independently adding each edge (i, j) with probability 0.5, with the requirement that the resulting graph is connected.",5.2. Random Pairwise MRFs,[0],[0]
"Pairwise potentials are of the same form as Ising models (i.e. exp[wijxixj ]), where interaction strengths wij are chosen uniformly from [0, 2] (attractive) or [−2, 2] (mixed) and the
bi are chosen uniformly from [−1, 1].
",5.2. Random Pairwise MRFs,[0],[0]
Figure 2a shows the performance of TBP with increasing sample size K on 15-node models.,5.2. Random Pairwise MRFs,[0],[0]
"Similar to gridstructured Ising models, TBP performs well as the sample size is increased.",5.2. Random Pairwise MRFs,[0],[0]
"Notably, TBP performs very well on attractive models where other methods perform poorly.
",5.2. Random Pairwise MRFs,[0],[0]
Figure 2b shows the performance of the algorithms as the graph size is increased to 30 nodes.,5.2. Random Pairwise MRFs,[0],[0]
"Interestingly, TBP starts to fail for mixed interaction models as the graph size is increased but remains very effective for attractive interaction models while other methods perform poorly.",5.2. Random Pairwise MRFs,[0],[0]
"Finally, we show results of TBP on two real-world datasets from the UAI 2014 Inference Competition, namely the Promedus and Linkage datasets.",5.3. UAI Competition Problems,[0],[0]
We chose these two problems following Zhu & Ermon (2015).,5.3. UAI Competition Problems,[0],[0]
"To compute the initial tensor rank decompositions, we use the non-negative cp nmu method from Bader et al. (2015), an iterative optimisation method based on (Lee & Seung, 2001).",5.3. UAI Competition Problems,[0],[0]
The initial potential functions are decomposed into mixtures with r components.,5.3. UAI Competition Problems,[0],[0]
We show results for r = 2 and r = 4.,5.3. UAI Competition Problems,[0],[0]
"We measure performance by the mean L1 error over all states and all variables
1
NSi N∑ i=1",5.3. UAI Competition Problems,[0],[0]
∑ xi |Pexact(Xi,5.3. UAI Competition Problems,[0],[0]
= xi)− Papprox(Xi,5.3. UAI Competition Problems,[0],[0]
"= xi)|,
where Si is the number of states of variableXi.",5.3. UAI Competition Problems,[0],[0]
"Results are averages over all problem instances in (Gogate, 2014).
",5.3. UAI Competition Problems,[0],[0]
We see that TBP outperforms Gibbs sampling on both problems3.,5.3. UAI Competition Problems,[0],[0]
"On the Linkage dataset, using r = 2 mix-
3We omit results for BP, MF and TRW for these problems because the libDAI package was unable to run them.
ture components for the initial decompositions performs best and increasing r does not improve performance.",5.3. UAI Competition Problems,[0],[0]
"On the Promedus dataset r = 4 performs better; in this case, increasing r may improve the accuracy of the decomposition of the initial potential functions.",5.3. UAI Competition Problems,[0],[0]
"For reference, the marginal error achieved by the random projection method of Zhu & Ermon (2015) is 0.09 ± 0.02 for Linkage and 0.21 ± 0.06 for Promedus.",5.3. UAI Competition Problems,[0],[0]
TBP with K = 105 achieved error of 0.08±0.01 for Linkage and 0.12±0.01 for Promedus despite using substantially less running time 4.,5.3. UAI Competition Problems,[0],[0]
"We proposed a new sampling-based approximate inference algorithm, tensor belief propagation, which uses a mixtureof-rank-1-tensors representation to approximate cluster potentials and messages in the junction tree algorithm.",6. Conclusion and Future Work,[0],[0]
"It
4TBP with K = 105 takes an average across all problem instances of approximately 10 minutes (Linkage) and 3 minutes (Promedus) per problem on our machine, and does not differ substantially for r = 2 vs r = 4.",6. Conclusion and Future Work,[0],[0]
"The results described by Zhu & Ermon (2015) were comparable in running time to 10 million iterations of Gibbs sampling, which we estimate would take several hours on our machine without parallelism.
",6. Conclusion and Future Work,[0],[0]
"gives consistent estimators, and performs well on a range of problems against well-known algorithms.
",6. Conclusion and Future Work,[0],[0]
"In this paper, we have not addressed how to perform the initial tensor decomposition.",6. Conclusion and Future Work,[0],[0]
"There exist well-known optimisation algorithms for this problem to minimize Euclidean error (Kolda & Bader, 2009), though it is not clear that this is the best objective for the purpose of TBP.",6. Conclusion and Future Work,[0],[0]
We are currently investigating the best way of formulating and solving this optimisation problem to yield accurate results during inference.,6. Conclusion and Future Work,[0],[0]
"Further, when constructing the junction tree, it is not clear whether min-fill and other well-known heuristics remain appropriate for TBP.",6. Conclusion and Future Work,[0],[0]
"In particular, these cost functions aim to minimise the induced width of the junction tree which is no longer directly relevant to the performance of the algorithm.",6. Conclusion and Future Work,[0],[0]
"Further work may investigate other heuristics, for example with the goal of minimising cluster variance.
",6. Conclusion and Future Work,[0],[0]
"Finally, approximate inference algorithms have applications in learning (for example, in the gradient computations when training restricted Boltzmann machines) and thus TBP may improve learning as well as inference in some applications.",6. Conclusion and Future Work,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
This work is supported by NUS AcRF Tier 1 grant R-252-000-639-114 and a QUT Vice-Chancellor’s Research Fellowship Grant.,Acknowledgements,[0],[0]
"We propose a new approximate inference algorithm for graphical models, tensor belief propagation, based on approximating the messages passed in the junction tree algorithm.",abstractText,[0],[0]
Our algorithm represents the potential functions of the graphical model and all messages on the junction tree compactly as mixtures of rank-1 tensors.,abstractText,[0],[0]
"Using this representation, we show how to perform the operations required for inference on the junction tree efficiently: marginalisation can be computed quickly due to the factored form of rank-1 tensors while multiplication can be approximated using sampling.",abstractText,[0],[0]
"Our analysis gives sufficient conditions for the algorithm to perform well, including for the case of high-treewidth graphs, for which exact inference is intractable.",abstractText,[0],[0]
We compare our algorithm experimentally with several approximate inference algorithms and show that it performs well.,abstractText,[0],[0]
Tensor Belief Propagation,title,[0],[0]
"Tensors have long been successfully used in several disciplines, including neuroscience, phylogenetics, statistics, signal processing, computer vision, and data mining.",1. Introduction,[0],[0]
"They are used to model multi-relational or multi-modal data, and their decompositions often reveal some underlying structures behind the observed data.",1. Introduction,[0],[0]
"See (Kolda & Bader, 2009) for a survey of such results.",1. Introduction,[0],[0]
"Recently, they have found applications in machine learning, particularly for learning various latent variable models (Anandkumar et al., 2012; Chaganty & Liang, 2014; Anandkumar et al., 2014a).
",1. Introduction,[0],[0]
"One popular decomposition method in such applications is the CP (Candecomp/Parafac) decomposition, which decomposes the given tensor as a sum of rank-one components.",1. Introduction,[0],[0]
"This is similar to the singular value decomposition (SVD) of matrices, and a popular approach for SVD is the power method, which is well-understood and has nice theoretical guarantee.",1. Introduction,[0],[0]
"As tensors can be seen as generalization of matrices to higher orders, one would hope that a natural generalization of the power method to tensors could inherit the success from the matrix case.",1. Introduction,[0],[0]
"However, the situation turns out to be much more complicated for tensors (see e.g. the discussion in (Anandkumar et al., 2014a)),
1Academia Sinica, Taiwan.",1. Introduction,[0],[0]
"Correspondence to: Po-An Wang <poanwang@iis.sinica.edu.tw>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"and in fact several problems related to tensor decomposition are known to be NP-hard (Hillar & Lim, 2013).",1. Introduction,[0],[0]
"Nevertheless, when the given tensor has some additional structure, the tensor decomposition problem becomes tractable again.",1. Introduction,[0],[0]
"In particular, for tensors having orthogonal decomposition, Anandkumar et al. (2014a) provided an efficient algorithm based on the tensor power method with theoretical guarantee.",1. Introduction,[0],[0]
"Still, as we will discuss later in Section 2, the seemingly subtle change of going from matrices to tensors makes some significant differences for the power method.
",1. Introduction,[0],[0]
"The first is that while the matrix power method can guarantee that a randomly selected initial vector will almost surely converge to the top singular vector, we have much less control of where the convergence goes in the tensor case.",1. Introduction,[0],[0]
"Consequently, most previous works based on the tensor power method with theoretical guarantee, such as (Anandkumar et al., 2014a;b; Wang & Anandkumar, 2016), require much more complicated procedures.",1. Introduction,[0],[0]
"In particular, they can only find the top k eigenvectors one by one, each time with the power method applied to a modified tensor, deflated from the original tensor according to the previously found vectors.",1. Introduction,[0],[0]
"Moreover, to find each vector, they need to sample several initial vectors and apply the power method on all of them, before selecting just one from them.",1. Introduction,[0],[0]
"In contrast, algorithms for matrices such as (Mitliagkas et al., 2013; Hardt & Price, 2014) are much simpler, as they can find the k vectors simultaneously by applying the power method only on k random initial vectors.",1. Introduction,[0],[0]
"The second difference, on the other hand, has a beneficial effect, which allows the tensor power method to converge exponentially faster than the matrix one when starting from good initial vectors.",1. Introduction,[0],[0]
Then a natural question is: can we inherit the best of both worlds?,1. Introduction,[0],[0]
"Namely, is it possible to have a simple algorithm which can find the k eigenvectors of a tensor simultaneously and converge faster than that for matrices?
",1. Introduction,[0],[0]
Our Results.,1. Introduction,[0],[0]
"As in previous works, we consider the slightly harder scenario in which we only have access to a noisy version of the tensor we want to decompose.",1. Introduction,[0],[0]
"This arises in applications such as learning latent variable models, in which the tensor we have access to is obtained from some empirical average of the observed data.",1. Introduction,[0],[0]
"Our main contribution is to answer the above question affirmatively.
",1. Introduction,[0],[0]
"First, we consider the batch setting in which we assume
that the given noisy tensor is stored somewhere and can be accessed whenever we want to.",1. Introduction,[0],[0]
"In this setting, we identify a sufficient condition such that if we have k initial vectors satisfying this condition, then we can apply the tensor power method on them simultaneously, which will come within some distance ε to the eigenvectors in O(log log 1ε ) iterations, with parameters related to eigenvalues considered as constant.",1. Introduction,[0],[0]
"To apply such a result, we need an efficient way to find such initial vectors.",1. Introduction,[0],[0]
"We show how to do this by choosing a good direction to project the tensor down to a matrix while preserving the eigengaps, and then applying the matrix power method for only a few iterations just to obtain vectors meeting that sufficient condition.",1. Introduction,[0],[0]
"The number of iterations needed here is only O(log d), independent of ε, where d is the dimension of the eigenvectors.
",1. Introduction,[0],[0]
The result stated above is for orthogonal tensors.,1. Introduction,[0],[0]
"On the other hand, it is known that an nonorthogonal tensor with linearly independent eigenvectors can be converted into an orthogonal one with the help of some whitening matrix.",1. Introduction,[0],[0]
"However, previous works usually pay little attention on how to find such a whitening matrix efficiently.",1. Introduction,[0],[0]
"According to (Anandkumar et al., 2014a), one way is via SVD on some second moment matrix, but doing this using the matrix power method would take longer to converge compared to the tensor power method which would then be applied on the whitened tensor.",1. Introduction,[0],[0]
"Our second contribution is to provide an efficient way to find a whitening matrix, by simply applying only one iteration of the matrix power method.
",1. Introduction,[0],[0]
"While most previous works on tensor decomposition focus on the batch setting, storing even a tensor of order three requires Ω(d3) space, which is infeasible for a large d. We show to avoid this in the streaming setting, with a stream of data arriving one at a time, which is the only source of information about the tensor.",1. Introduction,[0],[0]
"We provide a streaming algorithm using only O(kd) space, which is the smallest possible, just enough to store the k eigenvectors of dimension d. To achieve an approximation error ε, the total number of samples we need is O(kd log d+",1. Introduction,[0],[0]
"1ε2 log(d log 1 ε )).
",1. Introduction,[0],[0]
"Related Works There is a huge literature on tensor decomposition, and it is beyond the scope of this paper to give a comprehensive survey.",1. Introduction,[0],[0]
"Thus, we only compare our results to the most related ones, particularly those based on the power method.",1. Introduction,[0],[0]
"While different works may focus on different aspects, we are most interested in understanding how the error parameter ε affects various performance measures, having in mind a small ε.
",1. Introduction,[0],[0]
"First, the batch algorithm of Anandkumar et al. (2014a), using a better analysis in (Wang & Anandkumar, 2016), runs in time about O((k2 log k)(log log 1ε )), which can be made to run in O(k(log log 1ε ))",1. Introduction,[0],[0]
"iterations in parallel, while ours are O(k log log 1ε ) and O(log log 1 ε ), respectively.",1. Introduction,[0],[0]
"On
the other hand, one advantage of their algorithm is that its running time does not depend on the eigengaps, while ours has the dependence hidden above as some constant.
",1. Introduction,[0],[0]
"In the streaming setting, Wang & Anandkumar (2016) provided an algorithm using O(dk log k) memory and O( kε2 log( d ε )) samples, while ours only uses O(dk) memory and O( 1ε2 log(d log log 1 ε ))",1. Introduction,[0],[0]
"samples.
",1. Introduction,[0],[0]
1,1. Introduction,[0],[0]
"Nevertheless, the sample complexity of Wang & Anandkumar (2016) is also independent of the eigengaps, while ours has the dependence hidden above as a constant factor.
",1. Introduction,[0],[0]
"As one can see, our algorithms, which find the k eigenvectors simultaneously, allow us to save a factor of k in the time complexity and the sample complexity, although our bounds may become worse when the eigengaps are small.",1. Introduction,[0],[0]
"Thus, our algorithms can be seen as new options for users to choose from, depending on the data they are given.
",1. Introduction,[0],[0]
"Although not directed related, let us also compare to previous works on SVD.",1. Introduction,[0],[0]
"Two related ones, both based on the simultaneous matrix power method, are the batch algorithm of (Hardt & Price, 2014) which converges in O(log dε ) iterations, and the streaming algorithm of (Li et al., 2016) which requires O( 1ε2 log(d log 1 ε ))",1. Introduction,[0],[0]
samples.,1. Introduction,[0],[0]
Both bounds are worse than ours and also depend on the eigengaps.,1. Introduction,[0],[0]
"Thus, although one approach for orthogonal tensor decomposition is to reduce it to a matrix SVD problem, this does not appear to result in better performance than ours.
",1. Introduction,[0],[0]
"Finally, comparisons of the tensor power method with other approaches can be found in works such as (Anandkumar et al., 2014a; Wang & Anandkumar, 2016).",1. Introduction,[0],[0]
"For example, the online SGD approach of (Ge et al., 2015) works only for tensors of even orders and its sample complexity has a poor dependency on the dimension d.
Organization of the paper.",1. Introduction,[0],[0]
"First, we provide some preliminaries in Section 2.",1. Introduction,[0],[0]
"Then we present our batch algorithm for orthogonal and symmetric tensors of order three in Section 3, and then for general orthogonal tensors in Section 4.",1. Introduction,[0],[0]
"In Section 5, we introduce our whitening procedure for nonorthogonal but symmetry tensors.",1. Introduction,[0],[0]
"Finally, in Section 6, we present our algorithm for the streaming setting.",1. Introduction,[0],[0]
"Due to the space limitation, we will move all our proofs to the appendix in the supplementary material.",1. Introduction,[0],[0]
Let us first introduce some notations and definitions which we will use later.,2. Preliminaries,[0],[0]
Let R denote the set of real numbers and N the set of positive integers.,2. Preliminaries,[0],[0]
"Let N (0, 1) denote the standard normal distribution with mean 0 and variance 1,
1We use a different input distribution from theirs.",2. Preliminaries,[0],[0]
"The bound listed here is modified from theirs according to our distribution.
and let N d(0, 1), for d ∈ N, denote the d-variate one which has each of its d dimensions sampled independently from N (0, 1).",2. Preliminaries,[0],[0]
"For d ∈ N, let [d] denote the set {1, . . .",2. Preliminaries,[0],[0]
", d}.",2. Preliminaries,[0],[0]
"For a vector x, let ‖x‖ denote its L2 norm.",2. Preliminaries,[0],[0]
"For d ∈ N, let Id denote the d × d identity matrix.",2. Preliminaries,[0],[0]
"For a matrix A ∈ Rd×k, let Ai, for i ∈",2. Preliminaries,[0],[0]
"[k], denote its i-th column, and let Ai,j , for j ∈",2. Preliminaries,[0],[0]
"[d], be the j-th entry of Ai.",2. Preliminaries,[0],[0]
"Moreover, for a matrix A, let A> denote its transpose, and define its norm as ‖A‖ = maxx∈Rk ‖A·x‖ ‖x‖ , using the convention that 0 0 = 0.
",2. Preliminaries,[0],[0]
"Tensors are the focus of our paper, which can be seen as generalization of matrices to higher orders.",2. Preliminaries,[0],[0]
"For simplicity of presentation, we will use symmetric tensors of order three as examples in the following definitions.",2. Preliminaries,[0],[0]
"A real tensor T of order three can be seen as an three-dimensional array in Rd×d×d, for some d ∈ N, with its (i, j, k)-th entry denoted as Ti,j,k.",2. Preliminaries,[0],[0]
"For such a tensor T and three matrices A ∈ Rd×m1 , B ∈ Rd×m2 , C ∈ Rd×m3 , let T (A,B,C) be the tensor in Rm1×m2×m3 , with its (a, b, c)th entry defined as ∑ i,j,k∈[d]",2. Preliminaries,[0],[0]
"Ti,j,kAa,iBb,jCc,k.",2. Preliminaries,[0],[0]
"The norm of a tensor T we will use is the operator norm: ‖T‖ = maxx,y,z∈Rd |T (x,y,z)| ‖x‖‖y‖‖z‖ .
",2. Preliminaries,[0],[0]
The tensor decomposition problem.,2. Preliminaries,[0],[0]
"In this problem, there is a tensor T with some unknown decomposition
T = ∑ i∈[d] λi · ui ⊗ ui ⊗ ui,
with λi ≥ 0 and ui ∈ Rd for any",2. Preliminaries,[0],[0]
i ∈,2. Preliminaries,[0],[0]
[d].,2. Preliminaries,[0],[0]
Then given some k ∈,2. Preliminaries,[0],[0]
"[d] and ε ∈ (0, 1), our goal is to find λ̂i and ûi with
|λ̂i − λi| ≤ ε and ‖ûi",2. Preliminaries,[0],[0]
"− ui‖ ≤ ε, for every i ∈",2. Preliminaries,[0],[0]
"[k].
We will assume that∑ i∈[d]",2. Preliminaries,[0],[0]
λi ≤ 1 and ∀i ∈,2. Preliminaries,[0],[0]
[k] : λi > λi+1.,2. Preliminaries,[0],[0]
"(1)
As in previous works, we consider a slightly harder version of the problem, in which we only have access to some noisy version of T , instead of the noiseless T .",2. Preliminaries,[0],[0]
We will consider the following two settings.,2. Preliminaries,[0],[0]
"In the batch setting, we have access to some T̄ = T + Φ for the whole time, for some perturbation tensor Φ. In the streaming setting, we have a stream of data points",2. Preliminaries,[0],[0]
"x1, x2, . . .",2. Preliminaries,[0],[0]
"arriving one by one, which provide the only information we have about T , with each xτ ∈ Rd allowing us to compute some T̄τ with mean E[T̄ ] = T .",2. Preliminaries,[0],[0]
"In this streaming setting, we are particularly interested in the case of a large d which prohibits one to store a tensor of size d3 in memory.
",2. Preliminaries,[0],[0]
Power Method: Matrices versus Tensors.,2. Preliminaries,[0],[0]
"Note that a tensor of order two is just a matrix, and a popular approach for decomposing matrices is the so-called power
method, which works as follows.",2. Preliminaries,[0],[0]
"Suppose we are given a d × d matrix M = ∑ i∈[d] λi · ui ⊗ ui, with nonnegative λ1 > λ2 ≥ · · · and orthonormal vectors u1, . . .",2. Preliminaries,[0],[0]
", ud.",2. Preliminaries,[0],[0]
"The power method starts with some q(0) = ∑ i∈[d] ci · ui, usually chosen randomly, and then repeatedly performs the update q(t) = M · q(t−1), which results in
q(t) = ∑ i∈[d]",2. Preliminaries,[0],[0]
λi ( u>i q,2. Preliminaries,[0],[0]
(t−1) ) · ui = ∑ i∈[d],2. Preliminaries,[0],[0]
"( λtici ) · ui.
Note that for any i 6= 1, as λi < λ1, the coefficient λtici will soon become much smaller than the coefficient λt1c1 if c1 is not too small, which is likely to happen for a randomly chosen q(0).",2. Preliminaries,[0],[0]
"This has the effect that after normalization, q(t)/‖q(t)‖ approaches u1 quickly.
",2. Preliminaries,[0],[0]
"Now consider a tensor T = ∑
i∈[d] λi · ui ⊗ ui ⊗ ui, with nonnegative λ1 > λ2 ≥ · · · and orthonormal vectors u1, . . .",2. Preliminaries,[0],[0]
", ud.",2. Preliminaries,[0],[0]
"The tensor version of the power method again starts from a randomly chosen q(0) =∑
i∈[d] ci · ui, but now repeatedly performs the update q(t)",2. Preliminaries,[0],[0]
"= T (Id, q (t−1), q(t−1)), which in turn results in
q(t) = ∑ i∈[d] λi ( u>i q",2. Preliminaries,[0],[0]
(,2. Preliminaries,[0],[0]
t−1) ),2. Preliminaries,[0],[0]
2 · ui = ∑ i∈[d],2. Preliminaries,[0],[0]
"λ−1i (λici) 2t · ui.
",2. Preliminaries,[0],[0]
"The coefficient of each ui now has a different form from the matrix case, and this leads to the following two effects.
",2. Preliminaries,[0],[0]
"First, one now has much less control on what q(t)/‖q(t)‖ converges to.",2. Preliminaries,[0],[0]
"In fact, it can converge to any ui 6= u1 if ui has the largest value of λi|ci|, which happens with a good probability if λi is not much smaller than λ1.",2. Preliminaries,[0],[0]
"Consequently, to find the top k vectors u1, . .",2. Preliminaries,[0],[0]
.,2. Preliminaries,[0],[0]
", uk, previous works based on the power method all need much more complicated procedures (Anandkumar et al., 2014a), compared to those for matrices, as discussed in the introduction.
",2. Preliminaries,[0],[0]
"On the other hand, the different form of q(t) has the beneficial effect that the convergence is now exponentially faster than in the matrix case.",2. Preliminaries,[0],[0]
"More precisely, if λi|ci| <",2. Preliminaries,[0],[0]
"λj |cj |, than the gap between the coefficients (λici)2 t and (λjcj)2 t is now amplified much faster.",2. Preliminaries,[0],[0]
We will show how to inherit this nice property of faster convergence but at the same time avoid the difficulty discussed above.,2. Preliminaries,[0],[0]
"In this section, we focus on the special case in which the tensors to be decomposed are orthogonal, symmetric, and of order three.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Formally, there is an underlying tensor
T = ∑ i∈[d] λi · ui ⊗ ui ⊗ ui,
Algorithm 1 Robust tensor power method Input: Tensor T̄ ∈",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Rd×d×d and parameters k, L, S,N .",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Initialization Phase: Sample w1, . . .",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
", wL, Y (0) 1 , . . .",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
", Y (0) k ∼ N d(0, 1).
",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Compute w̄ =,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"1L ∑
j∈[L] T̄ (Id, wj , wj).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Compute M̄ = T̄,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"(Id, Id, w̄).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Factorize Y (0) as Z(0) ·R(0) by QR decomposition.,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"for s = 1 to S do
Compute Y (s) = M̄ · Z(s−1).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Factorize Y (s) as Z(s) ·R(s) by QR decomposition.
end for Tensor Power Phase: Let Q(0) = Z(S).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"for t = 1 to N do
Compute Y (t)j = T̄",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"(Id, Q (t−1) j , Q (t−1) j ), ∀j ∈",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"[k].
",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Factorize Y (t) as Q(t) ·R(t) by QR decomposition.,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"end for Output: ûj = Q (N) j and λ̂j = T̄ (ûj , ûj , ûj), ∀j ∈",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"[k].
with orthonormal vectors ui’s and real λi’s satisfying the condition (1).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Then given k ∈,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"[d] and ε ∈ (0, 1), our goal is to find approximates to those λi and ui within distance ε, but we only have access to some noisy tensor T̄ = T",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
+,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Φ for some symmetric perturbation tensor Φ.
",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Our algorithm is given in Algorithm 1, which consists of two phases: the initialization phase and the tensor power phase.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"The main phase is the tensor power phase, which we will discuss in detail in Subsection 3.1.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"For our tensor power phase to work, it needs to have a good starting point.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"This is provided by the initialization phase, which we will discuss in detail in Subsection 3.2.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Through these two subsections, we will prove Theorem 1 below, which summarizes the performance of our algorithm, according to the following parameters of the tensor:
γ = min i∈[k] λ2i",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"− λ2i+1 λ2i and ∆ = min i∈[k] λi − λi+1 4 .
",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Theorem 1.,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
Suppose ε ≤,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
λk2,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
and the perturbation tensor has the bound ‖Φ‖ ≤,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"min{ ∆ε
2 √ k , ∆3d ,
α0∆ 2
√ dk } for a small enough constant α0.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Then for some L = O( 1γ2 log d), S =",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"O( 1γ log d), and N = O(log( 1 γ log 1 ε )), our Algorithm 1 with high probability will output ûi and λ̂i with ‖ûi−ui‖ ≤ ε",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
and |λ̂i − λi| ≤ ε for every,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
i ∈,3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"[k].
Let us make some remarks about the theorem.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"First, the L samples are used to compute w̄ and M̄ , which can be done in a parallel way.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Second, our parameter γ is related to a parameter γ′ = mini∈[k",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
],3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"λi−λi+1 λi
used in (Hardt & Price, 2014), and it is easy to verify that γ ≥ γ′. Thus, our algorithm for tensors converges in O( 1γ log d + log( 1 γ log 1 ε )) rounds, which is faster than the O( 1γ′ log d + 1 γ′ log 1 ε )
rounds of (Hardt & Price, 2014) for matrices.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Note that our dependence on the error parameter ε is exponentially smaller than that of (Hardt & Price, 2014), which means that for a small ε, we can decompose tensors much faster than matrices.",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"Finally, compared to previous works on tensors, our convergence time, for a small ε is about O(log log 1ε ) while those in (Anandkumar et al., 2014a; Wang & Anandkumar, 2016) are at least Ω(k log log 1ε ).",3. Orthogonal and Symmetric Tensors of Order Three,[0],[0]
"The tensor power phase of our Algorithm 1 is based on our version of the tensor power method, which works as follows.",3.1. Our Robust Tensor Power Method,[0],[0]
"At each step t, we maintain a d × k matrix Q(t) with columns Q(t)1 , . . .",3.1. Our Robust Tensor Power Method,[0],[0]
", Q (t) k",3.1. Our Robust Tensor Power Method,[0],[0]
"as our current estimators for u1, . .",3.1. Our Robust Tensor Power Method,[0],[0]
.,3.1. Our Robust Tensor Power Method,[0],[0]
", uk, which is obtained by updating the previous estimators with the following two operations.
",3.1. Our Robust Tensor Power Method,[0],[0]
"The main operation is to apply the noisy tensor T̄ on them simultaneously to get a d× k matrix Y (t) with its j-th column computed as Y (t)j = T̄ (Id, Q (t−1) j , Q (t−1) j ), which equals ∑ i∈[d]",3.1. Our Robust Tensor Power Method,[0],[0]
"λi ( u>i Q (t−1) j )2 · ui + Φ̂(t)j ,
for Φ̂(t)j = Φ(Id, Q (t−1) j , Q (t−1) j ).",3.1. Our Robust Tensor Power Method,[0],[0]
"This implies that
∀i ∈",3.1. Our Robust Tensor Power Method,[0],[0]
[d] : u>i Y (t) j = λi,3.1. Our Robust Tensor Power Method,[0],[0]
"( u>i Q (t−1) j )2 + u>i Φ̂ (t) j , (2)
which shows the progress made by this operation.
",3.1. Our Robust Tensor Power Method,[0],[0]
"The second operation is to orthogonalize Y (t) as
Y (t) = Q(t) ·R(t),
by the QR decomposition via the Gram-Schmidt process, to obtain a d× k matrix Q(t) with columns Q(t)1 , . . .",3.1. Our Robust Tensor Power Method,[0],[0]
", Q (t) k , which then become our new estimators.",3.1. Our Robust Tensor Power Method,[0],[0]
"As we will show in Lemma 1 below, given a small enough ‖Φ‖, if we start with a full-rank Q(0), then each Q(t) also has full rank and consists of orthonormal columns, and each R(t) is invertible.",3.1. Our Robust Tensor Power Method,[0],[0]
"Moreover, although we apply the QR decomposition on the whole matrix Y (t) to obtain the matrix Q(t), it has the effect that for any m ∈",3.1. Our Robust Tensor Power Method,[0],[0]
"[k], the first m columns of Q(t) can be seen as obtained from the first m columns of Y (t) by a QR decomposition.",3.1. Our Robust Tensor Power Method,[0],[0]
This property is needed in our Lemma 1 and Theorem 2 below to guarantee the simultaneous convergence of Q(t)i to ui for every,3.1. Our Robust Tensor Power Method,[0],[0]
i ∈,3.1. Our Robust Tensor Power Method,[0],[0]
"[k].
Before stating Lemma 1 which guarantees the progress we make at each step, let us prepare some notations first.",3.1. Our Robust Tensor Power Method,[0],[0]
For a d × k matrix A and some m ∈,3.1. Our Robust Tensor Power Method,[0],[0]
"[k], let A[m] denote the d ×m matrix containing the first m columns of A.",3.1. Our Robust Tensor Power Method,[0],[0]
Let U denote the d× k matrix with the target vector ui as its i’th column.,3.1. Our Robust Tensor Power Method,[0],[0]
For a d× k matrix Q and some m ∈,3.1. Our Robust Tensor Power Method,[0],[0]
"[k], define
cosm(Q) =",3.1. Our Robust Tensor Power Method,[0],[0]
"min y∈Rm
∥∥∥U>[m] ·Q[m] · y∥∥∥ /‖Q[m] ·",3.1. Our Robust Tensor Power Method,[0],[0]
"y‖,
which equals the cosine of the m’th principal angle between the column spaces of U[m] and Q[m], let sinm(Q) =√
1− cos2m(Q), and let us use as the error measure
tanm(Q) = sinm(Q)/ cosm(Q).
",3.1. Our Robust Tensor Power Method,[0],[0]
"More information about the principal angles can be found in, e.g., (Golub & Van Loan, 1996).",3.1. Our Robust Tensor Power Method,[0],[0]
"Then we have the following lemma, which we prove in Appendix B.1.
Lemma 1.",3.1. Our Robust Tensor Power Method,[0],[0]
Fix any m ∈,3.1. Our Robust Tensor Power Method,[0],[0]
[k] and t ≥ 0.,3.1. Our Robust Tensor Power Method,[0],[0]
"Let Φ̂(t)[m] denote the d×m matrix with Φ̂(t)j = Φ(Id, Q (t−1) j , Q (t−1) j ) as its j’th column, and suppose∥∥∥Φ̂(t)[m]∥∥∥ < ∆ ·min{β, cos2m(Q(t−1))} , (3) for some β > 0.",3.1. Our Robust Tensor Power Method,[0],[0]
"Then for ρ = maxi∈[k](
λi+1 λi ) 1 4 , we have
tanm(Q (t))",3.1. Our Robust Tensor Power Method,[0],[0]
"≤ max { β,max{β, ρ} · tan2m(Q(t−1)) } .
",3.1. Our Robust Tensor Power Method,[0],[0]
"Observe that the guarantee provided by the lemma above has a similar form as that in (Hardt & Price, 2014) for matrices.",3.1. Our Robust Tensor Power Method,[0],[0]
"The main difference is that here in the tensor case, we have the error measure essentially squared after each step, which has the following two implications.",3.1. Our Robust Tensor Power Method,[0],[0]
"First, to guarantee that the error is indeed reduced, we need tanm(Q
(t−1)) to be small enough (say, less than one), unlike in the matrix case.",3.1. Our Robust Tensor Power Method,[0],[0]
"Next, if we indeed have a small enough tanm(Q(t−1)), then the error can be reduced in a much faster rate than in the matrix case.",3.1. Our Robust Tensor Power Method,[0],[0]
"Another difference is that here we provide the guarantee for all the k submatrices Q(t)[m], for m ∈",3.1. Our Robust Tensor Power Method,[0],[0]
"[k], instead of just one matrix Q
(t).",3.1. Our Robust Tensor Power Method,[0],[0]
This allows us to show the simultaneous convergence of each column Q(t)i to the target vector ui for every i ∈,3.1. Our Robust Tensor Power Method,[0],[0]
"[k], as given in the following, which we prove in Appendix B.2.
",3.1. Our Robust Tensor Power Method,[0],[0]
Theorem 2.,3.1. Our Robust Tensor Power Method,[0],[0]
"For any ε ∈ (0, λk2 ), there exists some N ≤",3.1. Our Robust Tensor Power Method,[0],[0]
O(log( 1γ log 1 ε )),3.1. Our Robust Tensor Power Method,[0],[0]
such that the following holds.,3.1. Our Robust Tensor Power Method,[0],[0]
Suppose the perturbation is bounded by ‖Φ‖ ≤,3.1. Our Robust Tensor Power Method,[0],[0]
∆ε 2 √ k,3.1. Our Robust Tensor Power Method,[0],[0]
and we start from some initial Q(0) with tanm Q(0) ≤ 1 for every m ∈,3.1. Our Robust Tensor Power Method,[0],[0]
[k].,3.1. Our Robust Tensor Power Method,[0],[0]
"Then for any t ≥ N , with ûi = Q(t)i and λ̂i = T̄ (ûi, ûi, ûi), we have ‖ui − ûi‖ ≤ ε",3.1. Our Robust Tensor Power Method,[0],[0]
"and |λi − λ̂i| ≤ ε, for every i ∈",3.1. Our Robust Tensor Power Method,[0],[0]
"[k].
Note that the convergence rate guaranteed by the theorem above is exponentially faster than that in (Hardt & Price, 2014) for matrices, assuming that we indeed can have such a good initial Q(0) to start with.",3.1. Our Robust Tensor Power Method,[0],[0]
"In the next subsection, we show how it can be found efficiently.",3.1. Our Robust Tensor Power Method,[0],[0]
"Our approach for finding a good initialization is to project the tensor down to a matrix and apply the matrix power
method for only a few steps just to make the tangents less than one.",3.2. Initialization Procedure,[0],[0]
"Although we could continue applying the matrix power method till reaching the much smaller target bound ε, this would take exponentially longer than by switching to the tensor power method as we actually do.
",3.2. Initialization Procedure,[0],[0]
"As mentioned above, we would first like to project the tensor T̄ down to a matrix.",3.2. Initialization Procedure,[0],[0]
"A naive approach is to sample a random vector w̄ and take the matrix T̄ (Id, Id, w̄)",3.2. Initialization Procedure,[0],[0]
"≈ T (Id, Id, w̄) = ∑ i∈[d]",3.2. Initialization Procedure,[0],[0]
λi(u > i w̄) · ui ⊗ ui.,3.2. Initialization Procedure,[0],[0]
"However, this may mess up the gaps between eigenvalues, which are needed to guarantee the convergence rate of the matrix power method.",3.2. Initialization Procedure,[0],[0]
"The reason is that as each u>i w̄ has mean zero, the coefficient λi(u>i w̄) also has mean zero and thus has a good chance of coming very close to others.",3.2. Initialization Procedure,[0],[0]
"To preserve the gaps, we would like to have u>i w̄ ≥ u>i+1w̄ for each i with high probability.",3.2. Initialization Procedure,[0],[0]
"To achieve this, let us first imagine sampling a random w ∈ Rd from N d(0, 1), and computing the vector w̄ = T̄ (Id, w, w), which is close to
T (Id, w, w) = ∑ i∈[d]",3.2. Initialization Procedure,[0],[0]
"λi(u > i w) 2 · ui.
Then one can show that for every i, E[(u>i w)2] = 1, so that E[w̄] ≈ E[T (Id, w, w)]",3.2. Initialization Procedure,[0],[0]
"= ∑ i∈[d] λiui, and
u>i",3.2. Initialization Procedure,[0],[0]
E[w̄] ≈ λi >,3.2. Initialization Procedure,[0],[0]
λi+1,3.2. Initialization Procedure,[0],[0]
"≈ u>i+1 E[w̄].
",3.2. Initialization Procedure,[0],[0]
"However, we want the gap-preserving guarantee to be in high probability, instead in expectation.",3.2. Initialization Procedure,[0],[0]
"Thus we go further by sampling not just one, but some number L of vectors w1, . . .",3.2. Initialization Procedure,[0],[0]
", wL independently from the distribution N d(0, 1), and then taking the average
w̄",3.2. Initialization Procedure,[0],[0]
"= 1
L ∑ j∈[L] T̄ (Id, wj , wj).",3.2. Initialization Procedure,[0],[0]
"(4)
The following lemma shows that such a w̄ is likely to have u>i w̄",3.2. Initialization Procedure,[0],[0]
"≈ λi, which we prove in Appendix B.3.",3.2. Initialization Procedure,[0],[0]
Lemma 2.,3.2. Initialization Procedure,[0],[0]
Suppose we have T̄ =,3.2. Initialization Procedure,[0],[0]
T + Φ with ‖Φ‖ ≤ ∆3d .,3.2. Initialization Procedure,[0],[0]
Then for some L ≤,3.2. Initialization Procedure,[0],[0]
"O( 1γ2 log d), the vector w̄ computed according to (4) with high probability satisfies∣∣u>i w̄",3.2. Initialization Procedure,[0],[0]
− λi∣∣ ≤ 14 (λiγ + 2∆) for every i ∈,3.2. Initialization Procedure,[0],[0]
"[k]. (5) With this w̄, we compute the matrix M̄ = T̄",3.2. Initialization Procedure,[0],[0]
"(Id, Id, w̄).",3.2. Initialization Procedure,[0],[0]
"As shown by the following lemma, which we prove in Appendix B.4, M̄ is close to a matrix with ui’s as eigenvectors and good gaps between eigenvalues.
",3.2. Initialization Procedure,[0],[0]
Lemma 3.,3.2. Initialization Procedure,[0],[0]
Suppose we have T̄ =,3.2. Initialization Procedure,[0],[0]
T + Φ.,3.2. Initialization Procedure,[0],[0]
"Then for any w̄ satisfying the condition (5) in Lemma 2, the matrix M̄ = T̄",3.2. Initialization Procedure,[0],[0]
"(Id, Id, w̄) can be decomposed as
M̄ = ∑ i∈[d]",3.2. Initialization Procedure,[0],[0]
"λ̄i · ui ⊗ ui + Φ̄,
for some λ̄i’s with λ̄i",3.2. Initialization Procedure,[0],[0]
"− λ̄i+1 ≥ ∆2, for i ∈",3.2. Initialization Procedure,[0],[0]
"[k], and Φ̄ = Φ(Id, Id, w̄) with ‖Φ̄‖ ≤ 2‖Φ‖.
With such a matrix M̄ , we next apply the matrix power method of (Hardt & Price, 2014) to find good approximates to its eigenvectors.",3.2. Initialization Procedure,[0],[0]
"More precisely, we sample an initial matrix Y (0) ∈ Rd×k by choosing each of its column independently according to the distribution N d(0, 1), and factorize it as Y (0) = Z(0) · R(0) by QR decomposition via the Gram-Schmidt process.",3.2. Initialization Procedure,[0],[0]
"Then at step s ≥ 1, we multiply the previous estimate Z(s−1) by M̄ to obtain Y (s) = M̄ · Z(s−1), factorize it as Y (s) = Z(s) ·",3.2. Initialization Procedure,[0],[0]
"R(s) by QR decomposition via the Gram-Schmidt process, and then take the orthonormal Z(s) as the new estimate.",3.2. Initialization Procedure,[0],[0]
The following lemma shows the number of steps needed to find a good enough Z(s).,3.2. Initialization Procedure,[0],[0]
Lemma 4.,3.2. Initialization Procedure,[0],[0]
Suppose we are given a matrix M̄,3.2. Initialization Procedure,[0],[0]
"having the decomposition described in Lemma 3, with ‖Φ̄‖ ≤ 2α0∆
2 √ dk
for a small enough constant α0.",3.2. Initialization Procedure,[0],[0]
Then there exists some S ≤,3.2. Initialization Procedure,[0],[0]
"O( 1γ log d) such that with high probability, we have tanm(Z (s))",3.2. Initialization Procedure,[0],[0]
≤ 1 for every m ∈,3.2. Initialization Procedure,[0],[0]
"[k] whenever s ≥ S.
This together with the previous two lemmas guarantee that given T̄ = T +Φ, with ‖Φ‖ ≤ min{ ∆3d , α0∆ 2",3.2. Initialization Procedure,[0],[0]
"√ dk
}, for a small enough constant α0, we can obtain with high probability a good Z(S) which can be used as the initial Q(0) for our tensor power phase.",3.2. Initialization Procedure,[0],[0]
"Combining this with Theorem 2 in the previous subsection, we then have our Theorem 1 given at the beginning of the section.",3.2. Initialization Procedure,[0],[0]
"In the previous section, we consider tensors which are orthogonal, symmetric, and of order three.",4. General Orthogonal Tensors,[0],[0]
"In this section, we show how to extend our results for general orthogonal tensors, to deal with higher orders first and then asymmetry.",4. General Orthogonal Tensors,[0],[0]
"To handle orthogonal and symmetric tensors of any order, only the initialization procedure needs to be modified.",4.1. Higher-Order Tensors,[0],[0]
"First, for tensors of any odd order, a straightforward modification is as follows.",4.1. Higher-Order Tensors,[0],[0]
Take for example a tensor of order 2r + 1.,4.1. Higher-Order Tensors,[0],[0]
"Now we simply compute
w̄",4.1. Higher-Order Tensors,[0],[0]
"= 1
L ∑ j∈[L] T̄ (Id, wj , . . .",4.1. Higher-Order Tensors,[0],[0]
", wj),
with 2r copies of wj , and similarly to Lemma 2, one can show that w̄ is likely to be close to the vector ∑ i∈[d] λi ·ui.",4.1. Higher-Order Tensors,[0],[0]
"With such a vector w̄, one can show that the matrix
M̄ = T̄ (Id, Id, w̄, . . .",4.1. Higher-Order Tensors,[0],[0]
", w̄), with 2r − 1 copies of w̄, is close to the matrix ∑
i∈[d]",4.1. Higher-Order Tensors,[0],[0]
"λ 2r i ·
ui ⊗ ui, similarly to Lemma 3.",4.1. Higher-Order Tensors,[0],[0]
"Then the rest is the same
as that in the previous section.",4.1. Higher-Order Tensors,[0],[0]
Note that this approach has the eigenvalues decreased exponentially in r.,4.1. Higher-Order Tensors,[0],[0]
"A different approach avoiding this is to compute M̄ directly as
M̄ = 1
L ∑ j∈[L] ( T̄ (Id, Id, wj , . .",4.1. Higher-Order Tensors,[0],[0]
.,4.1. Higher-Order Tensors,[0],[0]
", wj) )2 ,
which is close to
1
L ∑ j∈[L] (T (Id, Id, wj , . . .",4.1. Higher-Order Tensors,[0],[0]
", wj))",4.1. Higher-Order Tensors,[0],[0]
"2
= 1
L ∑ j∈[L] ∑ i∈[d]",4.1. Higher-Order Tensors,[0],[0]
"λ2i ( u>i wj )4r−2 · ui ⊗ ui =
∑ i∈[d]",4.1. Higher-Order Tensors,[0],[0]
λ2i 1 L ∑ j∈[L] ( u>i wj )4r−2 · ui ⊗ ui.,4.1. Higher-Order Tensors,[0],[0]
Then one can again show that such a matrix M̄ is likely to be close to the matrix ∑ i∈[d],4.1. Higher-Order Tensors,[0],[0]
"λ 2 i · ui ⊗ ui.
To handle tensors of even orders, the initialization is slightly different but the idea is similar.",4.1. Higher-Order Tensors,[0],[0]
"Given a tensor of order 2r, we again sample vectors w1, . . .",4.1. Higher-Order Tensors,[0],[0]
", wL as before, but now we compute the matrix directly as
M̄ = 1
L ∑ j∈[L] T̄ (Id, Id, wj , . . .",4.1. Higher-Order Tensors,[0],[0]
", wj),
with 2r − 2 copies of wj .",4.1. Higher-Order Tensors,[0],[0]
"As before, one can show that the matrix M̄ is likely to be close to the matrix ∑ i∈[d] λi · ui⊗ui.",4.1. Higher-Order Tensors,[0],[0]
Then again we can apply the matrix power method on M̄ and obtain a good initialization for the tensor power method as before.,4.1. Higher-Order Tensors,[0],[0]
"Note that now the eigenvalues are no longer squared, and the previous requirement on ‖Φ‖ can be slightly relaxed, with the dependence on ∆2 being replaced by ∆.",4.1. Higher-Order Tensors,[0],[0]
"For simplicity of presentation, let us focus on the third order case; the extension to higher orders is straightforward.",4.2. Asymmetric Tensors,[0],[0]
"That is, now the underlying tensor has the form
T = ∑ i∈[d] λi · ai ⊗ bi ⊗ ci,
with nonnegative λi’s satisfying the condition (1), together with three sets of orthonormal vectors of ai’s, bi’s, and ci’s.",4.2. Asymmetric Tensors,[0],[0]
"As before, we only have access to a noisy version T̄ of T .
",4.2. Asymmetric Tensors,[0],[0]
"The main modification of our algorithm is again to the initialization procedure, but the idea is also similar.",4.2. Asymmetric Tensors,[0],[0]
"To find a good initial matrix A for ai’s, we sample w1, . . .",4.2. Asymmetric Tensors,[0],[0]
", wL independently from N d(0, 1), and now compute the matrix
M̄",4.2. Asymmetric Tensors,[0],[0]
"= 1
L ∑ j∈[L] ( T̄ (Id, Id, wj) )",4.2. Asymmetric Tensors,[0],[0]
"( T̄ (Id, Id, wj) )> .
",4.2. Asymmetric Tensors,[0],[0]
"As before, it is not hard to show that
M̄",4.2. Asymmetric Tensors,[0],[0]
≈ ∑ i∈[d],4.2. Asymmetric Tensors,[0],[0]
1 L ∑ j∈[L] λ2i,4.2. Asymmetric Tensors,[0],[0]
"( c>i wj )2 · ai ⊗ ai, which is close to the matrix ∑ i∈[d]",4.2. Asymmetric Tensors,[0],[0]
λ 2 i · ai ⊗ ai with high probability.,4.2. Asymmetric Tensors,[0],[0]
"From the matrix M̄ , we can again apply the matrix power method to find a good initial matrix A. Similarly, we can find good initial matrices B and C for bi’s and ci’s, respectively.
",4.2. Asymmetric Tensors,[0],[0]
"Next, with such matrices, we would like to apply the tensor power method, which we modify as follows.",4.2. Asymmetric Tensors,[0],[0]
"Now at each step t, we take previous estimates A(t−1), B(t−1), C(t−1), and compute X(t)i = T̄ (Id, B (t−1) i , C (t−1) i ), Y (t)",4.2. Asymmetric Tensors,[0],[0]
i = T̄,4.2. Asymmetric Tensors,[0],[0]
(A (t−1),4.2. Asymmetric Tensors,[0],[0]
"i , Id, C (t−1) i ), Z (t) i = T̄",4.2. Asymmetric Tensors,[0],[0]
"(A (t−1) i , B (t−1)",4.2. Asymmetric Tensors,[0],[0]
"i , Id), for i ∈",4.2. Asymmetric Tensors,[0],[0]
"[k], followed by orthonormalizing X(t), Y (t), Z(t) to obtain the new estimates A(t), B(t), C(t) via QRdecomposition.",4.2. Asymmetric Tensors,[0],[0]
It is not hard to show that the resulting algorithm has a similar convergence rate as our Algorithm 1.,4.2. Asymmetric Tensors,[0],[0]
"In the previous section, we consider general orthogonal tensors, which can be asymmetric.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"In this section, we consider non-orthogonal tensors which are symmetric.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"We remark that for some latent variable models such as the multi-view model, the corresponding asymmetric tensors can be converted into symmetric ones (Anandkumar et al., 2014a), so that our result here can still be applied.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"For simplicity of exposition, let us again focus on the case of order three, so that the given tensor has the form T =∑
i∈[d] λi ·vi⊗vi⊗vi, but the vectors vi’s are no longer assumed to be orthogonal to each other.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Still we assume them to be linearly independent, and we again assume without loss of generality that ‖T‖ ≤ 1 and ‖vi‖",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"= 1 for each i. In addition, let us assume, as in previous works, that λj = 0 for j ≥ k + 1.2
Following (Anandkumar et al., 2014a), we would like to whiten such a tensor T into an orthogonal one, so that we can then apply our Algorithm 1.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"More precisely, our goal is to find a d× k matrix W such that the tensor T (W,W,W ) becomes orthogonal.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"As in (Anandkumar et al., 2014a), assume that we also have available a matrix
M = ∑ i∈[k] λi · vi ⊗ vi.3
Then for a whitening matrix, it suffices to find some W
2This assumption is not necessary.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
We assume it just to simplify the first step of our algorithm given below.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Without it, we can simply replace that step by the matrix power method used in our Algorithm 1, which takes more steps but can still do the job.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"3More generally, the weights λi in M are allowed to differ from those in T , but for simplicity we assume they are the same.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
such that W>MW = Ik.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"The reason is that
Ik = W >MW = ∑ i∈[k] λi · ( W>vi ) ⊗",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"( W>vi ) ,
which implies that the vectors √ λiW
>vi, for i ∈",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"[k], are orthonormal.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Then the tensor T (W,W,W ) equals∑
i∈[k] λi · (W>vi)⊗3 = ∑ i∈[k] 1√ λi · (√ λiW >vi )⊗3 ,
which has an orthogonal decomposition.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"According to (Anandkumar et al., 2014a), one way to find such a W is to do the spectral decomposition of M as UΛU>, with eigenvectors as columns of U , and let W = UΛ− 1 2 .",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"However, we will not take this approach, because finding a good approximate to U by the matrix power method would take longer to converge than the tensor power method which we will later apply to the whitened tensor.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Our key observation is that it suffices to find a d×k matrix Q such that the matrix P = Q>MQ is invertible, since we can then let W = QP− 1 2 and have
W>MW = P− 1 2Q>MQP− 1 2 = Ik.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"With such a W , the tensor T (W,W,W ) becomes orthogonal, so that we can decompose it4 to obtain σi = 1√λi and ui = √ λiW
>vi, from which we can recover λi = 1σ2i and
vi = σiQP 1 2ui if Q has orthonormal columns.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"As before, we consider a similar setting in which we only have access to a noisy M̄",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"= M + Φ̄, for some symmetric perturbation matrix Φ̄, in addition to the noisy tensor T̄ =",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"T+Φ. Then our algorithm for finding the whitening matrix consists of the following two steps:
1.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Sample a random matrix Z ∈ Rd×k with orthonormal columns, compute Ȳ = M̄Z, and factorize it as Ȳ = QR̄ by a QR decomposition.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
2.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Compute P̄ = Q>M̄Q and output W̄ = QP̄− 1 2 as
the whitening matrix.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
We analyze our algorithm in the following.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"First note that Q is computed in the same way as we compute Z(1) in Algorithm 1, and with λk+1 = 0",5. Nonorthogonal but Symmetric Tensors,[0],[0]
we are likely to have tank(Q),5. Nonorthogonal but Symmetric Tensors,[0],[0]
≈ 0,5. Nonorthogonal but Symmetric Tensors,[0],[0]
so that the matrix P = Q>MQ is invertible.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Formally, we have the following, which we prove in Appendix C.1.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
Lemma 5.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
Suppose ‖Φ̄‖ ≤ α0λk√ dk for a small enough constant α0.,5. Nonorthogonal but Symmetric Tensors,[0],[0]
Then with high probability we have σmax(P ),5. Nonorthogonal but Symmetric Tensors,[0],[0]
≤,5. Nonorthogonal but Symmetric Tensors,[0],[0]
λ1 and σmin(P ) ≥,5. Nonorthogonal but Symmetric Tensors,[0],[0]
"λk2 .
4To apply our Algorithm 1, we need to scale it properly, say by a factor of √ λk/k to make its norm at most one.
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Next, with a small enough ‖Φ̄‖, if P is invertible, then so is P̄ , and moreover, we have P̄− 1 2 ≈ P− 12 .",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"This is shown in the following, which we prove in Appendix C.2.
Lemma 6.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Fix any ∈ (0, 1) and suppose we have σmin(P ) ≥ 2 and ‖Φ̄‖ ≤ .",5. Nonorthogonal but Symmetric Tensors,[0],[0]
Then P̄ is invertible and ‖P̄− 12 − P− 12 ‖ ≤ 2 (σmin(P ))−2(σmax(P )),5. Nonorthogonal but Symmetric Tensors,[0],[0]
"1 2 .
",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Then, with a good P̄− 1 2 , we can obtain a good W̄ and have T̄ (W̄ , W̄ , W̄ )",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"close to T (W,W,W ) which has an orthogonal decomposition.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"This is shown in the following, which we prove in Appendix C.3.
Theorem 3.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Fix any ε ∈ (0, λk4 ) and suppose we have ‖Φ‖ ≤ α0λ 3 2
k ε and ‖Φ̄‖ ≤ α0εmin{ λk√ dk , λ3k√ λ1 }, for a
small enough constant α0.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"Then with high probability we have ‖T̄ (W̄ , W̄ , W̄ )",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"− T (W,W,W )‖ ≤ ε.",5. Nonorthogonal but Symmetric Tensors,[0],[0]
"In the previous sections, we consider the batch setting in which the tensor T̄ is assumed to be stored somewhere which can be accessed whenever we want to.",6. Streaming setting,[0],[0]
"However, storing such a tensor, say of order three, requires a space complexity of Ω(d3), which becomes impractical even for a moderate value of d. In this section, we study the possibility of achieving a space complexity of O(kd), which is the least amount of memory needed just to store the k vectors in Rd.",6. Streaming setting,[0],[0]
"More precisely, we consider the streaming setting, in which there is a stream of vectors x1, x2, . . .",6. Streaming setting,[0],[0]
arriving one at a time.,6. Streaming setting,[0],[0]
"We assume that each vector x is sampled independently from some distribution over Rd, with ‖x‖ ≤ 1 and some function g : Rd → Rd×d×d such that
• E[g(x)]",6. Streaming setting,[0],[0]
"= T , and given x, u, v ∈ Rd, g(x)(Id, u, v) can be computed in O(d) space.
",6. Streaming setting,[0],[0]
"Such a function g is known to exist for some latent variable models (Ge et al., 2015; Wang & Anandkumar, 2016).",6. Streaming setting,[0],[0]
"Given such a function, our algorithms in previous sections can all be converted to work in the streaming setting using O(kd) space.",6. Streaming setting,[0],[0]
"This is because all our operations involving tensors have the form T̄ (Id, u, v), for some u, v ∈ Rd, which can be realized as(
1 |J | ∑ t∈J g(xt)
)",6. Streaming setting,[0],[0]
"(Id, u, v)",6. Streaming setting,[0],[0]
"= 1
|J | ∑ t∈J (g(xt) (Id, u, v)) ,
for a collection J of samples, with the righthand side above clearly computable in O(kd) space.5",6. Streaming setting,[0],[0]
"Then depending on the distance we want between T̄ = 1|J| ∑ t∈J g(xt) and T ,
5This also includes the initialization phase in which we now do not store the matrix M̄ explicitly but instead replace the operation M̄Zi by T̄ (Id, Zi, w̄).
",6. Streaming setting,[0],[0]
we can choose a proper size for J .,6. Streaming setting,[0],[0]
"In fact, to save the total number of samples, we can follow the approach of (Li et al., 2016) by choosing different sizes in different iterations of the matrix or tensor power method.
",6. Streaming setting,[0],[0]
"Following (Wang & Anandkumar, 2016), let us take the specific case with g(x) = x⊗ x⊗ x",6. Streaming setting,[0],[0]
as a concrete example and focus on the orthogonal case studied in Section 3; it is not hard to convert other algorithms of ours to the streaming setting.,6. Streaming setting,[0],[0]
"One can show that in this specific case, we have E[x] = ∑ i∈[d] λiui so that there is a more efficient way to find a vector w̄ for producing the matrix M̄ in the initialization phase.",6. Streaming setting,[0],[0]
"Formally, we have the following lemma, which we prove in Appendix D.1.
",6. Streaming setting,[0],[0]
Lemma 7.,6. Streaming setting,[0],[0]
"There is an algorithm using O(d) space and O( log k∆2 ) samples to find some w̄ ∈ R
d satisfying the condition (5) in Lemma 2 with high probability.
",6. Streaming setting,[0],[0]
"With such a vector w̄, we can then use the streaming algorithm of (Li et al., 2016) to find a good initial matrix Z for the later tensor power phase.",6. Streaming setting,[0],[0]
"Formally, we have the following lemma, which we prove in Appendix D.2.
",6. Streaming setting,[0],[0]
Lemma 8.,6. Streaming setting,[0],[0]
"Given w̄ from Lemma 7, we can use O(kd) space and O(kd log d γ
∆4γ ) samples to find some Z ∈ R d×k
with tanm(Z) < 1, for any m ∈",6. Streaming setting,[0],[0]
"[k], with high probability.
",6. Streaming setting,[0],[0]
"Having such a matrix Z, we can proceed to the tensor power phase.",6. Streaming setting,[0],[0]
"Borrowing again the idea from (Li et al., 2016), let us partition the incoming data into blocks of increasing sizes, with the t’th block Jt used to carry out one tensor power iteration Y (t)i = T̄ (t)(Id, Q (t−1) i , Q (t−1) i ), for
i ∈",6. Streaming setting,[0],[0]
"[k], of Algorithm 1, with T̄ (t) = 1|Jt| ∑ τ∈Jt g(xτ ).",6. Streaming setting,[0],[0]
"Instead of preparing this T̄ (t) and then computing each Y (t)i , we now go through |Jt| steps of updates:
•",6. Streaming setting,[0],[0]
For τ ∈,6. Streaming setting,[0],[0]
Jt do: Y (t)i = Y (t),6. Streaming setting,[0],[0]
i + 1 |Jt| (x > τ,6. Streaming setting,[0],[0]
"Q (t−1) i ) 2xτ .
",6. Streaming setting,[0],[0]
The block sizes are chosen carefully to keep ‖T̄ (t),6. Streaming setting,[0],[0]
− T‖ small enough so that we can have tanm(Q(t)) decreased in a desirable rate.,6. Streaming setting,[0],[0]
"Here, we choose the parameters
βt = max { ρ2 t−1, ε
2
} and |Jt| = c0 log(dt)
∆2β2t , (6)
for a large enough constant c0, to make the condition (3) in Lemma 1 hold with high probability so that we have tanm(Q
(t))",6. Streaming setting,[0],[0]
≤ βt.,6. Streaming setting,[0],[0]
"In Appendix D.3, we summarize our algorithm and prove the following theorem.
",6. Streaming setting,[0],[0]
Theorem 4.,6. Streaming setting,[0],[0]
"Given ε ∈ (0, λk2 ), with high probability we can find λ̂i, ûi with |λ̂i − λi|, ‖ûi",6. Streaming setting,[0],[0]
"− ui‖ ≤ ε, for any i ∈",6. Streaming setting,[0],[0]
"[k], using O(kd) space and O(kd log d γ
∆4γ + log(d log( 1γ log 1 ε ))
∆2γε2 ) samples.",6. Streaming setting,[0],[0]
"Tensor decomposition is an important problem with many applications across several disciplines, and a popular approach for this problem is the tensor power method.",abstractText,[0],[0]
"However, previous works with theoretical guarantee based on this approach can only find the top eigenvectors one after one, unlike the case for matrices.",abstractText,[0],[0]
"In this paper, we show how to find the eigenvectors simultaneously with the help of a new initialization procedure.",abstractText,[0],[0]
"This allows us to achieve a better running time in the batch setting, as well as a lower sample complexity in the streaming setting.",abstractText,[0],[0]
Tensor Decomposition via Simultaneous Power Iteration,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1103–1114 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Multimodal sentiment analysis (Morency et al., 2011; Zadeh et al., 2016b; Poria et al., 2015) is an increasingly popular area of affective computing research (Poria et al., 2017) that focuses on generalizing text-based sentiment analysis to opinionated videos, where three communicative modalities are present: language (spoken words), visual (gestures), and acoustic (voice).
",1 Introduction,[0],[0]
"This generalization is particularly vital to part of the NLP community dealing with opinion mining and sentiment analysis (Cambria et al., 2017) since there is a growing trend of sharing opinions in videos instead of text, specially in social media (Facebook, YouTube, etc.).",1 Introduction,[0],[0]
"The central challenge in multimodal sentiment analysis is to model the inter-modality dynamics: the interactions between
† means equal contribution
language, visual and acoustic behaviors that change the perception of the expressed sentiment.
",1 Introduction,[0],[0]
Figure 1 illustrates these complex inter-modality dynamics.,1 Introduction,[0],[0]
"The utterance “This movie is sick” can be ambiguous (either positive or negative) by itself, but if the speaker is also smiling at the same time, then it will be perceived as positive.",1 Introduction,[0],[0]
"On the other hand, the same utterance with a frown would be perceived negatively.",1 Introduction,[0],[0]
A person speaking loudly “This movie is sick” would still be ambiguous.,1 Introduction,[0],[0]
These examples are illustrating bimodal interactions.,1 Introduction,[0],[0]
Examples of trimodal interactions are shown in Figure 1 when loud voice increases the sentiment to strongly positive.,1 Introduction,[0],[0]
"The complexity of inter-modality dynamics is shown in the second trimodal example where the utterance “This movie is fair” is still weakly positive, given the strong influence of the word “fair”.
",1 Introduction,[0],[0]
A second challenge in multimodal sentiment analysis is efficiently exploring intra-modality dynamics of a specific modality (unimodal interaction).,1 Introduction,[0],[0]
"Intra-modality dynamics are particularly
1103
challenging for the language analysis since multimodal sentiment analysis is performed on spoken language.",1 Introduction,[0],[0]
A spoken opinion such as “I think it was alright . . .,1 Introduction,[0],[0]
Hmmm . . .,1 Introduction,[0],[0]
let me think . . .,1 Introduction,[0],[0]
yeah . . .,1 Introduction,[0],[0]
no . . .,1 Introduction,[0],[0]
ok yeah”,1 Introduction,[0],[0]
almost never happens in written text.,1 Introduction,[0],[0]
"This volatile nature of spoken opinions, where proper language structure is often ignored, complicates sentiment analysis.",1 Introduction,[0],[0]
"Visual and acoustic modalities also contain their own intra-modality dynamics which are expressed through both space and time.
",1 Introduction,[0],[0]
"Previous works in multimodal sentiment analysis does not account for both intra-modality and intermodality dynamics directly, instead they either perform early fusion (a.k.a., feature-level fusion) or late fusion (a.k.a., decision-level fusion).",1 Introduction,[0],[0]
"Early fusion consists in simply concatenating multimodal features mostly at input level (Morency et al., 2011; Pérez-Rosas et al., 2013; Poria et al., 2016).",1 Introduction,[0],[0]
This fusion approach does not allow the intra-modality dynamics to be efficiently modeled.,1 Introduction,[0],[0]
This is due to the fact that inter-modality dynamics can be more complex at input level and can dominate the learning process or result in overfitting.,1 Introduction,[0],[0]
"Late fusion, instead, consists in training unimodal classifiers independently and performing decision voting (Wang et al., 2016; Zadeh et al., 2016a).",1 Introduction,[0],[0]
"This prevents the model from learning inter-modality dynamics in an efficient way by assuming that simple weighted averaging is a proper fusion approach.
",1 Introduction,[0],[0]
"In this paper, we introduce a new model, termed Tensor Fusion Network (TFN), which learns both the intra-modality and inter-modality dynamics end-to-end.",1 Introduction,[0],[0]
"Inter-modality dynamics are modeled with a new multimodal fusion approach, named Tensor Fusion, which explicitly aggregates unimodal, bimodal and trimodal interactions.",1 Introduction,[0],[0]
"Intramodality dynamics are modeled through three Modality Embedding Subnetworks, for language, visual and acoustic modalities, respectively.
",1 Introduction,[0],[0]
"In our extensive set of experiments, we show (a) that TFN outperforms previous state-of-the-art approaches for multimodal sentiment analysis, (b) the characteristics and capabilities of our Tensor Fusion approach for multimodal sentiment analysis, and (c) that each of our three Modality Embedding Subnetworks (language, visual and acoustic) are also outperforming unimodal state-of-the-art unimodal sentiment analysis approaches.",1 Introduction,[0],[0]
"Sentiment Analysis is a well-studied research area in NLP (Pang et al., 2008).",2 Related Work,[0],[0]
"Various approaches have been proposed to model sentiment from language, including methods that focus on opinionated words (Hu and Liu, 2004; Taboada et al., 2011; Poria et al., 2014b; Cambria et al., 2016), n-grams and language models (Yang and Cardie, 2012), sentiment compositionality and dependency-based analysis (Socher et al., 2013; Poria et al., 2014a; Agarwal et al., 2015; Tai et al., 2015), and distributional representations for sentiment (Iyyer et al., 2015).
",2 Related Work,[0],[0]
Multimodal Sentiment Analysis is an emerging research area that integrates verbal and nonverbal behaviors into the detection of user sentiment.,2 Related Work,[0],[0]
"There exist several multimodal datasets that include sentiment annotations, including the newly-introduced CMU-MOSI dataset (Zadeh et al., 2016b), as well as other datasets including ICT-MMMO (Wöllmer et al., 2013), YouTube (Morency et al., 2011), and MOUD (Pérez-Rosas et al., 2013), however CMUMOSI is the only English dataset with utterancelevel sentiment labels.",2 Related Work,[0],[0]
"The newest multimodal sentiment analysis approaches have used deep neural networks, including convolutional neural networks (CNNs) with multiple-kernel learning (Poria et al., 2015), SAL-CNN (Wang et al., 2016) which learns generalizable features across speakers, and support vector machines (SVMs) with a multimodal dictionary (Zadeh, 2015).
",2 Related Work,[0],[0]
"Audio-Visual Emotion Recognition is closely tied to multimodal sentiment analysis (Poria et al., 2017).",2 Related Work,[0],[0]
"Both audio and visual features have been shown to be useful in the recognition of emotions (Ghosh et al., 2016a).",2 Related Work,[0],[0]
"Using facial expressions and audio cues jointly has been the focus of many recent studies (Glodek et al., 2011; Valstar et al., 2016; Nojavanasghari et al., 2016).
",2 Related Work,[0],[0]
Multimodal Machine Learning has been a growing trend in machine learning research that is closely tied to the studies in this paper.,2 Related Work,[0],[0]
"Creative and novel applications of using multiple modalities have been among successful recent research directions in machine learning (You et al., 2016; Donahue et al., 2015; Antol et al., 2015; Specia et al., 2016; Tong et al., 2017).",2 Related Work,[0],[0]
"Multimodal Opinion Sentiment Intensity (CMUMOSI) dataset is an annotated dataset of video
0 100
200
300
400
500
600
Highly Negative Negative Weakly Negative Neutral Weakly Positive Positive Highly Positive
0% 10% 20% 30% 40% 50% 60% 70% 80% 90%
100%
5 10 15 20 25 30 35
Pe rc
en ta
ge o
f S en
tim en
t D eg
re es
Utterance Size
Highly Positive
Positive
Weakly Positive
Neutral
Weakly Negative
Negative
Highly Negative
N um
be r o
f O pi
ni on
S eg
m en
ts
Figure 2: Distribution of sentiment across different opinions (left) and opinion sizes (right) in CMU-MOSI.
opinions from YouTube movie reviews (Zadeh et al., 2016a).",3 CMU-MOSI Dataset,[0],[0]
"Annotation of sentiment has closely followed the annotation scheme of the Stanford Sentiment Treebank (Socher et al., 2013), where sentiment is annotated on a seven-step Likert scale from very negative to very positive.",3 CMU-MOSI Dataset,[0],[0]
"However, whereas the Stanford Sentiment Treebank is segmented by sentence, the CMU-MOSI dataset is segmented by opinion utterances to accommodate spoken language where sentence boundaries are not as clear as text.",3 CMU-MOSI Dataset,[0],[0]
There are 2199 opinion utterances for 93 distinct speakers in CMU-MOSI.,3 CMU-MOSI Dataset,[0],[0]
There are an average 23.2 opinion segments in each video.,3 CMU-MOSI Dataset,[0],[0]
Each video has an average length of 4.2 seconds.,3 CMU-MOSI Dataset,[0],[0]
"There are a total of 26,295 words in the opinion utterances.",3 CMU-MOSI Dataset,[0],[0]
These utterance are annotated by five Mechanical Turk annotators for sentiment.,3 CMU-MOSI Dataset,[0],[0]
The final agreement between the annotators is high in terms of Krippendorf’s alpha α = 0.77.,3 CMU-MOSI Dataset,[0],[0]
Figure 2 shows the distribution of sentiment across different opinions and different opinion sizes.,3 CMU-MOSI Dataset,[0],[0]
"CMU-MOSI dataset facilitates three prediction tasks, each of which we address in our experiments: 1) Binary Sentiment Classification 2) Five-Class Sentiment Classification (similar to Stanford Sentiment Treebank fine-grained classification with seven scale being mapped to five) and 3) Sentiment Regression in range [−3, 3].",3 CMU-MOSI Dataset,[0],[0]
"For sentiment regression, we report Mean-Absolute Error (lower is better) and correlation (higher is better) between the model predictions and regression ground truth.",3 CMU-MOSI Dataset,[0],[0]
"Our proposed TFN consists of three major components: 1) Modality Embedding Subnetworks take as input unimodal features, and output a rich modality embedding.",4 Tensor Fusion Network,[0],[0]
2),4 Tensor Fusion Network,[0],[0]
"Tensor Fusion Layer explicitly models the unimodal, bimodal and trimodal interactions using a 3-fold Cartesian product from modality embeddings.",4 Tensor Fusion Network,[0],[0]
"3) Sentiment Inference Subnetwork is a
network conditioned on the output of the Tensor Fusion Layer and performs sentiment inference.",4 Tensor Fusion Network,[0],[0]
"Depending on the task from Section 3 the network output changes to accommodate binary classification, 5-class classification or regression.",4 Tensor Fusion Network,[0],[0]
"Input to the TFN is an opinion utterance which includes three modalities of language, visual and acoustic.",4 Tensor Fusion Network,[0],[0]
The following three subsections describe the TFN subnetworks and their inputs in detail.,4 Tensor Fusion Network,[0],[0]
"Spoken Language Embedding Subnetwork: Spoken text is different than written text (reviews, tweets) in compositionality and grammar.",4.1 Modality Embedding Subnetworks,[0],[0]
We revisit the spoken opinion: “I think it was alright . . .,4.1 Modality Embedding Subnetworks,[0],[0]
Hmmm . . .,4.1 Modality Embedding Subnetworks,[0],[0]
let me think . . .,4.1 Modality Embedding Subnetworks,[0],[0]
yeah . . .,4.1 Modality Embedding Subnetworks,[0],[0]
no . . .,4.1 Modality Embedding Subnetworks,[0],[0]
ok yeah”.,4.1 Modality Embedding Subnetworks,[0],[0]
This form of opinion rarely happens in written language but variants of it are very common in spoken language.,4.1 Modality Embedding Subnetworks,[0],[0]
The first part conveys the actual message and the rest is speaker thinking out loud eventually agreeing with the first part.,4.1 Modality Embedding Subnetworks,[0],[0]
"The key factor in dealing with this volatile nature of spoken language is to build models that are capable of operating in presence of unreliable and idiosyncratic speech traits by focusing on important parts of speech.
",4.1 Modality Embedding Subnetworks,[0],[0]
Our proposed approach to deal with challenges of spoken language is to learn a rich representation of spoken words at each word interval and use it as input to a fully connected deep network (Figure 3).,4.1 Modality Embedding Subnetworks,[0],[0]
"This rich representation for ith word contains information from beginning of utterance through time, as well as ith word.",4.1 Modality Embedding Subnetworks,[0],[0]
"This way as the model is discovering the meaning of the utterance through time, if it encounters unusable information in word i+ 1 and arbitrary number of words after, the representation up until i is not diluted or lost.",4.1 Modality Embedding Subnetworks,[0],[0]
"Also, if the model encounters usable information again, it can recover by embedding those in the long short-term memory (LSTM).",4.1 Modality Embedding Subnetworks,[0],[0]
"The time-dependent
encodings are usable by the rest of the pipeline by simply focusing on relevant parts using the nonlinear affine transformation of time-dependent embeddings which can act as a dimension reducing attention mechanism.",4.1 Modality Embedding Subnetworks,[0],[0]
"To formally define our proposed Spoken Language Embedding Subnetwork (Ul), let l = {l1, l2, l3, . . .",4.1 Modality Embedding Subnetworks,[0],[0]
", lTl ; lt ∈ R300}, where Tl is the number of words in an utterance, be the set of spoken words represented as a sequence of 300-dimensional GloVe word vectors (Pennington et al., 2014).
",4.1 Modality Embedding Subnetworks,[0],[0]
"A LSTM network (Hochreiter and Schmidhuber, 1997) with a forget gate (Gers et al., 2000) is used to learn time-dependent language representations",4.1 Modality Embedding Subnetworks,[0],[0]
"hl = {h1, h2, h3, . . .",4.1 Modality Embedding Subnetworks,[0],[0]
", hTl ;ht ∈ R128} for words according to the following LSTM formulation.
",4.1 Modality Embedding Subnetworks,[0],[0]
i f o,4.1 Modality Embedding Subnetworks,[0],[0]
m  =  sigmoid sigmoid sigmoid tanh,4.1 Modality Embedding Subnetworks,[0],[0]
"Wld (XtWleht−1 )
ct = f ct−1",4.1 Modality Embedding Subnetworks,[0],[0]
+,4.1 Modality Embedding Subnetworks,[0],[0]
i m ht = o⊗ tanh(ct) hl,4.1 Modality Embedding Subnetworks,[0],[0]
=,4.1 Modality Embedding Subnetworks,[0],[0]
[h1;h2;h3; . . .,4.1 Modality Embedding Subnetworks,[0],[0]
";hTl ]
hl is a matrix of language representations formed from concatenation of h1, h2, h3, . . .",4.1 Modality Embedding Subnetworks,[0],[0]
hTl .,4.1 Modality Embedding Subnetworks,[0],[0]
"hl is then used as input to a fully-connected network that generates language embedding zl:
zl = Ul(l; Wl) ∈ R128
where Wl is the set of all weights in the Ul network (including Wld , Wle ,Wlfc , and blfc), σ is the sigmoid function.
",4.1 Modality Embedding Subnetworks,[0],[0]
"Visual Embedding Subnetwork: Since opinion videos consist mostly of speakers talking to the audience through close-up camera, face is the most important source of visual information.",4.1 Modality Embedding Subnetworks,[0],[0]
"The speaker’s face is detected for each frame (sampled at 30Hz) and indicators of the seven basic emotions
(anger, contempt, disgust, fear, joy, sadness, and surprise) and two advanced emotions (frustration and confusion) (Ekman, 1992) are extracted using FACET facial expression analysis framework1.",4.1 Modality Embedding Subnetworks,[0],[0]
"A set of 20 Facial Action Units (Ekman et al., 1980), indicating detailed muscle movements on the face, are also extracted using FACET.",4.1 Modality Embedding Subnetworks,[0],[0]
"Estimates of head position, head rotation, and 68 facial landmark locations also extracted per frame using OpenFace (Baltrušaitis et al., 2016; Zadeh et al., 2017).
",4.1 Modality Embedding Subnetworks,[0],[0]
Let the visual features v̂j =,4.1 Modality Embedding Subnetworks,[0],[0]
"[v1j , v 2 j , v 3 j , . . .",4.1 Modality Embedding Subnetworks,[0],[0]
", v p j ] for frame j of utterance video contain the set of p visual features, with Tv the number of total video frames in utterance.",4.1 Modality Embedding Subnetworks,[0],[0]
We perform mean pooling over the frames to obtain the expected visual features v =,4.1 Modality Embedding Subnetworks,[0],[0]
"[E[v1],E[v2],E[v3], . . .",4.1 Modality Embedding Subnetworks,[0],[0]
",E[vl]].",4.1 Modality Embedding Subnetworks,[0],[0]
v is then used as input to the Visual Embedding Subnetwork Uv.,4.1 Modality Embedding Subnetworks,[0],[0]
"Since information extracted using FACET from videos is rich, using a deep neural network would be sufficient to produce meaningful embeddings of visual modality.",4.1 Modality Embedding Subnetworks,[0],[0]
We use a deep neural network with three hidden layers of 32 ReLU units and weights Wv.,4.1 Modality Embedding Subnetworks,[0],[0]
Empirically we observed that making the model deeper or increasing the number of neurons in each layer does not lead to better visual performance.,4.1 Modality Embedding Subnetworks,[0],[0]
"The subnetwork output provides the visual embedding zv:
zv = Uv(v; Wv) ∈",4.1 Modality Embedding Subnetworks,[0],[0]
"R32
Acoustic Embedding Subnetwork: For each opinion utterance audio, a set of acoustic features are extracted using COVAREP acoustic analysis framework (Degottex et al., 2014), including 12 MFCCs, pitch tracking and Voiced/UnVoiced segmenting features (using the additive noise robust Summation of Residual Harmonics (SRH) method (Drugman and Alwan, 2011)), glottal source parameters (estimated by glottal inverse filtering based on GCI synchronous IAIF (Drugman et al., 2012; Alku, 1992; Alku et al., 2002, 1997; Titze and Sundberg, 1992; Childers and Lee, 1991)), peak slope parameters (Degottex et al., 2014), maxima dispersion quotients (MDQ) (Kane and Gobl, 2013), and estimations of the Rd shape parameter of the Liljencrants-Fant (LF) glottal model (Fujisaki and Ljungqvist, 1986).",4.1 Modality Embedding Subnetworks,[0],[0]
"These extracted features capture different characteristics of human voice and have been shown to be related to emotions (Ghosh et al., 2016b).
",4.1 Modality Embedding Subnetworks,[0],[0]
"1http://goo.gl/1rh1JN
For each opinion segment with Ta audio frames (sampled at 100Hz; i.e., 10ms), we extract the set of q acoustic features âj",4.1 Modality Embedding Subnetworks,[0],[0]
=,4.1 Modality Embedding Subnetworks,[0],[0]
"[a1j , a 2 j , a 3 j , . . .",4.1 Modality Embedding Subnetworks,[0],[0]
", a q j ] for audio frame j in utterance.",4.1 Modality Embedding Subnetworks,[0],[0]
"We perform mean pooling per utterance on these extracted acoustic features to obtain the expected acoustic features a = [E[a1],E[a2],E[a3], . . .",4.1 Modality Embedding Subnetworks,[0],[0]
",E[q]].",4.1 Modality Embedding Subnetworks,[0],[0]
"Here, a is the input to the Audio Embedding Subnetwork Ua.",4.1 Modality Embedding Subnetworks,[0],[0]
"Since COVAREP also extracts rich features from audio, using a deep neural network is sufficient to model the acoustic modality.",4.1 Modality Embedding Subnetworks,[0],[0]
"Similar to Uv, Ua is a network with 3 layers of 32 ReLU units with weights Wa.
",4.1 Modality Embedding Subnetworks,[0],[0]
"Here, we also empirically observed that making the model deeper or increasing the number of neurons in each layer does not lead to better performance.",4.1 Modality Embedding Subnetworks,[0],[0]
"The subnetwork produces the audio embedding za:
za = Ua(a;Wa) ∈",4.1 Modality Embedding Subnetworks,[0],[0]
R32,4.1 Modality Embedding Subnetworks,[0],[0]
"While previous works in multimodal research has used feature concatenation as an approach for multimodal fusion, we aim to build a fusion layer in TFN that disentangles unimodal, bimodal and trimodal dynamics by modeling each of them explicitly.",4.2 Tensor Fusion Layer,[0],[0]
"We call this layer Tensor Fusion, which is defined as the following vector field using three-fold Cartesian product:{
(zl, zv, za) | zl ∈",4.2 Tensor Fusion Layer,[0],[0]
"[ zl
1
] , zv ∈",4.2 Tensor Fusion Layer,[0],[0]
"[ zv
1
] , za ∈",4.2 Tensor Fusion Layer,[0],[0]
"[ za
1
]}
The extra constant dimension with value 1 generates the unimodal and bimodal dynamics.",4.2 Tensor Fusion Layer,[0],[0]
"Each neural coordinate (zl, zv, za) can be seen as a 3-D point in the 3-fold Cartesian space defined by the language, visual, and acoustic embeddings dimensions [zl1]T , [zv1]T , and [za1]T .
",4.2 Tensor Fusion Layer,[0],[0]
"This definition is mathematically equivalent to a differentiable outer product between zl, the visual representation zv, and the acoustic representation za.
",4.2 Tensor Fusion Layer,[0],[0]
zm =,4.2 Tensor Fusion Layer,[0],[0]
"[ zl
1
] ⊗",4.2 Tensor Fusion Layer,[0],[0]
"[ zv
1
] ⊗ [ za
1 ] Here⊗ indicates the outer product between vectors and zm ∈ R129×33×33 is the 3D cube of all possible combination of unimodal embeddings with seven semantically distinct subregions in Figure 4.",4.2 Tensor Fusion Layer,[0],[0]
"The first three subregions zl, zv, and za are unimodal embeddings from Modality Embedding Subnetworks forming unimodal interactions in Tensor Fusion.",4.2 Tensor Fusion Layer,[0],[0]
"Three subregions zl ⊗ zv, zl ⊗ za, and zv ⊗ za capture bimodal interactions in Tensor Fusion.",4.2 Tensor Fusion Layer,[0],[0]
"Finally, zl ⊗ zv ⊗ za captures trimodal interactions.
",4.2 Tensor Fusion Layer,[0],[0]
"Early fusion commonly used in multimodal research dealing with language, vision and audio, can be seen as a special case of Tensor Fusion with only unimodal interactions.",4.2 Tensor Fusion Layer,[0],[0]
"Since Tensor Fusion is mathematically formed by an outer product, it has no learnable parameters and we empirically observed that although the output tensor is high dimensional, chances of overfitting are low.
",4.2 Tensor Fusion Layer,[0],[0]
"We argue that this is due to the fact that the output neurons of Tensor Fusion are easy to interpret and semantically very meaningful (i.e., the manifold that they lie on is not complex but just high dimensional).",4.2 Tensor Fusion Layer,[0],[0]
"Thus, it is easy for the subsequent layers of the network to decode the meaningful information.",4.2 Tensor Fusion Layer,[0],[0]
"After Tensor Fusion layer, each opinion utterance can be represented as a multimodal tensor zm.",4.3 Sentiment Inference Subnetwork,[0],[0]
We use a fully connected deep neural network called Sentiment Inference Subnetwork Us with weights,4.3 Sentiment Inference Subnetwork,[0],[0]
Ws conditioned on zm.,4.3 Sentiment Inference Subnetwork,[0],[0]
The architecture of the network consists of two layers of 128 ReLU activation units connected to decision layer.,4.3 Sentiment Inference Subnetwork,[0],[0]
"The likelihood function of the Sentiment Inference Subnetwork is defined as follows, where φ is the sentiment prediction:
arg max φ p(φ | zm;Ws) = arg max φ Us(zm;Ws)
",4.3 Sentiment Inference Subnetwork,[0],[0]
"In our experiments, we use three variations of the Us network.",4.3 Sentiment Inference Subnetwork,[0],[0]
"The first network is trained for binary sentiment classification, with a single sigmoid output neuron using binary cross-entropy loss.",4.3 Sentiment Inference Subnetwork,[0],[0]
"The second network is designed for five-class sentiment classification, and uses a softmax probability function using categorical cross-entropy loss.",4.3 Sentiment Inference Subnetwork,[0],[0]
"The third network uses a single sigmoid output, using meansquarred error loss to perform sentiment regression.",4.3 Sentiment Inference Subnetwork,[0],[0]
"In this paper, we devise three sets of experiments each addressing a different research question:
Experiment 1: We compare our TFN with previous state-of-the-art approaches in multimodal sentiment analysis.
",5 Experiments,[0],[0]
Experiment 2: We study the importance of the TFN subtensors and the impact of each individual modality (see Figure 4).,5 Experiments,[0],[0]
"We also compare with the commonly-used early fusion approach.
",5 Experiments,[0],[0]
"Experiment 3: We compare the performance of our three modality-specific networks (language, visual and acoustic) with state-of-the-art unimodal approaches.
",5 Experiments,[0],[0]
Section 5.4 describes our experimental methodology which is kept constant across all experiments.,5 Experiments,[0],[0]
Section 6 will discuss our results in more details with a qualitative analysis.,5 Experiments,[0],[0]
"In this section, we compare the performance of TFN model with previously proposed multimodal sentiment analysis models.",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"We compare to the following baselines:
C-MKL (Poria et al., 2015)",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
Convolutional MKL-based model is a multimodal sentiment classification model which uses a CNN to extract textual features and uses multiple kernel learning for sentiment analysis.,5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"It is current SOTA (state of the art) on CMU-MOSI.
SAL-CNN (Wang et al., 2016)",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
Select-Additive Learning is a multimodal sentiment analysis model that attempts to prevent identity-dependent information from being learned in a deep neural network.,5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"We retrain the model for 5-fold cross-validation using the code provided by the authors on github.
",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"SVM-MD (Zadeh et al., 2016b) is a SVM model trained on multimodal features using early fusion.",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"The model used in (Morency et al., 2011) and (Pérez-Rosas et al., 2013) also similarly use SVM on multimodal concatenated features.",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
"We also present the results of Random Forest RF-MD to compare to another non-neural approach.
",5.1 E1: Multimodal Sentiment Analysis,[0],[0]
The results first experiment are reported in Table 1.,5.1 E1: Multimodal Sentiment Analysis,[0],[0]
TFN outperforms previously proposed neural and non-neural approaches.,5.1 E1: Multimodal Sentiment Analysis,[0],[0]
This difference is specifically visible in the case of 5-class classification.,5.1 E1: Multimodal Sentiment Analysis,[0],[0]
Table 4 shows the results of our ablation study.,5.2 E2: Tensor Fusion Evaluation,[0],[0]
"The first three rows are showing the performance of each modality, when no intermodality dynamics are modeled.",5.2 E2: Tensor Fusion Evaluation,[0],[0]
"From this first experiment, we observe that the language modality is the most predictive.
",5.2 E2: Tensor Fusion Evaluation,[0],[0]
"As a second set of ablation experiments, we test our TFN approach when only the bimodal subtensors are used (TFNbimodal) or when only the trimodal subtensor is used (TFNbimodal).",5.2 E2: Tensor Fusion Evaluation,[0],[0]
We observe that bimodal subtensors are more informative when used without other subtensors.,5.2 E2: Tensor Fusion Evaluation,[0],[0]
The most interesting comparison is between our full TFN model and a variant (TFNnotrimodal) where the trimodal subtensor is removed (but all the unimodal and bimodal subtensors are present).,5.2 E2: Tensor Fusion Evaluation,[0],[0]
"We observe a big improvement for the full TFN model, confirming the importance of the trimodal dynamics and the need for all components of the full tensor.
",5.2 E2: Tensor Fusion Evaluation,[0],[0]
"We also perform a comparison with the early fusion approach (TFNearly) by simply concatenating all three modality embeddings < zl, za, zv > and passing it directly as input to Us.",5.2 E2: Tensor Fusion Evaluation,[0],[0]
This approach was depicted on the left side of Figure 4.,5.2 E2: Tensor Fusion Evaluation,[0],[0]
"When looking at Table 4 results, we see that our TFN approach outperforms the early fusion approach2.",5.2 E2: Tensor Fusion Evaluation,[0],[0]
"In this experiment, we compare the performance of our Modality Embedding Networks with stateof-the-art approaches for language-based, visualbased and acoustic-based sentiment analysis.",5.3 E3: Modality Embedding Subnetworks Evaluation,[0],[0]
"We selected the following state-of-the-art approaches to include variety in their techniques,
2We also performed other comparisons with variants of the early fusion model TFNearly where we increased the number of parameters and neurons to replicate the numbers from our TFN model.",5.3.1 Language Sentiment Analysis,[0],[0]
"In all cases, the performances were similar to TFNearly (and lower than our TFN model).",5.3.1 Language Sentiment Analysis,[0],[0]
"Because of space constraints, we could not include them in this paper.
",5.3.1 Language Sentiment Analysis,[0],[0]
"based on dependency parsing (RNTN), distributional representation of text (DAN), and convolutional approaches (DynamicCNN).",5.3.1 Language Sentiment Analysis,[0],[0]
"When possible, we retrain them on the CMU-MOSI dataset (performances of the original pre-trained models are shown in parenthesis in Table 3) and compare them to our language only TFNlanguage.
",5.3.1 Language Sentiment Analysis,[0],[0]
"RNTN (Socher et al., 2013)The Recursive Neural Tensor Network is among the most well-known sentiment analysis methods proposed for both binary and multi-class sentiment analysis that uses dependency structure.
",5.3.1 Language Sentiment Analysis,[0],[0]
"DAN (Iyyer et al., 2015)",5.3.1 Language Sentiment Analysis,[0],[0]
"The Deep Average Network approach is a simple but efficient sentiment analysis model that uses information only from distributional representation of the words and not from the compositionality of the sentences.
",5.3.1 Language Sentiment Analysis,[0],[0]
"DynamicCNN (Kalchbrenner et al., 2014) DynamicCNN is among the state-of-the-art models in text-based sentiment analysis which uses a convolutional architecture adopted for the semantic modeling of sentences.
",5.3.1 Language Sentiment Analysis,[0],[0]
"CMK-L, SAL-CNN-L and SVM-MD-L are multimodal models from section using only language modality 5.1.
",5.3.1 Language Sentiment Analysis,[0],[0]
Results in Table 3 show that our model using only language modality outperforms state-of-theart approaches for the CMU-MOSI dataset.,5.3.1 Language Sentiment Analysis,[0],[0]
"While previous models are well-studied and suitable models for sentiment analysis in written language, they underperform in modeling the sentiment in spoken language.",5.3.1 Language Sentiment Analysis,[0],[0]
"We suspect that this underperformance is due to: RNTN and similar approaches rely heavily on dependency structure, which may not be present
in spoken language; DAN and similar sentence embeddings approaches can easily be diluted by words that may not relate directly to sentiment or meaning; D-CNN and similar convolutional approaches rely on spatial proximity of related words, which may not always be present in spoken language.",5.3.1 Language Sentiment Analysis,[0],[0]
"We compare the performance of our models using visual information (TFNvisual) with the following well-known approaches in visual sentiment analysis and emotion recognition (retrained for sentiment analysis):
3DCNN (Byeon and Kwak, 2014)",5.3.2 Visual Sentiment Analysis,[0],[0]
a network using 3D CNN is trained using the face of the speaker.,5.3.2 Visual Sentiment Analysis,[0],[0]
"Face of the speaker is extracted in every 6 frames and resized to 64× 64 and used as the input to the proposed network.
",5.3.2 Visual Sentiment Analysis,[0],[0]
"CNN-LSTM (Ebrahimi Kahou et al., 2015) is a recurrent model that at each timestamp performs convolutions over facial region and uses output to an LSTM.",5.3.2 Visual Sentiment Analysis,[0],[0]
"Face processing is similar to 3DCNN.
",5.3.2 Visual Sentiment Analysis,[0],[0]
"LSTM-FA similar to both baselines above, information extracted by FACET is used every 6 frames as input to an LSTM with a memory dimension of 100 neurons.
SAL-CNN-V, SVM-MD-V, CMKL-V, RF-V use only visual modality in multimodal baselines from Section 5.1.
",5.3.2 Visual Sentiment Analysis,[0],[0]
The results in Table 5 show that Uv is able to outperform state-of-the-art approaches on visual sentiment analysis.,5.3.2 Visual Sentiment Analysis,[0],[0]
"We compare the performance of our models using visual information (TFNacoustic) with the following well-known approaches in audio sentiment analysis
and emotion recognition (retrained for sentiment analysis):
HL-RNN (Lee and Tashev, 2015) uses an LSTM on high-level audio features.",5.3.3 Acoustic Sentiment Analysis,[0],[0]
"We use the same features extracted for Ua averaged over time slices of every 200 intervals.
",5.3.3 Acoustic Sentiment Analysis,[0],[0]
"Adieu-Net (Trigeorgis et al., 2016) is an endto-end approach for emotion recognition in audio using directly PCM features.
",5.3.3 Acoustic Sentiment Analysis,[0],[0]
"SER-LSTM (Lim et al., 2016) is a model that uses recurrent neural networks on top of convolution operations on spectrogram of audio.
SAL-CNN-A, SVM-MD-A, CMKL-A, RF-A use only acoustic modality in multimodal baselines from Section 5.1.",5.3.3 Acoustic Sentiment Analysis,[0],[0]
"All the models in this paper are tested using five-fold cross-validation proposed by CMUMOSI (Zadeh et al., 2016a).",5.4 Methodology,[0],[0]
"All of our experiments are performed independent of speaker identity, as no speaker is shared between train and test sets for generalizability of the model to unseen speakers in real-world.",5.4 Methodology,[0],[0]
The best hyperparameters are chosen using grid search based on model performance on a validation set (using last 4 videos in train fold).,5.4 Methodology,[0],[0]
"The TFN model is trained using the Adam optimizer (Kingma and Ba, 2014) with the learning rate 5e4.",5.4 Methodology,[0],[0]
"Uv and Ua, Us subnetworks are regularized using dropout on all hidden layers with p = 0.15 and L2 norm coefficient 0.01.",5.4 Methodology,[0],[0]
"The train, test and validation folds are exactly the same for all baselines.",5.4 Methodology,[0],[0]
"We analyze the impact of our proposed TFN multimodal fusion approach by comparing it with the
early fusion approach TFNearly and the three unimodal models.",6 Qualitative Analysis,[0],[0]
Table 6 shows examples taken from the CMU-MOSI dataset.,6 Qualitative Analysis,[0],[0]
Each example is described with the spoken words as well as the acoustic and visual behaviors.,6 Qualitative Analysis,[0],[0]
"The sentiment predictions and the ground truth labels range between strongly negative (-3) and strongly positive (+3).
",6 Qualitative Analysis,[0],[0]
"As a first general observation, we observe that the early fusion model TFNearly shows a strong preference for the language modality and seems to be neglecting the intermodality dynamics.",6 Qualitative Analysis,[0],[0]
We can see this trend by comparing it with the language unimodal model TFNlanguage.,6 Qualitative Analysis,[0],[0]
"In comparison, our TFN approach seems to capture more complex interaction through bimodal and trimodal dynamics and thus performs better.",6 Qualitative Analysis,[0],[0]
"Specifically, in the first example, the utterance is weakly negative where the speaker is referring to lack of funny jokes in the movie.",6 Qualitative Analysis,[0],[0]
"This example contains a bimodal interaction where the visual modality shows a negative expression (frowning) which is correctly captured by our TFN approach.
",6 Qualitative Analysis,[0],[0]
"In the second example, the spoken words are ambiguous since the model has no clue what a B is except a token, but the acoustic and visual modalities are bringing complementary evidences.",6 Qualitative Analysis,[0],[0]
Our TFN approach correctly identify this trimodal interaction and predicts a positive sentiment.,6 Qualitative Analysis,[0],[0]
"The third example is interesting since it shows an interaction where language predicts a positive sentiment
but the strong negative visual behaviors bring the final prediction of our TFN approach almost to a neutral sentiment.",6 Qualitative Analysis,[0],[0]
The fourth example shows how the acoustic modality is also influencing our TFN predictions.,6 Qualitative Analysis,[0],[0]
"We introduced a new end-to-end fusion method for sentiment analysis which explicitly represents unimodal, bimodal, and trimodal interactions between behaviors.",7 Conclusion,[0],[0]
Our experiments on the publiclyavailable CMU-MOSI dataset produced state-ofthe-art performance when compared against both multimodal approaches.,7 Conclusion,[0],[0]
"Furthermore, our approach brings state-of-the-art results for languageonly, visual-only and acoustic-only multimodal sentiment analysis on CMU-MOSI.",7 Conclusion,[0],[0]
This project was partially supported by Oculus research grant.,Acknowledgments,[0],[0]
We would like to thank the reviewers for their valuable feedback.,Acknowledgments,[0],[0]
"Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language.",abstractText,[0],[0]
"In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics.",abstractText,[0],[0]
"We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end.",abstractText,[0],[0]
The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice.,abstractText,[0],[0]
"In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.",abstractText,[0],[0]
Tensor Fusion Network for Multimodal Sentiment Analysis,title,[0],[0]
"Nowadays, the Recurrent Neural Network (RNN), especially its more advanced variants such as the LSTM and the GRU, belong to the most successful machine learning approaches when it comes to sequence modeling.",1. Introduction,[0],[0]
"Especially in Natural Language Processing (NLP), great improvements have been achieved by exploiting these Neu-
1Ludwig Maximilian University of Munich, Germany 2Siemens AG, Corporate Technology, Germany.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yinchong Yang <yinchong.yang@siemens.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
ral Network architectures.,1. Introduction,[0],[0]
"This success motivates efforts to also apply these RNNs to video data, since a video clip could be seen as a sequence of image frames.",1. Introduction,[0],[0]
"However, plain RNN models turn out to be impractical and difficult to train directly on video data due to the fact that each image frame typically forms a relatively high-dimensional input, which makes the weight matrix mapping from the input to the hidden layer in RNNs extremely large.",1. Introduction,[0],[0]
"For instance, in case of an RGB video clip with a frame size of say 160×120×3, the input vector for the RNN would already be 57, 600 at each time step.",1. Introduction,[0],[0]
"In this case, even a small hidden layer consisting of only 100 hidden nodes would lead to 5,760,000 free parameters, only considering the inputto-hidden mapping in the model.
",1. Introduction,[0],[0]
"In order to circumvent this problem, state-of-the-art approaches often involve pre-processing each frame using Convolution Neural Networks (CNN), a Neural Network model proven to be most successful in image modeling.",1. Introduction,[0],[0]
"The CNNs do not only reduce the input dimension, but can also generate more compact and informative representations that serve as input to the RNN.",1. Introduction,[0],[0]
"Intuitive and tempting as it is, training such a model from scratch in an endto-end fashion turns out to be impractical for large video datasets.",1. Introduction,[0],[0]
"Thus, many current works following this concept focus on the CNN part and reduce the size of RNN in term of sequence length (Donahue et al., 2015; Srivastava et al., 2015), while other works exploit pre-trained deep CNNs as pre-processor to generate static features as input to RNNs (Yue-Hei Ng et al., 2015; Donahue et al., 2015; Sharma et al., 2015).",1. Introduction,[0],[0]
"The former approach neglects the capability of RNNs to handle sequences of variable lengths and therefore does not scale to larger, more realistic video data.",1. Introduction,[0],[0]
"The second approach might suffer from suboptimal weight parameters by not being trained end-to-end (Fernando & Gould, 2016).",1. Introduction,[0],[0]
"Furthermore, since these CNNs are pretrained on existing image datasets, it remains unclear how well the CNNs can generalize to video frames that could be of totally different nature from the image training sets.
",1. Introduction,[0],[0]
"Alternative approaches were earlier applied to generate image representations using dimension reductions such as PCA (Zhang et al., 1997; Kambhatla & Leen, 1997; Ye et al., 2004) and Random Projection (Bingham & Mannila, 2001).",1. Introduction,[0],[0]
Classifiers were built on such features to perform object and face recognition tasks.,1. Introduction,[0],[0]
"These models, however,
are often restricted to be linear and cannot be trained jointly with the classifier.
",1. Introduction,[0],[0]
"In this work, we pursue a new direction where the RNN is exposed to the raw pixels on each frame without any CNN being involved.",1. Introduction,[0],[0]
"At each time step, the RNN first maps the large pixel input to a latent vector in a typically much lower dimensional space.",1. Introduction,[0],[0]
"Recurrently, each latent vector is then enriched by its predecessor at the last time step with a hidden-to-hidden mapping.",1. Introduction,[0],[0]
"In this way, the RNN is expected to capture the inter-frame transition patterns to extract the representation for the entire sequence of frames, analogous to RNNs generating a sentence representation based on word embeddings in NLP (Sutskever et al., 2014).",1. Introduction,[0],[0]
"In comparison with other mapping techniques, a direct input-to-hidden mapping in an RNN has several advantages.",1. Introduction,[0],[0]
First it is much simpler to train than deep CNNs in an end-to-end fashion.,1. Introduction,[0],[0]
Secondly it is exposed to the complete pixel input without the linear limitation as PCA and Random Projection.,1. Introduction,[0],[0]
"Thirdly and most importantly, since the input-to-hidden and hidden-to-hidden mappings are trained jointly, the RNN is expected to capture the correlation between spatial and temporal patterns.
",1. Introduction,[0],[0]
"To address the issue of having too large of a weight matrix for the input-to-hidden mapping in RNN models, we propose to factorize the matrix with the Tensor-Train decomposition (Oseledets, 2011).",1. Introduction,[0],[0]
"In (Novikov et al., 2015) the Tensor-Train has been applied to factorize a fullyconnected feed-forward layer that can consume image pixels as well as latent features.",1. Introduction,[0],[0]
"We conducted experiments on three large-scale video datasets that are popular benchmarks in the community, and give empirical proof that the proposed approach makes very simple RNN architectures competitive with the state-of-the-art models, even though they are of several orders of magnitude lower complexity.
",1. Introduction,[0],[0]
"The rest of the paper is organized as follows: In Section 2 we summarize the state-of-the-art works, especially in video classification using Neural Network models and the tensorization of weight matrices.",1. Introduction,[0],[0]
In Section 3 we first introduce the Tensor-Train model and then provide a detailed derivation of our proposed Tensor-Train RNNs.,1. Introduction,[0],[0]
In Section 4 we present our experimental results on three large scale video datasets.,1. Introduction,[0],[0]
"Finally, Section 5 serves as a wrap-up of our current contribution and provides an outlook of future work.
",1. Introduction,[0],[0]
"Notation We index an entry in a d-dimensional tensor A ∈ Rp1×p2×...×pd using round parentheses such as A(l1, l2, ..., ld) ∈ R and A(l1) ∈ Rp2×p3×...×pd , when we only write the first index.",1. Introduction,[0],[0]
"Similarly, we also use A(l1, l2) ∈",1. Introduction,[0],[0]
Rp3×p4×...×pd to refer to the sub-tensor specified by two indices l1 and l2.,1. Introduction,[0],[0]
The current approaches to model video data are closely related to models for image data.,2. Related Works,[0],[0]
"A large majority of these works use deep CNNs to process each frame as image, and aggregate the CNN outputs.",2. Related Works,[0],[0]
"(Karpathy et al., 2014) proposes multiple fusion techniques such as Early, Late and Slow Fusions, covering different aspects of the video.",2. Related Works,[0],[0]
"This approach, however, does not fully take the order of frames into account.",2. Related Works,[0],[0]
"(Yue-Hei Ng et al., 2015) and (Fernando & Gould, 2016) apply global pooling of frame-wise CNNs, before feeding the aggregated information to the final classifier.",2. Related Works,[0],[0]
An intuitive and appealing idea is to fuse these frame-wise spatial representations learned by CNNs using RNNs.,2. Related Works,[0],[0]
"The major challenge, however, is the computation complexity; and for this reason multiple compromises in the model design have to be made: (Srivastava et al., 2015) restricts the length of the sequences to be 16, while (Sharma et al., 2015) and (Donahue et al., 2015) use pre-trained CNNs.",2. Related Works,[0],[0]
"(Shi et al., 2015) proposed a more compact solution that applies convolutional layers as input-tohidden and hidden-to-hidden mapping in LSTM.",2. Related Works,[0],[0]
"However, they did not show its performance on large-scale video data.",2. Related Works,[0],[0]
"(Simonyan & Zisserman, 2014) applied two stacked CNNs, one for spatial features and the other for temporal ones, and fused the outcomes of both using averaging and a Support-Vector Machine as classifier.",2. Related Works,[0],[0]
"This approach is further enhanced with Residual Networks in (Feichtenhofer et al., 2016).",2. Related Works,[0],[0]
"To the best of our knowledge, there has been no published work on applying pure RNN models to video classification or related tasks.
",2. Related Works,[0],[0]
"The Tensor-Train was first introduced by (Oseledets, 2011) as a tensor factorization model with the advantage of being capable of scaling to an arbitrary number of dimensions.",2. Related Works,[0],[0]
"(Novikov et al., 2015) showed that one could reshape a fully connected layer into a high-dimensional tensor and then factorize this tensor using Tensor-Train.",2. Related Works,[0],[0]
This was applied to compress very large weight matrices in deep Neural Networks where the entire model was trained end-toend.,2. Related Works,[0],[0]
"In these experiments they compressed fully connected layers on top of convolution layers, and also proved that a Tensor-Train Layer can directly consume pixels of image data such as CIFAR-10, achieving the best result among all known non-convolutional models.",2. Related Works,[0],[0]
"Then in (Garipov et al., 2016) it was shown that even the convolutional layers themselves can be compressed with Tensor-Train Layers.",2. Related Works,[0],[0]
"Actually, in an earlier work by (Lebedev et al., 2014)",2. Related Works,[0],[0]
"a similar approach had also been introduced, but their CP factorization is calculated in a pre-processing step and is only fine tuned with error back propagation as a post processing step.
",2. Related Works,[0],[0]
"(Koutnik et al., 2014) performed two sequence classification tasks using multiple RNN architectures of relatively low dimensionality:",2. Related Works,[0],[0]
"The first task was to classify spoken
words where the input sequence had a dimension of 13 channels.",2. Related Works,[0],[0]
"In the second task, RNNs were trained to classify handwriting based on the time-stamped 4D spatial features.",2. Related Works,[0],[0]
"RNNs have been also applied to classify the sentiment of a sentence such as in the IMDB reviews dataset (Maas et al., 2011).",2. Related Works,[0],[0]
"In this case, the word embeddings form the input to RNN models and they may have a dimension of a few hundreds.",2. Related Works,[0],[0]
"The sequence classification model can be seen as a special case of the Encoder-Decoder-Framework (Sutskever et al., 2014) in the sense that a classifier decodes the learned representation for the entire sequence into a probabilistic distribution over all classes.",2. Related Works,[0],[0]
"In this section, we first give an introduction to the core ingredient of our proposed approach, i.e., the Tensor-Train Factorization, and then use this to formulate a so-called Tensor-Train Layer (Novikov et al., 2015) which replaces the weight matrix mapping from the input vector to the hidden layer in RNN models.",3. Tensor-Train RNN,[0],[0]
"We emphasize that such a layer is learned end-to-end, together with the rest of the RNN in a very efficient way.",3. Tensor-Train RNN,[0],[0]
A Tensor-Train Factorization (TTF) is a tensor factorization model that can scale to an arbitrary number of dimensions.,3.1. Tensor-Train Factorization,[0],[0]
"Assuming a d-dimensional target tensor of the form A ∈ Rp1×p2×...×pd , it can be factorized in form of:
Â(l1, l2, ..., ld) TTF = G1(l1) G2(l2) ... Gd(ld)",3.1. Tensor-Train Factorization,[0],[0]
"(1)
where
Gk ∈ Rpk×rk−1×rk , lk ∈",3.1. Tensor-Train Factorization,[0],[0]
"[1, pk] ∀k ∈",3.1. Tensor-Train Factorization,[0],[0]
"[1, d] and r0 = rd = 1.
(2)
As Eq. 1 suggests, each entry in the target tensor is represented as a sequence of matrix multiplications.",3.1. Tensor-Train Factorization,[0],[0]
The set of tensors {Gk}dk=1 are usually called core-tensors.,3.1. Tensor-Train Factorization,[0],[0]
"The complexity of the TTF is determined by the ranks [r0, r1, ..., rd].",3.1. Tensor-Train Factorization,[0],[0]
We demonstrate this calculation also in Fig. 1.,3.1. Tensor-Train Factorization,[0],[0]
"Please note that the dimensions and core-tensors are indexed from 1 to d while the rank index starts from 0; also note that the first and last ranks are both restricted to be 1, which implies that the first and last core tensors can be seen as matrices so that the outcome of the chain of multiplications in Eq. 1 is always a scalar.
",3.1. Tensor-Train Factorization,[0],[0]
If one imposes the constraint that each integer pk as in Eq.,3.1. Tensor-Train Factorization,[0],[0]
(1) can be factorized as pk = mk · nk ∀k ∈,3.1. Tensor-Train Factorization,[0],[0]
"[1, d], and consequently reshapes each Gk into G∗k ∈ Rmk×nk×rk−1×rk , then each index lk in Eq.",3.1. Tensor-Train Factorization,[0],[0]
"(1) and (2) can be uniquely rep-
resented with two indices (ik, jk), i.e.
ik = b lk nk c, jk",3.1. Tensor-Train Factorization,[0],[0]
"= lk − nkb lk nk c, (3) so that Gk(lk) =",3.1. Tensor-Train Factorization,[0],[0]
"G∗k(ik, jk) ∈ Rrk−1×rk .",3.1. Tensor-Train Factorization,[0],[0]
"(4)
Correspondingly, the factorization for the tensor A ∈ R(m1·n1)×(m2·n2)×...×(md·nd) can be rewritten equivalently to Eq.(1):
Â((i1, j1), (i2, j2), ..., (id, jd))",3.1. Tensor-Train Factorization,[0],[0]
"TTF = G∗1(i1, j1) G ∗ 2(i2, j2) ...",3.1. Tensor-Train Factorization,[0],[0]
"G ∗ d(id, jd).
",3.1. Tensor-Train Factorization,[0],[0]
"(5)
This double index trick (Novikov et al., 2015) enables the factorizing of weight matrices in a feed-forward layer as described next.",3.1. Tensor-Train Factorization,[0],[0]
Here we factorize the weight matrix W of a fullyconnected feed-forward layer denoted in ŷ = Wx+,3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"b.
First we rewrite this layer in an equivalent way with scalars as:
ŷ(j) = M∑ i=1",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"W (i, j) · x(i) + b(j) ∀j ∈",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"[1, N ] and with x ∈ RM , y ∈ RN .
(6)
Then, if we assume that M = ∏d
k=1mk, N =∏d k=1 nk i.e. both M and N can be factorized into two integer arrays of the same length, then we can reshape the input vector x and the output vector ŷ into two tensors with the same number of dimensions: X ∈ Rm1×m2×...×md ,Y ∈ Rn1×n2×...×nd , and the mapping function Rm1×m2×...×md → Rn1×n2×...×nd can be written as:
Ŷ(j1, j2, ..., jd)
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
= m1∑ i1=1 m2∑ i2=1 ...,3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"md∑ id=1 W((i1, j1), (i2, j2), ..., (id, jd))·
X (i1, i2, ..., id) +B(j1, j2, ..., jd).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"(7)
Note that Eq. (6) can be seen as a special case of Eq.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
(7) with d = 1.,3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"The d-dimensional double-indexed tensor of weights W in Eq.(7) can be replaced by its TTF representation:
Ŵ((i1, j1), (i2, j2), ..., (id, jd))",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"TTF = G∗1(i1, j1) G ∗ 2(i2, j2) ...",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"G ∗ d(id, jd).
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"(8)
Now instead of explicitly storing the full tensor W of size∏d k=1mk·nk =M ·N , we only store its TT-format, i.e., the
set of low-rank core tensors {Gk}dk=1 of size ∑d
k=1mk · nk · rk−1 · rk, which can approximately reconstruct W .
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"The forward pass complexity (Novikov et al., 2015) for one scalar in the output vector indexed by (j1, j2, ..., jd) turns out to beO(d ·m̃ · r̃2).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"Since one needs an iteration through all such tuples, yielding O(ñd), the total complexity for one Feed-Forward-Pass can be expressed as O(d · m̃ · r̃2 · ñd), where m̃ = maxk∈[1,d]mk, ñ = maxk∈[1,d] nk, r̃ = maxk∈[1,d] rk.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"This, however, would be O(M · N) for a fully-connected layer.
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"One could also compute the compression rate as the ratio between the number of weights in a fully connected layer and that in its compressed form as:
r = ∑d",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"k=1mknkrk−1rk∏d
k=1mknk .",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"(9)
For instance, an RGB frame of size 160 × 120 × 3 implies an input vector of length 57,600.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"With a hidden layer of size, say, 256 one would need a weight matrix consisting of 14,745,600 free parameters.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"On the other hand, a TTL that factorizes the input dimension with 8×20×20×18 is able to represent this matrix using 2,976 parameters with a TT-rank of 4, or 4,520 parameters with a TT-rank of 5 (Tab. 1), yielding compression rates of 2.0e-4 and 3.1e-4, respectively.
",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"For the rest of the paper, we term a fully-connected layer in form of ŷ = Wx + b, whose weight matrix W is factorized with TTF, a Tensor-Train Layer (TTL) and use the notation
ŷ =",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"TTL(W , b,x), or TTL(W ,x) (10)
where in the second case no bias is required.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"Please also note that, in contrast to (Lebedev et al., 2014) where the weight tensor is firstly factorized using non-linear LeastSquare method and then fine-tuned with Back-Propagation, the TTL is always trained end-to-end.",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
"For details on the gradients calculations please refer to Section 5 in (Novikov et al., 2015).",3.2. Tensor-Train Factorization of a Feed-Forward Layer,[0],[0]
In this work we investigate the challenge of modeling highdimensional sequential data with RNNs.,3.3. Tensor-Train RNN,[0],[0]
"For this reason,
we factorize the matrix mapping from the input to the hidden layer with a TTL.",3.3. Tensor-Train RNN,[0],[0]
"For an Simple RNN (SRNN), which is also known as the Elman Network, this mapping is realized as a vector-matrix multiplication, whilst in case of LSTM and GRU, we consider the matrices that map from the input vector to the gating units:",3.3. Tensor-Train RNN,[0],[0]
r[t],TT-GRU:,[0],[0]
"= σ(TTL(W r,x[t])",TT-GRU:,[0],[0]
"+U rh[t−1] + br)
z[t] = σ(TTL(W z,x[t])",TT-GRU:,[0],[0]
"+Uzh[t−1] + bz)
d[t] = tanh(TTL(W d,x[t])",TT-GRU:,[0],[0]
"+Ud(r[t] ◦ h[t−1]))
h[t] = (1− z[t]) ◦",TT-GRU:,[0],[0]
"h[t−1] + z[t] ◦ d[t],
(11)",TT-GRU:,[0],[0]
"k[t] = σ(TTL(W k,x[t])",TT-LSTM:,[0],[0]
"+Ukh[t−1] + bk)
",TT-LSTM:,[0],[0]
f,TT-LSTM:,[0],[0]
"[t] = σ(TTL(W f ,x[t]) +Ufh[t−1] + bf )
",TT-LSTM:,[0],[0]
"o[t] = σ(TTL(W o,x[t])",TT-LSTM:,[0],[0]
"+Uoh[t−1] + bo)
g[t] = tanh(TTL(W g,x[t])",TT-LSTM:,[0],[0]
"+Ugh[t−1] + bg)
c[t] = f",TT-LSTM:,[0],[0]
"[t] ◦ c[t−1] + k[t] ◦ g[t]
h[t] = o[t] ◦ tanh(c[t]).
(12)
One can see that LSTM and GRU require 4 and 3 TTLs, respectively, one for each of the gating units.",TT-LSTM:,[0],[0]
"Instead of calculating these TTLs successively (which we call vanilla TT-LSTM and vanilla TT-GRU), we increase n1 —the first 1 of the factors that form the output size N = ∏d k=1 nk in a TTL— by a factor of 4 or 3, and concatenate all the gates as one output tensor, thus parallelizing the computation.",TT-LSTM:,[0],[0]
"This trick, inspired by the implementation of standard LSTM and GRU in (Chollet, 2015), can further reduce the number of parameters, where the concatenation is actually participating in the tensorization.",TT-LSTM:,[0],[0]
"The compression rate for the input-to-hidden weight matrix W now becomes
r∗ =
∑d k=1mknkrk−1rk",TT-LSTM:,[0],[0]
"+ (c− 1)(m1n1r0r1)
",TT-LSTM:,[0],[0]
"c · ∏d k=1mknk (13)
where c = 4 in case of LSTM and 3 in case of GRU,
and one can show that r∗ is always smaller than r as in Eq. 9.",TT-LSTM:,[0],[0]
"For the former numerical example of a input frame size 160×120×3, a vanilla TT-LSTM would simply require 4 times as many parameters as a TTL, which would be 11,904 for rank 4 and 18,080 for rank 5.",TT-LSTM:,[0],[0]
"Applying this trick would, however, yield only 3,360 and 5,000 parameters for both ranks, respectively.",TT-LSTM:,[0],[0]
"We cover other possible settings of this numerical example in Tab. 1.
",TT-LSTM:,[0],[0]
"Finally to construct the classification model, we denote the i-th sequence of variable length Ti as a set of vectors
1Though in theory one could of course choose any nk.
{x[t]i } Ti t=1 with x",TT-LSTM:,[0],[0]
[t],TT-LSTM:,[0],[0]
i ∈ RM∀t.,TT-LSTM:,[0],[0]
For video data each x [t] i would be an RGB frame of 3 dimensions.,TT-LSTM:,[0],[0]
"For the sake of simplicity we denote an RNN model, either with or without TTL, with a function f(·):
h",TT-LSTM:,[0],[0]
[Ti] i = f({x,TT-LSTM:,[0],[0]
"[t] i } Ti t=1),",TT-LSTM:,[0],[0]
where h,TT-LSTM:,[0],[0]
"[Ti] i ∈ R N , (14)
which outputs the last hidden layer vector h[Ti]i out of a sequential input of variable length.",TT-LSTM:,[0],[0]
"This vector can be interpreted as a latent representation of the whole sequence, on top of which a parameterized classifier φ(·) with either softmax or logistic activation produces the distribution over all J classes:
P(yi = 1|{x",TT-LSTM:,[0],[0]
[t] i } Ti t=1) = φ(h,TT-LSTM:,[0],[0]
"[Ti] i )
",TT-LSTM:,[0],[0]
= φ(f(x [t] i } Ti t=1)) ∈,TT-LSTM:,[0],[0]
"[0, 1]J ,
(15)
The model is also illustrated in Fig. 2:",TT-LSTM:,[0],[0]
"In the following, we present our experiments conducted on three large video datasets.",4. Experiments,[0],[0]
"These empirical results demonstrate that the integration of the Tensor-Train Layer in plain RNN architectures such as a tensorized LSTM or GRU boosts the classification quality of these models tremendously when directly exposed to high-dimensional input data, such as video data.",4. Experiments,[0],[0]
"In addition, even though the plain architectures are of very simple nature and very low complexity opposed to the state-of-the-art solutions on these datasets, it turns out that the integration of the Tensor-Train Layer alone makes these simple networks very competitive to the state-of-the-art, reaching second best results in all cases.
",4. Experiments,[0],[0]
"UCF11 Data (Liu et al., 2009)",4. Experiments,[0],[0]
We first conduct experiments on the UCF11 – earlier known as the YouTube Action Dataset.,4. Experiments,[0],[0]
"It contains in total 1600 video clips belonging to 11 classes that summarize the human action visible in each video clip such as basketball shooting, biking, diving etc..",4. Experiments,[0],[0]
"These videos originate from YouTube and have natural background (’in the
wild’) and a resolution of 320 × 240.",4. Experiments,[0],[0]
"We generate a sequence of RGB frames of size 160 × 120 from each clip at an fps(frame per second) of 24, corresponding to the standard value in film and television production.",4. Experiments,[0],[0]
"The lengths of frame sequences vary therefore between 204 to 1492 with an average of 483.7.
",4. Experiments,[0],[0]
"For both the TT-GRUs and TT-LSTMs the input dimension at each time step is 160 × 120 × 3 = 57600 which is factorized as 8 × 20 × 20 × 18, the hidden layer is chosen to be 4 × 4 × 4 × 4 = 256 and the Tensor-Train ranks are [1, 4, 4, 4, 1].",4. Experiments,[0],[0]
"A fully-connected layer for such a mapping would have required 14,745,600 parameters to learn, while the input-to-hidden layer in TT-GRU and TT-LSTM consist of only 3,360 and 3,232, respectively.
",4. Experiments,[0],[0]
As the first baseline model we sample 6 random frames in ascending order.,4. Experiments,[0],[0]
"The model is a simple Multilayer Perceptron (MLP) with two layers of weight matrices, the first of which being a TTL.",4. Experiments,[0],[0]
The input is the concatenation of all 6 flattened frames and the hidden layer is of the same size as the hidden layer in TT-RNNs.,4. Experiments,[0],[0]
We term this model as Tensor-Train Multilayer Perceptron (TT-MLP) for the rest of the paper.,4. Experiments,[0],[0]
As the second baseline model we use plain GRUs and LSTMs that have the same size of hidden layer as their TT pendants.,4. Experiments,[0],[0]
"We follow (Liu et al., 2013) and perform for each experimental setting a 5-fold cross validation with mutual exclusive data splits.",4. Experiments,[0],[0]
"The mean and standard deviation of the prediction accuracy scores are reported in Tab. 2.
",4. Experiments,[0],[0]
The standard LSTM and GRU do not show large improvements compared with the TT-MLP model.,4. Experiments,[0],[0]
"The TT-LSTM and TT-GRU, however, do not only compress the weight matrix from over 40 millions to 3 thousands, but also significantly improve the classification accuracy.",4. Experiments,[0],[0]
It seems that plain LSTM and GRU are not adequate to model such high-dimensional sequential data because of the large weight matrix from input to hidden layer.,4. Experiments,[0],[0]
"Compared to some latest state-of-the-art performances in Tab. 3, our model —simple as it is— shows accuracy scores second to (Sharma et al., 2015), which uses pre-trained GoogLeNet CNNs plus 3-fold stacked LSTM with attention mechanism.",4. Experiments,[0],[0]
"Please note that a GoogLeNet CNN alone consists of over 6 million parameters (Szegedy et al., 2015).",4. Experiments,[0],[0]
"In term of runtime, the plain GRU and LSTM took on average more than 8 and 10 days to train, respectively; while the TTGRU and TT-LSTM both approximately 2 days.",4. Experiments,[0],[0]
Therefore please note the TTL reduces the training time by a factor of 4 to 5 on these commodity hardwares.,4. Experiments,[0],[0]
"The Hollywood2 dataset contains video clips from 69 movies, from which 33 movies serve as training set and 36 movies as test set.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"From these movies 823 training clips and 884 test clips are generated and each clip is assigned one or multiple of 12 action labels such as answering the phone, driving a car, eating or fighting a person.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
This data set is much more realistic and challenging since the same action could be performed in totally different style in front of different background in different movies.,"Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"Furthermore, there are often montages, camera movements and zooming within a single clip.
","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"The original frame sizes of the videos vary, but based on the majority of the clips we generate frames of size 234 × 100, which corresponds to the Anamorphic Format, at fps of 12.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"The length of training sequences varies from 29 to 1079 with an average of 134.8; while the length of test sequences varies from 30 to 1496 frames with an average of 143.3.
","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"The input dimension at each time step, being 234× 100× 3 = 70200, is factorized as 10× 18× 13× 30.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"The hidden layer is still 4 × 4 × 4 × 4 = 256 and the Tensor-Train ranks are [1, 4, 4, 4, 1].","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"Since each clip might have more
than one label (multi-class multi-label problem) we implement a logistic activated classifier for each class on top of the last hidden layer.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"Following (Marszałek et al., 2009)","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"we measure the performances using Mean Average Precision across all classes, which corresponds to the Area-UnderPrecision-Recall-Curve.
","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"As before we conduct experiments on this dataset using the plain LSTM, GRU and their respective TT modifications.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"The results are presented in in Tab. 4 and state-of-the-art in Tab. 5.
","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"(Fernando et al., 2015) and (Jain et al., 2013) use improved trajectory features with Fisher encoding (Wang & Schmid, 2013) and Histogram of Optical Flow (HOF) features (Laptev et al., 2008), respectively, and achieve so far the best score.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"(Sharma et al., 2015) and (Fernando & Gould, 2016) provide best scores achieved with Neural Network models but only the latter applies end-toend training.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"To this end, the TT-LSTM model provides the second best score in general and the best score with Neural Network models, even though it merely replaces the input-to-hidden mapping with a TTL.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"Please note the large difference between the plain LSTM/GRU and the TT-
LSTM/GRU, which highlights the significant performance improvements the Tensor-Train Layer contributes to the RNN models.
","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"It is also to note that, although the plain LSTM and GRU consist of up to approximately 23K as many parameters as their TT modifications do, the training time does not reflect such discrepancy due to the good parallelization power of GPUs.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"However, the obvious difference in their training qualities confirms that training larger models may require larger amounts of data.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
"In such cases, powerful hardwares are no guarantee for successful training.","Hollywood2 Data (Marszałek et al., 2009)",[0],[0]
This dataset consists of 1910 Youtube video clips of 47 prominent individuals such as movie stars and politicians.,"Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"In the simplest cases, where the face of the subject is visible as a long take, a mere frame level classification would suffice.","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"The major challenge, however, is posed by the fact that some videos involve zooming and/or changing the angle of view.","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"In such cases a single frame may not provide enough information for the classification task and we believe it is advantageous to apply RNN models that can aggregate frame level information over time.
","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
The original frame sizes of the videos vary but based on the majority of the clips we generate frames of size 160 × 120 at fps of 12.,"Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
The retrieved sequences have lengths varying from 2 to 85 with an average of 39.9.,"Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"The input dimension at each time step is 160 × 120 × 3 = 57600 which is factorized as 4× 20× 20× 36, the hidden layer is again 4× 4× 4× 4 = 256 and the Tensor-Train ranks are
[1, 4, 4, 4, 1].
","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"As expected, the baseline of TT-MLP model tends to perform well on the simpler video clips where the position of the face remains less changed over time, and can even outperform the plain GRU and LSTM.","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"The TT-GRU and TT-LSTM, on the other hand, provide accuracy very close to the best state-of-the-art model (Tab. 7) using Mean Sequence Sparse Representation-based Classification (Ortiz et al., 2013) as feature extraction.","Youtube Celebrities Face Data (Kim et al., 2008)",[0],[0]
"We applied 0.25 Dropout (Srivastava et al., 2014) for both input-to-hidden and hidden-to-hidden mappings in plain GRU and LSTM as well as their respective TT modifications; and 0.01 ridge regularization for the single-layered classifier.",Experimental Settings,[0],[0]
"The models were implemented in Theano (Bastien et al., 2012) and deployed in Keras (Chollet, 2015).",Experimental Settings,[0],[0]
"We used the Adam (Kingma & Ba, 2014) step rule for the updates with an initial learning rate 0.001.",Experimental Settings,[0],[0]
"We proposed to integrate Tensor-Train Layers into Recurrent Neural Network models including LSTM and GRU, which enables them to be trained end-to-end on highdimensional sequential data.",5. Conclusions and Future Work,[0],[0]
We tested such integration on three large-scale realistic video datasets.,5. Conclusions and Future Work,[0],[0]
"In comparison to the plain RNNs, which performed very poorly on these video datasets, we could empirically show that the integration of the Tensor-Train Layer alone significantly improves
the modeling performances.",5. Conclusions and Future Work,[0],[0]
"In contrast to related works that heavily rely on deep and large CNNs, one advantage of our classification model is that it is simple and lightweight, reducing the number of free parameters from tens of millions to thousands.",5. Conclusions and Future Work,[0],[0]
This would make it possible to train and deploy such models on commodity hardware and mobile devices.,5. Conclusions and Future Work,[0],[0]
"On the other hand, with significantly less free parameters, such tensorized models can be expected to be trained with much less labeled data, which are quite expensive in the video domain.
",5. Conclusions and Future Work,[0],[0]
"More importantly, we believe that our approach opens up a large number of possibilities to model high-dimensional sequential data such as videos using RNNs directly.",5. Conclusions and Future Work,[0],[0]
"In spite of its success in modeling other sequential data such as natural language, music data etc., RNNs have not been applied to video data in a fully end-to-end fashion, presumably due to the large input-to-hidden weight mapping.",5. Conclusions and Future Work,[0],[0]
"With TTRNNs that can directly consume video clips on the pixel level, many RNN-based architectures that are successful in other applications, such as NLP, can be transferred to modeling video data: one could implement an RNN autoencoder that can learn video representations similar to (Srivastava et al., 2015), an Encoder-Decoder Network (Cho et al., 2014) that can generate captions for videos similar to (Donahue et al., 2015), or an attention-based model that can learn on which frame to allocate the attention in order to improve the classification.
",5. Conclusions and Future Work,[0],[0]
"We believe that the TT-RNN provides a fundamental building block that would enable the transfer of techniques from fields, where RNNs have been very successful, to fields that deal with very high-dimensional sequence data –where RNNs have failed in the past.
",5. Conclusions and Future Work,[0],[0]
The source codes of our TT-RNN implementations and all the experiments in Sec. 4 are publicly available at https: //github.com/Tuyki/TT_RNN.,5. Conclusions and Future Work,[0],[0]
"In addition, we also provide codes of unit tests, simulation studies as well as experiments performed on the HMDB51 dataset (Kuehne et al., 2011).",5. Conclusions and Future Work,[0],[0]
The Recurrent Neural Networks and their variants have shown promising performances in sequence modeling tasks such as Natural Language Processing.,abstractText,[0],[0]
"These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix.",abstractText,[0],[0]
This may have prevented RNNs’ large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extractors.,abstractText,[0],[0]
"To address this challenge, we propose a new, more general and efficient approach by factorizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained simultaneously with the weights themselves.",abstractText,[0],[0]
"We test our model on classification tasks using multiple real-world video datasets and achieve competitive performances with state-of-the-art models, even though our model architecture is orders of magnitude less complex.",abstractText,[0],[0]
We believe that the proposed approach provides a novel and fundamental building block for modeling highdimensional sequential data with RNN architectures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling highdimensional sequential data.,abstractText,[0],[0]
Tensor-Train Recurrent Neural Networks for Video Classification,title,[0],[0]
Property testing is the study of algorithms that query their input a small number of times and distinguish between whether their input satisfies a given property or is “far” from satisfying that property.,1. Introduction,[0],[0]
"The quest for efficient testing algorithms was initiated by (Blum et al., 1993) and (Babai et al., 1991) and later explicitly formulated by (Rubinfeld & Sudan, 1996) and (Goldreich et al., 1998).",1. Introduction,[0],[0]
"Property testing can be viewed as a relaxation of the traditional notion of a decision problem, where the relaxation is quantified in terms of a distance parameter.",1. Introduction,[0],[0]
"There has been extensive work in this area over the last couple of decades; see, for instance, the surveys (Ron, 2008) and (Rubinfeld & Shapira, 2006) for some different perspectives.
",1. Introduction,[0],[0]
"*Equal contribution 1Department of Computer Science and Automation, Indian Institute of Science, Bangalore, India.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siddharth Barman <barman@iisc.ac.in>, Arnab Bhattacharyya <arnabb@iisc.ac.in>, Suprovat Ghoshal <suprovat@iisc.ac.in>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"As evident from these surveys, research in property testing has largely focused on properties of combinatorial and algebraic structures, such as bipartiteness of graphs, linearity of Boolean functions on the hypercube, membership in errorcorrecting codes or representability of functions as concise Boolean formulae.",1. Introduction,[0],[0]
"In this work, we study the question of testing properties of continuous structures, specifically properties of vectors and matrices over the reals.
",1. Introduction,[0],[0]
Our computational model is a natural extension of the standard property testing framework by allowing queries to be linear measurements of the input.,1. Introduction,[0],[0]
Let,1. Introduction,[0],[0]
P ⊂,1. Introduction,[0],[0]
Rd be a property of real vectors.,1. Introduction,[0],[0]
Let dist :,1. Introduction,[0],[0]
Rd → R>0 be a “distance” function such that dist(x) = 0 for all x ∈ P .,1. Introduction,[0],[0]
"We say that an algorithm A is a tester for P with respect to dist and with parameters ε, δ > 0",1. Introduction,[0],[0]
if for any input,1. Introduction,[0],[0]
"y ∈ Rn, the algorithm",1. Introduction,[0],[0]
"A observes My where M ∈ Rq×d is a randomized matrix and has the following guarantee:
(i)",1. Introduction,[0],[0]
"If y ∈ P , PrM[A(My) accepts] > 1− δ.
(ii) If dist(y) > ε, PrM[A(My) accepts] 6 δ.
",1. Introduction,[0],[0]
"We call each inner product between the rows of M and y a (linear) query, and the number of rows q = q(ε, δ) is the query complexity of the tester.",1. Introduction,[0],[0]
The running time of the tester A is its running time on the outcome of its queries.,1. Introduction,[0],[0]
"As typical in property testing, we do not count the time needed to evaluate the queries.",1. Introduction,[0],[0]
If P ⊂ Rd×p is a property of real matrices with an associated distance function dist :,1. Introduction,[0],[0]
"Rd×p → R>0, testing is defined similarly: given an input matrix Y ∈ Rd×p, the algorithm observes MY for a random matrix M ∈ Rq×d with analogous completeness and soundness properties.",1. Introduction,[0],[0]
A linear projection of an input vector or matrix to a low-dimensional space is also called a linear sketch or a linear measurement.,1. Introduction,[0],[0]
"The technique of obtaining small linear sketches of high-dimensional vectors has been used to great effect in algorithms for streaming (e.g., (Alon et al., 1996; McGregor, 2014)) and numerical linear algebra (see (Woodruff, 2014) for an excellent survey).",1. Introduction,[0],[0]
"Because GPUs are specially designed to optimize matrix-vector computation, many modern optimization and learning algorithms work with linear sketches of their input.
",1. Introduction,[0],[0]
"We focus on testing whether a vector is sparse with respect
to some basis.1",1. Introduction,[0],[0]
A vector x is said to be k-sparse if it has at most k nonzero coordinates.,1. Introduction,[0],[0]
Sparsity is a structural characteristic of signals of interest in a diverse range of applications.,1. Introduction,[0],[0]
"It is a pervasive concept throughout modern statistics and machine learning, and algorithms to solve inverse problems under sparsity constraints are among the most successful stories of the optimization community (see the book (Hastie et al., 2015)).",1. Introduction,[0],[0]
"The natural property testing question we consider is whether there exists a solution to a linear inverse problem under a sparsity constraint.
",1. Introduction,[0],[0]
"There are two settings in which we investigate the sparsity testing problem.
",1. Introduction,[0],[0]
"(a) In the first setting, the basis is not known in advance.",1. Introduction,[0],[0]
"For input vectors y1,y2, . . .",1. Introduction,[0],[0]
",yp ∈ Rd, the property to test is whether there exists a matrix A ∈ Rd×m and k-sparse unit vectors x1,x2, . .",1. Introduction,[0],[0]
.xp,1. Introduction,[0],[0]
∈,1. Introduction,[0],[0]
Rm such that yi = Axi for all,1. Introduction,[0],[0]
i ∈,1. Introduction,[0],[0]
[p].,1. Introduction,[0],[0]
Note that m is specified as a parameter and could be much larger than d (the overcomplete case).,1. Introduction,[0],[0]
"In this setting, we restrict the unknown A to be a (ε, k)-RIP matrix which means that (1 − ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ for any ksparse x.",1. Introduction,[0],[0]
"This is a standard assumption made in many related works (see Section 1.2 for details).
",1. Introduction,[0],[0]
"In this setting, we design an efficient tester for this property that projects the inputs toO(ε−2 log p) dimensions and, informally speaking, rejects if for all (ε, k)-RIP matrices",1. Introduction,[0],[0]
"A, there is some yi such that yi −Axi has large norm for all “approximately sparse” xi.
(b)",1. Introduction,[0],[0]
"In the second setting, a design matrix A ∈ Rd×m is known explicitly, and the property to test is whether a given input vector y ∈ Rd equals Ax for a k-sparse vector x ∈ Rm.",1. Introduction,[0],[0]
"For instance, A can be the Fourier basis or an overcomplete dictionary in an image processing application.",1. Introduction,[0],[0]
"We approach this problem in full generality, without putting any restriction on the structure of A.
Informally, our main result in this setting is that for any design matrix A, there exists a tester projecting the input y toO(k logm) dimensions that rejects if y−Ax has large norm for any O(k)-sparse x.",1. Introduction,[0],[0]
The running time of the tester is polynomial inm.,1. Introduction,[0],[0]
"As we describe in Section 1.2, previous work in numerical linear algebra yields a tester with the same query complexity and with qualitatively similar soundness guarantees but which requires running time exponential in m or assumptions about the matrix A.
Remark 1.1 (Problem Formulation).",1. Introduction,[0],[0]
"Note that the settings considered in the known and unknown design matrix settings
1With slight abuse of notation, we use the term basis to denote the set of columns of a design matrix.",1. Introduction,[0],[0]
"The columns might not be linearly independent.
are quite different from each other.",1. Introduction,[0],[0]
"In particular, for the known design setting, the input is a single vector.",1. Introduction,[0],[0]
"However, given a single input vector y ∈ Rd, the analogous unknown design testing question would be moot, since one can always consider the vector y to be the design matrix A, in which it trivially admits a 1-sparse representation.",1. Introduction,[0],[0]
"For the same reason, unknown design testing is interesting only when the number of vectors p exceeds m.
In both of the above tests, the measurement matrix is a random matrix with iid gaussian entries, chosen so as to preserve norms and certain other geometric properties upon dimensionality reduction.2 In particular, our testers are oblivious to the input.",1. Introduction,[0],[0]
It is a very interesting open question as to whether non-oblivious testers can strengthen the above results.,1. Introduction,[0],[0]
We now present our results more formally.,1.1. Our Results,[0],[0]
"For integer m > 0, let Sm−1 = {x ∈",1.1. Our Results,[0],[0]
"Rm : ‖x‖ = 1}, and let Spmk = {x ∈ Sm−1 : ‖x‖0 6 k}.3
Theorem 1.2 (Unknown Design Matrix).",1.1. Our Results,[0],[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and p, such that (",1.1. Our Results,[0],[0]
k/m)1/8,1.1. Our Results,[0],[0]
<,1.1. Our Results,[0],[0]
ε,1.1. Our Results,[0],[0]
< 1100 and k > 10 log 1 ε .,1.1. Our Results,[0],[0]
There exists a tester with query complexity O(ε−2 log (p/δ)),1.1. Our Results,[0],[0]
"which, given as input vectors y1,y2, . . .",1.1. Our Results,[0],[0]
",yp ∈ Rd, has the following behavior (where Y is the matrix having y1,y2, . . .",1.1. Our Results,[0],[0]
",yp as columns):
– Completeness: If Y admits a decomposition Y = AX, where A ∈ Rd×m satisfies (ε, k)-RIP and X ∈ Rm×p with each column of X in Spmk , then the tester accepts with probability > 1− δ.
– Soundness:",1.1. Our Results,[0],[0]
Suppose Y does not admit a decomposition Y = A(X + Z) +,1.1. Our Results,[0],[0]
"W with
1.",1.1. Our Results,[0],[0]
"The design matrix A ∈ Rd×m being (ε, k)-RIP, with ‖ai‖ = 1 for every",1.1. Our Results,[0],[0]
i ∈,1.1. Our Results,[0],[0]
[m].,1.1. Our Results,[0],[0]
2.,1.1. Our Results,[0],[0]
"The coefficient matrix X ∈ Rm×p being column wise `-sparse, where ` = O(k/ε4).
",1.1. Our Results,[0],[0]
3.,1.1. Our Results,[0],[0]
The error matrices Z ∈,1.1. Our Results,[0],[0]
Rm×p and W ∈,1.1. Our Results,[0],[0]
Rd×p,1.1. Our Results,[0],[0]
"satisfying
‖zi‖∞ 6 ε2, ‖wi‖2 6 O(ε1/4) for all i ∈",1.1. Our Results,[0],[0]
"[p].
Then the tester rejects with probability > 1− δ.",1.1. Our Results,[0],[0]
"2If evaluating the queries efficiently was an objective, one could also use sparse dimension reduction matrices (Dasgupta et al., 2010; Kane & Nelson, 2014; Bourgain et al., 2015), but we do not pursue this direction here.
3Here, ‖x‖0 denotes the the sparsity of the vector, ‖x‖0 := |{i ∈",1.1. Our Results,[0],[0]
[m] | xi 6= 0}|.,1.1. Our Results,[0],[0]
"Without any subscript, ‖ · ‖ denotes the `2-norm: ‖x‖ := √∑ i x 2 i .
",1.1. Our Results,[0],[0]
"The contrapositive of the soundness guarantee from the above theorem states that if the tester accepts, then matrix Y admits a factorization of the form Y = A(X+Z)+W, with error matrices Z and W having `∞ and `2 error bounds.",1.1. Our Results,[0],[0]
"The matrix X+Z is a sparse matrix with `∞-based thresholding, and W is an additive `2-error term.4
Theorem 1.3 (Known Design Matrix).",1.1. Our Results,[0],[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m such that ‖ai‖ = 1 for every i ∈",1.1. Our Results,[0],[0]
[m].,1.1. Our Results,[0],[0]
There exists a tester with query complexity O(kε−2 log(m/δ)) that behaves as follows for an input vector y ∈,1.1. Our Results,[0],[0]
"Rd:
– Completeness:",1.1. Our Results,[0],[0]
"If y = Ax for some x ∈ Spmk , then the tester accepts with probability 1.
– Soundness: If ‖Ax− y‖2 > ε for every x : ‖x‖0 6 K, then the tester rejects with probability > 1",1.1. Our Results,[0],[0]
− δ.,1.1. Our Results,[0],[0]
"Here, K = O(k/ε2).
",1.1. Our Results,[0],[0]
"The running time of the tester is poly(m, k, 1/ε).
",1.1. Our Results,[0],[0]
"A different way of stating the result is that the tester, using O(kε−2 log(m/δ)) linear queries, accepts with probability 1 if y = Ax for a k-sparse x ∈",1.1. Our Results,[0],[0]
Rm and rejects with probability 1− δ,1.1. Our Results,[0],[0]
"if ‖Ax− y‖ > ε‖x‖ for every O(k/ε2)sparse x. To complement this result, we show that a better tradeoff between the sparsity and reconstruction error is likely to be impossible.
",1.1. Our Results,[0],[0]
Theorem 1.4 (Hardness).,1.1. Our Results,[0],[0]
"Assume SAT does not have nO(log logn)-time algorithms, and let η be any constant less than 1.",1.1. Our Results,[0],[0]
"Then, there does not exist a polynomial time algorithm that, given input A ∈ Rd×m (where ‖ai‖ = 1 for every i ∈",1.1. Our Results,[0],[0]
"[m]), y ∈ Rd and ε > 0, distinguishes with constant probability between the following two cases: (i) y",1.1. Our Results,[0],[0]
"= Ax for a k-sparse x, and (ii) ‖y",1.1. Our Results,[0],[0]
"−Ax‖ > ε‖x‖η for every (k/ε2)-sparse x.
Note that the above hardness applies to any polynomial time algorithm, not just sketching algorithms.
",1.1. Our Results,[0],[0]
We also give tolerant variants of these testers (Theorems H.1 and H.2) which can handle bounded noise for the completeness case.,1.1. Our Results,[0],[0]
"Moreover, the tester for the known design case can be converted into a new sketching algorithm for sparse recovery (Theorem D.1).
",1.1. Our Results,[0],[0]
"Finally, we also give an algorithm for testing dimensionality, which is based on similar techniques.
",1.1. Our Results,[0],[0]
Theorem 1.5 (Testing Dimensionality).,1.1. Our Results,[0],[0]
"Fix ε, δ ∈ (0, 1), positive integers d, k and p, where k > 10ε2 log d. There exists a tester with query complexity O(p log δ−1), which
4Theorem 1.2 can be restated in terms of incoherent (instead of RIP) design matrices as well.",1.1. Our Results,[0],[0]
This follows from the fact that the incoherence and RIP constants of a matrix are order-wise equivalent.,1.1. Our Results,[0],[0]
"This observation is formalized in Appendix F.
gives as input vectors y1, . . .",1.1. Our Results,[0],[0]
",yp ⊂",1.1. Our Results,[0],[0]
"Sd−1, has the following behavior:
– Completeness: If rank(Y ) 6 k, then the tester accepts with probability > 1− δ.
– Soundness: If rankε(Y )",1.1. Our Results,[0],[0]
"> k′, then the tester rejects with probability > 1− δ.",1.1. Our Results,[0],[0]
"Here, k′ = 20k/ε2
The soundness criteria in the above Theorem is stated in terms of the ε-approximate rank of a matrix (see Definition E.1).",1.1. Our Results,[0],[0]
"This is a well-studied relaxation of the standard definition of rank, and has applications in approximation algorithms, communication complexity and learning theory (see (Alon et al., 2013) and references therein).",1.1. Our Results,[0],[0]
"Although, to the best of our knowledge, the testing problems we consider have not been explicitly investigated before, there are several related areas of study that frame our results in their proper context.
",1.2. Related Work,[0],[0]
Unknown Design setting.,1.2. Related Work,[0],[0]
"In the setting of the unknown design matrix, the question of recovering the design matrix and the sparse representation (as opposed to our problem of testing their existence) is called the dictionary learning or sparse coding problem.",1.2. Related Work,[0],[0]
"The first work to give a dictionary learning algorithm with provable guarantees was (Spielman et al., 2012) where the dictionary was restricted to be square.",1.2. Related Work,[0],[0]
"For the more common overcomplete setting, (Arora et al., 2014) and (Agarwal et al., 2014) independently gave algorithms with provable guarantees for dictionaries satisfying incoherence and RIP respectively.",1.2. Related Work,[0],[0]
All of these (as well as other more recent) works assume distributions from which the input samples are generated in an i.i.d fashion.,1.2. Related Work,[0],[0]
"In contrast, our work is in the agnostic setting and hence, is incomparable with these results.
",1.2. Related Work,[0],[0]
"It is known that the dictionary learning problem is NP-hard, even for square dictionaries (Razaviyayn et al., 2014; Tillmann, 2015).",1.2. Related Work,[0],[0]
"In fact, (Tillmann, 2015) shows that unless SAT has a quasi-polynomial time algorithm, it is impossible, given Y ∈ Rd×p, to approximate in polynomial time the minimum k upto a factor 2log
1−ε d (for any ε > 0) such that Y = AX where each column of X ∈ Rd×p is k-sparse.",1.2. Related Work,[0],[0]
"This motivates our bicriteria relaxation of both the sparsity as well as the additive error in Theorem 1.2.
",1.2. Related Work,[0],[0]
Known Design setting.,1.2. Related Work,[0],[0]
Some results about testing sparsity in the known design setting are implicit in recent work on streaming algorithms and oblivious subspace embeddings.,1.2. Related Work,[0],[0]
"Of particular interest are the following results:
Theorem 1.6 (Implicit in (Kane et al., 2010)).",1.2. Related Work,[0],[0]
"Fix ε ∈ (0, 1), positive integers m, k and an invertible matrix A ∈
Rm×m.",1.2. Related Work,[0],[0]
"Then, there is a tester with query complexity O(ε−2 log(m)) that, for an input y ∈ Rm, accepts with probability at least 2/3 if y = Ax for some k-sparse x ∈ Zm, and rejects with probability 2/3 if y 6=",1.2. Related Work,[0],[0]
Ax for all (1 + ε)k-sparse x ∈ Zm.,1.2. Related Work,[0],[0]
"The running time of the algorithm is poly(m, 1/ε).",1.2. Related Work,[0],[0]
"Theorem 1.7 (Implicit in prior work, see (Woodruff, 2014)).",1.2. Related Work,[0],[0]
"Fix ε, δ ∈ (0, 1) and positive integers d, k,m and a matrix A ∈ Rd×m.",1.2. Related Work,[0],[0]
"Then, there is a tester with query complexity O(kε−2 log(m/δ))",1.2. Related Work,[0],[0]
"that, for an input vector y ∈ Rd, accepts with probability 1 if y = Ax for some k-sparse x and rejects with probability at least 1− δ",1.2. Related Work,[0],[0]
if ‖y −Ax‖,1.2. Related Work,[0],[0]
> ε for all k-sparse x.,1.2. Related Work,[0],[0]
"The running time of the tester is the time required to solve the following optimization problem:
x̂ = arg min x′∈K ‖SAx′",1.2. Related Work,[0],[0]
− Sy‖ = arg min x′∈K ‖S(Ax′,1.2. Related Work,[0],[0]
"− y)‖
(1) where S ∈ Rq×d is a random sketch matrix (where q d) and K = {x : ‖x‖0 6 k}
Detailed descriptions of the algorithms and proof sketches for the above Theorems are given in Section B.4.",1.2. Related Work,[0],[0]
The algorithms from the above theorems come with significant limitations.,1.2. Related Work,[0],[0]
"In particular, the guarantees for Theorem 1.6 hold only when the design matrix is invertible.",1.2. Related Work,[0],[0]
"On the other hand, the running time for the algorithm in Theorem 1.7 is the cost of solving the optimization problem in Equation (1), which is known to be NP-hard for general matrices.
",1.2. Related Work,[0],[0]
"The problem of testing sparsity has also been studied in non-sketching settings as well, where the algorithm is allowed access to the entire input.",1.2. Related Work,[0],[0]
"In particular, (Natarajan, 1995) gave a bicriteria-approximation algorithm, where the blowup in the sparsity is proportional to ‖A†‖22 (which can be large if A is ill conditioned).
",1.2. Related Work,[0],[0]
Testing Dimensionality.,1.2. Related Work,[0],[0]
"In (Czumaj et al., 2000), some problems in computational geometry were studied from the property testing perspective, but the problems involved only discrete structures.",1.2. Related Work,[0],[0]
"(Krauthgamer & Sasson, 2003) studied the problem of testing dimensionality, but their notion of farness from being low-dimensional is different from ours5.",1.2. Related Work,[0],[0]
"(Chierichetti et al., 2017) gave approximation algorithms for computing approximate rank of the matrix, in the setting where the algorithms have full access to the input.",1.2. Related Work,[0],[0]
A standard approach to designing a testing algorithm for a property P is the following: we identify an alternative property P ′,1.3. Discussion,[0],[0]
"which can be tested efficiently and exactly, while satisfying the following:
5In their setup, a sequence of vectors y1, . . .",1.3. Discussion,[0],[0]
",yp is ε-far from being d-dimensional if at least εp vectors need to be removed to make it be of dimension d
(i) Completeness:",1.3. Discussion,[0],[0]
"If an instance satisfies P , then it satisfies P ′.
(ii) Soundness:",1.3. Discussion,[0],[0]
"If an instance satisfies P ′, the it is close to satisfying P .
",1.3. Discussion,[0],[0]
"In other words, we reduce the property testing problem to that of finding a efficiently testable property P ′, which can be interpreted as a surrogate for property P .",1.3. Discussion,[0],[0]
"The inherent geometric nature of the problems looked at in this paper motivate us to look for P ′s which are based around convex geometry and high dimensional probability.
",1.3. Discussion,[0],[0]
"For the unknown design setting, we are intuitively looking for a P ′",1.3. Discussion,[0],[0]
"based on a quantity ω that robustly captures sparsity and is easily computable using linear queries, in the sense that ω is small when the input vectors have a sparse coding and large when they are “far” from any sparse coding.",1.3. Discussion,[0],[0]
"Moreover, ω needs to be invariant with respect to isometries and nearly invariant with respect to near-isometries.",1.3. Discussion,[0],[0]
"A natural and widely-used measure of structure that satisfies the above mentioned properties is the gaussian width.
",1.3. Discussion,[0],[0]
Definition 1.8.,1.3. Discussion,[0],[0]
"The gaussian width of a set S ⊆ Rd is: ω(S) = Eg[supv∈S〈g,v〉] where g ∈ Rd is a random vector drawn from N(0, 1)d, i.e., a vector of independent standard normal variables.
",1.3. Discussion,[0],[0]
The gaussian width of S measures how well on average the vectors in S correlate with a randomly chosen direction.,1.3. Discussion,[0],[0]
It is invariant under orthogonal transformations of S as the distribution of g is spherically symmetric.,1.3. Discussion,[0],[0]
"It is a well-studied quantity in high-dimensional geometry ((Vershynin, 2015; Mendelson & Vershynin, 2002)), optimization ((Chandrasekaran et al., 2012; Amelunxen et al., 2013)) and statistical learning theory ((Bartlett & Mendelson, 2002)).",1.3. Discussion,[0],[0]
"The following bounds are well-known.
",1.3. Discussion,[0],[0]
"Lemma 1.9 (See, for example, (Rudelson & Vershynin, 2008; Vershynin, 2015)).
",1.3. Discussion,[0],[0]
(i),1.3. Discussion,[0],[0]
"If S is a finite subset of Sd−1, then ω(S) 6 √ 2 log |S|.
(ii) ω(Sd−1) 6 √ d
(iii) If S ⊆ Sd−1 is of dimension k, then ω(S) 6 √ k.
(iv) ω(Spdk) 6 2 √ 3k log(d/k) when d/k > 2",1.3. Discussion,[0],[0]
"and k > 4.
",1.3. Discussion,[0],[0]
"In the context of Theorems 1.2 and 1.5, one can observe that whenever a given set satisfies sparsity or dimensionality constraints, the gaussian width of such sets are small (points (iii) and (iv) from the above Lemma).",1.3. Discussion,[0],[0]
"Therefore, one can hope to test dimensionality or sparsity by computing an empirical estimate of the gaussian width and comparing the estimate to the results in Lemma 1.9.",1.3. Discussion,[0],[0]
"While completeness of such testers would follow directly from concentration of measure, establishing soundness would require us to show
that approximate converses of points (iii) and (iv) hold as well i.e., whenever the gaussian width of the set S is small, it can be approximated by sets which are approximately sparse in some design matrix (or have low rank).
",1.3. Discussion,[0],[0]
"For the soundness direction of Theorem 1.2, the above arguments are made precise using Lemma 3.3 and Theorem 3.2, which show that small gaussian width sets can be approximated by random projections of sparse vectors and vectors with small `∞-norm.",1.3. Discussion,[0],[0]
"For Theorem 1.5, we use lemma E.2 which shows that sets with small gaussian width have small approximate rank.
",1.3. Discussion,[0],[0]
"For the known design setting, we are looking for aP ′, which would ensure that if a given point y ∈ Rd satisfies P ′, then it is close to having a sparse representation in the matrix A. Towards this end, the approximate Carathéodory’s theorem states that if a point y ∈ Rd belonging to the convex-hull of A, then it is close to another point which admits a sparse representation.",1.3. Discussion,[0],[0]
"On the other hand, if a unit vector x ∈ Sd−1 ∩Rd+ were k-sparse to begin with , then it can be seen that the corresponding y = Ax would belong to the convex hull of √ k · A.",1.3. Discussion,[0],[0]
"These observations taken together, seem to suggest that one can take P ′ to be membership in the convex-hull of √",1.3. Discussion,[0],[0]
k ·A. This intuition is made precise in the analysis of the tester in Section 4.,1.3. Discussion,[0],[0]
Section 2 introduces notations and preliminaries used in the rest of the paper.,1.4. Organization,[0],[0]
"In Sections 3 and 4, we design and analyze the testers for the unknown and known basis setting respectively.",1.4. Organization,[0],[0]
Section 5 contains empirical results which supplement Section 3.,1.4. Organization,[0],[0]
"In Section B we prove additional lemmas used in the proof of Theorem 3.2, and in Section A we prove Theorem 3.2.",1.4. Organization,[0],[0]
"In Section C, we prove Theorem C.1, a stronger version of Theorem 1.4.",1.4. Organization,[0],[0]
"In Section D, we show that Theorem 1.3 yields a sketching algorithm for sparse recovery.",1.4. Organization,[0],[0]
"In Section E, we design and analyze the dimensionality tester.",1.4. Organization,[0],[0]
"In Section G, we describe the results for testing sparsity in the known case implicit in previous work.",1.4. Organization,[0],[0]
"Finally, in Section H, we give noise tolerant testers for the known and unknown basis settings.",1.4. Organization,[0],[0]
"Given S ⊂ Rd, we shall use conv(S) to denote the convex hull of S. For a vector x ∈ Rd, we use ‖ · ‖p to denote its `p-norm, and we will drop the indexing when p = 2.",2. Preliminaries,[0],[0]
"We denote the `2-distance of the point x to the set S by dist(x, S).",2. Preliminaries,[0],[0]
"We recall the definition of ε-isometry:
Definition 2.1.",2. Preliminaries,[0],[0]
Given sets S ⊂ Rm and S′ ⊂,2. Preliminaries,[0],[0]
"Rn (for some m,n ∈ N), we say that S′ is an ε-isometry of S, if there exists a mapping ψ",2. Preliminaries,[0],[0]
:,2. Preliminaries,[0],[0]
"S 7→ S′ which satisfies the following
property:
∀x,y ∈ S : (1−ε)‖x−y‖ 6 ‖ψ(x)−ψ(y)‖ 6 (1+ε)‖x−y‖
For the unknown design setting, we shall require the notion of Restricted Isometry Property, which is defined as follows:
Definition 2.2 ((ε, k)-RIP).",2. Preliminaries,[0],[0]
"A matrix A ∈ Rd×m satisfies (ε, k)-RIP, if for every x ∈ Spmk the following holds:
(1− ε)‖x‖ 6 ‖Ax‖ 6 (1 + ε)‖x‖ (2)
We use the following version of Gordon’s Theorem repeatedly in this work.
",2. Preliminaries,[0],[0]
"Theorem 2.3 (Gordon’s Theorem (Gordon, 1985)).",2. Preliminaries,[0],[0]
"Given S ⊂ SD−1 and a random gaussian matrix G ∼ 1√ d′ N(0, 1)d ′×D, we have
E G [ max x∈S ‖Gx‖2 ] 6 1 + ω(S)√ d′
It directly implies the following generalization of the Johnson-Lindenstrauss lemma.
",2. Preliminaries,[0],[0]
Theorem 2.4 (Generalized Johnson-Lindenstrauss lemma).,2. Preliminaries,[0],[0]
Let S ⊆ Sn−1.,2. Preliminaries,[0],[0]
Then there exists linear transformation Φ :,2. Preliminaries,[0],[0]
"Rn 7→ Rd′ , for d′ = O ( ω(S)2
ε2
) , such that Φ is an
ε-isometry on S. Moreover, Φ ∼ 1√ d′ N(0, 1)d ′×n is an ε-isometry on S with high probability.
",2. Preliminaries,[0],[0]
"It can be easily verified that the quantity maxx∈S ‖Gx‖2 is 1-Lipschitz with respect to G. Therefore, using Gaussian concentration for Lipschitz functions, we get the following corollary :
Corollary 2.5.",2. Preliminaries,[0],[0]
Let S and G be as in Theorem 2.3.,2. Preliminaries,[0],[0]
"Then for all ε > 0, we have
Pr G ( max x∈S ‖Gx‖2 > 1 + ( 1 + ε )",2. Preliminaries,[0],[0]
"ω(S)√ d′ ) 6 exp ( −O(εω(S))2
)",2. Preliminaries,[0],[0]
"The following lemma gives concentration for the gaussian width:
Lemma 2.6 (Concentration on the gaussian width (Boucheron et al., 2013)).",2. Preliminaries,[0],[0]
Let S ⊂ Rd.,2. Preliminaries,[0],[0]
"Let W = supv∈S〈g,v〉 where g is drawn from N(0, 1)d.",2. Preliminaries,[0],[0]
"Then:
Pr[|W −EW | > u] < 2e− u2 2σ2
",2. Preliminaries,[0],[0]
where σ2 = supv∈S ( ‖v‖22 ),2. Preliminaries,[0],[0]
.,2. Preliminaries,[0],[0]
"Notice that the bound is dimension independent.
",2. Preliminaries,[0],[0]
"Lastly, we shall use the `2-variant of the approximate Carathéodory’s Theorem:
Theorem 2.7.",2. Preliminaries,[0],[0]
"(Theorem 0.1.2 (Vershynin, 2016) )",2. Preliminaries,[0],[0]
"Given X = {w1, . . .",2. Preliminaries,[0],[0]
",wp} where ‖wi‖ 6 1 for every i ∈",2. Preliminaries,[0],[0]
[p].,2. Preliminaries,[0],[0]
"Then for every choice z ∈ conv ( X )
and k ∈ N, there exists wi1 ,wi2 , . . .",2. Preliminaries,[0],[0]
",wik such that∥∥∥∥1k ∑
j∈[k]
",2. Preliminaries,[0],[0]
wij,2. Preliminaries,[0],[0]
− z ∥∥∥∥ 6 2√k (3),2. Preliminaries,[0],[0]
"We record here simple lemmas bounding the number of linear queries needed to estimate the gaussian width of a set and the length of a vector.
",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
Lemma 2.8 (Estimating Gaussian Width using linear queries).,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"For any u > 4, ε ∈ (0, 1/2) and δ > 0, there is a randomized algorithm that given a set S ⊆ Rd and ‖v‖ ∈",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"[1 ± ε] for all v ∈ S, computes ω̂ such that ω(S)− u 6 ω̂ 6 ω(S)",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
+ u with probability at least 1− δ.,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"The algorithm makes O(log(1/δ) · |S|) linear queries to S.
Proof.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"By Lemma 2.6, for a random g ∼ N(0, 1)d, supv∈S〈g,v〉 is away from ω(S) by u with probability at most 2e−16/4.5 < 0.1.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"By the Chernoff bound, the median of O(log δ−1) trials will satisfy the conditions required of ω̂ with probability at least 1− δ.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
Lemma 2.9 (Estimating norm using linear queries).,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"Given ε ∈ (0, 1/2) and δ > 0, for any vector x ∈ Rd , only O(ε−2 log δ−1) linear queries to x suffice to decide whether ‖x‖ ∈",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"[1− ε, 1 + ε] with success probability 1− δ.
",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
Proof.,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"It is easy to verify that Eg∼N(0,1)d [〈g,x〉2] = ‖x‖2.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"Therefore, it can be estimated to a multiplicative error of (1 ± ε/2) by taking the average of the squares of linear measurements using O ( 1 ε2 log 1 δ ) -queries.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"For the case ‖x‖2 6 2, a multiplicative error (1± ε/2) implies an additive error of ε.",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"Furthermore, when ‖x‖2 > 2, a multiplicative error of (1± ε/2) implies that L > 2(1− ε/2)",2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
> 1,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
+ ε for ε < 1/2.,2.1. Algorithmic Estimation of Gaussian Width and Norm of a vector,[0],[0]
"In this section, we prove Theorem 1.2.",3. Analysis for Unknown Design setting,[0],[0]
"Let S denote the set {y1, . . .",3. Analysis for Unknown Design setting,[0],[0]
",yp}.",3. Analysis for Unknown Design setting,[0],[0]
"Our testing algorithm is shown in Algorithm 1.
",3. Analysis for Unknown Design setting,[0],[0]
The number of linear queries made by the tester is O(pε−2 log(p/δ)),3. Analysis for Unknown Design setting,[0],[0]
in Line 1 and O(p log δ−1) in Line 2.,3. Analysis for Unknown Design setting,[0],[0]
Assume that for each i ∈,3.1. Completeness,[0],[0]
"[p], yi = Axi for a matrix A ∈ Rd×m satisfying (ε, k)-RIP and xi ∈ Spmk .",3.1. Completeness,[0],[0]
"By definition
Algorithm 1 SparseTestUnknown 1: Use Lemma 2.9 to decide with probability at least 1− δ/2 if there exists yi such that ‖yi‖ 6∈",3.1. Completeness,[0],[0]
"[1− 2ε, 1 + 2ε].",3.1. Completeness,[0],[0]
"Reject if so.
2: Use Lemma 2.8 to obtain ω̂, an estimate of ω(S) within additive error √ 3k",3.1. Completeness,[0],[0]
"log(m/k) with probability at least
1− δ/2.",3.1. Completeness,[0],[0]
"3: Accept if ω̂ 6 4 √ 3k log(m/k), else reject.
of RIP, we know that 1− ε 6 ‖yi‖ 6 1 + ε, so that Line 1 of the algorithm will pass with probability at least 1− δ/2.",3.1. Completeness,[0],[0]
"From Lemma 1.9, we know that ω({x1, . .",3.1. Completeness,[0],[0]
.xp}),3.1. Completeness,[0],[0]
"6 2 √
3k log(m/k).",3.1. Completeness,[0],[0]
"Lemma 3.1 shows that the gaussian width of S is approximately the same; its proof, deferred to the appendix (Section B.4), uses Slepian’s Lemma (Lemma B.3).
",3.1. Completeness,[0],[0]
Lemma 3.1.,3.1. Completeness,[0],[0]
LetX ⊂,3.1. Completeness,[0],[0]
"Sm−1 be a finite set, and let S ⊂ Rd be an ε-isometric embedding of X .",3.1. Completeness,[0],[0]
"Then
(1− ε)ω(X) 6 ω(S) 6 (1 + ε)ω(X) (4)
Hence, the gaussian width of y1, . .",3.1. Completeness,[0],[0]
.,3.1. Completeness,[0],[0]
",yp is at most 2(1 + ε) √
3k log(m/k).",3.1. Completeness,[0],[0]
"Taking into account the additive error in Line 2, we see that with probability at least 1 − δ/2, ω̂ 6 (3 + 2ε) √ 3k",3.1. Completeness,[0],[0]
log(m/k) 6 4 √ 3k,3.1. Completeness,[0],[0]
log(m/k).,3.1. Completeness,[0],[0]
"Hence, the tester accepts with probability at least 1− δ.",3.1. Completeness,[0],[0]
"As mentioned before, in order to prove soundness we need to show that whenever the gaussian width of the set S is small, it is close to some sparse point-set.",3.2. Soundness,[0],[0]
Let ω∗ = 4 √ 3k log mk .,3.2. Soundness,[0],[0]
We shall break the analysis into two cases:,3.2. Soundness,[0],[0]
Case (i) { ω∗ >,3.2. Soundness,[0],[0]
"(ε/C)2 √ d } : For this case, we use the
fact random projection of discretized sparse point-sets (Definition A.1) form an appropriated cover of S.",3.2. Soundness,[0],[0]
"This is formalized in the following theorem, which in a sense shows an approximate inverse of Gordon’s Theorem for sparse vectors:
Theorem 3.2.",3.2. Soundness,[0],[0]
"Given ε > 0 and integers C, d, k and m, let n =",3.2. Soundness,[0],[0]
O ( k ε2 log(m/k) ) .,3.2. Soundness,[0],[0]
Suppose m > k/ε8.,3.2. Soundness,[0],[0]
Let Φ :,3.2. Soundness,[0],[0]
"Rm 7→ Rn be drawn from 1√ n N(0, 1)n×m.",3.2. Soundness,[0],[0]
"Then, for ` = O(kε−4), with high probability, the set Φnorm(Ŝp m
` ) is an O(ε 1/4)-cover of Sn−1, where
Φnorm(x) = Φ(x)/‖Φ(x)‖2.
",3.2. Soundness,[0],[0]
"The proof of the above Theorem is deferred to Section A. From the choice of parameters we have d 6 C
′k ε2 log m k
Therefore, using the above Theorem we know that there
exists (ε, k)-RIP matrix Φ ∈ Rd×m such that Φnorm ( Spm` ) is an O(ε1/4)-cover of Sd−1 (and therefore it is a ε1/4cover of S).",3.2. Soundness,[0],[0]
"Therefore, there exists X ∈ Rm×p such that Y = Φ(X) +",3.2. Soundness,[0],[0]
E where the columns of X and E satisfy the respective ‖ · ‖0 and ‖ · ‖2-upper bounds respectively.,3.2. Soundness,[0],[0]
Case (ii) { ω∗ 6 (ε/C)2 √ d } :,3.2. Soundness,[0],[0]
"For this case, we use the
following result on the concentration of `∞-norm: Lemma 3.3.",3.2. Soundness,[0],[0]
Given S ⊂,3.2. Soundness,[0],[0]
"Sd−1, we have
Pr R∼Od
[ max
y∈R(S) ‖y‖∞ 6 C
ω(S)
d1/2
] > 1
2
where Od is the orthogonal group in Rd i.e., R is a uniform random rotation.
",3.2. Soundness,[0],[0]
"Although this concentration bound is known, for completeness we give a proof in the appendix (Section B.7).",3.2. Soundness,[0],[0]
"From the above lemma, it follows that there exists R ∈ Od such that for any z ∈ Z := R(S) we have ‖z‖∞ 6 ε2 and therefore Y = R−1Z.",3.2. Soundness,[0],[0]
"Furthermore, since R is orthogonal, therefore the matrix R−1 is also orthogonal, and therefore it satisfies (ε, k)-RIP.
",3.2. Soundness,[0],[0]
"To complete the proof, we observe that even though the given factorization has inner dimension d, we can trivially extend it to one with inner dimension m. This can be done by constructing Φ =",3.2. Soundness,[0],[0]
"[ R−1 G ] with G ∼ 1√ d N(0, 1)d×m−d.",3.2. Soundness,[0],[0]
"Since ω∗ d, from Theorem 2.4 it follows that with high probability G (and consequently Φ) will satisfy (ε, k)-RIP.",3.2. Soundness,[0],[0]
"Finally, we construct Ẑ ∈ Rm×n by padding Z with m − d rows of zeros.",3.2. Soundness,[0],[0]
"Therefore, by construction Y = Φ · Ẑ, where for every i ∈",3.2. Soundness,[0],[0]
[p] we have ‖zi‖∞ 6 ε2.,3.2. Soundness,[0],[0]
Hence the claim follows.,3.2. Soundness,[0],[0]
"In this section, we describe and analyze the tester for the known design matrix case.",4. Analysis for the Known Design setting,[0],[0]
"The algorithm itself is a simple convex-hull membership test, which can be solved using a linear program.
",4. Analysis for the Known Design setting,[0],[0]
"Algorithm 2 SparseTest-KnownDesign 1: Set n = 100klog mδ , sample projection matrix Φ ∼
1√ n",4. Analysis for the Known Design setting,[0],[0]
"N(0, 1)n×d
2: Observe linear sketch ỹ = Φ(y) 3: Let A±",4. Analysis for the Known Design setting,[0],[0]
= A ∪ −A 4: Accept iff ỹ ∈ √ k · conv ( Φ(A±) ),4. Analysis for the Known Design setting,[0],[0]
We shall now prove the completeness and soundness guarantees of the above tester.,4. Analysis for the Known Design setting,[0],[0]
"The running time bound follows because convex hull membership reduces to linear programming.
",4. Analysis for the Known Design setting,[0],[0]
Completeness Let y = Ax where A ∈ Rd×m is an arbitrary matrix with ‖ai‖ = 1 for every,4. Analysis for the Known Design setting,[0],[0]
i ∈,4. Analysis for the Known Design setting,[0],[0]
[m].,4. Analysis for the Known Design setting,[0],[0]
"Furthermore ‖x‖2 = 1 and ‖x‖0 6 k. Therefore, by Cauchy-Schwartz we have ‖x‖1 6",4. Analysis for the Known Design setting,[0],[0]
√ k‖x‖2 = √ k.,4. Analysis for the Known Design setting,[0],[0]
"Hence, it follows that
y ∈ √ k · conv(A±).",4. Analysis for the Known Design setting,[0],[0]
"Since Φ : Rm 7→ Rd is a linear trans-
formation, we have Φ(y) ∈",4. Analysis for the Known Design setting,[0],[0]
√ k · conv(Φ(A±)).,4. Analysis for the Known Design setting,[0],[0]
"Therefore, the tester accepts with probability 1.
",4. Analysis for the Known Design setting,[0],[0]
"Soundness Consider the set Aε/√k which is the set of all (2k/ε2)-uniform convex combinations of √ k(A±) i.e.,
Aε/ √ k = { ∑ vi∈Ω ε2 2k vi : multiset Ω ∈ (√ k.A± )2k/ε2} (5)
",4. Analysis for the Known Design setting,[0],[0]
"Then, from the approximate Carathéodory theorem, it follows that Aε/√k is an ε-cover of √ k · conv ( A± ) .",4. Analysis for the Known Design setting,[0],[0]
"Furthermore, |Aε/√k| 6 (2m) 2k/ε2 .",4. Analysis for the Known Design setting,[0],[0]
"By our choice of n, with
probability at least 1 − δ/2, the set Φ ( {y} ∪ Aε/√k ) is ε-isometric to {y} ∪Aε/√k.
",4. Analysis for the Known Design setting,[0],[0]
Let Ãε/√k = Φ ( Aε/ √ k ) .,4. Analysis for the Known Design setting,[0],[0]
"Again, by the approximate Carathéodory’s theorem, the set Ãε/√k is an ε-cover of
Φ (√ k · conv(A±) ) .",4. Analysis for the Known Design setting,[0],[0]
Now suppose the test accepts y with probability at least δ.,4. Analysis for the Known Design setting,[0],[0]
"Then, with probability at least δ/2, the test accepts and the above ε-isometry conditions hold simultaneously.",4. Analysis for the Known Design setting,[0],[0]
"Then,
ỹ ∈ √ k · conv ( Φ(A±) )",4. Analysis for the Known Design setting,[0],[0]
"1⇒ dist ( ỹ, Ãε/ √ k ) 6 ε
2⇒ dist ( y, Aε/ √ k ) 6 ε(1− ε)−1 6 2ε
⇒ dist",4. Analysis for the Known Design setting,[0],[0]
(,4. Analysis for the Known Design setting,[0],[0]
"y, √ k · conv(A±) )",4. Analysis for the Known Design setting,[0],[0]
"6 2ε
where step 1 follows from the ε-cover guarantee of Ãε/√k, step 2 follows from the ε-isometry guarantee.",4. Analysis for the Known Design setting,[0],[0]
"Invoking the approximate Carathéodory theorem, we get that there exists ŷ",4. Analysis for the Known Design setting,[0],[0]
= Ax̂ ∈,4. Analysis for the Known Design setting,[0],[0]
√ k · conv(±A) such that ‖x̂‖0 6 O(k/ε2) and ‖ŷ − y‖ 6 O(ε).,4. Analysis for the Known Design setting,[0],[0]
This completes the soundness direction.,4. Analysis for the Known Design setting,[0],[0]
Our algorithm for the unknown design setting is based on the principle that the property of sparse representability in some basis admits an approximate characterization in terms of gaussian width.,5. Experimental Results,[0],[0]
This section provides experimental evidence which supplements our theoretical results.,5. Experimental Results,[0],[0]
"For the empirical study, we use the classic Barbara image (which is of size 512× 512 pixels).",5. Experimental Results,[0],[0]
"Specifically, we consider 9 subimages of size 100 × 100 pixels each (see Figure 1).",5. Experimental Results,[0],[0]
"For each such sub-image, we compute a matrix representation (by the standard technique of subdividing the images into patches, see, e.g., (Elad & Aharon, 2006)).",5. Experimental Results,[0],[0]
"In particular,
each sub-image is represented as a matrix Y of dimension 64 × 8649.",5. Experimental Results,[0],[0]
"Then, for each matrix Y corresponding to a sub-image, we estimate the gaussian width of the `2-column normalized matrix.",5. Experimental Results,[0],[0]
"In addition, setting the number of atoms m = 100 and sparsity k = 10, we run the k-SVD algorithm for 50 iterations and record the reconstruction error.6
Figure 2 shows the comparison between gaussian width and reconstruction error, in which we observe that there is an approximate correlation between the two quantities.",5. Experimental Results,[0],[0]
"In particular, for sub-images 2,7 and 8—which mostly consist of background—both the gaussian width and the reconstruction error is small.",5. Experimental Results,[0],[0]
"On the other hand, images 3, 6 and 9, which consist of intricate patterns and objects, have large gaussian width as well as large reconstruction error.",5. Experimental Results,[0],[0]
"Consequently, we can deduce that for sub-images with large gaussian width, in order to achieve low reconstruction error, one would have consider a larger number of atoms m or larger sparsity k.",5. Experimental Results,[0],[0]
"In this paper, we studied the problem of testing sparsity with respect to unknown and known bases.",6. Conclusion and Open Questions,[0],[0]
"While the optimization variants of these problems (namely Dictionary Learning and Sparse Recovery) are known to be NP-hard in the worst case, our results show that under appropriate relaxations, these problems admit efficient property testing algorithms.",6. Conclusion and Open Questions,[0],[0]
"Future work include designing testing algorithms for sparsity over an unknown basis with stronger
6For a matrix Y ∈ Rd×n approximated by overcomplete basis A and coefficient matrix X, the reconstruction error is equal to ‖Y −AX‖2F /(n.d).
guarantees or developing impossibility results.",6. Conclusion and Open Questions,[0],[0]
We also hope that this paper leads to study of property testing of other widely studied hypotheses in machine learning such as nonnegative rank and VC-dimension.,6. Conclusion and Open Questions,[0],[0]
We would like to thank David Woodruff for showing us the sketching-based tester described in Section 1.2.,Acknowledgements,[0],[0]
Sparsity is a basic property of real vectors that is exploited in a wide variety of machine learning applications.,abstractText,[0],[0]
"In this work, we describe property testing algorithms for sparsity that observe a lowdimensional projection of the input.",abstractText,[0],[0]
We consider two settings.,abstractText,[0],[0]
"In the first setting, we test sparsity with respect to an unknown basis: given input vectors y1, . . .",abstractText,[0],[0]
",yp ∈ R whose concatenation as columns forms Y ∈ Rd×p, does Y = AX for matrices A ∈ Rd×m",abstractText,[0],[0]
"and X ∈ Rm×p such that each column of X is k-sparse, or is Y “far” from having such a decomposition?",abstractText,[0],[0]
"In the second setting, we test sparsity with respect to a known basis: for a fixed design matrix A ∈ Rd×m, given input vector y ∈ R, is y = Ax for some ksparse vector x or is y “far” from having such a decomposition?",abstractText,[0],[0]
We analyze our algorithms using tools from high-dimensional geometry and probability.,abstractText,[0],[0]
Testing Sparsity over Known and Unknown Bases,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 201–210, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
"Real-time captioning provides deaf or hard of hearing people access to speech in mainstream classrooms, at public events, and on live television.",1 Introduction,[0],[0]
"To maintain consistency between the captions being read and other visual cues, the latency between when a word was said and when it is displayed must be under five seconds.",1 Introduction,[0],[0]
"The most common approach to real-time captioning is to recruit a trained stenographer with a special purpose phonetic keyboard, who transcribes the speech to text within approximately 5 seconds.",1 Introduction,[0],[0]
"Unfortunately, professional captionists are quite expensive ($150 per hour), must be recruited in blocks of an hour or more, and are difficult to schedule on short notice.",1 Introduction,[0],[0]
"Automatic speech recognition (ASR) (Saraclar et al., 2002) attempts to solve this
problem by converting speech to text completely automatically.",1 Introduction,[0],[0]
"However, the accuracy of ASR quickly plummets to below 30% when used on an untrained speaker’s voice, in a new environment, or in the absence of a high quality microphone (Wald, 2006b).
",1 Introduction,[0],[0]
"An alternative approach is to combine the efforts of multiple non-expert captionists (anyone who can type) (Lasecki et al., 2012; Lasecki and Bigham, 2012; Lasecki et al., 2013).",1 Introduction,[0],[0]
"In this approach, multiple non-expert human workers transcribe an audio stream containing speech in real-time, and their partial input is combined to produce a final transcript (see Figure 1).",1 Introduction,[0],[0]
"This approach has been shown to dramatically outperform ASR in terms of both accuracy and Word Error Rate (WER), even when using captionists drawn from Amazon’s Mechanical Turk.",1 Introduction,[0],[0]
"Furthermore, recall approached and even exceeded that of a trained expert stenographer with seven workers contributing, suggesting that the information is present to meet the performance of a stenographer.",1 Introduction,[0],[0]
"However, combining these captions involves real-time alignment of partial captions that may be incomplete and that often have spelling errors and inconsistent timestamps.",1 Introduction,[0],[0]
"In this paper, we present a more accurate combiner that leverages
201
Multiple Sequence Alignment (MSA) and Natural Language Processing to improve performance.
",1 Introduction,[0],[0]
Gauging the quality of captions is not easy.,1 Introduction,[0],[0]
"Although word error rate (WER) is commonly used in speech recognition, it considers accuracy and completeness, not readability.",1 Introduction,[0],[0]
"As a result, a lower WER does not always result in better understanding (Wang et al., 2003).",1 Introduction,[0],[0]
"We compare WER with two other commonly used metrics: BLEU (Papineni et al., 2002) and F-measure (Melamed et al., 2003), and report their correlation with that of 50 human evaluators.
",1 Introduction,[0],[0]
"The key contributions of this paper are as follows:
• We have implemented an A∗-search based Multiple Sequence Alignment algorithm (Lermen and Reinert, 2000) that can trade-off speed and accuracy by varying the heuristic weight and chunk-size parameters.",1 Introduction,[0],[0]
"We show that it outperforms previous approaches in terms of WER, BLEU score, and F-measure.
",1 Introduction,[0],[0]
"• We propose a beam-search based technique using the timing information of the captions that helps to restrict the search space and scales effectively to align longer sequences efficiently.
",1 Introduction,[0],[0]
"• We evaluate the correlation of WER, BLEU, and F-measure with 50 human ratings of caption readability, and found that WER was more highly correlated than BLEU score (Papineni et al., 2002), implying it may be a more useful metric overall when evaluating captions.",1 Introduction,[0],[0]
"Most of the previous research on real-time captioning has focused on Automated Speech Recognition (ASR) (Saraclar et al., 2002; Cooke et al., 2001; Pražák et al., 2012).",2 Related Work,[0],[0]
"However, experiments show that ASR systems are not robust enough to be applied for arbitrary speakers and in noisy environments (Wald, 2006b; Wald, 2006a; Bain et al., 2005; Bain et al., 2012; Cooke et al., 2001).",2 Related Work,[0],[0]
"To address these limitations of ASR-based techniques, the Scribe system collects partial captions from the crowd and then uses a graph-based incremental algorithm to combine them on the fly (Lasecki et al., 2012).",2.1 Crowd Captioning,[0],[0]
"The system incrementally
builds a chain graph, where each node represents a set of equivalent words entered by the workers and the link between nodes are adjusted according to the order of the input words.",2.1 Crowd Captioning,[0],[0]
"A greedy search is performed to identify the path with the highest confidence, based on worker input and an n-gram language model.",2.1 Crowd Captioning,[0],[0]
"The algorithm is designed to be used online, and hence has high speed and low latency.",2.1 Crowd Captioning,[0],[0]
"However, due to the incremental nature of the algorithm and due to the lack of a principled objective function, it is not guaranteed to find the globally optimal alignment for the captions.",2.1 Crowd Captioning,[0],[0]
"The problem of aligning and combining multiple transcripts can be mapped to the well-studied Multiple Sequence Alignment (MSA) problem (Edgar and Batzoglou, 2006).",2.2 Multiple Sequence Alignment,[0],[0]
"MSA is an important problem in computational biology (Durbin et al., 1998).",2.2 Multiple Sequence Alignment,[0],[0]
The goal is to find an optimal alignment from a given set of biological sequences.,2.2 Multiple Sequence Alignment,[0],[0]
"The pairwise alignment problem can be solved efficiently using dynamic programming in O(N2) time and space, where N is the sequence length.",2.2 Multiple Sequence Alignment,[0],[0]
"The complexity of the MSA problem grows exponentially as the number of sequences grows, and has been shown to be NP-complete (Wang and Jiang, 1994).",2.2 Multiple Sequence Alignment,[0],[0]
"Therefore, it is important to apply some heuristic to perform MSA in a reasonable amount of time.
",2.2 Multiple Sequence Alignment,[0],[0]
"Most MSA algorithms for biological sequences follow a progressive alignment strategy that first performs pairwise alignment among the sequences, and then builds a guide tree based on the pairwise similarity between these sequences (Edgar, 2004; Do et al., 2005; Thompson et al., 1994).",2.2 Multiple Sequence Alignment,[0],[0]
"Finally, the input sequences are aligned according to the order specified by the guide tree.",2.2 Multiple Sequence Alignment,[0],[0]
"While not commonly used for biological sequences, MSA with A∗-style search has been applied to these problems by Horton (1997) and Lermen and Reinert (2000).
",2.2 Multiple Sequence Alignment,[0],[0]
"Lasecki et al. explored MSA in the context of merging partial captions by using the off-the-shelf MSA tool MUSCLE (Edgar, 2004), replacing the nucleotide characters by English characters (Lasecki et al., 2012).",2.2 Multiple Sequence Alignment,[0],[0]
"The substitution cost for nucleotides was replaced by the ‘keyboard distance’ between English characters, learned from the physical layout of a keyboard and based on common spelling
errors.",2.2 Multiple Sequence Alignment,[0],[0]
"However, MUSCLE relies on a progressive alignment strategy and may result in suboptimal solutions.",2.2 Multiple Sequence Alignment,[0],[0]
"Moreover, it uses characters as atomic symbols instead of words.",2.2 Multiple Sequence Alignment,[0],[0]
Our approach operates on a per-word basis and is able to arrive at a solution that is within a selectable error-bound of optimal.,2.2 Multiple Sequence Alignment,[0],[0]
We start with an overview of the MSA problem using standard notations as described by Lermen and Reinert (2000).,3 Multiple Sequence Alignment,[0],[0]
"Let S1, . . .",3 Multiple Sequence Alignment,[0],[0]
", SK , K ≥ 2, be the K sequences over an alphabet Σ, and having length N1, . . .",3 Multiple Sequence Alignment,[0],[0]
", NK .",3 Multiple Sequence Alignment,[0],[0]
The special gap symbol is denoted by ‘−’ and does not belong to Σ. Let,3 Multiple Sequence Alignment,[0],[0]
"A = (aij) be a K × Nf matrix, where aij ∈ Σ ∪ {−}, and the ith row has exactly (Nf − Ni) gaps and is identical to Si if we ignore the gaps.",3 Multiple Sequence Alignment,[0],[0]
Every column of A must have at least one non-gap symbol.,3 Multiple Sequence Alignment,[0],[0]
"Therefore, the jth column of A indicates an alignment state for the jth position, where the state can have one of the 2K − 1 possible combinations.",3 Multiple Sequence Alignment,[0],[0]
"Our goal is to find the optimum alignment matrix AOPT that minimizes the sum of pairs (SOP) cost function:
c(A) = ∑
1≤i≤j≤K
c(Aij) (1)
where c(Aij) is the cost of the pairwise alignment between Si and Sj according to A. Formally, c(Aij) = ∑Nf
l=1 sub(ail, ajl), where sub(ail, ajl) denotes the cost of substituting ajl for ail.",3 Multiple Sequence Alignment,[0],[0]
"If ail and ajl are identical, the substitution cost is usually zero.",3 Multiple Sequence Alignment,[0],[0]
"For the caption alignment task, we treat each individual word as a symbol in our alphabet Σ.",3 Multiple Sequence Alignment,[0],[0]
The substitution cost for two words is estimated based on the edit distance between two words.,3 Multiple Sequence Alignment,[0],[0]
"The exact solution to the SOP optimization problem is NP-Complete, but many methods solve it approximately.",3 Multiple Sequence Alignment,[0],[0]
"In this paper, we adapt weighted A∗ search for approximately solving the MSA problem.",3 Multiple Sequence Alignment,[0],[0]
The problem of minimizing the SOP cost function for K sequences is equivalent to estimating the shortest path between a single source and single sink node in a K-dimensional lattice.,3.1 A∗ Search for MSA,[0],[0]
The total number of nodes in the lattice is (N1 + 1) ×,3.1 A∗ Search for MSA,[0],[0]
"(N2 +
Algorithm 1 MSA-A∗ Algorithm Require: K input sequences S = {S1, . . .",3.1 A∗ Search for MSA,[0],[0]
", SK} having
length N1, . . .",3.1 A∗ Search for MSA,[0],[0]
", NK , heuristic weight w, beam size b
1: start← 0K , goal←",3.1 A∗ Search for MSA,[0],[0]
"[N1, . . .",3.1 A∗ Search for MSA,[0],[0]
", NK ] 2: g(start)← 0, f(start)← w × h(start).",3.1 A∗ Search for MSA,[0],[0]
3: Q← {start} 4: while Q 6= ∅,3.1 A∗ Search for MSA,[0],[0]
do 5: n←,3.1 A∗ Search for MSA,[0],[0]
"EXTRACT-MIN(Q) 6: for all s ∈ {0, 1}K",3.1 A∗ Search for MSA,[0],[0]
"− {0K} do 7: ni ← n + s 8: if ni = goal then 9: Return the alignment matrix for the reconstructed
path from start to ni 10: else if ni 6∈ Beam(b) then 11: continue; 12: else 13: g(ni)← g(n) + c(n, ni) 14: f(ni)← g(ni) +",3.1 A∗ Search for MSA,[0],[0]
"w × h(ni) 15: INSERT-ITEM(Q, ni, f(ni))",3.1 A∗ Search for MSA,[0],[0]
"16: end if 17: end for 18: end while
1) × · · · × (NK + 1), each corresponding to a distinct position in K sequences.",3.1 A∗ Search for MSA,[0],[0]
"The source node is [0, . . .",3.1 A∗ Search for MSA,[0],[0]
", 0] and the sink node is [N1, . . .",3.1 A∗ Search for MSA,[0],[0]
", NK ].",3.1 A∗ Search for MSA,[0],[0]
"The dynamic programming algorithm for estimating the shortest path from source to sink treats each node position [n1, . . .",3.1 A∗ Search for MSA,[0],[0]
", nK ] as a state and calculates a matrix that has one entry for each node.",3.1 A∗ Search for MSA,[0],[0]
"Assuming the sequences have roughly same length N , the size of the dynamic programming matrix is O(NK).",3.1 A∗ Search for MSA,[0],[0]
"At each vertex, we need to minimize the cost over all its 2K − 1 predecessor nodes, and, for each such transition, we need to estimate the SOP objective function that requires O(K2) operations.",3.1 A∗ Search for MSA,[0],[0]
"Therefore, the dynamic programming algorithm has time complexity of O(K22KNK) and space complexity of O(NK), which is infeasible for most practical problem instances.",3.1 A∗ Search for MSA,[0],[0]
"However, we can efficiently solve it via heuristic A∗ search (Lermen and Reinert, 2000).
",3.1 A∗ Search for MSA,[0],[0]
"We use A∗ search based MSA (shown in Algorithm 1, illustrated in Figure 2) that uses a priority queue Q to store dynamic programming states corresponding to node positions in the K dimensional lattice.",3.1 A∗ Search for MSA,[0],[0]
Let n =,3.1 A∗ Search for MSA,[0],[0]
"[n1, . . .",3.1 A∗ Search for MSA,[0],[0]
", nK ] be any node in the lattice, s be the source, and t be the sink.",3.1 A∗ Search for MSA,[0],[0]
"The A∗ search can find the shortest path using a greedy Best First Search according to an evaluation function f(n), which is the summation of the cost func-
tions g(n) and the heuristic function h(n) for node n. The cost function g(n) denotes the cost of the shortest path from the source s to the current node n.",3.1 A∗ Search for MSA,[0],[0]
"The heuristic function h(n) is the approximate estimated cost of the shortest path from n to the destination t. At each step of the A∗ search algorithm, we extract the node with the smallest f(n) value from the priority queue Q and expand it by one edge.",3.1 A∗ Search for MSA,[0],[0]
The heuristic function h(n) is admissible if it never overestimates the cost of the cheapest solution from n to the destination.,3.1 A∗ Search for MSA,[0],[0]
An admissible heuristic function guarantees that A∗ will explore the minimum number of nodes and will always find the optimal solution.,3.1 A∗ Search for MSA,[0],[0]
"One commonly used admissible heuristic function is hpair(n):
hpair(n) = L(n → t) = ∑
1≤i<j≤K
c(A∗p(σ n i , σ n j ))
(2) where L(n → t) denotes the lower bound on the cost of the shortest path from n to destination t, A∗p is the optimal pairwise alignment, and σni is the suffix of node n in the i-th sequence.",3.1 A∗ Search for MSA,[0],[0]
A∗ search using the pairwise heuristic function hpair significantly reduces the search space and also guarantees finding the optimal solution.,3.1 A∗ Search for MSA,[0],[0]
We must be able to estimate hpair(n) efficiently.,3.1 A∗ Search for MSA,[0],[0]
It may appear that we need to estimate the optimal pairwise alignment for all the pairs of suffix sequences at every node.,3.1 A∗ Search for MSA,[0],[0]
"However, we can precompute the dynamic programming matrix over all the pair of sequences (Si, Sj) once from the backward direction, and then reuse these values at each node.",3.1 A∗ Search for MSA,[0],[0]
"This simple trick significantly speeds up the computation of hpair(n).
",3.1 A∗ Search for MSA,[0],[0]
"Despite the significant reduction in the search space, the A∗ search may still need to explore a large number of nodes, and may become too slow for real-time captioning.",3.1 A∗ Search for MSA,[0],[0]
"However, we can further improve the speed by following the idea of weighted A∗ search (Pohl, 1970).",3.1 A∗ Search for MSA,[0],[0]
"We modify the evaluation
function f(n) = g(n)+hpair(n) to a weighted evaluation function f ′(n) = g(n) + whpair(n), where w ≥ 1 is a weight parameter.",3.1 A∗ Search for MSA,[0],[0]
"By setting the value of w to be greater than 1, we increase the relative weight of the estimated cost to reach the destination.",3.1 A∗ Search for MSA,[0],[0]
"Therefore, the search prefers the nodes that are closer to the destination, and thus reaches the goal faster.",3.1 A∗ Search for MSA,[0],[0]
"Weighted A∗ search can significantly reduce the number of nodes to be examined, but it also loses the optimality guarantee of the admissible heuristic function.",3.1 A∗ Search for MSA,[0],[0]
We can trade-off between accuracy and speed by tuning the weight parameter w.,3.1 A∗ Search for MSA,[0],[0]
The computational cost of the A∗ search algorithm grows exponentially with increase in the number of sequences.,3.2 Beam Search using Time-stamps,[0],[0]
"However, in order to keep the crowdsourced captioning system cost-effective, only a small number of workers are generally recruited at a time (typically K ≤ 10).",3.2 Beam Search using Time-stamps,[0],[0]
"We, therefore, are more concerned about the growth in computational cost as the sequence length increases.
",3.2 Beam Search using Time-stamps,[0],[0]
"In practice, we break down the sequences into smaller chunks by maintaining a window of a given time interval, and we apply MSA only to the smaller chunks of captions entered by the workers during that time window.",3.2 Beam Search using Time-stamps,[0],[0]
"As the window size increases, the accuracy of our MSA based combining system increases, but so does the computational cost and latency.",3.2 Beam Search using Time-stamps,[0],[0]
"Therefore, it is important to apply MSA with a relatively small window size for real-time captioning applications.",3.2 Beam Search using Time-stamps,[0],[0]
"Another interesting application can be the offline captioning, for example, captioning an entire lecture and uploading the captions later.
",3.2 Beam Search using Time-stamps,[0],[0]
"For the offline captioning problem, we can focus less on latency and more on accuracy by aligning longer sequences.",3.2 Beam Search using Time-stamps,[0],[0]
"To restrict the search space from exploding with sequence length (N ), we apply a beam constraint on our search space using the time stamps of each captioned words.",3.2 Beam Search using Time-stamps,[0],[0]
"For example, if we
set the beam size to be 20 seconds, then we ignore any state in our search space that aligns two words having more than 20 seconds time lag.",3.2 Beam Search using Time-stamps,[0],[0]
"Given a fixed beam size b, we can restrict the number of priority queue removals by the A∗ algorithm to O(NbK).",3.2 Beam Search using Time-stamps,[0],[0]
The maximum size of the priority queue is O(NbK).,3.2 Beam Search using Time-stamps,[0],[0]
"For each node in the priority queue, for each of the O(2K) successor states, the objective function and heuristic estimation requires O(K2) operations and each priority queue insertion requires O(log(NbK)) i.e. O(log N + K log b) operations.",3.2 Beam Search using Time-stamps,[0],[0]
"Therefore, the overall worst case computational complexity is O ( NbK2K(K2 + log N + K log b) )
.",3.2 Beam Search using Time-stamps,[0],[0]
"Note that for fixed beam size b and number of sequences K, the computational cost grows as O(N log N) with the increase in N .",3.2 Beam Search using Time-stamps,[0],[0]
"However, in practice, weighted A∗ search explores much smaller number of states compared to this beam-restricted space.",3.2 Beam Search using Time-stamps,[0],[0]
"After aligning the captions by multiple workers in a given chunk, we need to combine them to obtain the final caption.",3.3 Majority Voting after Alignment,[0],[0]
We do that via majority voting at each position of the alignment matrix containing a nongap symbol.,3.3 Majority Voting after Alignment,[0],[0]
"In case of tie, we apply the language model to choose the most likely word.
",3.3 Majority Voting after Alignment,[0],[0]
"Often workers type in nonstandard symbols, abbreviations, or misspelled words that do not match with any other workers’ input and end up as a single word aligned to gaps in all the other sequences.",3.3 Majority Voting after Alignment,[0],[0]
"To filter out such spurious words, we apply a voting threshold (tv) during majority voting and filter out words having less than tv votes.",3.3 Majority Voting after Alignment,[0],[0]
Typically we set tv = 2 (see the example in Figure 3).,3.3 Majority Voting after Alignment,[0],[0]
"While applying the voting threshold improves the word error rate and readability, it runs the risk of loosing correct words if they are covered by only a single worker.",3.3 Majority Voting after Alignment,[0],[0]
We also experimented with a version of our system designed to incorporate the score from an n-gram language model into the search.,3.4 Incorporating an N-gram Language Model,[0],[0]
"For this purpose, we modified the alignment algorithm to produce a hypothesized output string as it moves through the input strings, as opposed to using voting to produce the final string as a post-processing step.",3.4 Incorporating an N-gram Language Model,[0],[0]
"The states for our dynamic programming are extended to include not only the current position in each input string, but also the last two words of the hypothesis string (i.e. [n1, . . .",3.4 Incorporating an N-gram Language Model,[0],[0]
", nK , wi−1, wi−2]) for use in computing the next trigram language model probability.",3.4 Incorporating an N-gram Language Model,[0],[0]
"We replace our sum-of-all-pairs objective function with the sum of the alignment cost of each input with the hypothesis string, to which we add the log of the language model probability and a feature for the total number of words in the hypothesis.",3.4 Incorporating an N-gram Language Model,[0],[0]
"Mathematically, we consider the hypothesis string to be the 0th row of the alignment matrix, making our objective function:
c(A) = ∑
1≤i≤K
c(A0,i) +",3.4 Incorporating an N-gram Language Model,[0],[0]
"wlen
Nf ∑
l=1
I(a0,l 6=",3.4 Incorporating an N-gram Language Model,[0],[0]
"−)
",3.4 Incorporating an N-gram Language Model,[0],[0]
"+ wlm
",3.4 Incorporating an N-gram Language Model,[0],[0]
"Nf ∑
l=1
log P (a0,l|a0,l−2, a0,l−1)
where wlm and wlen are negative constants indicating the relative weights of the language model probability and the length penalty.
",3.4 Incorporating an N-gram Language Model,[0],[0]
Extending states with two previous words results in a larger computational complexity.,3.4 Incorporating an N-gram Language Model,[0],[0]
"Given K sequences of length N each, we can have O(NK) distinct words.",3.4 Incorporating an N-gram Language Model,[0],[0]
"Therefore, the number distinct states is O(NbK(NK)2) i.e. O(N3K2bK).",3.4 Incorporating an N-gram Language Model,[0],[0]
"Each state can have O(K2K) successors, giving an overall computational complexity of O(N3K3bK2K(K2 + log N + log K + K log b)).",3.4 Incorporating an N-gram Language Model,[0],[0]
"Alternatively, if the vo-
cabulary size |V | is smaller than NK, the number of distinct states is bounded by O(NbK |V |2).",3.4 Incorporating an N-gram Language Model,[0],[0]
"Automated evaluation of speech to text captioning is known to be a challenging task (Wang et al., 2003).",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
Word Error Rate (WER) is the most commonly used metric that finds the best pairwise alignment between the candidate caption and the ground truth reference sentence.,3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"WER is estimated as S+I+D
N ,
where S, I , and D is the number of incorrect word substitutions, insertions, and deletions required to match the candidate sentence with reference, and N is the total number of words in the reference.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"WER has several nice properties such as: 1) it is easy to estimate, and 2) it tries to preserve word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"However, WER does not account for the overall ‘readability’ of text and thus does not always correlate well with human evaluation (Wang et al., 2003; He et al., 2011).
",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"The widely-used BLEU metric has been shown to agree well with human judgment for evaluating translation quality (Papineni et al., 2002).",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"However, unlike WER, BLEU imposes no explicit constraints on the word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"BLEU has been criticized as an ‘under-constrained’ measure (Callison-Burch et al., 2006) for allowing too much variation in word ordering.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Moreover, BLEU does not directly estimate recall, and instead relies on the brevity penalty.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Melamed et al. (2003) suggest that a better approach is to explicitly measure both precision and recall and combine them via F-measure.
",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Our application is similar to automatic speech recognition in that there is a single correct output, as opposed to machine translation where many outputs can be equally correct.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"On the other hand, unlike with ASR, out-of-order output is frequently produced by our alignment system when there is not enough overlap between the partial captions to derive the correct ordering for all words.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"It may be the case that even such out-of-order output can be of value to the user, and should receive some sort of partial credit that is not possible using WER.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"For this reason, we wished to systematically compare BLEU, F-measure, and WER as metrics for our task.
",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
We performed a study to evaluate the agreement of the three metrics with human judgment.,3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"We ran-
domly extracted one-minute long audio clips from four MIT OpenCourseWare lectures.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Each clip was transcribed by 7 human workers, and then aligned and combined using four different systems: the graph-based system, and three different versions of our weighted A∗ algorithm with different values of tuning parameters.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
Fifty people participated in the study and were split in two equal sized groups.,3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Each group was assigned two of the four audio clips, and each person evaluated all four captions for both clips.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Each participant assigned a score between 1 to 10 to these captions, based on two criteria: 1) the overall estimated agreement of the captions with the ground truth text, and 2) the readability and understandability of the captions.
",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Finally, we estimated the correlation coefficients (both Spearman and Pearson) for the three metrics discussed above with respect to the average score assigned by the human participants.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
The results are presented in Table 1.,3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"Among the three metrics, WER had the highest agreement with the human participants.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"This indicates that reconstructing the correct word order is in fact important to the users, and that, in this aspect, our task has more of the flavor of speech recognition than of machine translation.",3.5 Evaluation Metric for Speech to Text Captioning,[0],[0]
"We experiment with the MSA-A∗ algorithm for captioning different audio clips, and compare the results with two existing techniques.",4 Experimental Results,[0],[0]
Our experimental set up is similar to the experiments by Lasecki et al. (2012).,4 Experimental Results,[0],[0]
Our dataset consists of four 5-minute long audio clips extracted from lectures available on MIT OpenCourseWare.,4 Experimental Results,[0],[0]
The audio clips contain speech from electrical engineering and chemistry lectures.,4 Experimental Results,[0],[0]
Each audio clip is transcribed by ten non-expert human workers in real-time.,4 Experimental Results,[0],[0]
"We then combine these inputs using our MSA-A∗ algorithm, and also compare with the existing graph-based system and mul-
tiple sequence alignment using MUSCLE.
",4 Experimental Results,[0],[0]
"As explained earlier, we vary the four key parameters of the algorithm: the chunk size (c), the heuristic weight (w), the voting threshold (tv), and the beam size (b).",4 Experimental Results,[0],[0]
"The heuristic weight and chunk size parameters help us to trade-off between speed versus accuracy; the voting threshold tv helps improve precision by pruning words having less than tv votes, and beam size reduces the search space by restricting states to be inside a time window/beam.",4 Experimental Results,[0],[0]
"We use affine gap penalty (Edgar, 2004) with different gap opening and gap extension penalty.",4 Experimental Results,[0],[0]
We set gap opening penalty to 0.125 and gap extension penalty to 0.05.,4 Experimental Results,[0],[0]
"We evaluate the performance using the three standard metrics: Word Error Rate (WER), BLEU, and F-measure.",4 Experimental Results,[0],[0]
"The performance in terms of these metrics using different systems is presented in Figure 4.
",4 Experimental Results,[0],[0]
"Out of the five systems in Figure 4, the first three are different versions of our A∗ search based MSA algorithm with different parameter settings: 1) A∗10-t system (c = 10 seconds, tv = 2), 2) A∗-15-t (c = 15 seconds, tv = 2), and 3)",4 Experimental Results,[0],[0]
"A∗-15 (c = 15 seconds, tv = 1 i.e. no pruning while voting).",4 Experimental Results,[0],[0]
"For all three systems, the heuristic weight parameter w is set to 2.5 and beam size b = 20 seconds.",4 Experimental Results,[0],[0]
The other two systems are the existing graph-based system and multiple sequence alignment using MUSCLE.,4 Experimental Results,[0],[0]
"Among the three A∗ based algorithms, both A∗-15-t and A∗10-t produce better quality transcripts and outperform the existing algorithms.",4 Experimental Results,[0],[0]
Both systems apply the voting threshold that improves precision.,4 Experimental Results,[0],[0]
"The system A∗-15 applies no threshold and ends up producing many spurious words having poor agreement among the workers, and hence it scores worse in all the three metrics.",4 Experimental Results,[0],[0]
"The A∗-15-t achieves 57.4% average accuracy in terms of (1-WER), providing 29.6% improvement with respect to the graph-based system (average accuracy 42.6%), and 35.4% improvement with respect to the MUSCLE-based MSA system (average accuracy 41.9%).",4 Experimental Results,[0],[0]
"On the same set of audio clips, Lasecki et al. (2012) reported 36.6% accuracy using ASR (Dragon Naturally Speaking, version 11.5 for Windows), which is worse than all the crowd-based based systems used in this experiment.",4 Experimental Results,[0],[0]
"To measure the statistical significance of this improvement, we performed a t-test at both the dataset level (n = 4 clips) and the word level (n = 2862 words).",4 Experimental Results,[0],[0]
The improvement over the graph-based model was statistically significant with dataset level p-value 0.001 and word level p-value smaller than 0.0001.,4 Experimental Results,[0],[0]
"The average time to align each 15 second chunk with 10 input captions is ∼400 milliseconds.
",4 Experimental Results,[0],[0]
"We have also experimented with a trigram language model, trained on the British National Corpus (Burnard, 1995) having ∼122 million words.",4 Experimental Results,[0],[0]
The language-model-integrated A∗ search provided a negligible 0.21% improvement in WER over the A∗-15-t system on average.,4 Experimental Results,[0],[0]
The task of combining captions does not require recognizing words; it only requires aligning them in the correct order.,4 Experimental Results,[0],[0]
"This could explain why language model did not improve accuracy, as it does for speech recognition.",4 Experimental Results,[0],[0]
"Since the standard MSA-A∗ algorithm (without language model) produced comparable accuracy and faster running time, we used that version in the rest of the
experiments.
",4 Experimental Results,[0],[0]
"Next, we look at the critical speed versus accuracy trade-off for different values of the heuristic weight (w) and the chunk size (c) parameters.",4 Experimental Results,[0],[0]
"Since WER has been shown to correlate most with human judgment, we show the next results only with respect to WER.",4 Experimental Results,[0],[0]
"First, we fix the chunk size at different values, and then vary the heuristic weight parameter: w = 1.8, 2, 2.5, 3, 4, 6, and 8.",4 Experimental Results,[0],[0]
"The results are shown in Figure 5(a), where each curve represents how time and accuracy changed over the range of values of w and a fixed value of c. We observe that for smaller values of w, the algorithm is more accurate, but comparatively slower.",4 Experimental Results,[0],[0]
"As w increases, the search reaches the goal faster, but the quality of the solution degrades as well.",4 Experimental Results,[0],[0]
"Next, we fix w and vary chunk size c = 5, 10, 15, 20, 40, 60 second.",4 Experimental Results,[0],[0]
We repeat this experiment for a range of values of w and the results are shown in Figure 5(b).,4 Experimental Results,[0],[0]
"We can see that the accuracy improves steeply up to c = 20 seconds, and does not improve much beyond c = 40 seconds.",4 Experimental Results,[0],[0]
"For all these benchmarks, we set the beam size (b) to 20 seconds and voting threshold (tv) to 2.
",4 Experimental Results,[0],[0]
"In our tests, the beam size parameter (b) did not play a significant role in performance, and setting it to any reasonably large value (usually≥ 15 seconds) resulted in similar accuracy and running time.",4 Experimental Results,[0],[0]
"This is because the A∗ search with hpair heuristic already reduces the the search space significantly, and usually reaches the goal in a number of steps smaller than the state space size after the beam restriction.
",4 Experimental Results,[0],[0]
"Finally, we investigate how the accuracy of our algorithm varies with the number of inputs/workers.",4 Experimental Results,[0],[0]
We start with a pool of 10 input captions for one of the audio clips.,4 Experimental Results,[0],[0]
We vary the number of input captions (K) to the MSA-A∗ algorithm from 2 up to 10.,4 Experimental Results,[0],[0]
The quality of input captions differs greatly among the workers.,4 Experimental Results,[0],[0]
"Therefore, for each value of K, we repeat the experiment min ( 20, ( 10
K
))
times; each time we randomly select K input captions out of the total pool of 10.",4 Experimental Results,[0],[0]
"Figure 6 shows that accuracy steeply increases as the number of inputs increases to 7, and after that adding more workers does not provide much improvement in accuracy, but increases running time.",4 Experimental Results,[0],[0]
"In this paper, we show that the A∗ search based MSA algorithm performs better than existing algorithms for combining multiple captions.",5 Discussion and Future Work,[0],[0]
"The existing graph-based model has low latency, but it usually can not find a near optimal alignment because of its incremental alignment.",5 Discussion and Future Work,[0],[0]
"Weighted A∗ search on the other hand performs joint multiple sequence alignment, and is guaranteed to produce a solution having cost no more than (1 + ǫ) times the cost of the optimal solution, given a heuristic weight of (1+ ǫ).",5 Discussion and Future Work,[0],[0]
"Moreover, A∗ search allows for straightforward integration of an n-gram language model during the search.
",5 Discussion and Future Work,[0],[0]
"Another key advantage of the proposed algorithm is the ease with which we can trade-off between
speed and accuracy.",5 Discussion and Future Work,[0],[0]
The algorithm can be tailored to real-time by using a larger heuristic weight.,5 Discussion and Future Work,[0],[0]
"On the other hand, we can produce better transcripts for offline tasks by choosing a smaller weight.
",5 Discussion and Future Work,[0],[0]
It is interesting to compare our results with those achieved using the MUSCLE MSA tool of Edgar (2004).,5 Discussion and Future Work,[0],[0]
"One difference is that our system takes a hierarchical approach in that it aligns at the word level, but also uses string edit distance at the letter level as a substitution cost for words.",5 Discussion and Future Work,[0],[0]
"Thus, it is able to take advantage of the fact that individual transcriptions do not generally contain arbitrary fragments of words.",5 Discussion and Future Work,[0],[0]
"More fundamentally, it is interesting to note that MUSCLE and most other commonly used MSA tools for biological sequences make use of a guide tree formed by a hierarchical clustering of the input sequences.",5 Discussion and Future Work,[0],[0]
"The guide tree produced by the algorithms may or may not match the evolutionary tree of the organisms whose genomes are being aligned, but, nevertheless, in the biological application, such an underlying evolutionary tree generally exists.",5 Discussion and Future Work,[0],[0]
"In aligning transcriptions, there is no particular reason to expect individual pairs of transcriptions to be especially similar to one another, which may make the guide tree approach less appropriate.
",5 Discussion and Future Work,[0],[0]
"In order to get competitive results, the A∗ search based algorithm aligns sequences that are at least 7- 10 seconds long.",5 Discussion and Future Work,[0],[0]
"The delay for collecting the captions within a chunk can introduce latency, however,
each alignment usually takes less than 300 milliseconds, allowing us to repeatedly align the stream of words, even before the window is filled.",5 Discussion and Future Work,[0],[0]
This provides less accurate but immediate response to users.,5 Discussion and Future Work,[0],[0]
"Finally, when we have all the words entered in a chunk, we perform the final alignment and show the caption to users for the entire chunk.
",5 Discussion and Future Work,[0],[0]
"After aligning the input sequences, we obtain the final transcript by majority voting at each alignment position, which treats each worker equally and does not take individual quality into account.",5 Discussion and Future Work,[0],[0]
"Recently, some work has been done for automatically estimating individual worker’s quality for crowd-based data labeling tasks (Karger et al., 2011; Liu et al., 2012).",5 Discussion and Future Work,[0],[0]
Extending these methods for crowd-based text captioning could be an interesting future direction.,5 Discussion and Future Work,[0],[0]
"In this paper, we have introduced a new A∗ search based MSA algorithm for aligning partial captions into a final output stream in real-time.",6 Conclusion,[0],[0]
This method has advantages over prior approaches both in formal guarantees of optimality and the ability to trade off speed and accuracy.,6 Conclusion,[0],[0]
Our experiments on real captioning data show that it outperforms prior approaches based on a dependency graph model and a standard MSA implementation (MUSCLE).,6 Conclusion,[0],[0]
"An experiment with 50 participants explored whether exiting automatic metrics of quality matched human evaluations of readability, showing WER did best.
",6 Conclusion,[0],[0]
Acknowledgments Funded by NSF awards IIS1218209 and IIS-0910611.,6 Conclusion,[0],[0]
The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates.,abstractText,[0],[0]
"Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear.",abstractText,[0],[0]
"In this paper, we describe an improved method for combining partial captions into a final output based on weighted A search and multiple sequence alignment (MSA).",abstractText,[0],[0]
"In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds.",abstractText,[0],[0]
"Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%).",abstractText,[0],[0]
"The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem.",abstractText,[0],[0]
Text Alignment for Real-Time Crowd Captioning,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 763–772 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1071",text,[0],[0]
The number of pages required to print the content of the World Wide Web was estimated to 305 billion in a 2015 article1.,1 Background,[0],[0]
"While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace.",1 Background,[1.0],"['While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace.']"
"A recent study shows that the average webpage weights 1,200 KB with plain text accounting for up to 16% of that size2.
",1 Background,[0.9999999213477968],"['A recent study shows that the average webpage weights 1,200 KB with plain text accounting for up to 16% of that size2.']"
"While efficient distribution of textual data and computations are the key to deal with the increas-
1http://goo.gl/p9lt7V 2http://goo.gl/c41wpa
ing scale of textual search, similarity measures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information.",1 Background,[0],[0]
"These tasks are also often performed on dedicated document collections for domain-specific applications where text similarity measures can be directly applied.
",1 Background,[0],[0]
"Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015).",1 Background,[0],[0]
"However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora.
",1 Background,[0],[0]
"For instance, Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries with a length ranging from one word to short sequences of words.",1 Background,[0],[0]
"The proposed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs.",1 Background,[0],[0]
"In a more general study, Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task.",1 Background,[0],[0]
"In their experiments the best performance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in (Li et al., 2006).",1 Background,[0],[0]
"This word order similarity is computed by constructing first two vectors representing the common words between two given sentences and using their respective positions in the sentences as term
763
weights.",1 Background,[0],[0]
The similarity value is then obtained by subtracting the two vectors and taking the absolute value.,1 Background,[0],[0]
"While such representation takes into account the actual positions of the words, it does not allow detecting sub-sequence matches and takes into account missing words only by omission.
",1 Background,[0],[0]
"More generally, existing standalone (or traditional) text similarity measures rely on the intersections between token sets and/or text sizes and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein (Sankoff and Kruskal, 1983), Jaccard (Jain and Dubes, 1988) and Jaro (Jaro, 1989).",1 Background,[0],[0]
"The sequential nature of natural language is taken into account mostly through word n-grams and skipgrams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.
",1 Background,[0],[0]
"In this paper, we use intuitions from a common representation in DNA sequence alignment to design a new standalone similarity measure called TextFlow (XF).",1 Background,[0],[0]
The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches.,1 Background,[0],[0]
"Our contributions can be detailed further as follows:
•",1 Background,[0],[0]
"A novel standalone similarity measure which:
– exploits the full sequence of words in the compared texts.
– is asymmetric in a way that allows it to provide the best performance on different tasks (e.g., paraphrase detection, textual entailment and ranking).
– when required, it can be trained with a small set of parameters controlling the impact of sub-sequence matching, position gaps and unmatched words.
– provides consistent high performance across tasks and datasets compared to traditional similarity measures.
",1 Background,[0],[0]
"• A neural network architecture to train TextFlow parameters for specific tasks.
",1 Background,[1.0000000451165967],['• A neural network architecture to train TextFlow parameters for specific tasks.']
"• An empirical study on both performance consistency and standard evaluation measures, performed with eight datasets from three different tasks.
•",1 Background,[0],[0]
"A new evaluation measure, called CORE, used to better show the consistency of a system at high performance using both its rank average and rank variance when compared to competing systems over a set of datasets.
",1 Background,[0],[0]
"2 The TextFlow Similarity
XF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf. figure 1).",1 Background,[0.9738789372147009],['2 The TextFlow Similarity XF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf.']
We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf. figure 2).,1 Background,[0.9687554554265665],['We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf.']
The area under the curve is considered to be the distance between the two text pairs which is then normalized with the matrix surface.,1 Background,[0],[0]
"For practical computation, we transform this first intuitive representation using the delta of positions as in figure 3.",1 Background,[0],[0]
"In this setting, the Y axis is the delta of positions of a word occurring in the two texts being compared.",1 Background,[0],[0]
"If the word does not occur in the target text, the delta is considered to be a maximum reference value (l in figure 2).
",1 Background,[0],[0]
"The semantics are: the bigger the area under curve is, the lower the similarity between the compared texts.",1 Background,[1.0],"['The semantics are: the bigger the area under curve is, the lower the similarity between the compared texts.']"
"XF values are real numbers in the [0,1] interval, with 1 indicating a perfect match, and 0 indicating that the compared texts do not have any common tokens.",1 Background,[0],[0]
"With this representation, we are able to take into account all matched words and sub-sequences at the same time.",1 Background,[0],[0]
"The exact value for the XF similarity between two texts X = {x1, x2, .., xn} and Y = {y1, y2, .., ym} is therefore computed as:
XF (X,Y ) =",1 Background,[0],[0]
"1− 1 nm
n∑
i=2
1
Si Ti,i−1(X,Y )
",1 Background,[0],[0]
"− 1 nm
n∑
i=2
1
Si Ri,i−1(X,Y )
(1)
With Ti,i−1(X,Y ) corresponding to the triangular area in the [i − 1, i] step (cf. figure 3) and Ri,i−1(X,Y ) corresponding to the rectangular component.",1 Background,[0],[0]
"They are expressed as:
Ti,i−1(X,Y ) = |∆P",1 Background,[0],[0]
"(xi, X, Y )−∆P (xi−1, X, Y )",1 Background,[0],[0]
"|
2 (2)
and:
Ri,i−1(X,Y ) =",1 Background,[0],[0]
Min(∆P,1 Background,[0],[0]
"(xi, X, Y ),∆P (xi−1, X, Y ))",1 Background,[0],[0]
"(3)
With:
• ∆P (xi, X, Y ) the minimum difference between xi positions in X and Y .",1 Background,[0],[0]
xi position in X is multiplied by the factor |Y ||X| for normalization.,1 Background,[0],[0]
"If xi /∈ X ∩ Y , ∆P (xi, X, Y )
is set to the same reference value equal to m, (i.e., the cost of a missing word is set by default to the length of the target text), and:
• Si is the length of the longest matching sequence between X and Y including the word xi,",1 Background,[0.9930773951670847],"['If xi /∈ X ∩ Y , ∆P (xi, X, Y ) is set to the same reference value equal to m, (i.e., the cost of a missing word is set by default to the length of the target text), and: • Si is the length of the longest matching sequence between X and Y including the word xi, if xi ∈ X ∩ Y , or 1 otherwise.']"
"if xi ∈ X ∩ Y , or 1 otherwise.
",1 Background,[0],[0]
XF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input textX .,1 Background,[0],[0]
XF is an asymmetric similarity measure.,1 Background,[0],[0]
Its asymmetric aspect has interesting semantic applications as we show in the example below (cf. figure 2).,1 Background,[0],[0]
"The minimum value of XF provided the best differentiation between positive and negative text pairs when looking for semantic equivalence (i.e., paraphrases), the maximum value was among the top three for the textual entailment example.",1 Background,[0],[0]
"We conduct this comparison at a larger scale in the evaluation section.
",1 Background,[0],[0]
"We add 3 parameters to XF in order to represent the importance that should be given to position deltas (Position factor α), missing words (sensitivity factor β), and sub-sequence matching (sequence factor γ), such that:
XFα,β,γ(X,Y ) = 1− 1
βnm
n∑
i=2
α
Sγi T βi,i−1(X,Y )
",1 Background,[0],[0]
"− 1 βnm
n∑
i=2
α
Sγi Rβi,i−1(X,Y )
(4)
",1 Background,[0],[0]
"With:
T βi,i−1(X,Y ) = |∆βP",1 Background,[0],[0]
"(xi, X, Y )−∆βP (xi−1, X, Y )",1 Background,[0],[0]
"|
2 (5)
",1 Background,[0],[0]
"Rβi,i−1(X,Y ) =",1 Background,[0],[0]
"Min(∆βP (xi, X, Y ),∆βP (xi−1, X, Y )) (6) and:
• ∆βP (xi, X, Y ) = βm, if xi /∈ X ∩ Y
• α",1 Background,[0],[0]
<,1 Background,[0],[0]
β: forces missing words to always cost more than matched words.,1 Background,[0],[0]
•,1 Background,[0],[0]
"Sγi = {
1ifSi = 1orxi /∈",1 Background,[0],[0]
X ∩ Y γ SiforSi,1 Background,[0],[0]
>,1 Background,[0],[0]
"1
The γ factor increases or decreases the impact of sub-sequence matching, α applies to individual token matches whether inside or outside a sequence, and β increases or decreases the impact of
missing tokens as well as the normalization quantity βnm in equation 4 to keep the similarity values in the [0,1] range.",1 Background,[0],[0]
By default XF has canonical parameters set to 1.,2.1 Parameter Training,[0],[0]
"However, when needed, α, β, and γ can be learned on training data for a specific task.",2.1 Parameter Training,[0],[0]
"We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value.",2.1 Parameter Training,[0],[0]
"To do so we compute, for each input text pair, the coefficients vector that would lead exactly to the XF value when multiplied by the vector<",2.1 Parameter Training,[0],[0]
"αβ , α βγ , 1 >.",2.1 Parameter Training,[0],[0]
"Figure 5) presents the training neural network considering several types of sequences (or translations) of the input text pairs (e.g., lemmas, words, synsets).
",2.1 Parameter Training,[0],[0]
"We use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf. figure 6).",2.1 Parameter Training,[0],[0]
Datasets.,3 Evaluation,[0],[0]
"This evaluation was performed on 8 datasets from 3 different classification tasks: Tex-
tual Entailment Recognition, Paraphrase Detection, and ranking relevance.",3 Evaluation,[0.9999999459489001],"['This evaluation was performed on 8 datasets from 3 different classification tasks: Tex- tual Entailment Recognition, Paraphrase Detection, and ranking relevance.']"
"The datasets are as follows:
• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006).",3 Evaluation,[0],[0]
"Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment.",3 Evaluation,[0],[0]
"They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.
",3 Evaluation,[0],[0]
"• Guardian: an RTE dataset collected from 78,696 Guardian articles5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets.",3 Evaluation,[0],[0]
Positive examples were collected from the titles and first sentences.,3 Evaluation,[0],[0]
"Negative examples were collected from the same source by selecting consecutive sentences and random sentences.
",3 Evaluation,[0],[0]
"• SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with
5https://github.com/daoudclarke/ rte-experiment
3 labels: entailment, neutral and contradiction (Bowman et al., 2015).",3 Evaluation,[0],[0]
We discarded the contradiction pairs as they do not necessarily represent dissimilar sentences and are therefore a random noise w.r.t.,3 Evaluation,[0],[0]
"our similarity measure evaluation.
",3 Evaluation,[0],[0]
"• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.
•",3 Evaluation,[0.9949536392864246],"['• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.']"
"Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow (Nakov et al., 2016).",3 Evaluation,[0],[0]
"The dataset contains 3,169 training pairs and 700 test pairs.",3 Evaluation,[0],[0]
"Three labels are considered: ”Perfect Match”, ”Relevant” or ”Irrelevant”.",3 Evaluation,[0],[0]
"We combined the first two into the same positive category for our evaluation.
",3 Evaluation,[0],[0]
"• Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 English sentence pairs annotated with both similarity scores and relevance labels.
",3 Evaluation,[0],[0]
Features.,3 Evaluation,[0],[0]
"After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token:
• Word (plain text value)
• Lemma
• Part-Of-Speech (POS) tag
• WordNet Synset6",3 Evaluation,[0.9648615267006934],"['After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token: • Word (plain text value) • Lemma • Part-Of-Speech (POS) tag • WordNet Synset6 OR Lemma • WordNet Synset OR Lemma for Nouns • WordNet Synset OR Lemma for Verbs • WordNet Synset OR Lemma for Nouns and Verbs.']"
"OR Lemma
• WordNet Synset OR Lemma for Nouns
• WordNet Synset OR Lemma for Verbs
• WordNet Synset OR Lemma for Nouns and Verbs.
",3 Evaluation,[0],[0]
In the last 4 types of sequences the lemma is used when there is no corresponding WordNet synset.,3 Evaluation,[0],[0]
"In a first experiment we compare different aggregation functions on top of XF (minimum, maximum and average) in table 1.",3 Evaluation,[0],[0]
"We used the LibLinear7 SVM classifier for this task.
",3 Evaluation,[0],[0]
"In the second part of the evaluation, we use neural networks to compare the efficiency of XFc, XFt and other similarity measures with in the same setting.",3 Evaluation,[0],[0]
We use the neural net described in figure 5 for the trained versionXFt and the equivalent architecture presented in figure 6 for all other similarity measures.,3 Evaluation,[0],[0]
"For canonical XFc we use by default the best aggregation for the task as observed in table 3.
6https://wordnet.princeton.edu/ 7https://www.csie.ntu.edu.tw/˜cjlin/
liblinear/
Similarity Measures.",3 Evaluation,[1.0000000324418317],['For canonical XFc we use by default the best aggregation for the task as observed in table 3. liblinear/ Similarity Measures.']
"We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).",3 Evaluation,[0],[0]
Implementation.,3 Evaluation,[0],[0]
"XF was implemented in Java as an extension of the Simmetrics package, made available at this address9.",3 Evaluation,[0],[0]
The neural networks were implemented in Python with TensorFlow10.,3 Evaluation,[0],[0]
We also share the training sets used for both parameter training and evaluation.,3 Evaluation,[0],[0]
The evaluation was performed on a 4-core laptop with 32GB of RAM.,3 Evaluation,[0],[0]
The initial parameters for XFt were chosen with a random function.,3 Evaluation,[0],[0]
Evaluation Measures.,3 Evaluation,[0],[0]
"We use the standard accuracy values and F1, precision and recall for the
8https://github.com/Simmetrics/ simmetrics
9https://github.com/ymrabet/TextFlow 10https://www.tensorflow.org/
positive class (i.e., entailment, paraphrase, and ranking relevance).",3 Evaluation,[0],[0]
"We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called Consistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}:
CORE D,S,E (m) =
MIN p∈S ( AVG d∈D (RS(Ed(p))",3 Evaluation,[0],[0]
"+ Vd∈D(RS(Ed(p))) )
",3 Evaluation,[0],[0]
"AVG d∈D
( RS(Ed(m)) )",3 Evaluation,[0],[0]
+ Vd∈D ( RS(Ed(m)) ),3 Evaluation,[0],[0]
"(7)
With RS(Ed(m))",3 Evaluation,[0],[0]
the rank of m according to the evaluation measure E on dataset d w.r.t.,3 Evaluation,[0],[0]
competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets.,3 Evaluation,[1.0],['competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets.']
"The results in tables 2, 3, and 4 demonstrate the intuition.",3 Evaluation,[0],[0]
"Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems",3 Evaluation,[0],[0]
S. The maximum value of CORE is 1 for the best performing system according to its rank.,3 Evaluation,[0],[0]
"It also allows quantifying how consistently a system achieves high performance for the remaining systems.
",3 Evaluation,[0.9999999828918832],['It also allows quantifying how consistently a system achieves high performance for the remaining systems.']
"TextFlow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in (Achananuparp et al., 2008), with gaps of +1.0 in F1 and +6.1 accuracy on MSRP and +4.2 F1 and +2.7% accuracy on RTE 3.",3 Evaluation,[0],[0]
"4.1 Canonical Text Flow TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures.",4 Discussion,[0],[0]
It also reached the best precision average with a gap of +1.8 to +6.3.,4 Discussion,[0],[0]
"On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3 (cf. table 3).",4 Discussion,[0.9903477097465817],"['On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3 (cf.']"
"On a rank level, XFc had the best consistent rank for
accuracy, F1 and precision, and the second best for recall.",4 Discussion,[1.0000000818174484],"['On a rank level, XFc had the best consistent rank for accuracy, F1 and precision, and the second best for recall.']"
"When compared to state-of-the-art measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8.",4.2 Trained Text Flow,[0],[0]
"XFt also obtained the second best F1 average with a -1.0 gap, but with clear inconsistencies according to the dataset.",4.2 Trained Text Flow,[0],[0]
XFt obtained the best precision with a gap ranging from +0.8 to +7.1.,4.2 Trained Text Flow,[1.0],['XFt obtained the best precision with a gap ranging from +0.8 to +7.1.']
XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%.,4.2 Trained Text Flow,[1.0],['XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%.']
"Both its recall and F1 performance can be explained by the fact that the measure was trained to optimize accuracy, and not the F1 score for the positive class; which also suggests that the approach could be adapted to F1 optimization if needed.",4.2 Trained Text Flow,[0],[0]
"Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized.",4.3 Synthesis,[1.0],"['Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized.']"
"We argue that this consistency was made possible through the asymmetry of XF which allowed it to adapt to different kinds of similarities (i.e., equivalence/paraphrase, inference/entailment, and mutual distance/ranking).",4.3 Synthesis,[0],[0]
These results also show that the actual position difference is a relevant factor for text similarity.,4.3 Synthesis,[0],[0]
"We explain it mainly by the natural flow of language where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are
equivalent in meaning.",4.3 Synthesis,[0],[0]
"The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.",4.3 Synthesis,[0],[0]
"In additional experiments, we compared TFc and TFt with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens.",4.4 Additional Experiments,[0],[0]
The results were significantly lower on all datasets (between 3 and 10 points loss in accuracy) for both the soa measures and TextFlow variants.,4.4 Additional Experiments,[0],[0]
"This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of a new word inside a 3-words sequence leads to a tri-gram mismatch and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence.",4.4 Additional Experiments,[0],[0]
"This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will not cancel or reduce significantly the gains from the correct ordering of the words.",4.4 Additional Experiments,[0],[0]
"It must be noted here that not all languages grant the same level of importance to sequences and that additional multilingual tests have to be carried out.
",4.4 Additional Experiments,[0],[0]
"In addition to binary classification output such as textual entailment and paraphrase recognition, text similarity measures can be evaluated more precisely when we consider the correlation of their values for ranking purposes.
",4.4 Additional Experiments,[0],[0]
"We conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs.",4.4 Additional Experiments,[0],[0]
"The datasets have 750 sentence pairs each, and are extracted from
the Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl11.",4.4 Additional Experiments,[0],[0]
"When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correlation on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, noting that the Cosine similarity uses word frequencies when the evaluated version of TextFlow did not use word-level weights.
",4.4 Additional Experiments,[0],[0]
Including word weights is one of the promising perspectives in line with this work as it could be done simply by making the deltas vary according to the weight/importance of the (un)matched word.,4.4 Additional Experiments,[0],[0]
"Also, in such a setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. figure 3).",4.4 Additional Experiments,[0],[0]
"We conducted a preliminary test using the inverse document frequency of the words as extracted from Wikipedia with Gensim12, which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them.",4.4 Additional Experiments,[0],[0]
"Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances (Pedersen et al., 2004).",4.4 Additional Experiments,[0],[0]
We presented a novel standalone similarity measure that takes into account continuous word sequences.,5 Conclusion,[0],[0]
"An evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking.",5 Conclusion,[0],[0]
"Among the potential extensions of this work are the inclusion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness.",5 Conclusion,[0],[0]
"We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words.",5 Conclusion,[0],[0]
"This work was supported in part by the Intramural Research Program of the NIH, National Library of Medicine.
",Acknowledgements,[0],[0]
11goo.gl/NVnybD 12https://radimrehurek.com/gensim/,Acknowledgements,[0],[0]
"Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment.",abstractText,[0],[0]
"While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language.",abstractText,[0],[0]
Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts.,abstractText,[0],[0]
In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms.,abstractText,[0],[0]
"The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value.",abstractText,[0],[0]
"Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",abstractText,[0],[0]
TextFlow: A Text Similarity Measure based on Continuous Sequences,title,[0],[0]
