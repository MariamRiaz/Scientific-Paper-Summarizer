0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3622–3631 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3622",text,[0],[0]
"Despite the massive success brought by neural machine translation (NMT, Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), it has been noticed that the vanilla NMT often lags behind conventional machine translation systems, such as statistical phrase-based translation systems (PBMT, Koehn et al., 2003), for low-resource language pairs (see, e.g., Koehn and Knowles, 2017).",1 Introduction,[0.960582479690606],"['(iii) Finally, we compare the performance of our algorithms with several state of the art algorithms (Grill et al., 2015; Huang et al., 2006b; Jones et al., 1998; Kandasamy et al., 2016b;b; Srinivas et al., 2009) for black-box optimization in the multi-fidelity setting, on real and synthetic data-sets.']"
"In the past few years, various approaches have been proposed to address this issue.",1 Introduction,[0],[0]
"The first attempts at tackling this problem exploited the availability of monolingual corpora (Gulcehre
* Equal contribution.
",1 Introduction,[0],[0]
"et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016).",1 Introduction,[0],[0]
"It was later followed by approaches based on multilingual translation, in which the goal was to exploit knowledge from high-resource language pairs by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Lee et al., 2016; Johnson et al., 2016; Ha et al., 2016b).",1 Introduction,[0],[0]
"Its variant, transfer learning, was also proposed by Zoph et al. (2016), in which an NMT system is pretrained on a high-resource language pair before being finetuned on a target low-resource language pair.
",1 Introduction,[0],[0]
"In this paper, we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation.",1 Introduction,[0],[0]
"We start by arguing that the recently proposed model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) could be applied to low-resource machine translation by viewing language pairs as separate tasks.",1 Introduction,[0],[0]
This view enables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples (§3).,1 Introduction,[0],[0]
"Furthermore, the vanilla MAML however cannot handle tasks with mismatched input and output.",1 Introduction,[0],[0]
"We overcome this limitation by incorporating the universal lexical representation (Gu et al., 2018b) and adapting it for the meta-learning scenario (§3.3).
",1 Introduction,[0],[0]
We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation.,1 Introduction,[0],[0]
"We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages (Ro, Lv, Fi, Tr and Ko), in all cases translating to English.",1 Introduction,[0],[0]
"Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation
approach across all the target language pairs, and the gap grows as the number of training examples decreases.",1 Introduction,[0],[0]
Neural Machine Translation (NMT),2 Background,[0],[0]
"Given a source sentence X = {x1, ..., xT 0}, a neural machine translation model factors the distribution over possible output sentences Y = {y1, ..., yT } into a chain of conditional probabilities with a leftto-right causal structure:
p(Y |X; ✓) = T+1Y
t=1
p(yt|y0:t 1, x1:T 0 ; ✓), (1)
where special tokens y0 (hbosi) and yT+1 (heosi) are used to represent the beginning and the end of a target sentence.",2 Background,[0],[0]
These conditional probabilities are parameterized using a neural network.,2 Background,[0],[0]
"Typically, an encoder-decoder architecture (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) with a RNN-based decoder is used.",2 Background,[0],[0]
"More recently, architectures without any recurrent structures (Gehring et al., 2017; Vaswani et al., 2017) have been proposed and shown to speed up training while achieving state-of-the-art performance.
",2 Background,[0],[0]
"Low Resource Translation NMT is known to easily over-fit and result in an inferior performance when the training data is limited (Koehn and Knowles, 2017).",2 Background,[0],[0]
"In general, there are two ways for handling the problem of low resource translation: (1) utilizing the resource of unlabeled monolingual data, and (2) sharing the knowledge between low- and high-resource language pairs.",2 Background,[0],[0]
"Many research efforts have been spent on incorporating the monolingual corpora into machine translation, such as multi-task learning (Gulcehre et al., 2015; Zhang and Zong, 2016), back-translation (Sennrich et al., 2015), dual learning (He et al., 2016) and unsupervised machine translation with monolingual corpora only for both sides (Artetxe et al., 2017b; Lample et al., 2017; Yang et al., 2018).
",2 Background,[0],[0]
"For the second approach, prior researches have worked on methods to exploit the knowledge of auxiliary translations, or even auxiliary tasks.",2 Background,[0],[0]
"For instance, Cheng et al. (2016); Chen et al. (2017); Lee et al. (2017); Chen et al. (2018) investigate the use of a pivot to build a translation path between two languages even without any directed resource.",2 Background,[0],[0]
The pivot can be a third language or even an image in multimodal domains.,2 Background,[0],[0]
"When pivots are
not easy to obtain, Firat et al. (2016a); Lee et al. (2016); Johnson et al. (2016) have shown that the structure of NMT is suitable for multilingual machine translation.",2 Background,[0],[0]
"Gu et al. (2018b) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages.
",2 Background,[0],[0]
"All the previous work for multilingual NMT assume the joint training of multiple high-resource languages naturally results in a universal space (for both the input representation and the model) which, however, is not necessarily true, especially for very low resource cases.
",2 Background,[0],[0]
"Meta Learning In the machine learning community, meta-learning, or learning-to-learn, has recently received interests.",2 Background,[0],[0]
Meta-learning tries to solve the problem of “fast adaptation on new training data.”,2 Background,[0],[0]
"One of the most successful applications of meta-learning has been on few-shot (or oneshot) learning (Lake et al., 2015), where a neural network is trained to readily learn to classify inputs based on only one or a few training examples.",2 Background,[0],[0]
"There are two categories of meta-learning:
1.",2 Background,[0],[0]
"learning a meta-policy for updating model parameters (see, e.g., Andrychowicz et al., 2016; Ha et al., 2016a; Mishra et al., 2017)
2.",2 Background,[0],[0]
"learning a good parameter initialization for fast adaptation (see, e.g., Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017).
",2 Background,[0],[0]
"In this paper, we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category.",2 Background,[0],[0]
"More specifically, we extend the idea of model-agnostic metalearning (MAML, Finn et al., 2017) in the multilingual scenario.",2 Background,[0],[0]
"The underlying idea of MAML is to use a set of source tasks T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K to find the initialization of parameters ✓0 from which learning a target task T 0 would require only a small number of training examples.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"In the context of machine translation, this amounts to using many high-resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame-
ters.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"This process can be understood as
✓⇤ = Learn(T 0;MetaLearn(T 1, . . .",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
", T K)).
",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"That is, we meta-learn the initialization from auxiliary tasks and continue to learn the target task.",3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
We refer the proposed meta-learning method for NMT to MetaNMT.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
See Fig. 1 for the overall illustration.,3 Meta Learning for Low-Resource Neural Machine Translation,[0],[0]
"Given any initial parameters ✓0 (which can be either random or meta-learned),
the prior distribution of the parameters of a desired NMT model can be defined as an isotropic Guassian:
✓i ⇠ N (✓0i , 1/ ),
where 1/ is a variance.",3.1 Learn: language-specific learning,[0],[0]
"With this prior distribution, we formulate the language-specific learning process Learn(DT ; ✓0) as maximizing the logposterior of the model parameters given data DT :
Learn(DT ; ✓0) = argmax ✓ LDT (✓)
= argmax
✓
X
(X,Y )2DT
log p(Y |X, ✓) k✓ ✓0k2,
where we assume p(X|✓) to be uniform.",3.1 Learn: language-specific learning,[0],[0]
The first term above corresponds to the maximum likelihood criterion often used for training a usual NMT system.,3.1 Learn: language-specific learning,[0],[0]
"The second term discourages the newly learned model from deviating too much from the initial parameters, alleviating the issue of overfitting when there is not enough training data.",3.1 Learn: language-specific learning,[0],[0]
"In practice, we solve the problem above by maximizing the first term with gradient-based optimization and early-stopping after only a few update steps.
",3.1 Learn: language-specific learning,[0],[0]
"Thus, in the low-resource scenario, finding a good initialization ✓0 strongly correlates the final performance of the resulting model.",3.1 Learn: language-specific learning,[0],[0]
"We find the initialization ✓0 by repeatedly simulating low-resource translation scenarios using auxiliary, high-resource language pairs.",3.2 MetaLearn,[0],[0]
"Following Finn et al. (2017), we achieve this goal by defining the meta-objective function as
L(✓) =EkEDT k ,D0T k (2)2
64 X
(X,Y )2D0 T k
log p(Y |X;Learn(DT k ; ✓))
3
75 ,
where k ⇠ U({1, . . .",3.2 MetaLearn,[0],[0]
",K}) refers to one metalearning episode, and DT , D0T follow the uniform distribution over T ’s data.
",3.2 MetaLearn,[0],[0]
"We maximize the meta-objective function using stochastic approximation (Robbins and Monro, 1951) with gradient descent.",3.2 MetaLearn,[0],[0]
"For each episode, we uniformly sample one source task at random, T k.",3.2 MetaLearn,[0],[0]
"We then sample two subsets of training examples independently from the chosen task, DT k and D0T k .",3.2 MetaLearn,[0],[0]
We use the former to simulate languagespecific learning and the latter to evaluate its outcome.,3.2 MetaLearn,[0],[0]
"Assuming a single gradient step is taken only the with learning rate ⌘, the simulation is:
✓0k = Learn(DT k ;",3.2 MetaLearn,[0],[0]
✓) =,3.2 MetaLearn,[0],[0]
"✓ ⌘r✓LDT k (✓).
",3.2 MetaLearn,[0],[0]
"Once the simulation of learning is done, we evaluate the updated parameters ✓0k on D 0 T k , The gradient computed from this evaluation, which we refer to as meta-gradient, is used to update the
meta model ✓.",3.2 MetaLearn,[0],[0]
"It is possible to aggregate multiple episodes of source tasks before updating ✓:
✓ ✓ ⌘0 X
k
r✓LD 0 T k (✓0k),
where ⌘0 is the meta learning rate.",3.2 MetaLearn,[0],[0]
"Unlike a usual learning scenario, the resulting model ✓0 from this meta-learning procedure is not necessarily a good model on its own.",3.2 MetaLearn,[0],[0]
It is however a good starting point for training a good model using only a few steps of learning.,3.2 MetaLearn,[0],[0]
"In the context of machine translation, this procedure can be understood as finding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high-resource language pairs.
",3.2 MetaLearn,[0],[0]
"Meta-Gradient We use the following approximation property
H(x)v ⇡ r(x+ ⌫v) r(x) ⌫
to approximate the meta-gradient:1
r✓LD 0",3.2 MetaLearn,[0],[0]
(✓0),3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)r✓(✓ ⌘r✓LD(✓))
",3.2 MetaLearn,[0],[0]
= r✓0LD 0,3.2 MetaLearn,[0],[0]
(✓0) ⌘,3.2 MetaLearn,[0],[0]
r✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0)H✓(LD(✓))
⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
"(✓0) ⌘
⌫
 r✓LD(✓)
",3.2 MetaLearn,[0],[0]
"✓̂ r✓LD(✓) ✓ ,
where ⌫ is a small constant and
ˆ✓ = ✓ + ⌫r",3.2 MetaLearn,[0],[0]
✓0LD 0,3.2 MetaLearn,[0],[0]
"(✓0).
",3.2 MetaLearn,[0],[0]
"In practice, we find that it is also possible to ignore the second-order term, ending up with the following simplified update rule:
r✓LD 0 (✓0) ⇡ r✓0LD 0",3.2 MetaLearn,[0],[0]
(✓0).,3.2 MetaLearn,[0],[0]
"(3)
1We omit the subscript k for simplicity.
",3.2 MetaLearn,[0],[0]
"Related Work: Multilingual Transfer Learning The proposed MetaNMT differs from the existing framework of multilingual translation (Lee et al., 2016; Johnson et al., 2016; Gu et al., 2018b) or transfer learning (Zoph et al., 2016).",3.2 MetaLearn,[0],[0]
"The latter can be thought of as solving the following problem:
max ✓ Lmulti(✓) =",3.2 MetaLearn,[0],[0]
"Ek
2 4 X
(X,Y )2Dk
log p(Y |X; ✓)
3
5 ,
where Dk is the training set of the k-th task, or language pair.",3.2 MetaLearn,[0],[0]
"The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ✓0 found from solving the above problem.
",3.2 MetaLearn,[0],[0]
"The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target, low-resource language pair.",3.2 MetaLearn,[0],[0]
The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq.,3.2 MetaLearn,[0],[0]
(2).,3.2 MetaLearn,[0],[0]
"As we will see later in the experiments, this results in a substantial gap in the final performance on the low-resource task.
",3.2 MetaLearn,[0],[0]
"Illustration In Fig. 2, we contrast transfer learning, multilingual learning and meta-learning using three source language pairs (Fr-En, Es-En and Pt-En) and two target pairs (Ro-En and Lv-En).",3.2 MetaLearn,[0],[0]
"Transfer learning trains an NMT system specifically for a source language pair (Es-En) and finetunes the system for each target language pair (RoEn, Lv-En).",3.2 MetaLearn,[0],[0]
"Multilingual learning often trains a single NMT system that can handle many different language pairs (Fr-En, Pt-En, Es-En), which may or may not include the target pairs (Ro-En, LvEn).",3.2 MetaLearn,[0],[0]
"If not, it finetunes the system for each target pair, similarly to transfer learning.",3.2 MetaLearn,[0],[0]
Both of these however aim at directly solving the source tasks.,3.2 MetaLearn,[0],[0]
"On the other hand, meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks.",3.2 MetaLearn,[0],[0]
"This is done by repeatedly simulating the learning process on
low-resource languages using many high-resource language pairs (Fr-En, Pt-En, Es-En).",3.2 MetaLearn,[0],[0]
I/O mismatch across language pairs One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks.,3.3 Unified Lexical Representation,[0],[0]
"This, however, does not apply to machine translation in general due to the vocabulary mismatch across different languages.",3.3 Unified Lexical Representation,[0],[0]
"In multilingual translation, this issue has been tackled by using a vocabulary of sub-words (Sennrich et al., 2015) or characters (Lee et al., 2016) shared across multiple languages.",3.3 Unified Lexical Representation,[0],[0]
"This surface-level sharing is however limited, as it cannot be applied to languages exhibiting distinct orthography (e.g., IndoEuroepan languages vs. Korean.)
",3.3 Unified Lexical Representation,[0],[0]
"Universal Lexical Representation (ULR) We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network (Miller et al., 2016; Gulcehre et al., 2018), as was done successfully for low-resource machine translation recently by Gu et al. (2018b).",3.3 Unified Lexical Representation,[0],[0]
"We start with multilingual word embedding matrices ✏kquery 2 R|Vk|⇥d pretrained on large monolingual corpora, where Vk is the vocabulary of the k-th language.",3.3 Unified Lexical Representation,[0],[0]
"These embedding vectors can be obtained with small dictionaries of seed word pairs (Artetxe et al., 2017a; Smith et al., 2017) or in a fully unsupervised manner (Zhang et al., 2017; Conneau et al., 2018).",3.3 Unified Lexical Representation,[0],[0]
"We take one of these languages k0 to build universal lexical representation consisting of a universal embedding matrix ✏u 2 RM⇥d and a corresponding key matrix ✏key 2 RM⇥d, where M < |V 0k|.",3.3 Unified Lexical Representation,[0],[0]
Both ✏kquery and ✏key are fixed during meta-learning.,3.3 Unified Lexical Representation,[0],[0]
"We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by
✏0[x] = MX
i=1
↵i✏u[i],
where ↵i / exp 1⌧ ✏key[i]",3.3 Unified Lexical Representation,[0],[0]
>A✏kquery[x] and ⌧ is set to 0.05.,3.3 Unified Lexical Representation,[0],[0]
"This approach allows us to handle languages with different vocabularies using a fixed number of shared parameters (✏u, ✏key and A.)
",3.3 Unified Lexical Representation,[0],[0]
"Learning of ULR It is not desirable to update the universal embedding matrix ✏u when fine-
tuning on a small corpus which contains a limited set of unique tokens in the target language, as it could adversely influence the other tokens’ embedding vectors.",3.3 Unified Lexical Representation,[0],[0]
"We thus estimate the change to each embedding vector induced by languagespecific learning by a separate parameter ✏k[x]:
✏k[x] = ✏0[x] + ✏k[x].
",3.3 Unified Lexical Representation,[0],[0]
"During language-specific learning, the ULR ✏0[x] is held constant, while only ✏k[x] is updated, starting from an all-zero vector.",3.3 Unified Lexical Representation,[0],[0]
"On the other hand, we hold ✏k[x]’s constant while updating ✏u and A during the meta-learning stage.",3.3 Unified Lexical Representation,[0],[0]
"Target Tasks We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages: Romanian (Ro) from WMT’16,2 Latvian (Lv), Finnish (Fi), Turkish (Tr) from WMT’17,3 and Korean (Ko) from Korean Parallel Dataset.4 We use the officially provided train, dev and test splits for all these languages.",4.1 Dataset,[0],[0]
The statistics of these languages are presented in Table 1.,4.1 Dataset,[0],[0]
"We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes.
",4.1 Dataset,[0],[0]
"Source Tasks We use the following languages from Europarl5: Bulgarian (Bg), Czech (Cs), Danish (Da), German (De), Greek (El), Spanish (Es), Estonian (Et), French (Fr), Hungarian (Hu), Italian (It), Lithuanian (Lt), Dutch (Nl), Polish (Pl), Portuguese (Pt), Slovak (Sk), Slovene (Sl) and
2 http://www.statmt.org/wmt16/translation-task.html 3 http://www.statmt.org/wmt17/translation-task.html 4 https://sites.google.com/site/koreanparalleldata/ 5 http://www.statmt.org/europarl/
Swedish (Sv), in addition to Russian (Ru)6 to learn the intilization for fine-tuning.",4.1 Dataset,[0],[0]
"In our experiments, different combinations of source tasks are explored to see the effects from the source tasks.
",4.1 Dataset,[0],[0]
Validation We pick either Ro-En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks.,4.1 Dataset,[0],[0]
"This allows us to study the strict form of metalearning, in which target tasks are unknown during both training and model selection.
",4.1 Dataset,[0],[0]
"Preprocessing and ULR Initialization As described in §3.3, we initialize the query embedding vectors ✏kquery of all the languages.",4.1 Dataset,[0],[0]
"For each language, we use the monolingual corpora built from Wikipedia7 and the parallel corpus.",4.1 Dataset,[0],[0]
"The concatenated corpus is first tokenized and segmented using byte-pair encoding (BPE, Sennrich et al., 2016), resulting in 40, 000 subwords for each language.",4.1 Dataset,[0],[0]
"We then estimate word vectors using fastText (Bojanowski et al., 2016) and align them across all the languages in an unsupervised way
6 A subsample of approximately 2M pairs from WMT’17.",4.1 Dataset,[0],[0]
"7 We use the most recent Wikipedia dump (2018.5) from
https://dumps.wikimedia.org/backup-index.html.
using MUSE (Conneau et al., 2018) to get multilingual word vectors.",4.1 Dataset,[0],[0]
"We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ✏u.",4.1 Dataset,[0],[0]
"Model We utilize the recently proposed Transformer (Vaswani et al., 2017) as an underlying NMT system.",4.2 Model and Learning,[0],[0]
"We implement Transformer in this paper based on (Gu et al., 2018a)8 and modify it to use the universal lexical representation from §3.3.",4.2 Model and Learning,[0],[0]
"We use the default set of hyperparameters (dmodel = dhidden = 512, nlayer = 6, nhead = 8, nbatch = 4000, twarmup = 16000) for all the language pairs and across all the experimental settings.",4.2 Model and Learning,[0],[0]
"We refer the readers to (Vaswani et al., 2017; Gu et al., 2018a) for the details of the model.",4.2 Model and Learning,[0],[0]
"However, since the proposed metalearning method is model-agnostic, it can be easily extended to any other NMT architectures, e.g. RNN-based sequence-to-sequence models with attention (Bahdanau et al., 2015).
",4.2 Model and Learning,[0],[0]
8,4.2 Model and Learning,[0],[0]
"https://github.com/salesforce/nonauto-nmt
Learning We meta-learn using various sets of source languages to investigate the effect of source task choice.",4.2 Model and Learning,[0],[0]
"For each episode, by default, we use a single gradient step of language-specific learning with Adam (Kingma and Ba, 2014) per computing the meta-gradient, which is computed by the first-order approximation in Eq.",4.2 Model and Learning,[0],[0]
"(3).
",4.2 Model and Learning,[0],[0]
"For each target task, we sample training examples to form a low-resource task.",4.2 Model and Learning,[0],[0]
"We build tasks of 4k, 16k, 40k and 160k English tokens for each language.",4.2 Model and Learning,[0],[0]
We randomly sample the training set five times for each experiment and report the average score and its standard deviation.,4.2 Model and Learning,[0],[0]
"Each fine-tuning is done on a training set, early-stopped on a validation set and evaluated on a test set.",4.2 Model and Learning,[0],[0]
"In default without notation, datasets of 16k tokens are used.
",4.2 Model and Learning,[0],[0]
"Fine-tuning Strategies The transformer consists of three modules; embedding, encoder and decoder.",4.2 Model and Learning,[0],[0]
"We update all three modules during metalearning, but during fine-tuning, we can selectively tune only a subset of these modules.",4.2 Model and Learning,[0],[0]
"Following (Zoph et al., 2016), we consider three fine-tuning
strategies; (1) fine-tuning all the modules (all), (2) fine-tuning the embedding and encoder, but freezing the parameters of the decoder (emb+enc) and (3) fine-tuning the embedding only (emb).",4.2 Model and Learning,[0],[0]
vs. Multilingual Transfer Learning We metalearn the initial models on all the source tasks using either Ro-En or Lv-En as a validation task.,5 Results,[0],[0]
We also train the initial models to be multilingual translation systems.,5 Results,[0],[0]
"We fine-tune them using the four target tasks (Ro-En, Lv-En, Fi-En and Tr-En; 16k tokens each) and compare the proposed meta-learning strategy and the multilingual, transfer learning strategy.",5 Results,[0],[0]
"As presented in Fig. 3, the proposed learning approach significantly outperforms the multilingual, transfer learning strategy across all the target tasks regardless of which target task was used for early stopping.",5 Results,[0],[0]
We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches.,5 Results,[0],[0]
"With the proposed meta-learning and emb+enc fine-tuning, the final NMT systems trained using only a fraction of all available training examples achieve 2/3 (Ro-En) and 1/2 (Lv-En, Fi-En and Tr-En) of the BLEU score achieved by the models trained with full training sets.
",5 Results,[0],[0]
"vs. Statistical Machine Translation We also test the same Ro-En datasets with 16, 000 target tokens using the default setting of Phrase-based MT (Moses) with the dev set for adjusting the parameters and the test set for calculating the final performance.",5 Results,[0],[0]
"We obtain 4.79(±0.234) BLEU point, which is higher than the standard NMT performance (0 BLEU).",5 Results,[0],[0]
"It is however still lower than both the multi-NMT and meta-NMT.
",5 Results,[0],[0]
"Impact of Validation Tasks Similarly to training any other neural network, meta-learning still requires early-stopping to avoid overfitting to a
specific set of source tasks.",5 Results,[0],[0]
"In doing so, we observe that the choice of a validation task has nonnegligible impact on the final performance.",5 Results,[0],[0]
"For instance, as shown in Fig. 3, Fi-En benefits more when Ro-En is used for validation, while the opposite happens with Tr-En.",5 Results,[0],[0]
"The relationship between the task similarity and the impact of a validation task must be investigated further in the future.
",5 Results,[0],[0]
"Training Set Size We vary the size of the target task’s training set and compare the proposed meta-learning strategy and multilingual, transfer learning strategy.",5 Results,[0],[0]
We use the emb+enc fine-tuning on Ro-En and Fi-En.,5 Results,[0],[0]
Fig. 4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task’s training set.,5 Results,[0],[0]
"The gap between the meta-learning and transfer learning grows as the size shrinks, confirming the effectiveness of the proposed approach on extremely lowresource language pairs.
",5 Results,[0],[0]
"Impact of Source Tasks In Table 2, we present the results on all five target tasks obtained while varying the source task set.",5 Results,[0],[0]
We first see that it is always beneficial to use more source tasks.,5 Results,[0],[0]
"Although the impact of adding more source tasks varies from one language to another, there is up to 2⇥ improvement going from one source task to 18 source tasks (Lv-En, Fi-En, Tr-En and Ko-En).",5 Results,[0],[0]
"The same trend can be observed even without any fine-tuning (i.e., unsupervised translation, (Lample et al., 2017; Artetxe et al., 2017b)).",5 Results,[0],[0]
"In addition, the choice of source languages has different implications for different target languages.",5 Results,[0],[0]
"For instance, Ro-En benefits more from {Es, Fr, It, Pt} than from {De, Ru}, while the opposite effect is observed with all the other target tasks.
",5 Results,[0],[0]
Training Curves,5 Results,[0],[0]
The benefit of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig. 5.,5 Results,[0],[0]
"With the multilingual, transfer learning ap-
",5 Results,[0],[0]
"proach, we observe that training rapidly saturates and eventually degrades, as the model overfits to the source tasks.",5 Results,[0],[0]
MetaNMT,5 Results,[0],[0]
"on the other hand continues to improve and never degrades, as the metaobjective ensures that the model is adequate for fine-tuning on target tasks rather than for solving the source tasks.
",5 Results,[0],[0]
Sample Translations We present some sample translations from the tested models in Table 3.,5 Results,[0],[0]
Inspecting these examples provides the insight into the proposed meta-learning algorithm.,5 Results,[0],[0]
"For instance, we observe that the meta-learned model without any fine-tuning produces a word-by-word translation in the first example (Tr-En), which is due to the successful use of the universal lexcial representation and the meta-learned initialization.",5 Results,[0],[0]
"The system however cannot reorder tokens from Turkish to English, as it has not seen any training example of Tr-En.",5 Results,[0],[0]
"After seeing around 600 sentence pairs (16K English tokens), the model rapidly learns to correctly reorder tokens to form a better translation.",5 Results,[0],[0]
A similar phenomenon is observed in the Ko-En example.,5 Results,[0],[0]
These cases could be found across different language pairs.,5 Results,[0],[0]
"In this paper, we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high-resource languages pairs.",6 Conclusion,[0],[0]
"We based the proposed algorithm on the recently proposed model-agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation, resulting in MetaNMT.",6 Conclusion,[0],[0]
"Our extensive evaluation, using 18 high-resource source tasks and 5 low-resource target tasks, has shown that the proposed MetaNMT significantly outperforms the existing approach of multilingual, transfer learning in low-resource neural machine translation across all the language pairs considered.
",6 Conclusion,[0],[0]
The proposed approach opens new opportunities for neural machine translation.,6 Conclusion,[0],[0]
"First, it is a principled framework for incorporating various extra sources of data, such as source- and targetside monolingual corpora.",6 Conclusion,[0],[0]
"Second, it is a generic framework that can easily accommodate existing and future neural machine translation systems.",6 Conclusion,[0],[0]
This research was supported in part by the Facebook Low Resource Neural Machine Translation Award.,Acknowledgement,[0],[0]
This work was also partly supported by Samsung Advanced Institute of Technology (Next Generation Deep Learning: from pattern recognition to AI) and Samsung Electronics (Improving Deep Learning using Latent Structure).,Acknowledgement,[0],[0]
"KC thanks support by eBay, TenCent, NVIDIA and CIFAR.",Acknowledgement,[0],[0]
"In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for lowresource neural machine translation (NMT).",abstractText,[0],[0]
"We frame low-resource translation as a metalearning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks.",abstractText,[0],[0]
"We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages.",abstractText,[0],[0]
"We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks.",abstractText,[0],[0]
"We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples.",abstractText,[0],[0]
"For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (⇠ 600 parallel sentences).",abstractText,[0],[0]
Meta-Learning for Low-Resource Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 375–385 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Online platforms have revolutionized the way individuals collect and share information (O’Connor et al., 2010; Lee and Ma, 2012; Bakshy et al., 2015), but the vast bulk of online content is irrelevant or unpalatable to any given individual.",1 Introduction,[0],[0]
"A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015).
",1 Introduction,[0],[0]
"How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users?",1 Introduction,[0],[0]
We approach this problem from a microblog conversation recommendation framework.,1 Introduction,[0],[0]
"Where prior work has focused on the content of individual posts for recommendation (Chen
et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al.,",1 Introduction,[0],[0]
"2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new
1In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument.",1 Introduction,[0],[0]
"The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes.
375
and repeated entry into conversations based on a combination of topical and discourse features.
",1 Introduction,[0],[0]
"To illustrate the interplay between topics and discourse, Figure 1 displays two snippets of conversations on Twitter collected during the 2016 United States presidential election.",1 Introduction,[0],[0]
User U1 participates in both conversations.,1 Introduction,[0],[0]
"The first conversation is centered around Clinton, and U1, who is more typically involved with conversations about candidate Sanders, does not return.",1 Introduction,[0],[0]
"In the second conversation, however, U1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.
",1 Introduction,[0],[0]
"Effective conversation prediction and recommendation requires an understanding of both user interests and discourse behaviors, such as agreement, disagreement, inquiry, backchanneling, and emotional reactions.",1 Introduction,[0],[0]
"However, acquiring manual labels for both is a time-consuming process and hard to scale for new datasets.",1 Introduction,[0],[0]
"We instead propose a unified statistical learning framework for conversation recommendation, which jointly learns (1) hidden factors that reflect user interests based on conversation history, and (2) topics and discourse modes in ongoing conversations, as discovered by a novel probabilistic latent variable model.",1 Introduction,[0],[0]
"Our model is built on the success of collaborative filtering (CF) in recommendation systems, where latent dimensions of product ratings or movie reviews are extracted to better capture user preferences (Linden et al., 2003; Salakhutdinov and Mnih, 2008; Wang and Blei, 2011; McAuley and Leskovec, 2013).",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to model both topics and discourse modes as part of a CF framework and apply it to microblog conversation recommendation.2
Experimental results on two Twitter conversation datasets show that our proposed model yields significantly better performance than state-of-theart post-level recommendation systems.",1 Introduction,[0],[0]
"For example, by leveraging both topical content and discourse structure, our model achieves a mean average precision (MAP) of 0.76 on conversations about the U.S. presidential election, compared with 0.70 by McAuley and Leskovec (2013), which only considers topics.",1 Introduction,[0],[0]
"We further con-
2To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications.
ducted detailed analysis on the latent topics and discourse modes and find that our model can discover reasonable topic and discourse representations, which play an important role in characterizing reply behaviors.",1 Introduction,[0],[0]
"Finally, we also provide a pilot study on recommendation for first time replies, which shows that our model outperforms comparable recommendation systems.
",1 Introduction,[0],[0]
The rest of this paper is structured as follows.,1 Introduction,[0],[0]
The related work is discussed in Section 2.,1 Introduction,[0],[0]
We then present our microblog conversation recommendation model in Section 3.,1 Introduction,[0],[0]
The experimental setup and results are described in Sections 4 and 5.,1 Introduction,[0],[0]
"Finally, we conclude in Section 6.",1 Introduction,[0],[0]
"Social media has attracted increasing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011).",2 Related Work,[0],[0]
"The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post.",2 Related Work,[0],[0]
"Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015).",2 Related Work,[0],[0]
"These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016).
",2 Related Work,[0],[0]
"Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level.",2 Related Work,[0],[0]
"In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure.",2 Related Work,[0],[0]
"Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011).
",2 Related Work,[0],[0]
"Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017).",2 Related Work,[0],[0]
"Topic modeling
has been employed to identify conversation content on Twitter (Ritter et al., 2010).",2 Related Work,[0],[0]
"In this work, we propose a probabilistic model to capture both topics and discourse modes as latent variables.",2 Related Work,[0],[0]
"A further line of work studies the reposting and reply structure of conversations (Gómez et al., 2011; Laniado et al., 2011; Backstrom et al., 2013; Budak and Agrawal, 2013).",2 Related Work,[0],[0]
"But none of this work distinguishes the rich discourse functions of replies, which is modeled and exploited in our work.",2 Related Work,[0],[0]
Our proposed microblog conversation recommendation framework is based on collaborative filtering and a novel probabilistic graphical model.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Concretely, our objective function takes the form:
minL+ µ ·NLL(C |Θ) (1)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
This function encodes two types of information.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"First, L models user reply preference in a similar fashion to collaborative filtering (CF) (Hu et al., 2008; Pan et al., 2008).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"It captures topics of interests and discourse structures users are commonly involved (e.g., argumentation), and takes the form of mean square error (MSE) based on user reply history.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"This part is detailed in Section 3.1.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The second term, NLL(C |Θ), denotes the negative log-likelihood of a set of conversations C, with Θ containing all parameters.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"A probabilistic model is described in Section 3.2 that shows how the topical content and discourse structures of conversations are captured by these latent variables.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
The hyperparameter µ controls the trade-off between the two effects.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"`2 regularization is also added for parameters to avoid model overfitting.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For the rest of this section, we first present the construction of L andNLL(C |Θ) in Sections 3.1 and 3.2.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We then discuss how these two components can be mutually informed by each other in Section 3.3.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Finally, the generative process and parameter learning are described in Section 3.4.
3.1 Reply Preference (L) Our user reply preference modeling is built on the success of collaborative filtering (CF) for product ratings.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"However, classic CF problems, such as product recommendation, generally rely on explicit user feedback.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Unlike user ratings on products, our input lacks explicit feedback from users about negative preferences and nonresponse.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we follow one-class Collaborative Filtering (Hu et al., 2008; Pan et al., 2008),
which weights positive instances higher during training and is thus suited to our data.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c. ru,c equals 1 if u is in the conversation history; otherwise, it is 0.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The first term of objective (Eq. 1) takes the following form:
L = |U|∑
u=1
|C|∑
c=1
fu,c · (pu,c − ru,c)2 (2)
where U consists of users {u} and C is a set of conversations {c} in a dataset.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"fu,c is the corresponding weight for a conversation c and a target user u. Intuitively, it has a large value if positive feedback (user replied) is observed.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, we adapt the formulation from Pan et al. (2008):
fu,c = { s if ru,c = 1 (i.e., user replied) 1 if ru,c = 0
(3)
where s > 1, an integer hyperparameter to be tuned.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Inspired by prior models (Koren et al., 2009; McAuley and Leskovec, 2013), we propose the following latent factor model to describe pu,c:
pu,c = λ · γUu · γCc + (1− λ) ·",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
δUu · δCc,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"+ bu + bc + a (4)
γUu and γ C c are K-dimensional latent vectors that encode topic-specific information (where K is the number of latent topics) for users and conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, γUu reflects the topical interests of u, with higher value γUu,k indicating greater interest by u in topic k. γCc captures the extents that topics are discussed in conversation c.
Similarly, D-dimensional vectors δUu and δ C c capture discourse structures in shaping reply behaviors (where D is the number of discourse clusters).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"δUu reflects the discourse behaviors u prefers, such as u1 often enjoys arguments as in the second conversation of Figure 1, while δCc captures the discourse modes used throughout conversation c. By multiplying user and conversation factors, we can measure the corresponding similarity.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The predicted score pu,c thereby reflects the tendency for a user u to be involved in conversation c.
As pointed out by McAuley and Leskovec (2013), these latent vectors often encode hidden factors that are hard to interpret under a CF framework.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, in Section 3.2, we present a novel probabilistic model which can extract interpretable topics and discourse modes as word
distributions.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We then describe how they can be aligned with the latent vectors of γC and δU .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Parameter a is an offset parameter, bu and bc are user and conversation biases, and λ ∈",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"[0, 1] serves as the weight for trading offs of topic and discourse factors in reply preference modeling.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
3.2 Corpus Likelihood NLL(C |Θ),3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Here we present a novel probabilistic model that learns coherent word distributions for latent topics and discourse modes of conversations.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Formally, we assume that each conversation c ∈ C contains Mc messages, and each message m has Nc,m words.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We distinguish three latent components – discourse, topic, and background – underlying conversations, each with their own type of word distribution.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"At the corpus level, there are K topics represented by word distribution φTk (k = 1, 2, ...,K), while φDd (d = 1, 2, ..., D) represents the D discourse modes embedded in corpus.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"In addition, we add a background word distribution φB to capture general information (e.g., common words), which do not indicate either discourse or topic information.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"φDd , φ T k , and φ
B are all multinomial word distributions over vocabulary size V .",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Below describes more details.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Message-level Modeling.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Our model assigns two types of message-level multinomial variables to each message: zc,m reflects its latent topic and dc,m represents its discourse mode.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Topic assignments.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Due to the short nature of microblog posts, we assume each message m in conversation c contains only one topic, indexed as zc,m. This strategy has been proven useful to alleviate data sparsity for topic inference (Quan et al., 2015).",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
We further assume messages in the same conversation would focus on similar topics.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We thus draw topic zc,m ∼ θc, where θc denotes the fractions of topics discussed in conversation c.
Discourse assignments.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"To capture discourse behaviors of u, distribution πu is used to represent the discourse modes in messages posted by u.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"The discourse mode dc,m for message m is then generated from πuc,m , where uc,m is the author of m in c.
Word-level Modeling.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We aim to separate discourse, topic, and background information for conversations.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Therefore, for each word wc,m,n of message m, a ternary switcher xc,m,n ∈ {DISC, TOPIC,BACK} controls word wc,m,n to
fall into one of the three types: discourse, topic, and background.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Discourse words (DISC) are indicative of the discourse modes of messages.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = DISC (i.e., wc,m,n is assigned as a discourse word), word wc,m,n is generated from the discourse word distribution φDdc,m where dc,m is discourse assignment to",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"message m.
Topic words (TOPIC) describe the topical focus of a conversation.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When xc,m,n = TOPIC, wc,m,n is assigned as a topic word and generated from φTzc,m – word distribution given topic of m.
Background words (BACK) capture the general information that is not related to discourse or topic.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"When word wc,m,n is assigned as a background word (xc,m,n = BACK), it is drawn from background distribution φB .
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Switching among Topic, Discourse, and Background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"We further assume the word type switcher xc,m,n is sampled from a multinomial distribution which depends on the current discourse mode dc,m. The intuition is that messages of different discourse modes may show different distributions of the three word types.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"For instance, a statement message may contain more content words than a rhetorical question.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Specifically, xc,m,n ∼ Multi(τdc,m), where τd is a 3-dimension stochastic vector that expresses the appearing probabilities of three kinds of words (DISC, TOPIC, BACK), when the discourse assignment is d. Stop words and punctuations are forced to be labeled as discourse or background.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"By explicitly distinguishing different types of words with switcher xc,m,n, we can thus separate word distributions that reflect discourse, topic, and background information.
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
Likelihood.,3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Based on the message-level and the word-level generation process, the probability of observing words in the given corpus is:
Pr(C |θ,π,φ, τ , z,d,x)
=
C∏
c=1
Mc∏
m=1
θc,zc,mπuc,m,dc,m
× ∏
xc,m,n=BACK
τdc,m,BACKφ B wc,m,n
× ∏
xc,m,n=DISC
τdc,m,DISCφ D dc,m,wc,m,n
× ∏
xc,m,n=TOPIC
τdc,m,TOPICφ T zc,m,wc,m,n
(5)
",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"And we use negative log likelihood to model corpus likelihood effect in Eq. 1, i.e., NLL(C |Θ) =
− log(Pr(C |Θ), where parameters set Θ = {θ,π,φ, τ , z,d,x}.",3 The Joint Model of Topic and Discourse for Recommendation,[0],[0]
"Latent Variables
As mentioned above, the hidden factors discovered in Section 3.1 lack interpretability, which can be boosted by the learned latent topics and discourse modes in Section 3.2.",3.3 Mutually Informed User Preference and,[0],[0]
"However, it is nontrivial to link the topic-related parameters of γCc to the conversation topic distributions of θc, since the former takes real values from −∞ to +∞ while the latter is a stochastic vector.",3.3 Mutually Informed User Preference and,[0],[0]
"Therefore, we follow the strategy from McAuley and Leskovec (2013) to apply a softmax function over γCc :
θc,k = exp(κT γCc,k)∑K
k′=1 exp(κ T γCc,k′)
(6)
",3.3 Mutually Informed User Preference and,[0],[0]
"We further assume that the discourse mode preference by users, δUu , can also be informed by the discourse mode distribution captured by πu, i.e., a user who enjoys arguments may be willing to participate another.",3.3 Mutually Informed User Preference and,[0],[0]
"So similarly, we define:
πu,d = exp(κDδUu,d)∑D
d′=1 exp(κ DδUu,d′)
(7)
where κT and κD are learnable parameters that control the “peakiness” of the transformation.",3.3 Mutually Informed User Preference and,[0],[0]
"For example, a larger κT indicates a more focused conversation, while a smaller κT means users discuss diverse topics.
",3.3 Mutually Informed User Preference and,[0],[0]
"Finally, softmax transformation is also applied to φTk , φ D d , φ
B , and τd, as done in McAuley and Leskovec (2013), with additional parameters ψTk , ψDd , ψ
B , and χd (as shown in Figure 2).",3.3 Mutually Informed User Preference and,[0],[0]
This is to ensure that the distributions φ∗∗ and τd are stochastic vectors.,3.3 Mutually Informed User Preference and,[0],[0]
"In doing so, these distributions can be learned via optimizing ψ∗∗ and χd, which take any value and thus ensure that the cost function in Eq. 1 is optimized without considering any parameter constraints.",3.3 Mutually Informed User Preference and,[0],[0]
"Our word generation process is displayed in Figure 2 and described as follows:
• Compute topic distribution θc by Eq. 6 •",3.4 Generative Process and Model Learning,[0],[0]
"For message m = 1 to Mc:
– Compute discourse distribution πuc,m by Eq. 7 – Draw topic assignment zc,m ∼Multi(θc) – Draw discourse mode dc,m ∼Multi(πuc,m) –",3.4 Generative Process and Model Learning,[0],[0]
"For word index n = 1 to Nc,m: ∗ Draw word type xc,m,n ∼Multi(τd)
∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == BACK:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φB) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == DISC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φDdc,m) ∗",3.4 Generative Process and Model Learning,[0],[0]
"if xc,m,n == TOPIC:",3.4 Generative Process and Model Learning,[0],[0]
"Draw word wc,m,n ∼Multi(φTzc,m)
Parameter Learning.",3.4 Generative Process and Model Learning,[0],[0]
"For learning, we randomly initialize all learnable parameters and then alternate between the following two steps: Step 1.",3.4 Generative Process and Model Learning,[0],[0]
"Fix topic and discourse assignments z and d, and word type switcher x, then optimize the remaining parameters in Eq. 1 by L-BFGS (Nocedal, 1980):
Update a, b, γ∗, δ∗, κ∗, ψ∗, χ = argminL+ µ ·NLL(C |Θ) (8)
Step 2.",3.4 Generative Process and Model Learning,[0],[0]
"Sample topic and discourse assignments z and d at the message level and word type switcher x at the word level, using the distributions, computed according to parameters optimized in step 1:
Sample zc,m, dc,m, xc,m,n with probabilities p(zc,m = k) =",3.4 Generative Process and Model Learning,[0],[0]
"θc,k
p(dc,m = d) = πuc,m,d p(xc,m,n = BACK) = φ B wc,m,nτdc,m,BACK p(xc,m,n = DISC) = φ D dc,m,wc,m,nτdc,m,DISC p(xc,m,n = TOPIC) = φ T zc,m,wc,m,nτdc,m,TOPIC
(9)
Step 2 is analogous to Gibbs Sampling (Griffiths, 2002) in probabilistic graphical models, such as LDA (Blei et al., 2003).",3.4 Generative Process and Model Learning,[0],[0]
"However, distinguishing from previous models, the multinomial distributions in our models are not drawn from a Dirichlet prior.",3.4 Generative Process and Model Learning,[0],[0]
"Instead, they are computed based on the parameters learned in Step 1.
",3.4 Generative Process and Model Learning,[0],[0]
"Our learning process stops when the change of parameters is small (i.e., below a pre-specified
threshold).",3.4 Generative Process and Model Learning,[0],[0]
"Multiple restarts are tried, and similar results are achieved.",3.4 Generative Process and Model Learning,[0],[0]
Datasets.,4 Experimental Setup,[0],[0]
"We collected two microblog conversation datasets from Twitter for experiments3: one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC)4.",4 Experimental Setup,[0],[0]
"US Election was collected from January to June of 2016 using Twitter’s Streaming API5 with a small set of political keywords.6 To recover conversations, Tweet Search API7 was used to retrieve messages with the “inreply-to” relations to collect tweets in a recursive way until full conversations were recovered.
",4 Experimental Setup,[0],[0]
Statistics of the datasets are shown in Table 1.,4 Experimental Setup,[0],[0]
Figure 3 displays the number of conversations individual users participated in.,4 Experimental Setup,[0],[0]
"As can be seen, most users are involved in only a few conversations.",4 Experimental Setup,[0],[0]
"Simply leveraging personal chat history will not produce good performance for conversation
3The datasets are available at http://www.ccs. neu.edu/home/luwang/
4 http://trec.nist.gov/data/tweets/ 5https://developer.twitter.com/
en/docs/tweets/filter-realtime/ api-reference/post-statuses-filter.html
6Keyword list: “trump”, “hillary”, “clinton”, “president”, “politics”, and “election.”
",4 Experimental Setup,[0],[0]
"7https://developer.twitter.com/en/ docs/tweets/search/api-reference/ get-saved_searches-show-id
recommendation.",4 Experimental Setup,[0],[0]
"In our experiments, we predict whether a user will engage in a conversation given the previous messages in that conversation and past conversations the user is involved.",4 Experimental Setup,[0],[0]
"For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.8
Preprocessing and Hyperparameter Tuning.",4 Experimental Setup,[0],[0]
"For preprocessing, links, mentions (i.e., @username), and hashtags in tweets were replaced with generic tags of “URL”, “MENTION”, and “HASHTAG”.",4 Experimental Setup,[0],[0]
We then utilized the Twitter NLP tool9,4 Experimental Setup,[0],[0]
"(Gimpel et al., 2011; Owoputi et al., 2013) for tokenization and non-alphabetic token removal.",4 Experimental Setup,[0],[0]
We removed stop words and punctuations for all comparisons to ensure comparable performance.,4 Experimental Setup,[0],[0]
"We maintain a vocabulary with the 5,000 most frequent words.
",4 Experimental Setup,[0],[0]
"Our model parameters are tuned on the development set based on grid search, i.e. the parameters that give the lowest value for our objective are selected.",4 Experimental Setup,[0],[0]
"Specifically, the number of discourse modes (D) and topics (K) are tuned to be 10.",4 Experimental Setup,[0],[0]
"The trade-off parameter µ between user preference and corpus negative log-likelihood takes value of 0.1, and λ, the parameter for balancing topic and discourse, is set to 0.5.",4 Experimental Setup,[0],[0]
"Finally, the confidence parameter s takes a value of 200 to give higher weight for positive instances, i.e., a user replied to a conversation.
",4 Experimental Setup,[0],[0]
Evaluation Metrics.,4 Experimental Setup,[0],[0]
"Following prior work on social media post recommendation (Chen et al., 2012; Yan et al., 2012), we treat our task on conversation recommendation as a ranking problem.",4 Experimental Setup,[0],[0]
"Therefore, popular information retrieval evaluation metrics, including precision at K (P@K), mean average precision (MAP) (Manning et al., 2008), and normalized Discounted Cumulative Gain at K (nDCG@K) (Järvelin and Kekäläinen, 2002) are reported.",4 Experimental Setup,[0],[0]
The metrics are computed per user in the dataset and then averaged over all users.,4 Experimental Setup,[0],[0]
"The values range from 0.0 to 1.0, with higher values indicating better performance.
",4 Experimental Setup,[0],[0]
Baselines and Comparisons.,4 Experimental Setup,[0],[0]
"For comparison, we first consider three baselines: 1) ranking
8At least one turn per conversation is retained for training.",4 Experimental Setup,[0],[0]
"It is possible that one user only replies in either development set or test set, but it is rather infrequent.
",4 Experimental Setup,[0],[0]
"9http://www.cs.cmu.edu/˜ark/TweetNLP/
conversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).
",4 Experimental Setup,[0],[0]
"We further compare results with three established recommendation models: • OCCF: one-class Collaborative Filtering (Pan et al., 2008), which only considers users’ reply history without modeling content in conversations.",4 Experimental Setup,[0],[0]
• RSVM:,4 Experimental Setup,[0],[0]
"ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in Duan et al. (2010).",4 Experimental Setup,[0],[0]
"• CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied (Chen et al., 2012).
",4 Experimental Setup,[0],[0]
"Finally, we also adapt the “hidden factors as topics” (HFT) model proposed in McAuley and Leskovec (2013) (henceforth ADAPTED HFT).",4 Experimental Setup,[0],[0]
"Because the original model leverages the ratings for all product reviews and does not handle implicit user feedback well, we replace their user preference objective function with ours (Eq. 2).",4 Experimental Setup,[0],[0]
"In this section, we first discuss our main evaluation in Section 5.1.",5 Experimental Results,[0],[0]
"A case study and corresponding discussion are provided in Section 5.2 to provide further insights, which is followed by an analysis of the topics and discourse modes discovered by our model (Section 5.3).",5 Experimental Results,[0],[0]
We also examine our performance on first time replies (Section 5.4).,5 Experimental Results,[0],[0]
"Experimental results are displayed in Table 2, where our model yields statistically significantly better results than baselines and comparisons
(paired t-tests, p < 0.01).",5.1 Conversation Recommendation Results,[0],[0]
"For P@K, we only report P@1, because a significant amount of users participate only in 1 or 2 conversations.",5.1 Conversation Recommendation Results,[0],[0]
"For nDCG@K, different K values are experimented, which results in similar trend, so only nDCG@5 is reported.
",5.1 Conversation Recommendation Results,[0],[0]
"We find that the baselines that rank conversations with simple features (e.g., length or popularity) perform poorly.",5.1 Conversation Recommendation Results,[0],[0]
"This implies that generic algorithms that do not consider conversation content or user preference cannot produce reasonable recommendations.
",5.1 Conversation Recommendation Results,[0],[0]
"Although some non-baseline systems capture content in one way or another, only ADAPTED HFT and our model exploit latent topic models to better represent content in tweets, and outperform other methods.
",5.1 Conversation Recommendation Results,[0],[0]
"Compared to ADAPTED HFT, which only considers latent topics under a collaborative filtering framework, our model extracts both topics and discourse modes as latent variables, and shows superior performance on both datasets.",5.1 Conversation Recommendation Results,[0],[0]
"Our discourse variables go beyond topical content to capture social behaviors that affect user engagement, such as
arguments, question-asking, agreement, and other discourse modes.
",5.1 Conversation Recommendation Results,[0],[0]
Training with Varying Conversation History.,5.1 Conversation Recommendation Results,[0],[0]
"To test the model performance based different levels of user engagement history, we further experiment with varying the length of conversations for training.",5.1 Conversation Recommendation Results,[0],[0]
"Specifically, in addition to using 75% of conversation history, we also extract the first 25% and 50% of history as training.",5.1 Conversation Recommendation Results,[0],[0]
The rest of a conversation is separated equally for development and test.,5.1 Conversation Recommendation Results,[0],[0]
Figure 4 shows the MAP scores for US Election and TREC datasets.,5.1 Conversation Recommendation Results,[0],[0]
"The increasing MAP for all methods as the training history increases indicates that generally, conversation history is essential for recommendation.",5.1 Conversation Recommendation Results,[0],[0]
"Our model performs consistently better over different lengths of conversation histories.
",5.1 Conversation Recommendation Results,[0],[0]
Results for Varying Degree of Data Sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"From Table 1 and Figure 3, we observe that most users in our datasets are involved in only a few conversations.",5.1 Conversation Recommendation Results,[0],[0]
"In order to study the effects of data sparsity on recommendation models, we examine in Figure 5 the MAP scores for users engaged in a varying number of conversations, as measured on the TREC dataset.",5.1 Conversation Recommendation Results,[0],[0]
The results on the US Election dataset have similar distributions.,5.1 Conversation Recommendation Results,[0],[0]
"As we see, the prediction results become worse for users involved in fewer conversations.",5.1 Conversation Recommendation Results,[0],[0]
This indicates that data sparsity serves as a challenge for all recommendation models.,5.1 Conversation Recommendation Results,[0],[0]
We also observe that our model performs consistently better than other models over different degrees of sparsity.,5.1 Conversation Recommendation Results,[0],[0]
"This implies that effectively capturing discourse structure in conversation context is useful to mitigating the effects of
data sparsity on conversation recommendation.",5.1 Conversation Recommendation Results,[0],[0]
Here we present a case study based on the sample conversations in Figure 1.,5.2 Case Study and Discussion,[0],[0]
"Recall that user U1 is interested in conversations about Sanders, and also prefers more argumentative discourse, and thus returns in conversation c2 but not c1.
",5.2 Case Study and Discussion,[0],[0]
"Table 3 shows the predicted scores for the two conversations from OCCF, ADAPTED HFT, and our model (as in Eq. 2).",5.2 Case Study and Discussion,[0],[0]
"Both ADAPTED HFT and our model more accurately recommend c2 over c1, with our model producing a slightly higher recommendation score for c2.
",5.2 Case Study and Discussion,[0],[0]
Table 4 shows the latent dimension values for the learned topics and discourse modes for this user and these two conversations.,5.2 Case Study and Discussion,[0],[0]
"Based on human inspection, topic 1 appears to contain words about Sanders, which is the main topic in conversation c2.",5.2 Case Study and Discussion,[0],[0]
"Topic 2 is about Clinton, which is a dominating topic in conversation c1.",5.2 Case Study and Discussion,[0],[0]
"Our model also picks up user interest in topic 1 (Sanders), and thus assigns γUu1,1 a high value.",5.2 Case Study and Discussion,[0],[0]
"For discourse modes, our model also generates a high score for “argument” discourse (labeled via human inspection) for both the user and c2.",5.2 Case Study and Discussion,[0],[0]
Ablation Study.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"We have shown that joint modeling of topical content and discourse modes produces the superior performance for our model.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Here we provide an ablation study to examine the relative contributions of those two aspects by setting the trade-off parameter λ to 1.0 (topic only) or 0.0 (discourse only).,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Table 5 shows that topics or discourse individually improve slightly upon the comparison ADAPTED HFT, but only jointly do they improve significantly upon it.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Topic Coherence.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"To examine the quality of topics found by our model, we use the CV topic coherence score measured via the open-source toolkit Palmetto10, which has been shown to produce evaluation performance comparable to human judgment (Röder et al., 2015).",5.3 Further Analysis of Topic and Discourse,[0],[0]
"Our model achieves topic coherence scores of 0.343 and 0.376 on TREC and US Election datasets, compared to 0.338 and 0.371 for the topics from ADAPTED HFT.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
Sample Discourse Modes.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"While our topic word distributions are relatively unsurprising, of greater interest are the discourse mode word distributions.",5.3 Further Analysis of Topic and Discourse,[0],[0]
Table 6 shows a sample of discourse modes as labeled by human.,5.3 Further Analysis of Topic and Discourse,[0],[0]
"Although this is merely a qualitative human judgment at this point, there does appear to be a notable overlap in discourse modes between the two datasets even though they were learned separately.
",5.3 Further Analysis of Topic and Discourse,[0],[0]
10https://github.com/AKSW/Palmetto/,5.3 Further Analysis of Topic and Discourse,[0],[0]
"From a recommendation perspective, users may be interested in joining new conversations.",5.4 First Time Reply Results,[0],[0]
We thus compare each recommendation system for first time replies.,5.4 First Time Reply Results,[0],[0]
"For each user, we only evaluate for conversations where they are newcomers.",5.4 First Time Reply Results,[0],[0]
"Table 7 shows that, unsurprisingly, all systems perform poorly on this task, though our model performs slightly better.",5.4 First Time Reply Results,[0],[0]
"This suggests that other features, e.g., network structures or other discussion thread features, could usefully be included in future studies that target new conversations.",5.4 First Time Reply Results,[0],[0]
This paper has presented a framework for microblog conversation recommendation via jointly modeling topics and discourse modes.,6 Conclusion,[0],[0]
Experimental results show that our method can outperform competitive approaches that omit user discourse behaviors.,6 Conclusion,[0],[0]
Qualitative analysis shows that our joint model yields meaningful topics and discourse representations.,6 Conclusion,[0],[0]
This work is partly supported by Innovation and Technology Fund (ITF) Project,Acknowledgements,[0],[0]
"No. 6904333, General Research Fund (GRF) Project No. 14232816 (12183516), and National Science Foundation Grant IIS-1566382.",Acknowledgements,[0],[0]
"We thank Shuming Shi, Yan Song, and the three anonymous reviewers for the insightful suggestions on various aspects of this work.",Acknowledgements,[0],[0]
Millions of conversations are generated every day on social media platforms.,abstractText,[0],[0]
"With limited attention, it is challenging for users to select which discussions they would like to participate in.",abstractText,[0],[0]
Here we propose a new method for microblog conversation recommendation.,abstractText,[0],[0]
"While much prior work has focused on postlevel recommendation, we exploit both the conversational context, and user content and behavior preferences.",abstractText,[0],[0]
"We propose a statistical model that jointly captures: (1) topics for representing user interests and conversation content, and (2) discourse modes for describing user replying behavior and conversation dynamics.",abstractText,[0],[0]
Experimental results on two Twitter datasets demonstrate that our system outperforms methods that only model content without considering discourse.,abstractText,[0],[0]
Microblog Conversation Recommendation via Joint Modeling of Topics and Discourse,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 102–112 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space.",1 Introduction,[0],[0]
"These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts.
",1 Introduction,[0],[0]
"However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications.",1 Introduction,[0],[0]
The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods.,1 Introduction,[0],[0]
"These challenges are particularly acute when working with lowresource languages, where even unlabeled data may be difficult to obtain at scale.",1 Introduction,[0],[0]
"A typical solution is to abandon hope, by assigning a single OOV embedding to all terms that do not appear in the unlabeled data.
",1 Introduction,[0],[0]
We approach this challenge from a quasigenerative perspective.,1 Introduction,[0],[0]
"Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter.",1 Introduction,[0],[0]
"We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task.",1 Introduction,[0],[0]
"We call this model the MIMICK-RNN, for its ability to read a word’s spelling and mimick its distributional embedding.
",1 Introduction,[0],[0]
"Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features.",1 Introduction,[0],[0]
"As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors.",1 Introduction,[0],[0]
"Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words.
",1 Introduction,[0],[0]
"As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset (De Marneffe et al., 2014).",1 Introduction,[0],[0]
"Our model shows significant improvement
102
across the board against a single UNK-embedding backoff method, and obtains competitive results against a supervised character-embedding model, which is trained end-to-end on the target task.",1 Introduction,[0],[0]
"In low-resource settings, our approach is particularly effective, and is complementary to supervised character embeddings trained from labeled data.",1 Introduction,[0],[0]
The MIMICK-RNN therefore provides a useful new tool for tagging tasks in settings where there is limited labeled data.,1 Introduction,[0],[0]
Models and code are available at www.github.com/ yuvalpinter/mimick .,1 Introduction,[0],[0]
Compositional models for embedding rare and unseen words.,2 Related Work,[0],[0]
"Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure.",2 Related Work,[0],[0]
Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word’s morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings.,2 Related Work,[0],[0]
"While morphology-based approaches make use of meaningful linguistic substructures, they struggle with names and foreign language words, which include out-of-vocabulary morphemes.",2 Related Work,[0],[0]
"Character-based approaches avoid these problems: for example, Kim et al. (2016) train a recurrent neural network over words, whose embeddings are constructed by convolution over character embeddings; Wieting et al. (2016) learn embeddings of character ngrams, and then sum them into word embeddings.",2 Related Work,[0],[0]
"In all of these cases, the model for composing embeddings of subword units into word embeddings is learned by optimizing an objective over a large unlabeled corpus.",2 Related Work,[0],[0]
"In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained.",2 Related Work,[0],[0]
"This is similar to the “retrofitting” approach of Faruqui et al. (2015), but rather than smoothing embeddings over a graph, we learn a function to build embeddings compositionally.
",2 Related Work,[0],[0]
Supervised subword models.,2 Related Work,[0],[0]
Another class of methods learn task-specific character-based word embeddings within end-to-end supervised systems.,2 Related Work,[0],[0]
"For example, Santos and Zadrozny (2014) build word embeddings by convolution over char-
acters, and then perform part-of-speech (POS) tagging using a local classifier; the tagging objective drives the entire learning process.",2 Related Work,[0],[0]
"Ling et al. (2015) propose a multi-level long shortterm memory (LSTM; Hochreiter and Schmidhuber, 1997), in which word embeddings are built compositionally from an LSTM over characters, and then tagging is performed by an LSTM over words.",2 Related Work,[0],[0]
Plank et al. (2016) show that concatenating a character-level or bit-level LSTM network to a word representation helps immensely in POS tagging.,2 Related Work,[0],[0]
"Because these methods learn from labeled data, they can cover only as much of the lexicon as appears in their labeled training sets.",2 Related Work,[0],[0]
"As we show, they struggle in several settings: lowresource languages, where labeled training data is scarce; morphologically rich languages, where the number of morphemes is large, or where the mapping from form to meaning is complex; and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scripts.",2 Related Work,[0],[0]
"Furthermore, supervised subword models can be combined with MIMICK, offering additive improvements.
",2 Related Work,[0],[0]
Morphosyntactic attribute tagging.,2 Related Work,[0],[0]
"We evaluate our method on the task of tagging word tokens for their morphosyntactic attributes, such as gender, number, case, and tense.",2 Related Work,[0],[0]
"The task of morpho-syntactic tagging dates back at least to the mid 1990s (Oflazer and Kuruöz, 1994; Hajič and Hladká, 1998), and interest has been rejuvenated by the availability of large-scale multilingual morphosyntactic annotations through the Universal Dependencies (UD) corpus (De Marneffe et al., 2014).",2 Related Work,[0],[0]
"For example, Faruqui et al. (2016) propose a graph-based technique for propagating typelevel morphological information across a lexicon, improving token-level morphosyntactic tagging in 11 languages, using an SVM tagger.",2 Related Work,[0],[0]
"In contrast, we apply a neural sequence labeling approach, inspired by the POS tagger of Plank et al. (2016).",2 Related Work,[0],[0]
"We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordformbased protocol for creating these embeddings.",3 MIMICK Word Embeddings,[0],[0]
"By training a model over the existing vocabulary, we can later use that model for predicting the embedding of an unseen word.
",3 MIMICK Word Embeddings,[0],[0]
"Formally: given a language L, a vocabulary V ⊆ L of size V , and a pre-trained embeddings table W ∈ RV×d where each word {wk}Vk=1 is assigned a vector ek of dimension d, our model is trained to find the function f :",3 MIMICK Word Embeddings,[0],[0]
L → Rd such that the projected function f |V approximates the assignments f(wk),3 MIMICK Word Embeddings,[0],[0]
≈ ek.,3 MIMICK Word Embeddings,[0],[0]
"Given such a model, a new word wk∗ ∈ L \ V can now be assigned an embedding ek∗ = f(wk∗).
",3 MIMICK Word Embeddings,[0],[0]
Our predictive function of choice is a Word Type Character Bi-LSTM.,3 MIMICK Word Embeddings,[0],[0]
"Given a word with character sequence w = {ci}n1 , a forward-LSTM and a backward-LSTM are run over the corresponding character embeddings sequence {e(c)i }n1 .",3 MIMICK Word Embeddings,[0],[0]
"Let hnf represent the final hidden vector for the forward-LSTM, and let h0b represent the final hidden vector for the backward-LSTM.",3 MIMICK Word Embeddings,[0],[0]
"The word embedding is computed by a multilayer perceptron:
(1)f(w) = OT · g(Th ·",3 MIMICK Word Embeddings,[0],[0]
"[hnf ; h0b ] + bh) + bT ,
where Th, bh and OT , bT are parameters of affine transformations, and g is a nonlinear elementwise function.",3 MIMICK Word Embeddings,[0],[0]
"The model is presented in Figure 1.
",3 MIMICK Word Embeddings,[0],[0]
The training objective is similar to that of Yin and Schütze (2016).,3 MIMICK Word Embeddings,[0],[0]
"We match the predicted embeddings f(wk) to the pre-trained word embeddings ewk , by minimizing the squared Euclidean distance,
(2)L = ‖f(wk)− ewk‖22 .
",3 MIMICK Word Embeddings,[0],[0]
"By backpropagating from this loss, it is possible to obtain local gradients with respect to the parameters of the LSTMs, the character embeddings, and the output model.",3 MIMICK Word Embeddings,[0],[0]
"The ultimate output of the training phase is the character embeddings matrix C and the parameters of the neural network: M = {C,F,B,Th, bh,OT , bT }, where F,B are the forward and backward LSTM component parameters, respectively.",3 MIMICK Word Embeddings,[0],[0]
"The pretrained embeddings we use in our experiments are obtained from Polyglot (Al-Rfou et al., 2013), a multilingual word embedding effort.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Available for dozens of languages, each dataset contains 64-dimension embeddings for the 100,000 most frequent words in a language’s training corpus (of variable size), as well as an UNK embedding to be used for OOV words.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Even with this vocabulary size, querying words from respective UD corpora (train + dev + test) yields high
OOV rates: in at least half of the 23 languages in our experiments (see Section 5), 29.1% or more of the word types do not appear in the Polyglot vocabulary.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The token-level median rate is 9.2%.1
Applying our MIMICK algorithm to Polyglot embeddings, we obtain a prediction model for each of the 23 languages.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Based on preliminary testing on randomly selected held-out development sets of 1% from each Polyglot vocabulary (with error calculated as in Equation 2), we set the following hyper-parameters for the remainder of the experiments: character embedding dimension = 20; one LSTM layer with 50 hidden units; 60 training epochs with no dropout; nonlinearity function g =",3.1 MIMICK Polyglot Embeddings,[0],[0]
"tanh.2 We initialize character embeddings randomly, and use DyNet to implement the model (Neubig et al., 2017).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Nearest-neighbor examination.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"As a preliminary sanity check for the validity of our protocol, we examined nearest-neighbor samples in languages for which speakers were available: English, Hebrew, Tamil, and Spanish.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 1 presents selected English OOV words with
1Some OOV counts, and resulting model performance, may be adversely affected by tokenization differences between Polyglot and UD.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Notably, some languages such as Spanish, Hebrew and Italian exhibit relational synthesis wherein words of separate grammatical phrases are joined into one form (e.g. Spanish del = de + el, ‘from the-masc.sg.’).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For these languages, the UD annotations adhere to the sub-token level, while Polyglot does not perform subtokenization.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As this is a real-world difficulty facing users of out-of-the-box embeddings, we do not patch it over in our implementations or evaluation.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"2Other settings, described below, were tuned on the supervised downstream tasks.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
their nearest in-vocabulary Polyglot words computed by cosine similarity.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"These examples demonstrate several properties: (a) word shape is learned well (acronyms, capitalizations, suffixes); (b) the model shows robustness to typos (e.g., developiong, corssing); (c) part-of-speech is learned across multiple suffixes (pesky – euphoric, ghastly); (d) word compounding is detected (e.g., lawnmower – bookmaker, postman); (e) semantics are not learned well (as is to be expected from the lack of context in training), but there are surprises (e.g., flatfish – slimy, watery).",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Table 2 presents examples from Hebrew that show learned properties can be extended to nominal morphosyntactic attributes (gender, number – first two examples) and even relational syntactic subword forms such as genetive markers (third example).",3.1 MIMICK Polyglot Embeddings,[0],[0]
Names are learned (fourth example) despite the lack of casing in the script.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"Spanish examples exhibit wordshape and part-of-speech learning patterns with some loose semantics: for example, the plural adjective form prenatales is similar to other familyrelated plural adjectives such as patrimoniales and generacionales.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"Tamil displays some semantic similarities as well: e.g. enjineer (‘engineer’) predicts similarity to other professional terms such as kalviyiyal (‘education’), thozhilnutpa (‘technical’), and iraanuva (‘military’).
",3.1 MIMICK Polyglot Embeddings,[0],[0]
Stanford RareWords.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Stanford RareWord evaluation corpus (Luong et al., 2013) focuses on predicting word similarity between pairs involving low-frequency English words, predominantly ones with common morphological affixes.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"As these words are unlikely to be above the cutoff threshold for standard word embedding models, they emphasize the performance on OOV words.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For evaluation of our MIMICK model on the RareWord corpus, we trained the Variational Embeddings algorithm (VarEmbed; Bhatia et al., 2016) on a 20-million-token, 100,000- type Wikipedia corpus, obtaining 128-dimension
word embeddings for all words in the test corpus.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"VarEmbed estimates a prior distribution over word embeddings, conditional on the morphological composition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"For in-vocabulary words, a posterior is estimated from unlabeled data; for outof-vocabulary words, the expected embedding can be obtained from the prior alone.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"In addition, we compare to FastText (Bojanowski et al., 2016), a high-vocabulary, high-dimensionality embedding benchmark.
",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the “All pairs” condition.",3.1 MIMICK Polyglot Embeddings,[0],[0]
MIMICK also outperforms VarEmbed.,3.1 MIMICK Polyglot Embeddings,[0],[0]
"FastText can be considered an upper bound: with a vocabulary that is 25 times larger than the other models, it was missing words from only 44 pairs on this data.",3.1 MIMICK Polyglot Embeddings,[0],[0]
"The Universal Dependencies (UD) scheme (De Marneffe et al., 2014) features a minimal set of 17 POS tags (Petrov et al., 2012) and supports tagging further language-specific features using attribute-specific inventories.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"For example, a verb in Turkish could be assigned a value for the evidentiality attribute, one which is absent from Danish.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"These additional morphosyntactic attributes are marked in the UD dataset as optional per-token attribute-value pairs.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We extend this approach to morphosyntactic tagging by duplicating this projection layer for each attribute type.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to our multilayer perceptron (MLP) projection network is the hidden state produced for each token in the sentence by an underlying LSTM, and the output is
attribute-specific probability distributions over the possible values for each attribute on each token in the sequence.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Formally, for a given attribute a with possible values v ∈ Va, the tagging probability for the i’th word in a sentence is given by:
Pr(awi = v) =",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"(Softmax(φ(hi)))v , (3)
with
(4)φ(hi) = OaW · tanh(Wah ·",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"hi + bah) + baW ,
where hi is the i’th hidden state in the underlying LSTM, and φ(hi) is a two-layer feedforward neural network, with weights Wah and O a W .",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
We apply a softmax transformation to the output; the value at position v is then equal to the probability of attribute v applying to token wi.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"The input to the underlying LSTM is a sequence of word embeddings, which are initialized to the Polyglot vectors when possible, and to MIMICK vectors when necessary.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Alternative initializations are considered in the evaluation, as described in Section 5.2.
",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
Each tagged attribute sequence (including POS tags) produces a loss equal to the sum of negative log probabilities of the true tags.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
One way to combine these losses is to simply compute the sum loss.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"However, many languages have large differences in sparsity across morpho-syntactic attributes, as apparent from Table 4 (rightmost column).",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"We therefore also compute a weighted sum
loss, in which each attribute is weighted by the proportion of training corpus tokens on which it is assigned a non-NONE value.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"Preliminary experiments on development set data were inconclusive across languages and training set sizes, and so we kept the simpler sum loss objective for the remainder of our study.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
"In all cases, part-of-speech tagging was less accurate when learned jointly with morphosyntactic attributes.",4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
This may be because the attribute loss acts as POS-unrelated “noise” affecting the common LSTM layer and the word embeddings.,4 Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes,[0],[0]
The morphological complexity and compositionality of words varies greatly across languages.,5 Experimental Settings,[0],[0]
"While a morphologically-rich agglutinative language such as Hungarian contains words that carry many attributes as fully separable morphemes, a sentence in an analytic language such as Vietnamese may have not a single polymorphemic or inflected word in it.",5 Experimental Settings,[0],[0]
"To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns.",5 Experimental Settings,[0],[0]
"Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well.",5 Experimental Settings,[0],[0]
"We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agić (2017).
",5 Experimental Settings,[0],[0]
"As stated above, our approach is built on the Polyglot word embeddings.",5 Experimental Settings,[0],[0]
The intersection of the Polyglot embeddings and the UD dataset (version 1.4) yields 44 languages.,5 Experimental Settings,[0],[0]
"Of these, many are under-annotated for morphosyntactic attributes; we select twenty-three sufficiently-tagged languages, with the exception of Indonesian.3 Table 4 presents the selected languages and their typological properties.",5 Experimental Settings,[0],[0]
"As an additional proxy for mor-
3Vietnamese has no attributes by design; it is a pure analytic language.
",5 Experimental Settings,[0],[0]
"phological expressiveness, the rightmost column shows the proportion of UD tokens which are annotated with any morphosyntactic attribute.",5 Experimental Settings,[0],[0]
"As noted above, we use the UD datasets for testing our MIMICK algorithm on 23 languages4 with the supplied train/dev/test division.",5.1 Metrics,[0],[0]
"We measure partof-speech tagging by overall token-level accuracy.
",5.1 Metrics,[0],[0]
"For morphosyntactic attributes, there does not seem to be an agreed-upon metric for reporting performance.",5.1 Metrics,[0],[0]
Dzeroski et al. (2000) report pertag accuracies on a morphosyntactically tagged corpus of Slovene.,5.1 Metrics,[0],[0]
"Faruqui et al. (2016) report macro-averages of F1 scores of 11 languages from UD 1.1 for the various attributes (e.g., part-ofspeech, case, gender, tense); recall and precision were calculated for the full set of each attribute’s values, pooled together.5",5.1 Metrics,[0],[0]
Agić,5.1 Metrics,[0],[0]
"et al. (2013) report separately on parts-of-speech and morphosyntactic attribute accuracies in Serbian and Croatian, as well as precision, recall, and F1 scores per tag.",5.1 Metrics,[0],[0]
"Georgiev et al. (2012) report token-level accuracy for exact all-attribute tags (e.g. ‘Ncmsh’ for “Noun short masculine singular definite”) in Bulgarian, reaching a tagset of size 680.",5.1 Metrics,[0],[0]
Müller et al. (2013) do the same for six other languages.,5.1 Metrics,[0],[0]
"We report micro F1: each token’s value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment.",5.1 Metrics,[0],[0]
"Recall and
4When several datasets are available for a language, we use the unmarked corpus.
5Details were clarified in personal communication with the authors.
precision are calculated over the entire set, with F1 defined as their harmonic mean.",5.1 Metrics,[0],[0]
"We implement and test the following models:
No-Char.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot models, with unseen words assigned the Polyglot-supplied UNK vector.",5.2 Models,[0],[0]
"Following tuning experiments on all languages with cased script, we found it beneficial to first back off to the lowercased form for an OOV word if its embedding exists, and only otherwise assign UNK.
MIMICK.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot, with OOV embeddings inferred from a MIMICK model (Section 3) trained on the Polyglot embeddings.",5.2 Models,[0],[0]
"Unlike the No-Char case, backing off to lowercased embeddings before using the MIMICK output did not yield conclusive benefits and thus we report results for the more straightforward no-backoff implementation.
CHAR→TAG.",5.2 Models,[0],[0]
"Word embeddings are initialized from Polyglot as in the No-Char model (with lowercase backoff), and appended with the output of a character-level LSTM updated during training (Plank et al., 2016).",5.2 Models,[0],[0]
"This additional module causes a threefold increase in training time.
",5.2 Models,[0],[0]
Both.,5.2 Models,[0],[0]
"Word embeddings are initialized as in MIMICK, and appended with the CHAR→TAG LSTM.
",5.2 Models,[0],[0]
Other models.,5.2 Models,[0],[0]
"Several non-Polyglot embedding models were examined, all performed substantially worse than Polyglot.",5.2 Models,[0],[0]
"Two of these
are notable: a random-initialization baseline, and a model initialized from FastText embeddings (tested on English).",5.2 Models,[0],[0]
"FastText supplies 300-dimension embeddings for 2.51 million lowercase-only forms, and no UNK vector.6 Both of these embedding models were attempted with and without CHAR→TAG concatenation.",5.2 Models,[0],[0]
"Another model, initialized from only MIMICK output embeddings, performed well only on the language with smallest Polyglot training corpus (Latvian).",5.2 Models,[0],[0]
"A Polyglot model where OOVs were initialized using an averaged embedding of all Polyglot vectors, rather than the supplied UNK vector, performed worse than our No-Char baseline on a great majority of the languages.
",5.2 Models,[0],[0]
"Last, we do not employ type-based tagset restrictions.",5.2 Models,[0],[0]
All tag inventories are computed from the training sets and each tag selection is performed over the full set.,5.2 Models,[0],[0]
"Based on development set experiments, we set the following hyperparameters for all models on all languages: two LSTM layers of hidden size 128, MLP hidden layers of size equal to the number of each attribute’s possible values; momentum stochastic gradient descent with 0.01 learning rate; 40 training epochs (80 for 5K settings) with a dropout rate of 0.5.",5.3 Hyperparameters,[0],[0]
The CHAR→TAG models use 20-dimension character embeddings and a single hidden layer of size 128.,5.3 Hyperparameters,[0],[0]
We report performance in both low-resource and full-resource settings.,6 Results,[0],[0]
"Low-resource training sets were obtained by randomly sampling training sentences, without replacement, until a predefined token limit was reached.",6 Results,[0],[0]
We report the results on the full sets and on N = 5000 tokens in Table 5 (partof-speech tagging accuracy) and Table 6 (morphosyntactic attribute tagging micro-F1).,6 Results,[0],[0]
"Results for additional training set sizes are shown in Figure 2; space constraints prevent us from showing figures for all languages.
MIMICK as OOV initialization.",6 Results,[0],[0]
"In nearly all experimental settings on both tasks, across languages and training corpus sizes, the MIMICK embeddings significantly improve over the Polyglot UNK embedding for OOV tokens on both
6Vocabulary type-level coverage for the English UD corpus: 55.6% case-sensitive, 87.9% case-insensitive.
",6 Results,[0],[0]
POS and morphosyntactic tagging.,6 Results,[0],[0]
"For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative.",6 Results,[0],[0]
"Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word- and characterlevel sparsity in the UD",6 Results,[0],[0]
"corpus.7 In morphosyntactic tagging, gains are apparent for Slavic languages and Chinese, but also for agglutinative languages — especially Tamil and Turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type-level signal.8 To examine the effects on Slavic and agglutinative languages in a more fine-grained view, we present results of multiple training-set size experiments for each model, averaged over five repetitions (with different corpus samples), in Figure 2.
MIMICK vs. CHAR→TAG.",6 Results,[0],[0]
"In several languages, the MIMICK algorithm fares better than the CHAR→TAG model on part-of-speech tagging in low-resource settings.",6 Results,[0],[0]
"Table 7 presents the POS tagging improvements that MIMICK achieves over the pre-trained Polyglot models, with and without CHAR→TAG concatenation, with 10,000 tokens of training data.",6 Results,[0],[0]
"We obtain statistically significant improvements in most languages, even when CHAR→TAG is included.",6 Results,[0],[0]
"These improvements are particularly substantial for test-set tokens outside the UD training set, as shown in the right two columns.",6 Results,[0],[0]
"While test set OOVs are a strength of the CHAR→TAG model (Plank et al., 2016), in many languages there are still considerable improvements to be obtained from the application of MIMICK initialization.",6 Results,[0],[0]
"This suggests that with limited training data, the end-to-end CHAR→TAG model is unable to learn a sufficiently accurate representational mapping from orthography.",6 Results,[0],[0]
"We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained,
7Character coverage in Chinese Polyglot is surprisingly good: only eight characters from the UD dataset are unseen in Polyglot, across more than 10,000 unseen word types.
",7 Conclusion,[0],[0]
8Persian is officially classified as agglutinative but it is mostly so with respect to derivations.,7 Conclusion,[0],[0]
"Its word-level inflections are rare and usually fusional.
limited-vocabulary models, without need to access the originating corpus.",7 Conclusion,[0],[0]
"This method is particularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic.",7 Conclusion,[0],[0]
"Our method improves performance over word-based models on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology.",7 Conclusion,[0],[0]
"In addition, we present a BiLSTM approach for tagging morphosyntactic attributes at the token level.",7 Conclusion,[0],[0]
"In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters (Costa-jussà et al., 2017).",7 Conclusion,[0],[0]
"We thank Umashanthi Pavalanathan, Sandeep Soni, Roi Reichart, and our anonymous reviewers for their valuable input.",8 Acknowledgments,[0],[0]
We thank Manaal Faruqui and Ryan McDonald for their help in understanding the metrics for morphosyntactic tagging.,8 Acknowledgments,[0],[0]
The project was supported by project HDTRA1-15-10019 from the Defense Threat Reduction Agency.,8 Acknowledgments,[0],[0]
"Word embeddings improve generalization over lexical features by placing each word in a lower-dimensional space, using distributional information obtained from unlabeled data.",abstractText,[0],[0]
"However, the effectiveness of word embeddings for downstream NLP tasks is limited by out-of-vocabulary (OOV) words, for which embeddings do not exist.",abstractText,[0],[0]
"In this paper, we present MIMICK, an approach to generating OOV word embeddings compositionally, by learning a function from spellings to distributional embeddings.",abstractText,[0],[0]
"Unlike prior work, MIMICK does not require re-training on the original word embedding corpus; instead, learning is performed at the type level.",abstractText,[0],[0]
Intrinsic and extrinsic evaluations demonstrate the power of this simple approach.,abstractText,[0],[0]
"On 23 languages, MIMICK improves performance over a word-based baseline for tagging part-of-speech and morphosyntactic attributes.",abstractText,[0],[0]
It is competitive with (and complementary to) a supervised characterbased model in low-resource settings.,abstractText,[0],[0]
Mimicking Word Embeddings using Subword RNNs,title,[0],[0]
The sheer number and variety of online social networks (OSN) today is staggering.,1. Introduction,[0],[0]
"Although the purpose and the shaping of these networks vary generously, the majority of them has one aspect in common: the value of most OSNs is in its user data and the information that one can infer from the data.",1. Introduction,[0],[0]
"This, unfortunately, results in a big incentive for culprits to intrude OSNs and manipulate their data.",1. Introduction,[0],[0]
"One popular method of intruding and attacking an OSN is referred to as Sybil attack, where the intruder creates a whole bunch of fake (Sybil) accounts that are all under the attacker’s control.",1. Introduction,[0],[0]
"The intruder’s influence over the OSN
1MathPlan, 10587 Berlin, Germany 2Machine Learning Group, Berlin Institute of Technology, 10587 Berlin, Germany 3Berlin Big Data Center 4Max Planck Society 5Korea University.",1. Introduction,[0],[0]
Correspondence to: János,1. Introduction,[0],[0]
Höner <,1. Introduction,[0],[0]
"janos.hoener@campus.tuberlin.de>, Nico Görnitz <nico.goernitz@tu-berlin.de>, KlausRobert Müller <klaus-robert.mueller@tu-berlin.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
is multiplied by the number of accounts created, which opens possibilities of manipulation typically for gaining some monetary advantage in the end.
",1. Introduction,[0],[0]
"The term, Sybil attack, was coined by Douceur (2002) who showed that this kind of attack will be always possible unless a trusted agency certifies identities.",1. Introduction,[0],[0]
"Unfortunately, this approach is orthogonal to how OSNs grow.",1. Introduction,[0],[0]
The threshold of registration must be as low as possible to attract as many new users as possible.,1. Introduction,[0],[0]
"On the other hand, Sybil attacks can damage the value of OSNs significantly, which has been proved by the fact that Facebook shares dropped in 2012 after the company revealed that a significant share of its network is made up by Sybil accounts (The Associated Press, 2012).
",1. Introduction,[0],[0]
"There exists a number of “classic” feature-based solutions (Stein et al., 2011; Cao et al., 2012; Stringhini et al., 2010; Yang et al., 2014).",1. Introduction,[0],[0]
"However, up until now, it remains an unsolved problem as those methods can be evaded by cleverly designed attacking schemes (Bilge et al., 2009; Boshmaf et al., 2011; Wagner et al., 2012; Lowd & Meek, 2005) and manual detection is too expensive, time consuming, and simply unfeasible in large OSNs (Cao et al., 2012).
",1. Introduction,[0],[0]
More recent graph-based Sybil detection methods assume that honest (non-Sybil) nodes form a strongly connected subgraph and attackers can establish a limited amount of edges which leads to a sparse cut between the honest subraph and the Sybil nodes.,1. Introduction,[0],[0]
"The majority of the graph-based methods define trusted nodes, which the defender is sure to be honest, and use random walks (Yu et al., 2010; Danezis, 2009; Cao et al., 2012) or other typical graph-based algorithms like breadth-first-search (Tran et al., 2011) and belief propagation (Gong et al., 2014) to convey trust from the trusted nodes.",1. Introduction,[0],[0]
A node is identified as Sybil if sufficient ammount of trust is not delivered to it.,1. Introduction,[0],[0]
"Among random-walk based approaches, SybilRank is known to be the state-ofthe-art, of which the performance is theoretically guaranteed (Cao et al., 2012).",1. Introduction,[0],[0]
"However, the theory holds only under unrealistic topological assumptions of the network.",1. Introduction,[0],[0]
"In this paper, we show that the same theoretical guarantee can be obtained under more realistic situations.
",1. Introduction,[0],[0]
We further dicuss the robustness of the random walk approach against adversarial strategies.,1. Introduction,[0],[0]
"To this end, we formally introduce adversarial settings for graph-based Sybil
detection and derive an optimal attacking strategy that is based on the exploitation of trust leaks.",1. Introduction,[0],[0]
"Based on our analysis, we propose a transductive Sybil ranking (TSR), an integrated approach capable of adjusting edge weights based on sampled trust leaks.",1. Introduction,[0],[0]
We empirically show good performance of TSR against the state-of-the-art baselines on a variety of attacking scenarios using artificially generated data as well as real-world Facebook data.,1. Introduction,[0],[0]
"We are given a graph G = (V,E) consisting of nodes V and pairwise edges E between nodes.",2. Preliminaries,[0],[0]
"We denote GS = (VS , ES) the Sybil sub-graph, GH = (VH , EH) the disjunct honest sub-graph, and VT ✓ VH our trusted (verified nonSybil nodes) random walk seed nodes.",2. Preliminaries,[0],[0]
EA is the set of edges connecting any node in GS and any node in GH .,2. Preliminaries,[0],[0]
"Sybil Rank is considered the state-of-the-art graph-based method to detect Sybil accounts as it outperformed all its contestants (Cao et al., 2012).",2. Preliminaries,[0],[0]
It is also based on random walks and operates solely on the topology of the graph.,2. Preliminaries,[0],[0]
"Sybil Rank starts from the initial distribution {p(i)0 2 [0, 1]}|V |i=1",2. Preliminaries,[0],[0]
"(without superscript refers to a vector containing all elements), in which “trust” is assigned to the known honest nodes VT :
p (i) 0",2. Preliminaries,[0],[0]
"=
( 1
|VT",2. Preliminaries,[0],[0]
"| if vi 2 VT , 0 otherwise.
(1)
Then, it “propagates” the trust via a short (k steps) random walk:
p > k",2. Preliminaries,[0],[0]
= p >,2. Preliminaries,[0],[0]
k,2. Preliminaries,[0],[0]
"1Q = · · · = p>0 Qk , (2)
where Q 2 R|V |⇥|V",2. Preliminaries,[0],[0]
"| is the transition matrix through the edges with Qi,j = ( P j0 1[(i, j 0 ) 2 E]) 1, if (i, j) 2 E, and else 0.",2. Preliminaries,[0],[0]
"It is known that the stationary distribution ⇡ ⌘ p1 is the normalized degree distribution (Behrends, 2000)
⇡ > = ⇣ deg(v1) Vol(V ) , . . .",2. Preliminaries,[0],[0]
", deg(v|V |) Vol(V ) ⌘ , (3)
where deg(v) is the degree of node v, i.e., the number of all incident edges of v, and Vol(V ) = P v2V deg(v) is the sum of the degrees for all nodes in V .",2. Preliminaries,[0],[0]
"SybilRank conpensates the effect of degrees, and use the degree-normalized probability
p (i) = p",2. Preliminaries,[0],[0]
"(i) k /⇡ (i) (4)
as the ranking score, where a high ranking indicates a high probability of being an honest node.
",2. Preliminaries,[0],[0]
"Essentially, SybilRank relies on the assumption that the total number of attacking edges is bounded.",2. Preliminaries,[0],[0]
"Under this assumption, only a small fraction of the trust is propagated
through the sparse cut between the honest network and the Sybil nodes during the short random walk, while ”trust” go through the ”non-trusted” honest nodes through the dense connections within the the honest subgraph.
",2. Preliminaries,[0],[0]
Boshmaf et al. (2016) developed Integro to cope with a larger number of attacking edges.,2. Preliminaries,[0],[0]
"To this end, Integro introduces weights on the edges to bias the random walk, where the weights are determined after its pre-processing step to detect victims.",2. Preliminaries,[0],[0]
Here a victim is defined as a node that established a connection to one of the Sybil nodes.,2. Preliminaries,[0],[0]
"The set of all victim nodes is defined by Vv = {v 2 Vh : 9(v, s) 2 EA}.",2. Preliminaries,[0],[0]
"After the detection step, Integro lowers the weights to all incident edges to the detected victims, which prevents the trust to propagate through victim nodes.",2. Preliminaries,[0],[0]
"As the victims form a natural border between the honest and the Sybil graph, this reduces the overall flow of trust into the Sybil graph.",2. Preliminaries,[0],[0]
Boshmaf et al. found that traditional feature-based classification methods yield good and robust detection of victims.,2. Preliminaries,[0],[0]
"A notable advantage against the feature-based Sybil detection is that, unlike Sybils, victims generally do not behave adversarial, as they don’t have any incentive to ”hide”.",2. Preliminaries,[0],[0]
"More Realistic Assumptions
Cao et al. (2012) gave a security guarantee for SybilRank.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges and n := |V | be the number of all nodes in the graph.,3. SybilRank’s Security Guarantee Under,[0],[0]
their theory relies on the notion of trust leaks.,3. SybilRank’s Security Guarantee Under,[0],[0]
Definition 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
(Trust leaks) Let rk0 = P i2VH p (i) k0 be the trust that remains in the honest graph after k0 random walk steps.,3. SybilRank’s Security Guarantee Under,[0],[0]
We call l = Pk k0=1(rk0+1 rk0) the absolute trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Assume that the attacking edges are created randomly, following a distribution ↵(EA).",3. SybilRank’s Security Guarantee Under,[0],[0]
We call CH(k 0 ),3. SybilRank’s Security Guarantee Under,[0],[0]
"= E↵(EA)[ rk0+1 rk0 rk0
] the expected relative trust leak.
",3. SybilRank’s Security Guarantee Under,[0],[0]
CH(k 0 ) is actually a constant with respect to k0 under reasonable assumptions on ↵(EA).,3. SybilRank’s Security Guarantee Under,[0],[0]
The following lemma has been proved: Lemma 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model (Molloy & Reed, 1995).",3. SybilRank’s Security Guarantee Under,[0],[0]
"Then, the expected relative trust leak in each iteration is given by CH = gvol(VH) .
",3. SybilRank’s Security Guarantee Under,[0],[0]
This leads to a theoretical guarantee of SybilRank.,3. SybilRank’s Security Guarantee Under,[0],[0]
Theorem 1.,3. SybilRank’s Security Guarantee Under,[0],[0]
"(Cao et al., 2012) Assume that the graph G is created randomly, following the configuration model.",3. SybilRank’s Security Guarantee Under,[0],[0]
The total number of Sybils that are ranked higher than nonSybils by SybilRank is O(g log n).,3. SybilRank’s Security Guarantee Under,[0],[0]
"Theorem 1 implies good performance of SybilRank, but
it holds under the assumption that the attacking edges are created in the same process as the honest graph,1 which is not realistic.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Below, we show that the same guarantee is obtained under the following more realistic assumption: Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The graph G is constructed by the following steps:
1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Honest graph GH construction: GH is connected, nonbipartite, and scale free, i.e., the degree distribution follows the power law distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Sybil graph GS construction: The topology of GS is arbitrary.
",3. SybilRank’s Security Guarantee Under,[0],[0]
3.,3. SybilRank’s Security Guarantee Under,[0],[0]
Attacking edges EA generation:,3. SybilRank’s Security Guarantee Under,[0],[0]
"The attacking edges are genarated on all possible edges EA ⇢ VS ⇥ VH between the honest and the Sybil subgraphs with equal propability.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1, evaluating the expected trust leak is less straightforward.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Nevertheless, we can show that it results in the same formal security guarantee stated in Theorem 1.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"To properly compute the expected trust leak, the following random variables are defined.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv counts the number of attacking edges incident to node v, Yv = ⇡(v)
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Xv deg(v,GH)+Xv = ⇡(v) Xv deg(v,G) is the trust leak in node
v and Z = P
",3. SybilRank’s Security Guarantee Under,[0],[0]
v2VH Yv is the total trust leak.,3. SybilRank’s Security Guarantee Under,[0],[0]
Note that here ⇡(v) is the current amount of trust in node v and not the stationary distribution of the random walk.,3. SybilRank’s Security Guarantee Under,[0],[0]
This notation is used to avoid confusion with the probability mass function denoted by P .,3. SybilRank’s Security Guarantee Under,[0],[0]
"From Assumption 1 it follows that Xv is hypergeometrically distributed (Tuckwell, 1995) with the following parameters: the population size: N = |VH⇥VS |, successes: K = |{v}⇥ VS |, and the draws n = |EA|.",3. SybilRank’s Security Guarantee Under,[0],[0]
Let g := |EA| be the number of attacking edges.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Moreover, let nH := |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
| and nS,3. SybilRank’s Security Guarantee Under,[0],[0]
":= |VS | denote the number of honest nodes and Sybil nodes, respectively.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"The probability mass function of Xv is given by P (Xv =
k) =
✓ K
k ◆✓ N K n k ◆ / ✓ N n",3. SybilRank’s Security Guarantee Under,[0],[0]
◆,3. SybilRank’s Security Guarantee Under,[0],[0]
"and according to Tuckwell
(1995), its expected value can be computed by E[Xv] = n
K N = |EA| |{v}⇥VS ||VH⇥VS | = |EA| |VH",3. SybilRank’s Security Guarantee Under,[0],[0]
"| = g nH
.",3. SybilRank’s Security Guarantee Under,[0],[0]
"The final goal is to compute the expected value of Z. The linearity of the expected value yields E[Z] =
P v2VH E[Yv] and for the
expected value of Yv we get
E[Yv] = ⇡(v)deg(v,G) P1 k=0 kP (Xv = k)
= ⇡(v) deg(v,G)E[Xv] = ⇡(v) deg(v,G) g nH .
",3. SybilRank’s Security Guarantee Under,[0],[0]
"1This assumption is not explicitly stated in Cao et al. (2012), but apparent from their derivation.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Using this result, the expected value of Z becomes E[Z]",3. SybilRank’s Security Guarantee Under,[0],[0]
"=P v2VH E[Yv] = g nH P v2VH ⇡(v) deg(v,G) , where the right hand side still contains a sum that needs to be evaluated individually for each node to compute its actual value.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In order to “average out” this sum, we rely on the assumption that the honest nodes GH is power law-distributed (Barabási, 2009).",3. SybilRank’s Security Guarantee Under,[0],[0]
"To do this, a new random variable Dv is introduced which measures the degree of v. Then, the assumption results in the probability of a node having a degree of d being P (Dv = d) =",3. SybilRank’s Security Guarantee Under,[0],[0]
"d
⇣( ) , where ⇣ is the Riemann zeta function ⇣(s) := P1 n=0 n s (Barabási, 2009).
",3. SybilRank’s Security Guarantee Under,[0],[0]
"With this expression, it is possible to “average out” the exact topology of the graph by computing the expected value with respect to the newly defined random variable Dv:
E[Z] =",3. SybilRank’s Security Guarantee Under,[0],[0]
gnH P1 d=1 P v2VH ⇡(v) d P,3. SybilRank’s Security Guarantee Under,[0],[0]
(,3. SybilRank’s Security Guarantee Under,[0],[0]
"Dv = d)
= g nH P v2VH ⇡(v) P1 d=1 1 d d ⇣( )
= g nH P v2VH ⇡(v) ⇣( ) P1 d=1 d ( +1)
",3. SybilRank’s Security Guarantee Under,[0],[0]
= g nH ⇣( +1) ⇣,3. SybilRank’s Security Guarantee Under,[0],[0]
"( ) P v2VH ⇡(v).| {z }
Total trust in the honest graph
This yields the following lemma.",3. SybilRank’s Security Guarantee Under,[0],[0]
Lemma 2.,3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assumption 1 the expected relative trust leak in each iteration of the random walk is given by
˜ CH = g
nH ⇣( +1) ⇣( )| {z } =:e
where e < 1 is a constant that depends on the parameter of the assumed power law distribution for the degree distribution.
",3. SybilRank’s Security Guarantee Under,[0],[0]
"Although Lemma 2 gives a different expected relative trust leak from Lemma 1, the fact that the maximum number of connection for each node is bounded in every OSN and therefore O(nH) = O(vol(VH)) leads to the same asymptotic behavior as Theorem 2: Theorem 2.",3. SybilRank’s Security Guarantee Under,[0],[0]
"Under Assuption 1, the total number of Sybils that are ranked higher than non-Sybils by SybilRank is O(g log n).",3. SybilRank’s Security Guarantee Under,[0],[0]
"This result explicitly shows that, asymptotically, SybilRank’s security guarantee remains unchanged even under more realistic Assumption 1.",3. SybilRank’s Security Guarantee Under,[0],[0]
"In this section, we discuss adversarial strategies against graph-based Sybil detection methods.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Action Although attackers in general can take variety of actions, we restricts their action to adding attacking edges.
",4. Adversarial Strategies,[0],[0]
Definition 2 (Attacking strategy).,4. Adversarial Strategies,[0],[0]
"Given an honest graph GH and a Sybil graph GS , an attacking strategy describes the set of attacking edges established by the intruder.
",4. Adversarial Strategies,[0],[0]
"The cost of action is measured by the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Knowledge Generally, we focus on adversarial attacks against random walk based approaches.",4. Adversarial Strategies,[0],[0]
"That is, an attacker’s strategy for establishing edges from Sybil nodes to honest nodes in order to cloak an attacker’s Sybil sub-network.",4. Adversarial Strategies,[0],[0]
"For analysis, we assume different levels of knowledge that the attacker has on the defender’s strategy and information:
A.1 Strategy only.
",4. Adversarial Strategies,[0],[0]
"A.2 Strategy + topology.
",4. Adversarial Strategies,[0],[0]
"A.3 Strategy + topology + trusted nodes (positively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"B.1 Strategy + topology + trusted nodes (positively labeled nodes) + untrusted nodes (negatively labeled nodes).
",4. Adversarial Strategies,[0],[0]
"Here, we divided the level of access to inside information for the attacker into two groups.",4. Adversarial Strategies,[0],[0]
"In group A (i.e., A.1, A.2, A.3) attackers are able to gather sophisticated information based on publicly available sources, whereas in group B (i.e., B.1) either some back-channel provides non-public information (e.g. defender marked Sybil nodes based on their analysis), or, the attackers are provided with all information visible to the defenders.
",4. Adversarial Strategies,[0],[0]
"Clearly, it is too hard, if not impossible, to have an out-ofthe-box solution for the setting described in group B and we therefore resort our analysis on the settings in group A.",4. Adversarial Strategies,[0],[0]
"In the first case (A.1), no efficient adversarial strategies for graph-based random walk approaches is possible.",4. Adversarial Strategies,[0],[0]
The attackers must build up sufficient attacking edges to trusted nodes in order to absorb enough trust.,4. Adversarial Strategies,[0],[0]
"In A.3 (and A.2 as a special case) on the other hand, the attacker gained enough information to guide the creation of attacking edges efficiently.",4. Adversarial Strategies,[0],[0]
This paper focuses on this most interesting situation.,4. Adversarial Strategies,[0],[0]
"More specifically, we assume the following: the intruder knows defender’s strategy (algorithm details), the topology of the honest graph, and the set of trusted nodes, i.e., she knows about GH = (VH , EH) and VT .",4. Adversarial Strategies,[0],[0]
Based on that knowledge the intruder can establish attacking edges to honest nodes of her choice with the goal to create an attacking scenario where the applied defense method fails.,4. Adversarial Strategies,[0],[0]
"Although the exact topology of the Sybil graph is not specified any further, for the following results it is assumed that it is designed in a way that suits the intruder well.
",4. Adversarial Strategies,[0],[0]
"Attacker’s Goal Attackers can have various final goals, e.g., spamming honest users to earn money, feeding wrong information to honest nodes, stealing nonpublic information, damaging countries/companies, etc.",4. Adversarial Strategies,[0],[0]
"Depending on the goal, the objective of the optimal strategy can differ.",4. Adversarial Strategies,[0],[0]
"We assume that attacker’s try to maximize their influence and hence, have an inherent need to increase the number of attacking edges.
",4. Adversarial Strategies,[0],[0]
"Random-walk based approaches such as SybilRank and Integro rely on the fact that the absolute trust leak l from the honest graph to the Sybil graph is small (i.e., below the amount needed to reach the stationary distribution within the Sybil sub-graph) which ensures low trust scores for the Sybil nodes.",4. Adversarial Strategies,[0],[0]
"However, if enough trust is being propagated to the Sybil graph, trust values will be close to the stationary distribution in the Sybil graph as well as in the honest graph.",4. Adversarial Strategies,[0],[0]
"Consequently, the degree-normalized ranking values will be similar to the ones in the honest graph, which makes Sybil nodes indistinguishable from honest nodes and therefore disables the detector.",4. Adversarial Strategies,[0],[0]
Definition 3 (Disabling Attacking Strategy).,4. Adversarial Strategies,[0],[0]
Let GH and GS be the honest graph and the Sybil graph.,4. Adversarial Strategies,[0],[0]
Let l : 2E !,4. Adversarial Strategies,[0],[0]
R be the absolute trust leak as a function of an attacking strategy.,4. Adversarial Strategies,[0],[0]
"Then, an attacking strategy EA ⇢ VH ⇥",4. Adversarial Strategies,[0],[0]
"VS is said to be disabling if
l(EA)",4. Adversarial Strategies,[0],[0]
"td, (5) where td is the disabling threshold, which depends on the topology of the Sybil graph and the detection algorithm.
",4. Adversarial Strategies,[0],[0]
"Surely, an attacker does not aim for just any disabling strategy but for the one that comes at the lowest cost.",4. Adversarial Strategies,[0],[0]
"As the cost of an attacking strategy is assumed to be increasing in the number of attacking edges, an optimal/minimal disabling strategy is given by the following definition.",4. Adversarial Strategies,[0],[0]
Definition 4 (Optimal Disabling Strategy).,4. Adversarial Strategies,[0],[0]
"An attacking strategy AE is said to be optimal if it is the solution to the following optimization problem:
min EA⇢VH⇥VS |EA| (6)
s. t. l(EA) td.
To solve this, the disabling threshold td and the trust leak function must be known to the attacker.",4. Adversarial Strategies,[0],[0]
"Ignoring the edge weights (which are unknown to the attacker) the amount of trust needed within the Sybil graph to reach the stationary distribution of the random walk is given by td =P
vi2VS ⇡i = vol(VS) vol(V ) .",4. Adversarial Strategies,[0],[0]
To exactly evaluate l(EA) the entire random walk needs to be simulated which is infeasible for the attacker without knowing its exact length and the edge weights.,4. Adversarial Strategies,[0],[0]
A useful estimate is to consider only the first iteration.,4. Adversarial Strategies,[0],[0]
"The computation of this value is feasible and the trust leak per attacking edge is by far the
largest in the first iteration because all the trust is concentrated in the relatively small subset of trusted nodes VT .",4. Adversarial Strategies,[0],[0]
"The trust leak in the first iteration ˜l(EA) is given by l(EA) = P v2VT (v) deg(v,GH)+(v)
, where (v) is the attacking degree (i.e., the number of attacking edges) of node v.",4. Adversarial Strategies,[0],[0]
This leads to a greedy strategy where the intruder iteratively adds those attacking edges which produce the largest increase in ˜l.,4. Adversarial Strategies,[0],[0]
In the following the term adversarial strategy/attacker refers to this greedy strategy.,4. Adversarial Strategies,[0],[0]
"In this section, we propose our new method and derive its efficient solver.",5. Proposed Method,[0],[0]
"Our method is specifically designed to cope with a large number of attacking edges by minimizing “trust leaks”, that is, minimizing a sampled trust leak by adjusting the edge weights—a missing mechanism for SybilRank and Integro.
",5. Proposed Method,[0],[0]
"Transductive Sybil Ranking Combining the approach of Backstrom & Leskovec (2011) and SybilRank (Cao et al., 2012), our proposed method, called transductive Sybil ranking (TSR), tries to leverage potential prior knowledge, negative labels, to bias a short random walk so that random walk methods work even with the existence of a large number of attacking edges.
",5. Proposed Method,[0],[0]
Assume that all nodes carry attributes and n  |V,5. Proposed Method,[0],[0]
"| nodes are additionally attached with label information, i.e., the defender knows a subset of nodes are honest, and another subset of nodes are sybil.",5. Proposed Method,[0],[0]
"More formally, the defender is given labeled nodes L := {(xi, yi) 2 X ⇥",5. Proposed Method,[0],[0]
"{+1, 1}}ni=1 and unlabeled nodes U := {xi 2 X}|V |i=n+1.",5. Proposed Method,[0],[0]
"Since only the honest nodes can be trusted, VT ✓ {vi 2 V ; yi = +1} holds.
",5. Proposed Method,[0],[0]
"We define an edge feature function u,v between nodes u and v as u,v : X ⇥ X !",5. Proposed Method,[0],[0]
Y .,5. Proposed Method,[0],[0]
"A corresponding parameterized, non-negative scoring function fw : Y !",5. Proposed Method,[0],[0]
"R+ is learned during training and applied as edge weight au,v = fw( u,v) in the weighted adjacency matrix Q 2 R|V |⇥|V",5. Proposed Method,[0],[0]
"|:
Qu,v =
( au,vP x au,x
if (u, v) 2 E, 0 otherwise.
(7)
",5. Proposed Method,[0],[0]
"Throughout our experiments, we restrict ourselves to the following differentiable edge feature function:
fw( u,v) =",5. Proposed Method,[0],[0]
"(1 exp( w> u,v))",5. Proposed Method,[0],[0]
1.,5. Proposed Method,[0],[0]
"(8) Once the transition matrix is fixed, The remaining procedure is the same as SybilRank.",5. Proposed Method,[0],[0]
"Namely, starting form the initial distribution (1), k-steps random walk (2) is applied with the transition matrix (7).",5. Proposed Method,[0],[0]
"After that, the degreenormalized ranking probability (4) is used for classification.",5. Proposed Method,[0],[0]
"However, we are also given negatively labeled nodes,
which are used to train the parameter w of the edge feature function (8), so that p(i) < p(j), 8 i, j 2 {1, . . .",5. Proposed Method,[0],[0]
", n} with yi = 1 and yj = +1.",5. Proposed Method,[0],[0]
"In the spirit of regularized risk minimization (Vapnik, 1999), this problem is formalized as follows:
Definition 5 (TSR optimization problem).",5. Proposed Method,[0],[0]
"TSR solves a quadratically regularized, non-convex optimization problem with generic loss-functions h :",5. Proposed Method,[0],[0]
"[0, 1]⇥{+1, 1} !",5. Proposed Method,[0],[0]
"R:
minimize w F (w) =
2
kwk2 + nX
i=1
h(p (i) (w), yi) .",5. Proposed Method,[0],[0]
"(9)
Using the notion of p(i)(w) visually indicates that node ranking probabilities p are (non-linearly) dependent on the parameter vector w.",5. Proposed Method,[0],[0]
"As for the choice of loss-functions, we examine the following:
• Wilcoxon-Mann-Whitney (WMW) loss (Yan et al., 2003).",5. Proposed Method,[0],[0]
"WMW maximizes the area under the ROC curve:
h(p, y) =
nX
j=1
1[y = +1^yj = 1] ⇣ 1 + exp p pjb ⌘ 1 .
",5. Proposed Method,[0],[0]
"• Smooth hinge-loss variant A smooth variant of the classical support vector machine hinge-loss with two additional parameters: a decision boundary b 2 R and a scaling parameter a 2 R:
h(p, y) =
8 ><
>: 1 2 y(ap b) if y(ap b)  0, 1 2 (1 y(ap b))2 if 0 < y(ap b)  1, 0 if 1 < y(ap b).
",5. Proposed Method,[0],[0]
"In this work, we focus on smooth, differentiable lossfunctions only, ensuring fast convergence to local optima via gradient-based methods, i.e., fast second-order methods (BFGS).",5. Proposed Method,[0],[0]
"A pivotal point is hence, to assess the gradient w.r.t.",5. Proposed Method,[0],[0]
"w.
Gradient Computation The remaining of this section is dedicated to the derivation of the gradient:
",5. Proposed Method,[0],[0]
"@F (w) @w = @ kwk2 @2w + Pn i @h(p(i)(w),yi) @w ,
where the loss-function h can be further split into @h(p(i)(w),yi)
@w = @h(p(i)(w),yi) @p(i)(w) @p(i)(w) @w .",5. Proposed Method,[0],[0]
"Since we con-
strained ourselves to differentiable loss-function h(p, y), the partial derivative w.r.t.",5. Proposed Method,[0],[0]
p can be calculated rather straightforward.,5. Proposed Method,[0],[0]
"More complicated is the evaluation of
@p(i)
",5. Proposed Method,[0],[0]
@w = @ @w p(i)k,5. Proposed Method,[0],[0]
⇡(i) =,5. Proposed Method,[0],[0]
✓ @p(i)k @w ⇡ (i) p(i)k @⇡ (i) @w ◆ ⇡ (i) 2 .,5. Proposed Method,[0],[0]
"(10)
The derivative of the i-th component of ⇡ is given by:
@⇡(i)
@w =
⇣ @deg⇤(vi) @w vol(V ) @vol(V )@w deg⇤(vi) ⌘ vol(V ) 2,
where @deg ⇤(vi) @w = P e2E vi2e @ae @w = P e2E vi2e @fw( e) @w and @vol(V )",5. Proposed Method,[0],[0]
@w = 2 P e2E @ae @w = 2 P e2E @fw( e) @w .,5. Proposed Method,[0],[0]
As fw is said to be differentiable the only part of Eq.,5. Proposed Method,[0],[0]
"(10) that remains is the Jacobian @pk/@w.
Theorem 3.",5. Proposed Method,[0],[0]
"The derivative @pk/@w for k 1 is given by:
@pk",5. Proposed Method,[0],[0]
@w = ✓ k 1P l=0 plQ k 1 l ◆ @Q @w .,5. Proposed Method,[0],[0]
"(11)
(the proof is given in Appendix A).",5. Proposed Method,[0],[0]
"The derivative of Q, defined in Eq.",5. Proposed Method,[0],[0]
"(7), is given by
@Quv @w =
8 <
:
@auv",5. Proposed Method,[0],[0]
@w P x aux auv P x @aux,5. Proposed Method,[0],[0]
"@w
( P x aux) 2",5. Proposed Method,[0],[0]
"if (u, v) 2 E, 0 otherwise.
",5. Proposed Method,[0],[0]
"This completes the computation of the gradient and enables the application of gradient-based methods, i.e., BFGS to find a (locally) optimal estimate ŵ.",5. Proposed Method,[0],[0]
"By using this estimate, TSR weights the whole graph, with which a short random walk is performed to obtain the final ranking p.
Robustness of TSR against Attacks By using the negative label information, our TSR, in principle, monitors “trust leak” through random walk, and adjusts the edge weights so that the leak is minimized.",5. Proposed Method,[0],[0]
"As a result, the weights tend to be lower on the attacking edges (to reduce the propagation), and higher on the Sybil edges (to boost the stationary distribution).",5. Proposed Method,[0],[0]
"Thus, we can expect that our TSR, which is an advanced integrated method, is more robust against attacks than the SybilRank and the two-step Integro approach.",5. Proposed Method,[0],[0]
"To assess the robustness of the proposed method and the baseline methods, we generate artificially network topology and edge and node attributes in order to have full control of the underlying ground truth.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"We separately create two graphs, the honest and the Sybil graph.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Both use the generation method proposed by Holme & Kim (2002) for scale free networks.,6. Empirical Evaluation on Synthetic Data,[0],[0]
Node features are generated randomly and correlated through dependency injection.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"The edge features function u,v simply stacks node features of the two adjacent nodes xu",6. Empirical Evaluation on Synthetic Data,[0],[0]
and,6. Empirical Evaluation on Synthetic Data,[0],[0]
xv (see Appendix B for more details).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Connections between Sybil and honest graphs are established according to a random attacking strategy that iteratively adds attacking edges randomly, i.e., equally distributed on the set of all possible attacking edges VH ⇥",6. Empirical Evaluation on Synthetic Data,[0],[0]
"VS
or a adversarial attacking strategy that solves Problem (6) for optimal attacks.",6. Empirical Evaluation on Synthetic Data,[0],[0]
This strategy only chooses an honest node to be attacked next and the corresponding Sybil node is chosen randomly (equally distributed on the set of all Sybil nodes VS).,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We test our method, TSR, using the proposed loss functions and compare against the stateof-the-art methods SybilRank and Integro.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"As Integro depends on a preceding victim prediction, we simulated one that achieves highest possible rankings (ROC-AUC close to 1.0).2
Random Attacking Strategy We generated a sample network (|VH | = 200 and |VS | = 30) and select 15 honest nodes and 8 Sybil nodes randomly, which will be used as labeled examples for our TSR.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The labeled honest nodes are also used as the set VT of trusted seeding nodes for the random walks in all methods.,6. Empirical Evaluation on Synthetic Data,[0],[0]
We evaluate the performance in terms of ROC-AUC-values for the computed ranking.,6. Empirical Evaluation on Synthetic Data,[0],[0]
This procedure was repeated 20 times for varying number of attacking edges (10-200 edges).,6. Empirical Evaluation on Synthetic Data,[0],[0]
Figure 1 shows ROC-AUC curves for all methods under the random attacking setting.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"We can obsreve that our TSR, regardless of the choice of loss function, performs superior to the other methods.",6. Empirical Evaluation on Synthetic Data,[0],[0]
Integro’s accuracy deteriorates fast but still has an edge over SybilRank up to the point where the ROCAUC-value reaches 0.5.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"After that SybilRank and Integro essentially perform similar.
",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Adversarial Attacking Strategy For the adversarial setting, we ran the same benchmarks but this time attacking edges were added according to the adversarial attacking strategy.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Due to the much more aggressive setting, we varied the number of attacking edges from 1-40 and repeated this procedure 20 times to report averaged ROCAUC accuracies.",6. Empirical Evaluation on Synthetic Data,[0],[0]
The results are depicted in Figure 2.,6. Empirical Evaluation on Synthetic Data,[0],[0]
All choices of loss functions outperform SybilRank and Integro clearly.,6. Empirical Evaluation on Synthetic Data,[0],[0]
The results confirm our considerations that SybilRank’s performance drops fast and steep as soon as a certain amount of attacking edges is established.,6. Empirical Evaluation on Synthetic Data,[0],[0]
"Integro behaves more robust than SybilRank, but, ultimately, must resign after a few more attacking edges.",6. Empirical Evaluation on Synthetic Data,[0],[0]
"Again, our TSR is significantly more robust against adversarial attacks and can withstand higher number of attacking edges until its performance finally deteriorates.",6. Empirical Evaluation on Synthetic Data,[0],[0]
We also evaluated our method on a sample of the Facebook graph Leskovec & Mcauley collected from survey participants using the Facebook app.,7. Empirical Evaluation on Real-world Data,[0],[0]
"The dataset includes the topology (|V | = 4039 users and |E| = 88234 friend-
2SybiRank, Integro, and TSR rely on different information, and therefore, the fairness of comparison is not trivial.",7. Empirical Evaluation on Real-world Data,[0],[0]
"We discuss this issue in Appendix C.
ships) as well as node features for every node (see Table 1 for summary), Figure 3).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Node features are comprised of obfuscated categorical features of users profiles including education, work, hometown, language, last name, etc.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As with most of real world social graphs, the data exhibits strong multi-cluster structure, as seen in Figure 3 and Figure 4.",7. Empirical Evaluation on Real-world Data,[0],[0]
"These clusters pose additional challenges to the application of random walk-based methods as the trust propagation between two loosely inter-connected clusters is low (Cao et al., 2012; Boshmaf et al., 2016).",7. Empirical Evaluation on Real-world Data,[0],[0]
"Hence, trust seeds should be distributed among all clusters.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Following SybilRank and Integro (Cao et al., 2012), we employ the Louvian clustering method (Blondel et al., 2008) first.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"As common, the Sybil graph needs to be generated.",7. Empirical Evaluation on Real-world Data,[0],[0]
"For this purpose, a (small) subgraph was copied and declared as Sybil.",7. Empirical Evaluation on Real-world Data,[0],[0]
The attacking edges were created to link the honest and the Sybil graph following one of the attacking strategies (random or adversarial).,7. Empirical Evaluation on Real-world Data,[0],[0]
It was made sure that no Sybil node attacked one of the direct neighbors of its origin which is reasonable for most social graphs.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Edge features
for TSR are as follows: the number of shared features (in total), the number of shared friends, and the number of shared features within specific categories.",7. Empirical Evaluation on Real-world Data,[0],[0]
"The other experimental setup is the same as the previous section.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Random Attacks,7. Empirical Evaluation on Real-world Data,[0],[0]
The trusted nodes |VT,7. Empirical Evaluation on Real-world Data,[0],[0]
| = 50 were randomly distributed among all clusters and a small subset of Sybils |VD| = 30 was chosen as known Sybil nodes.,7. Empirical Evaluation on Real-world Data,[0],[0]
Attacking edges EA were established following the random attacking strategy ranging from |EA| = 1 to |EA| = 1400.,7. Empirical Evaluation on Real-world Data,[0],[0]
Experiments were repeated 10 times.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro was run with
sarial attacking scenario on the Facebook graph.
",7. Empirical Evaluation on Real-world Data,[0],[0]
"two levels of accuracy in simulated victim detection, i.e., perfect (AUROC = 1) and almost perfect (AUROC = 0.9).",7. Empirical Evaluation on Real-world Data,[0],[0]
Figure 5 shows the AUROC-values.,7. Empirical Evaluation on Real-world Data,[0],[0]
The detection performance of SybilRank is the lowest and drops soon as attacking edges increase.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Integro with the perfect victim detection outperforms the other methods, but with just a slight reduction in the victim detection accuracy (AUROC = 0.9), its performance drops significantly.",7. Empirical Evaluation on Real-world Data,[0],[0]
All versions of TSR perform almost on par with perfect version of Integro in the lower range of attacking edges (1—800).,7. Empirical Evaluation on Real-world Data,[0],[0]
"In the higher range (800—1400), the hinge loss drop fast to end up with a performance similar to Integro with the almost perfect victim detection.",7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the variant that uses the WMW-loss does not show this performance drop and stays close to the upper-bound of Integro.
",7. Empirical Evaluation on Real-world Data,[0],[0]
Adversarial Attacks The number of adversarial attack edges ranged from |EA| = 1 to |EA| = 45.,7. Empirical Evaluation on Real-world Data,[0],[0]
"Figure 6 shows
the recorded average AUROC-values.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Again, SybilRank’s performance drops the fastest and steepest and Integro is insignificantly better in this adversarial scenario.",7. Empirical Evaluation on Real-world Data,[0],[0]
Both variants of TSR performs better than the baselines.,7. Empirical Evaluation on Real-world Data,[0],[0]
"However, the WMW-loss variant performs only slightly better than SybilRank and Integro, while the hinge-loss variant keeps good performance even for a large number of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"As our future work, we will investigate which loss function should be chosen, depending on data and assumed attacker’s strategy.",7. Empirical Evaluation on Real-world Data,[0],[0]
"Overall, whereas SybilRank’s and Integro’s performance drops to an average AUROC-value below 0.5 at |EA| = 30, the hinge-loss variant of TSR still achieves an average value over 0.9 at the same amount of attacking edges.",7. Empirical Evaluation on Real-world Data,[0],[0]
"In this paper, we studied the problem of Sybil detection.",8. Conclusion & Outlook,[0],[0]
We first refined the security guarantees of random walk approaches towards more realistic assumptions.,8. Conclusion & Outlook,[0],[0]
"Then, we formalized and coined the adversarial setting and introduced optimal strategies for attackers.",8. Conclusion & Outlook,[0],[0]
"Further, we proposed a new method, transductive Sybil ranking (TSR), that leverages prior information, network topology as well as node and edge features.",8. Conclusion & Outlook,[0],[0]
"Unlike Integro, it is fused in a single optimization framework and can be solved efficiently by using gradient-based optimizer.",8. Conclusion & Outlook,[0],[0]
"In our empirical evaluation, we showed the advantages of our method and investigated the susceptibility of our method and baseline competitors to adversarial attacks.",8. Conclusion & Outlook,[0],[0]
"Further research will focus on the application of our method to real-world, large-scale OSNs.",8. Conclusion & Outlook,[0],[0]
"JH was supported by MathPlan GmbH and innoCampus, TU-Berlin.",9. Acknowledgments,[0],[0]
"SN, AB and KRM were supported by the German Ministry for Education and Research as Berlin Big Data Center BBDC, funding mark 01IS14013A. KRM thanks for the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (No.2017-0-00451).",9. Acknowledgments,[0],[0]
NG was supported by BMBF ALICE II grant 01IB15001B.,9. Acknowledgments,[0],[0]
Sybil detection is a crucial task to protect online social networks (OSNs) against intruders who try to manipulate automatic services provided by OSNs to their customers.,abstractText,[0],[0]
"In this paper, we first discuss the robustness of graph-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees towards more realistic assumptions.",abstractText,[0],[0]
"After that, we formally introduce adversarial settings for the graph-based Sybil detection problem and derive a corresponding optimal attacking strategy by exploitation of trust leaks.",abstractText,[0],[0]
"Based on our analysis, we propose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks.",abstractText,[0],[0]
Our empirical evaluation shows significant advantages of TSR over stateof-the-art competitors on a variety of attacking scenarios on artificially generated data and realworld datasets.,abstractText,[0],[0]
Minimizing Trust Leaks for Robust Sybil Detection,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1379–1388, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually. Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data. Among a series of methods, Random Walk is believed to be suitable for knowledge graph data. However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference. Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas. To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step. The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously. The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.",text,[0],[0]
"Recently, various knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), WordNet (Miller, 1995), Yago (Hoffart et al., 2013), have been built, and researchers begin to explore how to make use of structural information to promote performances of several inference-based NLP applications, such as
text entailment, knowledge base completion, question and answering.",1 Introduction,[0],[0]
"Creating useful formulas is one of the most important steps in inference, and an accurate and high coverage formula set will bring a great promotion for an inference system.",1 Introduction,[0],[0]
"For example, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w)⇒ Language(x, w) is a high-quality formula, which means people with the same nationality probably speak the same language.",1 Introduction,[0],[0]
"However, it is a challenge to create formulas for open-domain KBs, where there are a great variety of relation types and it is impossible to construct a complete formula set by hand.
",1 Introduction,[0],[0]
"Several data-driven methods, such as Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Markov Logic Network (MLN) (Richardson and Domingos, 2006), have been proposed to mine formulas automatically from KB data, which transform frequent sub-structures of KBs, e.g., paths or loops, into formulas.",1 Introduction,[0],[0]
"Figure 1.a shows a sub-graph extracted from Freebase, and the formula mentioned above about Language can be generated from the loop in Figure 1.d.",1 Introduction,[0],[0]
"However, the running time of these traditional probabilistic inference methods is unbearable over large-scale KBs.",1 Introduction,[0],[0]
"For example, MLN needs grounding for each candidate formula, i.e., it needs to enumerate all paths.",1 Introduction,[0],[0]
"Therefore, the computation complexity of MLN increases exponentially with the scale of a KB.
",1 Introduction,[0],[0]
"In order to handle large-scale KBs, the random walk is usually employed to replace enumerating all possible sub-structures.",1 Introduction,[0],[0]
"However, random walk is inefficient to find useful structures due to its completely randomized mechanism.",1 Introduction,[0],[0]
"For example in Fig-
1379
ure 1.b, the target path (yellow one) has a small probability to be visited, the reason is that the algorithm may select all the neighboring entity to transfer with an equal probability.",1 Introduction,[0],[0]
"This phenomenon is very common in KBs, e.g., each entity in Freebase has more than 30 neighbors in average, so there will be about 810,000 paths with length 4, and only several are useful.",1 Introduction,[0],[0]
"There have been two types of methods proposed to improve the efficiency of random walks, but they still meet serious problems, respectively.",1 Introduction,[0],[0]
1),1 Introduction,[0],[0]
Increasing rounds of random walks.,1 Introduction,[0],[0]
"More rounds of random walks will find more structures, but it will simultaneously introduce more noise and thus generate more false formulas.",1 Introduction,[0],[0]
"For example, the loop in Figure 1.c exists in Freebase, but it produces a false formula, Gender(x, y) ∧ Gender(z, y) ∧ Language(z, w)⇒ Language(x, w), which means people with the same gender speak the same language.",1 Introduction,[0],[0]
"This kind of structures frequently occur in KBs even the formulas are mined with a high confidence, because there are a lot of sparse structures in KBs which will lead to inaccurate confidence.",1 Introduction,[0],[0]
"According to our statistics, more than 90 percent of high-confidence formulas produced by random walk are noise.",1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
Employing heuristic rules to direct random walks.,1 Introduction,[0],[0]
"This method directs random walks to find useful structures by rewriting the state transition probability matrix, but the artificial heuristic rules may only apply to a little part of formulas.",1 Introduction,[0],[0]
"For example, PRA (Lao and Cohen, 2010; Lao et al., 2011) assumes the more narrow distributions of elements in a formula are, the higher score the formula will obtain.",1 Introduction,[0],[0]
"However, formulas with high scores in PRA are not always true.",1 Introduction,[0],[0]
"For example, the formula in Figure 1.c has a high score in PRA, but it is not true.",1 Introduction,[0],[0]
"Oppositely, formulas with low scores in PRA are not always useless.",1 Introduction,[0],[0]
"For example, the formula, Father(x, y) ∧ Father(y, z) ⇒ Grandfather(x, t), has a low score when x and y both have several sons, but it obviously is the most effective to infer Grandfather.",1 Introduction,[0],[0]
"According to our investigations, the situations are common in KBs.",1 Introduction,[0],[0]
"In this paper, we propose a Goal-directed Random Walk algorithm to resolve the above problems.",1 Introduction,[0],[0]
The algorithm employs the specific inference target as the direction at each step in the random walk process.,1 Introduction,[0],[0]
"In more detail, to achieve such a goaldirected mechanism, at each step of random walk, the algorithm dynamically estimates the potentials for each neighbor by using the ultimate goal, and assigns higher probabilities to the neighbors with higher potentials.",1 Introduction,[0],[0]
"Therefore, the algorithm is more inclined to visit structures which are beneficial to infer
the target and avoid transferring to noise structures.",1 Introduction,[0],[0]
"For example in Figure 1, when the inference target is what language a person speaks, the algorithm is more inclined to walk along Nationality edge than Gender, because Nationality has greater potential than Gender to infer Language.",1 Introduction,[0],[0]
We build a real potential function based on low-rank distributional representations.,1 Introduction,[0],[0]
The reason of replacing symbols by distributional representations is that the distributional representations have less parameters and latent semantic relationship in them can contribute to estimate potentials more precisely.,1 Introduction,[0],[0]
"In summary, the contributions of this paper are as follows.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the basic random walk, our approach direct random walks by the inference target, which increases efficiency of mining useful formulas and has a great capability of resisting noise.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Compared with the heuristic methods, our approach can learn the strategy of random walk automatically and dynamically adjust the strategy for different inference targets, while the heuristic methods need to write heuristic rules by hand and follow the same rule all the time.",1 Introduction,[0],[0]
"• The experiments on link prediction task prove that our approach has a high efficiency on mining formulas and has a good performance on both WN18 and FB15K datasets.
",1 Introduction,[0],[0]
"The rest of this paper is structured as follows, Section 2 introduces the basic random walk for mining formulas.",1 Introduction,[0],[0]
Section 3 describes our approach in detail.,1 Introduction,[0],[0]
The experimental results and related discussions are shown in Section 4.,1 Introduction,[0],[0]
"Section 5 introduces some related works, and finally, Section 6 concludes this paper.",1 Introduction,[0],[0]
"Mining frequent patterns from source data is a problem that has a long history, and for different specific tasks, there are different types of source data and different definitions of pattern.",2.1 Frequent Pattern Mining,[0],[0]
"Mining formulas is more like frequent subgraph mining, which employs paths or loops as frequent patterns and mines them from a KB.",2.1 Frequent Pattern Mining,[0],[0]
"For each relation type R, the algorithm enumerates paths from entity H to entity T for each triplet R(H,T ).",2.1 Frequent Pattern Mining,[0],[0]
These paths are normalized to formulas by replacing entities to variables.,2.1 Frequent Pattern Mining,[0],[0]
"For example, the loop in Figure 1.d, National-
ity(Bob, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Nationality(Stewart, America)",2.1 Frequent Pattern Mining,[0],[0]
"∧ Language(Bob, English) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(Stewart, English), can be normalized to the formula, Nationality(x, y) ∧ Nationality(z, y) ∧ Language(z, w) ⇒",2.1 Frequent Pattern Mining,[0],[0]
"Language(x, w).",2.1 Frequent Pattern Mining,[0],[0]
"Support and confidence are employed to estimate a formula, where the support value of a formula f : X ⇒ Y , noted as Sf , is defined as the proportion of paths in the KB which contains the body X , and the confidence value of X ⇒ Y , noted as Cf , is defined as the proportion of the paths that contains X which also meets X ⇒ Y .",2.1 Frequent Pattern Mining,[0],[0]
"Cf is calculated as follows,
Cf = Nf NX
(1)
",2.1 Frequent Pattern Mining,[0],[0]
whereNf is the total number of instantiated formula f and NX is the total number of instantiated X .,2.1 Frequent Pattern Mining,[0],[0]
Enumerating paths is a time consuming process and does not apply to large-scale KBs.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore, random walk on the graph is proposed to collect frequent paths instead of enumerating.",2.2 Random Walk on Knowledge Graph,[0],[0]
Random walk randomly chooses a neighbor to jump unlike enumerating which needs to search all neighbors.,2.2 Random Walk on Knowledge Graph,[0],[0]
"To estimate a formula f , the algorithm employs f ’s occurrence number during random walks N
′ f to approxi-
mate the total number Nf in Equation (1), and similarly employs N
′ X to approximate NX .",2.2 Random Walk on Knowledge Graph,[0],[0]
"Therefore,
f ’s confidence Cf can be approximatively estimated by N
′ f and N ′ X , noted as C ′ f .
",2.2 Random Walk on Knowledge Graph,[0],[0]
"Random walk maintains a state transition probability matrix P , and Pij means the probability of jumping from entity i to entity j. To make the confidence C
′",2.2 Random Walk on Knowledge Graph,[0],[0]
"f as close to the true confidence Cf as pos-
sible, the algorithm sets P as follows,
Pij = { 1/di, j ∈ Adj(i) 0, j /∈ Adj(i)",2.2 Random Walk on Knowledge Graph,[0],[0]
"(2)
where di is the out-degree of the entity i, Adj(i) is the set of adjacent entities of i, and ∑N j=1 Pij = 1.",2.2 Random Walk on Knowledge Graph,[0],[0]
Such a transition matrix means the algorithm may jump to all the neighboring entities with an equal probability.,2.2 Random Walk on Knowledge Graph,[0],[0]
"Such a random walk is independent from the inference target, so we call this type of random walk as a goalless random walk.",2.2 Random Walk on Knowledge Graph,[0],[0]
The goalless mechanism causes the inefficiency of mining useful structures.,2.2 Random Walk on Knowledge Graph,[0],[0]
"When we want to mine paths for R(H,T ), the algorithm cannot arrive at T from H
in the majority of rounds.",2.2 Random Walk on Knowledge Graph,[0],[0]
"Even though the algorithm recalls several paths for R(H,T ), some of them may generate noisy formulas for inferring R(H,T ).
",2.2 Random Walk on Knowledge Graph,[0],[0]
"To solve the above problem, several methods direct random walks by statically modifying P .",2.2 Random Walk on Knowledge Graph,[0],[0]
"For example, PRA sets Prij = P (j|i;r) |Ri| , P (j|i; r) = r(i,j) r(i,∗) , where P (j|i; r) is the probability of reaching node j from node i under the specific relation r, r(i, ∗) is the number of edges from i under r, and Ri is the number of relation types from i. Such a transition matrix implies the more narrow distributions of elements in a formula are, the higher score the formula will obtain, which can be viewed as the heuristic rule of PRA.",2.2 Random Walk on Knowledge Graph,[0],[0]
"We propose to use the inference target, ρ = R(H,T ), to direct random walks.",3.1 Goal-Directed Random Walk,[0],[0]
"When predicting ρ, our approach always directs random walks to find useful structures which may generate formulas to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"For different ρ, random walks are directed by modifying the transition matrix P in different ways.",3.1 Goal-Directed Random Walk,[0],[0]
"Our approach dynamically calculates Prij when jumping from entity i to entity j under relation r as follows,
",3.1 Goal-Directed Random Walk,[0],[0]
"Prij =    Φ(r(i, j), ρ)∑ k∈Adj(i) Φ(r(i, k), ρ) , j ∈ Adj(i)
0, j /∈ Adj(i)",3.1 Goal-Directed Random Walk,[0],[0]
"(3)
where Φ(r(i, j), ρ) is the r(i, j)’s potential which measures the potential contribution to infer ρ after walking to j.
Intuitively, if r(i, j) exits in a path from H to T and this path can generate a benefic formula to infer R(H,T ), the probability of jumping from i to j should larger and thus Φ(r(i, j), ρ) also should be larger.",3.1 Goal-Directed Random Walk,[0],[0]
"Reversely, if we cannot arrive at T within the maximal steps after jumping to j, or if the path produces a noisy formula leading to a wrong inference, Pij and Φ(r(i, j), ρ) should both be smaller.
",3.1 Goal-Directed Random Walk,[0],[0]
"To explicitly build a bridge between the potential Φ and the inference goal ρ, we maximize the likelihood of paths which can infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"First, we recursively define the likelihood of a path from H to t
as PpHt = PpHs ·",3.1 Goal-Directed Random Walk,[0],[0]
"Prst , where Prst is defined in Equation (3).",3.1 Goal-Directed Random Walk,[0],[0]
"We then classify a path pHt into three separate categories: a) t = T and pHt can produce a benefic formula to infer R(H,T ); b) t 6=",3.1 Goal-Directed Random Walk,[0],[0]
T ; c) t = T but pHt may generate a noisy formula which misleads inference.,3.1 Goal-Directed Random Walk,[0],[0]
"Finally, we define the likelihood function as follows,
maxPP = ∏
pHt∈P P apHt(1− PpHt) b+c (4)
where P is all paths found in the process of performing random walks for R(H,T ), and t may be equal to T or not.",3.1 Goal-Directed Random Walk,[0],[0]
"a, b, c are three 0-1 variables corresponding to the above categories a), b), c).",3.1 Goal-Directed Random Walk,[0],[0]
"Only one in a, b, c can be 1 when PHt belongs to the corresponding category.",3.1 Goal-Directed Random Walk,[0],[0]
We then transform maximizing PP to minimizing Lrw = − logPP and employ SGD to train it.,3.1 Goal-Directed Random Walk,[0],[0]
"In practice, there is not a clear-cut boundary between a) and c), so we divide the loss into two parts:",3.1 Goal-Directed Random Walk,[0],[0]
Lrw = Ltrw + λL inf rw .,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw is the loss of that t 6=,3.1 Goal-Directed Random Walk,[0],[0]
"T , and Linfrw is the loss of that pHT generates a noisy formula leading to a wrong inference.",3.1 Goal-Directed Random Walk,[0],[0]
λ is a super parameter to balance the two losses.,3.1 Goal-Directed Random Walk,[0],[0]
Ltrw and Linfrw have the same expression but are optimized in different stages.,3.1 Goal-Directed Random Walk,[0],[0]
"Ltrw can be optimized during random walks, while Linfrw should be optimized in the inference stage.",3.1 Goal-Directed Random Walk,[0],[0]
"We rewrite Lrw for a specific path p as follows,
Lrw(p) = −y logPp − (1− y) log (1− Pp) (5)
where y is the label of the path p and y = 1 if p is beneficial to infer ρ.",3.1 Goal-Directed Random Walk,[0],[0]
"To obtain the best Φ, we compute gradients of Lrw as follows,
∇Lrw(p) =",3.1 Goal-Directed Random Walk,[0],[0]
"(∇Lrw(r12),∇Lrw(r23), ...)
∇Lrw(rij)",3.1 Goal-Directed Random Walk,[0],[0]
"= ( ∂Lrw(rij) ∂Φrij , ∂Lrw(rij) ∂Φrik1 , ∂Lrw(rij) ∂Φrik2 , ...)
∂Lrw(rij)
",3.1 Goal-Directed Random Walk,[0],[0]
∂Φrij =,3.1 Goal-Directed Random Walk,[0],[0]
(Pp − y) · (1− Prij ),3.1 Goal-Directed Random Walk,[0],[0]
"Φrij · (1− Pp)
∂Lrw(rij) ∂Φrik",3.1 Goal-Directed Random Walk,[0],[0]
=,3.1 Goal-Directed Random Walk,[0],[0]
− (Pp − y) ·,3.1 Goal-Directed Random Walk,[0],[0]
"Prij
Φrij · (1− Pp) (6)
where ∇Lrw(rij) is the component of ∇Lrw(p) at rij .",3.1 Goal-Directed Random Walk,[0],[0]
"Φ(r(i,",3.1 Goal-Directed Random Walk,[0],[0]
"j), ρ) and Φ(r(i, k), ρ) are the potentials for all triplets r(i, j) ∈ p and r(i, k) /∈",3.1 Goal-Directed Random Walk,[0],[0]
"p, and rij is short for r(i, j).",3.1 Goal-Directed Random Walk,[0],[0]
"After iteratively updating Φrij and Φrik by the gradient of L t rw, the random walks can
be directed to find more paths fromH to T , and consequently it increases efficiency of the random walk.",3.1 Goal-Directed Random Walk,[0],[0]
"After updating Φrij and Φrik by the gradient ofL inf rw , random walk is more likely to find high-quality paths but not noise.",3.1 Goal-Directed Random Walk,[0],[0]
"Therefore, the goal-directed random walk increases efficiency of mining benefic formulas and has a great capability of resisting noise.",3.1 Goal-Directed Random Walk,[0],[0]
"The potential Φ(r(i, j), ρ) measures an implicit relationship between two triplets in the KB, so the total number of parameters is the square of the KB size.",3.2 Distributional Potential Function,[0],[0]
It is hard to precisely estimate all Φ because of the sparsity of training data.,3.2 Distributional Potential Function,[0],[0]
"To reduce the number of parameters, we represent each entity or relation in the KB as a low-rank numeric vector which is called embeddings (Bordes et al., 2013), and then we build a potential function Ψ on embeddings as Φ(r(i, j), ρ) = Ψ(Er(i,j), ER(H,T )), where Er(i,j) and ER(H,T ) are the embeddings of triplets.",3.2 Distributional Potential Function,[0],[0]
"In practice, we set Er(i,j) =",3.2 Distributional Potential Function,[0],[0]
"[Er, Ej ] and ER(H,T ) =",3.2 Distributional Potential Function,[0],[0]
"[ER, ET ] because Ei is the same for all triplets r(i, ∗), where [] is a concatenation operator.
",3.2 Distributional Potential Function,[0],[0]
"In the view of the neural network, our goaldirected mechanism is analogous to the attention mechanism.",3.2 Distributional Potential Function,[0],[0]
"At each step, the algorithm estimates attentions for each neighboring edges by Ψ. Therefore, there are several existing expressions of Ψ, e.g., the dot product (Sukhbaatar et al., 2015) and the single-layer perceptron (Bahdanau et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We will not compare different forms of Ψ, the detail comparison has been presented in the work (Luong et al., 2015).",3.2 Distributional Potential Function,[0],[0]
"We directly employ the simplest dot product for Ψ as follows,
Ψ(Er(i,j), ER(H,T ))",3.2 Distributional Potential Function,[0],[0]
"= σ(Er(i,j) · ER(H,T )) (7)
where σ is a nonlinear function and we set it as an exponential function.",3.2 Distributional Potential Function,[0],[0]
Ψ has no parameters beside KB embeddings which are updated during the training period.,3.2 Distributional Potential Function,[0],[0]
"To handle the dependence between goal-directed random walk and subsequent inference, we combine them into an integrated model and optimize them together.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the integrated model first collects formulas for R(H,T ), and then
Algorithm 1:",3.3 Integrated Inference Model,[0],[0]
"Train Integrated Inference Model
Input: KB, Ξ Output: Ψ, W , F 1: For ρ = R(H,T ) ∈ Ξ 2: Repeat ρ-directed Random Walk from H to t 3: Update Ψ by Ltrw 4: If t = T , then F = F ∩ fp 5: Calculate Linf and L inf rw by ρ 6: Update W by Linf 7: Update Ψ by Linfrw 8: Remove f ∈ F with little wf 9: Output Ψ, W , F
merges estimations of different formulas as features into a score function χ,
χ(ρ) =",3.3 Integrated Inference Model,[0],[0]
"∑
f∈Fρ δ(f) (8)
where Fρ is the formula set obtained by random walks for ρ, and δ(f) is an estimation of formula f .",3.3 Integrated Inference Model,[0],[0]
"The original frequent pattern mining algorithm employs formulas’ confidence as δ(f) directly, but it does not produce good results (Galárraga et al., 2013).",3.3 Integrated Inference Model,[0],[0]
"There are two ways to solve the problem: one is selecting another more suitable measure of f as δ(f) (Tan et al., 2002); the other is attaching a weight to each formula and learning weights with supervision, e.g., MLN (Richardson and Domingos, 2006) .",3.3 Integrated Inference Model,[0],[0]
We employ the latter method and set δ(f) =,3.3 Integrated Inference Model,[0],[0]
wf ·nf .,3.3 Integrated Inference Model,[0],[0]
"Finally, we employ a logistic regression classifier to predict R(H,T ), and the posterior probability of R(H,T ) is shown as follows,
P (ρ = y|χ) =",3.3 Integrated Inference Model,[0],[0]
"F(χ)y(1−F(χ))1−y
F(χ) = 1 1 + e−χ
(9)
where y is a 0-1 label of ρ.",3.3 Integrated Inference Model,[0],[0]
"Similar to Ltrw in Equation (5), we treat the negative logarithm of P (ρ = y|χ) as the loss of inference, Linf = − logP (ρ = y|χ), and turn to minimize it.",3.3 Integrated Inference Model,[0],[0]
"Moreover, the loss Linfrw of the above goal-directed random walk is influenced by the result of predicting R(H,T ), so Φrij and Φrik will be also updated.",3.3 Integrated Inference Model,[0],[0]
"Algorithm 1 shows the main process of training, where Ξ is the triplet set for training, Ψ is the potential function in Equation (7), F is the formula set, fp is
a formula generated from the path p, and H,T, t are entities in the KB.",3.3 Integrated Inference Model,[0],[0]
"To predict ρ = R(H,T ), the algorithm first performs multi rounds of random walks, and each random walk can find a path pHt (at line 2).",3.3 Integrated Inference Model,[0],[0]
"Then the algorithm decides to update Ψ by Ltrw based on whether t is T (at line 3), and adds the formula pf into the formula set when t = T (at line 4).",3.3 Integrated Inference Model,[0],[0]
"After random walks, the inference model predicts ρ, and computes Linf and L inf rw according to the prediction result (at line 5).",3.3 Integrated Inference Model,[0],[0]
"FinallyW and Ψ are updated by Linf and L inf rw (at line 6-7), respectively.",3.3 Integrated Inference Model,[0],[0]
"After training by all triplets in Ξ, the algorithm removes formulas with low weights from F (at line 8) and outputs the model (at line 9).",3.3 Integrated Inference Model,[0],[0]
"When we infer a new triplet by this model, the process is similar to Algorithm 1.",3.3 Integrated Inference Model,[0],[0]
We first compare our approach with several state-ofart methods on link prediction task to explore our approach’s overall ability of inference.,4 Experiments,[0],[0]
"Subsequently, we evaluate formulas mined by different random walk methods to explore whether the goal-directed mechanism can increase efficiency of mining useful structures.",4 Experiments,[0],[0]
"Finally, we dive deep into the formulas generated by our approach to analyze the characters of our approach.",4 Experiments,[0],[0]
"We conduct experiments on both WN18 and FB15K datasets which are subsets sampled from WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008), respectively, and Table 1 shows the statistics of them.",4.1 Datasets and Evaluation Setup,[0],[0]
"For the link prediction task, we predict the missing h or t for a triplet r(h, t) in test set.",4.1 Datasets and Evaluation Setup,[0],[0]
"The detail evaluation method is that t in r(h, t) is replaced by all entities in the KB and methods need to rank the right answer at the top of the list, and so does h in r(h, t).",4.1 Datasets and Evaluation Setup,[0],[0]
"We report the mean of those true answer ranks and the Hits@10 under both ’raw’ and ’filter’ as TransE (Bordes et al., 2013) does, where Hits@10 is the proportion of correct entities ranked in the top 10.
sults on relation form of government in FB15K.",4.1 Datasets and Evaluation Setup,[0],[0]
We employ two types of baselines.,4.2 Baselines,[0],[0]
"One type is based on random walks including: a) the basic random walk algorithm whose state transition probability matrix is shown in Equation (2); b) PRA in (Lao et al., 2011) which is a typical heuristic random walk algorithm.",4.2 Baselines,[0],[0]
"The other type is based on KB embeddings including TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",4.2 Baselines,[0],[0]
"(Lin et al., 2015b).",4.2 Baselines,[0],[0]
"These embedding-based methods have no explicit formulas, so we will not evaluate their performances on mining formulas.",4.2 Baselines,[0],[0]
We implement three random walk methods under a unified framework.,4.3 Settings,[0],[0]
"To predict r(h, ∗) quickly, we first select Top-K candidate instances, t1→K , by TransE as (Wei et al., 2015), and then the algorithm infers each r(h, ti) and ranks them by inference results.",4.3 Settings,[0],[0]
"We adjust parameters for our approach with the validate dataset, and the optimal configurations are set as follows.",4.3 Settings,[0],[0]
"The rounds of random walk is 10, learning rate is 0.0001, training epoch is 100, the size of candidate set is 500 for WN18 and 100 for FB15K, the embeddings have 50 dimensionalities for WN18 and 100 dimensionalities for FB15K, and the embeddings are initialized by TransE. For some relations, random walk truly finds no practicable formulas, so we employ TransE to improve per-
formance for these relations.",4.3 Settings,[0],[0]
"For embedding-based methods, we use reported results directly since the evaluation datasets are identical.",4.3 Settings,[0],[0]
"We show the results of link prediction for our approach and all baselines in Table 2 (* means the mean of ranks for random walk methods are evaluated in the Top-K subset), and we can obtain the following observations:
1)",4.4 Results on Link Prediction,[0],[0]
"Our approach achieves good performances on both WN18 and FB15K. On the FB15K, our approach outperforms all baselines.",4.4 Results on Link Prediction,[0],[0]
It indicates that our approach is effective for inference.,4.4 Results on Link Prediction,[0],[0]
"On the WN18, three random walk methods have similar performances.",4.4 Results on Link Prediction,[0],[0]
"The reason is that most entities in WN18 only have a small number of neighbors, so RW and PRA can also find useful structures in a few rounds.
2) For FB15K, the performances of RW and PRA are both poor and even worse than a part of embedding-based methods, but the performance of our approach is still the best.",4.4 Results on Link Prediction,[0],[0]
"The reason is that there are too many relation types in FB15K, so goalless random walks introduce lots of noise.",4.4 Results on Link Prediction,[0],[0]
"Oppositely, our approach has a great capability of resisting noise for the goal-directed mechanism.
3) RW and PRA have similar performances on both datasets, which indicates the heuristic rule of PRA does not apply to all relations and formulas.",4.4 Results on Link Prediction,[0],[0]
"To further explore whether the goal-directed mechanism can increase efficiency of mining paths, we compare the three random walk methods by the number of paths mined.",4.5 Paths Recall by Random Walks,[0],[0]
"For each triplet R(H,T )
in the training set, we perform 10 rounds of random walks fromH and record the number of times which arrive at T, noted as Arr@10.",4.5 Paths Recall by Random Walks,[0],[0]
We respectively select one relation type from WN18 and FB15K and show the comparison result in Figure 2.,4.5 Paths Recall by Random Walks,[0],[0]
"We can obtain the following observations:
1) With the increase of training epochs, Arr@10 of the goal-directed random walk first increases and then stays around a high value on both WN18 and FB15K, but the Arr@10 of RW and PRA always stay the same.",4.5 Paths Recall by Random Walks,[0],[0]
"This phenomenon indicates that the goal-directed random walk is a learnable model and can be trained to find more useful structures with epochs increasing, but RW and PRA are not.
2) RW and PRA always have similar Arr@10, which means PRA has not found more formulas.",4.5 Paths Recall by Random Walks,[0],[0]
This indicates that the heuristic rule of PRA is not always be beneficial to mining more structures for all relations.,4.5 Paths Recall by Random Walks,[0],[0]
"In Table 3, we show a small number of formulas mined by our approach from FB15K, and the formulas represent different types.",4.6 Example Formulas,[0],[0]
"Some formulas contain clear logic, e.g, Formula 1 means that if the writer x contributes a story to the film y and y is adapted from the book z, x is the writer of the book z.",4.6 Example Formulas,[0],[0]
"Some formulas have a high probability of being satisfied, e.g., Formula 3 means the wedding place probably is also the burial place for some people, and Formula 7 means the parent of the person x died of the disease and thus the person x has a high risk of suffering from the disease.",4.6 Example Formulas,[0],[0]
"Some formulas depend on synonyms, e.g., story by and works written have the similar meaning in Formula 2.",4.6 Example Formulas,[0],[0]
"However, there are still useless formulas, e.g, Formula 8 is useless be-
cause the body of the formula is same as the head.",4.6 Example Formulas,[0],[0]
"Such useless formula can be removed by a superrule, which is that the head of a formula cannot occur in its body.",4.6 Example Formulas,[0],[0]
"Our work has two aspects, which are related to mining formula automatically and inference on KBs, respectively.
",5 Related Work,[0],[0]
"Inductive Logic Programming (ILP) (Muggleton and De Raedt, 1994) and Association Rule Mining (ARM) (Agrawal et al., 1993) are both early works on mining formulas.",5 Related Work,[0],[0]
"FOIT (Quinlan, 1990) and SHERLOCK (Schoenmackers et al., 2010) are typical ILP systems, but the former one usually need a lot of negative facts and the latter one focuses on mining formulas from text.",5 Related Work,[0],[0]
"AMIE (Galárraga et al., 2013) is based on ARM and proposes a new measure for formulas instead of the confidence.",5 Related Work,[0],[0]
"Several structure learning algorithms (Kok and Domingos, 2005; Kok and Domingos, 2009; Kok and Domingos, 2010) based on Markov Logic Network (MLN) (Richardson and Domingos, 2006) can also learn first order logic formulas automatically, but they are too slow to run on large KBs.",5 Related Work,[0],[0]
"ProPPR (Wang et al., 2013; Wang et al., 2014a) performs structure learning by depth first searching on the knowledge graph, which is still not efficient enough to handle webscale KBs.",5 Related Work,[0],[0]
"PRA (Lao and Cohen, 2010; Lao et al., 2011) is a method based on random walks and employs heuristic rules to direct random walks.",5 Related Work,[0],[0]
"PRA is closely related to our approach, but unlike it, our approach dynamically calculates state transition prob-
abilities.",5 Related Work,[0],[0]
"Another method based on random walks (Wei et al., 2015) merges embedding similarities of candidates into the random walk as a priori, while our approach employs KB embeddings to calculate potentials for neighbors.
",5 Related Work,[0],[0]
"The majority of mining formula methods can perform inference on KBs, and besides them, a dozen methods based KB embeddings can also achieve the inference goal, and the typical ones of them are TransE (Bordes et al., 2013), Rescal (Nickel et al., 2011), TransH (Wang et al., 2014b), TransR",5 Related Work,[0],[0]
"(Lin et al., 2015b).",5 Related Work,[0],[0]
These embedding-based methods take advantage of the implicit relationship between elements of the KB and perform inference by calculating similarities.,5 Related Work,[0],[0]
"There are also methods which combine inference formulas and KB embeddings, such as PTransE (Lin et al., 2015a) and ProPPR+MF (Wang and Cohen, 2016).",5 Related Work,[0],[0]
"In this paper, we introduce a goal-directed random walk algorithm to increase efficiency of mining useful formulas and decrease noise simultaneously.",6 Conclusion and Future Works,[0],[0]
The approach employs the inference target as the direction at each steps in the random walk process and is more inclined to visit structures helpful to inference.,6 Conclusion and Future Works,[0],[0]
"In empirical studies, we show our approach achieves good performances on link prediction task over large-scale KBs.",6 Conclusion and Future Works,[0],[0]
"In the future, we are interested in exploring mining formulas directly in the distributional spaces which may resolve the sparsity of formulas.",6 Conclusion and Future Works,[0],[0]
"This work was supported by the Natural Science Foundation of China (No. 61533018), the National Basic Research Program of China (No. 2014CB340503) and the National Natural Science Foundation of China (No. 61272332).",7 Acknowledgments,[0],[0]
And this work was also supported by Google through focused research awards program.,7 Acknowledgments,[0],[0]
"Deep inference on a large-scale knowledge base (KB) needs a mass of formulas, but it is almost impossible to create all formulas manually.",abstractText,[0],[0]
"Data-driven methods have been proposed to mine formulas from KBs automatically, where random sampling and approximate calculation are common techniques to handle big data.",abstractText,[0],[0]
"Among a series of methods, Random Walk is believed to be suitable for knowledge graph data.",abstractText,[0],[0]
"However, a pure random walk without goals still has a poor efficiency of mining useful formulas, and even introduces lots of noise which may mislead inference.",abstractText,[0],[0]
"Although several heuristic rules have been proposed to direct random walks, they do not work well due to the diversity of formulas.",abstractText,[0],[0]
"To this end, we propose a novel goaldirected inference formula mining algorithm, which directs random walks by the specific inference target at each step.",abstractText,[0],[0]
"The algorithm is more inclined to visit benefic structures to infer the target, so it can increase efficiency of random walks and avoid noise simultaneously.",abstractText,[0],[0]
The experiments on both WordNet and Freebase prove that our approach is has a high efficiency and performs best on the task.,abstractText,[0],[0]
Mining Inference Formulas by Goal-Directed Random Walks,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 22–31, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
Physically situated dialogue differs from traditional human-computer dialogue in that interactions will make use of reference to a dialogue agent’s surroundings.,1 Introduction,[0],[0]
"Tasks may fail due to dependencies on specific environment configurations, such as when a robot’s path to a goal is blocked.",1 Introduction,[0],[0]
"People will often help; in navigation dialogues they tend to ask proactive, task-related questions instead of simply signaling communication failure (Skantze, 2005).",1 Introduction,[0],[0]
They supplement the agent’s representation of the environment and allow it to complete tasks.,1 Introduction,[0],[0]
The current study establishes an empirical basis for grounding in physically situated contexts.,1 Introduction,[0],[0]
"We had people provide recovery strategies for a robot in various situations.
",1 Introduction,[0],[0]
"The focus of this work is on recovery from situated grounding problems, a type of miscommunication that occurs when an agent fails to uniquely map a person’s instructions to its surroundings (Marge and Rudnicky, 2013).",1 Introduction,[0],[0]
"A referential ambiguity is where an instruction resolves to more than one possibility (e.g., “Search the room on the left” when there are multiple rooms on the agent’s left); an impossible-to-execute problem
fails to resolve to any action (e.g., same instruction but there are no rooms on the agent’s left).",1 Introduction,[0],[0]
"A common strategy evidenced in human-human corpora is for people to ask questions to recover from situated grounding problems (Tenbrink et al., 2010).
",1 Introduction,[0],[0]
"Dialogue divides into two levels: that of managing the actual dialogue—determining who has the floor, that an utterance was recognized, etc.—and",1 Introduction,[0],[0]
"the dialogue that serves the main joint activities that dialogue partners are carrying out, like a human-robot team exploring a new area (Bangerter and Clark, 2003).",1 Introduction,[0],[0]
"Most approaches to grounding in dialogue systems are managing the dialogue itself, making use of spoken language input as an indicator of understanding (e.g., (Bohus, 2007; Skantze, 2007)).",1 Introduction,[0],[0]
Situated grounding problems are associated with the main joint activities; to resolve them we believe that the recovery model must be extended to include planning and environment information.,1 Introduction,[0],[0]
"Flexible recovery strategies make this possible by enabling dialogue partners to coordinate their joint activities and accomplish tasks.
",1 Introduction,[0],[0]
We cast the problem space as one where the agent aims to select the most efficient recovery strategy that would resolve a user’s intended referent.,1 Introduction,[0],[0]
We expect that this efficiency is tied to the cognitive load it takes to produce clarifications.,1 Introduction,[0.9588029889820752],['We believe it is possible to extend our results to a setting where zero-mean noise is added to the function and its approximations.']
Viethen and Dale (2006) suggest a similar prediction in their study comparing human and automatically generated referring expressions of objects and their properties.,1 Introduction,[0],[0]
"We sought to answer the following questions in this work: • How good are people at detecting situated
grounding problems?",1 Introduction,[0],[0]
• How do people organize recovery strategies?,1 Introduction,[0],[0]
"• When resolving ambiguity, which properties do
people use to differentiate referents?",1 Introduction,[0],[0]
"• When resolving impossible-to-execute instruc-
tions, do people use active or passive ways to get the conversation back on track?
",1 Introduction,[0],[0]
"22
We determined the most common recovery strategies for referential ambiguity and impossible-toexecute problems.",1 Introduction,[0],[0]
Several patterns emerged that suggest ways that people expect agents to recover.,1 Introduction,[0],[0]
Ultimately we intend for dialogue systems to use such strategies in physically situated contexts.,1 Introduction,[0],[0]
Researchers have long observed miscommunication and recovery in human-human dialogue corpora.,2 Related Work,[0],[0]
"The HCRC MapTask had a direction giverdirection follower pair navigate two dimensional schematics with slightly different maps (Anderson et al., 1991).",2 Related Work,[0],[0]
Carletta (1992) proposed several recovery strategies following an analysis of this corpus.,2 Related Work,[0],[0]
"The SCARE corpus collected human-human dialogues in a similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008).
",2 Related Work,[0],[0]
"The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011).",2 Related Work,[0],[0]
Others have also developed recovery strategies for situated dialogue.,2 Related Work,[0],[0]
Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the grounding process.,2 Related Work,[0],[0]
"A similar study focused on resolving misunderstandings in the humanrobot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009).",2 Related Work,[0],[0]
"A body of work on referring expression generation uses object attributes to generate descriptions of referents (e.g., (Guhe and Bard, 2008; Garoufi and Koller, 2014)).",2 Related Work,[0],[0]
"Viethen and Dale (2006) compared human-authored referring expressions of objects to existing natural language generation algorithms and found them to have very different content.
",2 Related Work,[0],[0]
Crowdsourcing has been shown to provide useful dialogue data:,2 Related Work,[0],[0]
Manuvinakurike and DeVault (2015) used the technique to collect gameplaying conversations.,2 Related Work,[0],[0]
"Wang et al. (2012) and Mitchell et al. (2014) have used crowdsourced data for training, while others have used it in real time systems (Lasecki et al., 2013; Huang et al., 2014).",2 Related Work,[0],[0]
"In this study, participants came up with phrases that a search-and-rescue robot should say in response to an operator’s command.",3 Method,[0],[0]
"The participant’s task was to view scenes in a virtual envi-
ronment then formulate the robot’s response to an operator’s request.",3 Method,[0],[0]
"Participants listened to an operator’s verbal command then typed in a response.
",3 Method,[0],[0]
"Scenes displayed one of three situations: referential ambiguity (more than one possible action), impossible-to-execute (zero possible actions), and executable (one possible action).",3 Method,[0],[0]
The instructions showed some example problems.,3 Method,[0],[0]
All situations involved one operator and one robot.,3 Method,[0],[0]
"After instructions and a practice trial, participants viewed scenes in one of 10 different environments (see Figure 1).",3.1 Experiment Design,[0],[0]
"They would first watch a flyover video of the robot’s environment, then view a screen showing labels for all possible referable objects in the scene.",3.1 Experiment Design,[0],[0]
The participant would then watch the robot enter the first scene.,3.1 Experiment Design,[0],[0]
"The practice trial and instructions did not provide any examples of questions.
",3.1 Experiment Design,[0],[0]
The robot would stop and a spoken instruction from the operator would be heard.,3.1 Experiment Design,[0],[0]
The participant was free to replay the instruction multiple times.,3.1 Experiment Design,[0],[0]
They would then enter a response (say an acknowledgment or a question).,3.1 Experiment Design,[0],[0]
"Upon completion of the trial, the robot would move to a different scene, where the process was repeated.
",3.1 Experiment Design,[0],[0]
Only self-contained questions that would allow the operator to answer without follow-up were allowed.,3.1 Experiment Design,[0],[0]
Thus generic questions like “which one?” would not allow the operator to give the robot enough useful information to proceed.,3.1 Experiment Design,[0],[0]
"In the instructions, we suggested that participants include some detail about the environment in their ques-
tions.",3.1 Experiment Design,[0],[0]
Participants used a web form1 to view situations and provide responses.,3.1 Experiment Design,[0],[0]
"We recorded demographic information (gender, age, native language, native country) and time on task.",3.1 Experiment Design,[0],[0]
"The instructions had several attention checks (Paolacci et al., 2010) to ensure that participants were focusing on the task.
",3.1 Experiment Design,[0],[0]
We created fifty trials across ten environments.,3.1 Experiment Design,[0],[0]
Each environment had five trials that represented waypoints the robot was to reach.,3.1 Experiment Design,[0],[0]
Participants viewed five different environments (totaling twenty-five trials).,3.1 Experiment Design,[0],[0]
Each command from the remote operator to the robot was a route instruction in the robot navigation domain.,3.1 Experiment Design,[0],[0]
Trials were assembled in two groups and participants were assigned randomly to one (see Table 1).,3.1 Experiment Design,[0],[0]
Trial order was randomized according to a Latin Square.,3.1 Experiment Design,[0],[0]
"Scenes were of a 3D virtual environment at eye level, with the camera one to two meters behind the robot.",3.1.1 Scenes and Environments,[0],[0]
"Camera angle issues with environment objects caused this variation.
",3.1.1 Scenes and Environments,[0],[0]
Participants understood that the fictional operator was not co-located with the robot.,3.1.1 Scenes and Environments,[0],[0]
The USARSim robot simulation toolkit and the UnrealEd game map editor were used to create the environment.,3.1.1 Scenes and Environments,[0],[0]
"Cepstral’s SwiftTalker was used for the operator voice.
",3.1.1 Scenes and Environments,[0],[0]
"Of the fifty scenes, twenty-five (50%) had referential ambiguities, fifteen (30%) were impossible-to-execute, and ten (20%) were executable controls.",3.1.1 Scenes and Environments,[0],[0]
"The selection was weighted to referential ambiguity, as these were expected to produce greater variety in recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"We randomly assigned each of fifty trials a stimulus type according to this distribution, then divided the list into ten environments.",3.1.1 Scenes and Environments,[0],[0]
"The environments featured objects and doorways appropriate to the trial type, as well as waypoints.
1See http://goo.gl/forms/ZGpK3L1nPh for an example.
",3.1.1 Scenes and Environments,[0],[0]
"Referential Ambiguity We arranged the sources of information participants could use to describe referents, to enable analysis of the relationship between context and recovery strategies.",3.1.1 Scenes and Environments,[0],[0]
"The sources of information (i.e., “situated dimensions”) were: (1) intrinsic properties (either color or size), (2) history (objects that the robot already encountered), (3) egocentric proximity of the robot to candidate referents around it (the robot’s perspective is always taken), and (4) object proximity (proximity of candidate referents to other objects).",3.1.1 Scenes and Environments,[0],[0]
"Table 2 provides additional details.
",3.1.1 Scenes and Environments,[0],[0]
Scenes with referential ambiguity had up to four sources of information available.,3.1.1 Scenes and Environments,[0],[0]
"Information sources were evenly distributed across five trial types: one that included all four sources, and four that included all but one source of information (e.g., one division excluded using history information but did allow proximity, spatial, and object properties, one excluded proximity, etc.).
",3.1.1 Scenes and Environments,[0],[0]
Impossible-to-Execute The impossible-to-execute trials divided into two broad types.,3.1.1 Scenes and Environments,[0],[0]
Nine of the fifteen scenes were impossible because the operator’s command did not match to any referent in the environment.,3.1.1 Scenes and Environments,[0],[0]
"The other six scenes were impossible because a path to get to the matching referent was not possible.
",3.1.1 Scenes and Environments,[0],[0]
Executable Ten scenes were executable for the study and served as controls.,3.1.1 Scenes and Environments,[0],[0]
"The operator’s command mentioned existing, unambiguous referents.",3.1.1 Scenes and Environments,[0],[0]
Participants were aware of the robot’s capabilities before the start of the experiment.,3.1.2 Robot Capabilities,[0],[0]
The instructions said that the robot knew the locations of all objects in the environment and whether doors were closed or open.,3.1.2 Robot Capabilities,[0],[0]
"The robot also knew the color and size of objects in the environment (intrinsic properties), where objects were relative to the robot itself and to other objects (proximity), when objects were right, left, in front, and behind it (spatial terms), the room and hallway locations of objects (location), and the places it has been (history, the robot kept track of which objects it had visited).",3.1.2 Robot Capabilities,[0],[0]
The robot could not pass through closed doors.,3.1.2 Robot Capabilities,[0],[0]
"We made five hypotheses about the organization and content of participant responses to situated grounding problems:
• Hypothesis 1: Participants will have more difficulty detecting impossible-to-execute scenes than ambiguous ones.",3.2 Hypotheses,[0],[0]
"Determining a robot’s tasks to be impossible requires good situation awareness (Nielsen et al., 2007) (i.e., an understanding of surroundings with respect to correctly completing tasks).",3.2 Hypotheses,[0],[0]
"Detecting referential ambiguity requires understanding the operator’s command and visually inspecting the space (Spivey et al., 2002); detecting impossible commands also requires recalling the robot’s capabilities and noticing obstacles.",3.2 Hypotheses,[0],[0]
"Previous research has noted that remote teleoperators have trouble establishing good situation awareness of a robot’s surroundings (Casper and Murphy, 2003; Burke et al., 2004).",3.2 Hypotheses,[0],[0]
"Moreover, obstacles near a robot can be difficult to detect with a restricted view as in the current study (Alfano and Michel, 1990; Arthur, 2000).",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypotheses 2a and 2b: Responses will more commonly be single, self-contained questions instead of a scene description followed by a question (2a for scenes with referential ambiguity, 2b for scenes that were impossible-toexecute).",3.2 Hypotheses,[0],[0]
"This should reflect the principle of least effort (Clark, 1996), and follow from Carletta’s (1992) observations in a similar dataset.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
Hypothesis 3: Responses will use the situated dimensions that require the least cognitive effort when disambiguating referents.,3.2 Hypotheses,[0],[0]
Viethen and Dale (2006) suggest that minimizing cognitive load for the speaker or listener would produce more human-like referring expressions.,3.2 Hypotheses,[0],[0]
"We predict that responses will mention visually salient features of the scene, such as color or size of referents, more than history or object proximity.",3.2 Hypotheses,[0],[0]
"Desimone and Duncan (1995) found that color and shape draw more attention than other
properties in visual search tasks when they are highly distinguishable.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypothesis 4: In cases of referential ambiguity where two candidate referents are present, responses will confirm one referent in the form of a yes-no question more than presenting a list.",3.2 Hypotheses,[0],[0]
"Results from an analysis of task-oriented dialogue suggests that people are efficient when asking clarification questions (Rieser and Moore, 2005).",3.2 Hypotheses,[0],[0]
"Additionally, Clark’s least effort principle (Clark, 1996) suggests that clarifying one referent using a yes-no confirmation would require less effort than presenting a list in two ways: producing a shorter question and constraining the range of responses to expect.",3.2 Hypotheses,[0],[0]
•,3.2 Hypotheses,[0],[0]
"Hypothesis 5: For impossible-to-execute instructions, responses will most commonly be ways for the robot to proactively work with the operator’s instruction, in an effort to get the conversation back on track.",3.2 Hypotheses,[0],[0]
"The other possible technique, to simply declare that the problem is not possible, will be less common.",3.2 Hypotheses,[0],[0]
This is because participants will believe such a strategy will not align with the task goal of having the robot say something that will allow it to proceed with the task.,3.2 Hypotheses,[0],[0]
"Skantze found that in human-human navigation dialogues, people would prefer to look for alternative ways to proceed rather than simply express nonunderstanding (Skantze, 2005).",3.2 Hypotheses,[0],[0]
"The key independent variable in this study was the stimulus type that the participant viewed (i.e., referential ambiguity, impossible-to-execute, or executable).",3.3 Measures,[0],[0]
"Dependent variables were observational measurements, presented below.",3.3 Measures,[0],[0]
"We report Fleiss’ kappa score for inter-annotator agreement
between three native English speaking annotators on a subset of the data.
",3.3 Measures,[0],[0]
Correctness (κ = 0.77):,3.3 Measures,[0],[0]
"Whether participants correctly determined the situation as ambiguous, impossible, or executable.",3.3 Measures,[0],[0]
Annotators labeled correctness based on the content of participant responses.,3.3 Measures,[0],[0]
This measure assessed participant accuracy for detecting situated grounding problems.,3.3 Measures,[0],[0]
"Either correct or incorrect.
",3.3 Measures,[0],[0]
"Sentence type (κ = 0.82): Either declarative, interrogative, imperative, or exclamatory (Cowan, 2008).
",3.3 Measures,[0],[0]
Question type (κ = 0.92): Sentences that needed an answer from the operator.,3.3 Measures,[0],[0]
"The three types were yes-no questions, alternative questions (which presented a list of options and includes wh- questions that used sources from Table 2), and generic wh- questions (Cowan, 2008).
",3.3 Measures,[0],[0]
Situated dimensions in response (κ = 0.75):,3.3 Measures,[0],[0]
The capability (or capabilities) that the participant mentioned when providing a response.,3.3 Measures,[0],[0]
"The types were intrinsic (color or size), object proximity, egocentric proximity, and history.
",3.3 Measures,[0],[0]
"Projected belief (impossible-to-execute trials only, κ = 0.80):",3.3 Measures,[0],[0]
"The participant’s belief about the next task, given the current operator instruction (projected onto the robot).",3.3 Measures,[0],[0]
"The types were unknown (response indicates participant is unsure what to do next), ask for more (ask for more details), propose alternative (propose alternative object), ask for help (ask operator to physically manipulate environment), and off topic.",3.3 Measures,[0],[0]
We recruited 30 participants.,3.4 Participation,[0],[0]
"All participants completed the web form through the Amazon Mechanical Turk (MTurk) web portal2, all were located in the United States and had a task approval rate ≥95%.",3.4 Participation,[0],[0]
The group included 29 self-reported native English speakers born in the United States; 1 self-reported as a native Bangla speaker born in Bangladesh.,3.4 Participation,[0],[0]
The gender distribution was 15 male to 15 female.,3.4 Participation,[0],[0]
"Participants ranged in age from 22 to 52 (mean: 33 years, std. dev.: 7.7).",3.4 Participation,[0],[0]
They were paid between $1 and $2 for their participation.,3.4 Participation,[0],[0]
"We
2https://www.mturk.com
collected a total of 750 responses.",3.4 Participation,[0],[0]
We analyzed the measures by tabulating frequencies for each possible value.,4 Results,[0],[0]
Table 3 presents some example responses.,4 Results,[0],[0]
"In general, participants were good at detecting situated grounding problems.",4.1 Correctness,[0],[0]
"Out of 750 responses, 667 (89%) implied the correct scene type.",4.1 Correctness,[0],[0]
"We analyzed correctness across actual stimulus types (ambiguous, impossible-to-execute, executable) using a mixed-effects analysis of variance model3, with participant included as a random effect and trial group as a fixed effect.
",4.1 Correctness,[0],[0]
Hypothesis 1 predicted that participants will do better detecting scenes with referential ambiguity than those that were impossible-to-execute; the results support this hypothesis.,4.1 Correctness,[0],[0]
"Actual stimulus type had a significant main effect on correctness (F[2, 58] = 12.3, p < 0.001); trial group did not (F[1, 28] = 0.1, p = 0.72).",4.1 Correctness,[0],[0]
Participants had significantly worse performance detecting impossible-to-execute scenes compared to ambiguous ones (p< 0.001; Tukey HSD test).,4.1 Correctness,[0],[0]
"In fact, they were four times worse; of the impossible-toexecute scenes, participants failed to detect that 22% (50/225) of them were impossible, compared to 5% (17/375) of scenes with referential ambiguity.",4.1 Correctness,[0],[0]
"Of the 150 instructions that were executable, participants failed to detect 11% (16/150) of them as such.",4.1 Correctness,[0],[0]
"We analyzed the 358 responses where participants correctly detected referential ambiguity.
",4.2 Referential Ambiguity,[0],[0]
"3This approach computed standard least squares regression using reduced maximum likelihood (Harville, 1977).
",4.2 Referential Ambiguity,[0],[0]
"Hypothesis 2a predicted that participants would more commonly ask single, self-contained questions instead of describing the scene and asking a question.",4.2 Referential Ambiguity,[0],[0]
We assessed this by counting sentence types within a response.,4.2 Referential Ambiguity,[0],[0]
Responses that had both a declarative sentence and an interrogative would fit this case.,4.2 Referential Ambiguity,[0],[0]
The results confirmed this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"Only 4.5% (16/358) of possible responses had a declarative and an interrogative.
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 3 predicted that participants would use the situated dimensions that require the least cognitive effort when disambiguating referents.,4.2 Referential Ambiguity,[0],[0]
"More specifically, the most common mentions will be those that are visually apparent (intrinsic properties like color and size), while those that require more processing would have fewer mentions (history and to a lesser extent object proximity and egocentric proximity).",4.2 Referential Ambiguity,[0],[0]
"We measured this by tabulating mentions of situated dimensions in all 358 correct participant responses, summarized in Figure 2.",4.2 Referential Ambiguity,[0],[0]
Multiple dimensions could occur in a single response.,4.2 Referential Ambiguity,[0],[0]
The results support this hypothesis.,4.2 Referential Ambiguity,[0],[0]
"By far, across all ambiguous scenarios, the most mentioned dimension was an intrinsic property.",4.2 Referential Ambiguity,[0],[0]
"More than half of all situated dimensions used were intrinsic (59%, 242/410 total mentions).",4.2 Referential Ambiguity,[0],[0]
"This was followed by the dimensions that we hypothesize require more cognitive effort: egocentric proximity had 30% (125/410) of mentions, object proximity 9.5% (39/410), and history 1% (4/410).",4.2 Referential Ambiguity,[0],[0]
"Of the intrinsic dimensions mentioned, most were only color (61%, 148/242), followed by size (33%, 81/242), and using both (5%, 13/242).
",4.2 Referential Ambiguity,[0],[0]
Hypothesis 4 predicted that participants would ask yes-no confirmation questions in favor of presenting lists when disambiguating a referent with exactly two candidates.,4.2 Referential Ambiguity,[0],[0]
"The results suggest that the opposite is true; people strongly preferred to
list options, even when a confirmation question about one would have been sufficient.",4.2 Referential Ambiguity,[0],[0]
"Of the 285 responses that were correctly detected as ambiguous and were for scenes of exactly two possible referents, 74% (212/285) presented a list of options.",4.2 Referential Ambiguity,[0],[0]
Only 14% (39/285) asked yes-no confirmation questions.,4.2 Referential Ambiguity,[0],[0]
The remaining 34 questions (12%) were generic wh-questions.,4.2 Referential Ambiguity,[0],[0]
These results held in scenes where three options were present.,4.2 Referential Ambiguity,[0],[0]
"Overall 72% (259/358) presented a list of options, while 16% (58/358) asked generic wh-questions and 11% (41/358) asked yes-no confirmations.",4.2 Referential Ambiguity,[0],[0]
"We analyzed the 175 responses where participants correctly identified impossible-to-execute situations.
",4.3 Impossible-to-Execute,[0],[0]
Hypothesis 2b predicted that participants would more often only ask a question than also describe the scene.,4.3 Impossible-to-Execute,[0],[0]
Results confirmed this hypothesis.,4.3 Impossible-to-Execute,[0],[0]
"42% (73/175) of responses simply asked a question, while 22% (39/175) used only a declarative.",4.3 Impossible-to-Execute,[0],[0]
"More than a third included a declarative as well (36%, 63/175).",4.3 Impossible-to-Execute,[0],[0]
"The general organization to these was to declare the problem then ask a question about it (89%, 56/63).
",4.3 Impossible-to-Execute,[0],[0]
"Hypothesis 5 predicted that responses for impossible-to-execute instructions will more commonly be proactive and make suggestions, instead of simply declaring that an action was not possible.",4.3 Impossible-to-Execute,[0],[0]
"Table 4 summarizes the results, which confirmed this hypothesis.",4.3 Impossible-to-Execute,[0],[0]
The most common belief that participants had for the robot was to have it propose an alternative referent to the impossible one specified by the operator.,4.3 Impossible-to-Execute,[0],[0]
The next-most common was to have the robot simply express uncertainty about what to do next.,4.3 Impossible-to-Execute,[0],[0]
"Though this belief occurred in about a third of responses, the remaining responses were all proactive ways for the robot to get the conversation back on track (i.e., propose alternative, ask for more, and ask for help).",4.3 Impossible-to-Execute,[0],[0]
"The results largely support the hypotheses, with the exception of Hypothesis 4.",5 Discussion,[0],[0]
"They also provide information about how people expect robots to recover from situated grounding problems.
",5 Discussion,[0],[0]
"Correctness Participants had the most trouble detecting impossible-to-execute scenes, supporting Hypothesis 1.",5 Discussion,[0],[0]
"An error analysis of the 50 responses for this condition had participants responding as if the impossible scenes were possible (62%, 31/50).",5 Discussion,[0],[0]
"The lack of good situation awareness was a factor, which agrees with previous findings in the human-robot interaction literature (Casper and Murphy, 2003; Burke et al., 2004).",5 Discussion,[0],[0]
We found that participants had trouble with a specific scene where they confused the front and back of the robot (9 of the 31 impossibleexecutable responses were for this scene).,5 Discussion,[0],[0]
"Note that all scenes showed the robot entering the room with the same perspective, facing forward.
",5 Discussion,[0],[0]
"Referential Ambiguity Results for Hypothesis 2a showed that participants overwhelmingly asked only a single, self-contained question as opposed to first stating that there was an ambiguity.",5 Discussion,[0],[0]
"Participants also preferred to present a list of options, despite the number of possible candidates.",5 Discussion,[0],[0]
"This contradicted Hypothesis 4. Rieser and Moore (2005) found that in task-oriented human-human dialogues, clarification requests aim to be as efficient as possible; they are mostly partially formed.",5 Discussion,[0],[0]
The results in our study were not of real-time dialogue; we isolated specific parts of what participants believed to be human-computer dialogue.,5 Discussion,[0],[0]
"Moreover, Rieser and Moore were observing clarifications at Bangerter and Clark’s (2003) dialogue management level; we were observing them in service of the joint activity of navigating the robot.",5 Discussion,[0],[0]
"We believe that this difference resulted in participants using caution by disambiguating with lists.
",5 Discussion,[0],[0]
"These results suggest that dialogue systems should present detection of referential ambiguity implicitly, and as a list.",5 Discussion,[0],[0]
"Generic wh- questions (e.g., “which one?” without presenting a followon list) are less desirable because they don’t constrain what the user can say, and don’t provide any indication of what the dialogue system can understand.",5 Discussion,[0],[0]
"A list offers several benefits: it grounds awareness of surroundings, presents a fixed set of options to the user, and constrains the range of
linguistic responses.",5 Discussion,[0],[0]
"This could also extend to general ambiguity, as in when there are a list of matches to a query, but that is outside the scope of this work.",5 Discussion,[0],[0]
"Lists may be less useful as they grow in size; in our study they could not grow beyond three candidates.
",5 Discussion,[0],[0]
The data also supported Hypothesis 3.,5 Discussion,[0],[0]
Participants generally preferred to use situated dimensions that required less effort to describe.,5 Discussion,[0],[0]
"Intrinsic dimensions (color and size) had the greatest count, followed by egocentric proximity, object proximity, and finally using history.",5 Discussion,[0],[0]
"We attribute these results to the salient nature of intrinsic properties compared to ones that must be computed (i.e., egocentric and object proximity require spatial processing, while history requires thinking about previous exchanges).",5 Discussion,[0],[0]
This also speaks to a similar claim by Viethen and Dale (2006).,5 Discussion,[0],[0]
"Responses included color more than any other property, suggesting that an object’s color draws more visual attention than its size.",5 Discussion,[0],[0]
"Bright colors and big shapes stand out most in visual search tasks; we had more of the former than the latter (Desimone and Duncan, 1995).
",5 Discussion,[0],[0]
"For an ambiguous scene, participants appear to traverse a salience hierarchy (Hirst et al., 1994) whereby they select the most visually salient feature that also uniquely teases apart candidates.",5 Discussion,[0],[0]
"While the salience hierarchy varies depending on the current context of a referent, we anticipate such a hierarchy can be defined computationally.",5 Discussion,[0],[0]
"Others have proposed similar processes for referring expression generation (Van Der Sluis, 2005; Guhe and Bard, 2008).",5 Discussion,[0],[0]
One way to rank salience on the hierarchy could be predicted mental load; we speculate that this is a reason why history was barely mentioned to disambiguate.,5 Discussion,[0],[0]
"Another would be to model visual attention, which could explain why color was so dominant.
",5 Discussion,[0],[0]
"Note that only a few dimensions were “competing” at any given time, and their presence in the scenes was equal (save for history, which had slightly fewer due to task design constraints).",5 Discussion,[0],[0]
"Egocentric proximity, which uses spatial language to orient candidate referents relative to the robot, had a moderate presence.",5 Discussion,[0],[0]
"When intrinsic properties were unavailable in the scene, responses most often used this property.",5 Discussion,[0],[0]
"We found that sometimes participants would derive this property even if it wasn’t made prototypical in the scene (e.g., referring to a table as “left” when it was in front and
off to the left side of the robot).",5 Discussion,[0],[0]
This suggests that using egocentric proximity to disambiguate makes a good fallback strategy when nothing else works.,5 Discussion,[0],[0]
"Another situated dimension emerged from the responses, disambiguation by location (e.g., “Do you mean the box in this room or the other one?”).",5 Discussion,[0],[0]
"Though not frequent, it provides another useful technique to disambiguate when visually salient properties are not available.
",5 Discussion,[0],[0]
"Our findings differ from those of Carlson and Hill (2009) who found that salience is not as prominent as spatial relationships between a target (in the current study, this would be the robot) and other objects.",5 Discussion,[0],[0]
Our study did not direct participants to formulate spatial descriptions; they were free to compose responses.,5 Discussion,[0],[0]
"In addition, our work directly compares intrinsic properties for objects of the same broad type (e.g., disambiguation of a doors of different colors).",5 Discussion,[0],[0]
"Our findings suggest the opposite of Moratz et al. (2003), who found that when pointing out an object, describing its position may be better than describing its attributes in human-robot interactions.",5 Discussion,[0],[0]
"Their study only had one object type (cube) and did not vary color, size, or proximity to nearby objects.",5 Discussion,[0],[0]
"As a result, participants described objects using spatial terms.",5 Discussion,[0],[0]
"In our study, we explored variation of several attributes to determine participants’ preferences.
",5 Discussion,[0],[0]
Impossible-to-Execute Results supported Hypothesis 2b.,5 Discussion,[0],[0]
Most responses had a single sentence type.,5 Discussion,[0],[0]
"Although unanticipated, a useful strategy emerged: describe the problem that makes the scene impossible, then propose an alternative referent.",5 Discussion,[0],[0]
This type of strategy helped support Hypothesis 5.,5 Discussion,[0],[0]
"Responses for impossible scenes largely had the participant proactively presenting a way to move the task forward, similar to what Skantze (2005) observed in human-human dialogues.",5 Discussion,[0],[0]
This suggests that participants believed the robot should ask directed questions to recover.,5 Discussion,[0],[0]
These questions often took the form of posing alternative options.,5 Discussion,[0],[0]
We used the Amazon Mechanical Turk web portal to gather responses in this study.,5.1 Limitations,[0],[0]
"As such we could not control the participant environment when taking the study, but we did include attention checks.",5.1 Limitations,[0],[0]
"Participants did not interact with a
dialogue system.",5.1 Limitations,[0],[0]
Instead we isolated parts of the interaction that were instances of where the robot would have to say something in response to an instruction.,5.1 Limitations,[0],[0]
We asked participants to provide what they think the robot should say; there was no ongoing interaction.,5.1 Limitations,[0],[0]
"However, we maintained continuity by presenting videos of the robot navigating through the environment as participants completed the task.",5.1 Limitations,[0],[0]
"The robot was represented in a virtual environment, which prevents us from understanding if there are any influencing factors that may impact results if the robot were in physical form or co-present with the participant.",5.1 Limitations,[0],[0]
Recovery strategies allow situated agents like robots to recover from misunderstandings by using the human dialogue partner.,6 Conclusions,[0],[0]
We conducted a study that collected recovery strategies for physically situated dialogue with the goal of establishing an empirical basis for grounding in physically situated contexts.,6 Conclusions,[0],[0]
"We crowdsourced 750 written strategies across 30 participants and analyzed their situated properties and how they were organized.
",6 Conclusions,[0],[0]
We found that participants’ recovery strategies minimize cognitive effort and indicate a desire to successfully complete the task.,6 Conclusions,[0],[0]
"For disambiguation, there was a preference for strategies that use visually salient properties over ones that require additional mental processing, like spatial reasoning or memory recall.",6 Conclusions,[0],[0]
"For impossible-to-execute scenes, responses more often presented alternative referents than just noting non-understanding.",6 Conclusions,[0],[0]
"We should note that some differences between our findings and those of others may in part rest on differences in task and environment, though intrinsic variables such as mental effort will likely persist over different situations.
",6 Conclusions,[0],[0]
"In future work, we intend to use these data to model salience ranking in similar contexts.",6 Conclusions,[0],[0]
We will further assess the hypothesis that participants’ preferences in this study will enhance performance in a spoken dialogue system that deploys similar strategies.,6 Conclusions,[0],[0]
The authors thank Prasanna Kumar Muthukumar and Juneki Hong for helping to annotate recovery strategies.,Acknowledgments,[0],[0]
"We also thank Taylor Cassidy, Arthur William Evans, and the anonymous reviewers for their valuable comments.",Acknowledgments,[0],[0]
We describe an empirical study that crowdsourced human-authored recovery strategies for various problems encountered in physically situated dialogue.,abstractText,[0],[0]
The purpose was to investigate the strategies that people use in response to requests that are referentially ambiguous or impossible to execute.,abstractText,[0],[0]
"Results suggest a general preference for including specific kinds of visual information when disambiguating referents, and for volunteering alternative plans when the original instruction was not possible to carry out.",abstractText,[0],[0]
Miscommunication Recovery in Physically Situated Dialogue,title,[0],[0]
Feature selection is an important step in extracting interpretable patterns from data.,1. Introduction,[0],[0]
"It has numerous applications in a wide range of areas, including natural-language processing, genomics, and chemistry.",1. Introduction,[0],[0]
"Suppose that there are n or-
*These authors contributed equally and are listed alphabetically 1Department of Electrical Engineering, Stanford University, Stanford, California 2Department of Computer Science, Rice University, Houston, Texas 3Department of Electrical and Computer Engineering, Rice University, Houston, Texas.",1. Introduction,[0],[0]
"Correspondence to: Anshumali Shrivastava <anshumali@rice.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"dered pairs (Xi, yi)i∈[n], where Xi ∈ Rp are p-dimensional feature vectors, and yi ∈ R are scalar outputs.",1. Introduction,[0],[0]
"Feature selection aims to identify a small subset of features (coordinates of the p-dimensional feature vector) that best models the relationship between the data Xi and the output yi.
",1. Introduction,[0],[0]
A significant complication that is common in modern engineering and scientific applications is that the feature space p is ultra high-dimensional.,1. Introduction,[0],[0]
"For example, Weinberger introduced a dataset with 16 trillion (p = 1013) unique features (Weinberger et al., 2009).",1. Introduction,[0],[0]
A 16 trillion dimensional feature vector (of double 8 bytes) requires 128 terabytes of working memory.,1. Introduction,[0],[0]
Problems from modern genetics are even more challenging.,1. Introduction,[0],[0]
A particularly useful way to represent a long DNA sequence is by a feature vector that counts the occurrence frequency of all length-K sub-strings called K-mers.,1. Introduction,[0],[0]
"This representation plays an important role in large-scale regression problems in computational biology (Wood & Salzberg, 2014; Bray et al., 2015; Vervier et al., 2016; Aghazadeh et al., 2016).",1. Introduction,[0],[0]
"Typically, K is chosen to be larger than 12, and these strings are composed of all possible combinations of 16 characters ({A,T,C,G} in addition to 12 wild card characters).",1. Introduction,[0],[0]
"In this case, the feature vector dimension is p = 1612 = 248.",1. Introduction,[0],[0]
"A vector of size 248 single-precision variables requires approximately 1 petabyte of space!
",1. Introduction,[0],[0]
"For ultra large-scale feature selection problems, it is impossible to run standard explicit regularization-based methods like `1-regularization (Shalev-Shwartz & Tewari, 2011; Tan et al., 2014) or to select hyperparameters with a constrained amount of memory (Langford et al., 2009).",1. Introduction,[0],[0]
"This is not surprising, because these methods are not scalable in terms of memory and computational time (Duchi et al., 2008).",1. Introduction,[0],[0]
Another important operational concern is that most datasets represent features in the form of strings or tokens.,1. Introduction,[0],[0]
"For example, with DNA or n-gram datasets, features are represented by strings of characters.",1. Introduction,[0],[0]
"Even in click-through data (McMahan et al., 2013), features are indexed by textual tokens.",1. Introduction,[0],[0]
Observe that mapping each of these strings to a vector component requires maintaining a dictionary whose size equals the length of the feature vector.,1. Introduction,[0],[0]
"As a result, one does not even have the capability to create a numerical exact vector representation of the features.
",1. Introduction,[0],[0]
"Typically, when faced with such large machine learning tasks, the practitioner chooses to do feature hashing (Weinberger et al., 2009).",1. Introduction,[0],[0]
Consider a 3-gram string “abc”.,1. Introduction,[0],[0]
"With feature hashing, one uses a lossy, random hash function h : strings → {0, 1, 2, . . .",1. Introduction,[0],[0]
", R} to map “abc” to a feature number h(abc) in the range {0, 1, 2, . .",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
", R}.",1. Introduction,[0],[0]
This is extremely convenient because it enables one to avoid creating a large look-up dictionary.,1. Introduction,[0],[0]
"Furthermore, this serves as a dimensionality reduction technique, reducing the problem dimension to R. Unfortunately, this convenience comes at a cost.",1. Introduction,[0],[0]
"Given that useful dimensionality reduction is strictly surjective (i.e., R < p), we lose the identity of the original features.",1. Introduction,[0],[0]
"This is not a viable option if one cares about both feature selection and interpretability.
",1. Introduction,[0],[0]
"One reason to remain hopeful is that in such highdimensional problems, the data vectors Xi are extremely sparse (Wood & Salzberg, 2014).",1. Introduction,[0],[0]
"For instance, the DNA sequence of an organism contains only a small fraction (at most the length of the DNA sequence) of p = 1612 features.",1. Introduction,[0],[0]
"The situation is similar whether we are predicting clickthrough rates of users on a website or if we seek n-gram representations of text documents (Mikolov et al., 2013).",1. Introduction,[0],[0]
"In practice, ultra high-dimensional data is almost always ultrasparse.",1. Introduction,[0],[0]
"Thus, loading a sparse data vector into memory is usually not a concern.",1. Introduction,[0],[0]
"The problem arises in the intermediate stages of traditional methods, where dense iterates need to be tracked in the main memory.",1. Introduction,[0],[0]
"One popular approach is to use greedy thresholding methods (Maleki, 2009; Mikolov et al., 2013; Jain et al., 2014; 2017) combined with stochastic gradient descent (SGD) to prevent the feature vector β from becoming too dense and blowing up in memory.",1. Introduction,[0],[0]
"In these methods, the intermediate iterates are regularized at each step, and a full gradient update is never stored nor computed (since this is memory and computation intensive).",1. Introduction,[0],[0]
"However, it is well known that greedy thresholding can be myopic and can result in poor convergence.",1. Introduction,[0],[0]
We clearly observe this phenomenon in our evaluations.,1. Introduction,[0],[0]
"See Section 5 for details.
",1. Introduction,[0],[0]
"In this paper we tackle the ultra large-scale feature selection problem, i.e., feature selection with billions or more dimensions.",1. Introduction,[0],[0]
"We propose a novel feature selection algorithm called MISSION, a Memory-efficient, Iterative Sketching algorithm for Sparse feature selectION.",1. Introduction,[0],[0]
"MISSION, that takes on all the concerns outlined above.",1. Introduction,[0],[0]
"MISSION matches the accuracy performance of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) (Agarwal et al., 2014) on real-world datasets.",1. Introduction,[0],[0]
"However, in contrast to VW, MISSION can perform feature selection exceptionally well.",1. Introduction,[0],[0]
"Furthermore, MISSION significantly surpasses the performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is currently the popular feature selection alternative concerning the problem sizes we consider.
",1. Introduction,[0],[0]
Contributions:,1. Introduction,[0],[0]
"In this work, we show that the two-decade old Count-Sketch data structure (Charikar et al., 2002) from the streaming algorithms literature is ideally suited for ultra large-scale feature selection.",1. Introduction,[0],[0]
The Count-Sketch data structure enables us to retain the convenience of feature hashing along with the identity of important features.,1. Introduction,[0],[0]
"Moreover, Count-Sketch can accumulate gradients updates over several iterations because of linear aggregation.",1. Introduction,[0],[0]
This aggregation eliminates the problem of myopia associated with existing greedy thresholding approaches.,1. Introduction,[0],[0]
"The aggregation phenomenon also extends to recent parallel works which employ count sketches in streaming domain (Tai et al., 2018).
",1. Introduction,[0],[0]
"In particular, we force the parameters (or feature vector) to reside in a memory-efficient Count-Sketch data structure.",1. Introduction,[0],[0]
SGD gradient updates are easily applied to the Count-Sketch.,1. Introduction,[0],[0]
"Instead of moving in the gradient direction and then greedily projecting into a subspace defined by the regularizer (e.g., in the case of LASSO-based methods), MISSION adds the gradient directly into the Count-Sketch data structure, where it aggregates with all the past updates.",1. Introduction,[0],[0]
See Fig. 1 for the schematic.,1. Introduction,[0],[0]
"At any point of time in the iteration, this data structure stores a compressed, randomized, and noisy sketch of the sum of all the gradient updates, while preserving the information of the heavy-hitters—the coordinates that accumulate the highest amount of energy.",1. Introduction,[0],[0]
"In order to find an estimate of the feature vector, MISSION queries the CountSketch.",1. Introduction,[0],[0]
"The Count-Sketch is used in conjunction with a top-k heap, which explicitly stores the features with the heaviest weights.",1. Introduction,[0],[0]
"Only the features in the top-k heap are considered active, and the rest are set to zero.",1. Introduction,[0],[0]
"However, a representation for every weight is stored, in compressed form, inside the Count-Sketch.
",1. Introduction,[0],[0]
"We demonstrate that MISSION surpasses the sparse recovery performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is the only other method we could run at our scale.",1. Introduction,[0],[0]
"In addition, experiments suggest that the memory requirements of MISSION scale well with the dimensionality p of the problem.",1. Introduction,[0],[0]
"MISSION matches the
accuracy of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) on real-world, large-scale datasets.",1. Introduction,[0],[0]
"Moreover, MISSION achieves comparable or even better accuracy while using significantly fewer features.",1. Introduction,[0],[0]
"In the streaming setting, we are given a high-dimensional vector β ∈",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
Rp that is too costly to store in memory.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
We see only a very long sequence of updates over time.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The only information available at time t is of the form (i,∆), which means that coordinate i is incremented (or decremented) by the amount ∆. We are given a limited amount of storage, on the order of O(log p), which means that we can never store the entire sequence of updates.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Sketching algorithms aim to estimate the value of current item i, after any number of updates using only O(log p) memory.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Accurate estimation of heavy coordinates is desirable.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
Count-Sketch is a popular algorithm for estimation in the streaming setting.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Count-Sketch keeps a matrix of counters (or bins) S of size d×w ∼ O(log p), where d andw are chosen based on the accuracy guarantees.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The algorithm uses d random hash functions hj for j ∈ {1, 2, ..., d} to map the vector’s components to bins w, hj : {1, 2, ..., p} → {1, 2, ..., w} Every component i of the vector is hashed to d different bins.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"In particular, for any row j of sketch S, component i is hashed into bin S(j, hj(i)).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"In addition to hj , Count-Sketch uses d random sign functions to map the components of the vectors randomly to {+1, −1}, i.e., si : {1, 2, ..., D} → {+1,−1}.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"An illustration of this sketch data structure with three hash functions in shown inside Fig. 1.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
The Count-Sketch supports two operations: UPDATE(item,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"i, increment ∆) and QUERY(item i).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
The UPDATE operation updates the sketch with any observed increment.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"More formally, for an increment ∆ to an item i, the sketch is updated by adding sj(i)∆ to the cell S(j, hj(i))",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"∀j ∈ {1, 2, ..., d}.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"The QUERY operation returns an estimate for component i, the median of all the d different associated counters.
",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"It has been shown that, for any sequence of streaming updates (addition or subtraction) to the vector β, Count-Sketch provides an unbiased estimate of any component i, β̂i such that the following holds with high probability,
βi",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
− ||β||2 ≤ β̂i ≤,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
βi + ||β||2.,2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"(1)
It can be shown that the Eq. (1) is sufficient to achieve near-optimal guarantees for sparse recovery with the given space budget.",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Furthermore, these guarantees also meet the best compressed sensing lower bounds in terms of the number of counters (or measurements) needed for sparse recovery (Indyk, 2013).",2. Review: Streaming Setting and the Count-Sketch Algorithm,[0],[0]
"Consider the feature selection problem in the ultra highdimensional setting: We are given the dataset (Xi, yi) for i ∈",3. Problem Formulation,[0],[0]
"[n] = {1, 2, . . .",3. Problem Formulation,[0],[0]
",",3. Problem Formulation,[0],[0]
"n},",3. Problem Formulation,[0],[0]
where Xi ∈,3. Problem Formulation,[0],[0]
Rp and yi ∈ R denote the ith measured and response variables.,3. Problem Formulation,[0],[0]
We are interested in finding the k-sparse (k non-zero entries) feature vector (or regressor),3. Problem Formulation,[0],[0]
β ∈,3. Problem Formulation,[0],[0]
"Rp from the optimization problem
min ‖β‖0=k
‖y −Xβ‖2, (2)
",3. Problem Formulation,[0],[0]
"where X = {X1,X2, . . .",3. Problem Formulation,[0],[0]
",Xn} and y =",3. Problem Formulation,[0],[0]
"[y1, y1, . . .",3. Problem Formulation,[0],[0]
", yn] denote the data matrix and response vector and the `0-norm ‖β‖0 counts the number of non-zero entries in β.
",3. Problem Formulation,[0],[0]
We are interested in solving the feature selection problem for ultra high-dimensional datasets where the number of features p is so large that a dense vector (or matrix) of size p cannot be stored explicitly in memory.,3. Problem Formulation,[0],[0]
"Among the menagerie of feature selection algorithms, the class of hard thresholding algorithms have the smallest memory footprint: Hard thresholding algorithms retain only the top-k values and indices of the entire feature vector using O(klog(p))",3.1. Hard Thresholding Algorithms,[0],[0]
"memory (Jain et al., 2014; Blumensath & Davies, 2009).",3.1. Hard Thresholding Algorithms,[0],[0]
"The iterative hard thresholding (IHT) algorithm generates the following iterates for the ith variable in an stochastic gradient descent (SGD) framework
βt+1",3.1. Hard Thresholding Algorithms,[0],[0]
← Hk(βt − 2λ,3.1. Hard Thresholding Algorithms,[0],[0]
"( yi −Xiβt )T Xi) (3)
The sparsity of the feature vector βt, enforced by the hard thresholding operator Hk, alleviates the need to store a vector of size O(p) in the memory in order to keep track of the changes of the features over the iterates.
",3.1. Hard Thresholding Algorithms,[0],[0]
"Unfortunately, because it only retains the top-k elements of β, the hard thresholding procedure greedily discards the information of the non top-k coordinates from the previous iteration.",3.1. Hard Thresholding Algorithms,[0],[0]
"In particular, it clips off coordinates that might add to the support set in later iterations.",3.1. Hard Thresholding Algorithms,[0],[0]
"This drastically affects the performance of hard thresholding algorithms in realworld scenarios where the design matrix X is not random, normalized, or well-conditioned.",3.1. Hard Thresholding Algorithms,[0],[0]
"In this regime, the gradient terms corresponding to the true support typically arrive in lagging order and are prematurely clipped in early iterations by Hk.",3.1. Hard Thresholding Algorithms,[0],[0]
"The effect of these lagging gradients is present even in the SGD framework, because the gradients are quite noisy, and only a small fraction of the energy of the true gradient is expressed in each iteration.",3.1. Hard Thresholding Algorithms,[0],[0]
"It is not difficult to see that these small energy, high noise signals can easily cause the greedy hard thresholding operator to make sub-optimal or incorrect decisions.",3.1. Hard Thresholding Algorithms,[0],[0]
"Ideally, we want to accumulate the gradients to get enough confidence in signal and to average out any noise.
",3.1. Hard Thresholding Algorithms,[0],[0]
"Algorithm 1 MISSION Initialize: β0 = 0, S (Count-Sketch), λ (Learning Rate) while not stopping criteria do
Find the gradient update gi = λ",3.1. Hard Thresholding Algorithms,[0],[0]
( 2 (yi −Xiβt) T Xi ),3.1. Hard Thresholding Algorithms,[0],[0]
Add the gradient update to the sketch gi → S Get the top-k heavy-hitters from the sketch βt+1,3.1. Hard Thresholding Algorithms,[0],[0]
"← S end while Return: The top-k heavy-hitters from the Count-Sketch
This aforementioned problem is in fact symptomatic of all other thresholding variants including the Iterative algorithm with inversion (ITI) (Maleki, 2009) and the Partial hard thresholding (PHT) algorithm (Jain et al., 2017).",3.1. Hard Thresholding Algorithms,[0],[0]
We now describe the MISSION algorithm.,4. The MISSION Algorithm,[0],[0]
"First, we initialize the Count-Sketch S and the feature vector βt=0 with zeros entries.",4. The MISSION Algorithm,[0],[0]
The Count-Sketch hashes a p-dimensional vector into O(log2p) buckets (Recall Fig. 1).,4. The MISSION Algorithm,[0],[0]
"We discuss this particular choice for the size of the Count-Sketch and the memory-accuracy trade offs of MISSION in Sections 5.3 and 6.1.
",4. The MISSION Algorithm,[0],[0]
"At iteration t, MISSION selects a random row Xi from the data matrix X and computes the stochastic gradient update term using the learning rate λ via gi = 2λ",4. The MISSION Algorithm,[0],[0]
(yi −Xiβt) T Xi i.e. the usual gradient update that minimizes the unconstrained quadratic loss ‖y−Xβ‖22.,4. The MISSION Algorithm,[0],[0]
The data vector Xi and the corresponding stochastic gradient term are sparse.,4. The MISSION Algorithm,[0],[0]
"We then add the non-zero entries of the stochastic gradient term {gij : ∀j gij > 0} to the Count-Sketch S. Next, MISSION queries the top-k values of the sketch to form βt+1.",4. The MISSION Algorithm,[0],[0]
We repeat the same procedure until convergence.,4. The MISSION Algorithm,[0],[0]
MISSION returns the top-k values of the Count-Sketch as the final output of the algorithm.,4. The MISSION Algorithm,[0],[0]
"The MISSION algorithm is detailed in Alg. 1. MISSION easily extends to other loss functions such as the hinge loss and logistic loss.
",4. The MISSION Algorithm,[0],[0]
MISSION is Different from Greedy Thresholding:,4. The MISSION Algorithm,[0],[0]
Denote the gradient vector update at any iteration t as ut.,4. The MISSION Algorithm,[0],[0]
"It is not difficult to see that starting with an all-zero vector β0, at any point of time t, the Count-Sketch state is equivalent to the sketch of the vector ∑t i=1",4. The MISSION Algorithm,[0],[0]
ut.,4. The MISSION Algorithm,[0],[0]
"In other words, the sketch aggregates the compressed aggregated vector.",4. The MISSION Algorithm,[0],[0]
"Thus, even if an individual SGD update is noisy and contains small signal energy, thresholding the Count-Sketch is based on the average update over time.",4. The MISSION Algorithm,[0],[0]
This averaging produces a robust signal that cancels out the noise.,4. The MISSION Algorithm,[0],[0]
"We can therefore expect MISSION to be superior over thresholding.
",4. The MISSION Algorithm,[0],[0]
"In the supplementary materials, we present initial theoretical results on the convergence of MISSION.",4. The MISSION Algorithm,[0],[0]
"Our results show
that, under certain assumptions, the full-gradient-descent version of MISSION converges geometrically to the true parameter β ∈",4. The MISSION Algorithm,[0],[0]
Rp up to some additive constants.,4. The MISSION Algorithm,[0],[0]
"The exploration of these assumptions and the extension to the SGD version of MISSION are exciting avenues for future work.
",4. The MISSION Algorithm,[0],[0]
"Feature Selection with the Ease of Feature Hashing: As argued earlier, the features are usually represented with strings, and we do not have the capability to map each string to a unique index in a vector without spendingO(p) memory.",4. The MISSION Algorithm,[0],[0]
"Feature hashing is convenient, because we can directly access every feature using hashes.",4. The MISSION Algorithm,[0],[0]
We can use any lossy hash function for strings.,4. The MISSION Algorithm,[0],[0]
MISSION only needs a few independent hash functions (3 in our Count-Sketch implementation) to access any component.,4. The MISSION Algorithm,[0],[0]
"The top-k estimation is done efficiently using a heap data structure of size k. Overall, we only access the data using efficient hash functions, which can be easily implemented in large-scale systems.",4. The MISSION Algorithm,[0],[0]
We designed a set of simulations to evaluate MISSION in a controlled setting.,5. Simulations,[0],[0]
"In contrast to the ultra large-scale, real-world experiments of Section 6, in the section the data matrices are drawn from a random Gaussian distribution and the ground truth features are known.",5. Simulations,[0],[0]
We first demonstrate the advantage of MISSION over greedy thresholding in feature selection.,5.1. Phase Transition,[0],[0]
"For this experiment, we modify MISSION slightly to find the root of the algorithmic advantage of MISSION: we replace the Count-Sketch with an “identity” sketch, or a sketch with a single hash function, h(i) = i.",5.1. Phase Transition,[0],[0]
"In doing so, we eliminate the complexity that Count-Sketch adds to the algorithm, so that the main difference between MISSION and IHT is that MISSION accumulates the gradients.",5.1. Phase Transition,[0],[0]
"To improve stability, we scale the non top-k elements of S by a factor γ ∈ (0, 1) that begins very near 1 and is gradually decreased until the algorithm converges.",5.1. Phase Transition,[0],[0]
"It is also possible to do this scaling in the CountSketch version of MISSION efficiently by exploiting the linearity of the sketch.
",5.1. Phase Transition,[0],[0]
Fig. 2 illustrates the empirical phase transition curves for sparse recovery using MISSION and the hard thresholding algorithms.,5.1. Phase Transition,[0],[0]
The phase transition curves show the points where the algorithm successfully recovers the features in > 50% of the random trails.,5.1. Phase Transition,[0],[0]
"MISSION shows a better phase transition curve compared to IHT by a considerable gap.
",5.1. Phase Transition,[0],[0]
Table 1.,5.1. Phase Transition,[0],[0]
Comparison of MISSION against hard thresholding algorithms in feature selection under adversarial effects.,5.1. Phase Transition,[0],[0]
We first report the percentage of instances in which the algorithms accurately find the solution (ACC) with no attenuation (α = 1) over 100 random trials.,5.1. Phase Transition,[0],[0]
"We then report the mean of the maximum level of attenuation α applied to the columns of design X before the algorithms fail to recover the support of β (over the trials that all algorithms can find the solution with α = 1).
",5.1. Phase Transition,[0],[0]
"(n, k) MISSION IHT ITI PHT",5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
ACCα=1 α,5.1. Phase Transition,[0],[0]
"ACCα=1 α (100, 2) 100% 2.68 ± 0.37 100% 1.49 ± 0.33 91% 1.33 ± 0.23 64% 2.42 ± 0.87 (100, 3) 100% 2.52 ± 0.36 92% 1.36 ± 0.46 70% 1.15 ± 0.20 42% 2.05 ± 0.93 (100, 4) 100% 2.53 ± 0.23 72% 1.92 ± 0.91 37% 1.03 ± 0.09 39% 2.13 ± 1.07 (200, 5) 100% 4.07 ± 0.36 99",5.1. Phase Transition,[0],[0]
"% 2.34 ± 1.12 37% 1.15 ± 0.22 83% 2.75 ± 1.30 (200, 6) 100% 4.17 ± 0.24 97% 2.64 ± 1.14 23% 1.11 ± 0.12 73% 2.26 ± 1.33 (200, 7) 100% 4.07 ± 0.11 83% 1.64 ± 1.01 14% 1.11 ± 0.12 75% 3.39 ± 1.36
0.0 0.2 0.4 0.6 0.8 1.0
n/p
0.0
0.2
0.4
0.6
0.8
1.0
s/ n
MISSION IHT ITI",5.1. Phase Transition,[0],[0]
"PHT
Figure 2.",5.1. Phase Transition,[0],[0]
Empirical phase transition in recovering a binary feature vector β in p = 1000-dimensional space with a Gaussian data matrix X.,5.1. Phase Transition,[0],[0]
We illustrate the empirical 50% probability of success curves averaged over T = 20 trials.,5.1. Phase Transition,[0],[0]
MISSION outperforms the thresholding algorithms by a large margin.,5.1. Phase Transition,[0],[0]
"A major problem with the IHT algorithm, especially in largescale SGD settings, is with thresholding the coordinates with small gradients in the earlier iterations.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"IHT misses these coordinates, since they become prominent only after the gradients accumulate with the progression of the algorithm.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"The problem is amplified with noisy gradient updates such as SGD, which is unavoidable for large datasets.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
This phenomenon occurs frequently in sparse recovery problems.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"For example, when the coordinates that correspond to the columns of the data matrix with smaller energy lag in the iterations of gradient descent algorithm, IHT thresholds these lagging-gradient coordinates in first few iterations, and they never show up again in the support.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0.9575833063515385],"['Finally, in this work we work in the noise-less setting where the function and the approximations are deterministic.']"
"In contrast, MISSION retains a footprint of the gradients of all the previous iterations in the Count-Sketch.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"When the total sum of the gradient of a coordinate becomes prominent, the coordinate joins the support after querying the top-k heavy hitters from the Count-Sketch.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
We illustrate this phenomena in sparse recovery using synthetic experiments.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"We recover sparse vector β from its random linear measurements y = Xβ, where the energy of X is imbalanced across its columns.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"In this case, the gradients corresponding to the
columns (coordinates) with smaller energy typically lag and are thresholded by IHT.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"To this end, we first construct a random Gaussian data matrix X ∈ R900×1000, pick a sparse vector β that is supported on an index set I , and then attenuate the energy of the columns of X supported by the indices in I by an attenuation factor of α = {1, 1.25, 1.5, 1.75, 2, . . .",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
", 5}.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
Note that α = 1 implies that no attenuation is applied to the matrix.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"In Table 1, we report the maximum attenuation level applied to a column of data matrix X before the algorithms fail to fully recover the support set I from y = βX.",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"We observe that MISSION is consistently and up to three times more robust against adversarial attenuation of the columns of the data matrix in various design settings.
",5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
The robustness of MISSION to the attenuation of the columns of X in sparse recovery task suggests that the Count-Sketch data structure enables gradient-based optimization methods such as IHT to store a footprint (or sketch) of all the gradients from the previous iterations and deliver them back when they become prominent.,5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding,[0],[0]
"in MISSION
In this section we demonstrate that the memory requirements of MISSION grows polylogarithmically in the dimension of the problem p.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
We conduct a feature selection experiment with a data matrix X ∈ R100×p whose entries are drawn from i.i.d. random Gaussian distributions with zero mean and unit variance.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"We run MISSION and IHT to recover the feature vector β from the output vector y = Xβ, where the feature vector β is a k = 5-sparse vector with random support.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
We repeat the same experiment 1000 times with different realizations for the sparse feature vector β and report the results in Fig. 3.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
The left plot illustrates the feature selection accuracy of the algorithms as the dimension of the problem p grows.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"The right plot illustrates the minimum memory requirements of the algorithms to recover the features with 100% accuracy.
",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
The size of the Count-Sketch in MISSION scales only polylogarithmically with the dimension of the problem.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
This is surprising since the aggregate gradient in a classical SGD framework becomes typically dense in early iterations and thus requires a memory of order O(p).,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"MISSION, however, stores only the essential information of the features in the sketch using a poly-logarithmic sketch size.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
Note that IHT sacrifices accuracy to achieve a small memory footprint.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
At every iteration IHT eliminates all the information except for the top-k features.,5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"We observe that, using only a logarithmic factor more memory, MISSION has a significant advantage over IHT in recovering the ground truth features.",5.3. Logarithmic Scaling of the Count-Sketch Memory,[0],[0]
"All experiments were performed on a single machine, 2x Intel Xeon E5-2660 v4 processors (28 cores / 56 threads) with 512 GB of memory.",6. Experiments,[0],[0]
The code1 for training and running our randomized-hashing approach is available online.,6. Experiments,[0],[0]
"We designed the experiments to answer these questions:
1.",6. Experiments,[0],[0]
Does MISSION outperform IHT in terms of classification accuracy?,6. Experiments,[0],[0]
"In particular, how much does myopic thresholding affect IHT in practice?
2.",6. Experiments,[0],[0]
"How well does MISSION match the speed and accuracy of feature hashing (FH)?
3.",6. Experiments,[0],[0]
"How does changing the number of top-k features affect the accuracy and behaviour of the different methods?
4.",6. Experiments,[0],[0]
"What is the effect of changing the memory size of the Count-Sketch data structure on the classification accuracy of MISSION in read-world datasets?
5.",6. Experiments,[0],[0]
"Does MISSION scale well in comparison to the different methods on the ultra large-scale datasets (> 350 GB in size)?
1https://github.com/rdspring1/MISSION",6. Experiments,[0],[0]
"Datasets: We used four datasets in the experiments: 1) KDD2012, 2) RCV1, 3) Webspam–Trigram, 4) DNA2.",6.1. Large-scale Feature Extraction,[0],[0]
"The statistics of these datasets are summarized in Table 2.
",6.1. Large-scale Feature Extraction,[0],[0]
The DNA metagenomics dataset is a multi-class classification task where the model must classify 15 different bacteria species using DNA K-mers.,6.1. Large-scale Feature Extraction,[0],[0]
We sub-sampled the first 15 species from the original dataset containing 193 species.,6.1. Large-scale Feature Extraction,[0],[0]
We use all of the species in the DNA Metagenomics dataset for the large-scale experiments (See Section 6.2).,6.1. Large-scale Feature Extraction,[0],[0]
"Following standard procedures, each bacterial species is associated with a reference genome.",6.1. Large-scale Feature Extraction,[0],[0]
Fragments are sampled from the reference genome until each nucleotide is covered c times on average.,6.1. Large-scale Feature Extraction,[0],[0]
The fragments are then divided into K-mer sub-strings.,6.1. Large-scale Feature Extraction,[0],[0]
We used fragments of length 200 and K-mers of length 12.,6.1. Large-scale Feature Extraction,[0],[0]
"Each model was trained and tested with mean coverage c = {0.1, 1} respectively.",6.1. Large-scale Feature Extraction,[0],[0]
"For more details, see (Vervier et al., 2016).",6.1. Large-scale Feature Extraction,[0],[0]
"The feature extraction task is to find the DNA K-mers that best represent each bacteria class.
",6.1. Large-scale Feature Extraction,[0],[0]
"We implemented the following approaches to compare and contrast against our approach: For all methods, we used the logistic loss for binary classification and the cross-entropy loss for multi-class classification.
MISSION:",6.1. Large-scale Feature Extraction,[0],[0]
As described in Section 4.,6.1. Large-scale Feature Extraction,[0],[0]
"Iterative Hard Thresholding (IHT): An algorithm where, after each gradient update, a hard threshold is applied to the features.",6.1. Large-scale Feature Extraction,[0],[0]
"Only the top-k features are kept active, while the rest are set to zero.",6.1. Large-scale Feature Extraction,[0],[0]
"Since the features are strings or integers, we used a sorted heap to store and manipulate the top-k elements.",6.1. Large-scale Feature Extraction,[0],[0]
This was the only algorithm we could successfully run over the large datasets on our single machine.,6.1. Large-scale Feature Extraction,[0],[0]
Batch IHT: A modification to IHT that uses mini-batches such that the gradient sparsity is the same as the number of elements in the count-sketch.,6.1. Large-scale Feature Extraction,[0],[0]
We accumulate features and then sort and prune to find the top-k features.,6.1. Large-scale Feature Extraction,[0],[0]
"This accumulate, sort, prune process is repeated several times during training.",6.1. Large-scale Feature Extraction,[0],[0]
Note:,6.1. Large-scale Feature Extraction,[0],[0]
"This setup requires significantly more memory than MISSION, because it explicitly stores the feature strings.",6.1. Large-scale Feature Extraction,[0],[0]
The memory cost of maintaining a set of string features can be orders of magnitude more than the flat array used by MISSION.,6.1. Large-scale Feature Extraction,[0],[0]
"See Bloom Filters (Broder & Mitzenmacher, 2004) and related literature.",6.1. Large-scale Feature Extraction,[0],[0]
"This setup is not scalable to large-scale datasets.
",6.1. Large-scale Feature Extraction,[0],[0]
"2http://projects.cbio.mines-paristech.fr/ largescalemetagenomics/
Feature Hashing (FH): A standard machine learning algorithm for dimensionality reduction that reduces the memory cost associated with large datasets.",6.1. Large-scale Feature Extraction,[0],[0]
FH is not a feature selection algorithm and cannot identify important features.,6.1. Large-scale Feature Extraction,[0],[0]
"(Agarwal et al., 2014)
",6.1. Large-scale Feature Extraction,[0],[0]
Experimental Settings: The MISSION and IHT algorithms searched for the same number of top-k features.,6.1. Large-scale Feature Extraction,[0],[0]
"To ensure fair comparisons, the size of the Count-Sketch and the feature vector allocated for the FH model were equal.",6.1. Large-scale Feature Extraction,[0],[0]
The size of the MISSION and FH models were set to the nearest power of 2 greater than the number of features in the dataset.,6.1. Large-scale Feature Extraction,[0],[0]
"For all the experiments, the Count-Sketch data structure used 3 hash functions, and the model weights were divided equally among the hash arrays.",6.1. Large-scale Feature Extraction,[0],[0]
"For example, with the (Tiny) DNA metagenomics dataset, we allocated 24 bits or 16,777,216 weights for the FH model.",6.1. Large-scale Feature Extraction,[0],[0]
"Given 3 hash functions and 15 classes, roughly 372,827 elements were allocated for each class in the Count-Sketch.
MISSION, IHT, FH Comparison: Fig. 4 shows that MISSION surpasses IHT in classification accuracy in all four datasets, regardless of the number of features.",6.1. Large-scale Feature Extraction,[0],[0]
"In addition, MISSION closely matches FH, which is significant because FH is allowed to model a much larger set of features than MISSION or IHT.",6.1. Large-scale Feature Extraction,[0],[0]
"MISSION is 2–4× slower than FH, which is expected given that MISSION has the extra overhead of using a heap to track the top-k features.
MISSION’s accuracy rapidly rises with respect to the number of top-k features, while IHT’s accuracy plateaus and then grows slowly to match MISSION.",6.1. Large-scale Feature Extraction,[0],[0]
This observation corroborates our insight that the greedy nature of IHT hurts performance.,6.1. Large-scale Feature Extraction,[0],[0]
"When the number of top-k elements is small, the capacity of IHT is limited, so it picks the first set of features that provides good performance, ignoring the rest.",6.1. Large-scale Feature Extraction,[0],[0]
"On the other hand, MISSION decouples the memory from the top-k ranking, which is based on the aggregated gradients in the compressed sketch.",6.1. Large-scale Feature Extraction,[0],[0]
"By the linear property of the count-sketch, this ensures that the heavier entries occur in the top-k features with high probability.
",6.1. Large-scale Feature Extraction,[0],[0]
"Count-Sketch Memory Trade-Off: Fig. 5 shows how MISSION’s accuracy degrades gracefully, as the size of the Count-Sketch decreases.",6.1. Large-scale Feature Extraction,[0],[0]
"In this experiment, MISSION only used the top 500K features for classifying the Tiny DNA metagenomics dataset.",6.1. Large-scale Feature Extraction,[0],[0]
"When the top-k to Count-Sketch ratio is 1, then 500K weights were allocated for each class and hash array in the Count-Sketch data structure.",6.1. Large-scale Feature Extraction,[0],[0]
"The Batch IHT baseline was given 8,388,608 memory elements per class, enabling it to accumulate a significant number of features before thresholding to find the top-k features.",6.1. Large-scale Feature Extraction,[0],[0]
"This experiment shows that MISSION immediately outperforms IHT and Batch IHT, once the top-k to Count-Sketch ratio is 1:1.",6.1. Large-scale Feature Extraction,[0],[0]
"Thus, MISSION provides a unique memory-accuracy knob at any given value of top-k.",6.1. Large-scale Feature Extraction,[0],[0]
"Here we demonstrate that MISSION can extract features from three large-scale datasets: Criteo 1TB, Splice-Site, and DNA Metagenomics.
",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Criteo 1TB: The Criteo 1TB3 dataset represents 24 days of click-through logs—23 days (training) + 1 day (testing).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The task for this dataset is click-through rate (CTR) prediction— How likely is a user to click an ad?,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The dataset contains over 4 billion (training) and 175 million (testing) examples (2.5 TB of disk space).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The performance metric is Area Under the ROC Curve (AUC).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The VW baseline4 achieved 0.7570 AUC score.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"MISSION and IHT scored close to the VW baseline with 0.751 AUC using only the top 250K features.
",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Splice-Site: The task for this dataset is to distinguish between true and fake splice sites using the local context around the splice site in-question.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"The dataset is highly skewed (few positive, many negative values), and so the performance metric is average precision (AP).",6.2. Ultra Large-Scale Feature Selection,[0],[0]
Average precision is the precision score averaged over all recall scores ranging from 0 to 1.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
The dataset contains over 50 million (training) and 4.6 million (testing) examples (3.2 TB of disk space).,6.2. Ultra Large-Scale Feature Selection,[0],[0]
All the methods were trained for a single epoch with a learning rate of 0.5.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"MISSION, Batch IHT, and SGD IHT tracked the top 16,384 features.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
"FH, MISSION, and Batch IHT used 786,432 extra memory elements.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
MISSION significantly outperforms Batch IHT and SGD IHT by 2.3%.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
"Also, unlike in Fig. 5, the extra memory did not help Batch IHT, since it performed the same as SGD IHT.",6.2. Ultra Large-Scale Feature Selection,[0],[0]
MISSION (17.5 hours) is 15% slower than FH (15 hours) in wall-clock running time.,6.2. Ultra Large-Scale Feature Selection,[0],[0]
DNA Metagenomics:,AP 0.522 0.510 0.498 0.498,[0],[0]
This experiment evaluates MISSION’s performance on a medium-sized metagenomics dataset.,AP 0.522 0.510 0.498 0.498,[0],[0]
"The parameters from the Tiny (15 species) dataset in Section 6.1 are shared with this experiment, except the
3https://www.kaggle.com/c/criteo-display-ad-challenge 4https://github.com/rambler-digital-solutions/
criteo-1tb-benchmark
DNA - Tiny (15 Species) - Top-K: 500K
number of species is increased to 193.",AP 0.522 0.510 0.498 0.498,[0],[0]
The size of a sample batch with mean coverage c = 1 increased from 7 GB (Tiny) to 68 GB (Medium).,AP 0.522 0.510 0.498 0.498,[0],[0]
Each round (mean coverage c = 0.25) contains 3.45 million examples and about 16.93 million unique non-zero features (p).,AP 0.522 0.510 0.498 0.498,[0],[0]
MISSION and IHT tracked the top 2.5 million features per class.,AP 0.522 0.510 0.498 0.498,[0],[0]
"The FH baseline used 231 weights, about 11.1 million weights per class, and we allocated the same amount of space for the Count-Sketch.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Each model was trained on a dataset with coverage c = 5.
",AP 0.522 0.510 0.498 0.498,[0],[0]
"Fig. 6 shows the evolution of classification accuracy over time for MISSION, IHT, and the FH baseline.",AP 0.522 0.510 0.498 0.498,[0],[0]
"After 5 epochs, MISSION closely matches the FH baseline.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Note: MISSION converges faster than IHT such that MISSION is 1–4 rounds ahead of IHT, with the gap gradually increasing over time.",AP 0.522 0.510 0.498 0.498,[0],[0]
"On average, the running time of MISSION is 1–2× slower than IHT.",AP 0.522 0.510 0.498 0.498,[0],[0]
"However, this experiment demonstrates that since MISSION converges faster, it actually needs less time to reach a certain accuracy level.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Therefore, MISSION is effectively faster and more accurate than IHT.",AP 0.522 0.510 0.498 0.498,[0],[0]
"Scalability and Parallelism: IHT finds the top-k features after each gradient update, which requires sorting the features based on their weights before thresholding.",7. Implementation Details and Discussion,[0],[0]
"The speed of the sorting process is improved by using a heap data
DNA - Medium (193 Species) - Top-K: 2.5M
structure, but it is still costly per update.",7. Implementation Details and Discussion,[0],[0]
"MISSION also uses a heap to store its top-k elements, but it achieves the same accuracy as IHT with far fewer top-k elements because of the Count-Sketch.",7. Implementation Details and Discussion,[0],[0]
"(Recall Section 4)
",7. Implementation Details and Discussion,[0.9525266542826527],"['(ii) Under Assumption 3: R⇤  2 ⌫ ⇢ ⇣ 2C(⌫,⇢)K ⇤( 1⇢ d(⌫,⇢) 1) ⌘ 1 d(⌫,⇢)+1 .']"
Another suggested improvement for the top-k heap is to use lazy updates.,7. Implementation Details and Discussion,[0],[0]
"Updating the weight of a feature does not change its position in the heap very often, but still requires an O(log n) operation.",7. Implementation Details and Discussion,[0],[0]
"With lazy updates, the heap is updated only if it the change is significant.",7. Implementation Details and Discussion,[0],[0]
|xt,7. Implementation Details and Discussion,[0],[0]
"− x0| ≥ , i.e. the new weight at time t exceeds the original value by some threshold.",7. Implementation Details and Discussion,[0],[0]
This tweak significantly reduces the number of heap updates at the cost of slightly distorting the heap.,7. Implementation Details and Discussion,[0],[0]
"In this paper, we presented MISSION, a new framework for ultra large-scale feature selection which maintains an efficient, approximate representation for the features using a Count-Sketch data structure.",8. Conclusion and Future Work,[0],[0]
"MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features.
",8. Conclusion and Future Work,[0],[0]
"Going forward, we are interested in leveraging our MISSION framework to explore pairwise or higher-order interaction features.",8. Conclusion and Future Work,[0],[0]
"Interaction features are important for scientific discovery, e.g., in genomics (Basu et al., 2018), however, the exponential dimensionality growth of interactions has hindered further progress.",8. Conclusion and Future Work,[0],[0]
We believe MISSION will enable more scientific discoveries from big data in future.,8. Conclusion and Future Work,[0],[0]
"AAA, DL, GD, and RB were supported by the DOD Vannevar Bush Faculty Fellowship grant N00014-18-1-2047, NSF grant CCF-1527501, ARO grant W911NF-15-1-0316, AFOSR grant FA9550-14-1-0088, ONR grant N00014-17-12551, DARPA REVEAL grant HR0011-16-C-0028, and an ONR BRC grant for Randomized Numerical Linear Algebra.",Acknowledgements,[0],[0]
"RS and AS were supported by NSF-1652131, AFOSR-YIP FA9550-18-1-0152, and ONR BRC grant for Randomized Numerical Linear Algebra.",Acknowledgements,[0],[0]
The authors would also like to thank NVIDIA and Amazon for gifting computing resources.,Acknowledgements,[0],[0]
Feature selection is an important challenge in machine learning.,abstractText,[0],[0]
It plays a crucial role in the explainability of machine-driven decisions that are rapidly permeating throughout modern society.,abstractText,[0],[0]
"Unfortunately, the explosion in the size and dimensionality of real-world datasets poses a severe challenge to standard feature selection algorithms.",abstractText,[0],[0]
"Today, it is not uncommon for datasets to have billions of dimensions.",abstractText,[0],[0]
"At such scale, even storing the feature vector is impossible, causing most existing feature selection methods to fail.",abstractText,[0],[0]
"Workarounds like feature hashing, a standard approach to large-scale machine learning, helps with the computational feasibility, but at the cost of losing the interpretability of features.",abstractText,[0],[0]
"In this paper, we present MISSION, a novel framework for ultra large-scale feature selection that performs stochastic gradient descent while maintaining an efficient representation of the features in memory using a Count-Sketch data structure.",abstractText,[0],[0]
MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features while using only O(log p) working memory.,abstractText,[0],[0]
"We demonstrate that MISSION accurately and efficiently performs feature selection on real-world, large-scale datasets with billions of dimensions.",abstractText,[0],[0]
MISSION: Ultra Large-Scale Feature Selection using Count-Sketches,title,[0],[0]
Many modern data sets consist of data that is gathered adaptively: the choice of whether to collect more data points of a given type depends on the data already collected.,1. Introduction,[0],[0]
"For example, it is common in industry to conduct “A/B” tests to make decisions about many things, including ad targeting, user interface design, and algorithmic modifications, and this A/B testing is often conducted using “bandit learning algorithms”",1. Introduction,[0],[0]
"(Bubeck et al., 2012), which adaptively select treatments to show to users in an effort to find the best treatment as quickly as possible.",1. Introduction,[0],[0]
"Similarly, sequen-
1Department of Statistics, The Wharton School, University of Pennsylvania 2Department of Computer Science, University of Pennsylvania.",1. Introduction,[0],[0]
Correspondence to: Seth Neel,1. Introduction,[0],[0]
"<sethneel93@gmail.com>, Aaron Roth <aaroth@cis.upenn.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"1This extended abstract is missing many details, proofs, and results that can be found in the full version (Neel & Roth, 2018).
",1. Introduction,[0],[0]
"tial clinical trials may halt or re-allocate certain treatment groups due to preliminary results, and empirical scientists may initially try and test multiple hypotheses and multiple treatments, but then decide to gather more data in support of certain hypotheses and not others, based on the results of preliminary statistical tests.
",1. Introduction,[0],[0]
"Unfortunately, as demonstrated by (Nie et al., 2017), the data that results from adaptive data gathering procedures will often exhibit substantial bias.",1. Introduction,[0],[0]
"As a result, subsequent analyses that are conducted on the data gathered by adaptive procedures will be prone to error, unless the bias is explicitly taken into account.",1. Introduction,[0],[0]
This can be difficult.,1. Introduction,[0],[0]
"(Nie et al., 2017) give a selective inference approach: in simple stochastic bandit settings, if the data was gathered by a specific stochastic algorithm that they design, they give an MCMC based procedure to perform maximum likelihood estimation to recover de-biased estimates of the underlying distribution means.",1. Introduction,[0],[0]
"In this paper, we give a related, but orthogonal approach whose simplicity allows for a substantial generalization beyond the simple stochastic bandits setting.",1. Introduction,[0],[0]
"We show that in very general settings, if the data is gathered by a differentially private procedure, then we can place strong bounds on the bias of the data gathered, without needing any additional de-biasing procedure.",1. Introduction,[0],[0]
"Via elementary techniques, this connection implies the existence of simple stochastic bandit algorithms with nearly optimal worst-case regret bounds, with very strong bias guarantees.",1. Introduction,[0],[0]
"By leveraging existing connections between differential privacy and adaptive data analysis (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016), we can extend the generality of our approach to bound not just bias, but to correct for effects of adaptivity on arbitrary statistics of the gathered data.",1. Introduction,[0],[0]
"Since the data being gathered will generally be useful for some as yet unspecified scientific analysis, rather than just for the narrow problem of mean estimation, our technique allows for substantially broader possibilities compared to past approaches.",1. Introduction,[0],[0]
"This paper has three main contributions:
1.",1.1. Our Results,[0],[0]
"Using elementary techniques, we provide explicit bounds on the bias of empirical arm means maintained by bandit algorithms in the simple stochastic
setting that make their selection decisions as a differentially private function of their observations.",1.1. Our Results,[0],[0]
"Together with existing differentially private algorithms for stochastic bandit problems, this yields an algorithm that obtains an essentially optimal worst-case regret bound, and guarantees minimal bias (on the order of O(1/ √ K · T )) for the empirical mean maintained for every arm.",1.1. Our Results,[0],[0]
"In the full version (Neel & Roth, 2018), we also extend our results to the linear contextual bandit problem, proving new bounds for a private linear UCB algorithm along the way.
2.",1.1. Our Results,[0],[0]
"We then make a general observation, relating adaptive data gathering to an adaptive analysis of a fixed dataset (in which the choice of which query to pose to the dataset is adaptive).",1.1. Our Results,[0],[0]
This lets us apply the large existing literature connecting differential privacy to adaptive data analysis.,1.1. Our Results,[0],[0]
"In particular, it lets us apply the max-information bounds of (Dwork et al., 2015b; Rogers et al., 2016) to our adaptive data gathering setting.",1.1. Our Results,[0],[0]
"This allows us to give much more general guarantees about the data collected by differentially private collection procedures, that extend well beyond bias.",1.1. Our Results,[0],[0]
"For example, it lets us correct the p-values for arbitrary hypothesis tests run on the gathered data.
",1.1. Our Results,[0],[0]
3.,1.1. Our Results,[0],[0]
"Finally, we run a set of experiments that measure the bias incurred by the standard UCB algorithm in the stochastic bandit setting, contrast it with the low bias obtained by a private UCB algorithm, and show that there are settings of the privacy parameter that simultaneously can make bias statistically insignificant, while having competitive empirical regret with the non-private UCB algorithm.",1.1. Our Results,[0],[0]
We also demonstrate in the linear contextual bandit setting how failing to correct for adaptivity can lead to false discovery when applying t-tests for non-zero regression coefficients on an adaptively gathered dataset.,1.1. Our Results,[0],[0]
This paper bridges two recent lines of work.,1.2. Related Work,[0],[0]
"Our starting point is two recent papers: (Villar et al., 2015) empirically demonstrate in the context of clinical trials that a variety of simple stochastic bandit algorithms produce biased sample mean estimates (Similar results have been empirically observed in the context of contextual bandits (Dimakopoulou et al., 2017)).",1.2. Related Work,[0],[0]
"(Nie et al., 2017) prove that simple stochastic bandit algorithms that exhibit two natural properties (satisfied by most commonly used algorithms, including UCB and Thompson Sampling) result in empirical means that exhibit negative bias.",1.2. Related Work,[0],[0]
"They then propose a heuristic algorithm which computes a maximum likelihood estimator for the sample means from the empirical means gathered by a modified UCB algorithm which adds Gumbel noise to
the decision statistics.",1.2. Related Work,[0],[0]
"(Deshpande et al., 2017) propose a debiasing procedure for ordinary least-squares estimates computed from adaptively gathered data that trades off bias for variance, and prove a central limit theorem for their method.",1.2. Related Work,[0],[0]
"In contrast, the methods we propose in this paper are quite different.",1.2. Related Work,[0],[0]
"Rather than giving an ex-post debiasing procedure, we show that if the data were gathered in a differentially private manner, no debiasing is necessary.",1.2. Related Work,[0],[0]
"The strength of our method is both in its simplicity and generality: rather than proving theorems specific to particular estimators, we give methods to correct the p-values for arbitrary hypothesis tests that might be run on the adaptively gathered data.
",1.2. Related Work,[0],[0]
"The second line of work is the recent literature on adaptive data analysis (Dwork et al., 2015c;b; Hardt & Ullman, 2014; Steinke & Ullman, 2015; Russo & Zou, 2016; Wang et al., 2016; Bassily et al., 2016; Hardt & Blum, 2015; Cummings et al., 2016; Feldman & Steinke, 2017a;b) which draws a connection between differential privacy (Dwork et al., 2006) and generalization guarantees for adaptively chosen statistics.",1.2. Related Work,[0],[0]
"The adaptivity in this setting is dual to the setting we study in the present paper: In the adaptive data analysis literature, the dataset itself is fixed, and the goal is to find techniques that can mitigate bias due to the adaptive selection of analyses.",1.2. Related Work,[0],[0]
"In contrast, here, we study a setting in which the data gathering procedure is itself adaptive, and can lead to bias even for a fixed set of statistics of interest.",1.2. Related Work,[0],[0]
"However, we show that adaptive data gathering can be re-cast as an adaptive data analysis procedure, and so the results from the adaptive data analysis literature can be ported over.",1.2. Related Work,[0],[0]
"In a simple stochastic bandit problem, there are K unknown distributions Pi over the unit interval [0,1], each with (unknown) mean µi.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Over a series of rounds t ∈ {1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", T}, an algorithm A chooses an arm it ∈",2.1. Simple Stochastic Bandit Problems,[0],[0]
"[K], and observes a reward yit,t ∼ Pit .",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Given a sequence of choices i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT , the pseudo-regret of an algorithm is defined to be:
Regret((P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK), i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT )",2.1. Simple Stochastic Bandit Problems,[0],[0]
= T ·max i µi − T∑ t=1,2.1. Simple Stochastic Bandit Problems,[0],[0]
"µit
We say that regret is bounded if we can put a bound on the quantity Regret((P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK), i1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", iT ) in the worst case over the choice of distributions P1, . . .",2.1. Simple Stochastic Bandit Problems,[0],[0]
", PK , and with high probability or in expectation over the randomness of the algorithm and of the reward sampling.
",2.1. Simple Stochastic Bandit Problems,[0],[0]
"As an algorithm A interacts with a bandit problem, it generates a history Λ , which records the sequence of actions
taken and rewards observed thus far:",2.1. Simple Stochastic Bandit Problems,[0],[0]
"Λt = {(i`, yi`,`)} t−1 `=1.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"We denote the space of histories of length T by HT = ([K]× R)T .
",2.1. Simple Stochastic Bandit Problems,[0],[0]
The definition of an algorithm A induces a sequence of T (possibly randomized) selection functions ft : Ht−1,2.1. Simple Stochastic Bandit Problems,[0],[0]
"→ [K], which map histories onto decisions of which arm to pull at each round.",2.1. Simple Stochastic Bandit Problems,[0],[0]
"In the contextual bandit problem, decisions are endowed with observable features.",2.2. Contextual Bandit Problems,[0],[0]
"Our algorithmic results in this paper focus on the linear contextual bandit problem, but our general connection between adaptive data gathering and differential privacy extends beyond the linear case.",2.2. Contextual Bandit Problems,[0],[0]
"For simplicity of exposition, we specialize to the linear case here.
",2.2. Contextual Bandit Problems,[0],[0]
"There are K arms i, each of which is associated with an unknown d-dimensional linear function represented by a vector of coefficients θi ∈ Rd with ||θi||2 ≤ 1.",2.2. Contextual Bandit Problems,[0],[0]
"In rounds t ∈ {1, . . .",2.2. Contextual Bandit Problems,[0],[0]
", T}, the algorithm is presented with a context xi,t ∈ Rd for each arm i with ||xi,t||2 ≤ 1, which may be selected by an adaptive adversary as a function of the past history of play.",2.2. Contextual Bandit Problems,[0],[0]
"We write xt to denote the set of all K contexts present at round t. As a function of these contexts, the algorithm then selects an arm it, and observes a reward yit,t. The rewards satisfy E [yi,t] = θi·xi,t and are bounded to lie in [0, 1].
",2.2. Contextual Bandit Problems,[0],[0]
"In the contextual setting, histories incorporate observed context information as well:",2.2. Contextual Bandit Problems,[0],[0]
"Λt = {(i`, x`, yi`,`)} t−1 `=1.
",2.2. Contextual Bandit Problems,[0],[0]
"Again, the definition of an algorithmA induces a sequence of T (possibly randomized) selection functions ft : Ht−1× Rd×K → [K], which now maps both a history and a set of contexts at round t to a choice of arm at round t.",2.2. Contextual Bandit Problems,[0],[0]
"Above we’ve characterized a bandit algorithmA as gathering data adaptively using a sequence of selection functions ft, which map the observed history",2.3. Data Gathering in the Query Model,[0],[0]
Λt ∈ Ht−1 to the index of the next arm pulled.,2.3. Data Gathering in the Query Model,[0],[0]
In this model only after the arm is chosen is a reward drawn from the appropriate distribution.,2.3. Data Gathering in the Query Model,[0],[0]
"Then the history is updated, and the process repeats.
",2.3. Data Gathering in the Query Model,[0],[0]
"In this section, we observe that whether the reward is drawn after the arm is “pulled,” or in advance, is a distinction without a difference.",2.3. Data Gathering in the Query Model,[0],[0]
"We cast this same interaction into the setting where an analyst asks an adaptively chosen sequence of queries to a fixed dataset, representing the arm rewards.",2.3. Data Gathering in the Query Model,[0],[0]
The process of running a bandit algorithm A up to time T can be formalized as the adaptive selection of T queries against a single database of size T - fixed in advance.,2.3. Data Gathering in the Query Model,[0],[0]
"The formalization consists of observing two things.
",2.3. Data Gathering in the Query Model,[0],[0]
"First, by the principle of deferred randomness, we can view any (simple or contextual) bandit algorithm as operating in a setting in the rewards available for every arm at every time step have been sampled before the start of the algorithm, rather than online as the algorithm makes its selections.",2.3. Data Gathering in the Query Model,[0],[0]
"Second, the choice of arm pulled at time t by the bandit algorithm can be viewed as the answer to an adaptively selected query against this fixed dataset of rewards.
",2.3. Data Gathering in the Query Model,[0],[0]
"Adaptive data analysis is formalized as an interaction in which a data analystA performs computations on a dataset D, observes the results, and then may choose the identity of the next computation to run as a function of previously computed results (Dwork et al., 2015c;a).",2.3. Data Gathering in the Query Model,[0],[0]
"A sequence of recent results shows that if the queries are differentially private in the dataset D, then they will not in general overfit D, in the sense that the distribution over results induced by computing q(D) will be “similar” to the distribution over results induced if q were run on a new dataset, freshly sampled from the same underlying distribution (Dwork et al., 2015c;a; Bassily et al., 2016; Dwork et al., 2015b; Rogers et al., 2016).",2.3. Data Gathering in the Query Model,[0],[0]
"We will be more precise about what these results say in Section 4.
",2.3. Data Gathering in the Query Model,[0],[0]
"Recall that histories Λ record the choices of the algorithm, in addition to its observations.",2.3. Data Gathering in the Query Model,[0],[0]
It will be helpful to introduce notation that separates out the choices of the algorithm from its observations.,2.3. Data Gathering in the Query Model,[0],[0]
"In the simple stochastic setting and the contextual setting, given a history",2.3. Data Gathering in the Query Model,[0],[0]
"Λt, an action history ΛAt =",2.3. Data Gathering in the Query Model,[0],[0]
"(i1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", it−1) ∈",2.3. Data Gathering in the Query Model,[0],[0]
"[K]t−1 denotes the portion of the history recording the actions of the algorithm.
",2.3. Data Gathering in the Query Model,[0],[0]
"In the simple stochastic setting, a bandit tableau is a T ×K matrix D ∈",2.3. Data Gathering in the Query Model,[0],[0]
"( [0, 1]K )T .",2.3. Data Gathering in the Query Model,[0],[0]
"Each row Dt of D is a vector of K real numbers, intuitively representing the rewards that would be available to a bandit algorithm at round t for each of the K arms.",2.3. Data Gathering in the Query Model,[0],[0]
"In the contextual setting, a bandit tableau is represented by a pair of T ×K matrices: D ∈",2.3. Data Gathering in the Query Model,[0],[0]
"( [0, 1]K
)T and C ∈ ( (Rd)K )T .",2.3. Data Gathering in the Query Model,[0],[0]
"Intuitively, C represents the contexts presented to a bandit algorithm",2.3. Data Gathering in the Query Model,[0],[0]
"A at each round: each row Ct corresponds to a set of K contexts, one for each arm.",2.3. Data Gathering in the Query Model,[0],[0]
"D again represents the rewards that would be available to the bandit algorithm at round t for each of the K arms.
",2.3. Data Gathering in the Query Model,[0],[0]
"We write Tab to denote a bandit tableau when the setting has not been specified: implicitly, in the simple stochastic case, Tab = D, and in the contextual case, Tab = (D,C).
",2.3. Data Gathering in the Query Model,[0],[0]
Given a bandit tableau and a bandit algorithm,2.3. Data Gathering in the Query Model,[0],[0]
"A, we have the following interaction:
We denote the subset of the reward tableau D corresponding to rewards that would have been revealed to a bandit algorithmA given action history ΛAt , by ΛAt (D).",2.3. Data Gathering in the Query Model,[0],[0]
"Concretely if ΛAt = (i1, . .",2.3. Data Gathering in the Query Model,[0],[0]
.,2.3. Data Gathering in the Query Model,[0],[0]
", it−1) then Λ A t (D) =",2.3. Data Gathering in the Query Model,[0],[0]
"{(i`, yi`,`)} t−1 `=1.",2.3. Data Gathering in the Query Model,[0],[0]
"Given a selection function ft and an action history ΛAt , de-
Interact Inputs: Time horizon T , bandit algorithm A, and bandit tableau Tab (D in the simple stochastic case, (D,C) in the contextual case)
1: for t = 1 to T do 2: (contextual case) ShowA contexts Ct,1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", Ct,K 3: Let A play action it 4: Show A reward Dt,it 5: end for 6: Return: (i1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", iT )
fine the query qΛAt as qΛAt (D) = ft(Λ A t (D)).
",2.3. Data Gathering in the Query Model,[0],[0]
We now define Algorithms Bandit and InteractQuery.,2.3. Data Gathering in the Query Model,[0],[0]
"Bandit is a standard contextual bandit algorithm defined by selection functions ft, and InteractQuery is the Interact routine that draws the rewards in advance, and at time t selects action it as the result of query qΛAt .",2.3. Data Gathering in the Query Model,[0],[0]
"With the above definitions in hand, it is straightforward to show that the two Algorithms are equivalent, in that they induce the same joint distribution on their outputs.",2.3. Data Gathering in the Query Model,[0],[0]
"In both algorithms for convenience we assume we are in the linear contextual setting, and we write ηit to denote the i.i.d. error distributions of the rewards, conditional on the contexts.
",2.3. Data Gathering in the Query Model,[0],[0]
"Bandit Inputs: T, k, {xit}, {θi}, ft,Λ0 = ∅
1: for t = 1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", T : do 2: Let it = ft(Λt−1) 3: Draw yit,t ∼ xit,t · θit + ηit 4: Update Λt = Λt−1 ∪ (it, yit,t) 5: end for 6: Return: ΛT
InteractQuery Inputs: T, k,D : Dit = θi · xit + ηit, ft
1: for t = 1, . . .",2.3. Data Gathering in the Query Model,[0],[0]
", T : do 2: Let qt = qΛAt−1 3: Let it = qt(D) 4: Update ΛAt = Λ",2.3. Data Gathering in the Query Model,[0],[0]
"A t−1 ∪ it 5: end for 6: Return: ΛAT
Claim 1.",2.3. Data Gathering in the Query Model,[0],[0]
"Let P1,t be the joint distribution induced by Algorithm Bandit on Λt at time t, and let P2,t be the joint distribution induced by Algorithm InteractQuery on Λt = Λ A t (D).",2.3. Data Gathering in the Query Model,[0],[0]
"Then ∀t P1,t = P2,t.
",2.3. Data Gathering in the Query Model,[0],[0]
"The upshot of this equivalence is that we can import existing results that hold in the setting in which the dataset
is fixed, and queries are adaptively chosen.",2.3. Data Gathering in the Query Model,[0],[0]
"There are a large collection of results of this form that apply when the queries are differentially private (Dwork et al., 2015c; Bassily et al., 2016; Rogers et al., 2016) which apply directly to our setting.",2.3. Data Gathering in the Query Model,[0],[0]
"In the next section we formally define differential privacy in the simple stochastic and contextual bandit setting, and leave the description of the more general transfer theorems to Section 4.",2.3. Data Gathering in the Query Model,[0],[0]
We will be interested in algorithms that are differentially private.,2.4. Differential Privacy,[0],[0]
"In the simple stochastic bandit setting, we will require differential privacy with respect to the rewards.",2.4. Differential Privacy,[0],[0]
"In the contextual bandit setting, we will also require differential privacy with respect to the rewards, but not necessarily with respect to the contexts.
",2.4. Differential Privacy,[0],[0]
"We now define the neighboring relation we need to define bandit differential privacy:
Definition 1.",2.4. Differential Privacy,[0],[0]
"In the simple stochastic setting, two bandit tableau’s D,D′ are reward neighbors if they differ in at most a single row: i.e. if there exists an index ` such that for all t 6=",2.4. Differential Privacy,[0],[0]
"`, Dt = D′t.
",2.4. Differential Privacy,[0],[0]
"In the contextual setting, two bandit tableau’s (D,C), (D′, C ′) are reward neighbors if C = C ′ and D and D′ differ in at most a single row: i.e. if there exists an index ` such that for all t 6=",2.4. Differential Privacy,[0],[0]
"`, Dt = D′t.
",2.4. Differential Privacy,[0],[0]
"Note that changing a context does not result in a neighboring tableau: this neighboring relation will correspond to privacy for the rewards, but not for the contexts.",2.4. Differential Privacy,[0],[0]
Remark 1.,2.4. Differential Privacy,[0],[0]
"Note that we could have equivalently defined reward neighbors to be tableaus that differ in only a single entry, rather than in an entire row.",2.4. Differential Privacy,[0],[0]
"The distinction is unimportant in a bandit setting, because a bandit algorithm will be able to observe only a single entry in any particular row.
",2.4. Differential Privacy,[0],[0]
Definition 2.,2.4. Differential Privacy,[0],[0]
"A bandit algorithm A is ( , δ) reward differentially private if for every time horizon T and every pair of bandit tableau Tab,Tab′ that are reward neighbors, and every subset S ⊆",2.4. Differential Privacy,[0],[0]
"[K]T :
P [Interact(T,A,Tab) ∈ S] ≤
e P [ Interact(T,A,Tab′) ∈ S ]",2.4. Differential Privacy,[0],[0]
"+ δ
",2.4. Differential Privacy,[0],[0]
"If δ = 0, we say that A is -differentially private.",2.4. Differential Privacy,[0],[0]
We begin by showing that differentially private algorithms that operate in the stochastic bandit setting compute empirical means for their arms that are nearly unbiased.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Together
with known differentially private algorithms for stochastic bandit problems, the result is an algorithm that obtains a nearly optimal (worst-case) regret guarantee while also guaranteeing that the collected data is nearly unbiased.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"We could (and do) obtain these results by combining the reduction to answering adaptively selected queries given by Theorem 1 with the standard generalization theorems in adaptive data analysis (e.g. Corollary 2 in its most general form), but we first prove these de-biasing results from first principles to build intuition.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Theorem 1.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Let A be an ( , δ)-differentially private algorithm in the stochastic bandit setting.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Then, for all i ∈",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[K], and all t, we have:∣∣∣E [Ŷ ti − µi]∣∣∣ ≤",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
(e − 1 + Tδ)µi Remark 2.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Note that since µi ∈,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[0, 1], and for 1, e ≈ 1+ , this theorem bounds the bias by roughly +Tδ.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Often, we will have δ = 0 and so the bias will be bounded by roughly .
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Proof.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
First we fix some notation.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Fix any time horizon T , and let (ft)t∈[T ] be the sequence of selection functions induced by algorithm A. Let 1{ft(Λt)=i} be the indicator for the event that arm i is pulled at time t. We can write the random variable representing the sample mean of arm i at time T as
Ŷ Ti = T∑ t=1 1{ft(Λt)=i}∑T t′=1 1{ft′ (Λt′ )",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"=i} yit
where we recall that yi,t is the random variable representing the reward for arm",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"i at time t. Note that the numerator (ft(Λt) = i) is by definition independent of yi,t, but the denominator ( ∑T t′=1 1{ft′ (Λt′ )=i}) is not, because for t
′",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"> tΛt′ depends on yi,t.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"It is this dependence that leads to bias in adaptive data gathering procedures, and that we must argue is mitigated by differential privacy.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
We recall that the random variable NTi represents the number of times arm i is pulled through round T : NTi =∑T t′=1 1{ft′ (Λt′ ),3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
=i}.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Using this notation, we write the sample mean of arm i at time T , as:
Ŷ Ti = T∑ t=1 1{ft(Λt)=i} NTi · yit
We can then calculate:
E[Ŷ ti ] = T∑ t=1 E[ 1{ft(Λt)=i} NTi yit]
= T∑ t=1 E yit∼Pi",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[yit · E A [ 1{ft(Λt)=i} NTi |yit]]
where the first equality follows by the linearity of expectation, and the second follows by the law of iterated expectation.
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Our goal is to show that the conditioning in the inner expectation does not substantially change the value of the expectation.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Specifically, we want to show that all t, and any value yit, we have
E[ 1{ft(Λt)=i}
Ni |yit] ≥ e− E[
1{ft(Λt)=i}
NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− δ
If we can show this, then we will have
E[Ŷ Ti ] ≥ (e− T∑ t=1 E[ 1{ft(Λt)=i} NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− Tδ) · µi
= (e− E[ NTi NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− Tδ) · µi = (e− − Tδ) · µi
which is what we want (The reverse inequality is symmetric).
",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
This is what we now show to complete the proof.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Observe that for all t, i, the quantity 1{ft(Λt)=i}Ni can be derived as a post-processing of the sequence of choices (f1(Λ1), . . .",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
", fT (ΛT )), and is therefore differentially private in the observed reward sequence.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Observe also that the quantity 1{ft(Λt)=i}
NTi is bounded in [0, 1].",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Hence (by a
lemma in the full version) for any pair of values yit, y′it, we have E[
1{ft(Λt)=i} NTi |yit] ≥ e− E[ 1{ft(Λt)=i} NTi |y′it]− δ.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"All
that remains is to observe that there must exist some value y′it such that E[ 1{ft(Λt)=i} Ni |y′it] ≥ E[ 1{ft(Λt)=i} Ni ].",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"(Otherwise, this would contradict",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
Ey′it∼Pi,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"[E[ 1{ft(Λt)=i} Ni |y′it]] = E[ 1{ft(Λt)=i}
NTi ])",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"Fixing any such y′it implies that for all yit
E[ 1{ft(Λt)=i}
Ni |yit] ≥ e− E[
1{ft(Λt)=i}
NTi |y′i,t]− δ
≥ e− E[ 1{ft(Λt)=i}
NTi ]",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"− δ
as desired.",3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
The upper bound on the bias follows symmetrically.,3. Privacy Reduces Bias in Stochastic Bandit Problems,[0],[0]
"There are existing differentially private variants of the classic UCB algorithm ((Auer et al., 2002; Agrawal, 1995; Lai & Robbins, 1985)), which give a nearly optimal tradeoff between privacy and regret (Mishra & Thakurta, 2014; Tossou & Dimitrakakis, 2017; 2016).",3.1. A Private UCB Algorithm,[0],[0]
"For completeness, we give a simple version of a private UCB algorithm in the full version which we use in our experiments.",3.1. A Private UCB Algorithm,[0],[0]
"Here, we simply quote the relevant theorem, which is a consequence of a theorem in (Tossou & Dimitrakakis, 2016):
Theorem 2.",3.1. A Private UCB Algorithm,[0],[0]
"(Tossou & Dimitrakakis, 2016)",3.1. A Private UCB Algorithm,[0],[0]
"There is an - differentially private algorithm that obtains expected regret bounded by:
O ( max ( lnT · (ln ln(T ) + ln(1/ )) , √ kT log T ))
",3.1. A Private UCB Algorithm,[0],[0]
"Thus, we can take to be as small as = O( ln 1.5 T√ kT )
while still having a regret bound of O( √ kT log T ), which is nearly optimal in the worst case (over instances) (Audibert & Bubeck, 2009).
",3.1. A Private UCB Algorithm,[0],[0]
"Combining the above bound with Theorem 1, and letting =",3.1. A Private UCB Algorithm,[0],[0]
"O( ln
1.5 T√ kT ), we have:
Corollary 1.",3.1. A Private UCB Algorithm,[0],[0]
"There exists a simple stochastic bandit algorithm that simultaneously guarantees that the bias of the empirical average for each arm i is bounded by O(µi · ln
1.5 T√ kT
) and guarantees expected regret bounded by O( √ kT log T ).
",3.1. A Private UCB Algorithm,[0],[0]
"Of course, other tradeoffs are possible using different values of .",3.1. A Private UCB Algorithm,[0],[0]
"For example, the algorithm of (Tossou & Dimitrakakis, 2016) obtains sub-linear regret so long as = ω( ln
2 T T ).",3.1. A Private UCB Algorithm,[0],[0]
"Thus, it is possible to obtain non-trivial regret while guaranteeing that the bias of the empirical means remains as low as polylog(T )/T .",3.1. A Private UCB Algorithm,[0],[0]
"Up through this point, we have focused our attention on showing how the private collection of data mitigates the effect that adaptivity has on bias, in both the stochastic and (in the full version) contextual bandit problems.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"In this section, we draw upon more powerful results from the adaptive data analysis literature to go substantially beyond bias: to correct the p-values of hypothesis tests applied to adaptively gathered data.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"These p-value corrections follow from the connection between differential privacy and a quantity called max information, which controls the extent to which the dependence of selected test on the dataset can distort the statistical validity of the test (Dwork et al., 2015b; Rogers et al., 2016).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"We briefly define max information, state the connection to differential privacy, and illustrate how max information bounds can be used to perform adaptive analyses in the private data gathering framework.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Definition 3 (Max-Information (Dwork et al., 2015b).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let X,Z be jointly distributed random variables over domain (X ,Z).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let X ⊗ Z denote the random variable that draws independent copies of X,Z according to their marginal distributions.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"The β-approximate maxinformation between X,Z, denoted Iβ(X,Z), is defined
as:
Iβ(X,Z) = log sup O⊂(X×Z), P[(X,Z)∈O]>β",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
P,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[(X,Z) ∈ O]−",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
β P,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
[X ⊗ Z ∈,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"O]
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Following (Rogers et al., 2016), define a test statistic t : D → R, where D is the space of all datasets.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"For D ∈ D, given an output a = t(D), the p-value associated with the test t on dataset D is p(a) =",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
PD∼P0,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[t(D) ≥ a], where P0 is the null hypothesis distribution.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Consider an algorithm A, mapping a dataset to a test statistic.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Definition 4 (Valid p-value Correction Function (Rogers et al., 2016).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
A function γ :,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[0, 1] → [0, 1] is a valid p-value correction function for A if the procedure:
1.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Select a test statistic t = A(D)
2.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Reject the null hypothesis if p(t(D)) ≤ γ(α)
has probability at most α of rejection, when D ∼ P0.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then the following theorem gives a valid p-value correction function when (D,A(D)) have bounded β-approximate max information.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Theorem 3 ((Rogers et al., 2016).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be a datadependent algorithm for selecting a test statistics such that Iβ(X,A(X))",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
≤,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
k.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then the following function γ is a valid p-value correction function for A: γ(α) = max(α−β
2k , 0)
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Finally, we can connect max information to differential privacy, which allows us to leverage private algorithms to perform arbitrary valid statistical tests.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Theorem 4 (Theorem 20 from (Dwork et al., 2015b).).",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be an -differentially private algorithm, let P be an arbitrary product distribution over datasets of size n, and let D ∼ P .",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then for every β > 0:
Iβ(D,A(D)) ≤ log(e)( 2n/2 + √ n log(2/β)/2)
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Remark 3.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We note that a hypothesis of this theorem is that the data is drawn from a product distribution.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"In the contextual bandit setting, this corresponds to rows in the bandit tableau being drawn from a product distribution.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"This will be the case if contexts are drawn from a distribution at each round, and then rewards are generated as some fixed stochastic function of the contexts.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Note that contexts (and even rewards) can be correlated with one another within a round, so long as they are selected independently across rounds.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We now formalize the process of running a hypothesis test against an adaptively collected dataset.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
A bandit algorithm A generates a history ΛT ∈ HT .,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Let the reward portion of the gathered dataset be denoted by DA.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"We define an adaptive test statistic selector as follows.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Definition 5.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
Fix the reward portion of a bandit tableau D and bandit algorithmA.,4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"An adaptive test statistic selector is a function s from action histories to test statistics such that s(ΛAT ) is a real-valued function of the adaptively gathered dataset DA.
",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Importantly, the selection of the test statistic s(ΛAT ) can depend on the sequence of arms pulled by A (and in the contextual setting, on all contexts observed), but not otherwise on the reward portion of the tableau D. For example, tA = s(Λ",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"A T ) could be the t-statistic corresponding to the null hypothesis that the arm i∗ which was pulled the great-
est number of times has mean µ: tA(DA) = ∑NT i∗ t=1 yi∗t−µ√
NT i∗
By virtue of Theorems 3 and 4, and our view of adaptive data gathering as adaptively selected queries, we get the following corollary:
Corollary 2.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Let A be an reward differentially private bandit algorithm, and let s be an adaptive test statistic selector.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Fix β > 0, and let γ(α) =
α
2log(e)( 2T/2+
√ T log(2/β)/2) , for α ∈",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"[0, 1].",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"Then for any adaptively selected statistic tA = s(ΛAT ), and any product distribution P corresponding to the null hypothesis for tA
PD∼P,A [p(tA(D))",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
≤ γ(α)],4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
"≤ α
If we set = O(1/ √ T ) in Corollary 2, then γ(α) = O(α)– i.e. a valid p-value correction that only scales α by a constant.",4. Max Information & Arbitrary Hypothesis Tests,[0],[0]
We first validate our theoretical bounds on bias in the simple stochastic bandit setting.,5. Experiments,[0],[0]
"As expected the standard UCB algorithm underestimates the mean at each arm, while the private UCB algorithm of (?) obtains very low bias.",5. Experiments,[0],[0]
"While using the suggested by the theory effectively reduces bias and achieves near optimal asymptotic regret, the resulting private algorithm only achieves non-trivial regret for large T due to large constants and logarithmic factors in our bounds.",5. Experiments,[0],[0]
"This motivates a heuristic choice of that provides no theoretical guarantees on bias reduction, but leads to regret that is comparable to the non-private UCB algorithm.",5. Experiments,[0],[0]
We find empirically that even with this large choice of we achieve an 8 fold reduction in bias relative to UCB.,5. Experiments,[0],[0]
"This is consistent with the observation that our guarantees hold in the worst-case, and suggests that there is room for improvement in our theoretical bounds — both improving constants in the worst-case bounds on bias and on regret, and for proving instance specific bounds.",5. Experiments,[0],[0]
"Finally, we show that in the linear contextual bandit setting collecting data adaptively with a linear UCB algorithm and then conducting t-tests for regression coefficients yields incorrect inference (absent a p-value correction).",5. Experiments,[0],[0]
"These findings
confirm the necessity of our methods when drawing conclusions from adaptively gathered data.",5. Experiments,[0],[0]
In our first stochastic bandit experiment we set K = 20 and T = 500.,5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The K arm means are equally spaced between 0 and 1 with gap ∆ = .05, with µ0 = 1.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"We run UCB and -private UCB for T rounds with = .05, and after each run compute the difference between the sample mean at each arm and the true mean.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"We repeat this process 10, 000 times, averaging to obtain high confidence estimates of the bias at each arm.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The average absolute bias over all arms for private UCB was .00176, with the bias for every arm being statistically indistinguishable from 0 at 95% confidence (see Figure 1 for confidence intervals) while the average absolute bias (over arms) for UCB was .0698, or over 40 times higher.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The most biased arm had a measured bias of roughly 0.14, and except for the top 4 arms, the bias of each arm was statistically significant.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"It is worth noting that private UCB achieves bias significantly lower than the = .05 guaranteed by the theory, indicating that the theoretical bounds on bias obtained from differential privacy are conservative.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Figures 1, 2 show the bias at each arm for private UCB vs. UCB, with 95% confidence intervals around the bias at each arm.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Not only is the bias for private UCB an order of magnitude smaller on average, it does not exhibit the systemic negative bias evident in Figure 2.
",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Noting that the observed reduction in bias for = .05 exceeded that guaranteed by the theory, we run a second experiment withK = 5, T = 100000,∆ = .05, and = 400, averaging results over 1000 iterations.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
Figure 5 shows that private UCB achieves sub-linear regret comparable with UCB.,5.1. Stochastic Multi-Armed Bandit,[0],[0]
"While = 400 provides no meaningful theoretical guarantee, the average absolute bias at each arm mean obtained by the private algorithm was .0015 (statistically indistinguishable from 0 at 95% confidence for each arm), while the non-private UCB algorithm obtained average bias .011, 7.5 times larger.",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"The bias reduction for the arm with the smallest mean (for which the bias is the worst with the non private algorithm) was by more than a factor of 10.
",5.1. Stochastic Multi-Armed Bandit,[0],[0]
"Figures 3,4 show the bias at each arm for the private and non-private UCB algorithms together with 95% confidence intervals; again we observe a negative skew in the bias for UCB, consistent with the theory in (Nie et al., 2017).",5.1. Stochastic Multi-Armed Bandit,[0],[0]
Our second experiment confirms that adaptivity leads to bias in the linear contextual bandit setting in the context of hypothesis testing – and in particular can lead to false discovery in testing for non-zero regression coefficients.,5.2. Linear Contextual Bandits,[0],[0]
"The set up is as follows: for K = 5 arms, we observe rewards
yi,t ∼ N (θ′ixit, 1), where θi, xit ∈ R5, ||θi|| = ||xit|| = 1.",5.2. Linear Contextual Bandits,[0],[0]
"For each arm i, we set θi1 = 0.",5.2. Linear Contextual Bandits,[0],[0]
"Subject to these constraints, we pick the θ parameters uniformly at random (once per run), and select the contexts x uniformly at random (at each round).",5.2. Linear Contextual Bandits,[0],[0]
"We run a linear UCB algorithm (OFUL (?)) for T = 500 rounds, and identify the arm i∗ that has been selected most frequently.",5.2. Linear Contextual Bandits,[0],[0]
We then conduct a z-test for whether the first coordinate of θi∗ is equal to 0.,5.2. Linear Contextual Bandits,[0],[0]
"By construction the null hypothesis H0 : θi∗1 = 0 of the experiment is true, and absent adaptivity, the p-value should be distributed uniformly at random.",5.2. Linear Contextual Bandits,[0],[0]
"In particular, for any value of α the probability that the corresponding p-value is less than α is exactly α.",5.2. Linear Contextual Bandits,[0],[0]
"We record the observed p-value, and repeat the experiment 1000 times, displaying the histogram of observed p-values in Figure 6.",5.2. Linear Contextual Bandits,[0],[0]
"As expected, the adaptivity of the data gathering process leads the p-values to exhibit a strong downward skew.",5.2. Linear Contextual Bandits,[0],[0]
The dotted blue line demarcates α = .05.,5.2. Linear Contextual Bandits,[0],[0]
"Rather than probability .05 of falsely rejecting the null hypothesis at 95% confidence, we observe that 76% of the observed p-values fall below the .05 threshold.",5.2. Linear Contextual Bandits,[0],[0]
"This shows that a careful p-value correction in the style of Section 2.3 is essential even for simple testing of regression coefficients, lest bias lead to false discovery.",5.2. Linear Contextual Bandits,[0],[0]
"Data that is gathered adaptively — via bandit algorithms, for example — exhibits bias.",abstractText,[0],[0]
This is true both when gathering simple numeric valued data — the empirical means kept track of by stochastic bandit algorithms are biased downwards — and when gathering more complicated data — running hypothesis tests on complex data gathered via contextual bandit algorithms leads to false discovery.,abstractText,[0],[0]
"In this paper, we show that this problem is mitigated if the data collection procedure is differentially private.",abstractText,[0],[0]
"This lets us both bound the bias of simple numeric valued quantities (like the empirical means of stochastic bandit algorithms), and correct the p-values of hypothesis tests run on the adaptively gathered data.",abstractText,[0],[0]
"Moreover, there exist differentially private bandit algorithms with near optimal regret bounds: we apply existing theorems in the simple stochastic case, and give a new analysis for linear contextual bandits.",abstractText,[0],[0]
We complement our theoretical results with experiments validating our theory1.,abstractText,[0],[0]
Mitigating Bias in Adaptive Data Gathering via Differential Privacy,title,[0],[0]
Estimating generative models from unlabeled data is one of the challenges in unsupervised learning.,1. Introduction,[0],[0]
"Recently, several latent variable approaches have been proposed to learn flexible density estimators together with efficient sampling, such as generative adversarial networks (GANs) (Goodfellow et al., 2014), variational autoencoders (Kingma & Welling, 2014; Rezende et al., 2014), iterative transformation of noise (Sohl-Dickstein et al., 2015), or non-volume preserving transformations (Dinh et al., 2017).
",1. Introduction,[0],[0]
"In this work we focus on GANs, currently the most con-
*Equal contribution 1Université Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France. 2Université Paris Sud, INRIA, équipe TAU, Gif-sur-Yvette, 91190, France.",1. Introduction,[0],[0]
"3Facebook Artificial Intelligence Research Paris, France.",1. Introduction,[0],[0]
"Correspondence to: Corentin Tallec <corentin.tallec@inria.fr>, Thomas Lucas <thomas.lucas@inria.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"vincing source of samples of natural images (Karras et al., 2018).",1. Introduction,[0],[0]
GANs consist of a generator and a discriminator network.,1. Introduction,[0],[0]
"The generator maps samples from a latent random variable with a basic prior, such as a multi-variate Gaussian, to the observation space.",1. Introduction,[0],[0]
This defines a probability distribution over the observation space.,1. Introduction,[0],[0]
A discriminator network is trained to distinguish between generated samples and true samples in the observation space.,1. Introduction,[0],[0]
"The generator, on the other hand, is trained to fool the discriminator.",1. Introduction,[0],[0]
"In an idealized setting with unbounded capacity of both networks and infinite training data, the generator should converge to the distribution from which the training data has been sampled.
",1. Introduction,[0],[0]
"In most adversarial setups, the discriminator classifies individual data samples.",1. Introduction,[0],[0]
"Consequently, it cannot directly detect discrepancies between the distribution of generated samples and global statistics of the training distribution, such as its moments or quantiles.",1. Introduction,[0],[0]
"For instance, if the generator models a restricted part of the support of the target distribution very well, this can fool the discriminator at the level of individual samples, a phenomenon known as mode dropping.",1. Introduction,[0],[0]
In such a case there is little incentive for the generator to model other parts of the support of the target distribution.,1. Introduction,[0],[0]
"A more thorough explanation of this effect can be found in (Salimans et al., 2016).
",1. Introduction,[0],[0]
"In order to access global distributional statistics, imagine a discriminator that could somehow take full probability distributions as its input.",1. Introduction,[0],[0]
This is impossible in practice.,1. Introduction,[0],[0]
"Still, it is possible to feed large batches of training or generated samples to the discriminator, as an approximation of the corresponding distributions.",1. Introduction,[0],[0]
The discriminator can compute statistics on those batches and detect discrepancies between the two distributions.,1. Introduction,[0],[0]
"For instance, if a large batch exhibits only one mode from a multimodal distribution, the discriminator would notice the discrepancy right away.",1. Introduction,[0],[0]
"Even though a single batch may not encompass all modes of the distribution, it will still convey more information about missing modes than an individual example.
",1. Introduction,[0],[0]
"Training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task too easy, as a single bad sample reveals the whole batch as synthetic.",1. Introduction,[0],[0]
"Instead, we introduce a “mixed” batch discrimination task in which the discriminator needs to predict the ratio of real samples in a batch.
1
This use of batches differs from traditional minibatch learning.",1. Introduction,[0],[0]
"The batch is not used as a computational trick to increase parallelism, but as an approximate distribution, on which to compute global statistics.
",1. Introduction,[0],[0]
"A naive way of doing so would be to concatenate the samples in the batch, feeding the discriminator a single tensor containing all the samples.",1. Introduction,[0],[0]
"However, this is parameterhungry, and the computed statistics are not automatically invariant to the order of samples in the batch.",1. Introduction,[0],[0]
"To compute functions that depend on the samples only through their distribution, it is necessary to restrict the class of discriminator networks to permutation-invariant functions of the batch.",1. Introduction,[0],[0]
"For this, we adapt and extend an architecture from McGregor (2007) to compute symmetric functions of the input.",1. Introduction,[0],[0]
"We show this can be done with minimal modification to existing architectures, at a negligible computational overhead w.r.t.",1. Introduction,[0],[0]
"ordinary batch processing.
",1. Introduction,[0],[0]
"In summary, our contributions are the following:
• Naively training the discriminator to discriminate “pure” batches with only real or only synthetic samples makes its task way too easy.",1. Introduction,[0],[0]
"We introduce a discrimination loss based on mixed batches of true and fake samples, that avoids this pitfall.",1. Introduction,[0],[0]
We derive the associated optimal discriminator.,1. Introduction,[0],[0]
• We provide a principled way of defining neural networks that are permutation-invariant over a batch of samples.,1. Introduction,[0],[0]
"We formally prove that the resulting class of functions comprises all symmetric continuous functions, and only symmetric functions.",1. Introduction,[0],[0]
"• We apply these insights to GANs, with good experimental results, both qualitatively and quantitatively.
",1. Introduction,[0],[0]
"We believe that discriminating between distributions at the batch level provides an equally principled alternative to approaches to GANs based on duality formulas (Nowozin et al., 2016; Gulrajani et al., 2017; Arjovsky et al., 2017).",1. Introduction,[0],[0]
The training of generative models via distributional rather than pointwise information has been explored in several recent contributions.,2. Related work,[0],[0]
"Batch discrimination (Salimans et al., 2016) uses a handmade layer to compute batch statistics which are then combined with sample-specific features to enhance individual sample discrimination.",2. Related work,[0],[0]
Karras et al. (2018) directly compute the standard deviation of features and feed it as an additional feature to the last layer of the network.,2. Related work,[0],[0]
"Both methods use a single layer of handcrafted batch statistics, instead of letting the discriminator learn arbitrary batch statistics useful for discrimination as in our approach.",2. Related work,[0],[0]
"Moreover, in both methods the discriminator still assesses single samples, rather than entire batches.",2. Related work,[0],[0]
"Radford et al. (2015) reported improved results with batch normalization
in the discriminator, which may also be due to reliance on batch statistics.
",2. Related work,[0],[0]
"Other works, such as (Li et al., 2015) and (Dziugaite et al., 2015), replace the discriminator with a fixed distributional loss between true and generated samples, the maximum mean discrepancy, as the criterion to train the generative model.",2. Related work,[0],[0]
"This has the advantage of relieving the inherent instability of GANs, but lacks the flexibility of an adaptive discriminator.
",2. Related work,[0],[0]
The discriminator we introduce treats batches as sets of samples.,2. Related work,[0],[0]
Processing sets prescribes the use of permutation invariant networks.,2. Related work,[0],[0]
"There has been a large body of work around permutation invariant networks, e.g (McGregor, 2007; 2008; Qi et al., 2016; Zaheer et al., 2017; Vaswani et al., 2017).",2. Related work,[0],[0]
"Our processing is inspired by (McGregor, 2007; 2008) which designs a special kind of layer that provides the desired invariance property.",2. Related work,[0],[0]
The network from McGregor (2007) is a multi-layer perceptron in which the single hidden layer performs a batchwise computation that makes the result equivariant by permutation.,2. Related work,[0],[0]
"Here we show that stacking such hidden layers and reducing the final layer with a permutation invariant reduction, covers the whole space of continuous permutation invariant functions.
",2. Related work,[0],[0]
"Zaheer et al. (2017) first process each element of the set independently, then aggregate the resulting representation using a permutation invariant operation, and finally process the permutation invariant quantity.",2. Related work,[0],[0]
"Qi et al. (2016) process 3D point cloud data, and interleave layers that process points independently, and layers that apply equivariant transformations.",2. Related work,[0],[0]
"The output of their networks are either permutation equivariant for pointcloud segmentation, or permutation invariant for shape recognition.",2. Related work,[0],[0]
"In our approach we stack permutation equivariant layers that combine batch
information and sample information at every level, and aggregate these in the final layer using a permutation invariant operation.
",2. Related work,[0],[0]
"More complex approaches to permutation invariance or equivariance appear in (Guttenberg et al., 2016).",2. Related work,[0],[0]
"We prove, however, that our simpler architecture already covers the full space of permutation invariant functions.
",2. Related work,[0],[0]
Improving the training of GANs has received a lot of recent attention.,2. Related work,[0],[0]
"For instance, Arjovsky et al. (2017), Gulrajani et al. (2017) and Miyato et al. (2018) constrain the Lipschitz constant of the network and show that this stabilizes training and improves performance.",2. Related work,[0],[0]
Karras et al. (2018) achieved impressive results by gradually increasing the resolution of the generated images as training progresses.,2. Related work,[0],[0]
"Using a batch of samples rather than individual samples as input to the discriminator can provide global statistics about
the distributions of interest.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
Such statistics could be useful to avoid mode dropping.,3. Adversarial learning with permutation-invariant batch features,[0],[0]
"Adversarial learning (Goodfellow et al., 2014) can easily be extended to the batch discrimination case.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"For a fixed batch size B, the corresponding two-player optimization procedure becomes
min G max D Ex1,...,xB∼D",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"[logD(x1, . . .",3. Adversarial learning with permutation-invariant batch features,[0],[0]
", xB)]",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"+ (1)
Ez1,...,zB∼Z",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"[log(1−D(G(z1), . . .",3. Adversarial learning with permutation-invariant batch features,[0],[0]
", G(zB)))",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"]
with D the empirical distribution over data, Z a distribution over the latent variable that is the input of the generator, G a pointwise generator and D a batch discriminator.1 This leads to a learning procedure similar to the usual GAN algorithm, except that the loss encourages the discriminator to output 1 when faced with an entire batch of real data, and 0 when faced with an entire batch of generated data.
",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"Unfortunately, this basic procedure makes the work of the discriminator too easy.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"As the discriminator is only faced with batches that consist of either only training samples or only generated samples, it can base its prediction on any subset of these samples.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"For example, a single poor generated sample would be enough to reject a batch.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"To cope with this deficiency, we propose to sample batches that mix both training and generated data.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"The discriminator’s task is to predict the proportion of real images in the batch, which is clearly a permutation invariant quantity.",3. Adversarial learning with permutation-invariant batch features,[0],[0]
"A naive approach to sampling mixed batches would be, for each batch index, to pick a datapoint from either real or generated images with probability 12 .",3.1. Batch smoothing as a regularizer,[0],[0]
"This is necessarily ill behaved: as the batch size increases, the ratio of training data to generated data in the batch tends to 12 by the law of large numbers.",3.1. Batch smoothing as a regularizer,[0],[0]
"Consequently, a discriminator always predicting 12 would achieve very low error with large batch sizes, and provide no training signal to the generator.
",3.1. Batch smoothing as a regularizer,[0],[0]
"Instead, for each batch we sample a ratio p from a distribution P on [0, 1], and construct a batch by picking real samples with probability p and generated samples with probability 1− p.",3.1. Batch smoothing as a regularizer,[0],[0]
"This forces the discriminator to predict across an entire range of possible values of p.
Formally, suppose we are given a batch of training data x ∈ RB×n",3.1. Batch smoothing as a regularizer,[0],[0]
and a batch of generated data x̃ ∈ RB×n.,3.1. Batch smoothing as a regularizer,[0],[0]
"To mix x and x̃, a binary vector β is sampled from B (p)B , a B-dimensional Bernoulli distribution with parameter p.",3.1. Batch smoothing as a regularizer,[0],[0]
"The mixed batch with mixing vector β is denoted
mβ(x, x̃)",3.1. Batch smoothing as a regularizer,[0],[0]
:= x β + x̃ (1− β).,3.1. Batch smoothing as a regularizer,[0],[0]
"(2) 1The generator G could also be modified to produce batches of data, which can help to cover more modes per batch, but this deviates from the objective of learning a density estimator from which we can draw i.i.d.",3.1. Batch smoothing as a regularizer,[0],[0]
"samples.
",3.1. Batch smoothing as a regularizer,[0],[0]
"This apparently wastes some samples, but we can reuse the discarded samples by using 1− β in the next batch.
",3.1. Batch smoothing as a regularizer,[0],[0]
"The discriminator has to predict the ratio of real images, #βB where #β is the sum of the components of β.",3.1. Batch smoothing as a regularizer,[0],[0]
"As a loss on the predicted ratio, we use the Kullback–Leibler divergence between a Bernoulli distribution with the actual ratio of real images, and a Bernoulli distribution with the predicted ratio.",3.1. Batch smoothing as a regularizer,[0],[0]
"The divergence between Bernoulli distributions with parameters u and v is
KL(B (u) ||",3.1. Batch smoothing as a regularizer,[0],[0]
B (v)),3.1. Batch smoothing as a regularizer,[0],[0]
= u log u v +,3.1. Batch smoothing as a regularizer,[0],[0]
(1− u) log 1− u1− v .,3.1. Batch smoothing as a regularizer,[0],[0]
"(3)
Formally, the discriminator D will minimize the objective
Ep∼P, β∼B(p)B KL ( B (
#β B
)",3.1. Batch smoothing as a regularizer,[0],[0]
||,3.1. Batch smoothing as a regularizer,[0],[0]
"B (D(mβ(x, x̃))) ) ,
(4)
where the expectation is over sampling p from a distribution P , typically uniform on [0, 1], then sampling a mixed minibatch.",3.1. Batch smoothing as a regularizer,[0],[0]
"For clarity, we have omitted the expectation over the sampling of training and generated samples
The generator is trained with the loss
Ep∼P, β∼B(p)B log(D(mβ(x, x̃))).",3.1. Batch smoothing as a regularizer,[0],[0]
"(5)
This loss, which is not the generator loss associated to the min-max optimization problem, is known to saturate less (Goodfellow et al., 2014).
",3.1. Batch smoothing as a regularizer,[0],[0]
"In some experimental cases, using the discriminator loss (4) with P = U([0, 1]) made discriminator training too difficult.",3.1. Batch smoothing as a regularizer,[0],[0]
"To alleviate some of the difficulty, we sampled the mixing variable p from a reduced symmetric union of intervals [0, γ] ∪ [1 − γ, 1].",3.1. Batch smoothing as a regularizer,[0],[0]
"With low γ, all generated batches are nearly purely taken from either real or fake data.",3.1. Batch smoothing as a regularizer,[0],[0]
We refer to this training method as batch smoothing-γ.,3.1. Batch smoothing as a regularizer,[0],[0]
"Batch smoothing-0 corresponds to no mixing, while batch smoothing-0.5 corresponds to equation (4).",3.1. Batch smoothing as a regularizer,[0],[0]
"The optimal discriminator for batch smoothing can be computed explicitly, for p ∼ U([0, 1]), and extends the usual GAN discriminator when B = 1.",3.2. The optimal discriminator for batch smoothing,[0],[0]
Proposition 1.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"The optimal discriminator for the loss (4),
given a batch y ∈ RB×N , is
D∗(y) = 12 punbalanced(y) pbalanced(y)
(6)
where the distribution pbalanced and punbalanced on batches are defined as
pbalanced(y)",3.2. The optimal discriminator for batch smoothing,[0],[0]
"= 1 B + 1 ∑
β∈{0,1}B
p1(y)βp2(y)1−β( B #β )
punbalanced(y) = 2 B + 1 ∑
β∈{0,1}B
p1(y)βp2(y)1−β( B #β )",3.2. The optimal discriminator for batch smoothing,[0],[0]
"#β B .
(7)
in which p1 is the data distribution and p2 the distribution of generated samples, and where p1(y)β is shorthand for p1(y1)β1 . . .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"p1(yB)βB .
",3.2. The optimal discriminator for batch smoothing,[0],[0]
The proof is technical and is deferred to the supplementary material.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"For non-uniform beta distributions on p, a similar result holds, with different coefficients depending on #β and B in the sum.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
These heavy expressions can be interpreted easily.,3.2. The optimal discriminator for batch smoothing,[0],[0]
"First, in the case B = 1, the optimal discriminator reduces to the optimal discriminator for a standard GAN, D∗ = p1(y)p1(y)+p2(y) .
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Actually pbalanced(y) is simply the distribution of batches y under our procedure of sampling p uniformly, then sampling β ∼ B (p)B .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"The binomial coefficients put on equal footing contributions with different true/fake ratios.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"The generator loss (5), when faced with the optimal discriminator, is the Kullback–Leibler divergence between pbalanced and punbalanced (up to sign and a constant log(2)).",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Since punbalanced puts more weight on batches with higher #β (more true samples), this brings fake samples closer to true ones.
",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Since pbalanced and punbalanced differ by a factor 2#β/B, the ratio D∗ = 12 punbalanced(y) pbalanced(y) is simply the expectation of #β/B under a probability distribution on β that is proportional to p1(y)βp2(y)1−β(
B #β ) .",3.2. The optimal discriminator for batch smoothing,[0],[0]
"But this is the posterior distribution on
β given the batch y and the uniform prior on the ratio p.",3.2. The optimal discriminator for batch smoothing,[0],[0]
"Thus, the optimal discriminator is just the posterior mean of the ratio of true samples, D∗(y) =",3.2. The optimal discriminator for batch smoothing,[0],[0]
IEβ|y [ #β B ] .,3.2. The optimal discriminator for batch smoothing,[0],[0]
This is standard when minimizing the expected divergence between Bernoulli distributions and the approach can therefore be extended to non-uniform priors on p as shown in section 9.,3.2. The optimal discriminator for batch smoothing,[0],[0]
Computing statistics of probability distributions from batches of i.i.d.,4. Permutation invariant networks,[0],[0]
"samples requires to compute quantities that
are invariant to permuting the order of samples within the batch.",4. Permutation invariant networks,[0],[0]
In this section we propose a permutation equivariant layer that can be used together with a permutation invariant aggregation operation to build networks that are permutation invariant.,4. Permutation invariant networks,[0],[0]
"We also provide a sketch of proof (fully developed in the supplementary material) that this architecture is able to reach all symmetric continuous functions, and only represents such functions.",4. Permutation invariant networks,[0],[0]
"A naive way of achieving invariance to batch permutations is to consider the batch dimension as a regular feature dimension, and to randomly reorder the batches at each step.",4.1. Building a permutation invariant architecture,[0],[0]
"This multiplies the input dimension by the batch size, and thus greatly increases the number of trainable parameters.",4.1. Building a permutation invariant architecture,[0],[0]
"Moreover, this only provides approximate invariance to batch permutation, as the network has to infer the invariance based on the training data.
",4.1. Building a permutation invariant architecture,[0],[0]
"Instead, we propose to directly build invariance into the architecture.",4.1. Building a permutation invariant architecture,[0],[0]
"This method drastically reduces the number of parameters compared to the naive approach, bringing it back in line with ordinary networks, and ensures strict invariance to batch permutation.
",4.1. Building a permutation invariant architecture,[0],[0]
Let us first formalize the notion of batch permutation invariance and equivariance.,4.1. Building a permutation invariant architecture,[0],[0]
"A function f from RB×l to RB×L is batch permutation equivariant if permuting samples in the batch results in the same permutation of the outputs: for any permutation σ of the inputs,
f(xσ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xσ(B))",4.1. Building a permutation invariant architecture,[0],[0]
"= f(x)σ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", f(x)σ(B).",4.1. Building a permutation invariant architecture,[0],[0]
"(8)
For instance, any regular neural network or other function treating the inputs x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB independently in parallel, is batch permutation equivariant.
",4.1. Building a permutation invariant architecture,[0],[0]
"A function f from RB×l to RL is batch permutation invariant if permuting the inputs in the batch does not change the output: for any permutation on batch indices σ,
f(xσ(1), . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xσ(B))",4.1. Building a permutation invariant architecture,[0],[0]
"= f(x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB).",4.1. Building a permutation invariant architecture,[0],[0]
"(9)
The mean, the max or the standard deviation along the batch axis are all batch permutation invariant.
",4.1. Building a permutation invariant architecture,[0],[0]
"Permutation equivariant and permutation invariant functions can be obtained by combining ordinary, parallel treatment of batch samples with an additional batch-averaging operation that performs an average of the activations across the batch direction.",4.1. Building a permutation invariant architecture,[0],[0]
"In our architecture, this averaging is the only form of interaction between different elements of the batch.",4.1. Building a permutation invariant architecture,[0],[0]
"It is one of our main results that such operations are sufficient to recover all invariant functions.
",4.1. Building a permutation invariant architecture,[0],[0]
"Formally, on a batch of data x ∈ RB×n, our proposed batch
permutation invariant network fθ is defined as
fθ(x) = 1 B B∑ b=1",4.1. Building a permutation invariant architecture,[0],[0]
(φθp ◦ φθp−1 ◦ . . .,4.1. Building a permutation invariant architecture,[0],[0]
"◦ φθ0(x))b (10)
where each φθi is a batch permutation equivariant function from RB×li−1 to RB×li , where the li’s are the layer sizes.
",4.1. Building a permutation invariant architecture,[0],[0]
"The equivariant layer operation φθ with l input features and L output features comprises an ordinary weight matrix Λ ∈ Rl×L that treats each data point of the batch independently (“non-batch-mixing”), a batch-mixing weight matrix Γ ∈ Rl×L, and a bias vector β ∈ RL.",4.1. Building a permutation invariant architecture,[0],[0]
"As in regular neural networks, Λ processes each data point in the batch independently.",4.1. Building a permutation invariant architecture,[0],[0]
"On the other hand, the weight matrix Γ operates after computing an average across the whole batch.",4.1. Building a permutation invariant architecture,[0],[0]
"Defining ρ as the batch average for each feature,
ρ(x1, . . .",4.1. Building a permutation invariant architecture,[0],[0]
", xB)",4.1. Building a permutation invariant architecture,[0],[0]
:= 1 B B∑ b=1,4.1. Building a permutation invariant architecture,[0],[0]
"xb (11)
the permutation-equivariant layer φ is formally defined as φθ(x)b := µ ( β + xbΛ + ρ(x)Γ ) (12)
where µ is a nonlinearity, b is a batch index, and the parameter of the layer is θ = (β,Λ,Γ).",4.1. Building a permutation invariant architecture,[0],[0]
The networks constructed above are permutation invariant by construction.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"However, it is unclear a priori that all permutation invariant functions can be represented this way: the functions that can be approximated to arbitrary precision by those networks could be a strict subset of the set of permutation invariant functions.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"The optimal solution for the discriminator could lie outside this subset, making our construction too restrictive.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"We now show this is not the case: our architecture satisfies a universal approximation theorem for permutation-invariant functions.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
Theorem 1.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
The set of networks that can be constructed by stacking as in Eq.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
(10) the layers φ defined in Eq.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"(12), with sigmoid nonlinearities except on the output layer, is dense in the set of permutation-invariant functions (for the topology of uniform convergence on compact sets).
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"While the case of one-dimensional features is relatively simple, the multidimensional case is more intricate, and the detailed proof is given in the supplementary material.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"Let us describe the key ideas underlying the proof.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"The standard universal approximation theorem for neural networks proves the following: for any continuous function f , we can find a network that given a batch x = (x1, . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", xB), computes (f(x1), . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", f(xB)).",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"This is insufficient for our purpose as it provides no way of mixing information between samples in the batch.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"First, we prove that the set of functions that can be approximated to arbitrary precision by our networks is an algebra, i.e., a vector space stable under products.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"From this point on, it remains to be shown that this algebra contains a generative family of the continuous symmetric functions.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"To prove that we can compute the sum of two functions f1 and f2, compute f1 and f2 on different channels (this is possible even if f1 and f2 require different numbers of layers, by filling in with the identity if necessary).",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"Then sum across channels, which is possible in (12).
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"To compute products, first compute f1 and f2 on different channels, then apply the universal approximation theorem to turn this into log f1 and log f2, then add, then take the exponential thanks to the universal approximation theorem.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"The key point is then the following: the algebra of all permutation-invariant polynomials over the components of (x1, . . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
", xB) is generated as an algebra by the averages 1 B (f(x1) + . .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
.+ f(xB)),4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
when f ranges over all functions of single batch elements.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"This non-trivial algebraic statement is proved in the supplementary material.
",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"By construction, such functions 1B (f(x1)+. .",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
".+f(xB)) are readily available in our architecture, by computing f as in an ordinary network and then applying the batch-averaging operation ρ in the next layer.",4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
Further layers provide sums and products of those thanks to the algebra property.,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
We can conclude with a symmetric version of the Stone–Weierstrass theorem (polynomials are dense in continuous functions).,4.2. Networks of equivariant layers provide universal approximation of permutation invariant functions,[0],[0]
"In our experiments, we apply the constructions above to standard, deep convolutional neural networks.",4.3. Practical architecture,[0],[0]
"In practice, for the linear operations Λ and Γ in (12) we use convolutional kernels (of size 3× 3) acting over xb and ρ(x) respectively.",4.3. Practical architecture,[0],[0]
Weight tensors Λ and Γ are also reweighted like so that at the start of training ρ(x) does not contribute disproportionately compared with other features:,4.3. Practical architecture,[0],[0]
Λ̃ = |B||B|+1 Λ and Γ̃ = 1|B|+1 Γ where |B| denotes the size of batch B.,4.3. Practical architecture,[0],[0]
"While these coefficients could be learned, we have found this explicit initialization to improve training.",4.3. Practical architecture,[0],[0]
"Figure 1 shows how to modify standard CNN architectures to adapt each layer to our method.
",4.3. Practical architecture,[0],[0]
"In the first setup, which we refer to as BGAN, a permutation invariant reduction is done at the end of the discriminator, yielding a single prediction per batch, which is evaluated with the loss in (4).",4.3. Practical architecture,[0],[0]
"We also introduce a setup, M-BGAN, where we swap the order of averaging and applying the loss.",4.3. Practical architecture,[0],[0]
"2 Namely, letting y be the single target for the batch (in our case, the proportion of real samples), the BGAN case translates into
L((o1, . . .",4.3. Practical architecture,[0],[0]
", oB), y) = ` (
1 B B∑ i=1",4.3. Practical architecture,[0],[0]
"oi, y
) (13)
2This was initially a bug that worked.
while M-BGAN translates to
L((o1, . . .",4.3. Practical architecture,[0],[0]
", oB), y) = 1 B B∑ i=1",4.3. Practical architecture,[0],[0]
"`(oi, y) (14)
where L is the final loss function, ` is the KL loss function used in (4), (o1, . .",4.3. Practical architecture,[0],[0]
.,4.3. Practical architecture,[0],[0]
", ob) is the output of the last equivariant layer, and y is the target for the whole batch.
",4.3. Practical architecture,[0],[0]
Both these losses are permutation invariant.,4.3. Practical architecture,[0],[0]
A more detailled explanation of M-BGAN is given in Section 11.,4.3. Practical architecture,[0],[0]
The synthetic dataset from Zhang et al. (2017) is explicitly designed to test mode dropping.,5.1. Synthetic 2D distributions,[0],[0]
The data are sampled from a mixture of concentrated Gaussians in the 2D plane.,5.1. Synthetic 2D distributions,[0],[0]
"We compare standard GAN training, “mixup” training (Zhang et al., 2017), and batch smoothing using the BGAN from Section 4.3.
",5.1. Synthetic 2D distributions,[0],[0]
"In all cases, the generators and discriminators are three-layer ReLU networks with 512 units per layer.",5.1. Synthetic 2D distributions,[0],[0]
The latent variables of the generator are 2-dimensional standard Gaussians.,5.1. Synthetic 2D distributions,[0],[0]
"The models are trained on their respective losses using the Adam (Kingma & Ba, 2015) optimizer, with default parameters.",5.1. Synthetic 2D distributions,[0],[0]
"The discriminator is trained for five steps for each generator step.
",5.1. Synthetic 2D distributions,[0],[0]
The results are summarized in Figure 3.,5.1. Synthetic 2D distributions,[0],[0]
Batch smoothing and mixup have similar effects.,5.1. Synthetic 2D distributions,[0],[0]
Results for BGAN and M-BGAN are qualitatively similar on this dataset and we only display results for BGAN.,5.1. Synthetic 2D distributions,[0],[0]
"The standard GAN setting quickly diverges, due to its inability to fit several modes simultaneously, while both batch smoothing and mixup successfully fit the majority of modes of the distribution.",5.1. Synthetic 2D distributions,[0],[0]
"Next, we consider image generation on the CIFAR10 dataset.",5.2. Experimental results on CIFAR10,[0],[0]
"We use the simple architecture from (Miyato et al., 2018), minimally modified to obtain permutation invariance thanks to (12).",5.2. Experimental results on CIFAR10,[0],[0]
All other architectural choices are unchanged.,5.2. Experimental results on CIFAR10,[0],[0]
"The same Adam hyperparameters from (Miyato et al., 2018) are used for all models: α = 2e−4, β1 = 0.5, β2 = 0.999, and no learning rate decay.",5.2. Experimental results on CIFAR10,[0],[0]
"We performed hyperparameter search for the number of discrimination steps between each generation step, ndisc, over the range {1, . . .",5.2. Experimental results on CIFAR10,[0],[0]
", 5}, and for the batch smoothing parameter γ over [0.2, 0.5].",5.2. Experimental results on CIFAR10,[0],[0]
"All models are trained for 400, 000 iterations, counting both generation and discrimination steps.",5.2. Experimental results on CIFAR10,[0],[0]
"We compare smoothed BGAN and M-BGAN, and the same network trained with spectral normalization (Miyato et al., 2018) (SN), and gradient penalty (Gulrajani et al., 2017) on both the Wasserstein (Arjovsky et al., 2017) (WGP) and the standard loss (GP).",5.2. Experimental results on CIFAR10,[0],[0]
"We also compare to a model using the batch-discrimination layer from (Salimans et al., 2016), adding a final batch discrimination layer to the architecture of (Miyato et al., 2018).",5.2. Experimental results on CIFAR10,[0],[0]
"All models are evaluated by reporting the Inception Score and the Fréchet Inception Distance (Heusel et al., 2017) and results are summarized in Table 2.",5.2. Experimental results on CIFAR10,[0],[0]
"Figure 4 displays sample images generated with our best model.
",5.2. Experimental results on CIFAR10,[0],[0]
Figure 5.2 highlights the training dynamics of each model3.,5.2. Experimental results on CIFAR10,[0],[0]
"On this architecture, M-BGAN heavily outperforms both batch discrimination and our other variants, and yields results similar to, or slightly better than (Miyato et al., 2018).",5.2. Experimental results on CIFAR10,[0],[0]
"Model trained with batch smoothing display results on par with batch discrimination, and much better than without batch smoothing.
",5.2. Experimental results on CIFAR10,[0],[0]
"3For readability, a slight smoothing is performed on the curves.",5.2. Experimental results on CIFAR10,[0],[0]
"To check the effect of the batch smoothing parameter γ on the loss, we plot the discriminator and generator losses of the network for different γ’s.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"The smaller the γ, the purer the batches.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
We would expect discriminator training to be more difficult with larger γ.,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
The results corroborate this insight (Fig. 2).,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
BGAN and M-BGAN behave similarly and we only report on BGAN in the figure.,5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"The discriminator loss is not directly affected by an increase in γ, but the generator loss is lower for larger γ, revealing the relative advantage of the generator on the discriminator.
",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"This suggests to increase γ if the discriminator dominates learning, and to decrease γ if the discriminator is stuck at a high value in spite of poor generated samples.",5.3. Effect of batch smoothing on the generator and discriminator losses,[0],[0]
"Finally, on the celebA face dataset, we adapt the simple architecture of (Miyato et al., 2018) to the increased resolution by adding a layer to both networks.",5.4. Qualitative results on celebA,[0],[0]
"For optimization we use Adam with β1 = 0, β2 = 0.9, α = 1e",5.4. Qualitative results on celebA,[0],[0]
"− 4, and ndisc = 1.",5.4. Qualitative results on celebA,[0],[0]
"Fig. 5 dislays BGAN samples with pure batches, and BGAN and M-BGAN samples with γ = .5.",5.4. Qualitative results on celebA,[0],[0]
The visual quality of the samples is reasonable; we believe that an improvement is visible from pure batches to M-BGAN.,5.4. Qualitative results on celebA,[0],[0]
"We introduced a method to feed batches of samples to the discriminator of a GAN in an principled way, based on two observations: feeding all-fake or all-genuine batches to a discriminator makes its task too easy; second, a simple architectural trick makes it possible to provably recover all functions of the batch as an unordered set.",6. Conclusion,[0],[0]
"Experimentally, this provides a new, alternative method to reduce mode dropping and reach good quantitative scores in GAN training.",6. Conclusion,[0],[0]
This work has been partially supported by the grant ANR-16CE23-0006,ACKNOWLEDGMENTS,[0],[0]
“Deep in France” and LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01).,ACKNOWLEDGMENTS,[0],[0]
Generative adversarial networks (GANs) are powerful generative models based on providing feedback to a generative network via a discriminator network.,abstractText,[0],[0]
"However, the discriminator usually assesses individual samples.",abstractText,[0],[0]
"This prevents the discriminator from accessing global distributional statistics of generated samples, and often leads to mode dropping: the generator models only part of the target distribution.",abstractText,[0],[0]
"We propose to feed the discriminator with mixed batches of true and fake samples, and train it to predict the ratio of true samples in the batch.",abstractText,[0],[0]
The latter score does not depend on the order of samples in a batch.,abstractText,[0],[0]
"Rather than learning this invariance, we introduce a generic permutation-invariant discriminator architecture.",abstractText,[0],[0]
This architecture is provably a universal approximator of all symmetric functions.,abstractText,[0],[0]
"Experimentally, our approach reduces mode collapse in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.",abstractText,[0],[0]
Mixed batches and symmetric discriminators for GAN training,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 438–443 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
438
Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance. Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success. In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account. The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios. Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1.",text,[0],[0]
Discourse parsing aims to identify the structure and relationship between different element discourse units (EDUs).,1 Introduction,[0],[0]
"As a fundamental topic in natural language processing, discourse parsing can assist many down-stream applications such as summarization (Louis et al., 2010), sentiment analysis (Polanyi and van den Berg, 2011) and question-answering (Ferrucci et al., 2010).",1 Introduction,[0],[0]
"However, the performance of discourse parsing is still far from perfect, especially for EDUs that are distant to each other in the discourse.",1 Introduction,[0],[0]
"In fact, as found in (Jia et al., 2018), the discourse parsing performance drops quickly as the dependency span increases.",1 Introduction,[0],[0]
"The reason may be twofold:
1Code for replicating our experiments is available at https://github.com/PKUYeYuan/ACL2018 CFDP.
",1 Introduction,[0],[0]
"Firstly, as discussed in previous works (Joty et al., 2013), it is important to address discourse structure characteristics, e.g., through modeling lexical chains in a discourse, for discourse parsing, especially in dealing with long span scenarios.",1 Introduction,[0],[0]
"However, most existing approaches mainly focus on studying the semantic and syntactic aspects of EDU pairs, in a more local view.",1 Introduction,[0],[0]
"Discourse cohesion reflects the syntactic or semantic relationship between words or phrases in a discourse, and, to some extent, can indicate the topic changing or threads in a discourse.",1 Introduction,[0],[0]
"Discourse cohesion includes five situations, including reference, substitution, ellipsis, conjunction and lexical cohesion (Halliday and Hasan, 1989).",1 Introduction,[0],[0]
"Here, lexical cohesion reflects the semantic relationship of words, and can be modeled as the recurrence of words, synonym and contextual words.
",1 Introduction,[0],[0]
"However, previous works do not well model the discourse cohesion within the discourse parsing task, or do not even take this issue into account.",1 Introduction,[0],[0]
"Morris and Hirst (1991) proposes to utilize Roget thesauri to form lexical chains (sequences of semantically related words that can reflect the topic shifts within a discourse), which are used to extract features to characterize discourse structures.",1 Introduction,[0],[0]
"(Joty et al., 2013) uses lexical chain feature to model multi-sentential relation.",1 Introduction,[0],[0]
"Actually, these simplified cohesion features can already improve parsing performance, especially in long spans.
",1 Introduction,[0],[0]
"Secondly, in modern neural network methods, modeling discourse cohesion as part of the networks is not a trivial task.",1 Introduction,[0],[0]
"One can still use off-the-shell tools to obtain lexical chains, but these tools can not be jointly optimized with the main neural network parser.",1 Introduction,[0],[0]
"We argue that characterizing discourse cohesion implicitly within a unified framework would be more
straightforward and effective for our neural network based parser.",1 Introduction,[0],[0]
"As shown in Figure 1, the 12 EDUs in the given discourse talk about different topics, marked with 3 different colors, which could be captured by a memory network that maintains several memory slots.",1 Introduction,[0],[0]
"In discourse parsing, such an architecture may help to cluster topically similar or related EDUs into the same memory slot, and each slot could be considered as a representation that maintains a specific topic or thread within the current discourse.",1 Introduction,[0],[0]
"Intuitively, we could also treat such a mechanism as a way to capture the cohesion characteristics of the discourse, just like the lexical chain features used in previous works, but without relying on external tools or resources.
",1 Introduction,[0],[0]
"In this paper, we investigate how to exploit discourse cohesion to improve discourse parsing.",1 Introduction,[0],[0]
Our contribution includes: 1) we design a memory network method to capture discourse cohesion implicitly in order to improve discourse parsing.,1 Introduction,[0],[0]
2),1 Introduction,[0],[0]
"We choose bidirectional long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) with an attention mechanism to represent EDUs directly from embeddings, and use simple position features to capture shallow discourse structures, without relying on off-the-shelf tools or resources.",1 Introduction,[0],[0]
Experiments on the RST corpus show that the memory based discourse cohesion model can help better capture discourse structure information and lead to significant improvement over traditional feature based discourse parsing methods.,1 Introduction,[0],[0]
"Our parser is an arc-eager style transition system (Nivre, 2003) with 2 stacks and a queue as shown in Figure 2, which is similar in spirit with (Dyer et al., 2015; Ballesteros et al., 2015).",2 Model overview,[0],[0]
"We follow the conventional data structures in transition-based dependency parsing, i.e., a queue (B) of EDUs to be processed, a stack (S) to store the partially constructed discourse trees, and a stack (A) to represent the history of transitions (actions combined with discourse relations).
",2 Model overview,[0],[0]
"In our parser, the transition actions include Shift, Reduce, Left-arc and Right-arc.",2 Model overview,[0],[0]
"At each step, the parser chooses to take one of the four actions and pushes the selected transition into A. Shift pushes the first EDU in queue B to the top of the stack S, while Reduce pops the top item of S. Left-arc connects the first EDU (head) in B to the top EDU (dependent) in S and then pops the top item of S, while Right-arc connects the top EDU (head) of S to the first EDU (dependent) in B and then pushes B’s first EDU to the top of S. A parse tree can be finally constructed until B is empty and S only contains a complete discourse tree.",2 Model overview,[0],[0]
"For more details, please refer to (Nivre, 2003).
",2 Model overview,[0],[0]
"As shown in Figure 2, at time t, we characterize the current parsing process by preserving the top two elements in B, top three elements in A and the root EDU in the partially constructed tree at the top of S. We first concatenate the embeddings of the preserved elements in each data structure to obtain the embeddings of S, B and A.",2 Model overview,[0],[0]
"We then append the three representations with the position2 features (introduced in Section 2.1), respectively.",2 Model overview,[0],[0]
"We pass them through one ReLU layer and two fully connected layers with ReLU as their activation functions to obtain the final state representation pt at time t, which will be used to determine the best transition to take at t.
Next, we apply an affine transformation to pt and feed it to a softmax layer to get the distribution over all possible decisions (actions combined with discourse relations).",2 Model overview,[0],[0]
"We train our model using the automatically generated oracle action sequences as the gold-standard annotations, and utilize cross entropy as the loss function.",2 Model overview,[0],[0]
"We perform greedy search during decoding.
...
...
...
l1
l2
Pt
Word
...
a1 a2 an
POS Position1
slotj
match{ weighted sum
ReLU
FC1(ReLU)
FC2(ReLU)
S B
A
...
...
wi
softmax
RA(Li)SH ...
sloti
...
wi
} match weighted sum
Bi-LSTM
RA(Li)
SH
（1）
（2） （2）
Position2
Memory network1
Memory network2
SRefined BRefined
...",2 Model overview,[0],[0]
"Bi-LSTM
...
Figure 2: Our discourse parsing framework: (1) Basic EDU representation module; (2) Memory networks to capture the discourse cohesion so as to obtain the refined representations of S and B. RA(Li)",2 Model overview,[0],[0]
means that the chosen action is Right-arc and its relation is List.,2 Model overview,[0],[0]
SH means Shift.,2 Model overview,[0],[0]
a1 to an are weights for the attention mechanism of the bidirectional LSTM.,2 Model overview,[0],[0]
"As mentioned in previous work (Jia et al., 2018), when the top EDUs in S and B are far from each other in the discourse, i.e., with a long span, the parser will be prone to making wrong decisions.",2.1 Discourse Structures,[0],[0]
"To deal with these long-span cases, one should take discourse structures into account, e.g., extracting features from the structure of a long discourse or analyzing and characterizing different topics discussed in the discourse.
",2.1 Discourse Structures,[0],[0]
"We, therefore, choose two kinds of position features to reflect the structure information, which can be viewed as a shallow form of discourse cohesion.",2.1 Discourse Structures,[0],[0]
"The first one describes the position of an EDU alone, while the second represents the spatial relationship between the top EDUs of S and B. (1) Position1: the positions of the EDU in the sentence, paragraph and discourse, respectively.",2.1 Discourse Structures,[0],[0]
"(2) Position2: whether the top EDUs of S and B are in the same sentence/paragraph or not, and the distance between them.",2.1 Discourse Structures,[0],[0]
"Basic EDU representation: In our model, the EDUs in both S and B follow the same representation method, and we take an EDU in B as an example as shown in Figure 2.",3 Memory based Discourse Cohesion,[0],[0]
"The basic representation for an EDU is built by concatenating three components, i.e., word, POS and Position1.",3 Memory based Discourse Cohesion,[0],[0]
"Regarding word, we feed the
sequence of words in the EDU to a bi-directional Long Short Term Memory (LSTM) with attention mechanism and obtain the final word representation by concatenating the two final outputs from both directions.",3 Memory based Discourse Cohesion,[0],[0]
"Here, we use pre-trained Glove (Pennington et al., 2014) as the word embeddings.",3 Memory based Discourse Cohesion,[0],[0]
"We get the POS tags from Stanford CoreNLP toolkit (Manning et al., 2014), and similarly, send the POS tag sequence of the EDU to a bi-directional LSTM with attention mechanism to obtain the final POS representation.",3 Memory based Discourse Cohesion,[0],[0]
"For concise, we omit the bi-directional LSTM network structure for POS in Figure 2, which is the same as the one for word.",3 Memory based Discourse Cohesion,[0],[0]
"The Position1 feature vectors are randomly initialized and we expect them to work as a proxy to capture the shallow discourse structure information.
",3 Memory based Discourse Cohesion,[0],[0]
"Memory Refined Representation: Besides the shallow structure features, we design a memory network component to cluster EDUs with similar topics to the same memory slot to alleviate the long span issues, as illustrated in Figure 1.",3 Memory based Discourse Cohesion,[0],[0]
"We expect these memory slots can work as lexical chains, which can maintain different threads within the discourse.",3 Memory based Discourse Cohesion,[0],[0]
"Such a memory mechanism has the advantage that it can perform the clustering automatically and does not rely on extra tools or resources to train.
",3 Memory based Discourse Cohesion,[0],[0]
"Concretely, we match the representations of S and B with their corresponding memory networks, respectively, to get their discourse cohesion clues, which are used to improve the original representations.",3 Memory based Discourse Cohesion,[0],[0]
"Take B as an example, we first compute the similarity between the representation of B (Vb) and each memory slot mi in B’s memory.",3 Memory based Discourse Cohesion,[0],[0]
"We adopt the cosine similarity as our metric as below:
Sim[x, y] = x · y ‖x‖ · ‖y‖
(1)
",3 Memory based Discourse Cohesion,[0],[0]
"Then, we use this cosine similarity to produce a normalized weight wi for each memory slot.",3 Memory based Discourse Cohesion,[0],[0]
"We introduce a strength factor λ to improve the focus.
",3 Memory based Discourse Cohesion,[0],[0]
"wi = exp(λSim[Vb,mi])∑ j exp(λSim[Vb,mj ])
(2)
",3 Memory based Discourse Cohesion,[0],[0]
"Finally, we get the discourse cohesion clue of B (denoted by BCoh) from its memory according to the weighted sum of mi.
BCoh = ∑ i wimi (3)
",3 Memory based Discourse Cohesion,[0],[0]
"We concatenateBCoh (the discourse cohesion clue of B) and the original embedding of B to get the refined representation Brefined for B. Similarly, we concatenate SCoh and the embedding of S to get the refined representation Srefined for S, as shown in Figure 2.",3 Memory based Discourse Cohesion,[0],[0]
"In our experiments, each memory contains 20 slots, which are randomly initialized and optimized during training.",3 Memory based Discourse Cohesion,[0],[0]
Dataset:,4 Evaluation and Results,[0],[0]
"We use the RST Discourse Treebank (Carlson et al., 2001) with the same split as in (Li et al., 2014), i.e., 312 for training, 30 for development and 38 for testing.",4 Evaluation and Results,[0],[0]
"We experiment with two set of relations, the 111 types of fine-grained relations and the 19 types of coarse-grained relations, respectively.
",4 Evaluation and Results,[0],[0]
Evaluation Metrics:,4 Evaluation and Results,[0],[0]
"In the Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), head is the core of a discourse, and a dependent gives supporting evidence to its head with certain relationship.",4 Evaluation and Results,[0],[0]
"We adopt unlabeled accuracy UAS (the ratio of EDUs that correctly identify their heads) and labeled accuracy LAS (the ratio of EDUs that have both correct heads and relations) as our evaluation metrics.
",4 Evaluation and Results,[0],[0]
"Baselines: We compare our method with the following baselines and models: (1) Perceptron: We re-implement the perceptron based arc-eager style dependency discourse parser as mentioned in (Jia et al., 2018) with coarse-grained relation.",4 Evaluation and Results,[0],[0]
"The Perceptron model chooses words, POS tags, positions and length features, totally 100 feature templates, with the early update strategy (Collins and Roark, 2004).",4 Evaluation and Results,[0],[0]
(2) Jia18:,4 Evaluation and Results,[0],[0]
"Jia et al. (2018) implement a transition-based discourse parser with stacked LSTM, where they choose a two-layer LSTM to represent EDUs by encoding four kinds of features including words, POS tags, positions and length features.",4 Evaluation and Results,[0],[0]
(3) Basic EDU representation (Basic): Our discourse parser with the basic EDU representation method mentioned in Section 3.,4 Evaluation and Results,[0],[0]
(4) Memory refined representation (Refined): Our full parser equipped with the basic EDU representation method and the memory networks to capture the discourse cohesion mentioned in Section 3.,4 Evaluation and Results,[0],[0]
"(5) MST-full (Li et al., 2014): a graph-based dependency discourse parser with carefully selected 6 sets of features including words, POS tags, positions,
length, syntactic and semantic similarity features, which achieves the state-of-art performance on the RST Treebank.",4 Evaluation and Results,[0],[0]
We list the overall discourse parsing performance in Table 1.,4.1 Results,[0],[0]
"Here, Jia18, a stack LSTM based method (Jia et al., 2018), outperforms the traditional Perceptron method, but falls behind our Basic model with word, POS tags and Position features.",4.1 Results,[0],[0]
"The reason may be that representing EDUs directly from the sequence of word/POS embeddings could probably capture the semantic meaning of EDUs, which is especially useful for taking into account synonyms or paraphrases that often confuse traditional feature-based methods.",4.1 Results,[0],[0]
"We can also see that Basic(word+pos+position) significantly outperforms Basic(word+pos), as the Position features may play a crucial role in providing useful structural clues to our parser.",4.1 Results,[0],[0]
"Such position information can also be considered as a shallow treatment to capture the discourse cohesion, especially for long span scenarios.",4.1 Results,[0],[0]
"When using the memory network, our Refined method achieves better performance than the Basic(word+pos+position) in both UAS and LAS.",4.1 Results,[0],[0]
"The reason may come from the ability of the memory networks in simulating the lexical chains within a discourse, where the memory networks can model the discourse cohesion so as to provide topical or structural clues to our parser.",4.1 Results,[0],[0]
"We use SIGF V2 (Padó, 2006) to perform significance test for the discussed models.",4.1 Results,[0],[0]
"We find that the Basic(word+pos+position) method significantly outperforms (Jia et al., 2018), and our Refined model performs significantly better than Basic(word+pos+position) (with p < 0.1).
",4.1 Results,[0],[0]
"However, when compared with MST-full (Li et al., 2014), our models still fall behind this state-of-the-art method.",4.1 Results,[0],[0]
"The main reason might be that MST-full follows a global graph-based dependency parsing framework, where their high order methods (in cubic time complexity) can directly analyze the relationship between any EDUs pairs in the discourse, while, we choose the transition-based local method with linear time complexity, which can only investigate the top EDUs in S and B according to the selected actions, thus usually has a lower performance than the global graph-based methods, but with a
lower (linear) time complexity.",4.1 Results,[0],[0]
"On the other hand, the neural network components help us maintain much fewer features than MST-full, which carefully selects 6 different sets of features that are usually obtained using extra tools and resources.",4.1 Results,[0],[0]
"And, the neural network design is flexible enough to incorporate various clues into a uniform framework, just like how we introduce the memory networks as a proxy to capture discourse cohesion.
",4.1 Results,[0],[0]
"In the RST corpus, when the distance between two EDUs is larger, there are usually fewer numbers of such EDU pairs, but the parsing performance for those long span cases drops more significantly.",4.1 Results,[0],[0]
"For example, the LAS is even lower than 5% for those dependencies that have a range of 6 EDUs.",4.1 Results,[0],[0]
We take a detailed look at the parsing performance for dependencies at different lengths (from 1 to 6 as an example) using coarse-grained relations.,4.1 Results,[0],[0]
"As shown in Table 2, compared with the Basic method, both UAS and LAS of the Refined method are improved significantly in almost all spans, where we observe more prominent improvement for the UAS in larger spans such as span 5 and span 6, with about 8.70% and 6.38%, respectively.
",4.1 Results,[0],[0]
"Finally, let us take a detailed comparison between Refined and Basic to investigate the advantages of capturing discourse cohesion.",4.1 Results,[0],[0]
"Note that, our Refined method wins Basic in almost all relations.",4.1 Results,[0],[0]
"Here, we discuss one typical relation List, which often indicates a long span
dependency between a pair of EDUs.",4.1 Results,[0],[0]
"In the test set of RST, the average span for List is 7.55, with the max span of 69.",4.1 Results,[0],[0]
"Our Refined can successfully identify 55 of them, with an average span of 9.02 and the largest one of 63, while, the Basic method can only identify 41 edges labeled with List, which are mostly shorter cases, with an average span of 1.32 and the largest one of 5.",4.1 Results,[0],[0]
"More detailedly, there are 18 edges that are correctly identified by our Refined but missed by the Basic method.",4.1 Results,[0],[0]
The average span of those dependencies is 25.39.,4.1 Results,[0],[0]
"It is easy to find that without further considerations in discourse structures, the Basic method has limited ability in correctly identifying longer span dependencies.",4.1 Results,[0],[0]
"And those comparisons prove again that our Refined can take better advantage of modeling discourse cohesion, which enables our model to perform better in long span scenarios.",4.1 Results,[0],[0]
"In this paper, we propose to utilize memory networks to model discourse cohesion automatically.",5 Conclusions,[0],[0]
"By doing so we could capture the topic change or threads within a discourse, which can further improve the discourse parsing performance, especially for long span scenarios.",5 Conclusions,[0],[0]
Experimental results on the RST Discourse Treebank show that our proposed method can characterize the discourse cohesion efficiently and archive significant improvement over traditional feature based discourse parsing methods.,5 Conclusions,[0],[0]
"We would like to thank our anonymous reviewers, Bingfeng Luo, and Sujian Li for their helpful comments and suggestions, which greatly improved our work.",Acknowledgments,[0],[0]
"This work is supported by National High Technology R&D Program of China (Grant No.2015AA015403), and Natural Science Foundation of China (Grant No. 61672057, 61672058).",Acknowledgments,[0],[0]
"For any correspondence, please contact Yansong Feng.",Acknowledgments,[0],[0]
Identifying long-span dependencies between discourse units is crucial to improve discourse parsing performance.,abstractText,[0],[0]
"Most existing approaches design sophisticated features or exploit various off-the-shelf tools, but achieve little success.",abstractText,[0],[0]
"In this paper, we propose a new transition-based discourse parser that makes use of memory networks to take discourse cohesion into account.",abstractText,[0],[0]
"The automatically captured discourse cohesion benefits discourse parsing, especially for long span scenarios.",abstractText,[0],[0]
"Experiments on the RST discourse treebank show that our method outperforms traditional featured based methods, and the memory based discourse cohesion can improve the overall parsing performance significantly 1.",abstractText,[0],[0]
Modeling Discourse Cohesion for Discourse Parsing via Memory Network,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4758–4765 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4758",text,[0],[0]
Over two decades after the seminal work by Picard (1997),1 Introduction,[0],[0]
"the quest of Affective Computing, to ease the interaction with computers by giving them a sense of how emotions shape our perception and behavior, is still far from being fulfilled.",1 Introduction,[0],[0]
"Undoubtedly, major progress has been made in NLP, with sentiment analysis being one of the most vivid and productive areas in recent years (Liu, 2015).
",1 Introduction,[0],[0]
"However, the vast majority of contributions has focused on polarity prediction, typically only distinguishing between positive and negative feeling
*",1 Introduction,[0],[0]
These authors contributed equally to this work.,1 Introduction,[0],[0]
"Anneke Buffone designed and supervised the crowdsourcing task and the survey described in Section 2, and provided psychological background knowledge.",1 Introduction,[0],[0]
"Sven Buechel was responsible for corpus creation, data analysis, and modeling.",1 Introduction,[0],[0]
"The technical set-up of the crowdsourcing task and the survey was done jointly by both first authors.
",1 Introduction,[0],[0]
"†Work conducted while being at the University of Pennsylvania.
or evaluation, usually in social media postings or product reviews (Rosenthal et al., 2017; Socher et al., 2013).",1 Introduction,[0],[0]
"Only very recently, researchers started exploring more sophisticated models of human emotion on a larger scale (Wang et al., 2016; Abdul-Mageed and Ungar, 2017; Mohammad and Bravo-Marquez, 2017a; Buechel and Hahn, 2017, 2018a,b).",1 Introduction,[0],[0]
"Yet such approaches, often rooted in psychological theory, also turned out to be more challenging in respect to annotation and modeling (Strapparava and Mihalcea, 2007).
",1 Introduction,[0],[0]
"Surprisingly, one of the most valuable affective phenomena for improving human-machine interaction has received surprisingly little attention: Empathy.",1 Introduction,[0],[0]
"Prior work focused mostly on spoken dialogue, commonly addressing conversational agents, psychological interventions, or call center applications (McQuiggan and Lester, 2007; Fung et al., 2016; Pérez-Rosas et al., 2017; Alam et al., 2017).
",1 Introduction,[0],[0]
"In contrast, to the best of our knowledge, only three contributions (Xiao et al., 2012; Gibson et al., 2015; Khanpour et al., 2017) previously addressed text-based empathy prediction1 (see Section 4 for details).",1 Introduction,[0],[0]
"Yet, all of them are limited in three ways: (a) neither of their corpora are available leaving the NLP community without shared data, (b) empathy ratings were provided by others than the one actually experiencing it which qualifies only as a weak form of ground truth, and (c) their notion of empathy is quite basic, falling short of current and past theory.
",1 Introduction,[0],[0]
1 Psychological studies commonly distinguish between state and trait empathy.,1 Introduction,[0],[0]
"While the former construct describes the amount of empathy a person experiences as a direct result of encountering a given stimulus, the latter refers to how empathetic one is on average and across situations.",1 Introduction,[0],[0]
This studies exclusively addresses state empathy.,1 Introduction,[0],[0]
"For a contribution addressing trait empathy from an NLP perspective, see AbdulMageed et al. (2017).
",1 Introduction,[0],[0]
In this contribution we present the first publicly available gold standard for text-based empathy prediction.,1 Introduction,[0],[0]
It is constructed using a novel annotation methodology which reliably captures empathy assessments via multi-item scales.,1 Introduction,[0],[0]
"The corpus as well as our work as a whole is also unique in being—to the best of our knowledge—the first computational approach differentiating multiple types of empathy, empathic concern and personal distress, a distinction well recognized throughout psychology and other disciplines.2",1 Introduction,[0],[0]
Background.,2 Corpus Design and Methodology,[0],[0]
Most psychological theories of empathic states are focused on reactions to negative rather than positive events.,2 Corpus Design and Methodology,[0],[0]
"Empathy for positive events remains less well understood and is thought to be regulated differently (Morelli et al., 2015).",2 Corpus Design and Methodology,[0],[0]
Thus we focus on empathetic reactions to need or suffering.,2 Corpus Design and Methodology,[0],[0]
"Despite the fact that everyone has an immediate, implicit understanding of empathy, research has been vastly inconsistent in its definition and operationalization (Cuff et al., 2016).",2 Corpus Design and Methodology,[0],[0]
"There is agreement, however, that there are multiple forms of empathy (see below).",2 Corpus Design and Methodology,[0],[0]
"The by far most widely cited state empathy scale is Batson’s Empathic Concern – Personal Distress Scale (Batson et al., 1987), henceforth empathy and distress.
",2 Corpus Design and Methodology,[0],[0]
"Distress is a self-focused, negative affective state that occurs when one feels upset due to witnessing an entity’s suffering or need, potentially via “catching” the suffering target’s negative emotions.",2 Corpus Design and Methodology,[0],[0]
"Empathy is a warm, tender, and compassionate feeling for a suffering target.",2 Corpus Design and Methodology,[0],[0]
"It is other-focused, retains self-other separation, and is marked by relatively more positive affect (Batson and Shaw, 1991; Goetz et al., 2010; Mikulincer and Shaver, 2010; Sober and Wilson, 1997).
",2 Corpus Design and Methodology,[0],[0]
Selection of News Stories.,2 Corpus Design and Methodology,[0],[0]
"Two research interns (psychology undergraduates) collected a total of 418 articles from popular online news platforms, selected to likely evoke empathic reactions, after being briefed on the goal and background of this study.",2 Corpus Design and Methodology,[0],[0]
"These articles were then used to elicit empathic responses in participants.
",2 Corpus Design and Methodology,[0],[0]
Acquiring Text and Ratings.,2 Corpus Design and Methodology,[0],[0]
The corpus acquisition was set up as a crowdsourcing task on MTurk.com pointing to a Qualtrics.com questionnaire.,2 Corpus Design and Methodology,[0],[0]
"The participants completed back-
2Data and code are available at: https://github. com/wwbp/empathic_reactions
ground measures on demographics and personality, and then proceeded to the main part of the survey where they read a random selection of five of the news articles.",2 Corpus Design and Methodology,[0],[0]
"After reading each of the articles, participants were asked to rate their level of empathy and distress before describing their thoughts and feelings about it in writing.
",2 Corpus Design and Methodology,[0],[0]
"In contrast to previous work, this set-up allowed us to acquire empathy scores of the actual writer of a text, instead of having to rely on an external evaluation by third parties (often student assistants with background in computer science).",2 Corpus Design and Methodology,[0],[0]
"Arguably, our proposed annotation methodology yields more appropriate gold data, yet also leads to more variance in the relationship between linguistic features and empathic state ratings.",2 Corpus Design and Methodology,[0],[0]
That is because each rating reflects a single individual’s feelings rather than a more stable average assessment by multiple raters.,2 Corpus Design and Methodology,[0],[0]
"To account for this, we use multi-item scales as is common practice in psychology.",2 Corpus Design and Methodology,[0],[0]
"I.e., participants give ratings for multiple items measuring the same construct (e.g., empathy) which are then averaged to obtain more reliable results.",2 Corpus Design and Methodology,[0],[0]
"As far as we know, this is the first time that multiitem scales are used in sentiment analysis.3
In our case, participants used Batson’s Empathic Concern – Personal Distress Scale (see above), i.e, rating 6 items for empathy (e.g., warm, tender, moved) and 8 items for distress (e.g., troubled, disturbed, alarmed) using a 7-point scale for each of those (see Appendix for details).",2 Corpus Design and Methodology,[0],[0]
"After rating their empathy, participants were asked to share their feelings about the article as they would with a friend in a private message or with a group of friends as a social media post in 300 to 800 characters.",2 Corpus Design and Methodology,[0],[0]
"Our final gold standard consists of these messages combined with the numeric ratings for empathy and distress.
",2 Corpus Design and Methodology,[0],[0]
"In sum, 403 participants completed the survey.",2 Corpus Design and Methodology,[0],[0]
"Median completion time was 32 minutes and each participant received 4 USD as compensation.
",2 Corpus Design and Methodology,[0],[0]
Post-Processing.,2 Corpus Design and Methodology,[0],[0]
Each message was manually reviewed by the authors.,2 Corpus Design and Methodology,[0],[0]
"Responses which deviated from the task description (e.g., mere copying from the articles at display) were removed (31 responses, 155 messages), leading to a total 1860 messages in our final corpus.",2 Corpus Design and Methodology,[0],[0]
"Gold ratings for empathy and distress were derived by averaging the respective items of the two multi-item scales.
",2 Corpus Design and Methodology,[0],[0]
"3 Here, we use sentiment as an umbrella term subsuming semantic orientation, emotion, as well as highly related concepts such as empathy.",2 Corpus Design and Methodology,[0],[0]
"For a first impression of the language of our new gold standard, we provide illustrative examples in Table 1.",3 Corpus Analysis,[0],[0]
"The participant in Example (1) displays higher empathy than distress, (2) displays higher distress than empathy, and (3) shows neither empathic state, but employs sarcasm, colloquialisms and social-media-style acronyms to express lack of emotional response to the article.",3 Corpus Analysis,[0],[0]
"As can be seen, the language of our corpus is diverse and authentic, featuring many phenomena of natural language which render its computational understanding difficult, thus constituting a sound but challenging gold standard for empathy prediction.
",3 Corpus Analysis,[0],[0]
Token Counts.,3 Corpus Analysis,[0],[0]
"We tokenized the 1860 messages using NLTK tools (Bird, 2006).",3 Corpus Analysis,[0],[0]
"In total, our corpus amounts to 173, 686 tokens.",3 Corpus Analysis,[0],[0]
"Individual message length varies between 52 and 198 tokens, the median being 84.",3 Corpus Analysis,[0],[0]
"See Appendix for details.
",3 Corpus Analysis,[0],[0]
Rating Distribution.,3 Corpus Analysis,[0],[0]
"Figure 1 displays the bivariate distribution of empathy and distress rat-
ings.",3 Corpus Analysis,[0],[0]
"As can be seen both target variables have a clear linear dependence, yet show only a moderate Pearson correlation of r=.451, similar to what was found in prior research (Batson et al., 1987, 1997).",3 Corpus Analysis,[0],[0]
"This finding supports that the two scales capture distinct affective phenomena and underscores the importance of our decision to describe empathic states in terms of multiple target variables, constituting a clear advancement over previous work.",3 Corpus Analysis,[0],[0]
"Both kinds of ratings show good coverage over the full range of the scales.
",3 Corpus Analysis,[0],[0]
Reliability of Ratings.,3 Corpus Analysis,[0],[0]
"Since each message is annotated by only one rater, its author, typical measures of inter-rater agreement are not applicable.",3 Corpus Analysis,[0],[0]
"Instead, we compute split-half reliability (SHR), a standard approach in psychology (Cronbach, 1947) which also becomes increasingly popular in sentiment analysis (Mohammad and BravoMarquez, 2017a; Buechel and Hahn, 2018a).",3 Corpus Analysis,[0],[0]
"SHR is computed by splitting the ratings for the individual scale items (e.g., warm, tender, etc. for empathy) of all participants randomly into two groups, averaging the individual item ratings for each group and participant, and then measuring the correlation between both groups.",3 Corpus Analysis,[0],[0]
"This process is repeated 100 times with random splits, before again averaging the results.",3 Corpus Analysis,[0],[0]
"Doing so for empathy and distress, we find very high4 SHR values of r=.875 and .924, respectively.",3 Corpus Analysis,[0],[0]
"In this section, we provide experimental results for modeling empathy and distress ratings based on the participants’ messages (see Section 2).",4 Modeling Empathy and Distress,[0],[0]
"We examine three different types of models, varying in
4 For a comparison against previously reported SHR values for different emotional categories, see Mohammad and Bravo-Marquez (2017b).
design complexity.",4 Modeling Empathy and Distress,[0],[0]
"Distinct models were trained for empathy and distress prediction.
",4 Modeling Empathy and Distress,[0],[0]
"First, ten percent of our newly created gold standard were randomly sampled to be used in development experiments.",4 Modeling Empathy and Distress,[0],[0]
"Then, the main experiment was conducted using 10-fold crossvalidation (CV), providing each model with identical train-test splits to increase reliability.",4 Modeling Empathy and Distress,[0],[0]
"The dev set was excluded for the CV experiment.
",4 Modeling Empathy and Distress,[0],[0]
Model performance is measured in terms of Pearson correlation r between predicted values and the human gold ratings.,4 Modeling Empathy and Distress,[0],[0]
"Thus, we phrase the prediction of empathy and distress as regression problems.
",4 Modeling Empathy and Distress,[0],[0]
"The input to our models is based on word embeddings, namely the publicly available FastText embeddings which were trained on Common Crawl (≈600B tokens) (Bojanowski et al., 2017; Mikolov et al., 2018).
",4 Modeling Empathy and Distress,[0],[0]
Ridge.,4 Modeling Empathy and Distress,[0],[0]
"Our first approach is Ridge regression, an `2-regularized version of linear regression.",4 Modeling Empathy and Distress,[0],[0]
The centroid of the word embeddings of the words in a message is used as features (embedding centroid).,4 Modeling Empathy and Distress,[0],[0]
"The regularization coefficient α is automatically chosen from {1, .5, .1, ..., .0001} during training.
FFN.",4 Modeling Empathy and Distress,[0],[0]
"Our second approach is a Feed-Forward Net with two hidden layers (256 and 128 units, respectively) with ReLU activation.",4 Modeling Empathy and Distress,[0],[0]
"Again, the embedding centroid is used as features.
CNN.",4 Modeling Empathy and Distress,[0],[0]
"The last approach is a Convolutional Neural Net.5 We use a single convolutional layer with filter sizes 1 to 3, each with 100 output channels, followed by an average pooling layer and a dense layer of 128 units.",4 Modeling Empathy and Distress,[0],[0]
"ReLUs were used for the convolutional and again for the dense layer.
",4 Modeling Empathy and Distress,[0],[0]
"Both deep learning models were trained using the Adam optimizer (Kingma and Ba, 2015) with a fixed learning rate of 10−3 and a batch size of 32.",4 Modeling Empathy and Distress,[0],[0]
We trained for a maximum of 200 epochs yet applied early stopping if the performance on the validation set did not improve for 20 consecutive epochs.,4 Modeling Empathy and Distress,[0],[0]
"We applied dropout with probabilities of .2, .5",4 Modeling Empathy and Distress,[0],[0]
and .5,4 Modeling Empathy and Distress,[0],[0]
"on input, dense and pooling layers, respectively.",4 Modeling Empathy and Distress,[0],[0]
Moreover `2 regularization of .001 was applied to the weights of conv and dense layers.,4 Modeling Empathy and Distress,[0],[0]
"Word embeddings were not updated.
",4 Modeling Empathy and Distress,[0],[0]
The results are provided in Table 2.,4 Modeling Empathy and Distress,[0],[0]
"As can be seen, all of our models achieve satisfying performance figures ranging between r=.379 and .444,
5 Recurrent models did not perform well during development due to high sequence length.
",4 Modeling Empathy and Distress,[0],[0]
given the assumed difficulty of the task (see Section 3).,4 Modeling Empathy and Distress,[0],[0]
"On average over the two target variables, the CNN performs best, followed by Ridge and the FFN.",4 Modeling Empathy and Distress,[0],[0]
"While the CNN significantly outperforms the other models in every case, the differences between Ridge and the FFN are not statistically significant for either empathy or distress.6",4 Modeling Empathy and Distress,[0],[0]
The improvements of the CNN over the other two approaches are much more pronounced for distress than for empathy.,4 Modeling Empathy and Distress,[0],[0]
"Since only the CNN is able to capture semantic effects from composition and word order, our data suggest that these phenomena are more important for predicting distress, whereas lexical features alone already perform quite well for empathy.
",4 Modeling Empathy and Distress,[0],[0]
Discussion.,4 Modeling Empathy and Distress,[0],[0]
"In comparison to closely related tasks such as emotion prediction (Mohammad and Bravo-Marquez, 2017a)",4 Modeling Empathy and Distress,[0],[0]
our performance figures for empathy and distress prediction are generally lower.,4 Modeling Empathy and Distress,[0],[0]
"However, given the small amount of previous work for the problem at hand, we argue that our results are actually quite strong.",4 Modeling Empathy and Distress,[0],[0]
"This becomes obvious, again, in comparison with emotion analysis where early work achieved correlation values around r=.3 at most (Strapparava and Mihalcea, 2007).",4 Modeling Empathy and Distress,[0],[0]
"Yet state-of-the-art performance literally doubled over the last decade (Beck, 2017), in part due to much larger training sets.
",4 Modeling Empathy and Distress,[0],[0]
"Comparison to the limited body of previous work in text-based empathy prediction is difficult for a number of reasons, e.g., differences in domain, evaluation metric, as well as methodology and linguistic level of annotation.",4 Modeling Empathy and Distress,[0],[0]
"Khanpour et al. (2017) annotate and model empathy in online health communities on the sentence-level, whereas the instances in our corpus are much longer and comprise multiple sentences.",4 Modeling Empathy and Distress,[0],[0]
"In contrast to our work, they treat empathy prediction as a classification problem.",4 Modeling Empathy and Distress,[0],[0]
"Their best performing model, a CNN-LSTM, achieves an F-score of .78.",4 Modeling Empathy and Distress,[0],[0]
"Gibson
6We use a two-tailed t-test for paired samples based on the results of the individual CV runs; p < .05.
et al. (2015) predict therapists’ empathy in motivational interviews.",4 Modeling Empathy and Distress,[0],[0]
Each therapy session transcript received one numeric score.,4 Modeling Empathy and Distress,[0],[0]
"Thus, each prediction is based on much more language data than our individual messages comprise.",4 Modeling Empathy and Distress,[0],[0]
"Their best model achieves a Spearman rank correlation of .61 using n-gram and psycholinguistic features.
",4 Modeling Empathy and Distress,[0],[0]
"Our contribution goes beyond both of these studies by, first, enriching empathy prediction with personal distress and, second, by annotating and modeling the empathic state actually felt by the writer, instead of relying on external assessments.",4 Modeling Empathy and Distress,[0],[0]
"This contribution was the first to attempt empathy prediction in terms of multiple target variables, empathic concern and personal distress.",5 Conclusion,[0],[0]
"We proposed a novel annotation methodology capturing empathic states actually felt by the author of a statement, instead of relying on third-party assessments.",5 Conclusion,[0],[0]
"To ensure high reliability in this singlerating setting, we employ multi-item scales in line with best practices in psychology.",5 Conclusion,[0],[0]
"Hereby we create the first publicly available gold standard for empathy prediction in written language, our survey being set-up and supervised by an expert psychologist.",5 Conclusion,[0],[0]
"Our analysis shows that the data set excels with high rating reliability and an authentic and diverse language, rich of challenging phenomena such as sarcasm.",5 Conclusion,[0],[0]
"We provide experimental results for three different predictive models, our CNN turning out superior.",5 Conclusion,[0],[0]
"Sven Buechel would like to thank his doctoral advisor Udo Hahn, JULIE Lab, for funding his research visit at the University of Pennsylvania.",Acknowledgments,[0],[0]
"Details on Stimulus and Instructions
Before being used in our survey, the selected news articles were categorized by the research interns who gathered them in terms of their intensity of suffering (major or minor), cause of suffering (political, human, nature or other), patient of suffering (humans, animals, environment, or other) and scale of suffering (individual or mass).",A Supplemental Material,[0],[0]
Research interns also provided a short list of key words for each article.,A Supplemental Material,[0],[0]
"This additional information was gathered to examine the influence of these factors on empathy elicitation and modeling performance in later studies.
",A Supplemental Material,[0],[0]
"At the beginning of the survey participants completed background items covering general demographics (including age, gender, and ethnicity), the most commonly used trait empathy scale, the Interpersonal Reactivity Index (Davis, 1980), a brief assessment of the Big 5 personality traits (Gosling et al., 2003), life satisfaction (Diener et al., 1985), as well as a brief measure of generalized trust.
",A Supplemental Material,[0],[0]
"After reading each of the articles, participants rated their level of empathic concern and personal distress using multi-item scales.",A Supplemental Material,[0],[0]
"Figure 2
shows a cropped screenshot of the survey hosted on Qualtrics.com.",A Supplemental Material,[0],[0]
"The first six items (warm, tender, sympathetic, softhearted, moved, and compassionate) refer to empathy.",A Supplemental Material,[0],[0]
"The last eight items (worried, upset, troubled, perturbed, grieved, disturbed, alarmed, and distressed) refer to distress.
",A Supplemental Material,[0],[0]
"After completing the rating items, participants were instructed to describe their reactions in writing as follows: Now that you have read this article, please write a message to a friend or friends about your feelings and thoughts regarding the article you just read.",A Supplemental Material,[0],[0]
This could be a private message to a friend or something you would post on social media.,A Supplemental Material,[0],[0]
Please do not identify your intended friend(s) — just write your thoughts about the article as if you were communicating with them.,A Supplemental Material,[0],[0]
"Please use between 300 and 800 characters.
",A Supplemental Material,[0],[0]
"Further Corpus Analyses
The word clouds in Figure 3 and Figure 4 show 1- grams of our corpus which correlate significantly (Benjamini-Hochberg corrected p < .05) with high empathy and high distress ratings, respectively.",A Supplemental Material,[0],[0]
"In the word clouds, larger size indicates higher correlation and the color scale, gray-bluered, indicates word frequency, dark red being most prevalent.",A Supplemental Material,[0],[0]
"The Differential Language Analysis Toolkit (Schwartz et al., 2017) was utilized for this analysis.",A Supplemental Material,[0],[0]
"As can be seen, the word clouds display high face-validity, giving further evidence for the soundness of our acquisition methodology.
",A Supplemental Material,[0],[0]
Figure 5 displays the distribution of the message length of our corpus in tokens.,A Supplemental Material,[0],[0]
As can be seen the majority of messages contain between 60 and 100 tokens.,A Supplemental Material,[0],[0]
Yet outliers go up to almost 200.,A Supplemental Material,[0],[0]
The introduction of a character cap for the writing task proved successful in comparison to a pilot study where this measure has not been in place.,A Supplemental Material,[0],[0]
"In the latter case, the maximum number of tokens was nearly twice as high due to even stronger outliers.",A Supplemental Material,[0],[0]
Computational detection and understanding of empathy is an important factor in advancing human-computer interaction.,abstractText,[0],[0]
"Yet to date, textbased empathy prediction has the following major limitations: It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus.",abstractText,[0],[0]
"In contrast, this contribution presents the first publicly available gold standard for empathy prediction.",abstractText,[0],[0]
It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using multiitem scales.,abstractText,[0],[0]
"This is also the first computational work distinguishing between multiple forms of empathy, empathic concern, and personal distress, as recognized throughout psychology.",abstractText,[0],[0]
"Finally, we present experimental results for three different predictive models, of which a CNN performs the best.",abstractText,[0],[0]
Modeling Empathy and Distress in Reaction to News Stories,title,[0],[0]
"Proceedings of the SIGDIAL 2018 Conference, pages 20–31, Melbourne, Australia, 12-14 July 2018. c©2018 Association for Computational Linguistics
20",text,[0],[0]
"Every person is unique, yet they often share general patterns of behavior.",1 Introduction,[0],[0]
"Theories of personality aim to explain these patterns in terms of personality traits, e.g. the Big Five traits of extraversion or agreeableness.",1 Introduction,[0],[0]
"Previous work has shown: (1) the language that people generate includes linguistic features that express these personality traits; (2) it is possible to train models to automatically recognize a person’s personality from his language; and (3) it is possible to automatically train models for natural language generation that express personality traits (Pennebaker and King, 1999; Mairesse et al., 2007; Mairesse and Walker, 2011; Gill et al., 2012).
",1 Introduction,[0],[0]
"A distinct line of work has shown that people adapt to one another’s conversational behaviors and that conversants reliably re-use or mimic many
different aspects of their partner’s verbal and nonverbal behaviors, including lexical and syntactical traits, accent, speech rate, pause length, etc.",1 Introduction,[0],[0]
"(Coupland et al., 1988; Willemyns et al., 1997; Brennan and Clark, 1996; Branigan et al., 2010; Coupland et al., 1988; Parent and Eskenazi, 2010; Reitter et al., 2006a; Chartrand and Bargh, 1999; Hu et al., 2014).",1 Introduction,[0],[0]
"Previous work primarily focuses on developing methods on measuring adaptation in dialog, and studies have shown that adaptation measures are correlated with task success (Reitter and Moore, 2007), and that social variables such as power affect adaptation (Danescu-Niculescu-Mizil et al., 2012).
",1 Introduction,[0],[0]
We posit that it is crucial to enable adaptation in computer agents in order to make them more human-like.,1 Introduction,[0],[0]
"However, we need models to control the amount of adaptation in natural language generation.",1 Introduction,[0],[0]
"A primary challenge is that dialogs exhibit many different types of linguistic features, any or all of which, in principle, could be adapted.",1 Introduction,[0],[0]
"Previous work has often focused on individual features when measuring adaptation, and referring expressions have often been the focus, but the conversants in the dialog in Figure 1 from the ArtWalk Corpus appear to be adapting to the discourse marker okay in D98 and F98, the hedge kinda like in F100, and to the adjectival phrase like a vase in D101.
",1 Introduction,[0],[0]
"Therefore we propose a novel adaptation measure, Dialog Adaptation Score (DAS), which can model adaptation on any subset of linguistic features and can be applied on a turn by turn basis to any segment of dialog.",1 Introduction,[0],[0]
"Consider the example shown in Table 1, where the context (prime) is taken from an actual dialog.",1 Introduction,[0],[0]
"A response (target) with no adaptation makes the utterance stiff (DAS = 0), and too much adaptation (to all four discourse markers in prime, DAS = 1) makes the utterance unnatural.",1 Introduction,[0],[0]
"Our hypothesis is that we can learn models to approximate the appropriate amount of adaptation from the actual human response to the context (to discourse marker “okay”, DAS = 0.25).
",1 Introduction,[0],[0]
Conversants in dialogs express their own personality and adapt to their dialog partners simultaneously.,1 Introduction,[0],[0]
Our measure of adaptation produces models for adaptive natural language generation (NLG) for dialogs that integrates the predictions of both personality theories and adaptation theories.,1 Introduction,[0],[0]
"NLGs need to operate as a dialog unfolds on a turnby-turn basis, thus the requirements for a model of adaptation for NLG are different than simply measuring adaptation.
",1 Introduction,[0],[0]
Context:,1 Introduction,[0],[0]
okay alright,1 Introduction,[0],[0]
so,1 Introduction,[0],[0]
yeah,1 Introduction,[0],[0]
Im looking at 123 Locust right now Linguistic Features:,1 Introduction,[0],[0]
Discourse markers:,1 Introduction,[0],[0]
"okay, alright, so, yeah",1 Introduction,[0],[0]
"Referring expressions: 123 Locust Syntactic structures: VP->VBP+VP, VP->VBG+PP+ADVB ...
",1 Introduction,[0],[0]
We apply our method to multiple corpora to investigate how the dialog situation and speaker roles affect the level and type of adaptation to the other speaker.,1 Introduction,[0],[0]
"We show that:
• Different feature sets and conversational situations can have different adaptation models; • Speakers usually adapt more when they have
the initiative; • The degree of adaptation may vary over the
course of a dialog, and decreases as the adaptation window size increases.",1 Introduction,[0],[0]
Our goal is an algorithm for adaptive natural language generation (NLG) that controls the system output at each step of the dialog.,2 Method and Overview,[0],[0]
Our first aim therefore is a measure of dialog adaptation that can be applied on a turn by turn basis as a dialog unfolds.,2 Method and Overview,[0],[0]
"For this purpose, previous measures of dialog adaptation (Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2011) have two limitations: (1) their calculation require the complete dialog, and (2) they focus on single features and do not provide a model to control the interaction of multiple parameters in a single output, while our method measures adaptation with respect to any set of features.",2 Method and Overview,[0.9540640477404443],"['Then in Section 4.2, we provide Algorithm 2 that searches for the optimal smoothness by spawning O(log⇤) instances of Algorithm 1 with a carefully designed sequence of smoothness parameters (⌫, ⇢) as arguments.']"
"We further compare our method to existing measures in Section 6.
",2 Method and Overview,[0],[0]
"Measures of adaptation focus on prime-target pairs: (p, t), in which the prime contains linguistic features that the target may adapt to.",2 Method and Overview,[0],[0]
"While linguistic adaptation occur beyond the next turn, we simplify the calculation by using a window size of 1 for most experiments: for every utterance in the dialog (prime), we consider the next utterance by a different speaker as the target, if any.",2 Method and Overview,[0],[0]
We show the decay of adaptation with increasing window size in a separate experiment.,2 Method and Overview,[0],[0]
"When generating (p, t) pairs, it is possible to consider only speaker A adapting to speaker B (target=A), only speaker B adapting to speaker A (target=B), or both at the same time (target=Both).",2 Method and Overview,[0],[0]
"In the following definition, FCi(p) is the count of features in prime p of the i-th (p, t) pair, n is the total number of prime-target pairs in which FCi(p) 6= 0, similarly, FCi(p ∧ t) is the count of features in both prime p and target t.",2 Method and Overview,[0],[0]
"We define Dialog Adaptation Score (DAS) as:
DAS = 1
n n∑ i=1",2 Method and Overview,[0],[0]
"FCi(p ∧ t) FCi(p)
",2 Method and Overview,[0],[0]
"Within a feature set, DAS reflects the average probability that features in prime are adapted in target across all prime-target pairs in a dialog.",2 Method and Overview,[0],[0]
"Thus our Dialog Adaptation Score (DAS) models adaptation with respect to feature sets, providing a wholedialog adaptation model or a turn-by-turn adaptation model.",2 Method and Overview,[0],[0]
"The strength of DAS is the ability to model different classes of features related to individual differences such as personalities or social variables of interest such as status.
",2 Method and Overview,[0],[0]
DAS scores measured using various feature sets can be used as a vector model to control adaptation in Natural Language Generation (NLG).,2 Method and Overview,[0],[0]
"Although
we leave the application of DAS to NLG to future work, here we describe how we expect to use it.",2 Method and Overview,[0],[0]
"We consider the use of DAS with three NLG architectures: Overgeneration and Rank, Statistical Parameterized NLG, and Neural NLG.",2 Method and Overview,[0],[0]
Overgenerate and Rank.,2 Method and Overview,[0],[0]
"In this approach, different modules propose a possibly large set of next utterances in parallel, which are then fed to a (trained) ranker that outputs the top-ranked utterance.",2 Method and Overview,[0],[0]
"Previous work on adaptation/alignment in NLG has made use of this architecture (Brockmann, 2009; Buschmeier et al., 2010).",2 Method and Overview,[0],[0]
We can rank the generated responses based on the distances between their DAS vectors and learned DAS adaptation model.,2 Method and Overview,[0],[0]
The response with the smallest distance is the response with the best amount of adaptation.,2 Method and Overview,[0],[0]
We can also emphasize specific feature sets by giving weights to different dimensions of the vector and calculating weighted distance.,2 Method and Overview,[0],[0]
"For instance, in order to adapt more to personality and avoid too much lexical mimicry, one could prioritize related LIWC features, and adapt by using words from the same LIWC categories.",2 Method and Overview,[0],[0]
Statistical Parameterized NLG.,2 Method and Overview,[0],[0]
"Some NLG engines provide a list of parameters that can be controlled at generation time (Paiva and Evans, 2004; Lin and Walker, 2017).",2 Method and Overview,[0],[0]
DAS scores can be used as generation decision probabilities.,2 Method and Overview,[0],[0]
A DAS score of 0.48 for the LIWC feature set indicates that the probability of adapting to LIWC features in discourse context (prime) is 0.48.,2 Method and Overview,[0],[0]
"By mapping DAS scores to generation parameters, the generator could be directly controlled to exhibit the correct amount of adaptation for any feature set.",2 Method and Overview,[0],[0]
Neural NLG.,2 Method and Overview,[0],[0]
"Recent work in Neural NLG (NNLG) explores controlling stylistic variation in outputs using a vector to encode style parameters, possibly in combination with the use of a context vector to represent the dialog context (Ficler and Goldberg, 2017; Oraby et al., 2018).",2 Method and Overview,[0],[0]
The vector based probabilities that are represented in the DAS adaptation model could be encoded into the context vector in NNLG.,2 Method and Overview,[0],[0]
"No other known adaptation measures could be used in this way.
",2 Method and Overview,[0],[0]
"We hypothesize that different conversational contexts may lead to more or less adaptive behavior, so we apply DAS on four human-human dialog corpora: two task-oriented dialog corpora that were designed to elicit adaptation (ArtWalk and Walking Around), one topic-centric spontaneous dialog corpus (Switchboard), and the MapTask Corpus used in much previous work.",2 Method and Overview,[0],[0]
"We obtain linguistic
features using fully automatic annotation tools, described in Section 4.",2 Method and Overview,[0],[0]
We learn models of adaptation from these dialogs on various feature sets.,2 Method and Overview,[0],[0]
We first validate the DAS measure by showing that DAS distinguishes original dialogs from dialogs where the orders of the turns have been randomized.,2 Method and Overview,[0],[0]
We then show how DAS varies as a function of the feature sets used and the dialog corpora.,2 Method and Overview,[0],[0]
"We also show how DAS can be used for fine-grained adaptation by applying DAS to individual dialog segments, and individual speakers, and illustrating the differences in adaptation as a function of these variables.",2 Method and Overview,[0],[0]
"Finally, we show how DAS scores decrease as the adaptation window size increases.",2 Method and Overview,[0],[0]
We develop models of adaptation using DAS on the following four corpora.,3 Corpora,[0],[0]
"ArtWalk Corpus (AWC).1 Figure 1 provides a sample of the Artwalk Corpus (Liu et al., 2016), a collection of mobile-to-Skype conversations between friend and stranger dyads performing a real world-situated task that was designed to elicit adaptation behaviors.",3 Corpora,[0],[0]
"Every dialog involves a stationary director on campus, and a follower downtown.",3 Corpora,[0],[0]
"The director provided directions to help the follower find 10 public art pieces such as sculptures, mosaics, or murals in downtown Santa Cruz.",3 Corpora,[0],[0]
The director had access to Google Earth views of the follower’s route and a map with locations and pictures of art pieces.,3 Corpora,[0],[0]
The corpus consists of transcripts of 24 friend and 24 stranger dyads (48 dialogs).,3 Corpora,[0],[0]
"In total, it contains approximately 185,000 words and 23,000 turns, from conversations that ranged from 24 to 55 minutes, or 197 to 691 turns.",3 Corpora,[0],[0]
"It includes referent negotiation, direction-giving, and small talk (non-task talk).2 Walking Around Corpus (WAC).3 The Walking Around Corpus",3 Corpora,[0],[0]
"(Brennan et al., 2013) consists of spontaneous spoken dialogs produced by 36 pairs of people, collected in order to elicit adaptation behaviors, as illustrated by Figure 2.",3 Corpora,[0],[0]
"In each dialog, a director navigates a follower using a mobile phone to 18 destinations on a medium-sized campus.",3 Corpora,[0],[0]
"Directors have access to a digital map marked with
1https://nlds.soe.ucsc.edu/artwalk 2For AWC and WAC, we remove annotations such as speech overlap, noises (laugh, cough) and indicators for short pauses, leaving only clean text.",3 Corpora,[0],[0]
"If more than one consecutive dialog turn has the same speaker, we merge them into one dialog turn.
",3 Corpora,[0],[0]
"3https://catalog.ldc.upenn.edu/ ldc2015s08
target destinations, labels (e.g. “Ship sculpture”), photos and followers’ real time location.",3 Corpora,[0],[0]
"Followers carry a cell phone with GPS, and a camera in order to take pictures of the destinations they visit.",3 Corpora,[0],[0]
Each dialog ranges from 175 to 885 turns.,3 Corpora,[0],[0]
"The major differences between AWC and WAC are (1) in order to elicit novel referring expressions and possible linguistic adaptation, destinations in AWC do not have provided labels; (2) AWC happens in a more open world setting (downtown) compared to WAC (university campus).",3 Corpora,[0],[0]
"Map Task Corpus (MPT).4 The Map Task Corpus (Anderson et al., 1991) is a set of 128 cooperative task-oriented dialogs involving two participants.",3 Corpora,[0],[0]
Each dialog ranges from 32 to 438 turns.,3 Corpora,[0],[0]
A director and a follower sit opposite one another.,3 Corpora,[0],[0]
Each has a paper map which the other cannot see (the maps are not identical).,3 Corpora,[0],[0]
The director has a route marked on their map; the follower has no route.,3 Corpora,[0],[0]
The participants’ goal is to reproduce the director’s route on the follower’s map.,3 Corpora,[0],[0]
"All maps consist of line drawing landmarks labelled with their names, such as “parked van”, “east lake”, or “white mountain”.",3 Corpora,[0],[0]
"Figure 3 shows an excerpt from the Map Task Corpus. Switchboard Corpus (SWBD).5 Switchboard (Godfrey et al., 1992) is a collection of two-speaker telephone conversations from all areas of the United States.",3 Corpora,[0],[0]
"An automatic operator handled the calls (giving recorded prompts, selecting and dialing another speaker, introducing discussion topics and recording the dialog).",3 Corpora,[0],[0]
"70 topics were provided, for example: pets, child care, music, and buying a car.",3 Corpora,[0],[0]
"Each topic has a corresponding prompt message played to the first speaker, e.g. “find out what kind of pets the
4http://groups.inf.ed.ac.uk/maptask/ 5https://catalog.ldc.upenn.edu/
ldc97s62
other caller has.”",3 Corpora,[0],[0]
"A subset of 200K utterances of Switchboard have also been tagged with dialog act tags (Jurafsky et al., 1997).",3 Corpora,[0],[0]
Each dialog contains 14 to 373 turns.,3 Corpora,[0],[0]
"Figure 1 provides an example of dialog act tags, such as b - Acknowledge (Backchannel), sv - Statement-opinion, sd - Statement-non-opinion, and % - Uninterpretable.",3 Corpora,[0],[0]
"We focus on this subset of the corpus.
",3 Corpora,[0],[0]
"Dialogs in SWBD have a different style from the three task-oriented, direction-giving corpora.",3 Corpora,[0],[0]
"Figure 4 illustrates how the SWBD dialogs are often lopsided: from utterance 14 to 18, speaker B states his opinion with verbose dialog turns, whereas speaker A only acknowledges and backchannels; from utterance 19 to 22, speaker A acts as the main speaker, whereas speaker B backchannels.",3 Corpora,[0],[0]
"Some theories of discourse define dialog turns as extending over backchannels, and we posit that this
would allow us to measure adaptation more faithfully, so we utilize the SWBD dialog act tags to filter turns that only contain backchannels, keeping only dialog turns with tags sd (Statement-nonopinion), sv (Statement-opinion), and bf (Summarize/reformulate).6",3 Corpora,[0],[0]
We then merge consecutive dialog turns from the same speaker.,3 Corpora,[0],[0]
"We consider the following feature sets: unigram, bigram, referring expressions, hedges/discourse markers, and Linguistic Inquiry and Word Count (LIWC) features.",4 Experimental Setup,[0],[0]
"Previous computational work on measuring linguistic adaptation in textual corpora have largely focused on lexical and syntactical features, which are included as baselines.",4 Experimental Setup,[0],[0]
"Referring expressions and discourse markers are key features that are commonly studied for adaptation behaviors in task-oriented dialogs, which are often hand annotated.",4 Experimental Setup,[0],[0]
Here we automatically extract these features by rules.,4 Experimental Setup,[0],[0]
"To model adaptation on the personality level, we draw features that correlate significantly with personality ratings from LIWC features.",4 Experimental Setup,[0],[0]
"We hypothesize that our feature sets will demonstrate different adaptation models.
",4 Experimental Setup,[0],[0]
"We lemmatize, POS tag and derive constituency structures using Stanford CoreNLP",4 Experimental Setup,[0],[0]
"(Manning et al., 2014).",4 Experimental Setup,[0],[0]
We then extract the following linguistic features from annotations and raw text.,4 Experimental Setup,[0],[0]
The following example features are based on D137 in Figure 2.,4 Experimental Setup,[0],[0]
Unigram Lemma/POS.,4 Experimental Setup,[0],[0]
We use lemma combined with POS tags to distinguish word senses.,4 Experimental Setup,[0],[0]
"E.g., lemmapos building/NN and lemmapos brick/NNS in D137.",4 Experimental Setup,[0],[0]
Bigram Lemma.,4 Experimental Setup,[0],[0]
"E.g., bigram the-brick and bigram side-of in D137.",4 Experimental Setup,[0],[0]
Syntactic Structure.,4 Experimental Setup,[0],[0]
"Following Reitter et al. (2006b), we take all the subtrees from a constituency parse tree (excluding the leaf nodes that contain words) as features.",4 Experimental Setup,[0],[0]
"E.g., syntax VP->VBP+PP and syntax ADJP-> DT+JJ in D137.",4 Experimental Setup,[0],[0]
The difference is that we use Stanford Parser rather than hand annotations.,4 Experimental Setup,[0],[0]
Referring Expression.,4 Experimental Setup,[0],[0]
Referring expressions are usually noun phrases.,4 Experimental Setup,[0],[0]
"We start by taking all constituency subtrees with root NP, then map the subtrees to their actual phrases in the text and remove all articles from the phrase, e.g., referexp little-concrete
6The filtering process removes 48.1% original dialog turns, but only 12.6% of the words.",4 Experimental Setup,[0],[0]
"Filtered dialogs have 3 to 85 dialog turns each.
and referexp math-building in D137.",4 Experimental Setup,[0],[0]
Hedge/Discourse Marker.,4 Experimental Setup,[0],[0]
"Hedges are mitigating words used to lessen the impact of an utterance, such as “actually” and “somewhat”.",4 Experimental Setup,[0],[0]
"Discourse markers are words or phrases that manage the flow and structure of discourse, such as “you know” and “I mean”.",4 Experimental Setup,[0],[0]
"We construct a dictionary of hedges and discourse markers, and use string matching to extract features, e.g., hedge you-know and hedge like in D137.",4 Experimental Setup,[0],[0]
LIWC.,4 Experimental Setup,[0],[0]
"Linguistic Inquiry and Word Count (Pennebaker et al., 2001) is a text analysis program that counts words in over 80 linguistic (e.g., pronouns, conjunctions), psychological (e.g., anger, positive emotion), and topical (e.g., leisure, money) categories.",4 Experimental Setup,[0],[0]
"E.g., liwc second-person and liwc informal in D137.",4 Experimental Setup,[0],[0]
"Because DAS features are binary, features such as Word Count and Number of New Lines are excluded.",4 Experimental Setup,[0],[0]
Personality LIWC.,4 Experimental Setup,[0],[0]
"Previous work reports for each LIWC feature whether it is significantly correlated with each Big Five trait (Mairesse et al., 2007) on conversational data (Mehl et al., 2006).",4 Experimental Setup,[0],[0]
"For each trait, we create feature sets consisting of such features.",4 Experimental Setup,[0],[0]
See Table 2.,4 Experimental Setup,[0],[0]
"In this section, we apply our DAS measure on the corpora introduced in Section 3.",5 Experiments on Modeling Adaptation,[0],[0]
"We first establish that our novel DAS measure is valid by testing whether it can distinguish dialogs in their original order vs. dialogs with randomly scrambled turns (the order of dialog turns are randomized within speakers), inspired by similar approaches in previous work (Gandhe and Traum, 2008; Ward and Litman, 2007; Barzilay and Lapata, 2005).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"We calculate DAS scores for original dialogs and randomized dialogs using target=Both
(Sec. 2) to obtain overall adaptation scores for both speakers.
",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
We first test on lexical features (unigram and bigram) as in previous work.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Then we add additional linguistic features (syntactic structure, referring expression, and discourse marker).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
These five features (see Section 4) are referred to as “all but LIWC”.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Finally, we test DAS validity using the higher level LIWC features.
",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"We perform paired t-tests on DAS scores for original dialogs and DAS scores for randomized dialogs, pairing every original dialog with its randomized dialog.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Table 3 shows the number of dialogs in each corpus, the average DAS scores of all dialogs within the corpus and p-values of corresponding t-tests.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"Although the differences between the average scores are relatively small, the differences in almost all paired t-tests are extremely statistically significant (cells in bold, p < 0.0001).",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
The paired t-test on MPT using LIWC features shows a significant difference between the two test groups (p < 0.05).,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
The original dialog corpora achieve higher average DAS scores than the randomized corpora for all 12 original-random pairs.,5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
"The results show that DAS measure is sensitive to dialog turn order, as it should be if it is measuring dialog coherence and adaptation.",5.1 Validity Test: Original vs. Randomized Dialogs,[0],[0]
This experiment aims to broadly examine the differences in adaptation across different corpora and feature sets.,5.2 Adaptation across corpora and across features,[0],[0]
"We first compute DAS on the whole
dialog level for each feature set from Section 4, and then calculate the average across the corpus.",5.2 Adaptation across corpora and across features,[0],[0]
We use target=Both (Sec 2) to obtain an overall measure of adaptation and leave calculating finegrained DAS measures to Section 5.3.,5.2 Adaptation across corpora and across features,[0],[0]
Table 4 provides results.,5.2 Adaptation across corpora and across features,[0],[0]
"We will refer to features in row 1 to 6 as “linguistic features” and row 7 to 11 as “personality features”.
",5.2 Adaptation across corpora and across features,[0],[0]
"Comparing columns, we first examine the DAS scores across different corpora.",5.2 Adaptation across corpora and across features,[0],[0]
All p-values reported below are from paired t-tests.,5.2 Adaptation across corpora and across features,[0],[0]
"The two most similar corpora, the AWC and WAC, show no significant difference on linguistic features (p = 0.43).",5.2 Adaptation across corpora and across features,[0],[0]
"At the same time, the AWC and WAC do differ from the other two corpora.",5.2 Adaptation across corpora and across features,[0],[0]
This demonstrates that the DAS reflects real similarities and differences across corpora.,5.2 Adaptation across corpora and across features,[0],[0]
"MPT shows lower DAS scores on all linguistic features except for lemma (word repetition), where it achieves the highest DAS score.",5.2 Adaptation across corpora and across features,[0],[0]
"With respect to personality features, WAC has significantly higher DAS scores than AWC (p < 0.05), possibly because of the different experiment settings: college student participants are more comfortable around their own campus than in downtown.",5.2 Adaptation across corpora and across features,[0],[0]
MPT shows significantly lower DAS scores on personality features than AWC and WAC (p < 0.05).,5.2 Adaptation across corpora and across features,[0],[0]
This may be because the MPT setting is the most constrained of the four corpora: being fixed in topic and location means dialogs are less likely to be influenced by environmental factors or to contain social chit chat.,5.2 Adaptation across corpora and across features,[0],[0]
SWBD has the highest DAS scores in all feature sets except for referring expression.,5.2 Adaptation across corpora and across features,[0],[0]
The higher DAS in nonreferring features could be because the social chit chat allows more adaptation to occur.,5.2 Adaptation across corpora and across features,[0],[0]
"In addition, the dialogs we measure in SWBD are backchannelfiltered.",5.2 Adaptation across corpora and across features,[0],[0]
"The lower referring expression (respective to other SWBD scores) could be because SWBD does not require the referring expressions necessary
for the other three task-related corpora.",5.2 Adaptation across corpora and across features,[0],[0]
"We posit that the DAS adaptation models we present can be used in existing NLG architectures, described in Sec. 2.",5.2 Adaptation across corpora and across features,[0],[0]
"The AWC column in Table 4 shows adaptation model in the form of a DAS vector obtained from the ArtWalk Corpus.
",5.2 Adaptation across corpora and across features,[0],[0]
"Comparing rows, we then examine DAS scores among different features sets.",5.2 Adaptation across corpora and across features,[0],[0]
"LIWC has the highest DAS score among linguistic features, ranging from 0.48 to 0.71.",5.2 Adaptation across corpora and across features,[0],[0]
"While other linguistic features are largely content-specific, LIWC consists of higher level features that cover broader categories, thus its high DAS scores are expected.",5.2 Adaptation across corpora and across features,[0],[0]
"The DAS scores for the lemma feature range from 0.14 to 0.29, followed by Syntactic Structure (0.11 to 0.28), Hedge (0.17 to 0.25) and Bigram (0.01 to 0.07).",5.2 Adaptation across corpora and across features,[0],[0]
"Referring Expression has the lowest DAS score (0.01 to 0.03), possibly because our automatic extraction of referring expressions creates numerous subsets of one referring expression.",5.2 Adaptation across corpora and across features,[0],[0]
"Among personality features, Emotion Stability, Agreeableness, and Openness to Experience traits are adapted more than Extraversion and Conscientiousness.",5.2 Adaptation across corpora and across features,[0],[0]
We leave to future work the question of why these traits have higher DAS scores.,5.2 Adaptation across corpora and across features,[0],[0]
Our primary goal is to model adaptation at a finegrained level in order to provide fine-grained control of an NLG engine.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"To that end, we report results for adaptation models on a per dialog-segment and per-speaker basis.
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Reliable discourse segmentation is notoriously difficult (Passonneau and Litman, 1996), thus we heuristically divide each task-oriented dialog into segments based on number of destinations on the map: this effectively divides the dialog into subtasks.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Since each dialog in SWBD only has one topic, we divide SWBD into 5 segments.7 We compute DAS for each segment, and take an average across all dialogs in the corpus for each segment.
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We compare all LIWC features vs. extraversion LIWC features because they provide high DAS scores across corpora.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We also aim to explore the dynamics between two conversants on the extraversion scale.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
Figure 5 in Appendix illustrates how DAS varies as a function of speaker and dialog segment.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In AWC, scores for all LIWC features
7To ensure two way adaptation exists in every segment (both speaker A adapting to B, and B adapting to A), the minimum length (number of turns) of each segment is 3.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"Thus we only work with dialogs longer than 15 turns in SWBD.
slightly decrease as dialogs progress (Fig. 5(a)), while extraversion features show a distinct increasing trend with correlation coefficients ranging from 0.7 to 0.86 (Fig. 5(b)), despite being a subset of all LIWC features.8 Average DAS displays the same decreasing trend in all and extraversion LIWC features for SWBD (Fig. 5(g) and 5(h)).",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We speculate that this might be due to the setup of SWBD: as the dialogs progress, conversants have less to discuss about the topic and are less interested.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We also calculate per segment adaptation in WAC and MPT, but their DAS scores do not show overall trends across the length of the dialog (Fig. 5(c) to 5(f)).
",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We also explore whether speaker role and initiative affects adaptation.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"We use target=Both, target=D, and target=F to calculate DAS for each target.9",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
We hypothesize that directors and followers adapt differently in task-oriented dialogs.,5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In all task-oriented corpora (AWC, WAC, and MPT), we observe generally higher DAS scores with target=D, indicating that in order to drive the dialogs, directors adapt more to followers.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
"In SWBD, the speaker initiating the call (who brings up the discussion topic and may therefore drive the conversation) generally exhibits more adaptation.",5.3 Adaptation by Dialog Segment and Speaker,[0],[0]
This experiment aims to examine the trend of DAS scores as the window size increases.,5.4 Adaptation on Different Window Sizes,[0],[0]
We begin with a window size of 1 and gradually increase it to 5.,5.4 Adaptation on Different Window Sizes,[0],[0]
"For a window size of n, the target utterance t is paired with the n-th utterance from a different speaker preceding t, if any.",5.4 Adaptation on Different Window Sizes,[0],[0]
"For example, in Figure 1, when window size is 3, target D100 is paired with prime F97; target D99 does not have any prime, thus no pair is formed.
",5.4 Adaptation on Different Window Sizes,[0],[0]
"Similar to Sec. 5.1, we compare DAS scores between dialogs in their original order vs. dialogs with randomly scrambled turns.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We hypothesize that similar to the results of repetition decay measures (Reitter et al., 2006a; Ward and Litman, 2007; Pietsch et al., 2012), the DAS scores of original dialogs would decrease as the window size increases.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We use target=both to obtain overall adaptation scores involving both speakers, and calculate DAS with all but the Personality LIWC feature sets introduced in Sec. 4.",5.4 Adaptation on Different Window Sizes,[0],[0]
"We first compute DAS on the whole dialog level for each window size, and then calculate the average DAS for each window size
8Using Simple Linear Regression in Weka 3.8.1.",5.4 Adaptation on Different Window Sizes,[0],[0]
"9In task-oriented dialogs, D stands for Director, F for Fol-
lower.",5.4 Adaptation on Different Window Sizes,[0],[0]
"In SWBD, D stands for the speaker initiating the call.
across the corpus.",5.4 Adaptation on Different Window Sizes,[0],[0]
"Results show that DAS scores for the original dialogs in all corpora decrease as window size increases, while DAS scores for the randomized dialogs stay relatively stable.",5.4 Adaptation on Different Window Sizes,[0],[0]
Figure 6 in Appendix shows plots of average DAS scores on different window sizes for original and randomized dialogs.,5.4 Adaptation on Different Window Sizes,[0],[0]
Plots of the AWC and WAC show similar trends.,5.4 Adaptation on Different Window Sizes,[0],[0]
Experiments with larger window sizes show that the original and random scores meet at window size 6 - 7 (with different versions of randomized dialogs).,5.4 Adaptation on Different Window Sizes,[0],[0]
"In MapTask, the original and random scores meet at window size 3 - 4.",5.4 Adaptation on Different Window Sizes,[0],[0]
"In SWBD, original and random scores meet at window size 2.",5.4 Adaptation on Different Window Sizes,[0],[0]
"Recent measures of linguistic adaptation fall into three categories: probabilistic measures, repetition decay measures, and document similarity measures (Xu and Reitter, 2015).",6 Related Work,[0],[0]
Probabilistic measures compute the probability of a single linguistic feature appearing in the target after its appearance in the prime.,6 Related Work,[0],[0]
"Some measures in this category focus more on comparing adaptation amongst features and do not handle turn by turn adaptation (Church, 2000; Stenchikova and Stent, 2007).",6 Related Work,[0],[0]
"Moreover, these measures produce scores for individual features, which need aggregation to reflect overall adaptivity (Danescu-Niculescu-Mizil et al., 2011, 2012).",6 Related Work,[0],[0]
"Document similarity measures calculate the similarity between prime and target by measuring the number of features that appear in both prime and target, normalized by the size of the two text sets (Wang et al., 2014).",6 Related Work,[0],[0]
"Both probabilistic measures and document similarity measures require the whole dialog to be complete before calculation.
",6 Related Work,[0],[0]
Repetition decay measures observe the decay rate of repetition probability of linguistic features.,6 Related Work,[0],[0]
"Previous work has fit the probability of linguistic feature repetition decrease with the distance between prime and target in logarithmic decay models (Reitter et al., 2006a,b; Reitter, 2008), linear decay models (Ward and Litman, 2007), and exponential decay models (Pietsch et al., 2012).
",6 Related Work,[0],[0]
Previous work on linguistic adaptation in natural language generation has also attempted to use adaptation models learned from human conversations.,6 Related Work,[0],[0]
"The alignment-capable microplanner SPUD prime (Buschmeier et al., 2009, 2010) uses the repetition decay model from Reitter (2008) as part of the activation functions for linguistic structures.",6 Related Work,[0],[0]
"However, the parameters are not learned from real
data.",6 Related Work,[0],[0]
"Repetition decay models do well in statistical parameterized NLG, but is hard to apply to overgenerate and rank NLG.",6 Related Work,[0],[0]
Isard et al. (2006) apply a pre-trained n-grams adaptation model to generate conversations.,6 Related Work,[0],[0]
"Hu et al. (2014) explore the effects of adaptation to various features by human evaluations, but their generator is not capable of deciding which features to adapt based on input context.",6 Related Work,[0],[0]
Dušek and Jurčı́ček (2016) use a seq2seq model to generate responses adapting to previous context.,6 Related Work,[0],[0]
They utilize an n-gram match ranker that promotes outputs with phrase overlap with context.,6 Related Work,[0],[0]
Our learned adaptation models could serve as a ranker.,6 Related Work,[0],[0]
"In addition to n-grams, DAS could produce models with any combinations of feature sets, providing more versatile adaptation behavior.",6 Related Work,[0],[0]
"To obtain models of linguistic adaptation, most measures could only measure an individual feature at a time, and need the whole dialog to calculate the measure (Church, 2000; Stenchikova and Stent, 2007; Danescu-Niculescu-Mizil et al., 2012; Pietsch et al., 2012; Reitter et al., 2006b; Ward and Litman, 2007).",7 Discussion and Future Work,[0],[0]
"This paper proposes the Dialog Adaptation Score (DAS) measure, which can be applied to NLG because it can be calculated on any segment of a dialog, and for any feature set.
",7 Discussion and Future Work,[0],[0]
"We first validate our measure by showing that the average DAS of original dialogs is significantly higher than randomized dialogs, indicating that it is sensitive to dialog priming as intended.",7 Discussion and Future Work,[0],[0]
"We then use DAS to show that feature sets such as LIWC, Syntactic Structure, and Hedge/Discourse Marker are adapted more than Bigram and Referring Expressions.",7 Discussion and Future Work,[0],[0]
We also demonstrate how we can use DAS to develop fine-grained models of adaptation: e.g. DAS applied to model adaptation in extraversion displays a distinct trend compared to all LIWC features in the task-oriented dialog corpus AWC.,7 Discussion and Future Work,[0],[0]
"Finally, we show that the degree of adaptation decreases as the window size increases.",7 Discussion and Future Work,[0],[0]
We leave to future work the implementation and evaluation of DAS adaptation models in natural language generation systems.,7 Discussion and Future Work,[0],[0]
"This research was supported by NSF CISE RI EAGER #IIS-1044693, NSF CISE CreativeIT #IIS1002921, NSF CHS #IIS-1115742, Nuance Foundation Grant SC-14-74, and auxiliary REU supplements.",Acknowledgement,[0],[0]
Previous work has shown that conversants adapt to many aspects of their partners’ language.,abstractText,[0],[0]
"Other work has shown that while every person is unique, they often share general patterns of behavior.",abstractText,[0],[0]
"Theories of personality aim to explain these shared patterns, and studies have shown that many linguistic cues are correlated with personality traits.",abstractText,[0],[0]
"We propose an adaptation measure for adaptive natural language generation for dialogs that integrates the predictions of both personality theories and adaptation theories, that can be applied as a dialog unfolds, on a turn by turn basis.",abstractText,[0],[0]
"We show that our measure meets criteria for validity, and that adaptation varies according to corpora and task, speaker, and the set of features used to model it.",abstractText,[0],[0]
"We also produce fine-grained models according to the dialog segmentation or the speaker, and demonstrate the decaying trend of adaptation.",abstractText,[0],[0]
Modeling Linguistic and Personality Adaptation for Natural Language Generation,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2289–2299 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2289",text,[0],[0]
"Understanding a story requires reasoning about the causal links between the events in the story and the mental states of the characters, even when those relationships are not explicitly stated.",1 Introduction,[0],[0]
"As shown by the commonsense story cloze shared task (Mostafazadeh et al., 2017)",1 Introduction,[0],[0]
", this reasoning is remarkably hard for both statistical and neural machine readers – despite being trivial for humans.",1 Introduction,[0],[0]
This stark performance gap between humans and machines is not surprising as most powerful language models have been designed to effectively learn local fluency patterns.,1 Introduction,[0],[0]
"Consequently, they generally lack the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters’ mental states or predicting their plausible next actions.
",1 Introduction,[0],[0]
"In this paper, we construct a new annotation formalism to densely label commonsense short stories (Mostafazadeh et al., 2016) in terms of the mental states of the characters.",1 Introduction,[0],[0]
"The result-
The band instructor told the band to start playing.",1 Introduction,[0],[0]
"They grew tired and started playing worse after a while.
",He often stopped the music when players were off-tone.,[0],[0]
"The instructor was furious and threw his chair.
",He often stopped the music when players were off-tone.,[0],[0]
"He cancelled practice and expected us to perform
tomorrow.
",He often stopped the music when players were off-tone.,[0],[0]
"Instructor Players
E M E M
E M E M
E M E M
E M E M
E M E M
confide nt
[esteem]",He often stopped the music when players were off-tone.,[0],[0]
"[anger]
need re st
[esteem]
frustrate d
angry afraid
[disgust, fear]
",He often stopped the music when players were off-tone.,[0],[0]
"[esteem]
M EE
M [stability]
Figure 1: A story example with partial annotations for motivations (dashed) and emotional reactions (solid).",He often stopped the music when players were off-tone.,[0],[0]
"Open text explanations are in black (e.g., “frustrated”) and formal theory labels are in blue with brackets (e.g., “[esteem]”).
",He often stopped the music when players were off-tone.,[0],[0]
ing dataset offers three unique properties.,He often stopped the music when players were off-tone.,[0],[0]
"First, as highlighted in Figure 1, the dataset provides a fully-specified chain of motivations and emotional reactions for each story character as preand post-conditions of events.",He often stopped the music when players were off-tone.,[0],[0]
"Second, the annotations include state changes for entities even when they are not mentioned directly in a sentence (e.g., in the fourth sentence in Figure 1, players would feel afraid as a result of the instructor throwing a chair), thereby capturing implied effects unstated in the story.",He often stopped the music when players were off-tone.,[0],[0]
"Finally, the annotations encompass both formal labels from multiple theories of psychology (Maslow, 1943; Reiss, 2004; Plutchik, 1980) as well as open text descriptions of motivations and emotions, providing a comprehensive mapping between open text explanations and label categories (e.g., “to spend time with her son”
!",He often stopped the music when players were off-tone.,[0],[0]
Maslow’s category love).,He often stopped the music when players were off-tone.,[0],[0]
"Our corpus1 spans across 15k stories, amounting to 300k low-level annotations for around 150k character-line pairs.
",He often stopped the music when players were off-tone.,[0],[0]
"Using our new corpus, we present baseline performance on two new tasks focusing on mental state tracking of story characters: categorizing motivations and emotional reactions using theory labels, as well as describing motivations and emotional reactions using open text.",He often stopped the music when players were off-tone.,[0],[0]
Empirical results demonstrate that existing neural network models including those with explicit or latent entity representations achieve promising results.,He often stopped the music when players were off-tone.,[0],[0]
"Understanding people’s actions, motivations, and emotions has been a recurring research focus across several disciplines including philosophy and psychology (Schachter and Singer, 1962; Burke, 1969; Lazarus, 1991; Goldman, 2015).",2 Mental State Representations,[0],[0]
We draw from these prior works to derive a set of categorical labels for annotating the step-by-step causal dynamics between the mental states of story characters and the events they experience.,2 Mental State Representations,[0],[0]
"We use two popular theories of motivation: the “hierarchy of needs” of Maslow (1943) and the “basic motives” of Reiss (2004) to compile 5 coarse-grained and 19 fine-grained motivation categories, shown in Figure 2.",2.1 Motivation Theories,[0],[0]
"Maslow’s “hierarchy of needs” are comprised of five categories, ranging from physiological needs to spiritual growth, which we use as coarse-level categories.",2.1 Motivation Theories,[0],[0]
Reiss (2004) proposes 19 more fine-grained categories that provide a more informative range of motivations.,2.1 Motivation Theories,[0],[0]
"For example, even though they both relate
1We make our dataset publicly available at https:// uwnlp.github.io/storycommonsense/
to the physiological needs Maslow category, the food and rest motives from Reiss (2004) are very different.",2.1 Motivation Theories,[0],[0]
"While the Reiss theory allows for finergrained annotations of motivation, the larger set of abstract concepts can be overwhelming for annotators.",2.1 Motivation Theories,[0],[0]
"Motivated by Straker (2013), we design a hybrid approach, where Reiss labels are annotated as sub-categories of Maslow categories.",2.1 Motivation Theories,[0],[0]
"Among several theories of emotion, we work with the “wheel of emotions” of Plutchik (1980), as it has been a common choice in prior literature on emotion categorization (Mohammad and Turney, 2013; Zhou et al., 2016).",2.2 Emotion Theory,[0],[0]
We use the eight basic emotional dimensions as illustrated in Figure 2.,2.2 Emotion Theory,[0],[0]
"In addition to the motivation and emotion categories derived from psychology theories, we also obtain open text descriptions of character mental states.",2.3 Mental State Explanations,[0],[0]
"These open text descriptions allow learning computational models that can explain the mental states of characters in natural language, which is likely to be more accessible and informative to end users than having theory categories alone.",2.3 Mental State Explanations,[0],[0]
"Collecting both theory categories and open text also allows us to learn the automatic mappings between the two, which generalizes the previous work of Mohammad and Turney (2013) on emotion category mappings.",2.3 Mental State Explanations,[0],[0]
"In this study, we choose to annotate the simple commonsense stories introduced by Mostafazadeh et al. (2016).",3 Annotation Framework,[0],[0]
"Despite their simplicity, these stories pose a significant challenge to natural language understanding models (Mostafazadeh et al., 2017).
",3 Annotation Framework,[0],[0]
"In addition, they depict multiple interactions between story characters, presenting rich opportunities to reason about character motivations and reactions.",3 Annotation Framework,[0],[0]
"Furthermore, there are more than 98k such stories currently available covering a wide range of everyday scenarios.
",3 Annotation Framework,[0],[0]
"Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis (Mohammad and Turney, 2013; Deng and Wiebe, 2015), entity tracking (Hoffart et al., 2011; Weston et al., 2015), and story understanding (Goyal et al., 2010; Ouyang and McKeown, 2015; Lukin et al., 2016), our study is the first to annotate the full chains of mental state effects for story characters.",3 Annotation Framework,[0],[0]
"This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories.",3 Annotation Framework,[0],[0]
"In prior literature, annotations of this complexity have typically been performed by experts (Deng and Wiebe, 2015; Ouyang and McKeown, 2015).",3 Annotation Framework,[0],[0]
"While reliable, these annotations are prohibitively expensive to scale up.",3 Annotation Framework,[0],[0]
"Therefore, we introduce a new annotation framework that pipelines a set of smaller isolated tasks as illustrated in Figure 3.",3 Annotation Framework,[0],[0]
All annotations were collected using crowdsourced workers from Amazon Mechanical Turk.,3 Annotation Framework,[0],[0]
We describe the components and workflow of the full annotation pipeline shown in Figure 3 below.,3.1 Annotation Pipeline,[0],[0]
"The example story in the figure is used to illustrate the output of various steps in the pipeline (full annotations for this example are in the appendix).
",3.1 Annotation Pipeline,[0],[0]
(1) Entity Resolution The first task in the pipeline aims to discover (1) the set of characters,3.1 Annotation Pipeline,[0],[0]
Ei in each story i and (2) the set of sentences Sij in which a specific character j 2,3.1 Annotation Pipeline,[0],[0]
"Ei is ex-
plicitly mentioned.",3.1 Annotation Pipeline,[0],[0]
"For example, in the story in Figure 3, the characters identified by annotators are “I/me” and “My cousin”, whom appear in sentences {1, 4, 5} and {1, 2, 3, 4, 5}, respectively.
",3.1 Annotation Pipeline,[0],[0]
We use Sij to control the workflow of later parts of the pipeline by pruning future tasks for sentences that are not tied to characters.,3.1 Annotation Pipeline,[0],[0]
"Because Sij is used to prune follow-up tasks, we take a high recall strategy to include all sentences that at least one annotator selected.
",3.1 Annotation Pipeline,[0],[0]
(2a) Action Resolution The next task identifies whether a character j appearing in a sentence k is taking any action to which a motivation can be attributed.,3.1 Annotation Pipeline,[0],[0]
We perform action resolution only for sentences k 2 Sij .,3.1 Annotation Pipeline,[0],[0]
"In the running example, we would want to know that the cousin in line 2 is not doing anything intentional, allowing us to omit this line in the next pipeline stage (3a) where a character’s motives are annotated.",3.1 Annotation Pipeline,[0],[0]
"Description of state (e.g., “Alex is feeling blue”) or passive event participation (e.g., “Alex trips”) are not considered volitional acts for which the character may have an underlying motive.",3.1 Annotation Pipeline,[0],[0]
"For each line and story character pair, we obtain 4 annotations.",3.1 Annotation Pipeline,[0],[0]
"Because pairs can still be filtered out in the next stage of annotation, we select a generous threshold where only 2 annotators must vote that an intentional action took place for the sentence to be used as an input to the motivation annotation task (3a).
",3.1 Annotation Pipeline,[0],[0]
(2b),3.1 Annotation Pipeline,[0],[0]
Affect Resolution This task aims to identify all of the lines where a story character j has an emotional reaction.,3.1 Annotation Pipeline,[0],[0]
"Importantly, it is often possible to infer the emotional reaction of a character j even when the character does not explicitly appear in a sentence k.",3.1 Annotation Pipeline,[0],[0]
"For instance, in Figure 3, we want to annotate the narrator’s reaction to line 2 even though they are not mentioned because their emotional response is inferrable.",3.1 Annotation Pipeline,[0],[0]
"We obtain 4 an-
notations per character per line.",3.1 Annotation Pipeline,[0],[0]
"The lines with at least 2 annotators voting are used as input for the next task: (3b) emotional reaction.
",3.1 Annotation Pipeline,[0],[0]
(3a) Motivation We use the output from the action resolution stage (2a) to ask workers to annotate character motives in lines where they intentionally initiate an event.,3.1 Annotation Pipeline,[0],[0]
"We provide 3 annotators a line from a story, the preceding lines, and a specific character.",3.1 Annotation Pipeline,[0],[0]
They are asked to produce a free response sentence describing what causes the character’s behavior in that line and to select the most related Maslow categories and Reiss subcategories.,3.1 Annotation Pipeline,[0],[0]
"In Figure 3, an annotator described the motivation of the narrator in line 1 as wanting “to have company” and then selected the love (Maslow) and family (Reiss) as categorical labels.",3.1 Annotation Pipeline,[0],[0]
"Because many annotators are not familiar with motivational theories, we require them to complete a tutorial the first time they attempt the task.
",3.1 Annotation Pipeline,[0],[0]
"(3b) Emotional Reaction Simultaneously, we use the output from the affect resolution stage (2b) to ask workers what the emotional response of a character is immediately following a line in which they are affected.",3.1 Annotation Pipeline,[0],[0]
"As with the motives, we give 3 annotators a line from a story, its previous context, and a specific character.",3.1 Annotation Pipeline,[0],[0]
We ask them to describe in open text how the character will feel following the event in the sentence (up to three emotions).,3.1 Annotation Pipeline,[0],[0]
"As a follow-up, we ask workers to compare their free responses against Plutchik categories by using 3-point likert ratings.",3.1 Annotation Pipeline,[0],[0]
"In Figure 3, we include a response for the emotional reaction of the narrator in line 1.",3.1 Annotation Pipeline,[0],[0]
"Even though the narrator was not mentioned directly in that line, an annotator recorded that they will react to their cousin being a slob by feeling “annoyed” and selected the Plutchik categories for sadness, disgust and anger.",3.1 Annotation Pipeline,[0],[0]
Cost The tasks corresponding to the theory category assignments are the hardest and most expensive in the pipeline (⇠$4 per story).,3.2 Dataset Statistics and Insights,[0],[0]
"Therefore, we obtain theory category labels only for a third of our annotated stories, which we assign to the development and test sets.",3.2 Dataset Statistics and Insights,[0],[0]
"The training data is annotated with a shortened pipeline with only open text descriptions of motivations and emotional reactions from two workers (⇠$1 per story).
",3.2 Dataset Statistics and Insights,[0],[0]
"Scale Our dataset to date includes a total of 300k low-level annotations for motivation and emotion across 15,000 stories (randomly selected from the ROC story training set).",3.2 Dataset Statistics and Insights,[0],[0]
"It covers over 150,000 character-line pairs, in which 56k character-line pairs have an annotated motivation and 105k have an annotated change in emotion (i.e. a label other than none).",3.2 Dataset Statistics and Insights,[0],[0]
"Table 1 shows the break down across training, development, and test splits.",3.2 Dataset Statistics and Insights,[0],[0]
"Figure 4 shows the frequency of different labels being selected for motivational and emotional categories in cases with positive change.
",3.2 Dataset Statistics and Insights,[0],[0]
"Agreements For quality control, we removed workers who consistently produced low-quality work, as discussed in the Appendix.",3.2 Dataset Statistics and Insights,[0],[0]
"In the categorization sets (Maslow, Reiss and Plutchik), we compare the performance of annotators by treating each individual category as a binary label (1
if they included the category in their set of responses) and averaging the agreement per category.",3.2 Dataset Statistics and Insights,[0],[0]
"For Plutchik scores, we count ‘moderately associated’ ratings as agreeing with ‘highly’ associated’ ratings.",3.2 Dataset Statistics and Insights,[0],[0]
The percent agreement and Krippendorff’s alpha are shown in Table 2.,3.2 Dataset Statistics and Insights,[0],[0]
"We also compute the percent agreement between the individual annotations and the majority labels.2
These scores are difficult to interpret by themselves, however, as annotator agreement in our categorization system has a number of properties that are not accounted for by these metrics (disagreement preferences – joy and trust are closer than joy and anger – that are difficult to quantify in a principled way, hierarchical categories map-
2Majority label for the motivation categories is what was agreed upon by at least two annotators per category.",3.2 Dataset Statistics and Insights,[0],[0]
"For emotion categories, we averaged the point-wise ratings and counted a category if the average rating was 2.
",3.2 Dataset Statistics and Insights,[0],[0]
"ping Reiss subcategories from Maslow categories, skewed category distributions that inflate PPA and deflate KA scores, and annotators that could select multiple labels for the same examples).
",3.2 Dataset Statistics and Insights,[0],[0]
"To provide a clearer understanding of agreement within this dataset, we create aggregated confusion matrices for annotator pairs.",3.2 Dataset Statistics and Insights,[0],[0]
"First, we sum the counts of combinations of answers between all paired annotations (excluding none labels).",3.2 Dataset Statistics and Insights,[0],[0]
"If an annotator selected multiple categories, we split the count uniformly among the selected categories.",3.2 Dataset Statistics and Insights,[0],[0]
We compute NPMI over the total confusion matrix.,3.2 Dataset Statistics and Insights,[0],[0]
"In Figure 5, we show the NPMI confusion matrix for motivational categories.
",3.2 Dataset Statistics and Insights,[0],[0]
"In the motivation annotations, we find the highest scores on the diagonal (i.e., Reiss agreement), with most confusions occurring between Reiss motives in the same Maslow category (outlined black in Figure 5).",3.2 Dataset Statistics and Insights,[0],[0]
"Other disagreements generally involve Reiss subcategories that are thematically similar, such as serenity (mental relaxation) and rest (physical relaxation).",3.2 Dataset Statistics and Insights,[0],[0]
"We provide this analysis for Plutchik categories in the appendix, finding high scores along the diagonal with disagreements typically occurring between categories in a “positive emotion” cluster (joy, trust) or a “negative emotion” cluster (anger, disgust,sadness).",3.2 Dataset Statistics and Insights,[0],[0]
The multiple modes covered by the annotations in this new dataset allow for multiple new tasks to be explored.,4 Tasks,[0],[0]
"We outline three task types below, covering a total of eight tasks on which to evaluate.
",4 Tasks,[0],[0]
"Differences between task type inputs and outputs are summarized in Figure 6.
",4 Tasks,[0],[0]
State Classification,4 Tasks,[0],[0]
"The three primary tasks involve categorizing the psychological states of story characters for each of the label sets (Maslow, Reiss, Plutchik) collected for the dev and test splits of our dataset.",4 Tasks,[0],[0]
"In each classification task, a model is given a line of the story (along with optional preceding context lines) and a character and predicts the motivation (or emotional reaction).",4 Tasks,[0],[0]
"A binary label is predicted for each of the Maslow needs, Reiss motives or Plutchik categories.
",4 Tasks,[0],[0]
"Annotation Classification Because the dev and test sets contain paired classification labels and free text explanations, we propose three tasks where a model must predict the correct Maslow/Reiss/Plutchik label given an emotional reaction or motivation explanation.
",4 Tasks,[0],[0]
"Explanation Generation Finally, we can use the free text explanations to train models to describe the psychological state of a character in free text (examples in Figure 4).",4 Tasks,[0],[0]
These explanations allow for two conditional generation tasks where the model must generate the words describing the emotional reaction or motivation of the character.,4 Tasks,[0],[0]
The general model architectures for the three tasks are shown in Figure 6.,5 Baseline Models,[0],[0]
We describe each model component below.,5 Baseline Models,[0],[0]
"The state classification and explanation generation models could be trained separately or in a multi-task set-up.
",5 Baseline Models,[0],[0]
"In the state classification and explanation generation tasks, a model is given a line from a story
x s containing N words {ws0, ws1, . . .",5 Baseline Models,[0],[0]
", wsN} from vocabulary V , a character in that story ej 2 E where E is the set of characters in the story, and (optionally) the preceding sentences in the story C = {x0 . . .",5 Baseline Models,[0],[0]
",xs 1} containing words from vocabulary V .",5 Baseline Models,[0],[0]
"A representation for a character’s psychological state is encoded as:
h e = Encoder(x s,C[ej ]) (1)
where C[ej ] corresponds to the concatenated subset of sentences in C where ej appears.",5 Baseline Models,[0],[0]
"While the end classifier or decoder is different for each task, we use the same set of encoders based on word embeddings, common neural network architectures, or memory networks to formulate a representation of the sentence and character, he.",5.1 Encoders,[0],[0]
"Unless specified, he is computed by encoding separate vector representations for the sentence (xs ! hs) and character-specific context (C[ej ] !",5.1 Encoders,[0],[0]
hc) and concatenating these encodings (he =,5.1 Encoders,[0],[0]
[hc;hs]).,5.1 Encoders,[0],[0]
"We describe the encoders below:
TF-IDF We learn a TD-IDF model on the full training corpus of Mostafazadeh et al. (2016) (excluding the stories in our dev/test sets).",5.1 Encoders,[0],[0]
"To encode the sentence, we extract TF-IDF features for its words, yielding vs 2 RV .",5.1 Encoders,[0],[0]
"A projection and nonlinearity is applied to these features to yield hs:
h s =",5.1 Encoders,[0],[0]
"(Wsv s + bs) (2)
where Ws 2 Rd⇥H .",5.1 Encoders,[0],[0]
"The character vector hc is encoded in the same way on sentences in the context pertaining to the character.
",5.1 Encoders,[0],[0]
"GloVe We extract pretrained Glove vectors (Pennington et al., 2014) for each word in V .",5.1 Encoders,[0],[0]
"The word embeddings are max-pooled, yielding embedding vs 2 RH , where H is the dimensionality of the Glove vectors.",5.1 Encoders,[0],[0]
"Using this max-pooled representation, hs and hc are extracted in the same manner as for TF-IDF features (Equation 2).
",5.1 Encoders,[0],[0]
CNN We implement a CNN text categorization model using the same configuration as Kim (2014) to encode the sentence words.,5.1 Encoders,[0],[0]
"A sentence is represented as a matrix, vs 2 RM⇥d where each row is a word embedding xsn for a word wsn 2 xs.
vs =",5.1 Encoders,[0],[0]
"[xs0, x s 1, . . .",5.1 Encoders,[0],[0]
",",5.1 Encoders,[0],[0]
"x s N ] (3)
h s = CNN(vs) (4)
where CNN represents the categorization model from (Kim, 2014).",5.1 Encoders,[0],[0]
The character vector hc is encoded in the same way with a separate CNN.,5.1 Encoders,[0],[0]
"Implementation details are provided in the appendix.
",5.1 Encoders,[0],[0]
LSTM,5.1 Encoders,[0],[0]
A two-layer bi-LSTM encodes the sentence words and concatenates the final time step hidden states from both directions to yield hs.,5.1 Encoders,[0],[0]
"The character vector hc is encoded the same way.
",5.1 Encoders,[0],[0]
REN We use the “tied” recurrent entity network from Henaff et al. (2017).,5.1 Encoders,[0],[0]
"A memory cell m is initialized for each of the J characters in the story, E = {e0, . . .",5.1 Encoders,[0],[0]
", eJ}.",5.1 Encoders,[0],[0]
The REN reads documents one sentence at a time and updates mj for ej 2 E after reading each sentence.,5.1 Encoders,[0],[0]
"Unlike the previous encoders, all sentences of the context C are given to the REN along with the sentence xs.",5.1 Encoders,[0],[0]
The model learns to distribute encoded information to the correct memory cells.,5.1 Encoders,[0],[0]
"The representation passed to the downstream model is:
h e = {mj}s (5)
where {mj}s is the memory vector in the cell corresponding to ej after reading xs.",5.1 Encoders,[0],[0]
"Implementation details are provided in the appendix.
",5.1 Encoders,[0],[0]
"NPN We also include the neural process network from Bosselut et al. (2018) with “tied” entities, but “untied” actions that are not grounded to particular concepts.",5.1 Encoders,[0],[0]
The memory is initialized and accessed similarly as the REN.,5.1 Encoders,[0],[0]
Exact implementation details are provided in the appendix.,5.1 Encoders,[0],[0]
"Once the sentence-character encoding he is extracted, the state classifier predicts a binary label ŷz for every category z 2 Z where Z is the set of category labels for a particular psychological theory (e.g., disgust, surprise, etc. in the Plutchik wheel).",5.2 State Classifier,[0],[0]
"We use logistic regression as a classifier:
ŷz =",5.2 State Classifier,[0],[0]
"(Wzh e + bz) (6)
where Wz and bz are a label-specific set of weights and biases for classifying each label z 2 Z .",5.2 State Classifier,[0],[0]
"The explanation generator is a single-layer LSTM (Hochreiter and Schmidhuber, 1997) that receives the encoded sentence-character representation he and predicts each word yt in the explanation using the same method from Sutskever et al. (2014).",5.3 Explanation Generator,[0],[0]
Implementation details are provided in the appendix.,5.3 Explanation Generator,[0],[0]
"For annotation classification tasks, words from open-text explanations are encoded with TF-IDF features.",5.4 Annotation Classifier,[0],[0]
The same classifier architecture from Section 5.2 is used to predict the labels.,5.4 Annotation Classifier,[0],[0]
State Classification,6.1 Training,[0],[0]
The dev set D is split into two portions of 80% (D1) and 20% (D2).,6.1 Training,[0],[0]
D1 is used to train the classifier and encoder.,6.1 Training,[0],[0]
D2 is used to tune hyperparameters.,6.1 Training,[0],[0]
"The model is trained to minimize the weighted binary cross entropy of predicting a class label yz for each class z:
L = ZX
z=1
zyz log ŷz+(1 z)(1 yz) log(1 ŷz)
(7) where Z is the number of labels in each of the three classifications tasks and z is defined as:
z = 1 e p P (yz) (8)
where P (yz) is the marginal class probability of a positive label for z in the training set.
",6.1 Training,[0],[0]
Annotation Classification,6.1 Training,[0],[0]
The dev set is split in the same manner as for state classification.,6.1 Training,[0],[0]
The TF-IDF features are trained on the set of training annotations Dt coupled with those from D1.,6.1 Training,[0],[0]
The model must minimize the same loss as in Equation 7.,6.1 Training,[0],[0]
"Details are provided in the appendix.
",6.1 Training,[0],[0]
Explanation Generation We use the training set of open annotations to train a model to predict explanations.,6.1 Training,[0],[0]
"The decoder is trained to minimize the negative loglikelihood of predicting each word in the explanation of a character’s state:
Lgen = TX
t=1
logP (yt|y0, ..., yt 1,he) (9)
where he is the sentence-character representation produced by an encoder from Section 5.1.",6.1 Training,[0],[0]
"Classification For the state and annotation classification task, we report the micro-averaged precision (P), recall (R), and F1 score of the Plutchik, Maslow, and Reiss prediction tasks.",6.2 Metrics,[0],[0]
We report the results of selecting a label at random in the top two rows of Table 3.,6.2 Metrics,[0],[0]
"Note that random is low because the distribution of positive instances for each
category is very uneven: macro-averaged positive class probabilities of 8.2, 1.7, and 9.9% per category for Maslow, Reiss, and Plutchik respectively.
",6.2 Metrics,[0],[0]
"Generation Because explanations tend to be short sequences (Figure 4) with high levels of synonymy, traditional metrics such as BLEU are inadequate for evaluating generation quality.",6.2 Metrics,[0],[0]
We use the vector average and vector extrema metrics from Liu et al. (2016) computed using the Glove vectors of generated and reference words.,6.2 Metrics,[0],[0]
We report results in Table 5 on the dev set and compare to a baseline that randomly samples an example from the dev set as a generated sequence.,6.2 Metrics,[0],[0]
Story Context vs. No Context,6.3 Ablations,[0],[0]
Our dataset is motivated by the importance of interpreting story context to categorize emotional reactions and motivations of characters.,6.3 Ablations,[0],[0]
"To test this importance, we ablate hc, the representation of the context sentences pertaining to the character, as an input to the state classifier for each encoder (except the REN and NPN).",6.3 Ablations,[0],[0]
"In Table 3, this ablation is the first row for each encoder presented.
",6.3 Ablations,[0],[0]
"Explanation Pretraining Because the state classification and explanation generation tasks use the same models to encode the story, we explore initializing a classification encoder with parameters trained on the generation task.",6.3 Ablations,[0],[0]
"For the CNN, LSTM, and REN encoders, we pretrain a generator to produce emotion or motivation explana-
tions.",6.3 Ablations,[0],[0]
We use the parameters from the emotion or motivation explanation generators to initialize the Plutchik or Maslow/Reiss classifiers respectively.,6.3 Ablations,[0],[0]
"State Classification We show results on the test set for categorizing Maslow, Reiss, and Plutchik states in Table 3.",7 Experimental Results,[0],[0]
"Despite the difficulty of the task, all models outperform the random baseline.",7 Experimental Results,[0],[0]
"Interestingly, the performance boost from adding entity-specific contextual information (i.e., not ablating hc) indicates that the models learn to condition on a character’s previous experience to classify its mental state at the current time step.",7 Experimental Results,[0],[0]
This effect can be seen in a story about a man whose flight is cancelled.,7 Experimental Results,[0],[0]
"The model without context predicts the same emotional reactions for the man, his wife and the pilot, but with context correctly predicts that the pilot will not have a reaction while predicting that the man and his wife will feel sad.
",7 Experimental Results,[0],[0]
"For the CNN, LSTM, REN, and NPN models, we also report results from pretraining encoder parameters using the free response annotations from the training set.",7 Experimental Results,[0],[0]
"This pretraining offers a clear performance boost for all models on all three prediction tasks, showing that the parameters of the encoder can be pretrained on auxiliary tasks providing emotional and motivational state signal.
",7 Experimental Results,[0],[0]
"The best performing models in each task are most effective at predicting Maslow physiological needs, Reiss food motives, and Plutchik reactions of joy.",7 Experimental Results,[0],[0]
"The relative ease of predicting motivations
related to food (and physiological needs generally) may be because they involve a more limited and concrete set of actions such as eating or cooking.
",7 Experimental Results,[0],[0]
Annotation Classification Table 4 shows that a simple model can learn to map open text responses to categorical labels.,7 Experimental Results,[0],[0]
"This further supports our hypothesis that pretraining a classification model on the free-response annotations could be helpful in boosting performance on the category prediction.
",7 Experimental Results,[0],[0]
"Explanation Generation Finally, we provide results for the task of generating explanations of motivations and emotions in Table 5.",7 Experimental Results,[0],[0]
"Because the explanations are closely tied to emotional and motivation states, the randomly selected explanation can often be close in embedding space to the reference explanations, making the random baseline fairly competitive.",7 Experimental Results,[0],[0]
"However, all models outperform the strong baseline on both metrics, indicating that the generated short explanations are closer semantically to the reference annotation.",7 Experimental Results,[0],[0]
Mental State Annotations Incorporating emotion theories into NLP tasks has been explored in previous projects.,8 Related work,[0],[0]
"Ghosh et al. (2017) modulate language model distributions by increasing the probability of words that express certain affective LIWC (Tausczik and Pennebaker, 2016) categories.",8 Related work,[0],[0]
"More generally, various projects tackle the problem of generating text from a set of attributes like sentiment or generic-ness (Ficler and Goldberg, 2017; Dong et al., 2017).",8 Related work,[0],[0]
"Similarly,
there is also a body of research in reasoning about commonsense stories and discourse (Li and Jurafsky, 2017; Mostafazadeh et al., 2016) or detecting emotional stimuli in stories (Gui et al., 2017).",8 Related work,[0],[0]
"Previous work in plot units (Lehnert, 1981) developed formalisms for affect and mental state in story narratives that included motivations and reactions.",8 Related work,[0],[0]
"In our work, we collect mental state annotations for stories to used as a new resource in this space.
",8 Related work,[0],[0]
"Modeling Entity State Recently, novel works in language modeling (Ji et al., 2017; Yang et al., 2016), question answering (Henaff et al., 2017), and text generation (Kiddon et al., 2016; Bosselut et al., 2018) have shown that modeling entity state explicitly can boost performance while providing a preliminary interface for interpreting a model’s prediction.",8 Related work,[0],[0]
"Entity modeling in these works, however, was limited to tracking entity reference (Kiddon et al., 2016; Yang et al., 2016; Ji et al., 2017), recognizing entity state similarity (Henaff et al., 2017) or predicting simple attributes from entity states (Bosselut et al., 2018).",8 Related work,[0],[0]
Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories.,8 Related work,[0],[0]
We present a large scale dataset as a resource for training and evaluating mental state tracking of characters in short commonsense stories.,9 Conclusion,[0],[0]
This dataset contains over 300k low-level annotations for character motivations and emotional reactions.,9 Conclusion,[0],[0]
We provide benchmark results on this new resource.,9 Conclusion,[0],[0]
"Importantly, we show that modeling character-specific context and pretraining on freeresponse data can boost labeling performance.",9 Conclusion,[0],[0]
"While our work only use information present in our dataset, we view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations.",9 Conclusion,[0],[0]
We thank the reviewers for their insightful comments.,Acknowledgments,[0],[0]
"We also thank Bowen Wang, xlab members, Martha Palmer, Tim O’Gorman, Susan W. Brown, and Ghazaleh Kazeminejad for helpful discussions on inter-annotator agreement and the annotation pipeline.",Acknowledgments,[0],[0]
"This work was supported in part by NSF GRFP DGE-1256082, NSF IIS1714566, IIS-1524371, Samsung AI, and DARPA CwC (W911NF-15-1-0543).",Acknowledgments,[0],[0]
Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines.,abstractText,[0],[0]
"To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions.",abstractText,[0],[0]
"Our work presents a new largescale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.",abstractText,[0],[0]
Modeling Naive Psychology of Characters in Simple Commonsense Stories,title,[0],[0]
"Reasoning about other agents’ intentions and being able to predict their behavior is important in multi-agent systems, in which the agents might have different, and sometimes competing, goals.",1. Introduction,[0],[0]
"In this paper, we introduce a new approach for estimating other agents’ unknown goals from their behavior and using those estimates to choose actions.",1. Introduction,[0],[0]
"We demonstrate that in the proposed tasks, using an explicit model of the other player leads to better performance than simply considering the other agent as part of the environment.
",1. Introduction,[0],[0]
"We frame the problem as a two-player stochastic game (Shapley, 1953), in which each agent is randomly assigned a different goal from a fixed set, which is shared between the agents.",1. Introduction,[0],[0]
"Players have full visibility of the environment, but no direct knowledge of the other’s goal and no communication channel.",1. Introduction,[0],[0]
"The reward obtained by each agent at the end of an episode depends on the goals of both agents, so an optimal policy must take into account both of their goals.
",1. Introduction,[0],[0]
"The key idea of this work is that as a first approximation
1New York University, New York City, USA 2Facebook AI Research, New York City, USA.",1. Introduction,[0],[0]
"Correspondence to: Roberta Raileanu <raileanu@cs.nyu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
of understanding what the other player is trying to achieve, an agent should ask itself “what would be my goal if I had acted as the other player had?”.",1. Introduction,[0],[0]
We instantiate this idea by parametrizing the agent’s action and value functions with a neural network that takes as input the observation state and a goal.,1. Introduction,[0],[0]
"As the agent plays the game, it uses its own policy (with the input expressed in the other agent’s frame of reference) to maximize the likelihood of the other’s observed actions and optimize directly over the goal representation to infer the other agent’s unknown goal.",1. Introduction,[0],[0]
"In contrast with the current literature, our approach does not require building any model of the other agent in order to infer its intention and predict its behavior.",1. Introduction,[0],[0]
"Background: A two-player Markov game is defined by a set of states S describing the possible configurations of all agents, a set of actions A1,A2 and a set of observations S1,S2 for each agent, and a transition function T : S × A1 ×A2 → S which gives the probability distribution on the next state as a function of current state and actions.",2. Approach,[0],[0]
"Each agent i chooses actions by sampling from a stochastic policy πi : S × Ai → [0, 1].",2. Approach,[0],[0]
The reward function of each agent is: ri : S ×A1 ×A2 → R.,2. Approach,[0],[0]
Each agent i aims to maximize its discounted return from time t onward: Rit = ∑∞ t=0,2. Approach,[0],[0]
"γ
trit, where rit is the reward obtained by agent i at time t and γ ∈ (0, 1] is the discount factor.",2. Approach,[0],[0]
"In this work, we consider both cooperative and adversarial settings.",2. Approach,[0],[0]
"In cooperative games, the agents have the same reward function: r1 = r2.
",2. Approach,[0],[0]
"We now describe Self Other-Modeling (SOM), a new approach for inferring other agents’ goals in an online fashion and using these estimates to choose actions.",2. Approach,[0],[0]
"To decide an action and to estimate the value of a state, we use a neural network f that takes as input its own goal zself , an estimate of the other player’s goal z̃other, and the observation state sself , and outputs a probability distribution over actions π and a value estimate V , such that for each agent i playing the game we have:[
πi V i ] = f(siself , z i self , z̃ i other; θ i) .
",2. Approach,[0],[0]
"Here θi are agent i’s parameters for f , which has one softmax output for the policy, one linear output for the value function, and all the non-output layers shared.",2. Approach,[0],[0]
"The actions
are sampled from policy πi.",2. Approach,[0],[0]
"The state siself contains the observation features from agent i’s viewpoint.
",2. Approach,[0],[0]
We propose that each agent models the behavior of the other player using its own policy.,2. Approach,[0],[0]
"Thus, each agent uses its own network f in two ways: acting mode, in which the agent uses f to choose its actions and inference mode, in which the agent uses f to infer the other agent’s goal.",2. Approach,[0],[0]
"For notation purposes, whenever f is used in acting mode (inference mode) we will refer to it as fself (fother):
acting mode: fself (sself , zself , z̃other; θ) (1)
inference mode: fother(sother, z̃other, zself ; θ).",2. Approach,[0],[0]
"(2)
The two modes have different relative placements of the network’s inputs zself and z̃other.",2. Approach,[0],[0]
"Additionally, since the environment is fully observed, the observation state of the two agents differs only by the specification of the agent’s identity on the map (i.e. each agent will be able to distinguish between its own location and the other’s location).",2. Approach,[0],[0]
"Hence, in acting mode, the network fself will take as input sself (with the identity of the acting agent at the location of the self ) and in inference mode, the network fother will take as input sother (with the identity of the acting agent at the location of the other).
",2. Approach,[0],[0]
"At each step, the agent uses equation (2) to output an estimate of the probability distribution over the other agent’s actions.",2. Approach,[0],[0]
"Then, the agent uses supervision of the other’s true action to backpropagate through fother (without updating its paramters) and directly optimize over its input z̃other, the estimate of the other agent’s goal.",2. Approach,[0],[0]
The number of optimization steps used to update z̃other is a hyperparameter that can vary with the game.,2. Approach,[0],[0]
The new estimate z̃other is used as input to fself in (1) for choosing the self agent’s next action.,2. Approach,[0],[0]
"Figure 1 illustrates this technique.
",2. Approach,[0],[0]
"Note that the network f is never updated during inference mode (i.e. using supervision of the other agent’s actions), f ’s parameters θ are updated only at the end of each episode using Asynchronous Advantage Actor-Critic (A3C) (Mnih et al., 2016) with reward signal obtained by the self agent.",2. Approach,[0],[0]
"In contrast, z̃other is updated (multiple times) at each step in the game.
",2. Approach,[0],[0]
Algorithm 1 represents the pseudo-code for training a SOM agent for one episode.,2. Approach,[0],[0]
The procedure is formulated from the viewpoint of a single agent.,2. Approach,[0],[0]
"Since the goals are discrete in all the tasks considered here, the agent’s goal zself is encoded as a one-hot vector of dimension equal to the total number of possible goals in the game.",2. Approach,[0],[0]
"In line 6, siself is the self’s observation state from the perspective of agent i, which is the same as the other’s observation state from the perspective of agent j, sjother.
",2. Approach,[0],[0]
"We consider a continuous vector z̃other of the same dimension as zself , such that the estimate of the other agent’s
Algorithm 1 SOM training for one episode 1: procedure SELF OTHER-MODELING 2: for k := 1, num players do 3: z̃kother ← 1ngoals1ngoals 4: game.reset() 5: for step := 1, episode length do 6: siself = s j other ← game.get state()
7: z̃OH,iother = one hot(argmax(softmax(z̃",2. Approach,[0],[0]
"i other)) 8: πiself , V i self ← f iself (siself , ziself , z̃ OH,i other; θ i)
9: aiself ∼ πiself 10: game.action(aiself ) 11: for k : = 1, num inference steps do 12: z̃GS,jother = gumbel soft(softmax(z̃",2. Approach,[0],[0]
j other)),2. Approach,[0],[0]
13:,2. Approach,[0],[0]
"π̃jother← f j other(s j other, z̃ GS,j other, z j self ; θ j) 14: loss = cross entropy loss(π̃jother, a i self ) 15: loss.backward() 16: update(z̃jother) 17: for k := 1, num players do 18: policy.update(θk)
goal is a sample from the Categorical distribution with class probabilities softmax(z̃other).",2. Approach,[0],[0]
"Thus, the estimate of the other’s goal is given by the one-hot vector z̃OHother, as shown in line 7.",2. Approach,[0],[0]
"At the beginning of each game, the estimate of the other’s goal z̃OHother is randomly initialized, as illustrated in line 3, where 1ngoals represents a vector of all ones with the size equal to the number of possible goals.
",2. Approach,[0],[0]
"In inference mode, the estimate of the other agent’s goal is expressed as a sample from the Gumbel-Softmax distribution (Jang et al., 2016; Maddison et al., 2016), z̃GSother, as shown in line 12, where gumbel soft(p) = softmax[g + log(p)]), with g sampled from the Gumbel distribution and the softmax temperature τ = 1.",2. Approach,[0],[0]
"To update the estimate of the other’s goal, we directly optimize z̃other by using the cross-entropy loss to backpropagate through fother (lines 14, 15, 16).
",2. Approach,[0],[0]
"The agents’ policies are parametrized by long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997)
with two fully-connected linear layers, and exponential linear unit (ELU) (Clevert et al., 2015) activations.",2. Approach,[0],[0]
"The weights of the networks are initialized with semi-orthogonal matrices, as described in Saxe et al. (2013) and zero bias.",2. Approach,[0],[0]
Multi-Agent Learning.,3. Related Work,[0],[0]
"Recent work in deep multi-agent RL focuses on partially visible, fully cooperative settings (Foerster et al., 2016a;b; Omidshafiei et al., 2017) and emergent communication (Lazaridou et al., 2016; Foerster et al., 2016a; Sukhbaatar et al., 2016; Das et al., 2017; Mordatch & Abbeel, 2017).",3. Related Work,[0],[0]
"Lerer & Peysakhovich (2017) design RL agents that are able to maintain cooperation in complex social dilemmas by generalizing a well-known game theoretic strategy called tit-for-tat (Axelrod, 2006), to multiagent Markov games.",3. Related Work,[0],[0]
Leibo et al. (2017) considers semicooperative multi-agent environments in which the agents develop cooperative or competitive strategies depending on the task type and reward structure.,3. Related Work,[0],[0]
"Similarly, Lowe et al. (2017) proposes a centralized actor-critic architecture for efficient training in settings with such mixed strategies.",3. Related Work,[0],[0]
"Our setting is different since we do not allow communication between the agents, so the players have to indirectly reason about others’ intentions from their observed behavior.
",3. Related Work,[0],[0]
Intent Recognition.,3. Related Work,[0],[0]
"Research on plan, activity, and intent recognition has a long history, but it usually assumes domain knowledge or a form of rationality and uses techniques such as Bayesian inference or Hidden Markov Models (Sukthankar et al., 2014).",3. Related Work,[0],[0]
"The field of inverse reinforcement learning (IRL) (Russell, 1998; Ng et al., 2000; Abbeel & Ng, 2004) is also related to the problem considered here.",3. Related Work,[0],[0]
"IRL’s aim is to infer the reward function of an agent by observing its behavior, which is assumed to be nearly optimal.",3. Related Work,[0],[0]
"In contrast, our approach uses the observed actions of the other player to directly infer its goal in an online manner, which is then used by the agent when acting in the environment.",3. Related Work,[0],[0]
"This avoids the need for collecting offline samples of the other’s (state, action) pairs in order to estimate its reward function and use it to learn a policy.",3. Related Work,[0],[0]
"The more recent papers by Hadfield-Menell et al. (2016; 2017) are also concerned with the problem of inferring intentions, but their focus is on human-robot interaction and value alignment.",3. Related Work,[0],[0]
"Motivated by similar goals, Chandrasekaran et al. (2017) consider the problem of building a theory of AI’s mind, in order to improve human-AI interaction and the interpretability of AI systems.",3. Related Work,[0],[0]
"Recent work in cognitive science attempts to understand human decision-making by using a hierarchical model of social agency that infers human intentions for choosing a strategy (Kleiman-Weiner et al., 2016).",3. Related Work,[0],[0]
"However, none of these papers design algorithms that explicitly model other artificial agents in the environment or estimate their intentions, with the purpose of improving their decision
making.
",3. Related Work,[0],[0]
Modeling Other Agents.,3. Related Work,[0],[0]
Opponent modeling has been extensively studied in games of imperfect information.,3. Related Work,[0],[0]
Yet most previous approaches focuses on developing models with domain-specific probabilistic priors or strategy parametrizations.,3. Related Work,[0],[0]
"In contrast, our work proposes a more general framework for opponent modeling.",3. Related Work,[0],[0]
"Davidson (1999) uses an MLP to predict opponent actions given a game history, but the agents cannot adapt to their opponents’ behavior online.",3. Related Work,[0],[0]
"Lockett et al. (2007) designs a neural network architecture to identify the opponent type by learning a mixture of weights over a given set of cardinal opponents, but the game does not unfold within the RL framework.
",3. Related Work,[0],[0]
The closest work to ours is Foerster et al. (2017) and He et al. (2016).,3. Related Work,[0],[0]
Foerster et al. (2017) designs RL agents that take into account the learning of other agents in the environment when updating their own policies.,3. Related Work,[0],[0]
This enables the agents to discover self-interested yet collaborative strategies such as tit-for-tat in the iterated prisoner’s dilemma.,3. Related Work,[0],[0]
"While our work does not explicitly attempt to shape the learning of other agents, it has the advantage that agents can update their beliefs during an episode and change their strategies online to gain more reward.",3. Related Work,[0],[0]
"Our setting is also different in that it considers that each agent has some hidden information needed by the other player to maximize its return.
",3. Related Work,[0],[0]
"Our work is very much in line with He et al. (2016), where the authors build a general framework for modeling other agents in the reinforcement learning setting.",3. Related Work,[0],[0]
He et al. (2016) proposes a model that jointly learns a policy and the behavior of opponents by encoding observations of the opponent into a DQN.,3. Related Work,[0],[0]
Their Mixture of Experts architecture is able to discover different opponent strategy patterns in two competitive tasks.,3. Related Work,[0],[0]
"In our approach, rather than using hand designed features of the other agent’s behavior, the agent models others using its own policy.",3. Related Work,[0],[0]
"Another difference is that in this work, the agent runs an optimization over the input vector to infer the other agent’s hidden goal, rather than using a feed-forward network.",3. Related Work,[0],[0]
"In the experiments below, we show that SOM outperforms an adaptation of the method of He et al. (2016) to our setting.",3. Related Work,[0],[0]
"In this section, we evaluate our model SOM on three tasks:
•",4. Experiments,[0],[0]
"The coin game, in Section 4.2, which is a fully cooperative task where the agents’ roles are symmetric.
",4. Experiments,[0],[0]
"• The recipe game, in Section 4.3, which is adversarial, but with symmetric roles.
",4. Experiments,[0],[0]
"• The door game, in Section 4.4, which is fully cooperative but has asymmetric roles for the two players.
",4. Experiments,[0],[0]
We compare SOM to three other baselines and to a model that has access to the ground truth of the other agent’s goal.,4. Experiments,[0],[0]
"All the tasks considered are created in the Mazebase gridworld environment (Sukhbaatar et al., 2015).",4. Experiments,[0],[0]
"TRUE-OTHER-GOAL (TOG): We provide an upper bound on the performance of our model given by a policy network which takes the other agent’s true goal as input zother, as well as the state features sself and its own goal zself .",4.1. Baselines,[0],[0]
"Since this model has direct access to the true goal of the other agent, it does not need a separate network to model the behavior of the other agent.",4.1. Baselines,[0],[0]
"The architecture of TOG is the same as the one of SOM’s policy network, f .
",4.1. Baselines,[0],[0]
NO-OTHER-MODEL (NOM): The first baseline we use only takes as inputs the observation states sself and its own goal zself .,4.1. Baselines,[0],[0]
"NOM has the same architecture as the one used for SOM’s policy network, fself .",4.1. Baselines,[0],[0]
"This baseline does not explicitly model the other agent’s policy, goal, or actions.
",4.1. Baselines,[0],[0]
"INTEGRATED-POLICY-PREDICTOR (IPP): Starting with the architecture and inputs of NOM, we construct a stronger baseline, IPP, which has an additional final linear layer that outputs a probability distribution over the next action of the other agent.",4.1. Baselines,[0],[0]
"Besides the A3C loss used to train the policy of this network, we also add a cross-entropy loss to train the prediction of the other agent’s action, using observations of its true actions.
",4.1. Baselines,[0],[0]
SEPARATE-POLICY-PREDICTOR (SPP): He et al. (2016) propose an opponent modeling framework based on DQN.,4.1. Baselines,[0],[0]
"In their approach, a neural network (separate from the learned Q-network) is trained to predict the opponents actions given hand crafted state information specific to the opponent.",4.1. Baselines,[0],[0]
"An intermediate hidden representation from this network is given as input to the Q-network.
",4.1. Baselines,[0],[0]
We adapt the model of He et al. (2016) to our setting.,4.1. Baselines,[0],[0]
"In particular, we use A3C instead of DQN",4.1. Baselines,[0],[0]
"and we do not use the task-specific features used to represent the hidden goal of the opponent.
",4.1. Baselines,[0],[0]
"The resulting model, SPP, consists of two separate networks, a policy network for deciding the agent’s actions, and an opponent network for predicting the other agent’s actions.",4.1. Baselines,[0],[0]
"The opponent network takes as input its own state observation sself and goal zself , and outputs a probability distribution for the action taken by the other agent at the next step, as well as its hidden (recurrent) state.",4.1. Baselines,[0],[0]
"As in IPP, we train the opponent policy predictor with a cross-entropy loss using the true actions of the other agent.",4.1. Baselines,[0],[0]
"At each step, the hidden (recurrent) state outputted by this network is taken as input by the agent’s policy network, along with the observation state and its own goal.",4.1. Baselines,[0],[0]
"Both the policy network and the opponent policy predictor are LSTMs with the same
architecture as SOM.
",4.1. Baselines,[0],[0]
"In contrast to SOM, SPP does not explicitly infer the other agent’s goal.",4.1. Baselines,[0],[0]
"Rather, it builds an implicit model of the opponent by predicting the agent’s actions at each time step.",4.1. Baselines,[0],[0]
"In SOM, an inferred goal is given as additional input to the policy network.",4.1. Baselines,[0],[0]
"The analog of the inferred goal in SPP is the hidden (recurrent) state obtained from the opponent policy predictor which is given as an additional input to the policy network.
",4.1. Baselines,[0],[0]
Training Details.,4.1. Baselines,[0],[0]
"In all our experiments, we train the agents’ policies using A3C (Mnih et al., 2016) with an entropy coefficient of 0.01, a value loss coefficient of 0.5, and a discount factor of 0.99.",4.1. Baselines,[0],[0]
"The parameters of the agents’ policies are optimized using Adam (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999, = 1×10−8, and weight decay 0.",4.1. Baselines,[0],[0]
"SGD with a learning rate of 0.1 was used for inferring the other agent’s goal, z̃other.
",4.1. Baselines,[0],[0]
The hidden layer dimension of the policy network was 64 for the Coin and Recipe Games and 128 for the Door Game.,4.1. Baselines,[0],[0]
"We use a learning rate of 1×10−4 for all games and models.
",4.1. Baselines,[0],[0]
The observation state s is represented by few-hot vectors indicating the locations of all the objects in the environment (including the other player).,4.1. Baselines,[0],[0]
"The dimension of this input state is 1 × nfeatures, where the number of features is 384, 192, and 900 for the Coin, Recipe, and Door games, respectively.
",4.1. Baselines,[0],[0]
"For each experiment, we trained the models using 5 different random seeds.",4.1. Baselines,[0],[0]
"All the results shown are for 10 optimization updates of z̃other at each step in the game, unless mentioned otherwise.",4.1. Baselines,[0],[0]
"First, we evaluate the model on a fully cooperative task, in which the agents can gain more reward when using both of their goals rather than only their own goal.",4.2. Coin Game.,[0],[0]
So it is in the best interest of each agent to estimate the other player’s goal and use that information when taking actions.,4.2. Coin Game.,[0],[0]
"The game, shown in the left diagram of Figure 4, takes place on a 8× 8 grid containing 12 coins of 3 different colors (4 coins of each color).",4.2. Coin Game.,[0],[0]
"At the beginning of each episode, the agents are randomly assigned one of the three colors.",4.2. Coin Game.,[0],[0]
"The action space consists of: go up, down, left, right, or pass.",4.2. Coin Game.,[0],[0]
"Once an agent steps on a coin, that coin disappears from the grid.",4.2. Coin Game.,[0],[0]
The game ends after 20 steps.,4.2. Coin Game.,[0],[0]
"The reward received by both agents at the end of the game is given by the formula below:
R(cself , cother) =",4.2. Coin Game.,[0],[0]
(n self Cself + notherCself ),4.2. Coin Game.,[0],[0]
"2 + (nselfCother + n other Cother )2
− (nselfCneither + n other Cneither )2,
where notherCself is the number of coins of the self’s goal-color, which were collected by the other agents, and nselfCneither is
the number of coins corresponding to neither of the agents’ goals, collected by the self.",4.2. Coin Game.,[0],[0]
"For the example in Figure 4, agent 1 has Cself = orange and Cother = cyan, while agent 2’s Cself is cyan and Cother is orange.",4.2. Coin Game.,[0],[0]
"Cneither is red for both agents.
",4.2. Coin Game.,[0],[0]
"The role of the penalty for collecting coins that do not correspond to any of the agents’ goals is to avoid convergence to a greedy policy in which the agents can gain a non-negligible amount of reward by collecting all the coins in their proximity, without any regard to their color.
",4.2. Coin Game.,[0],[0]
"To maximize its return, each agent needs maximize the number of collected coins of its own or its collaborator’s color, and minimize the number of coins of the remaining color.",4.2. Coin Game.,[0],[0]
"Hence, when both agents are able to infer their collaborators’ goals with high accuracy and as early as possible in the game, they can use that information to maximize their
shared utility.
",4.2. Coin Game.,[0],[0]
Figure 3 shows the mean and standard deviation of the reward across 5 runs with different random seeds obtained by SOM.,4.2. Coin Game.,[0],[0]
Our model clearly outperforms all other baselines on this task.,4.2. Coin Game.,[0],[0]
"We also show the empirical upper bound on the reward using the model which takes as input the true color assigned to the other agent.
",4.2. Coin Game.,[0],[0]
Figure 2 analyzes the strategies of the different models by looking at the proportion of coins of each type collected by the agents.,4.2. Coin Game.,[0],[0]
"The optimal strategy is for each agent to maximize nselfCself + n self Cother
and minimize nselfCneither .",4.2. Coin Game.,[0],[0]
"Due to the randomness in the initial conditions (in particular, the locations of coins in the environment), this amounts to each agent collecting an equal number of coins of its own and of the other’s color on average, across a large number of episodes (i.e. n̄selfCself = n̄ self Cother ).
",4.2. Coin Game.,[0],[0]
"Indeed, this is the strategy learned by the model with perfect information of the other agent’s goal (TOG).",4.2. Coin Game.,[0],[0]
"SOM also learns to collect significantly more Other than Neither coins (although not as many as Self coins), indicating its ability to distinguish between the two types, at least during some of the episodes.",4.2. Coin Game.,[0],[0]
"This means that SOM can accurately infer the other agent’s goal early enough during the episode and use that information to collect more Other Coins, thus gaining more reward than if it were only using its own goal to direct its actions.
",4.2. Coin Game.,[0],[0]
"In contrast, the agents trained with the three baseline models collect significantly more Self coins, and as many Other as Neither coins on average.",4.2. Coin Game.,[0],[0]
"This shows that they learn to use their own goal for gaining reward, but they are unable to use the hidden goal of the other agent for further increasing their returns.",4.2. Coin Game.,[0],[0]
"Even if IPP and SPP are able to predict the actions of the other player with an accuracy of about 50%, they do not learn to distinguish between the coins that would increase (Other) and those that would decrease (Neither)
their reward.",4.2. Coin Game.,[0],[0]
This shows the weaknesses of using an implicit model of the other agent to maximize reward on certain tasks.,4.2. Coin Game.,[0],[0]
"Agents in adversarial scenarios have competing goals, so the ability of inferring the opponent’s goal could better inform the agent’s actions.",4.3. Recipe Game.,[0],[0]
"With this motivation in mind, we evaluate our model on a game in which the agents have to craft certain compositional recipes, each containing multiple items found in the environment.",4.3. Recipe Game.,[0],[0]
"The agents are given as input the names of their goal-recipes, without the corresponding components needed to make it.",4.3. Recipe Game.,[0],[0]
"The resources in the environment are scarce, so only one of the agents can craft its recipe within one episode.
",4.3. Recipe Game.,[0],[0]
"As illustrated in Figure 4 (center), there are 4 types of items: {sun, star, moon, lightning} and 4 recipes: {sun, sun, star}; {star, star, moon}; {moon, moon, lightning}; {lightning, lightning, sun}.",4.3. Recipe Game.,[0],[0]
"The game is played in a 4× 6 grid, which contains 8 items in total, 2 of each type.
",4.3. Recipe Game.,[0],[0]
"At the beginning of each episode, we randomly assign a recipe to one of the agents, and then we randomly pick a recipe for the other agent so that it has overlapping items with the recipe of the first agent.",4.3. Recipe Game.,[0],[0]
This ensures that the agents are competing for resources within each episode.,4.3. Recipe Game.,[0],[0]
"At the end of the episode, each agent receives a reward of +1 for crafting its own recipe and a penalty of -0.1 for each item it picked up not needed for making its recipe.
",4.3. Recipe Game.,[0],[0]
We designed the layout of the grid so that neither agent has an initial advantage by being closer to the scarce resource.,4.3. Recipe Game.,[0],[0]
"At the beginning of each episode, one of the agents starts on the left-most column of the grid, while the other one starts on the right-most column, at the same y-coordinate.",4.3. Recipe Game.,[0],[0]
Their initial y-coordinate as well as which agent starts on the left/right is randomized.,4.3. Recipe Game.,[0],[0]
"Similarly, one item of each of the 4 different types is placed at random in the grid formed
by the second and third columns of the maze, from left to right.",4.3. Recipe Game.,[0],[0]
"The rest of the items are placed in the forth and fifth columns, so that the symmetry with respect to the vertical axis is preserved (i.e. items of the same type are placed at the same y-coordinate, and symmetric x-coordinates).
",4.3. Recipe Game.,[0],[0]
"Agents have six actions to choose from: pass, go up, down, left, right, or pick up (for picking up an item, which then disappears from the grid).",4.3. Recipe Game.,[0],[0]
The first agent to take an action is randomized.,4.3. Recipe Game.,[0],[0]
"The game ends after 50 steps.
",4.3. Recipe Game.,[0],[0]
"We pretrain all baselines on a version of the game which does not have overlapping recipes, in order to ensure that all the models learn to pick up the corresponding items, given a recipe as goal.",4.3. Recipe Game.,[0],[0]
All of the models learn to craft their assigned recipes ∼ 90% of the time on this simpler task.,4.3. Recipe Game.,[0],[0]
"Then, we continue training the models on the adversarial task in which their recipes overlap in each episode.",4.3. Recipe Game.,[0],[0]
"SOM is initialized with a pretrained NOM network.
",4.3. Recipe Game.,[0],[0]
Figure 5 shows the winning fraction for different pairs played against each other in the Recipe game.,4.3. Recipe Game.,[0],[0]
"For the first 100k episodes, the models are not being trained.",4.3. Recipe Game.,[0],[0]
"We can see that SOM significantly outperfroms NOM, IPP, and SPP, winning ∼ 75 − 80% of the time, while the baselines can only win ∼ 15− 20% of the games.",4.3. Recipe Game.,[0],[0]
"SPP ties against NOM, and TOG outperforms SOM by a large margin.",4.3. Recipe Game.,[0],[0]
We also played the same types of agents against each other and they all win ∼ 40− 50% of the games.,4.3. Recipe Game.,[0],[0]
"In this section, we show that on a collaborative task with asymmetric roles and multiple possible partners, the agents can learn to figure out what role they should be playing in each game based on their partners’ actions.
",4.4. Door Game.,[0],[0]
"In the Door game, two agents are located in a 5 × 9 grid, with 5 goals behind 5 doors on the left wall, and 5 switches on the right wall of the grid.",4.4. Door Game.,[0],[0]
"The game starts with the two players in random squares on the grid, except for the ones occupied by the goals, doors, or switches, as illustrated in Figure 4.",4.4. Door Game.,[0],[0]
"Agents can take any of the five actions: go up, down, left, right or pass.",4.4. Door Game.,[0],[0]
An action is invalid if it moves the player outside of the border or to a square occupied by a block or closed door.,4.4. Door Game.,[0],[0]
Both agents receive +3 reward when either one of them steps on its goal and they are penalized -0.1 for each step they take.,4.4. Door Game.,[0],[0]
The game ends when one of them gets to its goal or after 22 steps.,4.4. Door Game.,[0],[0]
"All the goals are behind doors which are open only as long as one of the agents sits on the corresponding switch for that door.
",4.4. Door Game.,[0],[0]
"At the beginning of an episode, each of the two players is randomly selected from a pool of 5 agents and receives as input a random number from 1 to 5 corresponding to its goal.",4.4. Door Game.,[0],[0]
Each of the 5 agents has its own policy which gets updated at the end of each episode they play.,4.4. Door Game.,[0],[0]
"Note that the agents’
identities are not visible (i.e. there is no indication in the state features that specifies the id’s of the agents playing during a given episode).",4.4. Door Game.,[0],[0]
"This restriction is important in order to ensure that the agents cannot gain advantage by specializing into the two roles needed to win (i.e. goal-goer and switch-puller) and identifying the specialization of the other player by simply observing its unique id.
The agents need to cooperate in order to receive reward.",4.4. Door Game.,[0],[0]
"In contrast to our previous tasks, the two players must take different roles.",4.4. Door Game.,[0],[0]
"In fact, the player who sits on the switch should ignore its own goal and instead infer the other’s goal, while the player who goes to its goal does not need to infer the other’s goal, but only use its own.",4.4. Door Game.,[0],[0]
"In order to sit on the correct switch, an agent has to infer the other player’s goal from their observed actions.",4.4. Door Game.,[0],[0]
"The only way in which an agent can use its own policy to model the other player is if each agent learns to play both roles of the game, i.e. go to its own goal and also open its collaborator’s door by sitting on the corresponding switch.",4.4. Door Game.,[0],[0]
"Indeed, we see that the agents learn to play both roles and they are able to use their own policies to infer the other player’s goals when needed.
",4.4. Door Game.,[0],[0]
Fig 6 shows the mean and standard deviation of the winning fraction obtained by one of the agents on the Door game.,4.4. Door Game.,[0],[0]
"While our model is still able to outperform the three baselines, the gap between the performance of our model and that of IPP or SPP (an approximate version of (He et al., 2016)) is smaller than in the previous tasks.",4.4. Door Game.,[0],[0]
"However, this is a more difficult task for our model since it needs the agent to learn both roles before effectively using its own policy to infer the other agent’s goal.",4.4. Door Game.,[0],[0]
"The plot shows that SOM actually performs worse than IPP and SPP during the initial part of training, before outperforming them.",4.4. Door Game.,[0],[0]
"Nevertheless, we see that SOM training allows the agents to play both roles in an asymmetric cooperative game, and to infer the goal and role of the other player.",4.4. Door Game.,[0],[0]
"In this section we further analyze the ability of the SOM models to infer other’s intended goals.
",4.5. Analyzing the goal inference,[0],[0]
Figure 7 shows the fraction of episodes in which the goal of the other agent is correctly inferred.,4.5. Analyzing the goal inference,[0],[0]
"We consider that the goal is correctly inferred only when the estimate of the other’s goal remains accurate until the end of the game, so that we avoid counting the episodes in which the agent might infer the correct goal by chance at some intermediate step in the game.",4.5. Analyzing the goal inference,[0],[0]
"In all the games, the SOM agent learns to infer the other player’s goal with a mean accuracy ranging
from ∼",4.5. Analyzing the goal inference,[0],[0]
60− 80%.,4.5. Analyzing the goal inference,[0],[0]
"Comparing the second plot in Figure 2 with the left plot in Figure 7, one can observe that the SOM agent starts distinguishing Other from Neither coins after approximately 2M training episodes, which coincides with the time when the mean accuracy of the inferred goal converges to ∼ 75%.",4.5. Analyzing the goal inference,[0],[0]
"The Door Game (right) presents higher variance since the agents learn to use and infer the other’s goal at different stages during training.
",4.5. Analyzing the goal inference,[0],[0]
Figure 8 shows the cumulative distribution of the step at which the goal of the other player is correctly inferred (and remains the same until the end of the game).,4.5. Analyzing the goal inference,[0],[0]
The cumulative distribution is computed over the episodes in which the goal is correctly inferred before the end of the game.,4.5. Analyzing the goal inference,[0],[0]
"In the Coin (blue) and Recipe (red) games, 80% of the times the agent correctly infers the goal of the other, it does so in the first five steps.",4.5. Analyzing the goal inference,[0],[0]
The distribution for the Door (green) game indicates that the agent needs more steps on average to correctly infer the goal.,4.5. Analyzing the goal inference,[0],[0]
This explains in part why the SOM agent only slightly outperforms the SPP baseline.,4.5. Analyzing the goal inference,[0],[0]
"If the agent does not infer the other’s goal early enough in the episode, it cannot efficiently use it to maximize its return.
",4.5. Analyzing the goal inference,[0],[0]
"Figure 9 shows how the performance of the agent varies with
the number of optimization updates performed on z̃other at each step in the game.",4.5. Analyzing the goal inference,[0],[0]
"As expected, the agent’s reward (blue) generally increases with the number of inference steps, as does the fraction of episodes in which the goal is correctly inferred.",4.5. Analyzing the goal inference,[0],[0]
"One should note that increasing the number of inference steps from 10 to 20 only translates into less than 0.45% performance gain, while increasing it from 1 to 5 translates into a performance gain of 6.9% on the Coin game, suggesting that there is a certain threshold above which increasing the number of inference steps will not significantly improve performance.",4.5. Analyzing the goal inference,[0],[0]
Summary.,5. Discussion,[0],[0]
"In this paper, we introduced a new approach for inferring other agents’ hidden goals from their behavior and using those estimates to choose actions.",5. Discussion,[0],[0]
"We demonstrated that the agents are able to estimate others’ hidden goals in both cooperative and competitive settings, which enables them to converge to better policies.",5. Discussion,[0],[0]
"In the proposed tasks, using an explicit model of the other player led to better performance than simply considering the other agent as part of the environment.
Strengths.",5. Discussion,[0],[0]
Some of the main advantages of our method are its simplicity and flexibility.,5. Discussion,[0],[0]
"This method does not require any extra parameters to model other agents in the environment, can be trained with any reinforcement learning algorithm, and can be easily integrated with any network architecture.",5. Discussion,[0],[0]
"SOM can also be adapted to settings with more than two players, since the agent can use its own policy to model the behavior of any number of agents and infer their goals.",5. Discussion,[0],[0]
"Moreover, it can be easily generalized to numerous other environments and tasks.
Limitations.",5. Discussion,[0],[0]
Our approach is based on the assumption that the agents are identical or that their transition functions are independent and identically distributed.,5. Discussion,[0],[0]
"Hence, the framework is expected to be more suitable for symmetric games, in which the agents share a fixed set of goals and have similar abilities, and we expect a degradation of performance for asymmetric games.",5. Discussion,[0],[0]
Our experiments confirm this observation.,5. Discussion,[0],[0]
"Another limitation of SOM is that it requires a longer training time than other baselines, since we backpropagate through the network at each step.",5. Discussion,[0],[0]
"However, their online nature is essential in adapting to the behavior of other agents in the environment.
",5. Discussion,[0],[0]
Future Work.,5. Discussion,[0],[0]
"We plan to extend this work by evaluating the models on more complex environments and model deviations from the assumption that the players have identical policies, given a certain goal and state of the world.",5. Discussion,[0],[0]
"Another important avenue for future research is to design models that can adapt to non-stationary strategies of others in the environment, as well as to tasks with hierarchical goals.",5. Discussion,[0],[0]
This work was partially supported by ONR grant N0001413-1-0646.,Acknowledgements,[0],[0]
The authors wish to thank Alex Peysakhovich and Adam Lerer for helpful discussions.,Acknowledgements,[0],[0]
We consider the multi-agent reinforcement learning setting with imperfect information.,abstractText,[0],[0]
"The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns.",abstractText,[0],[0]
"We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner.",abstractText,[0],[0]
"We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.",abstractText,[0],[0]
Modeling Others using Oneself in Multi-Agent Reinforcement Learning,title,[0],[0]
"Being able to anticipate upcoming content is a core property of human language processing (Kutas et al., 2011; Kuperberg and Jaeger, 2016) that has received a lot of attention in the psycholinguistic literature in recent years.",1 Introduction,[0],[0]
Expectations about upcoming words help humans comprehend language in noisy settings and deal with ungrammatical input.,1 Introduction,[0],[0]
"In this paper, we use a computational model to address the question of how different layers of knowledge (linguistic knowledge as well as common-sense knowledge) influence human anticipation.
",1 Introduction,[0],[0]
Here we focus our attention on semantic predictions of discourse referents for upcoming noun phrases.,1 Introduction,[0],[0]
"This task is particularly interesting because it allows us to separate the semantic task of antic-
ipating an intended referent and the processing of the actual surface form.",1 Introduction,[0],[0]
"For example, in the context of I ordered a medium sirloin steak with fries.",1 Introduction,[0],[0]
"Later, the waiter brought . . .",1 Introduction,[0],[0]
", there is a strong expectation of a specific discourse referent, i.e., the referent introduced by the object NP of the preceding sentence, while the possible referring expression could be either the steak I had ordered, the steak, our food, or it.",1 Introduction,[0],[0]
Existing models of human prediction are usually formulated using the informationtheoretic concept of surprisal.,1 Introduction,[0],[0]
"In recent work, however, surprisal is usually not computed for DRs, which represent the relevant semantic unit, but for the surface form of the referring expressions, even though there is an increasing amount of literature suggesting that human expectations at different levels of representation have separable effects on prediction and, as a consequence, that the modelling of only one level (the linguistic surface form) is insufficient (Kuperberg and Jaeger, 2016; Kuperberg, 2016; Zarcone et al., 2016).",1 Introduction,[0],[0]
"The present model addresses this shortcoming by explicitly modelling and representing common-sense knowledge and conceptually separating the semantic (discourse referent) and the surface level (referring expression) expectations.
",1 Introduction,[0],[0]
"Our discourse referent prediction task is related to the NLP task of coreference resolution, but it substantially differs from that task in the following ways: 1) we use only the incrementally available left context, while coreference resolution uses the full text; 2) coreference resolution tries to identify the DR for a given target NP in context, while we look at the expectations of DRs based only on the context
ar X
iv :1
70 2.
",1 Introduction,[0],[0]
"03 12
1v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
0 Fe
b 20
before the target NP is seen.",1 Introduction,[0],[0]
The distinction between referent prediction and prediction of referring expressions also allows us to study a closely related question in natural language generation: the choice of a type of referring expression based on the predictability of the DR that is intended by the speaker.,1 Introduction,[0],[0]
"This part of our work is inspired by a referent guessing experiment by Tily and Piantadosi (2009), who showed that highly predictable referents were more likely to be realized with a pronoun than unpredictable referents, which were more likely to be realized using a full NP.",1 Introduction,[0],[0]
"The effect they observe is consistent with a Gricean point of view, or the principle of uniform information density (see Section 5.1).",1 Introduction,[0],[0]
"However, Tily and Piantadosi do not provide a computational model for estimating referent predictability.",1 Introduction,[0],[0]
"Also, they do not include selectional preference or common-sense knowledge effects in their analysis.
",1 Introduction,[0],[0]
"We believe that script knowledge, i.e., commonsense knowledge about everyday event sequences, represents a good starting point for modelling conversational anticipation.",1 Introduction,[0],[0]
This type of common-sense knowledge includes temporal structure which is particularly relevant for anticipation in continuous language processing.,1 Introduction,[0],[0]
"Furthermore, our approach can build on progress that has been made in recent years in methods for acquiring large-scale script knowledge; see Section 1.1.",1 Introduction,[0],[0]
Our hypothesis is that script knowledge may be a significant factor in human anticipation of discourse referents.,1 Introduction,[0],[0]
"Explicitly modelling this knowledge will thus allow us to produce more human-like predictions.
",1 Introduction,[0],[0]
"Script knowledge enables our model to generate anticipations about discourse referents that have already been mentioned in the text, as well as anticipations about textually new discourse referents which have been activated due to script knowledge.",1 Introduction,[0],[0]
"By modelling event sequences and event participants, our model captures many more long-range dependencies than normal language models are able to.",1 Introduction,[0],[0]
"As an example, consider the following two alternative text passages:
We got seated, and had to wait for 20 minutes.",1 Introduction,[0],[0]
"Then, the waiter brought the ...
We ordered, and had to wait for 20 minutes.",1 Introduction,[0],[0]
"Then, the waiter brought the ...
Preferred candidate referents for the object posi-
tion of the waiter brought the ... are instances of the food, menu, or bill participant types.",1 Introduction,[0],[0]
"In the context of the alternative preceding sentences, there is a strong expectation of instances of a menu and a food participant, respectively.
",1 Introduction,[0],[0]
This paper represents foundational research investigating human language processing.,1 Introduction,[0],[0]
"However, it also has the potential for application in assistant technology and embodied agents.",1 Introduction,[0],[0]
"The goal is to achieve human-level language comprehension in realistic settings, and in particular to achieve robustness in the face of errors or noise.",1 Introduction,[0],[0]
"Explicitly modelling expectations that are driven by common-sense knowledge is an important step in this direction.
",1 Introduction,[0],[0]
"In order to be able to investigate the influence of script knowledge on discourse referent expectations, we use a corpus that contains frequent reference to script knowledge, and provides annotations for coreference information, script events and participants (Section 2).",1 Introduction,[0],[0]
"In Section 3, we present a large-scale experiment for empirically assessing human expectations on upcoming referents, which allows us to quantify at what points in a text humans have very clear anticipations vs. when they do not.",1 Introduction,[0],[0]
"Our goal is to model human expectations, even if they turn out to be incorrect in a specific instance.",1 Introduction,[0],[0]
The experiment was conducted via Mechanical Turk and follows the methodology of Tily and Piantadosi (2009).,1 Introduction,[0],[0]
"In section 4, we describe our computational model that represents script knowledge.",1 Introduction,[0],[0]
"The model is trained on the gold standard annotations of the corpus, because we assume that human comprehenders usually will have an analysis of the preceding discourse which closely corresponds to the gold standard.",1 Introduction,[0],[0]
"We compare the prediction accuracy of this model to human predictions, as well as to two baseline models in Section 4.3.",1 Introduction,[0],[0]
One of them uses only structural linguistic features for predicting referents; the other uses general script-independent selectional preference features.,1 Introduction,[0],[0]
"In Section 5, we test whether surprisal (as estimated from human guesses vs. computational models) can predict the type of referring expression used in the original texts in the corpus (pronoun vs. full referring expression).",1 Introduction,[0],[0]
"This experiment also has wider implications with respect to the on-going discussion of whether the referring expression choice is dependent on predictability, as predicted by the uniform information density hy-
pothesis.",1 Introduction,[0],[0]
"The contributions of this paper consist of:
• a large dataset of human expectations, in a variety of texts related to every-day activities.",1 Introduction,[0],[0]
"• an implementation of the conceptual distinction
between the semantic level of referent prediction and the type of a referring expression.",1 Introduction,[0],[0]
"• a computational model which significantly im-
proves modelling of human anticipations.",1 Introduction,[0],[0]
"• showing that script knowledge is a significant
factor in human expectations.",1 Introduction,[0],[0]
"• testing the hypothesis of Tily and Piantadosi
that the choice of the type of referring expression (pronoun or full NP) depends on the predictability of the referent.",1 Introduction,[0],[0]
"Scripts represent knowledge about typical event sequences (Schank and Abelson, 1977), for example the sequence of events happening when eating at a restaurant.",1.1 Scripts,[0],[0]
"Script knowledge thereby includes events like order, bring and eat as well as participants of those events, e.g., menu, waiter, food, guest.",1.1 Scripts,[0],[0]
"Existing methods for acquiring script knowledge are based on extracting narrative chains from text (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014; Rudinger et al., 2015; Modi, 2016; Ahrendt and Demberg, 2016) or by eliciting script knowledge via Crowdsourcing on Mechanical Turk (Regneri et al., 2010; Frermann et al., 2014; Modi and Titov, 2014).
",1.1 Scripts,[0],[0]
"Modelling anticipated events and participants is motivated by evidence showing that event representations in humans contain information not only about the current event, but also about previous and future states, that is, humans generate anticipations about event sequences during normal language
comprehension (Schütz-Bosbach and Prinz, 2007).",1.1 Scripts,[0],[0]
"Script knowledge representations have been shown to be useful in NLP applications for ambiguity resolution during reference resolution (Rahman and Ng, 2012).",1.1 Scripts,[0],[0]
"Ordinary texts, including narratives, encode script structure in a way that is too complex and too implicit at the same time to enable a systematic study of script-based expectation.",2 Data: The InScript Corpus,[0],[0]
"They contain interleaved references to many different scripts, and they usually refer to single scripts in a point-wise fashion only, relying on the ability of the reader to infer the full event chain using their background knowledge.
",2 Data: The InScript Corpus,[0],[0]
"We use the InScript corpus (Modi et al., 2016) to study the predictive effect of script knowledge.",2 Data: The InScript Corpus,[0],[0]
InScript is a crowdsourced corpus of simple narrative texts.,2 Data: The InScript Corpus,[0],[0]
"Participants were asked to write about a specific activity (e.g., a restaurant visit, a bus ride, or a grocery shopping event) which they personally experienced, and they were instructed to tell the story as if explaining the activity to a child.",2 Data: The InScript Corpus,[0],[0]
This resulted in stories that are centered around a specific scenario and that explicitly mention mundane details.,2 Data: The InScript Corpus,[0],[0]
"Thus, they generally realize longer event chains associated with a single script, which makes them particularly appropriate to our purpose.
",2 Data: The InScript Corpus,[0],[0]
"The InScript corpus is labelled with event-type, participant-type, and coreference information.",2 Data: The InScript Corpus,[0],[0]
"Full verbs are labeled with event type information, heads of all noun phrases with participant types, using scenario-specific lists of event types (such as enter bathroom, close drain and fill water for the “taking a bath” scenario) and participant types (such as bather, water and bathtub).",2 Data: The InScript Corpus,[0],[0]
"On average, each template offers a choice of 20 event types and 18 participant
to guess the upcoming referent (indicated by XXXXXX above).",2 Data: The InScript Corpus,[0],[0]
"They can either choose from the previously activated referents, or they can write something new.
types.",2 Data: The InScript Corpus,[0],[0]
The InScript corpus consists of 910 stories addressing 10 scenarios (about 90 stories per scenario).,2 Data: The InScript Corpus,[0],[0]
"The corpus has 200,000 words, 12,000 verb instances with event labels, and 44,000 head nouns with participant instances.",2 Data: The InScript Corpus,[0],[0]
"Modi et al. (2016) report an inter-annotator agreement of 0.64 for event types and 0.77 for participant types (Fleiss’ kappa).
",2 Data: The InScript Corpus,[0],[0]
We use gold-standard event- and participant-type annotation to study the influence of script knowledge on the expectation of discourse referents.,2 Data: The InScript Corpus,[0],[0]
"In addition, InScript provides coreference annotation, which makes it possible to keep track of the mentioned discourse referents at each point in the story.",2 Data: The InScript Corpus,[0],[0]
We use this information in the computational model of DR prediction and in the DR guessing experiment described in the next section.,2 Data: The InScript Corpus,[0],[0]
An example of an annotated InScript story is shown in Figure 1.,2 Data: The InScript Corpus,[0],[0]
"We use the InScript corpus to develop computational models for the prediction of discourse refer-
ents (DRs) and to evaluate their prediction accuracy.",3 Referent Cloze Task,[0],[0]
"This can be done by testing how often our models manage to reproduce the original discourse referent (cf. also the “narrative cloze” task by (Chambers and Jurafsky, 2008) which tests whether a verb together with a role can be correctly guessed by a model).",3 Referent Cloze Task,[0],[0]
"However, we do not only want to predict the “correct” DRs in a text but also to model human expectation of DRs in context.",3 Referent Cloze Task,[0],[0]
"To empirically assess human expectation, we created an additional database of crowdsourced human predictions of discourse referents in context using Amazon Mechanical Turk.",3 Referent Cloze Task,[0],[0]
"The design of our experiment closely resembles the guessing game of (Tily and Piantadosi, 2009) but extends it in a substantial way.
",3 Referent Cloze Task,[0],[0]
"Workers had to read stories of the InScript corpus 1 and guess upcoming participants: for each target NP, workers were shown the story up to this NP excluding the NP itself, and they were asked to guess the next person or object most likely to be referred to.",3 Referent Cloze Task,[0],[0]
"In case they decided in favour of a discourse referent already mentioned, they had to choose among the available discourse referents by clicking an NP in the preceding text, i.e., some noun with a specific, coreference-indicating color; see Figure 2.",3 Referent Cloze Task,[0],[0]
"Otherwise, they would click the “New” button, and would in turn be asked to give a short description of the new person or object they expected to be mentioned.",3 Referent Cloze Task,[0],[0]
"The percentage of guesses that agree with the actually referred entity was taken as a basis for estimating the surprisal.
",3 Referent Cloze Task,[0],[0]
"The experiment was done for all stories of the test set: 182 stories (20%) of the InScript corpus, evenly taken from all scenarios.",3 Referent Cloze Task,[0],[0]
"Since our focus is on the effect of script knowledge, we only considered those NPs as targets that are direct dependents of script-related events.",3 Referent Cloze Task,[0],[0]
Guessing started from the third sentence only in order to ensure that a minimum of context information was available.,3 Referent Cloze Task,[0],[0]
"To keep the complexity of the context manageable, we restricted guessing to a maximum of 30 targets and skipped the rest of the story (this applied to 12% of the stories).",3 Referent Cloze Task,[0],[0]
"We collected 20 guesses per NP for 3346 noun phrase instances, which amounts to a total of around 67K guesses.",3 Referent Cloze Task,[0],[0]
"Workers selected a con-
1The corpus is available at : http://www.sfb1102.",3 Referent Cloze Task,[0],[0]
"uni-saarland.de/?page_id=2582
text NP in 68% of cases and “New” in 32% of cases.",3 Referent Cloze Task,[0],[0]
Our leading hypothesis is that script knowledge substantially influences human expectation of discourse referents.,3 Referent Cloze Task,[0],[0]
The guessing experiment provides a basis to estimate human expectation of already mentioned DRs (the number of clicks on the respective NPs in text).,3 Referent Cloze Task,[0],[0]
"However, we expect that script knowledge has a particularly strong influence in the case of first mentions.",3 Referent Cloze Task,[0],[0]
"Once a script is evoked in a text, we assume that the full script structure, including all participants, is activated and available to the reader.
",3 Referent Cloze Task,[0],[0]
Tily and Piantadosi (2009) are interested in second mentions only and therefore do not make use of the worker-generated noun phrases classified as “New”.,3 Referent Cloze Task,[0],[0]
"To study the effect of activated but not explicitly mentioned participants, we carried out a subsequent annotation step on the worker-generated noun phrases classified as “New”.",3 Referent Cloze Task,[0],[0]
"We presented annotators with these noun phrases in their contexts (with co-referring NPs marked by color, as in the MTurk experiment) and, in addition, displayed all participant types of the relevant script (i.e., the script associated with the text in the InScript corpus).",3 Referent Cloze Task,[0],[0]
Annotators did not see the “correct” target NP.,3 Referent Cloze Task,[0],[0]
"We asked annotators to either (1) select the participant type instantiated by the NP (if any), (2) label the NP as unrelated to the script, or (3), link the NP to an overt antecedent in the text, in the case that the NP is actually a second mention that had been erroneously labeled as new by the worker.",3 Referent Cloze Task,[0],[0]
Option (1) provides a basis for a fine-grained estimation of first-mention DRs.,3 Referent Cloze Task,[0],[0]
"Option (3), which we added when we noticed the considerable number of overlooked antecedents, serves as correction of the results of the M-Turk experiment.",3 Referent Cloze Task,[0],[0]
"Out of the 22K annotated “New” cases, 39% were identified as second mentions, 55% were linked to a participant type, and 6% were classified as really novel.",3 Referent Cloze Task,[0],[0]
"In this section, we describe the model we use to predict upcoming discourse referents (DRs).",4 Referent Prediction Model,[0],[0]
"Our model should not only assign probabilities to DRs already explicitly introduced in the preceding text fragment (e.g., “bath” or “bathroom” for the
cloze task in Figure 2) but also reserve some probability mass for ‘new’ DRs, i.e., DRs activated via the script context or completely novel ones not belonging to the script.",4.1 Model,[0],[0]
"In principle, different variants of the activation mechanism must be distinguished.",4.1 Model,[0],[0]
"For many participant types, a single participant belonging to a specific semantic class is expected (referred to with the bathtub or the soap).",4.1 Model,[0],[0]
"In contrast, the “towel” participant type may activate a set of objects, elements of which then can be referred to with a towel or another towel.",4.1 Model,[0],[0]
"The “bath means” participant type may even activate a group of DRs belonging to different semantic classes (e.g., bubble bath and salts).",4.1 Model,[0],[0]
"Since it is not feasible to enumerate all potential participants, for ‘new’ DRs we only predict their participant type (“bath means” in our example).",4.1 Model,[0],[0]
"In other words, the number of categories in our model is equal to the number of previously introduced DRs plus the number of participant types of the script plus 1, reserved for a new DR not corresponding to any script participant (e.g., cellphone).",4.1 Model,[0],[0]
"In what follows, we slightly abuse the terminology and refer to all these categories as discourse referents.
",4.1 Model,[0],[0]
"Unlike standard co-reference models, which predict co-reference chains relying on the entire document, our model is incremental, that is, when predicting a discourse referent d(t) at a given position t, it can look only in the history h(t) (i.e., the preceding part of the document), excluding the referring expression (RE) for the predicted DR.",4.1 Model,[0],[0]
We also assume that past REs are correctly resolved and assigned to correct participant types (PTs).,4.1 Model,[0],[0]
"Typical NLP applications use automatic coreference resolution systems, but since we want to model human behavior, this might be inappropriate, since an automated system would underestimate human performance.",4.1 Model,[0],[0]
"This may be a strong assumption, but for reasons explained above, we use gold standard past REs.
",4.1 Model,[0],[0]
"We use the following log-linear model (“softmax regression”):
p(d(t) = d|h(t))",4.1 Model,[0],[0]
"= exp(w T f(d, h(t)))∑
d′ exp(w T f(d′, h(t)))
,
where f is the feature function we will discuss in the following subsection, w are model parameters, and the summation in the denominator is over the
set of categories described above.",4.1 Model,[0],[0]
Some of the features included in f are a function of the predicate syntactically governing the unobservable target RE (corresponding to the DR being predicted).,4.1 Model,[0],[0]
"However, in our incremental setting, the predicate is not available in the history h(t) for subject NPs.",4.1 Model,[0],[0]
"In this case, we use an additional probabilistic model, which estimates the probability of the predicate v given the context h(t), and marginalize out its predictions:
p(d(t)=d|h(t))= ∑ v p(v|h(t))",4.1 Model,[0],[0]
"exp(w T f(d, h(t), v))∑ d′ exp(w T f(d′, h(t), v))
",4.1 Model,[0],[0]
"The predicate probabilities p(v|h(t)) are computed based on the sequence of preceding predicates (i.e., ignoring any other words) using the recurrent neural network language model estimated on our training set.2",4.1 Model,[0],[0]
"The expression f(d, h(t), v) denotes the feature function computed for the referent d, given the history composed of h(t) and the predicate v.",4.1 Model,[0],[0]
Our features encode properties of a DR as well as characterize its compatibility with the context.,4.2 Features,[0],[0]
We face two challenges when designing our features.,4.2 Features,[0],[0]
"First, although the sizes of our datasets are respectable from the script annotation perspective, they are too small to learn a richly parameterized model.",4.2 Features,[0],[0]
"For many of our features, we address this challenge by using external word embeddings3 and associate parameters with some simple similarity measures computed using these embeddings.",4.2 Features,[0],[0]
"Con-
",4.2 Features,[0],[0]
"2We used RNNLM toolkit (Mikolov et al., 2011; Mikolov et al., 2010) with default settings.
",4.2 Features,[0],[0]
"3We use 300-dimensional word embeddings estimated on Wikipedia with the skip-gram model of Mikolov et al. (2013): https://code.google.com/p/word2vec/
sequently, there are only a few dozen parameters which need to be estimated from scenario-specific data.",4.2 Features,[0],[0]
"Second, in order to test our hypothesis that script information is beneficial for the DR prediction task, we need to disentangle the influence of script information from general linguistic knowledge.",4.2 Features,[0],[0]
"We address this by carefully splitting the features apart, even if it prevents us from modeling some interplay between the sources of information.",4.2 Features,[0],[0]
We will describe both classes of features below; also see a summary in Table 1.,4.2 Features,[0],[0]
These features are based on Tily and Piantadosi (2009).,4.2.1 Shallow Linguistic Features,[0],[0]
"In addition, we consider a selectional preference feature.",4.2.1 Shallow Linguistic Features,[0],[0]
Recency feature.,4.2.1 Shallow Linguistic Features,[0],[0]
"This feature captures the distance lt(d) between the position t and the last occurrence of the candidate DR d. As a distance measure, we use the number of sentences from the last mention and exponentiate this number to make the dependence more extreme; only very recent DRs will receive a noticeable weight: exp(−lt(d)).",4.2.1 Shallow Linguistic Features,[0],[0]
This feature is set to 0 for new DRs.,4.2.1 Shallow Linguistic Features,[0],[0]
Frequency.,4.2.1 Shallow Linguistic Features,[0],[0]
The frequency feature indicates the number of times the candidate discourse referent d has been mentioned so far.,4.2.1 Shallow Linguistic Features,[0],[0]
We do not perform any bucketing.,4.2.1 Shallow Linguistic Features,[0],[0]
Grammatical function.,4.2.1 Shallow Linguistic Features,[0],[0]
This feature encodes the dependency relation assigned to the head word of the last mention of the DR or a special none label if the DR is new.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous subject indicator.,4.2.1 Shallow Linguistic Features,[0],[0]
This binary feature indicates whether the candidate DR d is coreferential with the subject of the previous verbal predicate.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous object indicator.,4.2.1 Shallow Linguistic Features,[0],[0]
The same but for the object position.,4.2.1 Shallow Linguistic Features,[0],[0]
Previous RE type.,4.2.1 Shallow Linguistic Features,[0],[0]
"This three-valued feature indicates whether the previous mention of the candidate DR d is a pronoun, a non-pronominal noun phrase, or has never been observed before.",4.2.1 Shallow Linguistic Features,[0],[0]
The selectional preference feature captures how well the candidate DR d fits a given syntactic position r of a given verbal predicate v.,4.2.2 Selectional Preferences Feature,[0],[0]
"It is computed as the cosine similarity simcos(xTd ,xv,r) of a vector-space representation of the DR xd and a structured vector-space representation of the pred-
icate xv,r.",4.2.2 Selectional Preferences Feature,[0],[0]
The similarities are calculated using a Distributional Memory approach similar to that of Baroni and Lenci (2010).,4.2.2 Selectional Preferences Feature,[0],[0]
"Their structured vector space representation has been shown to work well on tasks that evaluate correlation with human thematic fit estimates (Baroni and Lenci, 2010; Baroni et al., 2014; Sayeed et al., 2016) and is thus suited to our task.
",4.2.2 Selectional Preferences Feature,[0],[0]
"The representation xd is computed as an average of head word representations of all the previous mentions of DR d, where the word vectors are obtained from the TypeDM model of Baroni and Lenci (2010).",4.2.2 Selectional Preferences Feature,[0],[0]
"This is a count-based, third-order cooccurrence tensor whose indices are a word w0, a second word w1, and a complex syntactic relation r, which is used as a stand-in for a semantic link.",4.2.2 Selectional Preferences Feature,[0],[0]
"The values for each (w0, r, w1) cell of the tensor are the local mutual information (LMI) estimates obtained from a dependency-parsed combination of large corpora (ukWaC, BNC, and Wikipedia).
",4.2.2 Selectional Preferences Feature,[0],[0]
Our procedure has some differences with that of Baroni and Lenci.,4.2.2 Selectional Preferences Feature,[0],[0]
"For example, for estimating the fit of an alternative new DR (in other words, xd based on no previous mentions), we use an average over head words of all REs in the training set, a “null referent.”",4.2.2 Selectional Preferences Feature,[0],[0]
"xv,r is calculated as the average of the top 20 (by LMI) r-fillers for v in TypeDM; in other words, the prototypical instrument of rub may be represented by summing vectors like towel, soap, eraser, coin. . .",4.2.2 Selectional Preferences Feature,[0],[0]
"If the predicate has not yet been encountered (as for subject positions), scores for all scenario-relevant verbs are emitted for marginalization.",4.2.2 Selectional Preferences Feature,[0],[0]
"In this section, we describe features which rely on script information.",4.2.3 Script Features,[0],[0]
Our goal will be to show that such common-sense information is beneficial in performing DR prediction.,4.2.3 Script Features,[0],[0]
"We consider only two script features.
",4.2.3 Script Features,[0],[0]
Participant type fit This feature characterizes how well the participant type (PT) of the candidate DR d fits a specific syntactic role r of the governing predicate v; it can be regarded as a generalization of the selectional preference feature to participant types and also its specialisation to the considered scenario.,4.2.3 Script Features,[0],[0]
"Given the candidate DR d, its participant type p, and the syntactic
relation r, we collect all the predicates in the training set which have the participant type p in the position r.",4.2.3 Script Features,[0],[0]
"The embedding of the DR xp,r is given by the average embedding of these predicates.",4.2.3 Script Features,[0],[0]
"The feature is computed as the dot product of xp,r and the word embedding of the predicate v.
Predicate schemas The following feature captures a specific aspect of knowledge about prototypical sequences of events.",4.2.3 Script Features,[0],[0]
This knowledge is called predicate schemas in the recent co-reference modeling work of Peng et al. (2015).,4.2.3 Script Features,[0],[0]
"In predicate schemas, the goal is to model pairs of events such that if a DR d participated in the first event (in a specific role), it is likely to participate in the second event (again, in a specific role).",4.2.3 Script Features,[0],[0]
"For example, in the restaurant scenario, if one observes a phrase John ordered, one is likely to see John waited somewhere later in the document.",4.2.3 Script Features,[0],[0]
"Specific arguments are not that important (where it is John or some other DR), what is important is that the argument is reused across the predicates.",4.2.3 Script Features,[0],[0]
"This would correspond to the rule X-subject-of-order → X-subject-of-eat.4 Unlike the previous work, our dataset is small, so we cannot induce these rules directly as there will be very few rules, and the model would not generalize to new data well enough.",4.2.3 Script Features,[0],[0]
"Instead, we again encode this intuition using similarities in the real-valued embedding space.
",4.2.3 Script Features,[0],[0]
"Recall that our goal is to compute a feature ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
"indicating how likely a potential DR d is to follow, given the history h(t).",4.2.3 Script Features,[0],[0]
"For example, imag-
4In this work, we limit ourselves to rules where the syntactic function is the same on both sides of the rule.",4.2.3 Script Features,[0],[0]
"In other words, we can, in principle, encode the pattern X pushed Y → X apologized but not the pattern X pushed Y → Y cried.
",4.2.3 Script Features,[0],[0]
ine that the model is asked to predict the DR marked by XXXXXX in Figure 4.,4.2.3 Script Features,[0],[0]
"Predicate-schema rules can only yield previously introduced DRs, so the score ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
= 0 for any new DR d. Let us use “soap” as an example of a previously introduced DR and see how the feature is computed.,4.2.3 Script Features,[0],[0]
"In order to choose which inference rules can be applied to yield “soap”, we can inspect Figure 4.",4.2.3 Script Features,[0],[0]
"There are only two preceding predicates which have DR “soap” as their object (rubbed and grabbed), resulting in two potential rules X-object-of-grabbed→ X-object-of-rinsed and X-object-of-rubbed → X-object-of-rinsed.",4.2.3 Script Features,[0],[0]
"We define the score ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
as the average of the rule scores.,4.2.3 Script Features,[0],[0]
"More formally, we can write
ϕ(d, h(t))= 1 |N(d, h(t))| ∑
(u,v,r)∈N(d,h(t))
ψ(u, v, r), (1)
where ψ(u, v, r) is the score for a rule X-r-of-u → X-r-of-v, N(d, h(t))",4.2.3 Script Features,[0],[0]
"is the set of applicable rules, and |N(d, h(t))| denotes its cardinality.5",4.2.3 Script Features,[0],[0]
"We define ϕ(d, h(t))",4.2.3 Script Features,[0],[0]
"as 0, when the set of applicable rules is empty (i.e. |N(d, h(t))| = 0).
",4.2.3 Script Features,[0],[0]
"The scoring function ψ(u, v, r) as a linear func-
5In all our experiments, rather than considering all potential predicates in the history to instantiate rules, we take into account only 2 preceding verbs.",4.2.3 Script Features,[0],[0]
"In other words, u and v can be interleaved by at most one verb and |N(d, h(t))| is in {0, 1, 2}.
tion of a joint embedding xu,v of verbs u and v:
ψ(u, v, r) =",4.2.3 Script Features,[0],[0]
"αTr xu,v.
The two remaining questions are (1) how to define the joint embeddings xu,v, and (2) how to estimate the parameter vectorαr.",4.2.3 Script Features,[0],[0]
"The joint embedding of two predicates, xu,v, can, in principle, be any composition function of embeddings of u and v, for example their sum or component-wise product.",4.2.3 Script Features,[0],[0]
"Inspired by Bordes et al. (2013), we use the difference between the word embeddings:
ψ(u, v, r) = αTr (xu − xv),
where xu and xv are external embeddings of the corresponding verbs.",4.2.3 Script Features,[0],[0]
Encoding the succession relation as translation in the embedding space has one desirable property: the scoring function will be largely agnostic to the morphological form of the predicates.,4.2.3 Script Features,[0],[0]
"For example, the difference between the embeddings of rinsed and rubbed is very similar to that of rinse and rub (Botha and Blunsom, 2014), so the corresponding rules will receive similar scores.",4.2.3 Script Features,[0],[0]
"Now, we can rewrite the equation (1) as
ϕ(d, h(t))= αT r(h(t))
",4.2.3 Script Features,[0],[0]
"∑ (u,v,r)∈N(d,h(t))",4.2.3 Script Features,[0],[0]
"(xu − xv)
|N(d, h(t))| (2)
where r(h(t)) denotes the syntactic function corresponding to the DR being predicted (object in our example).
",4.2.3 Script Features,[0],[0]
"As for the parameter vector αr, there are again a number of potential ways how it can be estimated.",4.2.3 Script Features,[0],[0]
"For example, one can train a discriminative classifier to estimate the parameters.",4.2.3 Script Features,[0],[0]
"However, we opted for a simpler approach—we set it equal to the empirical estimate of the expected feature vector xu,v on the training set:6
αr = 1
Dr ∑ l,t δr(r(h",4.2.3 Script Features,[0],[0]
"(l,t)))",4.2.3 Script Features,[0],[0]
"∑ (u,v,r′)∈N(d(l,t),h(l,t)) (xu − xv), (3)
where l refers to a document in the training set, t is (as before) a position in the document, h(l,t) and
6This essentially corresponds to using the Naive Bayes model with the simplistic assumption that the score differences are normally distributed with spherical covariance matrices.
",4.2.3 Script Features,[0],[0]
"d(l,t) are the history and the correct DR for this position, respectively.",4.2.3 Script Features,[0],[0]
"The term δr(r′) is the Kronecker delta which equals 1 if r = r′ and 0, otherwise.",4.2.3 Script Features,[0],[0]
"Dr is the total number of rules for the syntactic function r in the training set:
Dr = ∑ l,t δr(r(h",4.2.3 Script Features,[0],[0]
"(l,t)))× |N(d(l,t), h(l,t))|.
",4.2.3 Script Features,[0],[0]
Let us illustrate the computation with an example.,4.2.3 Script Features,[0],[0]
"Imagine that our training set consists of the document in Figure 1, and the trained model is used to predict the upcoming DR in our referent cloze example (Figure 4).",4.2.3 Script Features,[0],[0]
"The training document includes the pair X-object-of-scrubbed→ X-object-of-rinsing, so the corresponding term (xscrubbed - xrinsing) participates in the summation (3) for αobj .",4.2.3 Script Features,[0],[0]
"As we rely on external embeddings, which encode semantic similarities between lexical items, the dot product of this term and (xrubbed - xrinsed) will be high.7 Consequently, ϕ(d, h(t)) is expected to be positive for d = “soap”, thus, predicting “soap” as the likely forthcoming DR.",4.2.3 Script Features,[0],[0]
"Unfortunately, there are other terms (xu − xv) both in expression (3) for αobj and in expression (2) for ϕ(d, h(t)).",4.2.3 Script Features,[0],[0]
"These terms may be
7The score would have been even higher, should the predicate be in the morphological form rinsing rather than rinsed.",4.2.3 Script Features,[0],[0]
"However, embeddings of rinsing and rinsed would still be sufficiently close to each other for our argument to hold.
irrelevant to the current prediction, as X-object-ofplugged → X-object-of-filling from Figure 1, and may not even encode any valid regularities, as Xobject-of-got → X-object-of-scrubbed (again from Figure 1).",4.2.3 Script Features,[0],[0]
This may suggest that our feature will be too contaminated with noise to be informative for making predictions.,4.2.3 Script Features,[0],[0]
"However, recall that independent random vectors in high dimensions are almost orthogonal, and, assuming they are bounded, their dot products are close to zero.",4.2.3 Script Features,[0],[0]
"Consequently, the products of the relevant (“non-random”) terms, in our example (xscrubbed - xrinsing) and (xrubbed - xrinsed), are likely to overcome the (“random”) noise.",4.2.3 Script Features,[0],[0]
"As we will see in the ablation studies, the predicateschema feature is indeed predictive of a DR and contributes to the performance of the full model.",4.2.3 Script Features,[0],[0]
"We would like to test whether our model can produce accurate predictions and whether the model’s guesses correlate well with human predictions for the referent cloze task.
",4.3 Experiments,[0],[0]
"In order to be able to evaluate the effect of script knowledge on referent predictability, we compare three models: our full Script model uses all of the features introduced in section 4.2; the Linguistic model relies only on the ‘linguistic features’ but not the script-specific ones; and the Base model includes all the shallow linguistic features.",4.3 Experiments,[0],[0]
The Base model differs from the linguistic model in that it does not model selectional preferences.,4.3 Experiments,[0],[0]
"Table 2 summarizes features used in different models.
",4.3 Experiments,[0],[0]
"The data set was randomly divided into training (70%), development (10%, 91 stories from 10 sce-
narios), and test (20%, 182 stories from 10 scenarios) sets.",4.3 Experiments,[0],[0]
"The feature weights were learned using L-BFGS (Byrd et al., 1995) to optimize the loglikelihood.",4.3 Experiments,[0],[0]
Evaluation against original referents.,4.3 Experiments,[0],[0]
We calculated the percentage of correct DR predictions.,4.3 Experiments,[0],[0]
See Table 3 for the averages across 10 scenarios.,4.3 Experiments,[0],[0]
We can see that the task appears hard for humans: their average performance reaches only 73% accuracy.,4.3 Experiments,[0],[0]
"As expected, the Base model is the weakest system (the accuracy of 31%).",4.3 Experiments,[0],[0]
Modeling selectional preferences yields an extra 18% in accuracy (Linguistic model).,4.3 Experiments,[0],[0]
"The key finding is that incorporation of script knowledge increases the accuracy by further 13%, although still far behind human performance (62% vs. 73%).",4.3 Experiments,[0],[0]
"Besides accuracy, we use perplexity, which we computed not only for all our models but also for human predictions.",4.3 Experiments,[0],[0]
This was possible as each task was solved by multiple humans.,4.3 Experiments,[0],[0]
We used unsmoothed normalized guess frequencies as the probabilities.,4.3 Experiments,[0],[0]
"As we can see from Table 3, the perplexity scores are consistent with the accuracies: the script model again outperforms other methods, and, as expected, all the models are weaker than humans.
",4.3 Experiments,[0],[0]
"As we used two sets of script features, capturing different aspects of script knowledge, we performed extra ablation studies (Table 4).",4.3 Experiments,[0],[0]
The experiments confirm that both feature sets were beneficial.,4.3 Experiments,[0],[0]
Evaluation against human expectations.,4.3 Experiments,[0],[0]
"In the previous subsection, we demonstrated that the incorporation of selectional preferences and, perhaps more interestingly, the integration of automatically acquired script knowledge lead to improved accuracy in predicting discourse referents.",4.3 Experiments,[0],[0]
Now we turn to another question raised in the introduction: does incorporation of this knowledge make our predictions more human-like?,4.3 Experiments,[0],[0]
"In other words, are we able to accurately estimate human expectations?",4.3 Experiments,[0],[0]
"This includes not only being sufficiently accurate but also making the same kind of incorrect predictions.
",4.3 Experiments,[0],[0]
"In this evaluation, we therefore use human guesses collected during the referent cloze task as our target.",4.3 Experiments,[0],[0]
We then calculate the relative accuracy of each computational model.,4.3 Experiments,[0],[0]
"As can be seen in Figure 5, the Script model, at approx.",4.3 Experiments,[0],[0]
"53% accuracy, is a lot more accurate in predicting human guesses than the Linguistic model and the Base model.",4.3 Experiments,[0],[0]
"We can also
observe that the margin between the Script model and the Linguistic model is a lot larger in this evaluation than between the Base model and the Linguistic model.",4.3 Experiments,[0],[0]
"This indicates that the model which has access to script knowledge is much more similar to human prediction behavior in terms of top guesses than the script-agnostic models.
",4.3 Experiments,[0],[0]
Now we would like to assess if our predictions are similar as distributions rather than only yielding similar top predictions.,4.3 Experiments,[0],[0]
"In order to compare the distributions, we use the Jensen-Shannon divergence (JSD), a symmetrized version of the KullbackLeibler divergence.
",4.3 Experiments,[0],[0]
"Intuitively, JSD measures the distance between two probability distributions.",4.3 Experiments,[0],[0]
A smaller JSD value is indicative of more similar distributions.,4.3 Experiments,[0],[0]
"Figure 6 shows that the probability distributions resulting from the Script model are more similar to human predictions than those of the Linguistic and Base models.
",4.3 Experiments,[0],[0]
"In these experiments, we have shown that script knowledge improves predictions of upcoming referents and that the script model is the best among our models in approximating human referent predictions.",4.3 Experiments,[0],[0]
"Using the referent prediction models, we next attempt to replicate Tily and Piantadosi’s findings that
the choice of the type of referring expression (pronoun or full NP) depends in part on the predictability of the referent.",5 Referring Expression Type Prediction Model (RE Model),[0],[0]
"The uniform information density (UID) hypothesis suggests that speakers tend to convey information at a uniform rate (Jaeger, 2010).",5.1 Uniform Information Density hypothesis,[0],[0]
"Applied to choice of referring expression type, it would predict that a highly predictable referent should be encoded using a short code (here: a pronoun), while an unpredictable referent should be encoded using a longer form (here: a full NP).",5.1 Uniform Information Density hypothesis,[0],[0]
"Information density is measured using the information-theoretic measure of the surprisal S of a message mi:
S(mi) =",5.1 Uniform Information Density hypothesis,[0],[0]
− logP (mi | context) UID has been very successful in explaining a variety of linguistic phenomena; see Jaeger et al. (2016).,5.1 Uniform Information Density hypothesis,[0],[0]
"There is, however, controversy about whether UID affects pronominalization.",5.1 Uniform Information Density hypothesis,[0],[0]
Tily and Piantadosi (2009) report evidence that writers are more likely to refer using a pronoun or proper name when the referent is easy to guess and use a full NP when readers have less certainty about the upcoming referent; see also Arnold (2001).,5.1 Uniform Information Density hypothesis,[0],[0]
"But other experiments (using highly controlled stimuli) have failed to find an effect of predictability on pronominalization (Stevenson et al., 1994; Fukumura and van Gompel, 2010; Rohde and Kehler, 2014).",5.1 Uniform Information Density hypothesis,[0],[0]
The present study hence contributes to the debate on whether UID affects referring expression choice.,5.1 Uniform Information Density hypothesis,[0],[0]
Our goal is to determine whether referent predictability (quantified in terms of surprisal) is correlated with the type of referring expression used in the text.,5.2 A model of Referring Expression Choice,[0],[0]
Here we focus on the distinction between pronouns and full noun phrases.,5.2 A model of Referring Expression Choice,[0],[0]
Our data also contains a small percentage (ca.,5.2 A model of Referring Expression Choice,[0],[0]
1%) of proper names (like “John”).,5.2 A model of Referring Expression Choice,[0],[0]
"Due to this small class size and earlier findings that proper nouns behave much like pronouns (Tily and Piantadosi, 2009), we combined pronouns and proper names into a single class of short encodings.
",5.2 A model of Referring Expression Choice,[0],[0]
"For the referring expression type prediction task, we estimate the surprisal of the referent from each of our computational models from Section 4 as well as the human cloze task.",5.2 A model of Referring Expression Choice,[0],[0]
"The surprisal of an upcoming discourse referent d(t) based on the previous context
h(t) is thereby estimated as: S(d(t)) =",5.2 A model of Referring Expression Choice,[0],[0]
"− log p(d(t) | h(t))
",5.2 A model of Referring Expression Choice,[0],[0]
"In order to determine whether referent predictability has an effect on referring expression type over and above other factors that are known to affect the choice of referring expression, we train a logistic regression model with referring expression type as a response variable and discourse referent predictability as well as a large set of other linguistic factors (based on Tily and Piantadosi, 2009) as explanatory variables.",5.2 A model of Referring Expression Choice,[0],[0]
"The model is defined as follows:
p(n(t) = n|d(t), h(t))",5.2 A model of Referring Expression Choice,[0],[0]
=,5.2 A model of Referring Expression Choice,[0],[0]
"exp(v Tg(n, dt, h(t)))∑
n′",5.2 A model of Referring Expression Choice,[0],[0]
"exp(v Tg(n′, dt, h(t)))
,
where d(t) and h(t) are defined as before, g is the feature function, and v is the vector of model parameters.",5.2 A model of Referring Expression Choice,[0],[0]
The summation in the denominator is over NP types (full NP vs. pronoun/proper noun).,5.2 A model of Referring Expression Choice,[0],[0]
We ran four different logistic regression models.,5.3 RE Model Experiments,[0],[0]
These models all contained exactly the same set of linguistic predictors but differed in the estimates used for referent type surprisal and residual entropy.,5.3 RE Model Experiments,[0],[0]
"One logistic regression model used surprisal estimates based on the human referent cloze task, while the three other models used estimates based on the three computational models (Base, Linguistic and Script).",5.3 RE Model Experiments,[0],[0]
"For our experiment, we are interested in the choice of referring expression type for those occurrences of references, where a “real choice” is possible.",5.3 RE Model Experiments,[0],[0]
We therefore exclude for our analysis reported below all first mentions as well as all first and second person pronouns (because there is no optionality in how to refer to first or second person).,5.3 RE Model Experiments,[0],[0]
This subset contains 1345 data points.,5.3 RE Model Experiments,[0],[0]
The results of all four logistic regression models are shown in Table 5.,5.4 Results,[0],[0]
We first take a look at the results for the linguistic features.,5.4 Results,[0],[0]
"While there is a bit of variability in terms of the exact coefficient estimates between the models (this is simply due to small correlations between these predictors and the predictors for surprisal), the effect of all of these features is largely consistent across models.",5.4 Results,[0],[0]
"For instance, the positive coefficients for the recency feature means that when a previous mention happened
very recently, the referring expression is more likely to be a pronoun (and not a full NP).
",5.4 Results,[0],[0]
"The coefficients for the surprisal estimates of the different models are, however, not significantly different from zero.",5.4 Results,[0],[0]
Model comparison shows that they do not improve model fit.,5.4 Results,[0],[0]
We also used the estimated models to predict referring expression type on new data and again found that surprisal estimates from the models did not improve prediction accuracy.,5.4 Results,[0],[0]
This effect even holds for our human cloze data.,5.4 Results,[0],[0]
"Hence, it cannot be interpreted as a problem with the models—even human predictability estimates are, for this dataset, not predictive of referring expression type.
",5.4 Results,[0],[0]
We also calculated regression models for the full dataset including first and second person pronouns as well as first mentions (3346 data points).,5.4 Results,[0],[0]
"The results for the full dataset are fully consistent with the findings shown in Table 5: there was no significant effect of surprisal on referring expression type.
",5.4 Results,[0],[0]
"This result contrasts with the findings by Tily and Piantadosi (2009), who reported a significant effect of surprisal on RE type for their data.",5.4 Results,[0],[0]
"In order to replicate their settings as closely as possible, we also included residualEntropy as a predictor in our model (see last predictor in Table 5); however, this did not change the results.",5.4 Results,[0],[0]
Our study on incrementally predicting discourse referents showed that script knowledge is a highly important factor in determining human discourse expectations.,6 Discussion and Future Work,[0],[0]
"Crucially, the computational modelling approach allowed us to tease apart the different factors that affect human prediction as we cannot manipulate this in humans directly (by asking them to “switch off” their common-sense knowledge).
",6 Discussion and Future Work,[0],[0]
"By modelling common-sense knowledge in terms of event sequences and event participants, our model captures many more long-range dependencies than normal language models.",6 Discussion and Future Work,[0],[0]
"The script knowledge is automatically induced by our model from crowdsourced scenario-specific text collections.
",6 Discussion and Future Work,[0],[0]
"In a second study, we set out to test the hypothesis that uniform information density affects referring expression type.",6 Discussion and Future Work,[0],[0]
"This question is highly controversial in the literature: while Tily and Piantadosi (2009) find a significant effect of surprisal on referring expression type in a corpus study very similar to ours, other studies that use a more tightly controlled experimental approach have not found an effect of predictability on RE type (Stevenson et al., 1994; Fukumura and van Gompel, 2010; Rohde and Kehler, 2014).",6 Discussion and Future Work,[0],[0]
"The present study, while replicating exactly the setting of T&P in terms of features and analysis, did not find support for a UID effect on RE type.",6 Discussion and Future Work,[0],[0]
"The difference in results between T&P 2009 and our results could be due to the different corpora and text sorts that were used; specifically, we would expect that larger predictability effects might be observable at script boundaries, rather than within a script, as is the case in our stories.
",6 Discussion and Future Work,[0],[0]
A next step in moving our participant prediction model towards NLP applications would be to replicate our modelling results on automatic textto-script mapping instead of gold-standard data as done here (in order to approximate human level of processing).,6 Discussion and Future Work,[0],[0]
"Furthermore, we aim to move to more complex text types that include reference to several scripts.",6 Discussion and Future Work,[0],[0]
"We plan to consider the recently published ROC Stories corpus (Mostafazadeh et al., 2016), a large crowdsourced collection of topically unrestricted short and simple narratives, as a basis for these next steps in our research.",6 Discussion and Future Work,[0],[0]
We thank the editors and the anonymous reviewers for their insightful suggestions.,Acknowledgments,[0],[0]
We would like to thank Florian Pusse for helping with the Amazon Mechanical Turk experiment.,Acknowledgments,[0],[0]
We would also like to thank Simon Ostermann and Tatjana Anikina for helping with the InScript corpus.,Acknowledgments,[0],[0]
"This research was partially supported by the German Research Foundation (DFG) as part of SFB 1102 ‘Information Density and Linguistic Encoding’, European Research Council (ERC) as part of ERC Starting Grant BroadSem (#678254), the Dutch National Science Foundation as part of NWO VIDI 639.022.518, and the DFG once again as part of the MMCI Cluster of Excellence (EXC 284).",Acknowledgments,[0],[0]
Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content.,abstractText,[0],[0]
Prediction also affects perception and might be a key to robustness in human language processing.,abstractText,[0],[0]
"In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts.",abstractText,[0],[0]
We find that script knowledge significantly improves model estimates of human predictions.,abstractText,[0],[0]
"In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.",abstractText,[0],[0]
Modeling Semantic Expectation: Using Script Knowledge for Referent Prediction,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 303–308 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Intuitively, a man can swallow a candy or paintball but not a desk.",1 Introduction,[0],[0]
"Equally so, one cannot plausibly eat a cake and then hold it.",1 Introduction,[0],[0]
What kinds of semantic knowledge are necessary for distinguishing a physically plausible event (or event sequence) from an implausible one?,1 Introduction,[0],[0]
"Semantic plausibility stands in stark contrast to the familiar selectional preference (Erk and Padó, 2010; Van de Cruys, 2014) which is concerned with the typicality of events (Table 1).",1 Introduction,[0],[0]
"For example, candy is a typical entity for man-swallow-* but paintball is not, even though both events are plausible physically.",1 Introduction,[0],[0]
"Also, some events are physically plausible but are never stated because humans avoid stating the obvious.",1 Introduction,[0],[0]
"Critically, semantic plausibility is sensitive to certain properties such as relative object size that are not explicitly encoded by selectional preferences (Bagherinezhad et al., 2016).",1 Introduction,[0],[0]
"Therefore, it is crucial that we learn to model these dimensions in addition to using classical distributional signals.
",1 Introduction,[0],[0]
"Semantic plausibility is pertinent and crucial in a multitude of interesting NLP tasks put forth previously, such as narrative schema (Chambers, 2013), narrative interpolation (Bowman et al., 2016), story understanding (Mostafazadeh et al., 2016), and paragraph reconstruction (Li and Jurafsky, 2017).",1 Introduction,[0],[0]
"Existing methods for these tasks, however, draw predominantly (if not only) on distributional data and produce rather weak performance.",1 Introduction,[0],[0]
"Semantic plausibility over subject-verbobject triples, while simpler than these other tasks, is a key building block that requires many of the same signals and encapsulates complex world knowledge in a binary prediction problem.
",1 Introduction,[0],[0]
"In this work, we show that world knowledge injection is necessary and effective for the semantic plausibility task, for which we create a robust, high-agreement dataset (details in section 3).",1 Introduction,[0],[0]
"Employing methods inspired by the recent work on world knowledge propagation through distributional context (Forbes and Choi, 2017; Wang et al., 2017), we accomplish the goal with minimal effort in manual annotation.",1 Introduction,[0],[0]
"Finally, we perform an indepth error analysis to point to future directions of work on semantic plausibility.
303",1 Introduction,[0],[0]
Simple events (i.e. S-V-O) have seen thorough investigation from the angle of selectional preference.,2 Related Work,[0],[0]
"While early works are resourcebased (Resnik, 1996; Clark and Weir, 2001), later work shows that unsupervised learning with distributional data yields strong performance (O’Seaghdha, 2010; Erk and Padó, 2010), which has recently been further improved upon with neural approaches (Van de Cruys, 2014; Tilk et al., 2016).",2 Related Work,[0],[0]
"Distribution-only models however, as will be shown, fail on the semantic plausibility task we propose.
",2 Related Work,[0],[0]
Physical world knowledge modeling appears frequently in more closely related work.,2 Related Work,[0],[0]
Bagherinezhad et al. (2016) combine computer vision and text-based information extraction to learn the relative sizes of objects; Forbes and Choi (2017) crowdsource physical knowledge along specified dimensions and employ belief propagation to learn relative physical attributes of object pairs.,2 Related Work,[0],[0]
"Wang et al. (2017) propose a multimodal LDA to learn the definitional properties (e.g. animal, fourlegged) of entities.",2 Related Work,[0],[0]
"Zhang et al. (2017) study the role of common-sense knowledge in natural language inference, which is inherently betweenevents rather than single-event focused.",2 Related Work,[0],[0]
"Prior work does not specifically handles the (singleevent) semantic plausibility task and related efforts do not necessarily adapt well to this task, as we will show, suggesting that new approaches are needed.",2 Related Work,[0],[0]
"To study the semantic plausibility of S-V-O events, specifically physical semantic plausibility, we create a dataset1 through Amazon Mechanical Turk with the following criteria in mind: (i) Robustness: Strong inter-annotator agreement; (ii) Diversity: A wide range of typical/atypical, plausible/implausible events; (iii) Balanced: Equal number of plausible and implausible events.
",3 Data,[0],[0]
"In creating physical events, we work with a fixed vocabulary of 150 concrete verbs and 450 concrete nouns from Brysbaert et al. (2014)’s word list, with a concreteness threshold of 4.95 (scale: 0-5).",3 Data,[0],[0]
"We take the following steps:
1Link: https://github.com/suwangcompling/ Modeling-Semantic-Plausibility-NAACL18/ tree/master/data.
",3 Data,[0],[0]
"(a) Have Turkers write down plausible or implausible S-V and V-O selections;
(b) Randomly generate S-V-O triples from collected S-V and V-O pairs;
(c) Send resulting S-V-O triples to Turkers to filter for ones with high agreement (by majority vote).
",3 Data,[0],[0]
(a) ensures diversity and the cleanness of data (compared with noisy selectional preference data collected unsupervised from free text): the Turkers are instructed (with examples) to (i) consider both typical and atypical selections (e.g. manswallow-* with candy or paintball); (ii) disregard metaphorical uses (e.g. feel-blue or fish-idea).,3 Data,[0],[0]
"2,000 pairs are collected in the step, balancing typical and atypical pairs.",3 Data,[0],[0]
"In (b), we manually filter error submissions in triple generation.",3 Data,[0],[0]
"For (c), 5 Turkers provide labels, and we only keep the ones that have ≥ 3 majority votes, resulting with 3,062 triples (of 4,000 annotated triples, plausibleimplausible balanced), with 100% ≥ 3 agreement, 95% ≥ 4 agreement, and 90% 5 agreement.
",3 Data,[0],[0]
"To empirically show the failure of distributiononly methods, we run Van de Cruys (2014)’s neural net classifier (hereforth NN), which is one of the strongest models designed for selectional preference (Figure 1, left-box).",3 Data,[0],[0]
Let x be the concatenation of the embeddings of the three words in an S-V-O triple.,3 Data,[0],[0]
"The prediction P (y|x) is computed as follows:
P (y = 1|x) = σ2(W2σ1(W1x))",3 Data,[0],[0]
"(1) where σ is a nonlinearity, W are weights, and we use 300D pretrained GloVe vectors (Pennington et al., 2014).",3 Data,[0],[0]
"The model achieves an accuracy of 68% (logistic regression baseline: 64%) after finetuning, verifying the intuition that distributional data alone cannot satisfactorily capture the semantics of physical plausibility.",3 Data,[0],[0]
"Recognizing that a distribution-alone method lacks necessary information, we collect a set of world knowledge features.",4 World Knowledge Features,[0],[0]
The feature types derive from inspecting the high agreement event triples for knowledge missing in distributional selection (e.g. relative sizes in man-swallowpaintball/desk).,4 World Knowledge Features,[0],[0]
"Previously, Forbes and Choi (2017) proposed a three level (3-LEVEL) featurization scheme, where an object-pair can take 3
values for, e.g. relative size: {−1, 0, 1} (i.e. lesser, similar, greater).",4 World Knowledge Features,[0],[0]
"This method, however, does not explain many cases we observed.",4 World Knowledge Features,[0],[0]
"For instance, man-hug-cat/ant, man is larger than both cat and ant, but the latter event is implausible.",4 World Knowledge Features,[0],[0]
3-LEVEL is also inefficient: k objects incur O(k2) elicitations.,4 World Knowledge Features,[0],[0]
"We thus propose a binning-by-landmark method, which is sufficiently fine-grained while still being efficient and easy for the annotator: given an entity n, the Turker decides to which of the landmarks n is closest to.",4 World Knowledge Features,[0],[0]
"E.g., for SIZE, we have the landmarks {watch, book, cat, person, jeep, stadium}, in ascending sizes.",4 World Knowledge Features,[0],[0]
"If n = dog, the Turker may put n in the bin corresponding to cat.",4 World Knowledge Features,[0],[0]
"The features2 are listed with their landmarks as follows: • SENTIENCE: rock, tree, ant, cat, chimp, man.",4 World Knowledge Features,[0],[0]
•,4 World Knowledge Features,[0],[0]
MASS-COUNT:,4 World Knowledge Features,[0],[0]
"milk, sand, pebbles, car.",4 World Knowledge Features,[0],[0]
"• PHASE: smoke, milk, wood.",4 World Knowledge Features,[0],[0]
•,4 World Knowledge Features,[0],[0]
"SIZE: watch, book, cat, person, jeep, stadium.",4 World Knowledge Features,[0],[0]
"• WEIGHT: watch, book, dumbbell, man, jeep, stadium.",4 World Knowledge Features,[0],[0]
"• RIGIDITY: water, skin, leather, wood, metal.
5",4 World Knowledge Features,[0],[0]
"Turkers provide annotations for all 450 nouns, and we obtained 93% ≥ 3 agreement, 85% ≥ 4 agreement, and 79% 5 agreement.
",4 World Knowledge Features,[0],[0]
"Our binning is sufficiently granular, which is crucial for semantic plausibility of an event in many cases.",4 World Knowledge Features,[0],[0]
"E.g. for man-hug-cat/ant, man, cat and ant fall in the 4th, 3rd and 1st bin, which suffices to explain why man-hug-cat is plausible while man-hug-ant is not.",4 World Knowledge Features,[0],[0]
"Compared to past work (Forbes and Choi, 2017), it is efficient.",4 World Knowledge Features,[0],[0]
"Each entity only needs one assignment in comparison to the landmarks to be located in a “global scale” (e.g. from the smallest to the largest objects), and even for extreme granularity, it only takes O(k log k) comparisons.",4 World Knowledge Features,[0],[0]
It is also intuitive: differences in bins capture the intuition that one can hug smaller objects as long as those objects are not too small.,4 World Knowledge Features,[0],[0]
We answer two questions: (i) Does world knowledge improve the accuracy of semantic plausibility classification?,5 Models,[0],[0]
"(ii) Can we minimize effort in knowledge feature annotation by learning from a
2We experimented with numerous feature types, e.g. size, temperature, shape, etc. and kept the subset that contributes most substantially to semantic plausibility classification.",5 Models,[0],[0]
"More details on the feature types in supplementary material (https://github.com/suwangcompling/ Modeling-Semantic-Plausibility-NAACL18/ tree/master/supplementary).
",5 Models,[0],[0]
small amount of training data?,5 Models,[0],[0]
"For question (i), we experiment with various methods to incorporate the features on top of the embedding-only NN (Section 3).",5 Models,[0],[0]
"Our architecture3 is outlined in Figure 1, where we ensemble the NN (left-box) and another feedforward net for features (WK, right-box) to produce the final prediction.",5 Models,[0],[0]
"For the feature net, the relative physical attributes of the subject-object pair can be encoded in 3-LEVEL (Section 4) or the bin difference (BINDIFF) scheme.4 For BIN-DIFF, given the two entities in an S-V-O event (i.e. S, O) ant and man, which are in the bins of the landmark watch (i.e. the 1st) and that of person (i.e. the 4th), the pair ant-man gets a BIN-DIFF value of 1−4 = −3.",5 Models,[0],[0]
"Exemplifying the featurization function f(s, o) with SIZE:
f3-L(SIZE(s), SIZE(o)) ∈ {−1, 0, 1} (2) fBIN(SIZE(s), SIZE(o))",5 Models,[0],[0]
"= BIN(s)− BIN(o) (3)
Then, given a featurization scheme, we may feed raw feature values (RAW VEC, for 3-LEVEL, e.g. concatenation of -1, 0 or 1 of all feature types, in that order, and in one-hot format), or feature embeddings (EMBEDDING, e.g. concatenation of embeddings looked up with feature values).",5 Models,[0],[0]
"Fi-
3More configuration details in supplementary material.",5 Models,[0],[0]
"4We also tried using bin numbers directly, however it does not produce ideal results (classification accuracy between 3- LEVEL and BIN-DIFF).",5 Models,[0],[0]
"Thus for brevity we drop this setup.
nally, let aNN,aWK be the penultimate-layer vectors of NN and WK (see Figure 1), we affine transform their concatenation to predict label ŷ with argmax on the final softmax layer:
ŷ",5 Models,[0],[0]
= argmax y softmax(σ(W,5 Models,[0],[0]
"[aNN;aWK] + b)) (4)
where σ is a ReLU nonlinearity.",5 Models,[0],[0]
"We will only report the results from the best-performing model configuration, which has BIN-DIFF + EMBEDDING.",5 Models,[0],[0]
"The model will be listed below as NN + WK-GOLD (i.e. with GOLD, Turker-annotated World Knowledge features).
",5 Models,[0],[0]
"For question (ii), we select a data-efficient feature learning model.",5 Models,[0],[0]
Following Forbes and Choi (2017) we evaluate the models with 5% or 20% of training data.,5 Models,[0],[0]
We experiment with several previously proposed techniques: (a) label spreading; (b) factor graph; (c) multi-LDA.,5 Models,[0],[0]
As a baseline we employ a simple but well-tuned logistic regressor (LR).,5 Models,[0],[0]
"We also initialize the factor graph with this LR, on account of its unexpectedly strong performance.5 Finally, observing that the feature types are inherently ordinal (e.g. SIZE from small to large), we also run ordinal logistic regression (Adeleke and Adepoju, 2010).",5 Models,[0],[0]
"For model selection we first evaluate the object-pair attribute data collected by Forbes and Choi (2017), 2.5k pairs labeled in the 3-LEVEL scheme.",5 Models,[0],[0]
We then compared the the LR and Ordinal-LR (our strongest models6 in this experiment) on 10k randomly generated object-pairs from our annotated nouns.,5 Models,[0],[0]
"The results are summarized in Table 2, where we see
5We verified our setup with the authors and they attributed the higher performance of our LR to hyperparameter choices.",5 Models,[0],[0]
"6Because the factor graph + LR gives very slight improvement, for simplicity we choose LR instead.
",5 Models,[0],[0]
"(i) 3-LEVEL propagation is much easier; (ii) our object-pairs are more challenging, likely due to sparsity with larger vocabulary size; (iii) ordinality information contributes substantially to performance.",5 Models,[0],[0]
The model that uses propagated features (w/ Ordinal-LR) will be listed as NN + WK-PROP.,5 Models,[0],[0]
"We evaluate the models on the task of classifying our 3,062 S-V-O triples by semantic plausibility (10-fold CV, taking the average over 20 runs with the same random seed).",6 Semantic Plausibility Results,[0],[0]
"We compare our three models in the 3-LEVEL and BIN-DIFF schemes, with NN + WK-PROP evaluated in 5% and 20% training conditions.",6 Semantic Plausibility Results,[0],[0]
The results are outlined in Table 3.,6 Semantic Plausibility Results,[0],[0]
Summarizing our findings: (i) world knowledge undoubtedly leads to a strong performance boost (∼8%); (ii) BIN-DIFF scheme works much better than 3-LEVEL — it manages to outperform the latter even with much weaker propagation accuracy; (iii) the accuracy loss with propagated features seems rather mild with 20% labeled training and the best scheme.,6 Semantic Plausibility Results,[0],[0]
"To understand what challenges remain in this task, we run the models above 200 times (10-fold CV, random shuffle at each run), and inspect the top 200 most frequently misclassified cases.",7 Error Analysis,[0],[0]
"The percentage statistics below are from counting the error cases.
",7 Error Analysis,[0],[0]
"In the cases where NN misclassifies while NN + WK-GOLD correctly classifies, 60% relates to SIZE and WEIGHT (e.g. missing man-hug-ant
(bad) or dog-pull-paper (good)).",7 Error Analysis,[0],[0]
PHASE takes up 18% (e.g. missing monkey-puff-smoke (good)).,7 Error Analysis,[0],[0]
"This validates the intuition that distributional contexts do not encode these types of world knowledge.
",7 Error Analysis,[0],[0]
"For cases often misclassified by all the models, we observe two main types of errors: (i) data sparsity; (ii) highly-specific attributes.
",7 Error Analysis,[0],[0]
Data sparsity (32%).,7 Error Analysis,[0],[0]
"man-choke-ant, e.g., is a singleton big-object-choke-small-object instance, and there are no distributionally similar verbs that can help (e.g. suffocate);",7 Error Analysis,[0],[0]
"For sun-heat-water, because the majority of the actions in the data are limited to solid objects, the models tend to predict implausible for whenever a gas/liquid appears as the object.
Highly-specific attributes (68%).",7 Error Analysis,[0],[0]
“long-tailed” physical attributes which are absent from our feature set are required.,7 Error Analysis,[0],[0]
"To exemplify a few:7
• edibility (21%).",7 Error Analysis,[0],[0]
"*-fry-egg (plausible) and *-fry-cup (implausible) are hard to distinguish because egg and cup are similar in SIZE/WEIGHT/..., however introducing large free-text data to help learn edibility misguides our model to mind selectional preference, causing mislabeling of other events.",7 Error Analysis,[0],[0]
• natural vs. artificial (18%).,7 Error Analysis,[0],[0]
"Turkers of-
ten think creating natural objects like moon or mountain is implausible but creating an equally big (but artificial) object like skyscraper is plausible.",7 Error Analysis,[0],[0]
• hollow objects (15%).,7 Error Analysis,[0],[0]
"plane-contain-shell
and purse-contain-scissors are plausible, but the hollow-object-can-contain-things attribute is failed to be captured.",7 Error Analysis,[0],[0]
• forefoot dexterity (5%).,7 Error Analysis,[0],[0]
"horse-hug-man is
implausible but bear-hug-man is plausible; For *-snatch-watch, girl is a plausible subject, but not pig.",7 Error Analysis,[0],[0]
"Obviously the dexterity of the forefoot of the agent matters here.
",7 Error Analysis,[0],[0]
"The analysis shows that the task and the dataset highlights the necessity for more sophisticated knowledge featurization and cleverer learning techniques (e.g. features from computer vision, propagation methods with stronger capacity to generalize) to reduce the cost of manual annotation.",7 Error Analysis,[0],[0]
7Percentages calculated with the 68% as the denominator.,7 Error Analysis,[0],[0]
Full list in supplementary material.,7 Error Analysis,[0],[0]
"We present the novel task of semantic plausibility, which forms the foundation of various interesting and complex NLP tasks in event semantics (Bowman et al., 2016; Mostafazadeh et al., 2016; Li and Jurafsky, 2017).",8 Conclusion,[0],[0]
"We collected a high-quality dedicated dataset, showed empirically that the conventional, distribution data only model fails on the task, and that clever world knowledge injection can help substantially with little annotation cost, which lends initial empirical support for the scalability of our approach in practical applications, i.e. labeling little but propagating well approximates performance with full annotation.",8 Conclusion,[0],[0]
"Granted that annotation-based injection method does not cover the full spectrum of leverageable world knowledge information (alternative/complementary sources being images and videos, e.g. Bagherinezhad et al. 2016), it is indeed irreplaceable in some cases (e.g. features such as WEIGHT or RIGIDITY are not easily learnable through visual modality), and in other cases presents a low-cost and effective option.",8 Conclusion,[0],[0]
"Finally, we also discovered the limitation of existing methods through a detailed error analysis, and thereby invite cross-area effort (e.g. multimodal knowledge features) in the future exploration in automated methods for semantic plausibility learning.",8 Conclusion,[0],[0]
This research was supported by NSF grant IIS 1523637.,Acknowledgments,[0],[0]
"Further, this material is based on research sponsored by DARPA under agreement number FA8750-18- 2-0017.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
We acknowledge the Texas Advanced Computing Center for providing grid resources that contributed to these results.,Acknowledgments,[0],[0]
We would also like to thank our reviewers for their insightful comments.,Acknowledgments,[0],[0]
"Distributional data tells us that a man can swallow candy, but not that a man can swallow a paintball, since this is never attested.",abstractText,[0],[0]
However both are physically plausible events.,abstractText,[0],[0]
This paper introduces the task of semantic plausibility: recognizing plausible but possibly novel events.,abstractText,[0],[0]
We present a new crowdsourced dataset of semantic plausibility judgments of single events such as man swallow paintball.,abstractText,[0],[0]
"Simple models based on distributional representations perform poorly on this task, despite doing well on selection preference, but injecting manually elicited knowledge about entity properties provides a substantial performance boost.",abstractText,[0],[0]
Our error analysis shows that our new dataset is a great testbed for semantic plausibility models: more sophisticated knowledge representation and propagation could address many of the remaining errors.,abstractText,[0],[0]
Modeling Semantic Plausibility by Injecting World Knowledge,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 688–697 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1064",text,[0],[0]
"Recently the sequence to sequence model (seq2seq) in neural machine translation (NMT) has achieved certain success over the state-ofthe-art of statistical machine translation (SMT)
∗Work done at Huawei Noah’s Ark Lab, HongKong.
on various language pairs (Bahdanau et al., 2015; Jean et al., 2015; Luong et al., 2015; Luong and Manning, 2015).",1 Introduction,[0],[0]
"However, Shi et al. (2016) show that the seq2seq model still fails to capture a lot of deep structural details, even though it is capable of learning certain implicit source syntax from sentence-aligned parallel corpus.",1 Introduction,[0],[0]
"Moreover, it requires an additional parsing-task-specific training mechanism to recover the hidden syntax in NMT.",1 Introduction,[0],[0]
"As a result, in the absence of explicit linguistic knowledge, the seq2seq model in NMT tends to produce translations that fail to well respect syntax.",1 Introduction,[0],[0]
"In this paper, we show that syntax can be well exploited in NMT explicitly by taking advantage of source-side syntax to improve the translation accuracy.
",1 Introduction,[0],[0]
"In principle, syntax is a promising avenue for translation modeling.",1 Introduction,[0],[0]
"This has been verified by tremendous encouraging studies on syntaxbased SMT that substantially improves translation by integrating various kinds of syntactic knowledge (Liu et al., 2006; Marton and Resnik, 2008;
688
Shen et al., 2008; Li et al., 2013).",1 Introduction,[0],[0]
"While it is yet to be seen how syntax can benefit NMT effectively, we find that translations of NMT sometimes fail to well respect source syntax.",1 Introduction,[0],[0]
Figure 1 (a) shows a Chinese-to-English translation example of NMT.,1 Introduction,[0],[0]
"In this example, the NMT seq2seq model incorrectly translates the Chinese noun phrase (i.e., 新 生/xinsheng 银行/yinhang) into a discontinuous phrase in English (i.e., new ... bank) due to the failure of capturing the internal syntactic structure in the input Chinese sentence.",1 Introduction,[0],[0]
"Statistics on our development set show that one forth of Chinese noun phrases are translated into discontinuous phrases in English, indicating the substantial disrespect of syntax in NMT",1 Introduction,[0],[0]
"translation.1 Figure 1 (b) shows another example with over translation, where the noun phrase 两/liang 个/ge 女孩/nvhai is translated twice in English.",1 Introduction,[0],[0]
"Similar to discontinuous translation, over translation usually happens along with the disrespect of syntax which results in the repeated translation of the same source words in multiple positions of the target sentence.
",1 Introduction,[0],[0]
"In this paper we are not aiming at solving any particular issue, either the discontinuous translation or the over translation.",1 Introduction,[0],[0]
"Alternatively, we address how to incorporate explicitly the source syntax to improve the NMT translation accuracy with the expectation of alleviating the issues above in general.",1 Introduction,[0],[0]
"Specifically, rather than directly assigning each source word with manually designed syntactic labels, as Sennrich and Haddow (2016) do, we linearize a phrase parse tree into a structural label sequence and let the model automatically learn useful syntactic information.",1 Introduction,[0],[0]
"On the basis, we systematically propose and compare several different approaches to incorporating the label sequence into the seq2seq NMT model.",1 Introduction,[0],[0]
Experimentation on Chinese-to-English translation demonstrates that all proposed approaches are able to improve the translation accuracy.,1 Introduction,[0],[0]
"As a background and a baseline, in this section, we briefly describe the NMT model with an attention mechanism by Bahdanau et al. (2015), which mainly consists of an encoder and a decoder, as shown in Figure 2.
",2 Attention-based NMT,[0],[0]
"Encoder The encoding of a source sentence is for1Manually examining 200 random such discontinuously translated noun phrases, we find that 90% of them should be continuously translated according to the reference translation.
mulated using a pair of neural networks, i.e., two recurrent neural networks (denoted bi-RNN): one reads an input sequence x =",2 Attention-based NMT,[0],[0]
"(x1, ..., xm) from left to right and outputs a forward sequence of hidden states ( −→ h1, ..., −→ hm), while the other operates from right to left and outputs a backward sequence ( ←− h1, ..., ←− hm).",2 Attention-based NMT,[0],[0]
Each source word xj is represented as hj (also referred to as word annotation vector): the concatenation of hidden states −→ hj and ←− hj .,2 Attention-based NMT,[0],[0]
"Such bi-RNN encodes not only the word itself but also its left and right context, which can provide important evidence for its translation.
",2 Attention-based NMT,[0],[0]
"Decoder The decoder is also an RNN that predicts a target sequence y = (y1, ..., yn).",2 Attention-based NMT,[0],[0]
"Each target word yi is predicted via a multi-layer perceptron (MLP) component which is based on a recurrent hidden state si, the previous predicted word yi−1, and a source-side context vector ci.",2 Attention-based NMT,[0],[0]
"Here, ci is calculated as a weighted sum over source annotation vectors (h1, ..., hm).",2 Attention-based NMT,[0],[0]
The weight vector αi ∈,2 Attention-based NMT,[0],[0]
"Rm over source annotation vectors is obtained by an attention model, which captures the correspondences between the source and the target languages.",2 Attention-based NMT,[0],[0]
The attention weight αij is computed based on the previous recurrent hidden state si−1 and source annotation vector hj .,2 Attention-based NMT,[0],[0]
"The conventional NMT models treat a sentence as a sequence of words and ignore external knowledge, failing to effectively capture various kinds of inherent structure of the sentence.",3 NMT with Source Syntax,[0],[0]
"To leverage external knowledge, specifically the syntax in the source side, we focus on the parse tree of a sentence and propose three different NMT models that explicitly consider the syntactic structure into encoding.",3 NMT with Source Syntax,[0],[0]
"Our purpose is to inform the NMT model the structural context of each word in its corresponding parse tree with the goal that the learned annotation vectors (h1, ..., hm) encode not
I love dogs
only the information of words and their surroundings, but also structural context in the parse tree.",3 NMT with Source Syntax,[0],[0]
"In the rest of this section, we use English sentences as examples to explain our methods.",3 NMT with Source Syntax,[0],[0]
"To obtain the structural context of a word in its parse tree, ideally the model should not only capture and remember the whole parse tree structure, but also discriminate the contexts of any two different words.",3.1 Syntax Representation,[0],[0]
"However, considering the lack of efficient way to directly model structural information, an alternative way is to linearize the phrase parse tree into a sequence of structural labels and learn the structural context through the sequence.",3.1 Syntax Representation,[0],[0]
"For example, Figure 3(c) shows the structural label sequence of Figure 3(b) in a simple way following a depth-first traversal order.",3.1 Syntax Representation,[0],[0]
"Note that linearizing a parse tree in a depth-first traversal order into a sequence of structural labels has also been widely adopted in recent advances in neural syntactic parsing (Vinyals et al., 2015; Choe and Charniak, 2016), suggesting that the linearized sequence can be viewed as an alternative to its tree structure.2
2We have also tried to include the ending brackets in the structural label sequence, as what (Vinyals et al., 2015; Choe
hwj and
←−−
hwj are the forward and
backward hidden states for word wj ,
−→
hli and
←−
hli are for structural label li, ewj is the word embedding for word wj , and ⊕ is for concatenation operator.
",3.1 Syntax Representation,[0],[0]
There is no doubt that the structural label sequence is much longer than its word sequence.,3.1 Syntax Representation,[0],[0]
"In order to obtain the structural label annotation vector for wi in word sequence, we simply look for wi’s part-of-speech (POS) tag in the label sequence and view the tag’s annotation vector as wi’s label annotation vector.",3.1 Syntax Representation,[0],[0]
This is because wi’s POS tag location can also represent wi’s location in the parse tree.,3.1 Syntax Representation,[0],[0]
"For example, in Figure 3, word w1 in (a) maps to l3 in (c) since l3 is the POS tag of w1.",3.1 Syntax Representation,[0],[0]
"Likewise, w2 maps to l5 and w3 to l7.",3.1 Syntax Representation,[0],[0]
"That is to say, we use l3’s learned annotation vector as w1’s label annotation vector.
and Charniak, 2016) do.",3.1 Syntax Representation,[0],[0]
"However, the performance gap is very small by adding the ending brackets or not.",3.1 Syntax Representation,[0],[0]
"In the next, we first propose two different encoders to augment word annotation vector with its corresponding label annotation vector, each of which consists of two RNNs 3: in one encoder, the two RNNs work independently (i.e., Parallel RNN Encoder) while in another encoder the two RNNs work in a hierarchical way (i.e., Hierarchical RNN Encoder).",3.2 RNN Encoders with Source Syntax,[0],[0]
The difference between the two encoders lies in how the two RNNs interact.,3.2 RNN Encoders with Source Syntax,[0],[0]
"Then, we propose the third encoder with a single RNN, which learns word and label annotation vectors stitchingly (i.e., Mixed RNN Encoder).",3.2 RNN Encoders with Source Syntax,[0],[0]
"Since any of the above three approaches focuses only on the encoder as to generate source annotation vectors along with structural information, we keep the rest part of the NMT models unchanged.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"Parallel RNN Encoder Figure 4 (a) illustrates our Parallel RNN encoder, which includes two parallel RNNs: i.e., a word RNN and a structural label RNN.",3.2 RNN Encoders with Source Syntax,[0],[0]
"On the one hand, the word RNN, as in conventional NMT models, takes a word sequence as input and output a word annotation vector for each word.",3.2 RNN Encoders with Source Syntax,[0],[0]
"On the other hand, the structural label RNN takes the structural label sequence of the word sequence as input and obtains a label annotation vector for each label.",3.2 RNN Encoders with Source Syntax,[0],[0]
"Besides, we concatenate each word’s word annotation vector and its POS tag’s label annotation vector as the final annotation vector for the word.",3.2 RNN Encoders with Source Syntax,[0],[0]
"For example, the final annotation vector for word love in Figure 4 (a) is [ −−→ hw2; ←−− hw2; −→ hl5; ←− hl5], where the first two subitems [ −−→ hw2; ←−− hw2] are the word annotation vector and the rest two subitems",3.2 RNN Encoders with Source Syntax,[0],[0]
"[ −→ hl5; ←− hl5] are its POS tag VBP’s label annotation vector.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"Hierarchical RNN Encoder Partially inspired by the model architecture of GNMT (Wu et al., 2016) which consists of multiple layers of LSTM RNNs, we propose a two-layer model architecture in which the lower layer is the structural label RNN while the upper layer is the word RNN, as shown in Figure 4 (b).",3.2 RNN Encoders with Source Syntax,[0],[0]
"We put the word RNN in the upper layer because each item in the word sequence can map into an item in the structural label sequence, while this does not hold if the order of the two RNNs is reversed.",3.2 RNN Encoders with Source Syntax,[0],[0]
"As shown in Figure 4 (b), for example, the POS tag VBP’s label annotation vector [ −→ hl5, ←− hl5] is concatenated with word
3Hereafter, we simplify bi-RNN as RNN.
",3.2 RNN Encoders with Source Syntax,[0],[0]
"love’s word embedding ew2 to feed as the input to the word RNN.
",3.2 RNN Encoders with Source Syntax,[0],[0]
Mixed RNN Encoder Figure 5 presents our Mixed RNN encoder.,3.2 RNN Encoders with Source Syntax,[0],[0]
"Similarly, the sequence of input is the linearization of its parse tree (as in Figure 3 (b)) following a depth-first traversal order, but being mixed with both words and structural labels in a stitching way.",3.2 RNN Encoders with Source Syntax,[0],[0]
"It shows that the RNN learns annotation vectors for both the words and the structural labels, though only the annotation vectors of words are further fed to decoding (e.g., ([ −→ h4, ←− h4], [ −→ h7, ←− h7], [ −→ h10, ←− h10])).",3.2 RNN Encoders with Source Syntax,[0],[0]
"Even though the annotation vectors of structural labels are not directly fed forward for decoding, the error signal is back propagated along the word sequence and allows the annotation vectors of structural labels being updated accordingly.",3.2 RNN Encoders with Source Syntax,[0],[0]
"Though all the three encoders model both word sequence and structural label sequence, the differences lie in their respective model architecture with respect to the degree of coupling the two sequences:
• In the Parallel RNN encoder, the word RNN and structural label RNN work in a parallel way.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"That is to say, the error signal back propagated from the word sequence would not affect the structural label RNN, and vice versa.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"In contrast, in the Hierarchical RNN encoder, the error signal back propagated from the word sequence has a direct impact on the structural label annotation vectors, and thus on the structural label embeddings.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Finally, the Mixed RNN encoder ties the structural label sequence and word sequence together in the closest way.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Therefore, the degrees of coupling the word and structural
label sequences in these three encoders are like this: Mixed RNN encoder > Hierarchical RNN encoder >",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Parallel RNN encoder.
",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
• Figure 4 and Figure 5 suggest that the Mixed RNN encoder is the simplest.,3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
"Moreover, comparing to conventional NMT encoders, the difference lies only in the length of the input sequence.",3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
Statistics on our training data reveal that the Mixed RNN encoder approximately triples the input sequence length compared to conventional NMT encoders.,3.3 Comparison of RNN Encoders with Source Syntax,[0],[0]
We have presented our approaches to incorporating the source syntax into NMT encoders.,4 Experimentation,[0],[0]
"In this section, we evaluate their effectiveness on Chinese-to-English translation.",4 Experimentation,[0],[0]
"Our training data for the translation task consists of 1.25M sentence pairs extracted from LDC corpora, with 27.9M Chinese words and 34.5M English words respectively.4",4.1 Experimental Settings,[0],[0]
"We choose NIST MT 06 dataset (1664 sentence pairs) as our development set, and NIST MT 02, 03, 04, and 05 datasets (878, 919, 1788 and 1082 sentence pairs, respectively) as our test sets.5",4.1 Experimental Settings,[0],[0]
"To get the source syntax for sentences on the source-side, we parse the Chinese sentences with Berkeley Parser 6 (Petrov and Klein, 2007) trained on Chinese TreeBank 7.0 (Xue et al., 2005).",4.1 Experimental Settings,[0],[0]
"We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.
",4.1 Experimental Settings,[0],[0]
"For efficient training of neural networks, we limit the maximum sentence length on both source and target sides to 50.",4.1 Experimental Settings,[0],[0]
"We also limit both the source and target vocabularies to the most frequent 16K words in Chinese and English, covering approximately 95.8% and 98.2% of the two corpora respectively.",4.1 Experimental Settings,[0],[0]
All the out-of-vocabulary words are mapped to a special token UNK.,4.1 Experimental Settings,[0],[0]
"Besides, the word embedding dimension is 620 and the size of a hidden layer is 1000.",4.1 Experimental Settings,[0],[0]
"All the other settings are the same as in Bahdanau et al.(2015).
",4.1 Experimental Settings,[0],[0]
"4The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
5http://www.itl.nist.gov/iad/mig/ tests/mt/
6https://github.com/slavpetrov/ berkeleyparser
The inventory of structural labels includes 16 phrase labels and 32 POS tags.",4.1 Experimental Settings,[0],[0]
"In both our Parallel RNN encoder and Hierarchical RNN encoder, we set the embedding dimension of these labels as 100 and the size of a hidden layer as 100.",4.1 Experimental Settings,[0],[0]
"Besides, the maximum structural label sequence length is set to 100.",4.1 Experimental Settings,[0],[0]
"In our Mixed RNN encoder, since we only have one input sequence, we equally treat the structural labels and words (i.e., a structural label is also initialized with 620 dimension embedding).",4.1 Experimental Settings,[0],[0]
"Compared to the baseline NMT model, the only different setting is that we increase the maximum sentence length on source-side from 50 to 150.
",4.1 Experimental Settings,[0],[0]
"We compare our method with two state-of-theart models of SMT and NMT:
• cdec (Dyer et al., 2010): an open source hierarchical phrase-based SMT system (Chiang, 2007) with default configuration and a 4-gram language model trained on the target portion of the training data.7
• RNNSearch: a re-implementation of the attentional NMT system (Bahdanau et al., 2015) with slight changes taken from dl4mt tutorial.8 For the activation function f of an RNN, RNNSearch uses the gated recurrent unit (GRU) recently proposed by (Cho et al., 2014b).",4.1 Experimental Settings,[0],[0]
"It incorporates dropout (Hinton et al., 2012) on the output layer and improves the attention model by feeding the lastly generated word.",4.1 Experimental Settings,[0],[0]
"We use AdaDelta (Zeiler, 2012) to optimize model parameters in training with the mini-batch size of 80.",4.1 Experimental Settings,[0],[0]
"For translation, a beam search with size 10 is employed.",4.1 Experimental Settings,[0],[0]
Table 1 shows the translation performances measured in BLEU score.,4.2 Experiment Results,[0],[0]
"Clearly, all the proposed NMT models with source syntax improve the translation accuracy over all test sets, although there exist considerable differences among different variants.
",4.2 Experiment Results,[0],[0]
Parameters The three proposed models introduce new parameters in different ways.,4.2 Experiment Results,[0],[0]
"As a baseline model, RNNSearch has 60.6M parameters.",4.2 Experiment Results,[0],[0]
"Due to the infrastructure similarity, the Parallel RNN system and the Hierarchical RNN system introduce
7https://github.com/redpony/cdec 8https://github.com/nyu-dl/
dl4mt-tutorial
the similar size of additional parameters, resulting from the RNN model for structural label sequences (about 0.1M parameters) and catering either the augmented annotation vectors (as shown in Figure 4 (a)) or the augmented word embeddings (as shown in Figure 4 (b))",4.2 Experiment Results,[0],[0]
(the remain parameters).,4.2 Experiment Results,[0],[0]
"It is not surprising that the Mixed RNN system does not require any additional parameters since though the input sequence becomes longer, we keep the vocabulary size unchanged, resulting in no additional parameters.
",4.2 Experiment Results,[0],[0]
Speed Introducing the source syntax slightly slows down the training speed.,4.2 Experiment Results,[0],[0]
"When running on a single GPU GeForce GTX 1080, the baseline model speeds 153 minutes per epoch with 14K updates while the proposed structural label RNNs in both Parallel RNN and Hierarchical RNN systems only increases the training time by about 6% (thanks to the small size of structural label embeddings and annotation vectors), and the Mixed RNN system spends 26% more training time to cater the triple sized input sequence.
",4.2 Experiment Results,[0],[0]
"Comparison with the baseline NMT model (RNNSearch) While all the three proposed NMT models outperform RNNSearch, the Parallel RNN system and the Hierarchical RNN system achieve similar accuracy (e.g., 36.6 v.s. 36.7).",4.2 Experiment Results,[0],[0]
"Besides, the Mixed RNN system achieves the best accuracy overall test sets with the only exception of NIST MT 02.",4.2 Experiment Results,[0],[0]
"Over all test sets, it outperforms RNNSearch by 1.4 BLEU points and outperforms the other two improved NMT models by 0.3∼0.4 BLEU points, suggesting the benefits of high degree of coupling the word sequence and the structural label sequence.",4.2 Experiment Results,[0],[0]
"This is very encouraging since the Mixed RNN encoder is the simplest, without introducing new parameters and with only slight additional training time.
",4.2 Experiment Results,[0],[0]
Comparison with the SMT model (cdec),4.2 Experiment Results,[0],[0]
Table 1 also shows that all NMT systems outperform the SMT system.,4.2 Experiment Results,[0],[0]
"This is very consistent with other studies on Chinese-to-English translation (Mi et al., 2016; Tu et al., 2017b; Wang et al., 2017).",4.2 Experiment Results,[0],[0]
"As the proposed Mixed RNN system achieves the best performance, we further look at the RNNSearch system and the Mixed RNN system to explore more on how syntactic information helps in translation.",5 Analysis,[0],[0]
"Following Bahdanau et al. (2015), we group sentences of similar lengths together and compute BLEU scores.",5.1 Effects on Long Sentences,[0],[0]
Figure 6 presents the BLEU scores over different lengths of input sentences.,5.1 Effects on Long Sentences,[0],[0]
It shows that Mixed RNN system outperforms RNNSearch over sentences with all different lengths.,5.1 Effects on Long Sentences,[0],[0]
"It also shows that the performance drops substantially
when the length of input sentences increases.",5.1 Effects on Long Sentences,[0],[0]
"This performance trend over the length is consistent with the findings in (Cho et al., 2014a; Tu et al., 2016, 2017a).",5.1 Effects on Long Sentences,[0],[0]
"We also observe that the NMT systems perform surprisingly bad on sentences over 50 in length, especially compared to the performance of SMT system (i.e., cdec).",5.1 Effects on Long Sentences,[0],[0]
"We think that the bad behavior of NMT systems towards long sentences (e.g., length of 50) is due to the following two reasons: (1) the maximum source sentence length limit is set as 50 in training, 9 making the learned models not ready to translate sentences over the maximum length limit; (2) NMT systems tend to stop early for long input sentences.",5.1 Effects on Long Sentences,[0],[0]
"Due to the capability of carrying syntactic information in source annotation vectors, we conjecture that our model with source syntax is also beneficial for alignment.",5.2 Analysis on Word Alignment,[0],[0]
"To test this hypothesis, we carry out experiments of the word alignment task on the evaluation dataset from Liu and Sun (2015), which contains 900 manually aligned Chinese-English sentence pairs.",5.2 Analysis on Word Alignment,[0],[0]
"We force the decoder to output reference translations, as to get automatic alignments between input sentences and their reference translations.",5.2 Analysis on Word Alignment,[0],[0]
"To evaluate alignment performance, we report the alignment error rate (AER) (Och and Ney, 2003) in Table 2.
",5.2 Analysis on Word Alignment,[0],[0]
Table 2 shows that source syntax information improves the attention model as expected by maintaining an annotation vector summarizing structural information on each source word.,5.2 Analysis on Word Alignment,[0],[0]
The above subsection examines the alignment performance at the word level.,5.3 Analysis on Phrase Alignment,[0],[0]
"In this subsection, we turn to phrase alignment analysis by moving from word unit to phrase unit.",5.3 Analysis on Phrase Alignment,[0],[0]
"Given a source phrase XP, we use word alignments to examine if the phrase is translated continuously (Cont.), or dis-
9Though the maximum source length limit in Mixed RNN system is set to 150, it approximately contains 50 words in maximum.
continuously (Dis.), or if it is not translated at all (Un.).
",5.3 Analysis on Phrase Alignment,[0],[0]
"There are some phrases, such as noun phrases (NPs), prepositional phrases (PPs) that we usually expect to have a continuous translation.",5.3 Analysis on Phrase Alignment,[0],[0]
"With respect to several such types of phrases, Table 3 shows how these phrases are translated.",5.3 Analysis on Phrase Alignment,[0],[0]
"From the table, we see that translations of RNNSearch system do not respect source syntax very well.",5.3 Analysis on Phrase Alignment,[0],[0]
"For example, in RNNSearch translations, 57.3%, 33.6%, and 9.1% of PPs are translated continuously, discontinuously, and untranslated, respectively.",5.3 Analysis on Phrase Alignment,[0],[0]
"Fortunately, our Mixed RNN system is able to have more continuous translation for those phrases.",5.3 Analysis on Phrase Alignment,[0],[0]
Table 3 also suggests that there is still much room for NMT to show more respect to syntax.,5.3 Analysis on Phrase Alignment,[0],[0]
"To estimate the over translation generated by NMT, we propose ratio of over translation (ROT):
ROT =
∑ wi t(wi)
|w| (1)
where |w| is the number of words in consideration, t(wi) is the times of over translation for word wi.",5.4 Analysis on Over Translation,[0],[0]
Given a word w and its translation e = e1e2 . . .,5.4 Analysis on Over Translation,[0],[0]
"en, we have:
t(w) =",5.4 Analysis on Over Translation,[0],[0]
"|e| − |uniq(e)| (2)
where |e| is the number of words in w’s translation e, while |uniq(e)| is the number of unique words in e.",5.4 Analysis on Over Translation,[0],[0]
"For example, if a source word 香
港/xiangkang is translated as hong kong hong kong, we say it being over translated 2 times.
",5.4 Analysis on Over Translation,[0],[0]
Table 4 presents ROT grouped by some typical POS tags.,5.4 Analysis on Over Translation,[0],[0]
It is not surprising that RNNSearch system has high ROT with respect to POS tags of NR (proper noun) and CD (cardinal number): this is due to the fact that the two POS tags include high percentage of unknown words which tend to be translated multiple times in translation.,5.4 Analysis on Over Translation,[0],[0]
Words of DT (determiner) are another source of over translation since they are usually translated to multiple the in English.,5.4 Analysis on Over Translation,[0],[0]
"It also shows that by introducing source syntax, Mixed RNN system alleviates the over translation issue by 18%: ROT drops from 5.5% to 4.5%.",5.4 Analysis on Over Translation,[0],[0]
We analyze the translation of source-side rare words that are mapped to a special token UNK.,5.5 Analysis on Rare Word Translation,[0],[0]
"Given a rare word w, we examine if it is translated into a non-UNK word (non-UNK), UNK (UNK), or if it is not translated at all (Un.).
",5.5 Analysis on Rare Word Translation,[0],[0]
Table 5 shows how source-side rare words are translated.,5.5 Analysis on Rare Word Translation,[0],[0]
The four POS tags listed in the table account for about 90% of all rare words in the test sets.,5.5 Analysis on Rare Word Translation,[0],[0]
It shows that in Mixed RNN system is more likely to translate source-side rare words into UNK on the target side.,5.5 Analysis on Rare Word Translation,[0],[0]
This is reasonable since the source side rare words tends to be translated into rare words in the target side.,5.5 Analysis on Rare Word Translation,[0],[0]
"Moreover, it is hard to obtain its correct non-UNK translation when a source-side rare word is replaced as UNK.
",5.5 Analysis on Rare Word Translation,[0],[0]
Note that our approach is compatible with with approaches of open vocabulary.,5.5 Analysis on Rare Word Translation,[0],[0]
"Taking the sub-
word approach (Sennrich et al., 2016) as an example, for a word on the source side which is divided into several subword units, we can synthesize subPOS nodes that cover these units.",5.5 Analysis on Rare Word Translation,[0],[0]
"For example, if misunderstand/VB is divided into units of mis and understand, we construct substructure (VB (VB-F mis) (VB-I understand)).",5.5 Analysis on Rare Word Translation,[0],[0]
"While there has been substantial work on linguistically motivated SMT, approaches that leverage syntax for NMT start to shed light very recently.",6 Related Work,[0],[0]
"Generally speaking, NMT can provide a flexible mechanism for adding linguistic knowledge, thanks to its strong capability of automatically learning feature representations.
",6 Related Work,[0],[0]
"Eriguchi et al. (2016) propose a tree-tosequence model that learns annotation vectors not only for terminal words, but also for non-terminal nodes.",6 Related Work,[0],[0]
They also allow the attention model to align target words to non-terminal nodes.,6 Related Work,[0],[0]
Our approach is similar to theirs by using source-side phrase parse tree.,6 Related Work,[0],[0]
"However, our Mixed RNN system, for example, incorporates syntax information by learning annotation vectors of syntactic labels and words stitchingly, but is still a sequenceto-sequence model, with no extra parameters and with less increased training time.
",6 Related Work,[0],[0]
Sennrich and Haddow (2016) define a few linguistically motivated features that are attached to each individual words.,6 Related Work,[0],[0]
"Their features include lemmas, subword tags, POS tags, dependency labels, etc.",6 Related Work,[0],[0]
"They concatenate feature embeddings with word embeddings and feed the concatenated em-
beddings into the NMT encoder.",6 Related Work,[0],[0]
"On the contrast, we do not specify any feature, but let the model implicitly learn useful information from the structural label sequence.
",6 Related Work,[0],[0]
Shi et al. (2016) design a few experiments to investigate if the NMT system without external linguistic input is capable of learning syntactic information on the source-side as a by-product of training.,6 Related Work,[0],[0]
"However, their work is not focusing on improving NMT with linguistic input.",6 Related Work,[0],[0]
"Moreover, we analyze what syntax is disrespected in translation from several new perspectives.
",6 Related Work,[0],[0]
Garcı́a-Martı́nez et al. (2016) generalize NMT outputs as lemmas and morphological factors in order to alleviate the issues of large vocabulary and out-of-vocabulary word translation.,6 Related Work,[0],[0]
The lemmas and corresponding factors are then used to generate final words in target language.,6 Related Work,[0],[0]
"Though they use linguistic input on the target side, they are limited to the word level features.",6 Related Work,[0],[0]
"Phrase level, or even sentence level linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time.",6 Related Work,[0],[0]
"In this paper, we have investigated whether and how source syntax can explicitly help NMT to improve its translation accuracy.
",7 Conclusion,[0],[0]
"To obtain syntactic knowledge, we linearize a parse tree into a structural label sequence and let the model automatically learn useful information through it.",7 Conclusion,[0],[0]
"Specifically, we have described three different models to capture the syntax knowledge, i.e., Parallel RNN, Hierarchical RNN, and Mixed RNN.",7 Conclusion,[0],[0]
Experimentation on Chinese-to-English translation shows that all proposed models yield improvements over a state-ofthe-art baseline NMT system.,7 Conclusion,[0],[0]
"It is also interesting to note that the simplest model (i.e., Mixed RNN) achieves the best performance, resulting in obtaining significant improvements of 1.4 BLEU points on NIST MT 02 to 05.
",7 Conclusion,[0],[0]
"In this paper, we have also analyzed the translation behavior of our improved system against the state-of-the-art NMT baseline system from several perspectives.",7 Conclusion,[0],[0]
Our analysis shows that there is still much room for NMT translation to be consistent with source syntax.,7 Conclusion,[0],[0]
"In our future work, we expect several developments that will shed more light on utilizing source syntax, e.g., designing novel syn-
tactic features (e.g., features showing the syntactic role that a word is playing) for NMT, and employing the source syntax to constrain and guild the attention models.",7 Conclusion,[0],[0]
"The authors would like to thank three anonymous reviewers for providing helpful comments, and also acknowledge Xing Wang, Xiangyu Duan, Zhengxian Gong for useful discussions.",Acknowledgments,[0],[0]
"This work was supported by National Natural Science Foundation of China (Grant No. 61525205, 61331011, 61401295).",Acknowledgments,[0],[0]
"Even though a linguistics-free sequence to sequence model in neural machine translation (NMT) has certain capability of implicitly learning syntactic information of source sentences, this paper shows that source syntax can be explicitly incorporated into NMT effectively to provide further improvements.",abstractText,[0],[0]
"Specifically, we linearize parse trees of source sentences to obtain structural label sequences.",abstractText,[0],[0]
"On the basis, we propose three different sorts of encoders to incorporate source syntax into NMT: 1) Parallel RNN encoder that learns word and label annotation vectors parallelly; 2) Hierarchical RNN encoder that learns word and label annotation vectors in a two-level hierarchy; and 3) Mixed RNN encoder that stitchingly learns word and label annotation vectors over sequences where words and labels are mixed.",abstractText,[0],[0]
Experimentation on Chinese-to-English translation demonstrates that all the three proposed syntactic encoders are able to improve translation accuracy.,abstractText,[0],[0]
"It is interesting to note that the simplest RNN encoder, i.e., Mixed RNN encoder yields the best performance with an significant improvement of 1.4 BLEU points.",abstractText,[0],[0]
"Moreover, an in-depth analysis from several perspectives is provided to reveal how source syntax benefits NMT.",abstractText,[0],[0]
Modeling Source Syntax for Neural Machine Translation,title,[0],[0]
"In many real-world domains, data acquisition is costly.",1. Introduction,[0],[0]
"For instance, magnetic resonance imaging (MRI) requires scan times proportional to the number of measurements, which can be significant for patients (Lustig et al., 2008).",1. Introduction,[0],[0]
"Geophysical applications like oil drilling require expensive simulation of seismic waves (Qaisar et al., 2013).",1. Introduction,[0],[0]
"Such appli-
1Computer Science Department, Stanford University, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Manik Dhar <dmanik@cs.stanford.edu>, Aditya Grover",1. Introduction,[0],[0]
"<adityag@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
cations, among many others, can benefit significantly from compressed sensing techniques to acquire signals efficiently (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",1. Introduction,[0],[0]
"In compressed sensing, we wish to acquire an n-dimensional signal x ∈ Rn using only m n measurements linear in x.",1. Introduction,[0],[0]
"The measurements could potentially be noisy, but even in the absence of any noise we need to impose additional structure on the signal to guarantee unique recovery.",1. Introduction,[0],[0]
"Classical results on compressed sensing impose structure by assuming the underlying signal to be approximately l-sparse in some known basis, i.e., the l-largest entries dominate the rest.",1. Introduction,[0],[0]
"For instance, images and audio signals are typically sparse in the wavelet and Fourier basis respectively (Mallat, 2008).",1. Introduction,[0],[0]
"If the matrix of linear vectors relating the signal and measurements satisfies certain mild conditions, then one can provably recover x with only m = O(l log nl ) measurements using LASSO (Tibshirani, 1996; Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006; Bickel et al., 2009).
",1. Introduction,[0],[0]
"Alternatively, structural assumptions on the signals being sensed can be learned from data, e.g., using a dataset of typical signals (Baraniuk et al., 2010; Peyre, 2010; Chen et al., 2010; Yu & Sapiro, 2011).",1. Introduction,[0],[0]
"Particularly relevant to this work, Bora et al. (2017) proposed an approach where structure is provided by a deep generative model learned from data.",1. Introduction,[0],[0]
"Specifically, the underlying signal x being sensed is assumed to be close to the range of a deterministic function expressed by a pretrained, latent variable modelG : Rk → Rn such that x ≈ G(z) where z ∈ Rk denote the latent variables.",1. Introduction,[0],[0]
"Consequently, the signal x is recovered by optimizing for a latent vector z that minimizes the `2 distance between the measurements corresponding to G(z) and the actual ones.",1. Introduction,[0],[0]
"Even though the objective being optimized in this case is non-convex, empirical results suggest that the reconstruction error decreases much faster than LASSO-based recovery as we increase the number of measurements.
",1. Introduction,[0],[0]
"A limitation of the above approach is that the recovered signal is constrained to be in the range of the generator function G. Hence, if the true signal being sensed is not in the range of G, the algorithm cannot drive the reconstruction error to zero even when m ≥ n",1. Introduction,[0],[0]
(even if we ignore error due to measurement noise and non-convex optimization).,1. Introduction,[0],[0]
"This is also observed empirically, as the reconstruction error of generative model-based recovery saturates as we keep
increasing the number of measurements m. On the other hand, LASSO-based recovery continues to shrink the error with increasing number of measurements, eventually outperforming the generative model-based recovery.
",1. Introduction,[0],[0]
"To overcome this limitation, we propose a framework that allows recovery of signals with sparse deviations from the set defined by the range of the generator function.",1. Introduction,[0],[0]
"The recovered signals have the general form of G(ẑ) + ν̂, where ν̂ ∈ Rn is a sparse vector.",1. Introduction,[0],[0]
This allows the recovery algorithm to consider signals away from the range of the generator function.,1. Introduction,[0],[0]
"Similar to LASSO, we relax the hardness in optimizing for sparse vectors by minimizing the `1 norm of the deviations.",1. Introduction,[0],[0]
"Unlike LASSO-based recovery, we can exploit the rich structure imposed by a (deep) generative model (at the expense of solving a hard optimization problem if G is non-convex).",1. Introduction,[0],[0]
"In fact, we show that LASSO-based recovery is a special case of our framework if the generator function G maps all z to the origin.",1. Introduction,[0],[0]
"Unlike generative model-based recovery, the signals recovered by our algorithm are not constrained to be in the range of the generator function.
",1. Introduction,[0],[0]
"Our proposed algorithm, referred to as Sparse-Gen, has desirable theoretical properties and empirical performance.",1. Introduction,[0],[0]
"Theoretically, we derive upper bounds on the reconstruction error for an optimal decoder with respect to the proposed model and show that this error vanishes with m = n measurements.",1. Introduction,[0],[0]
"We confirm our theory empirically, wherein we find that recovery using Sparse-Gen with variational autoencoders (Kingma & Welling, 2014) as the underlying generative model outperforms both LASSO-based and generative model-based recovery in terms of the reconstruction errors for the same number of measurements for MNIST and Omniglot datasets.",1. Introduction,[0],[0]
"Additionally, we observe significant improvements in the more practical and novel task of transfer compressed sensing where a generative model on a data-rich, source domain provides a prior for sensing a data-scarce, target domain.",1. Introduction,[0],[0]
"In this section, we review the necessary background and prior work in modeling domain specific structure in compressed sensing.",2. Preliminaries,[0],[0]
"We are interested in solving the following system of equations,
y = Ax (1)
where x ∈ Rn is the signal of interest being sensed through measurements y ∈ Rm, and A ∈ Rm×n is a measurement matrix.",2. Preliminaries,[0],[0]
"For efficient acquisition of signals, we will design measurement matrices such that m n. However, the system is under-determined whenever rank(A) <",2. Preliminaries,[0],[0]
"n. Hence, unique recovery requires additional assumptions on x. We now discuss two ways to model the structure of x.
Sparsity.",2. Preliminaries,[0],[0]
Sparsity in a well-chosen basis is natural in many domains.,2. Preliminaries,[0],[0]
"For instance, natural images are sparse in the wavelet basis whereas audio signals exhibit sparsity in the Fourier basis (Mallat, 2008).",2. Preliminaries,[0],[0]
"Hence, it is natural to assume the domain of signals x we are interested in recovering is
Sl(0) = {x : ‖x− 0‖0 ≤",2. Preliminaries,[0],[0]
l}.,2. Preliminaries,[0],[0]
"(2)
This is the set of l-sparse vectors with the `0 distance measured from the origin.",2. Preliminaries,[0],[0]
"Such assumptions dominate the prior literature in compressed sensing and can be further relaxed to recover approximately sparse signals (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2. Preliminaries,[0],[0]
Latent variable generative models.,2. Preliminaries,[0],[0]
"A latent variable model specifies a joint distribution Pθ(x, z) over the observed data x (e.g., images) and a set of latent variables z ∈",2. Preliminaries,[0],[0]
"Rk (e.g., features).",2. Preliminaries,[0],[0]
"Given a training set of signals {x1, · · · , xM}, we can learn the parameters θ of such a model, e.g., via maximum likelihood.",2. Preliminaries,[0],[0]
"When Pθ(x, z) is parameterized using deep neural networks, such generative models can effectively model complex, high-dimensional signal distributions for modalities such as images and audio (Kingma & Welling, 2014; Goodfellow et al., 2014).
",2. Preliminaries,[0],[0]
"Given a pretrained latent variable generative model with parameters θ, we can associate a generative model function G : Rk → Rn mapping a latent vector z to the mean of the conditional distribution Pθ(x|z).",2. Preliminaries,[0],[0]
"Thereafter, the space of signals that can be recovered with such a model is given by the range of the generator function,
SG = {G(z) : z ∈ Rk}.",2. Preliminaries,[0],[0]
"(3)
Note that the set is defined with respect to the latent vectors z, and we omit the dependence of G on the parameters θ (which are fixed for a pretrained model) for brevity.",2. Preliminaries,[0],[0]
"Signal recovery in compressed sensing algorithm typically involves solving an optimization problem consistent with the modeling assumptions on the domain of the signals being sensed.
",2.1. Recovery algorithms,[0],[0]
Sparse vector recovery using LASSO.,2.1. Recovery algorithms,[0],[0]
"Under the assumptions of sparsity, the signal x can be recovered by solving an `0 minimization problem (Candès & Tao, 2005; Donoho, 2006; Candès et al., 2006).
",2.1. Recovery algorithms,[0],[0]
"min x ‖x‖0
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (4)
",2.1. Recovery algorithms,[0],[0]
"The objective above is however NP-hard to optimize, and hence, it is standard to consider a convex relaxation,
min x ‖x‖1
s.t.",2.1. Recovery algorithms,[0],[0]
"Ax = y. (5)
",2.1. Recovery algorithms,[0],[0]
"In practice, it is common to solve the Lagrangian of the above problem.",2.1. Recovery algorithms,[0],[0]
We refer to this method as LASSO-based recovery due to similarities of the objective in Eq.,2.1. Recovery algorithms,[0],[0]
"(5) to the LASSO regularization used broadly in machine learning (Tibshirani, 1996).",2.1. Recovery algorithms,[0],[0]
"LASSO-based recovery is the predominant technique for recovering sparse signals since it involves solving a tractable convex optimization problem.
",2.1. Recovery algorithms,[0],[0]
In order to guarantee unique recovery to the underdetermined system in Eq.,2.1. Recovery algorithms,[0],[0]
"(1), the measurement matrix A is designed to satisfy the Restricted Isometry Property (RIP) or the Restricted Eigenvalue Condition (REC) for l-sparse matrices with high probability (Candès & Tao, 2005; Bickel et al., 2009).",2.1. Recovery algorithms,[0],[0]
We define these conditions below.,2.1. Recovery algorithms,[0],[0]
Definition 1.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter α ∈ (0, 1), a matrix A ∈ Rm×n is said to satisfy RIP(l, α) if ∀ x ∈ Sl(0),
(1− α)‖x‖2 ≤ ‖Ax‖2 ≤ (1 + α)‖x‖2.
",2.1. Recovery algorithms,[0],[0]
Definition 2.,2.1. Recovery algorithms,[0],[0]
Let Sl(0) ⊂,2.1. Recovery algorithms,[0],[0]
Rn be the set of l-sparse vectors.,2.1. Recovery algorithms,[0],[0]
"For some parameter γ > 0, a matrix A ∈ Rm×n is said to satisfy REC(l, γ) if ∀ x ∈ Sl(0),
‖Ax‖2 ≥ γ‖x‖2.
",2.1. Recovery algorithms,[0],[0]
"Intuitively, RIP implies that A approximately preserves Euclidean norms for sparse vectors and REC implies that sparse vectors are far from the nullspace of A. Many classes of matrices satisfy these conditions with high probability, including random Gaussian and Bernoulli matrices where every entry of the matrix is sampled from a standard normal and uniform Bernoulli distribution respectively (Baraniuk et al., 2008).
",2.1. Recovery algorithms,[0],[0]
Generative model vector recovery using gradient descent.,2.1. Recovery algorithms,[0],[0]
If the signals being sensed are assumed to lie close to the range SG of a generative model function G as defined in Eq.,2.1. Recovery algorithms,[0],[0]
"(3) , then we can recover the best approximation to the true signal by `2-minimization over z,
min z ‖AG(z)− y‖22.",2.1. Recovery algorithms,[0],[0]
"(6)
The function G is typically expressed as a deep neural network which makes the overall objective non-convex, but differentiable almost everywhere w.r.t z.",2.1. Recovery algorithms,[0],[0]
"In practice, good reconstructions can be recovered by gradient-based optimization methods.",2.1. Recovery algorithms,[0],[0]
"We refer to this method proposed by Bora et al. (2017) as generative model-based recovery.
",2.1. Recovery algorithms,[0],[0]
"To guarantee unique recovery, generative model-based recovery makes two key assumptions.",2.1. Recovery algorithms,[0],[0]
"First, the generator functionG is assumed to be L-Lipschitz, i.e., ∀ z1, z2 ∈ Rk,
‖G(z1)−G(z2)‖2 ≤ L‖z1",2.1. Recovery algorithms,[0],[0]
"− z2‖2.
",2.1. Recovery algorithms,[0],[0]
"Secondly, the measurement matrix A is designed to satisfy the Set-Restricted Eigenvalue Condition (S-REC) with high probability (Bora et al., 2017).
",2.1. Recovery algorithms,[0],[0]
Definition 3.,2.1. Recovery algorithms,[0],[0]
Let S ⊆ Rn.,2.1. Recovery algorithms,[0],[0]
"For some parameters γ > 0, δ ≥ 0, a matrix A ∈ Rm×n is said to satisfy the SREC(S, γ, δ) if ∀ x1, x2 ∈ S,
‖A(x1 − x2)‖2 ≥",2.1. Recovery algorithms,[0],[0]
γ‖x1,2.1. Recovery algorithms,[0],[0]
− x2‖2,2.1. Recovery algorithms,[0],[0]
"− δ.
",2.1. Recovery algorithms,[0],[0]
S-REC generalizes REC to an arbitrary set of vectors S as opposed to just considering the set of approximately sparse vectors Sl(0) and allowing an additional slack term δ.,2.1. Recovery algorithms,[0],[0]
"In particular, S is chosen to be the range of the generator function G for generative model-based recovery.",2.1. Recovery algorithms,[0],[0]
The modeling assumptions based on sparsity and generative modeling discussed in the previous section can be limiting in many cases.,3. The Sparse-Gen framework,[0],[0]
"On one hand, sparsity assumes a relatively weak prior over the signals being sensed.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we observe that the recovered signals xL have large reconstruction error ‖xL− x‖22 especially when the number of measurements m is small.",3. The Sparse-Gen framework,[0],[0]
"On the other hand, generative models imposes a very strong, but rigid prior which works well when the number of measurements is small.",3. The Sparse-Gen framework,[0],[0]
"However, the performance of the corresponding recovery methods saturates with increasing measurements since the recovered signal xG = G(zG) is constrained to lie in the range of the generator function G. If zG ∈ Rk is the optimum value returned by an optimization procedure for Eq.",3. The Sparse-Gen framework,[0],[0]
"(6), then the reconstruction error ‖xG − x‖22 is limited by the dimensionality of the latent space and the quality of the generator function.
",3. The Sparse-Gen framework,[0],[0]
"To sidestep the above limitations, we consider a strictly more expressive class of signals by allowing sparse deviations from the range of a generator function.",3. The Sparse-Gen framework,[0],[0]
"Formally, the domain of the recovered signals is given by,
Sl,G = ∪z∈Dom(G)Sl(G(z)) (7)
where Sl(G(z)) denotes the set of sparse vectors centered on G(z) and z varies over the domain of G (typically Rk).",3. The Sparse-Gen framework,[0],[0]
"We refer to this modeling assumption and the consequent algorithmic framework for recovery as Sparse-Gen.
Based on this modeling assumption, we will recover signals of the form G(z) + ν for some ν ∈",3. The Sparse-Gen framework,[0],[0]
Rn that is preferably sparse.,3. The Sparse-Gen framework,[0],[0]
"Specifically, we consider the optimization of a hybrid objective,
min z,ν ‖ν‖0
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (8)
In the above optimization problem the objective is nonconvex and non-differentiable, while the constraint is nonconvex (for general G), making the above optimization
problem hard to solve.",3. The Sparse-Gen framework,[0],[0]
"To ease the optimization problem, we propose two modifications.",3. The Sparse-Gen framework,[0],[0]
"First, we relax the `0 minimization to an `1 minimization similar to LASSO.
",3. The Sparse-Gen framework,[0],[0]
"min z,ν",3. The Sparse-Gen framework,[0],[0]
"‖ν‖1
s.t.",3. The Sparse-Gen framework,[0],[0]
"A (G(z) + ν) = y. (9)
",3. The Sparse-Gen framework,[0],[0]
"Next, we square the non-convex constraint on both sides and consider the Lagrangian of the above problem to get the final unconstrained optimization problem for Sparse-Gen,
min z,ν ‖ν‖1 + λ‖A (G(z) + ν)− y‖22 (10)
where λ is the Lagrange multiplier.
",3. The Sparse-Gen framework,[0],[0]
The above optimization problem is non-differentiable w.r.t.,3. The Sparse-Gen framework,[0],[0]
ν and non-convex w.r.t.,3. The Sparse-Gen framework,[0],[0]
z,3. The Sparse-Gen framework,[0],[0]
(if G is non-convex).,3. The Sparse-Gen framework,[0],[0]
"In practice, it can be solved in practice using gradient descent (since the non-differentiability is only at a finite number of points) or using sequential convex programming (SCP).",3. The Sparse-Gen framework,[0],[0]
"SCP is an effective heuristic for non-convex problems where the convex portions of the problem are solved using a standard convex optimization technique (Boyd & Vandenberghe, 2004).",3. The Sparse-Gen framework,[0],[0]
"In the case of Eq. (10), the optimization w.r.t. ν (for fixed z) is a convex optimization problem whereas the non-convexity typically involves differentiable terms (w.r.t. z) if G is a deep neural network.",3. The Sparse-Gen framework,[0],[0]
"Empirically, we find excellent recovery by standard first order gradient-based methods (Duchi et al., 2011; Tieleman & Hinton, 2012; Kingma & Ba, 2015).
",3. The Sparse-Gen framework,[0],[0]
"Unlike LASSO-based recovery which recovers only sparse signals, Sparse-Gen can impose a stronger domain-specific prior using a generative model.",3. The Sparse-Gen framework,[0],[0]
"If we fix the generator function to map all z to the origin, we recover LASSO-based recovery as a special case of Sparse-Gen. Additionally, Sparse-Gen is not constrained to recover signals over the range of G, as in the case of generative model-based recovery.",3. The Sparse-Gen framework,[0],[0]
"In fact, it can recover signals with sparse deviations from the range of G. Note that the sparse deviations can be
defined in a basis different from the canonical basis.",3. The Sparse-Gen framework,[0],[0]
"In such cases, we consider the following optimization problem,
min z,ν ‖Bν‖1 + λ‖A",3. The Sparse-Gen framework,[0],[0]
"(G(z) + ν)− y‖22 (11)
where B is a change of basis matrix that promotes sparsity of the vector Bν",3. The Sparse-Gen framework,[0],[0]
.,3. The Sparse-Gen framework,[0],[0]
Figure 1 illustrates the differences in modeling assumptions between Sparse-Gen and other frameworks.,3. The Sparse-Gen framework,[0],[0]
The proofs for all results in this section are given in the Appendix.,4. Theoretical Analysis,[0],[0]
"Our analysis and experiments account for measurement noise in compressed sensing, i.e.,
y = Ax+ .",4. Theoretical Analysis,[0],[0]
"(12)
Let ∆ :",4. Theoretical Analysis,[0],[0]
Rm → Rn denote an arbitrary decoding function used to recover the true signal x from the measurements y ∈ Rm.,4. Theoretical Analysis,[0],[0]
"Our analysis will upper bound the `2-error in recovery incurred by our proposed framework using mixed norm guarantees (in particular, `2/`1).",4. Theoretical Analysis,[0],[0]
"To this end, we first state some key definitions.",4. Theoretical Analysis,[0],[0]
"Define the least possible `1 error for recovering x under the Sparse-Gen modeling as,
σSl,G(x) = inf x̂∈Sl,G
‖x− x̂‖1
where the optimal x̂ is the closest point to x in the allowed domain Sl,G. We now state the main lemma guiding the theoretical analysis.",4. Theoretical Analysis,[0],[0]
Lemma 1.,4. Theoretical Analysis,[0],[0]
"Given a function G : Rk → Rn and measurement noise with ‖ ‖2 ≤ max, let A be any matrix that satisfies S-REC(S1.5l,G, (1− α), δ) and RIP(2l, α) for some α ∈ (0, 1), l > 0.",4. Theoretical Analysis,[0],[0]
"Then, there exists a decoder ∆ :",4. Theoretical Analysis,[0],[0]
"Rm → Rn such that,
‖x−∆(Ax+ )‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, where C0 = 2((1+α)(1−α)−1 +1), C1 = 2(1− α)−1, and δ′ = δ(1− α)−1.
",4. Theoretical Analysis,[0],[0]
The above lemma shows that there exists a decoder such that the error in recovery can be upper bounded for measurement matrices satisfying S-REC and RIP.,4. Theoretical Analysis,[0],[0]
Note that Lemma 1 only guarantees the existence of such a decoder and does not prescribe an optimization algorithm for recovery.,4. Theoretical Analysis,[0],[0]
"Apart from the errors due to the bounded measurement noise max and a scaled slack term appearing in the S-REC condition δ′, the major term in the upper bound corresponds to (up to constants) the minimum possible error incurred by the best possible recovery vector in Sl,G given by σl,G(x).",4. Theoretical Analysis,[0],[0]
"Similar terms appear invariably in the compressed sensing literature and are directly related to the modeling assumptions regarding x (for example, Theorem 8.3 in Cohen et al. (2009)).
",4. Theoretical Analysis,[0],[0]
"Our next lemma shows that random Gaussian matrices satisfy the S-REC (over the range of Lipschitz generative model functions) and RIP conditions with high probability for G with bounded domain, both of which together are sufficient conditions for Lemma 1 to hold.
",4. Theoretical Analysis,[0],[0]
Lemma 2.,4. Theoretical Analysis,[0],[0]
LetG : Bk(r)→ Rn be anL-Lipschitz function where Bk(r) = {z | z ∈,4. Theoretical Analysis,[0],[0]
"Rk, ‖z‖2 ≤ r} is the `2-norm ball in Rk.",4. Theoretical Analysis,[0],[0]
"For α ∈ (0, 1), if
m = O
( 1
α2
( k log ( Lr
δ
)",4. Theoretical Analysis,[0],[0]
+,4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
"then a random matrix A ∈ Rm×n with i.i.d. entries such that Aij ∼ N ( 0, 1m ) satisfies the S-REC(S1.5l,G, 1− α, δ) and RIP(2l, α) with 1− e−Ω(α2m) probability.
",4. Theoretical Analysis,[0],[0]
"Using Lemma 1 and Lemma 2, we can bound the error due to decoding with generative models and random Gaussian measurement matrices in the following result.
",4. Theoretical Analysis,[0],[0]
Theorem 1.,4. Theoretical Analysis,[0],[0]
Let G : Bk(r)→,4. Theoretical Analysis,[0],[0]
Rn be an L-Lipschitz function.,4. Theoretical Analysis,[0],[0]
"For any α ∈ (0, 1), l > 0, let A ∈ Rm×n be a random Gaussian matrix with
m = O
( 1
α2
( k log ( Lr
δ
) +",4. Theoretical Analysis,[0],[0]
l log(n/l) )),4. Theoretical Analysis,[0],[0]
rows of i.i.d.,4. Theoretical Analysis,[0],[0]
"entries scaled such that Ai,j ∼ N(0, 1/m).",4. Theoretical Analysis,[0],[0]
Let ∆ be the decoder satisfying Lemma 1.,4. Theoretical Analysis,[0],[0]
"Then, we have with 1− e−Ω(α2m) probability,
‖x−∆(Ax+ )",4. Theoretical Analysis,[0],[0]
"‖2 ≤ (2l)−1/2C0σl,G(x) +",4. Theoretical Analysis,[0],[0]
"C1 max + δ′
for all x ∈ Rn, ‖ ‖2 ≤ max, where C0, C1, γ, δ′ are constants defined in Lemma 1.
",4. Theoretical Analysis,[0],[0]
"From the above lemma, we see that the number of measurements needed to guarantee upper bounds on the reconstruction error of any signal with high probability depends on two terms.",4. Theoretical Analysis,[0],[0]
"The first term includes dependence on the Lipschitz constant L of the generative model function G. A high Lipschitz constant makes recovery harder (by requiring a larger
number of measurements), but only contributes logarithmically.",4. Theoretical Analysis,[0],[0]
"The second term, typical of results in sparse vector recovery, shows a logarithmic growth on the dimensionality n of the signals.",4. Theoretical Analysis,[0],[0]
"Ignoring logarithmic dependences and constants, recovery using Sparse-Gen requires about O(k + l) measurements for recovery.",4. Theoretical Analysis,[0],[0]
Note that Theorem 1 assumes access to an optimization oracle for decoding.,4. Theoretical Analysis,[0],[0]
"In practice, we consider the solutions returned by gradient-based optimization methods to a non-convex objective defined in Eq.",4. Theoretical Analysis,[0],[0]
"(11) that are not guaranteed to correspond to the optimal decoding in general.
",4. Theoretical Analysis,[0],[0]
"Finally, we obtain tighter bounds for the special case when G is expressed using a neural network with only ReLU activations.",4. Theoretical Analysis,[0],[0]
"These bounds do not rely explicitly on the Lipschitz constant L or require the domain of G to be bounded.
",4. Theoretical Analysis,[0],[0]
Theorem 2.,4. Theoretical Analysis,[0],[0]
"If G : Rk → Rn is a neural network of depth d with only ReLU activations and at most c nodes in each layer, then the guarantees of Theorem 1 hold for
m = O
( 1
α2
( (k + l)d log",4. Theoretical Analysis,[0],[0]
c+,4. Theoretical Analysis,[0],[0]
(k + l),4. Theoretical Analysis,[0],[0]
"log(n/l) )) .
",4. Theoretical Analysis,[0],[0]
"Our theoretical analysis formalizes the key properties of recovering signals using Sparse-Gen. As shown in Lemma 1, there exists a decoder for recovery based on such modeling assumptions that extends recovery guarantees based on vanilla sparse vector recovery and generative model-based recovery.",4. Theoretical Analysis,[0],[0]
Such recovery requires measurement matrices that satisfy both the RIP and S-REC conditions over the set of vectors that deviate in sparse directions from the range of a generative model function.,4. Theoretical Analysis,[0],[0]
"In Theorems 1-2, we observed that the number of measurements required to guarantee recovery with high probability grow almost linearly (with some logarithmic terms) with the latent space dimensionality k of the generative model and the permissible sparsity l for deviating from the range of the generative model.",4. Theoretical Analysis,[0],[0]
We evaluated Sparse-Gen for compressed sensing of highdimensional signals from the domain of benchmark image datasets.,5. Experimental Evaluation,[0],[0]
"Specifically, we considered the MNIST dataset of handwritten digits (LeCun et al., 2010) and the OMNIGLOT dataset of handwritten characters (Lake et al., 2015).",5. Experimental Evaluation,[0],[0]
"Both these datasets have the same data dimensionality (28× 28), but significantly different characteristics.",5. Experimental Evaluation,[0],[0]
The MNIST dataset has fewer classes (10 digits from 0-9) as opposed to Omniglot which shows greater diversity (1623 characters across 50 alphabets).,5. Experimental Evaluation,[0],[0]
"Additional experiments with generative adversarial networks on the CelebA dataset are reported in the Appendix.
Baselines.",5. Experimental Evaluation,[0],[0]
"We considered methods based on sparse vector recovery using LASSO (Tibshirani, 1996; Candès & Tao,
2005) and generative model based recovery using variational autoencoders (VAE) (Kingma & Welling, 2014; Bora et al., 2017).",5. Experimental Evaluation,[0],[0]
"For VAE training, we used the standard train/held-out splits of both datasets.",5. Experimental Evaluation,[0],[0]
Compressed sensing experiments that we report were performed on the entire test set of images.,5. Experimental Evaluation,[0],[0]
"The architecture and other hyperparameter details are given in the Appendix.
",5. Experimental Evaluation,[0],[0]
Experimental setup.,5. Experimental Evaluation,[0],[0]
"For the held-out set of instances, we artificially generated measurements y through a random matrix A ∈ Rm×n with entries sampled i.i.d.",5. Experimental Evaluation,[0],[0]
from a Gaussian with zero mean and standard deviation of 1/m. Measurement noise is sampled from zero mean and diagonal scalar covariance matrix with entries as 0.01.,5. Experimental Evaluation,[0],[0]
"For evaluation, we report the reconstruction error measured as ‖x̂− x‖p where x̂ is the recovered signal and p is a norm of interest, varying the number of measurementsm from 50 to the highest value of 750.",5. Experimental Evaluation,[0],[0]
"We report results for the p = {1, 2,∞} norms.
",5. Experimental Evaluation,[0],[0]
"We evaluated sensing of both continuous signals (MNIST) with pixel values in range [0, 1] and discrete signals (Omniglot) with binary pixel values {0, 1}.",5. Experimental Evaluation,[0],[0]
"For all algorithms considered, recovery was performed by optimizing over a continuous space.",5. Experimental Evaluation,[0],[0]
"In the case of sparse recovery methods (including Sparse-Gen) it is possible that unconstrained optimization returns signals outside the domain of interest, in which case they are projected to the required domain by simple clipping, i.e., any signal less than zero is clipped to 0 and similarly any signal greater than one is clipped to 1.
Results and Discussion.",5. Experimental Evaluation,[0],[0]
The reconstruction errors for varying number of measurements are given in Figure 2.,5. Experimental Evaluation,[0],[0]
"Consistent with the theory, the strong prior in generative modelbased recovery methods outperforms the LASSO-based methods for sparse vector recovery.",5. Experimental Evaluation,[0],[0]
"In the regime of low measurements, the performance of algorithms that can incorporate the generative model prior dominates over methods modeling sparsity using LASSO.",5. Experimental Evaluation,[0],[0]
"The performance of plain generative model-based methods however saturates with increasing measurements, unlike Sparse-Gen and LASSO which continue to shrink the error.",5. Experimental Evaluation,[0],[0]
"The trends are consistent for both MNIST and Omniglot, although we observe the relative magnitudes of errors in the case of Omniglot are much higher than that of MNIST.",5. Experimental Evaluation,[0],[0]
This is expected due to the increased diversity and variations of the structure of the signals being sensed in the case of Omniglot.,5. Experimental Evaluation,[0],[0]
We also observe the trends to be consistent across the various norms considered.,5. Experimental Evaluation,[0],[0]
One of the primary motivations for compressive sensing is to directly acquire the signals using few measurements.,5.1. Transfer compressed sensing,[0],[0]
"On the contrary, learning a deep generative model requires access to large amounts of training data.",5.1. Transfer compressed sensing,[0],[0]
"In several applications, getting the data for training a generative model might not be feasible.",5.1. Transfer compressed sensing,[0],[0]
"Hence, we test the generative model-based recovery on the novel task of transfer compressed sensing.
",5.1. Transfer compressed sensing,[0],[0]
Experimental setup.,5.1. Transfer compressed sensing,[0],[0]
We train the generative model on a source domain (assumed to be data-rich) and related to a data-hungry target domain we wish to sense.,5.1. Transfer compressed sensing,[0],[0]
"Given the matching dimensions of MNIST and Omniglot, we conduct experiments transferring from MNIST (source) to Omniglot (target) and vice versa.
Results and Discussion.",5.1. Transfer compressed sensing,[0],[0]
The reconstruction errors for the norms considered are given in Figure 3.,5.1. Transfer compressed sensing,[0],[0]
"For both the sourcetarget pairs, we observe that the Sparse-Gen consistently performs well.",5.1. Transfer compressed sensing,[0],[0]
Vanilla generative model-based recovery shows hardly an improvements with increasing measurements.,5.1. Transfer compressed sensing,[0],[0]
We can qualitatively see this phenomena for transferring from MNIST (source) to Omniglot (target) in Figure 4.,5.1. Transfer compressed sensing,[0],[0]
"With only m = 100 measurements, all models perform poorly and generative model based methods particularly continue to sense images similar to MNIST.",5.1. Transfer compressed sensing,[0],[0]
"On the other hand, there is a noticeable transition at m = 200 measurements for SparseVAE where it adapts better to the domain being sensed than plain generative model-based recovery and achieves lower reconstruction error.",5.1. Transfer compressed sensing,[0],[0]
"Since the introduction of compressed sensing over a decade ago, there has been a vast body of research studying various extensions and applications (Candès & Tao, 2005; Donoho,
2006; Candès et al., 2006).",6. Related Work,[0],[0]
"This work explores the effect of modeling different structural assumptions on signals in theory and practice.
",6. Related Work,[0],[0]
Themes around sparsity in a well-chosen basis has driven much of the research in this direction.,6. Related Work,[0],[0]
"For instance, the paradigm of model-based compressed sensing accounts for the interdependencies between the dimensions of a sparse data signal (Baraniuk et al., 2010; Duarte & Eldar, 2011; Gilbert et al., 2017).",6. Related Work,[0],[0]
"Alternatively, adaptive selection of basis vectors from a dictionary that best capture the structure of the particular signal being sensed has also been explored (Peyre, 2010; Tang et al., 2013).",6. Related Work,[0],[0]
"Many of these methods have been extended to recovery of structured tensors (Zhang et al., 2013; 2014).",6. Related Work,[0],[0]
"In another prominent line of research involving Bayesian compressed sensing, the sparseness assumption is formalized by placing sparsenesspromoting priors on the signals (Ji et al., 2008; He & Carin, 2009; Babacan et al., 2010; Baron et al., 2010).
",6. Related Work,[0],[0]
Research exploring structure beyond sparsity is relatively scarce.,6. Related Work,[0],[0]
Early works in this direction can be traced to Baraniuk & Wakin (2009) who proposed algorithms for recovering signals lying on a smooth manifold.,6. Related Work,[0],[0]
The generative model-based recovery methods consider functions that do not necessarily define manifolds since the range of a generator function could intersect with itself.,6. Related Work,[0],[0]
"Yu & Sapiro (2011) coined the term statistical compressed sensing and proposed
algorithms for efficient sensing of signals from a mixture of Gaussians.",6. Related Work,[0],[0]
The recent work in deep generative model-based recovery differs in key theoretical aspects as well in the use of a more expressive family of models based on neural networks.,6. Related Work,[0],[0]
A related recent work by Hand & Voroninski (2017) provides theoretical guarantees on the solution recovered for solving non-convex linear inverse problems with deep generative priors.,6. Related Work,[0],[0]
"Empirical advances based on well-designed deep neural network architectures that sacrifice many of the theoretical guarantees have been proposed for applications such as MRI (Mardani et al., 2017; 2018).",6. Related Work,[0],[0]
"Many recent methods propose to learn mappings of signals to measurements using neural networks, instead of restricting them to be linear, random matrices (Mousavi et al., 2015; Kulkarni et al., 2016; Chang et al., 2017; Lu et al., 2018).
",6. Related Work,[0],[0]
"Our proposed framework bridges the gap between algorithms that model structure using sparsity and enjoy good theoretical properties with advances in deep generative models, in particular their use for compressed sensing.",6. Related Work,[0],[0]
"The use of deep generative models as priors for compressed sensing presents a new outlook on algorithms for inexpen-
sive data acquisition.",7. Conclusion and Future Work,[0],[0]
"In this work, we showed that these priors can be used in conjunction with classical modeling assumptions based on sparsity.",7. Conclusion and Future Work,[0],[0]
"Our proposed framework, Sparse-Gen, generalizes both sparse vector recovery and recovery using generative models by allowing for sparse deviations from the range of a generative model function.",7. Conclusion and Future Work,[0],[0]
"The benefits of using such modeling assumptions are observed both theoretically and empirically.
",7. Conclusion and Future Work,[0],[0]
"In the future, we would like to design algorithms that can better model the structure within sparse deviations.",7. Conclusion and Future Work,[0],[0]
"Followup work in this direction can benefit from the vast body of prior work in structured sparse vector recovery (Duarte & Eldar, 2011).",7. Conclusion and Future Work,[0],[0]
"From a theoretical perspective, a better understanding of the non-convexity resulting from generative model-based recovery can lead to stronger guarantees and consequently better optimization algorithms for recovery.",7. Conclusion and Future Work,[0],[0]
"Finally, it would be interesting to extend Sparse-Gen for compressed sensing of other data modalities such as graphs for applications in network tomography and reconstruction (Xu et al., 2011).",7. Conclusion and Future Work,[0],[0]
"Real-world graph networks are typically sparse in the canonical basis and can be modeled effectively using deep generative models (Grover et al., 2018), which is consistent with the modeling assumptions of the Sparse-Gen framework.",7. Conclusion and Future Work,[0],[0]
"We are thankful to Tri Dao, Jonathan Kuck, Daniel Levy, Aditi Raghunathan, and Yang Song for helpful comments on early drafts.",Acknowledgements,[0],[0]
"This research was supported by Intel Corporation, TRI, a Hellman Faculty Fellowship, ONR, NSF (#1651565, #1522054, #1733686 ) and FLI (#2017-158687).",Acknowledgements,[0],[0]
AG is supported by a Microsoft Research PhD Fellowship.,Acknowledgements,[0],[0]
"In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal.",abstractText,[0],[0]
"Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model.",abstractText,[0],[0]
A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements.,abstractText,[0],[0]
"However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined.",abstractText,[0],[0]
"We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals.",abstractText,[0],[0]
"Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior.",abstractText,[0],[0]
"Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",abstractText,[0],[0]
Modeling Sparse Deviations for Compressed Sensing using Generative Models,title,[0],[0]
"Many languages exhibit fluency phenomena that are discontinuous in the surface string, and are thus not modelled well by traditional n-gram language models.",1 Introduction,[0],[0]
"Examples include morphological agreement, e.g. subject-verb agreement in languages that do not (exclusively) follow SVO word order, subcategorisation, and collocations involving distant, but syntactically linked words.
",1 Introduction,[0],[0]
Syntactic language models try to overcome the limitation to a local n-gram context by using syntactically related words (and non-terminals) as context information.,1 Introduction,[0],[0]
"Despite their theoretical attractiveness, it has proven difficult to improve SMT with parsers as language models (Och et al., 2004; Post and Gildea, 2008).
",1 Introduction,[0],[0]
"This paper describes an effective method to model, train, decode with, and weight a syntactic language model for SMT.",1 Introduction,[0],[0]
"While all these aspects are important for successfully applying a syntactic language model, our primary contributions are a novel dependency language model which improves over prior work by making relational modelling assumptions, which we argue are better suited for languages with a (relatively) free word order, and the use of a syntactic evaluation metric for optimizing the loglinear parameters of the SMT model.
",1 Introduction,[0],[0]
"While language models that operate on words linked through a dependency chain – called syntactic n-grams (Sidorov et al., 2013) – can improve translation, some of the improvement is invisible to an n-gram metric such as BLEU.",1 Introduction,[0],[0]
"As a result, tuning to BLEU does not show the full value of a syntactic language model.",1 Introduction,[0],[0]
"What does show its value is an optimization metric that operates on the same syntactic n-grams that are modelled by the dependency LM.
",1 Introduction,[0],[0]
The paper is structured as follows.,1 Introduction,[0],[0]
"Section 2 describes our relational dependency language model; section 3 describes our neural network training procedure, and the integration of the model into an SMT decoder.",1 Introduction,[0],[0]
We describe the syntactic evaluation metric we use for tuning in Section 4.,1 Introduction,[0],[0]
"The language models are evaluated on the basis of perplexity and SMT
169
Transactions of the Association for Computational Linguistics, vol. 3, pp.",1 Introduction,[0],[0]
"169–182, 2015.",1 Introduction,[0],[0]
Action Editor: Philipp Koehn.,1 Introduction,[0],[0]
"Submission batch: 11/2014; Revision batch 2/2015; Published 3/2015.
",1 Introduction,[0],[0]
c©2015 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY-NC-SA 4.0 license.
performance in section 5.",1 Introduction,[0],[0]
"We discuss related work in section 6, and finish with concluding remarks in section 7.",1 Introduction,[0],[0]
"As motivation, and working example for the model description, consider the dependency tree in Figure 1, which is taken from the output of our baseline string-to-tree SMT system.1 The output contains two errors:
• a morphological agreement error between the subject Ergebnisse (plural) and the finite verb wird (singular).
• a subcategorisation error: überraschen is transitive, but the translation has a prepositional phrase instead of an object.
",2 A Relational Dependency Language Model,[0],[0]
"While these errors might not have occurred if the words involved were adjacent to one another here and throughout the training set, non-adjacency is common, especially where the distance between subject and finite verb, or between a full verb and its arguments can be arbitrarily long.
",2 A Relational Dependency Language Model,[0],[0]
"Prior work on syntactic language modelling has typically focused on English, and we argue that some modelling decisions do not transfer well to other languages.",2 A Relational Dependency Language Model,[0],[0]
The dependency models proposed by Shen et al. (2010) and Zhang (2009) rely heavily on structural information such as the direction and distance of the dependent from the parent.,2 A Relational Dependency Language Model,[0],[0]
"In a language where the order of syntactic dependents is more flexible than in English, such as German2, grammatical function (and thus the inflection) is hard to predict from the dependent order.",2 A Relational Dependency Language Model,[0],[0]
"Instead, we make dependency labels, which encode grammatical relations, a core element of our model.3
1The tree is converted into constituency format for compatibility with SCFG decoding algorithms, with dependency edges represented as non-terminal nodes.
",2 A Relational Dependency Language Model,[0],[0]
"2German has a strict word order within noun phrases and for the placement of verbs, but has different word order for main clauses and subordinated clauses, and some flexibility in the order of dependents of a verb.
",2 A Relational Dependency Language Model,[0],[0]
"3Tsarfaty (2010) classifies parsing approaches into configurational approaches that rely on structural information, and relational ones that take grammatical relations as primitives.",2 A Relational Dependency Language Model,[0],[0]
"While she uses dependency syntax as a prototypical example of
Shen et al. (2010) propose a model that estimates probability of each token given its parent and/or preceding siblings.",2 A Relational Dependency Language Model,[0],[0]
"We start with a variant of their model that does not hard-code configurational modelling assumptions, and then extend it by including dependency labels.",2 A Relational Dependency Language Model,[0],[0]
"Let S be a sequence of terminal symbols w1, w2, ..., wn with a dependency topology T , and let hs(i) and ha(i) be lists of heads of preceding siblings and ancestors of wi according to T , from closest to furthest.",2.1 Unlabelled Model,[0],[0]
"In our example in Figure 1:
• w4 = jüngsten
• hs(4) = (der)
• ha(4) =",2.1 Unlabelled Model,[0],[0]
"(Umfrage,Ergebnisse,wird, )",2.1 Unlabelled Model,[0],[0]
Note that ha and its subsequences are instances of syntactic,2.1 Unlabelled Model,[0],[0]
n-grams.,2.1 Unlabelled Model,[0],[0]
"For this model, we follow related work and assume that T is available (Popel and Marecek, 2010), approximating P (S) as P (S|T ).",2.1 Unlabelled Model,[0],[0]
"We make the Markov assumption that the probability of each word only depends on its preceding siblings4 and ancestors, and decompose the probability of a sentence like this:
P (S) = P (w1, w2, ..., wn)
≈ n∏
i=1
P (wi|hs(i), ha(i))",2.1 Unlabelled Model,[0],[0]
"(1)
We further make the Markov assumption that only a fixed window of the closest q siblings, and the closest r ancestors, affect the probability of a word.
",2.1 Unlabelled Model,[0],[0]
"P (S) ≈ n∏
i=1
P (wi|hs(i)q1, ha(i)r1) (2)
",2.1 Unlabelled Model,[0],[0]
"Equation 2 represents our basic, unlabelled model.",2.1 Unlabelled Model,[0],[0]
"It differs from that of Shen et al. (2010) in two ways.
relational approaches, the dependency LM by Shen et al. (2010) would fall into the configurational category, while ours is relational.
",2.1 Unlabelled Model,[0],[0]
"4Shen et al. (2010) use the siblings that are between the word and its parent, i.e. the following siblings if the word comes before its parent.",2.1 Unlabelled Model,[0],[0]
"We believe both preceding and following siblings are potentially useful, but leave expansion of the context to future work.
",2.1 Unlabelled Model,[0],[0]
"die Ergebnisse der jüngsten Umfrage wird für viele überraschen .
",2.1 Unlabelled Model,[0],[0]
"root root
det
subj
det
attr
gmod
pp
pn
aux
sent
punct
$.
vroot
aux
VAFIN
subj
First, it uses separate context windows for siblings and ancestors.",2.1 Unlabelled Model,[0],[0]
"In contrast, Shen et al. (2010) treat the ancestor as the first symbol in a context window that is shared between the ancestor and siblings.",2.1 Unlabelled Model,[0],[0]
"Our formulation encodes our belief that the model should always assume dependence on the r nearest ancestor nodes, regardless of the number of siblings.",2.1 Unlabelled Model,[0],[0]
"Secondly, Shen et al. (2010) separate dependents to the left and to the right of the parent.",2.1 Unlabelled Model,[0],[0]
"While the fixed SVO verb order in English is compatible with such a separation, allowing PL to model subjects, PR to model objects, most arguments can occur before or after the head verb in German main clauses.",2.1 Unlabelled Model,[0],[0]
We thus argue that left and right dependents should be modelled by a single model to allow for sharing of statistical strength.5,2.1 Unlabelled Model,[0],[0]
The motivation for the inclusion of dependency labels is twofold.,2.2 Labelled Model,[0],[0]
"Firstly, having dependency labels in the context serves as a strong signal for the prediction of the correct inflectional form.",2.2 Labelled Model,[0],[0]
"Secondly, dependency labels are the appropriate level of ab-
5Similar arguments have been made for parsing of (relatively) free word-order languages, e.g. by Tsarfaty et al. (2009).
straction to model subcategorisation frames.",2.2 Labelled Model,[0],[0]
"Let D be a sequence of dependency labels l1, l2, ..., ln, with each label li being the label of the incoming arc at position i in T , and ls(i) and la(i) the list of dependency labels of the siblings and ancestors of wi, respectively.",2.2 Labelled Model,[0],[0]
"Continuing the example for w4, these are:
• l4 = attr
• ls(4) =",2.2 Labelled Model,[0],[0]
"(det)
• la(4) =",2.2 Labelled Model,[0],[0]
"(gmod, subj, vroot, sent)
",2.2 Labelled Model,[0],[0]
"We predict both the terminal symbols S and dependency labels D. The latter lets us model subcategorisation by penalizing unlikely relations, e.g. objects whose parent is an intransitive verb.",2.2 Labelled Model,[0],[0]
"We decompose P (S,D) into P (D)× P (S|D) to obtain:
P (S,D) = P (D)× P (S|D)
",2.2 Labelled Model,[0],[0]
"≈ n∏
i=1
",2.2 Labelled Model,[0],[0]
"Pl(i)× Pw(i)
Pl(i) =",2.2 Labelled Model,[0],[0]
"P (li|hs(i)q1, ls(i)q1, ha(i)r1, la(i)r1) Pw(i) =P (wi|hs(i)q1, ls(i)q1, ha(i)r1, la(i)r1, li)
(3)",2.2 Labelled Model,[0],[0]
We here discuss some details for the extraction of the context hs and ha.,2.3 Head and Label Extraction,[0],[0]
"Dependency structures require no language-specific head extraction rules, even in a converted constituency representation.",2.3 Head and Label Extraction,[0],[0]
"In the constituency representation shown in Figure 1, each non-terminal node in the tree that is not a preterminal has exactly one pre-terminal child.",2.3 Head and Label Extraction,[0],[0]
"The head of a non-terminal node can thus be extracted by identifying the pre-terminal child, and taking its terminal symbol as head.",2.3 Head and Label Extraction,[0],[0]
"An exception is the virtual node sent, which is added to the root of the tree to combine subtrees that are not connected in the original grammar, e.g. the main tree and the punctuation symbol.",2.3 Head and Label Extraction,[0],[0]
"If a node has no pre-terminal child, we use a special token as its head.
",2.3 Head and Label Extraction,[0],[0]
"If the sibling of a node is a pre-terminal node, we represent this through a special token in hs and ls.",2.3 Head and Label Extraction,[0],[0]
"We also use special out-of-bound tokens (separate for hs, ha, ls and la) to fill up the context window if the window is larger than the number of siblings and/or ancestors.
",2.3 Head and Label Extraction,[0],[0]
The context extraction rules are languageindependent and can be applied to any dependency structure.,2.3 Head and Label Extraction,[0],[0]
Language-specific or grammar-specific rules are possible in principle.,2.3 Head and Label Extraction,[0],[0]
"For instance, for verbal heads in German, one could consider separable verb prefixes part of the head, and thus model differences in subcategorisation between schlagen (Engl. beat) and schlagen ...",2.3 Head and Label Extraction,[0],[0]
vor (Engl. suggest).,2.3 Head and Label Extraction,[0],[0]
"The model in equation 3 still assumes the topology of the dependency tree to be given, and we remedy this by also predicting pre-terminal nodes, and a virtual STOP node as the last child of each node.",2.4 Predicting the Tree Topology,[0],[0]
"This models the position of the head in a subtree (through the prediction of pre-terminal nodes), and the probability that a word has no more dependents (by assigning probability mass to the STOP node).
",2.4 Predicting the Tree Topology,[0],[0]
"Instead of generating all n terminal symbols as in equation 3, we generate all m nodes in the dependency tree in top-down, depth-first order, with li being PT for pre-terminals, and the node label otherwise, and wi being either the head of the node, or if the node has no pre-terminal child.",2.4 Predicting the Tree Topology,[0],[0]
"Our final model is given in equation 4.
",2.4 Predicting the Tree Topology,[0],[0]
"P (S,D, T )",2.4 Predicting the Tree Topology,[0],[0]
"≈ m∏
i=1
{",2.4 Predicting the Tree Topology,[0],[0]
"Pl(i)× Pw(i), if wi 6= Pl(i), otherwise
(4) Figure 2 illustrates the prediction of a subtree of the dependency tree in Figure 1.",2.4 Predicting the Tree Topology,[0],[0]
"Note that T is encoded implicitly, and can be retrieved from D through a stack to which all nodes (except for preterminal and STOP nodes) are pushed after prediction, and from which the last node is popped when predicting a STOP node.",2.4 Predicting the Tree Topology,[0],[0]
"We extract all training instances from automatically parsed training text, and perform training with a standard feed-forward neural network (Bengio et al., 2003), using the NPLM toolkit (Vaswani et al., 2013).",3 Neural Network Training and SMT Decoding,[0],[0]
"Back-off smoothing schemes are unsatisfactory because it is unclear which part of the context should be forgotten first, and neural networks elegantly solve this problem.",3 Neural Network Training and SMT Decoding,[0],[0]
"We use two separate networks, one for Pw and one for Pl.",3 Neural Network Training and SMT Decoding,[0],[0]
"Both networks share the same input vocabulary, but are trained and applied independently.",3 Neural Network Training and SMT Decoding,[0],[0]
"The model input is a (2q+2r)-word context vector (+1 for Pw to encode li), each word being mapped to a shared embedding layer.",3 Neural Network Training and SMT Decoding,[0],[0]
"We use a single hidden layer with rectifiedlinear activation function, and noise-contrastive estimation (NCE).
",3 Neural Network Training and SMT Decoding,[0],[0]
We integrate our dependency language models into a string-to-tree SMT system as additional feature functions that score each translation hypothesis.,3 Neural Network Training and SMT Decoding,[0],[0]
"The model in equation 4 predicts P (S,D, T ).
",3 Neural Network Training and SMT Decoding,[0],[0]
"Obtaining the probability of the translation hypothesis P (S) would require the (costly) marginalization over all sequences of dependency labels D and topologies T , but like the SMT decoder itself, we approximate the search for the best translation by searching for the highest-scoring derivation, meaning that we directly integrate Pw and Pl as two features into the log-linear SMT model.",3 Neural Network Training and SMT Decoding,[0],[0]
"We use selfnormalized neural networks with precomputation of the hidden layer, which makes the integration into decoding reasonably fast.
",3 Neural Network Training and SMT Decoding,[0],[0]
"The decoder builds the translation bottom-up, and the full context is not available for all symbols in the hypothesis.",3 Neural Network Training and SMT Decoding,[0],[0]
"Vaswani et al. (2013) propose to use a special null word for unavailable context, their embedding being the weighted average of the input embeddings of all other words.",3 Neural Network Training and SMT Decoding,[0],[0]
"We adopt this strategy, with the difference that we use separate null words for each position in the context window in order to reflect distributional differences between the different positions, e.g. between ancestor labels and sibling labels.",3 Neural Network Training and SMT Decoding,[0],[0]
"Symbols are re-scored as more context becomes available in decoding, but poor approximations could affect pruning and thus lead to search errors.",3 Neural Network Training and SMT Decoding,[0],[0]
"In Table 1, we illustrate the use of null words with a 5-gram and a bigram NNLM model.",3 Neural Network Training and SMT Decoding,[0],[0]
"We observe a small increase in entropy when querying the 5-gram model with bigrams, compared to querying a bigram model directly.
",3 Neural Network Training and SMT Decoding,[0],[0]
Some hierarchical SMT systems allow glue rules which concatenate two subtrees.,3 Neural Network Training and SMT Decoding,[0],[0]
"Since the resulting glue structures do not occur in the training data, we do not estimate their probability in our model.",3 Neural Network Training and SMT Decoding,[0],[0]
"When encountering the root of a glue rule in our language model, we recursively evaluate its children, but ignore the glue node itself.",3 Neural Network Training and SMT Decoding,[0],[0]
This could introduce a bias towards using more glue rules during translation.,3 Neural Network Training and SMT Decoding,[0],[0]
"To counter this, and encourage the production of linguistically plausible trees, we assign a fixed, high cost to glue rules.",3 Neural Network Training and SMT Decoding,[0],[0]
"Glue rules thus play a small
role in our systems, with about 100 glue rule applications per 3000 sentences, and could be abandoned entirely.6",3 Neural Network Training and SMT Decoding,[0],[0]
"N-gram based metrics such as BLEU (Papineni et al., 2002) are still predominantly used to optimize the log-linear parameters of SMT systems, and (to a lesser extent) to evaluate the final translation systems.",4 Optimizing Syntactic N-grams,[0],[0]
"However, n-gram metrics are not well suited to measure fluency phenomena with string-level gaps, and there is a danger that BLEU underestimates the modelling power of dependency language models, resulting in a suboptimal assignment of loglinear weights.",4 Optimizing Syntactic N-grams,[0],[0]
"As an alternative metric that operates on the level of syntactic n-grams, we use a variant of the head-word chain metric (HWCM) (Liu and Gildea, 2005).
",4 Optimizing Syntactic N-grams,[0],[0]
"HWCM is a precision metric similar to BLEU, but instead of counting n-gram matches between the translation output and the reference, it compares head-word chains, or syntactic n-grams.",4 Optimizing Syntactic N-grams,[0],[0]
"HWCM is not only suitable for our task because it operates on the same structures as the dependency language models, but also because our string-to-tree SMT architecture produces trees that can be evaluated directly, without requiring a separate parse of the translation output, a task for which few parsers are optimized.",4 Optimizing Syntactic N-grams,[0],[0]
"For extracting syntactic n-grams from the reference translations of the respective development and test sets, we automatically parse them, using the same preprocessing as for training.
",4 Optimizing Syntactic N-grams,[0.9572420875609577],"['For our experiments, we use the scikit-learn implementation of SVM classifier and also the inbuilt KFold function for crossvalidation.']"
"We count syntactic n-grams of sizes 1 to 4, mirroring the typical usage of BLEU.",4 Optimizing Syntactic N-grams,[0],[0]
"Banerjee and Lavie (2005) have demonstrated the importance of recall in MT evaluation, and we compute the harmonic mean of precision and recall, which we denote HWCMf , instead of the original, precision-based metric.",4 Optimizing Syntactic N-grams,[0],[0]
We perform three evaluations of our dependency language models.,5 Evaluation,[0],[0]
"Our perplexity evaluation measures model perplexity on the 1-best output of a
6For efficiency reasons, our experimental systems only perform SCFG parsing for spans of up to 50 words, and use glue rules to concatenate partial derivations in longer sentences.",5 Evaluation,[0],[0]
"Better decoding algorithms have reduced the need for this limit (Sennrich, 2014).
baseline SMT system and a human reference translation.",5 Evaluation,[0],[0]
Our SMT evaluation integrates the model as a feature function in a string-to-tree SMT system and evaluates its impact on translation quality.,5 Evaluation,[0],[0]
"Finally, we quantify the effect of different language models on grammaticality by measuring the number of agreement errors of our SMT systems.
",5 Evaluation,[0],[0]
"We refer to the unlabelled variant of our model (equation 2) as DLM, and to the labelled variant (equation 4) as RDLM, emphasizing that the latter is a relational dependency LM.",5 Evaluation,[0],[0]
"We perform our experiments on English→German data from the WMT 2014 shared translation task (Bojar et al., 2014), consisting of about 4.5 million sentence pairs of parallel data and 120 million sentences of monolingual German data.",5.1 Data and Methods,[0],[0]
We train all language models on the German side of the parallel text and the monolingual data.,5.1 Data and Methods,[0],[0]
"We also perform some experiments on the English→Russian data from the same translation task, with 2 million sentence pairs of parallel data and 34 million sentences of monolingual Russian data.
",5.1 Data and Methods,[0],[0]
"For a 5-gram Neural Network LM baseline (NNLM), and the dependency language models, we train feed-forward Neural Network language models with the NPLM toolkit.",5.1 Data and Methods,[0],[0]
"We use 150 dimensions for the input embeddings, and a single hidden layer with 750 dimensions.",5.1 Data and Methods,[0],[0]
"We use a vocabulary of 500 000 words (70 for the output vocabulary of Pl), from which we draw 100 noise samples for NCE (50 for Pl).",5.1 Data and Methods,[0],[0]
"We train for two epochs, each epoch being a full traversal of the training text.",5.1 Data and Methods,[0],[0]
"For unknown words, we back-off to a special unk token for the sequence models and Pl, and to the pre-terminal symbol for the other dependency models.",5.1 Data and Methods,[0],[0]
"We report perplexity values with softmax normalization, but disable normalization during decoding, relying on the selfnormalization of NCE for efficiency.",5.1 Data and Methods,[0],[0]
"For the translation experiments with DLM and RDLM, we set the sibling window size q to 1, and the ancestor window size r to 2.7
We train baseline language models with interpolated modified Kneser-Ney smoothing with SRILM
7On our test set, a node has an average of 4.6 ancestors (σ = 2.5), and 1.2 left siblings (σ = 1.3).
",5.1 Data and Methods,[0],[0]
"(Stolcke, 2002).",5.1 Data and Methods,[0],[0]
The model in the SMT baseline uses the full vocabulary and a linear interpolation of component models for domain adaptation.,5.1 Data and Methods,[0],[0]
"For the perplexity evaluation, we use the same vocabulary and training data as for the Neural Network models.
",5.1 Data and Methods,[0],[0]
"For the English→German SMT evaluation, our baseline system is a string-to-tree SMT system with Moses (Koehn et al., 2007), with dependency parsing of the German texts (Sennrich et al., 2013).",5.1 Data and Methods,[0],[0]
"It is described in more detail in (Williams et al., 2014).",5.1 Data and Methods,[0],[0]
This setup was ranked 1–2 (out of 18) in the WMT 2014 shared translation task and is stateof-the art.,5.1 Data and Methods,[0],[0]
"Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead.",5.1 Data and Methods,[0],[0]
"For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing.
",5.1 Data and Methods,[0],[0]
"We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on BLEU and a linear interpolation of BLEU and HWCMf , and report both scores for evaluation.",5.1 Data and Methods,[0],[0]
"We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006).",5.1 Data and Methods,[0],[0]
"We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf .
",5.1 Data and Methods,[0],[0]
"5.2 Implementation notes on model by Shen et al. (2010)
",5.1 Data and Methods,[0],[0]
We reimplement the model by Shen et al. (2010) for our evaluation.,5.1 Data and Methods,[0],[0]
"The authors did not specify training and smoothing of their model, so we only adopt their definition of the context window, and use the same neural network architecture as for our other models.",5.1 Data and Methods,[0],[0]
"Specifically, we use two neural networks: one for left dependents, and one for right dependents.",5.1 Data and Methods,[0],[0]
"We use maximum-likelihood estimation for the head of root nodes, ignoring unseen events.",5.1 Data and Methods,[0],[0]
"To distinguish between parents and siblings in the context window, we double the input vocabulary and mark parents with a suffix.",5.1 Data and Methods,[0],[0]
"Like Shen et al. (2010), we ignore the
prediction of STOP labels, meaning that our implementation assumes the dependency topology to be given.",5.1 Data and Methods,[0],[0]
We use a trigram model like the original authors.,5.1 Data and Methods,[0],[0]
"Peter et al. (2012) experiment with higher orders variants, but do not consider grandparent nodes.",5.1 Data and Methods,[0],[0]
"We consider scalability to a larger ancestor context a real concern, since another duplication of the vocabulary may be necessary for each ancestor level.",5.1 Data and Methods,[0],[0]
There are a number of factors that make a direct comparison of the reference set perplexity unfair.,5.3 Perplexity,[0],[0]
"Mainly, the unlabelled dependency model DLM and the one by Shen et al. (2010) assume that the dependency topology is given; Pw even assumes this for the dependency labels D. Conversely, the full RDLM predicts the terminal sequence, the dependency labels, and the dependency topology, and we thus expect it to have a higher perplexity.8 Also note that we compare 5-gram n-gram models to 3- and 4- gram dependency models.",5.3 Perplexity,[0],[0]
"A more minor difference is that n-gram models also predict end-of-sentence tokens, which the dependency models do not.
",5.3 Perplexity,[0],[0]
"Rather than directly comparing perplexity between different models, our focus lies on a perplexity comparison between a human reference translation and the 1-best SMT output of a baseline transla-
8For better comparability, we measure perplexity per surface word, not per prediction.
tion system.",5.3 Perplexity,[0],[0]
"Our basic assumption is that the difference in perplexity (or cross-entropy) tells us whether a model contains information that is not already part of the baseline model, and if incorporating it into our SMT system can nudge the system towards producing a translation that is more similar to the reference.
",5.3 Perplexity,[0],[0]
Results for English→German are shown in table 2.,5.3 Perplexity,[0],[0]
"The baseline 5-gram language model with Kneser-Ney smoothing prefers the SMT output over the reference translation, which is natural given that this language model is part of the system producing the SMT output.",5.3 Perplexity,[0],[0]
"The 5-gram NNLM improves over the Kneser-Ney models, and happens to assign almost the same perplexity score to both texts.",5.3 Perplexity,[0],[0]
"This still means that it is less biased towards the SMT output than the baseline model, and can be a valuable addition to the model.
",5.3 Perplexity,[0],[0]
"The dependency language models all show a preference for the reference translation, with DLM having a stronger preference than the model by Shen et al. (2010), and RDLM having the strongest preference.",5.3 Perplexity,[0],[0]
"The direct comparison of DLM and Pw, which is the component of RDLM that predicts the terminal symbols, shows that dependency labels serve as a strong signal for predicting the terminals, confirming our initial hypothesis.",5.3 Perplexity,[0],[0]
The prediction of the dependency topology and labels through Pl means that the full RDLM has the highest perplexity of all models.,5.3 Perplexity,[0],[0]
"However, it also strongly prefers the human reference text over the baseline SMT output.",5.3 Perplexity,[0],[0]
Translation results for English→German with different language models added to our baseline are shown in Table 3.,5.4 Translation Quality,[0],[0]
"Considering the systems tuned on BLEU, we observe that the 5-gram NNLM and RDLM are best in terms of BLEU and TER, but that RDLM is the only winner9 according to HWCMf and METEOR.",5.4 Translation Quality,[0],[0]
"In particular, we observe a sizable gap of 0.6 HWCMf points between the NNLM and the RDLM systems, despite similar BLEU scores.",5.4 Translation Quality,[0],[0]
"The unlabelled DLM and the dependency LM by Shen et al. (2010), which are generally weaker than RDLM, also tend to improve HWCMf more than BLEU.",5.4 Translation Quality,[0],[0]
"This reflects the fact that the dependency
9We denote a system a winner if no other system [in the group of systems under consideration] is significantly better according to significance testing with Multeval.
LMs improve fluency along the syntactic n-grams that HWCM measures, whereas NNLM only improves local fluency, to which BLEU is most sensitive.",5.4 Translation Quality,[0],[0]
"The fact that the models cover different phenomena is also reflected in the fact that we see further gains from combining the 5-gram NNLM with the strongest dependency LM, RDLM, for a total improvement of 0.9–1.1 BLEU over the baseline.
",5.4 Translation Quality,[0],[0]
"If we use BLEU+HWCMf as our tuning objective, the difference between the models increases.",5.4 Translation Quality,[0],[0]
"Compared to the 5-gram NNLM, the RDLM system gains 0.8–0.9 points in HWCMf and 0.3–0.5 points in BLEU.",5.4 Translation Quality,[0],[0]
"Compared to the original baseline, tuned only on BLEU, the system with RDLM that is tuned on BLEU+HWCMf yields an improvement of 1.1– 1.3 BLEU and 1.3–1.4 HWCMf .
",5.4 Translation Quality,[0],[0]
"If we compare the same system being trained on both tuning objectives, we observe that tuning on BLEU+HWCMf , unsurprisingly, yields higher HWCMf scores than tuning on BLEU only.",5.4 Translation Quality,[0],[0]
What is more surprising is that adding HWCMf as a tuning objective also yields significantly higher BLEU on the test sets for 9 out of 10 data points.,5.4 Translation Quality,[0],[0]
The gap is larger for the two systems with RDLM (0.3–0.6 BLEU) than for the baseline or the NNLM system (0.1–0.2 BLEU).,5.4 Translation Quality,[0],[0]
"We hypothesize that the inclusion of HWCMf as a tuning metric reduces overfitting and encourages the production of more grammatically well-formed constructions, which we expect to be a robust objective across different texts, espe-
cially when coupled with a strong dependency language model such as RDLM.
",5.4 Translation Quality,[0],[0]
Some example translations are shown in table 4.,5.4 Translation Quality,[0],[0]
"They illustrate three error types in the baseline system:
1.",5.4 Translation Quality,[0],[0]
"an error in subject-verb agreement.
",5.4 Translation Quality,[0],[0]
2.,5.4 Translation Quality,[0],[0]
"a subcategorisation error: gelten is a valid translation of the intransitive meaning of apply, but cannot be used for transitive constructions, where anwenden is correct.
3.",5.4 Translation Quality,[0],[0]
"a collocation error: two separate collocations are conflated in the baseline translation:
• reach a decision on [...] eine Entscheidung über",5.4 Translation Quality,[0],[0]
"[...] treffen
• reach an agreement on [...] eine Einigung über",5.4 Translation Quality,[0],[0]
"[...] erzielen
All errors are due to inter-dependencies in the sentence that have string-level gaps, but which can be modelled through syntactic n-grams, and are corrected by the system with RDLM and tuning on BLEU+HWCMf .
",5.4 Translation Quality,[0],[0]
We evaluate a subset of the systems on an English→Russian task to test whether the improvements from adding RDLM and tuning on BLEU+HWCMf apply to other language pairs.,5.4 Translation Quality,[0],[0]
Results are shown in Table 5.,5.4 Translation Quality,[0],[0]
"The system with RDLM
is the consistent winner, and significantly outperforms the baseline for all metrics and test sets.",5.4 Translation Quality,[0],[0]
Tuning on BLEU+HWCMf results in further improvements in HWCMf and TER.,5.4 Translation Quality,[0],[0]
"Looking at the combined effect of adding RDLM and changing the tuning objective, we observe gains in BLEU by 0.5–0.9 points, and gains in HWCMf by 2.1–3.4 points.",5.4 Translation Quality,[0],[0]
"We argue that the dependency language models and HWCMf as a tuning metric improve grammaticality, and we are able to quantify one aspect thereof, morphological agreement, for English→German.",5.5 Morphological Agreement,[0],[0]
"Williams and Koehn (2011) introduce a unification grammar with hand-crafted agreement constraints to identify and suppress selected morphological agreement violations in German, namely in regards to noun phrase agreement, prepositional phrase agreement, and subject-verb agreement.",5.5 Morphological Agreement,[0],[0]
We can use their grammar to analyse the effect of different models on morphological agreement by counting the number of translations that violate at least one agreement constraint.,5.5 Morphological Agreement,[0],[0]
"We assume that the number of false posi-
tives (i.e. correct analyses that trigger an agreement violation) remains roughly constant throughout all systems, so that a reduction in the number of agreement violations is an indicator of better grammatical agreement.
",5.5 Morphological Agreement,[0],[0]
Table 6 shows the results.,5.5 Morphological Agreement,[0],[0]
"While the 5-gram NNLM reduces the number of agreement errors somewhat compared to the baseline (-18%), the reduction is greater for DLM (-34%) and RDLM (-46%).",5.5 Morphological Agreement,[0],[0]
"Neither the baseline nor the 5-gram NNLM
profits strongly from tuning on HWCMf , while the number of agreement errors is further reduced for the system with DLM (-41%) and RDLM (-54%).",5.5 Morphological Agreement,[0],[0]
"Adding the 5-gram NNLM to the RDLM system yields no further reduction on the number of agreement errors.
",5.5 Morphological Agreement,[0],[0]
"Enforcing the agreement constraints on the baseline system (tuned on BLEU+HWCMf ) provides us with a gain of 0.3 in both BLEU and HWCMf ; on the RDLM system, only 0.03.",5.5 Morphological Agreement,[0],[0]
"The fact that the benefit of enforcing the agreement constraints drops off more sharply than the number of constraint violations indicates that the remaining violations tend to be harder for the model to correct, e.g. because the translation model has not learned to produce the required inflection of a word, or because some of the remaining violations are false positives.",5.5 Morphological Agreement,[0],[0]
"While the dependency language models’ effect of improving morphological agreement is not (fully) cumulative with the benefit from enforcing the unification constraints formulated by Williams and Koehn (2011), our model has the advantage of being languageindependent, learning from the data itself rather than relying on manually developed, grammar-specific constraints, and covering a wider range of phenomena such as subcategorisation and syntactic collocations.
",5.5 Morphological Agreement,[0],[0]
"The results confirm that the RDLM is more effective at reducing morphological agreement errors than a similarly trained n-gram NNLM and the unlabelled DLM, and that adding HWCMf to the training objective is beneficial.",5.5 Morphological Agreement,[0],[0]
"On a a meta-evaluation level, we compare the rank correlation between the automatic metrics and the numer of agreement errors with Kendall’s τ correlation, and observe that he number of agreement errors is more strongly (negatively) correlated with HWCMf (τ = −0.92) than with BLEU (τ = −0.77), METEOR (τ = −0.54) or TER (τ = 0.69).",5.5 Morphological Agreement,[0],[0]
"This supports our theoretical expectation that HWCMf is more sensitive to morphological agreement, which is enforced along syntactic n-grams, than n-gram metrics such as BLEU, or the unigram metric METEOR.",5.5 Morphological Agreement,[0],[0]
"While there has been a wide range of dependency language models proposed (e.g. (Chelba et al., 1997;
Quirk et al., 2004; Shen et al., 2010; Zhang, 2009; Popel and Marecek, 2010)), there are vast differences in modelling assumptions.",6 Related Work,[0],[0]
"Our work is most similar to the dependency language model described in Shen et al. (2010), or the h-gram model proposed by Zhang (2009), both of which have been used for SMT.",6 Related Work,[0],[0]
"We make different modelling assumptions, relying less on configurational information, but including the prediction of dependency labels in the model.",6 Related Work,[0],[0]
"We argue that our relational modelling assumptions are more suitable for languages with a relatively free word order such as German.
",6 Related Work,[0],[0]
"To a lesser extent, our work is similar to other parsing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008).",6 Related Work,[0],[0]
"Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking.",6 Related Work,[0],[0]
"Also, there are major differences in the parsing models themselves.",6 Related Work,[0],[0]
"For instance, note that the structured LM by Chelba and Jelinek (2000) uses a binary branching structure, and that complex label sets would be required to encode subcategorisation frames in binary trees (Hockenmaier and Steedman, 2002).
",6 Related Work,[0],[0]
Our neural network is a standard feed-forward neural network as introduced by Bengio et al. (2003).,6 Related Work,[0],[0]
"Recently, recursive neural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and Zuidema, 2014).",6 Related Work,[0],[0]
"The recursive nature of such models allows for the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable.
",6 Related Work,[0],[0]
"While BLEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions.",6 Related Work,[0],[0]
"Some authors concluded that none of
the tested alternatives could consistently outperform BLEU (Cer et al., 2010; Callison-Burch et al., 2011).",6 Related Work,[0],[0]
"Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on BLEU; Lo et al. (2013) do the same for MEANT.
",6 Related Work,[0],[0]
"There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013).",6 Related Work,[0],[0]
"The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models.",6 Related Work,[0],[0]
Such a pipeline of prediction steps is less powerful than our joint prediction of stems and inflection.,6 Related Work,[0],[0]
"For instance, in example 2 in Table 4, our model chooses a different stem to match the subcategorisation frame of the translation; it is not possible to fix the baseline translation with inflection changes alone.",6 Related Work,[0],[0]
The main contribution of this paper is the description of a relational dependency language,7 Conclusion,[0],[0]
"model.10 We show that it is a valuable asset to a state-of-the-art SMT system by comparing perplexity values with other types of languages models, and by its integration into decoding, which results in improvements according to automatic MT metrics and reduces the number of agreement errors.",7 Conclusion,[0],[0]
"We show that the disfluencies that our model captures are qualitatively different from an n-gram Neural Network language model, with our model being more effective at modelling fluency phenomena along syntactic n-grams.
",7 Conclusion,[0],[0]
A second important contribution is the optimization of the log-linear parameters of an SMT system based on syntactic n-grams.,7 Conclusion,[0],[0]
We are to our knowledge the first to tune an SMT system on a non-shallow syntactic similarity metric.,7 Conclusion,[0],[0]
"Apart from showing improvements by tuning on HWCMf , our results also shed light on the interaction between models and tuning metrics.",7 Conclusion,[0],[0]
"With n-gram language models, the choice of tuning metric only had a small effect on the English→German translation results.",7 Conclusion,[0],[0]
"Only with dependency language models, which are able to model the syntactic n-grams that HWCM scores, did we see large improvements from adding
10We have released an implementation of RDLM and tuning on HWCMf as part of the Moses decoder.
HWCMf to the objective function.",7 Conclusion,[0],[0]
"On the one hand, this has implications when evaluating new model components: using an objective function that cannot capture the impact of a model component can result in false negatives because the model component will not receive an appropriate weight, and the model may thus seem to be of little use, even in a human evaluation.",7 Conclusion,[0],[0]
"On the other hand, it is an important finding for the evaluation of objective functions: the performance of an objective function is tied to the power of the underlying model.",7 Conclusion,[0],[0]
"Without a model that is able to model syntactic n-grams, we might have concluded that HWCM is of little help as an objective function.",7 Conclusion,[0],[0]
"Now, we hypothesize that HWCM is well-suited to optimize dependency language models because both operate on syntactic ngrams, just like BLEU and n-gram models are natural counterparts.
",7 Conclusion,[0],[0]
"The approach we present is languageindependent, and we evaluated it on SMT into German and Russian.",7 Conclusion,[0],[0]
"While we have no empirical data on the model’s effectiveness for other target languages, we suspect that syntactic n-grams are especially suited for modelling and evaluating translations into languages with inter-dependencies between distant words and relatively free word order, such as German, Czech, or Russian.
",7 Conclusion,[0],[0]
"In this work, we relied on parse hypotheses being provided by a string-to-tree SMT decoder, but other settings are conceivable for future work, such as performing n-best string reranking by coupling the relational dependency LM with a monolingual parse algorithm.",7 Conclusion,[0],[0]
"Another obvious extension of the relational dependency LM is the inclusion of more context, for instance through larger windows for siblings and ancestors, or source-context as in (Devlin et al., 2014).",7 Conclusion,[0],[0]
"Also, we believe that the model can benefit from further advances in neural network modelling, for instance recent findings that ensembles of networks outperform a single network (Mikolov et al., 2011; Devlin et al., 2014)",7 Conclusion,[0],[0]
I thank Bonnie Webber and the anonymous reviewers for their helpful suggestions and feedback.,Acknowledgements,[0],[0]
This research was funded by the Swiss National Science Foundation under grant P2ZHP1_148717.,Acknowledgements,[0],[0]
"The role of language models in SMT is to promote fluent translation output, but traditional n-gram language models are unable to capture fluency phenomena between distant words, such as some morphological agreement phenomena, subcategorisation, and syntactic collocations with string-level gaps.",abstractText,[0],[0]
Syntactic language models have the potential to fill this modelling gap.,abstractText,[0],[0]
We propose a language model for dependency structures that is relational rather than configurational and thus particularly suited for languages with a (relatively) free word order.,abstractText,[0],[0]
"It is trainable with Neural Networks, and not only improves over standard n-gram language models, but also outperforms related syntactic language models.",abstractText,[0],[0]
We empirically demonstrate its effectiveness in terms of perplexity and as a feature function in string-to-tree SMT from English to German and Russian.,abstractText,[0],[0]
We also show that using a syntactic evaluation metric to tune the log-linear parameters of an SMT system further increases translation quality when coupled with a syntactic language model.,abstractText,[0],[0]
Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 360–369, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Humans appear to organize and remember everyday experiences by imposing a narrative structure on them (Nelson, 1989; Thorne and Nam, 2009; Bruner, 1991; McAdams et al., 2006), and many genres of natural language text are therefore narratively structured, e.g. dinner table conversations, news articles, user reviews and blog posts (Polanyi, 1989; Jurafsky et al., 2014; Bell, 2005; Gordon et al., 2011).",1 Introduction,[0],[0]
"Moreover, there is broad consensus that understanding a narrative involves activating a representation, early in the narrative, of the protagonist and her goals and desires, and then maintaining that representation as the narrative evolves, as a vehicle for explaining the protagonist’s actions and tracking narrative outcomes (Elson, 2012; Rapp and Gerrig, 2006; Trabasso
and van den Broek, 1985; Lehnert, 1981).",1 Introduction,[0],[0]
"To date, there has been limited work on computational models for recognizing the expression of the protagonist’s goals and desires in narrative texts, and tracking their corresponding narrative outcomes.",1 Introduction,[0],[0]
"We introduce a new corpus DesireDB of∼3,500 first-person informal narratives with annotations for desires and their fulfillment status, available online.1 Because first-person narratives often revolve around the narrator’s private states and goals (Labov, 1972), this corpus is highly suitable as a testbed for identifying human desires and their outcomes.",1 Introduction,[0],[0]
"Moreover, first-person narratives allow the narrative protagonist (first-person) to be easily identified and tracked.",1 Introduction,[0],[0]
"Figure 1 illustrates examples of desire and goal expressions in our corpus.
DesireDB is open domain.",1 Introduction,[0],[0]
It contains a broad range of expressions of desires and goal statements in personal narratives.,1 Introduction,[0],[0]
It also includes the narrative context for each desire statement as shown in Figure 2.,1 Introduction,[0],[0]
"We include both prior and
1https://nlds.soe.ucsc.edu/DesireDB
360
post context of the desire expressions, since theories of narrative structure suggest that the evaluation points of a narrative can precede the expression of the events, goals and desires of the narrator (Labov, 1972; Swanson et al., 2014).
",1 Introduction,[0],[0]
"Our approach builds on seminal work on a computational model of Lehnert’s plot units, that applied modern NLP tools to tracking narrative affect states in Aesop’s Fables (Goyal et al., 2010; Lehnert, 1981; Goyal and Riloff, 2013).",1 Introduction,[0],[0]
"Our framing of the problem is also inspired by recent work that identifies three forms of desire expressions in short narratives from MCTest and SimpleWiki and develops models to predict whether desires are fulfilled or unfulfilled (Chaturvedi et al., 2016).",1 Introduction,[0],[0]
"However DesireDB’s narrative and sentence structure is more complex than either MCTest or SimpleWiki (Richardson et al., 2013; Coster and Kauchak, 2011).
",1 Introduction,[0],[0]
"We propose new features (Sec 4.1), as well as testing features used in previous work, and apply different classifiers to model desire fulfillment in our corpus.",1 Introduction,[0],[0]
We also directly compare to results on MCTest and SimpleWiki (Sec 4.4).,1 Introduction,[0],[0]
We apply LSTM models that distinguish between prior and post context and capture the flow of the narrative.,1 Introduction,[0],[0]
"Our best system, a Skip-Thought RNN model, achieves an F-measure of 0.70, while a logistic regression system achieves 0.66.",1 Introduction,[0],[0]
"Our models and features outperform Chaturvedi et al. (2016) on MCTest and SimpleWiki, while providing new results for a new corpus for tracking desires in first-person narratives.",1 Introduction,[0],[0]
"Moreover, analysis of our results shows that features representing the discourse structure (such as overt discourse relation markers) are the best predictors of fulfillment status of a desire or goal.",1 Introduction,[0],[0]
"We also show that both prior and post context are important for this task.
",1 Introduction,[0],[0]
We discuss related work in Sec. 2 and describe our corpus and annotations in Sec. 3.,1 Introduction,[0],[0]
Section 4 presents our features and methods for modeling desire fulfillment in narratives along with the experiments and results including comparison to previous work.,1 Introduction,[0],[0]
"Finally, we present conclusions and future directions in Sec. 5.",1 Introduction,[0],[0]
"There has recently been an upsurge in interest in computational models of narrative structure (Lehnert, 1981; Wilensky, 1982) and story understanding (Rahimtoroghi et al., 2016; Swanson
et al., 2014; Ouyang and McKeown, 2015, 2014).",2 Related Work,[0],[0]
"However there has been limited work on computational models for recognizing the expression of the protagonist’s goals and desires in narrative genres.
",2 Related Work,[0],[0]
"Our approach builds on work by Goyal and Riloff (2013) that applied modern NLP tools to track narrative affect states in Aesop’s Fables (Goyal et al., 2010).",2 Related Work,[0],[0]
They present a system called AESOP that uses a number of existing resources to identify affect states of the characters as part of deriving plot units.,2 Related Work,[0],[0]
"The motivation of modeling plot units is the idea that emotional reactions are central to the notion of a narrative and the main plot of a story can be modeled by tracking the transition between the affect states (Lehnert, 1981).",2 Related Work,[0],[0]
The AESOP system identifies affect states and creates links between them to model plot units and is evaluated on a small set of two-character fables.,2 Related Work,[0],[0]
They performed a manual annotation to examine different types of affect expressions in the narratives.,2 Related Work,[0],[0]
"Their study shows that many affect states arise from events where a character is acted upon in positive or negative ways, not explicit expression of emotions.",2 Related Work,[0],[0]
They also show that most of the affect states emerge by the expression of goals and plans and goal completion.,2 Related Work,[0],[0]
"Some of our features are motivated by the idea that implicit sentiment polarity can represent success or failure of goals and can be used to better model desire and goal
fulfillment in a narrative (Reed et al., 2017), although we cannot directly compare our findings to theirs because their annotations are not publicly available.
",2 Related Work,[0],[0]
"Chaturvedi et al. (2016) exploit two deliberately simplified datasets in order to model desire and its fulfillment: MCTest which contains 660 stories limited to content understandable by 7-year old children, and, SimpleWiki created from a dump of the Simple English Wikipedia discarding all the lists, tables and titles.",2 Related Work,[0],[0]
"They use desire statements matching a list of three verb phrases, wanted to, hoped to, and wished to.",2 Related Work,[0],[0]
Their context representation consists of five or fewer sentences following the desire expression.,2 Related Work,[0],[0]
They use BOW (Bag of Words) as baseline and apply unstructured and structured models for desire fulfillment modeling with different features motivated by narrative structure.,2 Related Work,[0],[0]
Their best result is achieved with a structured prediction model called Latent Structured Narrative Model (LSNM) which models the evolution of the narrative by associating a latent variable with each fragment of the context in the data.,2 Related Work,[0],[0]
"Their best unstructured model is a Logistic Regression classifier that uses all of their features.
",2 Related Work,[0],[0]
"Recent work on computational models of semantics provides an evaluation test for story understanding (Mostafazadeh et al., 2017).",2 Related Work,[0],[0]
"The task includes four-sentence stories, each with two possible endings where only one is correct.",2 Related Work,[0],[0]
"The goal is for each system to select the correct ending of the story by modeling different levels of semantics in narratives, such as lexical, sentential and discourse-level.",2 Related Work,[0],[0]
"The highest performing model with 75% accuracy used a linear regression classifier with several features such as neural language models and stylistic features to model the story coherence (Schwartz et al., 2017).",2 Related Work,[0],[0]
The results from other systems showed that sentiment is an important factor and using only sentiment features could achieve about 65% accuracy on the test.,2 Related Work,[0],[0]
DesireDB aims to provide a testbed for modeling desire and goals in personal narrative and predicting their fulfillment status.,3 DesireDB Corpus,[0],[0]
"We develop a systematic method to identify desire and goal statements, and then collect annotations to create goldstandard labels of fulfillment status as well as spans of text marked as evidence.",3 DesireDB Corpus,[0],[0]
"Our corpus is a subset of the Spinn3r corpus (Burton et al., 2011, 2009), consisting of firstperson narratives from six personal blog domains: livejournal.com, wordpress.com, blogspot.com, spaces.live.com, typepad.com, travelpod.com.",3.1 Identifying Desires and Goals,[0],[0]
"To create our dataset, we select only desire expressions involving some version of the first-person.",3.1 Identifying Desires and Goals,[0],[0]
"In first-person narratives, the narrator and protagonist naturally align which makes it much easier to identify and track the protagonist than in fiction or historical genre.",3.1 Identifying Desires and Goals,[0],[0]
"Thus, selecting narrative passages with expressions of desire relating to the first-person are very likely to discuss subsequent behaviors to achieve that desire and the end result.",3.1 Identifying Desires and Goals,[0],[0]
"Put simply, zooming in on first-person desires means that desire and its aftermath are more likely to be highly topical for the narrative.",3.1 Identifying Desires and Goals,[0],[0]
"This corpus, then, is highly suitable as a testbed for modeling human desires and their fulfillment.
",3.1 Identifying Desires and Goals,[0],[0]
"Human desires and goals can be expressed linguistically in many different ways, including both explicit verbal and nominal markers of desire or necessity (e.g., want, hope) and more general markers of urges (e.g., craving, hunger, thirst).",3.1 Identifying Desires and Goals,[0],[0]
"To systematically discover predicates that specify desires, we browsed FrameNet 1.7 (Baker et al., 1998) selecting frames that seemed likely to contain lexical units specifying desires: Beingnecessary, Desiring, Have-as-a-demand, Needing, Offer, Purpose, Request, Required-event, Scheduling, Seeking, Seeking-to-achieve, Stimulus-focus, Stimulate-emotion, and Worry.",3.1 Identifying Desires and Goals,[0],[0]
"We then selected 100 representative instances of that frame in English Gigaword (Parker et al., 2011) by first selecting the 10 most frequent lexical units in that frame, and then selecting 10 random instances per lexical unit.",3.1 Identifying Desires and Goals,[0],[0]
"One of the authors examined each set of 100 instances, estimating for each sentence whether the predicate specifies a goal that the surrounding text picks up on.",3.1 Identifying Desires and Goals,[0],[0]
"Because we were looking for predicates that reliably specify desires that motivate a protagonist’s actions, we eliminated frames where less than 80% of the sentences showed this characteristic.
",3.1 Identifying Desires and Goals,[0],[0]
"This resulted in a downsample to the following four frames: Desiring, Needing, Purpose, and Request.",3.1 Identifying Desires and Goals,[0],[0]
We selected only the verbal lexical units because we found that verbs were more likely to introduce goals than nouns or adjectives.,3.1 Identifying Desires and Goals,[0],[0]
"We examined 100 instances for each verbal lex-
ical unit, discarding as before.",3.1 Identifying Desires and Goals,[0],[0]
This resulted in 37 verbs.,3.1 Identifying Desires and Goals,[0],[0]
"For each verb, we systematically constructed and coded all past forms of the verb (e.g., was [verb]ing, had [verb]ed, had been [verb]ing, [verb]ed, didn’t",3.1 Identifying Desires and Goals,[0],[0]
"[verb], etc.)",3.1 Identifying Desires and Goals,[0],[0]
"because we posited that morphological form itself may convey likelihood of fulfillment (e.g., a past perfect I had wanted to ... signals that something changed, either the desire or fulfillment).",3.1 Identifying Desires and Goals,[0],[0]
"We initially experimented with both past and (historical) present, but past tense verb patterns resulted in much higher precision.",3.1 Identifying Desires and Goals,[0],[0]
"We counted the instances of these patterns in our dataset, and retained only those lemmas with at least 1000 instances across the corpus.
",3.1 Identifying Desires and Goals,[0],[0]
"We extract stories containing the verbal patterns of desire, with five sentences before and after the desire expression sentence as context (See Fig. 2).",3.1 Identifying Desires and Goals,[0],[0]
Our annotation results provide support that the evidence of desire fulfillment can be expressed before the desire statement.,3.1 Identifying Desires and Goals,[0],[0]
We also study the effect of prior and post context in understanding desire fulfillment in our experiments (Section 4) and show that using the narrative context preceding the desire statement improves the results.,3.1 Identifying Desires and Goals,[0],[0]
"We extracted ∼600K desire expressions with their context, and then sample 3,680 instances for annotation.",3.2 Data Annotation,[0],[0]
"This subset consists of 16 verbal patterns (when collapsing all morphological forms to their
head word).",3.2 Data Annotation,[0],[0]
A group of pre-qualified Mechanical Turkers then labelled each instance.,3.2 Data Annotation,[0],[0]
"The annotators labelled the fulfillment status of the desire expression sentence based on the prior and post context, by choosing from three labels: Fulfilled, Unfulfilled, and Unknown from the context.",3.2 Data Annotation,[0],[0]
They were also asked to mark the evidence for the label they had chosen by specifying a span of text in the narrative.,3.2 Data Annotation,[0],[0]
"For each data instance, we asked the Turkers to mark the subject of the desire expression and determine if the expressed desire is hypothetical (e.g., a conditional sentence) or not.
",3.2 Data Annotation,[0],[0]
The annotators were selected from a list of prequalified workers who had successfully passed a test on a textual entailment task with 100% correct answers.,3.2 Data Annotation,[0],[0]
They were provided with detailed instructions and examples as to how to label the desires and mark the evidence.,3.2 Data Annotation,[0],[0]
We also specified the desire expression verbal pattern using square brackets (as shown in Fig. 1 and 2) for more clarity.,3.2 Data Annotation,[0],[0]
Three annotators were assigned to work on each data instance.,3.2 Data Annotation,[0],[0]
"To generate the gold-standard labels we used majority vote and the cases with no agreement were labeled as ‘None’.
",3.2 Data Annotation,[0],[0]
"Table 1 reports the distribution of data and goldstandard labels (Ful:Fulfilled, Unf:Unfulfilled, Unk:",3.2 Data Annotation,[0],[0]
Unknown from the context).,3.2 Data Annotation,[0],[0]
About half of the desire expressions (53%) were labeled Fulfilled and about one third (31%) were labeled Unfulfilled.,3.2 Data Annotation,[0],[0]
"The annotators didn’t agree on about 2% of the instances, that were labeled None.",3.2 Data Annotation,[0],[0]
"As Tabel 1 shows, the distribution of labels is not uniform across different verbal patterns.",3.2 Data Annotation,[0],[0]
"For in-
stance, decided to and couldn’t wait are highly skewed towards Fulfilled as opposed to hoped to which includes 68% Unfulfilled instances.",3.2 Data Annotation,[0],[0]
"Some patterns seem to be harder to annotate, like wished to, which has the highest rate of Unknown (30%) and None (8%) among all.
",3.2 Data Annotation,[0],[0]
"Other than fulfillment status, for each data instance in our corpus we include the agreementscore which is the number of annotators that agreed on the assigned label.",3.2 Data Annotation,[0],[0]
"In addition, we provide the evidence as a part of the DesireDB data, by merging the text spans marked by the annotators as evidence.",3.2 Data Annotation,[0],[0]
"We compared the evidence spans pairwise to measure the overlap-score, indicating the number of pairs of annotators with overlapping responses.",3.2 Data Annotation,[0],[0]
An example is shown in Figure 3.,3.2 Data Annotation,[0],[0]
"The first part is the extracted data including the desire expression with prior and post context, and the second part is the gold-standard annotations.
",3.2 Data Annotation,[0],[0]
"To assess inter-annotator agreement for Fulfillment, we calculated Krippendorff-alpha Kappa (Krippendorff, 1970, 2004) for pairwise inter-annotator reliability, and, the average of Kappa between each annotator and the majority vote.",3.2 Data Annotation,[0],[0]
These two metrics are 0.63 and 0.88 respectively.,3.2 Data Annotation,[0],[0]
"Overall, 66% of the data was labeled with total agreement (where all three annotators agreed on the same label) and about 32% of data was labeled by two agreements and one disagreement.",3.2 Data Annotation,[0],[0]
We also examined the agreements across each label separately.,3.2 Data Annotation,[0],[0]
"For Fulfilled class, total agreement rate is 75%, which for Unfulfilled is 67%, and on Unknown from the context is 41%.",3.2 Data Annotation,[0],[0]
We believe this indicates that annotating unfulfilled desires was harder than fulfilled cases.,3.2 Data Annotation,[0],[0]
"For evidence marking, in 79% of the data all three annotators marked overlapping spans.",3.2 Data Annotation,[0],[0]
"We conducted a range of experiments on predicting fulfillment status of desires and goals, using different features and models, including LSTM architectures that can encode the sequential structure of the narratives.",4 Modeling Desire Fulfillment,[0],[0]
We first describe our features and models.,4 Modeling Desire Fulfillment,[0],[0]
"Then, we present our feature analysis study to examine their importance in modeling fulfillment.",4 Modeling Desire Fulfillment,[0],[0]
Finally we provide results of direct comparison to previous work on the existing corpora.,4 Modeling Desire Fulfillment,[0],[0]
"In our original informal examination of the DesireDB development data, we noticed several ways that a writer can signal (lack of) fulfillment of a desire like “I hoped to pick up a dictionary”.",4.1 Features Description,[0],[0]
"First, they may mention an outcome that entails (“The book I bought was...”) or strongly implies fulfillment (“I went back home happily.”).",4.1 Features Description,[0],[0]
"However, we noticed that in many cases of fulfillment, the ‘marker’ was simply the absence of any mention that things went wrong.",4.1 Features Description,[0],[0]
"For lack of fulfillment, while we found cases where writers explicitly state that their desire wasn’t met, we noted many instances where evidence came from mentioning that an enabling condition for fulfillment wasn’t met (“The bookstore was closed.”).
",4.1 Features Description,[0],[0]
"True machine understanding of these kinds of narrative structures requires robust models of the complex interplay of semantics (including negation) as well as world knowledge about the scripts for tasks like buying books, including what count as enabling conditions and entailers for fulfillment.",4.1 Features Description,[0],[0]
"While we hope to explore more articulated models in the future, for our experiments we considered reasonable proxies for the conditions mentioned above using existing resources (note that we also tested LSTM models described below, which may implicitly learn such relationships with sufficient data).",4.1 Features Description,[0],[0]
"One set (Desire Features) indexes properties of the desire expression (e.g., the desire verb) as well as overlap between the desired object/event and the surrounding context.",4.1 Features Description,[0],[0]
"The remaining features attempt to find general markers
for success or failure.",4.1 Features Description,[0],[0]
"One set (Discourse Features) looks for overt discourse relation markers that signal violation of expectation (e.g., ‘but’, ‘however’) or its opposite (e.g., ‘so’).",4.1 Features Description,[0],[0]
"Another uses the Connotation Lexicon (Feng et al., 2013) to model whether the context provides a positive or negative event.",4.1 Features Description,[0],[0]
All of these features are inspired by Chaturvedi et al. (2016).,4.1 Features Description,[0],[0]
"Finally, motivated by the AESOP modeling of affect states for identifying plot units (Goyal and Riloff, 2013), one set of features (Sentiment-Flow-Features) indexes whether there has been a change in sentiment in the surrounding context (which might be the mention of a thwarted effort or a hard won victory).",4.1 Features Description,[0],[0]
"Figure 4 provides an example of this.
",4.1 Features Description,[0],[0]
"In addition to a BOW (Bag of Words) baseline, we extracted the four types of features mentioned above.",4.1 Features Description,[0],[0]
"For features that examine the context around the desire expression, our experiments used the pre-context, the post-context, or both, as discussed below; context features are computed per sentence i of the context.",4.1 Features Description,[0],[0]
We also tested various ablations of these features described below as well.,4.1 Features Description,[0],[0]
"We now describe the full set of features in more detail.
",4.1 Features Description,[0],[0]
Desire-Features.,4.1 Features Description,[0],[0]
"From a desire expression of the form ‘X Ved S’, we extract the lexical feature",4.1 Features Description,[0],[0]
"Desire-Verb, the lemma for V. We also extract a list of focal words, the content words in embedded sentence S.",4.1 Features Description,[0],[0]
"In Figure 4, these are ‘do’, ‘go’, and ‘run’.",4.1 Features Description,[0],[0]
"The features Focal{Word,Synonym,Antonym}-Mention-i counts how many times each word, its synonyms, or its antonyms in WordNet (Fellbaum, 1998) are in the context, respectively.",4.1 Features Description,[0],[0]
"Similarly, Desire-SubjectMention-i marks if subject X is mentioned in the context.",4.1 Features Description,[0],[0]
"Finally, boolean First-Person-Subject indicates if X is first person (‘I’, ‘we’).
",4.1 Features Description,[0],[0]
Discourse-Features.,4.1 Features Description,[0],[0]
This class of features count how many of two classes of discourse relation markers (Violated-Expectation–i vs. MeetingExpectation–i) occur in the context.,4.1 Features Description,[0],[0]
"For the classes, we manually coded all overt discourse relation markers in the Penn Discourse",4.1 Features Description,[0],[0]
"Treebank three ways(violation, meeting, or neutral), leading to 15 meeting markers (‘accordingly’, ‘so’, ‘ultimately’, ‘finally’) and 31 violating (‘although’, ‘rather’, ‘yet’, ‘but’).",4.1 Features Description,[0],[0]
"In addition, we also tracked the presence of the most frequent of these (‘so’ and ‘but’, respectively) in the desire sentence itself by the booleans So-Present and But-Present.",4.1 Features Description,[0],[0]
Connotation-Features.,"1,366 953 380 70 2,780",[0],[0]
"Beyond the use of WordNet expansion for Focal-Word-Mention-i, we also used the Connotation Lexicon (Feng et al., 2013), a lexical resource marking very general connotation polarities (positive or negative) of words (as opposed to more specific sentiment lexicons).","1,366 953 380 70 2,780",[0],[0]
Connotation-Agree-i counts for each word w in focal words the number of words in the context that have the same connotation polarity as w. Connotation-Disgree-i is defined similarly.,"1,366 953 380 70 2,780",[0],[0]
Sentiment-Flow-Features.,"1,366 953 380 70 2,780",[0],[0]
"To model affect states, we compute a sentiment score for the desire expression sentence as well as each sentence in the context.","1,366 953 380 70 2,780",[0],[0]
"Then for each sentence of the context, the booleans Sentiment-Agree-i and SentimentDisagree-i mark whether that sentence and the desire expression sentence have the same sentiment polarity (see Figure 4).","1,366 953 380 70 2,780",[0],[0]
"While there is evidence suggesting that models of implicit sentiment (e.g., (Goyal et al., 2010; Reed et al., 2017)) could do much better at tracking affect states, here we use the Stanford Sentiment system (Socher et al., 2013).","1,366 953 380 70 2,780",[0],[0]
Our features are motivated by narrative characteristics but do not directly capture the sequential structure of the narratives.,4.2 LSTM Models,[0],[0]
"We thus apply neural network models suitable for sequence learning, in order to directly encode the order of the sentences in the story and distinguish between prior and post context.",4.2 LSTM Models,[0],[0]
"We use two different architectures of LSTM (Long Short-Term Memory) (Hochreiter and Schmidhuber, 1997) models to generate sentence embeddings and then apply a three-layer RNN (Recurrent Neural Network) for classification.",4.2 LSTM Models,[0],[0]
"We used Keras (Chollet, 2015) as a deep learning toolkit for implementing our experiments.",4.2 LSTM Models,[0],[0]
Skip-Thoughts.,4.2 LSTM Models,[0],[0]
"This is a sequential model that uses pre-trained skip-thoughts model (Kiros et al., 2015) as the embedding of sentences.",4.2 LSTM Models,[0],[0]
"It first concatenates features, if any, with embeddings, and then uses LSTM to generate a single representation for the context sequence, which is the output of the last unit.",4.2 LSTM Models,[0],[0]
"That single representation is then
concatenated with embedding-feature concatenation of desire sentence and is fed into a multi-layer network to yield a single binary output.",4.2 LSTM Models,[0],[0]
CNN-RNN.,4.2 LSTM Models,[0],[0]
"The only difference between the CNN-RNN model and Skip-Thought is that it uses the 1-dimensional convolution with maxover-time pooling introduced in (Kim, 2014) to generate the sentence embedding from word embedding, instead of using skip-thoughts.",4.2 LSTM Models,[0],[0]
"We use Google News Vectors (Mikolov et al., 2013) for the word embedding with different sizes from 1 to 7 for the kernel.
",4.2 LSTM Models,[0],[0]
"For our experiments, we first constructed a subset of DesireDB that we will call SimpleDesireDB, in order to be able to compare more directly to the models and data used in previous work.",4.2 LSTM Models,[0],[0]
"Chaturvedi et al. (2016) used three verb phrases to identify desire expressions (wanted to, hoped to, and wished to), so we selected a portion of our corpus including these patterns along with two other expressions (couldn’t wait to and decided to) to have sufficient data for experiments.",4.2 LSTM Models,[0],[0]
Table 2 shows the distribution of labels in this subset.,4.2 LSTM Models,[0],[0]
"For classification experiments we use data labeled as Fulfilled and Unfulfilled, thus the majority class accuracy is 59%.",4.2 LSTM Models,[0],[0]
"We split the data into Train (1,656), Dev (327), and Test (336) sets for the experiments.
",4.2 LSTM Models,[0],[0]
"Results of our two LSTM models for Fulfilled (Ful) and Unfulfilled (Unf) classes and the overall classification task (P:precision, R:recall) on Simple-DesireDB are presented in Table 3.",4.2 LSTM Models,[0],[0]
ALL feature set includes all the features described in Sec. 4.1 (without BOW).,4.2 LSTM Models,[0],[0]
"The results indicate that our features can considerably improve the model, compared to the BOW baseline (F1 improved from
0.65 to 0.70 for Skip-Thought).",4.2 LSTM Models,[0],[0]
"We also conducted 4 sets of experiments to study the importance of prior, post and the whole context in predicting fulfillment status, using our best model.",4.2 LSTM Models,[0],[0]
The results of Skip-Thought using different contextual representations are in Table 4 with ALL features.,4.2 LSTM Models,[0],[0]
The results indicate that adding features from prior context alone improves the results.,4.2 LSTM Models,[0],[0]
"The best results are obtained by including the whole context and desire sentence.
",4.2 LSTM Models,[0],[0]
We then experimented with our best model on all of DesireDB.,4.2 LSTM Models,[0],[0]
"We also trained Naive Bayes, SVM and Logistic Regression (LR) classifiers as baselines, with the best results on the Dev set achieved by Logistic Regression.",4.2 LSTM Models,[0],[0]
Table 5 shows the results of Skip-Thought and LR on DesireDB for different features on the test set.,4.2 LSTM Models,[0],[0]
"Our feature ablation study on the Dev set, discussed in Sec. 4.3, indicates that Discourse features are better predictors of fulfillment status, so we present results using only Discourse features in addition to BOW and ALL.
",4.2 LSTM Models,[0],[0]
All of the results indicate that similar features and methods achieve better results for the Fulfilled class as compared to Unfulfilled.,4.2 LSTM Models,[0],[0]
"We believe the reason is that identifying unfulfillment of a desire or goal is a more difficult task, as discussed in the annotation description in Section 3.2.",4.2 LSTM Models,[0],[0]
"To further our analysis on the annotation disagreements, we examined the cases where only two annotators agreed on the assigned label.",4.2 LSTM Models,[0],[0]
"From the expressions labeled Fulfilled by two annotators, 64% were labeled Unknown from the context by the disagreeing annotator, and only 36% were labeled Unfulfilled.",4.2 LSTM Models,[0],[0]
"However, these numbers for the Unfulfilled class are respectively 49% and 51%, indicating a
stronger disagreement between annotators when labeling Unfulfilled expressions.",4.2 LSTM Models,[0],[0]
We used the InfoGain measure to rank features based on their importance in modeling desire fulfillment.,4.3 Feature Selection Experiments,[0],[0]
"The top 5 features are: But-Present, Post-Context-Connotation-Disagree, Post-Context-Violated-Expectation, Desire-Verb, Is-First-Person.",4.3 Feature Selection Experiments,[0],[0]
We also tested different feature sets separately.,4.3 Feature Selection Experiments,[0],[0]
"We describe our experiment results below.
",4.3 Feature Selection Experiments,[0],[0]
The results of the feature ablation experiments using LR model are shown in Table 6.,4.3 Feature Selection Experiments,[0],[0]
The ALL feature set includes all the features described in Sec. 4.1 (without BOW).,4.3 Feature Selection Experiments,[0],[0]
We obtained high precision and F-measure using the Discourse features.,4.3 Feature Selection Experiments,[0],[0]
"We also experimented with our top feature from the InfoGain analysis, But-Present, which surprisingly achieves a high F-measure, compared to using ALL and Discourse feature sets.",4.3 Feature Selection Experiments,[0],[0]
The last row of Table 6 shows the results of using ALL features excluding But-Present.,4.3 Feature Selection Experiments,[0],[0]
This indicates that features motivated by narrative structure are primarily driving improvement.,4.3 Feature Selection Experiments,[0],[0]
"In previous work Chaturvedi et al. (2016) show that a model representing narrative structure could beat the BOW baseline, but they performed no systematic feature ablation.",4.3 Feature Selection Experiments,[0],[0]
"Our results suggest that ultimately, the presence of “but” is likely a central driver for their improvements as well.",4.3 Feature Selection Experiments,[0],[0]
"We directly compare our methods and features to the most relevant previous work (Chaturvedi et al., 2016).",4.4 Comparison to Previous Work,[0],[0]
They applied their models on two datasets and reported the results for the Fulfilled class.,4.4 Comparison to Previous Work,[0],[0]
"We present the same metrics in Table 7, using our best model Skip-Thought (SkipTh).",4.4 Comparison to Previous Work,[0],[0]
"We also present results of our LR model with our Discourse features, Discourse-LR, trained and tested on their corpora to compare to their features.",4.4 Comparison to Previous Work,[0],[0]
The first three rows show the results from Chaturvedi et al. (2016) for comparison.,4.4 Comparison to Previous Work,[0],[0]
"As described in Sec. 2, they used BOW as baseline, LSNM is their best model, and Unstruct-LR is their unstructured model that uses all of their features with LR.
",4.4 Comparison to Previous Work,[0],[0]
"On both corpora, Discourse-LR outperforms Unstruct-LR, showing that the Discourse features are stronger indicators of the desire fulfillment status when used with LR classifier.",4.4 Comparison to Previous Work,[0],[0]
"In addition, on SimpleWiki, LR-Discourse outperforms their structured model, LSNM (0.46 vs. 0.27 on F-1).",4.4 Comparison to Previous Work,[0],[0]
"We created a novel dataset, DesireDB, for studying the expression of desires and their fulfillment in narrative discourse.",5 Conclusion and Future Work,[0],[0]
"We show that contextual
features help with classification, and that both prior and post context are useful.",5 Conclusion and Future Work,[0],[0]
"Finally, we show that exploiting narrative structure is helpful, both directly in terms of the utility of discourse relation features and indirectly via the superior performance of a Skip-Thought LSTM model.
",5 Conclusion and Future Work,[0],[0]
"In future work, we plan to explore richer features and models for semantic and discourse-based features, as well as the utility of more narrativelyaware features.",5 Conclusion and Future Work,[0],[0]
"For instance, the sentiment flow features roughly track the notion that the arc of a narrative may implicitly reveal resolution of a goal via changes in affect states.",5 Conclusion and Future Work,[0],[0]
"We hope to examine whether there are other similar rough-grained measures of change over the entire narrative that can improve the results.
",5 Conclusion and Future Work,[0],[0]
DesireDB contains annotator-labeled spans for evidence for the annotator’s conclusions.,5 Conclusion and Future Work,[0],[0]
"While we have not used this labeling, we plan to use it in future work.",5 Conclusion and Future Work,[0],[0]
"Finally, we hope to turn to automatically detecting instances of desire expressions that give rise to the kind of goal-oriented narratives DesireDB contains.",5 Conclusion and Future Work,[0],[0]
"Here we have used highprecision search patterns but our annotations show that such patterns still admitted 134 hypothetical desires (e.g., ‘If I had wanted to buy a book’).",5 Conclusion and Future Work,[0],[0]
It would appear that distinguishing hypothetical vs. real desires itself could be an interesting problem.,5 Conclusion and Future Work,[0],[0]
"This research was supported by Nuance Foundation Grant SC-14-74, NSF Grant IIS-1302668-002 and IIS-1321102.",Acknowledgments,[0],[0]
"Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives.",abstractText,[0],[0]
There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes.,abstractText,[0],[0]
"However, to date, there has been limited work on computational models for this problem.",abstractText,[0],[0]
"We introduce a new dataset, DesireDB, which includes goldstandard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context.",abstractText,[0],[0]
"We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.",abstractText,[0],[0]
Modelling Protagonist Goals and Desires in First-Person Narrative,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1128–1137 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1128
We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate highquality abstractive conversation responses in accordance with designated emotions.",text,[0],[0]
A critical research problem for artificial intelligence is to design intelligent agents that can perceive and generate human emotions.,1 Introduction,[0],[0]
"In the past decade, there has been significant progress in sentiment analysis (Pang et al., 2002, 2008; Liu, 2012) and natural language understanding—e.g., classifying the sentiment of online reviews.",1 Introduction,[0],[0]
"To build empathetic conversational agents, machines must also have the ability to learn to generate emotional sentences.
",1 Introduction,[0],[0]
"One of the major challenges is the lack of largescale, manually labeled emotional text datasets.",1 Introduction,[0],[0]
"Due to the cost and complexity of manual annotation, most prior research studies primarily focus on small-sized labeled datasets (Pang et al., 2002; Maas et al., 2011; Socher et al., 2013), which are not ideal for training deep learning models with a large number of parameters.
",1 Introduction,[0],[0]
"In recent years, a handful of medium to large scale, emotional corpora in the area of emotion analysis (Go et al., 2016) and dialog (Li et al., 2017b) are proposed.",1 Introduction,[0],[0]
"However, all of them are limited to a traditional, small set of labels, for example, “happiness,” “sadness,” “anger,” etc. or simply binary “positive” and “negative.”",1 Introduction,[0],[0]
"Such coarse-grained classification labels make it difficult to capture the nuances of human emotion.
",1 Introduction,[0],[0]
"To avoid the cost of human annotation, we propose the use of naturally-occurring emoji-rich Twitter data.",1 Introduction,[0],[0]
We construct a dataset using Twitter conversations with emojis in the response.,1 Introduction,[0],[0]
"The fine-grained emojis chosen by the users in the response can be seen as the natural label for the emotion of the response.
",1 Introduction,[0],[0]
We assume that the emotions and nuances of emojis are established through the extensive usage by Twitter users.,1 Introduction,[0],[0]
"If we can create agents that
are able to imitate Twitter users’ language style when using those emojis, we claim that, to some extent, we have captured those emotions.",1 Introduction,[0],[0]
"Using a large collection of Twitter conversations, we then trained a conditional generative model to automatically generate the emotional responses.",1 Introduction,[0],[0]
"Figure 1 shows an example.
",1 Introduction,[0],[0]
"To generate emotional responses in dialogs, another technical challenge is to control the target emotion labels.",1 Introduction,[0],[0]
"In contrast to existing work (Huang et al., 2017) that uses information retrieval to generate emotional responses, the research question we are pursuing in this paper, is to design novel techniques that can generate abstractive responses of any given arbitrary emotions, without having human annotators to label a huge amount of training data.
",1 Introduction,[0],[0]
"To control the target emotion of the response, we investigate several encoder-decoder generation models, including a standard attention-based SEQ2SEQ model as the base model, and a more sophisticated CVAE model (Kingma and Welling, 2013; Sohn et al., 2015), as VAE is recently found convenient in dialog generation (Zhao et al., 2017).
",1 Introduction,[0],[0]
"To explicitly improve emotion expression, we then experiment with several extensions to the CVAE model, including a hybrid objective with policy gradient.",1 Introduction,[0],[0]
"The performance in emotion expression is automatically evaluated by a separate sentence-to-emoji classifier (Felbo et al., 2017).",1 Introduction,[0],[0]
"Additionally, we conducted a human evaluation to assess the quality of the generated emotional text.
",1 Introduction,[0],[0]
Results suggest that our method is capable of generating state-of-the-art emotional text at scale.,1 Introduction,[0],[0]
"Our main contributions are three-fold:
• We provide a publicly available, large-scale dataset of Twitter conversation-pairs naturally labeled with fine-grained emojis.
",1 Introduction,[0],[0]
"• We are the first to use naturally labeled emojis for conducting large-scale emotional response generation for dialog.
",1 Introduction,[0],[0]
"• We apply several state-of-the-art generative models to train an emotional response generation system, and analysis confirms that our models deliver strong performance.
",1 Introduction,[0],[0]
"In the next section, we outline related work on sentiment analysis and emoji on Twitter data, as well as neural generative models.",1 Introduction,[0],[0]
"Then, we will
introduce our new emotional research dataset and formalize the task.",1 Introduction,[0],[0]
"Next, we will describe the neural models we applied for the task.",1 Introduction,[0],[0]
"Finally, we will show automatic evaluation and human evaluation results, and some generated examples.",1 Introduction,[0],[0]
Experiment details can be found in supplementary materials.,1 Introduction,[0],[0]
"In natural language processing, sentiment analysis (Pang et al., 2002) is an area that involves designing algorithms for understanding emotional text.",2 Related Work,[0],[0]
Our work is aligned with some recent studies on using emoji-rich Twitter data for sentiment classification.,2 Related Work,[0],[0]
"Eisner et al. (2016) proposes a method for training emoji embedding EMOJI2VEC, and combined with word2vec (Mikolov et al., 2013), they apply the embeddings for sentiment classification.",2 Related Work,[0],[0]
"DeepMoji (Felbo et al., 2017) is closely related to our study: It makes use of a large, naturally labeled Twitter emoji dataset, and train an attentive bi-directional long short-term memory network (Hochreiter and Schmidhuber, 1997) model for sentiment analysis.",2 Related Work,[0],[0]
"Instead of building a sentiment classifier, our work focuses on generating emotional responses, given the context and the target emoji.
",2 Related Work,[0],[0]
"Our work is also in line with the recent progress of the application of Variational Autoencoder (VAE) (Kingma and Welling, 2013) in dialog generation.",2 Related Work,[0],[0]
"VAE (Kingma and Welling, 2013) encodes data in a probability distribution, and then samples from the distribution to generate examples.",2 Related Work,[0],[0]
"However, the original frameworks do not support end-to-end generation.",2 Related Work,[0],[0]
"Conditional VAE (CVAE) (Sohn et al., 2015; Larsen et al., 2015) was proposed to incorporate conditioning option in the generative process.",2 Related Work,[0],[0]
"Recent research in dialog generation shows that language generated by VAE models enjoy significantly greater diversity than traditional SEQ2SEQ models (Zhao et al., 2017), which is a preferable property toward building a true-to-life dialog agents.
",2 Related Work,[0],[0]
"In dialog research, our work aligns with recent advances in sequence-to-sequence models (Sutskever et al., 2014) using long shortterm memory networks (Hochreiter and Schmidhuber, 1997).",2 Related Work,[0],[0]
A slightly altered version of this model serves as our base model.,2 Related Work,[0],[0]
Our modification enabled it to condition on single emojis.,2 Related Work,[0],[0]
"Li
et al. (2016) use a reinforcement learning algorithm to improve the vanilla sequence-to-sequence model for non-task-oriented dialog systems, but their reinforced and its follow-up adversarial models (Li et al., 2017a) also do not model emotions or conditional labels.",2 Related Work,[0],[0]
"Zhao et al. (2017) recently introduced conditional VAE for dialog modeling, but neither did they model emotions in the conversations, nor explore reinforcement learning to improve results.",2 Related Work,[0],[0]
"Given a dialog history, Xie et.",2 Related Work,[0],[0]
al.’s work recommends suitable emojis for current conversation.,2 Related Work,[0],[0]
Xie et.,2 Related Work,[0],[0]
"al. (2016)compress the dialog history to vector representation through a hierarchical RNN and then map it to a emoji by a classifier, while in our model, the representation for original tweet, combined with the emoji embedding, is used to generate a response.",2 Related Work,[0],[0]
We start by describing our dataset and approaches to collecting and processing the data.,3 Dataset,[0],[0]
"Social media is a natural source of conversations, and people use emojis extensively within their posts.",3 Dataset,[0],[0]
"However, not all emojis are used to express emotion and frequency of emojis are unevenly distributed.",3 Dataset,[0],[0]
"Inspired by DeepMoji (Felbo et al., 2017), we use 64 common emojis as labels (see Table 1), and collect a large corpus of Twitter conversations con-
taining those emojis.",3 Dataset,[0],[0]
Note that emojis with the difference only in skin tone are considered the same emoji.,3 Dataset,[0],[0]
"We crawled conversation pairs consisting of an original post and a response on Twitter from 12th to 14th of August, 2017.",3.1 Data Collection,[0],[0]
The response to a conversation must include at least one of the 64 emoji labels.,3.1 Data Collection,[0],[0]
"Due to the limit of Twitter streaming API, tweets are filtered on the basis of words.",3.1 Data Collection,[0],[0]
"In our case, a tweet can be reached only if at least one of the 64 emojis is used as a word, meaning it has to be a single character separated by blank space.",3.1 Data Collection,[0],[0]
"However, this kind of tweets is arguably cleaner, as it is often the case that this emoji is used to wrap up the whole post and clusters of repeated emojis are less likely to appear in such tweets.
",3.1 Data Collection,[0],[0]
"For both original tweets and responses, only English tweets without multimedia contents (such as URL, image or video) are allowed, since we assume that those contents are as important as the text itself for the machine to understand the conversation.",3.1 Data Collection,[0],[0]
"If a tweet contains less than three alphabetical words, the conversation is not included in the dataset.",3.1 Data Collection,[0],[0]
Then we label responses with emojis.,3.2 Emoji Labeling,[0],[0]
"If there are multiple types of emoji in a response, we use the emoji with most occurrences inside the response.",3.2 Emoji Labeling,[0],[0]
"Among those emojis with same occurrences, we choose the least frequent one across the whole corpus, on the hypothesis that less frequent tokens better represent what the user wants to express.",3.2 Emoji Labeling,[0],[0]
See Figure 2 for example.,3.2 Emoji Labeling,[0],[0]
"During preprocessing, all mentions and hashtags are removed, and punctuation1 and emojis are separated if they are adjacent to words.",3.3 Data Preprocessing,[0],[0]
"Words with digits are all treated as the same special token.
",3.3 Data Preprocessing,[0],[0]
"In some cases, users use emojis and symbols in a cluster to express emotion extensively.",3.3 Data Preprocessing,[0],[0]
"To normalize the data, words with more than two repeated letters, symbol strings of more than one repeated punctuation symbols or emojis are shortened, for example, ‘!!!!’",3.3 Data Preprocessing,[0],[0]
"is shortened to ‘!’, and ‘yessss’ to ‘yess’.",3.3 Data Preprocessing,[0],[0]
Note that we do not reduce duplicate letters completely and convert the word to the ‘correct’ spelling (‘yes’ in the example) since the length of repeated letters represents the intensity of emotion.,3.3 Data Preprocessing,[0],[0]
"By distinguishing ‘yess’ from ‘yes’, the emotional intensity is partially preserved in our dataset.
",3.3 Data Preprocessing,[0],[0]
"Then all symbols, emojis, and words are tokenized.",3.3 Data Preprocessing,[0],[0]
"Finally, we build a vocabulary of size 20K according to token frequency.",3.3 Data Preprocessing,[0],[0]
"Any tokens outside the vocabulary are replaced by the same special token.
",3.3 Data Preprocessing,[0],[0]
"We randomly split the corpus into 596,959 /32,600/32,600",3.3 Data Preprocessing,[0],[0]
conversation pairs for train /validation/test set2.,3.3 Data Preprocessing,[0],[0]
Distribution of emoji labels within the corpus is presented in Table 1.,3.3 Data Preprocessing,[0],[0]
"In this work, our goal is to generate emotional responses to tweets with the emotion specified by an emoji label.",4 Generative Models,[0],[0]
We assembled several generative models and trained them on our dataset.,4 Generative Models,[0],[0]
"Traditional studies use deep recurrent architecture and encoder-decoder models to generate conversation responses, mapping original texts to target responses.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Here we use a sequence-to-sequence (SEQ2SEQ) model (Sutskever et al., 2014) with global attention mechanism (Luong et al., 2015) as our base model (See Figure 3).
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
We use randomly initialized embedding vectors to represent each word.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"To specifically model the
1Emoticons (e.g. ‘:)’, ‘(-:’) are made of mostly punctuation marks.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
They are not examined in this paper.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Common emoticons are treated as words during preprocessing.
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
2We will release the dataset with all tweets in its original form before preprocessing.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"To comply with Twitter’s policy, we will include the tweet IDs in our release, and provide a script for downloading the tweets using the official API.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"No information of the tweet posters is collected.
",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"emotion, we compute the embedding vector of the emoji label the same way as word embeddings.",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
The emoji embedding is further reduced to smaller size vector ve through a dense layer.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"We pass the embeddings of original tweets through a bidirectional RNN encoder of GRU cells (Schuster and Paliwal, 1997; Chung et al., 2014).",4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
The encoder outputs a vector vo that represents the original tweet.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
Then vo and ve are concatenated and fed to a 1-layer RNN decoder of GRU cells.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
A response is then generated from the decoder.,4.1 Base: Attention-Based Sequence-to-Sequence Model,[0],[0]
"Having similar encoder-decoder structures, SEQ2SEQ can be easily extended to a Conditional Variational Autoencoder (CVAE) (Sohn et al., 2015).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Figure 3 illustrates the model: response encoder, recognition network, and prior network
are added on top of the SEQ2SEQ model.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Response encoder has the same structure to original tweet encoder, but it has separate parameters.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"We use embeddings to represent Twitter responses and pass them through response encoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Mathematically, CVAE is trained by maximizing a variational lower bound on the conditional likelihood of x given c, according to:
p(x|c) = ∫ p(x|z, c)p(z|c)dz (1)
z, c and x are random variables.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
z is the latent variable.,4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In our case, the condition c =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"[vo; ve], target x represents the response.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Decoder is used to approximate p(x|z, c), denoted as pD(x|z, c).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Prior network is introduced to approximate p(z|c), denoted as pP (z|c).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Recognition network qR(z|x, c) is introduced to approximate true posterior p(z|x, c) and will be absent during generation phase.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"By assuming that the latent variable has a multivariate Gaussian distribution with a diagonal covariance matrix, the lower bound to log p(x|c) can then be written by:
−L(θD, θP , θR;x, c) =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"KL(qR(z|x, c)||pP (z|c)) −EqR(z|x,c)(log pD(x|z, c))
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"(2)
θD, θP , θR are parameters of those networks.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In recognition/prior network, we first pass the variables through an MLP to get the mean and log variance of z’s distribution.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Then we run a reparameterization trick (Kingma and Welling, 2013) to sample latent variables.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"During training, z by the recognition network is passed to the decoder and trained to approximate z′ by the prior network.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"While during testing, the target response is absent, and z′ by the prior network is passed to the decoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Our CVAE inherits the same attention mechanism from the base model connecting the original tweet encoder to the decoder, which makes our model deviate from previous works of CVAE on text data.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Based on the attention memory as well as c and z, a response is finally generated from the decoder.
",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"When handling text data, the VAE models that apply recurrent neural networks as the structure of their encoders/decoders may first learn to ignore the latent variable, and explain the data with the more easily optimized decoder.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"The latent
variables lose its functionality, and the VAE deteriorates to a plain SEQ2SEQ model mathematically (Bowman et al., 2015).",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
Some previous methods effectively alleviate this problem.,4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"Such methods are also important to keep a balance between the two items of the loss, namely KL loss and reconstruction loss.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"We use techniques of KL annealing, early stopping (Bowman et al., 2015) and bag-of-word loss (Zhao et al., 2017) in our models.",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"The general loss with bag-of-word loss (see supplementary materials for details) is rewritten as:
L′ =",4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
L+ Lbow (3),4.2 Conditional Variational Autoencoder (CVAE),[0],[0]
"In order to further control the emotion of our generation more explicitly, we combine policy gradient techniques on top of the CVAE above and proposed Reinforced CVAE model for our task.",4.3 Reinforced CVAE,[0],[0]
We first train an emoji classifier on our dataset separately and fix its parameters thereafter.,4.3 Reinforced CVAE,[0],[0]
The classifier is used to produce reward for the policy training.,4.3 Reinforced CVAE,[0],[0]
"It is a skip connected model of Bidirectional GRU-RNN layers (Felbo et al., 2017).
",4.3 Reinforced CVAE,[0],[0]
"During the policy training, we first get the generated response x′ by passing x and c through the CVAE, then feeding generation x′ to classifier and get the probability of the emoji label as reward R. Let θ be parameters of our network, REINFORCE algorithm (Williams, 1992) is used to maximize the expected reward of generated responses:
J (θ) = Ep(x|c)(Rθ(x, c))",4.3 Reinforced CVAE,[0],[0]
"(4)
The gradient of Equation 4 is approximated using the likelihood ratio trick (Glynn, 1990; Williams, 1992):
∇J (θ) =",4.3 Reinforced CVAE,[0],[0]
"(R− r)∇ |x|∑ t log p(xt|c, x1:t−1) (5)
r is the baseline value to keep estimate unbiased and reduce its variance.",4.3 Reinforced CVAE,[0],[0]
"In our case, we directly pass x through emoji classifier and compute the probability of the emoji label as r. The model then encourages response generation that has R > r.
As REINFORCE objective is unrelated to response generation, it may make the generation model quickly deteriorate to some generic responses.",4.3 Reinforced CVAE,[0],[0]
"To stabilize the training process, we propose two straightforward techniques to constrain the policy training:
1.",4.3 Reinforced CVAE,[0],[0]
Adjust rewards according to the position of the emoji label when all labels are ranked from high to low in order of the probability given by the emoji classifier.,4.3 Reinforced CVAE,[0],[0]
"When the probability of the emoji label is of high rank among all possible emojis, we assume that the model has succeeded in emotion expression, thus there is no need to adjust parameters toward higher probability in this response.",4.3 Reinforced CVAE,[0],[0]
"Modified policy gradient is written as:
∇J ′(θ) =",4.3 Reinforced CVAE,[0],[0]
"α(R− r)∇ |x|∑ t log p(xt|c, x1:t−1)
(6)
",4.3 Reinforced CVAE,[0],[0]
where α ∈,4.3 Reinforced CVAE,[0],[0]
"[0, 1] is a variant coefficient.",4.3 Reinforced CVAE,[0],[0]
"The higher R ranks in all types of emoji label, the closer α is to 0.
2.",4.3 Reinforced CVAE,[0],[0]
"Train Reinforced CVAE by a hybrid objective of REINFORCE and variational lower bound objective, learning towards both emotion accuracy and response appropriateness:
minθL′′ = L′",4.3 Reinforced CVAE,[0],[0]
− λJ ′,4.3 Reinforced CVAE,[0],[0]
"(7)
λ is a balancing coefficient, which is set to 1 in our experiments.
",4.3 Reinforced CVAE,[0],[0]
The algorithm outlining the training process of Reinforced CVAE can be found in the supplementary materials.,4.3 Reinforced CVAE,[0],[0]
We conducted several experiments to finalize the hyper-parameters of our models (Table 2).,5 Experimental Results and Analyses,[0],[0]
"During training, fully converged base SEQ2SEQ model is used to initialize its counterparts in CVAE models.",5 Experimental Results and Analyses,[0],[0]
Pretraining is vital to the success of our models since it is essentially hard for them to learn a latent variable space from total randomness.,5 Experimental Results and Analyses,[0],[0]
"For more details, please refer to the supplementary materials.
",5 Experimental Results and Analyses,[0],[0]
"In this section, we first report and analyze the general results of our models, including perplexity, loss and emotion accuracy.",5 Experimental Results and Analyses,[0],[0]
Then we take a closer look at the generation quality as well as our models’ capability of expressing emotion.,5 Experimental Results and Analyses,[0],[0]
"To generally evaluate the performance of our models, we use generation perplexity and top-1/top-5
emoji accuracy on the test set.",5.1 General,[0],[0]
Perplexity indicates how much difficulty the model is having when generating responses.,5.1 General,[0],[0]
"We also use top-5 emoji accuracy, since the meaning of different emojis may overlap with only a subtle difference.",5.1 General,[0],[0]
"The machine may learn that similarity and give multiple possible labels as the answer.
",5.1 General,[0],[0]
Note that we use the same emoji classifier for evaluation.,5.1 General,[0],[0]
"Its accuracy (see supplementary materials) may not seem perfect, but it is the stateof-the-art emoji classifier given so many classes.",5.1 General,[0],[0]
"Also, it’s reasonable to use the same classifier in training for automated evaluation, as is in (Hu et al., 2017).",5.1 General,[0],[0]
"We can obtain meaningful results as long as the classifier is able to capture the semantic relationship between emojis (Felbo et al., 2017).
",5.1 General,[0],[0]
"As is shown in Table 2, CVAE significantly reduces the perplexity and increases the emoji accuracy over base model.",5.1 General,[0],[0]
Reinforced CVAE also adds to the emoji accuracy at the cost of a slight increase in perplexity.,5.1 General,[0],[0]
"These results confirm that proposed methods are effective toward the generation of emotional responses.
",5.1 General,[0],[0]
"When converged, the KL loss is 27.0/25.5 for the CVAE/Reinforced CVAE respectively, and reconstruction loss 42.2/40.0.",5.1 General,[0],[0]
"The models achieved a balance between the two items of loss, confirming that they have successfully learned a meaningful latent variable.",5.1 General,[0],[0]
"SEQ2SEQ generates in a monotonous way, as several generic responses occur repeatedly, while the generation of CVAE models is of much more diversity.",5.2 Generation Diversity,[0],[0]
"To showcase this disparity, we calculated the type-token ratios of unigrams/bigrams/trigrams in generated responses as
the order of frequencies in the dataset.",5.2 Generation Diversity,[0],[0]
"No.0 is , for instance, No.1 and so on.",5.2 Generation Diversity,[0],[0]
Top: CVAE v. Base.,5.2 Generation Diversity,[0],[0]
Bottom: Reinforced CVAE v. CVAE.,5.2 Generation Diversity,[0],[0]
"If Reinforced CVAE scores higher, the margin is marked in orange.",5.2 Generation Diversity,[0],[0]
"If lower, in black.
",5.2 Generation Diversity,[0],[0]
the diversity score.,5.2 Generation Diversity,[0],[0]
"As shown in Table 3, results show that CVAE models beat the base models by a large margin.",5.2 Generation Diversity,[0],[0]
Diversity scores of Reinforced CVAE are reasonably compromised since it’s generating more emotional responses.,5.2 Generation Diversity,[0],[0]
There are potentially multiple types of emotion in reaction to an utterance.,5.3 Controllability of Emotions,[0],[0]
Our work makes it possible to generate a response to an arbitrary emotion by conditioning the generation on a specific type of emoji.,5.3 Controllability of Emotions,[0],[0]
"In this section, we generate one response in reply to each original tweet in the dataset and condition on each emoji of the selected 64 emo-
jis.",5.3 Controllability of Emotions,[0],[0]
"We may have recorded some original tweets with different replies in the dataset, but an original tweet only need to be used once for each emoji, so we eliminate duplicate original tweets in the dataset.",5.3 Controllability of Emotions,[0],[0]
"There are 30,299 unique original tweets in the test set.
",5.3 Controllability of Emotions,[0],[0]
Figure 4 shows the top-5 accuracy of each type of the first 32 emoji labels when the models generates responses from the test set conditioning on the same emoji.,5.3 Controllability of Emotions,[0],[0]
The results show that CVAE models increase the accuracy over every type of emoji label.,5.3 Controllability of Emotions,[0],[0]
"Reinforced CVAE model sees a bigger increase on the less common emojis, confirming the effect of the emoji-specified policy training.",5.3 Controllability of Emotions,[0],[0]
"We employed crowdsourced judges to evaluate a random sample of 100 items (Table 4), each being assigned to 5 judges on the Amazon Mechanical Turk.",5.4 Human Evaluation,[0],[0]
We present judges original tweets and generated responses.,5.4 Human Evaluation,[0],[0]
"In the first setting of human evaluation, judges are asked to decide which one of the two generated responses better reply the original tweet.",5.4 Human Evaluation,[0],[0]
"In the second setting, the emoji label is presented with the item discription, and judges are asked to pick one of the two generated responses that they decide better fits this emoji.",5.4 Human Evaluation,[0],[0]
(These two settings of evaluation are conducted separately so that it will not affect judges’ verdicts.),5.4 Human Evaluation,[0],[0]
Order of two generated responses under one item is permuted.,5.4 Human Evaluation,[0],[0]
"Ties are permitted for an-
swers.",5.4 Human Evaluation,[0],[0]
We batch five items as one assignment and insert an item with two identical outputs as the sanity check.,5.4 Human Evaluation,[0],[0]
"Anyone who failed to choose “tie” for that item is considered as a careless judge and is therefore rejected from our test.
",5.4 Human Evaluation,[0],[0]
We then conducted a simplified Turing test.,5.4 Human Evaluation,[0],[0]
"Each item we present judges an original tweet, its reply by a human, and its response generated from Reinforced CVAE model.",5.4 Human Evaluation,[0],[0]
We ask judges to decide which of the two given responses is written by a human.,5.4 Human Evaluation,[0],[0]
Other parts of the setting are similar to above-mentioned tests.,5.4 Human Evaluation,[0],[0]
"It turned out 18% of the test subjects mistakenly chose machine-generated responses as human written, and 27% stated that
they were not able to distinguish between the two responses.
",5.4 Human Evaluation,[0],[0]
"In regard of the inter-rater agreement, there are four cases.",5.4 Human Evaluation,[0],[0]
"The ideal situation is that all five judges choose the same answer for a item, and in the worst-case scenario, at most two judges choose the same answer.",5.4 Human Evaluation,[0],[0]
"In light of this, we have counted that 32%/33%/31%/5% of all items have 5/4/3/2 judges in agreement, showing that our experiment has a reasonably reliable inter-rater agreement.",5.4 Human Evaluation,[0],[0]
"We sampled some generated responses from all three models, and list them in Figure 5.",5.5 Case Study,[0],[0]
"Given
an original tweet, we would like to generate responses with three different target emotions.
",5.5 Case Study,[0],[0]
"SEQ2SEQ only chooses to generate most frequent expressions, forming a predictable pattern for its generation (See how every sampled response by the base model starts with “I’m”).",5.5 Case Study,[0],[0]
"On the contrary, generation from the CVAE model is diverse, which is in line with previous quantitative analysis.",5.5 Case Study,[0],[0]
"However, the generated responses are sometimes too diversified and unlikely to reply to the original tweet.
",5.5 Case Study,[0],[0]
Reinforced CVAE somtetimes tends to generate a lengthy response by stacking up sentences (See the responses to the first tweet when conditioning on the ‘folded hands’ emoji and the ‘sad face’ emoji).,5.5 Case Study,[0],[0]
"It learns to break the length limit of sequence generation during hybrid training, since the variational lower bound objective is competing with REINFORCE objective.",5.5 Case Study,[0],[0]
The situation would be more serious is λ in Equation 7 is set higher.,5.5 Case Study,[0],[0]
"However, this phenomenon does not impair the fluency of generated sentences, as can be seen in
Figure 5.",5.5 Case Study,[0],[0]
"In this paper, we investigate the possibility of using naturally annotated emoji-rich Twitter data for emotional response generation.",6 Conclusion and Future Work,[0],[0]
"More specifically, we collected more than half a million Twitter conversations with emoji in the response and assumed that the fine-grained emoji label chosen by the user expresses the emotion of the tweet.",6 Conclusion and Future Work,[0],[0]
We applied several state-of-the-art neural models to learn a generation system that is capable of giving a response with an arbitrarily designated emotion.,6 Conclusion and Future Work,[0],[0]
We performed automatic and human evaluations to understand the quality of generated responses.,6 Conclusion and Future Work,[0],[0]
We trained a large scale emoji classifier and ran the classifier on the generated responses to evaluate the emotion accuracy of the generated response.,6 Conclusion and Future Work,[0],[0]
"We performed an Amazon Mechanical Turk experiment, by which we compared our models with a baseline sequence-to-sequence model on metrics of relevance and emotion.",6 Conclusion and Future Work,[0],[0]
"Experimentally, it is shown that our model is capable of generating high-quality emotional responses, without the need of laborious human annotations.",6 Conclusion and Future Work,[0],[0]
Our work is a crucial step towards building intelligent dialog agents.,6 Conclusion and Future Work,[0],[0]
"We are also looking forward to transferring the idea of naturally-labeled emojis to task-oriented dialog and multi-turn dialog
generation problems.",6 Conclusion and Future Work,[0],[0]
"Due to the nature of social media text, some emotions, such as fear and disgust, are underrepresented in the dataset, and the distribution of emojis is unbalanced to some extent.",6 Conclusion and Future Work,[0],[0]
"We will keep accumulating data and increase the ratio of underrepresented emojis, and advance toward more sophisticated abstractive generation methods.",6 Conclusion and Future Work,[0],[0]
Generating emotional language is a key step towards building empathetic natural language processing agents.,abstractText,[0],[0]
"However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels.",abstractText,[0],[0]
"Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult.",abstractText,[0],[0]
"In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis.",abstractText,[0],[0]
We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence.,abstractText,[0],[0]
"We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text.",abstractText,[0],[0]
"Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate highquality abstractive conversation responses in accordance with designated emotions.",abstractText,[0],[0]
MOJITALK: Generating Emotional Responses at Scale,title,[0],[0]
"Word representation learning has become a research area of central importance in natural language processing (NLP), with its usefulness demonstrated across many application areas such as parsing (Chen and Manning, 2014; Johannsen et al., 2015), machine translation (Zou et al., 2013), and many others (Turian et al., 2010; Collobert et al.,
2011).",1 Introduction,[0],[0]
"Most prominent word representation techniques are grounded in the distributional hypothesis (Harris, 1954), relying on word co-occurrence information in large textual corpora (Curran, 2004; Turney and Pantel, 2010; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Levy and Goldberg, 2014; Schwartz et al., 2015, i.a.).
",1 Introduction,[0],[0]
"Morphologically rich languages, in which “substantial grammatical information. .",1 Introduction,[0],[0]
.,1 Introduction,[0],[0]
"is expressed at word level” (Tsarfaty et al., 2010), pose specific challenges for NLP.",1 Introduction,[0],[0]
"This is not always considered when techniques are evaluated on languages such as English or Chinese, which do not have rich morphology.",1 Introduction,[0],[0]
"In the case of distributional vector space models, morphological complexity brings two challenges to the fore:
1.",1 Introduction,[0],[0]
Estimating Rare Words: A single lemma can have many different surface realisations.,1 Introduction,[0],[0]
Naively treating each realisation as a separate word leads to sparsity problems and a failure to exploit their shared semantics.,1 Introduction,[0],[0]
"On the other hand, lemmatising the entire corpus can obfuscate the differences that exist between different word forms even though they share some aspects of meaning.
2.",1 Introduction,[0],[0]
Embedded Semantics:,1 Introduction,[0],[0]
"Morphology can encode semantic relations such as antonymy (e.g. literate and illiterate, expensive and inexpensive) or (near-)synonymy (north, northern, northerly).
",1 Introduction,[0],[0]
"In this work, we tackle the two challenges jointly by introducing a resource-light vector space finetuning procedure termed morph-fitting.",1 Introduction,[0],[0]
The proposed method does not require curated knowledge bases or gold lexicons.,1 Introduction,[0],[0]
"Instead, it makes use of the observation that morphology implicitly encodes semantic signals pertaining to synonymy (e.g., German word inflections katalanisch, katalanischem, katalanischer denote the same semantic concept in different grammatical roles), and antonymy (e.g., mature vs. immature), capitalising on the
ar X
iv :1
70 6.
00 37
",1 Introduction,[0],[0]
"7v 1
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
J un
2 01
7
proliferation of word forms in morphologically rich languages.",1 Introduction,[0],[0]
"Formalised as an instance of the post-processing semantic specialisation paradigm (Faruqui et al., 2015; Mrkšić et al., 2016), morphfitting is steered by a set of linguistic constraints derived from simple language-specific rules which describe (a subset of) morphological processes in a language.",1 Introduction,[0],[0]
"The constraints emphasise similarity on one side (e.g., by extracting morphological synonyms), and antonymy on the other (by extracting morphological antonyms), see Fig. 1 and Tab. 2.
",1 Introduction,[0],[0]
"The key idea of the fine-tuning process is to pull synonymous examples described by the constraints closer together in the transformed vector space, while at the same time pushing antonymous examples away from each other.",1 Introduction,[0],[0]
"The explicit post-hoc injection of morphological constraints enables: a) the estimation of more accurate vectors for lowfrequency words which are linked to their highfrequency forms by the constructed constraints;1 this tackles the data sparsity problem; and b) specialising the distributional space to distinguish between similarity and relatedness (Kiela et al., 2015), thus supporting language understanding applications such as dialogue state tracking (DST).2
As a post-processor, morph-fitting allows the integration of morphological rules with any distributional vector space in any language: it treats an input distributional word vector space as a black box and fine-tunes it so that the transformed space reflects the knowledge coded in the input morphological constraints (e.g., Italian words rispettoso and irrispetosa should be far apart in the trans-
1For instance, the vector for the word katalanischem which occurs only 9 times in the German Wikipedia will be pulled closer to the more reliable vectors for katalanisch and katalanischer, with frequencies of 2097 and 1383 respectively.
2Representation models that do not distinguish between synonyms and antonyms may have grave implications in downstream language understanding applications such as spoken dialogue systems: a user looking for ‘an affordable Chinese restaurant in west Cambridge’ does not want a recommendation for ‘an expensive Thai place in east Oxford’.
",1 Introduction,[0],[0]
"formed vector space, see Fig. 1).",1 Introduction,[0],[0]
"Tab. 1 illustrates the effects of morph-fitting by qualitative examples in three languages: the vast majority of nearest neighbours are “morphological” synonyms.
",1 Introduction,[0],[0]
"We demonstrate the efficacy of morph-fitting in four languages (English, German, Italian, Russian), yielding large and consistent improvements on benchmarking word similarity evaluation sets such as SimLex-999 (Hill et al., 2015), its multilingual extension (Leviant and Reichart, 2015), and SimVerb-3500 (Gerz et al., 2016).",1 Introduction,[0],[0]
"The improvements are reported for all four languages, and with a variety of input distributional spaces, verifying the robustness of the approach.
",1 Introduction,[0],[0]
"We then show that incorporating morph-fitted vectors into a state-of-the-art neural-network DST model results in improved tracking performance, especially for morphologically rich languages.",1 Introduction,[0],[0]
"We report an improvement of 4% on Italian, and 6% on German when using morph-fitted vectors instead of the distributional ones, setting a new state-of-theart DST performance for the two datasets.3
3There are no readily available DST datasets for Russian.",1 Introduction,[0],[0]
"Preliminaries In this work, we focus on four languages with varying levels of morphological complexity: English (EN), German (DE), Italian (IT), and Russian (RU).",2 Morph-fitting: Methodology,[0],[0]
These correspond to languages in the Multilingual SimLex-999 dataset.,2 Morph-fitting: Methodology,[0],[0]
"Vocabularies Wen, Wde, Wit, Wru are compiled by retaining all word forms from the four Wikipedias with word frequency over 10, see Tab. 3.",2 Morph-fitting: Methodology,[0],[0]
"We then extract sets of linguistic constraints from these (large) vocabularies using a set of simple language-specific if-then-else rules, see Tab. 2.4",2 Morph-fitting: Methodology,[0],[0]
These constraints (Sect. 2.2) are used as input for the vector space post-processing ATTRACT-REPEL algorithm (outlined in Sect. 2.1).,2 Morph-fitting: Methodology,[0],[0]
"The ATTRACT-REPEL model, proposed by Mrkšić et al. (2017b), is an extension of the PARAGRAM procedure proposed by Wieting et al. (2015).",2.1 The ATTRACT-REPEL Model,[0],[0]
It provides a generic framework for incorporating similarity (e.g. successful and accomplished) and antonymy constraints (e.g. nimble and clumsy) into pre-trained word vectors.,2.1 The ATTRACT-REPEL Model,[0],[0]
"Given the initial vector space and collections of ATTRACT and REPEL constraints A and R, the model gradually modifies the space to bring the designated word vectors closer together or further apart.",2.1 The ATTRACT-REPEL Model,[0],[0]
The method’s cost function consists of three terms.,2.1 The ATTRACT-REPEL Model,[0],[0]
"The first term pulls the ATTRACT examples (xl, xr) ∈",2.1 The ATTRACT-REPEL Model,[0],[0]
A closer together.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If BA denotes the current mini-batch of ATTRACT examples, this term can be expressed as:
A(BA) =",2.1 The ATTRACT-REPEL Model,[0],[0]
"∑
(xl,xr)∈BA
(ReLU (δatt + xltl",2.1 The ATTRACT-REPEL Model,[0],[0]
"− xlxr)
+ ReLU (δatt + xrtr − xlxr))
where δatt is the similarity margin which determines how much closer synonymous vectors should be to each other than to each of their respective negative examples.",2.1 The ATTRACT-REPEL Model,[0],[0]
"ReLU(x) = max(0, x) is the standard rectified linear unit (Nair and Hinton, 2010).",2.1 The ATTRACT-REPEL Model,[0],[0]
The ‘negative’ example ti for each word xi in any ATTRACT pair is the word vector closest to xi among the examples in the current minibatch (distinct from its target synonym and xi itself).,2.1 The ATTRACT-REPEL Model,[0],[0]
"This means that this term forces synonymous
4A native speaker can easily come up with these sets of morphological rules (or at least with a reasonable subset of them) without any linguistic training.",2.1 The ATTRACT-REPEL Model,[0],[0]
"What is more, the rules for DE, IT, and RU were created by non-native, non-fluent speakers with a limited knowledge of the three languages, exemplifying the simplicity and portability of the approach.
",2.1 The ATTRACT-REPEL Model,[0],[0]
"words from the in-batch ATTRACT constraints to be closer to one another than to any other word in the current mini-batch.
",2.1 The ATTRACT-REPEL Model,[0],[0]
The second term pushes antonyms away from each other.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If (xl, xr) ∈ BR is the current minibatch of REPEL constraints, this term can be expressed as follows:
R(BR) =",2.1 The ATTRACT-REPEL Model,[0],[0]
"∑
(xl,xr)∈BR
(ReLU (δrpl + xlxr − xltr)
+ ReLU (δrpl + xlxr − xrtr))
",2.1 The ATTRACT-REPEL Model,[0],[0]
"In this case, each word’s ‘negative’ example is the (in-batch) word vector furthest away from it (and distinct from the word’s target antonym).",2.1 The ATTRACT-REPEL Model,[0],[0]
"The intuition is that we want antonymous words from the input REPEL constraints to be further away from each other than from any other word in the current mini-batch; δrpl is now the repel margin.
",2.1 The ATTRACT-REPEL Model,[0],[0]
The final term of the cost function serves to retain the abundance of semantic information encoded in the starting distributional space.,2.1 The ATTRACT-REPEL Model,[0],[0]
"If xiniti is the initial distributional vector and V (B) is the set of all vectors present in the given mini-batch, this term (per mini-batch) is expressed as follows:
R(BA,BR) = ∑
xi∈V (BA∪BR)
",2.1 The ATTRACT-REPEL Model,[0],[0]
"λreg ∥∥∥xiniti − xi∥∥∥ 2
where λreg is the L2 regularisation constant.5",2.1 The ATTRACT-REPEL Model,[0],[0]
"This term effectively pulls word vectors towards their initial (distributional) values, ensuring that relations encoded in initial vectors persist as long as they do not contradict the newly injected ones.",2.1 The ATTRACT-REPEL Model,[0],[0]
"Semantic Specialisation with Constraints The fine-tuning ATTRACT-REPEL procedure is entirely driven by the input ATTRACT and REPEL sets of
5We use hyperparameter values δatt = 0.6, δrpl = 0.0, λreg = 10
−9 from prior work without fine-tuning.",2.2 Language-Specific Rules and Constraints,[0],[0]
"We train all models for 10 epochs with AdaGrad (Duchi et al., 2011).
constraints.",2.2 Language-Specific Rules and Constraints,[0],[0]
"These can be extracted from a variety of semantic databases such as WordNet (Fellbaum, 1998), the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015), or BabelNet (Navigli and Ponzetto, 2012; Ehrmann et al., 2014) as done in prior work (Faruqui et al., 2015; Wieting et al., 2015; Mrkšić et al., 2016, i.a.).",2.2 Language-Specific Rules and Constraints,[0],[0]
"In this work, we investigate another option: extracting constraints without curated knowledge bases in a spectrum of languages by exploiting inherent language-specific properties related to linguistic morphology.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This relaxation ensures a wider portability of ATTRACTREPEL to languages and domains without readily available or adequate resources.
",2.2 Language-Specific Rules and Constraints,[0],[0]
Extracting ATTRACT Pairs,2.2 Language-Specific Rules and Constraints,[0],[0]
"The core difference between inflectional and derivational morphology can be summarised in a few lines as follows: the former refers to a set of processes through which the word form expresses meaningful syntactic information, e.g., verb tense, without any change to the semantics of the word.",2.2 Language-Specific Rules and Constraints,[0],[0]
"On the other hand, the latter refers to the formation of new words with semantic shifts in meaning (Schone and Jurafsky, 2001; Haspelmath and Sims, 2013; Lazaridou et al., 2013; Zeller et al., 2013; Cotterell and Schütze, 2017).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"For the ATTRACT constraints, we focus on inflectional rather than on derivational morphology rules as the former preserve the full meaning of a word, modifying it only to reflect grammatical roles such as verb tense or case markers (e.g., (en_read, en_reads) or (de_katalanisch, de_katalanischer)).",2.2 Language-Specific Rules and Constraints,[0],[0]
"This choice is guided by our intent to fine-tune the original vector space in order to improve the embedded semantic relations.
",2.2 Language-Specific Rules and Constraints,[0],[0]
"We define two rules for English, widely recognised as morphologically simple (Avramidis and Koehn, 2008; Cotterell et al., 2016b).",2.2 Language-Specific Rules and Constraints,[0],[0]
"These are: (R1) if w1, w2 ∈Wen, where w2 = w1 + ing/ed/s, then add (w1, w2) and (w2, w1) to the set of ATTRACT constraints A.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This rule yields pairs such as (look, looks), (look, looking), (look, looked).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"If w[: −1] is a function which strips the last character from word w, the second rule is: (R2)
if w1 ends with the letter e and w1 ∈ Wen and w2 ∈ Wen, where w2 = w1",2.2 Language-Specific Rules and Constraints,[0],[0]
"[: −1] + ing/ed, then add (w1, w2) and (w2, w1) to A.",2.2 Language-Specific Rules and Constraints,[0],[0]
"This creates pairs such as (create, creating) and (create, created).",2.2 Language-Specific Rules and Constraints,[0],[0]
"Naturally, introducing more sophisticated rules is possible in order to cover for other special cases and morphological irregularities (e.g., sweep / swept), but in all our EN experiments, A is based on the two simple EN rules R1 and R2.
",2.2 Language-Specific Rules and Constraints,[0],[0]
"The other three languages, with more complicated morphology, yield a larger number of rules.",2.2 Language-Specific Rules and Constraints,[0],[0]
"In Italian, we rely on the sets of rules spanning: (1) regular formation of plural (libro / libri); (2) regular verb conjugation (aspettare / aspettiamo); (3) regular formation of past participle (aspettare / aspettato); and (4) rules regarding grammatical gender (bianco / bianca).",2.2 Language-Specific Rules and Constraints,[0],[0]
"Besides these, another set of rules is used for German and Russian: (5) regular declension (e.g., asiatisch / asiatischem).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"Extracting REPEL Pairs As another source of implicit semantic signals, W also contains words which represent derivational antonyms: e.g., two words that denote concepts with opposite meanings, generated through a derivational process.",2.2 Language-Specific Rules and Constraints,[0],[0]
"We use a standard set of EN “antonymy” prefixes: APen = {dis, il, un, in, im, ir, mis, non, anti} (Fromkin et al., 2013).",2.2 Language-Specific Rules and Constraints,[0],[0]
"If w1, w2 ∈ Wen, where w2 is generated by adding a prefix from APen to w1, then (w1, w2) and (w2, w1) are added to the set of REPEL constraints R. This rule generates pairs such as (advantage, disadvantage) and (regular, irregular).",2.2 Language-Specific Rules and Constraints,[0],[0]
"An additional rule replaces the suffix -ful with -less, extracting antonyms such as (careful, careless).
",2.2 Language-Specific Rules and Constraints,[0],[0]
"Following the same principle, we use APde = {un, nicht, anti, ir, in, miss}, APit = {in, ir, im, anti}, and APru = {не, анти}.",2.2 Language-Specific Rules and Constraints,[0],[0]
"For instance, this generates an IT pair (rispettoso, irrispettoso)",2.2 Language-Specific Rules and Constraints,[0],[0]
(see Fig. 1).,2.2 Language-Specific Rules and Constraints,[0],[0]
"For DE, we use another rule targeting suffix replacement: -voll is replaced by -los.
",2.2 Language-Specific Rules and Constraints,[0],[0]
We further expand the set of REPEL constraints by transitively combining antonymy pairs from the previous step with inflectional ATTRACT pairs.,2.2 Language-Specific Rules and Constraints,[0],[0]
"This step yields additional constraints such as (rispettosa, irrispettosi) (see Fig. 1).",2.2 Language-Specific Rules and Constraints,[0],[0]
The final A andR constraint counts are given in Tab. 3.,2.2 Language-Specific Rules and Constraints,[0],[0]
The full sets of rules are available as supplemental material.,2.2 Language-Specific Rules and Constraints,[0],[0]
"Training Data and Setup For each of the four languages we train the skip-gram with negative sampling (SGNS) model (Mikolov et al., 2013)
on the latest Wikipedia dump of each language.",3 Experimental Setup,[0],[0]
"We induce 300-dimensional word vectors, with the frequency cut-off set to 10.",3 Experimental Setup,[0],[0]
The vocabulary sizes |W | for each language are provided in Tab. 3.6,3 Experimental Setup,[0],[0]
"We label these collections of vectors SGNS-LARGE.
",3 Experimental Setup,[0],[0]
Other Starting Distributional Vectors We also analyse the impact of morph-fitting on other collections of well-known EN word vectors.,3 Experimental Setup,[0],[0]
These vectors have varying vocabulary coverage and are trained with different architectures.,3 Experimental Setup,[0],[0]
"We test standard distributional models: Common-Crawl GloVe (Pennington et al., 2014), SGNS vectors (Mikolov et al., 2013) with various contexts (BOW = bag-ofwords; DEPS = dependency contexts), and training data (PW = Polyglot Wikipedia from Al-Rfou et al. (2013); 8B = 8 billion token word2vec corpus), following (Levy and Goldberg, 2014) and (Schwartz et al., 2015).",3 Experimental Setup,[0],[0]
"We also test the symmetricpattern based vectors of Schwartz et al. (2016) (SymPat-Emb), count-based PMI-weighted vectors reduced by SVD (Baroni et al., 2014) (Count-SVD), a model which replaces the context modelling function from CBOW with bidirectional LSTMs (Melamud et al., 2016) (Context2Vec), and two sets of EN vectors trained by injecting multilingual information: BiSkip (Luong et al., 2015) and MultiCCA (Faruqui and Dyer, 2014).
",3 Experimental Setup,[0],[0]
"We also experiment with standard well-known distributional spaces in other languages (IT and DE), available from prior work (Dinu et al., 2015; Luong et al., 2015; Vulić and Korhonen, 2016a).
",3 Experimental Setup,[0],[0]
"Morph-fixed Vectors A baseline which utilises an equal amount of knowledge as morph-fitting, termed morph-fixing, fixes the vector of each word to the distributional vector of its most frequent inflectional synonym, tying the vectors of lowfrequency words to their more frequent inflections.",3 Experimental Setup,[0],[0]
"For each word w1, we construct a set of M + 1 words Ww1 = {w1, w′1, . . .",3 Experimental Setup,[0],[0]
", w′M} consisting of the word w1 itself and all M words which cooccur with w1 in the ATTRACT constraints.",3 Experimental Setup,[0],[0]
"We then choose the word w′max from the set Ww1 with the maximum frequency in the training data, and fix all other word vectors in Ww1 to its word vector.",3 Experimental Setup,[0],[0]
"The morph-fixed vectors (MFIX) serve as our primary baseline, as they outperformed another straightforward baseline based on stemming across
6Other SGNS parameters were set to standard values (Baroni et al., 2014; Vulić and Korhonen, 2016b): 15 epochs, 15 negative samples, global learning rate: .025, subsampling rate: 1e− 4.",3 Experimental Setup,[0],[0]
"Similar trends in results persist with d = 100, 500.
",3 Experimental Setup,[0],[0]
"all of our intrinsic and extrinsic experiments.
",3 Experimental Setup,[0],[0]
"Morph-fitting Variants We analyse two variants of morph-fitting: (1) using ATTRACT constraints only (MFIT-A), and (2) using both ATTRACT and REPEL constraints (MFIT-AR).",3 Experimental Setup,[0],[0]
"Evaluation Setup and Datasets The first set of experiments intrinsically evaluates morph-fitted vector spaces on word similarity benchmarks, using Spearman’s rank correlation as the evaluation metric.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"First, we use the SimLex-999 dataset, as well as SimVerb-3500, a recent EN verb pair similarity dataset providing similarity ratings for 3,500 verb pairs.7 SimLex-999 was translated to DE, IT, and RU by Leviant and Reichart (2015), and they crowdsourced similarity scores from native speakers.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"We use this dataset for our multilingual evaluation.8
Morph-fitting EN Word Vectors As the first experiment, we morph-fit a wide spectrum of EN distributional vectors induced by various architectures (see Sect. 3).",4 Intrinsic Evaluation: Word Similarity,[0],[0]
The results on SimLex and SimVerb are summarised in Tab. 4.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
The results with EN SGNS-LARGE vectors are shown in Fig. 3a.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Morphfitted vectors bring consistent improvement across all experiments, regardless of the quality of the initial distributional space.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
This finding confirms that the method is robust: its effectiveness does not depend on the architecture used to construct the initial space.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"To illustrate the improvements, note that the best score on SimVerb for a model trained on running text is achieved by Context2vec (ρ = 0.388); injecting morphological constraints into this vector space results in a gain of 7.1 ρ points.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Experiments on Other Languages We next extend our experiments to other languages, testing both morph-fitting variants.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"The results are summarised in Tab. 5, while Fig. 3a-3d show results for the morph-fitted SGNS-LARGE vectors.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"These scores confirm the effectiveness and robustness of morph-fitting across languages, suggesting that the idea of fitting to morphological constraints is indeed language-agnostic, given the set of languagespecific rule-based constraints.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"Fig. 3 also demon-
7Unlike other gold standard resources such as WordSim353 (Finkelstein et al., 2002) or MEN (Bruni et al., 2014), SimLex and SimVerb provided explicit guidelines to discern between semantic similarity and association, so that related but non-similar words (e.g. cup and coffee) have a low rating.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
"8Since Leviant and Reichart (2015) re-scored the original EN SimLex, we use their EN SimLex version for consistency.
strates that the morph-fitted vector spaces consistently outperform the morph-fixed ones.
",4 Intrinsic Evaluation: Word Similarity,[0],[0]
The comparison between MFIT-A and MFITAR indicates that both sets of constraints are important for the fine-tuning process.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
"MFIT-A yields consistent gains over the initial spaces, and (consistent) further improvements are achieved by also incorporating the antonymous REPEL constraints.",4 Intrinsic Evaluation: Word Similarity,[0],[0]
This demonstrates that both types of constraints are useful for semantic specialisation.,4 Intrinsic Evaluation: Word Similarity,[0],[0]
We also tried using other post-processing specialisation models from the literature in lieu of ATTRACT-REPEL using the same set of “morphological” synonymy and antonymy constraints.,Comparison to Other Specialisation Methods,[0],[0]
"We compare ATTRACT-REPEL to the retrofitting model
of (Faruqui et al., 2015) and counter-fitting (Mrkšić et al., 2017a).",Comparison to Other Specialisation Methods,[0],[0]
The two baselines were trained for 20 iterations using suggested settings.,Comparison to Other Specialisation Methods,[0],[0]
"The results for EN, DE, and IT are summarised in Fig. 2.",Comparison to Other Specialisation Methods,[0],[0]
They clearly indicate that MFIT-AR outperforms the two other post-processors for each language.,Comparison to Other Specialisation Methods,[0],[0]
We hypothesise that the difference in performance mainly stems from context-sensitive vector space updates performed by ATTRACT-REPEL.,Comparison to Other Specialisation Methods,[0],[0]
"Conversely, the other two models perform pairwise updates which do not consider what effect each update has on the example pair’s relation to other word vectors (for a detailed comparison, see (Mrkšić et al., 2017b)).
",Comparison to Other Specialisation Methods,[0],[0]
"Besides their lower performance, the two other specialisation models have additional disadvantages compared to the proposed morph-fitting model.",Comparison to Other Specialisation Methods,[0],[0]
"First, retrofitting is able to incorporate only synonymy/ATTRACT pairs, while our results demonstrate the usefulness of both types of constraints, both for intrinsic evaluation (Tab. 5) and downstream tasks (see later Fig. 3).",Comparison to Other Specialisation Methods,[0],[0]
"Second, counter-fitting is computationally intractable with SGNS-LARGE vectors, as its regularisation term involves the computation of all pairwise distances between words in the vocabulary.
",Comparison to Other Specialisation Methods,[0],[0]
"Further Discussion The simplicity of the used language-specific rules does come at a cost of occasionally generating incorrect linguistic constraints such as (tent, intent), (prove, improve) or (press, impress).",Comparison to Other Specialisation Methods,[0],[0]
"In future work, we will study how to fur-
ther refine extracted sets of constraints.",Comparison to Other Specialisation Methods,[0],[0]
"We also plan to conduct experiments with gold standard morphological lexicons on languages for which such resources exist (Sylak-Glassman et al., 2015; Cotterell et al., 2016b), and investigate approaches which learn morphological inflections and derivations in different languages automatically as another potential source of morphological constraints (Soricut and Och, 2015; Cotterell et al., 2016a; Faruqui et al., 2016; Kann et al., 2017; Aharoni and Goldberg, 2017, i.a.).",Comparison to Other Specialisation Methods,[0],[0]
Goal-oriented dialogue systems provide conversational interfaces for tasks such as booking flights or finding restaurants.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In slot-based systems, application domains are specified using ontologies that define the search constraints which users can express.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
An ontology consists of a number of slots and their assorted slot values.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In a restaurant search domain, sets of slot-values could include PRICE =",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"[cheap, expensive] or FOOD =",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"[Thai, Indian, ...].
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The DST model is the first component of modern dialogue pipelines (Young, 2010).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
It serves to capture the intents expressed by the user at each dialogue turn and update the belief state.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
This probability distribution over the possible dialogue states (defined by the domain ontology) is the system’s internal estimate of the user’s goals.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"It is used by the downstream dialogue manager component to choose the subsequent system response (Su et al., 2016).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The following example shows the true dialogue state in a multi-turn dialogue:
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User: What’s good in the southern part of town?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
inform(area=south) System: Vedanta is the top-rated Indian place.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User: How about something cheaper?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"inform(area=south, price=cheap) System: Seven Days is very popular.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Great hot pot.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
User:,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
What’s the address?,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"inform(area=south, price=cheap); request(address) System: Seven Days is at 66 Regent Street.
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The Dialogue State Tracking Challenge (DSTC) shared task series formalised the evaluation and provided labelled DST datasets (Henderson et al., 2014a,b; Williams et al., 2016).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"While a plethora of DST models are available based on, e.g., handcrafted rules (Wang et al., 2014) or conditional random fields (Lee and Eskenazi, 2013), the recent DST methodology has seen a shift towards neural-
network architectures (Henderson et al., 2014c,d; Zilka and Jurcicek, 2015; Mrkšić et al., 2015; Perez and Liu, 2017; Liu and Perez, 2017; Vodolán et al., 2017; Mrkšić et al., 2017a, i.a.).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Model: Neural Belief Tracker To detect intents in user utterances, most existing models rely on either (or both): 1) Spoken Language Understanding models which require large amounts of annotated training data; or 2) hand-crafted, domain-specific lexicons which try to capture lexical and morphological variation.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The Neural Belief Tracker (NBT) is a novel DST model which overcomes both issues by reasoning purely over pre-trained word vectors (Mrkšić et al., 2017a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
The NBT learns to compose these vectors into intermediate utterance and context representations.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
These are then used to decide which of the ontology-defined intents (goals) have been expressed by the user.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The NBT model keeps word vectors fixed during training, so that unseen, yet related words can be mapped to the right intent at test time (e.g. northern to north).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Data: Multilingual WOZ 2.0 Dataset,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Our DST evaluation is based on the WOZ dataset, released by Wen et al. (2017).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In this Wizard-of-Oz setup, two Amazon Mechanical Turk workers assumed the role of the user and the system asking/providing information about restaurants in Cambridge (operating over the same ontology and database used for DSTC2 (Henderson et al., 2014a)).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Users typed instead of speaking, removing the need to deal with noisy speech recognition.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In DSTC datasets, users would quickly adapt to the system’s inability to deal with complex queries.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Conversely, the WOZ setup allowed them to use sophisticated language.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The WOZ 2.0 release expanded the dataset to 1,200 dialogues (Mrkšić et al., 2017a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In this work, we use translations of this dataset to Italian and German, released by Mrkšić et al. (2017b).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Evaluation Setup The principal metric we use to measure DST performance is the joint goal accuracy, which represents the proportion of test set dialogue turns where all user goals expressed up to that point of the dialogue were decoded correctly (Henderson et al., 2014a).",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The NBT models for EN, DE and IT are trained using four variants of the SGNS-LARGE vectors: 1) the initial distributional vectors; 2) morph-fixed vectors; 3) and 4) the two variants of morph-fitted vectors (see Sect. 3).
",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"As shown by Mrkšić et al. (2017b), semantic specialisation of the employed word vectors ben-
efits DST performance across all three languages.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"However, large gains on SimLex-999 do not always induce correspondingly large gains in downstream performance.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In our experiments, we investigate the extent to which morph-fitting improves DST performance, and whether these gains exhibit stronger correlation with intrinsic performance.
Results and Discussion The dark bars (against the right axes) in Fig. 3 show the DST performance of NBT models making use of the four vector collections.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"IT and DE benefit from both kinds of morph-fitting: IT performance increases from 74.1→ 78.1 (MFIT-A) and DE performance rises even more: 60.6→ 66.3 (MFIT-AR), setting a new state-of-the-art score for both datasets.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"The morph-fixed vectors do not enhance DST performance, probably because fixing word vectors to their highest frequency inflectional form eliminates useful semantic content encoded in the original vectors.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"On the other hand, morph-fitting makes use of this information, supplementing it with semantic relations between different morphological forms.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"These conclusions are in line with the SimLex gains, where morph-fitting outperforms both distributional and morph-fixed vectors.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
English performance shows little variation across the four word vector collections investigated here.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"This corroborates our intuition that, as a morphologically simpler language, English stands to gain less from fine-tuning the morphological variation for downstream applications.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
This result again points at the discrepancy between intrinsic and extrinsic evaluation: the considerable gains in SimLex performance do not necessarily induce similar gains in downstream performance.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Additional discrepancies between SimLex and downstream DST performance are detected for German and Italian.,5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"While we observe a slight drop in SimLex performance with the DE MFIT-AR vectors compared to the MFIT-A ones, their relative performance is reversed in the DST task.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"On the other hand, we see the opposite trend in Italian, where the MFITA vectors score lower than the MFIT-AR vectors on SimLex, but higher on the DST task.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"In summary, we believe these results show that SimLex is not a perfect proxy for downstream performance in language understanding tasks.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
"Regardless, its performance does correlate with downstream performance to a large extent, providing a useful indicator for the usefulness of specific word vector
spaces for extrinsic tasks such as DST.",5 Downstream Task: Dialogue State Tracking (DST),[0],[0]
Semantic Specialisation A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together.,6 Related Work,[0],[0]
"Some models integrate such constraints into the training procedure, modifying the prior or the regularisation (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Kiela et al., 2015), or using a variant of the SGNS-style objective (Liu et al., 2015; Osborne et al., 2016).",6 Related Work,[0],[0]
"Another class of models, popularly termed retrofitting, injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors (Faruqui et al., 2015; Jauhar et al., 2015; Wieting et al., 2015; Nguyen et al., 2016; Mrkšić et al., 2016).",6 Related Work,[0],[0]
Morph-fitting falls into the latter category.,6 Related Work,[0],[0]
"However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the morphological richness of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words.
",6 Related Work,[0],[0]
Word Vectors and Morphology,6 Related Work,[0],[0]
The use of morphological resources to improve the representations of morphemes and words is an active area of research.,6 Related Work,[0],[0]
"The majority of proposed architectures encode morphological information, provided either as gold standard morphological resources (SylakGlassman et al., 2015) such as CELEX (Baayen et al., 1995) or as an external analyser such as Morfessor (Creutz and Lagus, 2007), along with distributional information jointly at training time in the language modelling (LM) objective (Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014; Cotterell and Schütze, 2015; Bhatia et al., 2016, i.a.).",6 Related Work,[0],[0]
"The key idea is to learn a morphological composition function (Lazaridou et al., 2013; Cotterell and Schütze, 2017) which synthesises the representation of a word given the representations of its constituent morphemes.",6 Related Work,[0],[0]
"Contrary to our work, these models typically coalesce all lexical relations.
",6 Related Work,[0],[0]
"Another class of models, operating at the character level, shares a similar methodology: such models compose token-level representations from subcomponent embeddings (subwords, morphemes, or characters) (dos Santos and Zadrozny, 2014; Ling et al., 2015; Cao and Rei, 2016; Kim et al., 2016;
",6 Related Work,[0],[0]
"Wieting et al., 2016; Verwimp et al., 2017, i.a.).",6 Related Work,[0],[0]
"In contrast to prior work, our model decouples the use of morphological information, now provided in the form of inflectional and derivational rules transformed into constraints, from the actual training.",6 Related Work,[0],[0]
"This pipelined approach results in a simpler, more portable model.",6 Related Work,[0],[0]
"In spirit, our work is similar to Cotterell et al. (2016b), who formulate the idea of post-training specialisation in a generative Bayesian framework.",6 Related Work,[0],[0]
Their work uses gold morphological lexicons; we show that competitive performance can be achieved using a non-exhaustive set of simple rules.,6 Related Work,[0],[0]
"Our framework facilitates the inclusion of antonyms at no extra cost and naturally extends to constraints from other sources (e.g., WordNet) in future work.",6 Related Work,[0],[0]
Another practical difference is that we focus on similarity and evaluate morph-fitting in a well-defined downstream task where the artefacts of the distributional hypothesis are known to prompt statistical system failures.,6 Related Work,[0],[0]
We have presented a novel morph-fitting method which injects morphological knowledge in the form of linguistic constraints into word vector spaces.,7 Conclusion and Future Work,[0],[0]
The method makes use of implicit semantic signals encoded in inflectional and derivational rules which describe the morphological processes in a language.,7 Conclusion and Future Work,[0],[0]
The results in intrinsic word similarity tasks show that morph-fitting improves vector spaces induced by distributional models across four languages.,7 Conclusion and Future Work,[0],[0]
"Finally, we have shown that the use of morph-fitted vectors boosts the performance of downstream language understanding models which rely on word representations as features, especially for morphologically rich languages such as German.
",7 Conclusion and Future Work,[0],[0]
"Future work will focus on other potential sources of morphological knowledge, porting the framework to other morphologically rich languages and downstream tasks, and on further refinements of the post-processing specialisation algorithm and the constraint selection.",7 Conclusion and Future Work,[0],[0]
This work is supported by the ERC Consolidator Grant LEXICAL:,Acknowledgments,[0],[0]
Lexical Acquisition Across Languages (no 648909).,Acknowledgments,[0],[0]
RR is supported by the IntelICRI grant: Hybrid Models for Minimally Supervised Information Extraction from Conversations.,Acknowledgments,[0],[0]
The authors are grateful to the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
"In this supplemental material, we provide a short comprehensive overview of simple languagespecific morphological rules in English (EN), German (DE), Italian (UT), and Russian (RU).",Morphological Rules,[0],[0]
These rules were used to build the sets of synonymous ATTRACT and antonymous REPEL constraints for our morph-fitting fine-tuning procedure.,Morphological Rules,[0],[0]
"As discussed in the paper, the linguistic constraints extracted from the rules require only a comprehensive list of vocabulary words in each language.",Morphological Rules,[0],[0]
A native speaker of each language used in our experiments is able to easily come up with these sets of morphological rules (or at least with a reasonable subset of rules) without any linguistic training.,Morphological Rules,[0],[0]
"What is more, the rules for German, Italian, and Russian were created by non-native and non-fluent speakers who have only a passive or limited knowledge of the three languages, exemplifying the simplicity and portability of the fine-tuning approach based on the shallow “morphological supervision”.",Morphological Rules,[0],[0]
"The simplicity is also confirmed by the short time used to compile the rules, ranging from a few minutes for English to approximately two hours for Russian.
",Morphological Rules,[0],[0]
"Different languages differ in their “morphological richness” (e.g., declension, verb conjugation, plural forming, gender) which consequently leads to the varying number of rules in each language.",Morphological Rules,[0],[0]
"However, all four languages in our study display morphological regularities described by simple morphological rules that are exploited to build sets of ATTRACT and REPEL linguistic constraints in each language from scratch.9
Vocabularies W in all four languages are labeled Wen, Wde, Wit, Wru.",Morphological Rules,[0],[0]
"We add the pairs (w1, w2) and (w2, w1) generated by the rules to the sets of constraints iff both w1, w2 ∈W .",Morphological Rules,[0],[0]
"After we generate all such constraints, since some constraints may have been generated by more than one rule, we remove all duplicates from the respective sets of ATTRACT and REPEL constraints.
",Morphological Rules,[0],[0]
"Before we start, we will define two simple functions: (i) the function",Morphological Rules,[0],[0]
"w[: −N ] strips the last
9Note that the rules for extracting ATTRACT constraints were additionally used to generate the Morph-SimLex evaluation set, also provided as supplemental material.
",Morphological Rules,[0],[0]
"N characters from the word w, (ii) the function w.ew(sub) tests if the word w ends with a sequence of characters sub.",Morphological Rules,[0],[0]
"For instance, create[: −1] returns creat, while create.ew(’s’) returns False and create.ew(’e’) returns True.",Morphological Rules,[0],[0]
"Inflectional Synonymy: ATTRACT As discussed in the paper, we rely on only two simple inflectional morphological rules in English: - w2 = w1 + ’s’/’ed’/’ing’.",English Rules,[0],[0]
"This rule yields constraints such as (speak, speaking), (turtle, turtles), or (clean, cleaned).",English Rules,[0],[0]
-,English Rules,[0],[0]
"If w1.ew(’e’), then w2 = w1[: −1] + ’ed’/’ing’.",English Rules,[0],[0]
"This rule yields constraints such as (create, creating), or (generate, generated).
",English Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in English: APen = {’dis’, ’il’, ’un’, ’in’, ’im’,
’ir’, ’mis’, ’non’, ’anti’}.",English Rules,[0],[0]
"We rely on the following derivational rules to extract REPEL pairs: -w2 = ap +w1, where ap ∈ APen.",English Rules,[0],[0]
"This rule yields constraints such as (mature, immature), (allow, disallow) or (regularity, irregularity).",English Rules,[0],[0]
-,English Rules,[0],[0]
"If w1.ew(’ful’), then w2 = w1[: −3] + ’less’.",English Rules,[0],[0]
"This rule yields constraints such as (cheerful, cheerless).
",English Rules,[0],[0]
"As mentioned in the paper, for all four languages we further expand the set of REPEL constraints by transitively combining antonymy pairs with inflectional ATTRACT pairs.",English Rules,[0],[0]
"In simple words, the friend of my enemy is my enemy.",English Rules,[0],[0]
"This means that, given an ATTRACT pair (allow, allows) and a REPEL pair (allow, disallow), we extract another REPEL pair (allows, disallow).",English Rules,[0],[0]
"Inflectional Synonymy: ATTRACT Being morphologically richer than English, the German language naturally requires more rules to describe its (inflectional) morphological richness and variation.",German Rules,[0],[0]
"First, we capture the regular declension of nouns and adjectives by the following heuristic: - Generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’em’/’en’/’er’/’es’}; take the Cartesian product on Ww1 ×Ww1 and then exclude (wi, wi)
pairs with identical words.",German Rules,[0],[0]
"This rule generates pairs such as (schottisch, schottische), (schottischem, schottischen).
",German Rules,[0],[0]
"The second set of rules describes regular verb morphology, i.e., verb conjugation in the present and past tense, and the formation of regular past participles.",German Rules,[0],[0]
"This set of rules may be expressed as: - If w1.ew(’en’), then w′1 = w1[: −2].",German Rules,[0],[0]
"If w′1.ew(’t’), then generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’st’/’ete’/’etest’/’etet’/’eten’, w2 = ’ge’+w′1+ ’et’}, else (if not w′1.ew(’t’)), generate a set of words Ww1 = {w1, w2|w2 = w1 + ’e’/’st’/’te’/’test’/’tet’/’ten’, w2 = ’ge’+w′1+ ’t’}.",German Rules,[0],[0]
We then take the Cartesian product on Ww1 ×Ww1 .,German Rules,[0],[0]
"Again, all pairs with identical words were discarded.",German Rules,[0],[0]
"This rule yields pairs such as (machen, machten), (mache, gemacht), (kaufst, kauft), or (arbeite, arbeitete) and (arbeiten, gearbeitet).
",German Rules,[0],[0]
"Another set of rules targets the regular formation of plural nouns: - If w1.ew(’ei’) or w1.ew(’heit’) or w1.ew(’keit’) or w1.ew(’schaft’) or w1.ew(’ung’), then w2 = w1 + ’en’.",German Rules,[0],[0]
"This rule yields pairs such as (wahrheit, wahrheiten) or (gemeinschaft, gemeinschaften).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’in’), then w2 = w1 + ’nen’.",German Rules,[0],[0]
"This rule generates pairs such as (lehrerin, lehrerinnen) or (lektorin, lektorinnen).",German Rules,[0],[0]
- Ifw1.ew(’a’/’i’/’o’/’u’/’y’),German Rules,[0],[0]
thenw2 = w1 + ’s’.,German Rules,[0],[0]
"This rule yields pairs such as (auto, autos).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’e’), then w2 = w1 + ’n’.",German Rules,[0],[0]
"This rule yields pairs such as (postkarte, postkarten).",German Rules,[0],[0]
- w2 = lumlaut(w1) +,German Rules,[0],[0]
"er, where the function lumlaut(w) replaces the last occurrence of the letter ’a’,’o’ or ’u’ with ’ä’,’ö’ or ’ü’.",German Rules,[0],[0]
"This rule generates pairs such as (wörterbuch, wörterbücher) or (stadt, städter).
",German Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in German: APde = {’un’, ’nicht’, ’anti’, ’ir’, ’in’,
’miss’}.",German Rules,[0],[0]
"We rely on the following derivational rules to extract REPEL pairs in German: - w2 = ap + w1, where ap ∈",German Rules,[0],[0]
APde.,German Rules,[0],[0]
"This rule yields constraints such as (aktiv, inaktiv), (wandelbar, unwandelbar) or (zyklone, antizyklone).",German Rules,[0],[0]
-,German Rules,[0],[0]
"If w1.ew(’voll’), then w2 = w1[: −4] + ’los’.",German Rules,[0],[0]
"This rule yields constraints such as (geschmackvoll, geschmacklos).
",German Rules,[0],[0]
"The set of REPEL is then again transitively expanded yielding pairs such as (relevant, irrelevanter) or (aktivem, inaktiv).",German Rules,[0],[0]
"Inflectional Synonymy: ATTRACT The first set of rules aims at capturing the regular plural forming in Italian (e.g., libro, libri) and regular differences in gender (e.g., rapido, rapida).",Italian Rules,[0],[0]
"We rely on the simple heuristic which can be expressed as follows: - If w1.ew(’a’/’e’/’o’/’i’), then generate a set of words Ww1 =",Italian Rules,[0],[0]
{w2|w2 = w1,Italian Rules,[0],[0]
"[: −1] + ’a’/’e’/’o’/’i’}, and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule yields pairs such as (nero, neri) or (generazione, generazioni).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ga’/’ca’), then w2 = w1 + ’he’.",Italian Rules,[0],[0]
"This rule generates pairs such as (tartaruga, tartarughe) or (bianca, bianche).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’go’), then w2 = w1 + ’hi’.",Italian Rules,[0],[0]
"This rule generates pairs such as (albergo, alberghi).
",Italian Rules,[0],[0]
The second set of rules targets regular verb conjugation in Italian and the formation of regular past participles.,Italian Rules,[0],[0]
"The following rules are used: - If w1.ew(’are’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −3] + ’iamo’/’ate’/’ano’/’o’/’i’/’a’/’ato’/’ata’/’ati’/’ate’}; take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule results in pairs such as (aspettare, aspettiamo).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ere’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −3] + ’iamo’/’ete’/’ono’/’o’/’i’/’e’/’uto’/’uta’/’uti’/’ute’}; take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Italian Rules,[0],[0]
"This rule results in pairs such as (ricevere, ricevete) or (riceve, ricevuto).",Italian Rules,[0],[0]
-,Italian Rules,[0],[0]
"If w1.ew(’ire’), then generate a set of words Ww1 = {w1, w2|w2 = w1",Italian Rules,[0],[0]
[: −3] + ’iamo’/’ite’/’ono’/’o’/’i’/’e’/’ito’/’ita’/’iti’/’ite’}; take the Cartesian product on Ww1 × Ww1 discarding pairs with identical words.,Italian Rules,[0],[0]
"This rule results in pairs such as (dormire, dormono) or (dormi, dormita).
",Italian Rules,[0],[0]
"Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes: APit = {’in’, ’ir’, ’im’, ’anti’}.",Italian Rules,[0],[0]
"The following derivational rule is used to extract REPEL pairs: - w2 = ap + w1, where ap ∈ APit.",Italian Rules,[0],[0]
"This rule yields constraints such as (attivo, inattivo) or (rispettosa, irrispettosa).
",Italian Rules,[0],[0]
"The set of REPEL was then expanded as before, e.g., with additional pairs such as (rispettosa, irrispettosi) generated.",Italian Rules,[0],[0]
Inflectional Synonymy: ATTRACT The first set of rules in Russian targets the regular forming of plural in Russian.,Russian Rules,[0],[0]
A few simple heuristics are used as follows: - w2 = w1 + ’и’/’ы’.,Russian Rules,[0],[0]
"This rule yields pairs such as (aльбом, aльбомы), transliterated as: (al’bom, al’bomy).",Russian Rules,[0],[0]
"- if w1.ew(’a’/’я’/’ь’), then w2 = w1",Russian Rules,[0],[0]
[: −1] + ’и’/’ы’.,Russian Rules,[0],[0]
"This rule generates pairs such as (песня, песни): (pesnja, pesni).",Russian Rules,[0],[0]
"- if w1.ew(’o’), then w2 = w1",Russian Rules,[0],[0]
[: −1] + ’a’.,Russian Rules,[0],[0]
"This rule generates pairs such as (письмо, письма): (pis’mo, pis’ma).",Russian Rules,[0],[0]
"- if w1.ew(’e’), then w2 = w1[: −1] + ’я’.",Russian Rules,[0],[0]
"This rule generates pairs such as (платье, платья): (plat’e, plat’ja).
",Russian Rules,[0],[0]
The next set of rules targets regular verb conjugation of Russian verbs as well as the regular formation of past participles.,Russian Rules,[0],[0]
"We again build a simple heuristic to extract ATTRACT pairs: - if w1.ew(’ти’/’ть’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’у’/’ю’/’ешь’/’ишь’/’ет’/’ит’/’ем’/’им’, w2 = w1[: −2]+ ’ете’/ите’/’ут’/’ют’/’ат’/’ят’, w2 = w1[: −2] + ’нный’/’нная’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (варить, варите) or (заканчиваю, заканчивают), transliterated as: (varit’, varite), (zakanchivaju, zakanchivajut).
",Russian Rules,[0],[0]
"Following that, we also utilise the regularities regarding declension processes in Russian, captured by the following rules: - if w1.ew(’a’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’e’/’y’/’ой’} and take the Cartesian product on Ww1 × Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работа, работой): (rabota, rabotoj).",Russian Rules,[0],[0]
"- if w1.ew(’я’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’e’/’ю’/’ей’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (линия, линию): (linija, liniju).",Russian Rules,[0],[0]
"- if w1.ew(’ы’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’ам’/’ами’/’ах’} and take the Cartesian product
on Ww1 × Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работам, работами): (rabotam, rabotami).",Russian Rules,[0],[0]
"- if w1.ew(’и’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −1] + ’ь’/’ям’/’ями’/’ях’} and take the Cartesian product on Ww1 ×Ww1 discarding pairs with identical words.",Russian Rules,[0],[0]
"This rule yields pairs such as (работам, работами): (rabotam, rabotami).
",Russian Rules,[0],[0]
"Yet another set of rules targets regular adjective comparison and gender: - if w1.ew(’ый’/’ой’/’ий’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ь’/’ее’/’ые’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (быстрый, быстрее): (bystryj, bystree).",Russian Rules,[0],[0]
"- if w1.ew(’ая’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ее’/’ыe’/’ый’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (новая, новыe): (novaja, novye).",Russian Rules,[0],[0]
"- if w1.ew(’oe’), then generate a set of words Ww1 = {w1, w2|w2 = w1[: −2] + ’ый’/’ыe’/’ая’}.",Russian Rules,[0],[0]
"This rule yields pairs such as (новое, новый): (novoe, novyj).
",Russian Rules,[0],[0]
Derivational Antonymy: REPEL We assume the following set of standard “antonymy” prefixes in Russian:,Russian Rules,[0],[0]
"APru = {не, анти’}, and simply use the following rule: - w2 = ap + w1, where ap ∈ APru.",Russian Rules,[0],[0]
"This rule yields constraints such as (адекватный, неадекватный) or (вирусная, антивирусная), transliterated as: (adekvatnyj, neadekvatnyj) and (virusnaja, antivirusnaja).
",Russian Rules,[0],[0]
"The further expansion of REPEL constraints yields pairs such as (адекватный, неадекватная): (adekvatnyj, neadekvatnaja).",Russian Rules,[0],[0]
We stress that the listed rules for all four languages are non-exhaustive and do not cover all possible inflectional and derivational morphological phenomena.,Further Discussion,[0],[0]
"More linguistic constraints may be extracted by resorting to more sophisticated rules covering finer-grained morphological processes (e.g., covering irregular plural forming or irregular verb conjugation and past participle forming, or non-standard declensions).",Further Discussion,[0],[0]
"Further, the listed rules, written by non-native speakers without any linguistic training in a very short time span, do not necessarily rely on established linguistic theories in each language, but are rather simple heuristics aiming to capture morphological regularities.",Further Discussion,[0],[0]
Morphologically rich languages accentuate two properties of distributional vector space models: 1) the difficulty of inducing accurate representations for lowfrequency word forms; and 2) insensitivity to distinct lexical relations that have similar distributional signatures.,abstractText,[0],[0]
"These effects are detrimental for language understanding systems, which may infer that inexpensive is a rephrasing for expensive or may not associate acquire with acquires.",abstractText,[0],[0]
"In this work, we propose a novel morph-fitting procedure which moves past the use of curated semantic lexicons for improving distributional vector spaces.",abstractText,[0],[0]
"Instead, our method injects morphological constraints generated using simple language-specific rules, pulling inflectional forms of the same word close together and pushing derivational antonyms far apart.",abstractText,[0],[0]
"In intrinsic evaluation over four languages, we show that our approach: 1) improves low-frequency word estimates; and 2) boosts the semantic quality of the entire word vector collection.",abstractText,[0],[0]
"Finally, we show that morph-fitted vectors yield large gains in the downstream task of dialogue state tracking, highlighting the importance of morphology for tackling long-tail phenomena in language understanding tasks.",abstractText,[0],[0]
Morph-fitting: Fine-Tuning Word Vector Spaces with Simple Language-Specific Rules,title,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 995–1000, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"SMT from a morphologically poor language like English into a language with richer morphology continues to be a problem, in particular when training data is sparse and/or the SMT system has insufficient modeling capabilities for morphological variation in the target language.",1 Introduction,[0],[0]
"Most previous approaches to this problem have utilized a translate-and-inflect method, where a first-pass SMT system is trained on lemmatized forms, and the correct inflection for every word is predicted in a second pass by statistical classifiers trained on a combination of source and target language features.",1 Introduction,[0],[0]
"This paper looks at morphological modeling from a different perspective, namely to improve SMT in a real-time speech-
to-speech translation system.",1 Introduction,[0],[0]
Our focus is on resolving those morphological translation errors that are most likely to cause confusions and misunderstandings in machine-translation mediated human-human dialogs.,1 Introduction,[0],[0]
"Due to the constraints imposed by a realtime system, previous approaches that rely on elaborate feature sets and multi-pass processing strategies are unsuitable for this problem.",1 Introduction,[0],[0]
The language pair of interest in this study is English and Iraqi Arabic (IA).,1 Introduction,[0],[0]
The latter is a spoken dialect of Arabic with few existing linguistic resources.,1 Introduction,[0],[0]
We therefore develop a low-resource approach that relies on sourceside dependency parses only.,1 Introduction,[0],[0]
We analyze its performance in combination with different types of parsers and different translation models.,1 Introduction,[0],[0]
Results show a significant improvement in translation performance in both automatic and manual evaluations.,1 Introduction,[0],[0]
"Moreover, the proposed method is sufficiently fast for a realtime system.",1 Introduction,[0],[0]
"Much work in SMT has addressed the issue of translating from morphologically-rich languages by preprocessing the source and/or target data by e.g., stemming and morphological decomposition (Popovic and Ney, 2004; Goldwater and McClosky, 2005), compound splitting (Koehn and Knight, 2003), or various forms of tokenization (Lee, 2004; Habash and Sadat, 2006).",2 Prior Work,[0],[0]
"In (Minkov et al., 2007; Toutanova et al., 2008) morphological generation was applied as a postprocessing step for translation into morphologically-rich languages.",2 Prior Work,[0],[0]
"A maximumentropy Markov model was trained to predict the correct inflection for every stemmed word in the
995
machine translation output from a first-pass system, conditioned on a set of lexical, morphological and syntactic features.",2 Prior Work,[0],[0]
"More recently, (Chahuneau et al., 2013) applied a similar translate-and-inflect approach, utilizing unsupervised in addition to supervised morphological analyses.",2 Prior Work,[0],[0]
"Inflection generation models were also used by (Fraser et al., 2012; Weller et al., 2013) for translation into German, and by (El Kholy and Habash, 2012) for Modern Standard Arabic.",2 Prior Work,[0],[0]
"(Sultan, 2011) added both syntactic information on the source side that was used in filtering the phrase table, plus postprocessing on the target side for English-Arabic translation.",2 Prior Work,[0],[0]
"Still other approaches enrich the translation system with morphology-aware feature functions or specific agreement models (Koehn and Hoang, 2007; Green and DeNero, 2012; Williams and Koehn, 2011).
",2 Prior Work,[0],[0]
"In contrast to the above studies, which have concentrated on text translation, this paper focuses on spoken language translation within a bilingual human-human dialog system.",2 Prior Work,[0],[0]
"Thus, our main goal is not to predict the correct morphological form of every word, but to prevent communication errors resulting from the mishandling of morphology.",2 Prior Work,[0],[0]
The intended use in a real-time dialog system imposes additional constraints on morphological modeling: any proposed approach should not add a significant computational burden to the overall system that might result in delays in translation or response generation.,2 Prior Work,[0],[0]
"Our goal is also complicated by the fact that our target language is a spoken dialect of Arabic, for which few linguistic resources (training data, lexicons, morphological analyzers) exist.",2 Prior Work,[0],[0]
"Lastly, Arabic written forms are morphologically highly ambiguous due to the lack of short vowel markers that signal grammatical categories.",2 Prior Work,[0],[0]
The first step in the dialog system used for this study consists of an automatic speech recognition (ASR) component that produces ASR hypotheses for the user’s speech input.,3 Dialog System and Analysis,[0],[0]
Several error detection modules then identify likely out-of-vocabulary and misrecognized words.,3 Dialog System and Analysis,[0],[0]
"This information is used by a clarification module that asks the user to rephrase these error segments; another module then combines the user’s answers into a merged, corrected representa-
tion before sending it to the translation engine.",3 Dialog System and Analysis,[0],[0]
"A machine translation error detection module analyzes the translation to check for errors, such as unknown words.",3 Dialog System and Analysis,[0],[0]
"If an error is found, another clarification subdialog is initiated; otherwise, the translation is sent to a text-to-speech engine to produce the acoustic output in the other language.",3 Dialog System and Analysis,[0],[0]
A schematic representation is shown in Figure 1.,3 Dialog System and Analysis,[0],[0]
"More details about the system can be found in (et al., 2013).",3 Dialog System and Analysis,[0],[0]
The system was evaluated in live mode with native IA speakers as part of the DARPA BOLT Phase-II benchmark evaluations.,3 Dialog System and Analysis,[0],[0]
The predefined scenarios included military and humanitarian assistance/disaster relief scenarios as well as general topics.,3 Dialog System and Analysis,[0],[0]
"All system interactions were logged and evaluated by bilingual human assessors.
",3 Dialog System and Analysis,[0],[0]
"During debriefing sessions with the users, some users voiced dissatisfaction with the translation quality, and a subsequent detailed error analysis was conducted on the logs of 30 interactions.",3 Dialog System and Analysis,[0],[0]
"Similar to previous studies (Condon et al., 2010)",3 Dialog System and Analysis,[0],[0]
we found that a frequently recurring problem was wrong morphological verb forms in the IA output.,3 Dialog System and Analysis,[0],[0]
Some examples are shown in Table 1.,3 Dialog System and Analysis,[0],[0]
"In Example 1, to make sure should be translated by a first-person plural verb but it is translated by a second-person plural form, changing the meaning to (you (pl.) make sure).",3 Dialog System and Analysis,[0],[0]
The desired verb form would be ntAkd.,3 Dialog System and Analysis,[0],[0]
"Similarly, in Example 2 the translation of transport should agree with the translations of someone and the preceding
auxiliary verb can (yqdr).",3 Dialog System and Analysis,[0],[0]
The correct form would be yqlk (he/she transports you) instead of nqlk (we transport you).,3 Dialog System and Analysis,[0],[0]
Such translation errors are confusing to users as they affect the understanding of basic semantic roles.,3 Dialog System and Analysis,[0],[0]
They tend to occur when translating English infinitival constructions (to+verb) or other syntactic constructions where English base verb forms need to be translated by a finite verb in IA.,3 Dialog System and Analysis,[0],[0]
"In these cases, explicit morphological features like person and number are required in Arabic but they are lacking in the English input.",3 Dialog System and Analysis,[0],[0]
An analysis of the SMT component showed that morphological translation errors primarily occur when a head word and its dependent (such as a verbal head and its subject noun dependent) are translated as part of different phrases or rules.,4 Approach,[0],[0]
"In that case, insufficient context is available to produce the correct translation.",4 Approach,[0],[0]
Our approach is to annotate syntactic dependencies on the source side using a statistical parser.,4 Approach,[0],[0]
"Based on the resulting dependency structures the source-side data is then tagged with explicit morphological verbal features using deterministic rules (e.g., subject nouns assign their person/number features to their verbal heads), and a new translation model is trained on this data.",4 Approach,[0],[0]
Our assumption is that words tagged with explicit morphological features will be aligned with their correct translations during training and will thus produce correctly inflected forms during testing even when the syntactic context is not available in the same phrase/rule.,4 Approach,[0],[0]
"For instance, the input sentence in Example 1 in Table 1 would be annotated as: you need-2sg to tell-2sg the locals to evacuate-3pl the area",4 Approach,[0],[0]
so we can-1pl secure-1pl the area to make1pl sure no one gets-3sg hurt.,4 Approach,[0],[0]
"This approach avoids the costly extraction of multiple features, subsequent statistical classification, and inflection generation during run time; moreover, it
does not require target-side annotation tools, an advantage when dealing with under-resourced spoken dialects.",4 Approach,[0],[0]
"There are, however, several potential issues with this approach.",4 Approach,[0],[0]
"First, introducing tags fragments the training data: the same word may receive multiple different tags, either due to genuine ambiguity or because of parser errors.",4 Approach,[0],[0]
"As a result, word alignment and phrase extraction may suffer from data sparsity.",4 Approach,[0],[0]
"Second, new word-tag combinations in the test data that were not observed in the training data will not have an existing translation.",4 Approach,[0],[0]
"Third, the performance of the model is highly dependent on the accuracy of the parser.",4 Approach,[0],[0]
"Finally, we make the assumption that the expression of person and number categories are matched across source and target language – in practice, we have indeed seen very few mismatched cases where e.g., a singular noun phrase in English is translated by a plural noun phrase in IA (see Section 6 below).
",4 Approach,[0],[0]
To address the first point the morph-tagged translation model can be used in a backoff procedure rather than as an alternative model.,4 Approach,[0],[0]
"In this case the baseline model is used by default, and the morphtagged model is only used whenever heads and dependents are translated as part of different phrases.",4 Approach,[0],[0]
Unseen translations for particular word-tag combinations in the test set could in principle be addressed by using a morphological analyzer to generate novel word forms with the desired inflections.,4 Approach,[0],[0]
"However, this would require identifying the correct stem for the word in question, generating all possible morphological forms, and either selecting one or providing all options to the SMT system, which again increases system load.",4 Approach,[0],[0]
We analyzed unseen word-tag combination in the test data but found that their percentage was very small (< 1%).,4 Approach,[0],[0]
"Thus, for these forms we back off to the untagged counterparts rather than generating new inflected forms.",4 Approach,[0],[0]
"To obtain better insight into the effect of parsing accuracy we compared the performance of two parsers in our
annotation pipeline: the Stanford parser (de Marneffe et al., 2006) (version 3.3.1) and the Macaon parser (Nasr et al., 2014).",4 Approach,[0],[0]
"The latter is an implementation of graph-based parsing (McDonald et al., 2005) where a projective dependency tree maximizing a score function is sought in the graph of all possible trees using dynamic programming.",4 Approach,[0],[0]
"It uses a 1st-order decoder, which is more robust to speech input as well as out-of-domain training data.",4 Approach,[0],[0]
"The features implemented reflect those of (Bohnet, 2010) (based on lexemes and part-of-speech tags).",4 Approach,[0],[0]
"The parser was trained on Penn-Treebank data transformed to match speech (lower-cased, no punctuation), with one iteration of self-training on the Transtac training set.",4 Approach,[0],[0]
"We also use the combination of both parsers, where source words are only tagged if the tags derived independently from each parser agree with each other.",4 Approach,[0],[0]
Development experiments were carried out on the Transtac corpus of dialogs in the military and medical domain.,5 Data and Baseline Systems,[0],[0]
"The number of sentence pairs is 762k for the training set, 6.9k for the dev set, 2.8k for eval set 1, and 1.8k for eval set 2.",5 Data and Baseline Systems,[0],[0]
"Eval set 1 has one reference per sentence, eval set 2 has four references.",5 Data and Baseline Systems,[0],[0]
"For the development experiments we used a phrase-based Moses SMT system with a hierarchical reordering model, tested on Eval set 1.",5 Data and Baseline Systems,[0],[0]
The language model was a backoff 6-gram model trained using Kneser-Ney discounting and interpolation of higher- and lower-order n-grams.,5 Data and Baseline Systems,[0],[0]
In addition to automatic evaluation we performed manual analyses of the accuracy of verbal features in the IA translations on a subset of 65 sentences (containing 143 verb forms) from the live evaluations described above.,5 Data and Baseline Systems,[0],[0]
"This analysis counts a verb form as correct if its morphological features for person and number are correct, although it may have the wrong lemma (e.g., wrong word sense).",5 Data and Baseline Systems,[0],[0]
The development experiments were designed to identify the setup that produces the highest verbal inflection accuracy.,5 Data and Baseline Systems,[0],[0]
"For final testing we used a more advanced SMT engine on Eval set 2.This system is the one used in the real-time dialog system; it contains a hierarchical phrase-based translation model, sparse features, and a neural network joint model (NNJM) (Devlin et al., 2014).",5 Data and Baseline Systems,[0],[0]
"Results in Table 2 show the comparison between the baseline, different parsers, and the combined system.",6 Experiments and Results,[0],[0]
We see that verbal inflection accuracy increases substantially from the baseline performance and is best for the Macaon parser.,6 Experiments and Results,[0],[0]
"Improvements over the baseline system without morphology are statistically significant; differences between the individual parsers are not (not, however, that the sample size for manual evaluation was quite small).
",6 Experiments and Results,[0],[0]
"BLEU is not affected negatively but even increases slightly - thus, data fragmentation does not seem to be a problem overall.",6 Experiments and Results,[0],[0]
"This may be due to the nature of the task and domain, which is results in fairly short, simple sentence constructions that can be adequately translated by a concatenation of shorter phrases rather than requiring longer phrases.",6 Experiments and Results,[0],[0]
Back-off systems (indicated by bo) and the combined system improve BLEU only trivially while decreasing verbal inflection accuracy by varying amounts.,6 Experiments and Results,[0],[0]
For testing within the dialog system we thus choose the Macaon parser and utilize a standard translation model rather than a backoff model.,6 Experiments and Results,[0],[0]
An added benefit is that the Macaon parser is already used in other components in the dialog system.,6 Experiments and Results,[0],[0]
"Using this setup we ran two experiments with dialog system’s SMT engine: first, we re-extracted phrases and rules based on the morph-tagged data and reoptimized the feature weights.",6 Experiments and Results,[0],[0]
"In the second experiment, we additionally applied the NNJM to the morph-tagged source text.",6 Experiments and Results,[0],[0]
To this end we include all the morphological variants of the original vocabulary that was used for the NNJM in the untagged baseline system.,6 Experiments and Results,[0],[0]
Table 3 shows the results.,6 Experiments and Results,[0],[0]
"The morph-tagged data improves the BLEU score under both conditions: in Experiment 1, the improve-
ment is almost a full BLEU point (0.91); in Experiment 2 the improvement is even larger (1.13), even though the baseline performance is stronger.",6 Experiments and Results,[0],[0]
"Both results are statistically significant at p = 0.05, using a paired bootstrap resampling test.",6 Experiments and Results,[0],[0]
"The combination of morph-tagged data and the more advanced modeling options (sparse features, NNJM) in this system seem to be beneficial.",6 Experiments and Results,[0],[0]
Improved translation performance may also be captured by the four reference translations as opposed to one in Eval set 1.,6 Experiments and Results,[0],[0]
"In order to assess the added computation cost
of our procedure we computed the decoding speed of the MT component in the dialog system for both the baseline and the morpho-tag systems.",6 Experiments and Results,[0],[0]
"In the baseline MT system (with NNJM) without morphotags, decoding takes 0.01572 seconds per word or 0.15408 seconds per sentence – these numbers were obtained on a Dell Precision M4800",6 Experiments and Results,[0],[0]
Laptop with a quad-core Intel i7-4930MX Processor and 32GB of RAM.,6 Experiments and Results,[0],[0]
Morpho-tagging only adds 0.00031 seconds per word or 0.0024 seconds per sentence.,6 Experiments and Results,[0],[0]
"Thus, our procedure is extremely efficient.
",6 Experiments and Results,[0],[0]
"An analysis of the remaining morphological translation errors not captured by our approach showed that in about 34% of all cases these were due to part-of-speech tagging or parser errors, i.e. verbs were mistagged as nouns rather than verbs and thus did not receive any morphological tags, or the parser hypothesized wrong dependency relations.",6 Experiments and Results,[0],[0]
In 53% of the cases the problem is the lack of more extensive discourse or contextual knowledge.,6 Experiments and Results,[0],[0]
"This includes constructions where there is no overt subject for a verb in the current utterance, and the appropriate underlying subject must be inferred from the preceding discourse or from knowledge of the situational context.",6 Experiments and Results,[0],[0]
"This is an instance of the more general problem of control (see e.g.,(Landau, 2013) for an overview of research in this area).",6 Experiments and Results,[0],[0]
It is exemplified by cases such as the following: 1.,6 Experiments and Results,[0],[0]
"The first step is to make sure that all personnel
are in your debrief.",6 Experiments and Results,[0],[0]
"Here, the underlying subject of “to make sure” could be a range of different candidates (I, you, we, etc.) and must be inferred from context.",6 Experiments and Results,[0],[0]
2.,6 Experiments and Results,[0],[0]
I can provide up to one platoon to help you guys cordon off the area.,6 Experiments and Results,[0],[0]
"In this case the statistical parser identified I as the subject of help, but platoon is more likely to be the controller and was in fact identified as the underlying subject by the annotator.",6 Experiments and Results,[0],[0]
"Such cases could potentially be resolved during the parsing step by integrating semantic information, e.g. as in (Bansal et al., 2014).",6 Experiments and Results,[0],[0]
"However, initial investigations with semantic features in the Macaon parser resulted in a significant slow-down of the parser.",6 Experiments and Results,[0],[0]
"In other cases, more sophisticated modeling of the entities and their relationships in the situational context will be required.",6 Experiments and Results,[0],[0]
"This clearly is an area for future study.
",6 Experiments and Results,[0],[0]
"Finally, in 13% of the cases, mistranslations are caused by a mismatch of number features across languages (e.g. number features for nouns such as family or people).",6 Experiments and Results,[0],[0]
We have shown that significant gains in BLEU and verbal inflection accuracy in speech-to-speech translation for English-IA can be achieved by incorporating morphological tags derived from dependency parse information in the source language.,7 Conclusion,[0],[0]
"The proposed method is fast, low-resource, and can easily be incorporated into a real-time dialog system.",7 Conclusion,[0],[0]
It adds negligible computational cost and does not require any target-language specific annotation tools.,7 Conclusion,[0],[0]
"Possible areas for future study include the use of discourse or and other contextual information to determine morphological agreement, application to other languages pairs/morphological agreement types, and learning the annotation rules from data.",7 Conclusion,[0],[0]
This study was funded by the Defense Advanced Research Projects Agency (DARPA) under contract HR0011-12-C-0016 - subcontract 19-000234.,Acknowledgments,[0],[0]
This paper addresses the problem of morphological modeling in statistical speech-tospeech translation for English to Iraqi Arabic.,abstractText,[0],[0]
An analysis of user data from a real-time MT-based dialog system showed that generating correct verbal inflections is a key problem for this language pair.,abstractText,[0],[0]
We approach this problem by enriching the training data with morphological information derived from sourceside dependency parses.,abstractText,[0],[0]
We analyze the performance of several parsers as well as the effect on different types of translation models.,abstractText,[0],[0]
"Our method achieves an improvement of more than a full BLEU point and a significant increase in verbal inflection accuracy; at the same time, it is computationally inexpensive and does not rely on target-language linguistic tools.",abstractText,[0],[0]
Morphological Modeling for Machine Translation of English-Iraqi Arabic Spoken Dialogs,title,[0],[0]
"ar X
iv :1
91 1.
04 91
6v 2
[ cs
.C L
] 1
2 Fe
b 20
21 Appeared in the proceedings of EMNLP 2016 (Austin, November). This version was
Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology. In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.1",text,[0],[0]
"In NLP, supervised morphological segmentation has typically been viewed as either a sequence-labeling or a segmentation task (Ruokolainen et al., 2016).",1 Introduction,[0],[0]
"In contrast, we consider a hierarchical approach, employing a context-free grammar (CFG).",1 Introduction,[0],[0]
"CFGs provide a richer model of morphology: They capture (i) the intuition that words themselves have internal constituents, which belong to different categories, as well as (ii) the order in which affixes are attached.",1 Introduction,[0],[0]
"Moreover, many morphological processes, e.g., compounding and reduplication, are best modeled as hierarchical; thus, context-free models are expressively more appropriate.
",1 Introduction,[0],[0]
"The purpose of morphological segmentation is to decompose words into smaller units, known as morphemes, which are typically taken to be the smallest meaning-bearing units in language.
",1 Introduction,[0],[0]
"1We found post publication that CELEX (Baayen et al., 1993) has annotated words for hierarchical morphological segmentation as well.
",1 Introduction,[0],[0]
This work concerns itself with modeling hierarchical structure over these morphemes.,1 Introduction,[0],[0]
Note a simple flat morphological segmentation can also be straightforwardly derived from the CFG parse tree.,1 Introduction,[0],[0]
"Segmentations have found use in a diverse set of NLP applications, e.g., automatic speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014), machine translation (Clifton and Sarkar, 2011) and parsing (Seeker and Çetinoğlu, 2015).",1 Introduction,[0],[0]
"In contrast to prior work, we focus on canonical segmentation, i.e., we seek to jointly model orthographic changes and segmentation.",1 Introduction,[0],[0]
"For instance, the canonical segmentation of untestably is un+test+able+ly, where we map ably to able+ly, restoring the letters le.
",1 Introduction,[0],[0]
We make two contributions: (i) We introduce a joint model for canonical segmentation with a CFG backbone.,1 Introduction,[0],[0]
We experimentally show that this model outperforms a semi-Markov model on flat segmentation.,1 Introduction,[0],[0]
"(ii) We release the first morphology treebank, consisting of 7454 English word types, each annotated with a full constituency parse.",1 Introduction,[0],[0]
Why should we analyze morphology hierarchically?,2 The Case For Hierarchical Structure,[0],[0]
"It is true that we can model much of morphology with finite-state machinery (Beesley and Karttunen, 2003), but there are, nevertheless, many cases where hierarchical structure appears requisite.",2 The Case For Hierarchical Structure,[0],[0]
"For instance, the flat segmentation of the word untestably7→un+test+able+ly is missing important information about how the word was derived.",2 The Case For Hierarchical Structure,[0],[0]
"The correct parse [[un[[test]able]]ly], on the other hand, does tell us that this is the order in which the complex form was derived:
test able 7−−→testable un 7−→untestable ly 7−→untestably.
",2 The Case For Hierarchical Structure,[0],[0]
"This gives us insight into the structure of the
lexicon—we expect that the segment testable exists as an independent word, but ably does not.
",2 The Case For Hierarchical Structure,[0],[0]
"Moreover, a flat segmentation is often semantically ambiguous.",2 The Case For Hierarchical Structure,[0],[0]
There are two potentially valid readings of untestably depending on how the negative prefix un scopes.,2 The Case For Hierarchical Structure,[0],[0]
The correct tree (see Figure 1) yields the reading “in the manner of not able to be tested.”,2 The Case For Hierarchical Structure,[0],[0]
A second—likely infelicitous reading—where the segment untest forms a constituent yields the reading “in a manner of being able to untest.”,2 The Case For Hierarchical Structure,[0],[0]
"Recovering the hierarchical structure allows us to select the correct reading; note there are even cases of true ambiguity; e.g., unlockable has two readings: “unable to be locked” and “able to be unlocked.”
",2 The Case For Hierarchical Structure,[0],[0]
"We also note that theoretical linguists often implicitly assume a context-free treatment of word formation, e.g., by employing brackets to indicate different levels of affixation.",2 The Case For Hierarchical Structure,[0],[0]
"Others have explicitly modeled word-internal structure with grammars (Selkirk, 1982; Marvin, 2002).",2 The Case For Hierarchical Structure,[0],[0]
"A novel component of this work is the development of a discriminative parser (Finkel et al., 2008; Hall et al., 2014) for morphology.",3 Parsing the Lexicon,[0],[0]
"The goal is to define a probability distribution over all trees that could arise from the input word, after reversal of orthographic and phonological processes.",3 Parsing the Lexicon,[0],[0]
We employ the simple grammar shown in Table 1.,3 Parsing the Lexicon,[0],[0]
"Despite its simplicity, it models the order in which morphemes are attached.
",3 Parsing the Lexicon,[0],[0]
"More formally, our goal is to map a surface form w (e.g., w=untestably) into its underlying canonical form u (e.g., u=untestablely) and then into a parse tree t over its morphemes.",3 Parsing the Lexicon,[0],[0]
"We assume u,w ∈ Σ∗, for some discrete alphabet Σ.2 Note
2For efficiency, we assume u ∈ Σ|w|+k , k = 5.
that a parse tree over the string implicitly defines a flat segmentation given our grammar—one can simply extract the characters spanned by all preterminals in the resulting tree.",3 Parsing the Lexicon,[0],[0]
"Before describing the joint model in detail, we first consider its pieces individually.",3 Parsing the Lexicon,[0],[0]
"To extract a canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016), we restore orthographic changes that occur during word formation.",3.1 Restoring Orthographic Changes,[0],[0]
"To this end, we define the score function
scoreη(u, a,w) = exp ( g(u, a,w)⊤η )
(1)
where a is a monotonic alignment between the strings u and w.",3.1 Restoring Orthographic Changes,[0],[0]
"The goal is for scoreη to assign higher values to better matched pairs, e.g., (w=untestably, u=untestablely).",3.1 Restoring Orthographic Changes,[0],[0]
"We refer to Dreyer et al. (2008) for a thorough exposition.
",3.1 Restoring Orthographic Changes,[0],[0]
"For ease of computation, we can encode this function as a weighted finite-state machine (WFST) (Mohri et al., 2002).",3.1 Restoring Orthographic Changes,[0],[0]
"This requires, however, that the feature function g factors over the topology of the finite-state encoding.",3.1 Restoring Orthographic Changes,[0],[0]
"Since our model conditions on the word w, the feature function g can extract features from any part of this string.",3.1 Restoring Orthographic Changes,[0],[0]
"Features on the output string, u, however, are more restricted.",3.1 Restoring Orthographic Changes,[0],[0]
"In this work, we employ a bigram model over output characters.",3.1 Restoring Orthographic Changes,[0],[0]
This implies that each state remembers exactly one character: the previous one.,3.1 Restoring Orthographic Changes,[0],[0]
See Cotterell et al. (2014) for details.,3.1 Restoring Orthographic Changes,[0],[0]
We can compute the score for two strings u and w using a weighted generalization of the Levenshtein algorithm.,3.1 Restoring Orthographic Changes,[0],[0]
"Computing the partition function requires a different dynamic program, which runs in O(|w|2 · |Σ|2) time.",3.1 Restoring Orthographic Changes,[0],[0]
"Note that since |Σ| ≈ 26 (lower case English letters), it takes
roughly 262 = 676 times longer to compute the partition function than to score a pair of strings.
",3.1 Restoring Orthographic Changes,[0],[0]
"Our model includes several simple feature tem-
plates, including features that fire on individual edit actions as well as conjunctions of edit actions and characters in the surrounding context.",3.1 Restoring Orthographic Changes,[0],[0]
See Cotterell et al. (2016) for details.,3.1 Restoring Orthographic Changes,[0],[0]
"Next, we need to score an underlying canonical form (e.g., u=untestablely) together with a parse tree (e.g., t=[[un[[test]able]]ly]).",3.2 Morphological Analysis as Parsing,[0],[0]
"Thus, we define the parser score with the following function
scoreω(t, u) = exp


∑
π∈Π(t)
f(π, u)⊤ω

 (2)
where Π(t) is the set of anchored productions in the tree t. An anchored production π is a grammar rule in Chomsky normal form attached to a span, e.g., Ai,k → Bi,jCj,k.",3.2 Morphological Analysis as Parsing,[0],[0]
"Each π is then assigned a weight by the linear function f(π, u)⊤ω, where the function f extracts relevant features from the anchored production as well as the corresponding span of the underlying form u.",3.2 Morphological Analysis as Parsing,[0],[0]
"This model is typically referred to as a weighted CFG (WCFG) (Smith and Johnson, 2007) or a CRF parser.
",3.2 Morphological Analysis as Parsing,[0],[0]
"For f , we define three span features: (i) indicator features on the span’s segment, (ii) an indicator feature that fires if the segment appears in an external corpus3 and (iii) the conjunction of the segment with the label (e.g., PREFIX) of the subtree root.",3.2 Morphological Analysis as Parsing,[0],[0]
"Following Hall et al. (2014), we employ an indicator feature for each production as well as production backoff features.",3.2 Morphological Analysis as Parsing,[0],[0]
"Our complete model is a joint CRF (Koller and Friedman, 2009) where each of
3We use the Wikipedia dump from 2016-05-01.
",4 A Joint Model,[0],[0]
the above scores are factors.,4 A Joint Model,[0],[0]
"We define the following probability distribution over trees, canonical forms and their alignments to the original word
pθ(t,a, u | w) = (3)
1
Zθ(w) scoreω(t, u) · scoreη(u, a,w)
where θ = {ω,η} is the parameter vector and the normalizing partition function as
Zθ(w) = ∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
(4)
∑
t′∈T (u′)
scoreω(t ′, u′) · scoreη(u ′, a, w)
where T (u) is the set of all parse trees for the string u.",4 A Joint Model,[0],[0]
"This involves a sum over all possible underlying orthographic forms and all parse trees for those forms.
",4 A Joint Model,[0],[0]
The joint approach has the advantage that it allows both factors to work together to influence the choice of the underlying form u.,4 A Joint Model,[0],[0]
This is useful as the parser now has access to which words are attested in the language; this helps guide the relatively weak transduction model.,4 A Joint Model,[0],[0]
"On the downside, the partition function Zθ now involves a sum over all strings in Σ|w|+k and all possible parses of each string!",4 A Joint Model,[0],[0]
"Finally, we define the marginal distribution over trees and underlying forms as
pθ(t, u | w) = ∑
a∈A(u,w)
pθ(t, a, u | w) (5)
where A(u,w) is the set of all monotonic alignments between u and w.",4 A Joint Model,[0],[0]
The marginalized form in eq. (5) is our final model of morphological segmentation since we are not interested in the latent alignments a.,4 A Joint Model,[0],[0]
"We use stochastic gradient descent to optimize the log-probability of the training data ∑N
n=1 log pθ(t (n), u(n) | w(n)); this requires the computation of the gradient of the partition function ∇θ logZθ.",4.1 Learning and Inference,[0],[0]
"We may view this gradient as an expectation:
∇θ logZθ(w) =",4.1 Learning and Inference,[0],[0]
"(6)
E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤


We provide the full derivation in Appendix A with an additional Rao-Blackwellization step that we make use of in the implementation.",4.1 Learning and Inference,[0],[0]
"While the sum over all underlying forms and trees in eq. (6) may be achieved in polynomial time (using the Bar-Hillel construction), we make use of an importance-sampling estimator, derived by Cotterell et al. (2016), which is faster in practice.",4.1 Learning and Inference,[0],[0]
"Roughly speaking, we approximate the hard-tosample-from distribution pθ by taking samples from an easy-to-sample-from proposal distribution q. Specifically, we employ a pipeline model for q consisting of WFST and then a WCFG sampled from consecutively.",4.1 Learning and Inference,[0],[0]
We then reweight the samples using the unnormalized score from pθ.,4.1 Learning and Inference,[0],[0]
"Importance sampling has found many uses in NLP ranging from language modeling (Bengio et al., 2003) and neural MT (Jean et al., 2015) to parsing (Dyer et al., 2016).",4.1 Learning and Inference,[0],[0]
"Due to a lack of space, we omit the derivation of the importance-sampled approximate gradient.",4.1 Learning and Inference,[0],[0]
We also decode by importance sampling.,4.2 Decoding,[0],[0]
"Given w, we sample canonical forms u and then run the CKY algorithm to get the highest scoring tree.",4.2 Decoding,[0],[0]
We believe our attempt to train discriminative grammars for morphology is novel.,5 Related Work,[0],[0]
"Nevertheless, other researchers have described parsers for morphology.",5 Related Work,[0],[0]
Most of this work is unsupervised: Johnson et al. (2007) applied a Bayesian PCFG to unsupervised morphological segmentation.,5 Related Work,[0],[0]
"Similarly, Adaptor Grammars (Johnson et al., 2006), a non-parametric Bayesian generalization of PCFGs, have been applied to the unsupervised version of the task (Botha and Blunsom, 2013; Sirts and Goldwater, 2013).",5 Related Work,[0],[0]
"Relatedly, Schmid (2005) performed unsupervised disambiguation of a German morphological analyzer (Schmid et al., 2004) using a PCFG, using the inside-outside algo-
rithm (Baker, 1979).",5 Related Work,[0],[0]
"Also, discriminative parsing approaches have been applied to the related problem of Chinese word segmentation (Zhang et al., 2014).",5 Related Work,[0],[0]
"Supervised morphological segmentation has historically been treated as a segmentation problem, devoid of hierarchical structure.",6 Morphological Treebank,[0],[0]
A core reason behind this is that—to the best of our knowledge— there are no hierarchically annotated corpora for the task.,6 Morphological Treebank,[0],[0]
"To remedy this, we provide tree annotations for a subset of the English portion of CELEX (Baayen et al., 1993).",6 Morphological Treebank,[0],[0]
We reannotated 7454 English types with a full constituency parse.4,6 Morphological Treebank,[0],[0]
The resource will be freely available for future research.,6 Morphological Treebank,[0],[0]
The annotation of the morphology treebank was guided by three core principles.,6.1 Annotation Guidelines,[0],[0]
The first principle concerns productivity: we exclusively annotate productive morphology.,6.1 Annotation Guidelines,[0],[0]
"In the context of morphology, productivity refers to the degree that native speakers actively employ the affix to create new words (Aronoff, 1976).",6.1 Annotation Guidelines,[0],[0]
"We believe that for NLP applications, we should focus on productive affixation.",6.1 Annotation Guidelines,[0],[0]
"Indeed, this sets our corpus apart from many existing morphologically annotated corpora such as CELEX.",6.1 Annotation Guidelines,[0],[0]
"For example, CELEX contains warmth7→warm+th, but th is not a productive suffix and cannot be used to create new words.",6.1 Annotation Guidelines,[0],[0]
"Thus, we do not want to analyze hearth7→hear+th or, in general, allow wug7→wug+th.",6.1 Annotation Guidelines,[0],[0]
"Second, we annotate for semantic coherence.",6.1 Annotation Guidelines,[0],[0]
"When there are several candidate parses, we choose the one that is best compatible with the compositional semantics of the derived form.
",6.1 Annotation Guidelines,[0],[0]
"Interestingly, multiple trees can be considered valid depending on the linguistic tier of interest.",6.1 Annotation Guidelines,[0],[0]
Consider the word unhappier.,6.1 Annotation Guidelines,[0],[0]
"From a semantic
4In many cases, we corrected the flat segmentation as well.
perspective, we have the parse",6.1 Annotation Guidelines,[0],[0]
[[un [happy]] er] which gives us the correct meaning “not happy to a greater degree.”,6.1 Annotation Guidelines,[0],[0]
"However, since the suffix er only attaches to mono- and bisyllabic words, we get [un[[happy] er]] from a phonological perspective.",6.1 Annotation Guidelines,[0],[0]
"In the linguistics literature, this problem is known as the bracketing paradox (Pesetsky, 1985; Embick, 2015).",6.1 Annotation Guidelines,[0],[0]
"We annotate exclusively at the syntactic-semantic tier.
",6.1 Annotation Guidelines,[0],[0]
"Thirdly, in the context of derivational morphology, we force spans to be words themselves.",6.1 Annotation Guidelines,[0],[0]
"Since derivational morphology—by definition—forms new words from existing words (Lieber and Štekauer, 2014), it follows that each span rooted with WORD or ROOT in the correct parse corresponds to a word in the lexicon.",6.1 Annotation Guidelines,[0],[0]
"For example, consider unlickable.",6.1 Annotation Guidelines,[0],[0]
"The correct parse, under our scheme, is [un [[lick] able]].",6.1 Annotation Guidelines,[0],[0]
"Each of the spans (lick, lickable and unlickable) exists as a word.",6.1 Annotation Guidelines,[0],[0]
"By contrast, the parse [[un [lick]] able] contains the span unlick, which is not a word in the lexicon.",6.1 Annotation Guidelines,[0],[0]
"The span in the segmented form may involve changes, e.g., [un [[achieve] able]], where achieveable is not a word, but achievable (after deleting e) is.",6.1 Annotation Guidelines,[0],[0]
We run a simple experiment to show the empirical utility of parsing words—we compare a WCFG-based canonical segmenter with the semiMarkov segmenter introduced in Cotterell et al. (2016).,7 Experiments,[0],[0]
We divide the corpus into 10 distinct train/dev/test splits with 5454 words for train and 1000 for each of dev and test.,7 Experiments,[0],[0]
"We report three evaluation metrics: full form accuracy, morpheme F1 (Van den Bosch and Daelemans, 1999) and average edit distance to the gold segmentation with boundaries marked by a distinguished symbol.",7 Experiments,[0],[0]
"For the WCFG model, we also report constituent F1— typical for sentential constituency parsing— as a baseline for future systems.",7 Experiments,[0],[0]
This F1 measures how well we predict the whole tree (not just a segmentation).,7 Experiments,[0],[0]
"For all models, we use L2 regularization and run 100 epochs of ADAGRAD (Duchi et al., 2011) with early stopping.",7 Experiments,[0],[0]
"We tune the regularization coefficient by grid search considering λ ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5}.",7 Experiments,[0],[0]
Table 2 shows the results.,7.1 Results and Discussion,[0],[0]
"The hierarchical WCFG model outperforms the flat semi-Markov model on
all metrics on the segmentation task.",7.1 Results and Discussion,[0],[0]
"This shows that modeling structure among the morphemes, indeed, does help segmentation.",7.1 Results and Discussion,[0],[0]
The largest improvements are found under the morpheme F1 metric (≈ 6.5 points).,7.1 Results and Discussion,[0],[0]
"In contrast, accuracy improves by < 1%.",7.1 Results and Discussion,[0],[0]
Edit distance is in between with an improvement of 0.2 characters.,7.1 Results and Discussion,[0],[0]
"Accuracy, in general, is an all or nothing metric since it requires getting every canonical segment correct.",7.1 Results and Discussion,[0],[0]
"Morpheme F1, on the other hand, gives us partial credit.",7.1 Results and Discussion,[0],[0]
"Thus, what this shows us is that the WCFG gets a lot more of the morphemes in the held-out set correct, even if it only gets a few more complete forms correct.",7.1 Results and Discussion,[0],[0]
We provide additional results evaluating the entire tree with constituency F1 as a future baseline.,7.1 Results and Discussion,[0],[0]
We presented a discriminative CFG-based model for canonical morphological segmentation and showed empirical improvements on its ability to segment words under three metrics.,8 Conclusion,[0],[0]
We argue that our hierarchical approach to modeling morphemes is more often appropriate than the traditional flat segmentation.,8 Conclusion,[0],[0]
"Additionally, we have annotated 7454 words with a morphological constituency parse.",8 Conclusion,[0],[0]
"The corpus is available online at
http://ryancotterell.github.io/data/morphological-treebank to allow for exact comparison and to spark future research.",8 Conclusion,[0],[0]
The first author was supported by a DAAD LongTerm Research Grant and an NDSEG fellowship.,Acknowledgements,[0],[0]
The third author was supported by DFG (SCHU 2246/10-1).,Acknowledgements,[0],[0]
"Here we provide the gradient of the log-partition function as an expectation:
∇θ logZθ(w) = 1
Zθ(w) ∇θZθ(w) (7)
= 1
Zθ(w) ∇θ


∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreω(t ′, u′) · scoreη(u ′, a, w)


",A Derivation of Eq. 6,[0],[0]
"= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
∇θ",A Derivation of Eq. 6,[0],[0]
"( scoreω(t ′, u′) · scoreη(u ′, a, w) )
= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
(
scoreη(u ′, a, w) ·",A Derivation of Eq. 6,[0],[0]
"∇ωscoreω(t ′, u′)
+ scoreω(t ′, u′) · ∇ηscoreη(u ′, a, w) )
",A Derivation of Eq. 6,[0],[0]
"= 1
Zθ(w)
∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreη(u ′, a, w) ·",A Derivation of Eq. 6,[0],[0]
"scoreω(t ′, u′)


∑
π∈Π(t′)
f(π, u′)⊤ + g(u′, a, w)⊤


= ∑
u′∈Σ|w|+k
∑
a∈A(u′,w)
∑
t′∈T (u′)
scoreη(u ′, a, w) · scoreω(t ′, u′)
Zθ(w)


∑
π∈Π(t′)
f(π, u)⊤ + g(u′, a, w)⊤


= E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤

 (8)
The result above can be further improved through Rao-Blackwellization.",A Derivation of Eq. 6,[0],[0]
"In this case, when we sample a tree–underlying form pair (t, u), we marginalize out all alignments that could have given rise to the sampled pair.",A Derivation of Eq. 6,[0],[0]
"The final derivation is show below:
∇θ logZθ(w) = E(t,a,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + g(u, a,w)⊤


= E(t,u)∼pθ(·|w)


∑
π∈Π(t)
f(π, u)⊤ + ∑
a∈A(u,w)
pθ(a | u,w)g(u, a,w) ⊤

 (9)
",A Derivation of Eq. 6,[0],[0]
This estimator in eq. (9) will have lower variance than eq.,A Derivation of Eq. 6,[0],[0]
(8).,A Derivation of Eq. 6,[0],[0]
"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output.",abstractText,[0],[0]
"In many cases, however, proper morphological analysis requires hierarchical structure— especially in the case of derivational morphology.",abstractText,[0],[0]
"In this work, we introduce a discriminative, joint model of morphological segmentation along with the orthographic changes that occur during word formation.",abstractText,[0],[0]
"To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model.",abstractText,[0],[0]
"Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.",abstractText,[0],[0]
"Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1066–1076, Denver, Colorado, May 31 – June 5, 2015. c©2015 Association for Computational Linguistics
In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script’s plotline, and speed up reading time. We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes. We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance. Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.",text,[0],[0]
"Each year, about 50,000 screenplays are registered with the WGA1, the Writers Guild of America.",1 Introduction,[0],[0]
Only a fraction of these make it through to be considered for production and an even smaller fraction to the big screen.,1 Introduction,[0],[0]
How do producers and directors navigate through this vast number of scripts available?,1 Introduction,[0],[0]
"Typically, production companies, agencies, and studios hire script readers, whose job is to analyze screenplays that come in, sorting the hopeful from the hopeless.",1 Introduction,[0],[0]
"Having read the script, a reader will generate a coverage report consisting of a logline (one or two sentences describing the story in a nutshell), a synopsis (a two- to three-page long summary of the script), comments explaining its appeal or problematic aspects, and a final verdict as to whether the script merits further consideration.",1 Introduction,[0],[0]
"A script excerpt
1The WGA is a collective term representing US TV and film writers.
from “Silence of the Lambs”, an American thriller released in 1991, is shown in Figure 1.
",1 Introduction,[0],[0]
"Although there are several screenwriting tools for authors (e.g., Final Draft is a popular application which automatically formats scripts to industry standards, keeps track of revisions, allows insertion of notes, and writing collaboratively online), there is a lack of any kind of script reading aids.",1 Introduction,[0],[0]
"Features of such a tool could be to automatically grade the quality of the script (e.g., thumbs up or down), generate
1066
synopses and loglines, identify main characters and their stories, or facilitate browsing (e.g., “show me every scene where there is a shooting”).",1 Introduction,[0],[0]
In this paper we explore whether current NLP technology can be used to address some of these tasks.,1 Introduction,[0],[0]
"Specifically, we focus on script summarization, which we conceptualize as the process of generating a shorter version of a screenplay, ideally encapsulating its most informative scenes.",1 Introduction,[0],[0]
"The resulting summaries can be used to enhance script browsing, give readers a rough idea of the script’s content and plotline, and speed up reading time.
",1 Introduction,[0],[0]
"So, what makes a good script summary?",1 Introduction,[0],[0]
"According to modern film theory, “all films are about nothing — nothing but character” (Monaco, 1982).",1 Introduction,[0],[0]
"Beyond characters, a summary should also highlight major scenes representative of the story and its progression.",1 Introduction,[0],[0]
"With this in mind, we define a script summary as a chain of scenes which conveys a narrative and smooth transitions from one scene to the next.",1 Introduction,[0],[0]
"At the same time, a good chain should incorporate some diversity (i.e., avoid redundancy), and focus on important scenes and characters.",1 Introduction,[0],[0]
We formalize the problem of selecting a good summary chain using a graph-theoretic approach.,1 Introduction,[0],[0]
"We represent scripts as (directed) bipartite graphs with vertices corresponding to scenes and characters, and edge weights to their strength of correlation.",1 Introduction,[0],[0]
"Intuitively, if two scenes are connected, a random walk starting from one would reach the other frequently.",1 Introduction,[0],[0]
"We find a chain of highly connected scenes by jointly optimizing logical progression, diversity, and importance.
",1 Introduction,[0],[0]
"Our contributions in this work are three-fold: we introduce a novel summarization task, on a new text genre, and formalize scene selection as the problem of finding a chain that represents a film’s story; we propose several novel methods for analyzing script content (e.g., identifying important characters and their interactions); and perform a large-scale human evaluation study using a question-answering task.",1 Introduction,[0],[0]
Experimental results show that our method produces summaries which are more informative compared to several competitive baselines.,1 Introduction,[0],[0]
"Computer-assisted analysis of literary text has a long history, with the first studies dating back to the
1960s (Mosteller and Wallace, 1964).",2 Related Work,[0],[0]
"More recently, the availability of large collections of digitized books and works of fiction has enabled researchers to observe cultural trends, address questions about language use and its evolution, study how individuals rise to and fall from fame, perform gender studies, and so on (Michel et al., 2010).",2 Related Work,[0],[0]
"Most existing work focuses on low-level analysis of word patterns, with a few notable exceptions.",2 Related Work,[0],[0]
Elson et al. (2010) analyze 19th century British novels by constructing a conversational network with vertices corresponding to characters and weighted edges corresponding to the amount of conversational interaction.,2 Related Work,[0],[0]
"Elsner (2012) analyzes characters and their emotional trajectories, whereas Nalisnick and Baird (2013) identify a character’s enemies and allies in plays based on the sentiment of their utterances.",2 Related Work,[0],[0]
"Other work (Bamman et al., 2013, 2014) automatically infers latent character types (e.g., villains or heroes) in novels and movie plot summaries.
",2 Related Work,[0],[0]
"Although we are not aware of any previous approaches to summarize screenplays, the field of computer vision is rife with attempts to summarize video (see Reed 2004 for an overview).",2 Related Work,[0],[0]
"Most techniques are based on visual information and rely on low-level cues such as motion, color, or audio (e.g., Rasheed et al. 2005).",2 Related Work,[0],[0]
Movie summarization is a special type of video summarization which poses many challenges due to the large variety of film styles and genres.,2 Related Work,[0],[0]
"A few recent studies (Weng et al., 2009; Lin et al., 2013) have used concepts from social network analysis to identify lead roles and role communities in order to segment movies into scenes (containing one or more shots) and create more informative summaries.",2 Related Work,[0],[0]
A surprising fact about this line of work is that it does not exploit the movie script in any way.,2 Related Work,[0],[0]
Characters are typically identified using face recognition techniques and scene boundaries are presumed unknown and are automatically detected.,2 Related Work,[0],[0]
"A notable exception are Sang and Xu (2010) who generate video summaries for movies, while taking into account character interaction features which they estimate from the corresponding screenplay.
",2 Related Work,[0],[0]
Our own approach is inspired by work in egocentric video analysis.,2 Related Work,[0],[0]
"An egocentric video offers a first-person view of the world and is captured from a wearable camera focusing on the user’s activities,
social interactions, and interests.",2 Related Work,[0],[0]
"Lu and Grauman (2013) present a summarization model which extracts subshot sequences while finding a balance of important subshots that are both diverse and provide a natural progression through the video, in terms of prominent visual objects (e.g., bottle, mug, television).",2 Related Work,[0],[0]
"We adapt their technique to our task, and show how to estimate character-scene correlations based on linguistic analysis.",2 Related Work,[0],[0]
We also interpret movies as social networks and extract a rich set of features from character interactions and their sentiment which we use to guide the summarization process.,2 Related Work,[0],[0]
"We compiled ScriptBase, a collection of 1,276 movie scripts, by automatically crawling web-sites which host or link entire movie scripts (e.g., imsdb.com).",3 ScriptBase: A Movie Script Corpus,[0],[0]
"The retrieved scripts were then cross-matched against Wikipedia2 and IMDB3 and paired with corresponding user-written summaries, plot sections, loglines and taglines (taglines are short snippets used by marketing departments to promote a movie).",3 ScriptBase: A Movie Script Corpus,[0],[0]
"We also collected metainformation regarding the movie’s genre, its actors, the production year, etc.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"ScriptBase contains movies comprising 23 genres; each movie is on average accompanied by 3 user summaries, 3 loglines, and 3 taglines.",3 ScriptBase: A Movie Script Corpus,[0],[0]
The corpus spans years 1909–2013.,3 ScriptBase: A Movie Script Corpus,[0],[0]
"Some corpus statistics are shown in Figure 2.
",3 ScriptBase: A Movie Script Corpus,[0],[0]
"The scripts were further post-processed with the Stanford CoreNLP pipeline (Manning et al., 2014) to perform tagging, parsing, named entity recognition and coreference resolution.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"They were also annotated with semantic roles (e.g., ARG0, ARG1), using the MATE tools (Björkelund et al., 2009).",3 ScriptBase: A Movie Script Corpus,[0],[0]
Our summarization experiments focused on comedies and thrillers.,3 ScriptBase: A Movie Script Corpus,[0],[0]
"We randomly selected 30 movies
2http://en.wikipedia.org 3http://www.imdb.com/
for training/development and 65 movies for testing.",3 ScriptBase: A Movie Script Corpus,[0],[0]
"As mentioned earlier, we define script summarization as the task of selecting a chain of scenes representing the movie’s most important content.",4 The Scene Extraction Model,[0],[0]
We interpret the term scene in the screenplay sense.,4 The Scene Extraction Model,[0],[0]
A scene is a unit of action that takes place in one location at one time (see Figure 1).,4 The Scene Extraction Model,[0],[0]
"We therefore need not be concerned with scene segmentation; scene boundaries are clearly marked, and constitute the basic units over which our model operates.
",4 The Scene Extraction Model,[0],[0]
"Let M = (S,C) represent a screenplay consisting of a set S = {s1,s2, . . .",4 The Scene Extraction Model,[0],[0]
",sn} of scenes, and a set C = {c1, . . .",4 The Scene Extraction Model,[0],[0]
",cm} of characters.",4 The Scene Extraction Model,[0],[0]
"We are interested in finding a list S′ = {si, . .",4 The Scene Extraction Model,[0],[0]
.sk},4 The Scene Extraction Model,[0],[0]
"of ordered, consecutive scenes subject to a compression rate m (see the example in Figure 3).",4 The Scene Extraction Model,[0],[0]
A natural interpretation of m in our case is the percentage of scenes from the original script retained in the summary.,4 The Scene Extraction Model,[0],[0]
"The extracted chain should contain (a) important scenes (i.e., critical for comprehending the story and its development); (b) diverse scenes that cover different aspects of the story; and (c) scenes which highlight the story’s progression from beginning to end.",4 The Scene Extraction Model,[0],[0]
"We therefore find the chain S′ maximizing the objective function Q(S′) which is the weighted sum of three terms: the story progression P, scene diversity D, and scene importance I:
S∗ = argmax S′⊂S Q(S′)",4 The Scene Extraction Model,[0],[0]
"(1) Q(S′) = λPP(S′)+λDD(S′)+λII(S′) (2)
In the following, we define each of the three terms.
",4 The Scene Extraction Model,[0],[0]
Scene-to-scene Progression The first term in the objective is responsible for selecting chains representing a logically coherent story.,4 The Scene Extraction Model,[0],[0]
"Intuitively, this means that if our chain includes a scene where a character commits an action, then scenes involving affected parties or follow-up actions should also be included.",4 The Scene Extraction Model,[0],[0]
"We operationalize this idea of progression in a story in terms of how strongly the characters in a selected scene si influence the transition to the next scene si+1:
P(S′)",4 The Scene Extraction Model,[0],[0]
"= |S′|−1 ∑ i=0 ∑ c∈Ci INF(si,si+1|c) (3)
We represent screenplays as weighted, bipartite graphs connecting scenes and characters:
B = (V,E) : V = C∪S
E = {(s,c,ws,c)|s ∈ S, c ∈C, ws,c ∈",4 The Scene Extraction Model,[0],[0]
"[0,1]}∪ {(c,s,wc,s)|c ∈C, s ∈ S, wc,s ∈",4 The Scene Extraction Model,[0],[0]
"[0,1]}
The set of vertices V corresponds to the union of characters C and scenes S. We therefore add to the bipartite graph one node per scene and one node per character, and two directed edges for each scene-character and character-scene pair.",4 The Scene Extraction Model,[0],[0]
An example of a bipartite graph is shown in Figure 4.,4 The Scene Extraction Model,[0],[0]
"We further assume that two scenes si and si+1 are tightly connected in such a graph if a random walk with restart (RWR; Tong et al. 2006; Kim et al. 2014) which starts in si has a high probability of ending in si+1.
",4 The Scene Extraction Model,[0],[0]
"In order to calculate the random walk stationary distributions, we must estimate the weights between a character and a scene.",4 The Scene Extraction Model,[0],[0]
"We are interested in how important a character is generally in the movie, and
specifically in a particular scene.",4 The Scene Extraction Model,[0],[0]
"For wc,s, we consider the probability of a character being important, i.e., of them belonging to the set of main characters:
wc,s = P(c ∈ main(M)), ∀(c,s,wc,s) ∈ E (4)
where P(c ∈main(M)) is some probability score associated with c being a main character in script M. For ws,c, we take the number of interactions a character is involved in relative to the total number of interactions in a specific scene as indicative of the character’s importance in that scene.",4 The Scene Extraction Model,[0],[0]
"Interactions refer to conversational interactions as well as relations between characters (e.g., who does what to whom):
ws,c = ∑
c′∈Cs inter(c,c′)
∑ c1,c2∈Cs
inter(c1,c2) , ∀(s,c,ws,c) ∈ E (5)
",4 The Scene Extraction Model,[0],[0]
We defer discussion of how we model probability P(c ∈Main(M)) and obtain interaction counts to Section 5.,4 The Scene Extraction Model,[0],[0]
"Weights ws,c and wc,s are normalized:
ws,c = ws,c
∑(s,c′,w′s,c) w ′",4 The Scene Extraction Model,[0],[0]
"s,c
, ∀(s,c,ws,c) ∈ E (6)
wc,s = wc,s
∑(c,s′,w′c,s) w ′",4 The Scene Extraction Model,[0],[0]
"c,s
, ∀(c,s,wc,s) ∈ E (7)
",4 The Scene Extraction Model,[0],[0]
"We calculate the stationary distributions of a random walk on a transition matrix T , enumerating over all vertices v (i.e., characters and scenes) in the bipartite graph B:
T (i, j) = { wi, j if (vi,v j,wi, j ∈ EB) 0",4 The Scene Extraction Model,[0],[0]
"otherwise
(8)
We measure the influence individual characters have on scene-to-scene transitions as follows.",4 The Scene Extraction Model,[0],[0]
"The stationary distribution rk for a RWR walker starting at node k is a vector that satisfies:
rk = (1− ε)Trk + εek (9)
where T is the transition matrix of the graph, ek is a seed vector, with all elements 0, except for element k which is set to 1, and ε is a restart probability parameter.",4 The Scene Extraction Model,[0],[0]
"In practice, our vectors rk and ek are indexed by the scenes and characters in a movie, i.e., they have length |S|+ |C|, and their nth element corresponds either to a known scene or character.",4 The Scene Extraction Model,[0],[0]
"In cases where
graphs are relatively small, we can compute r directly4 by solving:
rk = ε(I− (1− ε)T )−1ek (10)
",4 The Scene Extraction Model,[0],[0]
The lth element of r then equals the probability of the random walker being in state l in the stationary distribution.,4 The Scene Extraction Model,[0],[0]
"Let rck be the same as rk, but with the character node c of the bipartite graph being turned into a sink, i.e., all entries for c in the transition matrix T are 0.",4 The Scene Extraction Model,[0],[0]
"We can then define how a single character influences the transition between scenes si and si+1 as:
INF(si,si+1|c) = rsi",4 The Scene Extraction Model,[0],[0]
"[si+1]− rcsi [si+1] (11)
where rsi",4 The Scene Extraction Model,[0],[0]
[si+1] is shorthand for that element in the vector rsi that corresponds to scene si+1.,4 The Scene Extraction Model,[0],[0]
"We use the INF score directly in Equation (3) to determine the progress score of a candidate chain.
",4 The Scene Extraction Model,[0],[0]
Diversity The diversity term D(S′),4 The Scene Extraction Model,[0],[0]
"in our objective should encourage chains which consist of more dissimilar scenes, thereby avoiding redundancy.",4 The Scene Extraction Model,[0],[0]
"The diversity of chain S′ is the sum of the diversities of its successive scenes:
D(S′) = |S′|−1",4 The Scene Extraction Model,[0],[0]
"∑ i=1 d(si,si+1) (12)
",4 The Scene Extraction Model,[0],[0]
"The diversity d(si,si+1) of two scenes si and si+1 is estimated taking into account two factors: (a) do they have any characters in common, and (b) does the sentiment change from one scene to the next:
d(si,si+1) = dchar(si,si+1)+dsen(si,si+1)
2 (13)
where dchar(si,si+1) and dsen(si,si+1) respectively denote character and sentiment similarity between scenes.",4 The Scene Extraction Model,[0],[0]
"Specifically, dchar(si,si+1) is the relative character overlap between scenes si and si+1:
dchar(si,si+1)",4 The Scene Extraction Model,[0],[0]
=,4 The Scene Extraction Model,[0],[0]
"1− |Csi ∩Csi+1 ||Csi ∪Csi+1 | (14)
dchar will be 0 if two scenes share the same characters and 1 if no characters are shared.",4 The Scene Extraction Model,[0],[0]
"Analogously,
4We could also solve for r recursively which would be preferable for large graphs, since the performed matrix inversion is computationally expensive.
",4 The Scene Extraction Model,[0],[0]
"we define dsen, the sentiment overlap between two scenes as:
dsen(si,si+1)",4 The Scene Extraction Model,[0],[0]
"=1− k ·di f (si,si+1)k− k ·di f (si,si+1)+1 (15) di f (si,si+1) = 1
1+ |sen(si)− sen(si+1)| (16)
where the sentiment sen(s) of scene s is the aggregate sentiment score of all interactions in s:
sen(s) = ∑ c,c′∈Cs
sen(inter(c,c′))",4 The Scene Extraction Model,[0],[0]
"(17)
We explain how interactions and their sentiment are computed in Section 5.",4 The Scene Extraction Model,[0],[0]
"Again, dsen is larger if two scenes have a less similar sentiment.",4 The Scene Extraction Model,[0],[0]
"di f (si,si+1) becomes 1 if the sentiments are identical, and increasingly smaller for more dissimilar sentiments.",4 The Scene Extraction Model,[0],[0]
"The sigmoid-like function in Equation (15) scales dsen within range [0,1] to take smaller values for larger sentiment differences (factor k adjusts the curve’s smoothness).
",4 The Scene Extraction Model,[0],[0]
Importance The score I(S′) captures whether a chain contains important scenes.,4 The Scene Extraction Model,[0],[0]
"We define I(S′) as the sum of all scene-specific importance scores imp(si) of scenes contained in the chain:
I(S′) =",4 The Scene Extraction Model,[0],[0]
|S′|,4 The Scene Extraction Model,[0],[0]
"∑ i=1 imp(si) (18)
",4 The Scene Extraction Model,[0],[0]
"The importance imp(si) of a scene si is the ratio of lead to support characters within that scene:
imp(si) = ∑c: c∈Csi∧c∈main(M) 1
∑c: c∈Csi 1 (19)
where Csi is the set of characters present in scene si, and main(M) is the set of main characters in the movie.5 I(si) is 0 if a scene does not contain any main characters, and 1 if it contains only main characters (see Section 5 for how main(M) is inferred).
",4 The Scene Extraction Model,[0],[0]
Optimal Chain Selection We use Linear Programming to efficiently find a good chain.,4 The Scene Extraction Model,[0],[0]
"The objective is to maximize Equation (2), i.e., the sum of the terms for progress, diversity and importance,
5Whether scenes are important if they contain many main characters is an empirical question in its own right.",4 The Scene Extraction Model,[0],[0]
"For our purposes, we assume that this relation holds.
subject to their weights λ.",4 The Scene Extraction Model,[0],[0]
"We add a constraint corresponding to the compression rate, i.e., the number of scenes to be selected and enforce their linear order by disallowing non-consecutive combinations.",4 The Scene Extraction Model,[0],[0]
We use GLPK6 to solve the linear problem.,4 The Scene Extraction Model,[0],[0]
In this section we discuss several aspects of the implementation of the model presented in the previous section.,5 Implementation,[0],[0]
We explain how interactions are extracted and how sentiment is calculated.,5 Implementation,[0],[0]
"We also present our method for identifying main characters and estimating the weights ws,c and wc,s in the bipartite graph.
",5 Implementation,[0],[0]
Interactions The notion of interaction underlies many aspects of the model defined in the previous section.,5 Implementation,[0],[0]
"For instance, interaction counts are required to estimate the weights ws,c in the bipartite graph of the progression term (see Equation (5)), and in defining diversity (see Equations (15)–(17)).",5 Implementation,[0],[0]
"As we shall see below, interactions are also important for identifying main characters in a screenplay.
",5 Implementation,[0],[0]
"We use the term interaction to refer to conversations between two characters, as well as their relations (e.g., if a character kills another).",5 Implementation,[0],[0]
"For conversational interactions, we simply need to identify the speaker generating an utterance and the listener.",5 Implementation,[0],[0]
"Speaker attribution comes for free in our case, as speakers are clearly marked in the text (see Figure 1).",5 Implementation,[0],[0]
"Listener identification is more involved, especially when there are multiple characters in a scene.",5 Implementation,[0],[0]
We rely on a few simple heuristics.,5 Implementation,[0],[0]
"We assume that the previous speaker in the same scene, who is different from the current speaker, is the listener.",5 Implementation,[0],[0]
"If there is no previous speaker, we assume that the listener is the closest character mentioned in the speaker’s utterance (e.g., via a coreferring proper name or a pronoun).",5 Implementation,[0],[0]
"In cases where we cannot find a suitable listener, we assume the current speaker is the listener.
",5 Implementation,[0],[0]
We obtain character relations from the output of a semantic role labeler.,5 Implementation,[0],[0]
Relations are denoted by verbs whose ARG0 and ARG1 roles are character names.,5 Implementation,[0],[0]
We extract relations from the dialogue but also from scene descriptions.,5 Implementation,[0],[0]
"For example, in Figure 1 the description Suddenly, [...] he
6https://www.gnu.org/software/glpk/
clubs her over the head contains the relation clubs(MAN,CATHERINE).",5 Implementation,[0],[0]
"Pronouns are resolved to their antecedent using the Stanford coreference resolution system (Lee et al., 2011).
",5 Implementation,[0],[0]
"Sentiment We labeled lexical items in screenplays with sentiment values using the AFINN-96 lexicon (Nielsen, 2011), which is essentially a list of words scored with sentiment strength within the range",5 Implementation,[0],[0]
"[−5,+5].",5 Implementation,[0],[0]
The list also contains obscene words (which are often used in movies) and some Internet slang.,5 Implementation,[0],[0]
"By summing over the sentiment scores of individual words, we can work out the sentiment of an interaction between two characters, the sentiment of a scene (see Equation (17)), and even the sentiment between characters (e.g., who likes or dislikes whom in the movie in general).
",5 Implementation,[0],[0]
Main Characters,5 Implementation,[0],[0]
"The progress term in our summarization objective crucially relies on characters and their importance (see the weight wc,s in Equation (4)).",5 Implementation,[0],[0]
"Previous work (Weng et al., 2009; Lin et al., 2013) extracts social networks where nodes correspond to roles in the movie, and edges to their co-occurrence.",5 Implementation,[0],[0]
"Leading roles (and their communities) are then identified by measuring their centrality in the network (i.e., number of edges terminating in a given node).
",5 Implementation,[0],[0]
It is relatively straightforward to obtain a social network from a screenplay.,5 Implementation,[0],[0]
"Formally, for each movie we define a weighted and undirected graph:
G = {C,E}, : C = {c1, . .",5 Implementation,[0],[0]
.cn},5 Implementation,[0],[0]
", E = {(ci,c j,w)|ci,c j ∈C, w ∈",5 Implementation,[0],[0]
"N>0}
where vertices correspond to movie characters7, and edges denote character-to-character interactions.",5 Implementation,[0],[0]
Figure 5 shows an example of a social network for “The Silence of the Lambs”.,5 Implementation,[0],[0]
"Due to lack of space, only main characters are displayed, however the actual graph contains all characters (42 in this case).",5 Implementation,[0],[0]
"Importantly, edge weights are not normalized, but directly reflect the strength of association between different characters.
",5 Implementation,[0],[0]
We do not solely rely on the social network to identify main characters.,5 Implementation,[0],[0]
"We estimate P(c ∈ main(M)), the probability of c being a leading character in movie M, using a Multi Layer
7We assume one node per speaking role in the script.
",5 Implementation,[0],[0]
Perceptron (MLP) and several features pertaining to the structure of the social network and the script text itself.,5 Implementation,[0],[0]
"A potential stumbling block in treating character identification as a classification task is obtaining training data, i.e., a list of main characters for each movie.",5 Implementation,[0],[0]
"We generate a gold-standard by assuming that the characters listed under Wikipedia’s Cast section (or an equivalent section, e.g., Characters) are the main characters in the movie.
",5 Implementation,[0],[0]
"Examples of the features we used for the classification task include the barycenter of a character (i.e., the sum of its distance to all other characters),",5 Implementation,[0],[0]
"PageRank (Page et al., 1999), an eigenvectorbased centrality measure, absolute/relative interaction weight (the sum of all interactions a character is involved in, divided by the sum of all interactions in the network), absolute/relative number of sentences uttered by a character, number of times a character is described by other characters (e.g., He is a monster or She is nice), number of times a character talks about other characters, and type-tokenratio of sentences uttered by the character (i.e., rate of unique words in a character’s speech).",5 Implementation,[0],[0]
"Using these features, the MLP achieves an F1 of 79.0% on the test set.",5 Implementation,[0],[0]
It outperforms other classification methods such as Naive Bayes or logistic regression.,5 Implementation,[0],[0]
"Using the full-feature set, the MLP also obtains performance superior to any individual measure of graph connectivity.
",5 Implementation,[0],[0]
"Aside from Equation (4), lead characters also appear in Equation (19), which determines scene importance.",5 Implementation,[0],[0]
We assume a character c ∈ main(M) if it is predicted by the MLP with a probability ≥ 0.5.,5 Implementation,[0],[0]
Gold Standard Chains The development and tuning of the chain extraction model presented in Section 4 necessitates access to a gold standard of key scene chains representing the movie’s most important content.,6 Experimental Setup,[0],[0]
Our experiments concentrated on a sample of 95 movies (comedies and thrillers) from the ScriptBase corpus (Section 3).,6 Experimental Setup,[0],[0]
Performing the scene selection task for such a big corpus manually would be both time consuming and costly.,6 Experimental Setup,[0],[0]
"Instead, we used distant supervision based on Wikipedia to automatically generate a gold standard.
",6 Experimental Setup,[0],[0]
"Specifically, we assume that Wikipedia plots are representative of the most important content in a movie.",6 Experimental Setup,[0],[0]
"Using the alignment algorithm presented in Nelken and Shieber (2006), we align script sentences to Wikipedia plot sentences and assume that scenes with at least one alignment are part of the gold chain of scenes.",6 Experimental Setup,[0],[0]
We obtain many-to-many alignments using features such as lemma overlap and word stem similarity.,6 Experimental Setup,[0],[0]
"When evaluated on four movies8 (from the training set) whose content was manually aligned to Wikipedia plots, the aligner achieved a precision of .53 at a recall rate of .82 at deciding whether a scene should be aligned.",6 Experimental Setup,[0],[0]
Scenes are ranked according to the number of alignments they contain.,6 Experimental Setup,[0],[0]
"When creating gold chains at different compression rates, we start with the best-ranked scenes and then successively add lower ranked ones until we reach the desired compression rate.
",6 Experimental Setup,[0],[0]
System Comparison In our experiments we compared our scene extraction model (SceneSum) against three baselines.,6 Experimental Setup,[0],[0]
The first baseline was based on the minimum overlap (MinOv) of characters in consecutive scenes and corresponds closely to the diversity term in our objective.,6 Experimental Setup,[0],[0]
The second baseline was based on the maximum overlap (MaxOv) of characters and approximates the importance term in our objective.,6 Experimental Setup,[0],[0]
"The third baseline selects scenes at random (averaged over 1,000 runs).",6 Experimental Setup,[0],[0]
"Parameters for our models were tuned on the training set, weights for the terms in the objective were optimized to the following values: λP = 1.0, λD = 0.3, and λI = 0.1.",6 Experimental Setup,[0],[0]
"We set the restart probability of our random walker
8“Cars 2”, “Shrek”, “Swordfish”, and “The Silence of the Lambs”.
to ε = 0.5, and the sigmoid scaling factor in our diversity term to k =−1.2.
",6 Experimental Setup,[0],[0]
Evaluation We assessed the output of our model (and comparison systems) automatically against the gold chains described above.,6 Experimental Setup,[0],[0]
We performed experiments with compression rates in the range of 10% to 50% and measured performance in terms of F1.,6 Experimental Setup,[0],[0]
"In addition, we also evaluated the quality of the extracted scenes as perceived by humans, which is necessary, given the approximate nature of our gold standard.",6 Experimental Setup,[0],[0]
"We adopted a question-answering (Q&A) evaluation paradigm which has been used previously to evaluate summaries and document compression (Morris et al., 1992; Mani et al., 2002; Clarke and Lapata, 2010).",6 Experimental Setup,[0],[0]
"Under the assumption that the summary is to function as a replacement for the full script, we can measure the extent to which it can be used to find answers to questions which have been derived from the entire script and are representative of its core content.",6 Experimental Setup,[0],[0]
"The more questions a hypothetical system can answer, the better it is at summarizing the script as a whole.
",6 Experimental Setup,[0],[0]
Two annotators were independently instructed to read scripts (from our test set) and create Q&A pairs.,6 Experimental Setup,[0],[0]
"The annotators generated questions relating to the plot of the movie and the development of its characters, requiring an unambiguous answer.",6 Experimental Setup,[0],[0]
They compared and revised their Q&A pairs until a common agreed-upon set of five questions per movie was reached (see Table 1 for an example).,6 Experimental Setup,[0],[0]
"In addition, for every movie we asked subjects to name the main characters, and summarize its plot (in no more than four sentences).",6 Experimental Setup,[0],[0]
"Using Amazon Mechanical Turk (AMT)9, we elicited answers for eight scripts (four comedies and thrillers) in four summarization con-
9https://www.mturk.com/
ditions: using our model, the two baselines based on minimum and maximum character overlap, and the random system.",6 Experimental Setup,[0],[0]
"All models were assessed at the same compression rate of 20% which seems realistic in an actual application environment, e.g., computer aided summarization.",6 Experimental Setup,[0],[0]
The scripts were preselected in an earlier AMT study where participants were asked to declare whether they had seen the movies in our test set (65 in total).,6 Experimental Setup,[0],[0]
We chose the screenplays which had received the least viewings so as to avoid eliciting answers based on familiarity with the movie.,6 Experimental Setup,[0],[0]
"A total of 29 participants, all self-reported native English speakers, completed the Q&A task.",6 Experimental Setup,[0],[0]
The answers provided by the subjects were scored against an answer key.,6 Experimental Setup,[0],[0]
"A correct answer was marked with a score of one, and zero otherwise.",6 Experimental Setup,[0],[0]
"In cases where more answers were required per question, partial scores were awarded to each correct answer (e.g., 0.5).",6 Experimental Setup,[0],[0]
The score for a summary is the average of its question scores.,6 Experimental Setup,[0],[0]
"Table 2 shows the performance of SceneSum, our scene extraction model, and the three comparison systems (MaxOv, MinOv, Random) on the automatic gold standard at five compression rates.",7 Results,[0],[0]
"As can be seen, MaxOv performs best in terms of F1, followed by SceneSum.",7 Results,[0],[0]
We believe this is an artifact due to the way the gold standard was created.,7 Results,[0],[0]
Scenes with large numbers of main characters are more likely to figure in Wikipedia plot summaries and will thus be more frequently aligned.,7 Results,[0],[0]
"A chain based on maximum character overlap will focus on such scenes and will agree with the gold standard better compared to chains which take additional script properties into account.
",7 Results,[0],[0]
We further analyzed the scenes selected by SceneSum and the comparison systems with respect to their position in the script.,7 Results,[0],[0]
"Table 3 shows the av-
erage percentage of scenes selected from the beginning, middle, and end of the movie (based on an equal division of the number of scenes in the screenplay).",7 Results,[0],[0]
"As can be seen, the number of selected scenes tends to be evenly distributed across the entire movie.",7 Results,[0],[0]
"SceneSum has a slight bias towards the beginning of the movie which is probably natural, since leading characters appear early on, as well as important scenes introducing essential story elements (e.g., setting, points of view).
",7 Results,[0],[0]
The results of our human evaluation study are summarized in Table 4.,7 Results,[0],[0]
We observe that SceneSum summaries are overall more informative compared to those created by the baselines.,7 Results,[0],[0]
"In other words, AMT participants are able to answer more questions regarding the story of the movie when reading SceneSum summaries.",7 Results,[0],[0]
"In two instances (“A Nightmare on Elm Street 3” and “Mumford”), the overlap models score better, however, in this case the movies largely consist of scenes with the same characters and relatively little variation (“A Nightmare on Elm Street 3”), or the camera follows the main lead in his interactions with other characters (“Mumford”).",7 Results,[0],[0]
"Since our model is not so character-centric, it might be thrown off by non-character-based terms in its objective, leading to the selection of unfavorable scenes.",7 Results,[0],[0]
Table 4 also presents a break down of the different types of questions answered by our participants.,7 Results,[0],[0]
"Again, we see that in most cases a larger percentage is answered correctly when reading SceneSum summaries.
",7 Results,[0],[0]
"Overall, we observe that SceneSum extracts chains which encapsulate important movie content across the board.",7 Results,[0],[0]
"We should point out that although our movies are broadly classified as comedies and thrillers, they have very different structure and content.",7 Results,[0],[0]
"For example, “Little Athens” has a very loose plotline, “Living in Oblivion” has multi-
ple dream sequences, whereas “While She was Out” contains only a few characters and a series of important scenes towards the end.",7 Results,[0],[0]
"Despite this variety, SceneSum performs consistently better in our taskbased evaluation.",7 Results,[0],[0]
In this paper we have developed a graph-based model for script summarization.,8 Conclusions,[0],[0]
"We formalized the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes, which are diverse, important, and exhibit logical progression.",8 Conclusions,[0],[0]
A large-scale evaluation based on a question-answering task revealed that our method produces more informative summaries compared to several baselines.,8 Conclusions,[0],[0]
"In the future, we plan to explore model performance in a wider range of movie genres as well as its applicability to other NLP tasks (e.g., book summarization or event extraction).",8 Conclusions,[0],[0]
We would also like to automatically determine the compression rate which should presumably vary according to the movie’s length and content.,8 Conclusions,[0],[0]
"Finally, our long-term goal is to be able to generate loglines as well as movie plot summaries.
",8 Conclusions,[0],[0]
"Acknowledgments We would like to thank Rik Sarkar, Jon Oberlander and Annie Louis for their valuable feedback.",8 Conclusions,[0],[0]
"Special thanks to Bharat Ambati, Lea Frermann, and Daniel Renshaw for their help with system evaluation.",8 Conclusions,[0],[0]
"In this paper we study the task of movie script summarization, which we argue could enhance script browsing, give readers a rough idea of the script’s plotline, and speed up reading time.",abstractText,[0],[0]
We formalize the process of generating a shorter version of a screenplay as the task of finding an optimal chain of scenes.,abstractText,[0],[0]
"We develop a graph-based model that selects a chain by jointly optimizing its logical progression, diversity, and importance.",abstractText,[0],[0]
Human evaluation based on a question-answering task shows that our model produces summaries which are more informative compared to competitive baselines.,abstractText,[0],[0]
Movie Script Summarization as Graph-based Scene Extraction,title,[0],[0]
Support vector machines (SVMs) and Boosting have been two mainstream learning approaches during the past decade.,1. Introduction,[0],[0]
"The former (Cortes & Vapnik, 1995) roots in the statistical learning theory (Vapnik, 1995) with the central idea of searching a large margin separator, i.e., maximizing the smallest distance from the instances to the classification boundary in a RKHS (reproducing kernel Hilbert space).",1. Introduction,[0],[0]
"It is noteworthy that there is also a long history of applying margin theory to explain the latter (Freund & Schapire, 1995; Schapire et al., 1998), due to its tending to be empirically resistant to over-fitting (Reyzin & Schapire, 2006; Wang et al., 2011; Zhou, 2012).
",1. Introduction,[0],[0]
"1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China.",1. Introduction,[0],[0]
"Correspondence to: Zhi-Hua Zhou <zhouzh@lamda.nju.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Recently, the margin theory for Boosting has finally been defended (Gao & Zhou, 2013), and has disclosed that the margin distribution rather than a single margin is more crucial to the generalization performance.",1. Introduction,[0],[0]
It suggests that there may still exist large space to further enhance for SVMs.,1. Introduction,[0],[0]
"Inspired by this recognition, (Zhang & Zhou, 2014; 2016) proposed a binary classification method to optimize margin distribution by characterizing it through the firstand second-order statistics, which achieves quite satisfactory experimental results.",1. Introduction,[0],[0]
"Later, (Zhou & Zhou, 2016) extends the idea to an approach which is able to exploit unlabeled data and handle unequal misclassification cost.",1. Introduction,[0],[0]
"A brief summary of this line of early research can be found in (Zhou, 2014).
",1. Introduction,[0],[0]
"Although it has been shown that for binary classification, optimizing the margin distribution by maximizing the margin mean and minimizing the margin variance simultaneously can get superior performance, it still remains open for multi-class classification.",1. Introduction,[0],[0]
"Moreover, the margin for multiclass classification is much more complicated than that for binary class classification, which makes the resultant optimization be a difficult non-differentiable non-convex programming.",1. Introduction,[0],[0]
"In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine) to solve this problem efficiently.",1. Introduction,[0],[0]
"For optimization, we relax mcODM into a series of convex quadratic programming (QP), and extend the Block Coordinate Descent (BCD) algorithm (Tseng, 2001) to solve the dual of each QP.",1. Introduction,[0],[0]
The sub-problem of each iteration of BCD is also a QP.,1. Introduction,[0],[0]
"By exploiting its special structure, we derive a sorting algorithm to solve it which is much faster than general QP solvers.",1. Introduction,[0],[0]
"We further provide a generalization error bound based on Rademacher complexity, and further present the analysis of the relationship between generalization error and margin distribution for multi-class classification.",1. Introduction,[0],[0]
"Extensive experiments on twenty two data sets show the superiority of our method to all four versions of multi-class SVMs.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
Section 2 introduces some preliminaries.,1. Introduction,[0],[0]
Section 3 formulates the problem.,1. Introduction,[0],[0]
Section 4 presents the proposed algorithm.,1. Introduction,[0],[0]
Section 5 discusses some theoretical analyses.,1. Introduction,[0],[0]
Section 6 reports on our experimental studies and empirical observations.,1. Introduction,[0],[0]
Finally Section 7 concludes with future work.,1. Introduction,[0],[0]
We denote by X ∈ Rd the instance space and Y =,2. Preliminaries,[0],[0]
"[k] the label set, where [k] = {1, . . .",2. Preliminaries,[0],[0]
", k}.",2. Preliminaries,[0],[0]
Let D be an unknown (underlying) distribution over X × Y .,2. Preliminaries,[0],[0]
"A training set S = {(x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym)} ∈",2. Preliminaries,[0],[0]
(X × Y)m is drawn identically and independently (i.i.d.),2. Preliminaries,[0],[0]
according to distribution D. Let ϕ,2. Preliminaries,[0],[0]
: X 7→ H be a feature mapping associated to some positive definite kernel κ.,2. Preliminaries,[0],[0]
"For multi-class classification setting, the hypothesis is defined based on k weight vectors w1, . . .",2. Preliminaries,[0],[0]
",wk ∈ H, where each weight vector wl, l ∈ Y defines a scoring function x 7→ w⊤l ϕ(x) and the label of instance x is the one resulting in the largest score, i.e., h(x) =",2. Preliminaries,[0],[0]
argmaxl∈Y w ⊤ l ϕ(x).,2. Preliminaries,[0],[0]
"This decision function naturally leads to the following definition of the margin for a labeled instance (x, y):
γh(x, y) = w ⊤ y ϕ(x)−max l ̸=y w⊤l ϕ(x).
",2. Preliminaries,[0],[0]
"Thus h misclassifies (x, y) if and only if it produces a negative margin for this instance.
",2. Preliminaries,[0],[0]
"Given a hypothesis set H of functions mapping X to Y and the labeled training set S, our goal is to learn a function h ∈ H such that the generalization error R(h) = E(x,y)∼D[1h(x)̸=y] is small.",2. Preliminaries,[0],[0]
"To design optimal margin distribution machine for multiclass classification, we need to understand how to optimize the margin distribution.",3. Formulation,[0],[0]
"(Gao & Zhou, 2013) proved that, to characterize the margin distribution, it is important to consider",3. Formulation,[0],[0]
not only the margin mean,3. Formulation,[0],[0]
but also the margin variance.,3. Formulation,[0],[0]
"Inspired by this idea, (Zhang & Zhou, 2014; 2016) proposed the optimal margin distribution machine for binary classification, which characterizes the margin distribution according to the first- and second-order statistics, that is, maximizing the margin mean and minimizing the margin variance simultaneously.",3. Formulation,[0],[0]
"Specifically, let γ̄ denote the margin mean, and the optimal margin distribution machine can be formulated as:
min w,γ̄,ξi,ϵi Ω(w)− ηγ̄",3. Formulation,[0],[0]
+,3. Formulation,[0],[0]
λ m m∑ i=1,3. Formulation,[0],[0]
(ξ2i + ϵ,3. Formulation,[0],[0]
2,3. Formulation,[0],[0]
"i )
s.t. γh(xi, yi) ≥ γ̄",3. Formulation,[0],[0]
"− ξi, γh(xi, yi) ≤ γ̄ + ϵi, ∀i,
where Ω(w) is the regularization term to penalize the model complexity, η and λ are trading-off parameters, ξi and ϵi are the deviation of the margin γh(xi, yi) to the margin mean.",3. Formulation,[0],[0]
It’s evident that ∑m i=1(ξ 2,3. Formulation,[0],[0]
i + ϵ 2,3. Formulation,[0],[0]
i )/m,3. Formulation,[0],[0]
"is exactly the margin variance.
",3. Formulation,[0],[0]
"By scaling w which doesn’t affect the final classification results, the margin mean can be fixed as 1, then the de-
viation of the margin of (xi, yi) to the margin mean is |γh(xi, yi)",3. Formulation,[0],[0]
"− 1|, and the optimal margin distribution machine can be reformulated as
min w,ξi,ϵi
Ω(w)",3. Formulation,[0],[0]
"+ λ
m m∑ i=1",3. Formulation,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2
s.t. γh(xi, yi) ≥ 1− θ − ξi, γh(xi, yi) ≤ 1 + θ + ϵi, ∀i.
where µ ∈ (0, 1] is a parameter to trade off two different kinds of deviation (larger or less than margin mean).",3. Formulation,[0],[0]
θ ∈,3. Formulation,[0],[0]
"[0, 1) is a parameter of the zero loss band, which can control the number of support vectors, i.e., the sparsity of the solution, and (1− θ)2 in the denominator is to scale the second term to be a surrogate loss for 0-1 loss.
",3. Formulation,[0],[0]
"For multi-class classification, let the regularization term Ω(w)",3. Formulation,[0],[0]
= ∑k l=1 ∥wl∥2H/2,3. Formulation,[0],[0]
"and combine with the definition of margin, and we arrive at the formulation of mcODM,
min wl,ξi,ϵi
1
2 k∑ l=1",3. Formulation,[0],[0]
∥wl∥2H +,3. Formulation,[0],[0]
λ m m∑ i=1,3. Formulation,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2 (1)
s.t. w⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≥ 1− θ − ξi,
w⊤yiϕ(xi)−maxl ̸=yi w⊤l ϕ(xi) ≤ 1 + θ + ϵi, ∀i.
where λ, µ and θ are the parameters for trading-off described previously.",3. Formulation,[0],[0]
"Due to the max operator in the second constraint, mcODM is a non-differentiable non-convex programming, which is quite difficult to solve directly.
",4. Optimization,[0],[0]
"In this section, we first relax mcODM into a series of convex quadratic programming (QP), which can be much easier to handle.",4. Optimization,[0],[0]
"Specifically, at each iteration, we recast the first constraint as k − 1 linear inequality constraints:
w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, l ̸= yi,
and replace the second constraint with
w⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi,
where Mi = maxl ̸=yi w̄ ⊤ l ϕ(xi) and w̄l is the solution to the previous iteration.",4. Optimization,[0],[0]
"Then we can repeatedly solve the following convex QP problem until convergence:
min wl,ξi,ϵi
1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H +,4. Optimization,[0],[0]
λ m m∑ i=1,4. Optimization,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2 (2)
s.t. w⊤yiϕ(xi)−w ⊤ l ϕ(xi) ≥ 1− θ − ξi, ∀l ̸= yi,
w⊤yiϕ(xi)−Mi ≤ 1 + θ + ϵi, ∀i.
",4. Optimization,[0],[0]
"Introduce the lagrange multipliers ζli ≥ 0, l ̸= yi for the first k − 1 constraints and βi ≥ 0 for the last constraint respectively, the Lagrangian function of Eq. 2 leads to
L(wl, ξi, ϵi, ζ",4. Optimization,[0],[0]
"l i , βi)
= 1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H +,4. Optimization,[0],[0]
λ m m∑ i=1,4. Optimization,[0],[0]
"ξ2i + µϵ 2 i (1− θ)2
− m∑ i=1",4. Optimization,[0],[0]
∑,4. Optimization,[0],[0]
"l ̸=yi ζli(w ⊤ yiϕ(xi)−w ⊤ l ϕ(xi)− 1 + θ + ξi)
+",4. Optimization,[0],[0]
m∑ i=1 βi(w,4. Optimization,[0],[0]
⊤ yiϕ(xi)−Mi,4. Optimization,[0],[0]
"− 1− θ − ϵi),
By setting the partial derivations of variables {wl, ξi, ϵi} to zero, we have
wl = m∑ i=1",4. Optimization,[0],[0]
"δyi,l ∑ s̸=yi ζsi",4. Optimization,[0],[0]
"− (1− δyi,l)ζli − δyi,lβi ϕ(xi), ξi = m(1− θ)2
2λ
∑ l ̸=yi ζli , ϵi = m(1− θ)2 2λµ βi.",4. Optimization,[0],[0]
"(3)
where δyi,l equals 1 when yi = l and 0 otherwise.",4. Optimization,[0],[0]
"We further simplify the expression of wl as
wl = m∑ i=1",4. Optimization,[0],[0]
"(αli − δyi,lβi)ϕ(xi), (4)
by defining αli ≡ −ζli for ∀l ̸= yi and α yi",4. Optimization,[0],[0]
i ≡,4. Optimization,[0],[0]
"∑ s̸=yi
ζsi and substituting Eq.",4. Optimization,[0],[0]
"4 and Eq. 3 into the Lagrangian function, then we have the following dual problem
min αli,α yi",4. Optimization,[0],[0]
"i ,βi
1
2 k∑ l=1",4. Optimization,[0],[0]
∥wl∥2H + m(1− θ)2 4λ,4. Optimization,[0],[0]
m∑ i=1,4. Optimization,[0],[0]
"(αyii ) 2
+ m(1− θ)2
4λµ
m∑ i=1 β2i",4. Optimization,[0],[0]
+ (1− θ) m∑ i=1,4. Optimization,[0],[0]
∑,4. Optimization,[0],[0]
"l ̸=yi αli
+ (Mi + 1 + θ) m∑ i=1",4. Optimization,[0],[0]
"βi (5)
s.t. k∑
l=1
αli = 0, ∀i,
αli ≤ 0, ∀i,∀l ̸= yi, βi ≥ 0, ∀i.
",4. Optimization,[0],[0]
"The objective function in Eq. 5 involves m(k + 1) variables in total, so it is not easy to optimize with respect to all the variables simultaneously.",4. Optimization,[0],[0]
"Note that all the constraints can be partitioned into m disjoint sets, and the i-th set only involves α1i , . . .",4. Optimization,[0],[0]
", α k i , βi, so the variables can be divided into m decoupled groups and an efficient block coordinate descent algorithm (Tseng, 2001) can be applied.
",4. Optimization,[0],[0]
"Specifically, we sequentially select a group of k + 1 variables α1i , . . .",4. Optimization,[0],[0]
", α k i , βi associated with instance xi to minimize, while keeping other variables as constants, and repeat this procedure until convergence.
",4. Optimization,[0],[0]
"Algorithm 1 below details the kenrel mcODM.
",4. Optimization,[0],[0]
Algorithm 1 Kenrel mcODM 1: Input: Data set S. 2: Initialize α⊤ =,4. Optimization,[0],[0]
"[α11, . . .",4. Optimization,[0],[0]
", α k 1 , . . .",4. Optimization,[0],[0]
", α 1 m, . . .",4. Optimization,[0],[0]
", α k m] and
β⊤ =",4. Optimization,[0],[0]
"[β1, . . .",4. Optimization,[0],[0]
", βm] as zero vector. 3: while α and β not converge do 4: for i = 1, . . .",4. Optimization,[0],[0]
",m do 5: Mi ← maxl ̸=yi ∑m j=1(α",4. Optimization,[0],[0]
"l j − δyj ,lβj)κ(xj ,xi).",4. Optimization,[0],[0]
6: end for 7: Solve Eq. 5 by block coordinate descent method.,4. Optimization,[0],[0]
"8: end while 9: Output: α, β.",4. Optimization,[0],[0]
"The sub-problem in step 7 of Algorithm 1 is also a convex QP with k + 1 variables, which can be accomplished by some standard QP solvers.",4.1. Solving the sub-problem,[0],[0]
"However, by exploiting its special structure, i.e., only a small quantity of cross terms are involved, we can derive an algorithm to solve this subproblem just by sorting, which can be much faster than general QP solvers.
",4.1. Solving the sub-problem,[0],[0]
"Note that all variables except α1i , . . .",4.1. Solving the sub-problem,[0],[0]
", α k",4.1. Solving the sub-problem,[0],[0]
"i , βi are fixed, so we have the following sub-problem:
min αli,α yi",4.1. Solving the sub-problem,[0],[0]
"i ,βi ∑ l ̸=yi A 2 (αli) 2 + ∑ l ̸=yi",4.1. Solving the sub-problem,[0],[0]
"Blα l i + D 2 (αyii ) 2 −Aαyii βi
+Byiα yi i +
E 2 β2i + Fβi
s.t. k∑
l=1
αli = 0, (6)
αli ≤ 0, ∀l ̸= yi, βi ≥ 0.
where A = κ(xi,xi), Bl = ∑ j ̸=i κ(xi,xj)(α",4.1. Solving the sub-problem,[0],[0]
l j,4.1. Solving the sub-problem,[0],[0]
"−
δyj ,lβj)+1− θ for ∀l ̸= yi, Byi = ∑ j ̸=i κ(xi,xj)(α yi",4.1. Solving the sub-problem,[0],[0]
"j − δyj ,yiβj), D = A + m(1−θ)2 2λ , E = A + m(1−θ)2
2λµ and F ≡Mi + 1 + θ −Byi .
",4.1. Solving the sub-problem,[0],[0]
"The KKT conditions of Eq. 6 indicate that there are scalars ν, ρl and η such that
k∑ l=1 αli = 0, (7) αli ≤ 0, ∀l ̸= yi, (8)
βi ≥ 0, (9) ρlα l i = 0, ρl ≥ 0, ∀l ̸= yi, (10)
",4.1. Solving the sub-problem,[0],[0]
Aαli +,4.1. Solving the sub-problem,[0],[0]
"Bl − ν + ρl = 0, ∀l ̸= yi, (11) ηβi = 0, η ≥ 0, (12) −Aαyii +",4.1. Solving the sub-problem,[0],[0]
Eβi + F,4.1. Solving the sub-problem,[0],[0]
"− η = 0, (13) Dαyii −Aβi +Byi − ν = 0.",4.1. Solving the sub-problem,[0],[0]
"(14)
",4.1. Solving the sub-problem,[0],[0]
"According to Eq. 8, Eq. 10 and Eq. 11 are equivalent to
Aαli",4.1. Solving the sub-problem,[0],[0]
"+Bl − ν = 0, if αli < 0, ∀l ̸= yi, (15)",4.1. Solving the sub-problem,[0],[0]
Bl,4.1. Solving the sub-problem,[0],[0]
"− ν ≤ 0, if αli = 0, ∀l ̸= yi.",4.1. Solving the sub-problem,[0],[0]
"(16)
In the same way, Eq. 12 and Eq. 13 are equivalent to
−Aαyii",4.1. Solving the sub-problem,[0],[0]
+,4.1. Solving the sub-problem,[0],[0]
"Eβi + F = 0, if βi > 0, (17) −Aαyii + F ≥ 0, if βi = 0.",4.1. Solving the sub-problem,[0],[0]
"(18)
Thus KKT conditions turn to Eq. 7 - Eq. 9 and Eq. 14 - Eq. 18.",4.1. Solving the sub-problem,[0],[0]
"Note that
αli ≡ min ( 0,
ν",4.1. Solving the sub-problem,[0],[0]
"−Bl A
) , ∀l ̸= yi, (19)
satisfies KKT conditions Eq. 8 and Eq. 15 - Eq. 16",4.1. Solving the sub-problem,[0],[0]
"and βi ≡ max ( 0,
Aαyii − F E
) , (20)
satisfies KKT conditions Eq. 9 and Eq. 17 - Eq. 18.",4.1. Solving the sub-problem,[0],[0]
"By substituting Eq. 20 into Eq. 14, we obtain
Dαyii +Byi − ν = max ( 0, A
E (Aαyii − F )
) .",4.1. Solving the sub-problem,[0],[0]
"(21)
Let’s consider the following two cases in turn.
",4.1. Solving the sub-problem,[0],[0]
"Case 1: Aαyii ≤ F , according to Eq. 20 and Eq. 21, we have βi = 0 and α yi i = ν−Byi D .",4.1. Solving the sub-problem,[0],[0]
"Thus, A ν−Byi D ≤ F , which implies that ν ≤",4.1. Solving the sub-problem,[0],[0]
Byi + DFA .,4.1. Solving the sub-problem,[0],[0]
"Case 2: Aαyii > F , according to Eq. 20 and Eq. 21, we have βi = Aα yi",4.1. Solving the sub-problem,[0],[0]
i −F E > 0 and α yi i = Eν−AF−EByi DE−A2 .,4.1. Solving the sub-problem,[0],[0]
"Thus, A Eν−AF−EByi
DE−A2 > F , which implies that ν",4.1. Solving the sub-problem,[0],[0]
>,4.1. Solving the sub-problem,[0],[0]
"Byi + DF A .
",4.1. Solving the sub-problem,[0],[0]
The remaining task is to find ν such that Eq. 7 holds.,4.1. Solving the sub-problem,[0],[0]
"With Eq. 7 and Eq. 19, it can be shown that
ν =
AByi D + ∑ l:αli<0
Bl A D + |{l|α l i < 0}| , Case 1, (22)
ν",4.1. Solving the sub-problem,[0],[0]
"=
AEByi+A 2F DE−A2 + ∑",4.1. Solving the sub-problem,[0],[0]
"l:αli<0 Bl
AE DE−A2 + |{l|α l i < 0}|
, Case 2. (23)
",4.1. Solving the sub-problem,[0],[0]
"In both cases, the optimal ν takes the form of (P +∑ l:αli<0 Bl)/(Q+ |{l|αli < 0}|), where P and Q are some
constants.",4.1. Solving the sub-problem,[0],[0]
"(Fan et al., 2008) showed that it can be found by sorting {Bl} for ∀l ̸= yi in decreasing order and then sequentially adding them into an empty set Φ, until
ν∗ = P +
∑",4.1. Solving the sub-problem,[0],[0]
"l∈Φ Bl
Q+ |Φ| ≥",4.1. Solving the sub-problem,[0],[0]
"max l ̸∈Φ Bl. (24)
Note that the Hessian matrix of the objective function of Eq. 6 is positive definite, which guarantees the existence and uniqueness of the optimal solution, so only one of Eq. 22 and Eq. 23 can hold.",4.1. Solving the sub-problem,[0],[0]
"We can first compute ν∗ according to Eq. 24 for Case 1, and then check whether the constraint of ν is satisfied.",4.1. Solving the sub-problem,[0],[0]
"If not, we further compute ν∗ for Case 2.",4.1. Solving the sub-problem,[0],[0]
"Algorithm 2 summarizes the pseudo-code for solving the sub-problem.
",4.1. Solving the sub-problem,[0],[0]
"Algorithm 2 Solving the sub-problem 1: Input: Parameters A, B = {B1, . . .",4.1. Solving the sub-problem,[0],[0]
", Bk}, D,E, F .",4.1. Solving the sub-problem,[0],[0]
"2: Initialize B̂ ← B, then swap B̂1 and B̂yi , and sort
B̂\{B̂1} in decreasing order.",4.1. Solving the sub-problem,[0],[0]
"3: i← 2, ν ← AByi/D. 4: while i ≤ k and ν/(i− 2 +A/D) < B̂i do 5: ν ← ν + B̂i.",4.1. Solving the sub-problem,[0],[0]
6: i← i+ 1. 7: end while 8: if ν ≤,4.1. Solving the sub-problem,[0],[0]
"Byi +DF/A then 9: αli ← min(0, (ν −Bl)/A), l ̸= yi.
10: αyii ← (ν −Byi)/D. 11: βi ← 0. 12: else 13: i← 2, ν ← (AEB̂1 +A2F )/(DE",4.1. Solving the sub-problem,[0],[0]
−A2).,4.1. Solving the sub-problem,[0],[0]
14: while i ≤ k and ν/(i− 2+AE/(DE−A2)),4.1. Solving the sub-problem,[0],[0]
< B̂i do 15: ν ← ν + B̂i.,4.1. Solving the sub-problem,[0],[0]
"16: i← i+ 1. 17: end while 18: αli ← min(0, (ν −Bl)/A), l ̸= yi. 19: αyii ← (Eν −AF − EByi)/(DE −A2).",4.1. Solving the sub-problem,[0],[0]
20: βi ← (Aαyii − F ),4.1. Solving the sub-problem,[0],[0]
/E. 21: end if 22:,4.1. Solving the sub-problem,[0],[0]
"Output: α1i , . . .",4.1. Solving the sub-problem,[0],[0]
", αki , βi.",4.1. Solving the sub-problem,[0],[0]
"In section 4.1, the proposed method can efficiently deal with kernel mcODM.",4.2. Speedup for linear kernel,[0],[0]
"However, the computation of Mi in step 5 of Algorithm 1 and the computation of parameters B̄l in Algorithm 2 both involve the kernel matrix, whose inherent computational cost takes O(m2) time, so it might be computational prohibitive for large scale problems.
",4.2. Speedup for linear kernel,[0],[0]
"When linear kernel is used, these problems can be alleviated.",4.2. Speedup for linear kernel,[0],[0]
"According to Eq. 4, w is spanned by the training instance so it lies in a finite dimensional space under this
circumstance.",4.2. Speedup for linear kernel,[0],[0]
"By storing w1, . . .",4.2. Speedup for linear kernel,[0],[0]
",wk explicitly, the computational cost of Mi = maxl ̸=yi w ⊤",4.2. Speedup for linear kernel,[0],[0]
l xi can be much less.,4.2. Speedup for linear kernel,[0],[0]
"Moreover, note that B̄l = ∑ j ̸=i x ⊤",4.2. Speedup for linear kernel,[0],[0]
"i xj(α
",4.2. Speedup for linear kernel,[0],[0]
l j,4.2. Speedup for linear kernel,[0],[0]
"− δyj ,lβj)",4.2. Speedup for linear kernel,[0],[0]
"=∑m
j=1 x ⊤",4.2. Speedup for linear kernel,[0],[0]
"i xj(ᾱ l j−δyj ,lβ̄j)−x⊤i xi(ᾱli−δyi,lβ̄i) =",4.2. Speedup for linear kernel,[0],[0]
"w⊤l xi−
A(αli − δyi,lβi), so B̄l can also be computed efficiently.",4.2. Speedup for linear kernel,[0],[0]
"In this section, we study the statistical property of mcODM.",5. Analysis,[0],[0]
"To present the generalization bound of mcODM, we need to introduce the following loss function Φ,
Φ(z) = 1z≤0 + (z − 1 + θ)2
(1− θ)2 10<z≤1−θ,
γh,θ(x, y) = w ⊤ y ϕ(x)−max l∈Y {w⊤l ϕ(x)− (1− θ)1l=y},
where 1(·) is the indicator function that returns 1 when the argument holds, and 0 otherwise.",5. Analysis,[0],[0]
"As can be seen, γh,θ(x, y) is a lower bound of γh(x, y) and Φ(γh,θ(x, y))",5. Analysis,[0],[0]
"= Φ(γh(x, y)).
",5. Analysis,[0],[0]
Theorem 1.,5. Analysis,[0],[0]
"Let H = {(x, y) ∈ X",5. Analysis,[0],[0]
×,5. Analysis,[0],[0]
[k] 7→,5. Analysis,[0],[0]
w⊤y ϕ(x)| ∑k l=1,5. Analysis,[0],[0]
"∥wl∥2H ≤ Λ2} be the hypothesis space of mcODM, where ϕ :",5. Analysis,[0],[0]
X 7→ H is a feature mapping induced by some positive definite kernel κ.,5. Analysis,[0],[0]
"Assume that S ⊆ {x : κ(x,x) ≤ r2}, then for any δ > 0, with probability at least 1 − δ, the following generalization bound holds for any h ∈ H ,
R(h) ≤ 1 m m∑ i=1",5. Analysis,[0],[0]
"Φ(γh(xi, yi))",5. Analysis,[0],[0]
"+ 16rΛ 1− θ
√ 2πk
m + 3 √ ln 2δ 2m .
",5. Analysis,[0],[0]
Proof.,5. Analysis,[0],[0]
"Let H̃θ be the family of hypotheses mapping X × Y 7→ R defined by H̃θ = {(x, y) 7→ γh,θ(x, y) :",5. Analysis,[0],[0]
"h ∈ H}, with McDiarmid inequality (McDiarmid, 1989), yields the following inequality with probability at least 1− δ,
E[Φ(γh,θ(x, y))]",5. Analysis,[0],[0]
"≤ 1
m m∑ i=1 Φ(γh,θ(xi, yi))
",5. Analysis,[0],[0]
"+ 2RS(Φ ◦ H̃θ) + 3 √ ln 2δ 2m ,∀h ∈ H̃θ.
",5. Analysis,[0],[0]
"Note that Φ(γh,θ) = Φ(γh), R(h) = E[1γh(x,y)≤0] ≤",5. Analysis,[0],[0]
"E[1γh,θ(x,y)≤0] ≤ E[Φ(γh,θ(x, y))]",5. Analysis,[0],[0]
"and Φ(z) is 21−θ - Lipschitz function, by using Talagrand’s lemma (Mohri et al., 2012), we have
R(h) ≤ 1 m m∑ i=1",5. Analysis,[0],[0]
"Φ(γh(xi, yi))",5. Analysis,[0],[0]
"+ 4RS(H̃θ) 1− θ + 3 √ ln 2δ 2m .
",5. Analysis,[0],[0]
"According to Theorem 7 of (Lei et al., 2015), we have RS(H̃θ) ≤ 4rΛ",5. Analysis,[0],[0]
"√ 2πk/m and proves the stated result.
",5. Analysis,[0],[0]
Theorem 1 shows that we can get a tighter generalization bound for smaller rΛ and smaller θ.,5. Analysis,[0],[0]
"Since γ ≤ 2rΛ, so the former can be viewed as an upper bound of the margin.",5. Analysis,[0],[0]
"Besides, 1 − θ is the lower bound of the zero loss band of mcODM.",5. Analysis,[0],[0]
"This verifies that better margin distribution (i.e., larger margin mean and smaller margin variance) can yield better generalization performance, which is also consistent with the work of (Gao & Zhou, 2013).",5. Analysis,[0],[0]
"In this section, we empirically evaluate the effectiveness of our method on a broad range of data sets.",6. Empirical Study,[0],[0]
"We first introduce the experimental settings and compared methods in Section 6.1, and then in Section 6.2, we compare our method with four versions of multi-class SVMs, i.e., mcSVM (Weston & Watkins, 1999; Crammer & Singer, 2001; 2002), one-versus-all SVM (ovaSVM), one-versusone SVM (ovoSVM) (Ulrich, 1998) and error-correcting output code SVM (ecocSVM) (Dietterich & Bakiri, 1995).",6. Empirical Study,[0],[0]
"In addition, we also study the influence of the number of classes on generalization performance and margin distribution in Section 6.3.",6. Empirical Study,[0],[0]
"Finally, the computational cost is presented in Section 6.4.",6. Empirical Study,[0],[0]
We evaluate the effectiveness of our proposed methods on twenty two data sets.,6.1. Experimental Setup,[0],[0]
Table 1 summarizes the statistics of these data sets.,6.1. Experimental Setup,[0],[0]
"The data set size ranges from 150 to more than 581,012, and the dimensionality ranges from 4 to more than 62,061.",6.1. Experimental Setup,[0],[0]
"Moreover, the number of class ranges from 3 to 1,000, so these data sets cover a broad range of properties.",6.1. Experimental Setup,[0],[0]
All features are normalized into the interval,6.1. Experimental Setup,[0],[0]
"[0, 1].",6.1. Experimental Setup,[0],[0]
"For each data set, eighty percent of the instances are randomly selected as training data, and the rest are used as testing data.",6.1. Experimental Setup,[0],[0]
"For each data set, experiments are repeated for 10 times with random data partitions, and the average accuracies as well as the standard deviations are recorded.
mcODM is compared with four versions of multi-class SVMs, i.e., ovaSVM, ovoSVM, ecocSVM and mcSVM.",6.1. Experimental Setup,[0],[0]
These four methods can be roughly classified into two groups.,6.1. Experimental Setup,[0],[0]
The first group includes the first three methods by converting the multi-class classification problem into a set of binary classification problems.,6.1. Experimental Setup,[0],[0]
"Specially, ovaSVM consists of learning k scoring functions hl : X 7→ {−1,+1}, l ∈ Y , each seeking to discriminate one class l ∈ Y from all the others, as can be seen it need train k SVM models.",6.1. Experimental Setup,[0],[0]
"Alternatively, ovoSVM determines the scoring functions for all the combinations of class pairs, so it need train k(k − 1)/2 SVM models.",6.1. Experimental Setup,[0],[0]
"Finally, ecocSVM is a generalization of the former two methods.",6.1. Experimental Setup,[0],[0]
"This technique assigns to each class l ∈ Y a code word with length c, which serves as a signature for this class.",6.1. Experimental Setup,[0],[0]
"After training
c binary SVM models h1(·), . . .",6.1. Experimental Setup,[0],[0]
", hc(·), the class predicted for each testing instance is the one whose signatures is the closest to [h1(x), . . .",6.1. Experimental Setup,[0],[0]
", hc(x)] in Hamming distance.",6.1. Experimental Setup,[0],[0]
"The weakness of these methods is that they may produce unclassifiable regions and their computational costs are usually quite large in practice, which can be observed in the following experiments.",6.1. Experimental Setup,[0],[0]
"On the other hand, mcSVM belongs to the second group.",6.1. Experimental Setup,[0],[0]
"It directly determines all the scroing functions at once, so the time cost is usually less than the former methods.",6.1. Experimental Setup,[0],[0]
"In addition, the unclassifiable regions are also resolved.
",6.1. Experimental Setup,[0],[0]
"For all the methods, the regularization parameter λ for mcODM or C for binary SVM and mcSVM is selected by 5-fold cross validation from [20, 22, . . .",6.1. Experimental Setup,[0],[0]
", 220].",6.1. Experimental Setup,[0],[0]
"For mcODM, the regularization parameters µ and θ are both selected from [0.2, 0.4, 0.6, 0.8].",6.1. Experimental Setup,[0],[0]
"For ecocSVM, the exhaustive codes strategy is employed, i.e., for each class, we construct a code of length 2k−1 − 1 as the signature.",6.1. Experimental Setup,[0],[0]
All the selections for parameters are performed on training sets.,6.1. Experimental Setup,[0],[0]
Table 2 summarizes the detailed results on twenty two data sets.,6.2. Results,[0],[0]
"As can be seen, the overall performance of our method is superior or highly competitive to the other compared methods.",6.2. Results,[0],[0]
"Specifically, mcODM performs significantly better than mcSVM/ovaSVM/ovoSVM/ecocSVM on 17/19/18/17 over 22 data sets respectively, and achieves the best accuracy on 20 data sets.",6.2. Results,[0],[0]
"In addition, as can be seen, in comparing with other four methods which don’t consider margin distribution, the win/tie/loss counts show that mcODM is always better or comparable, almost never worse than it.",6.2. Results,[0],[0]
"In this section we study the influence of the number of classes on generalization performance and margin distribution, respectively.",6.3. Influence of the Number of Classes,[0],[0]
"Figure 1 plots the generalization performance of all the five methods on data set segment, and similar observation can be found for other data sets.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"As can be seen, when the number of classes is less than four, all methods perform quite well.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"However, as the fifth class is added, the generalization performance of other four methods decreases drastically.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"This might be attributable to the introduction of some noisy data, which SVM-style algorithms are very sensitive to since they optimize the minimum margin.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"On the other hand, our method considers the whole margin distribution, so it can be robust to noise and relatively more stable.",6.3.1. GENERALIZATION PERFORMANCE,[0],[0]
"Figure 2 plots the frequency histogram of margin distribution produced by mcSVM, ovaSVM and mcODM on data set segment as the number of classes increases from two to seven.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"As can be seen, when the number of classes is less than four, all methods can achieve good margin distribution, whereas with the increase of the number of classes, the other two methods begin to produce negative margins.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"At the same time, the distribution of our method becomes
“sharper”, which prevents the instance with small margin, so our method can still perform relatively well as the number of classes increases, which is also consistent with the observation from Figure 1.",6.3.2. MARGIN DISTRIBUTION,[0],[0]
"We compare the single iteration time cost of our method with mcSVM, ovaSVM, ovoSVM on all the data sets except aloi, on which ovoSVM could not return results in 48 hours.",6.4. Time Cost,[0],[0]
All the experiments are performed with MATLAB 2012b on a machine with 8×2.60 GHz CPUs and 32GB main memory.,6.4. Time Cost,[0],[0]
The average CPU time (in seconds) on each data set is shown in Figure 3.,6.4. Time Cost,[0],[0]
"The binary SVM used in ovaSVM, ovoSVM and mcSVM are both implemented by the LIBLINEAR (Fan et al., 2008) package.",6.4. Time Cost,[0],[0]
"It can be seen that for small data sets, the efficiency of all the methods are similar, however, for data sets with more than ten classes, e.g., sector and rcv1, mcSVM and mcODM, which learn all the scroing functions at once, are much faster than ovaSVM and ovoSVM, owing to the inefficiency of binarydecomposition as discussed in Section 6.1.",6.4. Time Cost,[0],[0]
"Note that LIBLINEAR are very fast implementations of binary SVM and mcSVM, and this shows that our method is also computationally efficient.",6.4. Time Cost,[0],[0]
"Recent studies disclose that for binary class classification, maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution.",7. Conclusions,[0],[0]
"However, it remains open to the influence of margin distribution for multi-class classification.",7. Conclusions,[0],[0]
We try to answer this question in this paper.,7. Conclusions,[0],[0]
"After maximizing the margin mean and minimizing the margin variance simultaneously, the resultant optimization is a difficult non-differentiable non-convex programming.",7. Conclusions,[0],[0]
We propose mcODM to solve this problem efficiently.,7. Conclusions,[0],[0]
Extensive experiments on twenty two data sets validate the superiority of our method to four versions of multi-class SVMs.,7. Conclusions,[0],[0]
"In the future it will be interesting to extend mcODM to more general learning settings, i.e., multilabel learning and structured learning.",7. Conclusions,[0],[0]
This research was supported by the NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Industrialization.,Acknowledgements,[0],[0]
"Authors want to thank reviewers for helpful comments, and thank Dr. Wei Gao for reading a draft.",Acknowledgements,[0],[0]
"Recent studies disclose that maximizing the minimum margin like support vector machines does not necessarily lead to better generalization performances, and instead, it is crucial to optimize the margin distribution.",abstractText,[0],[0]
"Although it has been shown that for binary classification, characterizing the margin distribution by the firstand second-order statistics can achieve superior performance.",abstractText,[0],[0]
"It still remains open for multiclass classification, and due to the complexity of margin for multi-class classification, optimizing its distribution by mean and variance can also be difficult.",abstractText,[0],[0]
"In this paper, we propose mcODM (multi-class Optimal margin Distribution Machine), which can solve this problem efficiently.",abstractText,[0],[0]
"We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification.",abstractText,[0],[0]
Empirical study further shows that mcODM always outperforms all four versions of multi-class SVMs on all experimental data sets.,abstractText,[0],[0]
Multi-Class Optimal Margin Distribution Machine,title,[0],[0]
"Many tasks in scientific and engineering applications can be framed as bandit optimisation problems, where we need to sequentially evaluate a noisy black-box function f : X → R with the goal of finding its optimum.",1. Introduction,[0],[0]
"Some applications include hyper-parameter tuning in machine learning (Hutter et al., 2011; Snoek et al., 2012), optimal policy search (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and scientific experiments (Gonzalez et al., 2014; Parkinson et al., 2006).
",1. Introduction,[0],[0]
"*Equal contribution 1Carnegie Mellon University, Pittsburgh PA, USA 2Rice University, Houston TX, USA.",1. Introduction,[0],[0]
"Correspondence to: Kirthevasan Kandasamy <kandasamy@cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Typically, in such applications, each function evaluation is expensive, and conventionally, the bandit literature has focused on developing methods for finding the optimum while keeping the number of evaluations to f at a minimum.
",1. Introduction,[0],[0]
"However, with increasingly expensive function evaluations, conventional methods have become infeasible as a significant cost needs to be expended before we can learn anything about f .",1. Introduction,[0],[0]
"As a result, multi-fidelity optimisation methods have recently gained attention (Cutler et al., 2014; Kandasamy et al., 2016a; Li et al., 2016).",1. Introduction,[0],[0]
"As the name suggests, these methods assume that we have access to lower fidelity approximations to f which can be evaluated instead of f .",1. Introduction,[0],[0]
"The lower the fidelity, the cheaper the evaluation, but it provides less accurate information about f .",1. Introduction,[0],[0]
"For example, when optimising the configuration of an expensive real world robot, its performance can be approximated using cheaper computer simulations.",1. Introduction,[0],[0]
"The goal is to use the cheap approximations to guide search for the optimum of f , and reduce the overall cost of optimisation.",1. Introduction,[0],[0]
"However, most multi-fidelity work assume only a finite number of approximations.",1. Introduction,[0],[0]
"In this paper, we study multi-fidelity optimisation when there is access to a continuous spectrum of approximations.
",1. Introduction,[0],[0]
"To motivate this set up, consider tuning a classification algorithm over a space of hyper-parameters X by maximising a validation set accuracy.",1. Introduction,[0],[0]
The algorithm is to be trained using N• data points via an iterative algorithm for T• iterations.,1. Introduction,[0],[0]
"However, we wish to use fewer training points N <",1. Introduction,[0],[0]
N• and/or fewer iterations T < T• to approximate the validation accuracy.,1. Introduction,[0],[0]
We can view validation accuracy as a function g :,1. Introduction,[0],[0]
"[1, N•]",1. Introduction,[0],[0]
"× [1, T•]",1. Introduction,[0],[0]
"× X → R where evaluating g(N,T, x) requires training the algorithm with N points for T iterations with the hyper-parameters x.",1. Introduction,[0],[0]
"If the training complexity of the algorithm is quadratic in data size and linear in the number of iterations, then the cost of this evaluation is λ(N,T ) = O(N2T ).",1. Introduction,[0],[0]
"Our goal is to find the optimum when N = N•, and T = T•, i.e. we wish to maximise f(x) = g(N•, T•, x).
",1. Introduction,[0],[0]
"In this setting, while N,T are technically discrete choices, they are more naturally viewed as coming from a continuous 2 dimensional fidelity space, [1, N•]",1. Introduction,[0],[0]
×,1. Introduction,[0],[0]
"[1, T•].",1. Introduction,[0],[0]
"One might hope that cheaper queries to g(N,T, ·) with N,T less than N•, T• can be used to learn about g(N•, T•, ·) and consequently optimise it using less overall cost.",1. Introduction,[0],[0]
"Indeed, this
is the case with many machine learning algorithms where cross validation performance tends to vary smoothly with data set size and number of iterations.",1. Introduction,[0],[0]
"Therefore, one may use cheap low fidelity experiments with small (N,T ) to discard bad hyper-parameters and deploy expensive high fidelity experiments with large (N,T ) only in a small but promising region.",1. Introduction,[0],[0]
"The main theoretical result of this paper (Theorem 1) shows that our proposed algorithm, BOCA, exhibits precisely this behaviour.
",1. Introduction,[0],[0]
"Continuous approximations also arise in simulation studies: where simulations can be carried out at varying levels of granularity, on-line advertising: where an ad can be controlled by continuous parameters such as display time or target audience, and several other experiment design tasks.",1. Introduction,[0],[0]
"In fact, in many multi-fidelity papers, the finite approximations were obtained by discretising a continuous space (Huang et al., 2006; Kandasamy et al., 2016a).",1. Introduction,[0],[0]
"Here, we study a Bayesian optimisation technique that is directly designed for continuous fidelity spaces and is potentially applicable to more general spaces.",1. Introduction,[0],[0]
"Our main contributions are,
1.",1. Introduction,[0],[0]
A novel setting and model for multi-fidelity optimisation with continuous approximations using Gaussian process (GP) assumptions.,1. Introduction,[0],[0]
"We develop a novel algorithm, BOCA, for this setting.
2.",1. Introduction,[0],[0]
A theoretical analysis characterising the behaviour and regret bound for BOCA. 3.,1. Introduction,[0],[0]
"An empirical study which demonstrates that BOCA outperforms alternatives, both multi-fidelity and otherwise, on a series of synthetic problems and real examples in hyper-parameter tuning and astrophysics.
",1. Introduction,[0],[0]
"Related Work Bayesian optimisation (BO), refers to a suite of techniques for bandit optimisation which use a prior belief distribution for f .",1. Introduction,[0],[0]
"While there are several techniques for BO (de Freitas et al., 2012; Hernández-Lobato et al., 2014; Jones et al., 1998; Mockus, 1994; Thompson, 1933), our work will build on the Gaussian process upper confidence bound (GP-UCB) algorithm of Srinivas et al. (2010).",1. Introduction,[0],[0]
"GP-UCB models f as a GP and uses upper confidence bound (UCB) (Auer, 2003) techniques to determine the next point for evaluation.
",1. Introduction,[0],[0]
"BO techniques have been used in developing multi-fidelity optimisation methods in various applications such as hyperparameter tuning and industrial design (Forrester et al., 2007; Huang et al., 2006; Klein et al., 2015; Lam et al., 2015; Poloczek et al., 2016; Swersky et al., 2013).",1. Introduction,[0],[0]
"However, these methods are either problem specific and/or only use a finite number of fidelities.",1. Introduction,[0],[0]
"Further, none of them come with theoretical underpinnings.",1. Introduction,[0],[0]
"Recent work has studied multi-fidelity methods for specific problems such as hyperparameter tuning, active learning and reinforcement learning (Agarwal et al., 2011; Cutler et al., 2014; Li et al., 2016; Sabharwal et al., 2015; Zhang & Chaudhuri, 2015).",1. Introduction,[0],[0]
"While
some of the above tasks can be framed as optimisation problems, the methods themselves are specific to the problem considered.",1. Introduction,[0],[0]
"Our method is more general as it applies to any bandit optimisation task.
",1. Introduction,[0],[0]
Perhaps the closest work to us is that of Kandasamy et al. (2016a;b;c) who developed MF-GP-UCB assuming a finite number of approximations to f .,1. Introduction,[0],[0]
"While this line of work was the first to provide theoretical guarantees for multifidelity optimisation, it has two important shortcomings.",1. Introduction,[0],[0]
"First, they make strong assumptions, particularly a uniform bound on the difference between the expensive function and an approximation.",1. Introduction,[0],[0]
This does not allow for instances where an approximation might be good at certain regions but not at the other.,1. Introduction,[0],[0]
"In contrast, our probabilistic treatment between fidelities is is robust to such cases.",1. Introduction,[0],[0]
"Second, their model does not allow sharing information between fidelities; each approximation is treated independently.",1. Introduction,[0],[0]
"Not only is this wasteful as lower fidelities can provide useful information about higher fidelities, it also means that the algorithm might perform poorly if the fidelities are not designed properly.",1. Introduction,[0],[0]
We demonstrate this with an experiment in Section 4.,1. Introduction,[0],[0]
"On the other hand, our model allows sharing information across the fidelity space in a natural way.",1. Introduction,[0],[0]
"In addition, we can also handle continuous approximations whereas their method is strictly for a finite number of approximations.",1. Introduction,[0],[0]
"That said, BOCA inherits a key intuition from MF-GP-UCB, which is to choose a fidelity only if we have sufficiently reduced the uncertainty at all lower fidelities.",1. Introduction,[0],[0]
"Besides this, there are considerable differences in the mechanics of the algorithm and proof techniques.",1. Introduction,[0],[0]
"As we proceed, we will draw further comparisons to Kandasamy et al. (2016a).",1. Introduction,[0],[0]
Gaussian processes: A GP over a space X is a random process from X to R. GPs are typically used as a prior for functions in Bayesian nonparametrics.,2.1. Some Background Material,[0],[0]
It is characterised by a mean function µ : X → R and a covariance function (or kernel) κ :,2.1. Some Background Material,[0],[0]
"X 2 → R. If f ∼ GP(µ, κ), then f(x) is distributed normally N (µ(x), κ(x, x)) for all x ∈ X .",2.1. Some Background Material,[0],[0]
"Suppose that we are given n observations Dn = {(xi, yi)}ni=1 from this GP, where xi ∈ X , yi = f(xi)",2.1. Some Background Material,[0],[0]
+,2.1. Some Background Material,[0],[0]
i ∈ R,2.1. Some Background Material,[0],[0]
"and i ∼ N (0, η2).",2.1. Some Background Material,[0],[0]
"Then the posterior process f |Dn is also a GP with mean µn and covariance κn given by
µn(x) =",2.1. Some Background Material,[0],[0]
k >,2.1. Some Background Material,[0],[0]
"(K + η2I)−1Y, (1)
κn(x, x ′) = κ(x, x′)− k>(K +",2.1. Some Background Material,[0],[0]
"η2I)−1k′.
Here",2.1. Some Background Material,[0],[0]
Y ∈,2.1. Some Background Material,[0],[0]
"Rn is a vector with Yi = yi, and k, k′ ∈",2.1. Some Background Material,[0],[0]
"Rn are such that ki = κ(x, xi), k′i = κ(x
′, xi).",2.1. Some Background Material,[0],[0]
"The matrix K ∈ Rn×n is given by Ki,j = κ(xi, xj).",2.1. Some Background Material,[0],[0]
"We refer the reader to chapter 2 of Rasmussen & Williams (2006) for more on the basics of GPs and their use in regression.
",2.1. Some Background Material,[0],[0]
Radial kernels:,2.1. Some Background Material,[0],[0]
The prior covariance functions of GPs are typically taken to be radial kernels; some examples are the squared exponential (SE) and Matérn kernels.,2.1. Some Background Material,[0],[0]
"Using a radial kernel means that the prior covariance can be written as κ(x, x′) = κ0φ(‖x−x′‖) and depends only on the distance between x and x′. Here, the scale parameter κ0 captures the magnitude f could deviate from µ. The function φ : R+ → R+ is a decreasing function with ‖φ‖∞ = φ(0) = 1.",2.1. Some Background Material,[0],[0]
"In this paper, we will use the SE kernel in a running example to convey the intuitions in our methods.",2.1. Some Background Material,[0],[0]
"For the SE kernel, φ(r) = φh(r) = exp(−r2/(2h2)), where h ∈ R+, called the bandwidth of the kernel, controls the smoothness of the GP.",2.1. Some Background Material,[0],[0]
"When h is large, the samples drawn from the GP tend to be smoother as illustrated in Fig. 1.",2.1. Some Background Material,[0],[0]
"We will reference this observation frequently in the text.
",2.1. Some Background Material,[0],[0]
"GP-UCB: The Gaussian Process Upper Confidence Bound (GP-UCB) algorithm of Srinivas et al. (2010) is a method for bandit optimisation, which, like many other BO methods, models f as a sample from a Gaussian process.",2.1. Some Background Material,[0],[0]
"At time t, the next point xt for evaluating f is chosen via the following procedure.",2.1. Some Background Material,[0],[0]
"First, we construct an upper confidence bound ϕt(x) = µt−1(x) + β 1/2 t σt−1(x) for the GP.",2.1. Some Background Material,[0],[0]
µt−1 is the posterior mean of the GP conditioned on the previous t− 1 evaluations and σt−1 is the posterior standard deviation.,2.1. Some Background Material,[0],[0]
"Following other UCB algorithms (Auer, 2003), the next point is chosen by maximising ϕt, i.e. xt = argmaxx∈X ϕt(x).",2.1. Some Background Material,[0],[0]
The µt−1 term encourages an exploitative strategy – in that we want to query regions where we already believe f is high – and σt−1 encourages an exploratory strategy – in that we want to query where we are uncertain about f so that we do not miss regions which have not been queried yet.,2.1. Some Background Material,[0],[0]
"βt, which is typically increasing with t, controls the trade-off between exploration and exploitation.",2.1. Some Background Material,[0],[0]
We have provided a brief review of GP-UCB in Appendix A.1.,2.1. Some Background Material,[0],[0]
"Our goal in bandit optimisation is to maximise a function f : X → R, over a domain X .",2.2. Problem Set Up,[0],[0]
When we evaluate f at x ∈ X we observe y = f(x) + where E[ ] = 0.,2.2. Problem Set Up,[0],[0]
Let,2.2. Problem Set Up,[0],[0]
x? ∈,2.2. Problem Set Up,[0],[0]
argmaxx∈X f(x) be a maximiser of f and f? = f(x?),2.2. Problem Set Up,[0],[0]
be the maximum value.,2.2. Problem Set Up,[0],[0]
"An algorithm for bandit optimisation is a sequence of points {xt}t≥0, where, at time t, the algorithm chooses to evaluate f at xt based on previous queries and
observations {(xi, yi)}t−1i=1 .",2.2. Problem Set Up,[0],[0]
"After n queries to f , its goal is to achieve small simple regret Sn, as defined below.
",2.2. Problem Set Up,[0],[0]
"Sn = min t=1,...,n
f?",2.2. Problem Set Up,[0],[0]
− f(xt).,2.2. Problem Set Up,[0],[0]
"(2)
Continuous Approximations: In this work, we will let f be a slice of a function g that lies in a larger space.",2.2. Problem Set Up,[0],[0]
"Precisely, we will assume the existence of a fidelity space Z and a function g : Z×X → R defined on the product space of the fidelity space and domain.",2.2. Problem Set Up,[0],[0]
"The function f which we wish to maximise is related to g via f(·) = g(z•, ·), where z• ∈ Z .",2.2. Problem Set Up,[0],[0]
"For instance, in the hyper-parameter tuning example from Section 1, Z = [1, N•] ×",2.2. Problem Set Up,[0],[0]
"[1, T•]",2.2. Problem Set Up,[0],[0]
and z• =,2.2. Problem Set Up,[0],[0]
"[N•, T•].",2.2. Problem Set Up,[0],[0]
"Our goal is to find a maximiser x? ∈ argmaxx f(x) = argmaxx g(z•, x).",2.2. Problem Set Up,[0],[0]
We have illustrated this setup in Fig. 2.,2.2. Problem Set Up,[0],[0]
"In the rest of the manuscript, the term “fidelities” will refer to points z in the fidelity space Z .
",2.2. Problem Set Up,[0],[0]
"The multi-fidelity framework is attractive when the following two conditions are true about the problem.
",2.2. Problem Set Up,[0],[0]
1.,2.2. Problem Set Up,[0],[0]
"There exist fidelities z ∈ Z where evaluating g is cheaper than evaluating at z•. To this end, we will associate a known cost function λ : Z → R+.",2.2. Problem Set Up,[0],[0]
"In the hyper-parameter tuning example, λ(z) = λ(N,T ) = O(N2T ).",2.2. Problem Set Up,[0],[0]
"It is helpful to think of z• as being the most expensive fidelity, i.e. maximiser of λ, and that λ(z) decreases as we move away from z•. However, this notion is strictly not necessary for our algorithm or results.
",2.2. Problem Set Up,[0],[0]
2.,2.2. Problem Set Up,[0],[0]
"The cheap g(z, ·) evaluation gives us information about g(z•, ·).",2.2. Problem Set Up,[0],[0]
This is true if g is smooth across the fidelity space as illustrated in Fig. 2.,2.2. Problem Set Up,[0],[0]
"As we will describe shortly, this smoothness can be achieved by modelling g as a GP with an appropriate kernel for the fidelity space Z .
",2.2. Problem Set Up,[0],[0]
"In the above setup, a multi-fidelity algorithm is a sequence of query-fidelity pairs {(zt, xt)}t≥0 where, at time t, the algorithm chooses zt ∈ Z and xt ∈ X , and observes yt =
g(zt, xt) + where E[ ] = 0.",2.2. Problem Set Up,[0],[0]
"The choice of (zt, xt) can of course depend on the previous fidelity-query-observation triples {(zi, xi, yi)}t−1i=1 .
",2.2. Problem Set Up,[0],[0]
Multi-fidelity Simple Regret: We provide bounds on the simple regret S(Λ) of a multi-fidelity optimisation method after it has spent capital Λ of a resource.,2.2. Problem Set Up,[0],[0]
"Following Kandasamy et al. (2016a); Srinivas et al. (2010), we will aim to provide any capital bounds, meaning that an algorithm would be expected to do well for all values of (sufficiently large)",2.2. Problem Set Up,[0],[0]
"Λ. Say we have made N queries to g within capital Λ, i.e. N is the random quantity such that N = max{n",2.2. Problem Set Up,[0],[0]
≥ 1 : ∑n t=1 λ(zt) ≤ Λ}.,2.2. Problem Set Up,[0],[0]
"While the cheap evaluations at z 6= z• are useful in guiding search for the optimum of g(z•, ·), there is no reward for optimising a cheaper g(z, ·).",2.2. Problem Set Up,[0],[0]
"Accordingly, we define the simple regret after capital Λ as,
S(Λ) =  min t∈{1,...,N} s.t zt=z• f?",2.2. Problem Set Up,[0],[0]
"− f(xt) if we have queried at z•,
+∞ otherwise.
",2.2. Problem Set Up,[0],[0]
"This definition reduces to the single fidelity definition (2) when we only query g at z•. It is also similar to the definition in Kandasamy et al. (2016a), but unlike them, we do not impose additional boundedness constraints on f or g.
Before we proceed, we note that it is customary in the bandit literature to analyse cumulative regret.",2.2. Problem Set Up,[0],[0]
"However, the definition of cumulative regret depends on the application at hand (Kandasamy et al., 2016c) and the results in this paper can be extended to to many sensible notions of cumulative regret.",2.2. Problem Set Up,[0],[0]
"However, both to simplify exposition and since our focus in this paper is optimisation, we stick to simple regret.
",2.2. Problem Set Up,[0],[0]
"Assumptions: As we will be primarily focusing on continuous and compact domains and fidelity spaces, going forward we will assume, without any loss of generality, that X =",2.2. Problem Set Up,[0],[0]
"[0, 1]d and Z =",2.2. Problem Set Up,[0],[0]
"[0, 1]p.",2.2. Problem Set Up,[0],[0]
We discuss non-continuous settings briefly at the end of Section 3.,2.2. Problem Set Up,[0],[0]
"In keeping with similar work in the Bayesian optimisation literature, we will assume g ∼ GP(0, κ) and upon querying at (z, x) we observe y = g(z, x) + where ∼ N (0, η2).",2.2. Problem Set Up,[0],[0]
κ :,2.2. Problem Set Up,[0],[0]
(Z × X )2 → R is the prior covariance defined on the product space.,2.2. Problem Set Up,[0],[0]
"In this work, we will study exclusively κ of the following form,
κ([z, x], [z′, x′]) = κ0 φZ(‖z",2.2. Problem Set Up,[0],[0]
− z′‖)φX (‖x− x′‖).,2.2. Problem Set Up,[0],[0]
"(3)
Here, κ0 ∈ R+ is the scale parameter and φZ , φX are radial kernels defined on Z,X respectively.",2.2. Problem Set Up,[0],[0]
The fidelity space kernel φZ is an important component in this work.,2.2. Problem Set Up,[0],[0]
"It controls the smoothness of g across the fidelity space and hence determines how much information the lower fidelities provide about g(z•, ·).",2.2. Problem Set Up,[0],[0]
"For example, suppose that φZ was a SE kernel.",2.2. Problem Set Up,[0],[0]
"A favourable setting for a multi-fidelity method would be for φZ to have a large bandwidth hZ as that would imply
that g is very smooth across Z .",2.2. Problem Set Up,[0],[0]
We will see that hZ determines the behaviour and theoretical guarantees of BOCA in a natural way when φZ is the SE kernel.,2.2. Problem Set Up,[0],[0]
"To formalise this notion, we will define the following function ξ : Z →",2.2. Problem Set Up,[0],[0]
"[0, 1].
ξ(z) = √ 1− φZ(‖z",2.2. Problem Set Up,[0],[0]
"− z•‖)2 (4)
One interpretation of ξ(z) is that it measures the gap in information about g(z•, ·) when we query at z 6=",2.2. Problem Set Up,[0],[0]
"z•. That is, it is the price we have to pay, in information, for querying at a cheap fidelity.",2.2. Problem Set Up,[0],[0]
Observe that ξ increases when we move away from z• in the fidelity space.,2.2. Problem Set Up,[0],[0]
"For the SE kernel, it can be shown1 ξ(z)",2.2. Problem Set Up,[0],[0]
≈ ‖z−z•‖hZ .,2.2. Problem Set Up,[0],[0]
"For large hZ , g is smoother across Z and we can expect the lower fidelities to be more informative about f ; as expected the information gap ξ is small for large hZ .",2.2. Problem Set Up,[0],[0]
"If hZ is small and g is not smooth, the gap ξ is large and lower fidelities are not as informative.
",2.2. Problem Set Up,[0],[0]
"Before we present our algorithm for the above setup, we will introduce notation for the posterior GPs for g and f .",2.2. Problem Set Up,[0],[0]
"Let Dn = {(zi, xi, yi)}ni=1 be n fidelity, query, observation values from the GP g, where yi was observed when evaluating g(zi, xi).",2.2. Problem Set Up,[0],[0]
"We will denote the posterior mean and standard deviation of g conditioned on Dn by νn and τn respectively (νn, τn can be computed from (1) by replacing x←",2.2. Problem Set Up,[0],[0]
"[z, x]).",2.2. Problem Set Up,[0],[0]
"Therefore g(z, x)|Dn ∼ N (νn(z, x), τ2n(z, x)) for all (z, x) ∈ Z × X .",2.2. Problem Set Up,[0],[0]
"We will further denote
µn(·) = νn(z•, ·), σn(·) = τn(z•, ·), (5)
to be the posterior mean and standard deviation of g(z•, ·) = f(·).",2.2. Problem Set Up,[0],[0]
"It follows that f |Dn is also a GP and satisfies f(x)|Dn ∼ N (µn(x), σ2n(x)) for all x ∈ X .
3.",2.2. Problem Set Up,[0],[0]
"BOCA: Bayesian Optimisation with Continuous Approximations
BOCA is a sequential strategy to select a domain point xt ∈ X and fidelity zt ∈ Z at time t based on previous observations.",2.2. Problem Set Up,[0],[0]
"At time t, we will first construct an upper confidence bound ϕt for the function f we wish to optimise.",2.2. Problem Set Up,[0],[0]
"It takes the form,
ϕt(x) = µt−1(x) + β 1/2 t σt−1(x).",2.2. Problem Set Up,[0],[0]
"(6)
Recall from (5) that µt−1 and σt−1 are the posterior mean and standard deviation of f using the observations from the previous t−1 time steps at all fidelities, i.e. the entireZ×X space.",2.2. Problem Set Up,[0],[0]
"We will specify βt in Theorems 1, 8.",2.2. Problem Set Up,[0],[0]
"Following other UCB algorithms, our next point xt in the domain X for evaluating g is a maximiser of ϕt, i.e. xt ∈ argmaxx∈X ϕt(x).
",2.2. Problem Set Up,[0],[0]
"Next, we need to determine the fidelity zt ∈ Z to query g. 1Strictly, ξ(z) ≤",2.2. Problem Set Up,[0],[0]
"‖z− z•‖/hZ , but the inequality is tighter for larger hZ .",2.2. Problem Set Up,[0],[0]
"In any case, ξ is strictly decreasing with hZ .
",2.2. Problem Set Up,[0],[0]
"For this we will first select a subset Zt(xt) of Z as follows,
Zt(xt) = { z ∈ Z : λ(z) < λ(z•), τt−1(z, xt) > γ(z),
ξ(z) >",2.2. Problem Set Up,[0],[0]
β −1/2,2.2. Problem Set Up,[0],[0]
"t ‖ξ‖∞ } , (7)
where γ(z) = √ κ0 ξ(z)
( λ(z)
λ(z•)
)q .
",2.2. Problem Set Up,[0],[0]
"Here, ξ is the information gap function in (4) and τt−1 is the posterior standard deviation of g, and p, d are the dimensionalities of Z,X .",2.2. Problem Set Up,[0],[0]
The exponent q depends on the kernel used for φZ .,2.2. Problem Set Up,[0],[0]
"For e.g., for the SE kernel, q = 1/(p+ d + 2).",2.2. Problem Set Up,[0],[0]
We filter out the fidelities we consider at time t using three conditions as specified above.,2.2. Problem Set Up,[0],[0]
We elaborate on these conditions in more detail in Section 3.1.,2.2. Problem Set Up,[0],[0]
"If Zt is not empty, we choose the cheapest fidelity in this set, i.e. zt ∈ argminz∈Zt λ(z).",2.2. Problem Set Up,[0],[0]
"If Zt is empty, we choose zt = z•.
We have summarised the resulting procedure below in Algorithm 1.",2.2. Problem Set Up,[0],[0]
An important advantage of BOCA is that it only requires specifying the GP hyper-parameters for g such as the kernel κ.,2.2. Problem Set Up,[0],[0]
"In practice, this can be achieved by various effective heuristics such as maximising the GP marginal likelihood or cross validation which are standard in most BO methods.",2.2. Problem Set Up,[0],[0]
"In contrast, MF-GP-UCB of Kandasamy et al. (2016a) requires tuning several other hyper-parameters.
",2.2. Problem Set Up,[0],[0]
Algorithm 1 BOCA Input: kernel κ. •,2.2. Problem Set Up,[0],[0]
"Set ν0(·)← 0, τ0(·)← κ(·, ·)1/2, D0 ← ∅. • for t = 1, 2, . . .
",2.2. Problem Set Up,[0],[0]
1. xt,2.2. Problem Set Up,[0],[0]
← argmaxx∈X ϕt(x).,2.2. Problem Set Up,[0],[0]
See (6) 2.,2.2. Problem Set Up,[0],[0]
zt ← argminz∈Zt(xt)∪{z•} λ(z).,2.2. Problem Set Up,[0],[0]
See (7) 3.,2.2. Problem Set Up,[0],[0]
yt,2.2. Problem Set Up,[0],[0]
"← Query g at (zt, xt).",2.2. Problem Set Up,[0],[0]
4.,2.2. Problem Set Up,[0],[0]
"Dt ← Dt−1 ∪ {(zt, xt, yt)}.",2.2. Problem Set Up,[0],[0]
"Update posterior mean νt, and standard deviation τt for g conditioned on Dt.",2.2. Problem Set Up,[0],[0]
"We will now provide an intuitive justification for the three conditions in the selection criterion for zt, i.e., equation (7).",3.1. Fidelity Selection Criterion,[0],[0]
"The first condition, λ(z) < λ(z•) is fairly obvious; since we wish to optimise g(z•, ·) and since we are not rewarded for queries at other fidelities, there is no reason to consider fidelities that are more expensive than z•.
The second condition, τt−1(z, xt) > γ(z) says that we will only consider fidelities where the posterior variance is larger than a threshold γ(z) = √ κ0ξ(z)(λ(z)/λ(z•))
q , which depends critically on two quantities, the cost function λ and the information gap ξ.",3.1. Fidelity Selection Criterion,[0],[0]
"As a first step towards parsing this condition, observe that a reasonable multi-fidelity strategy should be inclined to query cheap fidelities and learn about
g before querying expensive fidelities.",3.1. Fidelity Selection Criterion,[0],[0]
"As γ(z) is monotonically increasing in λ(z), it becomes easier for a cheap z to satisfy τt−1(z, xt) > γ(z) and be included in Zt at time",3.1. Fidelity Selection Criterion,[0],[0]
"t. Moreover, since we choose zt to be the minimiser of λ in Zt, a cheaper fidelity will always be chosen over expensive ones if included in Zt.",3.1. Fidelity Selection Criterion,[0],[0]
"Second, if a particular fidelity z is far away from z•, it probably contains less information about g(z•, ·).",3.1. Fidelity Selection Criterion,[0],[0]
"Again, a reasonable multi-fidelity strategy should be discouraged from making such queries.",3.1. Fidelity Selection Criterion,[0],[0]
This is precisely the role of the information gap ξ which is increasing with ‖z,3.1. Fidelity Selection Criterion,[0],[0]
"− z•‖. As z moves away from z•, γ(z) increases and it becomes harder to satisfy τt−1(z, xt) > γ(z).",3.1. Fidelity Selection Criterion,[0],[0]
"Therefore, such a z is less likely to be included in Zt(xt) and be considered for evaluation.",3.1. Fidelity Selection Criterion,[0],[0]
"Our analysis reveals that setting γ as in (7) is a reasonable trade off between cost and information in the approximations available to us; cheaper fidelities cost less, but provide less accurate information about the function f we wish to optimise.",3.1. Fidelity Selection Criterion,[0],[0]
It is worth noting that the second condition is similar in spirit to Kandasamy et al. (2016a) who proceed from a lower to higher fidelity only when the lower fidelity variance is smaller than a threshold.,3.1. Fidelity Selection Criterion,[0],[0]
"However, while they treat the threshold as a hyper-parameter, we are able to explicitly specify theoretically motivated values.
",3.1. Fidelity Selection Criterion,[0],[0]
The third condition in (7) is ξ(z) > ‖ξ‖∞/β1/2t .,3.1. Fidelity Selection Criterion,[0],[0]
"Since ξ is increasing as we move away from z•, it says we should exclude fidelities inside a (small) neighbourhood of z•. Recall that if Zt is empty, BOCA will choose z• by default.",3.1. Fidelity Selection Criterion,[0],[0]
"But when it is not empty, we want to prevent situations where we get arbitrarily close to z• but not actually query at z•. Such pathologies can occur when we are dealing with a continuum of fidelities and this condition forces BOCA to pick z• instead of querying very close to it.",3.1. Fidelity Selection Criterion,[0],[0]
"Observe that since βt is increasing with t, this neighborhood is shrinking with time and therefore the algorithm will eventually have the opportunity to evaluate fidelities close to z•.",3.1. Fidelity Selection Criterion,[0],[0]
We now present our main theoretical contributions.,3.2. Theoretical Results,[0],[0]
"In order to simplify the exposition and convey the gist of our results, we will only present a simplified version of our theorems.",3.2. Theoretical Results,[0],[0]
"We will suppress constants, polylog terms, and other technical details that arise due to a covering argument in our proofs.",3.2. Theoretical Results,[0],[0]
"A rigorous treatment is available in Appendix B.
Maximum Information Gain: Up until this point, we have not discussed much about the kernel φX of the domain X .",3.2. Theoretical Results,[0],[0]
"Since we are optimising f over X , it is natural to expect that this will appear in the bounds.",3.2. Theoretical Results,[0],[0]
Srinivas et al. (2010) showed that the statistical difficulty of GP bandits is determined by the Maximum Information Gain (MIG) which measures the maximum information a subset of observations have about f .,3.2. Theoretical Results,[0],[0]
We denote it by Ψn(A) where A is a subset of X and n is the number of queries to f .,3.2. Theoretical Results,[0],[0]
"We refer the reader
to Appendix B for a formal definition of MIG.",3.2. Theoretical Results,[0],[0]
"For the current exposition however, it suffices to know that for radial kernels, Ψn(A) increases with n and the volume vol(A) of A.",3.2. Theoretical Results,[0],[0]
"For instance, when we use an SE kernel for φX , we have Ψn(A) ∝",3.2. Theoretical Results,[0],[0]
"vol(A) log(n)d+1and for a Matérn kernel with smoothness parameter ν, Ψn(A) ∝ vol(A)n1− ν 2ν+d(d+1) .",3.2. Theoretical Results,[0],[0]
"(Srinivas et al., 2010).",3.2. Theoretical Results,[0],[0]
"Let nΛ = bΛ/λ(z•)c denote the number of queries by a single fidelity algorithm within capital Λ. Srinivas et al. (2010) showed that the simple regret S(Λ) for GP-UCB after capital Λ can be bounded by,
Simple Regret for GP-UCB: S(Λ) .",3.2. Theoretical Results,[0],[0]
√ ΨnΛ(X ) nΛ .,3.2. Theoretical Results,[0],[0]
"(8)
In our analysis of BOCA we show that most queries to g at fidelity z• will be confined to a small subset of X which contains the optimum x?.",3.2. Theoretical Results,[0],[0]
"Precisely, after capital Λ, for any α ∈ (0, 1), we show there exists ρ > 0",3.2. Theoretical Results,[0],[0]
"such that the number of queries outside the following set Xρ is less than nαΛ.
Xρ = { x ∈ X",3.2. Theoretical Results,[0],[0]
: f? − f(x) ≤,3.2. Theoretical Results,[0],[0]
2ρ,3.2. Theoretical Results,[0],[0]
√ κ0 ‖ξ‖∞ } .,3.2. Theoretical Results,[0],[0]
"(9)
Here, ξ is from (4).",3.2. Theoretical Results,[0],[0]
"While it is true that any optimisation algorithm would eventually query extensively in a neighbourhood around the optimum, a strong result of the above form is not always possible.",3.2. Theoretical Results,[0],[0]
"For instance, for GP-UCB, the best achievable bound on the number of queries in any set that does not contain x? is n 1/2 Λ .",3.2. Theoretical Results,[0],[0]
"The fact that Xρ exists relies crucially on the multi-fidelity assumptions and that our algorithm leverages information from lower fidelities when querying at z•. As ξ is small when g is smooth across Z , the set Xρ will be small when the approximations are highly informative about g(z•, ·).",3.2. Theoretical Results,[0],[0]
"For e.g., when φZ is a SE kernel, we haveXρ ≈ {x ∈ X : f?−f(x) ≤ 2ρ √ κ0p/hZ}.",3.2. Theoretical Results,[0],[0]
"When hZ is large and g is smooth across Z , Xρ is small as the right side of the inequality is smaller.",3.2. Theoretical Results,[0],[0]
"As BOCA confines most of its evaluations to this small set containing x?, we will be able to achieve much better regret than GP-UCB.",3.2. Theoretical Results,[0],[0]
"When hZ is small and g is not smooth across Z , the set Xρ becomes large and the advantage of multi-fidelity optimisation diminishes.",3.2. Theoretical Results,[0],[0]
"One can similarly argue that for the Matérn kernel, as the parameter ν increases, g will be smoother across Z , and Xρ becomes smaller yielding better bounds on the regret.",3.2. Theoretical Results,[0],[0]
"Below, we provide an informal statement of our main theoretical result. .",3.2. Theoretical Results,[0],[0]
", will denote inequality and equality ignoring constant and polylog terms.
",3.2. Theoretical Results,[0],[0]
"Theorem 1 (Informal, Regret of BOCA).",3.2. Theoretical Results,[0],[0]
"Let g ∼ GP(0, κ) where κ satisfies (3).",3.2. Theoretical Results,[0],[0]
Choose βt d log(t/δ).,3.2. Theoretical Results,[0],[0]
"Then, for sufficiently large Λ and for all α ∈ (0, 1), there exists ρ depending on α such that the following bound holds with probability at least 1− δ.
S(Λ) .",3.2. Theoretical Results,[0],[0]
√ ΨnΛ(Xρ) nΛ + √ ΨnαΛ(X ),3.2. Theoretical Results,[0],[0]
"n2−αΛ
In the above bound, the latter term vanishes fast due to the n−(1−α/2)Λ dependence.",3.2. Theoretical Results,[0],[0]
"When comparing this with (8), we see that we outperform GP-UCB by a factor of √ ΨnΛ(Xρ)/ΨnΛ(X ) √
vol(Xρ)/vol(X ) asymptotically.",3.2. Theoretical Results,[0],[0]
"If g is smooth across the fidelity space, Xρ is small and the gains over GP-UCB are significant.",3.2. Theoretical Results,[0],[0]
"If g becomes less smooth across Z , the bound decays gracefully, but we are never worse than GP-UCB up to constant factors.
",3.2. Theoretical Results,[0],[0]
Theorem 1 also has similarities to the bounds of Kandasamy et al. (2016a) who also demonstrate better regret than GPUCB by showing that it is dominated by queries inside a set X ′,3.2. Theoretical Results,[0],[0]
which contains the optimum.,3.2. Theoretical Results,[0],[0]
"However, their bounds depend critically on certain threshold hyper-parameters which determine the volume of X ′",3.2. Theoretical Results,[0],[0]
among other terms in their regret.,3.2. Theoretical Results,[0],[0]
"The authors of that paper note that their bounds will suffer if these hyper-parameters are not chosen appropriately, but do not provide theoretically justified methods to make this choice.",3.2. Theoretical Results,[0],[0]
"In contrast, many of the design choices for BOCA fall out naturally of our modeling assumptions.",3.2. Theoretical Results,[0],[0]
"Beyond this analogue, our results are not comparable to Kandasamy et al. (2016a) as the assumptions are different.
",3.2. Theoretical Results,[0],[0]
"Extensions: While we have focused on continuousZ , many of the ideas here can be extended to other settings.",3.2. Theoretical Results,[0],[0]
"If Z is a discrete subset of [0, 1]p our work extends straightforwardly.",3.2. Theoretical Results,[0],[0]
We reiterate that this will not be the same as the finite fidelity MF-GP-UCB algorithm as the assumptions are different.,3.2. Theoretical Results,[0],[0]
"In particular, Kandasamy et al. (2016a) are not able to effectively share information across fidelities as we do.",3.2. Theoretical Results,[0],[0]
We also believe that Algorithm 1 can be extended to arbitrary fidelity spaces Z provided that a kernel can be defined on Z .,3.2. Theoretical Results,[0],[0]
Our results can also be extended to discrete domains X and various other kernels for φX by adopting techniques from Srinivas et al. (2010).,3.2. Theoretical Results,[0],[0]
"We compare BOCA to the following four baselines: (i) GPUCB, (ii) the GP-EI criterion in BO (Jones et al., 1998), (iii) MF-GP-UCB (Kandasamy et al., 2016a) and (iv) MF-SKO, the multi-fidelity sequential kriging optimisation method from Huang et al. (2006).",4. Experiments,[0],[0]
All methods are based on GPs and we use the SE kernel for both the fidelity space and domain.,4. Experiments,[0],[0]
"The first two are not multi-fidelity methods, while the last two are finite multi-fidelity methods2.",4. Experiments,[0],[0]
Kandasamy et al. (2016a) also study some naive multi-fidelity algorithms and demonstrate that they do not perform well; as such we will not consider such alternatives here.,4. Experiments,[0],[0]
"In all our experiments, the fidelity space was designed to be Z =",4. Experiments,[0],[0]
"[0, 1]p with z• = 1p =",4. Experiments,[0],[0]
"[1, . .",4. Experiments,[0],[0]
.,4. Experiments,[0],[0]
", 1] ∈",4. Experiments,[0],[0]
"Rp being the most expensive fi-
2To our knowledge, the only other work that applies to continuous approximations is Klein et al. (2015) which was developed specifically for hyper-parameter tuning.",4. Experiments,[0],[0]
"Further, their implementation is not made available and is not straightforward to implement.
delity.",4. Experiments,[0],[0]
"For MF-GP-UCB and MF-SKO, we used 3 fidelities (2 approximations) where the approximations were obtained at z = 0.3331p and z = 0.6671p in Z .",4. Experiments,[0],[0]
"Empirically, we found that both algorithms did reasonably well with 1-3 approximations, but did not perform well with a large number of approximations (> 5); even the original papers restrict experiments to 1-3 approximations.",4. Experiments,[0],[0]
Implementation details for all methods are given in Appendix C.1.,4. Experiments,[0],[0]
The results for the first set of synthetic experiments are given in Fig. 3.,4.1. Synthetic Experiments,[0],[0]
"The title of each figure states the function used, and the dimensionalities p, d of the fidelity space and domain.",4.1. Synthetic Experiments,[0],[0]
"To reflect the setting in our theory, we add Gaussian noise to the function value when observing g at any (z, x).",4.1. Synthetic Experiments,[0],[0]
This makes the problem more challenging than standard global optimisation problems where function evaluations are not noisy.,4.1. Synthetic Experiments,[0],[0]
"The functions g, the cost functions λ and the noise variances η2 are given in Appendix C.2.
",4.1. Synthetic Experiments,[0],[0]
The first two panels in Fig. 3 are simple sanity checks.,4.1. Synthetic Experiments,[0],[0]
"In both cases, Z = [0, 1], X = [0, 1] and the functions were sampled from GPs.",4.1. Synthetic Experiments,[0],[0]
"The GP was made known to all methods, i.e. all methods used the true GP in picking the next point.",4.1. Synthetic Experiments,[0],[0]
"In the first panel, we used an SE kernel with bandwidth 0.1 for φX and 1.0 for φZ .",4.1. Synthetic Experiments,[0],[0]
"g is smooth across Z in this setting, and BOCA outperforms other baselines.",4.1. Synthetic Experiments,[0],[0]
The curve starts mid-way as BOCA is yet to query at z• up until that point.,4.1. Synthetic Experiments,[0],[0]
"The second panel uses the same set up as the first except
we used bandwidth 0.01 for φZ .",4.1. Synthetic Experiments,[0],[0]
"Even though g is highly un-smooth across Z , BOCA does not perform poorly.",4.1. Synthetic Experiments,[0],[0]
This corroborates a claim that we made earlier that BOCA can naturally adapt to the smoothness of the approximations.,4.1. Synthetic Experiments,[0],[0]
"The other multi-fidelity methods suffer in this setting.
",4.1. Synthetic Experiments,[0],[0]
"In the remaining experiments, we use some standard benchmarks for global optimisation.",4.1. Synthetic Experiments,[0],[0]
We modify them to obtain g and add noise to the observations.,4.1. Synthetic Experiments,[0],[0]
"As the kernel and other GP hyper-parameters are unknown, we learn them by maximising the marginal likelihood every 25 iterations.",4.1. Synthetic Experiments,[0],[0]
We outperform all methods on all problems except in the case of the Borehole function where MF-GP-UCB does better.,4.1. Synthetic Experiments,[0],[0]
The last synthetic experiment is the Branin function given in Fig. 4(a).,4.1. Synthetic Experiments,[0],[0]
"We used the same set up as above, but use 10 fidelities for MF-GP-UCB and MF-SKO where the kth fidelity is obtained at z = k101p in the fidelity space.",4.1. Synthetic Experiments,[0],[0]
Notice that the performance of finite fidelity methods deteriorate.,4.1. Synthetic Experiments,[0],[0]
"In particular, as MF-GP-UCB does not share information across fidelities, the approximations need to be designed carefully for the algorithm to work well.",4.1. Synthetic Experiments,[0],[0]
Our more natural modelling assumptions prevent such pitfalls.,4.1. Synthetic Experiments,[0],[0]
We next present two real examples in astrophysics and hyper-parameter tuning.,4.1. Synthetic Experiments,[0],[0]
"We do not add noise to the observations, but treat it as optimisation tasks, where the goal is to maximise the function.",4.1. Synthetic Experiments,[0],[0]
"We use data on TypeIa supernova for maximum likelihood inference on 3 cosmological parameters, the Hubble con-
stant H0 ∈ (60, 80), the dark matter fraction ΩM ∈ (0, 1) and dark energy fraction ΩΛ ∈ (0, 1); hence d = 3.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"The likelihood is given by the Robertson-Walker metric, the computation of which requires a one dimensional numerical integration for each point in the dataset.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"Unlike typical maximum likelihood problems, here the likelihood is only accessible via point evaluations.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
We use the dataset from Davis et al (2007) which has data on 192 supernovae.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
We construct a p = 2 dimensional multi-fidelity problem where we can choose between data set size N ∈,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[50, 192] and perform the integration on grids of size G ∈",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[102, 106] via the trapezoidal rule.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"As the cost function for fidelity selection, we used λ(N,G) = NG as the computation time is linear in both parameters.",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
Our goal is to maximise the average log likelihood at z• =,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[192, 106].",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
For the finite fidelity methods we use three fidelities with the approximations available at z =,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"[97, 2.15× 103] and z = [145, 4.64× 104] (which correspond to 0.3331p and 0.6671p after rescaling as in Section 4.1).",4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
The results are given in Fig. 4(b) where we plot the maximum average log likelihood against wall clock time as that is the cost in this experiment.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
The plot includes the time taken by each method to tune the GPs and determine the next points/fidelities for evaluation.,4.2. Astrophysical Maximum Likelihood Inference,[0],[0]
"We use the 20 news groups dataset (Joachims, 1996) in a text classification task.",4.3. Support Vector Classification with 20 news groups,[0],[0]
"We obtain the bag of words representation for each document, convert them to tf-idf features and feed them to a support vector classifier.",4.3. Support Vector Classification with 20 news groups,[0],[0]
"The goal is to tune the regularisation penalty and the temperature of the rbf kernel both in the range [10−2, 103]; hence d = 2.",4.3. Support Vector Classification with 20 news groups,[0],[0]
The support vector implementation was taken from scikit-learn.,4.3. Support Vector Classification with 20 news groups,[0],[0]
We set this up as a 2 dimensional multi-fidelity problem where we can choose a dataset size N ∈,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[5000, 15000] and the number of training iterations T ∈ [20, 100].",4.3. Support Vector Classification with 20 news groups,[0],[0]
Each evaluation takes the given dataset of size N and splits it up into 5 to perform 5-fold cross validation.,4.3. Support Vector Classification with 20 news groups,[0],[0]
"As the cost function for fidelity selection, we used λ(N,T ) =",4.3. Support Vector Classification with 20 news groups,[0],[0]
"NT as
the training/validation complexity is linear in both parameters.",4.3. Support Vector Classification with 20 news groups,[0],[0]
Our goal is to maximise the cross validation accuracy at z• =,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[15000, 100].",4.3. Support Vector Classification with 20 news groups,[0],[0]
For the finite fidelity methods we use three fidelities with the approximations available at z =,4.3. Support Vector Classification with 20 news groups,[0],[0]
"[8333, 47] and z =",4.3. Support Vector Classification with 20 news groups,[0],[0]
"[11667, 73].",4.3. Support Vector Classification with 20 news groups,[0],[0]
The results are given in Fig. 4(c) where we plot the average cross validation accuracy against wall clock time.,4.3. Support Vector Classification with 20 news groups,[0],[0]
"We studied Bayesian optimisation with continuous approximations, by treating the approximations as arising out of a continuous fidelity space.",5. Conclusion,[0],[0]
"While previous multi-fidelity literature has predominantly focused on a finite number of approximations, BOCA applies to continuous fidelity spaces and can potentially be extended to arbitrary spaces.",5. Conclusion,[0],[0]
We bound the simple regret for BOCA and demonstrate that it is better than methods such as GP-UCB which ignore the approximations and that the gains are determined by the smoothness of the fidelity space.,5. Conclusion,[0],[0]
"When compared to existing multi-fidelity methods, BOCA is able to share information across fidelities effectively, has more natural modelling assumptions and has fewer hyper-parameters to tune.",5. Conclusion,[0],[0]
"Empirically, we demonstrate that BOCA is competitive with other baselines in synthetic and real problems.",5. Conclusion,[0],[0]
"Another nice feature of using continuous approximations is that it relieves the practitioner from having to design the approximations; she/he can specify the available approximations and let the algorithm decide how to choose them.
",5. Conclusion,[0],[0]
"Going forward, we wish to extend our theoretical results to more general settings.",5. Conclusion,[0],[0]
"For instance, we believe a stronger bound on the regret might be possible if φZ is a finite dimensional kernel.",5. Conclusion,[0],[0]
"Since finite dimensional kernels are typically not radial (Sriperumbudur et al., 2016), our analysis techniques will not carry over straightforwardly.",5. Conclusion,[0],[0]
Another line of work that we have alluded to is to study more general fidelity spaces with an appropriately defined kernel φZ .,5. Conclusion,[0],[0]
We would like to thank Renato Negrinho for reviewing an initial draft of this paper.,Acknowledgements,[0],[0]
This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887.,Acknowledgements,[0],[0]
KK is supported by a Facebook Ph.D. fellowship.,Acknowledgements,[0],[0]
"Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design.",abstractText,[0],[0]
"Recently, multifidelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications.",abstractText,[0],[0]
Multifidelity methods use cheap approximations to the function of interest to speed up the overall optimisation process.,abstractText,[0],[0]
"However, most multi-fidelity methods assume only a finite number of approximations.",abstractText,[0],[0]
"On the other hand, in many practical applications, a continuous spectrum of approximations might be available.",abstractText,[0],[0]
"For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data N and/or few training iterations T .",abstractText,[0],[0]
"Here, the approximations are best viewed as arising out of a continuous two dimensional space (N,T ).",abstractText,[0],[0]
"In this work, we develop a Bayesian optimisation method, BOCA, for this setting.",abstractText,[0],[0]
We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations.,abstractText,[0],[0]
BOCA outperforms several other baselines in synthetic and real experiments.,abstractText,[0],[0]
Multi-fidelity Bayesian Optimisation with Continuous Approximations,title,[0],[0]
"Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function. Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost. A canonical example is that of hyper-parameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations – this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion. We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning. We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret. We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.",text,[0],[0]
"Optimizing a black-box function f over a Euclidean domain X is a classical problem studied in several disciplines including computer science, mathematics, and operations research.",1. Introduction,[0],[0]
"It finds applications in many real world scientific and engineering tasks including scientific experimentation, industrial design, and model selection in statistics and machine learning (Martinez-Cantin et al., 2007; Parkinson et al., 2006; Snoek et al., 2012).",1. Introduction,[0],[0]
"Given a budget of n evaluations, an optimization algorithm operates sequentially – at time t, it chooses to evaluate f at xt based on its previous evaluations {xi, f(xi)} t 1 i=1 .",1. Introduction,[0],[0]
"At the end of n evaluations, it makes a recommendation x(n) and its performance is measured by its 1Univerity of Texas as Austin 2Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Rajat Sen <rajat.sen@utexas.edu>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"simple regret Rn,
Rn = sup x2X f(x) f(x(n)).
",1. Introduction,[0],[0]
Our study focuses on applications where exact evaluation of the function f is expensive.,1. Introduction,[0],[0]
"As an example, in the case of model selection, training and validating large neural networks can take several hours to days.",1. Introduction,[0],[0]
"Similarly, the simulation of an astrophysical process typically takes multiple days even on a cluster of super computers.",1. Introduction,[0],[0]
Traditional methods for black-box optimization are poorly suited for such applications because we need to invest a considerable number of evaluations to optimize f .,1. Introduction,[0],[0]
"This motivates studying the multi-fidelity setting where we have cheaper, but potentially biased approximations to the function (Cutler et al., 2014; Huang et al., 2006a; Kandasamy et al., 2016c; 2017).",1. Introduction,[0],[0]
"As an illustration, in a hyper-parameter tuning scenario, the task is to find the best set of hyper-parameters for training a machine learning model.",1. Introduction,[0],[0]
"In this setting, the black-box function that needs to be optimized is the validation error after training the learning algorithm to completion (a certain number of maximum iterations)",1. Introduction,[0],[0]
i.e X represents the allowed set of hyper-parameters while the function represents the validation error after training to completion.,1. Introduction,[0],[0]
"However, as training the algorithm till completion is expensive, we may choose to train the learning algorithm for a few iterations at chosen hyper-parameters and then test it on the validation set.",1. Introduction,[0],[0]
"These inexpensive validation errors act as the cheap approximations (fidelities) to the function value and can indeed provide valuable information regarding the quality of the hyper-parameters.
",1. Introduction,[0],[0]
"The multi-fidelity setup for black-box function optimization has been popularly studied in the Bayesian optimization setting (Huang et al., 2006b; Kandasamy et al., 2016b; 2017).",1. Introduction,[0],[0]
"However, in this paper we focus on another powerful framework for sequential black-box optimization that works with hierarchical partitioning of the function domain X .",1. Introduction,[0],[0]
"These tree-search based methods were initially motivated by an empirically successful heuristic UCT (Kocsis & Szepesvári, 2006), which subsequently lead to several theoretically founded algorithms for black-box optimization through hierarchical partitioning (Bubeck et al., 2011; Kleinberg et al., 2008; Munos, 2011; Valko et al., 2013).
",1. Introduction,[0],[0]
"In this work, we incorporate cheap approximations or fideli-
ties with tree-search based methods for black-box optimization.",1. Introduction,[0],[0]
"We assume access to a tree-like partitioning of the domain X similar to (Bubeck et al., 2011; Grill et al., 2015; Munos, 2011).",1. Introduction,[0],[0]
"The partitioning of the domain X is denoted as P and it contains hierarchical cells {Ph,i}, where h denotes the height of the cell and i denotes the index.",1. Introduction,[0],[0]
"A cell Ph,i at height h has K children {Ph+1,ik}Kk=1, which are distinct partitions of Ph,i. At height 0, there is only one partition P0,1 = X .",1. Introduction,[0],[0]
An example of such an hierarchical partition for X =,1. Introduction,[0],[0]
"[0, 1] and K = 2 would be:",1. Introduction,[0],[0]
"P0,1 =",1. Introduction,[0],[0]
"[0, 1], P1,1,P1,2 =",1. Introduction,[0],[0]
"[0, 0.5], (0.5, 1], P2,1 =",1. Introduction,[0],[0]
"[0, 0.25]... and so on.",1. Introduction,[0],[0]
Most of the prior work assume some smoothness property about the function and hierarchical partitioning.,1. Introduction,[0],[0]
"We follow a similar path adopting the smoothness assumptions in (Grill et al., 2015).",1. Introduction,[0],[0]
"This assumption states that there exists ⌫ and ⇢ 2 (0, 1) such that,
8h 0, 8x 2",1. Introduction,[0],[0]
"Ph,i⇤h , f(x) f(x ⇤) ⌫⇢h, (1)
where x⇤ is assumed to be the unique point in X such that f(x⇤) = supx2X f(x).",1. Introduction,[0],[0]
"This assumption basically says that the diameter of the function is bounded for all cells that contain the optima, and that this diameter goes down at a geometric rate with the height of the cell.",1. Introduction,[0],[0]
"We also adopt the definition of the well-known near-optimality dimension d(⌫, ⇢) which restricts the number of cells at height h that contain points close to the optima.",1. Introduction,[0],[0]
"This is an important quantity in the analysis of many tree-search based methods (Bubeck et al., 2011; Grill et al., 2015; Munos, 2011; Valko et al., 2013).
",1. Introduction,[0],[0]
In addition we also model that the function can be accessed at a continuous range of fidelities within Z =,1. Introduction,[0],[0]
"[0, 1],",1. Introduction,[0],[0]
where z = 0 is the cheapest fidelity and z = 1 is the most expensive one.,1. Introduction,[0],[0]
"For instance, in our hyper-parameter tuning example, z = 1 may correspond to training the learning algorithm for 1000 iterations while z = 0 represents training the algorithm to 50 iterations.",1. Introduction,[0],[0]
"When a function is evaluated at a point x 2 X with a fidelity z 2 Z , a value fz(x) is revealed such that |f(x) fz(x)| < ⇣(z) where ⇣(.) is a fixed bias function.",1. Introduction,[0],[0]
The bias function is monotonically decreasing in z with ⇣(1) = 0.,1. Introduction,[0],[0]
There is also a cost associated with these evaluations which is captured by a cost function : Z !,1. Introduction,[0],[0]
R+.,1. Introduction,[0],[0]
The cost function is assumed to be monotonically increasing in z.,1. Introduction,[0],[0]
"For instance, in hyper-parameter tuning the cost increases linearly with the number of iterations.",1. Introduction,[0],[0]
"The objective is to locate a point x such that f(x) is as close as possible to supx2X f(x) given a finite cost budget ⇤.
",1. Introduction,[0],[0]
"The following are the main contributions of this work:
(i)",1. Introduction,[0],[0]
We incorporate multiple fidelities/cheap approximations in black-box function optimization through hierarchical partitioning.,1. Introduction,[0],[0]
We propose and analyze two algorithms in this setting.,1. Introduction,[0],[0]
"Our first algorithm is known as MFDOO (Algorithm 1) which requires knowledge about the smoothness
parameter (⌫, ⇢).",1. Introduction,[0],[0]
"This algorithm is similar to DOO (Munos, 2011), however it is designed to explore coarser partitions at lower fidelities while exploring finer partitions at higher fidelities, when the algorithm zooms in on a promising area of the function domain.",1. Introduction,[0],[0]
"Motivated by recent work (Grill et al., 2015), we also propose a second algorithm MFPDOO (Algorithm 2), which does not require knowledge about the smoothness.",1. Introduction,[0],[0]
"This algorithm spawns several instances of MFDOO (Algorithm 1) with carefully selected parameters, at least one of which is bound to perform nearly as well as MFDOO when the smoothness parameters are known.
",1. Introduction,[0],[0]
"(ii) We provide simple regret bounds for both of our algorithms, given a fixed cost budget ⇤ for performing evaluations.",1. Introduction,[0],[0]
"First we show that when the smoothness parameters are known, MFDOO has simple regret of O(⇤ 1/d(⌫,⇢)+1) under some conditions on the bias and cost function.",1. Introduction,[0],[0]
"Here, d(⌫, ⇢) is the near-optimality dimension of the function with respect to parameters (⌫, ⇢).",1. Introduction,[0],[0]
"On the other hand a naive application of DOO (Munos, 2011)1 only using the highest fidelity z = 1 would yield a regret bound of O((⇤/ (1)) 1/d(⌫,⇢)).",1. Introduction,[0],[0]
"We also show that our second algorithm MFPDOO can obtain a simple regret bound of O((⇤/ log⇤) 1/d(⌫,⇢)+1) even when the smoothness parameters are not known.",1. Introduction,[0],[0]
"The precise details about our theoretical guarantees can be found in Section 5.
(iii) Finally, we compare the performance of our algorithms with several state of the art algorithms (Grill et al., 2015; Huang et al., 2006b; Jones et al., 1998; Kandasamy et al., 2016b;b; Srinivas et al., 2009) for black-box optimization in the multi-fidelity setting, on real and synthetic data-sets.",1. Introduction,[0],[0]
We demonstrate that our algorithms outperform the state of the art in most of these experiments.,1. Introduction,[0],[0]
"We build on a line of work on bandits and black-box optimization with hierarchical partitions (Bubeck et al., 2011; Kleinberg et al., 2008; Munos, 2011; Valko et al., 2013).",2. Related Work,[0],[0]
"These methods rely on the principle of optimism i.e they build upper bounds on the value of the functions inside different partitions using the already explored points x1..., xt.",2. Related Work,[0],[0]
"Then at time t+ 1, a point is chosen from the partition that has the highest value of this upper-bound.",2. Related Work,[0],[0]
"We closely follow the line of work initiated in (Munos, 2011) that was later extended to noisy function evaluations in (Grill et al., 2015; Valko et al., 2013).",2. Related Work,[0],[0]
"In (Munos, 2011) it was assumed that the function follows a local Lipschitz condition with respect to a semi-metric `, and the diameter of the hierarchical partitions with respect to this semi-metric decrease geometrically with height.",2. Related Work,[0],[0]
"Grill et al. (Grill et al., 2015) later merged these
1Note that DOO also requires knowledge of the smoothness parameters of the function.
",2. Related Work,[0],[0]
"two assumptions into one, by having a single condition that related the smoothness of the function with the hierarchical partition.",2. Related Work,[0],[0]
"In this work we adapt the regime of (Grill et al., 2015).",2. Related Work,[0],[0]
"However, we also model cheap approximations to the functions through a one-dimensional fidelity space.
",2. Related Work,[0],[0]
"Multi-fidelity optimization has had a rich history in many settings (Agarwal et al., 2011; Forrester et al., 2007; Huang et al., 2006a; Klein et al., 2016; Lam et al., 2015; Li et al., 2016; Poloczek et al., 2016; Sabharwal et al., 2015; Zhang & Chaudhuri, 2015), with those that are not applicationspecific focusing on a Bayesian framework without formal guarantees (we refer to (Kandasamy et al., 2017) for additional discussion).",2. Related Work,[0],[0]
Kandasamy et al. (2016c) propose and analyse a UCB style multi-fidelity algorithm for the K-armed bandit setting assuming a finite number of approximations to the K arms.,2. Related Work,[0],[0]
"They then extend this work to develop UCB algorithms for black-box optimization under Bayesian Gaussian process assumptions on f , both with a finite number of approximations and a continuous spectrum of approximations (Kandasamy et al., 2016a;b; 2017).",2. Related Work,[0],[0]
"In all these works, the relation between the approximations and the true function is known and appears in the form of uniform bounds on the approximation or a smoothness assumption arising out of the kernel of the Gaussian process.",2. Related Work,[0],[0]
In our work we merge the multi-fidelity setting with the hierarchical partitions framework.,2. Related Work,[0],[0]
We consider the problem of optimizing a function f : X !,3. Problem Setting,[0],[0]
R with black-box access at different fidelities.,3. Problem Setting,[0],[0]
"The aim is to locate a point x such that f(x) is as close as possible to supx2X f(x), given a finite budget for performing evaluations.
",3. Problem Setting,[0],[0]
"We assume that the function can be queried at a continuous range of fidelities in Z , [0, 1].",3. Problem Setting,[0],[0]
"When the function is queried at a point x 2 X with fidelity z 2 Z , a value fz(x) is revealed.",3. Problem Setting,[0],[0]
"We assume that |fz(x) f(x)|  ⇣(z), where ⇣ : Z ! R+ is a known bias function.",3. Problem Setting,[0],[0]
"It is also assumed that a single query at fidelity z incurs a cost (z), where : Z !",3. Problem Setting,[1.0],"['It is also assumed that a single query at fidelity z incurs a cost (z), where : Z !']"
R+ is a known cost function.,3. Problem Setting,[0],[0]
"We assume there is a unique point x⇤ 2 X at which supx2X f(x) is achieved.
",3. Problem Setting,[0],[0]
Bias and Cost Functions: The bias function ⇣ is assumed to be bounded and monotonically decreasing in z.,3. Problem Setting,[0],[0]
The optimal fidelity z is assumed to have zero bias i.e. ⇣(1) = 0.,3. Problem Setting,[1.0],['The optimal fidelity z is assumed to have zero bias i.e. ⇣(1) = 0.']
"The cost function is assumed to be bounded and monotonically increasing in z.
The multi-fidelity setting is motivated by engineering applications where cheap approximations are available.",3. Problem Setting,[0],[0]
"One promising use case is that of hyper-parameter tuning, where the validation performance of a learning algorithm at different hyper-parameters can be observed.",3. Problem Setting,[0],[0]
"The aim is to locate
the best hyper-parameter.",3. Problem Setting,[0],[0]
"In such a setting, cheap approximations are available for instance instead of evaluating the learning algorithm after a maximum of T iterations, one may choose to evaluate it after t < T iterations.",3. Problem Setting,[0],[0]
In this case T can be mapped to z = 1,3. Problem Setting,[0],[0]
and t can be mapped to a z < 1.,3. Problem Setting,[0],[0]
The cost function in this setting is proportional to the O(t) computation required.,3. Problem Setting,[0],[0]
"The bias function is monotonically decreasing with z, however may not be known exactly in practice.",3. Problem Setting,[0],[0]
"However, prior works in multi-fidelity setup (Kandasamy et al., 2016a;c; Kleinberg et al., 2008) have all assumed access to a known bias function for the theoretical guarantees.",3. Problem Setting,[0],[0]
"Even though we assume the bias function is known in theory, we shall see in our experiments in Section 6 that a simple parametric form of the bias function can be assumed and the parameters can be updated online during the course of our algorithm (similar to (Kandasamy et al., 2017)).
",3. Problem Setting,[0],[0]
Simple Regret:,3. Problem Setting,[0],[0]
The objective is to locate a point x such that f(x) is as close as possible to supx2X f(x) given a finite cost budget.,3. Problem Setting,[0],[0]
Let ⇤ be the total cost budget allowed.,3. Problem Setting,[0],[0]
"Consider an optimization policy that queries a sequence of points {x1, ..., xn(⇤)} at fidelities {z1, ..., zn(⇤)} respectively and finally returns a recommendation x⇤.",3. Problem Setting,[0],[0]
"Our main quantity of interest is the simple regret which is defined as,
R⇤ = sup x2X f(x) f (x⇤) , (2)
such that Pn(⇤)
i=1",3. Problem Setting,[0],[0]
(zi)  ⇤.,3. Problem Setting,[0],[0]
Note that the simple regret is always measured at the highest fidelity as we are only interested in optimizing the actual function.,3. Problem Setting,[0],[0]
"In this section we define the hierarchical partitions of the domain X that we assume access to and then provide our technical assumptions about the function and the hierarchical partitions.
Hierarchical Partitions: We assume access to a tree-like hierarchical partitioning P = {Ph,i} of the domain X , where, h denotes a depth parameter.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"For any depth h 0, the cells {Ph,i}1iIh denote a partitioning of the space X , where Ih is the number of cells at depth h. At depth 0",3.1. Hierarchical Partitions and Assumptions,[0.9841398408186005],"['For any depth h 0, the cells {Ph,i}1iIh denote a partitioning of the space X , where Ih is the number of cells at depth h. At depth 0 there is a single cell P0,1 = X .']"
"there is a single cell P0,1 = X .",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"A cell Ph,i can be split into K child nodes at depth h + 1.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"In what follows, querying a cell Ph,i would refer to evaluating the function at a fixed representative point xh,i 2 Ph,i at a chosen fidelity.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"The fixed representative point is usually chosen to be the coordinate wise mid-point for any given cell.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
As an illustrative example let us consider a hierarchical black-box optimization problem over the domain X =,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[0, 1] ⇥",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[0, 1].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Let us consider a hierarchical partition of this domain where the cells are of the form {x 2 X : b1,1  x1 < b1,2, b2,1  ",3.1. Hierarchical Partitions and Assumptions,[0.9840508925170944],"['Let us consider a hierarchical partition of this domain where the cells are of the form {x 2 X : b1,1  x1 < b1,2, b2,1  x2 < b2,2}.']"
x2 <,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"b2,2}.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Such a
cell will be denoted by the notation",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[b1,1, b1,2], [b2,1, b2,2]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Then a hierarchical partition with K = 2 starts with the root node P0,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0, 1], [0, 1]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"This can be further sub-divided into children cells at h = 1 given by P1,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0, 0.5], [0, 1]] and P1,2 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0, 1]].",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"P1,2 can be further partitioned into P2,1 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0, 0.5]] and P2,2 =",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[0.5, 1], [0.5, 1]] and so on.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
The fixed representative point for a cell,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"[[b1,1, b1,2], [b2,1, b2,2]] is chosen as the point [(b1,1 + b1,2)/2, (b2,1 + b2,2)/2].
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Black-box optimization is akin to a needle in a haystack problem without any conditions on the function f(x).,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Therefore, similar to prior work (Grill et al., 2015) we make the following smoothness assumption which depends on the properties of both the function f and the hierarchical partitioning P .",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Assumption 1 (Smoothness Decay).,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"There exists ⌫ and ⇢ 2 (0, 1) such that,
8h 0, 8x 2",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Ph,i⇤h , f(x) f(x ⇤) ⌫⇢h, (3)
where Ph,i⇤h is the unique partition of height h which contains x⇤.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"We also adopt the definition of the near-optimalitydimension for parameters (⌫, ⇢) from (Grill et al., 2015).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"This is a quantity that depends on the choice of parameters, the partitioning and the function itself.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
Definition 1.,3.1. Hierarchical Partitions and Assumptions,[0],[0]
"The near-optimality dimension of f with respect to parameters (⌫, ⇢) is given by,
d(⌫, ⇢) , inf d 0 2 R+ : 9C(⌫, ⇢), 8h 0,
Nh(2⌫⇢ h)  ",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"C(⌫, ⇢)⇢",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"d
0h o
(4)
where Nh(✏) is the number of cells",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Ph,i such that supx2Ph,i f(x) f(x ⇤) ✏.
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Let (⌫⇤, ⇢⇤) be the parameters with the minimum near optimality dimension d(⌫⇤, ⇢⇤).
",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Discussion: Access to hierarchical partitions have been assumed in a string of previous works on black-box optimization (Bubeck et al., 2011; Grill et al., 2015; Kleinberg et al., 2008; Munos, 2011; Slivkins, 2011; Valko et al., 2013).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"Many of these prior works assume a semi-metric ` over the domain X (Bubeck et al., 2011; Munos, 2011; Valko et al., 2013).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"In (Bubeck et al., 2011), it is assumed that the function satisfies a weak-Lipschitzness condition.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"More recent works (Munos, 2011; Valko et al., 2013) have assumed a local-smoothness property w.r.t the metric given by 8x 2 X , f(x⇤) f(x)  ",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"`(x, x⇤).",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"However, recently Grill et al. (Grill et al., 2015) have observed that Assumption 1 is sufficient to combine several assumptions about the semi-metric, the function and the hierarchical partitions into one combinatorial condition and similarly have adapted
the definition of the near-optimality dimension without the semi-metric.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
"It was depicted in (Grill et al., 2015) that prior algorithms like (Bubeck et al., 2011; Munos, 2011; Valko et al., 2013) can be shown to have good regret guarantees with this new set of assumptions, and therefore we adopt these assumptions in our work.",3.1. Hierarchical Partitions and Assumptions,[0],[0]
In this section we present two algorithms for black-box optimization using different fidelities and the hierarchical partitioning provided.,4. Algorithms,[0],[0]
"In Section 4.1, we provide Algorithm 1 which requires the knowledge of the optimal smoothness decay parameters (⌫⇤, ⇢⇤).",4. Algorithms,[0],[0]
"Then in Section 4.2, we provide Algorithm 2 that searches for the optimal smoothness by spawning O(log⇤) instances of Algorithm 1 with a carefully designed sequence of smoothness parameters (⌫, ⇢) as arguments.",4. Algorithms,[0],[0]
"In this section we provide an algorithm which takes as an argument the smoothness parameters (⌫, ⇢).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"We show in Section 5 that if the parameters provided match with the optimal parameters (⌫⇤, ⇢⇤) then the algorithm enjoys strong theoretical guarantees under some conditions on the bias and cost functions ⇣(z) and (z) respectively.
","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Algorithm 1 MFDOO: Multi-Fidelity Deterministic Optimistic Optimization
1: Arguments: (⌫, ⇢), ⇣(z), (z), P , ⇤ 2: Define zh = ⇣ 1(⌫⇢h) 3: Let T = {(0, 1)} be the tree initialized (root node
evaluated with fidelity z0).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0.977137645646295],"['Algorithm 2 MFPDOO: Multi-Fidelity Parallel Deterministic Optimistic Optimization 1: Arguments: (⌫max, ⇢max), ⇣(z), (z), P , ⇤ 2: Let N = (1/2)Dmax log(⇤/ log(⇤)) where Dmax = logK/ log(1/⇢max) 3: for i = 0 to N 1 do 4: Spawn MFDOO (Algorithm 1) with parameters (⌫max, ⇢ N/(N i) max ) with budget (⇤ N (1))/N 5: end for 6: Let x(i)⇤ be the point returned by the i th MFDOO instance for i 2 {0, .., N 1}.']"
Set of leaves at time t: Lt. 4: Time: t = 1; Cost: C = (z0).,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"5: while C  ⇤ do 6: Select the leaf (h, j) 2 Lt with maximum bh,j ,
fzh(xh,j) + ⌫⇢ h + ⇣(zh).
7: Expand this node; add to Tt the K children of (h, j).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
8: Evaluate the children at the fidelity level zh+1.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
t =,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
t+ 1. C = C +K (zh+1).,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
9: end while 10: Let h(⇤) be the height of the tree.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Return x⇤ = argmax(h(⇤),i)","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"fzh(⇤)(xh(⇤),i).
","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"In Algorithm 1, with some abuse of notation we define for all h 0, zh = ⇣ 1(⌫⇢h) i.e the fidelity at which the bias becomes less than or equal to the smoothness decay parameter at height h.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
All cells at height h are evaluated at the fidelity zh.,"4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"The intuition is that if x⇤ belongs to a cell Ph,i⇤ at height h that has been evaluated, then by Assumption 1 we have that all points in the cell are at least ⌫⇢
h optimal.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"Ideally, beyond this point we would only like to expand leaf nodes that are at least O(⌫⇢h) optimal,
which can only be achieved if the error due to the fidelities is O(⌫⇢h).","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"At each step, a leaf node with the highest upper bound parameter bh,i, is expanded and the children cells are evaluated.","4.1. Algorithm with known (⌫⇤, ⇢⇤)",[0],[0]
"In this section we describe an algorithm that does not require the optimal parameters (⌫⇤, ⇢⇤).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Algorithm 2 just requires ⇢max, ⌫max which are loose upper-bounds of ⇢⇤ and ⌫⇤ respectively.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"The algorithm proceeds by spawning O(log⇤) MFDOO instances with different (⌫, ⇢)’s which have been carefully designed.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Similar ideas were explored in a setting without fidelities in (Grill et al., 2015).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"In Section 5, we show that Algorithm 2 does almost as well as Algorithm 1 without requiring the optimal parameters as input.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[1.0000000787568286],"['In Section 5, we show that Algorithm 2 does almost as well as Algorithm 1 without requiring the optimal parameters as input.']"
Algorithm 2,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"MFPDOO: Multi-Fidelity Parallel Deterministic Optimistic Optimization
1: Arguments: (⌫max, ⇢max), ⇣(z), (z), P , ⇤ 2: Let N = (1/2)Dmax log(⇤/ log(⇤)) where Dmax =
logK/ log(1/⇢max) 3: for i = 0 to N 1 do 4: Spawn MFDOO (Algorithm 1) with parameters
(⌫max, ⇢ N/(N","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"i) max ) with budget (⇤ N (1))/N
5: end for 6: Let x(i)⇤ be the point returned by the i
th MFDOO instance for i 2 {0, .., N 1}.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Evaluate all {x(i)⇤ }i at the z = 1.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"Return the point x⇤ = x (i⇤) ⇤ where i ⇤ = argmaxi f(x (i) ⇤ ).
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Algorithm 2 proceeds by spawning N different MFDOO instances with the parameters specified in step 4 of the algorithm.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We will show in Theorem 2 that at least one of the MFDOO instances will have a performance comparable to Algorithm 1 supplied with parameters (⌫⇤, ⇢⇤) with a budget of O(⇤/N).","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[1.0],"['We will show in Theorem 2 that at least one of the MFDOO instances will have a performance comparable to Algorithm 1 supplied with parameters (⌫⇤, ⇢⇤) with a budget of O(⇤/N).']"
"Step 6 of the algorithm obtains the exact value of the points returned by all the MFDOO instances by evaluating them at the highest fidelity, and then chooses the one with the maximum value.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"This ensures that the highest performing MFDOO instance is selected.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Remark 1.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
Our algorithms and the theoretical results assume that the bias function ⇣(.) is known.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"However, in practice we do not assume perfect knowledge about the bias function.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We assume a simple parametric form of the bias function and update the parameters online, when the bias assumptions are violated.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"We provide more details in Section 6 and show that even without this knowledge, the algorithms perform better than other benchmarks.
","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"It should be noted that the different MFDOO instances created by Algorithm 2 can share information among each other, when multiple instances query the same partition at
very similar fidelities.","4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
This leads to huge improvements in practice in terms of effectively using the cost budget.,"4.2. Algorithm without the knowledge of (⌫⇤, ⇢⇤)",[0],[0]
"In this section we first prove a general result about the simple regret of Algorithm 1 which assumes access to the optimal parameters (⌫, ⇢).",5. Theoretical Results,[0],[0]
"This naturally implies a simple regret bound on Algorithm 1 when it is supplied with the parameters (⌫⇤, ⇢⇤) i.e. the ones that have the minimum near-optimality dimension.",5. Theoretical Results,[0],[0]
Then we refine these guarantees under some natural conditions on the bias and cost functions.,5. Theoretical Results,[1.0],['Then we refine these guarantees under some natural conditions on the bias and cost functions.']
"Finally, we show that Algorithm 2 can achieve guarantees close to Algorithm 1 with the optimal parameters, without having access to them.
",5. Theoretical Results,[0],[0]
"We first present the following general result about Algorithm 1.
Theorem 1.",5. Theoretical Results,[0],[0]
"Let h0 be the biggest number h such
hX
l=0
C(⌫, ⇢)K (zl)⇢ d(⌫,⇢)l  ",5. Theoretical Results,[0],[0]
"⇤.
",5. Theoretical Results,[0],[0]
Let h(⇤) = h0 + 1.,5. Theoretical Results,[0],[0]
"Then Algorithm 1 run with parameters (⌫, ⇢) (s.t ⌫ ⌫⇤, ⇢ ⇢⇤), incurs a simple regret of at most 2⌫⇢h(⇤) and terminates using a total cost of at most ⇤+K (1).
",5. Theoretical Results,[0],[0]
We defer the proof of Theorem 1 to Section A in the appendix.,5. Theoretical Results,[1.0],['We defer the proof of Theorem 1 to Section A in the appendix.']
"Note that the guarantee in Theorem 1 is the tightest when the parameters (⌫⇤, ⇢⇤) are supplied as the near optimality dimension d(⌫⇤, ⇢⇤) is the lowest.
",5. Theoretical Results,[0],[0]
"Now, we impose some natural conditions on the cost and bias functions.",5. Theoretical Results,[0],[0]
"We provide more specialized versions of the guarantees in Theorem 1 under these two conditions separately, which are described below.
Assumption 2.",5. Theoretical Results,[0],[0]
We assume that ⇣(.),5. Theoretical Results,[0],[0]
"and (.) are such that (z⇤h)  min{ h,⇤(1)} for some positive constant .",5. Theoretical Results,[0],[0]
"Here, z⇤h = ⇣ 1(⌫⇤⇢h⇤).
",5. Theoretical Results,[0],[0]
Motivation: The above assumption is motivated by the following hyper-parameter tuning scenario.,5. Theoretical Results,[0],[0]
Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a strongly convex and smooth function with gradient descent.,5. Theoretical Results,[1.0],['Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a strongly convex and smooth function with gradient descent.']
Let the fidelity denote a rescaled version of the number of steps in gradient descent n.,5. Theoretical Results,[0],[0]
We assume that at the optimal fidelity (N steps) we reach the optimal value of the function up to an error of ✏⇤.,5. Theoretical Results,[0.973361866041725],['Let the fidelity denote a rescaled version of the number of steps in gradient descent n. We assume that at the optimal fidelity (N steps) we reach the optimal value of the function up to an error of ✏⇤.']
Let zn = n/N .,5. Theoretical Results,[0],[0]
"At fidelity zn the error decays to ⇣(zn) = O(rn) for some r 2 (0, 1).",5. Theoretical Results,[0],[0]
The cost incurred is linear in the number of steps say (zn) =,5. Theoretical Results,[0],[0]
sn for s > 0.,5. Theoretical Results,[0],[0]
"In this setting it can be shown that if ⇣(zn) ⇠ ⌫⇤⇢h⇤ , then n = O(h) and therefore (zn) = O(h).
",5. Theoretical Results,[0],[0]
"The second assumption under which we provide specialized guarantees is as follows.
",5. Theoretical Results,[0],[0]
Assumption 3.,5. Theoretical Results,[0],[0]
We assume that ⇣(.),5. Theoretical Results,[0],[0]
"and (.) are such that (z⇤h)  min{ h ,⇤(1)} for some constant 2 (⇢, 1).",5. Theoretical Results,[0],[0]
"Here, z⇤h = ⇣ 1(⌫⇤⇢h⇤).
",5. Theoretical Results,[0],[0]
Motivation: Assumption 3 is motivated by a similar hyperparameter tuning scenario as above.,5. Theoretical Results,[0],[0]
Consider training a learning algorithm with a particular hyper-parameter that involves optimizing a smooth convex function with accelerated gradient descent.,5. Theoretical Results,[0],[0]
Let the fidelity denote a rescaled version of the number of steps in gradient descent n as above.,5. Theoretical Results,[0],[0]
At fidelity zn the error decays to ⇣(zn) = O(1/n2).,5. Theoretical Results,[0],[0]
The cost incurred is linear in the number of steps say (zn) =,5. Theoretical Results,[0],[0]
sn for s > 0.,5. Theoretical Results,[0],[0]
"In this setting it can be shown that if ⇣(zn) ⇠ ⌫⇤⇢h⇤ , then n = O( h) for = O( p ⇢).
",5. Theoretical Results,[0],[0]
"We are now at a position to introduce a specialized corollary of Theorem 1.
",5. Theoretical Results,[0],[0]
Corollary 1.,5. Theoretical Results,[0],[0]
"Algorithm 1 with parameters (⌫, ⇢) (s.t ⌫ ⌫⇤, ⇢ ⇢⇤) run with a total budget of ⇤ terminates with a total cost of at most ⇤ + K (1) and has the following properties: (i)",5. Theoretical Results,[0],[0]
Under Assumption 2: R⇤  2⌫ ⇣,5. Theoretical Results,[0],[0]
"C(⌫,⇢)K ⇤(1 ⇢d(⌫,⇢))",5. Theoretical Results,[0],[0]
"⌘ 1 d(⌫,⇢)+✏ for some small ✏ > 0, provided ⇤ is large enough.
",5. Theoretical Results,[0],[0]
"(ii) Under Assumption 3:
R⇤  2 ⌫ ⇢
⇣ 2C(⌫,⇢)K
⇤( 1⇢ d(⌫,⇢) 1)
⌘ 1 d(⌫,⇢)+1
.
",5. Theoretical Results,[0],[0]
"Comparison with DOO (Munos, 2011):",5. Theoretical Results,[0],[0]
"The above result can be directly compared to DOO (Munos, 2011) which is in the noiseless black-box optimization regime, without access to fidelities.",5. Theoretical Results,[0],[0]
"The simple regret of DOO under the same assumptions would scale as O ⇣ (⇤/ (1)) 1/d(⌫⇤,⇢⇤) ⌘
when all the evaluations are performed at the highest fidelity.",5. Theoretical Results,[0],[0]
"In contrast our bounds under Assumption 2 scales as O ⇣ (⇤/ ) 1/(d(⌫⇤,⇢⇤)+✏) ⌘ , where ✏ is a constant close
to zero.",5. Theoretical Results,[0],[0]
"Note that (z⇤h)  (1), and therefore = (1) trivially satisfies the inequality in Assumption 2.",5. Theoretical Results,[0],[0]
"Typically, is expected to be much less as compared to the highest fidelity cost (1).",5. Theoretical Results,[0],[0]
"For example in our hyper-parameter tuning example where the fidelity is the number of iterations (a maximum of N iterations), is a small constant (see the discussion on Assumption 2), while (1) can be O(N).",5. Theoretical Results,[0],[0]
This can lead to significant gains in simple regret as we show in our empirical results in Section 6.,5. Theoretical Results,[0],[0]
"Similarly, under Assumption 3 our simple regret scales as O ⇤ 1/(d(⌫⇤,⇢⇤)+1) , which can be much better than that of DOO (Munos, 2011) as the total budget is not divided by (1).
",5. Theoretical Results,[0],[0]
"Now, we will provide one of our main results which states that Algorithm 2 can recover simple regret bounds which
are close to that of Algorithm 1 even when the optimal smoothness parameters are not known.
",5. Theoretical Results,[0],[0]
Theorem 2.,5. Theoretical Results,[0],[0]
"Algorithm 2 when run with upper-bounds ⌫max and ⇢max with a total cost budget of ⇤ terminates after using up a cost of at most ⇤+O(K (1) log⇤) and has the following regret guarantees:
(i)",5. Theoretical Results,[0],[0]
"Under Assumption 2 the simple regret is O ⇣ (⌫max/⌫⇤) Dmax ✏+d(⌫⇤,⇢⇤)⇥",5. Theoretical Results,[0],[0]
"⇣ 2⇤
K Dmax log(⇤/ log⇤)
(1) K
⌘ 1✏+d(⌫⇤,⇢⇤) ◆
(ii)",5. Theoretical Results,[0],[0]
"Under Assumption 3 if ⇢max the simple regret is O ⇣ (⌫max/⌫⇤) 2Dmax 1+d(⌫⇤,⇢⇤)⇥",5. Theoretical Results,[0],[0]
"⇣ 2⇤
KDmax log(⇤/ log⇤)
(1) K
⌘ 11+d(⌫⇤,⇢⇤) ◆
.
",5. Theoretical Results,[0],[0]
"We defer the proof of this theorem to Appendix C.
Comparison with POO (Grill et al., 2015):",5. Theoretical Results,[0],[0]
"It is worthwhile to compare our result with that of POO (Grill et al., 2015) which uses only the highest fidelity.",5. Theoretical Results,[0],[0]
"It should be noted that POO is in a noisy setting, which gives rise to extra polylog factors in the bounds.",5. Theoretical Results,[0],[0]
"However, ignoring polylog factors the simple regret bound of POO would scale as O (⇤/(log(⇤/ (1))",5. Theoretical Results,[0],[0]
⇤ (1))),5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+2) .",5. Theoretical Results,[0],[0]
In contrast our bounds scale as O ⇣ (⇤/( log(⇤))),5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+✏) ⌘
and O (⇤/ log(⇤))",5. Theoretical Results,[0],[0]
"1/(d(⌫⇤,⇢⇤)+1) under assumptions 2 and 3 respectively.",5. Theoretical Results,[0],[0]
This can lead to much better performance at the same cost budget.,5. Theoretical Results,[0],[0]
We demonstrate this in our empirical results in Section 6.,5. Theoretical Results,[0],[0]
In this section we provide empirical results on synthetic and real datasets.,6. Empirical Results,[0],[0]
"We compare our algorithm with the following related works: (i) BOCA (Kandasamy et al., 2017) which is a multi-fidelity Gaussian Process (GP) based algorithm that can handle continuous fidelity spaces, (ii) MFGP-UCB (Kandasamy et al., 2016c) which is a GP based multi-fidelity method that can handle finite fidelities, (iii) GP-EI criterion in bayesian optimization (Jones et al., 1998), (iv) MF-SKO, the multi-fidelity sequential kriging optimisation method (Huang et al., 2006b), (v) GP-UCB (Srinivas et al., 2009) and (vi) MFPDOO(z = 1) which is a version of our algorithm that uses only the highest fidelity; this is very similar to POO (Grill et al., 2015) but in a noiseless setting.",6. Empirical Results,[0],[0]
"This algorithm is referred to as PDOO in the figures, which is essentially DOO (Munos, 2011) with the smoothness parameters tuned according to the scheme in POO (Grill et al., 2015).
",6. Empirical Results,[0],[0]
For our theoretical guarantees the bias function ⇣ is assumed to be known.,6. Empirical Results,[0],[0]
"However, in practice we assume a parametric
form for the bias function that is ⇣(z) = c(1 z) where c is initially set to a very small constant like 0.001 in our experiments.",6. Empirical Results,[0],[0]
The nature of Algorithm 2 is such that the same cells are queried at different fidelities by the different MFDOO instances spawned.,6. Empirical Results,[0],[0]
"If a cell is queried at two different fidelities z1 and z2 and the function values obtained are f1 and f2, then we update c to 2c whenever c|z1 z2| < |f1 f2|.",6. Empirical Results,[1.0],"['If a cell is queried at two different fidelities z1 and z2 and the function values obtained are f1 and f2, then we update c to 2c whenever c|z1 z2| < |f1 f2|.']"
The above update is only performed if |z1 z2| is greater than a specified threshold (0.0001 in our experiments).,6. Empirical Results,[1.0],['The above update is only performed if |z1 z2| is greater than a specified threshold (0.0001 in our experiments).']
"The hierarchical partitioning is performed according to a scheme similar to that of the DIRECT algorithm (Finkel, 2003), where each time a cell is split into K children the dimension that has the biggest width is split into K regions.",6. Empirical Results,[0],[0]
We set K = 2 in all our experiments.,6. Empirical Results,[0],[0]
"Now, we will present the results of our synthetic experiments.
",6. Empirical Results,[0],[0]
Our implementation can be found at https://github.com/rajatsen91/MFTREE DET.,6. Empirical Results,[1.0],['Our implementation can be found at https://github.com/rajatsen91/MFTREE DET.']
We evaluate all the algorithms on standard benchmark functions used in global optimization.,6.1. Synthetic Experiments,[1.0],['We evaluate all the algorithms on standard benchmark functions used in global optimization.']
The functions have been modified to incorporate the fidelity space Z =,6.1. Synthetic Experiments,[0],[0]
"[0, 1].",6.1. Synthetic Experiments,[0],[0]
"The setup followed is identical to the one in (Kandasamy et al., 2017), except that we only work in a one dimensional fidelity space.",6.1. Synthetic Experiments,[0],[0]
"Also, we perform our experiments in a noiseless setting and therefore no Gaussian noise is added to the function evaluations, unlike in (Kandasamy et al., 2017).",6.1. Synthetic Experiments,[0],[0]
Note that MF-GP-UCB and MF-SKO are finite fidelity methods.,6.1. Synthetic Experiments,[0],[0]
The approximations for these methods are obtained at z = 0.333 and z = 0.667.,6.1. Synthetic Experiments,[1.0],['The approximations for these methods are obtained at z = 0.333 and z = 0.667.']
"We provide more details about the synthetic functions and the fidelities in Appendix D. Our experiments are performed under the deterministic setting, where no noise is added to the approximations.",6.1. Synthetic Experiments,[0],[0]
"However, several of the algorithms that we compare to have a randomized component.",6.1. Synthetic Experiments,[0],[0]
"For these algorithms, the results are averaged over 10 experiments and the corresponding error bars are shown.",6.1. Synthetic Experiments,[0],[0]
In our algorithm we set the number of MFDOO instances spawned to be N = 0.1Dmax,6.1. Synthetic Experiments,[0],[0]
"log(⇤/ (1)), given
a total budget ⇤.",6.1. Synthetic Experiments,[0],[0]
"We set ⇢max = 0.95 and ⌫max = 2.0.
",6.1. Synthetic Experiments,[0],[0]
"The results of the synthetic experiments are shown in Figure 1(a)-(e), where the title of each figure shows the name of the function, the dimension of the domain (d) and the dimension of the fidelity space (p).",6.1. Synthetic Experiments,[0],[0]
We have p = 1 in all our experiments.,6.1. Synthetic Experiments,[0],[0]
"It can be observed the tree based methods outperform the other algorithms by a large margin, except in the experiments with the CurinExp function (Fig. 1c).",6.1. Synthetic Experiments,[0],[0]
"Tree-based methods can handle higher dimensions better, as we can see in the Hartman6 (Fig. 1b) and Borehole (Fig. 1e) function experiments.",6.1. Synthetic Experiments,[0],[0]
Note that MFPDOO also beats PDOO by a large margin which only uses the highest fidelity.,6.1. Synthetic Experiments,[0],[0]
"PDOO is essentially DOO (Munos, 2011) where the smoothness decay parameters are tuned according to the scheme in (Grill et al., 2015).",6.1. Synthetic Experiments,[0],[0]
"MFPDOO can effectively explore the space at cheaper fidelities and then expend the higher fidelities in promising regions of the domain, unlike PDOO.",6.1. Synthetic Experiments,[0],[0]
In this section we describe our experiments that involve tuning hyper-parameters for text classification.,6.2. Tuning SVM for News Group Classification,[1.0],['In this section we describe our experiments that involve tuning hyper-parameters for text classification.']
"For this purpose we use a subset of the 20 news group dataset (Joachims, 1996).",6.2. Tuning SVM for News Group Classification,[1.0],"['For this purpose we use a subset of the 20 news group dataset (Joachims, 1996).']"
"All the algorithms are used for tuning two hyperparameters: (i) the regularization penalty and (ii) the temperature of the rbf kernel both in the range of [10 2, 103].",6.2. Tuning SVM for News Group Classification,[0],[0]
"For our experiments, we use the scikit-learn implementation of SVM classifier and also the inbuilt KFold function for crossvalidation.",6.2. Tuning SVM for News Group Classification,[0],[0]
The bag of words in each of the text document is converted into tf-idf features before applying the classification models.,6.2. Tuning SVM for News Group Classification,[0],[0]
"We use a one-dimensional fidelity space, where the fidelity denotes the number of samples used to obtain 5-fold cross-validation accuracy.",6.2. Tuning SVM for News Group Classification,[1.0],"['We use a one-dimensional fidelity space, where the fidelity denotes the number of samples used to obtain 5-fold cross-validation accuracy.']"
z = 1 corresponds to 5000 samples which is the maximum number of samples in the subset of the data used.,6.2. Tuning SVM for News Group Classification,[0],[0]
z = 0 corresponds to 100 samples.,6.2. Tuning SVM for News Group Classification,[0],[0]
"Note that for the finite fidelity methods MF-SKO and MF-GP-UCB, approximations are obtained at z = 0.33 and z = 0.667.
",6.2. Tuning SVM for News Group Classification,[0],[0]
For our algorithms we set ⌫max = 1.0 and ⇢max = 0.9.,6.2. Tuning SVM for News Group Classification,[0],[0]
At the beginning of the experiment some of the budget is expended to obtain the function values at a point x with two different fidelities z1 = 0.8 and z2 = 0.2.,6.2. Tuning SVM for News Group Classification,[0],[0]
Thus the total budget spent in the initialization is ⇤(0.8) + (0.2).,6.2. Tuning SVM for News Group Classification,[0],[0]
The function values obtained are then used to initialize c in the bias function ⇣(z) = c(1 z).,6.2. Tuning SVM for News Group Classification,[0],[0]
The initial value of c is set to 2|f1 f2|/|z1 z2|.,6.2. Tuning SVM for News Group Classification,[0],[0]
"Thereafter, c is updated online according to the method detailed above.",6.2. Tuning SVM for News Group Classification,[0],[0]
"We set N = 0.5Dmax log(⇤/ (1)).
",6.2. Tuning SVM for News Group Classification,[0],[0]
The cross-validation accuracy obtained as a function of time is plotted in Fig. 1f for all the candidate algorithms.,6.2. Tuning SVM for News Group Classification,[0],[0]
"It can be observed that MFPDOO outperforms the other algorithms, especially in low-budget settings.",6.2. Tuning SVM for News Group Classification,[0],[0]
We considered the problem of black-box function optimization using hierarchical partitions in the presence of cheap approximations or fidelities.,7. Conclusion,[0],[0]
We propose two tree-search based algorithms which can navigate the domain effectively using cheaper fidelities for coarser partitions and more expensive ones while zeroing in on finer partitions.,7. Conclusion,[0],[0]
We analyze our algorithms under standard smoothness assumptions and provide simple regret guarantees given a cost budget ⇤.,7. Conclusion,[0],[0]
Our simple regret guarantees scale much better with respect to ⇤ as compared to other hierarchical partitioning based algorithms that do not use cheaper fidelities.,7. Conclusion,[0],[0]
"Our first algorithm (MFDOO) requires the knowledge of the smoothness parameters (⌫⇤, ⇢⇤) and has a simple regret bound of O(⇤ 1/(d(⌫⇤,⇢⇤)+1))",7. Conclusion,[0],[0]
"where d(⌫⇤, ⇢⇤) is the near-optimality dimension.",7. Conclusion,[0],[0]
"Our second algorithm (MFPDOO) can obtain a simple regret bound of O((⇤/ log⇤) 1/(d(⌫⇤,⇢⇤)+1))",7. Conclusion,[0],[0]
even when the smoothness parameter are unknown.,7. Conclusion,[0],[0]
"Finally, we empirically validate the performance of our algorithms on real and synthetic datasets, where they outperform the stateof-the art multi-fidelity algorithms.
",7. Conclusion,[0],[0]
This work opens up several interesting research directions.,7. Conclusion,[0],[0]
The theoretical guarantees of our algorithms assume some nice properties about the bias and cost functions.,7. Conclusion,[0],[0]
We believe it is possible to design more robust algorithms that have similar guarantees even for the bias and cost functions that are not well-designed.,7. Conclusion,[0],[0]
Our setting is also restricted to a one dimensional fidelity space.,7. Conclusion,[0],[0]
"However, in many application the fidelity space may be multi-dimensional.",7. Conclusion,[0],[0]
"For instance, in the hyper-parameter tuning one can choose to use less samples or train for lesser iterations.",7. Conclusion,[1.0],"['For instance, in the hyper-parameter tuning one can choose to use less samples or train for lesser iterations.']"
It is an interesting research direction to incorporate a multi-dimensional fidelity space with tree-search based algorithms.,7. Conclusion,[1.0],['It is an interesting research direction to incorporate a multi-dimensional fidelity space with tree-search based algorithms.']
"Finally, in this work we work in the noise-less setting where the function and the approximations are deterministic.",7. Conclusion,[0],[0]
We believe it is possible to extend our results to a setting where zero-mean noise is added to the function and its approximations.,7. Conclusion,[0],[0]
"This work is partially supported by NSF grant 1320175, ARO grant W911NF-17-1-0359, and the US DoT supported D-STOP Tier 1 University Transportation Center.",Acknowledgment,[0],[0]
"Motivated by settings such as hyper-parameter tuning and physical simulations, we consider the problem of black-box optimization of a function.",abstractText,[0],[0]
"Multi-fidelity techniques have become popular for applications where exact function evaluations are expensive, but coarse (biased) approximations are available at much lower cost.",abstractText,[0],[0]
A canonical example is that of hyper-parameter selection in a learning algorithm.,abstractText,[0],[0]
"The learning algorithm can be trained for fewer iterations – this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till completion.",abstractText,[0],[0]
We incorporate the multi-fidelity setup into the powerful framework of black-box optimization through hierarchical partitioning.,abstractText,[0],[0]
We develop tree-search based multi-fidelity algorithms with theoretical guarantees on simple regret.,abstractText,[0],[0]
We finally demonstrate the performance gains of our algorithms on both real and synthetic datasets.,abstractText,[0],[0]
Multi-Fidelity Black-Box Optimization with Hierarchical Partitions,title,[0],[0]
