0	25	Span-based neural constituency parsing (Cross and Huang, 2016; Stern et al., 2017a) has attracted attention due to its high accuracy and extreme simplicity.
3	69	But existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser (Cross and Huang, 2016) is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chartbased span parser (Stern et al., 2017a) performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries such as end-to-end discourse parsing (Hernault et al., 2010; Zhao and Huang, 2017) and integrated sentence boundary detection and parsing (Björkelund et al., 2016).
10	14	To solve this problem, we apply cube pruning (Chiang, 2007; Huang and Chiang, 2007) to improve the runtime toO(nb log b) which renders an observed complexity that is linear in n (with minor extra inexactness).
11	8	We make the following contributions: • We design the first neural parser that is both linear time and capable of searching over exponentially large space.1 • We are the first to apply cube pruning to incremental parsing, and achieves, for the first time, the complexity of O(nb log b), i.e., linear in sentence length and (almost) linear in beam size.
12	6	This leads to an observed complexity strictly linear in sentence length n. • We devise a novel loss function which penalizes wrong spans that cross gold-tree spans, and employ max-violation update (Huang et al., 2012) to train this parser with structured SVM and beam search.
14	9	It also achieves the highest F1 score on the Penn Treebank among single model end-to-end systems.
17	31	With (i, j) on top of the stack, the parser can either shift to push the next singleton span (j, j + 1) on the stack, or it can reduce to combine the top two spans, (k, i) and (i, j), forming the larger span (k, j).
18	20	After each shift/reduce action, the top-most span is labeled as either a constituent or with a null label ∅, which means that the subsequence is not a subtree in the final decoded parse.
20	21	1 https://github.com/junekihong/beam-span-parser
21	23	To get the feature representation of a span (i, j), we use the output sequence of a bi-directional LSTM (Cross and Huang, 2016; Stern et al., 2017a).
24	66	Like Stern et al. (2017a), we also decompose the score of a tree t to be the sum of the span scores: s(t) = ∑ (i,j,X)∈t s(i, j,X) (1) = ∑ (i,j)∈t max X s((fj − fi;bi − bj), X) (2) Note that X is a nonterminal label, a unary chain (e.g., S-VP), or null label ∅.2 In a shift-reduce setting, there are 2n − 1 steps (n shifts and n − 1 reduces) and after each step we take the best label for the resulting span; therefore there are exactly 2n−1 such (labeled) spans (i, j,X) in tree t. Also note that the choice of the label for any span (i, j) is only dependent on (i, j) itself (and not depending on any subtree information), thus the max over label X is independent of other spans, which is a nice property of span-based parsing (Cross and Huang, 2016; Stern et al., 2017a).
25	7	We now reformulate this DP parser in the above section as a shift-reduce parser.
26	98	We maintain a step index ` in order to perform action-synchronous beam search (see below).
27	31	Figure 1 shows how to represent a parsing stack using only the top span (i, j).
28	172	If the top span (i, j) shifts, it produces (j, j + 1), but if it reduces, it needs to know the second last span on the stack, (k, i), which is not represented in the current state.
29	8	This problem can be solved by graph-structure stack (Tomita, 1991; Huang and Sagae, 2010), which maintains, for each state p, a set of predecessor states π(p) that p can combine with on the left.
30	118	This is the way our actual code works (π(p) is implemented as a list of pointers, or “left pointers”), but here for simplicity of presentation we devise a novel but easier-to-understand formulation in Fig.
31	44	1, where we explicitly represent the set of predecessor states that state ` : (i, j) can combine with as `′ : (k, i) where `′ = `−2(j− i) + 1, i.e., (i, j) at step ` can combine with any (k, i) for any k at step `′.
34	9	The time complexity of this algorithm is O(n4) with the extra O(n) due to step index.3
36	18	At each time step, we maintain the top b parsing states, pruning off the rest.
38	66	With O(n) parsing actions our time complexity becomes linear in the length of the sentence.
39	47	However, Theorem 1 suggests that a parsing state p can have up to b predecessor states (“left pointers”), i.e., |π(p)| ≤ b because π(p) are all in the same step, a reduce action can produce up to b subsequent new reduced states.
41	20	Even though b2 is a constant, even modest values of b can make b2 dominate the length of the sentence.
44	10	To avoid inserting all b2 reduced states from the previous beam, we only consider each state’s highest scoring left pointer,5 and whenever we pop a reduced state from the heap, we iterate down its left pointers to insert the next non-duplicate reduced state back into the heap.
46	17	The initialization of the heap takes O(b) and popping b items takes O(b log b), giving us an overall improved runtime of O(nb log b).
47	17	We use a Structured SVM approach for training (Stern et al., 2017a; Shi et al., 2017).
49	6	At training time we perform lossaugmented decoding: t̂ = arg max t s∆(t) = arg max t s(t) + ∆(t, t∗).
51	23	The baseline loss function from Stern et al. (2017a) counts the incorrect labels (i, j,X) in the predicted tree: ∆base(t, t ∗) = ∑ (i,j,X)∈t 1 ( X 6= t∗(i,j) ) .
52	26	Note that X can be null ∅, and t∗(i,j) denotes the gold label for span (i, j), which could also be ∅.6 However, there are two cases where t∗(i,j) = ∅: a subspan (i, j) due to binarization (e.g., a span combining the first two subtrees in a ternary branching node), or an invalid span in t that crosses a gold span in t∗.
54	28	So we revise our loss function as: ∆new(t, t ∗) = ∑ (i,j,X)∈t 1 ( X 6= t∗(i,j) ∨ cross(i, j, t∗) ) where cross(i, j, t∗) = ∃ (k, l) ∈ t∗, and i < k < j < l or k < i < l < j.
