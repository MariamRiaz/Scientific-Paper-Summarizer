{"sections": [{"heading": "1. Introduction", "text": "Recurrent Neural Networks (RNNs) have been successfully used in many applications involving time series. This is because RNNs are well suited for sequential data as they process inputs one element at a time and store relevant information in their hidden state. In practice, however, training simple RNNs (sRNN) can be challenging due to the problem of exploding and vanishing gradients (Hochreiter et al., 2001). It has been shown that exploding gradients can occur when the transition matrix of an RNN has a spectral norm larger than one (Glorot & Bengio, 2010). This results\n1The University of Melbourne, Parkville, Australia 2Data61, CSIRO, Australia. Correspondence to: Zakaria Mhammedi <zak.mhammedi@data61.csiro.au>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nin an error surface, associated with some objective function, having very steep walls (Pascanu et al., 2013). On the other hand, when the spectral norm of the transition matrix is less than one, the information at one time step tend to vanish quickly after a few time steps. This makes it challenging to learn long-term dependencies in sequential data.\nDifferent methods have been suggested to solve either the vanishing or exploding gradient problem. The LSTM has been specifically designed to help with the vanishing gradient (Hochreiter & Schmidhuber, 1997). This is achieved by using gate vectors which allow a linear flow of information through the hidden state. However, the LSTM does not directly address the exploding gradient problem. One approach to solving this issue is to clip the gradients (Mikolov, 2012) when their norm exceeds some threshold value. However, this adds an extra hyperparameter to the model. Furthermore, if exploding gradients can occur within some parameter search space, the associated error surface will still have steep walls. This can make training challenging even with gradient clipping.\nAnother way to approach this problem is to improve the shape of the error surface directly by making it smoother, which can be achieved by constraining the spectral norm of the transition matrix to be less than or equal to one. However, a value of exactly one is best for the vanishing gradient problem. A good choice of the activation function between hidden states is also crucial in this case. These ideas have been investigated in recent works. In particular, the unitary RNN (Arjovsky et al., 2016) uses a special parametrisation to constrain the transition matrix to be unitary, and hence, of norm one. This parametrisation and other similar ones (Hyland & Ra\u0308tsch, 2017; Wisdom et al., 2016) have some advantages and drawbacks which we will discuss in more details in the next section.\nThe main contributions of this work are as follows:\n\u2022 We first show that constraining the search space of the transition matrix of an RNN to the set of unitary matrices U(n) is equivalent to limiting the search space to a subset of O(2n) (O(2n) is the set of 2n\u00d7 2n orthogonal matrices) of a new RNN with twice the hidden size. This suggests that it may not be necessary to\nar X\niv :1\n61 2.\n00 18\n8v 5\n[ cs\n.L G\n] 1\n3 Ju\nn 20\n17\nwork with complex matrices.\n\u2022 We present a simple way to parametrise orthogonal transition matrices of RNNs using Householder matrices, and we derive the expressions of the back-propagated gradients with respect to the new parametrisation. This new parametrisation can also be used in other deep architectures.\n\u2022 We develop an algorithm to compute the backpropagated gradients efficiently. Using this algorithm, we show that the worst case time complexity of one gradient step is of the same order as that of the sRNN."}, {"heading": "2. Related Work", "text": "Throughout this work we will refer to elements of the following sRNN architecture.\nh(t) = \u03c6(Wh(t\u22121) + V x(t)), (1)\no(t) = Y h(t), (2)\nwhere W , V and Y are the hidden-to-hidden, input-tohidden, and hidden-to-output weight matrices. h(t\u22121) and h(t) are the hidden vectors at time steps t\u2212 1 and t respectively. Finally, \u03c6 is a non-linear activation function. We have omitted the bias terms for simplicity.\nRecent research explored how the initialisation of the transition matrix W influences training and the ability to learn long-term dependencies. In particular, initialisation with the identity or an orthogonal matrix can greatly improve performance (Le et al., 2015). In addition to these initialisation methods, one study also considered removing the non-linearity between the hidden-to-hidden connections (Henaff et al., 2016), i.e. the term Wh(t\u22121) in Equation (1) is outside the activation function \u03c6. This method showed good results when compared to the LSTM on pathological problems exhibiting long-term dependencies.\nAfter training a model for a few iterations using gradient descent, nothing guarantees that the initial structures of the transition matrix will be held. In fact, its spectral norm can deviate from one, and exploding and vanishing gradients can be a problem again. It is possible to constrain the transition matrix to be orthogonal during training using special parametrisations (Arjovsky et al., 2016; Hyland & Ra\u0308tsch, 2017), which ensure that its spectral norm is always equal to one. The unitary RNN (uRNN) (Arjovsky et al., 2016) is one example where the hidden matrix W \u2208 Cn\u00d7n is the product of elementary matrices, consisting of reflection, diagonal, and Fourier transform matrices. When the size of hidden layer is equal to n, the transition matrix has a total of only 7n parameters. Another advantage of this parametrisation is computational efficiency - the matrix-vector product Wv, for some vector v, can be calculated in time complexity O(n log n). However, it has been shown that this\nparametrisation does not allow the transition matrix to span the full unitary group (Wisdom et al., 2016) when the size of the hidden layer is greater than 7. This may limit the expressiveness of the model.\nAnother interesting parametrisation (Hyland & Ra\u0308tsch, 2017) has been suggested which takes advantage of the algebraic properties of the unitary group U(n). The idea is to use the corresponding matrix Lie algebra u(n) of skew hermitian matrices. In particular, the transition matrix can be written as W = exp [\u2211n2 i=1 \u03bbiTi ] , where exp is the exponential matrix map and {Ti}n 2\ni=1 are predefined n \u00d7 n matrices forming a bases of the Lie algebra u(n). The learning parameters are the weights {\u03bbi}. The fact that the matrix Lie algebra u(n) is closed and connected ensures that the exponential mapping from u(n) to U(n) is surjective. Therefore, with this parametrisation the search space of the transition matrix spans the whole unitary group. This is one advantage over the original unitary parametrisation (Arjovsky et al., 2016). However, the cost of computing the matrix exponential to get W is O(n3), where n is the dimension of the hidden state. .\nAnother method (Wisdom et al., 2016) performs optimisation directly of the Stiefel manifold using the Cayley transformation. The corresponding model was called fullcapacity unitary RNN. Using this approach, the transition matrix can span the full set of unitary matrices. However, this method involves a matrix inverse as well as matrixmatrix products which have time complexity O(n3). This can be problematic for large neural networks when using stochastic gradient descent with a small mini-batch size.\nA more recent study (Vorontsov et al., 2017) investigated the effect of soft versus hard orthogonal constraints on the performance of RNNs. The soft constraint was applied by specifying an allowable range for the maximum singular value of the transition matrix. To this end, the transition matrix was factorised as W = USV \u2032, where U and V are orthogonal matrices and S is a diagonal matrix containing the singular values ofW . A soft orthogonal constraint consists of specifying small allowable intervals around 1 for the diagonal elements of S. Similarly to (Wisdom et al., 2016), the matrices U and V were updated at each training iteration using the Cayley transformation, which involves a matrix inverse, to ensure that they remain orthogonal.\nAll the methods discussed above, except for the original unitary RNN, involve a step that requires at least a O(n3) time complexity. All of them, except for one, require the use of complex matrices. Table 1 summarises the time complexities of various methods, including our approach, for one stochastic gradient step. In the next section, we show that imposing a unitary constraint on a transition matrix W \u2208 Cn\u00d7n is equivalent to imposing a special orthog-\nonal constraint on a new RNN with twice the hidden size. Furthermore, since the norm of orthogonal matrices is also always one, using the latter has the same theoretical benefits as using unitary matrices when it comes to the exploding gradient problem."}, {"heading": "3. Complex unitary versus orthogonal", "text": "We can show that when the transition matrixW \u2208 Cn\u00d7n of an RNN is unitary, there exists an equivalent representation of this RNN involving an orthogonal matrix W\u0302 \u2208 R2n\u00d72n.\nIn fact, consider a complex unitary transition matrix W = A + iB \u2208 Cn\u00d7n, where A and B are now real-valued matrices in Rn\u00d7n. We also define the following new variables\n\u2200t, h\u0302(t) = [ < ( h(t) ) = ( h(t) )] , V\u0302 = [< (V )= (V ) ] , W\u0302 = [ A \u2212B B A ] ,\nwhere < and = denote the real and imaginary parts of a complex number. Note that h\u0302(t) \u2208 R2n, W\u0302 \u2208 R2n\u00d72n, and V\u0302 \u2208 R2n\u00d7nx , where nx is the dimension of the input vector x(t) in Equation (1).\nAssuming that the activation function \u03c6 applies to the real and imaginary parts separately, it is easy to show that the update equation of the complex hidden state h(t) of the unitary RNN has the following real space representation\nh\u0302(t) = \u03c6(W\u0302 h\u0302(t\u22121) + V\u0302 x(t)). (3)\nEven when the activation function \u03c6 does not apply to the real and imaginary parts separately, it is still possible to find an equivalent representation in the real space. Consider the activation function proposed by (Arjovsky et al., 2016)\n\u03c3modRelU(z) = { (|z|+ b) z|z| , if |z|+ b > 0\n0, otherwise (4)\nwhere b is a bias vector. For a hidden state h\u0302 \u2208 R2n, the equivalent activation function in the real space representation is given by\n[ \u03c6\u0302(a) ] i =  \u221a a2i+a 2 ki +bk\u0303i\u221a a2i+a 2 ki ai, if \u221a a2i + a 2 ki + bk\u0303i > 0\n0, otherwise\nwhere ki = ((i+ n) mod 2n) and k\u0303i = (i mod n) for all i \u2208 {1, . . . , 2n}. The activation function \u03c6\u0302 is no longer applied to hidden units independently.\nNow we will show that the matrix W\u0302 is orthogonal. By definition of a unitary matrix, we have WW \u2217 = I where the \u2217 represents the conjugate transpose. This implies that AA\u2032 +BB\u2032 = I and BA\u2032 \u2212AB\u2032 = 0. And since we have\nW\u0302W\u0302 \u2032 = [ AA\u2032 +BB\u2032 AB\u2032 \u2212BA\u2032 BA\u2032 \u2212AB\u2032 AA\u2032 +BB\u2032 ] , (5)\nit follows that W\u0302W\u0302 \u2032 = I . Also note that W\u0302 has a special structure - it is a block-matrix.\nThe discussion above shows that using a complex, unitary transition matrix in Cn\u00d7n is equivalent to using an orthogonal matrix, belonging to a subset of O(2n), in a new RNN with twice the hidden size. This is why in this work we focus mainly on parametrising orthogonal matrices."}, {"heading": "4. Parametrisation of the transition matrix", "text": "Before discussing the details of our parametrisation, we first introduce a few notations. For n, k \u2208 N and 2 \u2264 k \u2264 n, letHk : Rk \u2192 Rn\u00d7n be defined as\nHk(u) =\n[ In\u2212k 0\n0 Ik \u2212 2 uu \u2032\n\u2016u\u20162\n] , (6)\nwhere Ik denotes the k-dimensional identity matrix. For u \u2208 Rk, Hk(u) is the Householder Matrix in O(n) representing the reflection about the hyperplane orthogonal to the vector (0\u2032n\u2212k,u\n\u2032)\u2032 \u2208 Rn and passing through the origin, where 0n\u2212k denotes the zero vector in Rn\u2212k.\nWe also define the mappingH1 : R\u2192 Rn\u00d7n as H1(u) = [ In\u22121 0 0 u ] . (7)\nNote that H1(u) is not necessarily a Householder reflection. However, when u \u2208 {1,\u22121},H1(u) is orthogonal.\nFinally, for n, k \u2208 N and 1 \u2264 k \u2264 n, we define\nMk : Rk \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Rn \u2192 Rn\u00d7n\n(uk, . . . ,un) 7\u2192 Hn(un) . . .Hk(uk).\nWe propose to parametrise the transition matrix W of an RNN using the mappings {Mk}. When using m reflection vectors {ui}, the parametrisation can be expressed as\nW =Mn\u2212m+1(un\u2212m+1, . . . ,un) = Hn(un) . . .Hn\u2212m+1(un\u2212m+1), (8)\nwhere ui \u2208 Ri for i \u2208 {n\u2212m+ 1, . . . , n}.\nFor the particular case where m = n in the above parametrisation, we have the following result. Theorem 1. The image ofM1 includes the set of all n\u00d7n orthogonal matrices, i.e. O(n) \u2282M1[R\u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Rn].\nNote that Theorem 1 would not be valid if H1(\u00b7) was a standard Householder reflection. In fact, in the twodimensional case, for instance, the matrix ( 1 0 0 \u22121 ) cannot be expressed as the product of exactly two standard Householder matrices.\nThe parametrisation in (8) has the following advantages:\n1. The parametrisation is smooth*, which is convenient for training with gradient descent. It is also flexible - a good trade-off between expressiveness and speed can be found by tuning the number of reflection vectors.\n2. The time and space complexities involved in one gradient calculation are, in the worst case, the same as that of the sRNN with the same number of hidden units. This is discussed in the following subsections.\n3. When m < n, the matrix W is always orthogonal, as long as the reflection vectors are nonzero. Form = n, the only additional requirement for W to be orthogonal is that u1 \u2208 {\u22121, 1}.\n4. Whenm = n, the transition matrix can span the whole set of n\u00d7n orthogonal matrices. In this case, the total number of parameters needed for W is n(n + 1)/2. This results in only n redundant parameters since the orthogonal set O(n) is a n(n\u2212 1)/2 manifold."}, {"heading": "4.1. Back-propagation algorithm", "text": "Let ui \u2208 Ri. Let U := (un| . . . |un\u2212m+1) \u2208 Rn\u00d7m be the parameter matrix constructed from the reflection vectors {ui}. In particular, the j-th column of U can be expressed using the zero vector 0j\u22121 \u2208 Rj\u22121 as\nU\u2217,j =\n[ 0j\u22121\nun\u2212j+1\n] \u2208 Rn, 1 \u2264 j \u2264 m. (9)\nLetL be a scalar loss function andC(t) :=Wh(t\u22121), where W is constructed using the {ui} vectors following Equation (8). In order to back-propagate the gradients through time, we need to compute the following partial derivatives\n\u2202L \u2202U (t) :=\n[ \u2202C(t)\n\u2202U ]\u2032 \u2202L \u2202C(t) , (10)\n\u2202L \u2202h(t\u22121) =\n[ \u2202C(t)\n\u2202h(t\u22121) ]\u2032 \u2202L \u2202C(t) , (11)\n*except on a subset of zero Lebesgue measure.\nat each time step t. Note that in Equation (10) h(t\u22121) is taken as a constant with respect to U . Furthermore, we have \u2202L\u2202U = \u2211T t=1 \u2202L \u2202U(t)\n, where T is the length of the input sequence. The gradient flow through the RNN at time step t is shown in Figure 1.\nBefore describing the algorithm to compute the backpropagated gradients \u2202L\n\u2202U(t) and \u2202L \u2202h(t\u22121) , we first derive their\nexpressions as a function of U , h(t\u22121) and \u2202L \u2202C(t)\nusing the compact WY representation (Joffrain et al., 2006) of the product of Householder reflections. Proposition 1. Let n,m \u2208 N s.t. m \u2264 n \u2212 1. Let ui \u2208 Ri and U = (un| . . . |un\u2212m+1) be the matrix defined in Equation (9). We have\nT := striu(U \u2032U) + 1\n2 diag(U \u2032U), (12)\nHn(un) . . .Hn\u2212m+1(un\u2212m+1) = I \u2212 UT\u22121U \u2032, (13)\nwhere striu(U \u2032U), and diag(U \u2032U) represent the strictly upper part and the diagonal of the matrix U \u2032U , respectively.\nEquation (13) is the compact WY representation of the product of Householder reflections. For the particular case where m = n, the RHS of Equation (13) should be replaced by ( I \u2212 UT\u22121U \u2032 ) H1(u1), where H1 is defined in (7) and U = (un| . . . |u2).\nThe following theorem gives the expressions of the gradients \u2202L\n\u2202U(t) and \u2202L \u2202h(t\u22121) when m \u2264 n\u2212 1 and h = h(t\u22121).\nAlgorithm 1 Local forward and backward propagations at time step t. For a matrix A, A\u2217,k denotes the k-th column.\n1: Inputs: h(t\u22121), \u2202L\u2202C , U = (un| . . . |un\u2212m+1). 2: Outputs: \u2202L\n\u2202U(t) , \u2202L \u2202h(t\u22121) , C(t) =Wh(t\u22121)\n3: Require: G \u2208 Rn\u00d7m, g \u2208 Rn, H \u2208 Rn\u00d7(m+1) 4: N \u2190 (\u2016un\u20162 , . . . , \u2016un\u2212m+1\u20162) 5: H\u2217,m+1 \u2190 h(t\u22121) 6: g \u2190 \u2202L\u2202C 7: for k = m to 1 do {Local Forward Propagation} 8: h\u0303k \u2190 2NkU \u2032 \u2217,kH\u2217,k+1\n9: H\u2217,k \u2190 H\u2217,k+1 \u2212 h\u0303kU\u2217,k 10: end for 11: for k = 1 to m do {Local Backward Propagation} 12: C\u0303k \u2190 2NkU \u2032 \u2217,kg 13: g \u2190 g \u2212 C\u0303kU\u2217,k 14: G\u2217,k \u2190 \u2212h\u0303kg \u2212 C\u0303kH\u2217,k+1 15: end for 16: C(t) \u2190 H\u2217,1 17: \u2202L\n\u2202h(t\u22121) \u2190 g\n18: \u2202L \u2202U(t) \u2190 G\nTheorem 2. Let n,m \u2208 N s.t. m \u2264 n\u22121. Let U \u2208 Rn\u00d7m, h \u2208 Rn, and C = (I \u2212 UT\u22121U \u2032)h, where T is defined in Equation (12). If L is a scalar loss function which depends on C, then we have\n\u2202L \u2202U =U [(h\u0303C\u0303 \u2032) \u25e6B\u2032 + (C\u0303h\u0303\u2032) \u25e6B]\u2212 \u2202L \u2202C h\u0303\u2032 \u2212 hC\u0303 \u2032, (14) \u2202L \u2202h = \u2202L \u2202C \u2212 UC\u0303, (15)\nwhere h\u0303 = T\u22121U \u2032h, C\u0303 = (T \u2032)\u22121U \u2032 \u2202L\u2202C , and B = striu(J) + 12I , with J being the m \u00d7m matrix of all ones and \u25e6 the Hadamard product.\nThe proof of Equations (14) and (15) is provided in Appendix A. Based on Theorem 2, Algorithm 1 performs the one-step forward-propagation (FP) and back-propagation (BP) required to compute C(t), \u2202L\n\u2202U(t) , and \u2202L \u2202h(t\u22121) . See\nAppendix B for more detail about how this algorithm is derived using Theorem 2.\nIn the next section we analyse the time and space complexities of this algorithm."}, {"heading": "4.2. Time and Space complexity", "text": "At each time step t, the flop count required by Algorithm 1 is (13n+ 2)m; 6nm for the one-step FP and (7n+ 2)m for the one-step BP. Note that the vector N only needs to be calculated at one time step. This reduces the flop count at the remaining time steps to (11n + 3)m. The fact that the matrix U has all zeros in its upper triangular part can be used to further reduce the total flop count to (11n\u2212 3m+\n5)m; (4n\u2212m+2)m for the one-step FP and (7n\u2212 2m+ 3)m for the one-step BP. See Appendix C for more details.\nNote that if the values of the matrices H , defined in Algorithm 1, are first stored during a \u201cglobal\u201d FP (i.e. through all time steps), then used in the BP steps, the time complexity\u2020 for a global FP and BP using one input sequence of length T are, respectively, \u2248 3n2T and \u2248 5n2T , when m \u2248 n and n 1. In contrast with the sRNN case with n hidden units, the global FP and BP have time complexities \u2248 2n2T and \u2248 3n2T . Hence, when m \u2248 n, the FP and BP steps using our parametrisation require only about twice more flops than the sRNN case with the same number of hidden units.\nNote, however, that storing the values of the matrices H at all time steps requires the storage of mnT values for one sequence of length T , compared with nT when only the hidden states {h(t)}t=1 are stored. When m 1 this may not be practical. One solution to this problem is to generate the matrices H locally at each BP step using U and h(t\u22121). This results in a global BP complexity of (11n \u2212 3m + 5)mT . Table 2 summarises the flop counts for the FP and BP steps. Note that these flop counts are for the case when m \u2264 n\u22121. When m = n, the complexity added due to the multiplication byH1(u1) is negligible."}, {"heading": "4.3. Extension to the Unitary case", "text": "Although we decided to focus on the set of real-valued orthogonal matrices, for the reasons given in Section 3, our parametrisation can readily be modified to apply to the general unitary case.\nLet H\u0302k : Ck \u2192 Cn\u00d7n, 2 \u2264 k \u2264 n, be defined by Equation (6) where the transpose sign \u2032 is replaced by the conjugate transpose \u2217. Furthermore, let H\u03021 : Rn \u2192 Cn\u00d7n be defined as H\u03021(\u03b8) = diag(ei\u03b81 , . . . , ei\u03b8n). With the new mappings {H\u0302k}k=nk=1 , we have the following corollary.\n\u2020We considered only the time complexity due to computations through the hidden-to-hidden connections of the network.\nCorollary 1. Let M\u03021 be the mapping defined as\nM\u03021 : Rn \u00d7 C2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Cn \u2192 Cn\u00d7n\n(\u03b8,u2, . . . ,un) 7\u2192H\u0302n(un) . . . H\u03022(u2)H\u03021(\u03b8).\nThe image of M\u03021 spans the full set of unitary matrices U(n) and any point on its image is a unitary matrix."}, {"heading": "5. Experiments", "text": "All RNN models were implemented using the python library theano (Theano Development Team, 2016). For efficiency, we implemented the one-step FP and BP algorithms described in Algorithm 1 using C code\u2021. We tested the new parametrisation on five different datasets all having long-term dependencies. We call our parametrised network oRNN (for orthogonal RNN). We set its activation function to the leaky_ReLU defined as \u03c6(x) = max( x10 , x). To ensure that the transition matrix of the oRNN is always orthogonal, we set the scalar u1 to -1 if u1 \u2264 0 and 1 otherwise after each gradient update. Note that the parameter matrixU in Equation (9) has all zeros in its upper triangular part. Therefore, after calculating the gradient of a loss with respect to U (i.e. \u2202L\u2202U ), the values in the upper triangular part are set to zero.\nFor all experiments, we used the adam method for stochastic gradient descent (Kingma & Ba, 2014). We initialised all the parameters using uniform distributions similar to (Arjovsky et al., 2016). The biases of all models were set to zero, except for the forget bias of the LSTM, which we set to 5 to facilitate the learning of long-term dependencies (Koutn\u0131\u0301k et al., 2014)."}, {"heading": "5.1. Sequence generation", "text": "In this experiment, we followed a similar setting to (Koutn\u0131\u0301k et al., 2014) where we trained RNNs to encode song excerpts. We used the track Manyrista from album Musica Deposita by Cuprum. We extracted five consecutive excerpts around the beginning of the song, each having 800 data points and corresponding to 18ms with a 44.1Hz sampling frequency. We trained an sRNN, LSTM, and oRNN for 5000 epochs on each of the pieces with five random seeds. For each run, the lowest Normalised Mean Squared Error (NMSE) during the 5000 epochs was recorded. For each model, we tested three different hidden sizes. The total number of parameters Np corresponding to these hidden sizes was approximately equal to 250, 500, and 1000. For the oRNN, we set the number of reflection vectors to the hidden size for each case, so that the transition matrix is allowed to span the full set of orthogonal\n\u2021Our implementation can be found at https://github. com/zmhammedi/Orthogonal_RNN.\nmatrices. The results are shown in Figures 2 and 3. All the learning rates were set to 10\u22123. The orthogonal parametrisation outperformed the sRNN and performed on average better than the LSTM."}, {"heading": "5.2. Addition Task", "text": "In this experiment, we followed a similar setting to (Arjovsky et al., 2016), where the goal of the RNN is to output the sum of two elements in the first dimension of a twodimensional sequence. The location of the two elements to be summed are specified by the entries in the second dimension of the input sequence. In particular, the first dimension of every input sequence consists of random numbers between 0 and 1. The second dimension has all zeros except for two elements equal to 1. The first unit entry\nis located in the first half of the sequence, and the second one in the second half. We tested two different sequence lengths T = 400, 800. All models were trained to minimise the Mean Squared Error (MSE). The baseline MSE for this task is 0.167; for a model that always outputs one.\nWe trained an oRNN with n = 128 hidden units and m = 16 reflections. We trained an LSTM and sRNN with hidden sizes 28 and 54, respectively, corresponding to a total number of parameters \u2248 3600 (i.e. same as the oRNN model). We chose a batch size of 50, and after each iteration, a new set of sequences was generated randomly. The learning rate for the oRNN was set to 0.01. Figure 4 displays the results for both lags.\nThe oRNN was able to beat the baseline MSE in less than 5000 iterations for both lags and for two different random initialisation seeds. This is in line with the results of the unitary RNN (Arjovsky et al., 2016)."}, {"heading": "5.3. Pixel MNIST", "text": "In this experiment, we used the MNIST image dataset. We split the dataset into training (55000 instances), validation (5000 instances), and test sets (10000 instances). We trained oRNNs with n \u2208 {128, 256} andm \u2208 {16, 32, 64}, where n and m are the number of hidden units and reflections vectors respectively, to minimise the cross-entropy error function. We experimented with (mini-batch size, learning rate) \u2208 {(1, 10\u22124), (50, 10\u22123)}.\nTable 3 compares the test performance of our best model against results available in the literature for unitary/orthogonal RNNs. Despite having fewer total number of parameters, our model performed better than three out the four models selected for comparison (all having \u2265 16K parameters). Figure 5 shows the validation accuracy as a function of the number of epochs of our oRNN model in Table 3.\nFigure 6 shows the effect of varying the number of reflection vectors m on the performance."}, {"heading": "5.4. Penn Tree Bank", "text": "In this experiment, we tested the oRNN on the task of character level prediction using the Penn Tree Bank Corpus. The data was split into training (5017K characters), validation (393K characters), and test sets (442K characters). The total number of unique characters in the corpus was 49. The vocabulary size was 10K and any other words were replaced by the special token <unk>. The number of characters per instance (i.e. char/line) in the training data ranged between 2 and 518 with an average of 118 char/line. We trained an oRNN and LSTM with hidden units 512 and 183 respectively, corresponding to a total of \u2248 180K parameters, for 20 epochs. We set the number of reflections to 510\nfor the oRNN. The learning rate was set to 0.0001 for both models with a mini-batch size of 1.\nSimilarly to (Pascanu et al., 2013) we considered two tasks: one where the model predicts one character ahead and the other where it predicts a character five steps ahead. It was suggested that solving the later task would require the learning of longer term correlations in the data rather than the shorter ones. Table 4 summarises the test results. The oRNN and LSTM performed similarly to each other on the one-step head prediction task. Whereas on the five-step ahead prediction task, the LSTM was better. The performance of both models on this task was close to the state of the art result for RNNs 3.74 bpc (Pascanu et al., 2013).\nNevertheless, our oRNN still outperformed the results of (Vorontsov et al., 2017) which used both soft and hard orthogonality constraints on the transition matrix. Their RNN was trained on 99% of the data (sentences with\u2264 300 characters) and had the same number of hidden units as the oRNN in our experiment. The lowest test cost achieved was 2.20(bpc) for the one-step-ahead prediction task."}, {"heading": "5.5. Copying task", "text": "We tested our model on the copy task described in details in (Gers et al., 2001; Arjovsky et al., 2016). Using an oRNN with the leaky_ReLU we were not able to reproduce the same performance as the uRNN (Arjovsky et al., 2016; Wisdom et al., 2016). However, we were able to achieve a comparable performance when using the OPLU activation function (Chernodub & Nowicki, 2016), which is a normpreserving activation function. In order to explore whether the poor performance of the oRNN was only due to the activation function, we tested the same activation as the uRNN\n(i.e. the real representation of modReLU defined in Equation (4)) on the oRNN. This did not improve the performance compared to the leaky_ReLU case suggesting that the block structure of the uRNN transition matrix, when expressed in the real space (see Section 3), may confer special benefits in some cases."}, {"heading": "6. Discussion", "text": "In this work, we presented a new parametrisation of the transition matrix of a recurrent neural network using Householder reflections. This method allows an easy and computationally efficient way to enforce an orthogonal constraint on the transition matrix which then ensures that exploding gradients do not occur during training. Our method could also be applied to other deep neural architectures to enforce orthogonality between hidden layers. Note that a \u201csoft\u201d orthogonal constraint could also be applied using our parametrisation by, for example, allowing u1 to vary continuously between -1 and 1.\nIt is important to note that our method is particularly advantageous for stochastic gradient descent when the minibatch size is close to 1. In fact, if B is the mini-batch size and T is the average length of the input sequences, then a network with n hidden units trained using other methods (Vorontsov et al., 2017; Wisdom et al., 2016; Hyland & Ra\u0308tsch, 2017) that enforce orthogonality (see Section 2), would have time complexityO(BTn2+n3). Clearly when BT n this becomes O(BTn2), which is the same time complexity as that of the sRNN and oRNN (with m = n). In contrast with the case of fully connected deep forward networks with no weight sharing between layers (# layer = L), the time complexity using our method is O(BLnm) whereas other methods discussed in this work (see Section 2) would have time complexity O(BLn2 + Ln3). The latter methods are less efficient in this case since B n is less likely to be the case compared with BT n when using SGD.\nFrom a performance point of view, further experiments should be performed to better understand the difference between the unitary versus orthogonal constraint."}, {"heading": "Acknowledgment", "text": "The authors would like to acknowledge Department of State Growth Tasmania for partially funding this work through SenseT. We would also like to thank Christfried Webers for his valuable feedback."}, {"heading": "A. Proofs", "text": "Sketch of the proof for Theorem 1. We need to the show that for every Q\u0303 \u2208 O(n), there exits a tuple of vectors (u1, . . . ,un) \u2208 R \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Rn such that Q\u0303 = M1(u1, . . . ,un). Algorithm 2 shows how a QR decomposition can be performed using the matrices {Hk(uk)}nk=1 while ensuring that the upper triangular matrix R has positive diagonal elements. If we apply this algorithm to an orthogonal matrix Q\u0303, we get a tuple (u1, . . . ,un) which satisfies\nQR = Hn(un) . . .H1(u1)R = Q\u0303.\nNote that the matrixRmust be orthogonal sinceR = Q\u2032Q\u0303. Therefore, R = I , since the only upper triangular matrix with positive diagonal elements is the identity matrix. Hence, we have\nM1(u1, . . . ,un) = Hn(un) . . .H1(u1) = Q\u0303.\nAlgorithm 2 QR decomposition using the mappings {Hk}. For a matrix B \u2208 Rn\u00d7n, {Bk,k}1\u2264k\u2264n denote its diagonal elements, and Bk..n,k = (Bk,k, . . . , Bn,k)\u2032 \u2208 Rn\u2212k+1. Require: A \u2208 Rn\u00d7n is a full-rank matrix. Ensure: Q and R where Q = Hn(un) . . .H1(u1) and R\nis upper triangular with positive diagonal elements such that A = QR R\u2190 A Q\u2190 I {Initialise Q to the identity matrix} for k = 1 to n\u2212 1 do\nif Rk,k == \u2016Rk..n,k\u2016 then un\u2212k+1 = (0, . . . , 0, 1)\n\u2032 \u2208 Rn\u2212k+1 else un\u2212k+1 \u2190 Rk..n,k \u2212 \u2016Rk..n,k\u2016 (1, 0, . . . , 0)\u2032 un\u2212k+1 \u2190 un\u2212k+1/ \u2016un\u2212k+1\u2016 end if R\u2190 Hn\u2212k+1(un\u2212k+1)R Q\u2190 QHn\u2212k+1(un\u2212k+1)\nend for u1 = sgn(Rn,n) \u2208 R R\u2190 H1(u1)R Q\u2190 QH1(u1)\nLemma 1. (?) Let A, B, and C be real or complex matrices, such that C = f(A,B) where f is some differentiable mapping. Let L be some scalar quantity which depends on C. Then we have the following identity\nTr(C \u2032 dC) = Tr(A \u2032 dA) + Tr(B \u2032 dB),\nwhere dA, dB, and dC represent infinitesimal perturbations and\nC := \u2202L \u2202C , A :=\n[ \u2202C\n\u2202A ]\u2032 \u2202L \u2202C , B := [ \u2202C \u2202B ]\u2032 \u2202L \u2202C .\nProof of Theorem 2. Let C = h \u2212 UT\u22121U \u2032h where (U, h) \u2208 Rn\u00d7m\u00d7Rn and T = striu(U \u2032U) + 12diag(U\n\u2032U). Notice that the matrix T can be written using the Hadamard product as follows\nT = B \u25e6 (U \u2032U), (16)\nwhere B = striu(Jm) + 12Im and Jm is the m\u00d7m matrix of all ones.\nCalculating the infinitesimal perturbations of C gives\ndC =(I \u2212 UT\u22121U \u2032)dh \u2212 dUT\u22121U \u2032h\u2212 UT\u22121dU \u2032h + UT\u22121dTT\u22121U \u2032h.\nUsing Equation (16) we can write\ndT = B \u25e6 (dU \u2032U + U \u2032dU).\nBy substituting this back into the expression of dC, multiplying the left and right-hand sides by C \u2032 , and applying the trace we get\nTr(C \u2032 dC) = Tr(C \u2032 (I \u2212 UT\u22121U \u2032)dh)\n\u2212 Tr(C \u2032dUT\u22121U \u2032h)\u2212 Tr(C \u2032UT\u22121dU \u2032h)\n+ Tr(C \u2032 UT\u22121(B \u25e6 (dU \u2032U + U \u2032dU))T\u22121U \u2032h).\nNow using the identity Tr(AB) = Tr(BA), where the second dimension of A agrees with the first dimension of B, we can rearrange the expression of Tr(C \u2032 dC) as follows\nTr(C \u2032 dC) = Tr(C \u2032 (I \u2212 UT\u22121U \u2032)dh)\n\u2212 Tr(T\u22121U \u2032hC \u2032dU)\u2212 Tr(hC \u2032UT\u22121dU \u2032)\n+ Tr(T\u22121U \u2032hC \u2032 UT\u22121(B \u25e6 (dU \u2032U + U \u2032dU))).\nTo simplify the expression, we will use the short notations\nC\u0303 = (T \u2032)\u22121U \u2032C,\nh\u0303 = T\u22121U \u2032h,\nTr(C \u2032 dC) becomes\nTr(C \u2032 dC) = Tr((C \u2032 \u2212 C\u0303 \u2032U \u2032)dh)\n\u2212 Tr(h\u0303C \u2032dU)\u2212 Tr(hC\u0303 \u2032dU \u2032) + Tr(h\u0303C\u0303 \u2032(B \u25e6 (dU \u2032U + U \u2032dU))).\nNow using the two following identities of the trace\nTr(A\u2032) = Tr(A), Tr(A(B \u25e6 C)) = Tr((A \u25e6B\u2032)C)),\nwe can rewrite Tr(C \u2032 dC) as follows\nTr(C \u2032 dC) =Tr((C \u2032 \u2212 C\u0303 \u2032U \u2032)dh)\n\u2212 Tr(h\u0303C \u2032dU)\u2212 Tr(hC\u0303 \u2032dU \u2032) + Tr((h\u0303C\u0303 \u2032 \u25e6B\u2032)dU \u2032U) + Tr((h\u0303C\u0303 \u2032 \u25e6B\u2032)U \u2032dU).\nBy rearranging and taking the transpose of the third and fourth term of the right-hand side we obtain\nTr(C \u2032 dC) =Tr((C \u2032 \u2212 C\u0303 \u2032U \u2032)dh)\n\u2212 Tr(h\u0303C \u2032dU)\u2212 Tr(C\u0303h\u2032dU) + Tr(((C\u0303h\u0303\u2032) \u25e6B)U \u2032dU) + Tr(((h\u0303C\u0303 \u2032) \u25e6B\u2032)U \u2032dU).\nFactorising by dU inside the Tr we get\nTr(C \u2032 dC) = Tr((C \u2032 \u2212 C\u0303 \u2032U \u2032)dh)\u2212\nTr((h\u0303C \u2032 + C\u0303h\u2032 \u2212 [ (C\u0303h\u0303\u2032) \u25e6B + (h\u0303C\u0303 \u2032) \u25e6B\u2032 ] U \u2032)dU).\nUsing lemma 1 we conclude that U =U [ (h\u0303C\u0303 \u2032) \u25e6B\u2032 + (C\u0303h\u0303\u2032) \u25e6B ] \u2212 Ch\u0303\u2032 \u2212 hC\u0303 \u2032,\nh =C \u2212 UC\u0303.\nSketch of the proof for Corollary 1. For any nonzero complex valued vector x \u2208 Cn, if we chose u = x+ ei\u03b8 \u2016x\u2016 e1 and H = \u2212e\u2212i\u03b8(I \u2212 2 uu \u2217\n\u2016u\u20162 ), where \u03b8 \u2208 R is such that x1 = e i\u03b8|x1|, we have (?)\nHx = ||x||e1 (17)\nTaking this fact into account, a similar argument to that used in the proof of Theorem 1 can be used here."}, {"heading": "B. Algorithm Explanation", "text": "Let U := (vi,j) 1\u2264j\u2264m 1\u2264i\u2264n . Then the element of the matrix T := striu(U \u2032U) + 12diag(U \u2032U) can be expressed as\nti,j = Ji \u2264 jK \u2211n k=j vk,ivk,j\n1 + \u03b4i,j ,\nwhere \u03b4i,j is the Kronecker delta and J\u00b7K is the Iversion bracket (i.e. JpK = 1 if p is true and JpK = 0 otherwise). In order to compute the gradients in Equations (14) and (15). we first need to compute h\u0303 = T\u22121U \u2032h and C\u0303 = (T \u2032)\u22121U \u2032 \u2202L\u2202C . This is equivalent to solving the triangular systems of equations T h\u0303 = U \u2032h and T \u2032C\u0303 = U \u2032 \u2202L\u2202C .\nSolving the triangular system T h\u0303 = U \u2032h. For 1 \u2264 k \u2264 m, we can express the k-th row of this system as\ntk,kh\u0303k + m\u2211 j=k+1 tk,j h\u0303j = n\u2211 j=k vj,khj ,\n= n\u2211 j=k vj,khj \u2212 m\u2211 j=k+1 n\u2211 r=j vr,kvr,j h\u0303j ,\n= n\u2211 r=k vr,khr \u2212 n\u2211 r=k+1 vr,k r\u2211 j=k+1 vr,j h\u0303j , (18)\n= U \u2032\u2217,k(h\u2212 m\u2211\nj=k+1\nU\u2217,j h\u0303j), (19)\nwhere the passage from Equation (18) to (19) is justified because vr,j = 0 for j > r. Therefore, \u2211r j=k+1 vr,j h\u0303j =\u2211m\nj=k+1 vr,j h\u0303j . By setting H\u2217,k+1 := h\u2212 \u2211m j=k+1 U\u2217,j h\u0303j , and noting that tk,k = U \u2032\u2217,kU\u2217,k\n2 , we get\nh\u0303k = 2\nU \u2032\u2217,kU\u2217,k U \u2032\u2217,kH\u2217,k+1, (20)\nH\u2217,k = H\u2217,k+1 \u2212 h\u0303kU\u2217,k. (21)\nEquations (20) and (21) explain the lines 8 and 9 in Algorithm 1. Note that H\u2217,1 = h \u2212 \u2211m j=1 U\u2217,j h\u0303j = h \u2212\u2211m\nj=1 U\u2217,j [T \u22121U \u2032h]j = h \u2212 UT\u22121U \u2032h = Wh. Hence, when h = h(t\u22121), we have H\u2217,1 = C(t), which explains line 16 in Algorithm 1.\nSolving the triangular system T \u2032C\u0303 = U \u2032 \u2202L\u2202C . Similarly to the previous case, we have for 1 \u2264 k \u2264 m\ntk,kC\u0303k + k\u22121\u2211 j=1 tj,kC\u0303j = n\u2211 j=k vj,k [ \u2202L \u2202C ] j ,\n= n\u2211 j=1 vj,k [ \u2202L \u2202C ] j \u2212 k\u22121\u2211 j=1 n\u2211 r=k vr,jvr,kC\u0303j , (22)\n= n\u2211 r=1 vr,k [ \u2202L \u2202C ] r \u2212 n\u2211 r=1 vr,k k\u22121\u2211 j=1 vr,jC\u0303j , (23)\n= U \u2032\u2217,k  \u2202L \u2202C \u2212 k\u22121\u2211 j=1 U\u2217,jC\u0303j  ,\nwhere the passage from Equation (22) to (23) is justified by the fact that \u2211n r=k vr,jvr,kC\u0303j = \u2211n r=1 vr,jvr,kC\u0303j (since vr,k = 0 for r < k).\nBy setting g := \u2202L \u2202C(t) \u2212 \u2211k\u22121 j=1 U\u2217,jC\u0303j , we can write C\u0303k = 2\nU \u2032\u2217,kU\u2217,k U \u2032\u2217,kg which explains the lines 12 and\n13 in Algorithm 1. Note also that after m-iterations in the backward propagation loop in Algorithm 1, we have g = \u2202L \u2202C(t) \u2212 \u2211m j=1 U\u2217,jC\u0303j = \u2202L \u2202C(t) \u2212UC\u0303 = \u2202L \u2202h(t\u22121)\n. This explains line 17 of Algorithm 1.\nFinally, note that from Equation (14), we have for 1 \u2264 i \u2264 n and 1 \u2264 k \u2264 m[ \u2202L \u2202U ] i,k =\u2212 [ \u2202L \u2202C ] i h\u0303k \u2212 hiC\u0303k+\nm\u2211 j=1 vi,j ( ((h\u0303C\u0303 \u2032) \u25e6B\u2032)j,k + ((C\u0303h\u0303\u2032) \u25e6B)j,k ) ,\n=\u2212 [ \u2202L \u2202C ] i h\u0303k \u2212 hiC\u0303k+\nm\u2211 j=1 vi,j ( h\u0303jC\u0303k Jk \u2264 jK 1 + \u03b4j,k + C\u0303j h\u0303k Jj \u2264 kK 1 + \u03b4j,k ) ,\n=\u2212 [ \u2202L \u2202C ] i h\u0303k \u2212 hiC\u0303k+\nm\u2211 j=1 vi,j ( h\u0303jC\u0303kJk < jK + C\u0303j h\u0303kJj \u2264 kK ) ,\n=C\u0303k  m\u2211 j=k+1 vi,j h\u0303j \u2212 hi  + h\u0303k\n k\u2211 j=1 vi,jC\u0303j \u2212 [ \u2202L \u2202C ] i  . Therefore, when C = C(t) and h = h(t\u22121) we have[\n\u2202L \u2202U (t) ] \u2217,k = \u2212C\u0303kH\u2217,k+1 \u2212 h\u0303kg,\nwhere g = \u2202L \u2202C(t) \u2212 \u2211k\u22121 j=1 C\u0303jU\u2217,j . This explains lines 14 and 18 of Algorithm 1."}, {"heading": "C. Time complexity", "text": "Table 5 shows the flop count for different operations in the local backward and forward propagation steps in Algorithm 1."}, {"heading": "D. Matlab implementation of Algorithm 1", "text": "\u2202h(t\u22121) (variable g is the code), and\n\u2202L \u2202U(t)\n(variable G is the code). The required inputs for the FP and BP are, respectively, the tuples (U, h(t\u22121)) and (U,C(t), \u2202L\n\u2202C(t) ).\nNote that \u2202L \u2202C(t) is variable BPg in the Matlab code."}], "year": 2017, "references": [{"title": "Unitary evolution recurrent neural networks", "authors": ["Arjovsky", "Martin", "Shah", "Amar", "Bengio", "Yoshua"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Norm-preserving orthogonal permutation linear unit activation functions (oplu)", "authors": ["Chernodub", "Artem", "Nowicki", "Dimitri"], "venue": "arXiv preprint arXiv:1604.02313,", "year": 2016}, {"title": "Applying LSTM to time series predictable through time-window approaches", "authors": ["F.A. Gers", "D. Eck", "J. Schmidhuber"], "venue": "In Dorffner, Gerg (ed.), Artificial Neural Networks \u2013 ICANN 2001 (Proceedings),", "year": 2001}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["Glorot", "Xavier", "Bengio", "Yoshua"], "venue": "In Aistats,", "year": 2010}, {"title": "Orthogonal rnns and long-memory tasks", "authors": ["Henaff", "Mikael", "Szlam", "Arthur", "LeCun", "Yann"], "venue": "arXiv preprint arXiv:1602.06662,", "year": 2016}, {"title": "Long shortterm memory", "authors": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "year": 1997}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "authors": ["Hochreiter", "Sepp", "Bengio", "Yoshua", "Frasconi", "Paolo", "Schmidhuber", "J\u00fcrgen"], "venue": "long-term dependencies,", "year": 2001}, {"title": "Learning unitary operators with help from u(n)", "authors": ["Hyland", "Stephanie L", "R\u00e4tsch", "Gunnar"], "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February", "year": 2017}, {"title": "Accumulating householder transformations, revisited", "authors": ["Joffrain", "Thierry", "Low", "Tze Meng", "Quintana-Ort\u0131", "Enrique S", "Geijn", "Robert van de", "Zee", "Field G Van"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "year": 2006}, {"title": "Adam: A method for stochastic optimization", "authors": ["Kingma", "Diederik P", "Ba", "Jimmy"], "venue": "CoRR, abs/1412.6980,", "year": 2014}, {"title": "A clockwork RNN", "authors": ["Koutn\u0131\u0301k", "Jan", "Greff", "Klaus", "Gomez", "Faustino J", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "year": 2014}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "authors": ["Le", "Quoc V", "Jaitly", "Navdeep", "Hinton", "Geoffrey E"], "venue": "CoRR, abs/1504.00941,", "year": 2015}, {"title": "Statistical language models based on neural networks", "authors": ["Mikolov", "Tom\u00e1\u0161"], "venue": "PhD thesis, Brno University of Technology,", "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "authors": ["Pascanu", "Razvan", "Mikolov", "Tomas", "Bengio", "Yoshua"], "venue": "ICML (3),", "year": 2013}, {"title": "On orthogonality and learning recurrent networks with long term dependencies", "authors": ["Vorontsov", "Eugene", "Trabelsi", "Chiheb", "Kadoury", "Samuel", "Pal", "Chris"], "venue": "arXiv preprint arXiv:1702.00071,", "year": 2017}, {"title": "Full-capacity unitary recurrent neural networks", "authors": ["Wisdom", "Scott", "Powers", "Thomas", "Hershey", "John R", "Roux", "Jonathan Le", "Atlas", "Les E"], "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016,", "year": 2016}], "id": "SP:52b4708e47ee762fc72050471e0a223d36006607", "authors": [{"name": "Zakaria Mhammedi", "affiliations": []}, {"name": "Andrew Hellicar", "affiliations": []}, {"name": "Ashfaqur Rahman", "affiliations": []}, {"name": "James Bailey", "affiliations": []}], "abstractText": "The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients. These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint. Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.", "title": "Efficient Orthogonal Parametrisation of Recurrent Neural Networks  Using Householder Reflections"}