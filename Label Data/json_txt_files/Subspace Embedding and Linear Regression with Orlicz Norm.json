{"sections": [{"text": "\u2211n i=1G(|xi|/\u03b1) \u2264 1} .\nWe consider the cases where the function G(\u00b7) grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given matrix A \u2208 Rn\u00d7d with Orlicz norm into a lower dimensional space with `2 norm. Specifically, we show how to efficiently find an embedding matrix S \u2208 Rm\u00d7n,m < n such that \u2200x \u2208 Rd,\u2126(1/(d log n)) \u00b7 \u2016Ax\u2016G \u2264 \u2016SAx\u20162 \u2264 O(d2 log n) \u00b7 \u2016Ax\u2016G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem minx\u2208Rd \u2016Ax\u2212b\u2016G, up to aO(d log2 n) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the `p low rank matrix approximation problem for 1 \u2264 p < 2."}, {"heading": "1. Introduction", "text": "Numerical linear algebra problems play a significant role in machine learning, data mining, and statistics. One of the most important such problems is the regression problem, see some recent advancements in, e.g., (Zhong et al., 2016; Bhatia et al., 2015; Jain & Tewari, 2015; Liu et al., 2014; Dhillon et al., 2013). In a linear regression problem, given a data matrix A \u2208 Rn\u00d7d with n data points A1, A2, \u00b7 \u00b7 \u00b7 , An in Rd and the response vector b \u2208 Rn, the goal is to find a set of coefficients x\u2217 \u2208 Rd such that\nx\u2217 = arg minx\u2208Rd l(Ax\u2212 b), (1) *Equal contribution 1Computer Science Department, Columbia University, New York City, NY 10027, U.S.A.. Correspondence to: Peilin Zhong <pz2225@columbia.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nwhere l : Rn \u2192 R+ is the loss function. When l(y) = \u2016y\u201622 = \u2211n i=1 y 2 i , then the problem is the classic least square regression problem (`2 regression). While there has been extensive research on efficient algorithms for solving `2 regression, it is not always a suitable loss function to use.\nIn many settings, alternative choices for a loss function lead to qualitatively better solutions x\u2217. For example, a popular such alternative is the least absolute deviation (`1) regression \u2014 with l(y) = \u2016y\u20161 = \u2211n i=1 |yi| \u2014 which leads to solutions that are more robust than those of `2 regression (see (Wikipedia; Gorard, 2005). In a nutshell, the `2 regression is suitable when the data contains Gaussian noise, whereas `1 \u2014 when the noise is Laplacian or sparse.\nA further popular class of loss functions l(\u00b7) arises from M-estimators, defined as l(y) = \u2211n i=1M(yi) where M(\u00b7) is an M-estimator function (see (Zhang, 1997) for a list of M-estimators). The benefit of (some) M-estimators is that they enjoy advantages of both `1 and `2 regression. For example, when M(\u00b7) is the Huber function (Huber et al., 1964), then the regression looks like `2 regression when yi is small, and looks like `1 regression otherwise. However, these loss functions come with a downside: they depend on the scale, and rescaling the data may give a completely different solution! Our contributions. We introduce a generic algorithmic technique for solving regression for an entire class of loss functions that includes the aforementioned examples, and in particular, a \u201cscale-invariant\u201d version of M-estimators. Specifically, our class consists of loss functions l(y) that are Orlicz norms, defined as follows: given a non-negative convex function G : R+ \u2192 R+ with G(0) = 0, for x \u2208 Rn,we can define \u2016x\u2016G to be an Orlicz norm with respect to G(\u00b7): \u2016x\u2016G , inf {\u03b1 > 0 | \u2211n i=1G(|xi|/\u03b1) \u2264 1} . Note that `p norm, for p \u2208 [1,\u221e), is a special case of Orlicz norm with G(x) = xp. Another important example is the following \u201cscale-free\u201d version of M-estimator. Taking f(\u00b7) to be a Huber function, i.e.\nf(x) = { x2/2 |x| \u2264 \u03b4 \u03b4(|x| \u2212 \u03b4/2) otherwise\nfor some constant \u03b4, we take G(x) = f(f\u22121(1)x). Then the norm \u2016x\u2016G looks like `2 norm when x is flat, and looks like `1 norm when x is sparse. Figure 1 shows the unit norm ball of this kind of Orlicz norm.\nOur main result is a generic algorithm for solving any regression problem Eqn. (1) with any loss function that is a \u201cnice\u201d Orlicz norm; see Section 2 for a formal definition of \u201cnice\u201d, and think of it as subquadratic for now.\nOur main result employs the concept of subspace embeddings, which is a powerful tool for solving numerical linear algebra problems, and as such has many applications beyond regression. We say that a subspace embedding matrix S \u2208 Rm\u00d7n embeds the column space of A \u2208 Rn\u00d7d (n > m) with u-norm into a subspace with v-norm, if \u2200x \u2208 Rd, we have \u2016Ax\u2016u/\u03b1 \u2264 \u2016SAx\u2016v \u2264 \u03b2\u2016Ax\u2016u where \u03b1\u03b2 is called distortion (approximation). A long line of work studied `2 regression problem based on `2 subspace embedding techniques; see, e.g., (Clarkson & Woodruff, 2009; 2013; Nelson & Nguye\u0302n, 2013). Furthermore, there are works on `p regression problem based on `p subspace embedding techniques (see, e.g. (Sohler & Woodruff, 2011; Meng & Mahoney, 2013; Clarkson et al., 2013; Woodruff & Zhang, 2013)), and similarly for M-estimators (Clarkson & Woodruff, 2015).\nOur overall results are composed of four parts:\n1. We develop the first subspace embedding method for all \u201cnice\u201d Orlicz norms. The embedding obtains a distortion factor polynomial in d, which was recently shown necessary (Wang & Woodruff, 2018). 2. Using the above subspace embedding, we obtain the first approximation algorithm for solving the linear regression problem with any \u201cnice\u201d Orlicz norm loss. 3. As a further illustration of the power of the subspace embedding method, we employ it towards improving on the best known result for another problem: `p low rank approximation for 1 \u2264 p < 2 from (Song et al., 2017), which is the \u201c`p-version of PCA\u201d. 4. Finally, we complement our theoretical results with experimental evaluation of our algorithms. Our experiments reveal that that the solution of regression under the Orlicz norm induced by Huber loss is much better than the solution given by regression under `1 or `2 norms, under natural noise distributions in practice. We also perform experiments for Orlicz regression with different Orlicz functions G and show their behavior under different noise settings, thus exhibiting the flexibility of our framework.\nTo the best of our knowledge, our algorithms are the first low distortion embedding and regression algorithms for general Orlicz norm. For the problem of low rank approximation under `p norm, p \u2208 [1, 2), our algorithms achieve simultaneously the best approximation and the best running time. In contrast, all the previous algorithms achieve either the best approximation, or the best running time, but not both at the same time.\nOur algorithms for subspace embedding and regression are simple, and in particular are not iterative. In particular, for the subspace embedding, the embedding matrix S is generated independently of the data. In the regression problem, we multiply the input with the embedding matrix, and thus reduce to the `2 regression problem, for which we can use any of the known algorithm.\nTechnical discussion. Next we highlight some of our techniques used to obtain the theoretical results.\nSubspace embedding. Our starting point is a technique introduced in (Andoni et al., 2017) for the Orlicz norms, which can be seen as an embedding that has guarantees for a fixed vector only. In contrast, our main challenge here is to obtain an embedding for all vectors x \u2208 Rn in a certain d-dimensional subspace. Consider a random diagonal matrix D \u2208 Rn\u00d7n with each diagonal entry is a \u201cgeneralized exponential\u201d random variable, i.e., drawn from a distribution with cumulative distribution function 1\u2212 e\u2212G(x). Then, for a fixed x \u2208 Rd, (Andoni et al., 2017) show that \u2016D\u22121Ax\u2016\u221e is not too small with high probability. We can combine this statement together with a net argument and the dilation bound on \u2016D\u22121Ax\u2016G, to argue that \u2200x \u2208 Rd, \u2016D\u22121Ax\u2016\u221e is not too small.\nThe other direction is more challenging \u2014 to show that for a given matrix A \u2208 Rn\u00d7d, and any fixed x \u2208 Rd, \u2016D\u22121Ax\u2016G cannot be too large. Once we show this \u201cdilation bound\u201d, we combine it with the well-conditioned basis argument (similar to (Dasgupta et al., 2009)), and prove that \u2200x \u2208 Rd, \u2016D\u22121Ax\u2016G cannot be too large. Overall, we have that \u2200x \u2208 Rd, \u2016D\u22121Ax\u2016G \u2264 O(d2 log n) \u00b7 \u2016Ax\u2016G, and \u2016D\u22121Ax\u2016\u221e \u2265 \u2126(1/(d log n)) \u00b7 \u2016Ax\u2016G. Since `2 norm is sandwiched by \u2016 \u00b7 \u2016G and `\u221e norm, we have that \u2200x \u2208 Rd,\u2126(1/(d log n)) \u00b7 \u2016Ax\u2016G \u2264 \u2016D\u22121Ax\u20162 \u2264 O(d2 log n) \u00b7 \u2016Ax\u2016G. Then, the remaining part is to use standard techniques (Woodruff & Zhang, 2013; Woodruff, 2014) to perform the `2 subspace embedding for the column space of D\u22121A. See Theorem 16 for details.\nThe actual proof of the dilation bound is the most technically intricate result. Traditionally, since the pth power of the `p norm is the sum of the pth power of all the entries, it is easy to bound the expectation by using linearity of the expectation. However it is impossible to apply this analysis to Orlicz norm directly since Orlicz norm is not an \u201dentrywise\u201d norm. Instead, we exploit a key observation that the\nOrlicz norm of vectors which are on the unit ball can be seen as the sum of contribution of each coordinate. Thus, we propose a novel analysis for any fixed vector by analyzing the behavior of the normalized vector which is on the unit Orlicz norm ball. To extend the dilation bound for a fixed vector to all the vectors in a subspace, we generalize the definition of `p norm well-conditioned basis to the Orlicz norm case, and then show that the Auerbach basis provides a good basis for Orlicz norm. To the best of our knowledge, this is the first time Auerbach basis are used to analyze the dilation bound of an embedding method for a norm distinct from an `p norm. See Section 3 for details.\nRegression with Orlicz norm. Here, given a matrix A \u2208 Rn\u00d7d, a vector b \u2208 Rn, the goal is to solve Equation 1 with Orlicz norm loss function. We can now solve this problem directly using the subspace embedding from above, in particular by applyingit to the column space of [A b]. We obtain an O(d3 log2 n) approximation ratio, which we can further improve by observing that it actually suffices to have the dilation bound on \u2016D\u22121Ax\u2217\u2016G only for the optimal solution x\u2217 (as opposed to for an arbitrary x). Using this observation, we improve the approximation ratio to O(d log2 n). See Theorem 18 for details. We evaluate the algorithm\u2019s performance and show that it performs well (even when n is not much larger than d). See Section 5.\n`p low rank matrix approximation. The `p norm is a special case of the Orlicz norm \u2016 \u00b7 \u2016G, where G(x) = xp. This connection allows us to consider the following problem: given A \u2208 Rn\u00d7d, n \u2265 d \u2265 k \u2265 1, find a rank-k matrix B \u2208 Rn\u00d7d such that \u2016A \u2212 B\u2016p is minimized. Here we consider the case of 1 \u2264 p < 2 and k = \u03c9(log n). The best known algorithm for this problem is from (Song et al., 2017), which uses the dense p-stable transform to achieves k2 \u00b7 poly(log n) approximation ratio. It has the downside that its runtime does not compare favorably to the golden standard of runtime linear in the sparsity of the input. To improve the runtime, one can apply the sparse p-stable transform and achieve input sparsity runtime, but that comes at the cost of an \u2126(k6) factor loss in the approximation ratio.\nUsing the above techniques, we develop an algorithm with best of both worlds: k2 \u00b7 poly(log n) approximation ratio and the input sparsity running time at the same time. In particular, the main inefficiency of the algorithm (Song et al., 2017) is the relaxation from `p norm to `2 norm, which incurs a further poly(k) approximation factor. In contrast, the embedding based on exponential random variables embeds `p norm to `2 norm directly, without further approximation loss. Our embedding also comes with its own pitfalls \u2014 as we now need to deal with mixed norms \u2014 thus requiring a new analysis of the overall algorithm. See Theorem 23 for details."}, {"heading": "2. Notations and preliminaries", "text": "In this paper, we denote R+ to be the set of nonnegative reals. Define [n] = {1, 2, \u00b7 \u00b7 \u00b7 , n}. Given a matrixA \u2208 Rn\u00d7d, \u2200i \u2208 [n], j \u2208 [d], Ai and Aj denotes the ith row and the jth column of A respectively. nnz(A) denotes the number of nonzero entries of A. The column space of A \u2208 Rd is {y | \u2203x \u2208 Rd, y = Ax}. \u2200p 6= 2, \u2016A\u2016p , ( \u2211 |Ai,j |p)1/p, i.e. entrywise p-norm. \u2016A\u2016F defines the Frobenius norm of A, i.e. ( \u2211 A2i,j)\n1/2. A\u2020 denotes the Moore-Penrose pseudoinverse of A. Given an invertible function f(\u00b7), let f\u22121(\u00b7) be the inverse function of f(\u00b7). If f(\u00b7) is not invertible in (\u2212\u221e,+\u221e) but it is invertible in [0,+\u221e), then we denote f\u22121(\u00b7) to be the inverse function of f(\u00b7) in domain [0,+\u221e). inf and sup denote the infimum and supremum respectively. f \u2032(x), f \u2032+(x), f \u2032 \u2212(x) denote the derivative, right derivative and left derivative of f(x), respectively. Similarly, define f \u2032\u2032(x) for the second derivatives, and we define f \u2032\u2032+(x) = limh\u21920+(f\n\u2032(x+h)\u2212f \u2032+(x))/h. In the following, we give the definition of Orlicz norm. Definition 1 (Orlicz norm) For any nonzero monotone nondecreasing convex function G : R+ \u2192 R+ with G(0) = 0. Define Orlicz norm \u2016 \u00b7 \u2016G as: \u2200n \u2208 Z, n \u2265 1, x \u2208 Rn, \u2016x\u2016G = inf {\u03b1 > 0 | \u2211n i=1G(|xi|/\u03b1) \u2264 1} . For any function G1(\u00b7) which is valid to define an Orlicz norm, we can always \u201csimplify/normalize\u201d the function to get another function G2 such that computing \u2016 \u00b7 \u2016G1 is equivalent to computing \u2016 \u00b7 \u2016G2 . Fact 2 Given a function G1 : R+ \u2192 R+ which can induce an Orlicz norm \u2016 \u00b7 \u2016G1 (Definition 1), define function G2 : R+ \u2192 R+ as the following:\nG2(x) =\n{ G1(G \u22121 1 (1)x) 0 \u2264 x \u2264 1\nsx\u2212 (s\u2212 1) x > 1 where s =\nsup {(G2(y)\u2212G2(x)) /(y \u2212 x) | 0 \u2264 x \u2264 y \u2264 1} . Then \u2016 \u00b7 \u2016G2 is a valid Orlicz norm. Furthermore, \u2200n \u2208 Z, n \u2265 1, x \u2208 Rn, we have \u2016x\u2016G1 = \u2016x\u2016G2/G\u221211 (1). Thus, without loss of generality, in this paper we consider the Orlicz norm induced by function G which satisfies G(1) = 1, and G(x) is a linear function for x > 1. In addition, we also require that G(x) grows no faster than quadratically in x. Thus, we define the property P of a function G : R \u2192 R+ as the following: 1) G is a nonzero monotone nondecreasing convex function in [0,\u221e); 2) G(0) = 0, G(1) = 1,\u2200x \u2208 R, G(x) = G(\u2212x); 3) G(x) is a linear function for x > 1, i.e. \u2203s > 0,\u2200x > 1, G(x) = sx + (1 \u2212 s); 4) \u2203\u03b4G > 0 such that G is twice differentiable on interval (0, \u03b4G). Furthermore, G\u2032+(0) and G\u2032\u2032+(0) exist, and either G \u2032 +(0) > 0 or G \u2032\u2032 +(0) > 0; 5) \u2203CG > 0,\u22000 < x < y,G(y)/G(x) \u2264 CG(y/x)2.\nThe condition 1 is required to define an Orlicz norm. The conditions 2,3 are required because we can always do the simplification/normalization (see Fact 2). The condition 4 is required for the smoothness of G. The condition 5 is due to the subquadratic growth condition. Subquadratic\nTable 1. Some of M-estimators.\nHUBER\n{\nx2/2 |x| \u2264 c c(|x| \u2212 c/2) |x| > c\n`1 \u2212 `2 2( \u221a\n1 + x2/2\u2212 1) \u201cFAIR\u201d c2 (|x|/c\u2212 log(1 + |x|/c))\ngrowth condition is necessary for sketching \u2211n i=1G(xi) with sketch size sub-polynomial in the dimension n, as shown by (Braverman & Ostrovsky, 2010). For example, if G(x) = xp for some p > 2, then \u2016 \u00b7 \u2016G is the same as \u2016 \u00b7 \u2016p. It is necessary to take \u2126(n1\u22122/p) space to sketch `p norm in n-dimensional space. Condition 5 is also necessary for 2-concave property, (Kwapien & Schuett, 1985; Kwapie & Schtt, 1989) shows that \u2016 \u00b7 \u2016G can be embedded into `1 space if and only if G is 2-concave. Although (Schtt, 1995) gives an explicit embedding to `1, it cannot be computed efficiently.\nThere are many potential choices of G(\u00b7) which satisfies property P , the following are some examples: 1) G(x) = |x|p for some 1 \u2264 p \u2264 2. In this case \u2016 \u00b7 \u2016G is exactly the `p norm \u2016 \u00b7 \u2016p; 2) G(x) can be a normalized M-estimator function (see (Zhang, 1997)), i.e. define f(x) to be one of the functions in Table 1. and let\nG(x) = { f(f\u22121(1)x) |x| \u2264 1 G\u2032\u2212(1)|x| \u2212 (G\u2032\u2212(1)\u2212 1) |x| > 1 .\nThe following presents some useful properties of function G with property P. See Appendix for details of proofs of the following Lemmas. Lemma 3 Given a function G(\u00b7) with property P , then \u22000 \u2264 x \u2264 1, x2/CG \u2264 G(x) \u2264 x. Lemma 4 Given a function G(\u00b7) with property P , then \u2200x \u2208 Rn, \u2016x\u20162/ \u221a CG \u2264 \u2016x\u2016G \u2264 \u2016x\u20161. Lemma 5 Given a function G(\u00b7) with property P , then \u22000 < x < y, we have y/x \u2264 G(y)/G(x). Lemma 6 Given a function G(\u00b7) with property P , there exist a constant \u03b1G > 0 which may depend on G, such that \u22000 \u2264 a, b, if ab \u2264 1, then G(a)G(b) \u2264 \u03b1GG(ab)."}, {"heading": "3. Subspace embedding for Orlicz norm using exponential random variables", "text": "In this section, we develop the subspace embedding under the Orlicz norms which are induced by functions G with the property P . We first show how to embed the subspace with \u2016 \u00b7 \u2016G norm into a subspace with `2 norm, and then we use dimensionality reduction techniques for the `2 norm. Overall, we will prove Theorem 16 stated at the end of this section. Before discussing the details, we give formal definitions of subspace embedding. Definition 7 (Subspace embedding for Orlicz norm) Given a matrix A \u2208 Rn\u00d7d, if S \u2208 Rm\u00d7n satisfies \u2200x \u2208 Rd, \u2016Ax\u2016G/\u03b1 \u2264 \u2016SAx\u2016v \u2264 \u03b2\u2016Ax\u2016G where \u03b1, \u03b2 \u2265 1, \u2016 \u00b7 \u2016v is a norm (can still be \u2016 \u00b7 \u2016G), then we say S embeds the column space of A with Orlicz norm into the column space of SA with v-norm. The distortion is \u03b1\u03b2.\nIf the distortion and the v-norm are clear from the context, we just say S is a subspace embedding matrix for A.\nDefinition 8 (Subspace embedding for `2 norm) Given a matrix A \u2208 Rn\u00d7d, if S \u2208 Rm\u00d7n satisfies \u2200x \u2208 Rd, (1\u2212 \u03b5)\u2016Ax\u201622 \u2264 \u2016SAx\u201622 \u2264 (1 + \u03b5)\u2016Ax\u201622, then we say S is a subspace embedding of column space of A.\nThere are many choices of `2 subspace embedding matrix A satisfying the above definition. Examples are: random sign JL matrix (Achlioptas, 2003; Clarkson & Woodruff, 2009), fast JL matrix (Ailon & Chazelle, 2009), and sparse embedding matrices (Clarkson & Woodruff, 2013; Meng & Mahoney, 2013; Nelson & Nguye\u0302n, 2013).\nThe main technical thrust is to embed \u2016\u00b7\u2016G into `2 norm. As the embedding matrix, we use S = \u03a0D\u22121 where \u03a0 is one of the above `2 embedding matrices andD is a diagonal matrix of which diagonal entries are i.i.d. random variables draw from the distribution with CDF 1 \u2212 e\u2212G(t). Equivalently, each entry on the diagonal of D is G\u22121(u), where u is an i.i.d. sample from the standard exponential distribution, i.e. CDF is 1 \u2212 e\u2212t. In Section 3.1, we will prove that \u2200x \u2208 Rd, \u2016D\u22121Ax\u2016Gwill not be too large. In Section 3.2, we will show that \u2200x \u2208 Rd, \u2016D\u22121Ax\u2016\u221e cannot be too small. Then due to Lemma 4, we know that \u2016D\u22121Ax\u20162 is a good estimator to \u2016Ax\u2016G. In Section 3.3, we show how to put all the ingredients together."}, {"heading": "3.1. Dilation bound", "text": "We construct a randomized linear map f : Rn \u2192 Rn: (x1, x2, ..., xn)\nf7\u2212\u2192 (x1/u1, x2/u2, ..., xn/un) where each ui is drawn from a distribution with CDF 1\u2212 e\u2212G(t). Notice that for proving the dilation bound, we do not need to assume ui are independent. Theorem 9 Given x \u2208 Rn, let \u2016 \u00b7 \u2016G be an Orlicz norm induced by function G(\u00b7) which has property P , and let f(x) = (x1/u1, x2/u2, ..., xn/un), where each ui is drawn from a distribution with CDF 1 \u2212 e\u2212G(t). Then with probability at least 1 \u2212 \u03b4 \u2212 O(1/n19), \u2016f(x)\u2016G \u2264 O(\u03b1G\u03b4\n\u22121 log(n))\u2016x\u2016G, where \u03b1G is a constant may depend on function G(\u00b7). Proof sketch: By taking union bound, we have \u2200i \u2208 [n], ui \u2265 G\u22121(1/n20) with high probability. Let \u03b1 = \u2016x\u2016G. For \u03b3 \u2265 1, we want to upper bound the probability that \u2016f(x)\u2016G \u2265 \u03b3\u03b1. This is equivalent to upper bound the probability that \u2016f(x)/(\u03b3\u03b1)\u2016G \u2265 1. Notice that Pr(\u2016f(x)/(\u03b3\u03b1)\u2016G \u2265 1) = Pr( \u2211 G(xi/\u03b1\u00b71/(\u03b3ui)) \u2265 1). By Markov inequality, it suffices to bound the expectation of \u2211 G(xi/\u03b1 \u00b71/(\u03b3ui)) conditioned on ui are not too small.\nBy lemma 6, \u2211 G(xi/\u03b1 \u00b7 1/(\u03b3ui)) \u2264 \u03b1G/\u03b3 \u00b7 \u2211 G(xi/\u03b1) \u00b7 1/G(ui). Because ui is not too small, the conditional expectation of 1/G(ui) is roughly O(log n). So the probability that \u2016f(x)\u2016G \u2265 \u03b3\u03b1 is bounded by O(\u03b1G log n/\u03b3), set \u03b3 = O(log n)\u03b1G/\u03b4, we can complete the proof. See appendix for the details of the whole proof.\nThe final step is to use a well-conditioned basis; see details in appendix.We then obtain the following theorem.\nTheorem 10 Let G(\u00b7) be a function which has property P. Given a matrix A \u2208 Rn\u00d7m with rank d \u2264 n, let D \u2208 Rn\u00d7n be a diagonal matrix of which each entry on the diagonal is drawn from a distribution with CDF 1 \u2212 e\u2212G(t). Then, with probability at least 0.99, \u2200x \u2208 Rm, \u2016D\u22121Ax\u2016G \u2264 O(\u03b1Gd\n2 log n)\u2016Ax\u2016G, where \u03b1G \u2265 1 is a constant which only depends on G(\u00b7)."}, {"heading": "3.2. Contraction bound", "text": "As in Section 3.1, we construct a randomized linear map f : Rn \u2192 Rn: (x1, x2, ..., xn) f7\u2212\u2192 (x1/u1, x2/u2, ..., xn/un) where each ui is an i.i.d. random variable drawn from a distribution with CDF 1\u2212e\u2212G(t). Notice that the difference from proving the dilation bound is that we need ui to be independent here. We use the following theorem:\nTheorem 11 (Lemma 3.1 of (Andoni et al., 2017)) Given x \u2208 Rn, let \u2016 \u00b7 \u2016G be an Orlicz norm induced by function G(\u00b7) which has property P , and let f(x) = (x1/u1, x2/u2, ..., xn/un) , where each ui is an i.i.d random variable drawn from a distribution with CDF 1 \u2212 e\u2212G(t). Then for \u03b1 \u2265 1, with probability at least 1\u2212 e\u2212\u03b1, \u2016f(x)\u2016\u221e \u2265 \u2016x\u2016G/\u03b1. By combining the result with the net argument (see appendix), and Theorems 11, 10, we get the following:\nTheorem 12 G(\u00b7) is a function with property P. Given a matrix A \u2208 Rn\u00d7m with rank d \u2264 n, let D \u2208 Rn\u00d7n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1\u2212 e\u2212G(t). Then, with probability at least 0.98, \u2200x \u2208 Rm,\u2126(1/(\u03b1\u2032Gd log n))\u2016Ax\u2016G \u2264 \u2016D\u22121Ax\u2016\u221e, where \u03b1\u2032G \u2265 1 is a constant which only depends on G(\u00b7). Proof sketch: Set \u03b5 = 1/poly(nd), we can build an \u03b5-net (see Appendix) N for the column space of A. By taking the union bound over all the net points, we have \u2200x \u2208 N, \u2016D\u22121x\u2016\u221e is not too small. Due to Theorem 10, we have \u2200x in the column space of A, \u2016D\u22121x\u2016G is not too large. Now, for any unit vector y in the column space of A, we can find the closest point x \u2208 N, and \u2016x\u2212 y\u20162 \u2264 \u03b5. Since \u2016D\u22121y\u2016\u221e \u2265 \u2016D\u22121x\u2016\u221e\u2212\u2016D\u22121(y\u2212x)\u2016\u221e, \u2016D\u22121x\u2016\u221e is not too small, and \u2016D\u22121(y \u2212 x)\u2016\u221e is not too large, we can get a lower bound for \u2016D\u22121y\u2016\u221e. See appendix for details."}, {"heading": "3.3. Putting it all together", "text": "We now combine Theorem 12, Theorem 10, and Lemma 4, to get the following theorem.\nTheorem 13 Let G(\u00b7) be a function which has property P. Given a matrix A \u2208 Rn\u00d7m with rank d \u2264 n, let D \u2208 Rn\u00d7n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1 \u2212 e\u2212G(t). Then, with probability at least 0.98, \u2200x \u2208 Rm,\u2126(1/(\u03b1\u2032Gd log n))\u2016Ax\u2016G \u2264\n\u2016D\u22121Ax\u20162 \u2264 O(\u03b1\u2032\u2032Gd2 log n)\u2016Ax\u2016G, where \u03b1\u2032\u2032G, \u03b1\u2032G \u2265 1 are two constants which only depend on G(\u00b7).\nThe above theorem successfully embeds \u2016 \u00b7 \u2016G into `2 space. We now use `2 subspace embedding to reduce the dimension. The following two theorems provide efficient `2 subspace embeddings. Theorem 14 ( (Clarkson & Woodruff, 2013)) Given matrix A \u2208 Rn\u00d7m with rank d. Let t = \u0398(d2/\u03b52), S = \u03a6Y \u2208 Rt\u00d7n, where Y \u2208 Rn\u00d7n is a diagonal matrix with each diagonal entry independently uniformly chosen to be \u00b11, \u03a6 \u2208 Rt\u00d7n is a binary matrix with \u03a6h(i),i = 1,\u2200i \u2208 [n], and remaining entries 0. Here h : [n] \u2192 [t] is a random hashing function such that for each i \u2208 [n], h(i) is uniformly distributed in [t]. Then with probability at least 0.99, \u2200x \u2208 Rm, (1 \u2212 \u03b5)\u2016Ax\u201622 \u2264 \u2016SAx\u201622 \u2264 (1 + \u03b5)\u2016Ax\u201622. Furthermore, SA can be computed in nnz(A) time.\nTheorem 15 (See e.g. (Woodruff, 2014)) Given matrix A \u2208 Rn\u00d7m with rank d. Let t = \u0398(d/\u03b52), S \u2208 Rt\u00d7n be a random matrix of i.i.d. standard Gaussian variables scaled by 1/ \u221a t. Then with probability at least 0.99, \u2200x \u2208 Rm, (1\u2212 \u03b5)\u2016Ax\u201622 \u2264 \u2016SAx\u201622 \u2264 (1 + \u03b5)\u2016Ax\u201622. We conclude the full theorem for our subspace embedding: Theorem 16 Let G(\u00b7) be a function which has property P. Given a matrix A \u2208 Rn\u00d7d, d \u2264 n, let D \u2208 Rn\u00d7n be a diagonal matrix of which each entry on the diagonal is an i.i.d. random variable drawn from the distribution with CDF 1 \u2212 e\u2212G(t). Let \u03a01 \u2208 Rt1\u00d7n be a sparse embedding matrix (see Theorem 14) and let \u03a02 \u2208 Rt2\u00d7t1 be a random Gaussian matrix (see Theorem 15) where t1 = \u2126(d2), t2 = \u2126(d). Then, with probability at least 0.9, \u2200x \u2208 Rd,\u2126(1/(\u03b1\u2032Gd log n))\u2016Ax\u2016G \u2264 \u2016\u03a02\u03a01D\u22121Ax\u20162 \u2264 O(\u03b1\u2032\u2032Gd\n2 log n)\u2016Ax\u2016G, where \u03b1\u2032\u2032G, \u03b1\u2032G \u2265 1 are two constants which only depend on G(\u00b7). Furthermore, \u03a02\u03a01D \u22121A can be computed in nnz(A) + poly(d) time."}, {"heading": "4. Applications", "text": "In this section, we discuss regression problem with Orlicz norm error measure, and low rank approximation problem with `p norm, which is a special case of the Orlicz norms."}, {"heading": "4.1. Linear regression under Orlicz norm", "text": "We first give the definition of regression problem with Orlicz norm. Definition 17 Function G(\u00b7) has property P . Given A \u2208 Rn\u00d7d, b \u2208 Rn, the goal is to solve the following minimization problem minx\u2208Rd \u2016Ax\u2212 b\u2016G.\nTheorem 18 Let G(\u00b7) have property P . Given A \u2208 Rn\u00d7d, b \u2208 Rn, Algorithm 1 can output a solution x\u0302 \u2208 Rd such that with probability at least 0.8, \u2016Ax\u0302 \u2212 b\u2016G \u2264 O(\u03b2Gd log\n2 n) minx\u2208Rd \u2016Ax \u2212 b\u2016G, where \u03b2G is a constant which may depend on G(\u00b7). In addition, the running time of Algorithm 1 is nnz(A) + poly(d).\nAlgorithm 1 Linear regression with Orlicz norm \u2016 \u00b7 \u2016G 1: Input: A \u2208 Rn\u00d7d, b \u2208 Rn. 2: Output: x\u0302. 3: Let t1 = \u0398(d2), t2 = \u0398(d). 4: Let \u03a01 \u2208 Rt1\u00d7n be a random sparse embedding matrix,\n\u03a02 \u2208 Rt2\u00d7t1 be a random gaussian matrix, and D \u2208 Rn\u00d7n be a random diagonal matrix with each diagonal entry independently drawn from distribution whose CDF is 1\u2212 e\u2212G(t). (See Theorem 16.)\n5: Compute x\u0302 = (\u03a02\u03a01D\u22121A)\u2020\u03a02\u03a01D\u22121b.\nProof sketch: Let S = \u03a02\u03a01D\u22121 be the subspace embedding for column space of [A b]. Let x\u2217 = arg minx\u2208Rd \u2016Ax \u2212 b\u2016G. Due to Theorem 16, \u2016Ax\u0302 \u2212 b\u2016G is bounded by O(d log n)\u2016S(Ax\u0302 \u2212 b)\u20162 \u2264 O(d log n)\u2016S(Ax\u2217\u2212b)\u20162 \u2264 O(d log n)\u2016D\u22121(Ax\u2217\u2212b)\u20162. Due to Theorem 9, \u2016D\u22121(Ax\u2217\u2212b)\u20162 \u2264 O(1)\u2016D\u22121(Ax\u2217\u2212 b)\u2016G \u2264 O(log n)\u2016Ax\u2217 \u2212 b\u2016G."}, {"heading": "4.2. Regression with combined loss function", "text": "In this section, we want to point out that our technique can be used on solving regression problem with more general cost function. Recall that the goal is to solve the minimization problem minx\u2208Rd \u2016Ax\u2212 b\u2016G. Now, we consider there are multiple goals, and we want to minimize a linear combination of the costs. Now we give the definition of regression problem with combined cost function.\nDefinition 19 Suppose functionG1(\u00b7), G2(\u00b7), ..., Gk(\u00b7) satisfies property P . Given A1 \u2208 Rn1\u00d7d, A2 \u2208 Rn2\u00d7d, ..., Ak \u2208 Rnk\u00d7d, b1 \u2208 Rn1 , b2 \u2208 Rn2 , ..., bk \u2208 Rnk , the goal is to solve the following minimization problem minx\u2208Rd \u2211k i=1 \u2016Aix\u2212 bi\u2016Gi .\nThe idea of solving this problem is that we can embed every term into l1 space, and then merge them into one term. By the standard technique, there is a way to embed l2 space to l1 space. We show the embedding as below. For the completeness, we put the proof of this lemma to the appendix.\nLemma 20 Let Q \u2208 Rt\u00d7n be a random matrix with each entry drawn uniformly from i.i.d. N (0, 1) Gaussian distribution. Let B = ( \u221a \u03c0/2/t) \u00b7Q. If t = \u2126( \u22122n log(n \u22121)), then with probability at least 0.98, \u2200x \u2208 Rn, \u2016Bx\u20161 \u2208 ((1\u2212 )\u2016x\u20162, (1 + )\u2016x\u20162).\nTheorem 21 Let k > 0 be a constant, and G1(\u00b7), G2(\u00b7), ..., Gk(\u00b7) satisfy property P . Given A1 \u2208 Rn1\u00d7d, A2 \u2208 Rn2\u00d7d, ..., Ak \u2208 Rnk\u00d7d, b1 \u2208 Rn1 , b2 \u2208 Rn2 , ..., bk \u2208 Rnk , Algorithm 2 can output a solution x\u0302 \u2208 Rd such that with probability at least 0.7, \u2211k i=1 \u2016Aix\u0302 \u2212 bi\u2016Gi \u2264 O(\u03b2\u2032Gd log 2 n) minx\u2208Rd \u2211k i=1 \u2016Aix\u2212 bi\u2016Gi , where \u03b2\u2032G is a constant which may depend on G1(\u00b7), G2(\u00b7), ..., Gk(\u00b7). In addition, the running time of Algorithm 2 is\u2211k\ni=1 nnz(Ai) + poly(d).\nAlgorithm 2 Linear regression with combined loss functions 1: Input: A1 \u2208 Rn1\u00d7d, A2 \u2208 Rn2\u00d7d, ..., Ak \u2208 Rnk\u00d7d, b1 \u2208\nRn1 , b2 \u2208 Rn2 , ..., bk \u2208 Rnk 2: Output: x\u0302. 3: Let t1 = \u0398(d2), t2 = \u0398(d), t3 = \u0398(t2 log(t2)). 4: Let \u03a0(1)1 \u2208 Rt1\u00d7n1 , \u00b7 \u00b7 \u00b7\u03a0 (k) 1 \u2208 Rt1\u00d7nk be k random sparse\nembedding matrices, \u03a0(1)2 , \u00b7 \u00b7 \u00b7 ,\u03a0 (k) 2 \u2208 Rt2\u00d7t1 be k random Gaussian matrices, and D(1) \u2208 Rn1\u00d7n1 , \u00b7 \u00b7 \u00b7 , D(k) \u2208 Rnk\u00d7nk be k random diagonal matrices where each diagonal entry of D(i) is independently drawn from distribution whose CDF is 1 \u2212 e\u2212Gi(t). (See Theorem 16.) Let Q(1), \u00b7 \u00b7 \u00b7 , Q(k) \u2208 Rt3\u00d7t2 be random matrices with each entry drawn uniformly from i.i.d. N (0, 1) Gaussian distribution. \u2200i \u2208 [k], let B(i) = ( \u221a \u03c0/2/t3) \u00b7 Q(i) (see Lemma 20.) Let B \u2208 Rkt3\u00d7kt2 ,\u03a02 \u2208 Rkt2\u00d7kt1 ,\u03a01 \u2208 Rkt1\u00d7 \u2211k j=1 nj , D \u2208 R \u2211k j=1 nj\u00d7 \u2211k j=1 nj be four block diagonal matrices such that \u2200i \u2208 [k], the ith block of B,\u03a02,\u03a01, D is B(i),\u03a0(i)2 ,\u03a0 (i) 1 , D\n(i) respectively. 5: Let A = [A>1 , A>2 , ..., A>k ] >, b = [b>1 , b > 2 , ..., b > k ] > and S =\nB\u03a02\u03a01D \u22121.\n6: Use classical method of solving l1 regression to get x\u0302 = arg minx\u2208Rd \u2016S(Ax\u2212 b)\u20161.\nProof Sketch: Let A = [A>1 , A>2 , ..., A>k ]>, b = [b>1 , b > 2 , ..., b > k ] >, and S = B\u03a02\u03a01D\u22121 be the subspace embedding for column space of [A b]. Let Si = B (i)\u03a0 (i) 2 \u03a0 (i) 1 (D\n(i))\u22121. Notice that \u2200x, \u2016S(Ax \u2212 b)\u20161 = \u2211k i=1 \u2016Si(Aix \u2212 bi)\u20161. Let x\u2217 =\narg minx\u2208Rd \u2211k i=1 \u2016Aix \u2212 bi\u2016Gi . Due to Theorem 16\nand Lemma 20, \u2211k i=1 \u2016Aix\u0302 \u2212 bi\u2016Gi is bounded by\nO(d log n) \u2211k i=1 \u2016Si(Aix\u0302 \u2212 bi)\u20161 = O(d log n)\u2016S(Ax\u0302 \u2212\nb)\u20161 \u2264 O(d log n)\u2016S(Ax\u2217 \u2212 b)\u20161 = \u2211k i=1 \u2016Si(Aix\u2217 \u2212\nbi)\u20161. Due to Theorem 16, \u2211k i=1 \u2016Si(Aix\u2217 \u2212 bi)\u20161 \u2264\nO(log n) \u2211k i=1 \u2016Aix\u2217 \u2212 bi\u2016Gi .\nOne application of the above Theorem is to solve the LASSO (Least Absolute Shrinkage Sector Operator) regression. In LASSO regression problem, the goal is to minimize \u2016Ax \u2212 b\u201622 + \u03bb\u2016x\u20161, where \u03bb is a parameter of regularizer. It is easy to show that it is equivalent to minimize \u2016Ax \u2212 b\u20162 + \u03bb\u2032\u2016x\u20161 for some other parameter \u03bb\u2032. When we look at \u2016Ax\u2212 b\u20162 + \u03bb\u2032\u2016x\u20161, we can set A1 = A, b1 = b, A2 = \u03bb\n\u2032I, b2 = 0, G1(\u00b7) \u2261 x2, G2(\u00b7) \u2261 x, then we are able to apply Theorem 21 to give a good approximation. The merit of our algorithm is that it is very simple, and can be computed very fast.\n4.3. `p norm low rank approximation using exponential random variables We discuss a special case of Orlicz norm \u2016 \u00b7 \u2016G, `p norm, i.e. G(x) \u2261 xp for p \u2208 [1, 2]. When rank parameter k is \u03c9(log n+log d), by using exponential random variables, we can significantly improve the approximation ratio of input sparsity time algorithms shown by (Song et al., 2017). The\nhigh level ideas combine the results of (Woodruff & Zhang, 2013; Song et al., 2017) and the dilation bound in Section 3. We define the problem in the following. See Appendix for the proof of Theorem 23. Definition 22 Let p \u2208 [1, 2]. Given A \u2208 Rn\u00d7d, n \u2265 d, k \u2208 Z, 1 \u2264 k \u2264 min(n, d), the goal is to solve the following minimization problem: minU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212A\u2016pp.\nAlgorithm 3 `p norm low rank approximation using exponential random variables. 1: Input: A \u2208 Rn\u00d7d, k \u2208 Z,min(n, d) \u2265 k \u2265 1. 2: Output: U\u0302 \u2208 Rn\u00d7k, V\u0302 \u2208 Rk\u00d7d. 3: Let t1 = \u0398(k2), t2 = \u0398(k), t3 = \u0398(k log k). 4: Let \u03a01, S1 \u2208 Rt1\u00d7n be two random sparse embedding matri-\nces, \u03a02, S2 \u2208 Rt2\u00d7t1 be two random gaussian matrices, and D1, D2 \u2208 Rn\u00d7n be two random diagonal matrices with each diagonal entry independently drawn from distribution whose CDF is 1\u2212 e\u2212t p\n. (See Theorem 16.) 5: Let T2, R \u2208 Rd\u00d7t3 be two random matrix, with i.i.d. entries\ndrawn from standard p-stable distribution. 6: Let S = S2S1D\u221211 , T1 = \u03a02\u03a01D \u22121 2 . 7: Solve X\u0302, Y\u0302 = arg minX\u2208Rt2\u00d7k,Y \u2208Rk\u00d7t3 \u2016T1ARXY SAT2\u2212 T1AT2\u20162F . 8: U\u0302 = ARX\u0302, V\u0302 = Y\u0302 SA.\nTheorem 23 Let 1 \u2264 p \u2264 2. Given A \u2208 Rn\u00d7d, n \u2265 d, k \u2208 Z, 1 \u2264 k \u2264 min(n, d), with probability at least 2/3, U\u0302 , V\u0302 outputted by Algorithm 3 satisfies: \u2016U\u0302 V\u0302 \u2212 A\u2016pp \u2264 \u03b1minU\u2208Rn\u00d7k,V \u2208Rk\u00d7d \u2016UV \u2212 A\u2016pp, where \u03b1 = O(min((k log k)4\u2212p log2p+2 n, (k log k)4\u22122p log4+p n)). In addition, the running time of Algorithm 3 is nnz(A) + (n+ d)poly(k)."}, {"heading": "5. Experiments", "text": "Implementation setups can be seen in appendix."}, {"heading": "5.1. Orlicz Norm Linear Regression", "text": "In this section, we show that our algorithm i) has reasonable and predictable performance under different scenarios and ii) is flexible, general and easy to use. We perform 3 sets of experiments. The first is to compare its performance with the standard `1 and `2 regression under different noise assumptions and dimensions of the regression problem; the second is to compare the performance of Orlicz regression with different G under different noise assumptions; the third is to experiment with Orlicz function G that is different from standard `p and Huber function. We evaluate the performance of our Orlicz norm linear regression algorithm on simulated data. Comparison with `1 and `2 regression We would like to see whether Orlicz norm linear regression leads to expected performance relative to `1 and `2 regression. We choose our Orlicz norm \u2016 \u00b7 \u2016G to be induced by the normalized Huber function where the Huber function is defined as\nf(x) = { x2/2 |x| \u2264 \u03b4 \u03b4 \u00b7 (|x| \u2212 \u03b4/2) o.w. . We chose the parameter \u03b4 to be 0.75. Intuitively, it is between `1 and `2\nnorm (see Figure 1). In all the simulations, we generate matrix A \u2208 Rn\u00d7d, ground truth x\u2217 \u2208 Rd, and b to be Ax\u2217 plus some particular noise. We evaluate the performance of each algorithm by the `2 distances between the output x and the ground truth x\u2217. In terms of algorithm details, since n, d are not too large in our simulation, we did not apply the `2 subspace embedding to reduce the dimension; we only use reciprocal exponential random diagonal embedding matrix to embed \u2016 \u00b7 \u2016G to `2 norm (see Theorem 13)1.\nWe experiment with two n, d combinations, i) n = 200, d = 10 ii) n = 100, d = 75, and 3 noise setting with i) Gaussian noise ii) sparse noise and iii) mixed noise (addition of i) and ii)), altogether 2 \u00d7 3 = 6 setting. The detail of data simulation can be seen in appendix. For each experiment we repeat 50 times and compute the mean. The results are shown in Table 2. Orlicz norm regression has better performance than `1 and `2 when the noise is mixed. When the noise is Gaussian or sparse, Orlicz norm regression works better than `1 and `2 respectively. We did not experiment with Huber loss regression, since if we rescale the data and make it small/large in absolute values, the Huber regression will degenerate into respectively `2/`1 regression (see Introduction). See appendix for results on approximation ratio.\nChoice of \u03b4 for G as a normalized Huber function We compare the performance of Orlicz norm regression induced by G as normalized Huber loss function with different \u03b4 under different noise assumptions. We fix n = 500, d = 30 and generate A and x as in the first set of experiments (see appendix). The noise is a mixture of N(0, 5) Gaussian noise and sparse noise on 1% entries with different scale of uniform noise from [\u2212s\u2016Ax\u2217\u20162, s\u2016Ax\u2217\u20162], where scale s is chosen from [0, 0.5, 1, 2]. Under each noise assumptions with different scale s, we compare the performance of Orlicz norm regression induced by G with \u03b4 from [0.05, 0.1, 0.2, 0.4, 1, 2]. We repeat each experiment 50 times and report the mean of the `2 distance between output x and the ground truth x\u2217. The result is shown in Figure 2. When the scale is 0/2, the noise is almost Gaussian/sparse and we expect `2/`1 norm and thus large/small \u03b4 to perform the best; anything scale lying in between these extremes will have an optimal \u03b4 in between. We observe the expected trend: as s increases, the performance is optimal with smaller \u03b4.\n1We use MATLAB\u2019s linprog to solve `1 regression.\nBeyond Huber function - A General Framework We explore a variant Orlicz function G and evaluate it under a particular setting; the evaluation criteria is the same as the first set of the experiments. The G is of the same form aforementioned, except that it now grows at the order of x1.5 when x is small. We denote it by G`1.5 , which is the normalization of function f , and f is defined as: f(x) ={ x1.5/1.5 x \u2264 \u03b4 \u03b40.5 \u00b7 (|x| \u2212 \u03b4/3) o.w. . We generate a 500\u00d730 matrix A and the ground truth vector x\u2217 in the same way as before, and then addN(0, 5) Gaussian noises and 1 sparse outlier with scale s = 100. We find that the modified G`1.5 under this settings outperforms `1, `2, `1.5, G\u03b4=0.25, G\u03b4=0.75 regression by a significant amount where G\u03b4=0.25, G\u03b4=0.75 are Orlicz norm induced by regular normalized Huber function with \u03b4 = 0.25, 0.75 respectively. The results are shown in Table 3. This experiment demonstrates that our algorithm is i) flexible enough to combine the advantage of norm functions, ii) general for any function that satisfies the nice property, and iii) easy to experiment with different settings, as long as we can compute G and G\u22121."}, {"heading": "5.2. `1 low rank matrix approximation", "text": "In this section, we evaluate the performance of the `1 low rank matrix approximation algorithm. We mainly compare the `1 norm error of our algorithm with the error of (Song et al., 2017) and standard PCA. Inputs are a matrix A \u2208 Rn\u00d7d and a rank parameter k; the goal is to output a rank k matrix B such that \u2016A \u2212 B\u20161 is as small as possible. The details of implementations are in the appendix. For each input, we run the algorithm 50 times and pick the best solution.\nDatasets. We first run experiment on synthetic data: we randomly choose two matrices U \u2208 R2000\u00d75, V \u2208 R5\u00d72000 with each entry drawn uniformly from (0, 1) Then we randomly choose 100 entries of UV , and add random outliers uniformly drawn from (\u2212100, 100) on those entries, thus\nwe can get a matrix A \u2208 R2000\u00d72000. In our experiment, \u2016A\u20161 is about 5.0\u00d7 106. Then, we run experiments on real datasets diabetes and glass in UCI repository(Bache & Lichman, 2013). The data matrix of diabetes has size 768\u00d7 8, and the data matrix of glass has size 214\u00d7 9. For each data matrix, we randomly add outliers on 1% number of entries.\nFor each dataset, we evaluate the \u2016A\u2212B\u20161. The result for the experiment on synthetic data is shown in Table 4, and the results for diabetes and glass are shown in Figure 3. The running time of algorithm in (Song et al., 2017) on diabetes and on glass are 5.69 and 11.97 seconds respectively, with ours being 3.18 and 3.74 seconds respectively. We also find that our algorithm consistently outperforms the other two alternatives (note that the y-coordinates are at log scale with base 10)."}, {"heading": "6. Conclusion and Future Work", "text": "In this paper we presented an efficient subspace embedding algorithm for orlicz norm and demonstrated its usefulness in regression/low rank approximation problem on synthetic and real datasets. Nevertheless, O(d log2 n) is still a large theoretical approximation factor, and hence it is worth i) investigating whether the theoretical approximation ratio can be smaller if input are under some statistical distribution ii) calculating the actual approximation ratio with ground truth obtained by some slower but more accurate optimization algorithm. It is also worth examining whether our exponential embedding sketching method preserves the statistical properties of the regression error, since we assumed a different noise distribution from Gaussian/double-exponential as a starting point (Raskutti & Mahoney, 2014; Lopes et al., 2018)."}, {"heading": "Acknowledgements", "text": "Research supported in part by Simons Foundation (#491119 to Alexandr Andoni), NSF (CCF-1617955, CCF- 1740833), and Google Research Award."}], "year": 2018, "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "authors": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences,", "year": 2003}, {"title": "The fast johnson\u2013lindenstrauss transform and approximate nearest neighbors", "authors": ["N. Ailon", "B. Chazelle"], "venue": "SIAM Journal on Computing,", "year": 2009}, {"title": "Approximate near neighbors for general symmetric norms", "authors": ["A. Andoni", "A. Nikolov", "I. Razenshteyn", "E. Waingarten"], "venue": "In Proceedings of the Forty-ninth annual ACM symposium on Theory of computing,", "year": 2017}, {"title": "Uci machine learning repository", "authors": ["K. Bache", "M. Lichman"], "venue": "URL http://archive. ics. uci. edu/ml,", "year": 2013}, {"title": "Robust regression via hard thresholding", "authors": ["K. Bhatia", "P. Jain", "P. Kar"], "venue": "Advances in Neural Information Processing Systems", "year": 2015}, {"title": "Zero-one frequency laws", "authors": ["V. Braverman", "R. Ostrovsky"], "venue": "In Proceedings of the forty-second ACM symposium on Theory of computing,", "year": 2010}, {"title": "Numerical linear algebra in the streaming model", "authors": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "In Proceedings of the forty-first annual ACM symposium on Theory of computing,", "year": 2009}, {"title": "Low rank approximation and regression in input sparsity time", "authors": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Sketching for mestimators: A unified approach to robust regression", "authors": ["K.L. Clarkson", "D.P. Woodruff"], "venue": "In Proceedings of the Twenty-Sixth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2015}, {"title": "Sampling algorithms and coresets for `p regression", "authors": ["A. Dasgupta", "P. Drineas", "B. Harb", "R. Kumar", "M.W. Mahoney"], "venue": "SIAM Journal on Computing,", "year": 2009}, {"title": "Revisiting a 90-year-old debate: the advantages of the mean deviation", "authors": ["S. Gorard"], "venue": "British Journal of Educational Studies,", "year": 2005}, {"title": "Robust estimation of a location parameter", "authors": ["Huber", "P. J"], "venue": "The Annals of Mathematical Statistics,", "year": 1964}, {"title": "Some combinatorial and probabilistic inequalities and their application to banach space theory", "authors": ["S. Kwapien", "C. Schuett"], "venue": "Studia Mathematica,", "year": 1985}, {"title": "Some combinatorial and probabilistic inequalities and their application to banach space theory ii", "authors": ["S. Kwapie", "C. Schtt"], "venue": "Studia Mathematica,", "year": 1989}, {"title": "Multivariate regression with calibration", "authors": ["H. Liu", "L. Wang", "T. Zhao"], "venue": "Advances in Neural Information Processing Systems", "year": 2014}, {"title": "Error estimation for randomized least-squares algorithms via the bootstrap", "authors": ["M.E. Lopes", "S. Wang", "M.W. Mahoney"], "venue": "arXiv preprint arXiv:1803.08021,", "year": 2018}, {"title": "Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression", "authors": ["X. Meng", "M.W. Mahoney"], "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,", "year": 2013}, {"title": "Faster numerical linear algebra algorithms via sparser subspace embeddings", "authors": ["J. Nelson", "Nguy\u00ean", "H.L. Osnap"], "venue": "In Foundations of Computer Science (FOCS),", "year": 2013}, {"title": "A statistical perspective on randomized sketching for ordinary least-squares", "authors": ["G. Raskutti", "M. Mahoney"], "venue": "arXiv preprint arXiv:1406.5986,", "year": 2014}, {"title": "On the embedding of 2-concave orlicz spaces into l1", "authors": ["C. Schtt"], "venue": "Studia Mathematica,", "year": 1995}, {"title": "Subspace embeddings for the `1-norm with applications", "authors": ["C. Sohler", "D.P. Woodruff"], "venue": "In Proceedings of the fortythird annual ACM symposium on Theory of computing,", "year": 2011}, {"title": "Low rank approximation with entrywise `1-norm error", "authors": ["Z. Song", "D.P. Woodruff", "P. Zhong"], "venue": "In Proceedings of the 49th Annual Symposium on the Theory of Computing. ACM, arXiv preprint arXiv:1611.00898,", "year": 2017}, {"title": "Tight bounds for `p oblivious subspace embeddings", "authors": ["R. Wang", "D.P. Woodruff"], "venue": "arXiv preprint arXiv:1801.04414,", "year": 2018}, {"title": "Subspace embeddings and `p regression using exponential random variables", "authors": ["D. Woodruff", "Q. Zhang"], "venue": "In Conference on Learning Theory, pp", "year": 2013}, {"title": "Sketching as a tool for numerical linear algebra", "authors": ["D.P. Woodruff"], "venue": "Foundations and Trends in Theoretical Computer Science,", "year": 2014}, {"title": "Parameter estimation techniques: A tutorial with application to conic fitting", "authors": ["Z. Zhang"], "venue": "Image and vision Computing,", "year": 1997}, {"title": "Mixed linear regression with multiple components", "authors": ["K. Zhong", "P. Jain", "I.S. Dhillon"], "venue": "Advances in Neural Information Processing Systems", "year": 2016}], "id": "SP:79df70814de320d27cc4ad0572da0578533eadca", "authors": [{"name": "Alexandr Andoni", "affiliations": []}, {"name": "Chengyu Lin", "affiliations": []}, {"name": "Ying Sheng", "affiliations": []}, {"name": "Peilin Zhong", "affiliations": []}, {"name": "Ruiqi Zhong", "affiliations": []}], "abstractText": "We consider a generalization of the classic linear regression problem to the case when the loss is an Orlicz norm. An Orlicz norm is parameterized by a non-negative convex function G : R+ \u2192 R+ with G(0) = 0: the Orlicz norm of a vector x \u2208 R is defined as \u2016x\u2016G = inf {\u03b1 > 0 | \u2211n i=1G(|xi|/\u03b1) \u2264 1} . We consider the cases where the function G(\u00b7) grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column space of a given matrix A \u2208 Rn\u00d7d with Orlicz norm into a lower dimensional space with `2 norm. Specifically, we show how to efficiently find an embedding matrix S \u2208 Rm\u00d7n,m < n such that \u2200x \u2208 R,\u03a9(1/(d log n)) \u00b7 \u2016Ax\u2016G \u2264 \u2016SAx\u20162 \u2264 O(d log n) \u00b7 \u2016Ax\u2016G. By applying this subspace embedding technique, we show an approximation algorithm for the regression problem minx\u2208Rd \u2016Ax\u2212b\u2016G, up to aO(d log n) factor. As a further application of our techniques, we show how to also use them to improve on the algorithm for the `p low rank matrix approximation problem for 1 \u2264 p < 2.", "title": "Subspace Embedding and Linear Regression with Orlicz Norm"}