1	30	Research into this topic has many potential practical applications.
6	29	The collection of simple-complex parallel sentences sparked a major advance for machine translationbased approaches to simplification.
16	50	It helps us to showcase the limitations of Wikipedia data in comparison and it provides potential remedies that may improve simplification research.
18	23	There are many hints in past publications that reflect the inadequacy of this resource, which we piece together in this paper to support our arguments.
19	84	Several different simplification datasets have been proposed (Bach et al., 2011; Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Woodsend and Lapata, 2011b), but most of these are derived from Wikipedia and not thoroughly analyzed.
20	209	Siddharthan (2014)’s excellent survey of text simplification research states that one of the most important questions that needs to be addressed is “how good is the quality of Simple English Wikipedia”.
21	34	To the best of our knowledge, we are the first to systematically quantify the quality of Simple English Wikipedia and directly answer this question.
22	22	We make our argument not as a criticism of others or ourselves, but as an effort to refocus research directions in the future (Eisenstein, 2013).
23	79	We hope to inspire the creation of higher quality simplification datasets, and to encourage researchers to think critically about existing resources and evaluation methods.
26	47	It has become a benchmark dataset for simplification largely because of its size and availability, and because follow-up papers (Woodsend and Lapata, 2011a; Coster and Kauchak, 2011; Wubben et al., 2012; Narayan and Gardent, 2014; Siddharthan and Angrosh, 2014; Angrosh et al., 2014) often compare with Zhu et al.’s system outputs to demonstrate further improvements.
45	59	The quality of “real simplifications” also varies: some sentences are simpler by only one word while the rest of sentence is still complex.
46	93	The main causes of non-simplifications and partial-simplifications in the parallel Wikipedia corpus include: 1) The Simple Wikipedia was created by volunteer contributors with no specific objective; 2) Very rarely are the simple articles complete re-writes of the regular articles in Wikipedia (Coster and Kauchak, 2011), which makes automatic sentence alignment errors worse; 3) As an encyclopedia, Wikipedia contains many difficult sentences with complex terminology.
55	26	To study how professional editors conduct text simplification, we have assembled a new simplification dataset that consists of 1,130 news articles.
74	37	Compared to the Newsela data, the Wikipedia corpus contains remarkably longer (more complex) words and the difference of sentence length before and after simplification is much smaller.
92	29	The two corpora are more agreeable on what the simple words are than what complex words need to be simplified.
94	69	The odds ratio of token t between two texts sets i and j is defined as: r (i−j) t = yit/y j t ni/nj (5) It reflects the difference of topics and degree of simplification between the Wikipedia and the Newsela data.
100	28	To extract theses patterns we parsed our corpus with the Stanford Parser (Klein and Manning, 2002) and applied its built-in head word identifier from Collins (2003).
102	87	However, as with word frequency (Table 8), complex syntactic patterns are retained more often in Wikipedia’s simplifications than in Newsela’s.
114	31	They noted that Simple Wikipedia is “less mature” with many articles that are just “stubs, comprising a single paragraph of just one or two sentences”.
121	33	Although discourse is known to affect readability, the relation between discourse and text simplification is still under-studied with the use of statistical methods (Williams et al., 2003; Siddharthan, 2006; Siddharthan and Katsos, 2010).
123	20	However, previous research that uses SimpleNormal Wikipedia largely focuses on sentence-level transformation, without taking large discourse structure into account.
131	28	They are not exchangeable in developing nor in evaluating simplification systems.
134	36	All simplification systems published in the ACL, NAACL, EACL, COLING and EMNLP main conferences since Zhu’s 2010 work compared solely on the same test set that consists of only 100 sentences from Wikipedia, except one paper that additionally experimented with 5 short news summaries.
139	21	Many researchers have studied simplification in the context of different audiences.
146	37	Once there is a well-defined objective, with constraints such as vocabulary size and sentence length, it is easier to fairly compare different systems.
167	21	Simple English Wikipedia played a vital role in inspiring simplification approaches based on statistical machine translation.
169	41	Other resources like the Newsela corpus are superior, since they provide a more consistent level of quality, target a particular audience, and approach the size of parallel Simple-Normal English Wikipedia.
170	306	We believe that simplification is an important area of research that has the potential for broader impact beyond NLP research.
171	241	But we must first adopt appropriate data sets and research methodologies.
172	42	Researchers can request the Newsela data following the instructions at: https://newsela.
