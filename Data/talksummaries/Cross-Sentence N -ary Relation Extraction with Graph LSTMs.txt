52	1	Our approach makes it very easy to use multi-task learning over both the n-ary relations and their sub-relations.
69	1	Jointly designing classifiers with graph LSTMs would be interesting future work.
75	1	), parametrization becomes a key problem.
90	1	Essentially, a copy of the graph is created for each step that serves as input for the next.
93	1	Effectively, gradients are backpropagated in a manner similar to loopy belief propagation (LBP).
102	1	The other DAG covers the right-to-left linear chain and the backward-pointing dependencies.
103	1	Figure 3 illustrates this strategy.
104	1	Effectively, we partition the original graph into the forward pass (left-to-right), followed by the backward pass (right-to-left), and construct the LSTMs accordingly.
113	1	In this paper, we explore two schemes that introduce more fined-grained parameters based on the edge types.
118	1	In graph LSTMs, a unit might have multiple predecessors (P (t)), for each of which (j) there is a forget gate ftj , and a typed weight matrix Um(t,j), where m(t, j) signifies the connection type between t and j.
132	1	In graph LSTMs, the encoding of linguistic knowledge is factored from the backpropagation strategy (Section 3.2), making it much more flexible, including introducing cycles.
137	1	Although existing systems have not yet been shown to improve crosssentence relation extraction (Quirk and Poon, 2017), it remains an important future direction to explore incorporating such analyses, especially after adapting them to the biomedical domains (Bell et al., 2016).
141	1	All classifiers share the same graph LSTMs representation learner and word embeddings, and can potentially help each other by pooling their supervision signals.
146	1	Hyper parameters were set based on preliminary experiments on a small development dataset.
148	1	We used a learning rate of 0.02 and trained for at most 30 epochs, with early stopping based on development data (Caruana et al., 2001; Graves et al., 2013).
153	1	This was repeated for 30 epochs.
163	1	As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts.
167	1	We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision.
169	1	After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples.
199	1	In Quirk and Poon (2017), the best system incorporated syntactic dependencies and outperformed the linear-chain variant (Base) by a large margin.
224	1	The resulting model was then used to extract relations from all PubMed Central articles.
225	1	Table 4 shows the number of candidates and extracted interactions.
229	1	Again, machine reading covers far more unique entities, especially with cross-sentence extraction.
231	1	Therefore, we randomly sampled extracted relation instances and asked three researchers knowledgeable in precision medicine to evaluate their correctness.
232	1	For each instance, the annotators were presented with the provenance: sentences with the drug, gene, and mutation highlighted.
251	1	Graph LSTMs exhibited a more commanding advantage over linear-chain LSTMs in this domain, substantially outperforming the latter (p < 0.01 by McNemarâ€™s chi-square test).
281	1	While there is much room to improve in both recall and precision, our results indicate that machine reading can already be useful in precision medicine.
285	15	Therefore, the most important goal is to attain high recall and reasonable precision.
286	88	Our current models are already quite capable.
287	87	Future directions include: interactive learning with user feedback; improving discourse modeling in graph LSTMs; exploring other backpropagation strategies; joint learning with entity linking; applications to other domains.
