{"sections": [{"text": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2337\u20132342, Austin, Texas, November 1-5, 2016. c\u00a92016 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "The size of labelled datasets has long been recognized as a bottleneck in the performance of natural language processing systems (Marcus et al., 1993; Petrov and McDonald, 2012). Such datasets are expensive to create, requiring expert linguists and extensive annotation guidelines. Even relatively large datasets, such as the Penn Treebank, are much smaller than required\u2014as demonstrated by improvements from semi-supervised learning (S\u00f8gaard and Rish\u00f8j, 2010; Weiss et al., 2015).\nWe take a step towards cheap, reliable annotations by introducing human-in-the-loop parsing, where\nnon-experts improve parsing accuracy by answering questions automatically generated from the parser\u2019s output. We develop the approach for CCG parsing, leveraging the link between CCG syntax and semantics to convert uncertain attachment decisions into natural language questions. The answers are used as soft constraints when re-parsing the sentence.\nPrevious work used crowdsourcing for less structured tasks such as named entity recognition (Werling et al., 2015) and prepositional phrase attachment (Jha et al., 2010). Our work is most related to that of Duan et al. (2016), which automatically generates paraphrases from n-best parses and gained significant improvement by re-training from crowdsourced judgments on two out-of-domain datasets. Choe and McClosky (2015) improve a parser by creating paraphrases of sentences, and then parsing the sentence and its paraphrase jointly. Instead of using paraphrases, we build on the approach of QA-SRL (He et al., 2015), which shows that untrained crowd workers can annotate predicate\u2013argument structures by writing question\u2013answer pairs.\nOur experiments for newswire and biomedical\n2337\ntext demonstrate improvements to parsing accuracy of 1.7 F1 on the sentences changed by re-parsing, while asking only less than 2 questions per sentence. The annotations we collected1 are a representationindependent resource that could be used to develop new models or human-in-the-loop algorithms for related tasks, including semantic role labeling and syntactic parsing with other formalisms."}, {"heading": "2 Mapping CCG Parses to Queries", "text": "Our annotation task consists of multiple-choice what-questions that admit multiple answers. To generate them, we produce question\u2013answer (QA) pairs from each parse in the 100-best scored output of a CCG parser and aggregate the results together.\nWe designed the approach to generate queries with high question confidence\u2014questions should be simple and grammatical, so annotators are more likely to answer them correctly\u2014and high answer uncertainty\u2014the parser should be uncertain about the answers, so there is potential for improvement.\nOur questions only apply to core arguments of verbs where the argument phrase is an NP, which account for many of the parser\u2019s mistakes. Prepositional phrase attachment mistakes are also a large source of errors\u2014we tried several approaches to generate questions for these, but the greater ambiguity and inconsistency among both annotators and the gold parses made it difficult to extract meaningful signal from the crowd.\nGenerating Question\u2013Answer Pairs Figure 1 shows how we generate QA pairs. Each QA pair corresponds to a dependency such that if the answer is correct, it indicates that the dependency is in the correct parse. We determine a verb\u2019s set of arguments by the CCG supertag assigned to it in the parse (see Steedman (2000) for an introduction to CCG). For example, in Figure 1 the word put takes the category ((S\\NP)/PP)/NP (not shown), indicating that it has a subject, a prepositional phrase argument, and an object. CCG parsing assigns dependencies to each argument position, even when the arguments are reordered (as with put\u2192 pizza) or span long distances (as with eat\u2192 I).\n1Our code and data are available at https://github. com/luheng/hitl_parsing.\nTo reduce the chance of parse errors causing nonsensical questions (for example, What did the pizza put something on?), we replace all noun phrases with something and delete unnecessary prepositional phrases. The exception to this is with copular predicates, where we include the span of the argument in the question (see Example 4 in Table 2).\nGrouping QA Pairs into Queries After generating QA pairs for every parse in the 100-best output of the parser, we pool the QA pairs by the head of the dependency used to generate them, its CCG category, and their question strings. We also compute marginalized scores for each question and answer phrase by summing over the scores of all the parses that generated them. Each pool becomes a query, and for each unique dependency used to generate QA pairs in that pool, we add a candidate answer to the query by choosing the answer phrase that has the highest marginalized score for that dependency. For example, if some parses generated the answer phrase pizza for the dependency eat \u2192 pizza, but most of the high-scoring parses generated the answer phrase the pizza, then only the pizza appears as an answer.\nFrom the resulting queries, we filter out questions and answers whose marginalized scores are below a certain threshold and queries that only have one answer choice. This way we only ask confident questions with uncertain answer lists."}, {"heading": "3 Crowdsourcing", "text": "We collected data on the crowdsourcing platform CrowdFlower.2 Annotators were shown a sentence, a question, and a list of answer choices. Annotators could choose multiple answers, which was useful in case of coordination (see Example 3 in Table 2). There was also a None of the above option for when no answer was applicable or the question was nonsensical.\nWe instructed annotators to only choose options that explicitly and directly answer the question, to encourage their answers to closely mirror syntax. We also instructed them to ignore who/what and someone/something distinctions and overlook mistakes where the question was missing a negation. The instructions included 6 example queries\n2www.crowdflower.com\nwith answers and explanations. We used CrowdFlower\u2019s quality control mechanism, displaying preannotated queries 20% of the time and requiring annotators to maintain high accuracy.\nDataset Statistics Table 3 shows how many sentences we asked questions for and the total number of queries annotated. We collected annotations for the development and test set for CCGbank (Hockenmaier and Steedman, 2007) as in-domain data and the test set of the Bioinfer corpus (Pyysalo et al., 2007) as out-of-domain. The CCGbank development set was used for building question generation heuristics and setting hyperparameters for reparsing.\n5 annotators answered each query; on CCGbank we required 85% accuracy on test questions and on Bioinfer we set the threshold at 80% because of the difficulty of the sentences. Table 4 shows inter-annotator agreement. Annotators unanimously chose the same set of answers for over 40% of the queries; an absolute majority is achieved for over 90% of the queries.\nQualitative Analysis Table 2 shows example queries from the CCGbank development set. Examples 1 and 2 show that workers could annotate longrange dependencies and scoping decisions, which are challenging for existing parsers.\nHowever, there are some cases where annotators disagree with the gold syntax, mostly involving semantic phenomena which are not reflected in the syntactic structure. Many cases involve coreference, where annotators often prefer a proper noun referent over a pronoun or indefinite (see Examples 4 and 5), even if it is not the syntactic argument of the verb. Example 6 shows a complex control structure, where the gold CCGbank syntax does not recover the true agent of build. CCGbank also does not distinguish between subject and object control. For these cases, our method could be used to extend existing treebanks. Another common error case involved partitives and related constructions, where the correct attachment is subtle\u2014as reflected by the annotators\u2019 split decision in Example 7.\nQuestion Quality Table 5 shows the percentage of questions that are answered with None of the above (written N/A below) by at most k annotators. On all domains, about 80% of the queries are considered answerable by all 5 annotators. To have a better understanding of the quality of automatically generated questions, we did a manual analysis on 50 questions for sentences from the CCGbank development set that are marked N/A by more than one annotator. Among the 50 questions, 31 of them are either generated from an incorrect supertag or unanswerable given the candidates. So the N/A answer\ncan provide useful signal that the parses that generated the question are likely incorrect. Common mistakes in question generation include: bad argument span in a copula question (4 questions), bad modality/negation (3 questions), and missing argument or particle (5 questions). Example 8 in Table 2 shows an example of a nonsensical question. While the parses agreed with the gold category S\\NP, the question they generated omitted the negation and the verb phrase that was elided in the original sentence. In this case, 3 out of 5 annotators were able to answer with the correct dependency, but such mistakes can make re-parsing more challenging.\nCost and Speed We paid 6 cents for each answer. With 5 judgments per query, 20% test questions, and CrowdFlower\u2019s 20% service fee, the average cost per query was about 46 cents. On average, we collected about 1000 judgments per hour, so we were able to annotate all the queries generated from the CCGbank test set within 15 hours."}, {"heading": "4 Re-Parsing with QA Annotation", "text": "To improve the output of the parser, we re-parse each sentence with an augmented scoring function that penalizes parses for disagreeing with annotators\u2019 choices. If q is a question, a is an answer to q, d is the dependency that produced the QA pair \u3008q, a\u3009, and v(a) annotators chose a, we add re-parsing constraints as follows: \u2022 If v(None of the above) \u2265 T+, penalize parses\nthat agree with q\u2019s supertag on the verb by wt\n\u2022 If v(a) \u2264 T\u2212, penalize parses containing d by w\u2212 \u2022 If v(a) \u2265 T+, penalize parses that do not contain d by w+ where T+, T\u2212, wt, w\u2212, and w+ are hyperparameters. We incorporate these penalties into the parsing model during decoding. By using soft constraints, we mitigate the risk of incorrect annotations worsening a high-confidence parse.\nSome errors are predictable: for example, if a is a non-possessive pronoun and is closer to the verb than its referent a\u2032, annotators often choose a\u2032 when a is correct (See Example 4 in Table 2). If a is a subspan of another answer a\u2032 and their votes differ by at most one (See Example 7 in Table 2), it is unlikely that both a and a\u2032 are correct. In these cases we use disjunctive constraints, where the parse needs to have at least one of the desired dependencies.\nExperimental Setup We use Lewis et al. (2016)\u2019s state-of-the-art CCG parser for our baseline. We chose the following set of hyperparameters based on performance on development data (CCG-Dev): w+ = 2.0, w\u2212 = 1.5, wt = 1.0, T+ = 3, T\u2212 = 0. In the Bioinfer dataset, we found during development that the pronoun/subspan heuristics were not as useful, so we did not use them in re-parsing.\nResults Table 6 shows our end-to-end parsing results. The larger improvement on out-of-domain sentences shows the potential for using our method for domain adaptation. There is a much smaller improvement on test data than development data, which may be related to the lower annotator agreement reported in Table 4.\nThere was much larger improvement (1.7 F1) on the subset of sentences that are changed after reparsing, as shown in Table 7. This suggests that our method could be effective for semi-supervised learning or re-training parsers. Overall improvements on CCGbank are modest, due to only modifying 10% of sentences."}, {"heading": "5 Discussion and Future Work", "text": "We introduced a human-in-the-loop framework for automatically correcting certain parsing mistakes. Our method identifies attachment uncertainty for core arguments of verbs and automatically generates\nquestions that can be answered by untrained annotators. These annotations improve performance, particularly on out-of-domain data, demonstrating for the first time that untrained annotators can improve state-of-the-art parsers.\nSentences modified by our framework show substantial improvements in accuracy, but only 10% of sentences are changed, limiting the effect on overall accuracy. This work is a first step towards a complete approach to human-in-the-loop parsing.\nFuture work will explore the possibility of asking questions about other types of parsing uncertainties, such as nominal and adjectival argument structure, and a more thorough treatment of prepositionalphrase attachment, including distinctions between arguments and adjuncts. We hope to scale these methods to large unlabelled corpora or other languages, to provide data for re-training parsers."}, {"heading": "Acknowledgments", "text": "This work was supported by the NSF (IIS-1252835, IIS-1562364), DARPA under the DEFT program through the AFRL (FA8750-13-2-0019), an Allen Distinguished Investigator Award, and a gift from Google. We are grateful to Chloe\u0301 Kiddon for helpful comments on the paper, and Kenton Lee for help with the CCG parser. We would also like to thank our workers on Crowdflower for their annotation and the anonymous reviewers for their valuable feedback."}], "year": 2016, "references": [{"title": "Parsing paraphrases with joint inference", "authors": ["Do Kook Choe", "David McClosky."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics.", "year": 2015}, {"title": "Generating disambiguating paraphrases for struc2341", "authors": ["Manjuan Duan", "Ethan Hill", "Michael White"], "year": 2016}, {"title": "Question-answer driven semantic role labeling: Using natural language to annotate natural language", "authors": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing.", "year": 2015}, {"title": "Ccgbank: a corpus of ccg derivations and dependency structures extracted from the penn treebank", "authors": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics.", "year": 2007}, {"title": "Corpus creation for new genres: A crowdsourced approach to pp attachment", "authors": ["Mukund Jha", "Jacob Andreas", "Kapil Thadani", "Sara Rosenthal", "Kathleen McKeown."], "venue": "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data", "year": 2010}, {"title": "Lstm ccg parsing", "authors": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics.", "year": 2016}, {"title": "Building a large annotated corpus of english: The penn treebank", "authors": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational Linguistics.", "year": 1993}, {"title": "Overview of the 2012 Shared Task on Parsing the Web", "authors": ["Slav Petrov", "Ryan McDonald."], "venue": "Notes of the First Workshop on Syntactic Analysis of NonCanonical Language (SANCL).", "year": 2012}, {"title": "Bioinfer: a corpus for information extraction in the biomedical domain", "authors": ["Sampo Pyysalo", "Filip Ginter", "Juho Heimonen", "Jari Bj\u00f6rne", "Jorma Boberg", "Jouni J\u00e4rvinen", "Tapio Salakoski."], "venue": "BMC bioinformatics.", "year": 2007}, {"title": "Semisupervised dependency parsing using generalized tritraining", "authors": ["Anders S\u00f8gaard", "Christian Rish\u00f8j."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics.", "year": 2010}, {"title": "The syntactic process", "authors": ["Mark Steedman"], "year": 2000}, {"title": "Structured training for neural network transition-based parsing", "authors": ["David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "year": 2015}, {"title": "On-the-job learning with bayesian decision theory", "authors": ["Keenon Werling", "Arun Tejasvi Chaganty", "Percy S Liang", "Christopher D Manning."], "venue": "Advances in Neural Information Processing Systems.", "year": 2015}], "id": "SP:e04b8b0916c52e2e99ce73d669fefdddd40441ef", "authors": [{"name": "Human-in-the-Loop Parsing", "affiliations": []}, {"name": "Luheng He", "affiliations": []}, {"name": "Julian Michael Mike Lewis", "affiliations": []}, {"name": "Luke Zettlemoyer", "affiliations": []}], "abstractText": "This paper demonstrates that it is possible for a parser to improve its performance with a human in the loop, by posing simple questions to non-experts. For example, given the first sentence of this abstract, if the parser is uncertain about the subject of the verb \u201cpose,\u201d it could generate the question What would pose something? with candidate answers this paper and a parser. Any fluent speaker can answer this question, and the correct answer resolves the original uncertainty. We apply the approach to a CCG parser, converting uncertain attachment decisions into natural language questions about the arguments of verbs. Experiments show that crowd workers can answer these questions quickly, accurately and cheaply. Our human-in-the-loop parser improves on the state of the art with less than 2 questions per sentence on average, with a gain of 1.7 F1 on the 10% of sentences whose parses are changed.", "title": "Human-in-the-Loop Parsing"}