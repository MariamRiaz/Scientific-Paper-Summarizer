0	24	Dependency parsers have been enhanced by the use of neural networks and embedding vectors (Chen and Manning, 2014; Weiss et al., 2015; Zhou et al., 2015; Alberti et al., 2015; Andor et al., 2016; Dyer et al., 2015).
1	19	When these dependency parsers process sentences in English and other languages that use symbols for word separations, they can be very accurate.
2	7	However, for languages that do not contain word separation symbols, dependency parsers are used in pipeline processes with word segmentation and POS tagging models, and encounter serious problems because of error propagations.
3	17	In particular, Chinese word segmentation is notoriously difficult because sentences are written without word dividers and Chinese words are not clearly defined.
4	111	Hence, the pipeline of word segmentation, POS tagging and dependency parsing always suffers from word seg- mentation errors.
6	30	As a result, pipeline models achieve dependency scores of around 80% for Chinese.
8	7	Many Chinese words play multiple grammatical roles with only one grammatical form.
10	54	Transition-based joint models for Chinese word segmentation, POS tagging and dependency parsing are proposed by Hatori et al. (2012) and Zhang et al. (2014).
11	16	Hatori et al. (2012) state that dependency information improves the performances of word segmentation and POS tagging, and develop the first transition-based joint word segmentation, POS tagging and dependency parsing model.
13	57	Although the models of Hatori et al. (2012) and Zhang et al. (2014) perform better than pipeline models, they rely on the one-hot representation of characters and words, and do not assume the similarities among characters and words.
14	35	In addition, not only words and characters but also many incomplete tokens appear in the transitionbased joint parsing process.
15	9	Such incomplete or unknown words (UNK) could become important cues for parsing, but they are not listed in dictionaries or pre-trained word embeddings.
16	37	Some recent studies show that character-based embeddings are effective in neural parsing (Ballesteros et al., 2015; Zheng et al., 2015), but their models could not be directly applied to joint models because they use given word segmentations.
17	16	To solve 1204 these problems, we propose neural network-based joint models for word segmentation, POS tagging and dependency parsing.
18	25	We use both character and word embeddings for known tokens and apply character string embeddings for unknown tokens.
19	9	Another problem in the models of Hatori et al. (2012) and Zhang et al. (2014) is that they rely on detailed feature engineering.
20	32	Recently, bidirectional LSTM (bi-LSTM) based neural network models with very few feature extraction are proposed (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016).
23	31	This biLSTM is similar to that of neural machine translation models of Bahdanau et al. (2014).
24	20	As a result, Kiperwasser and Goldberg (2016) achieve competitive scores with the previous state-of-theart models.
25	87	We also develop joint models with ngram character string bi-LSTM.
26	38	In the experiments, we obtain state-of-the-art Chinese word segmentation and POS tagging scores, and the pipeline of the dependency model achieves the better dependency scores than the previous joint models.
27	100	To the best of our knowledge, this is the first model to use embeddings and neural networks for Chinese full joint parsing.
28	26	Our contributions are summarized as follows: (1) we propose the first embedding-based fully joint parsing model, (2) we use character string embeddings for UNK and incomplete tokens.
29	56	(3) we also explore bi-LSTM models to avoid the detailed feature engineering in previous approaches.
30	11	(4) in experiments using Chinese corpus, we achieve state-of-the-art scores in word segmentation, POS tagging and dependency parsing.
31	22	All full joint parsing models we present in this paper use the transition-based algorithm in Section 2.1 and the embeddings of character strings in Section 2.2.
32	49	We present two neural networks: the feed-forward neural network models in Section 2.3 and the bi-LSTM models in Section 2.4.
33	13	Based on Hatori et al. (2012), we use a modified arc-standard algorithm for character transi- 技术有了新的进展。 Technology have made new progress.
34	109	The model consists of one buffer and one stack.
35	15	The buffer contains characters in the input sentence, and the stack contains words shifted from the buffer.
