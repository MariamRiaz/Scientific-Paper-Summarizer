0	27	Neural response generation (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016a; Wen et al., 2015; Yao et al., 2015; Luan et al., 2016; Xu et al., 2016; Wen et al., 2016; Li et al., 2016b; Su et al., 2016) is of growing interest.
1	32	The LSTM sequence-to-sequence (SEQ2SEQ) model (Sutskever et al., 2014) is one type of neural generation model that maximizes the probability of generating a response given the previous dialogue turn.
5	68	One concrete example is that SEQ2SEQ models tend to generate highly generic responses such as “I don’t know” regardless of the input (Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a).
7	42	Yet “I don’t know” is apparently not a good action to take, since it closes the conversation down.
10	43	In example 2 (bottom left), the dialogue falls into an infinite loop after three turns, with both agents generating dull, generic utterances like i don’t know what you are talking about and you don’t know what you are saying.
11	79	Looking at the entire conversation, utterance (4) turns out to be a bad action to take because it offers no way of continuing the conversation.1 1192 These challenges suggest we need a conversation framework that has the ability to (1) integrate developer-defined rewards that better mimic the true goal of chatbot development and (2) model the longterm influence of a generated response in an ongoing dialogue.
13	38	We introduce a neural reinforcement learning (RL) generation method, which can optimize long-term rewards designed by system developers.
38	26	A dialogue can be represented as an alternating sequence of sentences generated by the two agents: p1, q1, p2, q2, ..., pi, qi.
39	64	We view the generated sentences as actions that are taken according to a policy defined by an encoder-decoder recurrent neural network language model.
48	64	The dialogue history is further transformed to a vector representation by feeding the concatenation of pi and qi into an LSTM encoder model as described in Li et al. (2016a).
53	28	In this subsection, we discuss major factors that contribute to the success of a dialogue and describe how approximations to these factors can be operationalized in computable reward functions.
54	28	Ease of answering A turn generated by a machine should be easy to respond to.
58	34	The reward function is given as follows: r1 = − 1 NS ∑ s∈S 1 Ns log pseq2seq(s|a) (1) where NS denotes the cardinality of NS and Ns denotes the number of tokens in the dull response s. Although of course there are more ways to generate dull responses than the list can cover, many of these responses are likely to fall into similar regions in the vector space computed by the model.
62	22	r1 is further scaled by the length of target S. Information Flow We want each agent to contribute new information at each turn to keep the dialogue moving and avoid repetitive sequences.
64	34	Let hpi and hpi+1 denote representations obtained from the encoder for two consecutive turns pi and pi+1.
65	31	The reward is given by the negative log of the cosine similarity between them: r2 = − log cos(hpi , hpi+1) = − log hpi · hpi+1 ‖hpi‖‖hpi+1‖ (2) Semantic Coherence We also need to measure the adequacy of responses to avoid situations in which the generated replies are highly rewarded but are ungrammatical or not coherent.
74	18	For the first stage of training, we build on prior work of predicting a generated target sequence given dialogue history using the supervised SEQ2SEQ model (Vinyals and Le, 2015).
87	47	This mutual information score will be used as a reward and back-propagated to the encoder-decoder model, tailoring it to generate sequences with higher rewards.
89	56	The expected reward for a sequence is given by: J(θ) = E[m(â, [pi, qi])] (5) The gradient is estimated using the likelihood ratio trick: ∇J(θ) = m(â, [pi, qi])∇ log pRL(â|[pi, qi]) (6) We update the parameters in the encoder-decoder model using stochastic gradient descent.
95	19	The simulation proceeds as follows: at the initial step, a message from the training set is fed to the first agent.
118	38	The proposed RL model is first trained based on the mutual information objective and thus benefits from it in addition to the RL model.
127	29	pared against both the vanilla SEQ2SEQ model and the mutual information model.
140	41	The RL system produces responses that are significantly easier to answer than does the mutual information system, as demonstrated by the single-turn ease to answer setting (winning 52 percent of time and losing 23 percent of time), and also significantly higher quality multi-turn dialogues, as demonstrated by the multiturn general quality setting (winning 72 percent of time).
143	36	We also find that the RL model has a tendency to end a sentence with another question and hand the conversation over to the user.
144	80	From Table 1, we observe that the RL model manages to produce more interactive and sustained conversations than the mutual information model.
151	29	Another problem with the current model is that we can only afford to explore a very small number of candidates and simulated turns since the number of cases to consider grow exponentially.
152	85	We introduce a reinforcement learning framework for neural response generation by simulating dialogues between two agents, integrating the strengths of neural SEQ2SEQ systems and reinforcement learning for dialogue.
153	101	Like earlier neural SEQ2SEQ models, our framework captures the compositional models of the meaning of a dialogue turn and generates semantically appropriate responses.
154	158	Like reinforcement learning dialogue systems, our framework is able to generate utterances that optimize future reward, successfully capturing global properties of a good conversation.
155	138	Despite the fact that our model uses very simple, operationable heuristics for capturing these global properties, the framework generates more diverse, interactive responses that foster a more sustained conversation.
