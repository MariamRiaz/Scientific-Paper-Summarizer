3	15	In the absence of enough data they tend to overfit considerably; this restricts them from being applied in scenarios were labeled data are scarce, e.g. in medical applications such as MRI classification.
5	83	A simple example can be seen at Figure 1a; the predictive distribution becomes overly overconfident, i.e. assigns a high softmax probability, towards the wrong class for things it hasn’t seen before (e.g. an MNIST 3 rotated by 90 degrees).
6	20	This in effect makes them unsuitable for applications where decisions are made, e.g. when a doctor determines the disease of a patient based on the output of such a network.
7	22	A principled approach to address both of the aforementioned shortcomings is through a Bayesian inference procedure.
9	20	These distributions capture the parameter uncertainty of the network, and by subsequently integrating over them we can obtain better uncertainties about the predictions of the model.
13	34	Many works have considered the task of approximate Bayesian inference for neural networks using either Markov Chain Monte Carlo (MCMC) with Hamiltonian Dynamics (Neal, 1995), distilling SGD with Langevin Dynamics (Welling & Teh, 2011; Korattikara et al., 2015) or deterministic techniques such as the Laplace Approximation (MacKay, 1992), Expectation Propagation (Hernández-Lobato & Adams, 2015; HernándezLobato et al., 2015) and variational inference (Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016).
22	60	Assuming that p(Wi), qφ(Wi) are the prior and approximate posterior over the parameters of the i’th layer we can derive the following lower bound on the marginal log-likelihood of the dataset D using variational Bayes (Peterson, 1987; Hinton & Van Camp, 1993; Graves, 2011; Blundell et al., 2015; Kingma et al., 2015; Gal & Ghahramani, 2015b; Louizos & Welling, 2016): L(φ) = Eqφ(W1:L) [ log p(y|x,W1:L)+ + log p(W1:L)− log qφ(W1:L) ] , (1) where p̃(x,y) denotes the training data distribution and φ the parameters of the variational posterior.
26	17	Despite the fact that this leads to a straightforward lower bound for optimization, the approximation capability is quite limiting; it corresponds to just a unimodal “bump” on the very high dimensional space of the parameters of the neural network.
28	30	Nevertheless, both of the aforementioned methods are still, in a sense, limited; the true parameter posterior is more complex than delta peaks or correlated Gaussians.
30	14	Briefly, a normalizing flow is constructed by introducing parametrized bijective transformations, with easy to compute Jacobians, to random variables with simple initial densities.
32	48	Auxiliary random variables instead construct more flexible distributions by introducing latent variables in the posterior itself, thus defining the approximate posterior as a mixture of simple distributions.
35	26	Furthermore, by utilizing this procedure we also lose the benefits of local reparametrizations (Kingma et al., 2015; Louizos & Welling, 2016) which are possible with Gaussian approximate posteriors.
36	14	In order to simultaneously maintain the benefits of local reparametrizations and increase the flexibility of the approximate posteriors in a Bayesian neural network we will rely on auxiliary random variables (Agakov & Barber, 2004; Salimans et al., 2013; 2015; Ranganath et al., 2015; Maaløe et al., 2016); more specifically we will exploit the well known “multiplicative noise” concept, e.g. as in (Gaussian) Dropout (Srivastava et al., 2014), in neural networks and we will parametrize the approximate posterior with the following process: z ∼ qφ(z); W ∼ qφ(W|z), (3) where now the approximate posterior becomes a compound distribution, q(W) = ∫ q(W|z)q(z)dz, with z being a vector of random variables distributed according to the mixing density q(z).
37	47	To allow for local reparametrizations we will parametrize the conditional distribution for the weights to be a fully factorized Gaussian.
38	18	Therefore we assume the following form for the fully connected layers: qφ(W|z) = Din∏ i=1 Dout∏ j=1 N (ziµij , σ2ij), (4) where Din, Dout is the input and output dimensionality, and the following form for the kernels in convolutional networks: qφ(W|z) = Dh∏ i=1 Dw∏ j=1 Df∏ k=1 N (zkµijk, σ2ijk), (5) where Dh, Dw, Df are the height, width and number of filters for each kernel.
39	31	Note that we did not let z affect the variance of the Gaussian approximation; in a pilot study we found that this parametrization was prone to local optima due to large variance gradients, an effect also observed with the multiplicative parametrization of the Gaussian posterior (Kingma et al., 2015; Molchanov et al., 2017).
63	17	9,10,11,12, so as to avoid sampling the, potentially big, matrix W. With the standard normal prior and the fully factorized Gaussian posterior of eq.
90	30	Unless explicitly mentioned otherwise we use flows of length two for q(z) and r(z|W) with 50 hidden units for each step of the flow of q(z) and 100 hidden units for each step of the flow of r(z|W).
91	29	We used 100 posterior samples to estimate the predictive distribution for all of the models during testing and 1 posterior sample during training.
92	83	MNIST We trained on MNIST LeNet architectures using the priors and posteriors described at Table 1.
95	18	The classification performance of each model can be seen at Table 2; while our overall focus is not classification accuracy per se, we see that with the MNF posteriors we improve upon mean field reaching similar accuracies with Deep Ensembles.
96	18	notMNIST To evaluate the predictive uncertainties of each model we performed the task described at (Lakshminarayanan et al., 2016); we estimated the entropy of the predictive distributions on notMNIST5 from the LeNet architectures trained on MNIST.
122	17	Furthermore, Deep Ensembles and Dropout had similar uncertainties, with Deep Ensembles having lower accuracy on the observed classes.
128	16	MNIST On this scenario we observe interesting results if we plot the change in accuracy and entropy by varying the magnitude of the adversarial perturbation.
131	18	This is in contrast with MNFs; while the accuracy almost immediately drops close to random, the uncertainty simultaneously increases to almost maximum entropy.
147	45	This might suggest that Dropout uncertainty is probably not a good posterior approximation since by optimizing the dropout rates we do not seem to move closer to the true posterior predictive distribution.
157	20	Another promising direction is that of designing better priors for Bayesian neural networks.
158	15	For example (Neal, 1995) has identified limitations of Gaussian priors and proposes alternative priors such as the Cauchy.
159	122	Furthermore, the prior over the parameters also affects the type of uncertainty we get in our predictions; for instance we observed in our experiments a significant difference in uncertainty between Gaussian and log-uniform priors.
160	38	Since different problems require different types of uncertainty it makes sense to choose the prior accordingly, e.g. use an informative prior so as to alleviate adversarial examples.
