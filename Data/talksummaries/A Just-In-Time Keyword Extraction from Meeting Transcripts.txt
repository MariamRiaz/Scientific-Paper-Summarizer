16	27	The preceding utterances are also expressed as a history graph in which the weight of an edge is the temporal importance of the keywords connected by the edge.
17	5	To reflect the temporal history of utterances, forgetting curve (Wozniak, 1999) is adopted in updating the weights of edges in the history graph.
18	130	It expresses effectively not only the reciprocal relation between memory re- 888 tention and time, but also active recall that makes frequent words more consequential in keyword extraction.
21	28	The proposed method is evaluated with two kinds of data sets: the National Assembly transcripts in Korean and the ICSI meeting corpus (Janin et al., 2003) in English.
22	33	The experimental results show that it outperforms both the TFIDF framework (Frank et al., 1999; Liu et al., 2009) and the PageRank-based graph model (Wan et al., 2007).
23	63	One thing to note is that the proposed method improves even the supervised methods that do not reflect utterance time and topic relevance for the ICSI corpus.
24	34	This proves that it is critical to consider time and content of utterances simultaneously in keyword extraction from meeting transcripts.
25	27	The rest of the paper is organized as follows.
26	12	Section 2 reviews the related studies on keyword extraction.
29	10	Finally, Section 6 draws some conclusions.
53	6	We represent all the components in a meeting as graphs.
54	24	This is because graphs are effective to express the relationship between words, and the graph operations that are required for keyword extraction are also efficiently performed.
55	7	That is, whenever an utterance is spoken, it is represented as a graph (G1) of which nodes are the potential keywords in the utterance.
57	10	The summary of all preceding utterances is also represented as a history graph (G2).
60	12	This subgraph is labeled as G3 in Figure 1.
63	17	The keywords are so-called hub nodes of G4.
64	18	After keywords are extracted from the current utterance, the current utterance becomes a part of the history graph for the next utterance.
65	98	For this, the extracted keywords are also represented as a graph (G5), and it is merged into the current history G2.
68	29	If an edge is connecting two nouns from an old utterance, its weight becomes small.
69	30	In the same way, the weights for the edges from recent utterances get large.
70	6	The weights of the edges from G5 are 1, the largest possible value.
71	17	Current utterance graph is a graph-representation of the current utterance.
72	13	When current utterance consists of m words, we first extract the potential key- words from the current utterance.
73	2	Since all words within the current utterance are not keywords, some words are filtered out.
75	11	This approach filters out non-keywords using a stop-word list and POS tags of the words.
79	20	(1) In a meeting, preceding utterances affect the current utterance.
80	16	We assume that only the keywords of preceding utterances are effective.
