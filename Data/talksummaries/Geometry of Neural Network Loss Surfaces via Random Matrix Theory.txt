0	34	Neural networks have witnessed a resurgence in recent years, with a smorgasbord of architectures and configurations designed to accomplish ever more impressive tasks.
3	23	Nevertheless, an improved understanding of the loss surface could have a large impact on optimization (Saxe et al.; Dauphin et al., 2014; Choromanska et al., 2015; Neyshabur et al., 2015), architecture design, and generalization (Keskar et al.).
10	14	One shortcoming of prior theoretical results is that they are often derived in contexts far removed from practical neural network settings – for example, some work relies on results for generic random landscapes unrelated to neural networks, and other work draws on rather tenuous connections to spin-glass models.
11	33	While there is a lot to be gained from this type of analysis, it leaves open the possibility that characteristics of loss surfaces specific to neural networks may be lost in the more general setting.
12	29	In this paper, we focus narrowly on the setting of neural network loss surfaces and propose an analytical framework for studying the spectral density of the Hessian matrix in this context.
16	14	After establishing our methodology, we compute approximations to the Hessian at several levels of refinement.
17	28	One result is a prediction that for critical points of small index, the index scales like the energy to the 3/2 power.
21	57	The network output is ŷiµ = n1∑ k=1 W (2) ik [z (1) kµ ]+ , (1) and the residuals are eiµ = ŷiµ−yiµ.
26	33	Specifically, [H0]αβ ≡ 1 m n2,m∑ i,µ=1 ∂ŷiµ ∂θα ∂ŷiµ ∂θβ ≡ 1 m [JJT ]αβ , (3) where we have introduced the Jacobian matrix, J , and, [H1]αβ ≡ 1 m n2,m∑ i,µ=1 eiµ ( ∂2ŷiµ ∂θα∂θβ ) .
42	12	For a more thorough introduction, see, e.g., (Tao, 2012).
44	15	The degree of independence and the manner in which the Mij are distributed determine the type of random matrix ensemble to which M belongs.
59	16	Intuitively, the eigenspaces of two freely independent matrices are in “generic position” (Speicher, 2009), i.e. they are not aligned in any special way.
62	13	Given the Stieltjes transform G of a probability distribution ρ, the R transform is defined as the solution to the functional equation, R ( G(z) ) + 1 G(z) = z .
63	15	(13) The benefit of theR transform is that it linearizes free convolution, in the sense that, RA+B = RA +RB , (14) if A and B are freely independent.
64	35	It plays a role in free probability analogous to that of the log of the Fourier transform in commutative probability theory.
65	55	The prescription for computing the spectrum of A + B is as follows: 1) Compute the Stieltjes transforms of ρA and ρB ; 2) From the Stieltjes transforms, deduce the R transforms RA and RB ; 3) From RA+B = RA + RB , compute the Stieltjes transform GA+B ; and 4) Invert the Stieltjes transform to obtain ρA+B .
66	27	Having established some basic tools from random matrix theory, let us now turn to applying them to computing the limiting spectral density of the Hessian of a neural network at critical points.
67	17	Recall from above that we can decompose the Hessian into two parts, H = H0 + H1 and that H0 = JJ T /m.
68	51	Let us make the secondary assumption that at critical points, the elements of J and H1 are i.i.d.
74	12	(13) for theR transforms then gives, RH0(z) = 1 1− zφ , RH1(z) = 2 z .
75	16	(16) We proceed by computing theR transform of H , RH = RH0 +RH1 = 1 1− zφ + 2 z , (17) so that, using eqn.
78	10	From this root, the spectral density can be derived through the Stieltjes inversion formula, eqn.
93	11	(20) is the same behavior that was found in (Bray & Dean, 2007) in the context of the field theory of Gaussian random functions.
105	29	Generically we may expect some alignment between the eigenspaces of H0 and H1 so that free independence is violated; however, we find that this violation is often quite small in practice.
106	10	To perform this analysis, it suffices to examine the discrepancy between the distribution of H0 +H1 and that of H0 +QH1QT , where Q is an orthogonal random matrix of Haar measure (Chen et al., 2016).
109	16	First, we note that eiµ ∼ N (0, 2 ) is consistent with the definition of the energy in eqn.
110	18	Furthermore, because the gradient of the loss is proportional to the residuals, it vanishes in expectation (i.e. asm→∞), which specializes our analysis to critical points.
113	10	Altogether we believe this assumption is fairly mild.
120	18	In fact, if this were the case, it would go a long way to validate the approximations made in sec.
123	33	We examine one way of doing so in the next section.
