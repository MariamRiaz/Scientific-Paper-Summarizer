{"sections": [{"heading": "1. Introduction", "text": "Binary classification, with the goal of predicting a binary response given input features, is perhaps the classical problem in machine learning, with wide ranging applications. A key ingredient in binary classification is a performance measure, that quantifies how well a given classifier fits the data. While the performance measure of accuracy has been the predominant focus of both theory and practice, it has severe limitations in many practical settings, such as imbal-\n1University of Texas at Austin, Austin, Texas, USA 2University of Illinois at Urbana-Champaign, Champaign, Illinois, USA 3Carnegie Mellon University, Pittsburgh, Pennsylvania, USA. Correspondence to: Bowei Yan <boweiy@utexas.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nanced classes, and where different types of errors made by the classifier have different costs (Gu et al., 2009; Wallace et al., 2011). Accordingly, practitioners in applied machine learning settings such as information retrieval and medical diagnosis have developed complex performance metrics that capture important trade-offs between different types of errors; we have collated a few instances in Table 1. A key complication with many complex classification performance metrics, such as the F-measure (Manning et al., 2008) and Harmonic Mean (Kennedy et al., 2009), is that they cannot be decomposed into the sum or average of individual losses on each sample. Even the simple performance measure of precision \u2014 the fraction of correct positive predictions, among the set of positive predictions \u2014 is not a sum of individual losses on each sample. Thus, the standard theoretical and practical treatments of supervised learning, such as standard empirical risk minimization that minimizes the empirical expectation of a loss evaluated on a single random example, are not applicable.\nThis practical reality has motivated research into effective and efficient algorithms tailored to complex nondecomposable performance measures. One class of approaches extend standard empirical risk minimization to this non-decomposable setting, which often relies on strong assumptions on either the form of the classifiers, such as requiring linear classifiers (Narasimhan et al., 2015a), or restricted to specific performance measures such as F-measure (Parambath et al., 2015). An alternative approach is the plug-in estimator, where we first derive the form of the Bayes optimal classifier, estimate the statistical quantities associated with the Bayes optimal classifier, and finally \u201cplug-in\u201d the sample estimates of the population quantities to then obtain the overall estimate of the Bayes optimal classifier. In particular, for many complex performance metrics, the Bayes optimal classifier is simply a thresholding of the conditional probability of the positive class (Koyejo et al., 2014; Narasimhan et al., 2014), so that the plug-in estimator requires (a) an estimate of the conditional probability, and (b) the associated threshold. Plug-in methods have been of particular interest in non-parametric functional estimation as they typically require weaker assumptions on the function class and are often easy to implement.\nIn this paper, we seek to advance our understanding and practice of binary classification under complex non-\ndecomposable performance measures. We show that for a very broad class of performance measures, encompassing a large set of performance measures used in practice, the Bayes optimal classifier is simply a thresholding of the conditional probability of the response. Towards this general result, we identify two key properties that a performance measure could satisfy. The first is what we call a \u201cKarmic\u201d property that loosely has the performance measure be more sensitive to an increase in true positives and true negatives, and a decrease in false positives and false negatives. The second is a more technical property we call threshold-quasiconcavity, which in turn ensures the performance measure is well-behaved around an optimal threshold. As we show these properties are satisfied by performance metrics used in practice, and in particular, these conditions are milder than existing results that restrict either the structural form of the performance measures, or impose strong shape constraints such as particular monotonicities.\nOur general result has two main consequences, which we investigate further: one algorithmic, and the other for the analysis of classification error for general performance measures. As the algorithmic consequence, we leverage the derived form of the Bayes optimal classifier, and some additional general assumptions on the performance measures, to provide a tractable algorithm to estimate the threshold, which coupled with an estimator of the conditional probability, provides a tractable \u201cplug-in estimator\u201d of the Bayes optimal classifier. Towards the statistical analysis consequence, we provide an analysis of the excess classification error, but with respect to general non-decomposable performance measures, of the general class of plugin-estimators for our class of Bayes optimal classifiers. Our analysis of classification error rates for such plug-in classifiers depend on three natural quantities: the rate of convergence for the conditional probability estimate, the rate of convergence for the threshold estimate, and a measurement of noise in the data. For the last part, we extend margin or low-noise assumptions for binary classification with the accuracy performance measure to complex performance measures. Low noise assumptions, proposed by Mammen et al. (1999) in the context of the accuracy performance measure, bounds the noise level in the neighborhood of the Bayes optimal threshold i.e. 12 for standard classification. Under such a low-noise assumption, Audibert et al. (2007) derive fast convergence rates for plug-in classification rules based on the smoothness of the conditional probability function. Similar margin assumptions have also been introduced for density level set estimation by Polonik (1995). We provide a natural extension of such a low-noise assumption, under which we provide explicit rates of convergence of classification error with respect to complex performance measures. We provide corollaries for both parametric and non-parametric instances of our general class of plugin-classifiers.\nThe rest of the paper is organized as below. In Section 2 we introduce the problem and relevant notations. The characterization and properties of Bayes optimal classifier are derived in Section 3. We discuss the algorithm for estimating the plug-in estimator in Section 4, and present the statistical convergence guarantee in Section 5. Applications of the derived rate for two special cases, Gaussian generative model and \u03b2-Ho\u0308lder class conditional probability are presented in Section 6 where explicit convergence rates are provided. We conclude the paper in Section 7. Detailed proofs are deferred to the supplementary materials."}, {"heading": "2. Problem Setup and Preliminaries", "text": "Binary classification entails predicting a binary label Y \u2208 {\u00b11} associated with a feature vector X \u2208 X \u2282 Rd. Such a a function mapping f : X 7\u2192 {\u00b11} from the feature space X to the labels {\u00b11} is called a binary classifier. Let \u0398 = {f : X \u2192 {\u00b11}} denote a set of binary classifiers. We assume (X,Y ) has distribution P \u2208 P , and let \u03b7(x) := P(Y = 1|X = x) denote the conditional probability of the label Y given feature vector x.\nA key quantity is the confusion matrix, that consists of four population quantities: true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). Definition 2.1 (Confusion Matrix). For any classifier f : Rd 7\u2192 {\u00b11}, its confusion matrix is defined as C(f,P) := [TP(f,P),FP(f,P),FN(f,P),TN(f,P)] \u2208 [0, 1]4, where:\nTP(f,P) = P(Y = +1, f(X) = +1), FP(f,P) = P(Y = \u22121, f(X) = +1), FN(f,P) = P(Y = +1, f(X) = \u22121), TN(f,P) = P(Y = \u22121, f(X) = \u22121).\n(1)\nAnother key ingredient is the utility or performance measure U : \u0398\u00d7P \u2192 R, that measures the performance of a classifier. In this paper, we focus on complex binary classification performance measures that can be expressed as a function of the confusion matrix. Formally, U(f,P) = G(C(f,P)). When it is clear from context, we will drop the dependency of the distribution P in C and U . The confusion-matrix functions G corresponding to popular performance measures are listed in Table 1. Given the performance measure U , we are interested in the corresponding Bayes optimal classifier:\nf\u2217 = arg max f\u2208\u0398 U(f,P). (2)\nGiven any candidate classifier f , we are then interested in the excess risk U(f\u2217,P)\u2212 U(f,P), which is the utility gap between a given classifier f and the corresponding Bayes optimal. Assumption 1 (Karmic Performance Measure). The confusion-matrix function G corresponding to the performance measure U is Lipschitz continuous, and satisfies the\nTable 1. Examples of evaluation metrics. Notation: TPR = TPTP+FN ;TNR = TN TN+FP .\nMETRIC DEFINITION REFERENCE G(C)\nACCURACY TP + TN (1, 0, 0, 1)C ARITHMETIC MEAN (AM) (TPR + TNR)/2 MENON ET AL. (2013) ( C1\nC1+C3 + C4 C2+C4 )/2\nYOUDEN\u2019S INDEX TPR + TNR \u2212 1 YOUDEN (1950) C1 C1+C3 + C4 C2+C4 \u2212 1 F\u03b2 (1+\u03b22)TP (1+\u03b22)TP+\u03b22FN+FP VAN RIJSBERGEN (1974) (1+\u03b22,0,0,0)C (1+\u03b22,1,\u03b22)C LINEAR-FRACTIONAL a1TP+a2FP+a3FN+a4TN b1TP+b2FP+b3FN+b4TN KOYEJO ET AL. (2014) a TC bTC G-MEAN \u221a TPR \u00b7 TNR DASKALAKI ET AL. (2006) \u221a\nC1C4 (C1+C3)(C2+C4) Q-MEAN 1\u2212 \u221a\n(1\u2212TPR)2+(1\u2212TNR)2 2 KUBAT ET AL. (1997) 1\u2212\n\u221a (\nC3 C1+C3 )2+( C2 C2+C4 )2\n2\nH-MEAN 2 1/TPR+1/TNR KENNEDY ET AL. (2009) 2\n/( C1+C3\nC1 + C2+C4 C4 ) condition that \u2207G(C)T (1,\u22121,\u22121, 1)T \u2265 CB , for some constant CB > 0.\nWe term performance measures that satisfy the condition \u2207G(C)T (1,\u22121,\u22121, 1)T \u2265 CB as \u201cKarmic measures\u201d, since it guarantees a lower bound on the sensitivity of the performance measure in the direction of increasing true positives and true negatives, and decreasing false positives and false negatives. While our Karmic assumption slightly weakens the existing monotonicity assumption used in literature, it is worth pointing out that the analysis in (Narasimhan et al., 2014) requires not only monotonicity but also additional assumptions (Assumption B in (Narasimhan et al., 2014)). Assumption B assumes the existence and uniqueness of an optimal threshold, which turns out to be non-trivial to check. Our analysis on the threshold-quasi-concavity closes this gap.\nAssumption 1 is satisfied if G is strictly monotonically increasing with respect to TP,TN and decreasing with respect to FP,FN. Such an assumption is natural in that one would typically prefer a classifier with higher TP for fixed TN and vice versa (Narasimhan et al., 2015b). This condition is satisfied by most metrics in common use e.g. for the F1 measure, \u2207G(C)T (1,\u22121,\u22121, 1)T = 4(FP+FN)(2TP+FP+FN)2 is strictly positive as long as the data is not fully separable."}, {"heading": "2.1. Related Work", "text": "Representation of the Bayes Optimal Classifier. The Bayes optimal classifier under the accuracy metric is classically known to be a thresholding of the conditional probability of the response, with the threshold of 1/2 (see e.g. Devroye et al. (2013)). This property of Bayes optimal classifier having the thresholded form is called the probability thresholding principle for binary classification by Lewis (1995). Prior work has also shown that the thresholding principle, with a metric dependent threshold, for more complex specific measures such as F-measure (Jansche, 2007;\nZhao et al., 2013), Arithmetic Mean (AM) (Menon et al., 2013), linear-fractional performance metrics (Koyejo et al., 2014), and monotonic concave metrics (Narasimhan et al., 2015a).\nPlug-in Classifiers for Complex Metrics. For Bayes optimal classifiers that have thresholded form, a line of work has devised plug-in classifiers that then estimate the threshold, and the conditional probability of response. For the AM metric, Menon et al. (2013) show that the threshold is simply the proportion of the positive class. For linear fractional functions, Koyejo et al. (2014) provide an implicit characterization of the optimal threshold, but the solution of which in turn requires the knowledge of the optimal classifier, which is unknown in practice. As a practical estimator, Koyejo et al. (2014) propose an exhaustive search for the threshold over all data samples, and show that the resulting algorithm is consistent, but for which non-asymptotic convergence rates are not known. Narasimhan et al. (2014) also note the importance of estimating the optimal threshold, but do not provide practical algorithms. As we show in Section 3, the empirical risk as a function of the threshold is in general neither convex nor concave. Hence, care must be taken to construct an optimization algorithm that guarantees convergence to the true threshold.\nEstimators designed for specific Utility Functions. Perhaps the most studied non-decomposable performance metric is the F-measure (Nan et al., 2012; Joachims, 2005; Zhao et al., 2013), with wide use in information retrieval and related areas, and for which researchers have developed tailored estimators (Nan et al., 2012; Joachims, 2005) as well as risk bounds for these estimators (Zhao et al., 2013). For instance, Busa-Fekete et al. (2015) propose a scalable online F-measure estimator for large-scale datasets, with a root finding algorithm for the threshold update which exploits special properties of the F-measure. Similarly, for the\nArithmetic Mean (AM) measure, Menon et al. (2013) design a consistent optimization scheme, based on a balanced classification-calibrated surrogate to AM. Unfortunately, these techniques are not easily extended to general complex performance metrics.\nAlgorithms for General Classification Measures. Joachims (2005) poses the classification problem as a structured prediction problem, and for linear classifiers, propose a structural SVM solver, but for which neither consistency nor explicit convergence rates are known. Kar et al. (2014) proposes an online gradient descent algorithm which requires function classes that satisfy a uniform convergence property, which is difficult to verify apriori. Along similar lines, Narasimhan et al. (2015a) propose a stochastic gradient method, that involves a linearization of classification metric. Their proposed approach depends strongly on the assumption of a linear (or kernelized) classifier, and it is not obvious that the procedure can be extended to more complex non-linear function classes."}, {"heading": "3. The Bayes Optimal Classifier Revisited", "text": "In this section, we characterize the Bayes-optimal classifier for the broad class of Karmic performance measures, that satisfy Assumption 1. We then show that with one additional assumption, we call threshold-quasi-concavity, the optimal threshold can be guaranteed to be unique. This result will be crucial for the design and analysis of our computationally efficient threshold finding procedure in Section 4.\nDenote \u00b5 as the measure corresponding to the marginal distribution of X . The utility is Freche\u0301t differentiable, whose Freche\u0301t derivative of U may be computed as:\n[\u2207U(f)]x =\u2207G(C(f))T \u00b7 [\u2207C(f)]x\n= 1\n2\n( \u2207G(C)T (1,\u22121,\u22121, 1)T \u03b7(x)\n\u2212\u2207G(C)T (0,\u22121, 0, 1)T ) d\u00b5(x)\nFrom the Karmic measure Assumption 1, we know that \u2207G(C)T (1,\u22121,\u22121, 1)T > 0. We define the \u201cBayes critical set\u201d of G(f,P) for any f \u2208 F as the set of instances where the utility has zero derivative:\nA3(f) =\n{ x : \u03b7(x) = \u2207G(C)T (0,\u22121, 0, 1)T\n\u2207G(C)T (1,\u22121,\u22121, 1)T\n} .\nFor notational simplicity, we have omitted the dependency of gi on C(f). Similarly, we will use A\u22173 := A3(f\n\u2217) to denote the Bayes critical set.\nIn this paper we focus on distributions where the critical set of G(f, P ) satisfies P(A3(f)) = 0. For instance, this is true for any distribution that satisfies the following assumption.\nAssumption 2 (\u03b7-continuity). Let \u03bd denote the probability measure that is associated with random variable Z = \u03b7(X) = P (Y = 1|X), then \u03bd is absolutely continuous with respect to \u00b5. Furthermore, the density of \u03b7(X), denoted by p\u03b7(\u00b7), has full support on [0, 1], and is bounded everywhere.\nAbsolute-continuity guarantees the existence of the density of Z. Armed with the above assumption on the conditional probability of the response, we can then characterize the Bayes optimal classifier as follows.\nTheorem 3.1 (Bayes Optimal Classifier as a Thresholding Function). Suppose that U is a performance measure that satisfies Assumption 1, and that \u03b7(X) satisfies Assumption 2. Let f\u2217 be the Bayes classifier with respect to U and C\u2217 be its confusion matrix. Then, for all x \u2208 (A\u22173)c,\nf\u2217(x) = sign ( \u03b7(x)\u2212 \u2207G(C \u2217)T (0,\u22121, 0, 1)T\n\u2207G(C\u2217)T (1,\u22121,\u22121, 1)T\n) .\n(3)\nThreshold of Bayes optimal classifier For some performance measures, the optimal threshold reduces to an absolute constant; for instance it has the value of 1/2 for the accuracy measure U(f,P) = TP + TN (see e.g. (Devroye et al., 2013)). In the general case however, the optimal threshold \u03b4\u2217 is a solution of the fixed point equation:\n(\u2207G(C\u2217)T (0,\u22121, 0, 1)T )/(\u2207G(C\u2217)T (1,\u22121,\u22121, 1)T ) = \u03b4\u2217,\nwhich is fixed point equation due to the dependency of C\u2217\non the threshold \u03b4\u2217. Theorem 3.1 guarantees the existence of a solution to the above fixed point equation, but not its uniqueness. As we will show in Section 5, uniqueness can be achieved with some additional regularity assumptions.\nWe note that Theorem 3.1 only imposes a weak Karmic assumption on the performance measure, which as as stated in Section 2, is more general than even a simple strictly monotonicity assumption. In particular, it generalizes prior work such as (Koyejo et al., 2014; Menon et al., 2013), that impose more stringent assumptions (linear or linear fractional form of the measures, or strong monotonicity conditions).\nWe next briefly discuss why the critical set is crucial. Consider for instance the example studied in Narasimhan et al. (2014): with domain X = {x1, x2, x3}, a corresponding probability mass function (0.25, 0.5, 0.25), and the conditional probability \u03b7 = (0.49, 0.5, 0.51). Narasimhan et al. (2014) show that for this setting, and for the case of the Hmean measure, there exist at least two deterministic Bayes optima: (\u22121, 1,\u22121) and (1,\u22121, 1)}, which can be seen to not have a thresholded form i.e. it cannot be expressed as a (signed) thresholding of the conditional probability. Our analysis reveals why this is the case.\nFrom the threshold expression in (3) from Theorem 3.1, the optimal threshold can be computed explicitly as \u2207G(C\u2217)T (0,\u22121,0,1)T \u2207G(C\u2217)T (1,\u22121,\u22121,1)T = 1 2 . Thus, the Bayes critical set A\u22173 = {x : \u03b7(x) = 12} = {x2} has measure P (X \u2208 A3) = P (X = x2) = 1 2 > 0. It is clear that the Bayes optimal classifier may not take a thresholded form on the Bayes critical set."}, {"heading": "3.1. Uniqueness of the Bayes Optimal Threshold.", "text": "We are interested in characterizing mild conditions on the performance measure under which the fixed point equation characterizing the Bayes optimal threshold has a unique solution, under which case P (A\u22173) = 0 (guaranteed by the \u03b7-continuity Assumption 2).\nThe performance measure restricted to classifiers that are threshold functions of the conditional probability, can be rewritten as a function of the conditional probability \u03b7 and the threshold \u03b4.\nDefinition 3.1. We define V\u03b7(\u03b4,P) := U(f\u03b7,\u03b4,P) as the performance measure of any threshold classifier f\u03b7,\u03b4(x) = sign (\u03b7(x)\u2212 \u03b4). Its arguments are the threshold \u03b4, and distribution P, while the subscript \u03b7 notes its dependence on the conditional probability \u03b7.\nWe next introduce the definition of quasi-concavity, and the assumption of V being strictly quasi-concave. Definition 3.2. A function f : X \u2192 R is said to be quasiconcave if \u2200x, y \u2208 X , such that f(x) \u2264 f(y), it follows that \u3008\u2207f(x), y \u2212 x\u3009 \u2265 0. We further say that f is strictly quasiconcave if it is quasi-concave and its gradient only vanishes at the global optimum, i.e., f(y) < maxx\u2208X f(x)\u21d2 \u2016\u2207f(y)\u2016 > 0. Quasi-concave functions have super level sets are convex sets, and moreover by definition are unimodal i.e. have a unique maximal point.\nAssumption 3. (Threshold-Quasi-Concavity) The threshold-classifier performance measure V\u03b7(\u03b4,P) is strictly quasi-concave for \u03b4 \u2208 [0, 1].\nAssumption 3 seems abstract, but it entails that the performance measure is well-behaved as a function of the threshold. Moreover, it can be easily shown to hold for performance measures in practical use. We provide a proposition that shows that the assumption is satisfied for two important classes of performance measures: linear-fractional functions and concave functions.\nProposition 3.1. If Assumptions 1, 2 hold, and either: (a) G is twice continuously differentiable and concave, or (b) G is a linear fractional function G(C) = a\nTC bTC with |bTC| > 0. Then V\u03b7(\u03b4,P) is strictly quasi-concave. Theorem 3.2. Under Assumption 3, the fixed-point equa-\ntion:\n\u03b4 = \u2207G(C(f\u03b4),P)T (0,\u22121, 0, 1)T\n\u2207G(C(f\u03b4),P)T (1,\u22121,\u22121, 1)T , (4)\nwhere f\u03b4(x) = sign (\u03b7(x)\u2212 \u03b4), has a unique fixed point \u03b4\u2217 \u2208 (0, 1). Hence the threshold in Theorem 3.1 is uniquely defined.\nTheorems 3.1 and 3.2 have two key consequences: first, we can use the representation to design plugin-estimators of the Bayes optimal classifier; second, it facilitates the statistical analysis for rates of convergence. We will discuss each of these two consequences in the following sections."}, {"heading": "4. Algorithmic Consequence: Estimation of the Threshold", "text": "Theorem 3.1 shows that for Karmic performance measures, the Bayes optimal classifiers has the thresholded form as in Eq. (3), and moreover under the threshold-quasi-concavity Assumption 3, this threshold is unique. An immediate algorithmic consequence of this is to focus on plug-in classifiers that separately estimate the conditional probability, and the threshold. We present this plugin-classifier template in Algorithm 1. The template needs: (a) an estimator for conditional probability density \u03b7(x), and (b) an estimator for the threshold. For the convenience of analysis, we divide the set of samples into two independent subsets: the conditional probability estimator is estimated using one subset, and the threshold is estimated using the other. In the coming subsec-\nAlgorithm 1 Two-step Plug-in Classifier for General Metrics\n1: Input: Training sample {Xi, Yi}ni=1, utility measure U , conditional probability estimator \u03b7\u0302, stepsize \u03b1. 2: Randomly split the training sample into two subsets {X(1)i , Y (1) i } n1 i=1 and {X (2) i , Y (2) i } n2 i=1; 3: Estimate \u03b7\u0302 on {X(1)i , Y (1) i } n1 i=1; 4: Estimate \u03b4\u0302 with {X(2)i , Y (2) i };\n5: Output: f\u0302(x) = sign ( \u03b7\u0302 \u2212 \u03b4\u0302 ) .\ntions we discuss how to estimate the conditional probability and the threshold respectively."}, {"heading": "4.1. Estimation of Conditional Probability Function", "text": "The estimation of the conditional probability of the response plays a crucial role in the success of Algorithm 1, but we emphasize that it is not the focus of our paper. In particular, this is a well studied problem, and numerous methods have been proposed for both parametric and non-parametric model assumptions on the conditional probability function.\nIn this section we briefly discuss some common estimators, and defer additional details to Section 6.\nParametric methods. In a classical paper, Ng and Jordan (2002) compares two models of classification: one can either estimate P (Y ) and P (X|Y ) first, then get the conditional probability by Bayes rule (generative model approach); or directly estimate P (Y |X) (discriminative model approach). The two approaches can also be related. In particular, if P\u03b8Y (X|Y ) belongs to exponential family, we have\nP\u03b8Y (X|Y ) = h(x) exp (\u3008\u03b8Y , \u03c6(X)\u3009 \u2212A(\u03b8Y )) ,\nwhere \u03c6(X) is the set of sufficient statistics, \u03b8Y is the vector of the true canonical parameters, and A(\u03b8) is the logpartition function. Using Bayes rule, we then have:\nP (Y = 1|X) = 1 1 + exp (\u2212\u3008\u03b81 \u2212 \u03b80, \u03c6(X)\u3009+ c\u2217)\nwhere c\u2217 = A(\u03b80) \u2212 A(\u03b81). The conditional distribution can be seen to follow a logistic regression model, with the generative model sufficient statistics as the features, and the difference of the generative model parameters serving as the parameters of the discriminative model. A natural class of estimators for either the generative or discriminative models is based on Maximum likelihood Estimation (MLE). In Section 6, we derive the rate of convergence for the special case where the generative distributions are Gaussians with same covariances for both classes.\nNon-parametric methods. One can also estimate \u03b7(x) = P (Y = 1|X) non-parametrically, where a common model assumption is some form of smoothness on \u03b7(x). One popular class of smooth functions is the following.\nDefinition 4.1 (\u03b2-Ho\u0308lder class). Let \u03b2 > 0, denote b\u03b2c the maximal integer that is strictly less than \u03b2. For x \u2208 X and any b\u03b2c-times continuously differentiable real-valued function \u03b7 on X , we denote by \u03b7x its Taylor polynomial of degree b\u03b2c at point x,\n\u03b7x(x \u2032) = \u2211 s\u2264b\u03b2c (x\u2032 \u2212 x)s s! Ds\u03b7(x).\n\u03b2-Ho\u0308lder class is defined as the functions that satisfy, for \u2200x, x\u2032 \u2208 X ,\n|\u03b7x(x)\u2212 \u03b7x(x\u2032)| \u2264 C\u03b2\u2016x\u2212 x\u2032\u2016\u03b2 .\nIn particular, when 0 \u2264 \u03b2 < 1, we have |\u03b7(x)\u2212 \u03b7(x\u2032)| \u2264 C\u03b2\u2016x\u2212 x\u2032\u2016\u03b2 where \u03b2 > 0.\nWe can then estimate \u03b7(x) from this family of smooth functions via locally polynomial estimators (Audibert et al., 2007), or kernel (conditional) density estimators (Jiang, 2017) with a properly chosen bandwidth."}, {"heading": "4.2. Estimation of the Threshold", "text": "When V\u03b7 is quasi-concave, a key consequence is that its gradient with respect to the threshold suffices to provide ascent direction information. We leverage this consequence, and summarize a simple binary search algorithm based on the sign of V \u2032\u03b7(\u03b4,P) in Algorithm 2.\nAlgorithm 2 Binary search for the optimal threshold 1: Input: Training sample {Xi, Yi}ni=1, utility measure U , conditional probability estimator \u03b7\u0302, tolerance 0.\n2: \u03b4` = 0; \u03b4r = 1; 3: while |\u03b4` \u2212 \u03b4r| \u2265 0 do 4: Evaluate s = sign ( V \u2032\u03b7\u0302(\u03b4,Pn) ) ; 5: if s \u2265 0 then 6: \u03b4` = \u03b4`+\u03b4r 2 ; 7: else 8: \u03b4r = \u03b4`+\u03b4r 2 ;\n9: end if 10: end while 11: Output: \u03b4`+\u03b4r2 .\nIn the next section, we then analyze the rates of convergence for the excess generalization error of the plug-in classifier learned from Algorithm 1, and with threshold estimated via Algorithm 2."}, {"heading": "5. Statistical Analysis Consequence: Rates of Convergence", "text": "We next analyze the convergence rate of the excess utility. As we will show, the rates of convergence depend on three quantities: the noise level of the data distribution, the convergence rate of the conditional probability function, and the convergence rate of the threshold. We start by introducing some assumptions.\nWe assume that the estimator of the conditional probability of response satisfies the following condition. Assumption 4. Let Sn denote a sample set of size n, and \u03b7Sn denote the conditional probability estimator learnt from Sn. Then, for some absolute constants c1, c2 > 0, the conditional probability estimator satisfies the following condition:\nsup Sn\nP (|\u03b7Sn(x)\u2212 \u03b7(x)| \u2265 ) \u2264 c1 exp(\u2212c2 an 2) a.e.\nThe convergence rate also depends on the noise in the training labels, which is typically captured via the probability mass near the Bayes optimal threshold. Here we generalize the classical margin assumption (sometimes also called low noise assumption) of Audibert et al. (2007), developed for the accuracy metric, to the case where the optimal threshold is not a fixed constant 12 :\nAssumption 5. For some functionC0(\u03b4\u2217) > 0 that depends on the threshold \u03b4\u2217, there exists an \u03b1 \u2265 0 such that\nPX(0 < |\u03b7(X)\u2212 \u03b4\u2217| \u2264 t) \u2264 C0(\u03b4\u2217) t\u03b1.\nThe assumption characterizes the behavior of the regression function in the vicinity of the optimal threshold \u03b4\u2217. The case \u03b1 = 0 bounds the probability by a constant potentially larger than one, and is trivially satisfied. The other extreme case \u03b1 =\u221e is most advantageous for classification, since in this case the regression function \u03b7 is bounded away from the threshold.\nIn cases where the threshold is not an absolute constant (such as 1/2), it has to be estimated from data. We make the following assumption on its convergence rate.\nAssumption 6. Given a conditional probability estimate \u03b7\u0302 learned from an independent data source, the estimator \u03b4\u0302n of the threshold, from a sample set of size n, satisfies the following condition, for some absolute constant c3 > 0:\nP (\u2223\u2223\u2223U(sign (\u03b7\u0302 \u2212 \u03b4\u2217))\u2212 U(sign(\u03b7\u0302 \u2212 \u03b4\u0302))\u2223\u2223\u2223 \u2265 b\u22121n ) \u2264 n\u2212c3 .\nNote that Assumption 6 allows the rate bn to in turn depend on \u03b7\u0302, or more specifically, EX |\u03b7(X)\u2212 \u03b7\u0302(X)|. Moreover, it does not necessarily require that \u03b4\u0302 converge to \u03b4\u2217, only that their corresponding utility function values be close.\nArmed with these largely notational assumptions, we can now provide the rate for the overall data-splitting two-step plug-in classifier described in Algorithm 1:\nTheorem 5.1. Suppose Assumption 1 and 2 hold, and further that Assumptions 4 and 6 hold for some \u03b7\u0302 and \u03b4\u0302. Let U\u2217 = U(sign (\u03b7 \u2212 \u03b4\u2217) ,P) be the Bayes optimal utility. If we split the data as n1 = n2 = n2 , then with probability greater than 1\u2212 n\u2212c4 :\nU\u2217 \u2212 U ( sign ( \u03b7\u0302 \u2212 \u03b4\u0302 ) ,P ) \u2264 c5 max { a \u2212 1+\u03b12 n , b \u22121 n } .\nwhere c4, c5 > 0 are absolute constants."}, {"heading": "5.1. Key Lemmas", "text": "We provide a detailed proof of the theorem in the Appendix, but provide brief vignettes via some key lemmas that also provide some additional insight into the statistical analysis. A key tool when analyzing traditional binary classification is to turn the excess risk into an expectation of the absolute deviation of conditional probability from the threshold 12 . We show in the following lemma that a similar result holds with general optimal threshold:\nLemma 5.1. Let Cn and C\u2217 be the vectorized confusion matrices associated with fn = sign (\u03b7n \u2212 \u03b4\u2217)\nand f\u2217 = sign (\u03b7 \u2212 \u03b4\u2217) respectively, where \u03b4\u2217 is the threshold for the Bayes optimal classifier. Denote CG := \u2207G(C\u2217)T (1,\u22121,\u22121, 1), and CH := maxf \u2016\u22072G(C(f))\u2016op, where \u2016 \u00b7 \u2016op refers to the operator norm of a matrix. If for some constant c6, an \u2265 c6 ( CH\nCG min{\u03b4\u2217,1\u2212\u03b4\u2217}\n)2 , then\nG(C\u2217)\u2212 G(Cn) \u2265 1\n2 CGE[|\u03b7 \u2212 \u03b4\u2217|1(fn 6= f\u2217)],\nG(C\u2217)\u2212 G(Cn) \u2264 3\n2 CGE[|\u03b7 \u2212 \u03b4\u2217|1(fn 6= f\u2217)].\nThis lemma thus helps us control the excess utility via the error of the conditional probability estimator \u03b7\u0302 \u2212 \u03b7. Armed with this result, and additionally using Assumption 4 on the convergence rate of the conditional probability estimator, we can then show that the excess utility converges at the rate O(a\u2212 1+\u03b1 2 n ):\nLemma 5.2. Suppose that Assumptions 4 and 5 are satisfied, and that the Bayes optimal classifier is f\u2217 = sign (\u03b7 \u2212 \u03b4\u2217). Then there exists a constant c7 > 0 which depend on G and C(f\u2217), such that U(sign (\u03b7 \u2212 \u03b4\u2217))\u2212 U(sign (\u03b7n \u2212 \u03b4\u2217)) \u2264 c7a \u2212 1+\u03b12 n .\nLemma 5.2 describes the classification error rate when the optimal threshold is known. Stitching this together with Assumption 6 on the convergence rate of the threshold estimator can then be shown to yield the statement of Theorem 5.1."}, {"heading": "5.2. Risk Bound for the Plugin Classifier from", "text": "Algorithm 2\nPrior work on threshold estimation for plug-in classifiers have ranged over brute-force search (Koyejo et al., 2014) with no rates of convergence, level-set based methods (Parambath et al., 2015) for the specific class of linear fractional metrics, and Frank-Wolfe based methods (Narasimhan et al., 2015b) for the specific class of concave performance metrics. However these estimators, in addition to focus on specific performance metrics, are only able to achieve a convergence rate of O(max{E\u2016\u03b7\u0302(X) \u2212 \u03b7(X)\u20161, 1/ \u221a n}). This entails that even if when the conditional probability estimator has a fast convergence rate, the final convergence rate for these estimators will still be bounded by O(1/ \u221a n). In this section we show that our simple threshold search procedure in Algorithm 2 achieves a fast O(1/n) or O(1/an) rate of convergence by leveraging our analysis from Section 3.\nLemma 5.3. Assume that Assumptions 1, 2, 5 hold, and that the confusion-matrix function G corresponding to the performance measure U satisfies the same conditions as in Proposition 3.1. Let \u03b4\u0302 denote the output of Algorithm 2 with sample size n and tolerance \u03c4 = lognn , and \u03b7\u0302 denote\na conditional probability estimator satisfying Assumption 4 obtained on an independent sample set of size n. Denoting n\u0303 = min{n, an}, we then have that the rate bn in Assumption 6 satisfies: bn = log n\u0303n\u0303 .\nAn immediate corollary then gives the excess risk for the plug-in classifier.\nCorollary 5.1. Suppose Assumption 3 holds. If \u03c4 = logn n , n1 = n2 = n 2 , then there exist constants c8, c9 > 0, such that with probability at least 1\u2212min{n, an}\u2212c8 ,\nU(f\u2217,P)\u2212 U(f\u0302 ,P) \u2264 c9 max { log n\nn , log an an , a \u2212 1+\u03b12 n\n} ."}, {"heading": "6. Explicit Rates for Specific Conditional Probability Models", "text": "In this section, we analyze two special cases where we can achieve explicit rate of convergence for the conditional probability estimation. For the first example, we consider the Gaussian generative model. We will show that the rate of convergence for the excess utility obtained in Theorem 5.1 is O( lognn ) in this case. The second example is for nonparametric kernel estimators when the conditional probability function satisfies certain smoothness assumption."}, {"heading": "6.1. Gaussian Generative Model", "text": "Consider two Gaussian distributions with the same variance, without loss of generality we assume the covariance matrix is identity Id for both classes. We define an asymmetric mixture of two Gaussians indexed by the centers and mixing weights.\nP\u00b5,\u03ba : P (Y = 1) = \u03ba, P (Y = 0) = 1\u2212 \u03ba, X|Y = 1 \u223c N (\u00b5\n2 , Id\n) , X|Y = 0 \u223c N ( \u2212\u00b5\n2 , Id\n) . (5)\nAs stated in Section 4, we can compute the conditional probability and show that it can be fitted with a logistic regression model. Next we present results related to the key quantities in Theorem 5.1: an and \u03b1.\nLemma 6.1. Model defined in Eq. (5) with maximum likelihood estimator satisfies Assumption 4 with an = n.\nThe following lemma specifies the margin assumption parameter for the above model.\nLemma 6.2. The Gaussian generative model defined as in Eq. (5), satisfies Assumption 5 with \u03b1 = 1.\nCombining this result with Theorem 5.1 gives us the following corollary.\nCorollary 6.1. Assume Assumptions 1-5 hold, P is generated from Eq. (5). Let f\u0302 be the output of Algorithm 1 with\n\u03b7\u0302 estimated by MLE of logistic regression. We have with probability tending to 1, U(f\u2217,P)\u2212U(f\u0302 ,P) = O ( logn n ) .\nFor Gaussian generative models, fast rates of O( 1n ) are only known for 0-1 loss (Li et al., 2015). The logarithm factor can be further removed under 0-1 loss, or other cases when the threshold is known, as one can apply Lemma 5.2 with \u03b1 = 1 and get exactly the same rate as in Li et al. (2015). Corollary 6.1 generalizes this result for a much broader class of utility functions, when the threshold is unknown and estimated from data."}, {"heading": "6.2. \u03b2-Ho\u0308lder Densities", "text": "When the conditional probability function belongs to the \u03b2-Ho\u0308lder class as defined in Definition 4.1, we have the following lemma on the convergence rates of \u03b7n.\nLemma 6.3. For \u03b2-Ho\u0308lder conditional probability functions, there exists estimators such that Assumption 4 holds with an = n 2\u03b2 2\u03b2+d .\nExamples of such estimators include locally polynomial estimators (Audibert et al., 2007), or kernel (conditional) density estimators (Jiang, 2017). Combined with Theorem 5.1 we have the following corollary.\nCorollary 6.2. Assume Assumptions 1-5 hold and P be a distribution where P (Y = 1|X) belongs to \u03b2-Ho\u0308lder class. With locally polynomial estimators (Audibert et al., 2007) or kernel (conditional) density estimators (Jiang, 2017), we have: U(f\u2217,P)\u2212 U(f\u0302) = O ( n\u2212 (min{\u03b1,1}+1)\u03b2 2\u03b2+d ) .\nThe convergence rate obtained in Corollary 6.2 is faster than O( 1\u221a\nn ) if \u03b2 > max{ d2\u03b1 , d 2}. It is worth pointing out that the fast rate is obtained via a trade-off between the parameter \u03b1 and \u03b2: to have a very smooth conditional probability function \u03b7, i.e., a large value of \u03b2, it cannot deviate from the critical level very abruptly, hence \u03b1 has to be small."}, {"heading": "7. Conclusion", "text": "We study Bayes optimal classification for general performance metrics. We derive the form of the Bayes optimal classifier, provide practical algorithms to estimate this Bayes optimal classifier, and provide novel analysis of classification error with respect to general performance metrics, and in particular show our estimators are not only consistent but have fast rates of convergence. We also provide corollaries of our general results for some special cases, such as when the inputs are drawn from a Gaussian mixture generative models, or when the conditional probability function lies in a Ho\u0308lder space, explicitly proving fast rates under mild regularity conditions."}, {"heading": "Acknowledgements", "text": "P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720, DMS-1264033, and PNC via the PNC Center for Financial Services Innovation."}], "year": 2018, "references": [{"title": "Fast learning rates for plug-in classifiers", "authors": ["References Jean-Yves Audibert", "Alexandre B Tsybakov"], "venue": "The Annals of statistics,", "year": 2007}, {"title": "Online f-measure optimization", "authors": ["R\u00f3bert Busa-Fekete", "Bal\u00e1zs Sz\u00f6r\u00e9nyi", "Krzysztof Dembczynski", "Eyke H\u00fcllermeier"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Evaluation of classifiers for an uneven class distribution problem", "authors": ["Sophia Daskalaki", "Ioannis Kopanas", "Nikolaos Avouris"], "venue": "Applied artificial intelligence,", "year": 2006}, {"title": "A probabilistic theory of pattern recognition, volume 31", "authors": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi", "G\u00e1bor Lugosi"], "venue": "Springer Science & Business Media,", "year": 2013}, {"title": "Evaluation measures of the classification performance of imbalanced data sets", "authors": ["Qiong Gu", "Li Zhu", "Zhihua Cai"], "venue": "In International Symposium on Intelligence Computation and Applications,", "year": 2009}, {"title": "A maximum expected utility framework for binary sequence labeling", "authors": ["Martin Jansche"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "year": 2007}, {"title": "Uniform convergence rates for kernel density estimation", "authors": ["Heinrich Jiang"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "A support vector method for multivariate performance measures", "authors": ["Thorsten Joachims"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "year": 2005}, {"title": "Online and stochastic gradient methods for nondecomposable loss functions", "authors": ["Purushottam Kar", "Harikrishna Narasimhan", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Learning without default: A study of one-class classification and the low-default portfolio problem", "authors": ["Kenneth Kennedy", "Brian Mac Namee", "Sarah Jane Delany"], "venue": "In Artificial Intelligence and Cognitive Science,", "year": 2009}, {"title": "Consistent binary classification with generalized performance metrics", "authors": ["Oluwasanmi O Koyejo", "Nagarajan Natarajan", "Pradeep K Ravikumar", "Inderjit S Dhillon"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Addressing the curse of imbalanced training sets: one-sided selection", "authors": ["Miroslav Kubat", "Stan Matwin"], "venue": "In ICML,", "year": 1997}, {"title": "Evaluating and optimizing autonomous text classification systems", "authors": ["David D Lewis"], "venue": "In Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval,", "year": 1995}, {"title": "Fast classification rates for high-dimensional gaussian generative models", "authors": ["Tianyang Li", "Adarsh Prasad", "Pradeep K Ravikumar"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Smooth discrimination analysis", "authors": ["Enno Mammen", "Alexandre B Tsybakov"], "venue": "The Annals of Statistics,", "year": 1999}, {"title": "Introduction to information retrieval, volume 1", "authors": ["Christopher D Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "year": 2008}, {"title": "On the statistical consistency of algorithms for binary classification under class imbalance", "authors": ["Aditya Menon", "Harikrishna Narasimhan", "Shivani Agarwal", "Sanjay Chawla"], "venue": "In International Conference on Machine Learning,", "year": 2013}, {"title": "Optimizing f-measure: A tale of two approaches", "authors": ["Ye Nan", "Kian Ming Chai", "Wee Sun Lee", "Hai Leong Chieu"], "venue": "arXiv preprint arXiv:1206.4625,", "year": 2012}, {"title": "On the statistical consistency of plug-in classifiers for non-decomposable performance measures", "authors": ["Harikrishna Narasimhan", "Rohit Vaish", "Shivani Agarwal"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Optimizing non-decomposable performance measures: A tale of two classes", "authors": ["Harikrishna Narasimhan", "Purushottam Kar", "Prateek Jain"], "venue": "In 32nd International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Consistent multiclass algorithms for complex performance measures", "authors": ["Harikrishna Narasimhan", "Harish Ramaswamy", "Aadirupa Saha", "Shivani Agarwal"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "authors": ["Andrew Y Ng", "Michael I Jordan"], "venue": "In Advances in neural information processing systems,", "year": 2002}, {"title": "Theory of optimizing pseudolinear performance measures: Application to f-measure", "authors": ["Shameem A Puthiya Parambath", "Nicolas Usunier", "Yves Grandvalet"], "venue": "arXiv preprint arXiv:1505.00199,", "year": 2015}, {"title": "Measuring mass concentrations and estimating density contour clusters-an excess mass approach", "authors": ["Wolfgang Polonik"], "venue": "The Annals of Statistics,", "year": 1995}, {"title": "Foundation of evaluation", "authors": ["Cornelis Joost Van Rijsbergen"], "venue": "Journal of Documentation,", "year": 1974}, {"title": "Class imbalance, redux", "authors": ["Byron C Wallace", "Kevin Small", "Carla E Brodley", "Thomas A Trikalinos"], "venue": "In Data Mining (ICDM),", "year": 2011}, {"title": "Youden. Index for rating diagnostic tests", "authors": ["J William"], "venue": "Cancer, 3(1):32\u201335,", "year": 1950}, {"title": "Beyond fano\u2019s inequality: bounds on the optimal f-score, ber, and cost-sensitive risk and their implications", "authors": ["Ming-Jie Zhao", "Narayanan Edakunni", "Adam Pocock", "Gavin Brown"], "venue": "The Journal of Machine Learning Research,", "year": 2013}], "id": "SP:75396205bdf530018392f66c6da9a2d768cb1356", "authors": [{"name": "Bowei Yan", "affiliations": []}, {"name": "Oluwasanmi Koyejo", "affiliations": []}, {"name": "Kai Zhong", "affiliations": []}, {"name": "Pradeep Ravikumar", "affiliations": []}], "abstractText": "Complex performance measures, beyond the popular measure of accuracy, are increasingly being used in the context of binary classification. These complex performance measures are typically not even decomposable, that is, the loss evaluated on a batch of samples cannot typically be expressed as a sum or average of losses evaluated at individual samples, which in turn requires new theoretical and methodological developments beyond standard treatments of supervised learning. In this paper, we advance this understanding of binary classification for complex performance measures by identifying two key properties: a so-called Karmic property, and a more technical threshold-quasiconcavity property, which we show is milder than existing structural assumptions imposed on performance measures. Under these properties, we show that the Bayes optimal classifier is a threshold function of the conditional probability of positive class. We then leverage this result to come up with a computationally practical plug-in classifier, via a novel threshold estimator, and further, provide a novel statistical analysis of classification error with respect to complex performance measures.", "title": "Binary Classification with Karmic, Threshold-Quasi-Concave Metrics"}