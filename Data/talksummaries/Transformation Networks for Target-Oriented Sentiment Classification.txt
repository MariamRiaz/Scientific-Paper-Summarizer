1	61	For example, in the sentence “I am pleased with the fast log on, and the long battery life”, the user mentions two targets “log on” and “better life”, and expresses positive sentiments over them.
2	45	The task is usually formulated as predicting a sentiment category for a (target, sentence) pair.
5	32	In these works, the attention weight based combination of word-level features for classification may introduce noise and downgrade the prediction accuracy.
6	19	For example, in “This dish is my favorite and I always get it and never get tired of it.”, these approaches tend to involve irrelevant words such as “never” and “tired” when they highlight the opinion modifier “favorite”.
7	12	To some extent, this drawback is rooted in the attention mechanism, as also observed in machine translation (Luong et al., 2015) and image captioning (Xu et al., 2015).
9	16	By this token, Convolutional Neural Networks (CNNs)—whose capability for extracting the informative n-gram features (also called “active local features”) as sentence representations has been verified in (Kim, 2014; Johnson and Zhang, 2015)— should be a suitable model for this classification problem.
10	24	However, CNN likely fails in cases where a sentence expresses different sentiments over multiple targets, such as “great food but the service was dreadful!”.
13	110	We propose a new architecture, named TargetSpecific Transformation Networks (TNet), to solve the above issues in the task of target sentiment classification.
25	59	• A novel Target-Specific Transformation component is proposed to better integrate target information into the word representations.
26	49	• A context-preserving mechanism is designed to forward the context information into a deep transformation architecture, thus, the model can learn more abstract contextualized word features from deeper networks.
31	57	The CPT layer incorporates the target information into the word representations via a novel Target-Specific Transformation (TST) component.
32	16	CPT also contains a contextpreserving mechanism, resembling identity mapping (He et al., 2016a,b) and highway connection (Srivastava et al., 2015a,b), allows preserving the context information and learning more abstract word-level features using a deep network.
33	23	The top most part is a position-aware convolutional layer which first encodes positional relevance between a word and a target, and then extracts informative features for classification.
47	13	This strategy may be inappropriate in some cases because different target words usually do not contribute equally.
48	31	For example, in the target “amd turin processor”, the word “processor” is more important than “amd” and “turin”, because the sentiment is usually conveyed over the phrase head, i.e.,“processor”, but seldom over modifiers (such as brand name “amd”).
49	64	Ma et al. (2017) attempted to overcome this issue by measuring the importance score between each target word representation and the averaged sentence vector.
52	11	We first employ another BiLSTM to obtain the target word representations hτ ∈ Rm×2dimh : hτj = [ −−−−→ LSTM(xτj ); ←−−−− LSTM(xτj )], j ∈ [1,m].
60	28	This strategy preserves context information by directly feeding the features before the transformation to the next layer.
66	25	Lossless Forwarding introduces the context information by directly adding back the contextualized features to the transformed features, which raises a question: Can the weights of the input and the transformed features be adjusted dynamically?
68	34	Similar to the gate mechanism in RNN variants (Jozefowicz et al., 2015), Adaptive Scaling introduces a gating function to control the passed proportions of the transformed features and the input features.
79	11	Then, we use v to help CNN locate the correct opinion w.r.t.
81	31	11, the words close to the target will be highlighted and those far away will be downgraded.
88	26	As shown in Table 1, we evaluate the proposed TNet on three benchmark datasets: LAPTOP and REST are from SemEval ABSA challenge (Pontiki et al., 2014), containing user reviews in laptop domain and restaurant domain respectively.
89	13	We also remove a few examples having the “conflict label” as done in (Chen et al., 2017); TWITTER is built by Dong et al. (2014), containing twitter posts.
97	22	For out-of-vocabulary words, we randomly sample their embeddings from the uniform distribution U(−0.25, 0.25), as done in (Kim, 2014).
115	20	It shows that the integration of target information into the word-level representations is crucial for good performance.
132	32	As our TNet involves multiple CPT layers, we investigate the effect of the layer number L. Specifically, we conduct experiments on the held-out training data of LAPTOP and vary L from 2 to 10, increased by 2.
145	51	Our TNet variants can predict target sentiment more accurately than RAM and BILSTM-ATT-G in the transitional sentences such as the first sentence by capturing correct trigram features.
149	13	For example, “long” in the fifth sentence is negative for “startup time”, while it could be positive for other targets such as “battery life” in the sixth sentence.
150	17	The sentiment of target-specific opinion word is conditioned on the given target.
