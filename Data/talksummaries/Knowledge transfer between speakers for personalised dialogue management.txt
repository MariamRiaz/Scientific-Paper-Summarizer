0	32	Partially observable Markov decision processes (POMDP) (Young et al., 2013) are a popular framework to model dialogue management as a reinforcement learning (RL) problem.
1	20	In a POMDP, a state tracker (Thomson and Young, 2010)(Williams, 2014) maintains a distribution over possible user goals (states), called the belief state, and RL methods (Sutton and Barto, 1998) are used to optimize a metric called cumulative reward, a score that combines dialogue success rate and dialogue length.
7	28	Taking this idea into dialogue management, if a similarity metric is defined between different speakers, this metric can be used to select which data from the source speakers is used to train the model, and even to weight the influence of the data from each speaker in the model.
12	61	This paper investigates knowledge transfer between speakers in the context of a spoken environmental control system personalised for speakers with dysarthria (Christensen et al., 2013), where the ASR is adapted as speaker specific data is gathered (Christensen et al., 2012), thus improving the ASR performance with usage.
14	23	Section 3 presents the experimental setup of the environmental control system and the different dysarthric simulated users, as well as the different features used to define the speaker similarities.
16	98	The objective of a POMDP based dialogue manager is to find the policy π(b) = a that maximizes the expected cumulative reward ci defined as the sum of immediate rewards from time step i until the dialogue is finished, where a ∈ A is the action taken by the manager, and the belief state b is a probability distribution over a discrete set of states S .
21	24	1 −γt−1  If the random variables qt are assumed to have a joint Gaussian distribution with zero mean and ∆Q(bi, ai) ∼ N (0, σ2), the system can be modelled as a GP (Rasmussen and Williams, 2005), with the covariance matrix determined by a kernel function defined independently over the belief and the action space (Engel et al., 2005): ki,j = k((bi, ai), (bj , aj)) = kb(bi,bj)ka(ai, aj) (4) To simplify the notation, from now on xi = (bi, ai) will be defined as each belief-action point, and KY,Y ′ as the matrix of size |Y| × |Y′| whose elements are computed by the kernel function (eq.
27	69	If the set of belief-action points Xt is redefined2 as Zt where zi = (bi, ai,bi+1, ai+1), with bi+1 and ai+1 set to any default values if ai is a terminal action, a kernel function between 2 temporal difference points can be defined as: ktdi,j = k td(zi, zj) = ktd((bi, ai,bi+1, ai+1), (bj , aj ,bj+1, aj+1)) = (ki,j + γiγjki+1,j+1 − γiki+1,j − γjki,j+1) (6) where ki,j is the kernel function in the beliefaction space (eq.
30	58	In the same way, when this kernel is used to compute the covariance vector between a new test point and the set Zt, as the new point x∗ = (b∗, a∗) lies in the belief-action space, it is redefined as z∗ = (b∗, a∗,b∗+1, a∗+1) with b∗+1 and a∗+1 set to default values.
37	29	Redefining the belief-action set of points Xt as the set of temporal difference points Zt also simplifies the selection of data points (e.g. to select inducing points in sparse models), because the dependency between consecutive points is well defined.
42	31	8, the policy π(b) = a can be computed as the action a that maximizes the Q-function from the current belief state b∗, but in order to avoid getting stuck in a local optimum, an exploration-exploitation approach should be taken.
47	36	In the context of this paper the different tasks will be dialogues with different speakers, and three points of transfer learning will be addressed: • How to transfer the knowledge • In the case of multiple source speakers, which data to transfer, and • How to weight data from different sources.
49	37	The most straightforward way to transfer the data in GP-RL is to initialise the set of temporal difference points Zt of the GP with the source points and then continue updating it with target data points as they are gathered through interaction.
55	22	Gašić et al. (2013 a) proposes to use the source points to train a prior GP, and use its posterior as mean function for the GP trained with the target points.
59	22	A third approach combines the two previous methods, using a portion of the transfer points to train a GP for the prior mean function, while the rest is used to initialise the set Zt of the GP that will be updated with target points.
66	23	The most straightforward way is to select the most similar points to the speaker from the transferred points.
72	31	4) by multiplying it by a new kernel in the speaker space ks as: kexti,j = k((bi, ai, si), (bj , aj , sj)) = kb(bi,bj)ka(ai, aj)ks(si, sj) (12) By adding this extra space to the data points, the covariance between points will not only depend on the similarity between points in the belief-action space, but also in the speaker space, reducing the covariance between two points that lie in different parts of the speaker space.
76	22	The system has a vocabulary of 36 commands and is organised in a tree setup where each node in the tree represents either a device (e.g. “TV”), a property of that device (e.g. “channel”), or actions that trigger some change in one of the devices (e.g. “one”, child of “channel”, will change the TV to channel one).
90	31	Each non-terminal node in the tree is modelled as an independent POMDP where the state set S is the set of possible goals of the node and the action setA is the set of actions associated with each goal plus an “ask” action, which requests the user to repeat his last command.
135	25	The performance of DTC-int is way below the other two metrics, suggesting that the information given by intelligibility assessments is a weak feature for source speaker selection (as it is done by humans, it might be very noisy).
139	54	DTC-randspk selects the ordering of the speakers from whom the data is transferred at random, and has a much worse performance than the similarity based method, but DTC-allspk selects the 1000 source points from all the speakers, selecting 1000 points at random from the pool of 4200 points and, as it can be seen, the reward obtained by this method is slightly better than with DTC-iv, even if the success rate is lower.
145	98	This might be because weighting the data does a kind of data selection, as the data points from source speakers closer to the target will have more influence than the further ones, while transferring points from all the speakers covers a bigger part of the belief-action space.
148	107	Finally iv-allspk-hyb plots the performance of the hybrid model when selecting the data from all the speakers and weighting it with the i-vector based similarity.
149	25	Even if it is computationally cheaper, it outperforms iv-allspk after 100 dialogues, suggesting that with a good similarity metric and data selection method, the hybrid model in section 3.3 is the best option to take.
150	28	When transferring knowledge between speakers in a GP-RL based policy, weighting the data by using a similarity metric between speakers, and to a lesser extent, selecting the data using this similarity, improves the performance of the dialogue manager.
151	51	By defining a kernel between temporal difference points and interpreting the Q-function as a GP regression problem where data points are in the TD space, sparse methods that allow the selection of the subset of inducing points such as DTC can be applied.
153	29	We showed that using part of the transferred data to train a prior GP for the mean function, and the rest to initialize the set of points of the GP, improves the performance of each of these approaches.
157	24	More computationally efficient ways to transfer the data could be studied.
158	35	Of the three metrics based on speaker features tested (speaker intelligibility, i-vectors and ASR accuracy), i-vectors outperformed the rest.
159	21	This suggest that i-vectors are a potentially good feature for speaker specific dialogue management and could be used in other tasks such as state tracking.
