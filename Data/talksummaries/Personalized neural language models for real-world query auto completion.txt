1	42	In practice, most search engines today use query auto completion (QAC) systems, consisting of suggesting queries as users type in the search box (Fiorini et al., 2017).
3	18	Historically, the query prediction task has been addressed by relying on query logs, particularly the popularity of past queries (BarYossef and Kraus, 2011; Lu et al., 2009).
4	19	The idea is to rely on the wisdom of the crowd, as popular queries matching a typed prefix are more likely to be the user’s intent.
10	16	In this work, we propose to improve the state-of-the-art approaches in neural QAC by integrating personalization and time sensitivity information as well as addressing current MPC limitations by diversifying the suggestions, thus approaching a production-ready architecture.
28	40	Recurrent Neural Network The difficulty of predicting queries given a prefix is that the number of candidates explodes as the query becomes longer.
30	46	However, they also introduce the vanishing gradient problem during backpropagation, preventing them from learning long-term dependencies.
31	30	Both gated recurrent units (GRU) (Cho et al., 2014) and long-short term memory cells (LSTMs) solve this limitation — albeit with a different approach — and are increasingly used.
33	18	GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).
34	39	Word embedded character-level Neural Language Model The main novelty in (Park and Chiba, 2017) is to combine a character-level neural language model with a word-embedded space character.
36	36	Therefore, they encode text sequences using one-hot encoding of characters, character embedding and pre-trained word embedding (using word2vec (Mikolov et al., 2013)) of the previous word when a space character is encountered.
41	16	U is a column matrix and a user u ∈ U is characterized by the union of words in their k past queries, i.e. Qu = ∪ki=1qi.
42	17	The objective is to reduce, for each user, the vocabulary used in their queries to a vector of a dimensionality d of choice, or Qu → Rd.
46	52	The model is trained by maximizing the probability of predicting the user u given the word wi, i.e.: 1 |U | ∑ u∈U ∑ wi∈Qu log P (u|wi).
47	28	(1) The resulting vectors are stored for each user ID and are used as input for the neural net (NN) (see Architecture section).
49	20	For this reason, we propose to integrate time features in the language model.
66	73	This approach has a high chance to output a locally optimal sequence and a common alternative is to use a beam search instead.
90	27	We implemented the method in (Park and Chiba, 2017) and used their best-performing model as a baseline.
91	58	We also compare our results to the standard MPC (Bar-Yossef and Kraus, 2011).
101	13	When we enrich the language model with user information, it becomes better for seen queries (+1.9%) while being about as fast.
105	25	We noticed that for Web search, MPC performs extremely well and is computationally cheap (0.24 seconds).
107	22	Since identifying if a query has been seen or not is done in constant time, we route the query either to MPC or to NQACUT and we note the overall performance as NQACUT+MPC.
108	22	This method provides a significant improvement over NQLM (+6.7%) overall while being faster on average.
109	17	Finally, appending NQACUT ’s results to MPC’s and reranking the list with LambdaMART provides the best results on this dataset, but at the expense of greater computational cost (+60%).
111	28	This shows the potential difficulties associated with real-world systems, which particularly occur in specialized domains.
114	21	This is even more true when the target queries are diverse as in specialized domains (Islamaj Dogan et al., 2009; Névéol et al., 2011).
122	22	Still, user information helps significantly for seen queries (+23%), probably because some users frequently check the same queries to keep up-to-date.
123	38	Time sensitivity seems to help significantly unseen queries (+21%) while significantly hurting the quality for seen queries (-47%).
127	19	First, MPC performs very well on seen queries for Web searches and it should be used on them.
130	25	On a specialized domain, the task is more challenging: fast approaches like MPC perform too poorly while more elaborate approaches do not meet production requirements.
136	140	Finally, we found that a lot more data was needed for the biomedical domain than for general Web search.
