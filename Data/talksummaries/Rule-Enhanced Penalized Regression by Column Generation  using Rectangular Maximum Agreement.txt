27	37	We solve the RMA problem by a similar branch-and-bound method procedure, implemented using parallel computing techniques.
28	36	To make our notation below more concise, we let X denote the matrix whose rows are X>1 , .
31	62	Let K be a set of pairs (a, b) ∈ Rn × Rn with a ≤ b, constituting a catalog of all the possible rules of the form (1) that we wish to be available to our regression model.
32	27	The set K will typically be extremely large: restricting each aj and bj to values that appear as xij for some i, which is sufficient to describe all possible distinct behaviors of rules of the form (1) on the dataset X , there are still∏n j=1 `j(`j + 1)/2 ≥ 3n possible choices for (a, b), where `j = | ⋃m i=1{xij}| is the number of distinct values for xij .
36	15	, βn) ∈ Rn and γ ∈ R|K|, let fβ0,β,γ( · ) denote the predictor function in (2).
38	42	For p = 2 and C = E > 0, this model is essentially the classic LASSO as originally proposed by Tibshirani (1996).
43	18	In either case, there are 2m constraints (other than nonnegativity), but the number of variables is 1 +m+ 2n+ 2 |K|.
44	29	Because of this potentially unwieldy number of variables, we propose to solve (4) by using the classical technique of column generation, which dates back to Ford & Fulkerson (1958) and Gilmore & Gomory (1961); see for example Section 7.3 of Griva et al. (2009) for a recent textbook treatment.
45	13	In brief, column generation cycles between solving two optimization problems, the restricted master problem and the pricing problem.
48	44	For each rule k ∈ K, these Lagrange multipliers yield respective reduced costs rc[γ+k ], rc[γ − k ] for the variables γ + k , γ − k that are in the master problem, but not the restricted master.
60	14	The rectangular maximum agreement (RMA) problem is max ∣∣w(Ω+ ∩ CvrX(a, b))− w(Ω− ∩ CvrX(a, b))∣∣ s.t.
85	31	For small positive values of δ, the set of boxes Kδ(X) excludes those corresponding to rules that “cut” between very closely spaced observations.
86	43	In this and the following two subsections, we describe the key elements of our branch-and-bound procedure for solving the RMA problem, assuming that the data X have already been preprocessed as above.
91	25	The root problem R of the branch-and-bound tree is R = (0, ` − 1,0, ` − 1), where where ` ∈ Nn is as output from Algorithm 1, and 0 and 1 respectively denote the vectors (0, 0, .
92	29	In branch-and-bound methods, the bounding function provides an upper bound (when maximizing) on the best possible objective value in the region of the search space corresponding to a subproblem.
103	15	In our case, branching a subproblem P = (a, a, b, b) involves choosing an explanatory variable j ∈ {1, .
107	16	,min{aj , bj} − 1 } , in which case the box cannot lie below v and we split P into two children based on the disjunction that either aj ≤ v (the box straddles v) or aj ≥ v + 1 (the box is above v).
116	18	To make this process as efficient as possible, we have developed specialized data structures for manipulating equivalence classes, and we analyze the branching possibilities in a particular order.
119	15	The pseudocode in Algorithm 2 describes our full REPR column generation procedure for solving (4), using the RMA preprocessing and branch-and-bound methods described above to solve the pricing problem.
131	20	, t} with zl > E + θ do 10: K ′ ← K ′ ∪ {kl} 11: end for 12: end for 13: return (β0, β := β+ − β−,K ′, γ := γ+ − γ−) commercial optimizer (Gurobi Optimization, 2016) to solve the restricted master problems.
132	27	We implemented the RMA algorithm using using the PEBBL C++ class library (Eckstein et al., 2015), an open-source C++ framework for parallel branch and bound.
133	13	PEBBL employs MPIbased parallelism (Gropp et al., 1994).
136	20	For preliminary testing of REPR, we selected 8 datasets from the UCI repository (Lichman, 2013), choosing small datasets with continuous response variables.
137	42	The first four columns of Table 1 summarize the number of observations m, the number of attributes n, and the maximum number of distinguishable box-based rules |K0(X)| for these data sets.
140	51	We found this rule of thumb to work well in pracice, but it likely merits further study.
154	56	Interestingly, neither of these figures shows appreciable evidence of overfitting by REPR, even when large numbers of rules are incorporated into the model.
156	72	REPR seems to outperform the other methods in predicting extreme response values, although it is somewhat worse than the other methods at predicting non-extreme values for MACHINE.
157	49	The last two columns of Table 1 show, for a 16-core Xeon E5-2660 workstation, REPR’s average total run time per data partition and the average number of search node per invocation of RMA.
162	119	Improved preprocessing may also prove helpful: judicious normalization of the input data (X, y) should assist in finding good parameter choices, and we are also working on more sophisticated discretization technique for preprocessing the RMA solver input, as well as branch selection heuristics that are more efficient for large `j .
163	112	It would be interesting to see how well REPR performs if the pricing problems are solved less exactly.
